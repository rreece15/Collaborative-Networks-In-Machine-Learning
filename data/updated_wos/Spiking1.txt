FN Clarivate Analytics Web of Science
VR 1.0
PT J
AU Kim, J
   Kim, H
   Huh, S
   Lee, J
   Choi, K
AF Kim, Jaehyun
   Kim, Heesu
   Huh, Subin
   Lee, Jinho
   Choi, Kiyoung
TI Deep neural networks with weighted spikes
SO NEUROCOMPUTING
DT Article
DE Spiking neural network; Weighted spike; Supervised learning
ID CLASSIFICATION; COMMUNICATION; PLASTICITY; ALGORITHM; RESUME
AB Spiking neural networks are being regarded as one of the promising alternative techniques to overcome the high energy costs of artificial neural networks. It is supported by many researches showing that a deep convolutional neural network can be converted into a spiking neural network with near zero accuracy loss. However, the advantage on energy consumption of spiking neural networks comes at a cost of long classification latency due to the use of Poisson-distributed spike trains (rate coding), especially in deep networks. In this paper, we propose to use weighted spikes, which can greatly reduce the latency by assigning a different weight to a spike depending on which time phase it belongs. Experimental results on MNIST, SVHN, CIFAR-10, and CIFAR-100 show that the proposed spiking neural networks with weighted spikes achieve significant reduction in classification latency and number of spikes, which leads to faster and more energy-efficient spiking neural networks than the conventional spiking neural networks with rate coding. We also show that one of the state-of-the-art networks the deep residual network can be converted into spiking neural network without accuracy loss. (c) 2018 Elsevier B.V. All rights reserved.
C1 [Kim, Jaehyun; Kim, Heesu; Huh, Subin; Choi, Kiyoung] Seoul Natl Univ, Dept Elect & Comp Engn, Seoul, South Korea.
   [Lee, Jinho] IBM Res, Burnet Rd, Austin, TX 78758 USA.
RP Choi, K (corresponding author), Seoul Natl Univ, Dept Elect & Comp Engn, Seoul, South Korea.
EM kchoi@snu.ac.kr
CR Adrian ED, 1926, J PHYSIOL-LONDON, V61, P49, DOI 10.1113/jphysiol.1926.sp002273
   ANDERSON EDGAR, 1936, ANN MISSOURI BOT GARD, V23, P457, DOI 10.2307/2394164
   [Anonymous], 2015, GPU BAS DEEP LEARN I
   Pérez-Carrasco JA, 2013, IEEE T PATTERN ANAL, V35, P2706, DOI 10.1109/TPAMI.2013.71
   Belatreche A, 2003, P IEEE CYB INT CHALL, P39
   Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Chollet F., 2015, KERAS
   Collobert R, 2008, P 25 ICML, P160, DOI [DOI 10.1145/1390156.1390177, 10.1145/1390156.1390177]
   Cruz-Albrecht JM, 2012, IEEE T BIOMED CIRC S, V6, P246, DOI 10.1109/TBCAS.2011.2174152
   Dan Y, 2004, NEURON, V44, P23, DOI 10.1016/j.neuron.2004.09.007
   Diehl P.U., 2015, 2015 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2015.7280696
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   Ghosh-Dastidar S, 2007, INTEGR COMPUT-AID E, V14, P187
   Glorot X., 2011, PMLR, Vvol 15, P315
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Heeger D., 2000, POISSON MODEL SPIKE
   Hinton GE., 2012, IMPROVING NEURAL NET
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Hyvarinen A., 2004, INDEPENDENT COMPONEN
   Iakymchuk T, 2015, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-015-0059-4
   Ioffe S., 2015, PR MACH LEARN RES, P448
   Izhikevich EM, 2002, BIOSYSTEMS, V67, P95, DOI 10.1016/S0303-2647(02)00067-9
   Kayser C, 2009, NEURON, V61, P597, DOI 10.1016/j.neuron.2009.01.008
   Kistler WM, 2002, BIOL CYBERN, V87, P416, DOI 10.1007/s00422-002-0359-5
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
   Krizhevsky Alex, 2009, LEARNING MULTIPLE LA
   LeCun Y, 1998, LECT NOTES COMPUT SC, V1524, P9, DOI 10.1007/3-540-49430-8_2
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee CY, 2015, JMLR WORKSH CONF PRO, V38, P562
   Lin M, 2014, 2014 INTERNATIONAL CONFERENCE ON MEDICAL BIOMETRICS (ICMB 2014), P1, DOI 10.1109/ICMB.2014.8
   Lin ZT, 2018, NEUROCOMPUTING, V275, P94, DOI 10.1016/j.neucom.2017.05.009
   Lisman J, 2008, SCHIZOPHRENIA BULL, V34, P974, DOI 10.1093/schbul/sbn060
   Masquelier T, 2007, PLOS COMPUT BIOL, V3, P247, DOI 10.1371/journal.pcbi.0030031
   McKennoch S, 2006, IEEE IJCNN, P3970
   Merolla P., 2011, IEEE CUST INT CIRC C, P1, DOI DOI 10.1109/CICC.2011.6055294
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Neftci E, 2014, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00272
   Netzer Y., 2011, READING DIGITS NATUR, V2, P5
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Rueckauer B., 2016, P NIPS WORKSH COMP S
   Schrauwen B, 2004, IEEE IJCNN, P471, DOI 10.1109/IJCNN.2004.1379954
   Silva SM, 2005, PROCEEDINGS OF THE 2005 INTERNATIONAL CONFERENCE ON NEURAL NETWORKS AND BRAIN, VOLS 1-3, P1354
   Strain TJ, 2010, INT J NEURAL SYST, V20, P463, DOI 10.1142/S0129065710002553
   Taherkhani A, 2015, IEEE T NEUR NET LEAR, V26, P3137, DOI 10.1109/TNNLS.2015.2404938
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   THORPE SJ, 1990, PARALLEL PROCESSING IN NEURAL SYSTEMS AND COMPUTERS, P91
   Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412
   Vreeken J., 2002, SPIKING NEURAL NETWO
   Widrow B., 1962, SELF ORG SYSTEMS, P435
   Widrow B., 1960, ADAPTIVE SWITCHING C, V4, P96, DOI DOI 10.21236/AD0241531
   Xie XR, 2017, NEUROCOMPUTING, V241, P152, DOI 10.1016/j.neucom.2017.01.086
   Xu Y, 2013, NEURAL COMPUT, V25, P1472, DOI 10.1162/NECO_a_00450
NR 56
TC 62
Z9 64
U1 8
U2 97
PD OCT 15
PY 2018
VL 311
BP 373
EP 386
DI 10.1016/j.neucom.2018.05.087
UT WOS:000438313100035
DA 2023-11-16
ER

PT C
AU Lee, C
   Kim, J
   Choi, K
AF Lee, Chaeun
   Kim, Jaehyun
   Choi, Kiyoung
GP IEEE
TI An RRAM-based Analog Neuron Design for the Weighted Spiking Neural
   network
SO 2019 INTERNATIONAL SOC DESIGN CONFERENCE (ISOCC)
SE International SoC Design Conference
DT Proceedings Paper
CT 16th International System-on-Chip Design Conference (ISOCC)
CY OCT 06-09, 2019
CL SOUTH KOREA
DE deep neural network; spiking neural network; weighted spiking neural
   network; analog neuron circuit; ASIC
AB Spiking neural networks (SNNs) are promising because they have the ability to represent signal strength information with a simple sequence of spikes having the same height. In this paper, we propose an RRAM-based analog neuron circuit for the weighted spiking neural network which is energy-efficient and hardware-friendly. We have designed the neuron circuit to show that the weighted spiking neural network can be implemented in analog and works properly.
C1 [Lee, Chaeun; Choi, Kiyoung] Seoul Natl Univ, Dept Elect & Comp Engn, Seoul, South Korea.
   [Kim, Jaehyun] Samsung Elect, Daegu, South Korea.
RP Lee, C (corresponding author), Seoul Natl Univ, Dept Elect & Comp Engn, Seoul, South Korea.
EM mki11@snu.ac.kr; jh6324.kim@samsung.com; kchoi@snu.ac.kr
CR Kim J, 2018, NEUROCOMPUTING, V311, P373, DOI 10.1016/j.neucom.2018.05.087
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Park S., 2019, P ACM IEEE DES AUT C
   Rueckauer B., 2016, ARXIV161204052
   Rueckauer B, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351295
NR 5
TC 0
Z9 0
U1 0
U2 3
PY 2019
BP 259
EP 260
UT WOS:000694734600038
DA 2023-11-16
ER

PT C
AU Li, SL
   Li, JP
AF Li, Shun-Li
   Li, Jian-Ping
GP IEEE
TI RESEARCH ON LEARNING ALGORITHM OF SPIKING NEURAL NETWORK
SO 2019 16TH INTERNATIONAL COMPUTER CONFERENCE ON WAVELET ACTIVE MEDIA
   TECHNOLOGY AND INFORMATION PROCESSING (ICWAMTIP)
SE International Computer Conference on Wavelet Active Media Technology and
   Information Processing
DT Proceedings Paper
CT 16th IEEE International Computer Conference on Wavelet Active Media
   Technology and Information Processing (ICCWAMTIP)
CY DEC 13-15, 2019
CL Univ Elect Sci & Technol China, Chengdu, PEOPLES R CHINA
HO Univ Elect Sci & Technol China
DE Spiking; Learning rulers; Unsupervised; Supervised
AB The spiking neural network is a new artificial neural network that expresses and transmits biological information through spiking sequences based on precise time coding. Compared with the traditional frequency-based neural network, spiking neural network has stronger bionics and computing power, which can more accurately simulate human brain and brain activity. This paper introduces the typical unsupervised spiking neural network learning method-STDP (Spike-Timing-Dependent Plasticity) and the classic supervised spiking neural network learning method-Tempotron and ReSuMe (Remote Supervised Method), combines the spatio-temporal transmission characteristics of biological neural information to analyze the characteristics and performance of various existing spiking sequence learning methods. This review contributes to the applicability research of various spiking neural network models.
C1 [Li, Shun-Li; Li, Jian-Ping] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 610054, Peoples R China.
RP Li, SL (corresponding author), Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 610054, Peoples R China.
EM 18782236401@163.com; 925774968@qq.com
CR Caporale N, 2008, ANNU REV NEUROSCI, V31, P25, DOI 10.1146/annurev.neuro.31.060407.125639
   [程龙 Cheng Long], 2018, [控制与决策, Control and Decision], V33, P923
   Erfanian S N, 2016, PLOS COMPUTATIONAL B, V12
   Gerstner W, 2002, SPIKING NEURON MODEL, DOI [10.1017/CBO9780511815706, DOI 10.1017/CBO9780511815706]
   Glackin C, 2011, NEURAL NETWORKS, V24, P247, DOI 10.1016/j.neunet.2010.11.008
   Gutig Robert, 2006, NATURE NEUROSCIENCE
   Hu J, 2013, NEURAL COMPUT, V25, P450, DOI 10.1162/NECO_a_00395
   Irina H, 2017, PLOS ONE, V12
   Kandel E.R., 2000, PRINCIPLES NEURAL SC
   [蔺想红 Lin Xianghong], 2015, [电子学报, Acta Electronica Sinica], V43, P577
   MAASS W, 1997, NETWORKS SPIKING NEU
   Masquelier T, 2009, NEURAL COMPUT, V21, P1259, DOI 10.1162/neco.2008.06-08-804
   Mehta MR, 2002, NATURE, V417, P741, DOI 10.1038/nature00807
   Muntean IL, 2012, INT SYMP SYMB NUMERI, P481, DOI 10.1109/SYNASC.2012.44
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Roy S, 2015, IEEE TRANSACTIONS ON, V28, P1
   Van Rullen R, 2001, NEURAL COMPUT, V13, P1255, DOI 10.1162/08997660152002852
   Xu Y, 2013, NEURAL COMPUT, V25, P1472, DOI 10.1162/NECO_a_00450
   Zenke F, 2017, NEURAL COMPUTATION, V30
NR 19
TC 2
Z9 2
U1 2
U2 33
PY 2019
BP 45
EP 48
DI 10.1109/iccwamtip47768.2019.9067608
UT WOS:000553538700010
DA 2023-11-16
ER

PT J
AU Zhou, YC
   Zhang, AG
AF Zhou, Yongcheng
   Zhang, Anguo
TI Improved integrate-and-fire neuron models for inference acceleration of
   spiking neural networks
SO APPLIED INTELLIGENCE
DT Article
DE Spiking neural network; Inference acceleration; Neural plasticity
ID PLASTICITY
AB We study the effects of different bio-synaptic membrane potential mechanisms on the inference speed of both spiking feed-forward neural networks and spiking convolutional neural networks. These mechanisms are inspired by biological neuron phenomena include electronic conduction in neurons and chemical neurotransmitter attenuation between presynaptic and postsynaptic neurons. In the area of spiking neural networks, we model some biological neural membrane potential updating strategies based on integrate-and-fire (I&F) spiking neurons. These include the spiking neuron model with membrane potential decay (MemDec), the spiking neuron model with synaptic input current superposition at spiking time (SynSup), and the spiking neuron model with synaptic input current accumulation (SynAcc). Experiment results show that compared with the general I&F model (one of the most commonly used spiking neuron models), SynSup and SynAcc can effectively improve the spiking inference speed of spiking feed-forward neural networks and spiking convolutional neural networks.
C1 [Zhou, Yongcheng] Ruijie Networks Co Ltd, Res Inst Ruijie, Fuzhou 350002, Peoples R China.
   [Zhang, Anguo] Fuzhou Univ, Coll Phys & Informat Engn, Fuzhou 350108, Peoples R China.
   [Zhang, Anguo] Key Lab Med Instrumentat & Pharmaceut Technol Fuj, Fuzhou 350116, Peoples R China.
RP Zhang, AG (corresponding author), Fuzhou Univ, Coll Phys & Informat Engn, Fuzhou 350108, Peoples R China.; Zhang, AG (corresponding author), Key Lab Med Instrumentat & Pharmaceut Technol Fuj, Fuzhou 350116, Peoples R China.
EM anguo.zhang@hotmail.com
CR Bodyanskiy Y, 2016, PROCEEDINGS OF THE 2016 IEEE FIRST INTERNATIONAL CONFERENCE ON DATA STREAM MINING & PROCESSING (DSMP), P104, DOI 10.1109/DSMP.2016.7583517
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Camuñas-Mesa L, 2012, IEEE J SOLID-ST CIRC, V47, P504, DOI 10.1109/JSSC.2011.2167409
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Carlson KD, 2014, FRONT NEUROSCI-SWITZ, V8, DOI 10.3389/fnins.2014.00010
   Diehl PU, 2015, IEEE IJCNN
   Fang HJ, 2010, NEURAL COMPUT, V22, P1060, DOI 10.1162/neco.2009.10-08-885
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Huang SY, 2014, J NEUROSCI, V34, P7575, DOI 10.1523/JNEUROSCI.0983-14.2014
   Humphries MD, 2007, NEURAL COMPUT, V19, P3216, DOI 10.1162/neco.2007.19.12.3216
   Johnson AP, 2018, IEEE T CIRCUITS-I, V65, P687, DOI 10.1109/TCSI.2017.2726763
   Kennedy J, 1995, 1995 IEEE INTERNATIONAL CONFERENCE ON NEURAL NETWORKS PROCEEDINGS, VOLS 1-6, P1942, DOI 10.1109/icnn.1995.488968
   Lehky SR, 2010, NEURAL COMPUT, V22, P1245, DOI 10.1162/neco.2009.07-08-823
   Lin ZT, 2017, ELECTRON LETT, V53, P1347, DOI 10.1049/el.2017.2219
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Masquelier T, 2007, PLOS COMPUT BIOL, V3, P247, DOI 10.1371/journal.pcbi.0030031
   McMahon DBT, 2012, CURR BIOL, V22, P332, DOI 10.1016/j.cub.2012.01.003
   Meliza CD, 2006, NEURON, V49, P183, DOI 10.1016/j.neuron.2005.12.009
   Merolla P, 2011, IEEE CUST INTEGR CIR
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   Rueckauer B, 2016, ARXIV161204052CSSTAT
   Schrauwen B, 2004, P 15 PRORISC WORKSH, P301
   Skocik MJ, 2014, IEEE T NEUR NET LEAR, V25, P1474, DOI 10.1109/TNNLS.2013.2294016
   Tino P, 2006, NEURAL COMPUT, V18, P591, DOI 10.1162/089976606775623360
   van Rossum MCW, 2002, J NEUROSCI, V22, P1956, DOI 10.1523/JNEUROSCI.22-05-01956.2002
   Ventura V, 2015, NEURAL COMPUT, V27, P1033, DOI 10.1162/NECO_a_00731
   Zhang AG, 2019, NEUROCOMPUTING, V365, P102, DOI 10.1016/j.neucom.2019.07.009
   Zhang SX, 2019, IEEE ACCESS, V7, P73685, DOI 10.1109/ACCESS.2019.2914424
NR 28
TC 7
Z9 8
U1 11
U2 63
PD APR
PY 2021
VL 51
IS 4
BP 2393
EP 2405
DI 10.1007/s10489-020-02017-3
EA NOV 2020
UT WOS:000585786200005
DA 2023-11-16
ER

PT J
AU Han, B
   Ankit, A
   Sengupta, A
   Roy, K
AF Han, Bing
   Ankit, Aayush
   Sengupta, Abhronil
   Roy, Kaushik
TI Cross-Layer Design Exploration for Energy-Quality Tradeoffs in Spiking
   and Non-Spiking Deep Artificial Neural Networks
SO IEEE TRANSACTIONS ON MULTI-SCALE COMPUTING SYSTEMS
DT Article
DE Convolutional neural networks; neuromorphic systems; non-spiking neural
   networks; power-energy efficiency; spiking neural networks
AB Deep learning convolutional artificial neural networks have achieved success in a large number of visual processing tasks and are currently utilized for many real-world applications like image search and speech recognition among others. However, despite achieving high accuracy in such classification problems, they involve significant computational resources. Over the past few years, non-spiking deep convolutional artificial neural network models have evolved into more biologically realistic and event-driven spiking deep convolutional artificial neural networks. Recent research efforts have been directed at developing mechanisms to convert traditional non-spiking deep convolutional artificial neural networks to the spiking ones where neurons communicate by means of spikes. However, there have been limited studies providing insights on the specific power, area, and energy benefits offered by the spiking deep convolutional artificial neural networks in comparison to their non-spiking counterparts. We perform a comprehensive study for hardware implementation of spiking/non-spiking deep convolutional artificial neural networks on MNIST, CIFAR10, and SVHN datasets. To this effect, we design AccelNN -a Neural Network Accelerator to execute neural network benchmarks and analyze the effects of circuit-architecture level techniques to harness event-drivenness. A comparative analysis between spiking and non-spiking versions of deep convolutional artificial neural networks is presented by performing trade-offs between recognition accuracy and corresponding power, latency and energy requirements.
C1 [Han, Bing; Ankit, Aayush; Sengupta, Abhronil; Roy, Kaushik] Purdue Univ, Dept Elect & Comp Engn, W Lafayette, IN 47906 USA.
RP Ankit, A (corresponding author), Purdue Univ, Dept Elect & Comp Engn, W Lafayette, IN 47906 USA.
EM han183@purdue.edu; aankit@purdue.edu; asengup@purdue.edu;
   kaushik@purdue.edu
CR Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   [Anonymous], 2010, 2010 IEEE INT S PARA
   [Anonymous], 2011, P 28 INT C INT C MAC
   [Anonymous], 2015, 2015 INT JOINT C NEU, DOI DOI 10.1109/IJCNN.2015.7280625
   [Anonymous], 2013, ARXIV PREPRINT ARXIV
   [Anonymous], 2012, PREDICTION CANDIDATE
   [Anonymous], 2014, P 2014 5 ASIA PACIFI, DOI DOI 10.1145/2637166.2637229
   Belhadj B., 2013, SIGARCH COMPUT ARCHI, V41, P3
   Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Chi P, 2016, CONF PROC INT SYMP C, P27, DOI 10.1109/ISCA.2016.13
   Ciresan DC, 2010, NEURAL COMPUT, V22, P3207, DOI 10.1162/NECO_a_00052
   Diehl P.U., 2015, 2015 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2015.7280696
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Du ZD, 2015, PROCEEDINGS OF THE 48TH ANNUAL IEEE/ACM INTERNATIONAL SYMPOSIUM ON MICROARCHITECTURE (MICRO-48), P494, DOI 10.1145/2830772.2830789
   Esmaeilzadeh H, 2012, INT SYMP MICROARCH, P449, DOI 10.1109/MICRO.2012.48
   Han S., 2015, C NEUR INF PROC SYST
   Indiveri G, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00073
   Khan MM, 2008, IEEE IJCNN, P2849, DOI 10.1109/IJCNN.2008.4634199
   Kheradpisheh S. R., 2016, COMPUT RES REPOSITOR
   Khodagholy D, 2015, NAT NEUROSCI, V18, P310, DOI 10.1038/nn.3905
   Kim S, 2011, 2011 11TH INTERNATIONAL CONFERENCE ON CONTROL, AUTOMATION AND SYSTEMS (ICCAS), P1
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
   Krizhevsky Alex, 2009, LEARNING MULTIPLE LA, V3, P6
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Muralimanohar N., 2009, INT S MICR
   Netzer Y., 2011, NEURAL INFORM PROCES, P5
   Powell M, 2000, ISLPED '00: PROCEEDINGS OF THE 2000 INTERNATIONAL SYMPOSIUM ON LOW POWER ELECTRONICS AND DESIGN, P90, DOI 10.1109/LPE.2000.876763
   Ramasubramanian SG, 2014, I SYMPOS LOW POWER E, P15, DOI 10.1145/2627369.2627625
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Stromatias E., 2013, 2013 INT JOINT C NEU, P1, DOI [DOI 10.1109/IJCNN.2013.6706927, 10.1109/ijcnn.2013.6706927]
   Tianshi Chen, 2012, 2012 IEEE International Symposium on Workload Characterization (IISWC 2012), P36, DOI 10.1109/IISWC.2012.6402898
NR 33
TC 9
Z9 9
U1 1
U2 6
PD OCT-DEC
PY 2018
VL 4
IS 4
BP 613
EP 623
DI 10.1109/TMSCS.2017.2737625
UT WOS:000457630400011
DA 2023-11-16
ER

PT C
AU Huang, ZN
   Luo, HY
   Guo, DH
AF Huang, Zenan
   Luo, Hongyin
   Guo, Donghui
GP IEEE
TI Application of Locally Connected Spiking Neural Network in Image
   Processing
SO PROCEEDINGS OF 2019 IEEE 13TH INTERNATIONAL CONFERENCE ON
   ANTI-COUNTERFEITING, SECURITY, AND IDENTIFICATION (IEEE-ASID'2019)
SE Proceedings of the International Conference on Anti-counterfeiting
   Security and Identification
DT Proceedings Paper
CT 13th IEEE International Conference on Anti-Counterfeiting, Security, and
   Identification (ASID)
CY OCT 25-27, 2019
CL Xiamen, PEOPLES R CHINA
DE spiking neural network; image processing; locally connected; network
   topology; sparse analysis
AB The rise of deep learning has accelerated the application of neural network in image processing. As the nearest neural network model to human biological cell system, spiking neural network (SNN) has shown great potential in image processing and pattern recognition. In this paper, a method of applying spiking neural network to image processing is proposed. Recent advances in neuroanatotny have provided favorable conditions for the application of local connections. By analyzing the connection modes of different sparsity of spiking neurons in space, spiking neural network can effectively perform robust image processing tasks. The experiment shows that the pre-set spiking neural network can acquire image features quickly and effectively without training.
C1 [Huang, Zenan; Guo, Donghui] Xiamen Univ, Dept Elect Engn, Xiamen, Peoples R China.
   [Luo, Hongyin] Xiamen RichIT Microelect Technol LTD, Xiamen, Peoples R China.
RP Guo, DH (corresponding author), Xiamen Univ, Dept Elect Engn, Xiamen, Peoples R China.
EM dhguo@xmu.edu.cn
CR [Anonymous], 2016, IEEE T CIRCUITS SY 2
   Brandao AS, 2016, IEEE LAT AM T, V14, P1361, DOI 10.1109/TLA.2016.7459621
   Hong C., 2019, IEEE T NEURAL NETWOR
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Jin KH, 2017, IEEE T IMAGE PROCESS, V26, P4509, DOI 10.1109/TIP.2017.2713099
   Mostafa H, 2018, IEEE T NEUR NET LEAR, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Neftci E, 2014, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00272
   Qin C, 2019, IEEE T MED IMAGING, V38, P280, DOI 10.1109/TMI.2018.2863670
   Shin HC, 2016, IEEE T MED IMAGING, V35, P1285, DOI 10.1109/TMI.2016.2528162
   Trivedi AR, 2014, IEEE T NANOTECHNOL, V13, P627, DOI 10.1109/TNANO.2014.2318046
   Yu Q, 2016, IEEE T NEUR NET LEAR, V27, P621, DOI 10.1109/TNNLS.2015.2416771
   Zhang SQ, 2019, IEEE ACCESS, V7, P32297, DOI 10.1109/ACCESS.2019.2901521
   Zheng N, 2018, IEEE T NEUR NET LEAR, V29, P4287, DOI 10.1109/TNNLS.2017.2761335
NR 13
TC 3
Z9 3
U1 1
U2 8
PY 2019
BP 108
EP 111
DI 10.1109/icasid.2019.8925219
UT WOS:000521754700023
DA 2023-11-16
ER

PT J
AU Fu, Q
   Dong, HB
AF Fu, Qiang
   Dong, Hongbin
TI Spiking Neural Network Based on Multi-Scale Saliency Fusion for Breast
   Cancer Detection
SO ENTROPY
DT Article
DE spiking neural network; YOLO; medical image; object detection
AB Deep neural networks have been successfully applied in the field of image recognition and object detection, and the recognition results are close to or even superior to those from human beings. A deep neural network takes the activation function as the basic unit. It is inferior to the spiking neural network, which takes the spiking neuron model as the basic unit in the aspect of biological interpretability. The spiking neural network is considered as the third-generation artificial neural network, which is event-driven and has low power consumption. It modulates the process of nerve cells from receiving a stimulus to firing spikes. However, it is difficult to train spiking neural network directly due to the non-differentiable spiking neurons. In particular, it is impossible to train a spiking neural network using the back-propagation algorithm directly. Therefore, the application scenarios of spiking neural network are not as extensive as deep neural network, and a spiking neural network is mostly used in simple image classification tasks. This paper proposed a spiking neural network method for the field of object detection based on medical images using the method of converting a deep neural network to spiking neural network. The detection framework relies on the YOLO structure and uses the feature pyramid structure to obtain the multi-scale features of the image. By fusing the high resolution of low-level features and the strong semantic information of high-level features, the detection precision of the network is improved. The proposed method is applied to detect the location and classification of breast lesions with ultrasound and X-ray datasets, and the results are 90.67% and 92.81%, respectively.
C1 [Fu, Qiang; Dong, Hongbin] Harbin Engn Univ, Coll Comp Sci & Technol, Harbin 150001, Peoples R China.
RP Dong, HB (corresponding author), Harbin Engn Univ, Coll Comp Sci & Technol, Harbin 150001, Peoples R China.
EM donghongbin@hrbeu.edu.cn
CR Al-Dhabyani W, 2020, DATA BRIEF, V28, DOI 10.1016/j.dib.2019.104863
   Aly GH, 2021, COMPUT METH PROG BIO, V200, DOI 10.1016/j.cmpb.2020.105823
   Balter LJT, 2019, NEUROIMAGE, V202, DOI 10.1016/j.neuroimage.2019.116098
   Benhammou Y, 2020, NEUROCOMPUTING, V375, P9, DOI 10.1016/j.neucom.2019.09.044
   Castro L, 2020, COGNITION, V204, DOI 10.1016/j.cognition.2020.104350
   Esfeh JM, 2020, CLIN MOL HEPATOL, V26, P54, DOI 10.3350/cmh.2019.0039
   Fu Q, 2021, NEUROCOMPUTING, V419, P47, DOI 10.1016/j.neucom.2020.07.109
   Fu Q, 2019, IEEE SYS MAN CYBERN, P152, DOI 10.1109/SMC.2019.8914526
   Gerstner W., 2002, SPIKING NEURON MODEL
   Girshick R., 2014, P IEEE C COMP VIS PA, P580
   Girshick R, 2012, RIGID TEMPLATES TRAM
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Hadadi I, 2021, RADIOGRAPHY, V27, P1027, DOI 10.1016/j.radi.2021.04.002
   He KM, 2015, IEEE T PATTERN ANAL, V37, P1904, DOI 10.1109/TPAMI.2015.2389824
   Heath M, 2001, IWDM 2000: 5TH INTERNATIONAL WORKSHOP ON DIGITAL MAMMOGRAPHY, P212
   Heath M, 1998, COMP IMAG VIS, V13, P457
   Huang R, 2018, IEEE INT CONF BIG DA, P2503, DOI 10.1109/BigData.2018.8621865
   Jemal A, 2011, CA-CANCER J CLIN, V61, P134, DOI [10.3322/caac.20115, 10.3322/caac.20107, 10.3322/caac.21492]
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Malisiewicz T, 2011, IEEE I CONF COMP VIS, P89, DOI 10.1109/ICCV.2011.6126229
   Perrone-Bertolotti M, 2020, NEUROIMAGE, V210, DOI 10.1016/j.neuroimage.2020.116574
   Redmon J, 2018, Arxiv, DOI arXiv:1804.02767
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031
   Rezaei Z, 2021, EXPERT SYST APPL, V182, DOI 10.1016/j.eswa.2021.115204
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Sánchez-Cauce R, 2021, COMPUT METH PROG BIO, V204, DOI 10.1016/j.cmpb.2021.106045
   Saunders DJ, 2019, NEURAL NETWORKS, V119, P332, DOI 10.1016/j.neunet.2019.08.016
   Singla C, 2022, MATER TODAY-PROC, V49, P3098, DOI 10.1016/j.matpr.2020.10.951
   Vasuki R, 2021, MATER TODAY-PROC, DOI [10.1016/j.matpr.2020.11.600, DOI 10.1016/J.MATPR.2020.11.600]
   Wang TX, 2021, NEUROCOMPUTING, V425, P96, DOI 10.1016/j.neucom.2020.10.100
   Yang B, 2021, PHOTODIAGN PHOTODYN, V33, DOI 10.1016/j.pdpdt.2021.102199
   Zhang S, 2018, PROC CVPR IEEE, P4203, DOI 10.1109/CVPR.2018.00442
   Zhang XL, 2017, NEUROIMAGE, V163, P231, DOI 10.1016/j.neuroimage.2017.09.050
NR 35
TC 1
Z9 1
U1 14
U2 26
PD NOV
PY 2022
VL 24
IS 11
AR 1543
DI 10.3390/e24111543
UT WOS:000881367800001
DA 2023-11-16
ER

PT C
AU Barchid, S
   Mennesson, J
   Djéraba, C
AF Barchid, Sami
   Mennesson, Jose
   Djeraba, Chaabane
GP IEEE
TI Deep Spiking Convolutional Neural Network for Single Object Localization
   Based On Deep Continuous Local Learning
SO 2021 INTERNATIONAL CONFERENCE ON CONTENT-BASED MULTIMEDIA INDEXING
   (CBMI)
SE International Workshop on Content-Based Multimedia Indexing
DT Proceedings Paper
CT 18th International Conference on Content-Based Multimedia Indexing (IEEE
   CBMI)
CY JUN 28-30, 2021
CL ELECTR NETWORK
DE Deep Spiking Neural Network; SNN; Convolution; Object Localization
AB With the advent of neuromorphic hardware, spiking neural networks can be a good energy-efficient alternative to artificial neural networks. However, the use of spiking neural networks to perform computer vision tasks remains limited, mainly focusing on simple tasks such as digit recognition. It remains hard to deal with more complex tasks (e.g. segmentation, object detection) due to the small number of works on deep spiking neural networks for these tasks. The objective of this paper is to make the first step towards modern computer vision with supervised spiking neural networks. We propose a deep convolutional spiking neural network for the localization of a single object in a grayscale image. We propose a network based on DECOLLE, a spiking model that enables local surrogate gradient-based learning. The encouraging results reported on Oxford-IIIT-Pet validates the exploitation of spiking neural networks with a supervised learning approach for more elaborate vision tasks in the future.
C1 [Barchid, Sami; Mennesson, Jose; Djeraba, Chaabane] Univ Lille, CNRS, Cent Lille, UMR 9189 CRIStAL, F-59000 Lille, France.
   [Mennesson, Jose] IMT Lille Douai, Inst Mines Telecom, Ctr Digital Syst, Douai, France.
RP Barchid, S (corresponding author), Univ Lille, CNRS, Cent Lille, UMR 9189 CRIStAL, F-59000 Lille, France.
EM sami.barchid@univ-lille.fr; jose.mennesson@imt-lille-douai.fr;
   chabane.djeraba@univ-lille.fr
CR Borst A, 1999, NAT NEUROSCI, V2, P947, DOI 10.1038/14731
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Caporale N, 2008, ANNU REV NEUROSCI, V31, P25, DOI 10.1146/annurev.neuro.31.060407.125639
   Cheng Yu, 2017, ARXIV171009282
   Falez P., 2019, THESIS U LILLE OCT
   Falez Pierre, 2019, 2019 INT JOINT C NEU, P1
   Gallego G, 2022, IEEE T PATTERN ANAL, V44, P154, DOI 10.1109/TPAMI.2020.3008413
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Kaiser J, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00424
   Kim S., 2020, AAAI CONF ARTIF INTE, V34, P11270
   King DB, 2015, ACS SYM SER, V1214, P1
   Kirkland P, 2020, IEEE IJCNN, DOI 10.1109/ijcnn48605.2020.9207075
   Lee C, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00119
   Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508
   Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106
   Liu WB, 2017, NEUROCOMPUTING, V234, P11, DOI 10.1016/j.neucom.2016.12.038
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Parkhi OM, 2012, PROC CVPR IEEE, P3498, DOI 10.1109/CVPR.2012.6248092
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Zamarreño-Ramos C, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00026
   Zenke F, 2018, NEURAL COMPUT, V30, P1514, DOI 10.1162/neco_a_01086
NR 23
TC 0
Z9 0
U1 3
U2 17
PY 2021
BP 124
EP 128
DI 10.1109/CBMI50038.2021.9461880
UT WOS:000713450200022
DA 2023-11-16
ER

PT J
AU Wang, XW
   Lin, XH
   Dang, XC
AF Wang, Xiangwen
   Lin, Xianghong
   Dang, Xiaochao
TI Supervised learning in spiking neural networks: A review of algorithms
   and evaluations
SO NEURAL NETWORKS
DT Review
DE Spiking neural network; Spike train; Spiking neuron; Supervised
   learning; Performance evaluation
ID TIMING-DEPENDENT PLASTICITY; EVENT-DRIVEN SIMULATION;
   ERROR-BACKPROPAGATION; GRADIENT DESCENT; SYNAPTIC PLASTICITY; TRAINING
   ALGORITHM; NEURONS; MODEL; STDP; RULE
AB As a new brain-inspired computational model of the artificial neural network, a spiking neural network encodes and processes neural information through precisely timed spike trains. Spiking neural networks are composed of biologically plausible spiking neurons, which have become suitable tools for processing complex temporal or spatiotemporal information. However, because of their intricately discontinuous and implicit nonlinear mechanisms, the formulation of efficient supervised learning algorithms for spiking neural networks is difficult, and has become an important problem in this research field. This article presents a comprehensive review of supervised learning algorithms for spiking neural networks and evaluates them qualitatively and quantitatively. First, a comparison between spiking neural networks and traditional artificial neural networks is provided. The general framework and some related theories of supervised learning for spiking neural networks are then introduced. Furthermore, the state-of-the-art supervised learning algorithms in recent years are reviewed from the perspectives of applicability to spiking neural network architecture and the inherent mechanisms of supervised learning algorithms. A performance comparison of spike train learning of some representative algorithms is also made. In addition, we provide five qualitative performance evaluation criteria for supervised learning algorithms for spiking neural networks and further present a new taxonomy for supervised learning algorithms depending on these five performance evaluation criteria. Finally, some future research directions in this research field are outlined. (c) 2020 Elsevier Ltd. All rights reserved.
C1 [Wang, Xiangwen; Lin, Xianghong; Dang, Xiaochao] Northwest Normal Univ, Coll Comp Sci & Engn, Lanzhou 730070, Peoples R China.
RP Lin, XH (corresponding author), Northwest Normal Univ, Coll Comp Sci & Engn, Lanzhou 730070, Peoples R China.
EM linxh@nwnu.edu.cn
CR Abusnaina AA, 2019, NEURAL PROCESS LETT, V49, P661, DOI 10.1007/s11063-018-9846-0
   Almási AD, 2016, NEUROCOMPUTING, V174, P31, DOI 10.1016/j.neucom.2015.02.092
   Amit DJ, 1997, NETWORK-COMP NEURAL, V8, P373, DOI 10.1088/0954-898X/8/4/003
   [Anonymous], 2012, NEUR NETW IJCNN 2012
   [Anonymous], THESIS
   [Anonymous], 2012 INT JOINT C NEU, DOI DOI 10.1109/IJCNN.2012.6252439
   [Anonymous], 2016, 2016 IEE INT C REB C, DOI [10.1109/ICRC.2016.7738691, DOI 10.1109/ICRC.2016.7738691]
   [Anonymous], 2015, 2015 INT JOINT C NEU, DOI DOI 10.1109/IJCNN.2015.7280592
   [Anonymous], 2011, ADV NEURAL INF PROCE
   [Anonymous], 2001, HDB BIOL PHYS
   Baldi P, 2016, NEURAL NETWORKS, V83, P51, DOI 10.1016/j.neunet.2016.07.006
   Banerjee A, 2016, NEURAL COMPUT, V28, P826, DOI 10.1162/NECO_a_00829
   Bear MF, 2003, PHILOS T ROY SOC B, V358, P649, DOI 10.1098/rstb.2002.1255
   Belatreche A, 2003, PROCEEDINGS OF THE 7TH JOINT CONFERENCE ON INFORMATION SCIENCES, P1524
   Belatreche A, 2007, SOFT COMPUT, V11, P239, DOI [10.1007/s00500-006-0065-7, 10.1007/S00500-006-0065-7]
   Belatreche A, 2006, NEW MATH NAT COMPUT, V2, P237, DOI 10.1142/S179300570600049X
   Beyeler M, 2013, NEURAL NETWORKS, V48, P109, DOI 10.1016/j.neunet.2013.07.012
   BIENENSTOCK EL, 1982, J NEUROSCI, V2, P32, DOI 10.1523/jneurosci.02-01-00032.1982
   Bing ZS, 2019, FRONT NEUROROBOTICS, V13, DOI 10.3389/fnbot.2019.00018
   Bohte Sander M., 2004, Natural Computing, V3, P195, DOI 10.1023/B:NACO.0000027755.02868.60
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Booij O, 2005, INFORM PROCESS LETT, V95, P552, DOI 10.1016/j.ipl.2005.05.023
   Borst A, 1999, NAT NEUROSCI, V2, P947, DOI 10.1038/14731
   Bouvier M, 2019, ACM J EMERG TECH COM, V15, DOI 10.1145/3304103
   Brea J, 2013, J NEUROSCI, V33, P9565, DOI 10.1523/JNEUROSCI.4098-12.2013
   Brette R, 2007, J COMPUT NEUROSCI, V23, P349, DOI 10.1007/s10827-007-0038-6
   Brockmeier AJ, 2014, NEURAL COMPUT, V26, P1080, DOI 10.1162/NECO_a_00591
   Buesing L, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1002211
   Burkitt AN, 2006, BIOL CYBERN, V95, P97, DOI 10.1007/s00422-006-0082-8
   Burkitt AN, 2006, BIOL CYBERN, V95, P1, DOI 10.1007/s00422-006-0068-6
   Caporale N, 2008, ANNU REV NEUROSCI, V31, P25, DOI 10.1146/annurev.neuro.31.060407.125639
   Carnell A., 2005, P ESANN, P363
   Chauvin Y., 1995, BACKPROPAGATION THEO
   Chen Z, 2013, COMPUT INTEL NEUROSC, V2013, DOI 10.1155/2013/251905
   Chen ZG, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P106
   Clopath C, 2010, NAT NEUROSCI, V13, P344, DOI 10.1038/nn.2479
   D'Haene M, 2014, NEURAL COMPUT, V26, P1055, DOI 10.1162/NECO_a_00587
   Dauwels Justin, 2008, ADV NEUROINFORMATION, P177
   Denève S, 2017, NEURON, V94, P969, DOI 10.1016/j.neuron.2017.05.016
   Dora S, 2015, APPL SOFT COMPUT, V36, P255, DOI 10.1016/j.asoc.2015.06.062
   Dora S, 2014, IEEE IJCNN, P2415, DOI 10.1109/IJCNN.2014.6889775
   Dorogyy Y, 2016, 2016 13TH INTERNATIONAL CONFERENCE ON MODERN PROBLEMS OF RADIO ENGINEERING, TELECOMMUNICATIONS AND COMPUTER SCIENCE (TCSET), P124, DOI 10.1109/TCSET.2016.7451989
   Du ZD, 2015, PROCEEDINGS OF THE 48TH ANNUAL IEEE/ACM INTERNATIONAL SYMPOSIUM ON MICROARCHITECTURE (MICRO-48), P494, DOI 10.1145/2830772.2830789
   Fang Hui-juan, 2008, Journal of Applied Sciences, V26, P638
   Farsa EZ, 2019, IEEE T CIRCUITS-II, V66, P1582, DOI 10.1109/TCSII.2019.2890846
   Feldman DE, 2012, NEURON, V75, P556, DOI 10.1016/j.neuron.2012.08.001
   Franosch JMP, 2013, NEURAL COMPUT, V25, P3113, DOI 10.1162/NECO_a_00520
   Frémaux N, 2010, J NEUROSCI, V30, P13326, DOI 10.1523/JNEUROSCI.6249-09.2010
   Fusi S, 2000, NEURAL COMPUT, V12, P2227, DOI 10.1162/089976600300014917
   Gardner B, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0161335
   Gardner B, 2015, NEURAL COMPUT, V27, P2548, DOI 10.1162/NECO_a_00790
   Gardner B, 2013, LECT NOTES COMPUT SC, V8131, P256, DOI 10.1007/978-3-642-40728-4_32
   GEORGOPOULOS AP, 1986, SCIENCE, V233, P1416, DOI 10.1126/science.3749885
   Gerstner W., 2002, SPIKING NEURON MODEL
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Gilra A, 2017, ELIFE, V6, DOI 10.7554/eLife.28295
   Glackin C, 2011, NEURAL NETWORKS, V24, P247, DOI 10.1016/j.neunet.2010.11.008
   Glaser JI, 2019, PROG NEUROBIOL, V175, P126, DOI 10.1016/j.pneurobio.2019.01.008
   Gütig R, 2014, CURR OPIN NEUROBIOL, V25, P134, DOI 10.1016/j.conb.2014.01.004
   Guo LL, 2017, INT J NEURAL SYST, V27, DOI 10.1142/S0129065717500022
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Haykin S., 2009, NEURAL NETWORKS LEAR, V3rd ed.
   HEBB D. O., 1949
   Henker S, 2012, J COMPUT NEUROSCI, V32, P309, DOI 10.1007/s10827-011-0353-9
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V116, P449, DOI 10.1113/jphysiol.1952.sp004717
   HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554
   Hu J, 2013, NEURAL COMPUT, V25, P450, DOI 10.1162/NECO_a_00395
   Huh D., 2018, ADV NEURAL INFORM PR, P1438
   Illing B, 2019, NEURAL NETWORKS, V118, P90, DOI 10.1016/j.neunet.2019.06.001
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, NEURAL COMPUT, V15, P1511, DOI 10.1162/089976603321891783
   Jeyasothy A, 2019, IEEE T NEUR NET LEAR, V30, P1231, DOI 10.1109/TNNLS.2018.2868874
   Kaabi MG, 2011, NEURAL COMPUT, V23, P1187, DOI 10.1162/NECO_a_00112
   Kasabov N., 2018, TIME SPACE SPIKING N, DOI 10.1007/978-3-662-57715-8
   Kasabov N, 2013, NEURAL NETWORKS, V41, P188, DOI 10.1016/j.neunet.2012.11.014
   Kasabov NK, 2014, NEURAL NETWORKS, V52, P62, DOI 10.1016/j.neunet.2014.01.006
   Kasinski A., 2006, International Journal of Applied Mathematics and Computer Science, V16, P101
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Kiselev M, 2017, IEEE IJCNN, P3806, DOI 10.1109/IJCNN.2017.7966336
   KNUDSEN EI, 1994, J NEUROSCI, V14, P3985
   Kreuz T, 2007, J NEUROSCI METH, V165, P151, DOI 10.1016/j.jneumeth.2007.05.031
   Kulkarni SR, 2018, NEURAL NETWORKS, V103, P118, DOI 10.1016/j.neunet.2018.03.019
   Kuriscak E, 2015, NEUROCOMPUTING, V152, P27, DOI 10.1016/j.neucom.2014.11.022
   Kusmierz L, 2017, CURR OPIN NEUROBIOL, V46, P170, DOI 10.1016/j.conb.2017.08.020
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lee WW, 2017, IEEE T NEUR NET LEAR, V28, P849, DOI 10.1109/TNNLS.2015.2509479
   Legenstein R, 2005, NEURAL COMPUT, V17, P2337, DOI 10.1162/0899766054796888
   Legenstein R, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.1000180
   Legenstein Robert, 2008, ADV NEURAL INFORM PR, P881
   Leon SJ, 2013, NUMER LINEAR ALGEBR, V20, P492, DOI 10.1002/nla.1839
   Li CY, 2004, NEURON, V41, P257, DOI 10.1016/S0896-6273(03)00847-X
   Lillicrap TP, 2019, CURR OPIN NEUROBIOL, V55, P82, DOI 10.1016/j.conb.2019.01.011
   Lin P, 2019, NEURAL COMPUT APPL, V31, P3933, DOI 10.1007/s00521-017-3336-6
   Lin Xiang-hong, 2009, Acta Electronica Sinica, V37, P1270
   Lin XH, 2018, LECT NOTES COMPUT SC, V11139, P222, DOI 10.1007/978-3-030-01418-6_22
   Lin XH, 2018, LECT NOTES COMPUT SC, V10954, P243, DOI 10.1007/978-3-319-95930-6_23
   Lin XH, 2017, NEUROCOMPUTING, V237, P59, DOI 10.1016/j.neucom.2016.08.087
   [蔺想红 Lin Xianghong], 2016, [电子学报, Acta Electronica Sinica], V44, P2877
   Lin XH, 2016, LECT NOTES ARTIF INT, V9773, P44, DOI 10.1007/978-3-319-42297-8_5
   [蔺想红 Lin Xianghong], 2015, [电子学报, Acta Electronica Sinica], V43, P577
   Lin Xianghong, 2011, Computer Engineering and Applications, V47, P41, DOI 10.3778/j.issn.1002-8331.2011.35.012
   Lin ZT, 2018, NEUROCOMPUTING, V275, P94, DOI 10.1016/j.neucom.2017.05.009
   Lobo JL, 2020, NEURAL NETWORKS, V121, P88, DOI 10.1016/j.neunet.2019.09.004
   Luo XL, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00559
   Luo Y, 2017, INT CONF ASIC, P565, DOI 10.1109/ASICON.2017.8252538
   Luo YL, 2018, LECT NOTES ARTIF INT, V11013, P29, DOI 10.1007/978-3-319-97310-4_4
   Ma Q, 2018, IOP CONF SER-MAT SCI, V435, DOI 10.1088/1757-899X/435/1/012049
   Maass W, 1997, NETWORK-COMP NEURAL, V8, P355, DOI 10.1088/0954-898X/8/4/002
   Maass W, 1996, NEURAL COMPUT, V8, P1, DOI 10.1162/neco.1996.8.1.1
   Maass W, 1997, NEURAL COMPUT, V9, P279, DOI 10.1162/neco.1997.9.2.279
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Matsuda S, 2016, IEEE IJCNN, P293, DOI 10.1109/IJCNN.2016.7727211
   Matsuda S, 2016, LECT NOTES COMPUT SC, V9948, P56, DOI 10.1007/978-3-319-46672-9_7
   Mattia M, 2000, NEURAL COMPUT, V12, P2305, DOI 10.1162/089976600300014953
   McCulloch WS., 1943, B MATH BIOPHYS, V5, P115, DOI DOI 10.1007/BF02478259
   McKennoch S, 2006, IEEE IJCNN, P3970
   McKennoch S, 2009, NEURAL COMPUT, V21, P9, DOI 10.1162/neco.2008.09-07-610
   Memmesheimer RM, 2014, NEURON, V82, P925, DOI 10.1016/j.neuron.2014.03.026
   Minsky M.L, 1969, PERCEPTRONS
   Mohemmed A, 2013, NEUROCOMPUTING, V107, P3, DOI 10.1016/j.neucom.2012.08.034
   Mohemmed A, 2012, INT J NEURAL SYST, V22, DOI 10.1142/S0129065712500128
   Mohemmed A, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P2969, DOI 10.1109/IJCNN.2011.6033611
   Mostafa H, 2018, IEEE T NEUR NET LEAR, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Nadasdy Z, 2009, FRONT SYST NEUROSCI, V3, DOI 10.3389/neuro.06.006.2009
   Naud R, 2011, NEURAL COMPUT, V23, P3016, DOI 10.1162/NECO_a_00208
   Oniz Y, 2014, J FRANKLIN I, V351, P3269, DOI 10.1016/j.jfranklin.2014.03.002
   Paiva ARC, 2009, NEURAL COMPUT, V21, P424, DOI 10.1162/neco.2008.09-07-614
   Park IM, 2013, IEEE SIGNAL PROC MAG, V30, P149, DOI 10.1109/MSP.2013.2251072
   Park IM, 2012, NEURAL COMPUT, V24, P2223, DOI 10.1162/NECO_a_00309
   Pavlidis NG, 2005, IEEE IJCNN, P2190
   Peng X, 2018, NEUROCOMPUTING, V292, P121, DOI 10.1016/j.neucom.2018.02.085
   Petro B, 2020, IEEE T NEUR NET LEAR, V31, P358, DOI 10.1109/TNNLS.2019.2906158
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Pfister JP, 2006, J NEUROSCI, V26, P9673, DOI 10.1523/JNEUROSCI.1425-06.2006
   Pfister JP, 2006, NEURAL COMPUT, V18, P1318, DOI 10.1162/neco.2006.18.6.1318
   Ponulak F., 2006, THESIS
   Ponulak F, 2008, INT J APPL MATH COMP, V18, P117, DOI 10.2478/v10006-008-0011-1
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Prieto A, 2016, NEUROCOMPUTING, V214, P242, DOI 10.1016/j.neucom.2016.06.014
   Pyle R, 2017, PHYS REV LETT, V118, DOI 10.1103/PhysRevLett.118.018103
   Qu H, 2015, NEUROCOMPUTING, V151, P310, DOI 10.1016/j.neucom.2014.09.034
   Quiroga RQ, 2013, PRINCIPLES OF NEURAL CODING, P1, DOI 10.1201/b14756
   Rezende Danilo J, 2011, ADV NEURAL INFORM PR, P136
   Rezende DJ, 2014, FRONT COMPUT NEUROSC, V8, DOI 10.3389/fncom.2014.00038
   Ros E, 2006, NEURAL COMPUT, V18, P2959, DOI 10.1162/neco.2006.18.12.2959
   ROSENBLATT F, 1958, PSYCHOL REV, V65, P386, DOI 10.1037/h0042519
   Roy S, 2016, IEEE T NEUR NET LEAR, V27, P1572, DOI 10.1109/TNNLS.2015.2447011
   Ruf B, 1997, NEURAL PROCESS LETT, V5, P9, DOI 10.1023/A:1009697008681
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Schæfer M, 2002, NEUROCOMPUTING, V48, P647, DOI 10.1016/S0925-2312(01)00633-6
   Schiess M, 2016, PLOS COMPUT BIOL, V12, DOI 10.1371/journal.pcbi.1004638
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Schrauwen B, 2004, IEEE IJCNN, P471, DOI 10.1109/IJCNN.2004.1379954
   Schrauwen B, 2003, IEEE IJCNN, P2825
   Schrauwen B, 2004, P 15 PRORISC WORKSH, P301
   Schrauwen B, 2006, IEEE IJCNN, P1797
   Schreiber S, 2003, NEUROCOMPUTING, V52-4, P925, DOI 10.1016/S0925-2312(02)00838-X
   Sengupta N, 2018, IEEE T NEUR NET LEAR, V29, P5249, DOI 10.1109/TNNLS.2018.2796023
   Shrestha SB, 2018, IEEE T NEUR NET LEAR, V29, P3126, DOI 10.1109/TNNLS.2017.2713125
   Shrestha SB, 2017, NEURAL NETWORKS, V96, P33, DOI 10.1016/j.neunet.2017.08.010
   Shrestha SB, 2015, NEURAL NETWORKS, V63, P185, DOI 10.1016/j.neunet.2014.12.001
   Shrestha SB, 2017, NEURAL NETWORKS, V86, P54, DOI 10.1016/j.neunet.2016.10.011
   Shrestha SB, 2016, IEEE IJCNN, P277, DOI 10.1109/IJCNN.2016.7727209
   Soula H, 2006, NEURAL COMPUT, V18, P60, DOI 10.1162/089976606774841567
   Sporea I, 2013, NEURAL COMPUT, V25, P473, DOI 10.1162/NECO_a_00396
   Strain TJ, 2010, INT J NEURAL SYST, V20, P463, DOI 10.1142/S0129065710002553
   Taherkhani A., 2019, IEEE T COGNITIVE DEV
   Taherkhani A., 2015, NEURAL NETWORKS IJCN, P1
   Taherkhani A, 2018, IEEE T NEUR NET LEAR, V29, P5394, DOI 10.1109/TNNLS.2018.2797801
   Taherkhani A, 2015, IEEE T NEUR NET LEAR, V26, P3137, DOI 10.1109/TNNLS.2015.2404938
   Tapson JC, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00153
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Tavanaei A, 2019, NEUROCOMPUTING, V330, P39, DOI 10.1016/j.neucom.2018.11.014
   Thiruvarudchelvan Vaenthan, 2013, Neural Information Processing. 20th International Conference, ICONIP 2013. Proceedings: LNCS 8227, P172, DOI 10.1007/978-3-642-42042-9_22
   Tino P, 2006, NEURAL COMPUT, V18, P591, DOI 10.1162/089976606775623360
   Toyota T, 2016, 2016 12TH INTERNATIONAL CONFERENCE ON NATURAL COMPUTATION, FUZZY SYSTEMS AND KNOWLEDGE DISCOVERY (ICNC-FSKD), P1773, DOI 10.1109/FSKD.2016.7603446
   Tuckwell HC, 2005, PHYSICA A, V351, P427, DOI 10.1016/j.physa.2004.11.059
   Urbanczik R, 2014, NEURON, V81, P521, DOI 10.1016/j.neuron.2013.11.030
   Urbanczik R, 2009, NEURAL COMPUT, V21, P340, DOI 10.1162/neco.2008.09-07-605
   Vazquez R. A., 2015, COMPUTATIONAL INTELL, V2015, P18
   Vázquez RA, 2011, LECT NOTES COMPUT SC, V6728, P242, DOI 10.1007/978-3-642-21515-5_29
   Wade JJ, 2010, IEEE T NEURAL NETWOR, V21, P1817, DOI 10.1109/TNN.2010.2074212
   Walter F, 2016, NEURAL PROCESS LETT, V44, P103, DOI 10.1007/s11063-015-9478-6
   Wang JF, 2010, IMECE 2009: PROCEEDINGS OF THE ASME INTERNATIONAL MECHANICAL ENGINEERING CONGRESS AND EXPOSITION, VOL 9, PTS A-C, P1
   Wang JL, 2017, IEEE T NEUR NET LEAR, V28, P30, DOI 10.1109/TNNLS.2015.2501322
   Wang JL, 2014, NEUROCOMPUTING, V144, P526, DOI 10.1016/j.neucom.2014.04.017
   Wang XW, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00252
   Wang XW, 2016, LECT NOTES COMPUT SC, V9772, P95, DOI 10.1007/978-3-319-42294-7_8
   WERBOS PJ, 1990, P IEEE, V78, P1550, DOI 10.1109/5.58337
   Whalley K, 2013, NAT REV NEUROSCI, V14, DOI 10.1038/nrn3532
   Whittington JCR, 2019, TRENDS COGN SCI, V23, P235, DOI 10.1016/j.tics.2018.12.005
   Whittington JCR, 2017, NEURAL COMPUT, V29, P1229, DOI 10.1162/NECO_a_00949
   Wu QX, 2006, NEUROCOMPUTING, V69, P1912, DOI 10.1016/j.neucom.2005.11.023
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Wysoski SG, 2006, LECT NOTES COMPUT SC, V4131, P61
   Xie XR, 2017, IEEE T NEUR NET LEAR, V28, P1411, DOI 10.1109/TNNLS.2016.2541339
   Xie XR, 2017, NEUROCOMPUTING, V241, P152, DOI 10.1016/j.neucom.2017.01.086
   Xie XR, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0150329
   Xin JG, 2001, IEEE IJCNN, P1772, DOI 10.1109/IJCNN.2001.938430
   Xu Y, 2019, NEURAL NETWORKS, V116, P11, DOI 10.1016/j.neunet.2019.03.017
   Xu Y, 2017, NEURAL NETWORKS, V93, P7, DOI 10.1016/j.neunet.2017.04.010
   Xu Y, 2013, NEURAL COMPUT, V25, P1472, DOI 10.1162/NECO_a_00450
   Xu Y, 2013, NEURAL NETWORKS, V43, P99, DOI 10.1016/j.neunet.2013.02.003
   Yang J, 2012, APPL MATH LETT, V25, P1118, DOI 10.1016/j.aml.2012.02.016
   Yang WY, 2012, NEURAL PROCESS LETT, V36, P135, DOI 10.1007/s11063-012-9226-0
   Yu Q, 2019, IEEE T CYBERNETICS, V49, P2178, DOI 10.1109/TCYB.2018.2821692
   Yu Q, 2017, INTEL SYST REF LIBR, V126, P115, DOI 10.1007/978-3-319-55310-8_6
   Yu Q, 2016, IEEE T NEUR NET LEAR, V27, P621, DOI 10.1109/TNNLS.2015.2416771
   Yu Q, 2014, NEUROCOMPUTING, V138, P3, DOI 10.1016/j.neucom.2013.06.052
   Yu Q, 2013, IEEE T NEUR NET LEAR, V24, P1539, DOI 10.1109/TNNLS.2013.2245677
   Yu QF, 2013, PLOS ONE, V8, DOI [10.1371/journal.pone.0059627, 10.1371/journal.pone.0078318]
   Zenke F, 2018, NEURAL COMPUT, V30, P1514, DOI 10.1162/neco_a_01086
   Zhang ML, 2019, IEEE T NEUR NET LEAR, V30, P123, DOI 10.1109/TNNLS.2018.2833077
   Zhang ML, 2018, IEEE T COGN DEV SYST, V10, P151, DOI 10.1109/TCDS.2017.2651943
   Zhang ML, 2017, NEUROCOMPUTING, V219, P333, DOI 10.1016/j.neucom.2016.09.044
   Zhang YQ, 2018, SENS IMAGING, V19, DOI 10.1007/s11220-018-0192-0
   [张玉平 Zhang Yuping], 2015, [计算机工程与科学, Computer Engineering and Science], V37, P348
   Zhao B, 2014, BIOMED CIRC SYST C, P667, DOI 10.1109/BioCAS.2014.6981814
   Zheng G, 2009, J COMPUT NEUROSCI, V26, P409, DOI 10.1007/s10827-008-0119-1
   Zheng N, 2018, IEEE T NANOTECHNOL, V17, P520, DOI 10.1109/TNANO.2018.2821131
   Zheng N, 2018, IEEE T NEUR NET LEAR, V29, P4287, DOI 10.1109/TNNLS.2017.2761335
NR 223
TC 97
Z9 102
U1 28
U2 232
PD MAY
PY 2020
VL 125
BP 258
EP 280
DI 10.1016/j.neunet.2020.02.011
UT WOS:000523306100024
DA 2023-11-16
ER

PT C
AU Wu, QX
   McGinnity, M
   Maguire, L
   Glackin, B
   Belatreche, A
AF Wu, QingXiang
   McGinnity, Martin
   Maguire, Liam
   Glackin, Brendan
   Belatreche, Ammar
BE Chen, K
   Wang, L
TI Learning mechanisms in networks of spiking neurons
SO TRENDS IN NEURAL COMPUTATION
SE Studies in Computational Intelligence
DT Proceedings Paper
CT 1st International Conference on Natural Computation (ICNC 2005)
CY AUG 27-29, 2005
CL Changsha, PEOPLES R CHINA
DE spiking neural networks; learning mechanism; spiking neuron models;
   spike timing-dependent plasticity; neuron encoding; co-ordinate
   transformation
ID SYNAPTIC PLASTICITY; SPATIAL ATTENTION; PARIETAL CORTEX; VISION; TOUCH;
   RESPONSES; MONKEY; MODEL; LINKS
AB In spiking neural networks, signals are transferred by action potentials. The information is encoded in the patterns of neuron activities or spikes. These features create significant differences between spiking neural networks and classical neural networks. Since spiking neural networks are based on spiking neuron models that are very close to the biological neuron model, many of the principles found in biological neuroscience can be used in the networks. In this chapter, a number of learning mechanisms for spiking neural networks are introduced. The learning mechanisms can be applied to explain the behaviours of networks in the brain; and also can be applied to artificial intelligent systems to process complex information represented by biological stimuli.
C1 [Wu, QingXiang] Fuzhou Univ, Sch Phys & OptoElectron Technol, Fuzhou 350007, Peoples R China.
   [McGinnity, Martin; Maguire, Liam; Glackin, Brendan; Belatreche, Ammar] Univ Ulster, Sch Comp & Intelligent Syst, Coleraine BT48 7JL, Londonderry, North Ireland.
RP Wu, QX (corresponding author), Fuzhou Univ, Sch Phys & OptoElectron Technol, Fuzhou 350007, Peoples R China.
CR [Anonymous], 2000, J NEUROSCI
   Atkins JE, 2003, VISION RES, V43, P2603, DOI 10.1016/S0042-6989(03)00470-X
   Bell CC, 1999, J EXP BIOL, V202, P1339
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Bi GQ, 1999, NATURE, V401, P792, DOI 10.1038/44573
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Choe Y, 1998, NEUROCOMPUTING, V21, P139, DOI 10.1016/S0925-2312(98)00040-X
   Colby CL, 1999, ANNU REV NEUROSCI, V22, P319, DOI 10.1146/annurev.neuro.22.1.319
   Dayan P., 2001, THEORETICAL NEUROSCI
   Deneve S, 2001, NAT NEUROSCI, V4, P826, DOI 10.1038/90541
   Eimer M, 2000, PSYCHOPHYSIOLOGY, V37, P697, DOI 10.1111/1469-8986.3750697
   Galati G, 2001, EUR J NEUROSCI, V14, P737, DOI 10.1046/j.0953-816x.2001.01674.x
   Gerstner W., 2002, SPIKING NEURON MODEL
   Graziano M. S. A., 1994, COGNITIVE NEUROSCIEN, P1021
   Gross CG., 1995, NEUROSCIENTIST, V1, P43, DOI DOI 10.1177/107385849500100107
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Iriki A, 1996, NEUROSCI RES, V25, P173
   JOHANSSON RS, 1987, EXP BRAIN RES, V66, P141
   Kennett S, 2001, CURR BIOL, V11, P1188, DOI 10.1016/S0960-9822(01)00327-X
   KOCH C, 1999, BIOPHYS COMPUTATION
   Lysetskiy M, 2002, NEURAL PROCESS LETT, V15, P225, DOI 10.1023/A:1015773115997
   MAASS W, 1991, PROCEEDINGS - 32ND ANNUAL SYMPOSIUM ON FOUNDATIONS OF COMPUTER SCIENCE, P767, DOI 10.1109/SFCS.1991.185447
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   MARISA TC, 2004, NEUROSCI LETT, V354, P22
   Meftah EM, 2002, J NEUROPHYSIOL, V88, P3133, DOI 10.1152/jn.00121.2002
   Melamed O, 2004, TRENDS NEUROSCI, V27, P11, DOI 10.1016/j.tins.2003.10.014
   MULLER E, 2003, HDKIP0322 U HEID
   Rizzolatti G, 1997, CURR OPIN NEUROBIOL, V7, P562, DOI 10.1016/S0959-4388(97)80037-2
   Sirosh J, 1997, NEURAL COMPUT, V9, P577, DOI 10.1162/neco.1997.9.3.577
   SOHN JW, 1999, P INT JOINT C NEUR N, V4, P2590
   Song S, 2001, NEURON, V32, P339, DOI 10.1016/S0896-6273(01)00451-2
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Spence C, 2000, J EXP PSYCHOL HUMAN, V26, P1298, DOI 10.1037//0096-1523.26.4.1298
   Taylor-Clarke M, 2002, CURR BIOL, V12, P233, DOI 10.1016/S0960-9822(01)00681-9
   THEUNISSEN F, 1995, J COMPUT NEUROSCI, V2, P149, DOI 10.1007/BF00961885
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   Wu QX, 2005, LECT NOTES COMPUT SC, V3610, P420
   WU QX, 2006, IN PRESS BRAIN INSPI
   WU QX, 2004, P INT C BRAIN INSP C
   Zhou YD, 2000, P NATL ACAD SCI USA, V97, P9777, DOI 10.1073/pnas.97.17.9777
   2002, IST200134712
NR 41
TC 14
Z9 14
U1 0
U2 3
PY 2007
VL 35
BP 171
EP +
UT WOS:000243355000007
DA 2023-11-16
ER

PT C
AU Bakó, L
   Székely, I
   Dávid, L
   Brassai, TS
AF Bako, Laszlo
   Szekely, Iuliu
   David, Laszlo
   Brassai, Tihamer Sandor
BE Margineanu, I
   Nicolaide, A
   Cernat, M
TI Simulation of spiking neural networks
SO PROCEEDINGS OF THE 9TH INTERNATIONAL CONFERENCE ON OPTIMIZATION OF
   ELECTRICAL AND ELECTRONIC EQUIPMENT, VOL III: INDUSTRIAL AUTOMATION AND
   CONTROL
DT Proceedings Paper
CT 9th International Conference on Optimization of Electrical and
   Electronic Equipment
CY MAY 20-22, 2004
CL Brasov, ROMANIA
DE neural networks; spiking neurons; simulation
AB Neurobiological experiments have lead to the development of the third neural networks generation, the neuromorphic artificial neural networks. One of the potential neural models is that based on the "spiking" neural behavior observed in the biological brain. In the paper the biological background, the mathematical modeling and computer simulation of spiking neural networks are discussed, with application to a letter recognition device.
C1 [Bako, Laszlo; David, Laszlo; Brassai, Tihamer Sandor] SAPIENTIA Hungarian Univ Transilvania, Targu Mures, Romania.
RP Bakó, L (corresponding author), SAPIENTIA Hungarian Univ Transilvania, Targu Mures, Romania.
CR ABELES M, 1982, ISRAEL J MED SCI, V18, P83
   Gerstner W., 2002, SPIKING NEURON MODEL
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   MAAS W, 1999, PULSED NEURAL NETWOR
   MAAS W, 1998, NETWORK COMPUTATION, V9, P1
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Trappenberg TP., 2002, FUNDAMENTALS COMPUTA
NR 7
TC 0
Z9 0
U1 1
U2 3
PY 2004
BP 179
EP 184
UT WOS:000255192200031
DA 2023-11-16
ER

PT C
AU Yu, JQ
   Zhang, WJ
   Zhang, L
AF Yu, Jie-qiong
   Zhang, Wen-juan
   Zhang, Lei
GP Destech Publicat Inc
TI Spiking Neural Network on Curve Fitting
SO 2ND INTERNATIONAL CONFERENCE ON MODELING, SIMULATION AND OPTIMIZATION
   TECHNOLOGIES AND APPLICATIONS (MSOTA 2018)
SE DEStech Transactions on Computer Science and Engineering
DT Proceedings Paper
CT 2nd International Conference on Modeling, Simulation and Optimization
   Technologies and Applications (MSOTA)
CY NOV 25-26, 2018
CL Xiamen, PEOPLES R CHINA
DE Spiking Neural Network; Curve fitting; MATLAB
ID PLASTICITY
AB Currently, curve fitting has been widely used in data processing. Spiking Neural Network is used to provide fast and accurate curve fitting with discrete data in this paper. First, the principle of Spiking Neural Network is introduced. Second, the Spiking Neural Network for curve fitting based on the MATLAB simulation platform is established. Finally, curve fitting of a linear function and an exponential function are made. The average error values are 0 and 0.3, the results show that Spiking Neural Network can be used in curve fitting effectively.
C1 [Zhang, Lei] Northeast Normal Univ, Sch Phys, Changchun 130024, Peoples R China.
   Northeast Normal Univ, Natl Demonstrat Ctr Expt Phys Educ, Changchun 130024, Peoples R China.
RP Zhang, L (corresponding author), Northeast Normal Univ, Sch Phys, Changchun 130024, Peoples R China.
CR Dan Y, 2004, NEURON, V44, P23, DOI 10.1016/j.neuron.2004.09.007
   Gerstner W., 2002, SPIKING NEURON MODEL
   Jin QT, 2011, ACTA PHYS SIN-CH ED, V60, DOI 10.7498/aps.60.098701
   Kempter R, 1999, PHYS REV E, V59, P4498, DOI 10.1103/PhysRevE.59.4498
   Kim JJ, 2002, NAT REV NEUROSCI, V3, P453, DOI 10.1038/nrn849
   Li C, 2012, ACTA PHYS SIN-CH ED, V61, DOI 10.7498/aps.61.070701
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   SCHUETZE SM, 1983, TRENDS NEUROSCI, V6, P164, DOI 10.1016/0166-2236(83)90078-4
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
NR 9
TC 0
Z9 0
U1 0
U2 4
PY 2018
BP 455
EP 461
UT WOS:000474464100075
DA 2023-11-16
ER

PT J
AU Ponulak, F
   Kasinski, A
AF Ponulak, Filip
   Kasinski, Andrzej
TI Introduction to spiking neural networks: Information processing,
   learning and applications
SO ACTA NEUROBIOLOGIAE EXPERIMENTALIS
DT Review
DE neural code; neural information processing; reinforcement learning;
   spiking neural networks; supervised learning; synaptic plasticity;
   unsupervised learning
ID LONG-TERM POTENTIATION; SYNAPTIC PLASTICITY; DEPENDENT PLASTICITY;
   ASSOCIATIVE MEMORY; NEURONAL NETWORKS; INTERNAL-MODELS; VISUAL-CORTEX;
   CEREBELLUM; DOPAMINE; COMPUTATION
AB The concept that neural information is encoded in the firing rate of neurons has been the dominant paradigm in neurobiology for many years. This paradigm has also been adopted by the theory of artificial neural networks. Recent physiological experiments demonstrate, however, that in many parts of the nervous system, neural code is founded on the timing of individual action potentials. This finding has given rise to the emergence of a new class of neural models, called spiking neural networks. In this paper we summarize basic properties of spiking neurons and spiking networks. Our focus is, specifically, on models of spike-based information coding, synaptic plasticity and learning. We also survey real-life applications of spiking models. The paper is meant to be an introduction to spiking neural networks for scientists from various disciplines interested in spike-based neural processing.
C1 [Ponulak, Filip; Kasinski, Andrzej] Poznan Univ Tech, Inst Control & Informat Engn, Poznan, Poland.
   [Ponulak, Filip] Princeton Univ, Princeton Neurosci Inst, Princeton, NJ 08544 USA.
   [Ponulak, Filip] Princeton Univ, Dept Mol Biol, Princeton, NJ 08544 USA.
RP Ponulak, F (corresponding author), Poznan Univ Tech, Inst Control & Informat Engn, Poznan, Poland.
EM filip.ponulak@put.poznan.pl
CR Abbott LF, 2000, NAT NEUROSCI, V3, P1178, DOI 10.1038/81453
   Abeles M., 1982, LOCAL CORTICAL CIRCU, DOI DOI 10.1007/978-3-642-81708-3
   Abeles M., 1994, TEMPORAL CODING BRAI, P39
   Achard P, 2008, FRONT COMPUT NEUROSC, V2, DOI 10.3389/neuro.10.008.2008
   Adrian ED, 1926, J PHYSIOL-LONDON, V61, P151, DOI 10.1113/jphysiol.1926.sp002281
   ALBUS J S, 1971, Mathematical Biosciences, V10, P25, DOI 10.1016/0025-5564(71)90051-4
   Amit DJ, 2003, NEURAL COMPUT, V15, P565, DOI 10.1162/089976603321192086
   [Anonymous], 2003, ADV NEURAL INFORM PR
   [Anonymous], 1991, INTRO THEORY NEURAL
   [Anonymous], P 15 PRORISC WORKSH
   [Anonymous], 1960, IRE WESCON CONVENTIO, DOI DOI 10.21236/AD0241531
   Atiya AF, 2000, IEEE T NEURAL NETWOR, V11, P697, DOI 10.1109/72.846741
   Bair W, 1996, NEURAL COMPUT, V8, P1185, DOI 10.1162/neco.1996.8.6.1185
   Bao SW, 2001, NATURE, V412, P79, DOI 10.1038/35083586
   Baras D, 2007, NEURAL COMPUT, V19, P2245, DOI 10.1162/neco.2007.19.8.2245
   Barea R., 2000, 8th European Symposium on Artificial Neural Networks. ESANN"2000. Proceedings, P233
   Barlow HB, 1989, NEURAL COMPUT, V1, P295, DOI 10.1162/neco.1989.1.3.295
   Baudry M, 1998, NEUROBIOL LEARN MEM, V70, P113, DOI 10.1006/nlme.1998.3842
   BAUER HU, 1993, PHYSICA D, V69, P380, DOI 10.1016/0167-2789(93)90101-6
   Beierholm U, 2001, J NEUROPHYSIOL, V86, P1858, DOI 10.1152/jn.2001.86.4.1858
   Belatreche A, 2003, P IEEE CYB INT CHALL, P39
   Belatreche A, 2007, SOFT COMPUT, V11, P239, DOI [10.1007/s00500-006-0065-7, 10.1007/S00500-006-0065-7]
   Belter D, 2008, P REC ADV NEUR S SEN, P14
   Bennett MR, 1999, BRAIN RES BULL, V50, P95, DOI 10.1016/S0361-9230(99)00094-5
   Berry MJ, 1997, P NATL ACAD SCI USA, V94, P5411, DOI 10.1073/pnas.94.10.5411
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Bohte S, 2000, P EUR S ART NEUR NET, P419
   Bohte S, 2003, THESIS U AMSTERDAM A
   Bohte Sander M., 2004, Natural Computing, V3, P195, DOI 10.1023/B:NACO.0000027755.02868.60
   Bohte SM, 2005, INFORM PROCESS LETT, V95, P519, DOI 10.1016/j.ipl.2005.05.018
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   Booij O, 2005, INFORM PROCESS LETT, V95, P552, DOI 10.1016/j.ipl.2005.05.023
   Borst A, 1999, NAT NEUROSCI, V2, P947, DOI 10.1038/14731
   Brembs B, 2002, SCIENCE, V296, P1706, DOI 10.1126/science.1069434
   Brody CD, 2003, NEURON, V37, P843, DOI 10.1016/S0896-6273(03)00120-X
   Burgsteiner Harald, 2005, P 9 INT C ENG APPL N, P129
   Buzsaki G., 2006, RHYTHMS BRAIN, DOI 10.1093/acprof:oso/9780195301069.001.0001
   C. Blake, 1998, UCI REPOSITORY MACHI
   Carey MR, 2005, NAT NEUROSCI, V8, P813, DOI 10.1038/nn1470
   Carrillo RR, 2008, BIOSYSTEMS, V94, P18, DOI 10.1016/j.biosystems.2008.05.008
   Cassidy A, 2006, P IEEE INT WORKSH BI
   Chapin J. K., 2000, NEURAL PROSTHESES RE
   Chen HT, 2011, IEEE T BIOMED CIRC S, V5, P160, DOI 10.1109/TBCAS.2010.2075928
   Choe Y, 2000, SEVENTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-2001) / TWELFTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE (IAAI-2000), P123
   Cios KJ, 2000, P SOC PHOTO-OPT INS, V4055, P324, DOI 10.1117/12.380586
   Citri A, 2008, NEUROPSYCHOPHARMACOL, V33, P18, DOI 10.1038/sj.npp.1301559
   Clopath C, 2008, ADV NEURAL INFORM PR, V20, P321
   Conn K, 2007, SUPERVISED REINFORCE
   De Schutter E, 2009, NEUROSCIENCE, V162, P816, DOI 10.1016/j.neuroscience.2009.02.040
   de Sousa G, 2009, LECT NOTES COMPUT SC, V5495, P413, DOI 10.1007/978-3-642-04921-7_42
   deCharms RC, 1998, P NATL ACAD SCI USA, V95, P15166, DOI 10.1073/pnas.95.26.15166
   Di Paolo EA, 2003, PHILOS T R SOC A, V361, P2299, DOI 10.1098/rsta.2003.1256
   Di Paolo EA, 2002, ADAPT BEHAV, V10, P243, DOI 10.1177/1059712302010003006
   Diesmann M, 1999, NATURE, V402, P529, DOI 10.1038/990101
   Dominey PF, 2000, LANG COGNITIVE PROC, V15, P87, DOI 10.1080/016909600386129
   Doya K, 1999, NEURAL NETWORKS, V12, P961, DOI 10.1016/S0893-6080(99)00046-5
   Du Bois-Reymond EH, 1848, STUDIES ANIMAL ELECT, V1
   ECKHORN R, 1988, BIOL CYBERN, V60, P121, DOI 10.1007/BF00202899
   Escobar MJ, 2009, INT J COMPUT VISION, V82, P284, DOI 10.1007/s11263-008-0201-1
   Faisal AA, 2008, NAT REV NEUROSCI, V9, P292, DOI 10.1038/nrn2258
   Farries MA, 2007, J NEUROPHYSIOL, V98, P3648, DOI 10.1152/jn.00364.2007
   FETZ EE, 1973, J NEUROPHYSIOL, V36, P179, DOI 10.1152/jn.1973.36.2.179
   Finelli LA, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.1000062
   Florian RV, 2007, NEURAL COMPUT, V19, P1468, DOI 10.1162/neco.2007.19.6.1468
   Florian RV, 2005, Seventh International Symposium on Symbolic and Numeric Algorithms for Scientific Computing, Proceedings, P299
   GAZE RM, 1970, PROC R SOC SER B-BIO, V175, P107, DOI 10.1098/rspb.1970.0015
   GEORGOPOULOS AP, 1986, ANNU REV NEUROSCI, V9, P147, DOI 10.1146/annurev.ne.09.030186.001051
   Gerstner W, 2002, BIOL CYBERN, V87, P404, DOI 10.1007/s00422-002-0353-y
   GERSTNER W, 1993, PHYS REV LETT, V71, P312, DOI 10.1103/PhysRevLett.71.312
   GERSTNER W, 1992, NETWORK-COMP NEURAL, V3, P139, DOI 10.1088/0954-898X/3/2/004
   Gerstner W, 1996, NATURE, V383, P76, DOI 10.1038/383076a0
   Gerstner W., 2002, SPIKING NEURON MODEL
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   Girard P, 2008, ANIM COGN, V11, P485, DOI 10.1007/s10071-008-0139-2
   Glackin C, 2008, LECT NOTES COMPUT SC, V5164, P258, DOI 10.1007/978-3-540-87559-8_27
   GRAY CM, 1989, P NATL ACAD SCI USA, V86, P1698, DOI 10.1073/pnas.86.5.1698
   GRAY CM, 1989, NATURE, V338, P334, DOI 10.1038/338334a0
   Gütig R, 2009, PLOS BIOL, V7, DOI 10.1371/journal.pbio.1000141
   Gurden H, 2000, J NEUROSCI, V20, part. no., DOI 10.1523/JNEUROSCI.20-22-j0003.2000
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Guyonneau R, 2004, J PHYSIOL-PARIS, V98, P487, DOI 10.1016/j.jphysparis.2005.09.004
   Hausler S., 2003, Complexity, V8, P39, DOI 10.1002/cplx.10089
   Hebb D., 2005, ORG BEHAV NEUROPSYCH
   Hines ML, 2004, J COMPUT NEUROSCI, V17, P7, DOI 10.1023/B:JCNS.0000023869.22017.2e
   Hinton G. E., 1999, UNSUPERVISED LEARNIN
   Hofstötter C, 2002, EUR J NEUROSCI, V16, P1361, DOI 10.1046/j.1460-9568.2002.02182.x
   Hopfield IJ, 2000, P NATL ACAD SCI USA, V97, P13919, DOI 10.1073/pnas.250483697
   Hopfield JJ, 2001, P NATL ACAD SCI USA, V98, P1282, DOI 10.1073/pnas.031567098
   HOPFIELD JJ, 1995, P NATL ACAD SCI USA, V92, P6655, DOI 10.1073/pnas.92.15.6655
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   Hopfield JJ, 2010, P NATL ACAD SCI USA, V107, P1648, DOI 10.1073/pnas.0913991107
   Huys QJM, 2007, NEURAL COMPUT, V19, P404, DOI 10.1162/neco.2007.19.2.404
   Inagaki K, 2008, LECT NOTES COMPUT SC, V4984, P902
   Ito M, 2005, PROG BRAIN RES, V148, P95, DOI 10.1016/S0079-6123(04)48009-1
   Ito M, 2000, IMAGE, LANGUAGE, BRAIN, P149
   Ito M, 2000, BRAIN RES, V886, P237, DOI 10.1016/S0006-8993(00)03142-5
   Ito M, 2008, NAT REV NEUROSCI, V9, P304, DOI 10.1038/nrn2332
   Izhikevich EM, 2002, BIOSYSTEMS, V67, P95, DOI 10.1016/S0303-2647(02)00067-9
   Izhikevich EM, 2003, TRENDS NEUROSCI, V26, P161, DOI 10.1016/S0166-2236(03)00034-1
   Izhikevich EM, 2008, P NATL ACAD SCI USA, V105, P3593, DOI 10.1073/pnas.0712231105
   Izhikevich EM, 2007, CEREB CORTEX, V17, P2443, DOI 10.1093/cercor/bhl152
   Jaeger H., 2001, SHORT TERM MEMORY EC
   Jaksa R., 1999, Journal of Electrical Engineering, V50, P185
   Jefferys JGR, 1996, TRENDS NEUROSCI, V19, P202, DOI 10.1016/S0166-2236(96)10023-0
   Jensen O, 1996, LEARN MEMORY, V3, P279, DOI 10.1101/lm.3.2-3.279
   Jin DHZ, 2002, PHYS REV E, V65, DOI 10.1103/PhysRevE.65.051922
   Johansson RS, 2004, NAT NEUROSCI, V7, P170, DOI 10.1038/nn1177
   Jörntell H, 2006, NEURON, V52, P227, DOI 10.1016/j.neuron.2006.09.032
   Joshi P, 2005, NEURAL COMPUT, V17, P1715, DOI 10.1162/0899766054026684
   Kandel E. R., 1991, PRINCIPLES NEURAL SC
   Kasinski A., 2006, International Journal of Applied Mathematics and Computer Science, V16, P101
   KAWATO M, 1992, TRENDS NEUROSCI, V15, P445, DOI 10.1016/0166-2236(92)90008-V
   KAWATO M, 1992, BIOL CYBERN, V68, P95, DOI 10.1007/BF00201431
   Kim Elmer K, 2010, Proc Symp Haptic Interface Virtual Env Teleoperator Syst, P195, DOI 10.1109/HAPTIC.2010.5444657
   Kiss T, 2006, INT J INTELL SYST, V21, P903, DOI 10.1002/int.20168
   Klampfl S, 2009, NEURAL COMPUT, V21, P911, DOI 10.1162/neco.2008.01-07-432
   Knoblauch A, 2003, THESIS U ULM ULM
   KNUDSEN EI, 1994, J NEUROSCI, V14, P3985
   KNUDSEN EI, 1991, SCIENCE, V253, P85, DOI 10.1126/science.2063209
   Kornprobst P, 2005, IEEE IJCNN, P1687
   Kroese B, 1996, INTRO NEURAL NETWORK
   Kumar A, 2010, NAT REV NEUROSCI, V11, P615, DOI 10.1038/nrn2886
   Landis F, 2010, NEURAL COMPUT, V22, P273, DOI 10.1162/neco.2009.12-08-926
   Lapique L., 1907, J PHYSL PATHOL GEN, V9, P620, DOI DOI 10.1007/S00422-007-0189-6
   Laurent G, 1996, TRENDS NEUROSCI, V19, P489, DOI 10.1016/S0166-2236(96)10054-0
   Lee K, 2008, NEUROCOMPUTING, V71, P3037, DOI 10.1016/j.neucom.2007.09.009
   Legenstein R, 2005, NEURAL COMPUT, V17, P2337, DOI 10.1162/0899766054796888
   Legenstein R, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.1000180
   Lestienne R, 2001, PROG NEUROBIOL, V65, P545, DOI 10.1016/S0301-0082(01)00019-3
   Linster C, 2010, FRONT COMPUT NEUROSC, V4, DOI 10.3389/fncom.2010.00157
   Liu RC, 2001, J NEUROPHYSIOL, V86, P2789, DOI 10.1152/jn.2001.86.6.2789
   LJUNGBERG T, 1992, J NEUROPHYSIOL, V67, P145, DOI 10.1152/jn.1992.67.1.145
   LOMO T, 1966, ACTA PHYSIOL SCAND, VS 68, P128
   Lovinger DM, 2010, NEUROPHARMACOLOGY, V58, P951, DOI 10.1016/j.neuropharm.2010.01.008
   Lukosevicius M, 2009, COMPUT SCI REV, V3, P127, DOI 10.1016/j.cosrev.2009.03.005
   Lumer ED, 2000, NEURAL COMPUT, V12, P181, DOI 10.1162/089976600300015943
   Ma JF, 2007, NEURAL COMPUT, V19, P2124, DOI 10.1162/neco.2007.19.8.2124
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Maass W, 1996, NEURAL COMPUT, V8, P1, DOI 10.1162/neco.1996.8.1.1
   Maass W, 2004, MATH COMP BIOL SER, P575
   Maass W, 2002, PHYS NEUR N, P373
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maass W, 1998, NEUROMORPHIC SYSTEMS, P21
   Maass W, 2002, P 2 INT WORKSH BIOL
   MAGLEBY KL, 1976, J PHYSIOL-LONDON, V257, P471, DOI 10.1113/jphysiol.1976.sp011379
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   MARR D, 1969, J PHYSIOL-LONDON, V202, P437, DOI 10.1113/jphysiol.1969.sp008820
   Martinez D, 2004, NATO SCI SER II-MATH, V159, P209
   Medina JF, 1999, J NEUROSCI, V19, P7140, DOI 10.1523/JNEUROSCI.19-16-07140.1999
   Melamed O, 2008, J COMPUT NEUROSCI, V25, P308, DOI 10.1007/s10827-008-0080-z
   Miall RC, 1996, NEURAL NETWORKS, V9, P1265, DOI 10.1016/S0893-6080(96)00035-4
   Midtgaard J., 2001, ENCY LIFE SCI, P1, DOI 10.1038/npg.els.0000149
   Mongillo G, 2008, SCIENCE, V319, P1543, DOI 10.1126/science.1150769
   Montgomery J, 2002, BIOL BULL, V203, P238, DOI 10.2307/1543417
   Moore S.C., 2002, THESIS U BATH BATH
   MULDER AJ, 1990, MED BIOL ENG COMPUT, V28, P483, DOI 10.1007/BF02441972
   Muresan R, 2002, P 2 WSEAS INT C ROB
   Natschlaeger T, 1998, NEUROMORPHIC SYSTEMS, P33
   Natschlaeger T., 1999, THESIS TU GRAZ AUSTR
   Natschlager T, 1998, NETWORK-COMP NEURAL, V9, P319, DOI 10.1088/0954-898X/9/3/003
   Neuenschwander S, 1996, NATURE, V379, P728, DOI 10.1038/379728a0
   Nikolic D, 2009, PLOS BIOL, V7, DOI 10.1371/journal.pbio.1000260
   O'Keefe J, 1993, Curr Opin Neurobiol, V3, P917, DOI 10.1016/0959-4388(93)90163-S
   Otani S, 1998, NEUROSCIENCE, V85, P669, DOI 10.1016/S0306-4522(97)00677-5
   Otmakhova NA, 1996, J NEUROSCI, V16, P7478
   Paugam-Moisy H, 2006, RR11 IDIAP MART, V592
   Perrinet L, 2004, NEUROCOMPUTING, V57, P125, DOI 10.1016/j.neucom.2004.01.010
   Perrinet L., 2002, 10th European Symposium on Artificial Neural Networks. ESANN'2002. Proceedings, P313
   Pfister JP, 2006, NEURAL COMPUT, V18, P1318, DOI 10.1162/neco.2006.18.6.1318
   Ponulak F, 2009, P MULT S REINF LEARN
   Ponulak F., 2006, PROC EPFL LATSIS S D, P119
   Ponulak F., 2005, RESUME NEW SUPERVISE
   Ponulak F, 2008, P REC ADV NEUR S SEN, P47
   Ponulak F, 2006, P EUR S ART NEUR NET, P623
   Ponulak F, 2008, P NIN C PERC LEARN M, P56
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Popovic D.B., 2000, CONTROL MOVEMENT PHY
   Raman B, 2004, P ADV NEUR INF PROC, V17, P1105
   Rieke F., 1997, SPIKES EXPLORING NEU
   Rochel O, 2002, P 16 EUR C SOL STAT
   Rojas R, 1996, NEURAL NETWORKS, P149, DOI 10.1007/978-3-642-61068-4{\_}7
   Rom R, 2007, IEEE T NEURAL NETWOR, V18, P542, DOI 10.1109/TNN.2006.890806
   ROSENBLATT F, 1958, PSYCHOL REV, V65, P386, DOI 10.1037/h0042519
   Rosenstein M.T., 2004, HDB LEARNING APPROXI, P359
   Ruf B, 1998, IEEE T NEURAL NETWOR, V9, P575, DOI 10.1109/72.668899
   Ruf B, 1998, THESIS TU GRAZ AUSTR
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Saal HP, 2009, J NEUROSCI, V29, P8022, DOI 10.1523/JNEUROSCI.0665-09.2009
   Sánchez-Montañés MA, 2002, IEEE T NEURAL NETWOR, V13, P619, DOI 10.1109/TNN.2002.1000128
   Savin C, 2010, PLOS COMPUT BIOL, V6, DOI 10.1371/journal.pcbi.1000757
   Sayenko DG, 2007, NEUROSCI LETT, V415, P294, DOI 10.1016/j.neulet.2007.01.037
   Schrauwen B., 2007, P 15 EUR S ART NEUR, P471
   Schrauwen B, 2006, IEEE IJCNN, P1797
   SCHUETZE SM, 1983, TRENDS NEUROSCI, V6, P164, DOI 10.1016/0166-2236(83)90078-4
   Schultz W, 2002, NEURON, V36, P241, DOI 10.1016/S0896-6273(02)00967-4
   SEJNOWSKI IJ, 1977, J THEOR BIOL, V69, P385, DOI 10.1016/0022-5193(77)90146-1
   Sejnowski T.J., 1989, NEURAL MODELS PLAST, V6, P94
   Shadlen MN, 1998, J NEUROSCI, V18, P3870
   Sherrington C. S., 1897, TXB PHYSL, V929
   SHIDARA M, 1993, NATURE, V365, P50, DOI 10.1038/365050a0
   Shin JH, 2010, IEEE T NEURAL NETWOR, V21, P1697, DOI 10.1109/TNN.2010.2050600
   Singer W, 1999, NEURON, V24, P49, DOI 10.1016/S0896-6273(00)80821-1
   Skinner B. F., 1965, SCI HUMAN BEHAV
   Soltani A, 2010, NAT NEUROSCI, V13, P112, DOI 10.1038/nn.2450
   Sommer FT, 2001, NEURAL NETWORKS, V14, P825, DOI 10.1016/S0893-6080(01)00064-8
   Sougné J, 2000, PERSP NEURAL COMP, P23
   Steil JJ, 2004, P IEEE INT JOINT C N
   Stein RB, 2005, NAT REV NEUROSCI, V6, P389, DOI 10.1038/nrn1668
   STEIN RB, 1967, BIOPHYS J, V7, P37, DOI 10.1016/S0006-3495(67)86574-3
   STENT GS, 1973, P NATL ACAD SCI USA, V70, P997, DOI 10.1073/pnas.70.4.997
   Sutton R. S., 2002, REINFORCEMENT LEARNI, V2nd
   Szatmáry B, 2010, PLOS COMPUT BIOL, V6, DOI 10.1371/journal.pcbi.1000879
   THAM CK, 1995, ROBOT AUTON SYST, V15, P247, DOI 10.1016/0921-8890(95)00005-Z
   Thorndike EL, 1901, PERFECT PRINCESS, V2, P1
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   THORPE SJ, 1990, PARALLEL PROCESSING IN NEURAL SYSTEMS AND COMPUTERS, P91
   Thorpe SJ, 1997, ADV NEUR IN, V9, P901
   Tiesinga P, 2008, NAT REV NEUROSCI, V9, P97, DOI 10.1038/nrn2315
   Tino P, 2005, LECT NOTES COMPUT SC, V3611, P666
   Tkacik G, 2010, P NATL ACAD SCI USA, V107, P14419, DOI 10.1073/pnas.1004906107
   Trach WT, 1996, BEHAV BRAIN SCI, V19, P411
   UDIN SB, 1985, CELL MOL NEUROBIOL, V5, P85, DOI 10.1007/BF00711087
   UDIN SB, 1981, J COMP NEUROL, V203, P575, DOI 10.1002/cne.902030403
   Van Rullen R, 2001, NEURAL COMPUT, V13, P1255, DOI 10.1162/08997660152002852
   VanRullen R, 2001, PERCEPTION, V30, P655, DOI 10.1068/p3029
   VanRullen R, 2005, TRENDS NEUROSCI, V28, P1, DOI 10.1016/j.tins.2004.10.010
   vanSteveninck RRD, 1997, SCIENCE, V275, P1805, DOI 10.1126/science.275.5307.1805
   Vasilaki E, 2009, PLOS COMPUT BIOL, V5, DOI 10.1371/journal.pcbi.1000586
   Veredas FJ, 2008, NEURAL NETWORKS, V21, P810, DOI 10.1016/j.neunet.2008.06.006
   Verstraeten D, 2005, INFORM PROCESS LETT, V95, P521, DOI 10.1016/j.ipl.2005.05.019
   VONDERMALSBURG C, 1985, BER BUNSEN PHYS CHEM, V89, P703, DOI 10.1002/bbpc.19850890625
   Wang XJ, 2008, NEURON, V60, P215, DOI 10.1016/j.neuron.2008.09.034
   Werbos P., 1974, REGRESSION NEW TOOLS
   Widrow B., 1962, SELF ORG SYSTEMS, P435
   WITTEN IH, 1977, INFORM CONTROL, V34, P286, DOI 10.1016/S0019-9958(77)90354-0
   Wolpert DM, 1998, TRENDS COGN SCI, V2, P338, DOI 10.1016/S1364-6613(98)01221-2
   Xin JG, 2001, IEEE IJCNN, P1772, DOI 10.1109/IJCNN.2001.938430
   Yamazaki T, 2007, EUR J NEUROSCI, V26, P2279, DOI 10.1111/j.1460-9568.2007.05837.x
   Zamani M., 2010, PROC INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2010.5596806
NR 240
TC 189
Z9 191
U1 10
U2 98
PY 2011
VL 71
IS 4
BP 409
EP 433
UT WOS:000299368200002
DA 2023-11-16
ER

PT C
AU Ltaief, M
   Bezine, H
   Alimi, AM
AF Ltaief, Mahmoud
   Bezine, Hala
   Alimi, Adel M.
BE Madureira, AM
   Abraham, A
   Gamboa, D
   Novais, P
TI Training a Spiking Neural Network to Generate Online Handwriting
   Movements
SO INTELLIGENT SYSTEMS DESIGN AND APPLICATIONS (ISDA 2016)
SE Advances in Intelligent Systems and Computing
DT Proceedings Paper
CT 16th International Conference on Intelligent Systems Design and
   Applications (ISDA)
CY DEC 16-18, 2016
CL Porto, PORTUGAL
DE Spiking neural network; Artificial neural network; Beta-elliptic model;
   Handwriting generation; Similarity degree
ID MODEL
AB In this paper we developed a spiking neural network model that learns to generate online handwriting movements. The architecture is a feed forward network with one hidden layer. The input layer uses a set of Beta elliptic parameters. The hidden layer contains both excitatory and inhibitory neurons. Whereas the output layer provides the script coordinates x(t) and y(t). The proposed spiking neural network has been trained according to Sander Bohet model. The trained spiking neural network has been successfully tested on MAYASTROUN data base. Also a comparative stady between the proposed spiking neural network and an artificial neural network proposed in a previous work is established.
C1 [Ltaief, Mahmoud; Bezine, Hala; Alimi, Adel M.] Univ Sfax, ENIS, REGIM Lab, Res Grp Intelligent Machines, BP 1173, Sfax 3038, Tunisia.
RP Ltaief, M (corresponding author), Univ Sfax, ENIS, REGIM Lab, Res Grp Intelligent Machines, BP 1173, Sfax 3038, Tunisia.
EM mahmoud.ltaief@ieee.org
CR Alimi A. M., 2003, TASK Quarterly, V7, P23
   [Anonymous], INT J COMPUT SCI INF
   [Anonymous], 2001, HDB BIOL PHYS
   Bezine H, 2004, NINTH INTERNATIONAL WORKSHOP ON FRONTIERS IN HANDWRITING RECOGNITION, PROCEEDINGS, P515, DOI 10.1109/IWFHR.2004.45
   Bezine H, 2007, INT J PATTERN RECOGN, V21, P5, DOI 10.1142/S0218001407005272
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Brunel N, 2007, BIOL CYBERN, V97, P337, DOI 10.1007/s00422-007-0190-0
   Gerstner W, 2002, BIOL CYBERN, V87, P404, DOI 10.1007/s00422-002-0353-y
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V116, P449, DOI 10.1113/jphysiol.1952.sp004717
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Li YH, 2007, PATTERN RECOGN LETT, V28, P278, DOI 10.1016/j.patrec.2006.07.009
   Ltaief M, 2012, INT C FRONT HANDWR R, P799
   Ltaief M., 2012, J INTELLIGENT LEARNI, V4, P256
   Maass W., 2001, CURRENT TRENDS THEOR, P680
   Natschläger T, 2002, THEOR COMPUT SCI, V287, P251, DOI 10.1016/S0304-3975(02)00099-3
   Njah S, 2012, INT CONF FRONT HAND, P308, DOI 10.1109/ICFHR.2012.230
   Rusu A, 2004, NINTH INTERNATIONAL WORKSHOP ON FRONTIERS IN HANDWRITING RECOGNITION, PROCEEDINGS, P226, DOI 10.1109/IWFHR.2004.54
   Schomaker L, 1998, ELECTRON COMMUN ENG, V10, P93, DOI 10.1049/ecej:19980302
   Schomaker L. R. B., 1991, THESIS
   Teulings H. L., 1986, EXPT PROTOCOL CURSIV, V419
   VanRullen R, 2005, TRENDS NEUROSCI, V28, P1, DOI 10.1016/j.tins.2004.10.010
NR 21
TC 0
Z9 0
U1 0
U2 1
PY 2017
VL 557
BP 289
EP 298
DI 10.1007/978-3-319-53480-0_29
UT WOS:000406998500029
DA 2023-11-16
ER

PT J
AU Tang, YH
   Zhang, BD
   Wu, JJ
   Hu, TJ
   Zhou, J
   Liu, FD
AF Tang YuHua
   Zhang BaiDa
   Wu JunJie
   Hu TianJiang
   Zhou Jing
   Liu FuDong
TI Parallel architecture and optimization for discrete-event simulation of
   spike neural networks
SO SCIENCE CHINA-TECHNOLOGICAL SCIENCES
DT Article
DE spike neural network; discrete event simulation; intelligent
   parallelization framework
ID NEURONS; MULTIPROCESSOR; INTEGRATE; COMPUTERS
AB Spike neural networks are inspired by animal brains, and outperform traditional neural networks on complicated tasks. However, spike neural networks are usually used on a large scale, and they cannot be computed on commercial, off-the-shelf computers. A parallel architecture is proposed and developed for discrete-event simulations of spike neural networks. Furthermore, mechanisms for both parallelism degree estimation and dynamic load balance are emphasized with theoretical and computational analysis. Simulation results show the effectiveness of the proposed parallelized spike neural network system and its corresponding support components.
C1 [Tang YuHua; Zhang BaiDa; Wu JunJie; Zhou Jing; Liu FuDong] Natl Univ Def Technol, Sch Comp, Dept Comp Sci & Technol, Changsha 410073, Hunan, Peoples R China.
   [Tang YuHua; Zhang BaiDa; Wu JunJie; Zhou Jing; Liu FuDong] Natl Univ Def Technol, State Key Lab High Performance Comp, Changsha 410073, Hunan, Peoples R China.
   [Hu TianJiang] Natl Univ Def Technol, Coll Mechatron Engn & Automat, Changsha 410073, Hunan, Peoples R China.
RP Zhang, BD (corresponding author), Natl Univ Def Technol, Sch Comp, Dept Comp Sci & Technol, Changsha 410073, Hunan, Peoples R China.
EM zhangbaida@gmail.com
CR Alamdari AS, 2004, BIOL NEURAL NETWORKS
   [Anonymous], 1999, HIGH PERFORMANCE CLU
   [Anonymous], 1998, BOOK GENESIS EXPLORI, DOI DOI 10.1007/978-1-4612-1634-63
   Brette R, 2007, J COMPUT NEUROSCI, V23, P349, DOI 10.1007/s10827-007-0038-6
   Brette R, 2007, NEURAL COMPUT, V19, P2604, DOI 10.1162/neco.2007.19.10.2604
   Carnevale N.T., 2006, NEURON BOOK, DOI DOI 10.1017/CBO9780511541612
   Cheung K, 2009, INT C FIELD PROGR TE, P78
   Davison AP, 2008, FRONT NEUROINF, V2, P1858
   Delorme A, 1999, NEUROCOMPUTING, V26-7, P989, DOI 10.1016/S0925-2312(99)00095-8
   Eppler JM, 2008, FRONTIERS NEUROINF, V2, P372
   Gerstner W., 2002, SPIKING NEURON MODEL
   Goddard NH, 1997, PARALLEL GENESIS LAR
   Goodman D, 2008, FRONT NEUROINF, V2, P138
   Goodman DFM, 2009, FRONT NEUROSCI-SWITZ, V3, P192, DOI 10.3389/neuro.01.026.2009
   Hennessy JL., 2003, COMPUTER ARCHITECTUR
   Hines ML, 2008, J COMPUT NEUROSCI, V25, P203, DOI 10.1007/s10827-007-0073-3
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Khan MM, 2008, IEEE IJCNN, P2849, DOI 10.1109/IJCNN.2008.4634199
   Lobb CJ, 2005, Workshop on Principles of Advanced and Distributed Simulation, Proceedings, P16, DOI 10.1109/PADS.2005.18
   Marc-Oliver G., 2007, SCHOLARPEDIA, DOI [DOI 10.4249/SCHOLARPEDIA.1430, 10.4249/scholarpedia.1430]
   Markram H, 2006, NAT REV NEUROSCI, V7, P153, DOI 10.1038/nrn1848
   Meier K, 2005, WIKIPEDIA, V3, P202
   Migliore M, 2006, J COMPUT NEUROSCI, V21, P119, DOI 10.1007/s10827-006-7949-5
   Mohraz K, 1997, IMACS WORLD C SCI CO, P23
   Morrison A, 2005, NEURAL COMPUT, V17, P1776, DOI 10.1162/0899766054026648
   Nageswaran JM, 2009, NEURAL NETWORKS, V22, P791, DOI 10.1016/j.neunet.2009.06.028
   Nageswaran JM, 2009, INT JOINT C NEUR NET, P287
   Pacheco P, 2000, NEUROCOMPUTING, V32, P1095, DOI 10.1016/S0925-2312(00)00283-6
   Pecevski D, 2009, FRONT NEUROINF, V3, P113
   Plesser HE, 2007, LECT NOTES COMPUT SC, V4641, P672
   Strey A, 2004, ADV PAR COM, V13, P201
   Vreeken J, 2002, UUCS2003008 I INF CO
   Wilson EC, 2001, P 10 SIAM C PAR PROC, P12
   Yang XJ, 2011, J COMPUT SCI TECH-CH, V26, P344, DOI 10.1007/s02011-011-1137-8
NR 34
TC 14
Z9 15
U1 0
U2 29
PD FEB
PY 2013
VL 56
IS 2
BP 509
EP 517
DI 10.1007/s11431-012-5084-2
UT WOS:000314913300028
DA 2023-11-16
ER

PT C
AU Lisitsa, D
   Zhilenkov, AA
AF Lisitsa, Daria
   Zhilenkov, Anton A.
GP IEEE
TI Prospects for the Development and Application of Spiking Neural Networks
SO PROCEEDINGS OF THE 2017 IEEE RUSSIA SECTION YOUNG RESEARCHERS IN
   ELECTRICAL AND ELECTRONIC ENGINEERING CONFERENCE (2017 ELCONRUS)
SE IEEE NW Russia Young Researchers in Electrical and Electronic
   Engineering Conference
DT Proceedings Paper
CT IEEE Russia Section Young Researchers in Electrical and Electronic
   Engineering Conference (EIConRus)
CY FEB 01-03, 2017
CL St Petersburg Electrotechn Univ LETI, St Petersburg, RUSSIA
HO St Petersburg Electrotechn Univ LETI
DE Spiking Neural Networks; neuron; synapse; neural network training;
   synaptic communications
AB The article discusses spiking neural networks, their uniqueness, their ability to training, architecture, and the possibility of a hardware implementation. Special attention is given to reveal the prospects for the development and application of spiking neural networks for the implementation in robotics and control systems.
C1 [Lisitsa, Daria; Zhilenkov, Anton A.] St Petersburg Natl Res Univ Informat Technol Mech, Dept Control Syst & Informat, Fac Control Syst & Robot, St Petersburg, Russia.
RP Lisitsa, D (corresponding author), St Petersburg Natl Res Univ Informat Technol Mech, Dept Control Syst & Informat, Fac Control Syst & Robot, St Petersburg, Russia.
EM Lisitsa.da@gmail.com; zhilenkovanton@gmail.com
CR Chernyi, 2015, TRANSPORT TELECOMMUN, V16
   CONNORS BW, 1990, TRENDS NEUROSCI, V13, P99, DOI 10.1016/0166-2236(90)90185-D
   Diehl P.U., 2015, 2015 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2015.7280696
   ERMENTROUT GB, 1986, SIAM J APPL MATH, V46, P233, DOI 10.1137/0146017
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Kiselev M. V., 2009, Proceedings 2009 International Joint Conference on Neural Networks (IJCNN 2009 - Atlanta), P1633, DOI 10.1109/IJCNN.2009.5178580
   Kiselev M, 2011, LECT NOTES COMPUT SC, V6593, P120, DOI 10.1007/978-3-642-20282-7_13
   Nyrkov A., 2016, P 26 INT DAAAM S 201
   Ponulak F, 2011, ACTA NEUROBIOL EXP, V71, P409
   Stanley KO, 2004, J ARTIF INTELL RES, V21, P63, DOI 10.1613/jair.1338
   Zhilenkov A, 2015, PROCEDIA ENGINEER, V100, P1247, DOI 10.1016/j.proeng.2015.01.490
NR 11
TC 4
Z9 4
U1 0
U2 11
PY 2017
BP 926
EP 929
UT WOS:000403395600228
DA 2023-11-16
ER

PT C
AU Bogdanov, I
   Mirsu, R
   Tiponut, V
AF Bogdanov, Ivan
   Mirsu, Radu
   Tiponut, Virgil
BE Mastorakis, NE
   Mladenov, V
   Bojkovic, Z
   Kartalopoulos, S
   Varonides, A
   Jha, M
TI MATLAB MODEL FOR SPIKING NEURAL NETWORKS
SO PROCEEDINGS OF THE 13TH WSEAS INTERNATIONAL CONFERENCE ON SYSTEMS:
   RECENT ADVANCES IN SYSTEMS
SE Mathematics and Computers in Science and Engineering
DT Proceedings Paper
CT 13th WSEAS International Conference on SYSTEMS held at the 13th WSEAS
   CSCC Multiconference
CY JUL 22-24, 2009
CL Rhodes Isl, GREECE
DE spiking neural networks; neural modeling; MATLAB modeling; neural
   synchronism
AB Spiking Neural Networks are the most realistic model compared to its biological counterpart. This paper introduces a MATLAB toolbox that is specifically designed for simulating, spiking neural networks. The toolbox includes a set of functions that are useful for: creating and organizing the desired architecture; updating stimuli signals, adapting synapses and simulating the network; extracting and visualizing the simulation results.
C1 [Bogdanov, Ivan; Mirsu, Radu; Tiponut, Virgil] POLITEHN Univ Timisoara, Str Vasile Parvan 2, Timisoara, Romania.
RP Bogdanov, I (corresponding author), POLITEHN Univ Timisoara, Str Vasile Parvan 2, Timisoara, Romania.
EM ivan.bogdanov@etc.upt.ro; radu.mirsu@etc.upt.ro;
   virgil.tiponut@etc.upt.ro
CR Gerstner W., 2002, SPIKING NEURON MODEL
   HANSELMAN D, 2001, MATLAB LANGUAGE TECH
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   KONKLE T, IMAGE SEGMENTATION U
   WILLS SA, 2004, THESIS U CAMBRIDGE
NR 5
TC 6
Z9 6
U1 1
U2 2
PY 2009
BP 533
EP +
UT WOS:000272165400077
DA 2023-11-16
ER

PT C
AU Xiao, F
   Li, JP
   Tian, J
   Wang, GS
AF Xiao Fei
   Li Jianping
   Tian Jie
   Wang Guangshuo
GP IEEE
TI IMAGE RECOGNITION ALGORITHM BASED ON SPIKING NEURAL NETWORK
SO 2022 19TH INTERNATIONAL COMPUTER CONFERENCE ON WAVELET ACTIVE MEDIA
   TECHNOLOGY AND INFORMATION PROCESSING (ICCWAMTIP)
SE International Computer Conference on Wavelet Active Media Technology and
   Information Processing
DT Proceedings Paper
CT 19th IEEE International Computer Conference on Wavelet Active Media
   Technology and Information Processing (ICCWAMTIP)
CY DEC 16-18, 2022
CL Univ Elect Sci & Technol China, Chengdu, PEOPLES R CHINA
HO Univ Elect Sci & Technol China
DE Image recognition; Convolutional neural network; Spiking Neural Network
AB Image recognition is one of the basic tasks of computer vision, and it is also one of the important research directions in the field of machine learning. The artificial neural network algorithm has achieved very remarkable results in image recognition, convolutional neural network is one of the most popular artificial neural network, it's also the main solution to image recognition currently. The spiking neural network is called the third-generation neural network, which is different from the previous generation of neural networks. Inspiring by neuroscience, spiking neural network try to build neural networks in a way closer to the human brain mechanism. Referring the application of artificial neural network in image recognition, we decide to use the convolutional neural network in image recognition, moreover, we combine the spiking neural network to construct a new neural network and try to apply it in the field of image classification.
C1 [Xiao Fei; Li Jianping; Tian Jie; Wang Guangshuo] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
RP Xiao, F (corresponding author), Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
EM 202021080416@std.uestc.edu.cn; jpl2222@uestc.edu.cn;
   202122080902@std.uestc.edu.cn; 202122080904@std.uestc.edu.cn
CR Eshraghian JK, 2023, Arxiv, DOI [arXiv:2109.12894, DOI 10.48550/ARXIV.2109.12894]
   Kim S, 2020, AAAI CONF ARTIF INTE, V34, P11270
   Taherkhani A, 2020, NEURAL NETWORKS, V122, P253, DOI 10.1016/j.neunet.2019.09.036
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Wang XW, 2020, NEURAL NETWORKS, V125, P258, DOI 10.1016/j.neunet.2020.02.011
NR 5
TC 0
Z9 0
U1 8
U2 15
PY 2022
DI 10.1109/ICCWAMTIP56608.2022.10016617
UT WOS:000932922500134
DA 2023-11-16
ER

PT C
AU Amin, HH
AF Amin, Hesham H.
GP ASME
TI CHARACTER IMAGE CLASSIFICATION BASED ON SPIKING NEURAL NETWORK
SO PROCEEDINGS OF THE 2ND INTERNATIONAL CONFERENCE ON ADVANCED COMPUTER
   THEORY AND ENGINEERING (ICACTE 2009), VOLS 1 AND 2
DT Proceedings Paper
CT 2nd International Conference on Advanced Computer Theory and Engineering
   (ICACTE 2009)
CY SEP 25-27, 2009
CL Cairo, EGYPT
AB Inspired by the principles from neuroscience, a spiking neural network is proposed to perform characters image processing. The main units of spiking neural network are spiking neurons which utilize inter spike time intervals as sources of information. Research on spiking neural networks has gained momentum in the last decade due to their ability to mimic biological neural network signals and their efficient computational capabilities. A supervised learning algorithm for spiking neural networks which receive input spike trains (presynaptic inputs) is proposed. In this algorithm, learning is performed in two stages: mapping of the input spike train into a spatio-temporal pattern; and use of a simple learning criteria to change synaptic weights. The proposed algorithm is then used to classify various character images.
C1 S Valley Univ, Dept Elect Engn, Aswan Fac Engn, Aswan 81542, Egypt.
RP Amin, HH (corresponding author), S Valley Univ, Dept Elect Engn, Aswan Fac Engn, Aswan 81542, Egypt.
EM hhamin@svu.edu.sg
CR Amin HH, 2005, IEICE T INF SYST, VE88D, P1893, DOI 10.1093/ietisy/e88-d.8.1893
   Gerstner W., 2002, SPIKING NEURON MODEL
   Hayashi Y., 2007, INT C SERIES, V1301, P132
   Hopfield IJ, 2000, P NATL ACAD SCI USA, V97, P13919, DOI 10.1073/pnas.250483697
   MAASS W, 2003, MODEL REAL TIME COMP, V15
   Maass W., 1999, PULSED NEURAL NETWOR
NR 6
TC 0
Z9 0
U1 0
U2 0
PY 2009
BP 1607
EP 1614
UT WOS:000271545700202
DA 2023-11-16
ER

PT C
AU Rosselló, JL
   de Paúl, I
   Canals, V
   Morro, A
AF Rossello, Josep L.
   de Paul, Ivan
   Canals, Vincent
   Morro, Antoni
BE Alippi, C
   Polycarpou, M
   Panayiotou, C
   Ellinas, G
TI Spiking Neural Network Self-configuration for Temporal Pattern
   Recognition Analysis
SO ARTIFICIAL NEURAL NETWORKS - ICANN 2009, PT I
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 19th International Conference on Artificial Neural Networks (ICANN 2009)
CY SEP 14-17, 2009
CL Limmassol, CYPRUS
DE Neural Networks; Spiking Neural Networks; Hardware implementation of
   Genetic Algorithms
ID NEURONS
AB In this work we provide design guidelines for the hardware implementation of Spiking Neural Networks. The proposed methodology is applied to temporal pattern recognition analysis. For this purpose the networks are trained using a simplified Genetic Algorithm. The proposed solution is applied to estimate the processing efficiency of Spiking Neural Networks.
C1 [Rossello, Josep L.; de Paul, Ivan; Canals, Vincent; Morro, Antoni] Univ Illes Balears, Dept Phys, Elect Syst Grp, Palma de Mallorca 07122, Spain.
RP Rosselló, JL (corresponding author), Univ Illes Balears, Dept Phys, Elect Syst Grp, Palma de Mallorca 07122, Spain.
EM j.rossello@uib.es
CR Gerstner W., 2002, SPIKING NEURON MODEL
   Malaka R, 2000, IEEE IJCNN, P486, DOI 10.1109/IJCNN.2000.859442
   Rosselló JL, 2008, IEICE ELECTRON EXPR, V5, P1042, DOI 10.1587/elex.5.1042
   Sala DM, 1999, IEEE T NEURAL NETWOR, V10, P953, DOI 10.1109/72.774270
NR 4
TC 0
Z9 0
U1 0
U2 1
PY 2009
VL 5768
BP 421
EP 428
UT WOS:000275896600044
DA 2023-11-16
ER

PT C
AU Mirsu, R
   Tiponut, V
   Gavrilut, I
AF Mirsu, Radu
   Tiponut, Virgil
   Gavrilut, Ioan
BE Mastorakis, NE
   Mladenov, V
   Bojkovic, Z
   Kartalopoulos, S
   Varonides, A
   Jha, M
   Simian, D
TI STORING INFORMATION WITH SPIKING NEURAL NETWORKS
SO PROCEEDINGS OF THE 13TH WSEAS INTERNATIONAL CONFERENCE ON COMPUTERS
SE Recent Advances in Computer Engineering
DT Proceedings Paper
CT 13th WSEAS International Conference on Computers
CY JUL 23-25, 2009
CL Rhodes, GREECE
DE spiking neural networks; spatial-temporal spike sequences;
   content-addressable memories; spike coding; neural modeling
AB Spiking Neural Networks encode information inside the characteristics of the spike trains they generate. There are several approaches to how this information can be coded. This paper presents coding by using spatial-temporal spike sequences. This approach is more complex compared to traditional coding schemes (rate code or phase code) but offers substantial increase in capacity due to a higher dimensionality of the representation. The paper proposes a method useful for storing spatial-temporal spike sequences with neural networks and also presents a quantitative evaluation of the network performance.
C1 [Mirsu, Radu; Tiponut, Virgil] Politehn Univ Timisoara, Dept Elect, B Dul Vasile Parvan 2, Timisoara, Romania.
   [Gavrilut, Ioan] Univ Oradea, Dept Elect, Oradea, Romania.
RP Mirsu, R (corresponding author), Politehn Univ Timisoara, Dept Elect, B Dul Vasile Parvan 2, Timisoara, Romania.
EM radu.mirsu@etc.upt.ro; virgil.tiponut@etc.upt.ro; gavrilut@uoradea.ro
CR ABELES M, 1993, J NEUROPHYSIOL, V70, P1629, DOI 10.1152/jn.1993.70.4.1629
   [Anonymous], 2003, HDB BRAIN THEORY NEU
   Bernhard F., 2005, SPIKING NEURONS GPUS
   Ekoon JHBW, 2006, IEEE I C ELECT CIRC, P1344
   Gerstner W, 2001, NEURAL NETWORKS, V14, P599, DOI 10.1016/S0893-6080(01)00053-3
   Gerstner W., 2002, SPIKING NEURON MODEL
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   INDIVIERI G, 2002, ADV NEURAL INFORM PR, V15
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Natschlager T, 1998, NETWORK-COMP NEURAL, V9, P319, DOI 10.1088/0954-898X/9/3/003
   Prut Y, 1998, J NEUROPHYSIOL, V79, P2857, DOI 10.1152/jn.1998.79.6.2857
   WILLS SA, 2004, THESIS U CAMBRIDGE
NR 12
TC 3
Z9 3
U1 0
U2 8
PY 2009
BP 318
EP +
UT WOS:000276790600046
DA 2023-11-16
ER

PT C
AU Dai, JH
   Liu, XC
   Zhang, SM
   Zhang, HJ
   Wang, QB
   Zheng, XX
AF Dai, Jianhua
   Liu, Xiaochun
   Zhang, Shaomin
   Zhang, Huaijian
   Wang, Qingbo
   Zheng, Xiaoxiang
BE Kim, T
   Yang, LT
   Park, JH
   Chang, ACC
   Vasilakos, T
   Zhang, Y
TI Spike Sorting Method Based on Two-Stage Radial Basis Function Networks
SO ADVANCES IN COMPUTATIONAL SCIENCE AND ENGINEERING
SE Communications in Computer and Information Science
DT Proceedings Paper
CT 2nd International Conference on Future Generation Communication and
   Networking
CY DEC 13-15, 2008
CL Hainan, PEOPLES R CHINA
DE RBF; spike sorting; neural network; K-means
ID ALGORITHM
AB In this paper, 2-stage Radial Basis Function (RBF) Network method is used for neural spike sorting. Firstly, raw signals are obtained from Neural Signal Simulator, and added white noise ranged from -10dB to -40dB. Secondly, spikes are detected out with matched filter from signals. Lastly, 2-stage RBF networks are constructed and the spikes are sorted using RBF networks. The experiments show that 2-stage RBF network is an effective tool for neural spike sorting.
C1 [Dai, Jianhua; Liu, Xiaochun; Zhang, Shaomin; Zhang, Huaijian; Wang, Qingbo; Zheng, Xiaoxiang] Zhejiang Univ, Qiushi Acad Adv Studies, Hangzhou 310027, Peoples R China.
RP Dai, JH (corresponding author), Zhejiang Univ, Qiushi Acad Adv Studies, Hangzhou 310027, Peoples R China.
EM jhdai@zju.edu.cn
CR CHEN S, 1991, IEEE T NEURAL NETWOR, V2, P302, DOI 10.1109/72.80341
   Chen S, 1999, IEEE T NEURAL NETWOR, V10, P1239, DOI 10.1109/72.788663
   DAI JH, 2008, P 2008 IEEE INT C GR, P172
   Lewicki MS, 1998, NETWORK-COMP NEURAL, V9, pR53, DOI 10.1088/0954-898X/9/4/001
   Mulgrew B, 1996, IEEE SIGNAL PROC MAG, V13, P50, DOI 10.1109/79.487041
   PFURTSCHELLER G, 1978, ELECTROEN CLIN NEURO, V44, P243, DOI 10.1016/0013-4694(78)90272-9
   SING JK, 2003, TENCON 2003 C CONV T, V2, P841
NR 7
TC 0
Z9 0
U1 0
U2 1
PY 2009
VL 28
BP 98
EP 105
UT WOS:000280184400008
DA 2023-11-16
ER

PT C
AU Sboev, A
   Serenko, A
   Rybka, R
AF Sboev, Alexander
   Serenko, Alexey
   Rybka, Roman
BE Klimov, VV
   Kelley, DJ
TI Correlation Encoding of Input Data for Solving a Classification Task by
   a Spiking Neural Network with Spike-Timing-Dependent Plasticity
SO BIOLOGICALLY INSPIRED COGNITIVE ARCHITECTURES 2021
SE Studies in Computational Intelligence
DT Proceedings Paper
CT 12th Annual International Conference of the
   Biologically-Inspired-Cognitive-Architectures-Society (BICA) on
   Biologically Inspired Cognitive Architectures
CY SEP 12-19, 2021
CL ELECTR NETWORK
DE Spiking neural networks; Spike-timing-dependent plasticity; Machine
   learning
AB We propose a new approach for encoding input data into spike sequences presented to the spiking neural network in a classification task: an input vector is represented by mutual correlations of input spike sequences. The accuracy obtained on the benchmark classification tasks of Fisher's Iris and Wisconsin breast cancer is comparable to the results of other existing approaches to spiking neural network learning on base of Spike-Timing-Dependent Plasticity.
C1 [Sboev, Alexander; Serenko, Alexey; Rybka, Roman] Natl Res Ctr, Kurchatov Inst, Moscow, Russia.
   [Sboev, Alexander] Natl Res Nucl Univ MEPhI, Moscow, Russia.
RP Sboev, A (corresponding author), Natl Res Ctr, Kurchatov Inst, Moscow, Russia.; Sboev, A (corresponding author), Natl Res Nucl Univ MEPhI, Moscow, Russia.
CR Auge D, 2021, NEURAL PROCESS LETT, V53, P4693, DOI 10.1007/s11063-021-10562-2
   Demin VA, 2021, NEURAL NETWORKS, V134, P64, DOI 10.1016/j.neunet.2020.11.005
   Fisher RA, 1936, ANN EUGENIC, V7, P179, DOI 10.1111/j.1469-1809.1936.tb02137.x
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Masquelier T, 2008, PLOS ONE, V3, DOI 10.1371/journal.pone.0001377
   Morrison A, 2008, BIOL CYBERN, V98, P459, DOI 10.1007/s00422-008-0233-1
   Saïghi S, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00051
   Sboev A, 2020, MATH METHOD APPL SCI, V43, P7802, DOI 10.1002/mma.6241
   Stanley K, MULTINEAT PORTABLE S
   STREET WN, 1993, P SOC PHOTO-OPT INS, V1905, P861, DOI 10.1117/12.148698
   van Rossum MCW, 2000, J NEUROSCI, V20, P8812
   Yu Q, 2014, NEUROCOMPUTING, V138, P3, DOI 10.1016/j.neucom.2013.06.052
NR 12
TC 0
Z9 0
U1 1
U2 1
PY 2022
VL 1032
BP 457
EP 462
DI 10.1007/978-3-030-96993-6_50
UT WOS:000833484200050
DA 2023-11-16
ER

PT C
AU Nazimov, AI
   Pavlov, AN
AF Nazimov, Alexey I.
   Pavlov, Alexey N.
BE Tuchin, VV
   Duncan, DD
   Larin, KV
   Leahy, MJ
   Wang, RK
TI Classification of spiking events with wavelet neural networks
SO DYNAMICS AND FLUCTUATIONS IN BIOMEDICAL PHOTONICS VIII
SE Proceedings of SPIE
DT Proceedings Paper
CT Conference on the Dynamics and Fluctuations in Biomedical Photonics VIII
CY JAN 22-24, 2011
CL San Francisco, CA
DE spike separation; neural networks; wavelet; principal component analysis
ID SPIKES
AB We study the problem of separation of extracellularly recorded spikes by means of an artificial neural network (NN) and its modifications - wavelet neural networks (WNN). Advantages of networks over the standard approaches such as, e. g., the principal component analysis (PCA) are discussed. Application of neural networks seems to be a highly efficient way to improve classification provided by PCA.
C1 [Nazimov, Alexey I.; Pavlov, Alexey N.] Saratov NG Chernyshevskii State Univ, Dept Phys, Radiophys & Nonlinear Dynam Chair, Saratov 410026, Russia.
RP Pavlov, AN (corresponding author), Saratov NG Chernyshevskii State Univ, Dept Phys, Radiophys & Nonlinear Dynam Chair, Astrakhanskaya Str 83, Saratov 410026, Russia.
EM pavlov.alexeyn@gmail.com
CR Harris KD, 2000, J NEUROPHYSIOL, V84, P401, DOI 10.1152/jn.2000.84.1.401
   Haykin S., 1999, NEURAL NETWORKS COMP
   Letelier JC, 2000, J NEUROSCI METH, V101, P93, DOI 10.1016/S0165-0270(00)00250-8
   Lewicki MS, 1998, NETWORK-COMP NEURAL, V9, pR53, DOI 10.1088/0954-898X/9/4/001
   Makarov VA, 2008, PROC SPIE, V6855, DOI 10.1117/12.769644
   Pavlov Alexey, 2007, Natural Computing, V6, P269, DOI 10.1007/s11047-006-9014-8
   Quiroga RQ, 2004, NEURAL COMPUT, V16, P1661, DOI 10.1162/089976604774201631
   Rosenblatt F., 1959, S HELD NAT PHYS LAB, V1, P421
   Rumelhart D.E., 1986, PARALLEL DISTRIBUTED, V1, DOI [10.21236/ada164453, 10.1016/b978-1-4832-1446-7.50035-2]
   ZHANG QG, 1992, IEEE T NEURAL NETWOR, V3, P889, DOI 10.1109/72.165591
NR 10
TC 1
Z9 1
U1 1
U2 5
PY 2011
VL 7898
AR 789815
DI 10.1117/12.876411
UT WOS:000292039000028
DA 2023-11-16
ER

PT C
AU Wu, QX
   Liao, XD
   Huang, X
   Cai, RT
   Cai, JY
   Liu, JQ
AF Wu, QingXiang
   Liao, Xiaodong
   Huang, Xi
   Cai, Rongtai
   Cai, Jianyong
   Liu, Jinqing
BE Tomar, G
TI Development of FPGA Toolbox for Implementation of Spiking Neural
   Networks
SO 2015 FIFTH INTERNATIONAL CONFERENCE ON COMMUNICATION SYSTEMS AND NETWORK
   TECHNOLOGIES (CSNT2015)
SE International Conference on Communication Systems and Network
   Technologies
DT Proceedings Paper
CT 5th International Conference on Communication Systems and Network
   Technologies (CSNT)
CY APR 04-06, 2015
CL MIR Labs & SRCEM Gwalior, Gwalior, INDIA
HO MIR Labs & SRCEM Gwalior
DE spiking neural networks; hardware implementation; neuron model; toolbox;
   FPGA
ID NEURONS; PLASTICITY
AB Since more and more new findings and principles of intelligence emerge from neuroscience, spiking neural networks become important topics in artificial intelligence domain. However, as high computational complexity of spiking neural networks it is difficult to implement them efficiently using software simulation. In this paper a new hardware implementation method is proposed. In order to implement spiking neural networks more simply, efficiently and rapidly, a toolbox, which is composed of components of spiking neural networks, is developed for neuroscientists, computer scientists and electronic engineers to implement and simulate spiking neural networks in hardware. Using the toolbox a spiking neural network is easy to implement on a FPGA (Field Programmable Gate Arrays) chip, because the toolbox takes advantages of Xilinx System Generator and works in Matlab Simulink environment. The graphic user interface enables users easy to design and simulate spiking neural networks on FPGAs and speed up run-time. This paper presents the methodology in development of the toolbox and the examples are used to show its promising application.
C1 [Wu, QingXiang; Liao, Xiaodong; Huang, Xi; Cai, Rongtai; Cai, Jianyong; Liu, Jinqing] Fujian Normal Univ, Coll Photon & Elect Engn, Minist Educ, Key Lab OptoElect Sci & Technol Med, Fuzhou 350007, Fujian, Peoples R China.
RP Wu, QX (corresponding author), Fujian Normal Univ, Coll Photon & Elect Engn, Minist Educ, Key Lab OptoElect Sci & Technol Med, Fuzhou 350007, Fujian, Peoples R China.
EM qxwu@fjnu.edu.cn
CR Abbot L., 1991, NEURAL NETWORKS BIOL, P179
   [Anonymous], XIL BLOCKS REF GUID
   [Anonymous], 1949, ORG BEHAV NEUROPHYSI
   [Anonymous], QUICK START GUID XIL
   Bell CC, 1997, NATURE, V387, P278, DOI 10.1038/387278a0
   Bi, 1999, NATURE, V401
   BIALEK W, 1991, SCIENCE, V252, P1854, DOI 10.1126/science.2063199
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Carrillo S, 2013, IEEE T PARALL DISTR, V24, P2451, DOI 10.1109/TPDS.2012.289
   Christodoulou C, 2002, NEURAL NETWORKS, V15, P891, DOI 10.1016/S0893-6080(02)00034-5
   Delorme A, 1999, NEUROCOMPUTING, V26-7, P989, DOI 10.1016/S0925-2312(99)00095-8
   Diehl PU, 2014, IEEE IJCNN, P4288, DOI 10.1109/IJCNN.2014.6889876
   Gerstner W, 1996, NATURE, V383, P76, DOI 10.1038/383076a0
   Gomar S, 2014, IEEE T CIRCUITS-I, V61, P1206, DOI 10.1109/TCSI.2013.2286030
   Indiveri G, 2000, SCIENCE, V288, P1189, DOI 10.1126/science.288.5469.1189
   Jahnke A, 1998, PULSED NEURAL NETWORKS, P237
   KAAS JH, 1991, ANNU REV NEUROSCI, V14, P137, DOI 10.1146/annurev.ne.14.030191.001033
   Koch C, 1996, IEEE SPECTRUM, V33, P38, DOI 10.1109/6.490055
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Ruf B, 1998, IEEE T NEURAL NETWOR, V9, P575, DOI 10.1109/72.668899
   Schoenauer T, 2002, IEEE T NEURAL NETWOR, V13, P205, DOI 10.1109/72.977304
   Sejnowski T., 1995, NATURE, V376
   Soleimani H, 2012, IEEE T CIRCUITS-I, V59, P2991, DOI 10.1109/TCSI.2012.2206463
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Wu QX, 2006, NEUROCOMPUTING, V69, P1912, DOI 10.1016/j.neucom.2005.11.023
   Zhuo Z., 2013, CCIS, V375, P147
NR 26
TC 5
Z9 5
U1 0
U2 6
PY 2015
BP 806
EP 810
DI 10.1109/CSNT.2015.216
UT WOS:000380431700163
DA 2023-11-16
ER

PT C
AU Tang, D
   Botzheim, J
   Kubota, N
   Yamaguchi, T
AF Tang, Dalai
   Botzheim, Janos
   Kubota, Naoyuki
   Yamaguchi, Toru
GP IEEE
TI Estimation of Human Transport Modes by Fuzzy Spiking Neural Network and
   Evolution Strategy in Informationally Structured Space
SO 2013 IEEE INTERNATIONAL WORKSHOP ON GENETIC AND EVOLUTIONARY FUZZY
   SYSTEMS (GEFS)
SE IEEE International Workshop on Genetic and Evolutionary Fuzzy Systems
DT Proceedings Paper
CT 6th IEEE International Workshop on Genetic and Evolutionary Fuzzy
   Systems (GEFS)
CY APR 16-19, 2013
CL Singapore, SINGAPORE
DE Informationally Structured Space; Fuzzy Spiking Neural Networks;
   Evolution Strategy
AB This paper analyzes the performance of human transport mode estimation by fuzzy spiking neural network in informationally structured space based on smart phone sensor. The importance of information structuralization is considered. In our previous work we applied spiking neural network to extract the human position in a room equipped with sensor network devices. In this paper fuzzy spiking neural network is applied to extract the human activity outdoors when equipped with smart phone sensor. We discuss how to update the base value by preprocessing for generating the input values to the spiking neurons. The learning method of the spiking neural network based on the time series of the measured data is explained as well. Evolution strategy is used for optimizing the parameters of the fuzzy spiking neural network. Several experimental results are presented for confirming the effectiveness of the proposed method.
C1 [Tang, Dalai; Botzheim, Janos; Kubota, Naoyuki; Yamaguchi, Toru] Tokyo Metropolitan Univ, Grad Sch Syst Design, Hino, Tokyo 1910065, Japan.
RP Tang, D (corresponding author), Tokyo Metropolitan Univ, Grad Sch Syst Design, 6-6 Asahigaoka, Hino, Tokyo 1910065, Japan.
EM tang@sd.tmu.ac.jp; botzheim@sd.tmu.ac.jp; kubota@tmu.ac.jp;
   yamachan@tmu.ac.jp
CR Anderson J.A., 1988, NEUROCOMPUTING
   [Anonymous], 2010, P IEEE WORLD C COMP
   [Anonymous], 1991, FDN GENETIC ALGORITH, DOI DOI 10.1016/B978-0-08-050684-5.50009-4
   Gerstner W, 1998, PULSED NEURAL NETWORKS, P3
   Kubota Naoyuki, 2009, RO-MAN 2009 - The 18th IEEE International Symposium on Robot and Human Interactive Communication, P346, DOI 10.1109/ROMAN.2009.5326082
   Kubota N., 2009, P ICROS SICE INT JOI
   Kubota N., 2009, P INT WORKSH ADV COM
   Kubota N, 2009, IEEE INTERNATIONAL SYMPOSIUM ON COMPUTATIONAL INTELLIGENCE IN ROBOTICS AND AUTOMATION, P165, DOI 10.1109/CIRA.2009.5423215
   Maass W., 1999, PULSED NEURAL NETWOR
   Morioka K., 2004, 2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566), P199
   Obo T., 2010, P 2010 IEEE WORLD C, P2215
   Satomi M., P IEEE S SER COMP IN
   Schwefel HP., 1981, NUMERICAL OPTIMIZATI
   Steventon A., 2005, INTELLIGENT SPACES A, P389
   Tang D., 2010, P WORLD AUT C WAC KO
   TANG DL, 2010, P 17 INT C NEUR INF, V6443, P25
NR 16
TC 9
Z9 9
U1 0
U2 1
PY 2013
BP 36
EP 43
UT WOS:000332956700006
DA 2023-11-16
ER

PT C
AU Malathi, M
   Faiyaz, KK
   Naveen, RM
   Nithish, C
AF Malathi, M.
   Faiyaz, K. K.
   Naveen, R. M.
   Nithish, C.
BE Chen, JIZ
   Tavares, JMRS
   Shi, F
TI Backpropagation in Spiking Neural Network Using Reverse Spiking
   Mechanism
SO THIRD INTERNATIONAL CONFERENCE ON IMAGE PROCESSING AND CAPSULE NETWORKS
   (ICIPCN 2022)
SE Lecture Notes in Networks and Systems
DT Proceedings Paper
CT 3rd International Conference on Image Processing and Capsule Networks
   (ICIPCN)
CY MAY 20-21, 2022
CL ELECTR NETWORK
DE Spiking neural network; Backpropagation; Leaky integrate and fire neuron
   model; Reverse spiking
AB This paper theorizes the concept of backpropagation in Spiking Neural Networks (SNN) using the reverse spiking mechanism. A Spiking Neural Network utilizes biologically-realistic models of neurons to operate, bridging the gap between neuroscience and machine learning. Leaky Integrate and Fire Neurons form the basis of this Spiking Neural Network, which is a combination of leaky resistors and capacitors. Synaptic currents I(t) serve as input to charge caps to produce potential V(t). Backpropagation in Spiking Neural Network has been a challenge for a long as its source of inspiration is biological neurons that naturally don't backpropagate. In hardware implementation of deep learning algorithms, these are the most demanding metrics. Here we constructed a multilayered SNN following reverse-spike backpropagation and simulated it using Brian Simulator.
C1 [Malathi, M.; Faiyaz, K. K.; Naveen, R. M.; Nithish, C.] Sri Krishna Coll Technol, Coimbatore, Tamil Nadu, India.
RP Malathi, M (corresponding author), Sri Krishna Coll Technol, Coimbatore, Tamil Nadu, India.
EM m.malathi@skct.edu.in; 18tuit029@skct.edu.in; 18tuit058@skct.edu.in;
   18tuit061@skct.edu.in
CR Alawad M, 2017, IEEE INT CONF BIG DA, P311, DOI 10.1109/BigData.2017.8257939
   Bashar A., 2019, J ARTIFICIAL INTELLI, V1, P73
   Coms a I.M., 2021, FRONT NEUROSCI-SWITZ, V15, P936
   Diehl PU, 2015, IEEE IJCNN
   Dutta S, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-07418-y
   Eshraghian JK, 2023, Arxiv, DOI [arXiv:2109.12894, DOI 10.48550/ARXIV.2109.12894]
   Goodman Dan, 2008, Front Neuroinform, V2, P5, DOI 10.3389/neuro.11.005.2008
   Han K.S., 2016, ANAL APPL MATH, V7, P58
   Iakymchuk T, 2015, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-015-0059-4
   Kheradpisheh SR, 2020, INT J NEURAL SYST, V30, DOI 10.1142/S0129065720500276
   Lee C, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00119
   Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508
   Lendave V., 2021, ANALYTICS INDIAMAGAZ
   Lyashenko B., 2022, CNVRG
   Manoharan J.S., 2021, J SOFT COMPUT PARADI, V3, P83
   Molka Daniel, 2010, 2010 International Conference on Green Computing (Green Comp), P123, DOI 10.1109/GREENCOMP.2010.5598316
   Patel K, 2021, Arxiv, DOI arXiv:2106.08921
   Snider GS, 2008, 2008 IEEE INTERNATIONAL SYMPOSIUM ON NANOSCALE ARCHITECTURES, P85, DOI 10.1109/NANOARCH.2008.4585796
   Stimberg M, 2019, ELIFE, V8, DOI 10.7554/eLife.47314
   Wikipedia contributors, 2022, WIKIPEDIA FREE E
   Wunderlich TC, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-91786-z
   Zhang JW, 2019, Arxiv, DOI arXiv:1906.01703
NR 22
TC 0
Z9 0
U1 1
U2 4
PY 2022
VL 514
BP 507
EP 518
DI 10.1007/978-3-031-12413-6_40
UT WOS:000892628600040
DA 2023-11-16
ER

PT J
AU Grüning, A
   Sporea, I
AF Gruening, Andre
   Sporea, Ioana
TI Supervised Learning of Logical Operations in Layered Spiking Neural
   Networks with Spike Train Encoding
SO NEURAL PROCESSING LETTERS
DT Article
DE Spiking neural networks; Supervised learning; Logical operation; Spike
   trains
ID TIMING-DEPENDENT PLASTICITY; RECURRENT NETWORKS; GRADIENT DESCENT;
   CONTEXT-FREE; NEURONS; CLASSIFICATION; DYNAMICS
AB Few algorithms for supervised training of spiking neural networks exist that can deal with patterns of multiple spikes, and their computational properties are largely unexplored. We demonstrate in a set of simulations that the ReSuMe learning algorithm can successfully be applied to layered neural networks. Input and output patterns are encoded as spike trains of multiple precisely timed spikes, and the network learns to transform the input trains into target output trains. This is done by combining the ReSuMe learning algorithm with multiplicative scaling of the connections of downstream neurons. We show in particular that layered networks with one hidden layer can learn the basic logical operations, including Exclusive-Or, while networks without hidden layer cannot, mirroring an analogous result for layered networks of rate neurons. While supervised learning in spiking neural networks is not yet fit for technical purposes, exploring computational properties of spiking neural networks advances our understanding of how computations can be done with spike trains.
C1 [Gruening, Andre; Sporea, Ioana] Univ Surrey, Dept Comp, Guildford GU2 7XH, Surrey, England.
RP Grüning, A (corresponding author), Univ Surrey, Dept Comp, Guildford GU2 7XH, Surrey, England.
EM a.gruning@surrey.ac.uk
CR BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181
   Bodén M, 2000, CONNECT SCI, V12, P197, DOI 10.1080/095400900750060122
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Booij O, 2005, INFORM PROCESS LETT, V95, P552, DOI 10.1016/j.ipl.2005.05.023
   Enderton H. B., 2001, MATH INTRO LOGIC, V2nd
   Gerstein GL, 2001, NEURAL NETWORKS, V14, P589, DOI 10.1016/S0893-6080(01)00042-9
   Gerstner W., 2002, SPIKING NEURON MODEL
   Glackin C, 2011, NEURAL NETWORKS, V24, P247, DOI 10.1016/j.neunet.2010.11.008
   Grüning A, 2006, CONNECT SCI, V18, P23, DOI 10.1080/09540090500317291
   Izhikevich EM, 2007, CEREB CORTEX, V17, P2443, DOI 10.1093/cercor/bhl152
   Kasinski A., 2006, International Journal of Applied Mathematics and Computer Science, V16, P101
   Kasinski A, 2005, LECT NOTES COMPUT SC, V3696, P145, DOI 10.1007/11550822_24
   Kistler WM, 2000, NEURAL COMPUT, V12, P385, DOI 10.1162/089976600300015844
   Legenstein R, 2005, NEURAL COMPUT, V17, P2337, DOI 10.1162/0899766054796888
   Legenstein R, 2008, PLOS COMPUT BIOL, V3, P1
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Minsky M., 1988, PERCEPTRONS INTRO CO
   Nemenman I, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.1000025
   Okun M., 2009, SCHOLARPEDIA, V4, P7467, DOI DOI 10.4249/SCHOLARPEDIA.7467
   Pfister JP, 2006, J NEUROSCI, V26, P9673, DOI 10.1523/JNEUROSCI.1425-06.2006
   Polikar R., 2006, IEEE Circuits and Systems Magazine, V6, P21, DOI 10.1109/MCAS.2006.1688199
   Ponulak F, 2008, INT J APPL MATH COMP, V18, P117, DOI 10.2478/v10006-008-0011-1
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Rodriguez P, 2001, NEURAL COMPUT, V13, P2093, DOI 10.1162/089976601750399326
   Rojas R, 1996, NEURAL NETWORKS, P149, DOI 10.1007/978-3-642-61068-4{\_}7
   Rostro-Gonzalez H., 2010, REVERSE ENG SPIKING
   Rumelhart D.E., 1987, LEARNING INTERNAL RE, P318
   Schrauwen B, 2004, IEEE IJCNN, P471, DOI 10.1109/IJCNN.2004.1379954
   Shepard JD, 2009, NEURON, V52, P475
   SIEGELMANN HT, 1995, J COMPUT SYST SCI, V50, P132, DOI 10.1006/jcss.1995.1013
   Sporea I, 2011, P INT JOINT C NEUR N
   Takase H, 2009, P IEEE INT JOINT C N
   Tino P, 2006, NEURAL COMPUT, V18, P591, DOI 10.1162/089976606775623360
   van Rossum MCW, 2001, NEURAL COMPUT, V13, P751, DOI 10.1162/089976601300014321
   Wade JJ, 2010, IEEE T NEURAL NETWOR, V21, P1817, DOI 10.1109/TNN.2010.2074212
NR 35
TC 6
Z9 10
U1 1
U2 21
PD OCT
PY 2012
VL 36
IS 2
BP 117
EP 134
DI 10.1007/s11063-012-9225-1
UT WOS:000308442300002
DA 2023-11-16
ER

PT C
AU Reid, D
   Hussain, AJ
   Tawfik, H
AF Reid, David
   Hussain, Abir Jaafar
   Tawfik, Hissam
GP IEEE
TI Spiking Neural Networks for Financial Data Prediction
SO 2013 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY AUG 04-09, 2013
CL Dallas, TX
DE Spiking neural network; Polychronisation; financial time series;
   nonstationary data
AB In this paper a novel application of a particular type of spiking neural network, a Polychronous Spiking Network, for financial time series prediction is introduced with the aim of exploiting the inherent temporal capabilities of the spiking neural model.
   The performance of the spiking neural network was benchmarked against two "traditional", rate-encoded, neural networks; a Multi-Layer Perceptron network and a Functional Link Neural Network. Three nonstationary and noisy time series are used to test these simulations: IBM stock data; US/Euro exchange rate data, and the price of Brent crude oil.
   The experiments demonstrated favourable prediction results for the Spiking Neural Network in terms of Annualised Return, for both 1-Step and 5-Step ahead predictions. These results were also supported by other relevant metrics such as Maximum Drawdown, Signal-To-Noise ratio, and Normalised Mean Square Error.
   This work demonstrated the applicability of polychronous spiking network to financial data forecasting and that it has the potential to function more effectively than traditional neural networks, in nonstationary environments.
C1 [Reid, David; Tawfik, Hissam] Liverpool Hope Univ, Dept Math & Comp Sci, Liverpool, Merseyside, England.
   [Hussain, Abir Jaafar] Liverpool John Moores Univ, Sch Comp & Math Sci, Liverpool L3 3AF, Merseyside, England.
RP Reid, D (corresponding author), Liverpool Hope Univ, Dept Math & Comp Sci, Liverpool, Merseyside, England.
EM reidd@hope.ac.uk; a.hussain@ljmu.ac.uk; tawfikh@hope.ac.uk
CR Abraham A, 2001, LECT NOTES COMPUT SC, V2074, P337
   Abu-Mostafa YS, 2001, IEEE T NEURAL NETWOR, V12, P653, DOI 10.1109/TNN.2001.935079
   Allen F, 1999, J FINANC ECON, V51, P245, DOI 10.1016/S0304-405X(98)00052-X
   [Anonymous], DERIVATIVES USE TRAD, DOI DOI 10.1002/F0R.935
   [Anonymous], 1991, CORTICONICS
   [Anonymous], 1998, PULSED NEURAL NETWOR
   Arthur JV, 2007, IEEE T NEURAL NETWOR, V18, P1815, DOI 10.1109/TNN.2007.900238
   Artyomov E., 2004, PATTERN RECOGNITION
   Box P., 2008, TIME SERIES ANAL FOR
   Cao LJ, 2003, IEEE T NEURAL NETWOR, V14, P1506, DOI 10.1109/TNN.2003.820556
   Cass R, 1996, CONTROL ENG PRACT, V4, P1579, DOI 10.1016/0967-0661(96)00173-6
   Durbin R, 1989, NEURAL COMPUT, V1, P133, DOI 10.1162/neco.1989.1.1.133
   Edelman G. M., 1987, NEURAL DARWINISM THE
   Ganatr A, 2010, INT J COMPUTER THEOR, V2, P1793
   Gerstner W, 1996, NEURAL COMPUT, V8, P1653, DOI 10.1162/neco.1996.8.8.1653
   Ghazali R, 2008, NEURAL COMPUT APPL, V17, P311, DOI 10.1007/s00521-007-0132-8
   Ghazali R, 2009, NEUROCOMPUTING, V72, P2359, DOI 10.1016/j.neucom.2008.12.005
   GILES CL, 1987, APPL OPTICS, V26, P4972, DOI 10.1364/AO.26.004972
   GILES CL, 1998, ENCODING GEOMETRIC I, P310
   Glackin C, 2009, THESIS U ULSTER
   Gou Fei, 1994, Proceedings of the SPIE - The International Society for Optical Engineering, V2243, P508
   Hellstrom T., 1997, IMATOM19977 MAL U DE
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Hussain AJ, 2006, IEEE INT C INN INF T
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Izhikevich EM, 2009, INT J BIFURCAT CHAOS, V19, P1733, DOI 10.1142/S0218127409023809
   Johnson C, 2010, IEEE WORLD C COMP IN
   Kaastra I, 1996, NEUROCOMPUTING, V10, P215, DOI 10.1016/0925-2312(95)00039-9
   Kaita T, 2002, PATTERN RECOGN LETT, V23, P977, DOI 10.1016/S0167-8655(02)00028-4
   Knowles A. C., 2005, THESIS LIVERPOOL J M
   Lawrence S, 2000, IEEE IJCNN, P114, DOI 10.1109/IJCNN.2000.857823
   Leerink L. R., 1995, Advances in Neural Information Processing Systems 7, P537
   Legenstein R, 2005, NEURAL COMPUT, V17, P2337, DOI 10.1162/0899766054796888
   LEVY WB, 1983, NEUROSCIENCE, V8, P791, DOI 10.1016/0306-4522(83)90010-6
   Liatsis P, 1999, P SOC PHOTO-OPT INS, V3647, P58, DOI 10.1117/12.341124
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Magdon-Ismail M, 1998, P IEEE, V86, P2184, DOI 10.1109/5.726786
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   Mirea L., 2002, 15 TRIENN WORLD C
   Nessler B., 2010, ADV NEURAL INFORM PR, V22, P1357
   Pao Y.-H., 1989, ADAPTIVE PATTERN REC
   PAO YH, 1995, NEUROCOMPUTING, V9, P149, DOI 10.1016/0925-2312(95)00066-F
   Pham D.T., 1995, NEURAL NETWORKS IDEN, DOI DOI 10.1007/978-1-4471-3244-8
   PLUMMER EA, 2000, TIME SERIES FORECAST
   SCHWAERZEL R, 1996, THESIS U TEXAS SAN A
   Shin Y., 1992, COMPUTATIONALLY EFFI
   Shin Y, 1991, P INT JOINT C NEUR N, P13, DOI DOI 10.1109/IJCNN.1991.155142
   Tawfik H., 1997, PREDICTION NONLINEAR
   Thimm G, 1995, OPTIMIZATION HIGH OR
   Thomas J, 1999, P GECCO 2000 WORKSH
   Thomason M., J COMPUTATIONAL INTE, V7, P36
   Wall JA, 2012, IEEE T NEUR NET LEAR, V23, P574, DOI 10.1109/TNNLS.2011.2178317
   Wong C, 2012, NEURAL COMPUTING APP
   Yao JT, 2000, NEUROCOMPUTING, V34, P79, DOI 10.1016/S0925-2312(00)00300-3
   ySitte J, 2000, IEEE T SYSTEMS MAN C, V30
NR 56
TC 0
Z9 0
U1 1
U2 2
PY 2013
UT WOS:000349557200433
DA 2023-11-16
ER

PT C
AU Cerezuela-Escudero, E
   Jimenez-Fernandez, A
   Paz-Vicente, R
   Dominguez-Morales, JP
   Dominguez-Morales, MJ
   Linares-Barranco, A
AF Cerezuela-Escudero, Elena
   Jimenez-Fernandez, Angel
   Paz-Vicente, Rafael
   Dominguez-Morales, Juan P.
   Dominguez-Morales, Manuel J.
   Linares-Barranco, Alejandro
BE Villa, AEP
   Masulli, P
   Rivero, AJP
TI Sound Recognition System Using Spiking and MLP Neural Networks
SO ARTIFICIAL NEURAL NETWORKS AND MACHINE LEARNING - ICANN 2016, PT II
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 25th International Conference on Artificial Neural Networks (ICANN)
CY SEP 06-09, 2016
CL Barcelona, SPAIN
DE Neuromorphic auditory hardware; Address-Event representation; Spiking
   neural networks; Sound recognition; Spike signal processing
ID MODEL
AB In this paper, we explore the capabilities of a sound classification system that combines a Neuromorphic Auditory System for feature extraction and an artificial neural network for classification. Two models of neural network have been used: Multilayer Perceptron Neural Network and Spiking Neural Network. To compare their accuracies, both networks have been developed and trained to recognize pure tones in presence of white noise. The spiking neural network has been implemented in a FPGA device. The neuromorphic auditory system that is used in this work produces a form of representation that is analogous to the spike outputs of the biological cochlea. Both systems are able to distinguish the different sounds even in the presence of white noise. The recognition system based in a spiking neural networks has better accuracy, above 91 %, even when the sound has white noise with the same power.
C1 [Cerezuela-Escudero, Elena; Jimenez-Fernandez, Angel; Paz-Vicente, Rafael; Dominguez-Morales, Juan P.; Dominguez-Morales, Manuel J.; Linares-Barranco, Alejandro] Univ Seville, Dept Architecture & Technol Comp, Robot & Technol Comp Lab, Seville, Spain.
RP Cerezuela-Escudero, E (corresponding author), Univ Seville, Dept Architecture & Technol Comp, Robot & Technol Comp Lab, Seville, Spain.
EM ecerezuela@atc.us.es
CR [Anonymous], 2010, 2010 IEEE PES INNOVA, DOI DOI 10.1109/ISGTEUROPE
   BOAHEN K, 1998, COMMUNICATING NEURON
   Cerezuela-Escudero E., 2015, PROC IEEE INT JOINT, P1, DOI [10.1109/IJCNN, DOI 10.1109/IJCNN]
   Cerezuela-Escudero E, 2013, LECT NOTES COMPUT SC, V7902, P179, DOI 10.1007/978-3-642-38679-4_17
   Guerrero-Turrubiates JD, 2014, INT CONF ELECTR COMM, P53, DOI 10.1109/CONIELECOMP.2014.6808567
   Domínguez-Morales M, 2011, LECT NOTES COMPUT SC, V6792, P389, DOI 10.1007/978-3-642-21738-8_50
   Dundur R.V., 2008, INT J ELECT COMPUT E, V45, P468
   Eriksson JL, 1999, J ACOUST SOC AM, V106, P1865, DOI 10.1121/1.427936
   Gerstner W., 2002, SPIKING NEURON MODEL
   Gomez-Rodriguez F, 2005, LECT NOTES COMPUT SC, V3512, P534
   Hamilton TJ, 2008, IEEE T BIOMED CIRC S, V2, P30, DOI 10.1109/TBCAS.2008.921602
   Indiveri G, 2006, IEEE T NEURAL NETWOR, V17, P211, DOI 10.1109/TNN.2005.860850
   Iwasa K, 2007, IEEE IJCNN, P902, DOI 10.1109/IJCNN.2007.4371078
   Jaeger H, 2002, 159 GMD  GERM NAT RE
   Lass N., 2012, CONT POLITICS AUSTR
   Leong MP, 2003, EURASIP J APPL SIG P, V2003, P629, DOI 10.1155/S1110865703303038
   Liu SC, 2015, EVENT-BASED NEUROMORPHIC SYSTEMS, P1, DOI 10.1002/9781118927601
   Liu SC, 2010, IEEE INT SYMP CIRC S, P2027, DOI 10.1109/ISCAS.2010.5537164
   LYON RF, 1988, IEEE T ACOUST SPEECH, V36, P1119, DOI 10.1109/29.1639
   Mahowald M., 1992, VLSI ANALOGS NEURONA
   Mendes JAG, 2008, CISP 2008: FIRST INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING, VOL 1, PROCEEDINGS, P221, DOI 10.1109/CISP.2008.741
   Newton MJ, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P1386, DOI 10.1109/IJCNN.2011.6033386
   Nielsen A.B., 2006, 2006 P INT C AC SPEE, V3, P788
   Pickles J., 2012, INTRO PHYSL HEARING
   Pishdadian Fatemeh, 2013, 2013 IEEE Digital Signal Processing and Signal Processing Education Meeting (DSP/SPE), P222, DOI 10.1109/DSP-SPE.2013.6642594
   Rios-Navarro A., 2014, LECT NOTES COMPUTER, V8681, P847
   Robert A, 1999, J ACOUST SOC AM, V106, P1852, DOI 10.1121/1.427935
   Thakur CS, 2014, IEEE INT SYMP CIRC S, P1853, DOI 10.1109/ISCAS.2014.6865519
   Thorpe SJ, 2010, IEEE INT SYMP CIRC S, P265, DOI 10.1109/ISCAS.2010.5537898
   WAIBEL A, 1989, IEEE T ACOUST SPEECH, V37, P328, DOI 10.1109/29.21701
   Wen B, 2009, IEEE T BIOMED CIRC S, V3, P444, DOI 10.1109/TBCAS.2009.2027127
NR 31
TC 3
Z9 3
U1 0
U2 4
PY 2016
VL 9887
BP 363
EP 371
DI 10.1007/978-3-319-44781-0_43
UT WOS:000389086400043
DA 2023-11-16
ER

PT C
AU Gavrilov, AV
   Panchenko, KO
AF Gavrilov, Andrey V.
   Panchenko, Konstantin O.
GP IEEE
TI Methods of Learning for Spiking Neural Networks. A Survey
SO 2016 13TH INTERNATIONAL SCIENTIFIC-TECHNICAL CONFERENCE ON ACTUAL
   PROBLEMS OF ELECTRONIC INSTRUMENT ENGINEERING (APEIE), VOL 2
SE International Conference on Actual Problems of Electronic Instrument
   Engineering
DT Proceedings Paper
CT 13th International Scientific-Technical Conference on Actual Problems of
   Electronics Instrument Engineering (APEIE)
CY OCT 03-06, 2016
CL Novosibirsk, RUSSIA
DE Spiking neural networks; training of neural networks; machine learning
AB The paper aims to give general performance on achievements and trends in techniques for training of spiking neural networks in context of its hardware implementation, i.e. of neuromorphic technology.
C1 [Gavrilov, Andrey V.] Novosibirsk State Tech Univ, Novosibirsk, Russia.
   [Panchenko, Konstantin O.] Motiv Ltd, Novosibirsk, Russia.
RP Gavrilov, AV (corresponding author), Novosibirsk State Tech Univ, Novosibirsk, Russia.
CR [Anonymous], 2012, ADV ARTIF NEURAL SYS, DOI DOI 10.1155/2012/713581
   [Anonymous], P 15 PRORISC WORKSH
   Belatreche A, 2003, P IEEE CYB INT CHALL, P39
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Carnell A., 2005, P ESANN, P363
   Carpenter G., 1991, PATTERN RECOGNITION
   Cassidy AS, 2013, NEURAL NETWORKS, V45, P4, DOI 10.1016/j.neunet.2013.05.011
   Cohen H., 1993, COURSE COMPUTATIONAL
   Diehl PU, 2015, IEEE IJCNN
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Esser SK, 2015, ADV NEUR IN, V28
   Henderson J. A., 2015, ARXIV150205777
   Hunsberger Eric, 2015, COMPUT SCI
   Kasabov N, 2013, NEURAL NETWORKS, V41, P188, DOI 10.1016/j.neunet.2012.11.014
   Kasinski A., 2006, International Journal of Applied Mathematics and Computer Science, V16, P101
   Moore SD., 2002, THESIS RHODES U GRAH
   Ponulak F., 2006, THESIS
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Rasa H., 2012, INT J COMPUTER APPL, V41, P33
   Ruf B, 1997, NEURAL PROCESS LETT, V5, P9, DOI 10.1023/A:1009697008681
   Saleh A.Y., 2014, INT J ADV SOFT COMPU, V6
   Schaffer JD, 2015, 2015 IEEE SYMPOSIUM ON COMPUTATIONAL INTELLIGENCE FOR SECURITY AND DEFENSE APPLICATIONS (CISDA), P68
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Tino P, 2005, LECT NOTES COMPUT SC, V3611, P666
   Wysoski SG, 2006, LECT NOTES COMPUT SC, V4131, P61
   Xin JG, 2001, IEEE IJCNN, P1772, DOI 10.1109/IJCNN.2001.938430
   Zhang X., 2013, P 52 IEEE C DEC CONT
NR 29
TC 9
Z9 9
U1 0
U2 8
PY 2016
BP 455
EP 460
UT WOS:000392625500105
DA 2023-11-16
ER

PT C
AU Kuroe, Y
   Miyoshi, S
   Hikawa, H
   Ito, H
   Motonaka, K
   Maeda, Y
AF Kuroe, Yasuaki
   Miyoshi, Seiji
   Hikawa, Hiroomi
   Ito, Hidetaka
   Motonaka, Kimiko
   Maeda, Yutaka
GP IEEE
TI A Synthesis Method of Spiking Neural Oscillators with Considering
   Asymptotic Stability
SO 2021 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUL 18-22, 2021
CL ELECTR NETWORK
DE neural oscillator; spiking neural network; synthesis method; stability
AB In artificial Spiking Neural Networks (SNNs) the information processing and transmission are carried out by spike trains in a manner similar to the generic biological neurons. Recently it has been reported that they are computationally more powerful than the conventional neural networks. In biological systems there are numerous examples of autonomously generated periodic activities. Several different periodic patterns are generated simultaneously in a living body. It is known that in biological systems there are specific neurons which generate such periodic patterns. This paper presents a method for synthesis of neural oscillators by spiking neural networks. We propose a learning method for synthesizing spiking neural networks which generate desired periodic spike trains with specified spike emission times. We also propose a method for making the periodic trajectory generated by the synthesized spiking neural oscillator asymptotically stable.
C1 [Kuroe, Yasuaki; Miyoshi, Seiji; Hikawa, Hiroomi; Ito, Hidetaka; Motonaka, Kimiko; Maeda, Yutaka] Kansai Univ, Fac Engn Sci, Suita, Osaka, Japan.
RP Kuroe, Y (corresponding author), Kansai Univ, Fac Engn Sci, Suita, Osaka, Japan.
EM kuroe@ieee.org; miyoshi@kansai-u.ac.jp; hikawa@kansai-u.ac.jp;
   h.ito@kansai-u.ac.jp; motonaka@kansai-u.ac.jp; maedayut@kansai-u.ac.jp
CR DOYA K, 1989, NEURAL NETWORKS, V2, P375, DOI 10.1016/0893-6080(89)90022-1
   GERSTNER W, 1993, ADV NEURAL INFORM PR, V6, P363
   Guckenheimer J., 2013, NONLINEAR OSCILLATIO
   Kimura H, 1998, 1998 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS - PROCEEDINGS, VOLS 1-3, P50, DOI 10.1109/IROS.1998.724595
   Kuroe Y., 2010, P INT JOINT C NEUR N, P2561
   KUROE Y, 1999, P IEEE INT JOINT C N, P1
   Kuroe Y., 2006, P IEEE INT JOINT C N, P7613
   LUENBERGER DG, 1973, INTRO LINEAR NONLINE, P194
   Maass W, 1997, NEURAL COMPUT, V9, P279, DOI 10.1162/neco.1997.9.2.279
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   MASS W, 1998, PULSED NEURAL NETS
   MASS W, 1996, ADV NEURAL INFORM PR, V7, P183
   MATSUOKA K, 1987, BIOL CYBERN, V56, P345, DOI 10.1007/BF00319514
   OCHER MB, 1964, INTRODUCTION TO HIGH
NR 14
TC 0
Z9 0
U1 0
U2 1
PY 2021
DI 10.1109/IJCNN52387.2021.9534255
UT WOS:000722581707079
DA 2023-11-16
ER

PT C
AU Syed, T
   Kakani, V
   Cui, XN
   Kim, H
AF Syed, Tehreem
   Kakani, Vijay
   Cui, Xuenan
   Kim, Hakil
GP IEEE
TI Spiking Neural Networks Using Backpropagation
SO 2021 IEEE REGION 10 SYMPOSIUM (TENSYMP)
SE IEEE Region 10 Symposium
DT Proceedings Paper
CT IEEE Region 10 Symposium (TENSYMP) - Good Technologies for Creating
   Future
CY AUG 23-25, 2021
CL Jeju, SOUTH KOREA
DE spiking neural networks; time-steps; processing time; backpropagation
AB Brain-inspired Spiking Neural Networks (SNNs) occur with well-known neuromorphic hardware that delivers extra energy compared to conventional artificial neural networks (ANNs). Nevertheless, exploiting the same network layers as conventional ANNs to persevere a task appears unsuitable. Previous works employ similar architectures as Artificial Neural Networks and transform them into Spiking Neural Networks to attain the most exemplary performance as conventional ANNs. Nevertheless, this conversion technique needs greater timesteps for training spiking neural networks (SNNs). In this work, rather than using the ANN to SNN conversion method, we exploit the SNNs training directly using spike-based backpropagation. Since utilizing SNNs with the spike-based backpropagation requires fewer timesteps compared to ANN to SNN transformation approach. This work evaluates the classification performance on public and private (MNIST, Fashion MNIST, and KITTI) datasets.
C1 [Syed, Tehreem; Kim, Hakil] Inha Univ, Elect & Comp Engn, Incheon, South Korea.
   [Kakani, Vijay] Inha Univ, Integrated Syst & Engn, Incheon, South Korea.
   [Cui, Xuenan] Inha Univ, Informat & Commun Engn, Incheon, South Korea.
RP Syed, T (corresponding author), Inha Univ, Elect & Comp Engn, Incheon, South Korea.
EM tehreem@inha.edu; vjkakani@inha.ac.kr; xncui@inha.ac.kr;
   hikim@inha.ac.kr
CR Bing ZS, 2019, FRONT NEUROROBOTICS, V13, DOI 10.3389/fnbot.2019.00018
   Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS)
   Cassidy AS, 2013, IEEE IJCNN
   Cheng Xiang, IJCAI
   Diehl PU, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON REBOOTING COMPUTING (ICRC)
   Diehl PU, 2015, IEEE IJCNN
   Diehl PU, 2016, IEEE IJCNN, P4278, DOI 10.1109/IJCNN.2016.7727758
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   Kaiming He, 2015, 2015 IEEE International Conference on Computer Vision (ICCV). Proceedings, P1026, DOI 10.1109/ICCV.2015.123
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Ledinauskas Eimantas, 2020, ARXIV200604436
   Lee C., 2019, INT C LEARN REPR ICL
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Netzer Y., 2011, READING DIGITS NATUR, V2, P5
   Ponulak F, 2011, ACTA NEUROBIOL EXP, V71, P409
   Roy K, 2019, NATURE, V575, P607, DOI 10.1038/s41586-019-1677-2
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Wu Yujie, 2019, P AAAI C ART INT, V33
   Zenke F, 2018, NEURAL COMPUT, V30, P1514, DOI 10.1162/neco_a_01086
   Zimmer Romain, 2019, ARXIV PREPRINT ARXIV
NR 22
TC 0
Z9 0
U1 7
U2 7
PY 2021
DI 10.1109/TENSYMP52854.2021.9550994
UT WOS:000786502700144
DA 2023-11-16
ER

PT C
AU Molas, JT
   Peralta, I
   Martinez, C
   Rufiner, HL
AF Tomas Molas, Jose
   Peralta, Ivan
   Martinez, Cesar
   Leonardo Rufiner, Hugo
BE Braidot, A
   Hadad, A
TI Development of a Library for Sound Classification Using Spiking Neural
   Network
SO VI LATIN AMERICAN CONGRESS ON BIOMEDICAL ENGINEERING (CLAIB 2014)
SE IFMBE Proceedings
DT Proceedings Paper
CT 6th Latin American Biomedical Engineering Conference (CLAIB)
CY OCT 29-31, 2014
CL Parana, ARGENTINA
DE automatic speech recognition; artificial neural networks; spiking neural
   networks
AB This article describes the development of a library for Matlab (The MathWorks, Inc.) for sounds recognition based on Spiking Neural Networks. The mathematical model of the integrate and fire type is presented. The main modules of the library are as follows: encoding in "spikes", spiking neural network and finally the training module. To illustrate its use, the library application to the task of phoneme recognition is presented.
C1 [Tomas Molas, Jose; Peralta, Ivan; Martinez, Cesar; Leonardo Rufiner, Hugo] Univ Nacl Entre Rios, Fac Ingn, Lab Cibernet, Oro Verde, Entre Rios, Argentina.
   [Tomas Molas, Jose; Martinez, Cesar; Leonardo Rufiner, Hugo] Univ Nacl Litoral, Fac Ingn & Cs Hidr, Ctr I D Senales Sistemas & Inteligencia Computac, Santa Fe, Argentina.
   [Leonardo Rufiner, Hugo] Consejo Nacl Invest Cient & Tecn, RA-1033 Buenos Aires, DF, Argentina.
RP Molas, JT (corresponding author), Univ Nacl Entre Rios, Fac Ingn, Lab Cibernet, CC 47 Suc 3,E3100,Ruta 11,Km 10, Oro Verde, Entre Rios, Argentina.
EM josetomasmolas@gmail.com; bioivanperalta@gmail.com;
   cmartinez@bioingenieria.edu.ar; lrufiner@bioingenieria.edu.ar
CR Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Bohte SM, 2000, IEEE IJCNN, P279, DOI 10.1109/IJCNN.2000.861316
   Hopfield IJ, 2000, P NATL ACAD SCI USA, V97, P13919, DOI 10.1073/pnas.250483697
   Maass W, 1998, PULSED NEURAL NETWORKS, P321
   Mattia M, 2000, NEURAL COMPUT, V12, P2305, DOI 10.1162/089976600300014953
   van Rossum MCW, 2001, NEURAL COMPUT, V13, P751, DOI 10.1162/089976601300014321
   Wilfred Kaplan, 1986, MATEMATICAS AVANZADA
   Wills Sebastian A, 2004, THESIS CAMBRIDGE U
   Wolfgang M., 2001, PULSED NEURAL NETWOR
NR 9
TC 0
Z9 0
U1 0
U2 2
PY 2014
VL 49
BP 651
EP 654
DI 10.1007/978-3-319-13117-7_166
UT WOS:000363767200165
DA 2023-11-16
ER

PT J
AU JANSEN, RF
AF JANSEN, RF
TI THE RECONSTRUCTION OF INDIVIDUAL SPIKE TRAINS FROM EXTRACELLULAR
   MULTINEURON RECORDINGS USING A NEURAL NETWORK EMULATION PROGRAM
SO JOURNAL OF NEUROSCIENCE METHODS
DT Article
DE MULTIUNIT SPIKE TRAIN; NEURAL NETWORK; SPIKE TRAIN RECONSTRUCTION
ID NEUROELECTRIC DATA
AB A method for the reconstruction of the individual spike trains from extracellular multineuron recordings is described.  A neural network emulation program is trained to recognize a sample set of digitized spikes.  The digitized spikes are fed into the neural network, and the network output is used to classify spikes in terms of the training set.  The system runs on any PC and its speed makes is especially well suited for the analysis of large amounts of data.
RP JANSEN, RF (corresponding author), FREE UNIV AMSTERDAM,DIV NEUROBIOL,DE BOELELAAN 1087,1081 HV AMSTERDAM,NETHERLANDS.
CR DINNING GJ, 1981, IEEE T BIOMEDICAL EN, V12, P804
   HOPFIELD JJ, 1985, BIOL CYBERN, V52, P141
   HOPFIELD JJ, 1984, P NATL ACAD SCI-BIOL, V81, P3088, DOI 10.1073/pnas.81.10.3088
   HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554
   JANSEN RF, 1984, J NEUROBIOL, V16, P1
   McCulloch Warren S., 1943, BULL MATH BIOPHYS, V5, P115, DOI 10.1007/BF02478259
   MCLELLAND JL, 1986, PARALLEL DISTRIBUTED
   SAGALNICOFF M, 1988, J NEUROSCI METH, V25, P181
   SARNA MF, 1988, J NEUROSCI METH, V25, P189, DOI 10.1016/0165-0270(88)90133-1
   SCHMIDT EM, 1984, J NEUROSCI METH, V12, P1, DOI 10.1016/0165-0270(84)90042-6
   SCHMIDT EM, 1984, J NEUROSCI METH, V12, P95, DOI 10.1016/0165-0270(84)90009-8
NR 11
TC 17
Z9 18
U1 0
U2 0
PD DEC
PY 1990
VL 35
IS 3
BP 203
EP 213
DI 10.1016/0165-0270(90)90125-Y
UT WOS:A1990EX50200003
DA 2023-11-16
ER

PT C
AU Xin, JG
   Embrechts, MJ
AF Xin, JG
   Embrechts, MJ
GP IEEE
   IEEE
   IEEE
   IEEE
TI Supervised learning with spiking neural networks
SO IJCNN'01: INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS, VOLS 1-4,
   PROCEEDINGS
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN 01)
CY JUL 15-19, 2001
CL WASHINGTON, D.C.
ID NEURONS
AB In this paper we derive a supervised learning algorithm for a spiking neural network which encodes information in the timing of spike trains. This algorithm is similar to the classical error back propagation algorithm for sigmoidal neural network but the learning parameter is adaptively changed. The algorithm is applied to the complex nonlinear classification problem and the results show that the spiking neural network is capable of performing nonlinearly separable classification tasks. Several issues concerning the spiking neural network are discussed.
C1 Rensselaer Polytech Inst, Dept Mech Engn Aeronaut Engn & Mech, Troy, NY 12180 USA.
RP Xin, JG (corresponding author), Rensselaer Polytech Inst, Dept Mech Engn Aeronaut Engn & Mech, 110 8th St, Troy, NY 12180 USA.
CR [Anonymous], PULSED NEURAL NETWOR
   BALDI P, 1988, BIOL CYBERN, V59, P313, DOI 10.1007/BF00332921
   BOHTE SM, P ESANN 2000
   Choe Y, 1998, NEUROCOMPUTING, V21, P139, DOI 10.1016/S0925-2312(98)00040-X
   ENGEL AK, 1992, TRENDS NEUROSCI, V15, P218, DOI 10.1016/0166-2236(92)90039-B
   GERSTNER W, 1995, PHYS REV E, V51, P738, DOI 10.1103/PhysRevE.51.738
   Gerstner W, 1996, NATURE, V383, P76, DOI 10.1038/383076a0
   Maass W, 1996, NEURAL COMPUT, V8, P1, DOI 10.1162/neco.1996.8.1.1
   Maass W, 1997, NEURAL COMPUT, V9, P279, DOI 10.1162/neco.1997.9.2.279
   MAASS W, 1999, MODELS NEURAL NETWOR, V4
   Natschlager T, 1998, NETWORK-COMP NEURAL, V9, P319, DOI 10.1088/0954-898X/9/3/003
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Shadlen MN, 1998, J NEUROSCI, V18, P3870
   Zhang KC, 1998, J NEUROPHYSIOL, V79, P1017, DOI 10.1152/jn.1998.79.2.1017
NR 14
TC 70
Z9 72
U1 0
U2 9
PY 2001
BP 1772
EP 1777
UT WOS:000172784800315
DA 2023-11-16
ER

PT C
AU Ahmed, K
   Shrestha, A
   Qiu, QR
   Wu, Q
AF Ahmed, Khadeer
   Shrestha, Amar
   Qiu, Qinru
   Wu, Qing
GP IEEE
TI Probabilistic Inference Using Stochastic Spiking Neural Networks on A
   Neurosynaptic Processor
SO 2016 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUL 24-29, 2016
CL Vancouver, CANADA
ID SYSTEM
AB Spiking neural networks are rapidly gaining popularity for their ability to perform efficient computation akin to the way a brain processes information. It has the potential to achieve low cost and high energy efficiency due to the distributed nature of neural computation and the use of low energy spikes for information exchange. A stochastic spiking neural network naturally can be used to realize Bayesian inference. IBM's TrueNorth is a neurosynaptic processor that has more than 1 million digital spiking neurons and 268 million digital synapses with less than 200 mW peak power. In this paper we propose the first work that converts an inference network to a spiking neural network that runs on the TrueNorth processor. Using inference-based sentence construction as a case study, we discuss algorithms that transform an inference network to a spiking neural network, and a spiking neural network to TrueNorth corelet designs. In our experiments, the TrueNorth spiking neural network constructed sentences have a matching accuracy of 88% while consuming an average power of 0.205 mW.
C1 [Ahmed, Khadeer; Shrestha, Amar; Qiu, Qinru] Syracuse Univ, Dept Elect Engn & Comp Sci, Syracuse, NY 13244 USA.
   [Wu, Qing] Air Force Res Lab, Informat Directorate, 525 Brooks Rd, Rome, NY 13441 USA.
RP Ahmed, K (corresponding author), Syracuse Univ, Dept Elect Engn & Comp Sci, Syracuse, NY 13244 USA.
EM tkhahmed@syr.edu; amshrest@syr.edu; qiqiu@syr.edu; qing.wu.2@us.af.mil
CR Amir A, 2013, NEURAL NETWORKS IJCN, P1
   Anand R, 2009, SELF-DEFENSE IN INTERNATIONAL RELATIONS, P1, DOI 10.1057/9780230245747
   [Anonymous], BAYESIAN COMPUTATION
   [Anonymous], 2016, ARXIV160104187
   [Anonymous], VERY LARGE SCOLE INT
   [Anonymous], 2002, J MATH PSYCHOL
   [Anonymous], 2010, SPIKE TIMING DEPENDE
   [Anonymous], 2016 INT JO IN PRESS
   [Anonymous], 2012, BOOK GENESIS EXPLORI, DOI DOI 10.1007/s10827-007-0038-6
   [Anonymous], NEUR NETW 2007 IJCNN
   [Anonymous], 2013, 2013 INT JOINT C NEU
   [Anonymous], ARXIV160104183
   [Anonymous], FRONTIERS NEUROSCIEN
   Behi T, 2012, 2012 6TH INTERNATIONAL CONFERENCE ON SCIENCES OF ELECTRONICS, TECHNOLOGIES OF INFORMATION AND TELECOMMUNICATIONS (SETIT), P701, DOI 10.1109/SETIT.2012.6481999
   Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Beyeler M, 2013, NEURAL NETWORKS, V48, P109, DOI 10.1016/j.neunet.2013.07.012
   Cassidy AS, 2014, INT CONF HIGH PERFOR, P27, DOI 10.1109/SC.2014.8
   Doya K., 2007, BAYESIAN BRAIN PROBA
   Engel TA, 2015, NAT COMMUN, V6, DOI 10.1038/ncomms7454
   Hecht-Nielsen R., 2007, CONFABULATION THEORY
   Hines ML, 1997, NEURAL COMPUT, V9, P1179, DOI 10.1162/neco.1997.9.6.1179
   Kusy B, 2009, 2009 INTERNATIONAL CONFERENCE ON INFORMATION PROCESSING IN SENSOR NETWORKS (IPSN 2009), P109
   Lee H, 2009, P 26 ANN INT C MACHI, P609
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Oram MW, 1999, J NEUROPHYSIOL, V81, P3021, DOI 10.1152/jn.1999.81.6.3021
   Preissl R, 2012, INT CONF HIGH PERFOR
   Qiu QR, 2013, IEEE T COMPUT, V62, P886, DOI 10.1109/TC.2012.50
   Schemmel J, 2010, IEEE INT SYMP CIRC S, P1947, DOI 10.1109/ISCAS.2010.5536970
   Seung HS, 2003, NEURON, V40, P1063, DOI 10.1016/S0896-6273(03)00761-X
NR 29
TC 9
Z9 9
U1 0
U2 2
PY 2016
BP 4286
EP 4293
UT WOS:000399925504069
DA 2023-11-16
ER

PT C
AU Ferrari, S
   Mehta, B
   Di Muro, G
   VanDongen, AMJ
   Henriquez, C
AF Ferrari, Silvia
   Mehta, Bhavesh
   Di Muro, Gianluca
   VanDongen, Antonius M. J.
   Henriquez, Craig
GP IEEE
TI Biologically Realizable Reward-Modulated Hebbian Training for Spiking
   Neural Networks
SO 2008 IEEE INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS, VOLS 1-8
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks
CY JUN 01-08, 2008
CL Hong Kong, PEOPLES R CHINA
AB Spiking neural networks have been shown capable of simulating sigmoidal artificial neural networks providing promising evidence that they too are universal function approximators. Spiking neural networks offer several advantages over sigmoidal networks, because they can approximate the dynamics of biological neuronal networks, and can potentially reproduce the computational speed observed in biological brains by enabling temporal coding. On the other hand, the effectiveness of spiking neural network training algorithms is still far removed from that exhibited by backpropagating sigmoidal neural networks. This paper presents a novel algorithm based on reward-modulated spike-timing-dependent plasticity that is biologically plausible and capable of training a spiking neural network to learn the exclusive-or (XOR) computation, through rate-based coding. The results show that a spiking neural network model with twenty-three nodes is able to learn the XOR gate accurately, and performs the computation on time scales of milliseconds. Moreover, the algorithm can potentially be verified in tight-sensitive neuronal networks grown in vitro by determining the spikes patterns that lead to the desired synaptic weights computed in silico when induced by blue light in vitro.
C1 [Ferrari, Silvia; Mehta, Bhavesh; Di Muro, Gianluca; VanDongen, Antonius M. J.; Henriquez, Craig] Duke Univ, Durham, NC 27707 USA.
RP Ferrari, S (corresponding author), Duke Univ, Durham, NC 27707 USA.
EM sferrari@duke.edu; bhavesh.mehta@duke.edu; gianluca.dimuro@duke.edu;
   vando005@mc.duke.edu; ch@duke.edu
CR [Anonymous], CONNECTIONISM PERSPE
   Burgsteiner H., 2006, ENG APPL ARTIFICIAL, V19
   Dayan P., 2001, THEORETICAL NEUROSCI
   DEVEN TJV, 2005, J NEUROSCIENCE, V25
   Florian R. V., 2007, NEURAL COMPUTATION, V19, P2007
   Gerstner W., 2006, SPIKING NEURON MODEL
   GRIGSTON JC, 2005, EUROPEAN J NEUROSCIE, V21
   Hugh GS, 2002, NEUROCOMPUTING, V44, P847, DOI 10.1016/S0925-2312(02)00482-4
   Legenstein R., 2005, NEURAL COMPUTATION, V17
   MAASS W, 2002, COMPUTATIONAL NEUROS
   Maass W., 1997, ADV NEURAL INFORM PR, V9
   MAASS W, 1996, P 7 AUSTR C NEUR NET
   MAINEN ZF, 1995, SCIENCE, V268, P1503, DOI 10.1126/science.7770778
   Pennartz CMA, 1997, NEUROSCIENCE, V81, P303, DOI 10.1016/S0306-4522(97)00118-8
   Perrett D., 1982, EXPT BRAIN RES, V47
   Pfister J. P., 2006, NEURAL COMPUTATION, V18
   POWERS RKD, 1996, J NEUROPHYSIOLOGY, V75
   Salinas E., 2001, NATURE REV NEUROSCIE, V2
   SEUNG HS, 2003, NEURON, V40
   VANDONGEN A, 1988, SCIENCE, V242
   VANDONGEN AM, VANDONGEN LAB
   Wysoski S. G., 2006, LECT NOTES COMPUTER, V4179
   XIE X, 2004, PHISICAL REV E, V69
NR 23
TC 5
Z9 5
U1 0
U2 1
PY 2008
BP 1780
EP 1786
DI 10.1109/IJCNN.2008.4634039
UT WOS:000263827201035
DA 2023-11-16
ER

PT C
AU Reid, D
   Hussain, AJ
   Tawfik, H
   Ghazali, R
AF Reid, David
   Hussain, Abir Jaafar
   Tawfik, Hissam
   Ghazali, Rozaida
BE Huang, DS
   Jo, KH
   Wang, L
TI Prediction of Physical Time Series Using Spiking Neural Networks
SO INTELLIGENT COMPUTING METHODOLOGIES
SE Lecture Notes in Artificial Intelligence
DT Proceedings Paper
CT 10th International Conference on Intelligent Computing (ICIC)
CY AUG 03-06, 2014
CL Taiyuan, PEOPLES R CHINA
DE spiking neural network; polychronous spiking network; physical time
   series prediction
AB Forecasting the behavior of naturally occurring phenomena by the analysis of time series based data is the basis of scientific experimental design. In this paper, we consider a novel application of a Polychronous Spiking Network for the prediction of sunspot and auroral electrojet index by exploiting the inherent temporal capabilities of this spiking neural model. The performance of this network is benchmarked against two "traditional", rate-encoded, neural networks; a Multi-Layer Perceptron network and a Functional Link Neural Network. The results indicate that the inherent temporal characteristics of the Polychronous Spiking Network make it extremely well suited to the processing of time series based data.
C1 [Reid, David; Tawfik, Hissam] Liverpool Hope Univ, Dept Math & Comp Sci, Liverpool, Merseyside, England.
   [Hussain, Abir Jaafar] Liverpool John Moores Univ, Byroom St, Liverpool L3 3AF, Merseyside, England.
   [Ghazali, Rozaida] Univ Tun Hussein Onn Malaysia, Fac Comp Sci & Informat Technol, Hussein, Malaysia.
RP Reid, D (corresponding author), Liverpool Hope Univ, Dept Math & Comp Sci, Liverpool, Merseyside, England.
EM reidd@hope.ac.uk; a.hussain@ljmu.ac.uk; tawfikh@hope.ac.uk;
   rozaida@uthm.edu.my
CR [Anonymous], 1991, CORTICONICS
   [Anonymous], 1998, PULSED NEURAL NETWOR
   Arthur JV, 2007, IEEE T NEURAL NETWOR, V18, P1815, DOI 10.1109/TNN.2007.900238
   Connor J., 1991, IJCNN-91-Seattle: International Joint Conference on Neural Networks (Cat. No.91CH3049-4), P301, DOI 10.1109/IJCNN.1991.155194
   Draye JPS, 1996, IEEE T SYST MAN CY B, V26, P692, DOI 10.1109/3477.537312
   Edelman G. M., 1987, NEURAL DARWINISM THE
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Huang CC, 2001, COMPUT-AIDED CIV INF, V16, P28, DOI 10.1111/0885-9507.00211
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Legenstein R, 2005, NEURAL COMPUT, V17, P2337, DOI 10.1162/0899766054796888
   LEVY WB, 1983, NEUROSCIENCE, V8, P791, DOI 10.1016/0306-4522(83)90010-6
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   MAKHOUL J, 1975, P IEEE, V63, P561, DOI 10.1109/PROC.1975.9792
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   Nessler B., 2010, ADV NEURAL INFORM PR, V22, P1357
   Rape R, 1994, IEEE INSTR MEAS TECH, P145
   Singh S., 1998, 6th European Congress on Intelligent Techniques and Soft Computing. EUFIT '98, P1901
   Tokinaga S, 1999, ELECTRON COMM JPN 3, V82, P31, DOI 10.1002/(SICI)1520-6440(199903)82:3<31::AID-ECJC4>3.0.CO;2-H
NR 19
TC 0
Z9 0
U1 0
U2 1
PY 2014
VL 8589
BP 816
EP 824
UT WOS:000345517200082
DA 2023-11-16
ER

PT C
AU Lin, XH
   Shi, GY
AF Lin, Xianghong
   Shi, Guoyong
BE Kurkova, V
   Manolopoulos, Y
   Hammer, B
   Iliadis, L
   Maglogiannis, I
TI A Supervised Multi-spike Learning Algorithm for Recurrent Spiking Neural
   Networks
SO ARTIFICIAL NEURAL NETWORKS AND MACHINE LEARNING - ICANN 2018, PT I
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 27th International Conference on Artificial Neural Networks (ICANN)
CY OCT 04-07, 2018
CL Rhodes, GREECE
DE Recurrent spiking neural networks; Supervised learning; Spike train
   inner product; Connectivity degree
AB The recurrent spiking neural networks include complex structures and implicit nonlinear mechanisms, the formulation of efficient supervised learning algorithm is difficult and remains an important problem in the research area. This paper proposes a new supervised multi-spike learning algorithm for recurrent spiking neural networks, which can implement the complex spatiotemporal pattern learning of spike trains. Using information encoded in precisely timed spike trains and their inner product operators, the error function is firstly constructed. Furthermore, the proposed algorithm defines the learning rules of synaptic weights based on inner product of spike trains. The algorithm is successfully applied to learn spike train patterns, and the high learning accuracy and efficiency are shown by the experimental results. In addition, the network structure parameters are analyzed, such as the neuron number and connectivity degree in the recurrent layer of spiking neural networks.
C1 [Lin, Xianghong; Shi, Guoyong] Northwest Normal Univ, Coll Comp Sci & Engn, Lanzhou 730070, Gansu, Peoples R China.
RP Lin, XH (corresponding author), Northwest Normal Univ, Coll Comp Sci & Engn, Lanzhou 730070, Gansu, Peoples R China.
EM linxh@nwnu.edu.cn
CR Allen J. N., 2005, 2005 48th IEEE International Midwest Symposium on Circuits and Systems (IEEE Cat. No. 05CH37691), P1741
   [Anonymous], 2014, INT J COMPUT APPL, V88, P40
   Bourdoukan R, 2015, ADV NEURAL INFORM PR, V28, P982
   Brodeur Simon, 2012, Artificial Neural Networks and Machine Learning - ICANN 2012. Proceedings 22nd International Conference on Artificial Neural Networks, P547, DOI 10.1007/978-3-642-33269-2_69
   Carnell A., 2005, P ESANN, P363
   Diehl PU, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON REBOOTING COMPUTING (ICRC)
   Gerstner W., 2002, SPIKING NEURON MODEL
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Gilra A, 2017, ELIFE, V6, DOI 10.7554/eLife.28295
   Kuroe Y., 2010, 2010 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2010.5596914
   [蔺想红 Lin Xianghong], 2015, [电子学报, Acta Electronica Sinica], V43, P577
   Paiva ARC, 2009, NEURAL COMPUT, V21, P424, DOI 10.1162/neco.2008.09-07-614
   Selvaratnam K., 2000, Transactions of the Institute of Systems, Control and Information Engineers, V13, P95, DOI 10.5687/iscie.13.3_95
   Shen JR, 2017, IEEE ENG MED BIO, P2900, DOI 10.1109/EMBC.2017.8037463
   Smith A. W, 2011, INT J NEURAL SYST, V1, P125
   Sporea I, 2013, NEURAL COMPUT, V25, P473, DOI 10.1162/NECO_a_00396
   Tino P, 2006, NEURAL COMPUT, V18, P591, DOI 10.1162/089976606775623360
   WERBOS PJ, 1990, P IEEE, V78, P1550, DOI 10.1109/5.58337
   Woo J, 2017, MALAYS J COMPUT SCI, V30, P258
NR 19
TC 8
Z9 8
U1 1
U2 10
PY 2018
VL 11139
BP 222
EP 234
DI 10.1007/978-3-030-01418-6_22
UT WOS:000463336400022
DA 2023-11-16
ER

PT C
AU Zheng, TY
   Li, F
   Du, XM
   Zhou, Y
   Li, N
   Gu, XF
AF Zheng, Ting-Ying
   Li, Fan
   Du, Xue-Mei
   Zhou, Yang
   Li, Na
   Gu, Xiao-Feng
GP IEEE
TI UNSUPERVISED IMAGE CLASSIFICATION WITH ADVERSARIAL SYNAPSE SPIKING
   NEURAL NETWORKS
SO 2019 16TH INTERNATIONAL COMPUTER CONFERENCE ON WAVELET ACTIVE MEDIA
   TECHNOLOGY AND INFORMATION PROCESSING (ICWAMTIP)
SE International Computer Conference on Wavelet Active Media Technology and
   Information Processing
DT Proceedings Paper
CT 16th IEEE International Computer Conference on Wavelet Active Media
   Technology and Information Processing (ICCWAMTIP)
CY DEC 13-15, 2019
CL Univ Elect Sci & Technol China, Chengdu, PEOPLES R CHINA
HO Univ Elect Sci & Technol China
DE Spiking neural networks; Adversarial synapses; Spike-timing dependent
   plasticity; Image classification
AB In this paper, we propose a spiking neural network architecture capable of learning different features between different labels. Our architecture, referred to as adversarial synapse spiking neural networks, pays more attention to the details of positive samples by inputting pairs of positive samples and negative samples. The proposed method uses a pair of adversarial input synapses, excitatory input synapses and inhibitory input synapses. Spike-timing dependent plasticity is used to train the weights in excitatory input synapses. Results shows the proposed method can significantly improve the image classification results of spiking neural networks.
C1 [Zheng, Ting-Ying; Li, Fan; Du, Xue-Mei; Zhou, Yang; Li, Na; Gu, Xiao-Feng] Univ Elect Sci & Technol China, Sch Informat & Software Engn, Chengdu 611731, Peoples R China.
RP Zheng, TY (corresponding author), Univ Elect Sci & Technol China, Sch Informat & Software Engn, Chengdu 611731, Peoples R China.
EM 1685728674@qq.com; lifan@uestc.edu.cn
CR Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Florence K. I., 2014, FRONTIERS COMPUTATIO, V8, p[53, Excitatory]
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LIU DC, 1989, MATH PROGRAM, V45, P503, DOI 10.1007/BF01589116
   Maass W, 1996, NEURAL COMPUT, V8, P1, DOI 10.1162/neco.1996.8.1.1
   Saunders D.J., 2018, INT JOINT C NEUR NET, P1
   WULFRAM G, 2002, SPIKING NEURON MODEL
   Zhang ML, 2018, IEEE T COGN DEV SYST, V10, P151, DOI 10.1109/TCDS.2017.2651943
NR 9
TC 0
Z9 0
U1 1
U2 4
PY 2019
BP 162
EP 165
UT WOS:000553538700034
DA 2023-11-16
ER

PT C
AU Bodyanskiy, Y
   Dolotov, A
   Pliss, I
   Malyar, M
AF Bodyanskiy, Yevgeniy
   Dolotov, Artem
   Pliss, Iryna
   Malyar, Mykola
BE Vynokurova, O
   Peleshko, D
TI A Fast Learning Algorithm of Self-Learning Spiking Neural Network
SO PROCEEDINGS OF THE 2016 IEEE FIRST INTERNATIONAL CONFERENCE ON DATA
   STREAM MINING & PROCESSING (DSMP)
DT Proceedings Paper
CT 1st IEEE International Conference on Data Stream Mining and Processing
   (DSMP)
CY AUG 23-27, 2016
CL Lviv, UKRAINE
DE self-learning spiking neural network; second order methods; temporal
   Hebbian rule
AB The paper introduces a Newton-type modification of temporal Hebbian rule-based learning algorithm of a self-learning spiking neural network. Similar to conventional artificial neural networks domain, the learning algorithm modification based on second-order optimization procedures allows of improving performance of the third generation neural networks. The experimental research results are presented to confirm the proposed improvement of self-learning spiking neural network learning algorithm.
C1 [Bodyanskiy, Yevgeniy; Dolotov, Artem; Pliss, Iryna] Kharkiv Natl Univ Radio Elect, 14 Nauky Ave, UA-61166 Kharkov, Ukraine.
   [Malyar, Mykola] Uzhgorod Natl Univ, 3 Narodna Sq, UA-88000 Uzhgorod, Ukraine.
RP Bodyanskiy, Y (corresponding author), Kharkiv Natl Univ Radio Elect, 14 Nauky Ave, UA-61166 Kharkov, Ukraine.
EM yevgeniy.bodyanskiy@nure.ua; artem.dolotov@gmail.com;
   iryna.pliss@nure.ua; malyarmm@gmail.com
CR Bodyanskiy Ye., 2009, IMAGE PROCESSING, P357
   Doborjeh M. G., 2015, IEEE T BIOMED ENG
   Gruning A, 2014, ESANN
   Oniz Y, 2015, NEUROCOMPUTING, V149, P690, DOI 10.1016/j.neucom.2014.07.061
   Silva M., 2016, J VOICE IN PRESS
   Yang J, 2012, APPL MATH LETT, V25, P1118, DOI 10.1016/j.aml.2012.02.016
NR 6
TC 1
Z9 1
U1 0
U2 4
PY 2016
BP 104
EP 107
UT WOS:000390239100015
DA 2023-11-16
ER

PT C
AU Kuroe, Y
   Mori, T
AF Kuroe, Y
   Mori, T
GP IEEE
   IEEE
TI Spiking neural oscillators
SO SICE 2002: PROCEEDINGS OF THE 41ST SICE ANNUAL CONFERENCE, VOLS 1-5
DT Proceedings Paper
CT 41st Annual Conference of the
   Society-of-Instrument-and-Control-Engineers (SICE 2002)
CY AUG 05-07, 2002
CL OSAKA, JAPAN
DE spiking neural network; neural oscillator; limit cycle; learning method
AB In spiking neural networks (SNN's) the information processing is carried out by spike trains in a manner similar to the generic biological neurons. This paper presents a method for synthesis of neural oscillators by SNN's. We propose a learning method of SNN's such that they possess the desired periodic spike trains with specified spike emission times.
C1 Kyoto Inst Technol, Dept Elect & Informat Sci, Sakyo Ku, Kyoto 6068585, Japan.
RP Kuroe, Y (corresponding author), Kyoto Inst Technol, Dept Elect & Informat Sci, Sakyo Ku, Kyoto 6068585, Japan.
EM kuroe@dj.kit.ac.jp
CR DOYA K, 1989, NEURAL NETWORKS, V2, P375, DOI 10.1016/0893-6080(89)90022-1
   GERSTNER W, 1993, ADV NEURAL INFORM PR, V6, P363
   Guckenheimer J., 2013, NONLINEAR OSCILLATIO
   Kimura H, 1998, 1998 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS - PROCEEDINGS, VOLS 1-3, P50, DOI 10.1109/IROS.1998.724595
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   MASS W, 1998, PULSED NEURAL NETS
   MATSUOKA K, 1987, BIOL CYBERN, V56, P345, DOI 10.1007/BF00319514
   SELVARATNAM K, 2000, T I SYSTEMS CONTROL, V44, P95
NR 8
TC 1
Z9 1
U1 0
U2 0
PY 2002
BP 725
EP 729
UT WOS:000182534000155
DA 2023-11-16
ER

PT C
AU Ichishita, T
   Fujii, RH
AF Ichishita, T.
   Fujii, R. H.
BE Miyazaki, T
   Paik, I
   Wei, D
TI Performance evaluation of a temporal sequence learning spiking neural
   network
SO 2007 CIT: 7TH IEEE INTERNATIONAL CONFERENCE ON COMPUTER AND INFORMATION
   TECHNOLOGY, PROCEEDINGS
DT Proceedings Paper
CT 7th IEEE International Conference on Computer and Information Technology
CY OCT 16-19, 2007
CL Aizu Wakamatsu, JAPAN
AB The performance evaluation of a temporal sequence learning spiking neural network was carried out. Neural network characteristics that were evaluated included: long temporal sequence length recognition, factors that affect size of the neural network, and network robustness against random input noise. Music melodies of various lengths were used as temporal sequential input data for the evaluation. Results have shown that the spiking neural network can be made to learn inter-spike time sequences comprised of as many as 900 inter-spike times. The size of the neural network was influenced by the amount and hype of random noise used during the supervised learning phase. The spiking neural network system performance was approximately 90% accurate in recognizing sequences even in the presence of various types of random noise.
C1 [Ichishita, T.; Fujii, R. H.] Univ Aizu, Aizu Wakamatsu, Fukushima, Japan.
RP Ichishita, T (corresponding author), Univ Aizu, Aizu Wakamatsu, Fukushima, Japan.
EM fujii@u-aizu.ac.jp
CR Amin H, 2004, IEEE IJCNN, P477, DOI 10.1109/IJCNN.2004.1379956
   AMIN HH, 2005, 48 IEEE INT MIDW S C, P683
   Gerstner W., 2002, SPIKING NEURON MODEL
   HAYKIN S., 1999, NEURAL NETWORK COMPR
   Maass W, 1996, NEURAL COMPUT, V8, P1, DOI 10.1162/neco.1996.8.1.1
   Maass W., 1999, PULSED NEURAL NETWOR
   WANG DL, 1990, P IEEE, V78, P1536, DOI 10.1109/5.58329
NR 7
TC 1
Z9 1
U1 0
U2 2
PY 2007
BP 616
EP 620
DI 10.1109/CIT.2007.64
UT WOS:000252104400107
DA 2023-11-16
ER

PT C
AU Shrestha, SB
   Song, Q
AF Shrestha, Sumit Bam
   Song, Qing
GP IEEE
TI Event Based Weight Update for Learning Infinite Spike Train
SO 2016 15TH IEEE INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND
   APPLICATIONS (ICMLA 2016)
DT Proceedings Paper
CT 15th IEEE International Conference on Machine Learning and Applications
   (ICMLA)
CY DEC 18-20, 2016
CL Anaheim, CA
DE Spiking Neural Network (SNN); spike-event; spike train; supervised
   learning; multilayer network
ID GRADIENT DESCENT; NETWORKS; NEURONS
AB Supervised Learning methods for Spiking Neural Network are either able to learn spike train for a single neuron or able to learn first spike in a multilayer feedforward connection setting. The first group of learning methods do not use the computational benefits of hidden layer neuron whereas the second group of learning methods do not exploit the information transfer potential of spike train. Although, there have been few efforts to learn spike train in multilayer feedforward setting for spiking neural networks, the computational cost of these methods increases when spike train is considered for long period. We present spike event based weight update strategy that is able to learn spike train pattern in multilayer feedforward spiking neural network and is efficient and scalable for learning spike train pattern for indefinite period of time. We will compare this method with relevant spiking neural network learning algorithms based on different benchmark datasets and show the efficacy of this event based weight update learning.
C1 [Shrestha, Sumit Bam; Song, Qing] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore, Singapore.
RP Shrestha, SB (corresponding author), Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore, Singapore.
EM sumitbam001@e.ntu.edu.sg; eqsong@ntu.edu.sg
CR [Anonymous], 2013, INT JOINT C NEUR NET, DOI DOI 10.1109/AGILE.2013.7
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   Booij O, 2005, INFORM PROCESS LETT, V95, P552, DOI 10.1016/j.ipl.2005.05.023
   Butts DA, 2007, NATURE, V449, P92, DOI [10.1038/nature06105, 10.1038/natureO6105]
   Carnell A., 2005, P ESANN, P363
   Dauwels Justin, 2008, ADV NEUROINFORMATION, P177
   Florian RV, 2007, NEURAL COMPUT, V19, P1468, DOI 10.1162/neco.2007.19.6.1468
   GERSTNER W, 1995, PHYS REV E, V51, P738, DOI 10.1103/PhysRevE.51.738
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   Gollisch T, 2008, SCIENCE, V319, P1108, DOI 10.1126/science.1149639
   Haykin S., 2009, SER NEURAL NETWORKS, V10
   Jaeger H, 2001, 14834 GMD GERM NAT R, V148, P34
   Kasabov NK, 2014, NEURAL NETWORKS, V52, P62, DOI 10.1016/j.neunet.2014.01.006
   Maass W, 1996, NEURAL COMPUT, V8, P1, DOI 10.1162/neco.1996.8.1.1
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maass W, 1996, ADV NEUR IN, V8, P211
   MAINEN ZF, 1995, SCIENCE, V268, P1503, DOI 10.1126/science.7770778
   McKennoch S, 2006, IEEE IJCNN, P3970
   Paugam-Moisy H., 2011, HDB NATURAL COMPUTIN, V1, P335
   Paugam-Moisy H, 2008, NEUROCOMPUTING, V71, P1143, DOI 10.1016/j.neucom.2007.12.027
   Ponulak F., 2005, RESUME NEW SUPERVISE
   Ponulak F, 2011, ACTA NEUROBIOL EXP, V71, P409
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Ruf B, 1997, NEURAL PROCESS LETT, V5, P9, DOI 10.1023/A:1009697008681
   Schrauwen Benjamin, 2004, NEUR NETW 2004 P 200, V1
   Shadlen Michael N., 1994, Current Opinion in Neurobiology, V4, P569, DOI 10.1016/0959-4388(94)90059-0
   Shrestha SB, 2015, NEURAL NETWORKS, V63, P185, DOI 10.1016/j.neunet.2014.12.001
   Shrestha SB, 2013, ANN ALLERTON CONF, P506, DOI 10.1109/Allerton.2013.6736567
   Sporea I, 2013, NEURAL COMPUT, V25, P473, DOI 10.1162/NECO_a_00396
   Van Rullen R, 2001, NEURAL COMPUT, V13, P1255, DOI 10.1162/08997660152002852
   Xu Y, 2013, NEURAL NETWORKS, V43, P99, DOI 10.1016/j.neunet.2013.02.003
NR 32
TC 7
Z9 7
U1 0
U2 2
PY 2016
BP 333
EP 338
DI 10.1109/ICMLA.2016.92
UT WOS:000399100100052
DA 2023-11-16
ER

PT C
AU Watanabe, F
   Fujii, RH
AF Watanabe, Fuyuko
   Fujii, Robert H.
GP IEEE
TI Sequence Learning and Generation Using a Spiking Neural Network
SO 2012 FOURTH INTERNATIONAL CONFERENCE ON COMMUNICATIONS AND ELECTRONICS
   (ICCE)
DT Proceedings Paper
CT 4th International Conference on Communications and Electronics (ICCE)
CY AUG 01-03, 2012
CL Hue, VIETNAM
DE spiking neuron; sequence learning; sequence generation; Liquid State
   Machine
AB A new neural network that uses the ReSuMe supervised learning algorithm for generating a desired spike output sequence in response to a given input spike sequence is proposed. Possible advantages of the proposed new neural network system compared to the Liquid State Machine based ReSuMe network system include better learning convergence and a smaller neural network size.
C1 [Watanabe, Fuyuko; Fujii, Robert H.] Univ Aizu, Comp Syst Dept, Aizu Wakamatsu, Fukushima 9658580, Japan.
RP Fujii, RH (corresponding author), Univ Aizu, Comp Syst Dept, Aizu Wakamatsu, Fukushima 9658580, Japan.
EM fujii@u-aizu.ac.jp
CR Amin HH, 2005, IEICE T INF SYST, VE88D, P1893, DOI 10.1093/ietisy/e88-d.8.1893
   [Anonymous], COGNITIVE, DOI DOI 10.1207/S15516709C0G1402_1
   [Anonymous], 2005, RESUME NEW SUPERVISE
   Fujii R. H., 2007, 6 INT C MACH LEARN A, P217
   Fujii RH, 2006, LECT NOTES COMPUT SC, V4131, P780
   Gerstner W., 2002, SPIKING NEURON MODEL
   Konishi Y., 2004 IEEE INT JOINT, V4, P25
   Maass W, 2004, MATH COMP BIOL SER, P575
   Math Works Inc, MATLAB
   Ponulak F., 2009, SUPERVISED LEARNING
   Ponulak F., 2006, THESIS POZNAN U TECH
   WANG DL, 1995, IEEE T SYST MAN CYB, V25, P615, DOI 10.1109/21.370192
NR 12
TC 1
Z9 1
U1 0
U2 2
PY 2012
BP 500
EP 505
UT WOS:000310625800098
DA 2023-11-16
ER

PT C
AU Tian, J
   Li, JP
   Wang, GS
   Xiao, F
AF Tian Jie
   Li Jianping
   Wang Guangshuo
   Xiao Fei
GP IEEE
TI AN OVERVIEW OF SPIKINGNEURAL NETWORKS
SO 2022 19TH INTERNATIONAL COMPUTER CONFERENCE ON WAVELET ACTIVE MEDIA
   TECHNOLOGY AND INFORMATION PROCESSING (ICCWAMTIP)
SE International Computer Conference on Wavelet Active Media Technology and
   Information Processing
DT Proceedings Paper
CT 19th IEEE International Computer Conference on Wavelet Active Media
   Technology and Information Processing (ICCWAMTIP)
CY DEC 16-18, 2022
CL Univ Elect Sci & Technol China, Chengdu, PEOPLES R CHINA
HO Univ Elect Sci & Technol China
DE Spiking neural networks; Leaky Integrate and Fired Model; Izhikevich
   Model
AB In recent years, Artificial neural network has made great progress in image, machine perception and other aspects, and has a very good performance in the scope of deep learning. As a highly intensive neural network, Artificial neural network's performance has gradually reached saturation in today's increasing network demand, but its efficiency and consumption are still relatively large. Therefore, more and more attention has been paid to the peak neural network with low energy consumption in operating equipment. Spiking neural networks shows good performance of low power consumption when running on hardware. More and more researchers begin to use Spiking neural networks to study the performance of image recognition and other aspects. Although Spiking neural network has many limitations in accuracy and training difficulty, it has stimulated the research enthusiasm of many researchers. Spiking neural networks has developed rapidly, and many training methods can achieve the same or even higher accuracy than Artificial neural networks. In this paper, we further understand the advantages and framework of Spiking neural network through its development.
C1 [Tian Jie; Li Jianping; Wang Guangshuo; Xiao Fei] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
RP Tian, J (corresponding author), Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
EM 202122080902@std.uestc.edu.cn; jpl2222@uestc.edu.cn;
   202021080416@std.uestc.edu.cn; 202122080904@std.uestc.edu.cn
CR Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Fang W, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2641, DOI 10.1109/ICCV48922.2021.00266
   Hu YF, 2023, IEEE T NEUR NET LEAR, V34, P5200, DOI 10.1109/TNNLS.2021.3119238
   Stöckl C, 2021, NAT MACH INTELL, V3, DOI 10.1038/s42256-021-00311-4
   Zhao DC, 2022, INFORM SCIENCES, V610, P1, DOI 10.1016/j.ins.2022.07.152
   Zheng HL, 2021, AAAI CONF ARTIF INTE, V35, P11062
NR 6
TC 0
Z9 0
U1 2
U2 3
PY 2022
DI 10.1109/ICCWAMTIP56608.2022.10016558
UT WOS:000932922500080
DA 2023-11-16
ER

PT C
AU Abed, BAR
   Aziz, NA
   Ismail, AR
AF Abed, Bassam Abdul-Rahman
   Aziz, Normaziah Abdul
   Ismail, Amelia Ritahani
GP IEEE
TI Simple Recurrent Network in Real Time Astrocyte
SO 2015 INTERNATIONAL SYMPOSIUM ON INNOVATIONS IN INTELLIGENT SYSTEMS AND
   APPLICATIONS (INISTA) PROCEEDINGS
DT Proceedings Paper
CT International Symposium on Innovations in Intelligent SysTems and
   Applications (INISTA 2015)
CY SEP 02-04, 2015
CL Madrid, SPAIN
DE Simple recurrent network; Spiking Response Model; Astrocytes; Elman
   neural network; Jordan neural network
ID SPIKING NEURONS; MODEL
AB in this paper we propose simple recurrent network (SRN) and mathematical paradigm to model real time interaction of astrocyte in simplified spiking neural network (SRM0). Both Elman neural network and Jordan neural network are used in the proposed network. Elman neural network is connected to the synapse and Jordan neural network is connected to the postsynaptic neuron in tripartite synapse, the simulation results showed that whenever astrocytes were activating in a time window, more spikes will be fired for excitatory neurons. This is biologically justified since astrocytes induce excitability with variations in Ca2+ concentrations
C1 [Abed, Bassam Abdul-Rahman; Aziz, Normaziah Abdul; Ismail, Amelia Ritahani] Int Islamic Univ Malaysia, Dept Comp Sci, Kuala Lumpur, Malaysia.
RP Abed, BAR (corresponding author), Int Islamic Univ Malaysia, Dept Comp Sci, Kuala Lumpur, Malaysia.
EM Bassamabed2000@gmail.com; naa@iium.edu.my; amelia@iium.edu.my
CR Araque A, 1999, TRENDS NEUROSCI, V22, P208, DOI 10.1016/S0166-2236(98)01349-6
   Berger T. W., 2014, ENCY COMPUTATIONAL N, P1
   DESTEXHE A, 1994, NEURAL COMPUT, V6, P14, DOI 10.1162/neco.1994.6.1.14
   Gerstner W, 2001, NEURAL NETWORKS, V14, P599, DOI 10.1016/S0893-6080(01)00053-3
   Ghosh-Dastidar S, 2009, ADV INTEL SOFT COMPU, V61, P167
   Haghiri S., 2014, EL ENG ICEE 2014 22
   Haydon PG, 2006, PHYSIOL REV, V86, P1009, DOI 10.1152/physrev.00049.2005
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   Izhikevich EM, 2001, NEURAL NETWORKS, V14, P883, DOI 10.1016/S0893-6080(01)00078-8
   Kempter R, 1999, PHYS REV E, V59, P4498, DOI 10.1103/PhysRevE.59.4498
   Li Y., 2004, 2013 IEEE COMP SOC A
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Nadkarni S, 2004, PHYS BIOL, V1, P35, DOI 10.1088/1478-3967/1/1/004
   Perea G, 2009, TRENDS NEUROSCI, V32, P421, DOI 10.1016/j.tins.2009.05.001
   Pernice V, 2012, PHYS REV E, V85, DOI 10.1103/PhysRevE.85.031916
   Postnov DE, 2007, BIOSYSTEMS, V89, P84, DOI 10.1016/j.biosystems.2006.04.012
   Rezende D. J., 2011, ADV NEURAL INFORM PR
   Shiltagh N. A., 2014, INT J COMPUTER APPL, V88, P40
   Sneyd J., 2002, P S APPL MATH
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   Tino P, 2006, NEURAL COMPUT, V18, P591, DOI 10.1162/089976606775623360
   Valenza G, 2011, NEURAL NETWORKS, V24, P679, DOI 10.1016/j.neunet.2011.03.013
   Valenza G., 2013, ENG MED BIOL SOC EMB
   Volman V, 2007, NEURAL COMPUT, V19, P303, DOI 10.1162/neco.2007.19.2.303
   Wade JJ, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0029445
   Yuan CW, 2012, FRONT COMPUT NEUROSC, V6, DOI 10.3389/fncom.2012.00025
NR 27
TC 0
Z9 0
U1 0
U2 4
PY 2015
BP 302
EP 308
UT WOS:000380428200041
DA 2023-11-16
ER

PT J
AU Li, DH
   Zeng, SY
   Ma, CX
AF Li, Daihui
   Zeng, Shangyou
   Ma, Chengxu
TI Spike buffer: improve deep network performance by offset mechanism
SO JOURNAL OF ENGINEERING-JOE
DT Article
DE computational complexity; convolutional neural nets; feature selection;
   feature extraction; feature selection performance; integrated spike
   buffer; deep network performance; offset mechanism; offset buffer-bit;
   gradient spike function; convolution channels; deep convolution neural
   networks; invalid feature extraction; computational complexity;
   nonlinear mapping
AB For a well-designed neural network model, it is difficult to further improve its performance. This study proposes an offset mechanism called spike buffer, which can effectively improve the performance of the designed convolutional neural networks. The spike buffer introduces an offset buffer-bit and a gradient spike function in the convolution channels to enhance the expression of effective features and suppresses the extraction of invalid features. Without significantly increasing the computational complexity of deep convolution neural networks, it can improve the feature selection performance of convolution neural networks and enhance the ability of non-linear mapping, and can be easily embedded into various convolution neural networks. Experiments show that the performance of convolutional neural networks with integrated spike buffer can be effectively improved.
C1 [Li, Daihui; Zeng, Shangyou; Ma, Chengxu] Guangxi Normal Univ, Coll Elect Engn, Guilin, Guangxi, Peoples R China.
RP Li, DH (corresponding author), Guangxi Normal Univ, Coll Elect Engn, Guilin, Guangxi, Peoples R China.
EM ldhcv@sina.com
CR He K., 2015, ABS151203385 CORR
   He T, 2019, PROC CVPR IEEE, P558, DOI 10.1109/CVPR.2019.00065
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Jia X., 2018, P WORKSH SYST ML OP
   Krizhevsky A., TR20095 U TOR
NR 5
TC 0
Z9 0
U1 0
U2 5
PD SEP
PY 2020
VL 2020
IS 9
BP 751
EP 754
DI 10.1049/joe.2019.1235
UT WOS:000582219500002
DA 2023-11-16
ER

PT C
AU Li, JL
   Hu, WT
   Yuan, Y
   Huo, H
   Fang, T
AF Li, Jingling
   Hu, Weitai
   Yuan, Ye
   Huo, Hong
   Fang, Tao
BE Liu, D
   Xie, S
   Li, Y
   Zhao, D
   ElAlfy, ESM
TI Bio-Inspired Deep Spiking Neural Network for Image Classification
SO NEURAL INFORMATION PROCESSING (ICONIP 2017), PT II
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 24th International Conference on Neural Information Processing (ICONIP)
CY NOV 14-18, 2017
CL Guangzhou, PEOPLES R CHINA
DE Spiking neural networks; Convolution neural networks; IF neuron; Image
   classification
AB Spiking neural networks (SNNs) are a kind of data-driven and event-driven hierarchical networks, and they are closer to the biological mechanism than other traditional neural networks. In SNNs, signals are transmitted as spikes between neurons, and spike transmission is easily implemented on hardware platform for large-scale real-time deep network computing. However, the unsupervised learning methods for spike neurons, such as the STDP learning methods, generally are ineffective in training deep spiking neural networks for image classification application. In this paper, the network parameters (weights and bias) obtained from training a convolution neural network (CNN), are converted and utilized in a deep spiking neural network with the similar structure as the CNN, which make the deep SNN be capable of classifying images. Since the CNN is composed of analog neurons, there will be some transfer losses in the process of conversion. After the main sources of transfer losses are analyzed, some reasonable optimization strategies are proposed to reduce the losses while retain a higher accuracy, such as max-pooling, softmax and weight normalization. The deep spiking neural network proposed in this paper is closer to the biological mechanism in the design of neurons and our work is helpful for understanding the spike activity of the brain. The proposed deep SNN is evaluated on CIFAR and MNIST benchmarks and the experimental results have shown that the proposed deep SNN outperforms the state-of-the-art spiking network models.
C1 [Li, Jingling; Hu, Weitai; Yuan, Ye; Huo, Hong; Fang, Tao] Shanghai Jiao Tong Univ, Dept Automat, Shanghai, Peoples R China.
   [Li, Jingling; Hu, Weitai; Yuan, Ye; Huo, Hong; Fang, Tao] Minist Educ, Key Lab Syst Control & Informat Proc, Shanghai, Peoples R China.
RP Fang, T (corresponding author), Shanghai Jiao Tong Univ, Dept Automat, Shanghai, Peoples R China.; Fang, T (corresponding author), Minist Educ, Key Lab Syst Control & Informat Proc, Shanghai, Peoples R China.
EM jinglingli@sjtu.edu.cn; tfang@sjtu.edu.cn
CR Brader JM, 2007, NEURAL COMPUT, V19, P2881, DOI 10.1162/neco.2007.19.11.2881
   Camuñas-Mesa L, 2012, IEEE J SOLID-ST CIRC, V47, P504, DOI 10.1109/JSSC.2011.2167409
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Diehl P.U., 2015, 2015 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2015.7280696
   Folowosele F, 2008, 2008 IEEE BIOMEDICAL CIRCUITS AND SYSTEMS CONFERENCE - INTELLIGENT BIOMEDICAL SYSTEMS (BIOCAS), P181, DOI 10.1109/BIOCAS.2008.4696904
   Foster I, 2002, TECHNICAL REPORT
   Hunsberger Eric, 2015, COMPUT SCI
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
   Krizhevsky Alex, 2009, LEARNING MULTIPLE LA, V3, P6
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Maass W, 2004, J COMPUT SYST SCI, V69, P593, DOI 10.1016/j jcss.2004.04.001
   Neftci EO, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00241
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   Vincent E, 2008, INT CONF ACOUST SPEE, P109, DOI 10.1109/ICASSP.2008.4517558
   Yu AJ, 2002, NEURAL COMPUT, V14, P2857, DOI 10.1162/089976602760805313
NR 16
TC 8
Z9 8
U1 1
U2 14
PY 2017
VL 10635
BP 294
EP 304
DI 10.1007/978-3-319-70096-0_31
PN II
UT WOS:000576766300031
DA 2023-11-16
ER

PT C
AU Gavrilov, AV
   Maliavko, AA
   Yakimenko, AA
AF Gavrilov, Andrey V.
   Maliavko, Alexandr A.
   Yakimenko, Alexandr A.
GP IEEE
TI Key-Threshold Based Spiking Neural Network
SO 2017 SECOND RUSSIA AND PACIFIC CONFERENCE ON COMPUTER TECHNOLOGY AND
   APPLICATIONS (RPC 2017)
DT Proceedings Paper
CT 2nd Russia and Pacific Conference on Computer Technology and
   Applications (RPC)
CY SEP 25-29, 2017
CL Vladivostok, RUSSIA
DE neural networks; spiking neural networks; machine learning; pattern
   recognition
AB In the paper a novel model of Key-Threshold based Spiking Neural Network (KTSNN) is proposed. This neural network consists of quasi-neurons oriented to recognize any key spikes distributed in time (sequence of spikes) or in space (in synapses). Every neuron aims to recognize key (template of spikes) stored in its memory and to react by output spike at successfully detection. Software implementation of this model is suggested. Possible methods of learning, implementation and usage of this model are discussed.
C1 [Gavrilov, Andrey V.; Maliavko, Alexandr A.; Yakimenko, Alexandr A.] Novosibirsk State Tech Univ, Dept Comp Engn, Novosibirsk, Russia.
RP Gavrilov, AV (corresponding author), Novosibirsk State Tech Univ, Dept Comp Engn, Novosibirsk, Russia.
EM Andr_gavrilov@yahoo.com; translab@ngs.ru; al-le@yandex.ru
CR Gavrilov Andrey V., 2016, 2016 11th International Forum on Strategic Technology (IFOST), P521, DOI 10.1109/IFOST.2016.7884170
   Gavrilov A. V., 1991, NEURAL NETW WORLD, P59
   Gavrilov AV, 2016, INT CONF ACT PROB EL, P455, DOI 10.1109/APEIE.2016.7806372
   Gerstner W., 2002, SPIKING NEURON MODEL
   Ghosh-Dastidar S, 2009, ADV INTEL SOFT COMPU, V61, P167
   GRZYB BJ, 2009, INT JOINT C NEUR NET
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Monroe D, 2014, COMMUN ACM, V57, P13, DOI 10.1145/2601069
   Yanduo Zhang, 2009, J COMPUTERS, V4, P1183
NR 11
TC 2
Z9 2
U1 0
U2 2
PY 2017
BP 64
EP 67
UT WOS:000426976700015
DA 2023-11-16
ER

PT J
AU Bodyanskiy, Y
   Dolotov, A
AF Bodyanskiy, Ye.
   Dolotov, A.
TI ANALOG-DIGITAL SELF-LEARNING FUZZY SPIKING NEURAL NETWORK AND ITS
   LEARNING ALGORITHM BASED ON` WINNER-TAKES-MORE' RULE
SO RADIO ELECTRONICS COMPUTER SCIENCE CONTROL
DT Article
DE analog-digital architecture; self-learning fuzzy spiking neural network;
   automatic; control theory; unsupervised learning algorithm,
   'winner-takes-more' rule
ID NEURONS
AB Analog-digital architecture of self-learning fuzzy spiking neural network is proposed in this paper. Spiking neuron synapse and some are treated in terms of classical automatic control theory. Conventional unsupervised learning algorithm of spiking neural network is improved by applying `Winner-Takes-More' rule.
C1 [Bodyanskiy, Ye.; Dolotov, A.] Kharkov Natl Univ Radio Elect, Kharkov, Ukraine.
RP Bodyanskiy, Y (corresponding author), Kharkov Natl Univ Radio Elect, Kharkov, Ukraine.
CR [Anonymous], 2006, INNOVATIONS FUZZY CL
   [Anonymous], 1998, PULSED NEURAL NETWOR
   [Anonymous], TITLE ERROR
   Bezdek J.C., 2005, FUZZY MODELS ALGORIT
   Bodyanskiy Ye, 2008, WISSENSCHAFTLICHE BE, V100, P2360
   Bodyanskiy Ye, 2009, ADAPTIVE GUSTAFSON K, P17
   Bodyanskiy Ye, 2009, P INT C INT SYST DEC, V2, P154
   Bodyanskiy Ye, 2008, SCI P RIG TU INF TEC, P27
   Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   COTTRELL M, 1986, BIOL CYBERN, V53, P405, DOI 10.1007/BF00318206
   Gerstner W., 2002, SPIKING NEURON MODEL
   Haykin S., 1999, NEURAL NETWORKS COMP
   Jang J.-S. R., 1997, IEEE T AUTOM CONTROL, V42, P1482, DOI DOI 10.1109/TAC.1997.633847
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Natschlager T, 1998, NETWORK-COMP NEURAL, V9, P319, DOI 10.1088/0954-898X/9/3/003
   Ritter H., 1986, BIOL CYBERNETICS, P239
NR 16
TC 0
Z9 0
U1 0
U2 1
PY 2010
VL 1
BP 109
EP 116
UT WOS:000216948900020
DA 2023-11-16
ER

PT C
AU Li, J
   Liu, B
   Gao, WX
   Huang, XY
AF Li, Jing
   Liu, Bo
   Gao, Weixin
   Huang, Xiaoyan
BE Xu, B
TI Spiking neural network with synaptic plasticity for recognition
SO PROCEEDINGS OF 2018 IEEE 3RD ADVANCED INFORMATION TECHNOLOGY, ELECTRONIC
   AND AUTOMATION CONTROL CONFERENCE (IAEAC 2018)
DT Proceedings Paper
CT 3rd IEEE Advanced Information Technology, Electronic and Automation
   Control Conference (IAEAC)
CY OCT 12-14, 2018
CL Chongqing, PEOPLES R CHINA
DE spiking neural network; DSSN; synaptic plasticity; recognition
ID NEURONS; MODELS
AB The spiking neural network referred to the thirdgeneration of neurons and computational neural network simulates the mechanisms of networks in brain. It has the distributed mechanism and robust information processing way like the nervous system. This paper describes that a spiking neural network with the synaptic plasticity recognizes the input scenes. The Digital spiking silicon neuron (DSSN), a mathematical structure -based qualitative model, is used to reproduce the various behaviors of neurons. We also designed the synapse model in our spiking neural network to precisely describe the dynamics of the transmitter release and the postsynaptic current generation. There are three layers in our network. The spiking neurons in layer 1 and 2 with special receptive fields perform the edge detection and orientation selection, respectively. The synaptic plasticity is realized in synaptic connections between spiking neurons in layer 2 and the output layer. The changing of connection is based on the Hebbian learning rule which supposes that the time difference of two spikes modifies the value of connection. We evaluated our spiking neural network with the task of image recognition. The spiking neurons in the output layer fire with the high frequency in response to their relevant input scenes. The simulation results show that our spiking neural network can successfully recognize the input scenes learned before. The recognition is robust against various distortions.
C1 [Li, Jing; Gao, Weixin; Huang, Xiaoyan] Xian Shiyou Univ, Sch Elect Engn, Xian, Shaanxi, Peoples R China.
   [Liu, Bo] Xidian Univ, Key Lab Intelligent Percept & Image Understanding, Sch Artificial Intelligence,Minist Educ, Int Res Ctr Intelligent Percept & Computat,Joint, Xian, Shaanxi, Peoples R China.
RP Li, J (corresponding author), Xian Shiyou Univ, Sch Elect Engn, Xian, Shaanxi, Peoples R China.
EM lijing@xsyu.edu.cn; liub@xidian.edu.cn; Wxgao@xsyu.edu.cn;
   xyhuang1212@163.com
CR Beyeler M, 2013, NEURAL NETWORKS, V48, P109, DOI 10.1016/j.neunet.2013.07.012
   Butts DA, 2007, NATURE, V449, P92, DOI [10.1038/nature06105, 10.1038/natureO6105]
   Dora S, 2015, APPL SOFT COMPUT, V36, P255, DOI 10.1016/j.asoc.2015.06.062
   Egger V, 1999, NAT NEUROSCI, V2, P1098, DOI 10.1038/16026
   Gupta A, 2007, IEEE IJCNN, P53, DOI 10.1109/IJCNN.2007.4370930
   Hebb D.O., 1949, ORG BEHAV NEUROPSYCH
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Iakymchuk T, 2015, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-015-0059-4
   Izhikevich EM., 2007, DYNAMICAL SYSTEMS NE, DOI [DOI 10.1017/S0143385704000173, 10.7551/mitpress/2526.001.0001]
   Kohno T., 2007, P INT S ART LIF ROB
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Masquelier T, 2008, PLOS ONE, V3, DOI 10.1371/journal.pone.0001377
   Mohemmed A, 2013, NEUROCOMPUTING, V107, P3, DOI 10.1016/j.neucom.2012.08.034
   Morrison A, 2008, BIOL CYBERN, V98, P459, DOI 10.1007/s00422-008-0233-1
   Panzeri S, 2010, TRENDS NEUROSCI, V33, P111, DOI 10.1016/j.tins.2009.12.001
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Rekabdar B, 2015, J INTELL ROBOT SYST, V80, pS83, DOI 10.1007/s10846-015-0179-1
   STEIN RB, 1967, BIOPHYS J, V7, P37, DOI 10.1016/S0006-3495(67)86574-3
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   Wu QX, 2010, LECT NOTES COMPUT SC, V6215, P49
   Yu Q, 2014, NEUROCOMPUTING, V138, P3, DOI 10.1016/j.neucom.2013.06.052
NR 21
TC 1
Z9 1
U1 0
U2 4
PY 2018
BP 1728
EP 1732
UT WOS:000457576900340
DA 2023-11-16
ER

PT C
AU Tahtirvanci, A
   Durdu, A
   Yilmaz, B
AF Tahtirvanci, Aykut
   Durdu, Akif
   Yilmaz, Burak
GP IEEE
TI Classification of EEG Signals Using Spiking Neural Networks
SO 2018 26TH SIGNAL PROCESSING AND COMMUNICATIONS APPLICATIONS CONFERENCE
   (SIU)
SE Signal Processing and Communications Applications Conference
DT Proceedings Paper
CT 26th IEEE Signal Processing and Communications Applications Conference
   (SIU)
CY MAY 02-05, 2018
CL Izmir, TURKEY
DE spiking neural networks; EEG; artifical neural networks; Izhikevich
   neuron model
AB In signal processing applications of conventional artificial neural networks, the processing time of the data is high and the accuracy rates are not good enough. At the same time, time-dependent processing is not possible. In this study, classification of EEG signals was performed using an artificial neural network including the characteristics of spiking neural networks. Successful results were obtained using large data sets. Moreover, by using the neuron model of Eugene M. Izhikevich as the spiking neural network model, the EEG signals were processed biologically realistically.
C1 [Tahtirvanci, Aykut; Durdu, Akif] Selcuk Univ, Elekt Elekt Muhendisligi, Konya, Turkey.
   [Yilmaz, Burak] Konya Gida & Tarim Univ, Elekt Elekt Muhendisligi, Konya, Turkey.
RP Tahtirvanci, A (corresponding author), Selcuk Univ, Elekt Elekt Muhendisligi, Konya, Turkey.
EM aykuttahtirvanci@gmail.com; durdu.1@selcuk.edu.tr;
   burak.yilmaz@gidatarim.edu.tr
CR Andrzejak RG, 2001, PHYS REV E, V64, DOI 10.1103/PhysRevE.64.061907
   Botzheim J, 2012, JOINT INT CONF SOFT, P1954, DOI 10.1109/SCIS-ISIS.2012.6505305
   Canatalay P. J., 2016, THESIS, P26
   Durdu A, 2014, PROCEEDINGS OF THE 2014 16TH INTERNATIONAL CONFERENCE ON MECHATRONICS (MECHATRONIKA 2014), P370, DOI 10.1109/MECHATRONIKA.2014.7018286
   Ergene MC, 2016, INT C ELECT COMPUT, DOI 10.1109/ECAI.2016.7861177
   Gerstner W., 2002, SPIKING NEURON MODEL
   Gupta A, 2007, IEEE IJCNN, P53, DOI 10.1109/IJCNN.2007.4370930
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Kiani K., 2015, KBEI 2015
   Lapique L., 1907, J PHYSL PATHOL GEN, V9, P620, DOI DOI 10.1007/S00422-007-0189-6
   Liu Z, 2006, IEEE ICMA 2006: PROCEEDING OF THE 2006 IEEE INTERNATIONAL CONFERENCE ON MECHATRONICS AND AUTOMATION, VOLS 1-3, PROCEEDINGS, P19
   Lobov S, 2015, SENSORS-BASEL, V15, P27894, DOI 10.3390/s151127894
   Maass W., 1999, PULSED NEURAL NETWOR
   Ratnasingam S., 2011, NEURAL NETWORKS IJCN
   Vreeken J., 2003, SPIKING NEURAL NETWO
   Yalçin N, 2015, TURK J ELECTR ENG CO, V23, P421, DOI 10.3906/elk-1212-151
NR 17
TC 0
Z9 0
U1 1
U2 9
PY 2018
UT WOS:000511448500323
DA 2023-11-16
ER

PT C
AU Ma, Q
   Lin, XH
   Wang, XW
AF Ma, Qiang
   Lin, Xianghong
   Wang, Xiangwen
GP IOP
TI Supervised Learning of Single-Layer Spiking Neural Networks for Image
   Classification
SO 2018 2ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE
   APPLICATIONS AND TECHNOLOGIES (AIAAT 2018)
SE IOP Conference Series-Materials Science and Engineering
DT Proceedings Paper
CT 2nd International Conference on Artificial Intelligence Applications and
   Technologies (AIAAT)
CY AUG 08-10, 2018
CL Shanghai, PEOPLES R CHINA
AB The traditional artificial neural networks encode information through the spike firing rate. Spiking neural networks fall into the third-generation artificial neural network models, which use the precisely timed spike trains to encode neural information. The computational models can accurately simulate the neural network activities of human brain, and provide powerful capabilities of signal processing to solve the complex problem. In this paper, we propose a supervised learning algorithm for single-layer spiking neural networks based on the spike train kernel function, which can implement the complex spatio-temporal pattern learning of spike trains. Furthermore, a pattern classifier based on single-layer spiking neural networks is constructed for image recognition problem. We test the learning performance of the proposed algorithm by the image classification task on the LabelMe dataset. The experimental results show that the proposed algorithm has got good image classification accuracy for the test dataset, and the different sizes of receptive fields influence classification accuracies significantly.
C1 [Ma, Qiang; Lin, Xianghong; Wang, Xiangwen] Northwest Normal Univ, Coll Comp Sci & Engn, Lanzhou 730070, Gansu, Peoples R China.
RP Lin, XH (corresponding author), Northwest Normal Univ, Coll Comp Sci & Engn, Lanzhou 730070, Gansu, Peoples R China.
EM linxh@nwnu.edu.cn
CR Carnell A., 2005, P ESANN, P363
   Florian RV, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0040233
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Haykin S., 2009, NEURAL NETWORKS LEAR
   [蔺想红 Lin Xianghong], 2015, [电子学报, Acta Electronica Sinica], V43, P577
   Mukhopadhyay A K, 2018, 180209047 ARXIV
   Paugam-Moisy H., 2012, HDB NATURAL COMPUTIN, V1, P1, DOI [DOI 10.1007/978-3-540-92910-9_10, 10.1007/978-3-540-92910-9_10]
   Russell BC, 2008, INT J COMPUT VISION, V77, P157, DOI 10.1007/s11263-007-0090-8
   Schliebs S, 2012, INT J NEURAL SYST, V22, P1659
   Tapson JC, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00153
   Yu Q, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0078318
   2018, MATER TODAY PROC 1, V5, P360
   2010, NEURAL COMPUTATION, V22, P467
   2008, NEUROCOMPUTING, V71, P2055, DOI DOI 10.1016/J.NEUCOM.2007.10.020
   2014, NEURAL NETWORKS, V52, P62, DOI DOI 10.1016/J.NEUNET.2014.01.006
   2003, NEUROCOMPUTING, V52, P925, DOI DOI 10.1016/S0925-2312(02)00838-X
   2010, NEURAL PROCESS LETT, V32, P131, DOI DOI 10.1007/S11063-010-9149-6
   2013, NEURAL COMPUTATION, V25, P450
NR 18
TC 2
Z9 2
U1 0
U2 3
PY 2018
VL 435
AR 012049
DI 10.1088/1757-899X/435/1/012049
UT WOS:000467426500049
DA 2023-11-16
ER

PT C
AU Amin, HH
   Deabes, W
   Bouazza, K
AF Amin, Hesham H.
   Deabes, Wael
   Bouazza, Kheireddine
GP IEEE
TI Clustering of User Activities Based on Adaptive Threshold Spiking Neural
   Networks
SO 2017 NINTH INTERNATIONAL CONFERENCE ON UBIQUITOUS AND FUTURE NETWORKS
   (ICUFN 2017)
SE International Conference on Ubiquitous and Future Networks
DT Proceedings Paper
CT 9th International Conference on Ubiquitous and Future Networks (ICUFN)
CY JUL 04-07, 2017
CL Milan, ITALY
DE smart environment; spiking neural network; adaptive threshold
ID MODEL
AB Spiking neural networks are utilized in solving hard computation problems in intelligent systems. Spiking neural networks have a high computational power due to the implicit employment of various parameters such as input times and values in addition to neuron threshold, synaptic delays, and weights in their structures. On the other hand, smart environment techniques are emergent science in this decade. Intelligent systems represented by spiking neural network models and smart environments represented by sensors readings are utilized in this research for clustering users' activities during some period of time. A new learning algorithm for spiking neural network based on adaptation of the internal neuron threshold is proposed. Threshold adaptation is employed to help a spiking neuron to fire the lowest number of output spikes and to preserve all information of the input spike train on the same time. Simulations show that the clustering algorithm has encouraging results.
C1 [Amin, Hesham H.; Deabes, Wael; Bouazza, Kheireddine] Umm Al Qura Univ, Univ Coll, Comp Sci Dept, Mecca, Saudi Arabia.
   [Amin, Hesham H.] Aswan Univ, Fac Engn, Comp & Syst Dept, Aswan Governorate, Egypt.
   [Deabes, Wael] Mansoura Univ, Fac Engn, Comp & Syst Dept, Mansoura, Egypt.
RP Amin, HH (corresponding author), Umm Al Qura Univ, Univ Coll, Comp Sci Dept, Mecca, Saudi Arabia.
EM hhabuelhasan@uqu.edu.sa
CR Amin H. H., 2005, 2005 48th IEEE International Midwest Symposium on Circuits and Systems (IEEE Cat. No. 05CH37691), P683
   Amin H. H., 2011, SPIKING NEURAL NETWO
   Amin HH, 2005, IEICE T INF SYST, VE88D, P1893, DOI 10.1093/ietisy/e88-d.8.1893
   [Anonymous], SENSORS
   Bourobou STM, 2015, SENSORS-BASEL, V15, P11953, DOI 10.3390/s150511953
   Cook DJ, 2013, COMPUTER, V46, P62, DOI 10.1109/MC.2012.328
   Fontaine B, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003560
   Gaikwad K., 2012, INT J COMPUT SCI ENG, V2, P27, DOI DOI 10.5121/CSEIJ.2012.2403
   Gerstner W., 2002, SPIKING NEURON MODEL
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Jakkula V., 2009, ADV INTELL ENV
   Khan S. S., 2016, REV FALL DETECTION T
   Maass W., 1996, NEURAL COMPUT
   Mehr H. D., 2016, SMART GRID C FAIR IC, P1
   Nguyen L. T., 2015, UBICOMP ISWC 15, P1463
   Oster M., 2004, CIRCUITS SYST 2004 I
   Rafsanjani HN, 2015, ENERGIES, V8, P10996, DOI 10.3390/en81010996
   Rivera-Illingworth F, 2006, 2 IET INT C INT ENV, V2006, pv1
   Suryadevara NK, 2013, ENG APPL ARTIF INTEL, V26, P2641, DOI 10.1016/j.engappai.2013.08.004
   Trabelsi D, 2013, IEEE T AUTOM SCI ENG, V10, P829, DOI 10.1109/TASE.2013.2256349
   van Kasteren Tim, 2007, 3rd IET International Conference on Intelligent Environments, IE 07, P209
   Zheng D., 2015, INT J SMART HOME, V9, P207, DOI [10.14257/ijsh.2015.9.2.19, DOI 10.14257/IJSH.2015.9.2.19]
   Zhong LL, 2010, INT CONF COMP SCI, P295, DOI 10.1109/ICCSIT.2010.5564569
NR 23
TC 12
Z9 13
U1 1
U2 2
PY 2017
BP 1
EP 6
UT WOS:000425924400001
DA 2023-11-16
ER

PT C
AU Castellano, M
   Pipa, G
AF Castellano, Marta
   Pipa, Gordon
BE Mladenov, V
   KoprinkovaHristova, P
   Palm, G
   Villa, AEP
   Appollini, B
   Kasabov, N
TI Memory Trace in Spiking Neural Networks
SO ARTIFICIAL NEURAL NETWORKS AND MACHINE LEARNING - ICANN 2013
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 23rd International Conference on Artificial Neural Networks (ICANN)
CY SEP 10-OCT 13, 2013
CL Techn Univ Sofia, Sofia, BULGARIA
HO Techn Univ Sofia
DE spiking neural networks; memory trace; delayed-dynamical systems;
   reservoir computing
ID MODELS
AB Spiking neural networks have a limited memory capacity, such that a stimulus arriving at time t would vanish over a timescale of 200-300 milliseconds [1]. Therefore, only neural computations that require history dependencies within this short range can be accomplished. In this paper, the limited memory capacity of a spiking neural network is extended by coupling it to an delayed-dynamical system. This presents the possibility of information exchange between spiking neurons and continuous delayed systems.
C1 [Castellano, Marta; Pipa, Gordon] Univ Osnabruck, Inst Cognit Sci, D-49069 Osnabruck, Germany.
RP Castellano, M (corresponding author), Univ Osnabruck, Inst Cognit Sci, D-49069 Osnabruck, Germany.
EM mcastellano@uos.de; gpipa@uos.de
CR Appeltant L, 2011, NAT COMMUN, V2, DOI 10.1038/ncomms1476
   Buonomano DV, 2009, NAT REV NEUROSCI, V10, P113, DOI 10.1038/nrn2558
   Churchland MM, 2012, NATURE, V487, P51, DOI 10.1038/nature11129
   Durstewitz D, 2000, NAT NEUROSCI, V3, P1184, DOI 10.1038/81460
   Forde J.E., THESIS
   Ganguli S, 2008, P NATL ACAD SCI USA, V105, P18970, DOI 10.1073/pnas.0804451105
   Jager H., 2001, 147 GMD
   Körding KP, 2004, NATURE, V427, P244, DOI 10.1038/nature02169
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Maass W, 2004, J PHYSIOL-PARIS, V98, P315, DOI 10.1016/j.jphysparis.2005.09.020
   Maass W, 2007, PLOS COMPUT BIOL, V3, P15, DOI 10.1371/journal.pcbi.0020165
   MACKEY M, 1977, SCIENCE
   Mayor J., 2005, PHYS REV E, V72, P15
   Natschlager T., 2003, COMPUTER MODELS ANAL, P121
   Pascanu R., 2010, NEURAL NETWORKS, V1, P123
NR 15
TC 0
Z9 0
U1 1
U2 6
PY 2013
VL 8131
BP 264
EP 271
UT WOS:000342695200033
DA 2023-11-16
ER

PT C
AU Yusoff, N
   Grüning, A
AF Yusoff, Nooraini
   Gruening, Andre
BE Diamantaras, K
   Duch, W
   Iliadis, LS
TI Supervised Associative Learning in Spiking Neural Network
SO ARTIFICIAL NEURAL NETWORKS-ICANN 2010, PT I
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 20th International Conference on Artificial Neural Networks
CY SEP 15-18, 2010
CL Thessaloniki, GREECE
DE Spiking neural network; Associative learning; Supervised learning;
   Excitatory-Inhibitory network; Izhikevich spiking neurons
ID NEURONS; MODEL
AB In this paper, we propose a simple supervised associative learning approach for spiking neural networks. In an excitatory-inhibitory network paradigm with Izhikevich spiking neurons, synaptic plasticity is implemented on excitatory to excitatory synapses dependent on both spike emission rates and spike timings. As results of learning, the network is able to associate not just familiar stimuli but also novel stimuli observed through synchronised activity within the same subpopulation and between two associated subpopulations.
C1 [Yusoff, Nooraini; Gruening, Andre] Univ Surrey, Fac Engn & Phys Sci, Dept Comp, Surrey GU2 7XH, England.
RP Yusoff, N (corresponding author), Univ Surrey, Fac Engn & Phys Sci, Dept Comp, Surrey GU2 7XH, England.
EM n.yusoff@surrey.ac.uk; a.gruning@surrey.ac.uk
CR BLOOM F, 2001, BRAIN MIND BEHAVIOUR
   Brunel N, 2009, J COGNITIVE NEUROSCI, V21, P2300, DOI 10.1162/jocn.2008.21156
   CRICK F, 1989, NATURE, V337, P129, DOI 10.1038/337129a0
   Dayan P, 2005, THEORETICAL NEUROSCI
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Kasinski A., 2006, International Journal of Applied Mathematics and Computer Science, V16, P101
   Mongillo G, 2003, EUR J NEUROSCI, V18, P2011, DOI 10.1046/j.1460-9568.2003.02908.x
   Paugam-Moisy H, 2008, NEUROCOMPUTING, V71, P1143, DOI 10.1016/j.neucom.2007.12.027
NR 9
TC 0
Z9 0
U1 2
U2 12
PY 2010
VL 6352
BP 224
EP 229
PN I
UT WOS:000287889800030
DA 2023-11-16
ER

PT C
AU Roy, D
   Chakraborty, I
   Roy, K
AF Roy, Deboleena
   Chakraborty, Indranil
   Roy, Kaushik
BE Bertino, E
   Chang, CK
   Chen, P
   Damiani, E
   Goul, M
   Oyama, K
TI Scaling Deep Spiking Neural Networks with Binary Stochastic Activations
SO 2019 IEEE INTERNATIONAL CONFERENCE ON COGNITIVE COMPUTING (IEEE ICCC
   2019)
DT Proceedings Paper
CT 4th IEEE International Conference on Cognitive Computing (IEEE ICCC)
   Part of the IEEE World Congress on Services
CY JUL 08-13, 2019
CL Milan, ITALY
DE Spiking Neural Networks; Deep Learning; Binary Neural Networks;
   Stochastic Activations
AB The modern era has witnessed a proliferation of portable devices that use Artificial Intelligence (AI) to enhance user experiences. Majority of these AI tasks are performed by large neural networks, which require a good amount of memory and compute power. This has resulted in a growing interest in Spiking Neural Networks (SNNs) which communicate through binary activations or 'spikes', as they offer a bio-plausible and energy efficient alternative to traditional deep neural networks (DNNs). In this work, we present deep spiking neural networks with binary stochastic activations that are tailored for implementation on emerging hardware platforms. We evaluate two deep neural network models, VGG-9 and VGG-16 on CIFAR-10 and CIFAR-100 datasets, respectively, with binary stochastic activations. We achieve state of the accuracy and achieve 1.4x improvement in energy consumption because of spike-based communication versus a network with ReLU neurons. We further investigate extremely quantized version of these networks having binary weights and show an energy benefit of 28x over full-precision neural networks. Thus we present scalable deep spiking neural networks that achieve performance comparable to DNNs while achieving substantial energy benefit.
C1 [Roy, Deboleena; Chakraborty, Indranil; Roy, Kaushik] Purdue Univ, Elect & Comp Engn, W Lafayette, IN 47907 USA.
RP Roy, D (corresponding author), Purdue Univ, Elect & Comp Engn, W Lafayette, IN 47907 USA.
EM roy77@purdue.edu; ichakra@purdue.edu; kaushik@purdue.edu
CR Benayoun M, 2010, PLOS COMPUT BIOL, V6, DOI 10.1371/journal.pcbi.1000846
   Bengio, 2016, ABS160202830 CORR
   Cheng Yu, 2017, ARXIV171009282
   Collobert R., 2011, BIGLEARN NIPS WORKSH
   Garg I., 2018, ARXIV181206224
   Han S., 2015, C NEUR INF PROC SYST
   Hochreiter S., 2001, GRADIENT FLOW RECURR
   Howard Andrew G., 2017, MOBILENETS EFFICIENT
   Iandola Forrest N., 2016, P IEEE C COMPUTER VI
   Ioffe S., 2015, PR MACH LEARN RES, P448
   Keckler SW, 2011, IEEE MICRO, V31, P7, DOI 10.1109/MM.2011.89
   Kingma DP., 2017, ARXIV
   Krizhevsky Alex, 2009, LEARNING MULTIPLE LA
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Nessler B, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003037
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Radford A., 2018, IMPROVING LANGUAGE U
   Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Sengupta A, 2018, INT EL DEVICES MEET
   Sengupta A, 2016, SCI REP-UK, V6, DOI 10.1038/srep30039
   Sengupta A, 2016, IEEE T ELECTRON DEV, V63, P2963, DOI 10.1109/TED.2016.2568762
   Severa W, 2019, NAT MACH INTELL, V1, P86, DOI 10.1038/s42256-018-0015-y
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Tuma T, 2016, NAT NANOTECHNOL, V11, P693, DOI [10.1038/NNANO.2016.70, 10.1038/nnano.2016.70]
   Zhou Shuchang, 2016, ARXIV160606160
NR 27
TC 12
Z9 12
U1 0
U2 1
PY 2019
BP 50
EP 58
DI 10.1109/ICCC.2019.00020
UT WOS:000556773800007
DA 2023-11-16
ER

PT C
AU Kolesnytskyj, OK
   Kutsman, VV
   Skorupski, K
   Arshidinova, M
AF Kolesnytskyj, Oleh K.
   Kutsman, Vladislav V.
   Skorupski, Krzysztof
   Arshidinova, Mukaddas
BE Romaniuk, RS
   Linczuk, M
TI Neurocomputer architecture based on spiking neural network and its
   optoelectronic implementation
SO PHOTONICS APPLICATIONS IN ASTRONOMY, COMMUNICATIONS, INDUSTRY, AND
   HIGH-ENERGY PHYSICS EXPERIMENTS 2019
SE Proceedings of SPIE
DT Proceedings Paper
CT 44th WILGA Symposium on Photonics Applications and Web Engineering
CY MAY 26-JUN 02, 2019
CL Wilga, POLAND
DE neurocomputer; neurocomputer architecture; spiking neural network;
   construction principles; hardware implementation
ID SYSTEM
AB The paper clarifies neurocomputer and neurocomputer architecture term definitions. The choice of spiking neural network as neurocomputer operating unit is substantiated. The spiking neurocomputer organization principles are formulated by analyzing and generalization of the current level of knowledge on neurocomputer architecture (based on analogy with the well-known von Neumann digital computer organization principles). Analytical overview of current projects on spiking neural networks hardware implementation is conducted. Their major disadvantages are highlighted. Optoelectronic hardware implementation of spiking neural network is proposed as such that is free of mentioned disadvantages due to usage of optical signals for communication between neurons, as well as organization of learning through hardware. The main technical parameters of the proposed spiking neural network are estimated.
C1 [Kolesnytskyj, Oleh K.; Kutsman, Vladislav V.] Vinnytsia Natl Tech Univ, Khmelnytske Hwy 95, UA-21000 Vinnytsia, Vinnytska Oblas, Ukraine.
   [Skorupski, Krzysztof] Lublin Univ Technol, Nadbystrzycka 38A, PL-20618 Lublin, Poland.
   [Arshidinova, Mukaddas] Al Farabi Kazakh Natl Univ, Alma Ata 050040, Kazakhstan.
RP Kolesnytskyj, OK (corresponding author), Vinnytsia Natl Tech Univ, Khmelnytske Hwy 95, UA-21000 Vinnytsia, Vinnytska Oblas, Ukraine.
CR [Anonymous], 2016, INTERNET ED SCI IES
   Bardachenko V. F., 2003, Upravlyayushchie Sistemy i Mashiny, P73
   Chepurna O, 2015, PROC SPIE, V9816, DOI 10.1117/12.2229030
   Galushkin A. I., 2000, NEUROCOMPUTERS, P528
   Grytsenko V. I., 2004, CONTROL SYSTEMS MACH, P3
   Kolesnytskyj K, 2010, OPTICAL MEMORY NEURA, V19, P154
   Komartsova L. G., 2004, NEUROCOMPUTERS STUDY, P400
   Kozemiako V. P., 2012, P SOC PHOTO-OPT INS, V8698
   Krak Yu V., 2017, Journal of Automation and Information Sciences, V49, P65
   Krug P.G., 2002, NEURAL NETWORKS NEUR, P176
   Kryvonos IG, 2017, CYBERN SYST ANAL+, V53, P495, DOI 10.1007/s10559-017-9951-5
   Kukharchuk VV, 2017, PROC SPIE, V10445, DOI 10.1117/12.2280974
   Kvyetnyy R. N., 2016, P SOC PHOTO-OPT INS, V10031
   Kvyetnyy R, 2016, PROC SPIE, V10031, DOI 10.1117/12.2249164
   Kvyetnyy R, 2015, PROC SPIE, V9816, DOI 10.1117/12.2229103
   Maass W., 2001, PULSED NEURAL NETWOR, P377
   Mirkes E. M., 1998, NEYROCOMPUTER
   Natschlager T., 2002, SPECIAL ISSUE FDN IN, P39, DOI [DOI 10.1017/CBO9781107415324.004, 10.1017/CBO9781107415324.004]
   Tenderenda T, 2012, PROC SPIE, V8426, DOI 10.1117/12.922556
   von Neumann J, 1951, CEREBRAL MECH BEHAV, P1, DOI DOI 10.1126/SCIENCE.115.2990.440
   Vyatkin SI, 2017, PROC SPIE, V10445, DOI 10.1117/12.2280983
   Wojcik W., 2017, INFORM TECHNOLOGY ME
   Wójcik W, 2008, PRZ ELEKTROTECHNICZN, V84, P277
   Wójcik W, 2012, PRZ ELEKTROTECHNICZN, V88, P316
   Wójcik W, 2005, PROC SPIE, V5958, DOI 10.1117/12.622921
   Wójcik W, 2009, METROL MEAS SYST, V16, P649
   Zabolotna NI, 2014, PROC SPIE, V9166, DOI 10.1117/12.2061105
   Zabolotna NI, 2013, PROC SPIE, V8698, DOI 10.1117/12.2019715
   Zabolotna NI, 2015, PROC SPIE, V9816, DOI 10.1117/12.2229018
NR 29
TC 0
Z9 0
U1 1
U2 5
PY 2019
VL 11176
AR 1117609
DI 10.1117/12.2536607
UT WOS:000511104400008
DA 2023-11-16
ER

PT J
AU Jha, A
   Huang, CR
   Peng, HT
   Shastri, B
   Prucnal, PR
AF Jha, Aashu
   Huang, Chaoran
   Peng, Hsuan-Tung
   Shastri, Bhavin
   Prucnal, Paul R.
TI Photonic Spiking Neural Networks and Graphene-on-Silicon Spiking Neurons
SO JOURNAL OF LIGHTWAVE TECHNOLOGY
DT Article
DE Photonics; Neurons; Optical resonators; Hardware; Laser modes; Optical
   pumping; Biological neural networks; Neural networks; nonlinear
   photonics; photonic integrated circuits
ID TIMING-DEPENDENT PLASTICITY; COMPUTATIONAL POWER; SEMICONDUCTOR-LASER;
   OPTICAL-INJECTION; SELF-PULSATIONS; EXCITABILITY; BISTABILITY;
   GENERATION; DYNAMICS; DESIGN
AB Spiking neural networks are known to be superior over artificial neural networks for their computational power efficiency and noise robustness. The benefits of spiking coupled with the high-bandwidth and low-latency of photonics can enable highly-efficient, noise-robust, high-speed neural processors. The landscape of photonic spiking neurons consists of an overwhelming majority of excitable lasers and a few demonstrations on nonlinear optical cavities. The silicon platform is best poised to host a scalable photonic technology given its CMOS-compatibility and low optical loss. Here, we present a survey of existing photonic spiking neurons, and propose a novel spiking neuron based on a hybrid graphene-on-silicon microring cavity. A comparison among a representative sample of photonic spiking devices is also presented. Finally, we discuss methods employed in training spiking neural networks, their challenges as well as the application domain that can be enabled by photonic spiking neural hardware.
C1 [Jha, Aashu; Huang, Chaoran; Peng, Hsuan-Tung; Prucnal, Paul R.] Princeton Univ, Dept Elect & Comp Engn, Princeton, NJ 08544 USA.
   [Huang, Chaoran] Chinese Univ Hong Kong, Shatin, Hong Kong 999077, Peoples R China.
   [Shastri, Bhavin] Queens Univ, Dept Phys Engn Phys & Astron, Kingston, ON K7L 3N6, Canada.
   [Shastri, Bhavin] Vector Inst, Toronto, ON, Canada.
RP Jha, A (corresponding author), Princeton Univ, Dept Elect & Comp Engn, Princeton, NJ 08544 USA.
EM aashuj@princeton.edu; chaoranh@princeton.edu; hpeng@princeton.edu;
   bhavin.shastri@queensu.ca; prucnal@princeton.edu
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   Alexander K, 2013, OPT EXPRESS, V21, P26182, DOI 10.1364/OE.21.026182
   Amir A, 2017, PROC CVPR IEEE, P7388, DOI 10.1109/CVPR.2017.781
   Amodei D, 2021, AI AND COMPUTE
   [Anonymous], 2001, PHYS REV LETT
   Pérez-Carrasco JA, 2013, IEEE T PATTERN ANAL, V35, P2706, DOI 10.1109/TPAMI.2013.71
   Anwani N., 2015, P INT JOINT C NEUR N, P1, DOI DOI 10.1109/IJCNN.2015
   Ataloglou VG, 2018, PHYS REV A, V97, DOI 10.1103/PhysRevA.97.063836
   Barbay S, 2011, OPT LETT, V36, P4476, DOI 10.1364/OL.36.004476
   Barland S, 2003, PHYS REV E, V68, DOI 10.1103/PhysRevE.68.036209
   Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Bohte S. M., 2000, 8th European Symposium on Artificial Neural Networks. ESANN"2000. Proceedings, P419
   Bristow AD, 2007, APPL PHYS LETT, V90, DOI 10.1063/1.2737359
   Brunstein M, 2012, PHYS REV A, V85, DOI 10.1103/PhysRevA.85.031803
   Chakraborty I, 2019, PHYS REV APPL, V11, DOI 10.1103/PhysRevApplied.11.014063
   Chakraborty I, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-31365-x
   Chatzidimitriou D, 2015, J APPL PHYS, V118, DOI 10.1063/1.4926501
   Chen SW, 2012, OPT EXPRESS, V20, P7454, DOI 10.1364/OE.20.007454
   Coomans W, 2010, PHYS REV A, V81, DOI 10.1103/PhysRevA.81.033802
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Diehl P.U., 2015, 2015 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2015.7280696
   Dremetsika E, 2016, OPT LETT, V41, P3281, DOI 10.1364/OL.41.003281
   Dubbeldam JLA, 1999, OPT COMMUN, V159, P325, DOI 10.1016/S0030-4018(98)00568-9
   Dubbeldam JLA, 1999, PHYS REV E, V60, P6580, DOI 10.1103/PhysRevE.60.6580
   Falez P, 2019, PATTERN RECOGN, V93, P418, DOI 10.1016/j.patcog.2019.04.016
   Feldmann J, 2019, NATURE, V569, P208, DOI 10.1038/s41586-019-1157-8
   Feng Q, 2019, APPL PHYS LETT, V114, DOI 10.1063/1.5064832
   Först M, 2007, OPT LETT, V32, P2046, DOI 10.1364/OL.32.002046
   Fok MP, 2013, OPT LETT, V38, P419, DOI 10.1364/OL.38.000419
   Gelens L, 2010, PHYS REV A, V82, DOI 10.1103/PhysRevA.82.063841
   Goulding D, 2007, PHYS REV LETT, V98, DOI 10.1103/PhysRevLett.98.153903
   Gu T, 2012, NAT PHOTONICS, V6, P554, DOI 10.1038/nphoton.2012.147
   Gutig R., NATURE NEUROSCI, V9, P420
   Hazan H, 2018, FRONT NEUROINFORM, V12, DOI 10.3389/fninf.2018.00089
   Hejda M., APL PHOTONICS, V6, P2021
   Hsuan-Tung Peng, 2018, IEEE Journal of Selected Topics in Quantum Electronics, V24, DOI 10.1109/JSTQE.2018.2840448
   Huang CR, 2021, IEEE J SEL TOP QUANT, V27, DOI 10.1109/JSTQE.2020.2998073
   Huang C, 2019, IEEE PHOTONIC TECH L, V31, P1834, DOI 10.1109/LPT.2019.2948903
   Hurtado A, 2012, APPL PHYS LETT, V100, DOI 10.1063/1.3692726
   Hurtado A, 2010, OPT EXPRESS, V18, P9423, DOI 10.1364/OE.18.009423
   Ishizawa A, 2017, SCI REP-UK, V7, DOI 10.1038/srep45520
   Izhikevich EM, 2001, NEURAL NETWORKS, V14, P883, DOI 10.1016/S0893-6080(01)00078-8
   Izhikevich EM, 2003, TRENDS NEUROSCI, V26, P161, DOI 10.1016/S0166-2236(03)00034-1
   Jha A, 2020, OPT LETT, V45, P2287, DOI 10.1364/OL.387497
   Kelleher B, 2011, PHYS REV E, V83, DOI 10.1103/PhysRevE.83.026207
   Kik PG, 1998, MRS BULL, V23, P48, DOI 10.1557/S0883769400030268
   Krauskopf B, 2003, OPT COMMUN, V215, P367, DOI 10.1016/S0030-4018(02)02239-3
   Larotonda MA, 2002, PHYS REV A, V65, DOI 10.1103/PhysRevA.65.033812
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508
   Lent R, 2018, IEEE T COGN COMMUN, V4, P860, DOI 10.1109/TCCN.2018.2865387
   Lin Q, 2007, OPT EXPRESS, V15, P16604, DOI 10.1364/OE.15.016604
   Liu WL, 2016, J LIGHTWAVE TECHNOL, V34, P3466, DOI 10.1109/JLT.2016.2567456
   Maass W, 1996, NEURAL COMPUT, V8, P1, DOI 10.1162/neco.1996.8.1.1
   Maass W, 2004, J COMPUT SYST SCI, V69, P593, DOI 10.1016/j jcss.2004.04.001
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Marini A, 2017, PHYS REV B, V95, DOI 10.1103/PhysRevB.95.125408
   Mesaritakis C, 2020, 2020 OPTICAL FIBER COMMUNICATIONS CONFERENCE AND EXPOSITION (OFC)
   MOHRLE M, 1992, IEEE PHOTONIC TECH L, V4, P976, DOI 10.1109/68.157120
   Nahmias MA, 2013, IEEE J SEL TOP QUANT, V19, DOI 10.1109/JSTQE.2013.2257700
   Obeid I, 2004, IEEE T BIO-MED ENG, V51, P905, DOI 10.1109/TBME.2004.826683
   PAN ZG, 1993, APPL PHYS LETT, V63, P2999, DOI 10.1063/1.110264
   Paszke A., 2019, ADV NEURAL INFORM PR
   Peng HT, 2020, IEEE J SEL TOP QUANT, V26, DOI 10.1109/JSTQE.2019.2927582
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Prucnal PR, 2016, ADV OPT PHOTONICS, V8, P228, DOI 10.1364/AOP.8.000228
   Prucnal PR, 2017, NEUROMORPHIC PHOTONICS, P1
   Ren QS, 2015, OPT EXPRESS, V23, P25247, DOI 10.1364/OE.23.025247
   Romagnoli M, 2018, NAT REV MATER, V3, P392, DOI 10.1038/s41578-018-0040-9
   Romeira B, 2013, OPT EXPRESS, V21, P20931, DOI 10.1364/OE.21.020931
   Roy K, 2019, NATURE, V575, P607, DOI 10.1038/s41586-019-1677-2
   Selmi F, 2014, PHYS REV LETT, V112, DOI 10.1103/PhysRevLett.112.183902
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Shastri B. J., NATURE PHOTON, V15, P2021
   Shastri BJ, 2016, SCI REP-UK, V6, DOI 10.1038/srep19126
   Shastri BJ, 2014, OPT QUANT ELECTRON, V46, P1353, DOI 10.1007/s11082-014-9884-4
   Song ZW, 2020, IEEE J SEL TOP QUANT, V26, DOI 10.1109/JSTQE.2020.2975564
   SOREF RA, 1987, IEEE J QUANTUM ELECT, V23, P123, DOI 10.1109/JQE.1987.1073206
   Spühler GJ, 1999, J OPT SOC AM B, V16, P376, DOI 10.1364/JOSAB.16.000376
   Thacker HD, 2010, ELEC COMP C, P240, DOI 10.1109/ECTC.2010.5490965
   Toole R, 2015, OPT EXPRESS, V23, P16133, DOI 10.1364/OE.23.016133
   Uesugi T, 2006, OPT EXPRESS, V14, P377, DOI 10.1364/OPEX.14.000377
   Van Gasse K, 2019, OPT EXPRESS, V27, P293, DOI 10.1364/OE.27.000293
   Van Vaerenbergh T, 2012, OPT EXPRESS, V20, P20292, DOI 10.1364/OE.20.020292
   Vermeulen N, 2016, PHYS REV APPL, V6, DOI 10.1103/PhysRevApplied.6.044006
   Wieczorek S, 2002, PHYS REV LETT, V88, DOI 10.1103/PhysRevLett.88.063901
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Xiang SY, 2021, IEEE T NEUR NET LEAR, V32, P2494, DOI 10.1109/TNNLS.2020.3006263
   Xiang SY, 2019, IEEE J SEL TOP QUANT, V25, DOI 10.1109/JSTQE.2019.2911565
   Xiang SY, 2018, IEEE J QUANTUM ELECT, V54, DOI 10.1109/JQE.2018.2879484
   Yacomotti AM, 2006, PHYS REV LETT, V97, DOI 10.1103/PhysRevLett.97.143904
NR 93
TC 12
Z9 13
U1 12
U2 45
PD MAY 1
PY 2022
VL 40
IS 9
BP 2901
EP 2914
DI 10.1109/JLT.2022.3146157
UT WOS:000794237000026
DA 2023-11-16
ER

PT C
AU Xiao, F
   Li, JP
   Tian, J
   Wang, GS
AF Xiao Fei
   Li Jianping
   Tian Jie
   Wang Guangshuo
GP IEEE
TI RESEARCH ON IMAGE CLASSIFICATION COMBINING WAVELET ANALYSIS AND SPIKING
   NEURAL NETWORK
SO 2022 19TH INTERNATIONAL COMPUTER CONFERENCE ON WAVELET ACTIVE MEDIA
   TECHNOLOGY AND INFORMATION PROCESSING (ICCWAMTIP)
SE International Computer Conference on Wavelet Active Media Technology and
   Information Processing
DT Proceedings Paper
CT 19th IEEE International Computer Conference on Wavelet Active Media
   Technology and Information Processing (ICCWAMTIP)
CY DEC 16-18, 2022
CL Univ Elect Sci & Technol China, Chengdu, PEOPLES R CHINA
HO Univ Elect Sci & Technol China
DE Wavelet Analysis; Spiking Neural Networks; Image Classification
AB Wavelet analysis is a variant of Fourier analysis, which can be used for time-frequency analysis of signal processing, and has achieved remarkable results in the field of image processing. The spike neural network is called the third-generation neural network, which is different from the previous generation, the neural network of the spike neural network is more inspired by neuroscience, and this neural network is constructed in a way closer to the human brain mechanism, which can be applied to many machine learning tasks. Image classification is one of the basic tasks in the field of computer vision. We explore the application of wavelet analysis to the training process of the spiking neural network, before the original data is input into the neural network, we process it with wavelet transform, so that the characteristics of the input data are easier to be learned by the neural network.
C1 [Xiao Fei; Li Jianping; Tian Jie; Wang Guangshuo] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
RP Xiao, F (corresponding author), Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
EM 202021080416@std.uestc.edu.cn; jpl2222@uestc.edu.cn;
   202122080902@std.uestc.edu.cn; 202122080904@std.uestc.edu.cn
CR Abdulelah Abdulrahman Asma, 2021, J PHYS C SERIES, V1879
   Bing Han, 2020, EUROPEAN C COMPUTER
   Eshraghian JK, 2023, Arxiv, DOI [arXiv:2109.12894, DOI 10.48550/ARXIV.2109.12894]
   Kasabov Nikola K., 2019, TIME SPACE SPIKING N
   Lee G., 2019, J OPEN SOURCE SOFTW, V4, P1237, DOI [DOI 10.21105/JOSS.01237, 10.21105/joss.01237]
NR 5
TC 0
Z9 0
U1 2
U2 4
PY 2022
DI 10.1109/ICCWAMTIP56608.2022.10016484
UT WOS:000932922500006
DA 2023-11-16
ER

PT J
AU Bodyanskyy, Y
   Dolotov, AI
   Malysheva, DM
AF Bodyanskyy, Ye
   Dolotov, A., I
   Malysheva, D. M.
TI SELFLEARNING FUZZY SPIKING NEURAL NETWORK BASED ON DISCRETE SECOND-
   ORDER CRITICALLY DUMPED RESPONSE UNITS FOR FUZZY CLUSTERING TASKS
SO RADIO ELECTRONICS COMPUTER SCIENCE CONTROL
DT Article
DE fuzzy clustering; spike; fuzzy spiking neural network; classical
   automatic control theory; second-order damped response units;
   pulse-position threshold detection system
AB Hybrid neural networks based on the idea of combining spiking neural networks and the principles of fuzzy logic are considered. The architecture of self-learning fuzzy spiking neural network based on discrete second-order critically damped response units is proposed.
   It is proposed to define a spiking neural network in terms of apparatus of classical automatic control theory based on the Laplace transform and z-transform. It is shown that a spiking neural network is a pulse-position threshold detection system based on second-order damped response units. Such kind of description allows using it as an analog-digital system in technical problems solving.
   The output layer takes firing times of spikes arriving from the second layer, and either performs fuzzy partitioning of the input patterns using probabilistic approach
C1 [Bodyanskyy, Ye; Dolotov, A., I; Malysheva, D. M.] Kharkov Natl Univ Radio Elect, Kharkov, Ukraine.
RP Bodyanskyy, Y (corresponding author), Kharkov Natl Univ Radio Elect, Kharkov, Ukraine.
CR Bezdek J.C., 2005, FUZZY MODELS ALGORIT
   Bodyanskij E. V., 2008, MAT MIZHNARODNOI NAU, P12
   Bodyanskiy Y, 2008, PRO BIENN BALT EL C, P213, DOI 10.1109/BEC.2008.4657517
   Bodyanskiy Ye, 2008, WISSENSCHAFTLICHE BE, P53
   Bodyanskiy Ye, 2009, SCI J RIGA TU, P66
   Bodyanskiy Ye, 2008, SCI P RIG TU INF TEC, P27
   Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   Dorf P. C., 2008, MODERN CONTROL SYSTE
   Gerstner W., 2002, SPIKING NEURON MODEL
   Goodwin G. C., 2001, CONTROL SYSTEMS DESI
   Haykin S., 1999, NEURAL NETWORKS COMP
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Natschlaeger T., 1998, SPATIAL TEMPORAL PAT, P319
   Phillips C., 2000, FEEDBACK CONTROL SYS
   Sato-llic M., 2006, INNOVATIONS FUZZY CL
   Sun, 1997, NEUROFUZZY SOFT COMP, V42, P1482
NR 17
TC 0
Z9 0
U1 0
U2 0
PY 2012
VL 2
BP 134
EP 140
UT WOS:000216955900023
DA 2023-11-16
ER

PT C
AU Wu, DD
   Lin, XH
   Du, PG
AF Wu, Doudou
   Lin, Xianghong
   Du, Pangao
GP IEEE
TI An Adaptive Structure Learning Algorithm for Multi-Layer Spiking Neural
   Networks
SO 2019 15TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND
   SECURITY (CIS 2019)
DT Proceedings Paper
CT 15th International Conference on Computational Intelligence and Security
   (CIS)
CY DEC 13-16, 2019
CL Macao, PEOPLES R CHINA
DE adaptive structure; spiking neural networks; pruning neurons; inner
   product of spike trains
AB Due to the complex multi-layer structure and implicit nonlinear mechanism, the formulation of efficient learning methods for spiking neural networks with dynamically adaptive structure is difficult. This paper presents an adaptive structure learning algorithm for multi-layer spiking neural networks, in which the synaptic weights are modified according to the supervised learning algorithm based on inner product of spike sequences. The main contribution of this work lies in the fact that the proposed algorithm is able to dynamically prune neurons of the hidden layer. The proposed algorithm is successfully applied to learn spikes sequences. The experimental results verify the effectiveness of the adaptive structure learning algorithm in multi-layer spiking neural networks. Moreover, the adaptive structure networks achieve better performance in the spike train leaning task than the fixed structure networks.
C1 [Wu, Doudou; Lin, Xianghong; Du, Pangao] Northwest Normal Univ, Sch Comp Sci & Engineer, Lanzhou, Peoples R China.
RP Lin, XH (corresponding author), Northwest Normal Univ, Sch Comp Sci & Engineer, Lanzhou, Peoples R China.
EM wdd_suda@163.com; linxh@nwnu.edu.cn; 18809424482@163.com
CR Bohte S. M., 2000, 8th European Symposium on Artificial Neural Networks. ESANN"2000. Proceedings, P419
   Caporale N, 2008, ANNU REV NEUROSCI, V31, P25, DOI 10.1146/annurev.neuro.31.060407.125639
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Legenstein R, 2005, NEURAL COMPUT, V17, P2337, DOI 10.1162/0899766054796888
   Lin X., 2018, SCI PRESS
   Lin XH, 2017, NEUROCOMPUTING, V237, P59, DOI 10.1016/j.neucom.2016.08.087
   McKennoch S., CANADA, P3970
   Moore S. C., 2002, COMPUT SCI
   Oniz Y, 2014, J FRANKLIN I, V351, P3269, DOI 10.1016/j.jfranklin.2014.03.002
   Schrauwen B., PROC, P301
   Siegelbaum S. A., 2000, HILL
   Silva S. M., 2007, CANADA, P3978
   Wang J., 2015, SPRINGER INT
   Wang JL, 2014, NEUROCOMPUTING, V144, P526, DOI 10.1016/j.neucom.2014.04.017
   Wu Q. X., 2006, NEUROCOMPUTING, V69, P1912
NR 16
TC 2
Z9 2
U1 0
U2 6
PY 2019
BP 98
EP 102
DI 10.1109/CIS.2019.00029
UT WOS:000557886100021
DA 2023-11-16
ER

PT J
AU Nobukawa, S
   Nishimura, H
   Yamanishi, T
AF Nobukawa, Sou
   Nishimura, Haruhiko
   Yamanishi, Teruya
TI PATTERN CLASSIFICATION BY SPIKING NEURAL NETWORKS COMBINING
   SELF-ORGANIZED AND REWARD-RELATED SPIKE-TIMING-DEPENDENT PLASTICITY
SO JOURNAL OF ARTIFICIAL INTELLIGENCE AND SOFT COMPUTING RESEARCH
DT Article
DE spiking neural network; spike timing-dependent plasticity;
   dopamine-modulated spike timing-dependent plasticity; pattern
   classification
ID RECOGNITION; STDP
AB Many recent studies have applied to spike neural networks with spike-timing-dependent plasticity (STDP) to machine learning problems. The learning abilities of dopamine-modulated STDP (DA-STDP) for reward-related synaptic plasticity have also been gathering attention. Following these studies, we hypothesize that a network structure combining self-organized STDP and reward-related DA-STDP can solve the machine learning problem of pattern classification. Therefore, we studied the ability of a network in which recurrent spiking neural networks are combined with STDP for non-supervised learning, with an output layer joined by DA-STDP for supervised learning, to perform pattern classification. We confirmed that this network could perform pattern classification using the STDP effect for emphasizing features of the input spike pattern and DA-STDP supervised learning. Therefore, our proposed spiking neural network may prove to be a useful approach for machine learning problems.
C1 [Nobukawa, Sou] Chiba Inst Technol, Dept Comp Sci, 2-17-1 Tsudanuma, Narashino, Chiba 2750016, Japan.
   [Nishimura, Haruhiko] Univ Hyogo, Grad Sch Appl Informat, 7-1-28 Chuo Ku, Kobe, Hyogo 6508588, Japan.
   [Yamanishi, Teruya] Fukui Univ Technol, Dept Management & Informat Sci, 3-6-1 Gakuen, Fukui, Fukui 9108505, Japan.
RP Nobukawa, S (corresponding author), Chiba Inst Technol, Dept Comp Sci, 2-17-1 Tsudanuma, Narashino, Chiba 2750016, Japan.
EM nobukawa@cs.it-chiba.ac.jp
CR Chou TS, 2015, FRONT NEUROROBOTICS, V9, DOI 10.3389/fnbot.2015.00006
   DiBernardo M, 2008, APPL MATH SCI, V163, P1, DOI 10.1007/978-1-84628-708-4
   Florian RV, 2007, NEURAL COMPUT, V19, P1468, DOI 10.1162/neco.2007.19.6.1468
   Frémaux N, 2010, J NEUROSCI, V30, P13326, DOI 10.1523/JNEUROSCI.6249-09.2010
   Ge CJ, 2017, INFORM SCIENCES, V399, P30, DOI 10.1016/j.ins.2017.03.006
   Ghani A, 2008, LECT NOTES COMPUT SC, V5163, P513, DOI 10.1007/978-3-540-87536-9_53
   Hiratani N, 2013, FRONT COMPUT NEUROSC, V6, DOI 10.3389/fncom.2012.00102
   Izhikevich EM, 2007, CEREB CORTEX, V17, P2443, DOI 10.1093/cercor/bhl152
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Jin YYZ, 2017, IEEE IJCNN, P2007, DOI 10.1109/IJCNN.2017.7966097
   Kasabov Nikola, 2012, Artificial Neural Networks in Pattern Recognition. Proceedings of the 5th INNS IAPR TC 3 GIRPR Workshop, ANNPR 2012, P225, DOI 10.1007/978-3-642-33212-8_21
   Kasabov N, 2015, INFORM SCIENCES, V294, P565, DOI 10.1016/j.ins.2014.06.028
   Kasabov NK, 2014, NEURAL NETWORKS, V52, P62, DOI 10.1016/j.neunet.2014.01.006
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Kulkarni SR, 2018, NEURAL NETWORKS, V103, P118, DOI 10.1016/j.neunet.2018.03.019
   Lee JM, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00191
   Lin XH, 2017, NEUROCOMPUTING, V237, P59, DOI 10.1016/j.neucom.2016.08.087
   Lin ZT, 2018, NEUROCOMPUTING, V275, P94, DOI 10.1016/j.neucom.2017.05.009
   Marblestone AH, 2016, FRONT COMPUT NEUROSC, V10, DOI 10.3389/fncom.2016.00094
   Mejias JF, 2012, PHYS REV LETT, V108, DOI 10.1103/PhysRevLett.108.228102
   Mozafari M., 2018, IEEE T NEURAL NETWOR, V29, P1
   Nobukawa S, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-01511-y
   Nobukawa S, 2016, NEURAL COMPUT, V28, P2505, DOI 10.1162/NECO_a_00894
   Panzeri S, 2015, TRENDS COGN SCI, V19, P162, DOI 10.1016/j.tics.2015.01.002
   Quiroga RQ, 2013, PRINCIPLES OF NEURAL CODING, P1, DOI 10.1201/b14756
   Rabinovich MI, 2006, REV MOD PHYS, V78, P1213, DOI 10.1103/RevModPhys.78.1213
   Schweighofer N, 2004, P NATL ACAD SCI USA, V101, P4655, DOI 10.1073/pnas.0305966101
   Tavanaei A, 2018, 2018 INT JOINT C NEU, DOI DOI 10.1109/IJCNN.2018.8489104
   Tavanaei A, 2018, NEURAL NETWORKS, V105, P294, DOI 10.1016/j.neunet.2018.05.018
   Verstraeten D, 2005, INFORM PROCESS LETT, V95, P521, DOI 10.1016/j.ipl.2005.05.019
   Warlaumont AS, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0145096
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Yamaguchi K, 2018, PROCEEDINGS 2018 12TH FRANCE-JAPAN AND 10TH EUROPE-ASIA CONGRESS ON MECHATRONICS, P195, DOI 10.1109/MECATRONICS.2018.8495720
   Yanduo Zhang, 2009, J COMPUTERS, V4, P1183
   Zhang Y, 2015, IEEE T NEUR NET LEAR, V26, P2635, DOI 10.1109/TNNLS.2015.2388544
NR 35
TC 9
Z9 9
U1 5
U2 32
PD OCT
PY 2019
VL 9
IS 4
BP 283
EP 291
DI 10.2478/jaiscr-2019-0009
UT WOS:000486918800004
DA 2023-11-16
ER

PT S
AU Espinal, A
   Carpio, M
   Ornelas, M
   Puga, H
   Melin, P
   Sotelo-Figueroa, M
AF Espinal, Andres
   Carpio, Martin
   Ornelas, Manuel
   Puga, Hector
   Melin, Patricia
   Sotelo-Figueroa, Marco
BE Castillo, O
   Melin, P
   Pedrycz, W
   Kacprzyk, J
TI Comparing Metaheuristic Algorithms on the Training Process of Spiking
   Neural Networks
SO RECENT ADVANCES ON HYBRID APPROACHES FOR DESIGNING INTELLIGENT SYSTEMS
SE Studies in Computational Intelligence
DT Article; Book Chapter
ID NEURONS
AB Spiking Neural Networks are considered as the third generation of Artificial Neural Networks. In these networks, spiking neurons receive/send the information by timing of events (spikes) instead by the spike rate; as their predecessors do. Spikeprop algorithm, based on gradient descent, was developed as learning rule for training SNNs to solve pattern recognition problems; however this algorithm trends to be trapped in local minima and has several limitations. For dealing with the supervised learning on Spiking Neural Networks without the drawbacks of Spikeprop, several metaheuristics such as: Evolutionary Strategy, Particle Swarm Optimization, have been used to tune the neural parameters. This work compares the performance and the impact of some metaheuristics used for training spiking neural networks.
C1 [Espinal, Andres; Carpio, Martin; Ornelas, Manuel; Puga, Hector; Sotelo-Figueroa, Marco] Inst Tecnol Leon, Leon, Mexico.
   [Melin, Patricia] Inst Tecnol Tijuana, Tijuana, Baja California, Mexico.
RP Puga, H (corresponding author), Inst Tecnol Leon, Leon, Mexico.
EM andres.espinal@itleon.edu.mx; jmcarpio61@hotmail.com;
   mornelas67@yahoo.com.mx; pugahector@yaho.com; pmelin@tectijuana.edu.mx;
   marco.sotelo@itleon.edu.mx
CR [Anonymous], NEURAL NETWORK MODEL
   [Anonymous], P 14 INT C COMP SUPP
   [Anonymous], 2010, BIOL INSPIRED NEURAL
   Belatreche A, 2003, PROCEEDINGS OF THE 7TH JOINT CONFERENCE ON INFORMATION SCIENCES, P1524
   Belatreche A, 2007, SOFT COMPUT, V11, P239, DOI [10.1007/s00500-006-0065-7, 10.1007/S00500-006-0065-7]
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   Duda RO., 2012, PATTERN CLASSIFICATI, VSecond edition ed
   Elizondo D, 1997, Int J Neural Syst, V8, P535, DOI 10.1142/S0129065797000513
   GERSTNER W, 1995, PHYS REV E, V51, P738, DOI 10.1103/PhysRevE.51.738
   Gerstner W., 2002, SPIKING NEURON MODEL
   Haykin S., 1999, NEURAL NETWORKS COMP
   Johnson C, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P1802, DOI 10.1109/IJCNN.2011.6033443
   Kennedy J, 1995, 1995 IEEE INTERNATIONAL CONFERENCE ON NEURAL NETWORKS PROCEEDINGS, VOLS 1-6, P1942, DOI 10.1109/icnn.1995.488968
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maass W., 1996, NEURAL INFORM PROCES, P211
   Rechenberg I., 1973, EVOLUTIONS STRATEGIE
   Zurada J., 1992, INTRO ARTIFICIAL NEU
NR 18
TC 3
Z9 3
U1 0
U2 1
PY 2014
VL 547
BP 391
EP 403
DI 10.1007/978-3-319-05170-3_27
D2 10.1007/978-3-319-05170-3
UT WOS:000341823700028
DA 2023-11-16
ER

PT J
AU Wang, X
   Zhong, MH
   Cheng, OYE
   Xie, JJ
   Zhou, YC
   Ren, J
   Liu, MY
AF Wang, Xuan
   Zhong, Minghong
   Cheng, Hoiyuen
   Xie, Junjie
   Zhou, Yingchu
   Ren, Jun
   Liu, Mengyuan
TI SpikeGoogle: Spiking Neural Networks with GoogLeNet-like inception
   module
SO CAAI TRANSACTIONS ON INTELLIGENCE TECHNOLOGY
DT Article
DE GoogLeNet; inception; Spiking Neural Networks
AB Spiking Neural Network is known as the third-generation artificial neural network whose development has great potential. With the help of Spike Layer Error Reassignment in Time for error back-propagation, this work presents a new network called SpikeGoogle, which is implemented with GoogLeNet-like inception module. In this inception module, different convolution kernels and max-pooling layer are included to capture deep features across diverse scales. Experiment results on small NMNIST dataset verify the results of the authors' proposed SpikeGoogle, which outperforms the previous Spiking Convolutional Neural Network method by a large margin.
C1 [Wang, Xuan; Zhong, Minghong; Cheng, Hoiyuen; Xie, Junjie; Liu, Mengyuan] Sun Yat Sen Univ, Sch Intelligent Syst Engn, Shenzhen, Peoples R China.
   [Wang, Xuan; Zhong, Minghong; Cheng, Hoiyuen; Xie, Junjie; Liu, Mengyuan] Guangdong Prov Key Lab Fire Sci & Intelligent Eme, Guangzhou, Peoples R China.
   [Zhou, Yingchu] Shenzhen Acad Metrol & Qual Inspect, Shenzhen, Peoples R China.
   [Ren, Jun] Infocare Syst Ltd, Auckland, New Zealand.
RP Liu, MY (corresponding author), Sun Yat Sen Univ, Sch Intelligent Syst Engn, Shenzhen, Peoples R China.
EM nkliuyifang@gmail.com
CR Pérez-Carrasco JA, 2013, IEEE T PATTERN ANAL, V35, P2706, DOI 10.1109/TPAMI.2013.71
   Belatreche A., 2012, 2012 INT JOINT C NEU, P1
   Brown RE, 2021, FRONT BEHAV NEUROSCI, V15, DOI 10.3389/fnbeh.2021.732195
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Cordone L., 2021, ARXIV210412579V1
   Deng L, 2020, NEURAL NETWORKS, V121, P294, DOI 10.1016/j.neunet.2019.09.005
   Diehl PU, 2015, IEEE IJCNN
   Fang W, 2021, INCORPORATING LEARNA, P2661
   Ganguly C., 2020, IEEE 16 IND COUNC IN
   GERSTNER W, 1995, PHYS REV E, V51, P738, DOI 10.1103/PhysRevE.51.738
   Gerstner W, 2018, SCHOLARPEDIA, V3, P1343
   Haessig G., 2017, ARXIV171009820V1
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/TPAMI.2019.2913372, 10.1109/CVPR.2018.00745]
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang YP, 2016, NEURAL COMPUT, V28, P1503, DOI 10.1162/NECO_a_00851
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Javed A, 2021, J PARALLEL DISTR COM, V154, P82, DOI 10.1016/j.jpdc.2021.03.013
   Kim S., 2019, ARXIV190306530V2
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
   Lam MWY, 2019, INT CONF ACOUST SPEE, P7235, DOI 10.1109/ICASSP.2019.8683660
   Lee C, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00119
   Lee JM, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00191
   Min L., 2013, ARXIV PREPRINT ARXIV
   Neil D, 2016, IEEE INT SYMP CIRC S, P2282, DOI 10.1109/ISCAS.2016.7539039
   Orchard G, 2015, FRONT NEUROSCI-SWITZ, V9, DOI [10.3389/fhins.2015.00437, 10.3389/fnins.2015.00437]
   Paugam-Moisy H., 2012, COMPUTING SPIKING NE, ppp 335
   Radford A., 2016, UNSUPERVISED REPRESE, DOI DOI 10.1007/978-3-319-71589-6_9
   Roy K, 2019, NATURE, V575, P607, DOI 10.1038/s41586-019-1677-2
   Shrestha SB, 2018, ADV NEUR IN, V31
   Srinivasa N, 2012, IEEE T NEUR NET LEAR, V23, P1526, DOI 10.1109/TNNLS.2012.2207738
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Taherkhani A, 2020, NEURAL NETWORKS, V122, P253, DOI 10.1016/j.neunet.2019.09.036
   Tie-Lin Z., 2020, CHIN J COMPUT, P0
   Vidal AR, 2018, IEEE ROBOT AUTOM LET, V3, P994, DOI 10.1109/LRA.2018.2793357
   Wang YQ, 2020, ACM COMPUT SURV, V53, DOI 10.1145/3386252
   Wysoski SG, 2010, NEURAL NETWORKS, V23, P819, DOI 10.1016/j.neunet.2010.04.009
   Xianghong L., 2011, COMPUT ENG APPL J, P41
   Xiao M., 2021, ADV NEURAL INFORM PR
   Yangfan H., 2020, ARXIV180501352V2
   Zenke F., 2017, NEURAL COMPUT, DOI DOI 10.1162/NECO_A_01086
NR 41
TC 5
Z9 5
U1 8
U2 36
PD SEP
PY 2022
VL 7
IS 3
BP 492
EP 502
DI 10.1049/cit2.12082
EA APR 2022
UT WOS:000777962900001
DA 2023-11-16
ER

PT C
AU Sboev, A
   Litvinova, T
   Vlasov, D
   Serenko, A
   Moloslnikov, I
AF Sboev, Alexander
   Litvinova, Tatiana
   Vlasov, Danila
   Serenko, Alexey
   Moloslnikov, Ivan
BE Boukhanovsky, A
   Bubak, M
   Balakhontceva, M
TI On the applicability of spiking neural network models to solve the task
   of recognizing gender hidden in texts
SO 5TH INTERNATIONAL YOUNG SCIENTIST CONFERENCE ON COMPUTATIONAL SCIENCE,
   YSC 2016
SE Procedia Computer Science
DT Proceedings Paper
CT 5th International Young Scientist Conference on Computational Science
   (YSC)
CY OCT 26-28, 2016
CL Krakow, POLAND
DE supervised learning; spike-timing-dependent plasticity; artificial
   neural networks; spiking neural networks
ID PLASTICITY
AB Two approaches to utilize spiking neural networks, applicable for implementing in neuromorphic hardware with ultra-low power consumption, in the task of recognizing gender of a text author are analyzed. The first one is to obtain synaptic weights for the spiking network by training a formal network. We show the results obtained with this approach. The second one is a creation of a supervised learning algorithm for spiking networks that would be based on biologically plausible plasticity rules. We discuss possible ways to construct such algorithms.
C1 [Sboev, Alexander; Vlasov, Danila] MEPhl Natl Res Nucl Univ, Moscow, Russia.
   [Sboev, Alexander; Litvinova, Tatiana; Serenko, Alexey; Moloslnikov, Ivan] Natl Res Ctr, Kurchatov Inst, Moscow, Russia.
   [Sboev, Alexander] Plekhaflov Russian Univ Econ, Moscow, Russia.
   [Sboev, Alexander; Vlasov, Danila; Moloslnikov, Ivan] JSC Concern Systemprom, Moscow, Russia.
   [Sboev, Alexander] Moscow Technol Univ, MIREA, Moscow, Russia.
RP Sboev, A (corresponding author), MEPhl Natl Res Nucl Univ, Moscow, Russia.; Sboev, A (corresponding author), Natl Res Ctr, Kurchatov Inst, Moscow, Russia.; Sboev, A (corresponding author), Plekhaflov Russian Univ Econ, Moscow, Russia.; Sboev, A (corresponding author), JSC Concern Systemprom, Moscow, Russia.; Sboev, A (corresponding author), Moscow Technol Univ, MIREA, Moscow, Russia.
EM Sboev_AG@nrcki.ru
CR Brader JM, 2007, NEURAL COMPUT, V19, P2881, DOI 10.1162/neco.2007.19.11.2881
   Diehl P., 2015, FRONTIERS COMPUTATIO
   Diehl PU, 2015, IEEE IJCNN
   Eliasmith C., 2013, BUILD BRAIN NEURAL A, DOI DOI 10.1093/ACPROF:OSO/9780199794546.001.0001
   Franosch JMP, 2013, NEURAL COMPUT, V25, P3113, DOI 10.1162/NECO_a_00520
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Legenstein R, 2005, NEURAL COMPUT, V17, P2337, DOI 10.1162/0899766054796888
   Morrison A, 2008, BIOL CYBERN, V98, P459, DOI 10.1007/s00422-008-0233-1
   Sboev A., MACHINE LEA IN PRESS
   Zagorovskaya OV, 2012, MIR NAUKI KULTURY OB, V3, P387
NR 10
TC 0
Z9 0
U1 0
U2 5
PY 2016
VL 101
BP 187
EP 196
DI 10.1016/j.procs.2016.11.023
UT WOS:000390604100022
DA 2023-11-16
ER

PT C
AU Kuroda, K
   Hasegawa, M
AF Kuroda, Kaori
   Hasegawa, Mikio
BE Villa, AEP
   Masulli, P
   Rivero, AJP
TI Method for Estimating Neural Network Topology Based on SPIKE-Distance
SO ARTIFICIAL NEURAL NETWORKS AND MACHINE LEARNING - ICANN 2016, PT I
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 25th International Conference on Artificial Neural Networks (ICANN)
CY SEP 06-09, 2016
CL Barcelona, SPAIN
DE Partialization analysis; SPIKE-distance; Neural network structure;
   Connectivity; Spike sequence
AB To understand information processing in the brain, it is important to clarify the neural network topology. We have already proposed the method of estimating neural network topology only from observed multiple spike sequences by quantifying distance between spike sequences. To quantify distance between spike sequences, the spike time metric was used in the conventional method. However, the spike time metric involves a parameter. Then, we have to set an optimal parameter in the spike time metric. In this paper, we used the SPIKE-distance instead of the spike time metric and applied a partialization analysis to the SPIKE-distance. The SPIKE-distance is a parameter-free measure which can quantify the distance between spike sequences. Using the SPIKE-distance, we estimate the network topology. As a result, the proposed method exhibits higher performance than the conventional method.
C1 [Kuroda, Kaori; Hasegawa, Mikio] Tokyo Univ Sci, Dept Elect Engn, Fac Engn, Katsuhika Ku, 6-3-1 Niijuku, Tokyo 1258585, Japan.
RP Kuroda, K (corresponding author), Tokyo Univ Sci, Dept Elect Engn, Fac Engn, Katsuhika Ku, 6-3-1 Niijuku, Tokyo 1258585, Japan.
EM kuroda@ee.kagu.tus.ac.jp; hasegawa@ee.kagu.tus.ac.jp
CR Eichler M, 2003, BIOL CYBERN, V89, P289, DOI 10.1007/s00422-003-0400-3
   Frenzel S, 2007, PHYS REV LETT, V99, DOI 10.1103/PhysRevLett.99.204101
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Kreuz T, 2013, J NEUROPHYSIOL, V109, P1457, DOI 10.1152/jn.00873.2012
   Kuroda K, 2014, PHYSICA A, V415, P194, DOI 10.1016/j.physa.2014.08.001
   Kuroda K, 2011, PHYSICA A, V390, P4002, DOI 10.1016/j.physa.2011.06.026
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Schelter B, 2006, PHYS REV LETT, V96, DOI 10.1103/PhysRevLett.96.208103
   Smirnov D, 2007, CHAOS, V17, DOI 10.1063/1.2430639
   Victor JD, 1997, NETWORK-COMP NEURAL, V8, P127, DOI 10.1088/0954-898X/8/2/003
NR 10
TC 1
Z9 1
U1 1
U2 2
PY 2016
VL 9886
BP 91
EP 98
DI 10.1007/978-3-319-44778-0_11
UT WOS:000389086300011
DA 2023-11-16
ER

PT C
AU Sheng, YP
   Wang, YY
   Wang, L
   Zhao, GF
AF Sheng, Yongpan
   Wang, Yangyang
   Wang, Lu
   Zhao, Gaofeng
BE Li, JP
   Bloshanskii, I
   Ahmad, I
   Yang, SX
TI A COMPARISON OF LEARNING RULES IN PULSE-BASED NEURAL NETWORKS
SO 2016 13TH INTERNATIONAL COMPUTER CONFERENCE ON WAVELET ACTIVE MEDIA
   TECHNOLOGY AND INFORMATION PROCESSING (ICCWAMTIP)
SE International Computer Conference on Wavelet Active Media Technology and
   Information Processing
DT Proceedings Paper
CT 13th IEEE International Computer Conference on Wavelet Active Media
   Technology and Information Processing (ICCWAMTIP)
CY DEC 16-18, 2016
CL Univ Elect Sci & Technol China, Chengdu, PEOPLES R CHINA
HO Univ Elect Sci & Technol China
DE Learning method; Spiking neural networks
ID SPIKE TRAINS; INFORMATION; PRECISION; ALGORITHM
AB With a similar mechanism of processing spikes as biological neural systems, spiking neural networks (SNNs) are expected to perform a better performance than traditional rate based neural networks. Despite the progress in the application of SNNs, how does the brain learn information from spiking based codes remains an open question. To train a spiking neuron output desired spike trains, many learning methods have been proposed, and can be mainly divided into two classes: spike-driven methods and membrane potential-driven methods. Moreover, different learning algorithms have different advantages in different applications. Therefore, in this paper, we analysis and compare the existing learning algorithm for spiking neurons, including learning efficiency, learning accuracy.
C1 [Sheng, Yongpan; Wang, Yangyang; Zhao, Gaofeng] Univ Elect Sci & Technol, Dept Comp Sci & Engn, Chengdu, Sichuan, Peoples R China.
   [Wang, Lu] Cent Univ Finance & Econ, Network Informat Ctr, Beijing, Peoples R China.
   [Wang, Lu] Cent Univ Finance & Econ, Digital Campus Construct Off, Beijing, Peoples R China.
RP Sheng, YP (corresponding author), Univ Elect Sci & Technol, Dept Comp Sci & Engn, Chengdu, Sichuan, Peoples R China.
EM shengyp2011@163.com
CR Bair W, 1996, NEURAL COMPUT, V8, P1185, DOI 10.1162/neco.1996.8.6.1185
   Berry MJ, 1998, J NEUROSCI, V18, P2200
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Borst A, 1999, NAT NEUROSCI, V2, P947, DOI 10.1038/14731
   Florian RV, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0040233
   Gollisch T, 2008, SCIENCE, V319, P1108, DOI 10.1126/science.1149639
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Kempter R, 1999, ADV NEUR IN, V11, P125
   Maass W, 1997, NEURAL COMPUT, V9, P279, DOI 10.1162/neco.1997.9.2.279
   Maass W., NOISY SPIKING NEURON
   Mohemmed A, 2012, INT J NEURAL SYST, V22, DOI 10.1142/S0129065712500128
   Ponulak F., 2005, TECH REP
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Reinagel P, 2000, J NEUROSCI, V20, P5392, DOI 10.1523/JNEUROSCI.20-14-05392.2000
   Schreiber S, 2003, NEUROCOMPUTING, V52-4, P925, DOI 10.1016/S0925-2312(02)00838-X
   Uzzell VJ, 2004, J NEUROPHYSIOL, V92, P780, DOI 10.1152/jn.01171.2003
   van Rossum MCW, 2001, NEURAL COMPUT, V13, P751, DOI 10.1162/089976601300014321
   Victor JD, 1997, NETWORK-COMP NEURAL, V8, P127, DOI 10.1088/0954-898X/8/2/003
   Xu Y, 2013, NEURAL COMPUT, V25, P1472, DOI 10.1162/NECO_a_00450
   Xu Y, 2013, NEURAL NETWORKS, V43, P99, DOI 10.1016/j.neunet.2013.02.003
   Yu Q, 2013, IEEE T NEUR NET LEAR, V24, P1539, DOI 10.1109/TNNLS.2013.2245677
   Yu QF, 2013, PLOS ONE, V8, DOI [10.1371/journal.pone.0059627, 10.1371/journal.pone.0078318]
   Zhang M., 2015, P 18 AS PAC S INT EV, V1
   Zhang Malu, 2016, NEUROCOMPUTING
NR 24
TC 0
Z9 0
U1 0
U2 2
PY 2016
BP 95
EP 98
UT WOS:000428746400019
DA 2023-11-16
ER

PT J
AU Soltic, S
   Kasabov, N
AF Soltic, Snjezana
   Kasabov, Nikola
TI KNOWLEDGE EXTRACTION FROM EVOLVING SPIKING NEURAL NETWORKS WITH RANK
   ORDER POPULATION CODING
SO INTERNATIONAL JOURNAL OF NEURAL SYSTEMS
DT Article
DE Evolving spiking neural networks; SNN; rank order population coding;
   knowledge discovery; fuzzy rules
ID SENSORS; IMPLEMENTATION; COMPUTATION; NEURONS; MODEL; FILMS; RULE
AB This paper demonstrates how knowledge can be extracted from evolving spiking neural networks with rank order population coding. Knowledge discovery is a very important feature of intelligent systems. Yet, a disproportionally small amount of research is centered on the issue of knowledge extraction from spiking neural networks which are considered to be the third generation of artificial neural networks. The lack of knowledge representation compatibility is becoming a major detriment to end users of these networks. We show that a high-level knowledge can be obtained from evolving spiking neural networks. More specifically, we propose a method for fuzzy rule extraction from an evolving spiking network with rank order population coding. The proposed method was used for knowledge discovery on two benchmark taste recognition problems where the knowledge learnt by an evolving spiking neural network was extracted in the form of zero-order Takagi-Sugeno fuzzy IF-THEN rules.
C1 [Soltic, Snjezana] Manukau Inst Technol, Sch Elect Engn, Auckland, New Zealand.
   [Soltic, Snjezana; Kasabov, Nikola] Auckland Univ Technol, KEDRI, Auckland, New Zealand.
RP Soltic, S (corresponding author), Manukau Inst Technol, Sch Elect Engn, Auckland, New Zealand.
EM ssoltic@manukau.ac.nz; nkasabov@aut.ac.nz
CR Belatreche A, 2007, SOFT COMPUT, V11, P239, DOI [10.1007/s00500-006-0065-7, 10.1007/S00500-006-0065-7]
   Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   Booij O, 2005, INFORM PROCESS LETT, V95, P552, DOI 10.1016/j.ipl.2005.05.023
   Brody CD, 2003, NEURON, V37, P843, DOI 10.1016/S0896-6273(03)00120-X
   Cortina M, 2006, ANAL BIOANAL CHEM, V385, P1186, DOI 10.1007/s00216-006-0530-2
   de Sousa HC, 2002, VII BRAZILIAN SYMPOSIUM ON NEURAL NETWORKS, PROCEEDINGS, P13, DOI 10.1109/SBRN.2002.1181428
   Eurich CW, 2000, NEURAL COMPUT, V12, P1519, DOI 10.1162/089976600300015240
   Gallardo J, 2003, ANAL BIOANAL CHEM, V377, P248, DOI 10.1007/s00216-003-2042-7
   Gautrais J, 1998, BIOSYSTEMS, V48, P57, DOI 10.1016/S0303-2647(98)00050-1
   Gerstner W, 1996, NATURE, V383, P76, DOI 10.1038/383076a0
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   KASABOV N, 2002, EVOLVING CONNECTIONI
   Kasabov N, 2008, IEEE COMPUT INTELL M, V3, P23, DOI 10.1109/MCI.2008.926584
   Kasabov N, 2010, NEURAL NETWORKS, V23, P16, DOI 10.1016/j.neunet.2009.08.010
   Kasabov N, 2009, LECT NOTES COMPUT SC, V5506, P3, DOI 10.1007/978-3-642-02490-0_1
   Kasabov Nikola, 2009, Natural Computing, V8, P199, DOI 10.1007/s11047-008-9066-z
   Kistler WM, 1997, NEURAL COMPUT, V9, P1015, DOI 10.1162/neco.1997.9.5.1015
   Loiselle S, 2005, IEEE IJCNN, P2076
   Maass W, 1998, PULSED NEURAL NETWORKS, P55
   Maguire LP, 2007, NEUROCOMPUTING, V71, P13, DOI 10.1016/j.neucom.2006.11.029
   Martinelli E, 2006, SENSOR ACTUAT B-CHEM, V119, P234, DOI 10.1016/j.snb.2005.12.029
   Pearson MJ, 2007, IEEE T NEURAL NETWOR, V18, P1472, DOI 10.1109/TNN.2007.891203
   Riul A, 2004, SENSOR ACTUAT B-CHEM, V98, P77, DOI 10.1016/j.snb.2003.09.025
   Riul A, 2002, LANGMUIR, V18, P239, DOI 10.1021/la011017d
   Rossello JL, 2009, INT J NEURAL SYST, V19, P465, DOI 10.1142/S0129065709002166
   Soltic S, 2008, IEEE IJCNN, P2091, DOI 10.1109/IJCNN.2008.4634085
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   Wide P, 1998, IEEE T INSTRUM MEAS, V47, P1072, DOI 10.1109/19.746559
   Wu QX, 2006, IEEE SYS MAN CYBERN, P2796, DOI 10.1109/ICSMC.2006.385297
   WYSOSKI S, 2010, NEURAL NETW IN PRESS
   Wysoski SG, 2008, NEUROCOMPUTING, V71, P2563, DOI 10.1016/j.neucom.2007.12.038
   Zuppicich A, 2009, LECT NOTES COMPUT SC, V5506, P1129, DOI 10.1007/978-3-642-02490-0_137
NR 35
TC 45
Z9 45
U1 0
U2 25
PD DEC
PY 2010
VL 20
IS 6
BP 437
EP 445
DI 10.1142/S012906571000253X
UT WOS:000284647300002
DA 2023-11-16
ER

PT C
AU Stitt, JP
   Gaumond, RP
   Frazier, JL
   Hanson, FE
AF Stitt, JP
   Gaumond, RP
   Frazier, JL
   Hanson, FE
BE LaCourse, JR
TI An artificial neural network for neural spike classification
SO PROCEEDINGS OF THE IEEE 23RD NORTHEAST BIOENGINEERING CONFERENCE
DT Proceedings Paper
CT IEEE 23rd Northeast Bioengineering Conference
CY MAY 21-22, 1997
CL UNIV NEW HAMPSHIRE, NEW ENGLAND CTR, DURHAM, NH
HO UNIV NEW HAMPSHIRE, NEW ENGLAND CTR
AB We discuss an Artificial Neural Network capable of sorting neural spikes contained in a mixed spike train, The ANN performs very well when compared with conventional optimal methods of Template Matching and Principal Components.
RP Stitt, JP (corresponding author), PENN STATE UNIV,UNIVERSITY PK,PA 16802, USA.
NR 0
TC 1
Z9 1
U1 0
U2 0
PY 1997
BP 15
EP 16
DI 10.1109/NEBC.1997.594936
UT WOS:A1997BJ05P00008
DA 2023-11-16
ER

PT J
AU Lin, XH
   Zhang, MW
   Wang, XW
AF Lin, Xianghong
   Zhang, Mengwei
   Wang, Xiangwen
TI Supervised Learning Algorithm for Multilayer Spiking Neural Networks
   with Long-Term Memory Spike Response Model
SO COMPUTATIONAL INTELLIGENCE AND NEUROSCIENCE
DT Article
ID ERROR-BACKPROPAGATION; GRADIENT DESCENT; CLASSIFICATION; NEURONS; SPACE
AB As a new brain-inspired computational model of artificial neural networks, spiking neural networks transmit and process information via precisely timed spike trains. Constructing efficient learning methods is a significant research field in spiking neural networks. In this paper, we present a supervised learning algorithm for multilayer feedforward spiking neural networks; all neurons can fire multiple spikes in all layers. The feedforward network consists of spiking neurons governed by biologically plausible long-term memory spike response model, in which the effect of earlier spikes on the refractoriness is not neglected to incorporate adaptation effects. The gradient descent method is employed to derive synaptic weight updating rule for learning spike trains. The proposed algorithm is tested and verified on spatiotemporal pattern learning problems, including a set of spike train learning tasks and nonlinear pattern classification problems on four UCI datasets. Simulation results indicate that the proposed algorithm can improve learning accuracy in comparison with other supervised learning algorithms.
C1 [Lin, Xianghong; Zhang, Mengwei; Wang, Xiangwen] Northwest Normal Univ, Coll Comp Sci & Engn, Lanzhou 730070, Peoples R China.
RP Lin, XH (corresponding author), Northwest Normal Univ, Coll Comp Sci & Engn, Lanzhou 730070, Peoples R China.
EM linxh@nwnu.edu.cn
CR Abidi MH, 2021, COMPUT STAND INTER, V76, DOI 10.1016/j.csi.2021.103518
   Alazab M, 2020, IEEE ACCESS, V8, P85454, DOI 10.1109/ACCESS.2020.2991067
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Booij O, 2005, INFORM PROCESS LETT, V95, P552, DOI 10.1016/j.ipl.2005.05.023
   Caporale N, 2008, ANNU REV NEUROSCI, V31, P25, DOI 10.1146/annurev.neuro.31.060407.125639
   Carnell A., 2005, P ESANN, P363
   Cyr A, 2019, COMPUT INTEL NEUROSC, V2019, DOI 10.1155/2019/8361369
   Dua D., 2019, MACHINE LEARNING REP
   Fu SY, 2012, COMPUT INTEL NEUROSC, V2012, DOI 10.1155/2012/946589
   Gadekallu TR, 2021, J REAL-TIME IMAGE PR, V18, P1383, DOI 10.1007/s11554-020-00987-8
   GERSTNER W, 1992, NETWORK-COMP NEURAL, V3, P139, DOI 10.1088/0954-898X/3/2/004
   Gerstner W, 2018, SCHOLARPEDIA, V3, P1343
   Gerstner W., 2002, SPIKING NEURON MODEL
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Hong CF, 2020, IEEE T NEUR NET LEAR, V31, P1285, DOI 10.1109/TNNLS.2019.2919662
   Jeyasothy A, 2019, IEEE T NEUR NET LEAR, V30, P1231, DOI 10.1109/TNNLS.2018.2868874
   KNUDSEN EI, 1994, J NEUROSCI, V14, P3985
   Kreiman G., 2004, Physics of Life Reviews, V1, P71, DOI 10.1016/j.plrev.2004.06.001
   Li XM, 2020, NEURAL PLAST, V2020, DOI 10.1155/2020/8851351
   Lin XH, 2017, NEUROCOMPUTING, V237, P59, DOI 10.1016/j.neucom.2016.08.087
   [蔺想红 Lin Xianghong], 2016, [电子学报, Acta Electronica Sinica], V44, P2877
   [蔺想红 Lin Xianghong], 2015, [电子学报, Acta Electronica Sinica], V43, P577
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   McKennoch S, 2006, IEEE IJCNN, P3970
   Mohemmed A, 2012, INT J NEURAL SYST, V22, DOI 10.1142/S0129065712500128
   Mostafa Hesham, 2018, IEEE Trans Neural Netw Learn Syst, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Paiva ARC, 2009, NEURAL COMPUT, V21, P424, DOI 10.1162/neco.2008.09-07-614
   Park IM, 2013, IEEE SIGNAL PROC MAG, V30, P149, DOI 10.1109/MSP.2013.2251072
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Schreiber S, 2003, NEUROCOMPUTING, V52-4, P925, DOI 10.1016/S0925-2312(02)00838-X
   Shrestha SB, 2018, IEEE T NEUR NET LEAR, V29, P3126, DOI 10.1109/TNNLS.2017.2713125
   Sporea I, 2013, NEURAL COMPUT, V25, P473, DOI 10.1162/NECO_a_00396
   Taherkhani A, 2020, IEEE T COGN DEV SYST, V12, P427, DOI 10.1109/TCDS.2019.2909355
   Wade JJ, 2010, IEEE T NEURAL NETWOR, V21, P1817, DOI 10.1109/TNN.2010.2074212
   Walter F, 2016, NEURAL PROCESS LETT, V44, P103, DOI 10.1007/s11063-015-9478-6
   Wang XW, 2020, NEURAL NETWORKS, V125, P258, DOI 10.1016/j.neunet.2020.02.011
   Wang XW, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00252
   Xu Y, 2013, NEURAL NETWORKS, V43, P99, DOI 10.1016/j.neunet.2013.02.003
   Yang J, 2012, APPL MATH LETT, V25, P1118, DOI 10.1016/j.aml.2012.02.016
   Yu Q, 2017, INTEL SYST REF LIBR, V126, P115, DOI 10.1007/978-3-319-55310-8_6
   Yu Q, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0078318
   Zenke F, 2018, NEURAL COMPUT, V30, P1514, DOI 10.1162/neco_a_01086
   [张玉平 Zhang Yuping], 2015, [计算机工程与科学, Computer Engineering and Science], V37, P348
NR 45
TC 6
Z9 6
U1 5
U2 25
PD NOV 24
PY 2021
VL 2021
AR 8592824
DI 10.1155/2021/8592824
UT WOS:000770251500010
DA 2023-11-16
ER

PT C
AU Lin, XH
   Li, Y
   Zhao, JC
AF Lin, Xianghong
   Li, Ying
   Zhao, Jichang
BE Huang, DS
   Bevilacqua, V
   Premaratne, P
   Gupta, P
TI Topological Structure Analysis of Developmental Spiking Neural Networks
SO INTELLIGENT COMPUTING THEORIES AND APPLICATION, ICIC 2017, PT I
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 13th International Conference on Intelligent Computing (ICIC)
CY AUG 07-10, 2017
CL Liverpool, ENGLAND
DE Spiking neural network; Developmental process; Recurrent genetic
   regulatory network; Scale-free; Small-world
AB The complex network structure of biological brains is obtained through the developmental processes. Type and complexity of network structure directly reflect the ability of the network to deal with information processing. In this paper, we propose a developmental method for creating recurrent spiking neural networks based on genetic regulatory network model. This research investigates the developmental process of spiking neural networks, and analyzes the network structure in the different parameter settings, such as the number of regulatory nodes, the weights scale of genetic networks, and the developmental scale. The experimental results show that the developmental spiking neural networks have the similar topological characteristics as biological networks, namely scale-free and small-world properties.
C1 [Lin, Xianghong; Li, Ying; Zhao, Jichang] Northwest Normal Univ, Sch Comp Sci & Engn, Lanzhou 730070, Gansu, Peoples R China.
RP Lin, XH (corresponding author), Northwest Normal Univ, Sch Comp Sci & Engn, Lanzhou 730070, Gansu, Peoples R China.
EM linxh@nwnu.edu.cn
CR Ahmadizar F, 2015, ENG APPL ARTIF INTEL, V39, P1, DOI 10.1016/j.engappai.2014.11.003
   [Anonymous], 2016, 2016 IEE INT C REB C, DOI [10.1109/ICRC.2016.7738691, DOI 10.1109/ICRC.2016.7738691]
   Fazekas I, 2013, J PROBAB STAT, V2013, DOI 10.1155/2013/707960
   Federici D, 2005, NEURAL NETWORKS, V18, P746, DOI 10.1016/j.neunet.2005.06.006
   Gruau F., 1994, NEURAL NETWORK SYNTH
   Homma N., 2001, ISCAS 2001. The 2001 IEEE International Symposium on Circuits and Systems (Cat. No.01CH37196), P171, DOI 10.1109/ISCAS.2001.922012
   Kasabov NK, 2014, NEURAL NETWORKS, V52, P62, DOI 10.1016/j.neunet.2014.01.006
   Kitano H., 1990, Complex Systems, V4, P461
   Lones M.A., 2016, COMPUTING ARTIFICIAL
   Longabaugh WJR, 2005, DEV BIOL, V283, P1, DOI 10.1016/j.ydbio.2005.04.023
   Martín M, 2016, TRANSCR-AUSTIN, V7, P32, DOI 10.1080/21541264.2015.1130118
   Mehlhorn H., 2013, SMALL WORLD PROPERTY, P1957
   Suzuki T, 2015, INT J MOD PHYS C, V26, DOI 10.1142/S0129183115501223
   Weaver D C, 1999, Pac Symp Biocomput, P112
NR 14
TC 1
Z9 1
U1 0
U2 5
PY 2017
VL 10361
BP 89
EP 100
DI 10.1007/978-3-319-63309-1_9
UT WOS:000441208400009
DA 2023-11-16
ER

PT J
AU Yamazaki, K
   Vo-Ho, VK
   Bulsara, D
   Le, N
AF Yamazaki, Kashu
   Vo-Ho, Viet-Khoa
   Bulsara, Darshan
   Le, Ngan
TI Spiking Neural Networks and Their Applications: A Review
SO BRAIN SCIENCES
DT Review
DE spiking neural networks; biological neural network; autonomous robot;
   robotics; computer vision; neuromorphic hardware; toolkits; survey;
   review
ID MODEL; ORGANIZATION; STDP
AB The past decade has witnessed the great success of deep neural networks in various domains. However, deep neural networks are very resource-intensive in terms of energy consumption, data requirements, and high computational costs. With the recent increasing need for the autonomy of machines in the real world, e.g., self-driving vehicles, drones, and collaborative robots, exploitation of deep neural networks in those applications has been actively investigated. In those applications, energy and computational efficiencies are especially important because of the need for real-time responses and the limited energy supply. A promising solution to these previously infeasible applications has recently been given by biologically plausible spiking neural networks. Spiking neural networks aim to bridge the gap between neuroscience and machine learning, using biologically realistic models of neurons to carry out the computation. Due to their functional similarity to the biological neural network, spiking neural networks can embrace the sparsity found in biology and are highly compatible with temporal code. Our contributions in this work are: (i) we give a comprehensive review of theories of biological neurons; (ii) we present various existing spike-based neuron models, which have been studied in neuroscience; (iii) we detail synapse models; (iv) we provide a review of artificial neural networks; (v) we provide detailed guidance on how to train spike-based neuron models; (vi) we revise available spike-based neuron frameworks that have been developed to support implementing spiking neural networks; (vii) finally, we cover existing spiking neural network applications in computer vision and robotics domains. The paper concludes with discussions of future perspectives.
C1 [Yamazaki, Kashu; Vo-Ho, Viet-Khoa; Le, Ngan] Univ Arkansas, Dept Comp Sci & Comp Engn, Fayetteville, AR 72701 USA.
   [Bulsara, Darshan] Univ Calif San Diego, Dept Elect & Comp Engn, La Jolla, CA 92093 USA.
RP Le, N (corresponding author), Univ Arkansas, Dept Comp Sci & Comp Engn, Fayetteville, AR 72701 USA.
EM kyamazak@uark.edu; khoavoho@uark.edu; dbulsara@ucsd.edu; thile@uark.edu
CR Angelidis E., 2021, ARXIV, DOI [10.1088/2634-4386/ac1b76, DOI 10.1088/2634-4386/AC1B76]
   [Anonymous], 2016, ARXIV
   [Anonymous], 2017, ARXIV
   [Anonymous], KERAS
   Balaji A., 2020, ARXIV
   Bekolay T., 2013, COGNITIVE SCI SOC, P169
   Bekolay T, 2014, FRONT NEUROINFORM, V7, DOI 10.3389/fninf.2013.00048
   Bell CC, 1997, NATURE, V387, P278, DOI 10.1038/387278a0
   Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Bing Han, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13555, DOI 10.1109/CVPR42600.2020.01357
   Bing ZS, 2019, FRONT NEUROROBOTICS, V13, DOI 10.3389/fnbot.2019.00018
   Bohte S. M., 2000, 8th European Symposium on Artificial Neural Networks. ESANN"2000. Proceedings, P419
   Brette R, 2005, J NEUROPHYSIOL, V94, P3637, DOI 10.1152/jn.00686.2005
   Burbank KS, 2015, PLOS COMPUT BIOL, V11, DOI 10.1371/journal.pcbi.1004566
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Chakraborty B., 2021, ARXIV, DOI [10.1109/TIP.2021.3122092, DOI 10.1109/TIP.2021.3122092]
   Chen Q., 2021, PROC IEEE INT S CIRC, P1, DOI DOI 10.1038/NMETH.1318
   Chou TS, 2018, IEEE IJCNN
   Cuevas-Arteaga B, 2017, LECT NOTES COMPUT SC, V10305, P548, DOI 10.1007/978-3-319-59153-7_47
   Cun Y., 1988, P 1988 CONN MOD SUMM, P21, DOI DOI 10.3168/JDS.S0022-0302(88)79586-7
   Cybenko G., 1989, Mathematics of Control, Signals, and Systems, V2, P303, DOI 10.1007/BF02551274
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Dayan P, 2005, THEORETICAL NEUROSCI
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Diehl PU, 2015, IEEE IJCNN
   Donati E, 2014, BIOMED CIRC SYST C, P512, DOI 10.1109/BioCAS.2014.6981775
   Dupeyroux J., 2020, ARXIV
   Fan Y, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P445, DOI 10.1145/2993148.2997632
   Fang Wei, 2021, ADV NEURAL INFORM PR
   Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   Galluppi F, 2012, BIOMED CIRC SYST C, P91, DOI 10.1109/BioCAS.2012.6418493
   Gautrais J, 1998, BIOSYSTEMS, V48, P57, DOI 10.1016/S0303-2647(98)00050-1
   Gerstner W., 2002, SPIKING NEURON MODEL
   Girshick R., 2015, ARXIV
   Godet P, 2021, INT C PATT RECOG, P2462, DOI 10.1109/ICPR48806.2021.9412269
   Han J, 1995, LECT NOTES COMPUT SC, V930, P195
   Hazan H., 2018, ARXIV, DOI [10.3389/fninf.2018.00089, DOI 10.3389/FNINF.2018.00089]
   Hazan H, 2020, ANN MATH ARTIF INTEL, V88, P1237, DOI 10.1007/s10472-019-09665-3
   He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]
   He Kaiming, 2016, PROC CVPR IEEE
   He WH, 2020, NEURAL NETWORKS, V132, P108, DOI 10.1016/j.neunet.2020.08.001
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2007, DYNAMICAL SYSTEMS NE
   Izhikevich EM, 2007, CEREB CORTEX, V17, P2443, DOI 10.1093/cercor/bhl152
   Jiang Z., 2020, P 2020 IEEE RSJ INT, P7124
   Jin X, 2019, IEEE I CONF COMP VIS, P1345, DOI 10.1109/ICCV.2019.00143
   Kasabov N. K., 2019, TIME SPACE SPIKING N, DOI DOI 10.1007/978-3-662-57715-8
   Kasabov NK, 2014, NEURAL NETWORKS, V52, P62, DOI 10.1016/j.neunet.2014.01.006
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Kim S., 2019, ARXIV, DOI [10.1609/aaai.v34i07.6787, DOI 10.1609/AAAI.V34I07.6787]
   Kim Y.D., 2022, ARXIV
   Kirkland P, 2020, IEEE IJCNN, DOI 10.1109/ijcnn48605.2020.9207075
   Komer B, 2015, THESIS U WATERLOO CA
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   LeCun Y, 1998, LECT NOTES COMPUT SC, V1524, P9, DOI 10.1007/3-540-49430-8_2
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee C.p., 2020, ARXIV
   Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508
   LESHNO M, 1993, NEURAL NETWORKS, V6, P861, DOI 10.1016/S0893-6080(05)80131-5
   Li GY, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2383
   Li XM, 2018, PHYSICA A, V491, P716, DOI 10.1016/j.physa.2017.08.053
   Li Y., 2021, INT C MACHINE LEARNI, V139, P6316
   Long J., 2014, ARXIV
   Luo Y., 2020, ARXIV
   Luo YL, 2020, IEEE ACCESS, V8, P46007, DOI 10.1109/ACCESS.2020.2978163
   Marc-Oliver G., 2007, SCHOLARPEDIA, DOI [DOI 10.4249/SCHOLARPEDIA.1430, 10.4249/scholarpedia.1430]
   Masquelier T, 2007, PLOS COMPUT BIOL, V3, P247, DOI 10.1371/journal.pcbi.0030031
   McCulloch Warren S., 1943, BULL MATH BIOPHYS, V5, P115, DOI 10.1007/BF02478259
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Meunier C, 2002, TRENDS NEUROSCI, V25, P558, DOI 10.1016/S0166-2236(02)02278-6
   Morris RGM, 1999, BRAIN RES BULL, V50, P437, DOI 10.1016/S0361-9230(99)00182-3
   Mozafari M., 2018, ARXIV, DOI [10.1016/j.patcog.2019.05.015, DOI 10.1016/J.PATCOG.2019.05.015]
   Mozafari M, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00625
   Nair V., 2010, PROC 27 INT C INT C
   Nelson M.E., 2005, DATABASING BRAIN DAT, P285
   Orchard G, 2015, FRONT NEUROSCI-SWITZ, V9, DOI [10.3389/fhins.2015.00437, 10.3389/fnins.2015.00437]
   Panda P, 2016, IEEE IJCNN, P299, DOI 10.1109/IJCNN.2016.7727212
   Parameshwara C. M., 2021, ARXIV
   Paredes-Valles F., 2018, ARXIV, DOI [10.1109/TPAMI.2019.2903179, DOI 10.1109/TPAMI.2019.2903179]
   Paszke A, 2019, ADV NEUR IN, V32
   Patel K., 2021, ARXIV
   Rasmussen D., 2018, ARXIV, DOI [10.1007/s12021-019-09424-z, DOI 10.1007/S12021-019-09424-Z]
   Rasmussen D, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0180234
   Ren S., 2015, ARXIV150601497, DOI DOI 10.1109/TPAMI.2016.2577031
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   ROSENBLATT F, 1958, PSYCHOL REV, V65, P386, DOI 10.1037/h0042519
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Shrestha S.B., 2018, ARXIV
   Shrestha SB, 2018, ADV NEUR IN, V31
   Siddoway B, 2014, NEUROPHARMACOLOGY, V78, P38, DOI 10.1016/j.neuropharm.2013.07.009
   Simonyan K., 2014, VERY DEEP CONVOLUTIO
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Sonoda S., 2015, ARXIV
   Sporea I., 2012, ARXIV, DOI [10.1162/NECO_a_00396, DOI 10.1162/NECO_A_00396]
   Stagsted RK, 2020, IEEE INT C INT ROBOT, P10939, DOI 10.1109/IROS45743.2020.9340861
   Stagsted RK, 2020, ROBOTICS: SCIENCE AND SYSTEMS XVI
   Stemmler M, 1999, NAT NEUROSCI, V2, P521, DOI 10.1038/9173
   Stimberg M, 2019, ELIFE, V8, DOI 10.7554/eLife.47314
   STRASSBERG AF, 1993, NEURAL COMPUT, V5, P843, DOI 10.1162/neco.1993.5.6.843
   STRICKHOLM A, 1981, BIOPHYS J, V35, P677, DOI 10.1016/S0006-3495(81)84820-5
   Strohmer B, 2020, FRONT NEUROROBOTICS, V14, DOI 10.3389/fnbot.2020.00041
   Susi G, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-91513-8
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tang G, 2019, ARXIV
   Tang G., 2020, ARXIV
   Tang G., 2018, ARXIV
   Tavanaei A., 2016, ARXIV
   van Rossum MCW, 2001, NEURAL COMPUT, V13, P751, DOI 10.1162/089976601300014321
   Yavuz E, 2016, SCI REP-UK, V6, DOI 10.1038/srep18854
   Zenke F, 2014, FRONT NEUROINFORM, V8, DOI 10.3389/fninf.2014.00076
   Zhang DQ, 2018, LECT NOTES COMPUT SC, V11212, P373, DOI 10.1007/978-3-030-01237-3_23
   Zhou Q, 2020, IEEE ACCESS, V8, P224162, DOI 10.1109/ACCESS.2020.3044646
   Zhou Q, 2020, IEEE ACCESS, V8, P101309, DOI 10.1109/ACCESS.2020.2998098
   Zhou SB, 2020, IEEE ACCESS, V8, P76903, DOI 10.1109/ACCESS.2020.2990416
   Zhou Yanqi, 2020, 1015 3, Patent No. 0310414
NR 122
TC 38
Z9 38
U1 58
U2 150
PD JUL
PY 2022
VL 12
IS 7
AR 863
DI 10.3390/brainsci12070863
UT WOS:000831451800001
HC Y
HP N
DA 2023-11-16
ER

PT J
AU Wall, J
   Glackin, C
AF Wall, Julie
   Glackin, Cornelius
TI Spiking neural network connectivity and its potential for temporal
   sensory processing and variable binding
SO FRONTIERS IN COMPUTATIONAL NEUROSCIENCE
DT Editorial Material
DE cell assembly; spiking neural network; spike timing; biological neurons;
   learning; connectivity; sensory processing
C1 [Wall, Julie] Univ London, Sch Elect Engn & Comp Sci, Multimedia & Vis Res Grp, London, England.
   [Glackin, Cornelius] Univ Hertfordshire, Dept Comp Sci, Adapt Syst Res Grp, Hatfield AL10 9AB, Herts, England.
RP Wall, J (corresponding author), Univ London, Sch Elect Engn & Comp Sci, Multimedia & Vis Res Grp, London, England.
EM julie.wall@qmul.ac.uk
CR Aimone JB, 2013, FRONT COMPUT NEUROSC, V7, DOI 10.3389/fncom.2013.00150
   Börgers C, 2013, FRONT COMPUT NEUROSC, V7, DOI 10.3389/fncom.2013.00033
   Buice MA, 2013, FRONT COMPUT NEUROSC, V7, DOI 10.3389/fncom.2013.00162
   Dockendorf K, 2013, FRONT COMPUT NEUROSC, V7, DOI 10.3389/fncom.2013.00080
   Garrido JA, 2013, FRONT COMPUT NEUROSC, V7, DOI 10.3389/fncom.2013.00064
   Kaplan BA, 2013, FRONT COMPUT NEUROSC, V7, DOI 10.3389/fncom.2013.00112
   Moujahid A, 2012, FRONT COMPUT NEUROSC, V6, DOI 10.3389/fncom.2012.00095
   Ponulak F, 2013, FRONT COMPUT NEUROSC, V7, DOI 10.3389/fncom.2013.00098
   Srinivasa N, 2013, FRONT COMPUT NEUROSC, V7, DOI 10.3389/fncom.2013.00010
NR 9
TC 3
Z9 3
U1 0
U2 11
PD DEC 19
PY 2013
VL 7
AR 182
DI 10.3389/fncom.2013.00182
UT WOS:000329069300001
DA 2023-11-16
ER

PT J
AU Cancan, M
AF Cancan, Murat
TI On Ev-Degree and Ve-Degree Topological Properties of Tickysim Spiking
   Neural Network
SO COMPUTATIONAL INTELLIGENCE AND NEUROSCIENCE
DT Article
AB Topological indices are indispensable tools for analyzing networks to understand the underlying topology of these networks. Spiking neural network architecture (SpiNNaker or TSNN) is a million-core calculating engine which aims at simulating the behavior of aggregates of up to a billion neurons in real time. Tickysim is a timing-based simulator of the interchip interconnection network of the SpiNNaker architecture. Tickysim spiking neural network is considered to be highly symmetrical network classes. Classical degree-based topological properties of Tickysim spiking neural network have been recently determined. Ev-degree and ve-degree concepts are two novel degrees recently defined in graph theory. Ev-degree and ve-degree topological indices have been defined as parallel to their corresponding counterparts. In this study, we investigate the ev-degree and ve-degree topological properties of Tickysim spiking neural network. These calculations give the information about the underlying topology of Tickysim spiking neural network.
C1 [Cancan, Murat] Van Yuzuncu Yil Univ, Fac Educ, TR-65080 Van, Turkey.
RP Cancan, M (corresponding author), Van Yuzuncu Yil Univ, Fac Educ, TR-65080 Van, Turkey.
EM m_cencen@yahoo.com
CR Chellali M, 2017, DISCRETE MATH, V340, P31, DOI 10.1016/j.disc.2016.07.008
   Ediz S., 2017, CELAL BAYAR NIVERSIT, V13, P615, DOI [10.18466/cbayarfbe.339313, DOI 10.18466/CBAYARFBE.339313, 10.1177/1559827617729634]
   Ediz S., 2017, INT J SYST SCI APPL, V2, P87, DOI DOI 10.11648/J.IJSSAM.20170205.12
   Ediz S, 2018, INT J COMPUT SCI MAT, V9, P1, DOI 10.1504/IJCSM.2018.10011733
   Furber SB, 2013, IEEE T COMPUT, V62, P2454, DOI 10.1109/TC.2012.142
   Imran M, 2018, AXIOMS, V7, DOI 10.3390/axioms7040073
   Kulkarni SR, 2018, NEURAL NETWORKS, V103, P118, DOI 10.1016/j.neunet.2018.03.019
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Ponulak F, 2011, ACTA NEUROBIOL EXP, V71, P409
   Sahin B, 2018, IRAN J MATH CHEM, V9, P263, DOI 10.22052/ijmc.2017.72666.1265
   Shukla R, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00115
   Song T, 2019, IEEE T NANOBIOSCI, V18, P176, DOI 10.1109/TNB.2019.2896981
   Sun SY, 2019, IEICE ELECTRON EXPR, V16, DOI 10.1587/elex.16.20190004
   VanRullen R, 2005, TRENDS NEUROSCI, V28, P1, DOI 10.1016/j.tins.2004.10.010
NR 15
TC 19
Z9 19
U1 0
U2 23
PY 2019
VL 2019
AR 8429120
DI 10.1155/2019/8429120
UT WOS:000471906600001
DA 2023-11-16
ER

PT C
AU Sheik, S
AF Sheik, Sadique
BE In, V
   Longhini, P
   Palacios, A
TI Spike Based Information Processing in Spiking Neural Networks
SO PROCEEDINGS OF THE 4TH INTERNATIONAL CONFERENCE ON APPLICATIONS IN
   NONLINEAR DYNAMICS (ICAND 2016)
SE Lecture Notes in Networks and Systems
DT Proceedings Paper
CT 4th International Conference on Applications in Nonlinear Dynamics
   (ICAND)
CY 2016
CL Denver, CO
ID TIME; CAPACITY; PATTERNS; SPACE
AB Spiking neural networks are seen as the third generation of neural networks and the closest emulators of their biological counter parts. These networks use spikes as means of transmitting information between neurons. We study the merits and capacity of information transfer using spikes across different encoding and decoding schemes and show that spatio-temporal encoding scheme provides a very high efficiency in information transfer. We then explore learning rules based on neural dynamics that enable learning of spatio-temporal spike patterns. We explore various learning rules that can be used to learn spatio-temporal spike patterns.
C1 [Sheik, Sadique] Univ Calif San Diego, BioCircuits Inst, San Diego, CA 92093 USA.
RP Sheik, S (corresponding author), Univ Calif San Diego, BioCircuits Inst, San Diego, CA 92093 USA.
EM ssheik@ucsd.edu
CR Adrian E. D., 1928, BASIS SENSATION ACTI
   [Anonymous], 2009, ADV NEURAL INFORM PR
   BORST A, 1990, P NATL ACAD SCI USA, V87, P9363, DOI 10.1073/pnas.87.23.9363
   BRUNEL N, 1992, J PHYS A-MATH GEN, V25, P5017, DOI 10.1088/0305-4470/25/19/015
   CARR CE, 1988, P NATL ACAD SCI USA, V85, P8311, DOI 10.1073/pnas.85.21.8311
   CARR CE, 1990, J NEUROSCI, V10, P3227
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x
   Florian RV, 2008, LECT NOTES COMPUT SC, V5164, P368, DOI 10.1007/978-3-540-87559-8_38
   Florian RV, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0040233
   GARDNER E, 1988, J PHYS A-MATH GEN, V21, P257, DOI 10.1088/0305-4470/21/1/030
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Hussain S, 2012, 2012 IEEE ASIA PACIFIC CONFERENCE ON CIRCUITS AND SYSTEMS (APCCAS), P304, DOI 10.1109/APCCAS.2012.6419032
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Johansson RS, 2004, NAT NEUROSCI, V7, P170, DOI 10.1038/nn1177
   Kempter R, 1998, BIOSYSTEMS, V48, P105, DOI 10.1016/S0303-2647(98)00055-0
   Knüsel P, 2004, NEURAL COMPUT, V16, P2079, DOI 10.1162/0899766041732459
   Legenstein R, 2005, NEURAL COMPUT, V17, P2337, DOI 10.1162/0899766054796888
   Masquelier T, 2008, PLOS ONE, V3, DOI 10.1371/journal.pone.0001377
   Masquelier T, 2009, NEURAL COMPUT, V21, P1259, DOI 10.1162/neco.2008.06-08-804
   Minsky M., 1969, PERCEPTRONS INTRO CO
   Mohemmed A, 2012, INT J NEURAL SYST, V22, DOI 10.1142/S0129065712500128
   Natschlager T., 2002, SPECIAL ISSUE FDN IN, P39, DOI [DOI 10.1017/CBO9781107415324.004, 10.1017/CBO9781107415324.004]
   OKeefe J, 1996, NATURE, V381, P425, DOI 10.1038/381425a0
   PFEIFFER M, 2012, PLOS ONE, V7
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   RIEKE F, 1997, SPIKES READING NEURA
   ROSENBLATT F, 1958, PSYCHOL REV, V65, P386, DOI 10.1037/h0042519
   SHEIK S, 2013, THESIS
   SHEIK S, 2016, BIOM CIRC SYST C BIO
   Singer W, 1999, CURR OPIN NEUROBIOL, V9, P189, DOI 10.1016/S0959-4388(99)80026-9
   Stiefel KM, 2013, NEURAL COMPUT, V25, P510, DOI 10.1162/NECO_a_00400
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   van Rossum MCW, 2001, NEURAL COMPUT, V13, P751, DOI 10.1162/089976601300014321
   Victor JD, 1997, NETWORK-COMP NEURAL, V8, P127, DOI 10.1088/0954-898X/8/2/003
   Vreeken J, 2003, 2003008 UUCS, P1
   Widrow B., 1960, ADAPTIVE SWITCHING C, V4, P96, DOI DOI 10.21236/AD0241531
   Wyss R, 2003, P NATL ACAD SCI USA, V100, P324, DOI 10.1073/pnas.0136977100
NR 37
TC 0
Z9 0
U1 1
U2 3
PY 2017
VL 6
BP 177
EP 188
DI 10.1007/978-3-319-52621-8_16
UT WOS:000425537400016
DA 2023-11-16
ER

PT J
AU Yin, BJ
   Corradi, F
   Bohté, SM
AF Yin, Bojian
   Corradi, Federico
   Bohte, Sander M.
TI Accurate and efficient time-domain classification with adaptive spiking
   recurrent neural networks
SO NATURE MACHINE INTELLIGENCE
DT Article
ID NEUROSCIENCE; MACHINE; LSTM
AB Inspired by detailed modelling of biological neurons, spiking neural networks (SNNs) are investigated as biologically plausible and high-performance models of neural computation. The sparse and binary communication between spiking neurons potentially enables powerful and energy-efficient neural networks. The performance of SNNs, however, has remained lacking compared with artificial neural networks. Here we demonstrate how an activity-regularizing surrogate gradient combined with recurrent networks of tunable and adaptive spiking neurons yields the state of the art for SNNs on challenging benchmarks in the time domain, such as speech and gesture recognition. This also exceeds the performance of standard classical recurrent neural networks and approaches that of the best modern artificial neural networks. As these SNNs exhibit sparse spiking, we show that they are theoretically one to three orders of magnitude more computationally efficient compared to recurrent neural networks with similar performance. Together, this positions SNNs as an attractive solution for AI hardware implementations.
   The use of sparse signals in spiking neural networks, modelled on biological neurons, offers in principle a highly efficient approach for artificial neural networks when implemented on neuromorphic hardware, but new training approaches are needed to improve performance. Using a new type of activity-regularizing surrogate gradient for backpropagation combined with recurrent networks of tunable and adaptive spiking neurons, state-of-the-art performance for spiking neural networks is demonstrated on benchmarks in the time domain.
C1 [Yin, Bojian; Bohte, Sander M.] CWI, Machine Learning Grp, Amsterdam, Netherlands.
   [Corradi, Federico] Stichting Interuniv Microelekt Ctr IMEC Nederland, Eindhoven, Netherlands.
   [Bohte, Sander M.] Univ Amsterdam, Fac Sci, Amsterdam, Netherlands.
   [Bohte, Sander M.] Univ Groningen, Fac Sci & Engn, Groningen, Netherlands.
RP Yin, BJ (corresponding author), CWI, Machine Learning Grp, Amsterdam, Netherlands.
EM byin@cwi.nl
CR Amir A, 2017, PROC CVPR IEEE, P7388, DOI 10.1109/CVPR.2017.781
   Arjovsky M, 2016, PR MACH LEARN RES, V48
   Bellec G., 2018, ADV NEURAL INFORM PR
   Bellec G, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-17236-y
   Bohte S. M., 2000, 8th European Symposium on Artificial Neural Networks. ESANN"2000. Proceedings, P419
   Bohte SM, 2011, LECT NOTES COMPUT SC, V6791, P60, DOI 10.1007/978-3-642-21735-7_8
   Cramer B, 2022, IEEE T NEUR NET LEAR, V33, P2744, DOI 10.1109/TNNLS.2020.3044364
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   de Andrade D. C., 2018, NEURAL ATTENTION MOD
   Elfwing S, 2015, NEURAL NETWORKS, V64, P29, DOI 10.1016/j.neunet.2014.09.006
   Elfwing S, 2018, NEURAL NETWORKS, V107, P3, DOI 10.1016/j.neunet.2017.12.012
   Falez P., 2019, 2019 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2019.8852346
   Fang W., 2020, INCORPORATING LEARNA
   Garofolo JS, 1993, TIMIT ACOUSTIC PHONE, DOI DOI 10.35111/17GK-BN40
   Gerstner W, 1996, NATURE, V383, P76, DOI 10.1038/383076a0
   Gerstner W., 2002, SPIKING NEURON MODEL
   Graves A, 2005, NEURAL NETWORKS, V18, P602, DOI 10.1016/j.neunet.2005.06.042
   Hassabis D, 2017, NEURON, V95, P245, DOI 10.1016/j.neuron.2017.06.011
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Horowitz M, 2014, ISSCC DIG TECH PAP I, V57, P10, DOI 10.1109/ISSCC.2014.6757323
   Hunger R, 2005, FLOATING POINT OPERA
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Keijser J., INTERNEURON DIVERSIT, DOI [10.1101/2020.11.17.386920(2020, DOI 10.1101/2020.11.17.386920(2020]
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Kingma DP., 2017, ARXIV
   Kundu S, 2021, IEEE WINT CONF APPL, P3952, DOI 10.1109/WACV48630.2021.00400
   Laguna P, 1997, COMPUT CARDIOL, V24, P673, DOI 10.1109/CIC.1997.648140
   Li S, 2018, PROC CVPR IEEE, P5457, DOI 10.1109/CVPR.2018.00572
   Lichtsteiner P, 2008, IEEE J SOLID-ST CIRC, V43, P566, DOI 10.1109/JSSC.2007.914337
   Ludgate P.E., 1982, ORIGINS DIGITAL COMP, P73
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Paszke A., 2019, P ADV NEUR INF PROC
   Pellegrini T, 2021, IEEE W SP LANG TECH, P97, DOI 10.1109/SLT48900.2021.9383587
   Perez-Nieves N., 2021, NEURAL HETEROGENEITY, DOI [10.1101/2020.12.18.423468v2.full, DOI 10.1101/2020.12.18.423468V2.FULL]
   Raffel C., 2015, PHYTH SCI C SCIPY, P18, DOI [DOI 10.25080/MAJORA-7B98-3ED-003, DOI 10.25080/MAJORA-7B98E3ED-003]
   Roy K, 2019, NATURE, V575, P607, DOI 10.1038/s41586-019-1677-2
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Shewalkar A, 2019, J ARTIF INTELL SOFT, V9, P235, DOI 10.2478/jaiscr-2019-0006
   Shrestha SB, 2018, ADV NEUR IN, V31
   Wang SW, 2016, UIST 2016: PROCEEDINGS OF THE 29TH ANNUAL SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P851, DOI 10.1145/2984511.2984565
   Warden P, 2018, SPEECH COMMANDS DATA
   WERBOS PJ, 1990, P IEEE, V78, P1550, DOI 10.1109/5.58337
   Wong A., 2020, TINYSPEECH ATTENTION
   Wunderlich TC, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-91786-z
   Yin B, 2020, IEEE INTERNET THINGS, V7, P8748, DOI [10.1109/JIOT.2020.2996562, 10.1145/3407197.3407225]
   Zenke F, 2021, P IEEE, V109, P935, DOI 10.1109/JPROC.2020.3045625
   Zenke F, 2021, NEURON, V109, P571, DOI 10.1016/j.neuron.2021.01.009
   Zenke F, 2021, NEURAL COMPUT, V33, P899, DOI 10.1162/neco_a_01367
   Zenke F, 2018, NEURAL COMPUT, V30, P1514, DOI 10.1162/neco_a_01086
NR 50
TC 41
Z9 41
U1 8
U2 53
PD OCT
PY 2021
VL 3
IS 10
BP 905
EP +
DI 10.1038/s42256-021-00397-w
UT WOS:000707955000006
DA 2023-11-16
ER

PT J
AU Jin, DZ
AF Jin, DZ
TI Fast convergence of spike sequences to periodic patterns in recurrent
   networks
SO PHYSICAL REVIEW LETTERS
DT Article
ID PULSE-COUPLED OSCILLATORS; NEURONAL NETWORKS; NEURAL NETWORKS;
   PHASE-LOCKING; SYNCHRONIZATION; DYNAMICS; SYSTEMS; MEMORY; DELAYS
AB The dynamical attractors are thought to underlie many biological functions of recurrent neural networks. Here we show that stable periodic spike sequences with precise timings are the attractors of the spiking dynamics of recurrent neural networks with global inhibition. Almost all spike sequences converge within a finite number of transient spikes to these attractors. The convergence is fast, especially when the global inhibition is strong. These results support the possibility that precise spatiotemporal sequences of spikes are useful for information encoding and processing in biological neural networks.
C1 MIT, Howard Hughes Med Inst, Cambridge, MA 02139 USA.
   MIT, Dept Brain & Cognit Sci, Cambridge, MA 02139 USA.
RP Jin, DZ (corresponding author), MIT, Howard Hughes Med Inst, Cambridge, MA 02139 USA.
CR BOTTANI S, 1995, PHYS REV LETT, V74, P4189, DOI 10.1103/PhysRevLett.74.4189
   Bressloff PC, 2000, NEURAL COMPUT, V12, P91, DOI 10.1162/089976600300015907
   Chow CC, 1998, PHYSICA D, V118, P343, DOI 10.1016/S0167-2789(98)00082-7
   COHEN MA, 1983, IEEE T SYST MAN CYB, V13, P815, DOI 10.1109/TSMC.1983.6313075
   Ermentrout GB, 1998, P NATL ACAD SCI USA, V95, P1259, DOI 10.1073/pnas.95.3.1259
   Gerstner W, 1996, PHYS REV LETT, V76, P1755, DOI 10.1103/PhysRevLett.76.1755
   GERSTNER W, 1995, PHYS REV E, V51, P738, DOI 10.1103/PhysRevE.51.738
   Golomb D, 2000, NEURAL COMPUT, V12, P1095, DOI 10.1162/089976600300015529
   Hahnloser RHR, 2002, NATURE, V419, P65, DOI 10.1038/nature00974
   HANSEL D, 1995, NEURAL COMPUT, V7, P307, DOI 10.1162/neco.1995.7.2.307
   HERZ AVM, 1995, PHYS REV LETT, V75, P1222, DOI 10.1103/PhysRevLett.75.1222
   HOPFIELD JJ, 1984, P NATL ACAD SCI-BIOL, V81, P3088, DOI 10.1073/pnas.81.10.3088
   HOPFIELD JJ, 1995, P NATL ACAD SCI USA, V92, P6655, DOI 10.1073/pnas.92.15.6655
   Jin DHZ, 2002, PHYS REV E, V65, DOI 10.1103/PhysRevE.65.051922
   Laurent G, 1999, SCIENCE, V286, P723, DOI 10.1126/science.286.5440.723
   MIROLLO RE, 1990, SIAM J APPL MATH, V50, P1645, DOI 10.1137/0150098
   Seung HS, 1996, P NATL ACAD SCI USA, V93, P13339, DOI 10.1073/pnas.93.23.13339
   Stopfer M, 1999, NATURE, V402, P664, DOI 10.1038/45244
   TSODYKS M, 1993, PHYS REV LETT, V71, P1280, DOI 10.1103/PhysRevLett.71.1280
   vanVreeswijk C, 1996, PHYS REV E, V54, P5522, DOI 10.1103/PhysRevE.54.5522
   Zhang K, 1996, J NEUROSCI, V16, P2112
NR 21
TC 41
Z9 41
U1 0
U2 7
PD NOV 11
PY 2002
VL 89
IS 20
AR 208102
DI 10.1103/PhysRevLett.89.208102
UT WOS:000178938500052
DA 2023-11-16
ER

PT C
AU Lizrraga-Rodrguez, J
   Lechuga-Gutiérrez, L
   Bayro-Corrochano, E
AF Lizrraga-Rodrguez, Jorge
   Lechuga-Gutierrez, Luis
   Bayro-Corrochano, Eduardo
GP IEEE
TI Spike Quaternion Neural Networks Control for a Hand prosthesis
SO 2018 IEEE LATIN AMERICAN CONFERENCE ON COMPUTATIONAL INTELLIGENCE
   (LA-CCI)
DT Proceedings Paper
CT 5th IEEE Latin American Conference on Computational Intelligence
   (LA-CCI)
CY NOV 06-09, 2018
CL Guadalajara, MEXICO
DE Quaternion; Spike Neural Network
AB In this work, a robotic prosthesis controlled by Spike-type neural networks in conjunction with quaternions is developed. Neural networks are implemented from the development of myoelectric signals to the control of the hand prosthesis. Similarly, the development and obtaining of the prosthesis seen from a mechanical part is shown.
C1 [Lizrraga-Rodrguez, Jorge; Lechuga-Gutierrez, Luis; Bayro-Corrochano, Eduardo] CINVESTAV, Unidad Guadalajara, Dept Control Automat, Guadalajara, Jalisco, Mexico.
RP Lizrraga-Rodrguez, J (corresponding author), CINVESTAV, Unidad Guadalajara, Dept Control Automat, Guadalajara, Jalisco, Mexico.
EM jorgelizarrar@gdl.cinvestav.mx; lrlechuga@gdl.cinvestav.mx;
   edb@gdl.cinvestav.mx
CR [Anonymous], 2012, NON TRADITIONAL REF
   Delorme A, 1999, NEUROCOMPUTING, V26-7, P989, DOI 10.1016/S0925-2312(99)00095-8
   Gerstner W., 2002, SPIKING NEURON MODEL
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
NR 6
TC 0
Z9 0
U1 0
U2 1
PY 2018
UT WOS:000459239300037
DA 2023-11-16
ER

PT C
AU Yang, B
   Wang, YY
   Li, JP
AF Yang, Bo
   Wang, Yangyang
   Li, Jianping
BE Li, JP
   Bloshanskii, I
   Ahmad, I
   Yang, SX
TI A SPIKING-TIMING-BASED MODEL FOR CLASSIFICATION
SO 2016 13TH INTERNATIONAL COMPUTER CONFERENCE ON WAVELET ACTIVE MEDIA
   TECHNOLOGY AND INFORMATION PROCESSING (ICCWAMTIP)
SE International Computer Conference on Wavelet Active Media Technology and
   Information Processing
DT Proceedings Paper
CT 13th IEEE International Computer Conference on Wavelet Active Media
   Technology and Information Processing (ICCWAMTIP)
CY DEC 16-18, 2016
CL Univ Elect Sci & Technol China, Chengdu, PEOPLES R CHINA
HO Univ Elect Sci & Technol China
DE Spiking neural network; DL-ReSuMe; Classification; Machine Learning
ID NEURAL-NETWORKS
AB With a similar capability of processing spikes as biological neural systems, networks of spiking neurons are expected to achieve a performance similar to that of living brains. In recent days, many great achievements have been made in spiking neuron models and its learning algorithm. We build an integrated model using spiking neural networks (SNNs), which performs phase encoding method and delay learning-based ReSuMe (DL-ReSuMe) supervised learning algorithm to solve optical character recognition (OCR) task. In this way, the superiority of spiking-timing-based model will be displayed
C1 [Yang, Bo; Wang, Yangyang; Li, Jianping] Univ Elect Sci & Technol China, Coll Comp Sci & Engn, Chengdu 611731, Sichuan, Peoples R China.
RP Yang, B (corresponding author), Univ Elect Sci & Technol China, Coll Comp Sci & Engn, Chengdu 611731, Sichuan, Peoples R China.
EM 843716001@qq.com
CR Hu J, 2013, NEURAL COMPUT, V25, P450, DOI 10.1162/NECO_a_00395
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Sporea I, 2013, NEURAL COMPUT, V25, P473, DOI 10.1162/NECO_a_00396
   Taherkhani A, 2015, IEEE T NEUR NET LEAR, V26, P3137, DOI 10.1109/TNNLS.2015.2404938
   Yu QF, 2013, PLOS ONE, V8, DOI [10.1371/journal.pone.0059627, 10.1371/journal.pone.0078318]
   Zhang Malu, 2016, NEUROCOMPUTING
NR 6
TC 1
Z9 1
U1 0
U2 0
PY 2016
BP 99
EP 102
UT WOS:000428746400020
DA 2023-11-16
ER

PT C
AU Schuman, CD
   Plank, JS
   Bruer, G
   Anantharaj, J
AF Schuman, Catherine D.
   Plank, James S.
   Bruer, Grant
   Anantharaj, Jeremy
GP IEEE
TI Non-Traditional Input Encoding Schemes for Spiking Neuromorphic Systems
SO 2019 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUL 14-19, 2019
CL Budapest, HUNGARY
DE spiking neural networks; neuromorphic systems; input encoding;
   value-to-spike
ID NETWORKS
AB A key challenge for utilizing spiking neural networks or spiking neuromorphic systems for most applications is translating numerical data into spikes that are appropriate to apply as input to a spiking neural network. In this work, we present several approaches for encoding numerical values as spikes, including binning, spike-count encoding, and charge-injection encoding, and we show how these approaches can be combined hierarchically to form more complex encoding schemes. We demonstrate how these different encoding approaches perform on four different applications, running on four different neuromorphic systems that are based on spiking neural networks. We show that the input encoding method can have a significant effect on application performance and that the best input encoding method is application-specific.
C1 [Schuman, Catherine D.] Oak Ridge Natl Lab, Computat Data Analyt, Oak Ridge, TN 37830 USA.
   [Plank, James S.; Bruer, Grant; Anantharaj, Jeremy] Univ Tennessee, Dept EECS, Knoxville, TN USA.
RP Schuman, CD (corresponding author), Oak Ridge Natl Lab, Computat Data Analyt, Oak Ridge, TN 37830 USA.
EM schumancd@ornl.gov; jplank@utk.edu; gbruer@vols.utk.edu;
   jananth1@vols.utk.edu
CR Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   [Anonymous], 2016, INT JOINT C NEUR NET
   Arthur J.V., 2006, ADV NEURAL INFORM PR, P75
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Bruer G., 2018, INT C NEUR COMP SYST
   Cabessa J, 2014, INT J NEURAL SYST, V24, DOI 10.1142/S0129065714500294
   Chakma G., 2016, INT WORKSH POST MOOR
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Esser S. K., 2015, ADV NEURAL INFORM PR, P1117
   Gomez F, 2006, LECT NOTES COMPUT SC, V4212, P654
   Gopalakrishnan R., 2018, ARXIV180700578
   LeCun Y., 1998, MNIST DATABASE HANDW
   Liu C, 2015, INT J ANTENN PROPAG, V2015, DOI [10.1155/2015/256536, 10.1155/2015/252717]
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Mitchell D, 2018, GEOGR ANN B, V100, P2, DOI 10.1080/04353684.2018.1445478
   Mitchell JP, 2017, 2017 IEEE 5TH INTERNATIONAL SYMPOSIUM ON ROBOTICS AND INTELLIGENT SENSORS (IRIS), P136, DOI 10.1109/IRIS.2017.8250111
   Mostafa H, 2018, IEEE T NEUR NET LEAR, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Plank J. S., 2017, IEEE INT C REB COMP
   Plank J. S., 2018, TENNLAB EXPLORATORY
   Posch C, 2014, P IEEE, V102, P1470, DOI 10.1109/JPROC.2014.2346153
   Querlioz D, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P1775, DOI 10.1109/IJCNN.2011.6033439
   Schuman CD, 2016, IEEE IJCNN, P145, DOI 10.1109/IJCNN.2016.7727192
   Schuman CD, 2014, PROCEDIA COMPUT SCI, V41, P89, DOI 10.1016/j.procs.2014.11.089
   Schumann CL, 2019, AIDS BEHAV, V23, P5, DOI 10.1007/s10461-017-1727-4
   Vanarse A, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00115
   Wieland A. P., 1991, IJCNN-91-Seattle: International Joint Conference on Neural Networks (Cat. No.91CH3049-4), P667, DOI 10.1109/IJCNN.1991.155416
   Yanguas-Gil A., 2018, 43 ANN GOMACTECH C M
   Yi Y, 2016, MICROPROCESS MICROSY, V46, P175, DOI 10.1016/j.micpro.2016.03.009
NR 28
TC 14
Z9 14
U1 0
U2 1
PY 2019
UT WOS:000530893803071
DA 2023-11-16
ER

PT C
AU She, XY
   Saha, P
   Kim, D
   Long, Y
   Mukhopadhyay, S
AF She, Xueyuan
   Saha, Priyabrata
   Kim, Daehyun
   Long, Yun
   Mukhopadhyay, Saibal
GP IEEE
TI SAFE-DNN: A Deep Neural Network With Spike Assisted Feature Extraction
   For Noise Robust Inference
SO 2020 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN) held as part
   of the IEEE World Congress on Computational Intelligence (IEEE WCCI)
CY JUL 19-24, 2020
CL ELECTR NETWORK
DE deep learning; noise robustness; spiking neural network;
   spike-timing-dependent plasticity (STDP)
AB We present a Deep Neural Network with Spike Assisted Feature Extraction (SAFE-DNN) to improve robustness of classification under stochastic perturbation of inputs. The proposed network augments a DNN with unsupervised learning of low-level features using spiking neural network (SNN) with spike-timing-dependent plasticity (STDP). The complete network learns to ignore local perturbation while performing global feature detection and classification. The experimental results on CIFAR-10 and ImageNet subset demonstrate improved noise robustness for multiple DNN architectures without sacrificing accuracy on clean images.
C1 [She, Xueyuan; Saha, Priyabrata; Kim, Daehyun; Long, Yun; Mukhopadhyay, Saibal] Georgia Inst Technol, Sch Elect & Comp Engn, Atlanta, GA 30332 USA.
RP She, XY (corresponding author), Georgia Inst Technol, Sch Elect & Comp Engn, Atlanta, GA 30332 USA.
EM xshe@gatech.edu; priyabratasaha@gatech.edu; daehyun.kim@gatech.edu;
   yunlong@gatech.edu; saibal.mukhopadhyay@ece.gatech.edu
CR Bi Guo-qiang, 2001, ANN REV NEUROSCIENCE
   Bliss T. V P, 1973, J PHYSL
   Carew T J, 1981, J NEUROSCI
   Chu QX, 2014, 2014 INTERNATIONAL WORKSHOP ON ANTENNA TECHNOLOGY: "SMALL ANTENNAS, NOVEL EM STRUCTURES AND MATERIALS, AND APPLICATIONS" (IWAT), P5, DOI 10.1109/IWAT.2014.6958580
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Gerstner Wulfram, 1993, BIOL CYBERNETICS
   Hawkins R. D., 1983, SCIENCE
   Howard Andrew, 2018, INVERTED RESIDUALS L, DOI DOI 10.1109/CVPR.2018.00474
   Huang Gao, 2016, CORR
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90
   Lansdell Benjamin James, 2019, BIORXIV, P253351
   Levy William B., 1979, BRAIN RES
   Long Y, 2019, DES AUT TEST EUROPE, P1769, DOI [10.23919/date.2019.8715178, 10.23919/DATE.2019.8715178]
   Milyaev S., 2017, Pattern Recognition and Image Analysis, V27, P713
   Moreno-Bote Ruben, 2015, SCI REPORTS
   Na T., 2018, INT C AC SPEECH SIGN
   Nazare Tiago S, 2017, CIARP
   Querlioz D, 2013, IEEE T NANOTECHNOL, V12, P288, DOI 10.1109/TNANO.2013.2250995
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   She Xueyuan, 2019, 2019 DES AUT TEST EU
   She Xueyuan, 2019, ARXIV190905401
   Soltanayev S, 2018, ADV NEUR IN, V31
   Srinivasan G, 2016, SCI REP-UK, V6, DOI 10.1038/srep29545
   Taesik Na, 2019, 2019 IEEE INT JOINT
   Wang Y., 2018, IEEE T NEUR NET LEAR
   Xie J., 2012, ADV NEURAL INFORM PR, V25, DOI DOI 10.1109/AGRO-GEOINFORMATICS.2012.6311605
   Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206
   Zhussip Magauiya, 2018, CORR
NR 29
TC 0
Z9 0
U1 0
U2 0
PY 2020
DI 10.1109/ijcnn48605.2020.9207274
UT WOS:000626021405048
DA 2023-11-16
ER

PT C
AU Saunders, DJ
   Siegelmann, HT
   Kozma, R
   Ruszinkó, M
AF Saunders, Daniel J.
   Siegelmann, Hava T.
   Kozma, Robert
   Ruszinko, Miklos
GP IEEE
TI STDP Learning of Image Patches with Convolutional Spiking Neural
   Networks
SO 2018 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUL 08-13, 2018
CL Rio de Janeiro, BRAZIL
DE Spiking Neural Networks; STDP; Convolution; Machine Learning;
   Unsupervised Learning
AB Spiking neural networks are motivated from principles of neural systems and may possess unexplored advantages in the context of machine learning. A class of convolutional spiking neural networks is introduced, trained to detect image features with an unsupervised, competitive learning mechanism. Image features can be shared within subpopulations of neurons, or each may evolve independently to capture different features in different regions of input space. We analyze the time and memory requirements of learning with and operating such networks. The MNIST dataset is used as an experimental testbed, and comparisons are made between the performance and convergence speed of a baseline spiking neural network.
C1 [Saunders, Daniel J.; Siegelmann, Hava T.; Kozma, Robert] Univ Massachusetts Amherst, Coll Informat & Comp Sci, 140 Governors Dr, Amherst, MA 01003 USA.
   [Ruszinko, Miklos] Hungarian Acad Sci, Alfred Renyi Inst Math, 13-15 Realtanoda Utca, H-1053 Budapest, Hungary.
RP Saunders, DJ (corresponding author), Univ Massachusetts Amherst, Coll Informat & Comp Sci, 140 Governors Dr, Amherst, MA 01003 USA.
EM djsaunde@cs.umass.edu; hava@cs.umass.edu; rkozma@cs.umass.edu;
   ruszinko.miklos@renyi.mta.hu
CR [Anonymous], 1995, BACKPROPAGATION THEO
   [Anonymous], 2016, ARXIV161101421
   [Anonymous], 2016, NAT METHODS, DOI DOI 10.1038/nmeth.3707
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Diehl P., 2015, FRONTIERS COMPUTATIO
   Diehl P.U., 2015, 2015 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2015.7280696
   Gerstner W., 2002, SPIKING NEURON MODEL
   Goodman D. F. M., 2009, FRONTIERS COMPUTATIO
   Hazan H., 2018, 2018 INT JOINT C NEU, P1
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee SW, 2015, PLOS BIOL, V13, DOI 10.1371/journal.pbio.1002137
   Liu DQ, 2017, NEUROCOMPUTING, V249, P212, DOI 10.1016/j.neucom.2017.04.003
   Maass W, 1996, NEURAL COMPUT, V8, P1, DOI 10.1162/neco.1996.8.1.1
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
NR 14
TC 8
Z9 8
U1 2
U2 3
PY 2018
UT WOS:000585967404131
DA 2023-11-16
ER

PT C
AU Tang, DL
   Kubota, N
AF Tang, Dalai
   Kubota, Naoyuki
BE Wong, KW
   Mendis, BSU
   Bouzerdoum, A
TI Human Localization by Fuzzy Spiking Neural Network Based on
   Informationally Structured Space
SO NEURAL INFORMATION PROCESSING: THEORY AND ALGORITHMS, PT I
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 17th International Conference on Neural Information Processing
CY NOV 22-25, 2010
CL Sydney, AUSTRALIA
DE Robot Partners; Neural Networks; Sensor Networks; Human Localization
AB This paper analyzes the performance of the human localization by a spiking neural network in informationally structured space based on sensor networks. First, we discuss the importance of information structuralization. Next, we apply a spiking neural network to extract the human position in a room equipped with sensor network devices. Next, we propose how to update the base value as a method of preprocessing to generate input values to the spiking neurons, and the learning method of the spiking neural network based on the time series of measured data. Finally, we show several experimental results, and discuss the effectiveness of the proposed method.
C1 [Tang, Dalai; Kubota, Naoyuki] Tokyo Metropolitan Univ, Dept Syst Design, Tokyo 1910065, Japan.
RP Tang, DL (corresponding author), Tokyo Metropolitan Univ, Dept Syst Design, 6-6 Asahigaoka, Tokyo 1910065, Japan.
EM tang@sd.tmu.ac.jp; kubota@tmu.ac.jp
CR Anderson J.A., 1988, NEUROCOMPUTING
   [Anonymous], 2010, P IEEE WORLD C COMP
   CALLAGHAN V, 2005, BOOK ENTITLED INTELL, P389
   Corderio C., 2006, AD HOC SENSOR NETWOR
   Gerstner W, 1998, PULSED NEURAL NETWORKS, P3
   ISHIGURO H, 2005, P WORKSH NETW ROB SY
   KEMMOTSU K, 2005, P 1 JAP KOR JOINT S
   Khemapech I., 2005, P 6 ANN POSTGRADUATE
   KHEMAPECH I, 2005, P 6 ANN POSTGRAD S C
   Kubota Naoyuki, 2009, RO-MAN 2009 - The 18th IEEE International Symposium on Robot and Human Interactive Communication, P346, DOI 10.1109/ROMAN.2009.5326082
   KUBOTA N, 2006, P SICE ICCAS 2006, P4861
   Kubota N., 2009, P ICROS SICE INT JOI
   Kubota N, 2006, INT J COMPUT SCI NET, V6, P19
   LITTLE WA, 1975, BEHAV BIOL, V14, P115, DOI 10.1016/S0091-6773(75)90122-4
   MAASS W, 1999, PULSED NEURAL RURAL
   MORIOKA K, 2004, P 2004 IEEE RSJ INT, V1, P199
   Obo T., 2010, P 2010 IEEE WORLD C, P2215
   REMAGNINO P, 2005, BOOK ENTITLED AMBIEN, P1
   SATOH I, 2006, INT J DIGITAL LIB
   Satomi M., 2009, IEEE S SERIES COMPUT
NR 20
TC 5
Z9 5
U1 0
U2 0
PY 2010
VL 6443
BP 25
EP 32
UT WOS:000290457000004
DA 2023-11-16
ER

PT C
AU Kozdon, K
   Bentley, P
AF Kozdon, Katarzyna
   Bentley, Peter
GP IEEE
TI Wide Learning Using an Ensemble of Biologically-Plausible Spiking Neural
   Networks for Unsupervised Parallel Classification of Spatio-Temporal
   Patterns
SO 2017 IEEE SYMPOSIUM SERIES ON COMPUTATIONAL INTELLIGENCE (SSCI)
DT Proceedings Paper
CT IEEE Symposium Series on Computational Intelligence (IEEE SSCI)
CY NOV 27-DEC 01, 2017
CL Honolulu, HI
DE spiking neural network; spike timing dependent plasticity; pattern
   detection; parallel computing; ensemble network
ID NEURONS; CORTEX; INFORMATION; MODEL
AB Spiking neural networks have been previously used to perform tasks such as object recognition without supervision. One of the concerns relating to the spiking neural networks is their speed of operation and the number of iterations necessary to train and use the network. Here, we propose a biologically plausible model of a spiking neural network which is used in multiple, separately trained copies to process subsets of data in parallel. This ensemble of networks is tested by applying it to the task of unsupervised classification of spatio-temporal patterns. Results show that despite different starting weights and independent training, the networks produce highly similar spiking patterns in response to the same class of inputs, enabling classification with fast training time.
C1 [Kozdon, Katarzyna; Bentley, Peter] UCL, Dept Comp Sci, London, England.
RP Kozdon, K (corresponding author), UCL, Dept Comp Sci, London, England.
EM k.kozdon@cs.ucl.ac.uk; p.bentley@cs.ucl.ac.uk
CR Abbott L. F., LAPICQUES INTRO INTE
   BACKUS J, 1978, COMMUN ACM, V21, P613, DOI 10.1145/359576.359579
   Bair W, 1996, NEURAL COMPUT, V8, P1185, DOI 10.1162/neco.1996.8.6.1185
   BEAULIEU C, 1992, CEREB CORTEX, V2, P295, DOI 10.1093/cercor/2.4.295
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   D'amour J., 2015, NEURON
   Diehl P. U., 2016, LEARNING INFERRING R
   Florian RV, 2007, NEURAL COMPUT, V19, P1468, DOI 10.1162/neco.2007.19.6.1468
   Hebb DO, 1950, J CLIN PSYCHOL, V6, P307
   Hu J, 2013, NEURAL COMPUT, V25, P450, DOI 10.1162/NECO_a_00395
   Iakymchuk T., 2011, EURASIP J IMAGE VIDE
   Kelsom C, 2013, CELL BIOSCI, V3, DOI 10.1186/2045-3701-3-19
   Kheradpisheh S. R., STDP BASED SPIKING D
   Krogh A., 1995, Advances in Neural Information Processing Systems 7, P231
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Markram H., 1997, SCIENCE 80, V275
   Mel B. W., 2015, FRONT COMPUT NEUROSC, V9
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Nere A, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0036958
   Painkras E, 2013, IEEE J SOLID-ST CIRC, V48, P1943, DOI 10.1109/JSSC.2013.2259038
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Reinagel P, 2000, J NEUROSCI, V20, P5392, DOI 10.1523/JNEUROSCI.20-14-05392.2000
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Sahara S, 2012, J NEUROSCI, V32, P4755, DOI 10.1523/JNEUROSCI.6412-11.2012
   Shadlen MN, 1998, J NEUROSCI, V18, P3870
   Shim Y, 2016, PLOS COMPUT BIOL, V12, DOI 10.1371/journal.pcbi.1005137
   Troyer TW, 1997, NEURAL COMPUT, V9, P971, DOI 10.1162/neco.1997.9.5.971
   Wehrens R, 2007, J STAT SOFTW, V21, P1
   Yang Z., NEUROMORPHIC DEPTH M
   Yeo BTT, 2011, J NEUROPHYSIOL, V106, P1125, DOI 10.1152/jn.00338.2011
   Yu Q, 2016, IEEE T NEUR NET LEAR, V27, P621, DOI 10.1109/TNNLS.2015.2416771
   Zagoruyko Sergey, 2016, BRIT MACHINE VISION, P5
NR 32
TC 1
Z9 1
U1 0
U2 2
PY 2017
BP 3183
EP 3190
UT WOS:000428251403036
DA 2023-11-16
ER

PT J
AU Li, HG
   Yu, P
   Xia, TS
AF Li, Hongge
   Yu, Pan
   Xia, Tongsheng
TI Efficient hybrid neural network for spike sorting
SO JOURNAL OF SYSTEMS ENGINEERING AND ELECTRONICS
DT Article
DE neural network; spike sorting; implantable microsystem
ID INTERFACES; ALGORITHMS
AB Artificial neural network has been used successfully to develope the automatic spike extraction. In order to address some of the problems before the wireless transmission of the implantable chip, the automatic spike sorting method with low complexity and high efficiency is proposed based on the hybrid neural network with the principal component analysis network (PCAN) and normal boundary response (NBR) self-organizing mapping (SOM) network classifier. An automatic PCAN technique is used to reduce the dimension and eliminate the correlation of the spike signal. The NBR-SOM network performs the spike sorting challenge and improves the classification performance. The experimental results show that based on the hybrid neural network, the spike sorting method achieves the accuracy above 97.91% with signals containing five classes. The proposed NBR-SOM network classifier is to further improve the stability and effectiveness of the classification system.
C1 [Li, Hongge; Yu, Pan; Xia, Tongsheng] Beihang Univ, Sch Elect Informat Engn, Beijing 100191, Peoples R China.
RP Li, HG (corresponding author), Beihang Univ, Sch Elect Informat Engn, Beijing 100191, Peoples R China.
EM honygeli@buaa.edu.cn; yupanchina@hotmail.com; xiatongsheng@buaa.edu.cn
CR Bouguessa M, 2006, PATTERN RECOGN LETT, V27, P1419, DOI 10.1016/j.patrec.2006.01.015
   Cho J, 2007, NEURAL NETWORKS, V20, P274, DOI 10.1016/j.neunet.2006.12.002
   Clausen J, 2009, NATURE, V457, P1080, DOI 10.1038/4571080a
   Fiori S, 2000, NEURAL PROCESS LETT, V11, P209, DOI 10.1023/A:1009663626444
   García P, 1998, J NEUROSCI METH, V82, P59, DOI 10.1016/S0165-0270(98)00035-1
   Gibson S, 2008, IEEE ENG MED BIO, P5015, DOI 10.1109/IEMBS.2008.4650340
   Gosselin B, 2009, IEEE T NEUR SYS REH, V17, P346, DOI 10.1109/TNSRE.2009.2018103
   Horton PM, 2007, J NEUROSCI METH, V160, P52, DOI 10.1016/j.jneumeth.2006.08.013
   Kanungo T, 2002, IEEE T PATTERN ANAL, V24, P881, DOI 10.1109/TPAMI.2002.1017616
   Kohonen T., 1995, SELF ORG MAPS
   Kutlu Y, 2009, EXPERT SYST APPL, V36, P7567, DOI 10.1016/j.eswa.2008.09.052
   Li CH, 2009, EXPERT SYST APPL, V36, P3208, DOI 10.1016/j.eswa.2008.01.014
   Li H, 2011, ELECTRON LETT, V47, P367, DOI 10.1049/el.2010.3711
   Li HG, 2010, MICROELECTRON RELIAB, V50, P273, DOI 10.1016/j.microrel.2009.09.013
   Li JW, 2007, J SYST ENG ELECTRON, V18, P377, DOI 10.1016/S1004-4132(07)60101-7
   Moosung Chae, 2008, 2008 IEEE International Solid-State Circuits Conference - Digest of Technical Papers, P146
   Obeid I, 2004, IEEE T BIO-MED ENG, V51, P905, DOI 10.1109/TBME.2004.826683
   OJA E, 1992, NEURAL NETWORKS, V5, P927, DOI 10.1016/S0893-6080(05)80089-9
   Quiroga RQ, 2004, NEURAL COMPUT, V16, P1661, DOI 10.1162/089976604774201631
   Rizk M, 2007, J NEURAL ENG, V4, P309, DOI 10.1088/1741-2560/4/3/016
   Samsonova EV, 2006, NEURAL NETWORKS, V19, P935, DOI 10.1016/j.neunet.2006.05.003
   Schwartz AB, 2006, NEURON, V52, P205, DOI 10.1016/j.neuron.2006.09.019
   Simone F., 2000, NEURAL PROCESS LETT, V11, P209
   Wang ZG, 2009, PROG NAT SCI-MATER, V19, P1261, DOI 10.1016/j.pnsc.2009.02.005
   Wise KD, 2004, P IEEE, V92, P76, DOI 10.1109/JPROC.2003.820544
   Yang Z, 2009, NEUROCOMPUTING, V73, P412, DOI 10.1016/j.neucom.2009.07.013
   Zviagintsev A, 2006, J NEURAL ENG, V3, P35, DOI 10.1088/1741-2560/3/1/004
NR 27
TC 2
Z9 2
U1 0
U2 21
PD FEB
PY 2013
VL 24
IS 1
BP 157
EP 164
DI 10.1109/JSEE.2013.00020
UT WOS:000315160200020
DA 2023-11-16
ER

PT C
AU Dai, JH
   Liu, XC
   Zhang, SM
   Zhang, HJ
   Yi, Y
   Zheng, XX
AF Dai, Jianhua
   Liu, Xiaochun
   Zhang, Shaomin
   Zhang, Huaijian
   Yi, Yu
   Zheng, Xiaoxiang
TI Neuronal Spike Sorting based on 2-stage RBF Networks
SO FGCN: PROCEEDINGS OF THE 2008 SECOND INTERNATIONAL CONFERENCE ON FUTURE
   GENERATION COMMUNICATION AND NETWORKING, VOLS 1 AND 2
DT Proceedings Paper
CT 2nd International Conference on Future Generation Communications and
   Networking
CY DEC 13-15, 2008
CL Hainan, PEOPLES R CHINA
ID ALGORITHM
AB In this paper, 2-stage Radial Basis Function (ABF) Network method is used for neural spike sorting. Firstly, raw signals are obtained from Neural Signal Simulator, and added while noise ranged from -10dB to -40dB. Secondly, spikes are detected out with matched filter from signals. Lastly, 2-stage RBF networks are constructed and the spikes are sorted using RBF networks. The experiments show that 2-stage RBF network is an effective tool for neural spike sorting.
C1 [Dai, Jianhua; Liu, Xiaochun; Zhang, Shaomin; Zhang, Huaijian; Yi, Yu; Zheng, Xiaoxiang] Zhejiang Univ, Qiushi Acad Adv Studies, Hangzhou 310027, Peoples R China.
RP Dai, JH (corresponding author), Zhejiang Univ, Qiushi Acad Adv Studies, Hangzhou 310027, Peoples R China.
EM jhdai@zju.edu.cn
CR CHEN S, 1991, IEEE T NEURAL NETWOR, V2, P302, DOI 10.1109/72.80341
   Chen S, 1999, IEEE T NEURAL NETWOR, V10, P1239, DOI 10.1109/72.788663
   DAI JH, 2008, P 2008 IEEE INT C GR, P172
   Lewicki MS, 1998, NETWORK-COMP NEURAL, V9, pR53, DOI 10.1088/0954-898X/9/4/001
   Mulgrew B, 1996, IEEE SIGNAL PROC MAG, V13, P50, DOI 10.1109/79.487041
   PFURTSCHELLER G, 1978, ELECTROEN CLIN NEURO, V44, P243, DOI 10.1016/0013-4694(78)90272-9
   SING K, 2003, P 2003 C CONV TECHN, V2, P841
NR 7
TC 0
Z9 0
U1 0
U2 1
PY 2008
BP 999
EP 1002
UT WOS:000265567100214
DA 2023-11-16
ER

PT C
AU Yao, YL
   Yu, Q
   Wang, LB
   Dang, JW
AF Yao, Yanli
   Yu, Qiang
   Wang, Longbiao
   Dang, Jianwu
GP IEEE
TI An integrated system for robust gender classification with convolutional
   restricted Boltzmann machine and spiking neural network
SO 2019 IEEE SYMPOSIUM SERIES ON COMPUTATIONAL INTELLIGENCE (IEEE SSCI
   2019)
DT Proceedings Paper
CT IEEE Symposium Series on Computational Intelligence (SSCI)
CY DEC 06-09, 2019
CL Xiamen, PEOPLES R CHINA
DE Spiking Neural Network; Convolutional restricted Boltzmann machine;
   gender classification
AB Different from traditional artificial neural networks (ANNs), spiking neural networks (SNNs) represent and transfer information in spikes, which are considered more like human brain. SNNs contain time information, which make them more suitable for addressing time-structured speech signals. However, it still remains challenging for spiking neural network (SNN) to implement classification tasks based on speech due to the lack of a proper encoding. In this paper, an integrated spiking neural network is proposed to perform the gender classification task. The whole system consists of three functional parts, including encoding, learning and readout. As convolutional restricted Boltzmann machine (CRBM) has shown outstanding capability for unsupervised learning of auditory features, we adopt it in this paper as a feature extractor, followed by a spike-latency encoding layer that converts the feature values into spike times. Then these spikes are processed by the spiking neural networks with the tempotron learning rule. We use the TIMIT database to evaluate the performance of our system. Our results show that the as-proposed system is robust for gender classification across a wide range of noise levels.
C1 [Yao, Yanli; Yu, Qiang; Wang, Longbiao; Dang, Jianwu] Tianjin Univ, Sch Comp Sci & Technol, Tianjin Key Lab Cognit Comp & Applicat, Tianjin, Peoples R China.
RP Yao, YL (corresponding author), Tianjin Univ, Sch Comp Sci & Technol, Tianjin Key Lab Cognit Comp & Applicat, Tianjin, Peoples R China.
EM yaoyanli@tju.edu.cn; yuqiang@tju.edu.cn; longbiao_wang@tju.edu.cn;
   jdang@jaist.ac.jp
CR [Anonymous], 1980, READINGS SPEECH RECO
   [Anonymous], 2018, IEEE T CYBERNETICS
   Butts DA, 2007, NATURE, V449, P92, DOI [10.1038/nature06105, 10.1038/natureO6105]
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Garofolo J.S., 1993, 93 NASA STIRECON
   Gerstner W., 2002, SPIKING NEURON MODEL
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Kasabov NK, 2014, NEURAL NETWORKS, V52, P62, DOI 10.1016/j.neunet.2014.01.006
   Kusy B, 2009, 2009 INTERNATIONAL CONFERENCE ON INFORMATION PROCESSING IN SENSOR NETWORKS (IPSN 2009), P109
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lee S, 2009, 2009 IEEE INTERNATIONAL CONFERENCE ON NETWORK INFRASTRUCTURE AND DIGITAL CONTENT, PROCEEDINGS, P609, DOI 10.1109/ICNIDC.2009.5360944
   Lestienne R, 2001, PROG NEUROBIOL, V65, P545, DOI 10.1016/S0301-0082(01)00019-3
   Rospars JP, 2000, BIOSYSTEMS, V58, P133, DOI 10.1016/S0303-2647(00)00116-7
   Sailor HB, 2016, IEEE-ACM T AUDIO SPE, V24, P2341, DOI 10.1109/TASLP.2016.2607341
   Teh YW, 2001, ADV NEUR IN, V13, P908
   Tu EM, 2017, IEEE T NEUR NET LEAR, V28, P1305, DOI 10.1109/TNNLS.2016.2536742
   VanRullen R, 2005, TRENDS NEUROSCI, V28, P1, DOI 10.1016/j.tins.2004.10.010
   Verstraeten D, 2005, INFORM PROCESS LETT, V95, P521, DOI 10.1016/j.ipl.2005.05.019
   Yu Q., 2019, ARXIV190201094
   Yu Q., 2017, NEUROMORPHIC COGNITI
   Yu Q, 2019, INT CONF ACOUST SPEE, P890, DOI 10.1109/ICASSP.2019.8682963
   Yu Q, 2014, NEUROCOMPUTING, V138, P3, DOI 10.1016/j.neucom.2013.06.052
   Yu Q, 2013, IEEE T NEUR NET LEAR, V24, P1539, DOI 10.1109/TNNLS.2013.2245677
   2009, NATURAL IMAGE STAT, V39, P93
NR 27
TC 1
Z9 1
U1 0
U2 3
PY 2019
BP 2348
EP 2353
UT WOS:000555467202061
DA 2023-11-16
ER

PT C
AU Kuroe, Y
   Iima, H
AF Kuroe, Yasuaki
   Iima, Hitoshi
GP IEEE
TI <bold>A Learning Method for Synthesizing Spiking Neural
   Oscillators</bold>
SO 2006 IEEE INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORK PROCEEDINGS,
   VOLS 1-10
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT IEEE International Joint Conference on Neural Network
CY JUL 16-21, 2006
CL Vancouver, CANADA
AB In the biological systems there are numerous examples of autonomously generated periodic activities. Several different periodic patterns are generated simultaneously in a living body. It is known that in biological systems there are specific neurons which generate such periodic patterns. In spiking neural networks the information processing is carried out by spike trains in a manner similar to the generic biological neurons. This paper presents a method for synthesis of neural oscillators by spiking neural networks. We propose a learning method for synthesizing spiking neural networks which generate desired periodic spike trains with specified spike emission times. A method of stability analysis of the generated periodic spike trains is also discussed.
C1 [Kuroe, Yasuaki] Kyoto Inst Technol, Ctr Informat Sci, Sakyo Ku, Kyoto 6068585, Japan.
   [Iima, Hitoshi] Kyoto Inst Technol, Dept Informat Sci, Kyoto 6068585, Japan.
RP Kuroe, Y (corresponding author), Kyoto Inst Technol, Ctr Informat Sci, Sakyo Ku, Kyoto 6068585, Japan.
EM kuroe@kit.ac.jp; iima@kit.ac.jp
CR DOYA K, 1989, NEURAL NETWORKS, V2, P375, DOI 10.1016/0893-6080(89)90022-1
   GERSTNER W, 1993, ADV NEURAL INFORM PR, V6, P363
   GUCKENHEIMER J, 1988, NONLINEAR OSCILLATIO
   Kimura H, 1998, 1998 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS - PROCEEDINGS, VOLS 1-3, P50, DOI 10.1109/IROS.1998.724595
   LUENBERGER DG, 1973, INTRO LINEAR NONLINE, P194
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   MASS W, 1998, PULSED NEURAL NETS
   MATSUOKA K, 1987, BIOL CYBERN, V56, P345, DOI 10.1007/BF00319514
   SELVARATNAM K, 2000, T I SYSTEMS CONTROL, V44, P95
NR 9
TC 3
Z9 3
U1 0
U2 1
PY 2006
BP 3882
EP +
UT WOS:000245125907017
DA 2023-11-16
ER

PT C
AU Sandhu, HS
   Fang, LP
   Guan, L
AF Sandhu, Harmanjot Singh
   Fang, Liping
   Guan, Ling
BE LeonGarcia, A
   Lenort, R
   Holman, D
   Stas, D
   Krutilova, V
   Wicher, P
   Caganova, D
   Spirkova, D
   Golej, J
   Nguyen, K
TI Day-Ahead Electricity Spike Price Forecasting Using a Hybrid Neural
   Network-Based Method
SO SMART CITY 360
SE Lecture Notes of the Institute for Computer Sciences Social Informatics
   and Telecommunications Engineering
DT Proceedings Paper
CT International Conference on Big Data and Analytics for Smart Cities
   (BigDASC)
CY OCT 13-14, 2015
CL Toronto, CANADA
DE Neural network; Price spikes; Day-ahead forecasting; Electricity market
ID MARKET
AB A hybrid neural network-based method is presented to predict day-ahead electricity spike prices in a deregulated electricity market. First, prediction of day-ahead electricity prices is carried out by a neural network along with pre-processing data mining techniques. Second, a classifier is used to separate the forecasted prices into normal and spike prices. Third, a second neural network is trained over spike hours with selected features and is used to forecast day-ahead spike prices. Forecasted spike and normal prices are combined to produce the complete day-ahead hourly electricity price forecasting. Numerical experiments demonstrate that the proposed method can significantly improve the forecasting accuracy.
C1 [Sandhu, Harmanjot Singh; Fang, Liping] Ryerson Univ, Dept Mech & Ind Engn, Toronto, ON, Canada.
   [Guan, Ling] Ryerson Univ, Dept Elect & Comp Engn, Toronto, ON, Canada.
RP Sandhu, HS (corresponding author), Ryerson Univ, Dept Mech & Ind Engn, Toronto, ON, Canada.
EM harmanjotsingh.sandh@ryerson.ca; lfang@ryerson.ca; lguan@ee.ryerson.ca
CR Aggarwal SK, 2009, ELECTR POW COMPO SYS, V37, P495, DOI 10.1080/15325000802599353
   Amjady N, 2010, ELECTR POW SYST RES, V80, P318, DOI 10.1016/j.epsr.2009.09.015
   Azmira WARI, 2013, PROC INT CONF COMP, P103
   Baez-Rivera Y, 2006, NORTH AMER POW SYMP, P143, DOI 10.1109/NAPS.2006.360137
   Christensen TM, 2012, INT J FORECASTING, V28, P400, DOI 10.1016/j.ijforecast.2011.02.019
   Eichler M, 2014, J ENERGY MARKETS, V7, P55, DOI 10.21314/JEM.2014.104
   Hong YY, 2002, IEE P-GENER TRANSM D, V149, P621, DOI 10.1049/ip-gtd:20020371
   Lu X, 2005, ELECTR POW SYST RES, V73, P19, DOI 10.1016/j.epsr.2004.06.002
   Mandal P., 2012, 2012 IEEE Power & Energy Society General Meeting. New Energy Horizons - Opportunities and Challenges, DOI 10.1109/PESGM.2012.6345461
   Mandal P, 2006, NORTH AMER POW SYMP, P137, DOI 10.1109/NAPS.2006.360135
   Mandal P, 2009, IEEE T IND APPL, V45, P1888, DOI 10.1109/TIA.2009.2027542
   Sandhu H. S., 2014, P 11 INT C SERV SYST, P1024
   Shrivastava N. A., 2011, P INT C EN AUT SIGN, P1
   Veit DJ, 2006, INT J MANAG SCI ENG, V1, P83, DOI 10.1080/17509653.2006.10671000
   Wu W, 2006, LECT NOTES ARTIF INT, V4093, P205
   Zareipour H, 2007, IEEE T POWER SYST, V22, P1782, DOI 10.1109/TPWRS.2007.907979
   Zareipour H, 2007, ENERG POLICY, V35, P4739, DOI 10.1016/j.enpol.2007.04.006
   Zhang GP, 2005, EUR J OPER RES, V160, P501, DOI 10.1016/j.ejor.2003.08.037
   Zhao JH, 2007, IET GENER TRANSM DIS, V1, P647, DOI 10.1049/iet-gtd:20060217
   Zhao JH, 2007, IEEE T POWER SYST, V22, P376, DOI 10.1109/TPWRS.2006.889139
NR 20
TC 0
Z9 0
U1 0
U2 8
PY 2016
VL 166
BP 431
EP 442
DI 10.1007/978-3-319-33681-7_36
UT WOS:000393328700036
DA 2023-11-16
ER

PT J
AU Bodyanskiy, YV
   Vynokurova, EA
   Dolotov, AI
AF Bodyanskiy, Ye. V.
   Vynokurova, E. A.
   Dolotov, A. I.
TI Self-Learning Cascade Spiking Neural Network for Fuzzy Clustering Based
   on Group Method of Data Handling
SO JOURNAL OF AUTOMATION AND INFORMATION SCIENCES
DT Article
DE fuzzy clustering problem; fuzzy spiking neural network; architecture and
   learning algorithm; generalizing neural network; spiking neuron as a
   nonlinear dynamic system; hybrid architecture; high dimension problems
ID NEURONS
AB The fuzzy clustering problem in the presence of overlapping classes is considered. To solve the problem, it is introduced the architecture and learning algorithm of a fuzzy spiking neural network, generalizing neural networks of the third generation, which at present are developing intensively and have a number of advantages over traditional computational intelligence systems. The spiking neuron is described as a nonlinear dynamic system, which simplifies the hardware implementation. For problems with high dimension of input vectors-images it is proposed to use the hybrid architecture, which is based on a combination of cascade and GMDH-neural networks with self-learning cascade spiking neural networks, used as nodes, and ensures the increased speed of information processing.
C1 [Bodyanskiy, Ye. V.; Vynokurova, E. A.; Dolotov, A. I.] Kharkov Natl Univ Radioelect, Kharkov, Ukraine.
RP Bodyanskiy, YV (corresponding author), Kharkov Natl Univ Radioelect, Kharkov, Ukraine.
CR Amin H. H., 2011, SPIKING NEURAL NETWO
   [Anonymous], INF TECHNOL KNOWL
   [Anonymous], STUDIES COMPUTATIONA, DOI DOI 10.1007/978-3-319-05029-4_7
   [Anonymous], 2011, P 4 INT WORKSH IND M
   [Anonymous], 1998, PULSED NEURAL NETWOR
   Bodyanskiy Y, 2008, PRO BIENN BALT EL C, P213, DOI 10.1109/BEC.2008.4657517
   Bodyanskiy Ye., 2010, INT BOOK SERIES INFO, V20, P17
   Bodyanskiy Ye., 2012, P 13 INT BIENN BALT, P207
   Bodyanskiy Ye., 2009, IMAGE PROCESSING, P357
   Bodyanskiy Ye, 2008, SCI P RIG TU INF TEC, P27
   Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   Gerstner W., 2002, SPIKING NEURON MODEL
   Ivakhnenko A. G., 1985, MODELING NOISE IMMUN
   Ivakhnenko A.G., 1975, LONG TERM FORECASTIN
   IVAKHNENKO AG, 1971, IEEE T SYST MAN CYB, VSMC1, P364, DOI 10.1109/TSMC.1971.4308320
   KOLMOGOROV AN, 1985, MATH MECH
   Lebiere C., 1990, ADV NEURAL INFORM PR, P524
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Natschlager T, 1998, NETWORK-COMP NEURAL, V9, P319, DOI 10.1088/0954-898X/9/3/003
   Ye Bodyanskiy, 2009, INT BOOK SERIES INFO, V13, P19
NR 20
TC 8
Z9 8
U1 0
U2 13
PY 2013
VL 45
IS 3
BP 23
EP 33
DI 10.1615/JAutomatInfScien.v45.i3.30
UT WOS:000320968100003
DA 2023-11-16
ER

PT C
AU Du, CL
   Nan, Y
   Yan, R
AF Du, Chunlin
   Nan, Ying
   Yan, Rui
BE Sun, M
   Gao, H
TI Spike-based Learning Rules for Face Recognition
SO 2017 6TH DATA DRIVEN CONTROL AND LEARNING SYSTEMS (DDCLS)
DT Proceedings Paper
CT 6th IEEE Data Driven Control and Learning Systems Conference (DDCLS)
CY MAY 26-27, 2017
CL Chongqing, PEOPLES R CHINA
DE Face Recognition; HMAX; Tempotron; Spiking Neural Networks (SNNs)
ID OBJECT RECOGNITION; NEURON
AB This paper proposes a biologically plausible network architecture with spiking neurons for face recognition. This network consists of three parts: feature extraction, encoding and classification. Firstly, HMAX model with four layers (C1S1-C2-S2) is used to extract face features. The proposed feature extraction method can keep selectivity invariance and scale invariance. The next important part is to encode features to suitable spike trains for spiking neural networks. In the last part, the improved Tempotron learning rule is chosen to train the spiking neural networks with reduced computation and increased fault tolerance. In order to demonstrate the performance of spiking neural networks, four databases are tested in the experiment: Yale, Extend Yale B, ORL, and FERET.
C1 [Du, Chunlin; Nan, Ying; Yan, Rui] Sichuan Univ, Coll Comp Sci, Neuromorph Comp Res Ctr, Chengdu 610065, Sichuan, Peoples R China.
RP Du, CL (corresponding author), Sichuan Univ, Coll Comp Sci, Neuromorph Comp Res Ctr, Chengdu 610065, Sichuan, Peoples R China.
EM cdu@stu.scu.edu.cn; ryan@scu.edu.cn
CR Berry MJ, 1998, J NEUROSCI, V18, P2200
   Borst A, 1999, NAT NEUROSCI, V2, P947, DOI 10.1038/14731
   Brader JM, 2007, NEURAL COMPUT, V19, P2881, DOI 10.1162/neco.2007.19.11.2881
   Delorme A, 1999, NEUROCOMPUTING, V26-7, P989, DOI 10.1016/S0925-2312(99)00095-8
   Delorme A, 2001, NEURAL NETWORKS, V14, P795, DOI 10.1016/S0893-6080(01)00049-1
   Gawne TJ, 1996, J NEUROPHYSIOL, V76, P1356, DOI 10.1152/jn.1996.76.2.1356
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837
   Kempter R, 1999, ADV NEUR IN, V11, P125
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Panzeri S, 2010, TRENDS NEUROSCI, V33, P111, DOI 10.1016/j.tins.2009.12.001
   Perrinet L, 2004, IEEE T NEURAL NETWOR, V15, P1164, DOI 10.1109/TNN.2004.833303
   Riesenhuber M, 1999, NAT NEUROSCI, V2, P1019, DOI 10.1038/14819
   Serre T, 2005, PROC CVPR IEEE, P994
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Tsukada M, 2005, BIOL CYBERN, V92, P139, DOI 10.1007/s00422-004-0523-1
   Uzzell VJ, 2004, J NEUROPHYSIOL, V92, P780, DOI 10.1152/jn.01171.2003
   Van Rullen R, 2001, NEURAL COMPUT, V13, P1255, DOI 10.1162/08997660152002852
NR 20
TC 0
Z9 0
U1 0
U2 4
PY 2017
BP 536
EP 541
UT WOS:000426631900093
DA 2023-11-16
ER

PT C
AU Tang, TQ
   Luo, R
   Li, BX
   Li, H
   Wang, Y
   Yang, HZ
AF Tang, Tianqi
   Luo, Rong
   Li, Boxun
   Li, Hai
   Wang, Yu
   Yang, Huazhong
GP IEEE
TI Energy Efficient Spiking Neural Network Design with RRAM Devices
SO 2014 14TH INTERNATIONAL SYMPOSIUM ON INTEGRATED CIRCUITS (ISIC)
SE International Symposium on Integrated Circuits
DT Proceedings Paper
CT 14 International Symposium on Integrated Circuits (ISIC)
CY DEC 10-12, 2014
CL Marina Bay Sands, SINGAPORE
AB The brain-inspired neural networks have demonstrated great potential in big data analysis. The spiking neural network (SNN), which encodes the real world data into spike trains, promises great performance in computational ability and energy efficiency. Moreover, it is much more biologically plausible than the traditional artificial neural network (ANN), which keeps the input data in its original form. In this paper, we introduce an RRAM-based energy efficient implementation of STDP-based spiking neural network cascaded with ANN classifier. The recognition accuracy and power consumption are compared between SNN and traditional three-layer ANN. The experiments on the MNIST database demonstrate that the proposed RRAM-based spiking neural network requires only 14% of power consumption compared with RRAM-based artificial neural network with a slight accuracy decay (similar to 2%).
C1 [Tang, Tianqi; Luo, Rong; Li, Boxun; Wang, Yu; Yang, Huazhong] Tsinghua Univ, Dept EE, Tsinghua Natl Lab Informat Sci & Technol TNList, Beijing, Peoples R China.
   [Li, Hai] Univ Pittsburgh, Dept ECE, Pittsburgh, PA USA.
RP Tang, TQ (corresponding author), Tsinghua Univ, Dept EE, Tsinghua Natl Lab Informat Sci & Technol TNList, Beijing, Peoples R China.
EM yu-wang@mail.tsinghua.edu.cn
CR Chang LL, 2003, P IEEE, V91, P1860, DOI 10.1109/JPROC.2003.818336
   Doya K., 1989, IJCNN: International Joint Conference on Neural Networks (Cat. No.89CH2765-6), P27, DOI 10.1109/IJCNN.1989.118555
   Hu M, 2012, DES AUT CON, P498
   Indiveri G, 2003, PROCEEDINGS OF THE 2003 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL IV, P820
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Kuzum D, 2012, NANO LETT, V12, P2179, DOI 10.1021/nl201040y
   Li BX, 2013, I SYMPOS LOW POWER E, P242, DOI 10.1109/ISLPED.2013.6629302
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Masquelier T, 2007, PLOS COMPUT BIOL, V3, P247, DOI 10.1371/journal.pcbi.0030031
   Niu DM, 2013, ICCAD-IEEE ACM INT, P17, DOI 10.1109/ICCAD.2013.6691092
   Olshausen BA, 2003, J COGNITIVE NEUROSCI, V15, P154, DOI 10.1162/089892903321107891
   Querlioz D, 2012, IEEE INT SYMP NANO, P203
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Wulf W. A., 1995, Computer Architecture News, V23, P20, DOI 10.1145/216585.216588
   Yang JJS, 2013, NAT NANOTECHNOL, V8, P13, DOI [10.1038/nnano.2012.240, 10.1038/NNANO.2012.240]
   Yu S, 2013, FRONT SYST NEUROSCI, V7, DOI 10.3389/fnsys.2013.00042
NR 16
TC 5
Z9 5
U1 0
U2 12
PY 2014
BP 268
EP 271
UT WOS:000380456800047
DA 2023-11-16
ER

PT C
AU Leow, CS
   Goh, WL
   Gao, Y
AF Leow, Cong Sheng
   Goh, Wang Ling
   Gao, Yuan
GP IEEE
TI Sparsity Through Spiking Convolutional Neural Network for Audio
   Classification at the Edge
SO 2023 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, ISCAS
SE IEEE International Symposium on Circuits and Systems
DT Proceedings Paper
CT 56th IEEE International Symposium on Circuits and Systems (ISCAS)
CY MAY 21-25, 2023
CL Monterey, CA
DE spiking neural networks; voice detection; convolutional neural networks;
   neuromorphic
AB Convolutional neural networks (CNNs) have shown to be effective for audio classification. However, deep CNNs can be computationally heavy and unsuitable for edge intelligence as embedded devices are generally constrained by memory and energy requirements. Spiking neural networks (SNNs) offer potential as energy-efficient networks but typically underperform typical deep neural networks in accuracy. This paper proposes a spiking convolutional neural network (SCNN) that exhibits excellent accuracy of above 98 % on a multi-class audio classification task. Accuracy remains high with weight quantization to INT8-precision. Additionally, this paper examines the role of neuron parameters in co-optimizing activation sparsity and accuracy.
C1 [Leow, Cong Sheng; Gao, Yuan] ASTAR, Inst Microelect IME, Singapore, Singapore.
   [Goh, Wang Ling] Nanyang Technol Univ NTU, Sch Elect & Elect Engn, Singapore, Singapore.
RP Leow, CS (corresponding author), ASTAR, Inst Microelect IME, Singapore, Singapore.
EM leowcs@ime.a-star.edu.sg; ewlgoh@ntu.edu.sg; gaoy@ime.a-star.edu.sg
CR Cramer B, 2022, P NATL ACAD SCI USA, V119, DOI 10.1073/pnas.2109194119
   Eshraghian J. K., 2022, ARXIV220207221
   Eshraghian J. K., ARXIV210912894, V2021
   Hashemi M, 2019, J BIG DATA-GER, V6, DOI 10.1186/s40537-019-0263-7
   Hershey S, 2017, INT CONF ACOUST SPEE, P131, DOI 10.1109/ICASSP.2017.7952132
   Hu JH, 2020, 2020 THE 3RD INTERNATIONAL CONFERENCE ON INTELLIGENT AUTONOMOUS SYSTEMS (ICOIAS'2020), P77, DOI 10.1109/ICoIAS49312.2020.9081859
   Hunter K. L., 2021, ARXIV211213896
   Liaw R., 2018, ARXIV180705118
   Nunes JD, 2022, IEEE ACCESS, V10, P60738, DOI 10.1109/ACCESS.2022.3179968
   Oruh J, 2022, COMPUT INTEL NEUROSC, V2022, DOI 10.1155/2022/3364141
   Paszke A., 2017, NEUR INF PROC SYST N
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Research M., 2022, FVCOR COMM 279B7D0
   Rossbroich J, 2022, NEUROMORPH COMPUT EN, V2, DOI 10.1088/2634-4386/ac97bb
   Wu JB, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00836
   Yang K., 2021, 2021 IEEE INT C INT, P82, DOI [10.1109/ICTA53157.2021.9661980, DOI 10.1109/ICTA53157.2021.9661980]
   Yin BJ, 2021, NAT MACH INTELL, V3, P905, DOI 10.1038/s42256-021-00397-w
   Zhang YL, 2021, NEURAL COMPUT APPL, V33, P9703, DOI 10.1007/s00521-021-05736-x
NR 18
TC 0
Z9 0
U1 0
U2 0
PY 2023
DI 10.1109/ISCAS46773.2023.10181974
UT WOS:001038214602075
DA 2023-11-16
ER

PT C
AU Pavlidis, NG
   Tasoulis, DK
   Plagianakos, VP
   Nikiforidis, G
   Vrahatis, MN
AF Pavlidis, NG
   Tasoulis, DK
   Plagianakos, VP
   Nikiforidis, G
   Vrahatis, MN
GP IEEE
TI Spiking neural network training using evolutionary algorithms
SO PROCEEDINGS OF THE INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS
   (IJCNN), VOLS 1-5
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT IEEE International Joint Conference on Neural Networks (IJCNN 2005)
CY JUL 31-AUG 04, 2005
CL Montreal, CANADA
ID MULTILAYER FEEDFORWARD NETWORKS; BACKPROPAGATION
AB Networks of spiking neurons can perform complex non-linear computations in fast temporal coding just as well as rate coded networks. These networks differ from previous models in that spiking neurons communicate information by the timing, rather than the rate, of spikes. To apply spiking neural networks on particular tasks, a learning process is required. Most existing training algorithms are based on unsupervised Hebbian learning. In this paper, we investigate the performance of the Parallel Differential Evolution algorithm, as a supervised training algorithm for spiking neural networks. The approach was successfully tested on well-known and widely used classification problems.
C1 Univ Patras, Dept Math, GR-26500 Patras, Greece.
RP Pavlidis, NG (corresponding author), Univ Patras, Dept Math, GR-26500 Patras, Greece.
EM npav@math.upatras.gr; dtas@math.upatras.gr; vpp@math.upatras.gr;
   gnikif@med.upatras.gr; vrahatis@math.upatras.gr
CR [Anonymous], 2000, SOLVE IT MODERN HEUR
   [Anonymous], 1994, 2194 U KARLSR FAK IN
   [Anonymous], CONNECTIONISM PERSPE
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Crick F., 1990, Seminars in the Neurosciences, V2, P263
   Gerstner W, 1998, PULSED NEURAL NETWORKS, P3
   Haykin S., 1999, NEURAL NETWORKS COMP
   HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8
   Jahnke A, 1998, PULSED NEURAL NETWORKS, P237
   Maass W, 1997, NEURAL COMPUT, V9, P279, DOI 10.1162/neco.1997.9.2.279
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Magoulas GD, 1999, NEURAL COMPUT, V11, P1769, DOI 10.1162/089976699300016223
   Magoulas GD, 1997, NEURAL NETWORKS, V10, P69, DOI 10.1016/S0893-6080(96)00052-4
   Plagianakos V. P., 2002, Natural Computing, V1, P307, DOI 10.1023/A:1016545907026
   PLAGIANAKOS VP, 2000, IEEE INT JOINT C NEU
   SCHWEFEL HP, EVOLUTION OPTIMUM SE
   Storn R, 1997, J GLOBAL OPTIM, V11, P341, DOI 10.1023/A:1008202821328
   STORN R, 1999, IEEE T EVOLUTIONARY, V2, P33
   WHITE H, 1990, NEURAL NETWORKS, V3, P535, DOI 10.1016/0893-6080(90)90004-5
NR 19
TC 61
Z9 68
U1 0
U2 1
PY 2005
BP 2190
EP 2194
UT WOS:000235178003034
DA 2023-11-16
ER

PT C
AU Zuters, J
AF Zuters, Janis
BE Barzdins, J
   Kirikova, M
TI Random Spikes to Enhance Learning in Spiking Neural Networks
SO DATABASES AND INFORMATION SYSTEMS VI: SELECTED PAPERS FROM THE NINTH
   INTERNATIONAL BALTIC CONFERENCE (DB&IS 2010)
SE Frontiers in Artificial Intelligence and Applications
DT Proceedings Paper
CT 9th International Baltic Conference on Databases and Information Systems
   (Baltic DBandIS)
CY JUL 05-07, 2010
CL Riga, LATVIA
DE spiking activity propagation; random spikes; parallel input layers
ID PROPAGATION
AB In spiking neural networks (SNNs), unlike traditional artificial neural networks, signals are propagated via a 'pulse code' instead of a 'rate code'. This results in incorporating the time dimension into the network and thus theoretically ensures a higher computational power. The different principle of operation makes learning in SNNs complicated. In this paper, two ideas have been proposed to assure spiking activity propagation - random spikes and parallel input layers. The proposed ideas have been illustrated with experimental results.
C1 Univ Latvia, Fac Comp, LV-1586 Riga, Latvia.
RP Zuters, J (corresponding author), Univ Latvia, Fac Comp, Raina Bulvaris 19, LV-1586 Riga, Latvia.
EM janis.zuters@lu.lv
CR [Anonymous], PULSED NEURAL NETWOR
   Burkitt AN, 2006, BIOL CYBERN, V95, P1, DOI 10.1007/s00422-006-0068-6
   Fausett L, 1993, FUNDAMENTALS NEURAL
   Hebb D., 2005, ORG BEHAV NEUROPSYCH
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Kempter R, 1999, PHYS REV E, V59, P4498, DOI 10.1103/PhysRevE.59.4498
   Kistler WM, 2002, NEURAL COMPUT, V14, P987, DOI 10.1162/089976602753633358
   Kumar A, 2010, NAT REV NEUROSCI, V11, P615, DOI 10.1038/nrn2886
   MAAS W, 1999, PULSED NEURAL NETWOR
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Salerno M, 2010, IEEE MEDITERR ELECT, P1039, DOI 10.1109/MELCON.2010.5475902
   Seung HS, 2003, NEURON, V40, P1063, DOI 10.1016/S0896-6273(03)00761-X
   Zuters J, 2009, FRONT ARTIF INTEL AP, V187, P131, DOI 10.3233/978-1-58603-939-4-131
NR 13
TC 0
Z9 0
U1 0
U2 0
PY 2011
VL 224
BP 369
EP 379
DI 10.3233/978-1-60750-688-1-369
UT WOS:000325431100026
DA 2023-11-16
ER

PT C
AU Fu, Q
   Wang, XM
   Dong, HB
   Huang, RL
AF Fu, Qiang
   Wang, Xingmei
   Dong, Hongbin
   Huang, Ruolin
GP IEEE
TI Spiking Neurons with Differential Evolution Algorithm for Pattern
   Classification
SO 2019 IEEE INTERNATIONAL CONFERENCE ON SYSTEMS, MAN AND CYBERNETICS (SMC)
SE IEEE International Conference on Systems Man and Cybernetics Conference
   Proceedings
DT Proceedings Paper
CT IEEE International Conference on Systems, Man and Cybernetics (SMC)
CY OCT 06-09, 2019
CL Bari, ITALY
ID NEURAL-NETWORK
AB Recently deep learning has revolutionized the field of machine learning, for pattern recognition in particular. A deep neural network (DNN) requires a large number of labeled training samples, and the recognition accuracy is truly impressive, sometimes outperforming humans. Neurons in an artificial neural network (ANN) are modeled by a single, static, continuous-valued activation function. However, biological neurons employ discrete spikes to compute and transmit information. The information can be encoded by the spike times, spike rates, and spike phase, etc. As the third generation artificial neural networks, spiking neural networks (SNNs) are more closely mimic natural neural networks. SNNs can achieve the same goals as ANNs, and it has the ability to build a large-scale network structure (i.e. deep spiking neural network) to accomplish complex tasks. In this paper, a state-of-the-art manner, differential evolving spiking neural network (DESNN), is proposed for pattern classification. The XOR task, Iris data, and hand-written digits classification task on MNIST are used to validate the proposed training method. The experimental results show that the algorithm used in this work applies the fewer neurons and it is effective for pattern classification tasks.
C1 [Fu, Qiang; Wang, Xingmei; Dong, Hongbin; Huang, Ruolin] Harbin Engn Univ, Comp Sci & Technol Coll, Harbin, Peoples R China.
RP Fu, Q (corresponding author), Harbin Engn Univ, Comp Sci & Technol Coll, Harbin, Peoples R China.
EM qiangfu@hrbeu.edu.cn; donghongbin@hrbeu.edu.cn
CR [Anonymous], 2013, FRONTIERS NEUROSCIEN
   Diehl P.U., 2015, 2015 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2015.7280696
   Ditlevsen S, 2013, J MATH BIOL, V67, P239, DOI 10.1007/s00285-012-0552-7
   Esser Steve K., 2015, ADV NEURAL INFORM PR, P1
   Fu Q, 2017, 2017 17TH IEEE INTERNATIONAL CONFERENCE ON COMMUNICATION TECHNOLOGY (ICCT 2017), P1916, DOI 10.1109/ICCT.2017.8359963
   Hunsberger E., ARXIV151008829CSLG, P1
   Lee J. H., 2016, TRAINING DEEP SPIKIN, V10, P1, DOI DOI 10.HTTP://DX.D0I.0RG/10.3389/FNINS.2016.00508
   Liu ZH, 2015, APPL MATH COMPUT, V270, P261, DOI 10.1016/j.amc.2015.08.031
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Merolla P., 2011, IEEE CUST INT CIRC C, P1, DOI DOI 10.1109/CICC.2011.6055294
   MORRIS C, 1981, BIOPHYS J, V35, P193, DOI 10.1016/S0006-3495(81)84782-0
   Querlioz D, 2013, IEEE T NANOTECHNOL, V12, P288, DOI 10.1109/TNANO.2013.2250995
   Salza P, 2019, FUTURE GENER COMP SY, V92, P276, DOI 10.1016/j.future.2018.09.066
   Shrestha SB, 2015, NEURAL NETWORKS, V63, P185, DOI 10.1016/j.neunet.2014.12.001
   Storn R, 1997, J GLOBAL OPTIM, V11, P341, DOI 10.1023/A:1008202821328
   Tateno T, 2004, CHAOS, V14, P511, DOI 10.1063/1.1756118
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Tielin Z., 2018, P 32 AAAI C ART INT, P1
   Tsumoto K, 2006, NEUROCOMPUTING, V69, P293, DOI 10.1016/j.neucom.2005.03.006
   Vazquez Roberto A., 2010, 2010 7th International Conference on Electrical Engineering, Computing Science and Automatic Control (CCE 2010) (Formerly known as ICEEE), P424, DOI 10.1109/ICEEE.2010.5608622
   Wade JJ, 2010, IEEE T NEURAL NETWOR, V21, P1817, DOI 10.1109/TNN.2010.2074212
   Wan SS, 2018, KNOWL-BASED SYST, V160, P71, DOI 10.1016/j.knosys.2018.06.014
   Wang LJ, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON INFORMATION AND AUTOMATION (IEEE ICIA 2017), P1, DOI 10.1109/ICInfA.2017.8078873
   Wu QX, 2006, NEUROCOMPUTING, V69, P1912, DOI 10.1016/j.neucom.2005.11.023
   Zarei T, 2019, SOL ENERGY, V177, P595, DOI 10.1016/j.solener.2018.11.059
NR 25
TC 3
Z9 3
U1 0
U2 11
PY 2019
BP 152
EP 157
UT WOS:000521353900025
DA 2023-11-16
ER

PT C
AU Rosselló, JL
   Canals, V
   Morro, A
   de Paúl, I
AF Rossello, Josep L.
   Canals, Vincent
   Morro, Antoni
   de Paul, Ivan
BE Yu, W
   He, HB
   Zhang, N
TI Practical Hardware Implementation of Self-configuring Neural Networks
SO ADVANCES IN NEURAL NETWORKS - ISNN 2009, PT 3, PROCEEDINGS
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 6th International Symposium on Neural Networks
CY MAY 26-29, 2009
CL Wuhan, PEOPLES R CHINA
DE Neural Networks; Spiking Neural Networks; Hardware implementation of
   Genetic Algorithms
ID SPIKING NEURONS
AB This work provides practical guidelines for an efficient hardware implementation of Neural Networks. Networks are configured using a practical self-learning architecture that iterates a basic Genetic Algorithm. The learning methodology is based on the generation of random vectors that can be extracted from chaotic signals. The proposed solution is applied to estimate the processing efficiency of Spiking Neural Networks.
C1 [Rossello, Josep L.; Canals, Vincent; Morro, Antoni; de Paul, Ivan] Univ Illes Balears, Elect Syst Grp, Dept Phys, Palma de Mallorca 07122, Spain.
RP Rosselló, JL (corresponding author), Univ Illes Balears, Elect Syst Grp, Dept Phys, Palma de Mallorca 07122, Spain.
EM j.rossello@uib.es
CR Gerstner W., 2002, SPIKING NEURON MODEL
   Malaka R, 2000, IEEE IJCNN, P486, DOI 10.1109/IJCNN.2000.859442
   Rosselló JL, 2008, IEICE ELECTRON EXPR, V5, P1042, DOI 10.1587/elex.5.1042
   Sala DM, 1999, IEEE T NEURAL NETWOR, V10, P953, DOI 10.1109/72.774270
NR 4
TC 0
Z9 0
U1 0
U2 0
PY 2009
VL 5553
BP 1154
EP 1159
UT WOS:000268029200128
DA 2023-11-16
ER

PT J
AU Wunderlich, TC
   Pehle, C
AF Wunderlich, Timo C.
   Pehle, Christian
TI Event-based backpropagation can compute exact gradients for spiking
   neural networks
SO SCIENTIFIC REPORTS
DT Article
ID NEURONS; OPTIMIZATION; ARCHITECTURE; INTELLIGENCE; DESCENT; SYSTEMS
AB Spiking neural networks combine analog computation with event-based communication using discrete spikes. While the impressive advances of deep learning are enabled by training non-spiking artificial neural networks using the backpropagation algorithm, applying this algorithm to spiking networks was previously hindered by the existence of discrete spike events and discontinuities. For the first time, this work derives the backpropagation algorithm for a continuous-time spiking neural network and a general loss function by applying the adjoint method together with the proper partial derivative jumps, allowing for backpropagation through discrete spike events without approximations. This algorithm, EventProp, backpropagates errors at spike times in order to compute the exact gradient in an event-based, temporally and spatially sparse fashion. We use gradients computed via EventProp to train networks on the Yin-Yang and MNIST datasets using either a spike time or voltage based loss function and report competitive performance. Our work supports the rigorous study of gradient-based learning algorithms in spiking neural networks and provides insights toward their implementation in novel brain-inspired hardware.
C1 [Wunderlich, Timo C.; Pehle, Christian] Heidelberg Univ, Kirchhoff Institute Phys, D-69120 Heidelberg, Germany.
   [Wunderlich, Timo C.; Pehle, Christian] Charite, Berlin Inst Hlth, D-10117 Berlin, Germany.
RP Wunderlich, TC; Pehle, C (corresponding author), Heidelberg Univ, Kirchhoff Institute Phys, D-69120 Heidelberg, Germany.; Wunderlich, TC; Pehle, C (corresponding author), Charite, Berlin Inst Hlth, D-10117 Berlin, Germany.
EM timo.wunderlich@charite.de; christian.pehle@kip.uni-heidelberg.de
CR Aamir SA, 2018, IEEE T CIRCUITS-I, V65, P4299, DOI 10.1109/TCSI.2018.2840718
   Barton P. I., 2002, ACM Transactions on Modeling and Computer Simulation, V12, P256, DOI 10.1145/643120.643122
   Barton PI, 1998, IND ENG CHEM RES, V37, P966, DOI 10.1021/ie970738y
   Bell AJ, 2005, ADV NEURAL INFORM PR, P121
   Bellec G., 2018, ADV NEURAL INFORM PR
   Billaudelle S, 2020, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS45731.2020.9180960
   Bohte S. M., 2000, 8th European Symposium on Artificial Neural Networks. ESANN"2000. Proceedings, P419
   Booij O, 2005, INFORM PROCESS LETT, V95, P552, DOI 10.1016/j.ipl.2005.05.023
   Boybat I, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-04933-y
   Bradley A., 2019, PDE CONSTRAINED OPTI
   Chan V, 2007, IEEE T CIRCUITS-I, V54, P48, DOI 10.1109/TCSI.2006.887979
   Chen Ricky TQ, 2018, ADV NEURAL INFORM PR, V31, DOI DOI 10.1016/j.burns.2011.10.009
   Comsa I.M., 2020, ICASSP 2020 2020 IEE, DOI DOI 10.1109/ICASSP40776.2020.9053856
   Cramer B. etal, ARXIV200607239
   Cun Y., 1988, P 1988 CONN MOD SUMM, P21, DOI DOI 10.3168/JDS.S0022-0302(88)79586-7
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   De Backer W., 1964, INT S SENSITIVITY ME, V1, P168
   Esser SK, 2016, P NATL ACAD SCI USA, V113, P11441, DOI 10.1073/pnas.1604850113
   Feldmann J, 2019, NATURE, V569, P208, DOI 10.1038/s41586-019-1157-8
   Florian RV, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0040233
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   Galán S, 1999, APPL NUMER MATH, V31, P17, DOI 10.1016/S0168-9274(98)00125-1
   Gerstner W., 2002, SPIKING NEURON MODEL
   Goltz J., 2019, FAST DEEP ENERGY EFF
   Gronwall TH, 1918, ANN MATH, V20, P292
   Gütig R, 2016, SCIENCE, V351, DOI 10.1126/science.aab4113
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Huh D, 2018, ADV NEUR IN, V31
   Jia J, 2019, ADV NEUR IN, P9843
   Kheradpisheh SR, 2020, INT J NEURAL SYST, V30, DOI 10.1142/S0129065720500276
   Kingma DP., 2017, ARXIV
   Klikauer T, 2016, TRIPLEC-COMMUN CAPIT, V14, P260
   Krantz S.G., 2012, IMPLICIT FUNCTION TH
   Kriener, 2020, YIN YANG DATASET, DOI [10.1016/S0893-6080(97)00011-7, DOI 10.1016/S0893-6080(97)00011-7]
   Kumar Ravi, 2019, ADV NEURAL INFORM PR
   Kuroe Y., 2010, 2010 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2010.5596914
   Kuroe Y, 2006, IEEE IJCNN, P3882
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee JM, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00191
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Moradi S, 2018, IEEE T BIOMED CIRC S, V12, P106, DOI 10.1109/TBCAS.2017.2759700
   Mostafa H, 2018, IEEE T NEUR NET LEAR, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Neckar A, 2019, P IEEE, V107, P144, DOI 10.1109/JPROC.2018.2881432
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Neftci EO, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00324
   Ojika, 2020, ARXIV200308732
   Pehle C.-G., 2021, THESIS HEIDELBERG U
   Pei J, 2019, NATURE, V572, P106, DOI 10.1038/s41586-019-1424-8
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Pleiss G., 2017, CORR
   Pontryagin L. S., 1974, SELECTED WORKS
   Rotter S, 1999, BIOL CYBERN, V81, P381, DOI 10.1007/s004220050570
   Roy K, 2019, NATURE, V575, P607, DOI 10.1038/s41586-019-1677-2
   Rozenvasser E., 1967, AUTOMAT REM CONTR+, V3, P52
   Schemmel J, 2017, IEEE IJCNN, P2217, DOI 10.1109/IJCNN.2017.7966124
   Selvaratnam K., 2000, Transactions of the Institute of Systems, Control and Information Engineers, V13, P95, DOI 10.5687/iscie.13.3_95
   Serban R, 2019, J COMPUT NONLIN DYN, V14, DOI 10.1115/1.4044028
   Shrestha SB., 2018, ADV NEURAL INFORM PR, V31, P1412
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Wunderlich T, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00260
   Xu Y, 2013, NEURAL NETWORKS, V43, P99, DOI 10.1016/j.neunet.2013.02.003
   Yang WY, 2014, LECT NOTES COMPUT SC, V8866, P19, DOI 10.1007/978-3-319-12436-0_3
   Yusupov R., 2019, CONTROL SERIES, DOI [10.1201/9781420049749, DOI 10.1201/9781420049749]
   Zenke F, 2021, NEURAL COMPUT, V33, P899, DOI 10.1162/neco_a_01367
   Zenke F, 2018, NEURAL COMPUT, V30, P1514, DOI 10.1162/neco_a_01086
NR 66
TC 30
Z9 30
U1 1
U2 16
PD JUN 18
PY 2021
VL 11
IS 1
AR 12829
DI 10.1038/s41598-021-91786-z
UT WOS:000665057800001
DA 2023-11-16
ER

PT C
AU Connolly, CG
   Marian, I
   Reilly, RG
AF Connolly, CG
   Marian, I
   Reilly, RG
BE Bowman, H
   Labiouse, C
TI Approaches to efficient simulation with spiking neural networks
SO CONNECTIONIST MODELS OF COGNITION AND PERCEPTION II
SE Progress in Neural Processing
DT Proceedings Paper
CT 8th Neural Computation and Psychology Workshop
CY AUG 28-30, 2003
CL Univ Kent, Canterbury, ENGLAND
HO Univ Kent
AB The distinct computational properties of spiking neural networks are increasingly the focus of research in computational neuroscience. When modelling these networks efficiency issues are critical. In this paper we present several algorithms for the event-driven simulation of spiking neural networks on single processor systems, which facilitate the simulation of large, highly active networks.
C1 Univ Coll Dublin, Dept Comp Sci, Dublin 4, Ireland.
EM Colm.Connolly@ucd.ie; Ioana.Marian@epfl.ch; ronan@es.may.ie
CR [Anonymous], ADV NEURAL INFORM PR
   Bower J. M., 1998, BOOK GENESIS EXPLORI, DOI DOI 10.1007/978-1-4612-1634-6
   Braitenberg V., 1998, CORTEX STAT GEOMETRY, DOI 10.1007/978-3-662-03733-1
   Claverol ET, 2002, NEUROCOMPUTING, V47, P277, DOI 10.1016/S0925-2312(01)00629-4
   FOHLMEISTER C, 1995, NEURAL COMPUT, V7, P905, DOI 10.1162/neco.1995.7.5.905
   Gerstner W, 1998, PULSED NEURAL NETWORKS, P3
   Hines ML, 1997, NEURAL COMPUT, V9, P1179, DOI 10.1162/neco.1997.9.6.1179
   MARIAN I, 2003, THESIS U COLL DUBLIN
   Mattia M, 2000, NEURAL COMPUT, V12, P2305, DOI 10.1162/089976600300014953
   Thorpe S, 1996, NATURE, V381, P520, DOI 10.1038/381520a0
   Thorpe SJ, 1997, ADV NEUR IN, V9, P901
NR 11
TC 2
Z9 2
U1 0
U2 1
PY 2004
VL 15
BP 231
EP 240
DI 10.1142/9789812702784_0022
UT WOS:000229608200022
DA 2023-11-16
ER

PT C
AU Nguyen, HL
   Chu, D
AF Huy Le Nguyen
   Chu, Dominique
BE Ishibuchi, H
   Kwoh, CK
   Tan, AH
   Srinivasan, D
   Miao, C
   Trivedi, A
   Crockett, K
TI Incremental Neural Synthesis for Spiking Neural Networks
SO 2022 IEEE SYMPOSIUM SERIES ON COMPUTATIONAL INTELLIGENCE (SSCI)
DT Proceedings Paper
CT IEEE Symposium Series on Computational Intelligence (IEEE SSCI)
CY DEC 04-07, 2022
CL Singapore, SINGAPORE
DE Convolutional Spiking Neural Networks; Neural Synthesis; Incremental
   Learning; Multi-Spike Learning
ID NEURONS; TEMPOTRON; MACHINE
AB We present an iterative neural synthesis approach to train Convolutional Spiking Neural Networks for classification problems. Unlike previous neural synthesis methods which primarily compute the neuron firing rates, our method is designed to compute multiple spikes at arbitrary timings. As such, our approach is directly applicable to spatio-temporal problems using spiking network models. In our approach, each weight update is formulated as a linear Constraint Satisfaction Problem, which can then be solved using existing numerical techniques. On the MNIST, EMNIST, and ETH-80 image classification benchmarks, our approach demonstrates competitive with other models in the literature, while requiring relatively few training samples to converge to a good solution.
C1 [Huy Le Nguyen; Chu, Dominique] Univ Kent, Sch Comp, Canterbury, Kent, England.
RP Nguyen, HL (corresponding author), Univ Kent, Sch Comp, Canterbury, Kent, England.
EM hln7@kent.ac.uk; dfc@kent.ac.uk
CR Alyamkin S, 2018, Arxiv, DOI arXiv:1810.01732
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Borst A, 1999, NAT NEUROSCI, V2, P947, DOI 10.1038/14731
   Boucher-Routhier M, 2021, NEURAL NETWORKS, V144, P639, DOI 10.1016/j.neunet.2021.09.021
   Brette R, 2007, J COMPUT NEUROSCI, V23, P349, DOI 10.1007/s10827-007-0038-6
   Cohen G, 2017, IEEE IJCNN, P2921, DOI 10.1109/IJCNN.2017.7966217
   Cohen GK, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00184
   DeBole MV, 2019, COMPUTER, V52, P20, DOI 10.1109/MC.2019.2903009
   Demin VA, 2021, NEURAL NETWORKS, V134, P64, DOI 10.1016/j.neunet.2020.11.005
   Dodge S, 2017, 2017 26TH INTERNATIONAL CONFERENCE ON COMPUTER COMMUNICATION AND NETWORKS (ICCCN 2017)
   Eliasmith C., 2003, NEURAL ENG COMPUTATI
   Fang W, 2021, INCORPORATING LEARNA, P2661
   Gardner B, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0161335
   Geirhos Robert, 2018, ADV NEURAL INFORM PR, P7538, DOI DOI 10.5555/3327757.3327854
   Gerstner W., 2002, SPIKING NEURON MODEL
   Gondzio J, 2012, EUR J OPER RES, V218, P587, DOI 10.1016/j.ejor.2011.09.017
   Gütig R, 2016, SCIENCE, V351, DOI 10.1126/science.aab4113
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Hao YZ, 2020, NEURAL NETWORKS, V121, P387, DOI 10.1016/j.neunet.2019.09.007
   Huang GB, 2006, NEUROCOMPUTING, V70, P489, DOI 10.1016/j.neucom.2005.12.126
   Jin YYZ, 2018, ADV NEUR IN, V31
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Kheradpisheh SR, 2016, NEUROCOMPUTING, V205, P382, DOI 10.1016/j.neucom.2016.04.029
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
   Kulkarni SR, 2018, NEURAL NETWORKS, V103, P118, DOI 10.1016/j.neunet.2018.03.019
   Kuppili V, 2020, COMPUT INTELL-US, V36, P402, DOI 10.1111/coin.12242
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Ledinauskas E, 2020, Arxiv, DOI arXiv:2006.04436
   Lee C, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00119
   Lee WW, 2017, IEEE T NEUR NET LEAR, V28, P849, DOI 10.1109/TNNLS.2015.2509479
   Li D, 2016, PROCEEDINGS OF 2016 IEEE INTERNATIONAL CONFERENCES ON BIG DATA AND CLOUD COMPUTING (BDCLOUD 2016) SOCIAL COMPUTING AND NETWORKING (SOCIALCOM 2016) SUSTAINABLE COMPUTING AND COMMUNICATIONS (SUSTAINCOM 2016) (BDCLOUD-SOCIALCOM-SUSTAINCOM 2016), P477, DOI 10.1109/BDCloud-SocialCom-SustainCom.2016.76
   Li SL, 2020, AAAI CONF ARTIF INTE, V34, P4650
   Liang NY, 2006, IEEE T NEURAL NETWOR, V17, P1411, DOI 10.1109/TNN.2006.880583
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Memmesheimer RM, 2014, NEURON, V82, P925, DOI 10.1016/j.neuron.2014.03.026
   Naveros F, 2017, FRONT NEUROINFORM, V11, DOI 10.3389/fninf.2017.00007
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Orchard G, 2021, IEEE WRK SIG PRO SYS, P254, DOI 10.1109/SiPS52927.2021.00053
   Panda P, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00653
   Perez-Nieves N., 2021, ADV NEURAL INFORM PR, V34
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Roy K, 2019, NATURE, V575, P607, DOI 10.1038/s41586-019-1677-2
   Rubin R, 2010, PHYS REV LETT, V105, DOI 10.1103/PhysRevLett.105.218102
   Shi C, 2021, IEEE T CIRCUITS-II, V68, P1581, DOI 10.1109/TCSII.2021.3063784
   Shrestha SB, 2018, ADV NEUR IN, V31
   Shrestha SB, 2015, NEURAL NETWORKS, V63, P185, DOI 10.1016/j.neunet.2014.12.001
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song SM, 2021, NEURAL NETWORKS, V142, P205, DOI 10.1016/j.neunet.2021.05.002
   Tanneau M, 2021, MATH PROGRAM COMPUT, V13, P509, DOI 10.1007/s12532-020-00200-8
   Tapson J, 2013, NEURAL NETWORKS, V45, P94, DOI 10.1016/j.neunet.2013.02.008
   Tavanaei A, 2018, 2018 INT JOINT C NEU, DOI DOI 10.1109/IJCNN.2018.8489104
   Tissera MD, 2016, NEUROCOMPUTING, V174, P42, DOI 10.1016/j.neucom.2015.03.110
   Vaila Ruthvik, 2020, IEEE TETCI
   Valadez-Godínez S, 2020, NEURAL NETWORKS, V122, P196, DOI 10.1016/j.neunet.2019.09.026
   Vigneron A, 2020, IEEE IJCNN, DOI 10.1109/ijcnn48605.2020.9207239
   Wang RC, 2017, IEEE T BIOMED CIRC S, V11, P574, DOI 10.1109/TBCAS.2017.2666883
   WIDROW B, 1990, P IEEE, V78, P1415, DOI 10.1109/5.58323
   Widrow B, 2013, NEURAL NETWORKS, V37, P180, DOI 10.1016/j.neunet.2012.09.020
   Wright S. J., 1997, PRIMAL DUAL INTERIOR
   Yu Q, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0078318
   Zhang W, 2020, ADV NEURAL INFORM PR, V33, P12022, DOI DOI 10.48550/ARXIV.2002.10085
   Zhou XQ, 2020, 2020 3RD INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND BIG DATA (ICAIBD 2020), P39, DOI [10.1109/icaibd49809.2020.9137430, 10.1109/ICAIBD49809.2020.9137430]
NR 62
TC 0
Z9 0
U1 2
U2 2
PY 2022
BP 649
EP 656
DI 10.1109/SSCI51031.2022.10022275
UT WOS:000971973800087
DA 2023-11-16
ER

PT J
AU Brette, R
   Goodman, DFM
AF Brette, Romain
   Goodman, Dan F. M.
TI Simulating spiking neural networks on GPU
SO NETWORK-COMPUTATION IN NEURAL SYSTEMS
DT Review
DE Simulation; graphics cards; GPU; spiking neural networks; algorithms;
   parallel computing
ID NEURONS; CELLML; TOOLS
AB Modern graphics cards contain hundreds of cores that can be programmed for intensive calculations. They are beginning to be used for spiking neural network simulations. The goal is to make parallel simulation of spiking neural networks available to a large audience, without the requirements of a cluster. We review the ongoing efforts towards this goal, and we outline the main difficulties.
C1 [Brette, Romain] Ecole Normale Super, Equipe Audit, Dept Etud Cognit, F-75005 Paris, France.
   [Brette, Romain; Goodman, Dan F. M.] Univ Paris 05, Paris, France.
   [Brette, Romain; Goodman, Dan F. M.] CNRS, Lab Psychol Percept, Paris, France.
RP Brette, R (corresponding author), Ecole Normale Super, Equipe Audit, Dept Etud Cognit, 29 Rue Ulm, F-75005 Paris, France.
EM romain.brette@ens.fr
CR [Anonymous], METHODS NEURONAL MOD
   [Anonymous], 2001, JGI 01, DOI DOI 10.1145/376656.376823
   [Anonymous], BIOM SCI ENG C BSEC
   [Anonymous], 2011, FRONT NEUROINFORM
   [Anonymous], 2011, P 19 IR C EL ENG TEH
   [Anonymous], 2010, 2010 IEEE INT S PARA
   [Anonymous], NETWORK
   [Anonymous], APPL GPU COMPUTING
   [Anonymous], 2009, IEEE IJCNN
   [Anonymous], 2011, BMC NEUROSCI, DOI DOI 10.1186/1471-2202-12-S1-P330
   [Anonymous], PYCUDA GPU RUN TIME
   [Anonymous], BMC NEUROSCI
   [Anonymous], NEURON
   [Anonymous], 2010 INT JOINT C NEU
   Bell N, 2009, STUDENTS GUIDE TO THE MA TESOL, P1
   Bernhard F, 2006, LECT NOTES COMPUT SC, V3994, P236
   Brette R, 2007, J COMPUT NEUROSCI, V23, P349, DOI 10.1007/s10827-007-0038-6
   Fernandez A, 2008, 2008 11TH INTERNATIONAL WORKSHOP ON CELLULAR NEURAL NETWORKS AND THEIR APPLICATIONS, P208, DOI 10.1109/CNNA.2008.4588679
   Fidjeland AK, 2009, IEEE INT CONF ASAP, P137, DOI 10.1109/ASAP.2009.24
   Fidjeland Andreas K, 2010, 2010 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2010.5596678
   Garny A, 2008, PHILOS T R SOC A, V366, P3017, DOI 10.1098/rsta.2008.0094
   Gerstner W., 2002, SPIKING NEURON MODEL
   Gleeson P, 2010, PLOS COMPUT BIOL, V6, DOI 10.1371/journal.pcbi.1000815
   Goodman Dan, 2008, Front Neuroinform, V2, P5, DOI 10.3389/neuro.11.005.2008
   Goodman DFM, 2010, NEUROINFORMATICS, V8, P183, DOI 10.1007/s12021-010-9082-x
   Goodman DFM, 2009, FRONT NEUROSCI-SWITZ, V3, P192, DOI 10.3389/neuro.01.026.2009
   Han B, 2010, IEEE IJCNN
   Han B, 2010, APPL OPTICS, V49, pB83, DOI 10.1364/AO.49.000B83
   Hines ML, 2000, NEURAL COMPUT, V12, P995, DOI 10.1162/089976600300015475
   Hoffmann J, 2010, LECT NOTES COMPUT SC, V6352, P184, DOI 10.1007/978-3-642-15819-3_23
   Igarashi J, 2011, NEURAL NETWORKS, V24, P950, DOI 10.1016/j.neunet.2011.06.008
   Klöckner A, 2012, PARALLEL COMPUT, V38, P157, DOI 10.1016/j.parco.2011.09.001
   KOOTSEY JM, 1986, B MATH BIOL, V48, P427, DOI 10.1016/S0092-8240(86)90037-6
   MASCAGNI M, 1991, J NEUROSCI METH, V36, P105, DOI 10.1016/0165-0270(91)90143-N
   Miller AK, 2010, BMC BIOINFORMATICS, V11, DOI 10.1186/1471-2105-11-178
   Mutch J., 2010, CNS GPU BASED FRAMEW
   Nageswaran JM, 2009, NEURAL NETWORKS, V22, P791, DOI 10.1016/j.neunet.2009.06.028
   Owens JD, 2007, COMPUT GRAPH FORUM, V26, P80, DOI 10.1111/j.1467-8659.2007.01012.x
   Rossant C, 2011, FRONT NEUROSCI-SWITZ, V5, DOI [10.3389/fnins.2011.00009, 10.3389/fninf.2011.00009]
   Rossant Cyrille, 2010, Front Neuroinform, V4, P2, DOI 10.3389/neuro.11.002.2010
   Wang MC, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P3184, DOI 10.1109/IJCNN.2011.6033643
   Zhang Y, 2010, ACM SIGPLAN NOTICES, V45, P127, DOI 10.1145/1837853.1693472
NR 42
TC 43
Z9 44
U1 0
U2 32
PY 2012
VL 23
IS 4
BP 167
EP 182
DI 10.3109/0954898X.2012.730170
UT WOS:000311837300005
DA 2023-11-16
ER

PT C
AU Li, HG
   Yu, P
   Xia, TS
AF Li, Hongge
   Yu, Pan
   Xia, Tongsheng
GP IEEE
TI An efficient spike-sorting for implantable neural recording microsystem
   using hybrid neural network
SO 2012 ANNUAL INTERNATIONAL CONFERENCE OF THE IEEE ENGINEERING IN MEDICINE
   AND BIOLOGY SOCIETY (EMBC)
SE IEEE Engineering in Medicine and Biology Society Conference Proceedings
DT Proceedings Paper
CT 34th Annual International Conference of the IEEE
   Engineering-in-Medicine-and-Biology-Society (EMBS)
CY AUG 28-SEP 01, 2012
CL San Diego, CA
AB Automatic efficient spike sorting is one of the biggest challenges for the neural recording microsystem online. An unsupervised spike sorting method is proposed in this paper, based on the hybrid neural network with principal component analysis network (PCAN) and normal boundary response (NBR) self-organizing map network (SOMN) classifier. The PCAN extracted the spike features with the dimension reduced and correlation eliminated; The SOM network perform the spike distribution in the feature space, thus after convergence, the weights of the neurons demonstrate the spike cluster distribution in the feature space; At last the spike sorting was finished by computing the neurons' Normal Boundary Response (NBR) which determined the neurons' classes. The experimental results show that, based on hybrid neural network spiking sorting algorithm, it can achieve the accuracy above 97.91% with signals containing five classes. The novel classification algorithm proposed is to further improve the efficient and adaptive of classification system.
C1 [Li, Hongge] Beihang Univ, Sch Elect Informat Engn, Beijing, Peoples R China.
RP Li, HG (corresponding author), Beihang Univ, Sch Elect Informat Engn, Beijing, Peoples R China.
EM honggeli@buaa.edu.cn
CR [Anonymous], IEEE SOLID STATE CIR
   Clausen J, 2009, NATURE, V457, P1080, DOI 10.1038/4571080a
   GIBSON S, 2008, 30 IEEE EMBS, P5015
   Horton PM, 2007, J NEUROSCI METH, V160, P52, DOI 10.1016/j.jneumeth.2006.08.013
   Kanungo T, 2002, IEEE T PATTERN ANAL, V24, P881, DOI 10.1109/TPAMI.2002.1017616
   Kohonen T., 1995, SELF ORG MAPS
   Lewicki MS, 1998, NETWORK-COMP NEURAL, V9, pR53, DOI 10.1088/0954-898X/9/4/001
   Li H, 2011, ELECTRON LETT, V47, P367, DOI 10.1049/el.2010.3711
   Quiroga RQ, 2004, NEURAL COMPUT, V16, P1661, DOI 10.1162/089976604774201631
   Rizk M, 2007, J NEURAL ENG, V4, P309, DOI 10.1088/1741-2560/4/3/016
   SANGER TD, 1989, NEURAL NETWORKS, V2, P459, DOI 10.1016/0893-6080(89)90044-0
   Wang ZG, 2009, PROG NAT SCI-MATER, V19, P1261, DOI 10.1016/j.pnsc.2009.02.005
NR 12
TC 1
Z9 1
U1 0
U2 3
PY 2012
BP 5274
EP 5277
UT WOS:000313296505121
DA 2023-11-16
ER

PT J
AU Leone, G
   Raffo, L
   Meloni, P
AF Leone, Gianluca
   Raffo, Luigi
   Meloni, Paolo
TI A Bandwidth-Efficient Emulator of Biologically-Relevant Spiking Neural
   Networks on FPGA
SO IEEE ACCESS
DT Article
DE Neurons; Biological neural networks; Field programmable gate arrays;
   Real-time systems; Synapses; Neural networks; Biology; APSoC;
   fixed-point; FPGA; neural emulator; hardware accelerator; neural
   engineering; real-time; spiking neural network
ID MODEL
AB Closed-loop experiments involving biological and artificial neural networks would improve the understanding of neural cells functioning principles and lead to the development of new generation neuroprosthesis. Several technological challenges require to be faced, as the development of real-time spiking neural network emulators which could bear the increasing amount of data provided by new generation High-Density Multielectrode Arrays. This work focuses on the development of a real-time spiking neural network emulator addressing fully-connected neural networks. This work presents a new way to increase the number of synapses supported by real-time neural network accelerators. The proposed solution has been implemented on the Xilinx Zynq 7020 All-Programmable SoC and can emulate fully connected spiking neural networks counting up to 3,098 Izhikevich neurons and 9.6e6 synapses in real-time, with a resolution of 0.1 ms.
C1 [Leone, Gianluca] Univ Cagliari, Elect & Comp Engn, I-09124 Cagliari, Italy.
   [Raffo, Luigi] Univ Cagliari, Dept Elect & Elect Engn, I-09124 Cagliari, Italy.
   [Meloni, Paolo] Univ Cagliari, I-09124 Cagliari, Italy.
RP Meloni, P (corresponding author), Univ Cagliari, I-09124 Cagliari, Italy.
EM paolo.melon@unica.it
CR Ambroise M, 2013, 2013 47TH ANNUAL CONFERENCE ON INFORMATION SCIENCES AND SYSTEMS (CISS)
   [Anonymous], AXI DMA VERS 7 1 LOG
   Bandeira V., 2017, PROC INT EMBEDDED SY, P210, DOI [10.1007/978-3-319-90023-0_17, DOI 10.1007/978-3-319-90023-0_17]
   Bouvier M, 2019, ACM J EMERG TECH COM, V15, DOI 10.1145/3304103
   Carnevale NT., 2006, NEURON BOOK, DOI DOI 10.1017/CBO9780511541612
   Cassidy A, 2008, 2008 IEEE BIOMEDICAL CIRCUITS AND SYSTEMS CONFERENCE - INTELLIGENT BIOMEDICAL SYSTEMS (BIOCAS), P289, DOI 10.1109/BIOCAS.2008.4696931
   Cheung K, 2016, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00516
   DeFelipe J, 2012, FRONT NEUROANAT, V6, DOI [10.3389/fnana.2012.00022, 10.3389/fnsyn.2012.00002, 10.3389/fnana.2012.00005]
   Froemke RC, 2002, NATURE, V416, P433, DOI 10.1038/416433a
   Gerstner W., 2002, SPIKING NEURON MODEL
   Goodman DFM, 2009, FRONT NEUROSCI-SWITZ, V3, P192, DOI 10.3389/neuro.01.026.2009
   Gupta S, 2020, IEEE I C ELECT CIRC, DOI 10.1109/icecs49266.2020.9294790
   Han JH, 2020, TSINGHUA SCI TECHNOL, V25, P479, DOI 10.26599/TST.2019.9010019
   Hopkins M, 2020, PHILOS T R SOC A, V378, DOI 10.1098/rsta.2019.0052
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Khodamoradi Alireza, 2021, FPGA '21: The 2021 ACM/SIGDA International Symposium on Field-Programmable, P194, DOI 10.1145/3431920.3439283
   Leone G, 2020, IEEE ACCESS, V8, P218145, DOI 10.1109/ACCESS.2020.3042034
   Lisitsa D, 2017, IEEE NW RUSS YOUNG, P926, DOI 10.1109/EIConRus.2017.7910708
   Luo JW, 2016, IEEE T BIOMED CIRC S, V10, P742, DOI 10.1109/TBCAS.2015.2460232
   Marc-Oliver G., 2007, SCHOLARPEDIA, DOI [DOI 10.4249/SCHOLARPEDIA.1430, 10.4249/scholarpedia.1430]
   Painkras E, 2013, IEEE J SOLID-ST CIRC, V48, P1943, DOI 10.1109/JSSC.2013.2259038
   Panchapakesan S, 2021, I C FIELD PROG LOGIC, P286, DOI 10.1109/FPL53798.2021.00058
   Panchapakesan S, 2020, ANN IEEE SYM FIELD P, P242, DOI 10.1109/FCCM48280.2020.00075
   Pani D, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00090
   Rast AD, 2010, PROCEEDINGS OF THE 2010 COMPUTING FRONTIERS CONFERENCE (CF 2010), P21, DOI 10.1145/1787275.1787279
   Sripad A, 2018, NEURAL NETWORKS, V97, P28, DOI 10.1016/j.neunet.2017.09.011
   Steinmetz N.A., 2020, NEUROPIXELS 2 0 MINI
   Swadlow H. A., 2012, SCHOLARPEDIA, V7, DOI [10.4249/scholarpedia.1451, DOI 10.4249/SCHOLARPEDIA.1451]
NR 28
TC 2
Z9 2
U1 1
U2 10
PY 2022
VL 10
BP 76780
EP 76793
DI 10.1109/ACCESS.2022.3192826
UT WOS:000832930400001
DA 2023-11-16
ER

PT C
AU Wang, XQ
   Hou, ZG
   Tan, M
   Wang, YJ
   Huang, ZH
AF Wang Xiuqing
   Hou Zeng-Guang
   Tan Min
   Wang Yongji
   Huang Zhanhua
GP IEEE
TI Spiking Neural Networks and its Application for Mobile Robots
SO 2011 30TH CHINESE CONTROL CONFERENCE (CCC)
SE Chinese Control Conference
DT Proceedings Paper
CT 30th Chinese Control Conference
CY JUL 22-24, 2011
CL Yantai, PEOPLES R CHINA
DE Spiking Neural Networks; Mobile robots; Spike; Control
ID SYNAPTIC PLASTICITY; NEURONS; CONTROLLER; CIRCUITS; MODEL
AB In this paper, the third generation neural network-Spiking neural networks (SNNs) are introduced. SNNs' origination, characteristic, coding, the training methods, and the models of Spiking neurons and synaptics are also discussed. A survey for SNNs' research and application in mobile robots is made. Because of SNNs' unique characteristic: good biological plausibility, Spiking neurons involving both temporal and spatial information, faster and more effeciently computing, good robustnees and easy implementation by hardware, SNNs have been used in mobile robots' control, environment perception and robots' vision successfully.
C1 [Wang Xiuqing; Huang Zhanhua] Hebei Normal Univ, Shijiazhuang 050031, Peoples R China.
RP Wang, XQ (corresponding author), Hebei Normal Univ, Shijiazhuang 050031, Peoples R China.
EM xiuqingwang2004@yahoo.com.cn; zengguang.hou@ia.ac.cn; min.tan@ia.ac.cn;
   ywang@itechs.iscas.ac.cn
CR ALAMDARI ARSA, 2005, T ENG COMPUTING TECH, P49
   Alnajjar FSK, 2010, NEUROSCI RES, V68, pE403, DOI 10.1016/j.neures.2010.07.1788
   [Anonymous], THESIS GRAZ U TECHNO
   [Anonymous], PULSED NEURAL NETWOR
   Belatreche A, 2003, P IEEE CYB INT CHALL, P39
   Bi GQ, 2002, BIOL CYBERN, V87, P319, DOI 10.1007/s00422-002-0349-7
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   Burgsteiner H, 2005, LECT NOTES ARTIF INT, V3533, P121
   Carnell A., ESANN2005
   Carrillo RR, 2008, BIOSYSTEMS, V94, P18, DOI 10.1016/j.biosystems.2008.05.008
   Di Paolo EA, 2002, ADAPT BEHAV, V10, P243, DOI 10.1177/1059712302010003006
   Eckhorn R, 1990, NEURAL COMPUT, V2, P293, DOI 10.1162/neco.1990.2.3.293
   ECKHORN R, 1993, NEUROREPORT, V4, P243, DOI 10.1097/00001756-199303000-00004
   Floreano D., 2001, LNCS, P38
   Floreano D., 2005, ARTIFICIAL LIFE, V11
   Floreano D, 2006, INT J INTELL SYST, V21, P1005, DOI 10.1002/int.20173
   Florian RV, 2006, LECT NOTES COMPUT SC, V4095, P570
   Gemser W., 2002, SPIKING NEURON MODEL
   Gerstner W, 2002, BIOL CYBERN, V87, P404, DOI 10.1007/s00422-002-0353-y
   Gerstner W, 1996, NATURE, V383, P76, DOI 10.1038/383076a0
   Gerstner W., 2002, SPIKING NEURON MODEL
   Guo B., 1997, J ELECT, V14, P117
   Guo BL, 1996, SCI CHINA SER E, V39, P11
   Gupta A, 2000, SCIENCE, V287, P273, DOI 10.1126/science.287.5451.273
   Hagras H, 2004, IEEE INT CONF ROBOT, P4620, DOI 10.1109/ROBOT.2004.1302446
   Homma N., 2006, P 2006 INT JOINT C N, P7599
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   Joshi P, 2005, NEURAL COMPUT, V17, P1715, DOI 10.1162/0899766054026684
   Joshi P., 2007, THESIS GRAZ U TECHNO
   Kasabov N, 2010, NEURAL NETWORKS, V23, P16, DOI 10.1016/j.neunet.2009.08.010
   Kasabov Nikola, 2009, Natural Computing, V8, P199, DOI 10.1007/s11047-008-9066-z
   Kasinski A., 2006, International Journal of Applied Mathematics and Computer Science, V16, P101
   Kepecs A, 2002, BIOL CYBERN, V87, P446, DOI 10.1007/s00422-002-0358-6
   Kistler WM, 2002, BIOL CYBERN, V87, P416, DOI 10.1007/s00422-002-0359-5
   KUBOTA N, 2006, P 2006 IEEE WORLD C, P530
   Kuroe Y., 2006, P IEEE INT JOINT C N, P7613
   Legenstein R, 2005, NEURAL COMPUT, V17, P2337, DOI 10.1162/0899766054796888
   Liu J., 2008, P INT C INT ROB SYST, P2191
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Maass W, 2004, MATH COMP BIOL SER, P575
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maass W, 1997, ADV NEUR IN, V9, P211
   Maass W., 1999, PULSED NEURAL NETWOR
   Markram H, 1998, P NATL ACAD SCI USA, V95, P5323, DOI 10.1073/pnas.95.9.5323
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   NADASDY Z, 1998, THESIS RUTGERS U
   Natschlager T, 1998, NETWORK-COMP NEURAL, V9, P319, DOI 10.1088/0954-898X/9/3/003
   NATSCHLAGER T, 2003, NEUROSCIENCE DATABAS, P123, DOI DOI 10.1007/978-1-4615-1079-6_9
   Pfister JP, 2006, NEURAL COMPUT, V18, P1318, DOI 10.1162/neco.2006.18.6.1318
   Pfister JP, 2003, LECT NOTES COMPUT SC, V2714, P92
   Ruf B, 1997, NEURAL PROCESS LETT, V5, P9, DOI 10.1023/A:1009697008681
   Schliebs S, 2009, NEURAL NETWORKS, V22, P623, DOI 10.1016/j.neunet.2009.06.038
   Soula H., 2005, 2005 P AM ASS ART IN, P6
   Wang X.Q., P 2010 ICCEE, P250
   Wang XQ, 2008, NEUROCOMPUTING, V71, P655, DOI 10.1016/j.neucom.2007.08.025
   Wang XQ, 2009, 2009 INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND COMPUTATIONAL INTELLIGENCE, VOL I, PROCEEDINGS, P194, DOI 10.1109/AICI.2009.448
   Wang XQ, 2008, ICNC 2008: FOURTH INTERNATIONAL CONFERENCE ON NATURAL COMPUTATION, VOL 4, PROCEEDINGS, P125, DOI 10.1109/ICNC.2008.718
   Wysoski SG, 2008, NEUROCOMPUTING, V71, P2563, DOI 10.1016/j.neucom.2007.12.038
NR 60
TC 0
Z9 0
U1 0
U2 2
PY 2011
BP 4133
EP 4138
UT WOS:000312652104045
DA 2023-11-16
ER

PT C
AU Lee, S
   Kim, K
   Kim, J
   Kim, Y
   Myung, H
AF Lee, Seunghee
   Kim, Kyukwang
   Kim, Jinki
   Kim, Yeeun
   Myung, Hyun
GP IEEE
TI Spike-inspired Deep Neural Network Design Using Binary Weight
SO 2018 18TH INTERNATIONAL CONFERENCE ON CONTROL, AUTOMATION AND SYSTEMS
   (ICCAS)
SE International Conference on Control Automation and Systems
DT Proceedings Paper
CT 18th International Conference on Control, Automation and Systems (ICCAS)
CY OCT 17-20, 2018
CL SOUTH KOREA
DE Spike-inspired neural network; spike-inspired block; binary weight;
   modified DenseNet
AB Recently, deep learning has achieved great results in many fields such as image classification However, most general artificial neural networks with deep learning require graphics processing unit (GP Us) because of hard workload and it requires a large amount of power consumption. When we think about humans, we can do a lot of things without consuming a lot of power compared to computers or electric appliances. Human beings or organisms transmit and recognize information through signal transmission between neurons. This study aims to develop a novel deep neural network architecture which simulates the signaling system between biological neurons, unlike conventional neural networks. We propose a novel spike-inspired deep neural network structure with the spike-inspired block using binary weight motivated by spike's on and off mechanism. We have also designed a modified DenseNet architecture consisting of spike-inspired blocks. Our proposed method was tested and validated with MNIST datasets. The obtained results show the potential of a spike-inspired deep neural network.
C1 [Lee, Seunghee; Kim, Jinki; Myung, Hyun] Korea Adv Inst Sci & Technol, Dept Civil & Environm Engn, Daejeon 34141, South Korea.
   [Kim, Kyukwang; Kim, Yeeun; Myung, Hyun] Korea Adv Inst Sci & Technol, Robot Program, Daejeon 34141, South Korea.
RP Myung, H (corresponding author), Korea Adv Inst Sci & Technol, Dept Civil & Environm Engn, Daejeon 34141, South Korea.
EM seunghee.lee@kaist.ac.kr; kkim0214@kaist.ac.kr; rlawlsrl@kaistac.kr;
   yeeunk@kaist.ac.kr; hmyung@kaist.ac.kr
CR Abadi M., 2015, 12 USENIX S OPERATIN
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   King DB, 2015, ACS SYM SER, V1214, P1
   LeCun Y, 2018, MNIST HANDWRITTEN DI
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   NVIDIA, 2018, GTX 1080
   Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32
NR 7
TC 2
Z9 2
U1 0
U2 4
PY 2018
BP 247
EP 250
UT WOS:000457612300039
DA 2023-11-16
ER

PT J
AU Stöckl, C
   Maass, W
AF Stoeckl, Christoph
   Maass, Wolfgang
TI Optimized spiking neurons can classify images with high accuracy through
   temporal coding with two spikes
SO NATURE MACHINE INTELLIGENCE
DT Article
ID NETWORKS; DYNAMICS
AB Spike-based neuromorphic hardware promises to reduce the energy consumption of image classification and other deep-learning applications, particularly on mobile phones and other edge devices. However, direct training of deep spiking neural networks is difficult, and previous methods for converting trained artificial neural networks to spiking neurons were inefficient because the neurons had to emit too many spikes. We show that a substantially more efficient conversion arises when one optimizes the spiking neuron model for that purpose, so that it not only matters for information transmission how many spikes a neuron emits, but also when it emits those spikes. This advances the accuracy that can be achieved for image classification with spiking neurons, and the resulting networks need on average just two spikes per neuron for classifying an image. In addition, our new conversion method improves latency and throughput of the resulting spiking networks.
   Spiking neural networks could offer a low-energy consuming solution to deep learning applications on the edge and in mobile devices. Using temporal coding, where the timing of spikes carries extra information, a new method efficiently converts conventional artificial neural networks to spiking networks.
C1 [Stoeckl, Christoph; Maass, Wolfgang] Graz Univ Technol, Inst Theoret Comp Sci, Graz, Austria.
RP Maass, W (corresponding author), Graz Univ Technol, Inst Theoret Comp Sci, Graz, Austria.
EM maass@igi.tugraz.at
CR Bakken, 2020, PREPRINT BIORXIV, DOI 10.1101/2020.03.31.016972
   Bellec G, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-17236-y
   Billaudelle S, 2020, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS45731.2020.9180960
   Bing Han, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13555, DOI 10.1109/CVPR42600.2020.01357
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Frady E. Paxon, 2020, NICE '20: Proceedings of the Neuro-inspired Computational Elements Workshop, DOI 10.1145/3381755.3398695
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   García-Martín E, 2019, J PARALLEL DISTR COM, V134, P75, DOI 10.1016/j.jpdc.2019.07.007
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   Gouwens NW, 2019, NAT NEUROSCI, V22, P1182, DOI 10.1038/s41593-019-0417-0
   Harris KD, 2002, NATURE, V417, P738, DOI 10.1038/nature00808
   Hendrycks Dan, 2017, ICLR
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/TPAMI.2019.2913372, 10.1109/CVPR.2018.00745]
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90
   Kheradpisheh SR, 2020, INT J NEURAL SYST, V30, DOI 10.1142/S0129065720500276
   Kopanitsa, 2018, PREPRINT BIORXIV, DOI 10.1101/500447
   Krizhevsky Alex, 2009, LEARNING MULTIPLE LA
   Lee C, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00119
   Ling, 2001, PHYS FACTBOOK
   Maass W, 1997, NEURAL COMPUT, V9, P279, DOI 10.1162/neco.1997.9.2.279
   Maass W, 1998, COMPUTATIONAL NEUROSCIENCE: TRENDS IN RESEARCH, P221
   Markram H, 2004, NAT REV NEUROSCI, V5, P793, DOI 10.1038/nrn1519
   Parekh O, 2018, SPAA'18: PROCEEDINGS OF THE 30TH ACM SYMPOSIUM ON PARALLELISM IN ALGORITHMS AND ARCHITECTURES, P67, DOI 10.1145/3210377.3210410
   Ramachandran P., 2017, 6 INT C LEARN REPR I, P1, DOI DOI 10.48550/ARXIV.1710.05941
   Rathi Nitin, 2020, INT C LEARN REPR
   Rueckauer B, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351295
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Sterling P., 2015, PRINCIPLES NEURAL DE
   Stockl, 2019, PREPRINT
   Tan Mingxing, 2019, INT C MACH LEARN, P6105, DOI DOI 10.48550/ARXIV.1905.11946
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   Van Horn G, 2015, PROC CVPR IEEE, P595, DOI 10.1109/CVPR.2015.7298658
   Wozniak S, 2020, NAT MACH INTELL, V2, P325, DOI 10.1038/s42256-020-0187-0
   Yousefzadeh A, 2019, 2019 IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE CIRCUITS AND SYSTEMS (AICAS 2019), P81, DOI [10.1109/AICAS.2019.8771624, 10.1109/aicas.2019.8771624]
   Zhang QR, 2019, NEUROCOMPUTING, V323, P37, DOI 10.1016/j.neucom.2018.09.038
NR 38
TC 48
Z9 51
U1 4
U2 73
PD MAR
PY 2021
VL 3
IS 3
DI 10.1038/s42256-021-00311-4
EA MAR 2021
UT WOS:000628923100004
DA 2023-11-16
ER

PT J
AU Sandhu, HS
   Fang, LP
   Guan, L
AF Sandhu, Harmanjot Singh
   Fang, Liping
   Guan, Ling
TI Forecasting day-ahead price spikes for the Ontario electricity market
SO ELECTRIC POWER SYSTEMS RESEARCH
DT Article
DE Spike forecasting; Price forecasting; Electricity prices; Neural
   networks; Data mining
ID CONFIDENCE-INTERVAL ESTIMATION; NEURAL-NETWORK; PREDICTION; MACHINE;
   MODEL
AB A novel methodology based on neural networks is presented to forecast day-ahead electricity spikes and prices. Day-ahead electricity prices are forecasted by the first neural network trained using a data set consisting of similar price days. Next, spike prices are identified from the forecasted prices using a spike classifier, and these spikes are re-forecasted by using neural networks trained over historical spike hours. Finally, a data re-constructor is used to achieve the overall day-ahead electricity spike and price forecasting. Numerical experiments are conducted using data from the wholesale electricity market of Ontario, Canada, and significant improvements are achieved in terms of forecasting accuracy. (C) 2016 Elsevier B.V. All rights reserved.
C1 [Sandhu, Harmanjot Singh; Fang, Liping] Ryerson Univ, Dept Mech & Ind Engn, 350 Victoria St, Toronto, ON, Canada.
   [Guan, Ling] Ryerson Univ, Dept Elect & Comp Engn, 350 Victoria St, Toronto, ON, Canada.
RP Fang, LP (corresponding author), Ryerson Univ, Dept Mech & Ind Engn, 350 Victoria St, Toronto, ON, Canada.
EM Ifang@ryerson.ca
CR Aggarwal SK, 2009, ELECTR POW COMPO SYS, V37, P495, DOI 10.1080/15325000802599353
   Amjady N, 2011, APPL SOFT COMPUT, V11, P4246, DOI 10.1016/j.asoc.2011.03.024
   Amjady N, 2010, ELECTR POW SYST RES, V80, P318, DOI 10.1016/j.epsr.2009.09.015
   Catalao JPS, 2007, ELECTR POW SYST RES, V77, P1297, DOI 10.1016/j.epsr.2006.09.022
   Christensen TM, 2012, INT J FORECASTING, V28, P400, DOI 10.1016/j.ijforecast.2011.02.019
   Conejo AJ, 2005, INT J FORECASTING, V21, P435, DOI 10.1016/j.ijforecast.2004.12.005
   Cuaresma JC, 2004, APPL ENERG, V77, P87, DOI 10.1016/S0306-2619(03)00096-5
   Coelhoa LD, 2011, ELECTR POW SYST RES, V81, P74, DOI 10.1016/j.epsr.2010.07.015
   Eichler M, 2014, J ENERGY MARKETS, V7, P55, DOI 10.21314/JEM.2014.104
   Garcia RC, 2005, IEEE T POWER SYST, V20, P867, DOI 10.1109/TPWRS.2005.846044
   González AM, 2005, IEEE T POWER SYST, V20, P13, DOI 10.1109/TPWRS.2004.840412
   Guo JJ, 2004, IEEE T POWER SYST, V19, P1867, DOI 10.1109/TPWRS.2004.837759
   Hong YY, 2005, ELECTR POW SYST RES, V73, P151, DOI 10.1016/j.epsr.2004.07.002
   Hong YY, 2002, IEE P-GENER TRANSM D, V149, P621, DOI 10.1049/ip-gtd:20020371
   Lu X, 2005, ELECTR POW SYST RES, V73, P19, DOI 10.1016/j.epsr.2004.06.002
   Mandal P, 2007, IEEE T POWER SYST, V22, P2058, DOI 10.1109/TPWRS.2007.907386
   Mandal P, 2009, IEEE T IND APPL, V45, P1888, DOI 10.1109/TIA.2009.2027542
   Nogales FJ, 2006, J OPER RES SOC, V57, P350, DOI 10.1057/palgrave.jors.2601995
   Nogales FJ, 2002, IEEE T POWER SYST, V17, P342, DOI 10.1109/TPWRS.2002.1007902
   Reston JC, 2014, ELECTR POW SYST RES, V117, P115, DOI 10.1016/j.epsr.2014.08.006
   Rodriguez CP, 2004, IEEE T POWER SYST, V19, P366, DOI 10.1109/TPWRS.2003.821470
   Sandhu H. S., 2014, P 11 INT C SERV SYST
   Shafie-khah M, 2011, ENERG CONVERS MANAGE, V52, P2165, DOI 10.1016/j.enconman.2010.10.047
   Vahidinasab V, 2008, ELECTR POW SYST RES, V78, P1332, DOI 10.1016/j.epsr.2007.12.001
   Veit DJ, 2006, INT J MANAG SCI ENG, V1, P83, DOI 10.1080/17509653.2006.10671000
   Wu W, 2006, LECT NOTES ARTIF INT, V4093, P205
   Zareipour H, 2007, IEEE T POWER SYST, V22, P1782, DOI 10.1109/TPWRS.2007.907979
   Zareipour H, 2007, ENERG POLICY, V35, P4739, DOI 10.1016/j.enpol.2007.04.006
   Zhang GP, 2005, EUR J OPER RES, V160, P501, DOI 10.1016/j.ejor.2003.08.037
   Zhang JL, 2012, COMPUT IND ENG, V63, P695, DOI 10.1016/j.cie.2012.03.016
   Zhang L, 2005, IEEE T POWER SYST, V20, P59, DOI 10.1109/TPWRS.2004.840416
   Zhang L, 2003, IEEE T POWER SYST, V18, P99, DOI 10.1109/TPWRS.2002.807062
   Zhao JH, 2007, IET GENER TRANSM DIS, V1, P647, DOI 10.1049/iet-gtd:20060217
   Zhao JH, 2007, IEEE T POWER SYST, V22, P376, DOI 10.1109/TPWRS.2006.889139
NR 34
TC 40
Z9 40
U1 0
U2 22
PD DEC
PY 2016
VL 141
BP 450
EP 459
DI 10.1016/j.epsr.2016.08.005
UT WOS:000385598200043
DA 2023-11-16
ER

PT C
AU Asghar, MS
   Arslan, S
   Kim, H
AF Asghar, Malik Summair
   Arslan, Saad
   Kim, HyungWon
GP IEEE
TI Low Power Spiking Neural Network Circuit with Compact Synapse and Neuron
   Cells
SO 2020 17TH INTERNATIONAL SOC DESIGN CONFERENCE (ISOCC 2020)
SE International SoC Design Conference
DT Proceedings Paper
CT 17th International SoC Design Conference (ISOCC)
CY OCT 21-24, 2020
CL Yeosu, SOUTH KOREA
DE Spike Neural Network(SNN); Leaky Integrate and Fire(LIF); Neuromorphic;
   Artificial Neural Networks(ANN); Image classification; CMOS
AB Spiking neural networks performs efficient learning and recognition tasks by mimicking the neural biology of human brain. To realize a large-scale network on chip for mobile applications an area and power optimized electronic neuron along with synapse is essential. In this paper we present an analog CMOS based implementation of neuron and synapse circuits realized using 180nm process. The neurons integrate input currents from the synapse inputs and generate a spike output event based on the membrane potential. The proposed circuits have been optimized for area and power consumption and therefore can be used as key components to form a large spiking neural network.
C1 [Asghar, Malik Summair; Arslan, Saad; Kim, HyungWon] Chungbuk Natl Univ, Sch Elect Engn, Cheongju, South Korea.
   [Asghar, Malik Summair; Arslan, Saad] COMSATS Univ, Elect & Comp Engn Dept, Islamabad, Pakistan.
RP Asghar, MS (corresponding author), Chungbuk Natl Univ, Sch Elect Engn, Cheongju, South Korea.; Asghar, MS (corresponding author), COMSATS Univ, Elect & Comp Engn Dept, Islamabad, Pakistan.
EM summair@cbnu.ac.kr; saad@cbnu.ac.kr; hwkim@cbnu.ac.kr
CR Aamir SA, 2018, IEEE T CIRCUITS-I, V65, P4299, DOI 10.1109/TCSI.2018.2840718
   Aayush, 2018, P 54 ANN DES AUT C 2, P2385
   Kyuho Junyoung, J SEMICONDUCTOR TECH, V19, P129
   MEAD C, 1990, P IEEE, V78, P1629, DOI 10.1109/5.58356
   Miyashita D, 2016, IEEE ASIAN SOLID STA, P25, DOI 10.1109/ASSCC.2016.7844126
NR 5
TC 1
Z9 1
U1 0
U2 1
PY 2020
BP 157
EP 158
DI 10.1109/ISOCC50952.2020.9333105
UT WOS:000680824100077
DA 2023-11-16
ER

PT C
AU Masumori, A
   Ikegami, T
   Sinapayen, L
AF Masumori, Atsushi
   Ikegami, Takashi
   Sinapayen, Lana
GP IEEE
TI Predictive Coding as Stimulus Avoidance in Spiking Neural Networks
SO 2019 IEEE SYMPOSIUM SERIES ON COMPUTATIONAL INTELLIGENCE (IEEE SSCI
   2019)
DT Proceedings Paper
CT IEEE Symposium Series on Computational Intelligence (SSCI)
CY DEC 06-09, 2019
CL Xiamen, PEOPLES R CHINA
DE predictive coding; spiking neural networks; spike-timing dependent
   plasticity
ID MODEL
AB Predictive coding can be regarded as a function which reduces the error between an input signal and a top-down prediction. If reducing the error is equivalent to reducing the influence of stimuli from the environment, predictive coding can be regarded as stimulation avoidance by prediction. Our previous studies showed that action and selection for stimulation avoidance emerge in spiking neural networks through spike-timing dependent plasticity (STDP). In this study, we demonstrate that spiking neural networks with random structure spontaneously learn to predict temporal sequences of stimuli based solely on STDP.
C1 [Masumori, Atsushi; Ikegami, Takashi] Univ Tokyo, Grad Sch Arts & Sci, Tokyo, Japan.
   [Sinapayen, Lana] Sony Comp Sci Labs Inc, Tokyo, Japan.
   [Sinapayen, Lana] Earth Life Sci Inst, Tokyo, Japan.
RP Masumori, A (corresponding author), Univ Tokyo, Grad Sch Arts & Sci, Tokyo, Japan.
EM masumori@sacral.c.u-tokyo.ac.jp; ikeg@sacral.c.u-tokyo.ac.jp;
   lana.sinapayen@gmail.com
CR Bastos AM, 2012, NEURON, V76, P695, DOI 10.1016/j.neuron.2012.10.038
   Buonomano DV, 2000, J NEUROSCI, V20, P1129, DOI 10.1523/JNEUROSCI.20-03-01129.2000
   Clark A, 2013, BEHAV BRAIN SCI, V36, P181, DOI 10.1017/S0140525X12000477
   Friston KJ, 2009, PHILOS T R SOC B, V364, P1211, DOI 10.1098/rstb.2008.0300
   Huang YP, 2011, WIRES COGN SCI, V2, P580, DOI 10.1002/wcs.142
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Lotter W., 2017, 5 INT C LEARN REPR I
   Masumori A., 2017, 2 INT S SWARM BEH BI
   Masumori A., 2019, ARTIFICIAL LIFE
   Masumori A, 2015, ECAL 2015: THE THIRTEENTH EUROPEAN CONFERENCE ON ARTIFICIAL LIFE, P373, DOI 10.7551/978-0-262-33027-5-ch067
   Masumori A, 2017, FOURTEENTH EUROPEAN CONFERENCE ON ARTIFICIAL LIFE (ECAL 2017), P275
   Masumori A, 2018, 2018 CONFERENCE ON ARTIFICIAL LIFE (ALIFE 2018), P163
   Rao RPN, 1999, NAT NEUROSCI, V2, P79, DOI 10.1038/4580
   Rao RPN, 2001, NEURAL COMPUT, V13, P2221, DOI 10.1162/089976601750541787
   Sinapayen L, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0170388
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Wacongne C, 2012, J NEUROSCI, V32, P3665, DOI 10.1523/JNEUROSCI.5003-11.2012
NR 17
TC 3
Z9 3
U1 2
U2 4
PY 2019
BP 271
EP 277
UT WOS:000555467200038
DA 2023-11-16
ER

PT J
AU Hamedani, K
   Liu, LJ
   Hu, SY
   Ashdown, J
   Wu, JS
   Yi, Y
AF Hamedani, Kian
   Liu, Lingjia
   Hu, Shiyan
   Ashdown, Jonathan
   Wu, Jinsong
   Yi, Yang
TI Detecting Dynamic Attacks in Smart Grids Using Reservoir Computing: A
   Spiking Delayed Feedback Reservoir Based Approach
SO IEEE TRANSACTIONS ON EMERGING TOPICS IN COMPUTATIONAL INTELLIGENCE
DT Article
DE Spiking neural networks; recurrent neural network; delayed feedback
   reservoir; false data injection
AB Spiking neural networks have been widely used for supervised pattern recognition exploring the underlying spatio-temporal correlation. Meanwhile, spatio-temporal correlation manifests significantly between different components in a smart grid making the spiking neural network a desirable candidate for false data injection attack detection. In this paper, we develop a spiking-neural-network-based technique for dynamic cyber-attack detection in a smart grid. This is achieved through judiciously integrating spiking neurons with a special recurrent neural network called the delayed feedback reservoir computing. The inter-spike interval encoding is also explored in the precise-spike-driven synaptic plasticity based training process. The simulation results suggest that the introduced method outperforms multi-layer perceptrons and can achieve a significantly better performance compared to the state-of-the-art techniques. Furthermore, our analysis indicates that the delay value in the delayed feedback reservoir will have a substantial impact on the overall system performance.
C1 [Hamedani, Kian; Liu, Lingjia; Yi, Yang] Virginia Polytech Inst & State Univ, Bradley Dept Elect & Comp Engn, Blacksburg, VA 24061 USA.
   [Hu, Shiyan] Michigan Technol Univ, Dept Elect & Comp Engn, Houghton, MI 49931 USA.
   [Hu, Shiyan] Univ Essex, Sch Comp Sci & Elect Engn, Colchester C04 3SQ, Essex, England.
   [Ashdown, Jonathan] Air Force Res Lab, Dept Informat Directorate, Rome, NY 13441 USA.
   [Wu, Jinsong] Univ Chile, Dept Elect Engn, Santiago 8370451, Chile.
RP Liu, LJ (corresponding author), Virginia Polytech Inst & State Univ, Bradley Dept Elect & Comp Engn, Blacksburg, VA 24061 USA.; Wu, JS (corresponding author), Univ Chile, Dept Elect Engn, Santiago 8370451, Chile.
EM hkian@vt.edu; ljliu@ieee.org; shiyan.hu.us@ieee.org;
   jonathan.ashdown@us.af.mil; wujs@ieee.org; yangyi8@vt.edu
CR Appeltant L, 2011, NAT COMMUN, V2, DOI 10.1038/ncomms1476
   Cui SG, 2012, IEEE SIGNAL PROC MAG, V29, P106, DOI 10.1109/MSP.2012.2185911
   Esmalifalak M, 2017, IEEE SYST J, V11, P1644, DOI 10.1109/JSYST.2014.2341597
   Esmalifalak M, 2013, IEEE T SMART GRID, V4, P160, DOI 10.1109/TSG.2012.2224391
   Esser SK, 2016, P NATL ACAD SCI USA, V113, P11441, DOI 10.1073/pnas.1604850113
   Gawne TJ, 1996, J NEUROPHYSIOL, V76, P1356, DOI 10.1152/jn.1996.76.2.1356
   GILES CL, 1987, APPL OPTICS, V26, P4972, DOI 10.1364/AO.26.004972
   Hamedani K, 2018, IEEE T IND INFORM, V14, P734, DOI 10.1109/TII.2017.2769106
   He YB, 2017, IEEE T SMART GRID, V8, P2505, DOI 10.1109/TSG.2017.2703842
   Hegger R, 1998, PHYS REV LETT, V81, P558, DOI 10.1103/PhysRevLett.81.558
   Hu J, 2013, NEURAL COMPUT, V25, P450, DOI 10.1162/NECO_a_00395
   Kim J, 2014, CONF REC ASILOMAR C, P345, DOI [10.7230/KOSCAS.2014.34.345, 10.1109/ACSSC.2014.7094460]
   [刘宇庆 LIU Yu-qing], 2009, [中国土壤与肥料, Soil and Fertilizer Sciences in China], P1
   Lukosevicius M, 2009, COMPUT SCI REV, V3, P127, DOI 10.1016/j.cosrev.2009.03.005
   Mohemmed A, 2012, INT J NEURAL SYST, V22, DOI 10.1142/S0129065712500128
   Mosleh S, 2018, IEEE T NEUR NET LEAR, V29, P4694, DOI 10.1109/TNNLS.2017.2766162
   Nadasdy Z, 2009, FRONT SYST NEUROSCI, V3, DOI 10.3389/neuro.06.006.2009
   Ozay M, 2016, IEEE T NEUR NET LEAR, V27, P1773, DOI 10.1109/TNNLS.2015.2404803
   Panzeri S, 2010, TRENDS NEUROSCI, V33, P111, DOI 10.1016/j.tins.2009.12.001
   Yan J, 2016, IEEE IJCNN, P1395, DOI 10.1109/IJCNN.2016.7727361
   Yang K, 2018, IEEE WIREL COMMUN, V25, P19, DOI 10.1109/MWC.2017.1800079
   Yu Q, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0078318
   Zhao CY, 2016, IEEE T MULTI-SCALE C, V2, P265, DOI 10.1109/TMSCS.2016.2607164
   Zhao CY, 2015, ACM J EMERG TECH COM, V12, DOI 10.1145/2738040
   Zhao H., 2017, IEEE T ENERGY CONVER, VPP, P1
   Zimmerman RD, 2011, IEEE T POWER SYST, V26, P12, DOI 10.1109/TPWRS.2010.2051168
NR 26
TC 25
Z9 25
U1 5
U2 11
PD JUN
PY 2020
VL 4
IS 3
BP 253
EP 264
DI 10.1109/TETCI.2019.2902845
UT WOS:000682799900006
DA 2023-11-16
ER

PT J
AU Markowska-Kaczmar, U
   Koldowski, M
AF Markowska-Kaczmar, Urszula
   Koldowski, Mateusz
TI Spiking neural network vs multilayer perceptron: who is the winner in
   the racing car computer game
SO SOFT COMPUTING
DT Article
DE Computer game controller; Spiking neural network; Multilayer perceptron;
   Evolutionary algorithm
ID CONTROLLERS
AB The paper presents two neural based controllers for the computer car racing game. The controllers represent two generations of neural networks-a multilayer perceptron and a spiking neural network. They are trained by an evolutionary algorithm. Efficiency of both approaches is experimentally tested and statistically analyzed.
C1 [Markowska-Kaczmar, Urszula; Koldowski, Mateusz] Wroclaw Univ Technol, PL-50370 Wroclaw, Poland.
RP Markowska-Kaczmar, U (corresponding author), Wroclaw Univ Technol, Wyb Wyspianskiego 27, PL-50370 Wroclaw, Poland.
EM urszula.markowska-kaczmar@pwr.edu.pl
CR [Anonymous], J HEURISTICS
   [Anonymous], 1998, PULSED NEURAL NETWOR
   Batllori R, 2011, PROCEDIA COMPUT SCI, V6, DOI 10.1016/j.procs.2011.08.060
   Bouganis A., 2010, P 2010 INT JOINT C N, P1, DOI DOI 10.1109/IJCNN.2010.5596525
   Cao JB, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND BIOMIMETICS, VOLS 1-5, P1900, DOI 10.1109/ROBIO.2007.4522457
   Cardamone L, 2009, IEEE C EVOL COMPUTAT, P2622, DOI 10.1109/CEC.2009.4983271
   Charles D, 2004, P 5 INT C COMPUTER G, P163
   Derrac J, 2011, SWARM EVOL COMPUT, V1, P3, DOI 10.1016/j.swevo.2011.02.002
   Dzienkowski BJ, 2010, LECT NOTES ARTIF INT, V6071, P110, DOI 10.1007/978-3-642-13541-5_12
   Ebner Marc, 2009, 2009 IEEE Symposium on Computational Intelligence and Games (CIG), P279, DOI 10.1109/CIG.2009.5286465
   Floreano D, 2001, LNCS, V2217
   Florian RV, 2006, LECT NOTES COMPUT SC, V4095, P570
   Gerstner W., 2002, SPIKING NEURON MODEL
   Hagras H, 2004, IEEE INT CONF ROBOT, P4620, DOI 10.1109/ROBOT.2004.1302446
   Huemer A, 2009, STUD COMPUT INTELL, V258, P225
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Lee W.-P., 1999, Soft Computing, V3, P63, DOI 10.1007/s005000050054
   Michalewicz Z., 1992, GENETIC ALGORITHMS D, V3rd
   Mitic M, 2014, SOFT COMPUT, V18, P1011, DOI 10.1007/s00500-013-1121-8
   Oh SY, 1998, P 5 WORLDS C INT TRA
   Picek Stjepan, 2012, 2012 35th International Convention on Information and Communication Technology, Electronics and Microelectronics, P1064
   Qualls J, 2009, EVOLUTIONARY COMPUTA
   Togelius J, 2005, IEEE C EVOL COMPUTAT, P1906
   Togelius J, 2006, IEEE C EVOL COMPUTAT, P1172
   Wang XQ, 2008, NEUROCOMPUTING, V71, P655, DOI 10.1016/j.neucom.2007.08.025
   Wiklendt L, 2009, NEURAL COMPUT APPL, V18, P369, DOI 10.1007/s00521-008-0187-1
   Yee E., 2011, Proceedings of the 2011 11th International Conference on Hybrid Intelligent Systems (HIS 2011), P411, DOI 10.1109/HIS.2011.6122141
   Yee Elias E., 2013, International Journal of Computer Information Systems and Industrial Management Applications, V5, P365
   Zheng J, 2013, EVALUATING IMPACT HE, V9
NR 29
TC 8
Z9 8
U1 0
U2 10
PD DEC
PY 2015
VL 19
IS 12
SI SI
BP 3465
EP 3478
DI 10.1007/s00500-014-1515-2
UT WOS:000365164400010
DA 2023-11-16
ER

PT J
AU Yin, BJ
   Corradi, F
   Bohté, S
AF Yin, Bojian
   Corradi, Federico
   Bohte, Sander
TI Effective and Efficient Spiking Recurrent Neural Networks
SO ERCIM NEWS
DT Article
AB Although inspired by biological brains, the neural networks that are the foundation of modern artificial intelligence (AI) use exponentially more energy than their counterparts in nature, and many local "edge" applications are energy constrained. New learning frameworks and more detailed, spiking neural models can be used to train high-performance spiking neural networks (SNNs) for significant and complex tasks, like speech recognition and EGC-analysis, where the spiking neurons communicate only sparingly. Theoretically, these networks outperform the energy efficiency of comparable classical artificial neural networks by two or three orders of magnitude.
C1 [Yin, Bojian; Bohte, Sander] CWI, Amsterdam, Netherlands.
   [Corradi, Federico] IMEC Holst Ctr, Eindhoven, Netherlands.
RP Bohté, S (corresponding author), CWI, Amsterdam, Netherlands.
EM S.M.Bohte@cwi.nl
CR Maass W, 1997, NEURAL COMPUT, V9, P279, DOI 10.1162/neco.1997.9.2.279
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Yin B, 2020, IEEE INTERNET THINGS, V7, P8748, DOI [10.1109/JIOT.2020.2996562, 10.1145/3407197.3407225]
NR 3
TC 0
Z9 0
U1 0
U2 3
PD APR
PY 2021
IS 125
SI SI
BP 9
EP 10
UT WOS:000637044400003
DA 2023-11-16
ER

PT C
AU Kreutz, F
   Gerhards, P
   Vogginger, B
   Knobloch, K
   Mayr, CG
AF Kreutz, Felix
   Gerhards, Pascal
   Vogginger, Bernhard
   Knobloch, Klaus
   Mayr, Christian Georg
GP IEEE
TI Applied Spiking Neural Networks for Radar-based Gesture Recognition
SO 2021 7TH INTERNATIONAL CONFERENCE ON EVENT BASED CONTROL, COMMUNICATION,
   AND SIGNAL PROCESSING (EBCCSP)
DT Proceedings Paper
CT 7th International Conference on Event-Based Control, Communication and
   Signal Processing (EBCCSP)
CY JUN 23-25, 2021
CL ELECTR NETWORK
DE Spiking Neural Network; Surrogate Gradient; Radar Gesture Recognition;
   Spike Coding
AB Spiking neural networks offer a promising approach for low power edge applications, especially when run on neuromorphic hardware. However, there are no well established approaches to setup such networks for real world applications. We demonstrate the use of spiking neural networks on the basis of radar data-based gesture recognition, while taking three different angle-encoding schemes into account, considering a two antenna based angle estimation. The surrogate gradient approach is used for direct training, while achieving a reasonable accuracy on all proposed encoding schemes. This work proposes a baseline approach for spiking networks and the corresponding encoding for the use in radar-based gesture classification.
C1 [Kreutz, Felix; Gerhards, Pascal; Knobloch, Klaus] Infineon Technol Dresden, Dresden, Germany.
   [Vogginger, Bernhard; Mayr, Christian Georg] Tech Univ Dresden, Dresden, Germany.
RP Kreutz, F (corresponding author), Infineon Technol Dresden, Dresden, Germany.
EM felix.kreutz@Infineon.com; Pascal.Gerhards@Infineon.com;
   Bernhard.Vogginger@tu-dresden.de; Klaus.Knobloch@Infineon.com;
   christian.mayr@tu-dresden.de
CR Banerjee D, 2020, IEEE IJCNN, DOI 10.1109/ijcnn48605.2020.9206853
   Bellec Guillaume, LONG SHORT TERM MEMO, P11
   Dekker B, 2017, EUROP RADAR CONF, P163, DOI 10.23919/EURAD.2017.8249172
   Gamba J., 2020, RADAR SIGNAL PROCESS, V1st
   Kim Y, 2017, PROC EUR CONF ANTENN, P1258, DOI 10.23919/EuCAP.2017.7928465
   Neftci EO, 2019, Arxiv, DOI arXiv:1901.09948
   Pellegrini T., 2020, LOW ACTIVITY SUPERVI
   Schuman CD, 2019, IEEE IJCNN
   Stephan M, 2021, INT C PATT RECOG, P9529, DOI 10.1109/ICPR48806.2021.9412858
   Wang SW, 2016, UIST 2016: PROCEEDINGS OF THE 29TH ANNUAL SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P851, DOI 10.1145/2984511.2984565
   Wang W, 2019, IEEE ACCESS, V7, P117165, DOI 10.1109/ACCESS.2019.2936604
   Yin BJ, 2020, Arxiv, DOI arXiv:2005.11633
   Zhang ZY, 2019, IEEE SENS J, V19, P6811, DOI 10.1109/JSEN.2019.2910810
   Zhang ZY, 2018, IEEE SENS J, V18, P3278, DOI 10.1109/JSEN.2018.2808688
NR 14
TC 1
Z9 1
U1 1
U2 1
PY 2021
DI 10.1109/EBCCSP53293.2021.9502357
UT WOS:000860940700001
DA 2023-11-16
ER

PT C
AU Chang, HY
   Li, JP
   Feng, MC
AF Chang, Haoyang
   Li, Jianping
   Feng, Mingchao
GP IEEE
TI SUPERVISED AND AGGREGATE-LABEL LEARNING ALGORITHM OF SPIKING NEURAL
   NETWORK
SO 2019 16TH INTERNATIONAL COMPUTER CONFERENCE ON WAVELET ACTIVE MEDIA
   TECHNOLOGY AND INFORMATION PROCESSING (ICWAMTIP)
SE International Computer Conference on Wavelet Active Media Technology and
   Information Processing
DT Proceedings Paper
CT 16th IEEE International Computer Conference on Wavelet Active Media
   Technology and Information Processing (ICCWAMTIP)
CY DEC 13-15, 2019
CL Univ Elect Sci & Technol China, Chengdu, PEOPLES R CHINA
HO Univ Elect Sci & Technol China
DE Spiking neurons; Spiking neural networks; Supervised learning;
   Aggregate-label learning; Synaptic weight
AB Spiking Neural Networks (SNNs) is closer to biological neural working mechanisms. Compared with the traditional neural network using rate coding, SNNs are able to process and abstract features from the temporal dynamics encoded in spike signals, thus prompting SNNs more biologically plausible and easier to implement on hardware. This paper describes the existing supervised and aggregate-label classic learning algorithms of SNNs, analyzes the merits and demerits of these algorithms, discusses the development direction of SNNs, and provides a basis for the research of efficient learning algorithms.
C1 [Chang, Haoyang; Li, Jianping] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu, Peoples R China.
   [Feng, Mingchao] JD AI Res, Chengdu, Peoples R China.
RP Chang, HY (corresponding author), Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu, Peoples R China.
EM 201821080204@std.uestc.edu.cn; jpli2222@uestc.edu.cn;
   fengmingchao@jd.com
CR Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   BROWN TH, 1990, ANNU REV NEUROSCI, V13, P475, DOI 10.1146/annurev.ne.13.030190.002355
   Butts DA, 2007, NATURE, V449, P92, DOI [10.1038/nature06105, 10.1038/natureO6105]
   Gerstner W., 2002, SPIKING NEURON MODEL
   Gütig R, 2016, SCIENCE, V351, DOI 10.1126/science.aab4113
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Izhikevich E. M., 2003, IEEE T NEURAL NETWOR, P35
   Johnson C, 2009, NEURAL NETWORKS, V22, P833, DOI 10.1016/j.neunet.2009.06.033
   Maass W, 1997, NEURAL COMPUT, V9, P279, DOI 10.1162/neco.1997.9.2.279
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   MaBouDi H, 2017, PLOS COMPUT BIOL, V13, DOI 10.1371/journal.pcbi.1005551
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Rapp H., 2018, ARXIV180507569
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Wu JB, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00836
   Wu J, 2019, FREE RADICAL RES, V53, P139, DOI 10.1080/10715762.2018.1549732
   Xia SM, 2018, INT CONF INSTR MEAS, P1, DOI 10.1109/IMCCC.2018.00008
   Xu Y, 2013, NEURAL NETWORKS, V43, P99, DOI 10.1016/j.neunet.2013.02.003
   Yu Q, 2013, IEEE T NEUR NET LEAR, V24, P1539, DOI 10.1109/TNNLS.2013.2245677
   Zenke F, 2018, NEURAL COMPUT, V30, P1514, DOI 10.1162/neco_a_01086
   Zheng Y, 2018, METHODS MOL BIOL, V1724, P1, DOI 10.1007/978-1-4939-7562-4_1
NR 21
TC 0
Z9 0
U1 0
U2 0
PY 2019
BP 33
EP 36
UT WOS:000553538700007
DA 2023-11-16
ER

PT J
AU Yang, GB
   Lee, WY
   Seo, Y
   Lee, CS
   Seok, W
   Park, J
   Sim, D
   Park, C
AF Yang, Geunbo
   Lee, Wongyu
   Seo, Youjung
   Lee, Choongseop
   Seok, Woojoon
   Park, Jongkil
   Sim, Donggyu
   Park, Cheolsoo
TI Unsupervised Spiking Neural Network with Dynamic Learning of Inhibitory
   Neurons
SO SENSORS
DT Article
DE Bayesian inference; leaky integrate-and-fire model; spike
   timing-dependent plasticity; spiking neural network; unsupervised
   learning
ID DECISION-THEORY; PLASTICITY; MODEL; STDP
AB A spiking neural network (SNN) is a type of artificial neural network that operates based on discrete spikes to process timing information, similar to the manner in which the human brain processes real-world problems. In this paper, we propose a new spiking neural network (SNN) based on conventional, biologically plausible paradigms, such as the leaky integrate-and-fire model, spike timing-dependent plasticity, and the adaptive spiking threshold, by suggesting new biological models; that is, dynamic inhibition weight change, a synaptic wiring method, and Bayesian inference. The proposed network is designed for image recognition tasks, which are frequently used to evaluate the performance of conventional deep neural networks. To manifest the bio-realistic neural architecture, the learning is unsupervised, and the inhibition weight is dynamically changed; this, in turn, affects the synaptic wiring method based on Hebbian learning and the neuronal population. In the inference phase, Bayesian inference successfully classifies the input digits by counting the spikes from the responding neurons. The experimental results demonstrate that the proposed biological model ensures a performance improvement compared with other biologically plausible SNN models.
C1 [Yang, Geunbo; Seo, Youjung; Lee, Choongseop; Sim, Donggyu; Park, Cheolsoo] Kwangwoon Univ, Dept Comp Engn, Seoul 01897, South Korea.
   [Lee, Wongyu; Seok, Woojoon] Kwangwoon Univ, Dept Intelligent Informat & Embedded Software Eng, Seoul 01897, South Korea.
   [Park, Jongkil] Korea Inst Sci & Technol KIST, Ctr Neuromorph Engn, Seoul 02792, South Korea.
RP Sim, D; Park, C (corresponding author), Kwangwoon Univ, Dept Comp Engn, Seoul 01897, South Korea.
EM rmsqhwkd2@gmail.com; eew1729@gmail.com; youjungseo0317@gmail.com;
   cndtjq97@gmail.com; swj20000@gmail.com; jongkil@kist.re.kr;
   dgsim@kw.ac.kr; parkcheolsoo@kw.ac.kr
CR Auge D, 2021, NEURAL PROCESS LETT, V53, P4693, DOI 10.1007/s11063-021-10562-2
   Baudry M, 1998, NEUROBIOL LEARN MEM, V70, P113, DOI 10.1006/nlme.1998.3842
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   BIENENSTOCK EL, 1982, J NEUROSCI, V2, P32, DOI 10.1523/jneurosci.02-01-00032.1982
   Bohte S. M., 2000, 8th European Symposium on Artificial Neural Networks. ESANN"2000. Proceedings, P419
   Brader JM, 2007, NEURAL COMPUT, V19, P2881, DOI 10.1162/neco.2007.19.11.2881
   Burkitt AN, 2006, BIOL CYBERN, V95, P1, DOI 10.1007/s00422-006-0068-6
   Carroll BJ, 2018, J NEUROPHYSIOL, V119, P290, DOI 10.1152/jn.00447.2017
   Chamberland S, 2012, FRONT NEUROSCI-SWITZ, V6, DOI 10.3389/fnins.2012.00165
   Cohen G, 2017, IEEE IJCNN, P2921, DOI 10.1109/IJCNN.2017.7966217
   CONSUL PC, 1973, TECHNOMETRICS, V15, P791, DOI 10.2307/1267389
   Cooke SF, 2006, BRAIN, V129, P1659, DOI 10.1093/brain/awl082
   Demin V, 2018, FRONT NEUROINFORM, V12, DOI 10.3389/fninf.2018.00079
   Deneve S., 2004, P ADV NEURAL INFORM, VVolume 17
   DiCarlo JJ, 2012, NEURON, V73, P415, DOI 10.1016/j.neuron.2012.01.010
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Fatahi M, 2016, Arxiv, DOI arXiv:1604.06751
   Finkelstein A, 2004, COMPUTER, V37, P26, DOI 10.1109/MC.2004.1297236
   Friston KJ, 2006, J PHYSIOL-PARIS, V100, P70, DOI 10.1016/j.jphysparis.2006.10.001
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   Gerstner W., 2011, NEURON COGNITION VIA, P1
   Gerstner W, 2002, SPIKING NEURON MODEL, DOI [10.1017/CBO9780511815706, DOI 10.1017/CBO9780511815706]
   Han Jaewook, 2021, IEIE Transactions on Smart Processing & Computing, V10, P291
   Hao YZ, 2020, NEURAL NETWORKS, V121, P387, DOI 10.1016/j.neunet.2019.09.007
   Hazan H, 2018, FRONT NEUROINFORM, V12, DOI 10.3389/fninf.2018.00089
   HEBB D. O., 1949
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Hussain S, 2014, IEEE INT SYMP CIRC S, P2640, DOI 10.1109/ISCAS.2014.6865715
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2003, NEURAL COMPUT, V15, P1511, DOI 10.1162/089976603321891783
   Jaiswal A, 2017, IEEE T ELECTRON DEV, V64, P1818, DOI 10.1109/TED.2017.2671353
   Jonas P, 2007, SCHOLARPEDIA, V2, P3286, DOI DOI 10.4249/SCHOLARPEDIA.3286
   Khacef L, 2020, ELECTRONICS-SWITZ, V9, DOI 10.3390/electronics9101605
   Körding K, 2007, SCIENCE, V318, P606, DOI 10.1126/science.1142998
   Kohn A, 2016, ANNU REV NEUROSCI, V39, P237, DOI 10.1146/annurev-neuro-070815-013851
   Kohonen T, 2000, IEEE T NEURAL NETWOR, V11, P574, DOI 10.1109/72.846729
   Kording KP, 2006, TRENDS COGN SCI, V10, P319, DOI 10.1016/j.tics.2006.05.003
   Krotov D, 2019, P NATL ACAD SCI USA, V116, P7723, DOI 10.1073/pnas.1820458116
   Laurens J, 2007, BIOL CYBERN, V96, P389, DOI 10.1007/s00422-006-0133-1
   LeCun Y., MNIST DATABASE HANDW
   Liu YC, 2014, J NEUROSCI, V34, P1344, DOI 10.1523/JNEUROSCI.2566-13.2014
   Lodish H., 2000, MOL CELL BIOL
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Merolla P, 2011, IEEE CUST INTEGR CIR
   Neil D, 2014, IEEE T VLSI SYST, V22, P2621, DOI 10.1109/TVLSI.2013.2294916
   O'reilly R.C., 2000, COMPUTATIONAL EXPLOR
   Panzeri S, 2015, TRENDS COGN SCI, V19, P162, DOI 10.1016/j.tics.2015.01.002
   Pouget A, 2013, NAT NEUROSCI, V16, P1170, DOI 10.1038/nn.3495
   Querlioz D, 2013, IEEE T NANOTECHNOL, V12, P288, DOI 10.1109/TNANO.2013.2250995
   Rao RPN, 1999, NAT NEUROSCI, V2, P79, DOI 10.1038/4580
   Rashed-Al-Mahfuz M, 2021, BIOMED ENG LETT, V11, P147, DOI 10.1007/s13534-021-00185-w
   Rousselet GA, 2002, NAT NEUROSCI, V5, P629, DOI 10.1038/nn866
   Ruder S, 2017, Arxiv, DOI arXiv:1609.04747
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Sacramento J, 2018, ADV NEUR IN, V31
   Sanger TD, 2003, CURR OPIN NEUROBIOL, V13, P238, DOI 10.1016/S0959-4388(03)00034-5
   Saunders DJ, 2019, NEURAL NETWORKS, V119, P332, DOI 10.1016/j.neunet.2019.08.016
   Sboev A, 2018, PROCEDIA COMPUT SCI, V123, P494, DOI 10.1016/j.procs.2018.01.075
   She XY, 2021, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.615756
   Stevenson IH, 2009, PLOS COMPUT BIOL, V5, DOI 10.1371/journal.pcbi.1000629
   Tavanaei A, 2018, 2018 INT JOINT C NEU, DOI DOI 10.1109/IJCNN.2018.8489104
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Valencia D, 2022, BIOMED ENG LETT, V12, P185, DOI 10.1007/s13534-022-00217-z
   Vazquez AL, 2018, CEREB CORTEX, V28, P4105, DOI 10.1093/cercor/bhy225
   Widrow B, 2019, ARTIFICIAL INTELLIGENCE IN THE AGE OF NEURAL NETWORKS AND BRAIN COMPUTING, P1, DOI 10.1016/B978-0-12-815480-9.00001-3
   Yang JQ, 2020, NANO ENERGY, V74, DOI 10.1016/j.nanoen.2020.104828
   Zador AM, 2019, NAT COMMUN, V10, DOI 10.1038/s41467-019-11786-6
   Zhang JW, 2019, Arxiv, DOI arXiv:1906.01703
   Zhang W, 2003, NAT REV NEUROSCI, V4, P885, DOI 10.1038/nrn1248
   Zhong Y., 2022, IEIE T SMART PROCESS, V11, P332, DOI [10.5573/IEIESPC.2022.11.5.332, DOI 10.5573/IEIESPC.2022.11.5.332]
NR 71
TC 0
Z9 0
U1 4
U2 4
PD AUG
PY 2023
VL 23
IS 16
AR 7232
DI 10.3390/s23167232
UT WOS:001056799000001
DA 2023-11-16
ER

PT J
AU Rosselló, JL
   de Paúl, I
   Canals, V
AF Rossello, Jose L.
   de Paul, Ivan
   Canals, Vincent
TI Self-configuring spiking neural networks
SO IEICE ELECTRONICS EXPRESS
DT Article
DE neural networks; spiking neural networks; hardware implementation of
   genetic algorithms
AB We present a simple architecture for Spiking Neural Networks self-configuration. It consists in the hardware implementation of a simple Genetic Algorithm that may be used to obtain optimum network configurations. The proposed solution is applied to estimate the processing efficiency of different networks. Based on the results we develop a new performance metric to calibrate the processing capacity of SNNs.
C1 [Rossello, Jose L.; de Paul, Ivan; Canals, Vincent] Univ Balearic Isl, Elect Syst Grp, Palma de Mallorca 07122, Balearic Isl, Spain.
RP Rosselló, JL (corresponding author), Univ Balearic Isl, Elect Syst Grp, Cra Valldemossa Km 7-5, Palma de Mallorca 07122, Balearic Isl, Spain.
EM j.rossello@uib.es
CR Dowrick T, 2007, IEEE IJCNN, P715, DOI 10.1109/IJCNN.2007.4371045
   Gerstner W., 2002, SPIKING NEURON MODEL
   LANGLOIS N, 2000, P INT JOINT C NEUR N, V4, P485
   Malaka R., 2000, P INT JOINT C NEUR N, V6, P486, DOI 10.1109/IJCNN.2000.859442
   Pearson M. J., 2005, Proceedings. 2005 International Conference on Field Programmable Logic and Applications (IEEE Cat. No.05EX1155), P582
   Sala DM, 1999, IEEE T NEURAL NETWOR, V10, P953, DOI 10.1109/72.774270
   TORIKAI H, 2007, P INT JOINT C NEUR N, P2677
NR 7
TC 3
Z9 3
U1 0
U2 9
PD NOV 25
PY 2008
VL 5
IS 22
BP 921
EP 926
DI 10.1587/elex.5.921
UT WOS:000263988600002
DA 2023-11-16
ER

PT J
AU Yang, SF
   Wu, Q
   Li, RF
AF Yang, Shufan
   Wu, Qiang
   Li, Renfa
TI A case for spiking neural network simulation based on configurable
   multiple-FPGA systems
SO COGNITIVE NEURODYNAMICS
DT Article
DE Spiking neural network; Visual cortex; FPGA; Configurable
ID MODEL; CAT
AB Recent neuropsychological research has begun to reveal that neurons encode information in the timing of spikes. Spiking neural network simulations are a flexible and powerful method for investigating the behaviour of neuronal systems. Simulation of the spiking neural networks in software is unable to rapidly generate output spikes in large-scale of neural network. An alternative approach, hardware implementation of such system, provides the possibility to generate independent spikes precisely and simultaneously output spike waves in real time, under the premise that spiking neural network can take full advantage of hardware inherent parallelism. We introduce a configurable FPGA-oriented hardware platform for spiking neural network simulation in this work. We aim to use this platform to combine the speed of dedicated hardware with the programmability of software so that it might allow neuroscientists to put together sophisticated computation experiments of their own model. A feed-forward hierarchy network is developed as a case study to describe the operation of biological neural systems (such as orientation selectivity of visual cortex) and computational models of such systems. This model demonstrates how a feed-forward neural network constructs the circuitry required for orientation selectivity and provides platform for reaching a deeper understanding of the primate visual system. In the future, larger scale models based on this framework can be used to replicate the actual architecture in visual cortex, leading to more detailed predictions and insights into visual perception phenomenon.
C1 [Yang, Shufan; Wu, Qiang; Li, Renfa] Hunan Univ, Embedded Syst & Networking Lab, Sch Comp & Commun, Changsha 410082, Hunan, Peoples R China.
RP Yang, SF (corresponding author), Hunan Univ, Embedded Syst & Networking Lab, Sch Comp & Commun, Changsha 410082, Hunan, Peoples R China.
EM sfyang@hnu.cn
CR [Anonymous], 2003, COMPUTER ARCHITECTUR
   Becker J, 2007, P IEEE, V95, P438, DOI 10.1109/JPROC.2006.888404
   Burkitt AN, 2006, BIOL CYBERN, V95, P1, DOI 10.1007/s00422-006-0068-6
   Chen K, 2002, NEURAL NETWORKS, V15, P423, DOI 10.1016/S0893-6080(02)00028-X
   Culurciello E, 2003, IEEE J SOLID-ST CIRC, V38, P281, DOI 10.1109/JSSC.2002.807412
   Dayan P., 2001, THEORETICAL NEUROSCI
   Gerstner W., 2002, SPIKING NEURON MODEL
   Girau B, 2007, NEUROCOMPUTING, V70, P1186, DOI 10.1016/j.neucom.2006.11.009
   Gokhale M, 2005, RECONFIGURABLE COMPU
   GOTARREDONA S, 2009, T NEUR NETW, V20, P115
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Hubel D. H., 2004, BRAIN VISUAL PERCEPT
   HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837
   Hunt Jonathan J, 2011, Neural Syst Circuits, V1, P3, DOI 10.1186/2042-1001-1-3
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Kandel ER, 2000, PRINCIPLE NEUROSCIEN
   Koch Christof, 1999, P1
   Li GX, 2010, J NEUROSCI METH, V193, P62, DOI 10.1016/j.jneumeth.2010.07.031
   LOWEL S, 1994, J NEUROSCI, V14, P7451
   *NALL LTD, 2006, BENNYEY PCI X V4 REF
   Omondi AR, 2006, FPGA IMPLEMENTATIONS OF NEURAL NETWORKS, P1, DOI 10.1007/0-387-28487-7_1
   Rossmann M., 1996, Proceedings of the Fifth International Conference on Microelectronics for Neural Networks and Fuzzy Systems. MicroNeuro'96, P322, DOI 10.1109/MNNFS.1996.493810
   Satoh S, 2009, COGN NEURODYNAMICS, V3, P1, DOI 10.1007/s11571-008-9065-x
   *SPIKENET, 2011, SPIKENET NEUR NETW S
   UPEGUI A, 2004, LNCS, P233
   Wandell B. A., 1995, FDN VISION
   WILDIE M, 2009, P FIELD PROGR TECHN
   WORGOTTER F, 1991, J NEUROSCI, V11, P1959
   GEN NEURAL SIMULATIO
   2010, NEURON SOFTWARE SIMU
NR 30
TC 4
Z9 4
U1 0
U2 19
PD SEP
PY 2011
VL 5
IS 3
BP 301
EP 309
DI 10.1007/s11571-011-9170-0
UT WOS:000295979000008
DA 2023-11-16
ER

PT J
AU Kim, G
   Kim, K
   Choi, S
   Jang, HJ
   Jung, SO
AF Kim, Giseok
   Kim, Kiryong
   Choi, Sara
   Jang, Hyo Jung
   Jung, Seong-Ook
TI Area- and Energy-Efficient STDP Learning Algorithm for Spiking Neural
   Network SoC
SO IEEE ACCESS
DT Article
DE Neurons; History; Synapses; Artificial neural networks; Firing;
   Hardware; Biological neural networks; Spike-time dependent plasticity
   (STDP); time-step scaled STDP (TS-STDP); post-neuron spike-referred STDP
   (PR-STDP); spiking neural network (SNN)
ID SYNAPTIC PLASTICITY; MEMORY
AB Recently, spiking neural networks have gained attention owing to their energy efficiency. All-to-all spike-time dependent plasticity is a popular learning algorithm for spiking neural networks because it is suitable for nondifferentiable spike event-based learning and requires fewer computations than back-propagation-based algorithms. However, the hardware implementation of all-to-all spike-time dependent plasticity is limited by the large storage area required for spike history and large energy consumption caused by frequent memory access. We propose a time-step scaled spike-time dependent plasticity to reduce the storage area required for spike history by reducing the area of the spike-time dependent plasticity learning circuit by 60% and a post-neuron spike-referred spike-time dependent plasticity to reduce the energy consumption by 99.1% by efficiently accessing the memory while learning. The accuracy of Modified National Institute of Standards and Technology image classification degraded by less than 2% when both time-step scaled spike-time dependent plasticity and post-neuron spike-referred spike-time dependent plasticity were applied. Thus, the proposed hardware-friendly spike-time dependent plasticity algorithms make all-to-all spike-time dependent plasticity implementable in more compact areas while reducing energy consumption and experiencing insignificant accuracy degradation.
C1 [Kim, Giseok; Kim, Kiryong; Choi, Sara; Jang, Hyo Jung; Jung, Seong-Ook] Yonsei Univ, Sch Elect & Elect Engn, Seoul 03722, South Korea.
   [Choi, Sara; Jang, Hyo Jung] Samsung Elect Co Ltd, Yongin 17113, South Korea.
RP Jang, HJ (corresponding author), Yonsei Univ, Sch Elect & Elect Engn, Seoul 03722, South Korea.; Jang, HJ (corresponding author), Samsung Elect Co Ltd, Yongin 17113, South Korea.
EM sjung@yonsei.ac.kr
CR Abbott LF, 2000, NAT NEUROSCI, V3, P1178, DOI 10.1038/81453
   Cassidy A, 2011, IEEE INT SYMP CIRC S, P673
   Chen GK, 2019, IEEE J SOLID-ST CIRC, V54, P992, DOI 10.1109/JSSC.2018.2884901
   Chen ZY, 2020, IEEE ACCESS, V8, P115537, DOI 10.1109/ACCESS.2020.3004509
   Cho H, 2018, IEEE T BIOMED CIRC S, V12, P161, DOI 10.1109/TBCAS.2017.2762002
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Gerstner W., 2002, SPIKING NEURON MODEL
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Indiveri G, 2015, P IEEE, V103, P1379, DOI 10.1109/JPROC.2015.2444094
   Jeon J, 2020, IEEE ACCESS, V8, P96899, DOI 10.1109/ACCESS.2020.2995887
   Jiang P, 2019, IEEE ACCESS, V7, P59069, DOI 10.1109/ACCESS.2019.2914929
   Kang ZY, 2019, INT SYMP ASYNCHRON C, P48, DOI 10.1109/ASYNC.2019.00015
   Kim J, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00829
   Lei XY, 2019, IEEE ACCESS, V7, P124087, DOI 10.1109/ACCESS.2019.2927169
   Li BY, 2018, IEEE ACCESS, V6, P1560, DOI 10.1109/ACCESS.2017.2779462
   Liu N, 2019, IEEE ACCESS, V7, P173989, DOI 10.1109/ACCESS.2019.2957019
   Ponulak F, 2011, ACTA NEUROBIOL EXP, V71, P409
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Putra RVW, 2020, IEEE T COMPUT AID D, V39, P3601, DOI 10.1109/TCAD.2020.3013049
   Querlioz D, 2013, IEEE T NANOTECHNOL, V12, P288, DOI 10.1109/TNANO.2013.2250995
   Roberts PD, 2002, BIOL CYBERN, V87, P392, DOI 10.1007/s00422-002-0361-y
   Sandin F., 2014, J SOFTWARE ENG APPL, V7, P387, DOI [DOI 10.4236/JSEA.2014.75035, 10.4236/jsea.2014.75035]
   Sim J., 2019, P 34 INT TECH C CIRC, P1
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Wan JF, 2018, IEEE ACCESS, V6, P55419, DOI 10.1109/ACCESS.2018.2871724
   Yang XH, 2019, IEEE ACCESS, V7, P123788, DOI 10.1109/ACCESS.2019.2938900
NR 28
TC 3
Z9 4
U1 1
U2 9
PY 2020
VL 8
BP 216922
EP 216932
DI 10.1109/ACCESS.2020.3041946
UT WOS:000597214200001
DA 2023-11-16
ER

PT C
AU Talukder, R
   Porte, X
   Brunner, D
AF Talukder, Ria
   Porte, Xavier
   Brunner, Daniel
BE Volpe, G
   Pereira, JB
   Brunner, D
   Ozcan, A
TI Realisation of large-scale Photonic Spiking hardware system
SO EMERGING TOPICS IN ARTIFICIAL INTELLIGENCE (ETAI) 2022
SE Proceedings of SPIE
DT Proceedings Paper
CT Emerging Topics in Artificial Intelligence (ETAI)
CY AUG 21-25, 2022
CL San Diego, CA
DE spiking neural network; artificial neural network; liquid-state machine;
   reservoir computer
ID NETWORKS
AB An efficient photonic hardware integration of neural networks can benefit us from the inherent properties of parallelism, high-speed data processing and potentially low energy consumption. In artificial neural networks (ANN), neurons are classified as static, single and continuous-valued. On contrary, information transmission and computation in biological neurons occur through spikes, where spike time and rate play a significant role. Spiking neural networks (SNNs) are thereby more biologically relevant along with additional benefits in terms of hardware friendliness and energy-efficiency. Considering all these advantages, we designed a photonic reservoir computer (RC) based on photonic recurrent spiking neural networks (SNN) i.e. a liquid state machine. It is a scalable proof-of-concept experiment, comprising more than 30,000 neurons. This system presents an excellent testbed for demonstrating next generation bio-inspired learning in photonic systems.
C1 [Talukder, Ria; Porte, Xavier; Brunner, Daniel] Univ Bourgogne Franche Comte, UMR CNRS 6174, FEMTO ST Opt Dept, 15B Ave Mountboucons, F-25030 Besancon, France.
RP Talukder, R (corresponding author), Univ Bourgogne Franche Comte, UMR CNRS 6174, FEMTO ST Opt Dept, 15B Ave Mountboucons, F-25030 Besancon, France.
EM ria.talukder@femto-st.fr; javier.porte@femto-st.fr;
   daniel.brunner@femto-st.fr
CR [Anonymous], 2021, INTEL ADV NEUROMORPH
   Boahen K, 2017, COMPUT SCI ENG, V19, P14, DOI 10.1109/MCSE.2017.33
   Bueno J, 2018, OPTICA, V5, P756, DOI 10.1364/OPTICA.5.000756
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   nih, BRAIN INITIATIVE
   Ortega-Piwonka I., 2022, OPT MATER EXPRESS, P1
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Wang XW, 2020, NEURAL NETWORKS, V125, P258, DOI 10.1016/j.neunet.2020.02.011
   Ward-Foxton S., 2021, INNATERA UNVEILS NEU
NR 12
TC 0
Z9 0
U1 1
U2 1
PY 2022
VL 12204
AR 122040A
DI 10.1117/12.2633530
UT WOS:000944082700009
DA 2023-11-16
ER

PT J
AU Li, GQ
   Deng, L
   Chua, YS
   Li, P
   Neftci, EO
   Li, HH
AF Li, Guoqi
   Deng, Lei
   Chua, Yansong
   Li, Peng
   Neftci, Emre O.
   Li, Haizhou
TI Editorial: Spiking Neural Network Learning, Benchmarking, Programming
   and Executing
SO FRONTIERS IN NEUROSCIENCE
DT Editorial Material
DE deep spiking neural networks; SNN learning algorithms; programming
   framework; SNN benchmarks; neuromorphics
ID INTELLIGENCE
C1 [Li, Guoqi] Tsinghua Univ, Ctr Brain Inspired Comp Res, Dept Precis Instrument, Beijing, Peoples R China.
   [Li, Guoqi] Tsinghua Univ, Beijing Innovat Ctr Future Chips, Beijing, Peoples R China.
   [Deng, Lei; Li, Peng] Univ Calif Santa Barbara, Dept Elect & Comp Engn, Santa Barbara, CA 93106 USA.
   [Chua, Yansong] Huawei Technol, Shenzhen, Guangdong, Peoples R China.
   [Neftci, Emre O.] Univ Calif Irvine, Dept Cognit Sci, Irvine, CA 92717 USA.
   [Li, Haizhou] Natl Univ Singapore, Dept Elect Engn, Singapore, Singapore.
RP Li, GQ (corresponding author), Tsinghua Univ, Ctr Brain Inspired Comp Res, Dept Precis Instrument, Beijing, Peoples R China.; Li, GQ (corresponding author), Tsinghua Univ, Beijing Innovat Ctr Future Chips, Beijing, Peoples R China.
EM liguoqi@mail.tsinghua.edu.cn
CR Abadi M., 2016, TENSORFLOW LARGE SCA
   Aimar A, 2019, IEEE T NEUR NET LEAR, V30, P644, DOI 10.1109/TNNLS.2018.2852335
   Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   Al-Rfou Rami, 2016, ARXIV160502688
   [Anonymous], 2016, MATH PROBL ENG, DOI DOI 10.1155/2016/3763512
   Pérez-Carrasco JA, 2013, IEEE T PATTERN ANAL, V35, P2706, DOI 10.1109/TPAMI.2013.71
   Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Bottou Leon<prime>, 2012, NEURAL NETWORKS TRIC, P421, DOI DOI 10.1007/978-3-642-35289-8_25
   Cheng T, 2016, AIDS BEHAV, V20, P377, DOI 10.1007/s10461-015-1101-3
   Chollet F., 2015, KERAS
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Davison Andrew P, 2008, Front Neuroinform, V2, P11, DOI 10.3389/neuro.11.011.2008
   Dean J., 2012, ADV NEURAL INFORM PR, V25, DOI DOI 10.5555/2999134.2999271
   Deng L., 2019, ARXIV191100822
   Diehl P.U., 2015, 2015 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2015.7280696
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Franosch JMP, 2013, NEURAL COMPUT, V25, P3113, DOI 10.1162/NECO_a_00520
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   Han S., 2016, P INT C LEARN REPR, DOI DOI 10.48550/ARXIV.1510.00149
   Han S, 2016, CONF PROC INT SYMP C, P243, DOI 10.1109/ISCA.2016.30
   Hazan H, 2018, FRONT NEUROINFORM, V12, DOI 10.3389/fninf.2018.00089
   Hecht-Nielsen R., 1989, IJCNN: International Joint Conference on Neural Networks (Cat. No.89CH2765-6), P593, DOI 10.1109/IJCNN.1989.118638
   Huang HT, 2018, IEEE T NANOTECHNOL, V17, P645, DOI 10.1109/TNANO.2017.2732698
   Huang XW, 2015, ACTA POLYM SIN, P1133
   Ioffe S., 2015, PR MACH LEARN RES, P448
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Jouppi NP, 2017, 44TH ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA 2017), P1, DOI 10.1145/3079856.3080246
   Kheradpisheh SR, 2016, NEUROCOMPUTING, V205, P382, DOI 10.1016/j.neucom.2016.04.029
   Kingma DP., 2017, ARXIV
   Novikov A., 2015, TENSORIZING NEURAL N, P442
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   Paszke A., 2019, ADV NEURAL INFORM PR
   Paugam-Moisy H., 2006, SUPERVISED LEARNING
   Pei J, 2019, NATURE, V572, P106, DOI 10.1038/s41586-019-1424-8
   Querlioz D, 2013, IEEE T NANOTECHNOL, V12, P288, DOI 10.1109/TNANO.2013.2250995
   Roy K, 2019, NATURE, V575, P607, DOI 10.1038/s41586-019-1677-2
   Schemmel Johannes, 2012, 2012 IEEE International Symposium on Circuits and Systems - ISCAS 2012, DOI 10.1109/ISCAS.2012.6272131
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Wu J, 2019, J IMMUNOL RES, V2019, DOI 10.1155/2019/1749803
   Wu YJ, 2019, AAAI CONF ARTIF INTE, P1311
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Zhou Shuchang, 2016, ARXIV160606160
NR 42
TC 2
Z9 2
U1 0
U2 13
PD APR 15
PY 2020
VL 14
AR 276
DI 10.3389/fnins.2020.00276
UT WOS:000530469200001
DA 2023-11-16
ER

PT C
AU Han, HG
   Wang, LD
   Qiao, JF
   Feng, G
AF Han, Honggui
   Wang, Lidan
   Qiao, Junfei
   Feng, Gang
GP IEEE
TI A Spiking-based Mechanism for Self-organizing RBF Neural Networks
SO PROCEEDINGS OF THE 2014 INTERNATIONAL JOINT CONFERENCE ON NEURAL
   NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUL 06-11, 2014
CL Beijing, PEOPLES R CHINA
DE Spiking-based mechanism; spiking-based growing algorithm;
   self-organizing radial basis function neural network; nonlinear system
ID GLOBAL SENSITIVITY-ANALYSIS; LEARNING ALGORITHM; FUNCTION APPROXIMATION;
   GENETIC ALGORITHMS; MODEL SELECTION
AB In this paper, a spiking growing algorithm (SGA) is proposed for optimizing the structure of radial basis function (RBF) neural network. Inspired by the synchronous behavior of spiking neurons, the spiking strength (ss) of the hidden neurons is defined as the criteria of SGA, which investigates a new way to simulate the connections between hidden and output neurons of RBF neural network. This SGA-based RBF (SGA-RBF) neural network can self-organize the hidden neurons online, to achieve the appropriate network efficiency. Meanwhile, to ensure the accuracy of SGA-RBF neural network, the structure-adjusting and parameters-training phases are performed simultaneously. Simulation results demonstrate that the proposed method can obtain a higher precision in comparison with some other existing methods.
C1 [Han, Honggui; Wang, Lidan; Qiao, Junfei] Beijing Univ Technol, Coll Elect & Control Engn, Beijing, Peoples R China.
   [Han, Honggui; Feng, Gang] City Univ Hong Kong, Dept Mech & Biomed Engn, Kowloon, Hong Kong, Peoples R China.
   [Feng, Gang] Nanjing Univ Sci & Technol, Nanjing 210094, Jiangsu, Peoples R China.
RP Han, HG (corresponding author), Beijing Univ Technol, Coll Elect & Control Engn, Beijing, Peoples R China.
EM rechardhan@sina.com; wanglidan01@163.com; isibox@sina.com;
   megfeng@cityu.edu.hk
CR Alexander S., 2012, NEURAL NETWORKS, V33, P67
   Alexandridis A, 2003, NEURAL NETWORKS, V16, P1003, DOI 10.1016/S0893-6080(03)00052-2
   Alexandridis A, 2013, IEEE T NEUR NET LEAR, V24, P219, DOI 10.1109/TNNLS.2012.2227794
   Bortman M, 2009, IEEE T NEURAL NETWOR, V20, P1039, DOI 10.1109/TNN.2009.2019270
   CHEN S, 1991, IEEE T NEURAL NETWOR, V2, P302, DOI 10.1109/72.80341
   CHEN S, 1995, IEEE T SIGNAL PROCES, V43, P1713, DOI 10.1109/78.398734
   Chen W.-K, 1993, LINEAR NETWORKS SYST, P123
   Enrique R., 2008, IEEE TRANSCATIONS NE, V19, P431
   Ferentinos KP, 2005, NEURAL NETWORKS, V18, P934, DOI 10.1016/j.neunet.2005.03.010
   Ferrari S, 2010, IEEE T NEURAL NETWOR, V21, P275, DOI 10.1109/TNN.2009.2036438
   Glackin C, 2012, NEURAL NETWORKS, V32, P26, DOI 10.1016/j.neunet.2012.02.020
   Gomm JB, 2000, IEEE T NEURAL NETWOR, V11, P306, DOI 10.1109/72.839002
   Han HG, 2012, J PROCESS CONTR, V22, P1103, DOI 10.1016/j.jprocont.2012.04.002
   Han HG, 2012, IEEE T NEUR NET LEAR, V23, P342, DOI 10.1109/TNNLS.2011.2178559
   Huang GB, 2005, IEEE T NEURAL NETWOR, V16, P57, DOI 10.1109/TNN.2004.836241
   Huang GB, 2004, IEEE T SYST MAN CY B, V34, P2284, DOI 10.1109/TSMCB.2004.834428
   Islam MM, 2009, IEEE T SYST MAN CY B, V39, P1590, DOI 10.1109/TSMCB.2009.2021849
   KADIRKAMANATHAN V, 1993, NEURAL COMPUT, V5, P954, DOI 10.1162/neco.1993.5.6.954
   Leng G, 2006, IEEE T FUZZY SYST, V14, P755, DOI 10.1109/TFUZZ.2006.877361
   Lu YW, 1998, IEEE T NEURAL NETWOR, V9, P308, DOI 10.1109/72.661125
   Lu YW, 1997, NEURAL COMPUT, V9, P461, DOI 10.1162/neco.1997.9.2.461
   MACKAY DJC, 1992, NEURAL COMPUT, V4, P415, DOI [10.1162/neco.1992.4.3.415, 10.1162/neco.1992.4.3.448]
   Moody J, 1989, NEURAL COMPUT, V1, P281, DOI 10.1162/neco.1989.1.2.281
   Platt J, 1991, NEURAL COMPUT, V3, P213, DOI 10.1162/neco.1991.3.2.213
   Qiao JF, 2012, AUTOMATICA, V48, P1729, DOI 10.1016/j.automatica.2012.05.034
   Qiao JF, 2010, INT J NEURAL SYST, V20, P63, DOI 10.1142/S0129065710002243
   Saltelli A, 1999, TECHNOMETRICS, V41, P39, DOI 10.2307/1270993
   Shi D, 2005, NEURAL NETWORKS, V18, P951, DOI 10.1016/j.neunet.2005.02.006
   Sudret B, 2008, RELIAB ENG SYST SAFE, V93, P964, DOI 10.1016/j.ress.2007.04.002
   Venkatesh S, 2011, EXPERT SYST APPL, V38, P8978, DOI 10.1016/j.eswa.2011.01.115
   Walczak B, 1996, ANAL CHIM ACTA, V331, P177, DOI 10.1016/0003-2670(96)00202-4
   Xie TT, 2012, IEEE T NEUR NET LEAR, V23, P609, DOI 10.1109/TNNLS.2012.2185059
   Xu Y, 2013, NEURAL NETWORKS, V43, P99, DOI 10.1016/j.neunet.2013.02.003
   Yao JJ, 2012, AASRI PROC, V1, P183, DOI 10.1016/j.aasri.2012.06.029
   Yingwei L., 1997, IEEE P CONTR THEOR D, V144, P1
   Zhou P, 2011, NEUROCOMPUTING, V74, P3628, DOI 10.1016/j.neucom.2011.07.011
NR 36
TC 1
Z9 3
U1 0
U2 1
PY 2014
BP 3775
EP 3782
UT WOS:000371465703128
DA 2023-11-16
ER

PT J
AU Kampakis, S
AF Kampakis, Stylianos
TI Improved Izhikevich neurons for spiking neural networks
SO SOFT COMPUTING
DT Article
DE Izhikevich neuron; Spiking neural network; Rebound spiking;
   Classification; Receptive fields; Biologically plausible
ID MODEL
AB Spiking neural networks constitute a modern neural network paradigm that overlaps machine learning and computational neurosciences. Spiking neural networks use neuron models that possess a great degree of biological realism. The most realistic model of the neuron is the one created by Alan Lloyd Hodgkin and Andrew Huxley. However, the Hodgkin-Huxley model, while accurate, is computationally very inefficient. Eugene Izhikevich created a simplified neuron model based on the Hodgkin-Huxley equations. This model has better computational efficiency than the original proposed by Hodgkin and Huxley, and yet it can successfully reproduce all known firing patterns. However, there are not many articles dealing with implementations of this model for a functional neural network. This study presents a spiking neural network architecture that utilizes improved Izhikevich neurons with the purpose of evaluating its speed and efficiency. Since the field of spiking neural networks has reinvigorated the interest in biological plausibility, biological realism was an additional goal. The network is tested on the correct classification of logic gates (including XOR) and on the iris dataset. Results and possible improvements are also discussed.
C1 Aristotle Univ Thessaloniki, Dept Comp & Elect Engn, GR-54006 Thessaloniki, Greece.
RP Kampakis, S (corresponding author), Aristotle Univ Thessaloniki, Dept Comp & Elect Engn, Ano Tzoumagias 1, GR-54006 Thessaloniki, Greece.
EM stylianos.kampakis@gmail.com
CR Belatreche A, 2006, NEW MATH NAT COMPUT, V2, P237, DOI 10.1142/S179300570600049X
   Bohte S. M., 2001, IEEE T NEURAL NETW, VXX
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Carnell A., 2005, P EUR S ART NEUR NET
   Eurich CW, 2000, NEURAL COMPUT, V12, P1519, DOI 10.1162/089976600300015240
   Fisher RA, 1936, ANN EUGENIC, V7, P179, DOI 10.1111/j.1469-1809.1936.tb02137.x
   Gerstner W., 2002, SPIKING NEURON MODEL
   Glackin C, 2011, NEURAL NETWORKS, V24, P247, DOI 10.1016/j.neunet.2010.11.008
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Iannella N, 2001, NEURAL NETWORKS, V14, P933, DOI 10.1016/S0893-6080(01)00080-6
   Izhikevich E, SIMPLE MODEL SPIKING
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Kasinski A., 2006, International Journal of Applied Mathematics and Computer Science, V16, P101
   Legenstein R, 2005, NEURAL COMPUT, V17, P2337, DOI 10.1162/0899766054796888
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maass W, 2001, PULSED NEURAL NETWOR
   MINSKY M, 1972, PERCEPTIONS INTRO CO
   Mitra P, 2007, VISUAL NEUROSCI, V24, P79, DOI 10.1017/S0952523807070101
   Nieder A, 2009, ANNU REV NEUROSCI, V32, P185, DOI 10.1146/annurev.neuro.051508.135550
   Person AL, 2005, NEURON, V46, P129, DOI 10.1016/j.neuron.2004.12.057
   Pfister JP, 2003, ICANN ICONIP 2003
   Rieke F., 1996, SPIKES EXPLORING NEU
   Shadlen Michael N., 1994, Current Opinion in Neurobiology, V4, P569, DOI 10.1016/0959-4388(94)90059-0
   Sougne JP, 2000, CONNECTIONIST MODELS
   Supèr H, 2011, J COGNITIVE NEUROSCI, V23, P491, DOI 10.1162/jocn.2010.21512
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   Valko M, 2005, ART INT 2005 EP 2005
   Wang H, 2009, INT C INF ENG COMP S
   Wu QX, 2007, LECT NOTES ARTIF INT, V4682, P26
   Xiaoli T, 2004, INT C ART INT 2004
NR 31
TC 8
Z9 8
U1 0
U2 19
PD JUN
PY 2012
VL 16
IS 6
BP 943
EP 953
DI 10.1007/s00500-011-0793-1
UT WOS:000305247600002
DA 2023-11-16
ER

PT J
AU Kim, R
   Li, YH
   Sejnowski, TJ
AF Kim, Robert
   Li, Yinghao
   Sejnowski, Terrence J.
TI Simple framework for constructing functional spiking recurrent neural
   networks
SO PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES OF THE UNITED STATES OF
   AMERICA
DT Article
DE spiking neural networks; recurrent neural networks; rate neural networks
ID CHAOS; PATTERNS; MODELS
AB Cortical microcircuits exhibit complex recurrent architectures that possess dynamically rich properties. The neurons that make up these microcircuits communicate mainly via discrete spikes, and it is not clear how spikes give rise to dynamics that can be used to perform computationally challenging tasks. In contrast, continuous models of rate-coding neurons can be trained to perform complex tasks. Here, we present a simple framework to construct biologically realistic spiking recurrent neural networks (RNNs) capable of learning a wide range of tasks. Our framework involves training a continuous-variable rate RNN with important biophysical constraints and transferring the learned dynamics and constraints to a spiking RNN in a one-to-one manner. The proposed framework introduces only 1 additional parameter to establish the equivalence between rate and spiking RNN models. We also study other model parameters related to the rate and spiking networks to optimize the one-to-one mapping. By establishing a close relationship between rate and spiking models, we demonstrate that spiking RNNs could be constructed to achieve similar performance as their counterpart continuous rate networks.
C1 [Kim, Robert; Li, Yinghao; Sejnowski, Terrence J.] Salk Inst Biol Studies, Computat Neurobiol Lab, La Jolla, CA 92037 USA.
   [Kim, Robert] Univ Calif San Diego, Neurosci Grad Program, La Jolla, CA 92093 USA.
   [Kim, Robert] Univ Calif San Diego, Med Scientist Training Program, La Jolla, CA 92093 USA.
   [Sejnowski, Terrence J.] Univ Calif San Diego, Inst Neural Computat, La Jolla, CA 92093 USA.
   [Sejnowski, Terrence J.] Univ Calif San Diego, Div Biol Sci, La Jolla, CA 92093 USA.
RP Kim, R; Sejnowski, TJ (corresponding author), Salk Inst Biol Studies, Computat Neurobiol Lab, La Jolla, CA 92037 USA.; Kim, R (corresponding author), Univ Calif San Diego, Neurosci Grad Program, La Jolla, CA 92093 USA.; Kim, R (corresponding author), Univ Calif San Diego, Med Scientist Training Program, La Jolla, CA 92093 USA.; Sejnowski, TJ (corresponding author), Univ Calif San Diego, Inst Neural Computat, La Jolla, CA 92093 USA.; Sejnowski, TJ (corresponding author), Univ Calif San Diego, Div Biol Sci, La Jolla, CA 92093 USA.
EM rkim@salk.edu; terry@salk.edu
CR Abbott LF, 2016, NAT NEUROSCI, V19, P350, DOI 10.1038/nn.4241
   Alemi A, 2018, AAAI CONF ARTIF INTE, P588
   [Anonymous], 2016, 2016 IEE INT C REB C, DOI [10.1109/ICRC.2016.7738691, DOI 10.1109/ICRC.2016.7738691]
   Barak O, 2013, PROG NEUROBIOL, V103, P214, DOI 10.1016/j.pneurobio.2013.02.002
   Bengio Y, 2013, INT CONF ACOUST SPEE, P8624, DOI 10.1109/ICASSP.2013.6639349
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Cavanagh SE, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-05873-3
   Chaisangmongkon W, 2017, NEURON, V93, P1504, DOI 10.1016/j.neuron.2017.03.002
   Denève S, 2016, NAT NEUROSCI, V19, P375, DOI 10.1038/nn.4243
   DePasquale B., 2016, ARXIV160107620
   Diehl P.U., 2015, 2015 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2015.7280696
   Enel P, 2016, PLOS COMPUT BIOL, V12, DOI 10.1371/journal.pcbi.1004967
   Felsen G, 2002, NEURON, V36, P945, DOI 10.1016/S0896-6273(02)01011-5
   GOLDMANRAKIC PS, 1995, NEURON, V14, P477, DOI 10.1016/0896-6273(95)90304-6
   Huh D., 2018, ADV NEURAL INFORM PR, V31, P1433
   Hunsberger E., 2016, ABS161105141 CORR
   Kim CM, 2018, ELIFE, V7, DOI 10.7554/eLife.37124
   Kim R., SIMPLE FRAMEWORK CON
   Laje R, 2013, NAT NEUROSCI, V16, P925, DOI 10.1038/nn.3405
   Lee JM, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00191
   Mante V, 2013, NATURE, V503, P78, DOI 10.1038/nature12742
   Martens J., 2018, P 28 INT C INT C MAC, P1033
   Mastrogiuseppe F, 2018, NEURON, V99, P609, DOI 10.1016/j.neuron.2018.07.003
   Miconi T, 2017, ELIFE, V6, DOI 10.7554/eLife.20899
   Nicola W, 2017, NAT COMMUN, V8, DOI 10.1038/s41467-017-01827-3
   Pascanu Razvan, 2013, P 30 INT C INT C MAC, V28
   Rajan K, 2016, NEURON, V90, P128, DOI 10.1016/j.neuron.2016.02.009
   Rueckauer B., 2016, ARXIV161204052
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Shahidi N, 2019, NAT NEUROSCI, V22, P1148, DOI 10.1038/s41593-019-0406-3
   SOMPOLINSKY H, 1988, PHYS REV LETT, V61, P259, DOI 10.1103/PhysRevLett.61.259
   Song HF, 2017, ELIFE, V6, DOI [10.7554/elife.21492, 10.7554/eLife.21492]
   Song HF, 2016, PLOS COMPUT BIOL, V12, DOI 10.1371/journal.pcbi.1004792
   Stokes MG, 2013, NEURON, V78, P364, DOI 10.1016/j.neuron.2013.01.039
   Sussillo D, 2009, NEURON, V63, P544, DOI 10.1016/j.neuron.2009.07.018
   Thalmeier D, 2016, PLOS COMPUT BIOL, V12, DOI 10.1371/journal.pcbi.1004895
   Ujfalussy BB, 2015, ELIFE, V4, DOI 10.7554/eLife.10056
   Wang JX, 2018, NAT NEUROSCI, V21, P860, DOI 10.1038/s41593-018-0147-8
   Wang XJ, 2008, NEURON, V60, P215, DOI 10.1016/j.neuron.2008.09.034
   Wasmuht DF, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-05961-4
   WERBOS PJ, 1990, P IEEE, V78, P1550, DOI 10.1109/5.58337
   Yang GR, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms12815
   Zhang ZW, 2018, PLOS COMPUT BIOL, V14, DOI 10.1371/journal.pcbi.1005925
   Zick JL, 2018, NEURON, V98, P1243, DOI 10.1016/j.neuron.2018.05.010
NR 44
TC 32
Z9 32
U1 4
U2 19
PD NOV 5
PY 2019
VL 116
IS 45
BP 22811
EP 22820
DI 10.1073/pnas.1905926116
UT WOS:000494457400059
DA 2023-11-16
ER

PT C
AU Shi, GY
   Liang, JG
   Cui, Y
AF Shi, Guoyong
   Liang, Jungang
   Cui, Yong
BE Reformat, M
   Zhang, D
   Bourbakis, N
TI A Supervised Learning Rule for Recurrent Spiking Neural Networks with
   Weighted Spikes
SO 2022 IEEE 34TH INTERNATIONAL CONFERENCE ON TOOLS WITH ARTIFICIAL
   INTELLIGENCE, ICTAI
SE Proceedings-International Conference on Tools With Artificial
   Intelligence
DT Proceedings Paper
CT 34th IEEE International Conference on Tools with Artificial Intelligence
   (ICTAI)
CY OCT 31-NOV 02, 2022
CL ELECTR NETWORK
DE Recurrent spiking neural networks; Spike weight; Supervised learning
   rule; Context layer with connectivity
AB As a brain-inspired artificial neural network computational model, a recurrent spiking neural network is composed of biologically plausible spiking neurons, which has taken on increasing importance in this study field mainly include complex network structure and implicit nonlinear mechanism. The paper presents a learning rule with spike weight for recurrent spiking neural networks, allowing for real-time communication system of complex spatiotemporal spike trains simulating organisms. First, a context layer with connectivity through copying the hidden layer between the input layer and the output layer is provided. The total error of the network for learning spike train patterns is then introduced, as well as a rule of spike weight based on different phases of spikes. Furthermore, the proposed supervised learning rule defines the learning process for synaptic weights in all layers based on the power-up of weighted spikes to transmit more information. In addition, the learning algorithm has been successfully tested and evaluated for major factors, such as different lengths and frequencies for desired output spike trains, demonstrating that high precision learning is possible even with limited iterative resources. Finally, an analysis of the recurrent layer parameters is conducted, including neuron number and connectivity degree.
C1 [Shi, Guoyong; Liang, Jungang; Cui, Yong] Southwest Petr Univ, Sch Informat, Nanchong 637001, Peoples R China.
RP Shi, GY (corresponding author), Southwest Petr Univ, Sch Informat, Nanchong 637001, Peoples R China.
EM shiguoyong@swpu.edu.cn; liangjungang@swpu.edu.cn; cuiyong@swpu.edu.cn
NR 0
TC 0
Z9 0
U1 0
U2 0
PY 2022
BP 522
EP 527
DI 10.1109/ICTAI56018.2022.00083
UT WOS:000992544400075
DA 2023-11-16
ER

PT C
AU Qiu, HN
   Garratt, M
   Howard, D
   Anavatti, S
AF Qiu, Huanneng
   Garratt, Matthew
   Howard, David
   Anavatti, Sreenatha
BE Sundaram, S
TI Evolving Spiking Neural Networks for Nonlinear Control Problems
SO 2018 IEEE SYMPOSIUM SERIES ON COMPUTATIONAL INTELLIGENCE (IEEE SSCI)
DT Proceedings Paper
CT 8th IEEE Symposium Series on Computational Intelligence (IEEE SSCI)
CY NOV 18-21, 2018
CL Bengaluru, INDIA
DE Spiking Neural Networks; neuroevolution; recurrent networks
ID MODEL
AB Spiking Neural Networks are powerful computational modelling tools that have attracted much interest because of the bioinspired modelling of synaptic interactions between neurons. Most of the research employing spiking neurons has been non-behavioural and discontinuous. Comparatively, this paper presents a recurrent spiking controller that is capable of solving nonlinear control problems in continuous domains using a popular topology evolution algorithm as the learning mechanism. We propose two mechanisms necessary to the decoding of continuous signals from discrete spike transmission: (i) a background current component to maintain frequency sufficiency for spike rate decoding, and (ii) a general network structure that derives strength from topology evolution. We demonstrate that the proposed spiking controller can learn significantly faster to discover functional solutions than sigmoidal neural networks in solving a classic nonlinear control problem.
C1 [Qiu, Huanneng; Garratt, Matthew; Anavatti, Sreenatha] Univ New South Wales, Sch Engn & Informat Technol, Australian Def Force Acad, Canberra, ACT, Australia.
   [Howard, David] CSIRO, DATA61, Robot & Autonomous Syst Grp, Brisbane, Qld, Australia.
RP Qiu, HN (corresponding author), Univ New South Wales, Sch Engn & Informat Technol, Australian Def Force Acad, Canberra, ACT, Australia.
EM huanneng.qiu@student.unsw.edu.au; m.garratt@adfa.edu.au;
   david.howard@data61.csiro.au; s.anavatti@adfa.edu.au
CR Abbott LF, 2016, NAT NEUROSCI, V19, P350, DOI 10.1038/nn.4241
   [Anonymous], COMPUTING SPIKING NE
   [Anonymous], P IEEE CICC SEP, DOI DOI 10.1109/CICC.2011.6055293
   [Anonymous], 2016, S SERIES COMPUTATION
   BARTO AG, 1983, IEEE T SYST MAN CYB, V13, P834, DOI 10.1109/TSMC.1983.6313077
   Batllori R, 2011, PROCEDIA COMPUT SCI, V6, DOI 10.1016/j.procs.2011.08.060
   Eiben A. E., 2015, NATURAL COMPUTING SE, V2nd
   Fellous JM, 2003, NEUROSCIENCE, V122, P811, DOI 10.1016/j.neuroscience.2003.08.027
   Floreano D., 2003, P 8 INT C ART LIF, P335
   Gamez D, 2012, BIOINSPIR BIOMIM, V7, DOI 10.1088/1748-3182/7/2/025008
   Gerstner W., 2002, SPIKING NEURON MODEL
   Howard D., 2014, ARTIFICIAL LIFE, V14
   Howard D, 2016, NEURAL PROCESS LETT, V44, P125, DOI 10.1007/s11063-015-9451-4
   Howard G, 2012, IEEE T EVOLUT COMPUT, V16, P711, DOI 10.1109/TEVC.2011.2170199
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Kang TS, 2017, IEEE IJCNN, P2443, DOI 10.1109/IJCNN.2017.7966153
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Pardoe D, 2005, GECCO 2005: GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, VOLS 1 AND 2, P1379
   Rice KL, 2009, 2009 INTERNATIONAL CONFERENCE ON RECONFIGURABLE COMPUTING AND FPGAS, P451, DOI 10.1109/ReConFig.2009.77
   Shepherd III J., 2010, P 12 ANN C GENETIC E, P1131, DOI DOI 10.1145/18304831830693
   Stanley KO, 2002, EVOL COMPUT, V10, P99, DOI 10.1162/106365602320169811
   THEUNISSEN F, 1995, J COMPUT NEUROSCI, V2, P149, DOI 10.1007/BF00961885
   Vasu MC, 2017, LECT NOTES COMPUT SC, V10613, P236, DOI 10.1007/978-3-319-68600-4_28
NR 24
TC 8
Z9 8
U1 0
U2 4
PY 2018
BP 1367
EP 1373
UT WOS:000459238800187
DA 2023-11-16
ER

PT C
AU Nageswaran, JM
   Richert, M
   Dutt, N
   Krichmar, JL
AF Nageswaran, Jayram Moorkanikara
   Richert, Micah
   Dutt, Nikil
   Krichmar, Jeffrey L.
GP IEEE
TI Towards Reverse Engineering The Brain: Modeling Abstractions and
   Simulation Frameworks
SO PROCEEDINGS OF THE 2010 18TH IEEE/IFIP INTERNATIONAL CONFERENCE ON VLSI
   AND SYSTEM-ON-CHIP
SE IEEE-IFIP International Conference on VLSI and System-on-Chip
DT Proceedings Paper
CT 18th IEEE/IFIP International Conference on VLSI and System-on-Chip
CY SEP 27-29, 2010
CL Complutense Univ Madrid, Comp Sci Sch, Madrid, SPAIN
HO Complutense Univ Madrid, Comp Sci Sch
DE Spiking neural networks; GPU; vision; computational neuroscience;
   parallel processing; synapse
ID NETWORKS
AB Biological neural systems are well known for their robust and power-efficient operation in highly noisy environments. Biological circuits are made up of low-precision, unreliable and massively parallel neural elements with highly reconfigurable and plastic connections. Two of the most interesting properties of the neural systems are its self-organizing capabilities and its template architecture. Recent research in spiking neural networks has demonstrated interesting principles about learning and neural computation. Understanding and applying these principles to practical problems is only possible if large-scale spiking neural simulators can be constructed. Recent advances in low-cost multiprocessor architectures make it possible to build large-scale spiking network simulators. In this paper we review modeling abstractions for neural circuits and frameworks for modeling, simulating and analyzing spiking neural networks.
C1 [Nageswaran, Jayram Moorkanikara; Dutt, Nikil; Krichmar, Jeffrey L.] Univ Calif Irvine, Dept Comp Sci, Irvine, CA 92717 USA.
   [Richert, Micah; Krichmar, Jeffrey L.] Univ Calif Irvine, Dept Cognit Sci, Irvine, CA 92717 USA.
RP Nageswaran, JM (corresponding author), Univ Calif Irvine, Dept Comp Sci, Irvine, CA 92717 USA.
EM jmoorkan@uci.edu; mrichert@uci.edu; dutt@uci.edu; jkrichma@uci.edu
CR Abbott LF, 2000, NAT NEUROSCI, V3, P1178, DOI 10.1038/81453
   [Anonymous], PHILOS T ROYAL B JAN
   [Anonymous], 2010, ULTRA LOW POWER BIOE
   [Anonymous], NEUR PAR OPT
   [Anonymous], AI 50 FUTURE ARTIFIC
   [Anonymous], BRAIN ARCHITECTURE U
   [Anonymous], 2002, J MATH PSYCHOL
   [Anonymous], COGNITIVE SYSTEMS IN
   [Anonymous], NATURE REV NEURO FEB
   [Anonymous], BIOL CYBERNETICS NOV
   [Anonymous], 2003, COMPUTATIONAL NEUROS
   [Anonymous], IEEE INT JOINT C NEU
   [Anonymous], P INT JOINT C NEUR N
   [Anonymous], INT C COGN NEUROSCIE
   [Anonymous], PULSED NEURAL NETWOR
   [Anonymous], COMPUTER SCI REV
   [Anonymous], HDB BRAIN THEORY NEU
   Baars BJ, 2005, PROG BRAIN RES, V150, P45, DOI 10.1016/S0079-6123(05)50004-9
   BOWER JM, 1998, BOOK GENESIS
   Brette R, 2007, J COMPUT NEUROSCI, V23, P349, DOI 10.1007/s10827-007-0038-6
   Bullmore ET, 2009, NAT REV NEUROSCI, V10, P186, DOI 10.1038/nrn2575
   Churchland P., 1994, COMPUTATIONAL BRAIN
   Davison A.P., 2008, FRONTIERS NEUROINFOR, V2
   EDELMAN GM, 1993, NEURON, V10, P115, DOI 10.1016/0896-6273(93)90304-A
   Fukushima K, 2008, LECT NOTES COMPUT SC, V4984, P1041
   Furber S, 2008, STUD COMPUT INTELL, V115, P763, DOI 10.1098/rsif.2006.0177
   Hawkins J, 2004, INTELLIGENCE
   Hines ML, 1997, NEURAL COMPUT, V9, P1179, DOI 10.1162/neco.1997.9.6.1179
   Hubel D. H., 1995, EYE BRAIN VISION
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Jo SH, 2010, NANO LETT, V10, P1297, DOI 10.1021/nl904092h
   Laughlin SB, 2003, SCIENCE, V301, P1870, DOI 10.1126/science.1089662
   Maass W, 2001, PULSED NEURAL NETWOR
   MEAD C, 1990, P IEEE, V78, P1629, DOI 10.1109/5.58356
   Morrison A, 2005, NEURAL COMPUT, V17, P1776, DOI 10.1162/0899766054026648
   Mountcastle VB, 1997, BRAIN, V120, P701, DOI 10.1093/brain/120.4.701
   NAE, NAE GRAND CHALL ENG
   Nageswaran JM, 2009, NEURAL NETWORKS, V22, P791, DOI 10.1016/j.neunet.2009.06.028
   Nageswaran JM, 2009, INT J PARALLEL PROG, V37, P345, DOI 10.1007/s10766-009-0106-9
   Nassi JJ, 2009, NAT REV NEUROSCI, V10, P360, DOI 10.1038/nrn2619
   Serre T, 2007, IEEE T PATTERN ANAL, V29, P411, DOI 10.1109/TPAMI.2007.56
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   VanRullen R, 2002, VISION RES, V42, P2593, DOI 10.1016/S0042-6989(02)00298-5
   Vogels TP, 2005, ANNU REV NEUROSCI, V28, P357, DOI 10.1146/annurev.neuro.28.061604.135637
   Weber C, 2001, LECT NOTES ARTIF INT, V2036, P53
   Wiskott L, 2002, NEURAL COMPUT, V14, P715, DOI 10.1162/089976602317318938
NR 46
TC 9
Z9 9
U1 0
U2 9
PY 2010
BP 1
EP 6
UT WOS:000295220400001
DA 2023-11-16
ER

PT C
AU Amin, H
   Fujii, R
AF Amin, H
   Fujii, R
GP ieee
TI Spike train decoding scheme for a spiking neural network
SO 2004 IEEE INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS, VOLS 1-4,
   PROCEEDINGS
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT IEEE International Joint Conference on Neural Networks (IJCNN)
CY JUL 25-29, 2004
CL Budapest, HUNGARY
ID MOMENT
AB A spiking neural network models a type of biological neural system in which spike trains are used to convey information. A spike train consists of a series of asynchronous input spikes which are input into a neuron through a synapse. A new decoding scheme which can uniquely distinguish spike trains by using the relative spike arrival times in a spike train is proposed. Only a limited number of neurons are needed to implement the decoding scheme.
C1 Univ Aizu, Dept Comp Engn, Aizu Wakamatsu, Wakamatsu, Japan.
RP Amin, H (corresponding author), Univ Aizu, Dept Comp Engn, Aizu Wakamatsu, Wakamatsu, Japan.
CR ABELES M, 1993, J NEUROPHYSIOL, V70, P1629, DOI 10.1152/jn.1993.70.4.1629
   AMIN HH, 2004, P 12 EUR S ART NEUR, P355
   AMIN HH, 2003, 3D FORUM, V17, P191
   [Anonymous], 2003, ADV NEURAL INFORM PR
   Gerstner W., 2002, SPIKING NEURON MODEL
   Haykin S., 1999, NEURAL NETWORKS COMP
   Hopfield IJ, 2000, P NATL ACAD SCI USA, V97, P13919, DOI 10.1073/pnas.250483697
   Hopfield JJ, 2001, P NATL ACAD SCI USA, V98, P1282, DOI 10.1073/pnas.031567098
   Maass W., 1999, PULSED NEURAL NETWOR
   MAASS W, 2003, COMPUTATIONAL NEUROS, pCH18
   Natschlager T, 1998, NETWORK-COMP NEURAL, V9, P319, DOI 10.1088/0954-898X/9/3/003
   Ruf B, 1998, IEEE T NEURAL NETWOR, V9, P575, DOI 10.1109/72.668899
NR 12
TC 5
Z9 5
U1 0
U2 2
PY 2004
BP 477
EP 482
DI 10.1109/IJCNN.2004.1379956
UT WOS:000224941900082
DA 2023-11-16
ER

PT S
AU Fujii, RH
   Oozeki, K
AF Fujii, Robert H.
   Oozeki, Kenjyu
BE Kollias, S
   Stafylopatis, A
   Duch, W
   Oja, E
TI Temporal data encoding and SequenceLearning with spiking neural networks
SO ARTIFICIAL NEURAL NETWORKS - ICANN 2006, PT 1
SE Lecture Notes in Computer Science
DT Article; Proceedings Paper
CT 16th International Conference on Artificial Neural Networks (ICANN 2006)
CY SEP 10-14, 2006
CL Athens, GREECE
DE Spiking Neural Network; sequence learning; temporal data processing
AB Sequence Learning using a Spiking Neural Network (SNN) was performed. An SNN is a type of Artificial Neural Network (ANN) that uses input signal arrival time information to process temporal data. An SNN can learn not only combinational inputs but also sequential inputs over some limited amount of time without using a recurrent network. Music melodies were encoded using unit amplitude spikes having various inter-spike interval times. These spikes were then fed into an SNN learning system. The SNN learning system was able to recognize various melodies after learning. The SNN could identify the original and noise-added melody versions properly in most cases.
C1 Univ Aizu, Sch Engn & Comp Sci, Aizu Wakamatsu, Fukushima, Japan.
RP Fujii, RH (corresponding author), Univ Aizu, Sch Engn & Comp Sci, Aizu Wakamatsu, Fukushima, Japan.
EM fujii@u-aizu.ac.jp
CR Amin H, 2004, IEEE IJCNN, P477, DOI 10.1109/IJCNN.2004.1379956
   AMIN HH, 2005, 48 IEEE INT MIDW S C
   Gerstner W., 2002, SPIKING NEURON MODEL
   HAYKIN S., 1999, NEURAL NETWORK COMPR
   Maass W, 1996, NEURAL COMPUT, V8, P1, DOI 10.1162/neco.1996.8.1.1
   Maass W., 1999, PULSED NEURAL NETWOR
   WANG DL, 1990, P IEEE, V78, P1536, DOI 10.1109/5.58329
NR 7
TC 2
Z9 2
U1 0
U2 2
PY 2006
VL 4131
BP 780
EP 789
UT WOS:000241472100081
DA 2023-11-16
ER

PT J
AU Hosaka, R
   Araki, O
   Ikeguchi, T
AF Hosaka, Ryosuke
   Araki, Osamu
   Ikeguchi, Tohru
TI STDP provides the substrate for igniting synfire chains by
   spatiotemporal input patterns
SO NEURAL COMPUTATION
DT Article
ID SYNCHRONIZATION; DYNAMICS; CORTEX; PROPAGATION; MODULATION; PLASTICITY;
   SPIKING
AB Spike-timing-dependent synaptic plasticity (STDP), which depends on the temporal difference between pre- and postsynaptic action potentials, is observed in the cortices and hippocampus. Although several theoretical and experimental studies have revealed its fundamental aspects, its functional role remains unclear. To examine how an input spatiotemporal spike pattern is altered by STDP, we observed the output spike patterns of a spiking neural network model with an asymmetrical STDP rule when the input spatiotemporal pattern is repeatedly applied. The spiking neural network comprises excitatory and inhibitory neurons that exhibit local interactions. Numerical experiments show that the spiking neural network generates a single global synchrony whose relative timing depends on the input spatiotemporal pattern and the neural network structure. This result implies that the spiking neural network learns the transformation from spatiotemporal to temporal information. In the literature, the origin of the synfire chain has not been sufficiently focused on. Our results indicate that spiking neural networks with STDP can ignite synfire chains in the cortices.
C1 [Hosaka, Ryosuke; Ikeguchi, Tohru] Saitama Univ, Grad Sch Sci & Engn, Sakura Ku, Saitama 3388570, Japan.
   [Araki, Osamu] Tokyo Univ Sci, Dept Appl Phys, Shinjuku Ku, Tokyo 1628601, Japan.
RP Hosaka, R (corresponding author), Saitama Univ, Grad Sch Sci & Engn, Sakura Ku, Saitama 3388570, Japan.
EM hosaka@nls.ics.saitama-u.ac.jp; brainics@rs.kagu.tus.ac.jp;
   tohru@ics.saitama-u.ac.jp
CR Abbott LF, 2000, NAT NEUROSCI, V3, P1178, DOI 10.1038/81453
   ABELES M, 1993, J NEUROPHYSIOL, V70, P1629, DOI 10.1152/jn.1993.70.4.1629
   AERTSEN A, 1994, PHYSICA D, V75, P103, DOI 10.1016/0167-2789(94)90278-X
   AERTSEN AMHJ, 1989, J NEUROPHYSIOL, V61, P900, DOI 10.1152/jn.1989.61.5.900
   [Anonymous], 1991, ANATOMY CORTEX
   [Anonymous], 1991, CORTICONICS
   Araki O, 2001, NEURAL COMPUT, V13, P2799, DOI 10.1162/089976601317098538
   Bell CC, 1997, NATURE, V387, P278, DOI 10.1038/387278a0
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   DEBANNE D, 1998, J PHYSL, V15, P237
   deOliveira SC, 1997, J NEUROSCI, V17, P9248
   Diesmann M, 1999, NATURE, V402, P529, DOI 10.1038/990101
   Froemke RC, 2002, NATURE, V416, P433, DOI 10.1038/416433a
   Fujii H, 1996, NEURAL NETWORKS, V9, P1303, DOI 10.1016/S0893-6080(96)00054-8
   GERSTNER W, 1993, BIOL CYBERN, V69, P503, DOI 10.1007/BF01185422
   Gerstner W, 1996, NATURE, V383, P76, DOI 10.1038/383076a0
   GRAY CM, 1989, NATURE, V338, P334, DOI 10.1038/338334a0
   Gütig R, 2003, J NEUROSCI, V23, P3697
   HOPE ACA, 1968, J ROY STAT SOC B, V30, P582
   Ikegaya Y, 2004, SCIENCE, V304, P559, DOI 10.1126/science.1093173
   Izhikevich EM, 2004, CEREB CORTEX, V14, P933, DOI 10.1093/cercor/bhh053
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Levy N, 2001, NEURAL NETWORKS, V14, P815, DOI 10.1016/S0893-6080(01)00044-2
   LILEY DTJ, 1994, NETWORK-COMP NEURAL, V5, P175, DOI 10.1088/0954-898X/5/2/004
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   Masuda N, 2004, NEURAL COMPUT, V16, P627, DOI 10.1162/089976604772744938
   Matsumoto N, 2002, NEURAL COMPUT, V14, P2883, DOI 10.1162/089976602760805322
   Nádasdy Z, 1999, J NEUROSCI, V19, P9497
   Nishiyama M, 2000, NATURE, V408, P584, DOI 10.1038/35046067
   OKEEFE J, 1993, HIPPOCAMPUS, V3, P317, DOI 10.1002/hipo.450030307
   Rao RPN, 2001, NEURAL COMPUT, V13, P2221, DOI 10.1162/089976601750541787
   Reyes AD, 2003, NAT NEUROSCI, V6, P593, DOI 10.1038/nn1056
   Riehle A, 1997, SCIENCE, V278, P1950, DOI 10.1126/science.278.5345.1950
   Rubin J, 2001, PHYS REV LETT, V86, P364, DOI 10.1103/PhysRevLett.86.364
   Song S, 2001, NEURON, V32, P339, DOI 10.1016/S0896-6273(01)00451-2
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   VAADIA E, 1995, NATURE, V373, P515, DOI 10.1038/373515a0
   van Rossum MCW, 2002, J NEUROSCI, V22, P1956, DOI 10.1523/JNEUROSCI.22-05-01956.2002
   Whittington MA, 2000, INT J PSYCHOPHYSIOL, V38, P315, DOI 10.1016/S0167-8760(00)00173-2
   Zhang LI, 1998, NATURE, V395, P37, DOI 10.1038/25665
   Zhigulin VP, 2003, PHYS REV E, V67, DOI 10.1103/PhysRevE.67.021901
NR 41
TC 29
Z9 30
U1 0
U2 4
PD FEB
PY 2008
VL 20
IS 2
BP 415
EP 435
DI 10.1162/neco.2007.11-05-043
UT WOS:000252248200005
DA 2023-11-16
ER

PT C
AU Lisovskaya, A
   Skripnik, TN
AF Lisovskaya, Angelina
   Skripnik, Tatiana N.
GP IEEE
TI Processing of Neural System Information with the Use of Artificial
   Spiking Neural Networks
SO PROCEEDINGS OF THE 2019 IEEE CONFERENCE OF RUSSIAN YOUNG RESEARCHERS IN
   ELECTRICAL AND ELECTRONIC ENGINEERING (EICONRUS)
SE IEEE NW Russia Young Researchers in Electrical and Electronic
   Engineering Conference
DT Proceedings Paper
CT IEEE Conference of Russian Young Researchers in Electrical and
   Electronic Engineering (EIConRus)
CY JAN 28-31, 2019
CL St Petersburg Electrotechn Univ, RUSSIA
HO St Petersburg Electrotechn Univ
DE action potential; biological model; spiking neural network; pattern
   recognition; neural prostheses
AB The problems of information processing of the human nervous system for sensation of prostheses are considered. Processing is performed using artificial spiking neural networks. It describes hardware designs and computer computing problems for its implementation. For years of researches and developments in the field of neurotechnologies a big variety of the artificial neural networks somewhat reminding work of biological neural systems was created.
C1 [Lisovskaya, Angelina; Skripnik, Tatiana N.] St Petersburg State Marine Tech Univ, Dept Marine Elect, St Petersburg, Russia.
RP Skripnik, TN (corresponding author), St Petersburg State Marine Tech Univ, Dept Marine Elect, St Petersburg, Russia.
EM marine_electronics@corp.smtu.ru
CR Dang B, 2018, AIP CONF PROC, V2034, DOI 10.1063/1.5067350
   Ivanov A, 2018, IEEE WCNC
   Popov  A., 2018, 2018 IEEE C RUSS YOU
   Zhilenkov A., 2018, 2018 IEEE C RUSS YOU
NR 4
TC 1
Z9 1
U1 0
U2 0
PY 2019
BP 1183
EP 1186
DI 10.1109/eiconrus.2019.8656651
UT WOS:000469452600281
DA 2023-11-16
ER

PT C
AU Hu, JH
   Goh, WL
   Zhang, ZY
   Gao, Y
AF Hu, Jinhai
   Goh, Wang Ling
   Zhang, Zhongyi
   Gao, Yuan
GP IEEE
TI Voice Keyword Recognition Based on Spiking Convolutional Neural Network
   for Human-Machine Interface
SO 2020 THE 3RD INTERNATIONAL CONFERENCE ON INTELLIGENT AUTONOMOUS SYSTEMS
   (ICOIAS'2020)
DT Proceedings Paper
CT 3rd International Conference on Intelligent Autonomous Systems (ICoIAS)
CY FEB 26-29, 2020
CL Singapore, SINGAPORE
DE voice recognition; spiking neural network; convolution neural network
AB In this paper, a spiking convolutional neural network (SCNN) model for voice keyword recognition is presented. The model consists of an input pre-processing layer, a spiking neural network (SNN) layer with build-in filter bank and the convolutional neural network (CNN) layers. A 16-channel infinite impulse response (IIR) filter bank with energy detector extracts power from the voice signal band and converts it to spikes via the SNN layer. The spiking rate in a defined time window is used as the inputs to the following CNN layers for classification. The network is trained using a voice digit dataset, while the weights of the convolutional layers are adjusted through the training of spike-integration results obtained from the spiking layer. This model has been implemented for voice keyword recognition and achieved 96.0 % accuracy. The combination of SNN and CNN reduces the overall number of layer and neuron in the system without compromise in classification accuracy. It is suitable for low power hardware implementation in edge devices for human machine interface (HMI) applications.
C1 [Hu, Jinhai; Goh, Wang Ling; Zhang, Zhongyi] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore, Singapore.
   [Hu, Jinhai; Gao, Yuan] ASTAR, IC Design Dept, Inst Microelect, Singapore, Singapore.
RP Hu, JH (corresponding author), Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore, Singapore.; Hu, JH (corresponding author), ASTAR, IC Design Dept, Inst Microelect, Singapore, Singapore.
EM jhu017@e.ntu.edu.sg; gaoy@ime.a-star.edu.sg
CR Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Dominguez-Morales J., 2018, 2018 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2018.8489381
   Dong M, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0204596
   Du GM, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON SIGNAL AND IMAGE PROCESSING (ICSIP), P708, DOI 10.1109/SIPROCESS.2016.7888355
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   HAGAN MT, 1994, IEEE T NEURAL NETWOR, V5, P989, DOI 10.1109/72.329697
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V116, P449, DOI 10.1113/jphysiol.1952.sp004717
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Lee C, 2019, IEEE T COGN DEV SYST, V11, P384, DOI 10.1109/TCDS.2018.2833071
   Saunders D.J., 2018, INT JOINT C NEUR NET, P1
   Swietojanski P, 2014, IEEE SIGNAL PROC LET, V21, P1120, DOI 10.1109/LSP.2014.2325781
   Tavanaei A, 2018, 2018 INT JOINT C NEU, DOI DOI 10.1109/IJCNN.2018.8489104
   Tavanaei A, 2017, NEUROCOMPUTING, V240, P191, DOI 10.1016/j.neucom.2017.01.088
   VANDENBOOMGAARD R, 1992, CVGIP-GRAPH MODEL IM, V54, P252, DOI 10.1016/1049-9652(92)90055-3
   Wu JC, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351221
   Zhang Y, 2017, INT CONF ACOUST SPEE, P4845, DOI 10.1109/ICASSP.2017.7953077
NR 17
TC 1
Z9 1
U1 0
U2 2
PY 2020
BP 77
EP 82
UT WOS:000588576100015
DA 2023-11-16
ER

PT J
AU Chandra, B
   Babu, KVN
AF Chandra, B.
   Babu, K. V. Naresh
TI Classification of gene expression data using Spiking Wavelet Radial
   Basis Neural Network
SO EXPERT SYSTEMS WITH APPLICATIONS
DT Article
DE Radial Basis Neural Network; Wavelet Radial Basis Neural Network;
   Spiking neuron model
ID CANCER CLASSIFICATION; SELECTION; MODEL; NEURONS
AB The paper discusses how Spiking Wavelet Radial Basis Neural Network can be effectively used for the classification of gene expression data. A new spiking function has been proposed in the non-linear integrate and fire model and its inter spike interval is derived and used in the Wavelet Radial Basis Neural Network for the classification of gene expression data. The proposed model is termed as Spiking Wavelet Radial Basis Neural Network (SWRNN). The classification accuracy has been evaluated on various benchmark gene expression datasets. A comparative performance evaluation of the proposed model has been made with the Wavelet Radial Basis Neural Network (WRNN) and the standard available results in terms of classification accuracy. The comparison of the proposed SWRNN and WRNN has also been done in terms of execution time. It has been observed that the increase in classification accuracy for the proposed SWRNN is highly statistically significant for most of the gene expression datasets when compared with WRNN and the standard results. Thus incorporating a spiking function in an artificial neural network can make it more powerful for classification. (C) 2013 Published by Elsevier Ltd.
C1 [Chandra, B.; Babu, K. V. Naresh] Indian Inst Technol, Dept Math, Delhi, India.
RP Chandra, B (corresponding author), Indian Inst Technol, Dept Math, Delhi, India.
EM bchandra104@yahoo.co.in; vnareshiitd@gmail.com
CR Abbott LF, 1999, BRAIN RES BULL, V50, P303, DOI 10.1016/S0361-9230(99)00161-6
   ABBOTT LF, 1993, PHYS REV E, V48, P1483, DOI 10.1103/PhysRevE.48.1483
   [Anonymous], 1951, DISCRIMINATORY ANAL
   Berrar Daniel P, 2003, Pac Symp Biocomput, P5
   Buhmann MD., 2003, C MO AP C M, P11, DOI 10.1017/CBO9780511543241
   Burrus C. S., 1988, INTRO WAVELETS WAVEL
   Chen W., 2009, INT C ART INT COMP I
   Chu F., 2006, INT JOINT C NEUR NET
   FENG J, 2001, J PHYS A, V24, P1649
   Feng JF, 2003, IEEE T NEURAL NETWOR, V14, P326, DOI 10.1109/TNN.2003.809419
   Fourcaud-Trocmé N, 2003, J NEUROSCI, V23, P11628
   Friedman N, 1997, MACH LEARN, V29, P131, DOI 10.1023/A:1007465528199
   Gerstner W., 2002, SPIKING NEURON MODEL
   Guyon I, 2002, MACH LEARN, V46, P389, DOI 10.1023/A:1012487302797
   Hou MZ, 2011, APPL SOFT COMPUT, V11, P2173, DOI 10.1016/j.asoc.2010.07.016
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Latham PE, 2000, J NEUROPHYSIOL, V83, P808, DOI 10.1152/jn.2000.83.2.808
   Li LP, 2001, BIOINFORMATICS, V17, P1131, DOI 10.1093/bioinformatics/17.12.1131
   Narayanan A, 2004, NEUROCOMPUTING, V61, P217, DOI 10.1016/j.neucom.2003.10.017
   Paul TK, 2005, BIOSYSTEMS, V82, P208, DOI 10.1016/j.biosystems.2005.07.003
   Rumelhart D.E., 1987, LEARNING INTERNAL RE, P318
   Saravanan V, 2009, 2009 INTERNATIONAL CONFERENCE ON COMPUTER ENGINEERING AND TECHNOLOGY, VOL I, PROCEEDINGS, P137, DOI 10.1109/ICCET.2009.38
   SCHOLLES M, 1993, IEEE IJCNN, P2300
   SPECHT DF, 1990, NEURAL NETWORKS, V3, P109, DOI 10.1016/0893-6080(90)90049-Q
   STEIN RB, 1965, BIOPHYS J, V5, P173, DOI 10.1016/S0006-3495(65)86709-1
   Yadav A., 2005, P INT JOINT C NEUR N
   Yadav RN, 2003, INDIN 2003: IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL INFORMATICS, PROCEEDINGS, P124, DOI 10.1109/INDIN.2003.1300258
   Zhang RX, 2005, PROCEEDINGS OF THE 2005 IEEE SYMPOSIUM ON COMPUTATIONAL INTELLIGENCE IN BIOINFORMATICS AND COMPUTATIONAL BIOLOGY, P251
   Zhou XB, 2004, J BIOMED INFORM, V37, P249, DOI 10.1016/j.jbi.2004.07.009
NR 29
TC 13
Z9 13
U1 1
U2 29
PD MAR
PY 2014
VL 41
IS 4
BP 1326
EP 1330
DI 10.1016/j.eswa.2013.08.030
PN 1
UT WOS:000330158700035
DA 2023-11-16
ER

PT C
AU Bakó, L
   Brassai, ST
   Székely, I
AF Bako, L.
   Brassai, S. T.
   Szekely, I.
BE Cernat, M
   Nicolaide, A
   Margineanu, I
TI Fully parallel implementation of spiking neural networks on FPGA
SO PROCEEDINGS OF THE 10TH INTERNATIONAL CONFERENCE ON OPTIMIZATION OF
   ELECTRICAL AND ELECTRONIC EQUIPMENT, VOL III: INDUSTRIAL AUTOMATION AND
   CONTROL
DT Proceedings Paper
CT 10th International Conference on Optimization of Electrical and
   Electronic Equipment (OPTIM 2006)
CY MAY 18-19, 2006
CL Brasov, ROMANIA
DE neuromorphic neural networks; spiking neurons; simulation; hardware
   implementation; FPGA
AB Neurobiological research has lead to the birth of third generation (neuromorphic) artificial neural networks. One of these models is based on the natural "spiking" neural behavior, which creates the basis of our research. Following the developed mathematical "pulse reactive" model, a novel FPGA built hardware spiking neuron is introduced along with a network of these new model neurons. The modular neuron structure, acquired signals and a process control application are given.
C1 [Bako, L.; Brassai, S. T.; Szekely, I.] Sapientia Hungarian Univ Transylvania, Targu Mures, Romania.
RP Bakó, L (corresponding author), Sapientia Hungarian Univ Transylvania, Targu Mures, Romania.
CR Abeles M., 1991, CORTICONICS NEURAL C
   [Anonymous], NEURAL MOL BASES LEA
   [Anonymous], THE VHDL COOKBOOK
   BAKO L, 2004, ELECT TELECOMUNICATI, V49, P214
   CIRSTEA M, 2001, PRACTICAL GUIDE VHDL
   FRANK G, 1995, P INT C NEUR NETW IC, V4, P2014
   Gerstner W, 1998, PULSED NEURAL NETWORKS, P3
   HARTMANN G, 1997, P 6 INT C MICR NEUR, P130
   Haykin S., 1999, NEURAL NETWORKS COMP
   HEBB D. O., 1949
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Jahnke A, 1998, PULSED NEURAL NETWORKS, P237
   JAHNKE A, 1997, P 1997 INT C ART NEU, P1187
   KELSO SR, 1986, P NATL ACAD SCI USA, V83, P5326, DOI 10.1073/pnas.83.14.5326
   PEREZURIBE A, 1999, THESIS SWISS FEDERAL
   PEREZURIBE A, 1996, LECT NOTES COMPUTER, V1112, P383
   SCHOENAUER T, 1998, P VIDYNN 98
   STANTON PK, 1989, NATURE, V339, P215, DOI 10.1038/339215a0
   STENT GS, 1973, P NATL ACAD SCI USA, V70, P997, DOI 10.1073/pnas.70.4.997
   Trappenberg TP., 2002, FUNDAMENTALS COMPUTA
   WIDROW B, 1999, P 1960 IRE WESCON CO, V4, P96
   Willshaw D, 1990, NEURAL COMPUT, V2, P85, DOI 10.1162/neco.1990.2.1.85
   Wolff C, 1999, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON MICROELECTRONICS FOR NEURAL, FUZZY AND BIO-INSPIRED SYSTEMS, MICORNEURO'99, P324, DOI 10.1109/MN.1999.758882
NR 23
TC 0
Z9 0
U1 0
U2 1
PY 2006
BP 135
EP 142
UT WOS:000256418900023
DA 2023-11-16
ER

PT C
AU Nevarez, Y
   Garcia-Ortiz, A
   Rotermund, D
   Pawelzik, KR
AF Nevarez, Yarib
   Garcia-Ortiz, Alberto
   Rotermund, David
   Pawelzik, Klaus R.
GP IEEE
TI Accelerator Framework of Spike-By-Spike Neural Networks for Inference
   and Incremental Learning in Embedded Systems
SO 2020 9TH INTERNATIONAL CONFERENCE ON MODERN CIRCUITS AND SYSTEMS
   TECHNOLOGIES (MOCAST)
DT Proceedings Paper
CT 9th International Conference on Modern Circuits and Systems Technologies
   (MOCAST)
CY SEP 07-09, 2020
CL ELECTR NETWORK
DE Artificial intelligence; spiking neural networks; hardware accelerator;
   embedded systems; FPGA
AB Although artificial Spiking Neural Networks provide numerous advantages versus the traditional non spiking ones, their high complexity is limiting the use to server computers or dedicated ASIC implementations. As an alternative, the recently proposed Spike-by-Spike (SbS) Neural Networks provide reduced complexity while adding noise-robustness. In this work we propose an accelerator framework for inference and incremental learning targeting resource-constrained devices. The proposed architecture automatically distributes computational tasks to multiple accelerator units. This is the first SbS neural network implementation for embedded systems. Demonstration on a Xilinx Zynq-7020 achieves 99% of accuracy on MNIST dataset classification and a 5x latency enhancement compared to a Core-i7 computer running equivalent network topologies. To facilitate the research in this domain, the entire SbS accelerator framework is available as an open-source project.
C1 [Nevarez, Yarib; Garcia-Ortiz, Alberto] Univ Bremen, Inst Electrodynam & Microelect, Bremen, Germany.
   [Rotermund, David; Pawelzik, Klaus R.] Univ Bremen, Inst Theoret Phys, Bremen, Germany.
RP Nevarez, Y (corresponding author), Univ Bremen, Inst Electrodynam & Microelect, Bremen, Germany.
EM nevarez@item.uni-bremen.de; agaracia@item.uni-bremen.de;
   davrot@neuro.uni-bremen.de; pawelzik@neuro.uni-bremen.de
CR Abderrahmane N, 2020, NEURAL NETWORKS, V121, P366, DOI 10.1016/j.neunet.2019.09.024
   Bouvier M, 2019, ACM J EMERG TECH COM, V15, DOI 10.1145/3304103
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Ernst U, 2007, NEURAL COMPUT, V19, P1313, DOI 10.1162/neco.2007.19.5.1313
   Painkras E, 2013, IEEE J SOLID-ST CIRC, V48, P1943, DOI 10.1109/JSSC.2013.2259038
   Pawelzik K. R., 2019, MASSIVELT PARALLEL F
   Rajski J, 2005, IEEE T COMPUT AID D, V24, P622, DOI 10.1109/TCAD.2005.844111
   Rotermund D, 2019, FRONT COMPUT NEUROSC, V13, DOI 10.3389/fncom.2019.00055
NR 8
TC 0
Z9 0
U1 0
U2 1
PY 2020
DI 10.1109/mocast49295.2020.9200288
UT WOS:000632590300051
DA 2023-11-16
ER

PT C
AU Lee, K
   Shi, HC
AF Lee, Kyunghee
   Shi, Hongchi
GP IEEE
TI A Modular Approach to Construction of Spiking Neural Networks
SO 2019 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUL 14-19, 2019
CL Budapest, HUNGARY
DE spiking neural networks; modular approach; Coulomb energy function;
   learning algorithm; SpikeProp
AB In this paper, we propose a modular approach to construction of multi-layer spiking neural networks with a Coulomb energy function based learning algorithm for training each module. In this approach, a single-layer spiking neural network is constructed and trained with the Coulomb energy function based learning algorithm. If the learning result is not sufficiently good, another layer is added, and the input is the output of the previous layer. The process continues until a desired learning result is achieved. The approach eliminates the need for advance determination of the number of hidden layers and the need for error-backpropagation training in multi-layer spiking neural networks. Experimental results of classifying a two-ring-shaped dataset and segmenting an aerial image show that our proposed modular multi-layer spiking neural network requires a simple learning algorithm and achieves better results compared with other approaches.
C1 [Lee, Kyunghee] Pyeongtaek Univ, Dept Informat & Commun, Pyeongtaek, South Korea.
   [Shi, Hongchi] Texas State Univ, Dept Comp Sci, San Marcos, TX USA.
RP Lee, K (corresponding author), Pyeongtaek Univ, Dept Informat & Commun, Pyeongtaek, South Korea.
EM khlee@ptu.ac.kr; hs15@txstate.edu
CR Ahmadi P, 2013, 2013 6TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING (CISP), VOLS 1-3, P566, DOI 10.1109/CISP.2013.6744061
   BACHMANN CM, 1987, P NATL ACAD SCI USA, V84, P7529, DOI 10.1073/pnas.84.21.7529
   Bohte Sander M., 2004, Natural Computing, V3, P195, DOI 10.1023/B:NACO.0000027755.02868.60
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Booij O, 2005, INFORM PROCESS LETT, V95, P552, DOI 10.1016/j.ipl.2005.05.023
   Delshad Ehsan, 2010, 2010 5th International Symposium on Telecommunications (IST), P944, DOI 10.1109/ISTEL.2010.5734158
   Gerstner W., 2002, SPIKING NEURON MODEL
   Guojia Hou, 2017, IAENG International Journal of Computer Science, V44, P445
   Jain AK, 2010, PATTERN RECOGN LETT, V31, P651, DOI 10.1016/j.patrec.2009.09.011
   Maass W., 1999, PULSED NEURAL NETWOR
   Matsuda S, 2016, IEEE IJCNN, P293, DOI 10.1109/IJCNN.2016.7727211
   Meftah B, 2010, NEURAL PROCESS LETT, V32, P131, DOI 10.1007/s11063-010-9149-6
   Nakayama T, 2016, 2016 12TH INTERNATIONAL CONFERENCE ON NATURAL COMPUTATION, FUZZY SYSTEMS AND KNOWLEDGE DISCOVERY (ICNC-FSKD), P1778, DOI 10.1109/FSKD.2016.7603447
   Schrauwen B, 2004, IEEE IJCNN, P471, DOI 10.1109/IJCNN.2004.1379954
   Scofield C. L., 1988, IEEE International Conference on Neural Networks (IEEE Cat. No.88CH2632-8), P271, DOI 10.1109/ICNN.1988.23857
   Shrestha SB, 2013, ANN ALLERTON CONF, P506, DOI 10.1109/Allerton.2013.6736567
   Shrestha SB, 2016, IEEE IJCNN, P277, DOI 10.1109/IJCNN.2016.7727209
   Wakamatsu T, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P951, DOI 10.1109/IJCNN.2011.6033325
   Whalley K, 2013, NAT REV NEUROSCI, V14, DOI 10.1038/nrn3532
NR 19
TC 0
Z9 0
U1 0
U2 1
PY 2019
UT WOS:000530893800059
DA 2023-11-16
ER

PT C
AU Kaiser, J
   Tieck, JCV
   Hubschneider, C
   Wolf, P
   Weber, M
   Hoff, M
   Friedrich, A
   Wojtasik, K
   Roennau, A
   Kohlhaas, R
   Dillmann, R
   Zollner, JM
AF Kaiser, Jacques
   Tieck, J. Camilo Vasquez
   Hubschneider, Christian
   Wolf, Peter
   Weber, Michael
   Hoff, Michael
   Friedrich, Alexander
   Wojtasik, Konrad
   Roennau, Arne
   Kohlhaas, Ralf
   Dillmann, Rudiger
   Zollner, J. Marius
GP IEEE
TI Towards a framework for end-to-end control of a simulated vehicle with
   spiking neural networks
SO 2016 IEEE INTERNATIONAL CONFERENCE ON SIMULATION, MODELING, AND
   PROGRAMMING FOR AUTONOMOUS ROBOTS (SIMPAR)
DT Proceedings Paper
CT IEEE International Conference on Simulation, Modeling, and Programming
   for Autonomous Robots (SIMPAR)
CY DEC 13-16, 2016
CL San Francisco, CA
ID RETINA
AB Spiking neural networks are in theory more computationally powerful than rate-based neural networks often used in deep learning architectures. However, unlike rate-based neural networks, it is yet unclear how to train spiking networks to solve complex problems. There are still no standard algorithms and it is preventing roboticists to use spiking networks, yielding a lack of Neurorobotics applications. The contribution of this paper is twofold. First, we present a modular framework to evaluate neural self-driving vehicle applications. It provides a visual encoder from camera images to spikes inspired by the silicon retina (DVS), and a steering wheel decoder based on an agonist antagonist muscle model. Secondly, using this framework, we demonstrate a spiking neural network which controls a vehicle end-to-end for lane following behavior. The network is feed-forward and relies on hand-crafted feature detectors. In future work, this framework could be used to design more complex networks and use the evaluation metrics for learning.
C1 [Kaiser, Jacques; Tieck, J. Camilo Vasquez; Hubschneider, Christian; Wolf, Peter; Weber, Michael; Roennau, Arne; Kohlhaas, Ralf; Dillmann, Rudiger; Zollner, J. Marius] FZI Res Ctr Informat Technol, D-76131 Karlsruhe, Germany.
   [Hoff, Michael; Friedrich, Alexander; Wojtasik, Konrad; Dillmann, Rudiger; Zollner, J. Marius] KIT, Karlsruhe, Germany.
RP Kaiser, J (corresponding author), FZI Res Ctr Informat Technol, D-76131 Karlsruhe, Germany.
EM jkaiser@fzi.de; tieck@fzi.de; hubschneider@fzi.de; wolf@fzi.de;
   mweber@fzi.de; michael.hoff@student.kit.edu;
   alexander.friedrich3@student.kit.edu; konrad.wojtasik@student.kit.edu;
   roennau@fzi.de; kohlhaas@fzi.de; dillmann@fzi.de; zoellner@fzi.de
CR Ambrosano A, 2016, LECT NOTES COMPUT SC, V9793, P16, DOI 10.1007/978-3-319-42417-0_2
   [Anonymous], 2016, ARXIV160104862
   [Anonymous], 2016, ARXIV160104187
   [Anonymous], 2010, COMPUTABILITY CONTEX
   [Anonymous], 2015, P INT C LEARN REPR
   Bender P, 2014, IEEE INT VEH SYM, P420, DOI 10.1109/IVS.2014.6856487
   Bojarski Mariusz, 2016, arXiv
   Bouganis A., 2010, P 2010 INT JOINT C N, P1, DOI DOI 10.1109/IJCNN.2010.5596525
   Braitenberg V., 1986, VEHICLES EXPT SYNTHE
   Burgsteiner Harald, 2005, P 9 INT C ENG APPL N, P129
   Davison Andrew P, 2008, Front Neuroinform, V2, P11, DOI 10.3389/neuro.11.011.2008
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Falotico E., 2016, CONNECTING ARTIFICIA
   Galtier MN, 2013, NEURAL COMPUT, V25, P2815, DOI 10.1162/NECO_a_00512
   Giulioni M, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00035
   Giusti A, 2016, IEEE ROBOT AUTOM LET, V1, P661, DOI 10.1109/LRA.2015.2509024
   Koenig N., 2004, 2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566), P2149
   Krüger N, 2013, IEEE T PATTERN ANAL, V35, P1847, DOI 10.1109/TPAMI.2012.272
   Lichtsteiner P, 2008, IEEE J SOLID-ST CIRC, V43, P566, DOI 10.1109/JSSC.2007.914337
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Mafrica S, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON VEHICULAR ELECTRONICS AND SAFETY (ICVES), P96, DOI 10.1109/ICVES.2015.7396901
   Mafrica S, 2015, OPT EXPRESS, V23, P5614, DOI 10.1364/OE.23.005614
   Mahler J., 2016, INT C ROB AUT
   Marc-Oliver G., 2007, SCHOLARPEDIA, DOI [DOI 10.4249/SCHOLARPEDIA.1430, 10.4249/scholarpedia.1430]
   Mueggler E., 2014, EVENT BASED 6 DOF PO
   Orchard G., 2015, FRONTIERS NEUROSCIEN, V9, P1
   Paugam-Moisy H., 2012, HDB NATURAL COMPUTIN, V1, P1, DOI [DOI 10.1007/978-3-540-92910-9_10, 10.1007/978-3-540-92910-9_10]
   Probst Dimitri, 2012, Artificial Neural Networks and Machine Learning - ICANN 2012. Proceedings of the 22nd International Conference on Artificial Neural Networks, P209, DOI 10.1007/978-3-642-33269-2_27
   Quigley M, 2009, IEEE INT CONF ROBOT, P3604
   Schiess M., 2016, PLOS COMPUTATIONAL B, V12, P1
   Serrano-Gotarredona T., 2015, FRONT NEUROSCI-SWITZ, V9, P1
   Singla N., 2014, INT J INF COMPUT TEC, V4, P1559
   Tatai G., 1998, P HUNG NAT C AG BAS
   VALETON JM, 1983, VISION RES, V23, P1549, DOI 10.1016/0042-6989(83)90168-2
   Zofka M., 2016, INT C SIM MOD PROGR
NR 35
TC 36
Z9 37
U1 0
U2 4
PY 2016
BP 127
EP 134
UT WOS:000405933700019
DA 2023-11-16
ER

PT C
AU Kozdon, K
   Bentley, P
AF Kozdon, Katarzyna
   Bentley, Peter
BE Ikegami, T
   Virgo, N
   Witkowski, O
   Oka, M
   Suzuki, R
   Iizuka, H
TI The Evolution of Training Parameters for Spiking Neural Networks with
   Hebbian Learning
SO 2018 CONFERENCE ON ARTIFICIAL LIFE (ALIFE 2018)
DT Proceedings Paper
CT Conference on Artificial Life (ALIFE)
CY JUL 23-27, 2018
CL Tokyo, JAPAN
AB Spiking neural networks, thanks to their sensitivity to the timing of the inputs, are a promising tool for unsupervised processing of spatio-temporal data. However, they do not perform as well as the traditional machine learning approaches and their real-world applications are still limited. Various supervised and reinforcement learning methods for optimising spiking neural networks have been proposed, but more recently the evolutionary approach regained attention as a tool for training neural networks.
   Here, we describe a simple evolutionary approach for optimising spiking neural networks. This is the first published use of evolutionary algorithm to develop hyperparameters for fully unsupervised spike-timing-dependent learning for pattern clustering using spiking neural networks. Our results show that combining evolution and unsupervised learning leads to faster convergence on the optimal solutions, better stability of fit solutions and higher fitness of the whole population than using each approach separately.
C1 [Kozdon, Katarzyna; Bentley, Peter] UCL, Gower St, London WC1E 6BT, England.
   [Bentley, Peter] Braintree Ltd, 7 Gower St, London WC1E 6DP, England.
RP Kozdon, K (corresponding author), UCL, Gower St, London WC1E 6BT, England.
EM k.kozdon@cs.ucl.ac.uk
CR Abbott LF, 1999, BRAIN RES BULL, V50, P303, DOI 10.1016/S0361-9230(99)00161-6
   [Anonymous], 1896, AM NAT, DOI 10.1086/276408
   [Anonymous], 1992, COMPLEX SYST
   Belew R.K, 1992, SFI STUDIES SCI COMP, P511
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Bohte S. M., 2000, 8th European Symposium on Artificial Neural Networks. ESANN"2000. Proceedings, P419
   Chalmers D. J., 1990, P 1990 CONN MOD SUMM, P1
   Conti E., 2017, IMPROVING EXPLORATIO
   D'amour JA, 2015, NEURON, V86, P514, DOI 10.1016/j.neuron.2015.03.014
   Floreano D., 1996, From Animals to Animats 4. Proceedings of the Fourth International Conference on Simulation of Adaptive Behavior, P402
   Greve RB, 2016, GECCO'16: PROCEEDINGS OF THE 2016 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P117, DOI 10.1145/2908812.2908930
   Hebb DO, 1950, J CLIN PSYCHOL, V6, P307
   Hinton G. E., 1987, Complex Systems, V1, P495
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Hopfield JJ, 2001, P NATL ACAD SCI USA, V98, P1282, DOI 10.1073/pnas.031567098
   HUBEL DH, 1970, J PHYSIOL-LONDON, V206, P419, DOI 10.1113/jphysiol.1970.sp009022
   Kalia A, 2014, P NATL ACAD SCI USA, V111, P2035, DOI 10.1073/pnas.1311041111
   Kozdon K, 2017, 2017 IEEE SYMPOSIUM SERIES ON COMPUTATIONAL INTELLIGENCE (SSCI), P3183
   Lehman J., 2017, ES IS MORE THAN JUST
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Saggie K, 2004, NEUROCOMPUTING, V58, P303, DOI 10.1016/j.neucom.2004.01.060
   Salimans Tim, 2017, ABS170303864 CORR
   Stanley KO, 2003, IEEE C EVOL COMPUTAT, P2557
   Thomas BB, 2004, J NEUROSCI METH, V138, P7, DOI 10.1016/j.jneumeth.2004.03.007
   Wehrens R, 2007, J STAT SOFTW, V21, P1
   Wexler B. E., 2010, WORLD CULTURAL PSYCH, P11
   WILSON PD, 1966, J COMP PHYSIOL PSYCH, V61, P87, DOI 10.1037/h0022873
NR 30
TC 4
Z9 4
U1 0
U2 2
PY 2018
BP 276
EP 283
UT WOS:000494826700054
DA 2023-11-16
ER

PT J
AU Shi, MT
   Zhang, TL
   Zeng, Y
AF Shi, Mengting
   Zhang, Tielin
   Zeng, Yi
TI A Curiosity-Based Learning Method for Spiking Neural Networks (vol 14,
   7, 2020)
SO FRONTIERS IN COMPUTATIONAL NEUROSCIENCE
DT Correction
DE curiosity; spiking neural network; novelty; STDP; voltage-driven
   plasticity-centric SNN
C1 [Shi, Mengting; Zhang, Tielin; Zeng, Yi] Chinese Acad Sci, Inst Automat, Res Ctr Brain Inspired Intelligence, Beijing, Peoples R China.
   [Zeng, Yi] Univ Chinese Acad Sci, Beijing, Peoples R China.
   [Zeng, Yi] Chinese Acad Sci, Ctr Excellence Brain Sci & Intelligence Technol, Shanghai, Peoples R China.
   [Zeng, Yi] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing, Peoples R China.
RP Zeng, Y (corresponding author), Chinese Acad Sci, Inst Automat, Res Ctr Brain Inspired Intelligence, Beijing, Peoples R China.; Zeng, Y (corresponding author), Univ Chinese Acad Sci, Beijing, Peoples R China.; Zeng, Y (corresponding author), Chinese Acad Sci, Ctr Excellence Brain Sci & Intelligence Technol, Shanghai, Peoples R China.; Zeng, Y (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing, Peoples R China.
EM yi.zeng@ia.ac.cn
CR Sejnowski T. J., 1987, Complex Systems, V1, P145
   Shi MT, 2020, FRONT COMPUT NEUROSC, V14, DOI 10.3389/fncom.2020.00007
NR 2
TC 0
Z9 0
U1 0
U2 7
PD APR 15
PY 2020
VL 14
AR 28
DI 10.3389/fncom.2020.00028
UT WOS:000531238100001
DA 2023-11-16
ER

PT C
AU Asghar, MS
   Arslan, S
   Kim, H
AF Asghar, Malik Summair
   Arslan, Saad
   Kim, HyungWon
GP IEEE
TI Current Multiplier Based Synapse and Neuron Circuits for Compact SNN
   Chip
SO 2021 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS (ISCAS)
SE IEEE International Symposium on Circuits and Systems
DT Proceedings Paper
CT IEEE International Symposium on Circuits and Systems (IEEE ISCAS)
CY MAY 22-28, 2021
CL Daegu, SOUTH KOREA
DE Spiking Neural Network (SNN); Leaky Integrate and Fire (LIF);
   Neuromorphic; Artificial Neural Networks (ANN); Deep neural networks
   (DNN); Neural network Classifier (NNC); Image classification; CMOS
AB Spiking Neural Networks having biological plausible architecture are considered to be more suitable for energy efficient hardware implementation. When it comes to realize the hardware implementation of a large-scale Neural network for mobile applications, area and power consumption constraints become more critical. Optimizing Spiking neural network, constituting of neuron and synapse circuits, for area and power is essential. In this paper we present a more optimized version of Synapse and neuron circuits. We propose an analog CMOS implementation of a current multiplier charge injector-based Synapse and neuron circuit. The synapse circuit modulates the input spike rates by a trained weight value and injects an equivalent current. The neuron circuit integrates the injected synaptic current and evokes an output digital spike event. The circuit implementation is done using 65nm process design kit. The proposed circuit implementation exhibits all the temporal characteristics of Spiking neural networks. The circuit implementation has been optimized for area and power consumption and therefore can be easily constituted into a large-scale spiking neural network. Furthermore, the compact circuit implementation can benefit from the high resolution with very less increase in area and power.
C1 [Asghar, Malik Summair; Arslan, Saad; Kim, HyungWon] Chungbuk Natl Univ, Sch Elect Engn, Cheongju, South Korea.
   [Asghar, Malik Summair; Arslan, Saad] COMSATS Univ, Elect & Comp Engn Dept, Islamabad, Pakistan.
RP Asghar, MS (corresponding author), Chungbuk Natl Univ, Sch Elect Engn, Cheongju, South Korea.; Asghar, MS (corresponding author), COMSATS Univ, Elect & Comp Engn Dept, Islamabad, Pakistan.
EM summair@chungbuk.ac.kr; saad@chungbuk.ac.kr; hwkim@chungbuk.ac.kr
CR Aamir SA, 2018, IEEE T CIRCUITS-I, V65, P4299, DOI 10.1109/TCSI.2018.2840718
   Aayush A, P 54 ANN DES AUT C 2, P1
   [Anonymous], 2006, ADV NEURAL INFORM PR
   CHUA LO, 1971, IEEE T CIRCUITS SYST, VCT18, P507, DOI 10.1109/TCT.1971.1083337
   Ebong IE, 2012, P IEEE, V100, P2050, DOI 10.1109/JPROC.2011.2173089
   HODGKIN AL, 1990, B MATH BIOL, V52, P25, DOI 10.1016/S0092-8240(05)80004-7
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Kandel E. R., 2000, PRINCIPLES NEURAL SC, V4
   Kim S, 2017, ACS APPL MATER INTER, V9, P40420, DOI 10.1021/acsami.7b11191
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lee K, 2019, J SEMICOND TECH SCI, V19, P129, DOI 10.5573/JSTS.2019.19.1.129
   Li C, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-04484-2
   MEAD C, 1990, P IEEE, V78, P1629, DOI 10.1109/5.58356
   mead C.A., 1989, ANLAOG VLSI NEYRAL S
   Meier K, 2017, IEEE SPECTRUM, V54, P28, DOI 10.1109/MSPEC.2017.7934228
   Merolla P, 2011, IEEE CUST INTEGR CIR
   Miyashita D, 2016, IEEE ASIAN SOLID STA, P25, DOI 10.1109/ASSCC.2016.7844126
   Prezioso M, 2015, NATURE, V521, P61, DOI 10.1038/nature14441
   VONNEUMANN J, 1993, IEEE ANN HIST COMPUT, V15, P28
NR 19
TC 0
Z9 0
U1 2
U2 9
PY 2021
DI 10.1109/ISCAS51556.2021.9401173
UT WOS:000696765400119
DA 2023-11-16
ER

PT C
AU Du, PA
   Lin, XH
   Pi, XM
   Wang, XW
AF Du, Pangao
   Lin, Xianghong
   Pi, Xiaomei
   Wang, Xiangwen
BE Paul, R
TI An Unsupervised Learning Algorithm for Deep Recurrent Spiking Neural
   Networks
SO 2020 11TH IEEE ANNUAL UBIQUITOUS COMPUTING, ELECTRONICS & MOBILE
   COMMUNICATION CONFERENCE (UEMCON)
DT Proceedings Paper
CT 11th IEEE Annual Ubiquitous Computing, Electronics and Mobile
   Communication Conference (UEMCON)
CY OCT 28-31, 2020
CL ELECTR NETWORK
DE deep recurrent spiking neural network; unsupervised learning; recurrent
   spiking neural machines; reconstruction error
ID BACKPROPAGATION
AB Deep recurrent spiking neural networks (DRSNNs) are stacked with the recurrent spiking neural machine (RSNM) modules. However, because of their intricately discontinuous and complex recurrent structures, it is difficult to pre-training the synaptic weights of RSNMs by simple and effective learning method in deep recurrent network. This paper proposed a new unsupervised multi-spike learning rule and the RSNM is trained by this rule, the complex spatiotemporal pattern of spike trains are learned. The spike signal will complete the two processes of forward propagation and reverse reconstruction, and then adjust the synaptic weight according to the error. This algorithm is successfully applied to spike trains, the learning rate and neuron number in the RSNMs are analyzed. In addition, the layer-wise pre-training method of DRSNN is presented, and the reconstruction error shows the algorithm has a better learning effect.
C1 [Du, Pangao; Lin, Xianghong; Pi, Xiaomei; Wang, Xiangwen] Northwest Normal Univ, Coll Comp Sci & Engn, Lanzhou 730070, Peoples R China.
RP Lin, XH (corresponding author), Northwest Normal Univ, Coll Comp Sci & Engn, Lanzhou 730070, Peoples R China.
EM 2018211719@nwnu.edu.cn; linxh@nwnu.edu.cn; 2019211774@nwnu.edu.cn;
   wangxw2015@nwnu.edu.cn
CR [Anonymous], 2014, INT J COMPUT APPL, V88, P40
   Ant R. C., 2010, STAT SIGNAL PROCESSI, P265
   Bellec G., 2018, ARXIV PREPRINT ARXIV
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Costa RP, 2017, ADV NEUR IN, V30
   DEPASQUALE B, 2016, ARXIV PREPRINT ARXIV
   Gerstner W., 2002, SPIKING NEURON MODEL
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Gilra A, 2017, ELIFE, V6, DOI 10.7554/eLife.28295
   Huh D., 2018, GRADIENT DESCENT SPI, P1438
   Kasabov N, 2016, NEURAL NETWORKS, V78, P1, DOI 10.1016/j.neunet.2015.09.011
   Lin XH, 2018, LECT NOTES COMPUT SC, V11139, P222, DOI 10.1007/978-3-030-01418-6_22
   Lin XH, 2017, NEUROCOMPUTING, V237, P59, DOI 10.1016/j.neucom.2016.08.087
   Mohemmed A, 2012, INT J NEURAL SYST, V22, DOI 10.1142/S0129065712500128
   Paiva ARC, 2009, NEURAL COMPUT, V21, P424, DOI 10.1162/neco.2008.09-07-614
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Shen JR, 2017, IEEE ENG MED BIO, P2900, DOI 10.1109/EMBC.2017.8037463
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Tino P, 2006, NEURAL COMPUT, V18, P591, DOI 10.1162/089976606775623360
   Wang XW, 2020, NEURAL NETWORKS, V125, P258, DOI 10.1016/j.neunet.2020.02.011
   WERBOS PJ, 1990, P IEEE, V78, P1550, DOI 10.1109/5.58337
NR 21
TC 0
Z9 0
U1 1
U2 9
PY 2020
BP 603
EP 607
AR 1570680601
UT WOS:000652194300098
DA 2023-11-16
ER

PT J
AU Shrestha, SB
   Song, Q
AF Shrestha, Sumit Bam
   Song, Qing
TI Robust spike-train learning in spike-event based weight update
SO NEURAL NETWORKS
DT Article
DE Spiking neural network; Multilayer spike-train learning; Supervised
   learning; Weight convergence; Robust stability; Adaptive learning rate
ID NEURAL-NETWORK MODELS; GRADIENT DESCENT; NEURONS; CONVERGENCE; ALGORITHM
AB Supervised learning algorithms in a spiking neural network either learn a spike-train pattern for a single neuron receiving input spike-train from multiple input synapses or learn to output the first spike time in a feed forward network setting. In this paper, we build upon spike-event based weight update strategy to learn continuous spike-train in a spiking neural network with a hidden layer using a dead zone on-off based adaptive learning rate rule which ensures convergence of the learning process in the sense of weight convergence and robustness of the learning process to external disturbances. Based on different benchmark problems, we compare this new method with other relevant spike-train learning algorithms. The results show that the speed of learning is much improved and the rate of successful learning is also greatly improved. (C) 2017 Elsevier Ltd. All rights reserved.
C1 [Shrestha, Sumit Bam] Temasek Labs, 5A Engn Dr 1,09-02, Singapore 117411, Singapore.
   [Song, Qing] Nanyang Technol Univ, Sch Elect & Elect Engn, 50 Nanyang Ave, Singapore 639798, Singapore.
RP Song, Q (corresponding author), Nanyang Technol Univ, Sch Elect & Elect Engn, 50 Nanyang Ave, Singapore 639798, Singapore.
EM tslsbs@nus.edu.sg; eqsong@ntu.edu.sg
CR Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Booij O, 2005, INFORM PROCESS LETT, V95, P552, DOI 10.1016/j.ipl.2005.05.023
   Butts DA, 2007, NATURE, V449, P92, DOI [10.1038/nature06105, 10.1038/natureO6105]
   Carnell A., 2005, P ESANN, P363
   CLUETT WR, 1988, IEE PROC-D, V135, P133, DOI 10.1049/ip-d.1988.0019
   Dauwels Justin, 2008, ADV NEUROINFORMATION, P177
   Dethier J, 2011, I IEEE EMBS C NEUR E, P396, DOI 10.1109/NER.2011.5910570
   Florian RV, 2007, NEURAL COMPUT, V19, P1468, DOI 10.1162/neco.2007.19.6.1468
   GERSTNER W, 1995, PHYS REV E, V51, P738, DOI 10.1103/PhysRevE.51.738
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   Gollisch T, 2008, SCIENCE, V319, P1108, DOI 10.1126/science.1149639
   Gütig R, 2016, SCIENCE, V351, DOI 10.1126/science.aab4113
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Haykin S. S., 2009, NEURAL NETWORKS LEAR, V10
   Kumar A, 2010, NAT REV NEUROSCI, V11, P615, DOI 10.1038/nrn2886
   Lichman M., 2013, UCI MACHINE LEARNING
   Maass W, 1996, NEURAL COMPUT, V8, P1, DOI 10.1162/neco.1996.8.1.1
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maass W, 1997, ADV NEUR IN, V9, P211
   MAINEN ZF, 1995, SCIENCE, V268, P1503, DOI 10.1126/science.7770778
   Markram H, 2006, NAT REV NEUROSCI, V7, P153, DOI 10.1038/nrn1848
   McKennoch S, 2006, IEEE IJCNN, P3970
   Mohemmed A, 2013, NEUROCOMPUTING, V107, P3, DOI 10.1016/j.neucom.2012.08.034
   Mohemmed A, 2012, INT J NEURAL SYST, V22, DOI 10.1142/S0129065712500128
   ORTEGA R, 1985, IEEE T AUTOMAT CONTR, V30, P1179, DOI 10.1109/TAC.1985.1103890
   Paugam-Moisy H., 2011, HDB NATURAL COMPUTIN, V1, P335
   Ponulak F., 2005, RESUME NEW SUPERVISE
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Rom R, 2007, IEEE T NEURAL NETWOR, V18, P542, DOI 10.1109/TNN.2006.890806
   Schrauwen Benjamin, 2004, NEUR NETW 2004 P 200, V1
   Shadlen Michael N., 1994, Current Opinion in Neurobiology, V4, P569, DOI 10.1016/0959-4388(94)90059-0
   Shrestha S. B., 2017, IEEE T NEURAL NETWOR, P1
   Shrestha SB, 2016, 2016 15TH IEEE INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA 2016), P333, DOI [10.1109/ICMLA.2016.92, 10.1109/ICMLA.2016.0061]
   Shrestha SB, 2015, NEURAL NETWORKS, V63, P185, DOI 10.1016/j.neunet.2014.12.001
   Shrestha SB, 2013, ANN ALLERTON CONF, P506, DOI 10.1109/Allerton.2013.6736567
   Shrestha SB, 2017, NEURAL NETWORKS, V86, P54, DOI 10.1016/j.neunet.2016.10.011
   Sporea I, 2013, NEURAL COMPUT, V25, P473, DOI 10.1162/NECO_a_00396
   Van Rullen R, 2001, NEURAL COMPUT, V13, P1255, DOI 10.1162/08997660152002852
   Webb B, 2000, BIOL CYBERN, V82, P247, DOI 10.1007/s004220050024
   Xu Y, 2013, NEURAL NETWORKS, V43, P99, DOI 10.1016/j.neunet.2013.02.003
   Yu QF, 2013, PLOS ONE, V8, DOI [10.1371/journal.pone.0059627, 10.1371/journal.pone.0078318]
NR 41
TC 16
Z9 16
U1 0
U2 15
PD DEC
PY 2017
VL 96
BP 33
EP 46
DI 10.1016/j.neunet.2017.08.010
UT WOS:000413319800004
DA 2023-11-16
ER

PT C
AU Yusoff, N
   Ahmad, FK
AF Yusoff, Nooraini
   Ahmad, Farzana Kabir
BE Loo, CK
   Yap, KS
   Wong, KW
   Teoh, A
   Huang, K
TI Spiking Neural Network with Lateral Inhibition for Reward-Based
   Associative Learning
SO NEURAL INFORMATION PROCESSING (ICONIP 2014), PT I
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 21st International Conference on Neural Information Processing (ICONIP)
CY NOV 03-06, 2014
CL Kuching, MALAYSIA
DE Lateral inhibition; Spiking neural network; Associative Learning;
   Spike-time dependent plasticity
ID POLYCHRONIZATION
AB In this paper we propose a lateral inhibitory spiking neural network for reward-based associative learning with correlation in spike patterns for conflicting responses. The network has random and sparse connectivity, and we introduce a lateral inhibition via an anatomical constraint and synapse reinforcement. The spiking dynamic follows the properties of Izhikevich spiking model. The learning involves association of a delayed stimulus pair to a response using reward modulated spike-time dependent plasticity (STDP). The proposed learning scheme has improved our initial work by allowing learning in a more dynamic and competitive environment.
C1 [Yusoff, Nooraini; Ahmad, Farzana Kabir] Univ Utara Malaysia, Coll Arts & Sci, Sch Comp, Sintok 06010, Kedah, Malaysia.
RP Yusoff, N (corresponding author), Univ Utara Malaysia, Coll Arts & Sci, Sch Comp, Sintok 06010, Kedah, Malaysia.
EM nooraini@uum.edu.my
CR Botvinick MM, 2001, PSYCHOL REV, V108, P624, DOI 10.1037//0033-295X.108.3.624
   Brunel N, 2009, J COGNITIVE NEUROSCI, V21, P2300, DOI 10.1162/jocn.2008.21156
   Cutsuridis V, 2009, NEURAL NETWORKS, V22, P1120, DOI 10.1016/j.neunet.2009.07.009
   Florian RV, 2007, NEURAL COMPUT, V19, P1468, DOI 10.1162/neco.2007.19.6.1468
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2007, CEREB CORTEX, V17, P2443, DOI 10.1093/cercor/bhl152
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Jones EG, 2000, P NATL ACAD SCI USA, V97, P5019, DOI 10.1073/pnas.97.10.5019
   Kaplan GB, 2007, NEUROCOMPUTING, V70, P1414, DOI 10.1016/j.neucom.2006.05.009
   Paugam-Moisy H, 2008, NEUROCOMPUTING, V71, P1143, DOI 10.1016/j.neucom.2007.12.027
   Yusoff N, 2012, PROCEDIA ENGINEER, V41, P319, DOI 10.1016/j.proeng.2012.07.179
   Yusoff N, 2012, LECT NOTES COMPUT SC, V7663, P168, DOI 10.1007/978-3-642-34475-6_21
NR 12
TC 0
Z9 0
U1 0
U2 3
PY 2014
VL 8834
BP 327
EP 334
UT WOS:000432659500041
DA 2023-11-16
ER

PT J
AU Wang, RC
   Cohen, G
   Stiefel, KM
   Hamilton, TJ
   Tapson, JT
   van Schaik, A
AF Wang, Runchun
   Cohen, Gregory
   Stiefel, Klaus M.
   Hamilton, Tara Julia
   Tapson, Jonathan
   van schaik, Andre
TI An FPGA implementation of a polychronous spiking neural network with
   delay adaptation
SO FRONTIERS IN NEUROSCIENCE
DT Article
DE neuromorphic engineering; polychronous network; time multiplexing;
   spiking neurons; delay adaptation
ID POLYCHRONIZATION; NEURONS
AB We present an FPGA implementation of a re-configurable, polychronous spiking neural network with a large capacity for spatial-temporal patterns. The proposed neural network generates delay paths de novo, so that only connections that actually appear in the training patterns will be created. This allows the proposed network to use all the axons (variables) to store information. Spike Timing Dependent Delay Plasticity is used to fine-tune and add dynamics to the network. We use a time multiplexing approach allowing us to achieve 4096 (4k) neurons and up to 1.15 million programmable delay axons on a Virtex 6 FPGA. Test results show that the proposed neural network is capable of successfully recalling more than 95% of all spikes for 96% of the stored patterns. The tests also show that the neural network is robust to noise from random input spikes.
C1 [Wang, Runchun; Cohen, Gregory; Stiefel, Klaus M.; Hamilton, Tara Julia; Tapson, Jonathan; van schaik, Andre] Univ Western Sydney, MARCS Inst, Penrith, NSW 2751, Australia.
RP van Schaik, A (corresponding author), Univ Western Sydney, MARCS Inst, Locked Bag 1797, Penrith, NSW 2751, Australia.
EM a.vanschaik@uws.edu.au
CR Angelini L., 2007, P EUR S ART NEUR NET, P459
   [Anonymous], 2011, 45 ANN C INFORM SCI
   Belhadj Bilel, 2008, 2008 15th IEEE International Conference on Electronics, Circuits and Systems (ICECS 2008), P93, DOI 10.1109/ICECS.2008.4674799
   Boahen K., 2006, C P IEEE ENG M S6702
   Boahen KA, 2000, IEEE T CIRCUITS-II, V47, P416, DOI 10.1109/82.842110
   Eurich CW, 2000, NEUROCOMPUTING, V32, P741, DOI 10.1016/S0925-2312(00)00239-3
   Furber S., 2010, 2010 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2010.5596364
   Gerstner W, 1996, NATURE, V383, P76, DOI 10.1038/383076a0
   Giulioni M, 2012, FRONT NEUROSCI-SWITZ, V6, DOI 10.3389/fnins.2011.00149
   Indiveri G, 2006, IEEE T NEURAL NETWOR, V17, P211, DOI 10.1109/TNN.2005.860850
   Indiveri G, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00118
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Kim S, 2011, 2011 11TH INTERNATIONAL CONFERENCE ON CONTROL, AUTOMATION AND SYSTEMS (ICCAS), P1
   Linares-Barrancoa A, 2007, NEUROCOMPUTING, V70, P2692, DOI 10.1016/j.neucom.2006.07.020
   Merolla P, 2011, IEEE CUST INTEGR CIR
   Paugam-Moisy H, 2008, NEUROCOMPUTING, V71, P1143, DOI 10.1016/j.neucom.2007.12.027
   Pfeil T, 2012, FRONT NEUROSCI-SWITZ, V6, DOI 10.3389/fnins.2012.00090
   Ranhel J., 2011, Proceedings 2011 IEEE Symposium on Foundations of Computational Intelligence (FOCI 2011), P66, DOI 10.1109/FOCI.2011.5949465
   Serrano-Gotarredona R, 2009, IEEE T NEURAL NETWOR, V20, P1417, DOI 10.1109/TNN.2009.2023653
   STANFORD LR, 1987, SCIENCE, V238, P358, DOI 10.1126/science.3659918
   Stevens B, 1998, J NEUROSCI, V18, P9303
   SWADLOW HA, 1994, J NEUROPHYSIOL, V71, P437, DOI 10.1152/jn.1994.71.2.437
   SWADLOW HA, 1985, J NEUROPHYSIOL, V54, P1346, DOI 10.1152/jn.1985.54.5.1346
   Wang R., 2011, 2011 7 INT C INT SEN, P97, DOI [10.1109/ISSNIP.2011.6146572, DOI 10.1109/ISSNIP.2011.6146572]
   Wang RC, 2012, IEEE INT SYMP CIRC S, P2413, DOI 10.1109/ISCAS.2012.6271785
NR 25
TC 42
Z9 43
U1 2
U2 19
PY 2013
VL 7
AR 14
DI 10.3389/fnins.2013.00014
UT WOS:000346567300014
DA 2023-11-16
ER

PT C
AU Moore, BJ
   Berger, T
   Song, D
AF Moore, Bryan J.
   Berger, Theodore
   Song, Dong
GP IEEE
TI Validation of a Convolutional Neural Network Model for Spike
   Transformation Using a Generalized Linear Model
SO 42ND ANNUAL INTERNATIONAL CONFERENCES OF THE IEEE ENGINEERING IN
   MEDICINE AND BIOLOGY SOCIETY: ENABLING INNOVATIVE TECHNOLOGIES FOR
   GLOBAL HEALTHCARE EMBC'20
SE IEEE Engineering in Medicine and Biology Society Conference Proceedings
DT Proceedings Paper
CT 42nd Annual International Conference of the
   IEEE-Engineering-in-Medicine-and-Biology-Society (EMBC)
CY JUL 20-24, 2020
CL Montreal, CANADA
AB Identification of causal relationships of neural activity is one of the most important problems in neuroscience and neural engineering. We show that a novel deep learning approach using a convolutional neural network to model output neural spike activity from input neural spike activity is able to achieve high correlation between the predicted probability of spiking in the output neuron and the true probability of spiking in the output neuron for data generated with a generalized linear model. The convolutional neural network is also able to recover the true model variables (kernels) used to generate the probability of spiking in the output neuron. Based on the convolutional neural network model's validation via a generalized linear model, future work will include validation with non-linear models that use higher-order kernels.
C1 [Moore, Bryan J.; Berger, Theodore; Song, Dong] Univ Southern Calif, Viterbi Sch Engn, Ctr Neural Engn, Los Angeles, CA 90007 USA.
RP Moore, BJ (corresponding author), Univ Southern Calif, Viterbi Sch Engn, Ctr Neural Engn, Los Angeles, CA 90007 USA.
EM bryanjmo@usc.edu; berger@usc.edu; dsong@usc.edu
CR Biewald Lukas, 2020, EXPT TRACKING WEIGHT
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song D, 2007, IEEE T BIO-MED ENG, V54, P1053, DOI 10.1109/TBME.2007.891948
   Song D, 2018, IEEE T NEUR SYS REH, V26, P272, DOI 10.1109/TNSRE.2016.2604423
   Song D, 2013, J COMPUT NEUROSCI, V35, P335, DOI 10.1007/s10827-013-0455-7
   Zoumpourlis G, 2017, IEEE I CONF COMP VIS, P4771, DOI 10.1109/ICCV.2017.510
NR 6
TC 4
Z9 4
U1 0
U2 2
PY 2020
BP 3236
EP 3239
UT WOS:000621592203143
DA 2023-11-16
ER

PT C
AU Tao, XL
   Michel, HE
AF Tao, XL
   Michel, HE
BE Arabnia, HR
   Mun, Y
TI Data clustering via spiking neural networks through spike
   timing-dependent plasticity
SO IC-AI '04 & MLMTA'04 , VOL 1 AND 2, PROCEEDINGS
DT Proceedings Paper
CT International Conference on Artificial Intelligence/International
   Conference on Machine Learning, Models, Technologies and Applications
CY JUN 21-24, 2004
CL Las Vegas, NV
DE data clustering; spiking neural networks; Hebbian Learning;
   Spike-Timing-Dependent-Plasticity
ID NEURONS
AB A new spiking-neural-network model for partitioning data into clusters has been developed The learning process is based on the Spike Timing-Dependent Plasticity rule under the Hebbian Learning framework. With temporally encoded inputs, the synaptic efficiencies of the delays between the pre and postsynaptic spikes can store the information of different data clusters. Various simulation results show that the model is able to perform the data clustering successfully and reach a stable status given enough data samples.
C1 Univ Massachusetts, Dept Elect & Comp Engn, N Dartmouth, MA 02747 USA.
RP Tao, XL (corresponding author), Univ Massachusetts, Dept Elect & Comp Engn, N Dartmouth, MA 02747 USA.
CR Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Gerstner W, 2002, BIOL CYBERN, V87, P404, DOI 10.1007/s00422-002-0353-y
   HEBB DO, 1979, ORG BEHAV
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   Maass W., 1999, PULSED NEURAL NETWOR
   Natschlager T, 1998, NETWORK-COMP NEURAL, V9, P319, DOI 10.1088/0954-898X/9/3/003
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   van Rossum MCW, 2000, J NEUROSCI, V20, P8812
NR 8
TC 7
Z9 7
U1 0
U2 1
PY 2004
BP 168
EP 173
UT WOS:000226030400025
DA 2023-11-16
ER

PT C
AU Mirsu, R
   Tiponut, V
AF Mirsu, Radu
   Tiponut, Virgil
GP IEEE
TI Parallel Model for Spiking Neural Networks using MATLAB
SO 2010 9TH INTERNATIONAL SYMPOSIUM ON ELECTRONICS AND TELECOMMUNICATIONS
   (ISETC)
DT Proceedings Paper
CT 9th International Symposium on Electronics and Telecommunications
   (ISETC)
CY NOV 11-12, 2010
CL Timisoara, ROMANIA
DE spiking neural networks; neural modeling; parallel processing; MATLAB
   modelling
AB Spiking Neural Networks are the last generation of neural models. Because the model is recent, very few dedicated simulation frameworks exist. This paper proposes a simulation framework developed in MATLAB that can be useful at: designing the network, uploading input stimuli, simulating the network, processing and displaying the results. The framework can be run on a network of computers by exploiting the parallelism of the model. The result is an improved performance by reducing simulation time.
C1 [Mirsu, Radu; Tiponut, Virgil] Politehn Univ Timisoara, Dept Appl Elect, Timisoara, Romania.
RP Mirsu, R (corresponding author), Politehn Univ Timisoara, Dept Appl Elect, Timisoara, Romania.
EM radu.mirsu@etc.upt.ro
CR Bogdanov I, 2009, MATH COMPUT SCI ENG, P533
   Gerstner W., 2002, SPIKING NEURON MODEL
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Mirsu R, 2009, REC ADV COMPUT ENG, P318
   WILLS SA, 2004, THESIS U CAMBRIDGE
   YOROZU, 1987, IEEE T J MAGN JAPAN, V2, P740
NR 6
TC 1
Z9 1
U1 0
U2 2
PY 2010
BP 369
EP 372
DI 10.1109/ISETC.2010.5679345
UT WOS:000296356700079
DA 2023-11-16
ER

PT C
AU Zhou, XQ
   Song, ZY
   Wu, X
   Yan, R
AF Zhou, Xueqian
   Song, Zeyang
   Wu, Xi
   Yan, Rui
GP IEEE
TI A Spiking Deep Convolutional Neural Network Based on Efficient Spike
   Timing Dependent Plasticity
SO 2020 3RD INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND BIG
   DATA (ICAIBD 2020)
DT Proceedings Paper
CT 3rd International Conference on Artificial Intelligence and Big Data
   (ICAIBD)
CY MAY 28-31, 2020
CL ELECTR NETWORK
DE unsupervised learning; object recognition; spiking neural network; spike
   timing dependent plasticity
ID NEURONS
AB Due to the complex neurodynamics and the lack of effective framework, the recognition performance of spiking neural networks is still not satisfied in the real applications with complex data sets. In this paper, an unsupervised learning method for spiking neural networks is proposed to improve capacity for feature extraction. First a new weight learning strategy based on spike-timing-dependent plasticity (STDP) is designed. It regulates the strength of synaptic connections and conforms to biological neural characteristics. Then the neuron normalization mechanism is added after the convolutional and max pooling layers to adjust the neuronal selectivity and strengthen the eigenvalues of objects. In the last layer of networks, linear support vector machine is used to recognize objects from extracted spike trains. The proposed networks can retain more effective synaptic information and also achieve faster convergence. Experimental results show that, the proposed algorithm achieves higher accuracies on the data sets of Caltech 101 and ETH 80.
C1 [Zhou, Xueqian; Song, Zeyang; Wu, Xi; Yan, Rui] Sichuan Univ, Coll Comp Sci, Chengdu, Peoples R China.
RP Zhou, XQ (corresponding author), Sichuan Univ, Coll Comp Sci, Chengdu, Peoples R China.
EM 2017223049272@stu.scu.edu.cn; 2016141502072@stu.scu.edu.cn;
   wuxi9410@gmail.com; yanrui2006@gmail.com
CR [Anonymous], 2017, NEURAL NETWORKS
   Bellec G, 2018, LONG SHORT TERM MEMO
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Caporale N, 2008, ANNU REV NEUROSCI, V31, P25, DOI 10.1146/annurev.neuro.31.060407.125639
   DEVALOIS RL, 1965, COLD SPRING HARB SYM, V30, P567, DOI 10.1101/SQB.1965.030.01.055
   DiCarlo JJ, 2012, NEURON, V73, P415, DOI 10.1016/j.neuron.2012.01.010
   Diehl PU, 2015, IEEE IJCNN
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   ENGEL AK, 1992, TRENDS NEUROSCI, V15, P218, DOI 10.1016/0166-2236(92)90039-B
   Essen D. C. V., 1995, INTRO NEURAL ELECT N
   Falez P, 2019, MULTILAYERED SPIKING
   Ferré P, 2018, FRONT COMPUT NEUROSC, V12, DOI 10.3389/fncom.2018.00024
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Izhikevich EM, 2003, NEURAL COMPUT, V15, P1511, DOI 10.1162/089976603321891783
   Lee C, 2019, IEEE T COGN DEV SYST, V11, P384, DOI 10.1109/TCDS.2018.2833071
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Masquelier T, 2007, PLOS COMPUT BIOL, V3, P247, DOI 10.1371/journal.pcbi.0030031
   McKennoch S, 2009, NEURAL COMPUT, V21, P9, DOI 10.1162/neco.2008.09-07-610
   Mozafari M, 2018, IEEE T NEUR NET LEAR, V29, P6178, DOI 10.1109/TNNLS.2018.2826721
   Natschlager T., 1998, ONLINE CLUSTERING SP
   Nicola W, 2017, NAT COMMUN, V8, DOI 10.1038/s41467-017-01827-3
   Paugam-Moisy H., 2012, COMPUTING SPIKING NE, ppp 335
   Robert C. F., 2002, NATURE, V416
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Song ZY, 2019, LECT NOTES COMPUT SC, V11555, P361, DOI 10.1007/978-3-030-22808-8_36
   Wu Y., 2018, DIRECT TRAINING SPIK
   Yuille AL, 1989, NEURAL COMPUT, V1, P334, DOI 10.1162/neco.1989.1.3.334
NR 30
TC 1
Z9 1
U1 0
U2 4
PY 2020
BP 39
EP 45
DI 10.1109/icaibd49809.2020.9137430
UT WOS:000618763400008
DA 2023-11-16
ER

PT J
AU Bayro-Corrochano, E
   Lechuga-Gutiérrez, L
   Garza-Burgos, M
AF Bayro-Corrochano, Eduardo
   Lechuga-Gutierrez, Luis
   Garza-Burgos, Marcela
TI Geometric techniques for robotics and HMI: Interpolation and haptics in
   conformal geometric algebra and control using quaternion spike neural
   networks
SO ROBOTICS AND AUTONOMOUS SYSTEMS
DT Article
DE Geometric algebra; Conformal geometric algebra; Quaternion algebra;
   Graphics engineering; Interpolation; Haptics; Non-linear control; Spike
   neural networks; Quaternion spike neural networks; Human-machine
   interaction; Robotics; Medical robotics
AB In this work, by reformulating screw theory (generalization of quaternions) in the conformal geometric algebra framework, we address the interpolation, virtual reality, graphics engineering, haptics. We derive intuitive geometric equations to handle surface operations like in kidney surgery. The interpolation can handle the interpolation and dilation in 3D of points, lines, planes, circles and spheres. With this procedure, we interpolate trajectories of surgical instrument. Using quaternions, we formulate the quaternion spike neural network for control. This new neural network structure is based on Spike Neural Networks and developed using the quatemion algebra. The real valued training algorithm was extended so that it could make adjustments of the weights according to the properties and product of the quaternion algebra. In this spike neural network, we are taking into account two relevant ideas the use of Spike neural network which is the best model for oculo-motor control and the role of geometric computing. As illustration. the quaternion spike neural network is applied for control of robot manipulator. The experimental analysis shows promising possibilities for the use of this powerful geometric language to handle multiple tasks in human-machine interaction and robotics. (C) 2018 Elsevier B.V. All rights reserved.
C1 [Bayro-Corrochano, Eduardo; Lechuga-Gutierrez, Luis; Garza-Burgos, Marcela] CINVESTAV, Campus Guadalajara,Av Basque 1145, Zapopan 45019, Jalisco, Mexico.
RP Bayro-Corrochano, E (corresponding author), CINVESTAV, Campus Guadalajara,Av Basque 1145, Zapopan 45019, Jalisco, Mexico.
EM edb@gdl.cinvestav.mx
CR Bayro-Corrochano E, 2006, J MATH IMAGING VIS, V24, P55, DOI 10.1007/s10851-005-3615-1
   BayroCorrochano E, 2010, GEOMETRIC COMPUTING: FOR WAVELET TRANSFORMS, ROBOT VISION, LEARNING, CONTROL AND ACTION, P1, DOI 10.1007/978-1-84882-929-9
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   DEAVOURS CA, 1973, AM MATH MON, V80, P995, DOI 10.2307/2318774
   Delorme A, 1999, NEUROCOMPUTING, V26-7, P989, DOI 10.1016/S0925-2312(99)00095-8
   ETZEL KR, 1996, P 1996 ASME DES ENG
   Gerstner W., 2002, SPIKING NEURON MODEL
   Hamilton W. R., LECT QUATERNIONS, P1853
   Hamilton WR., 1969, ELEMENTS QUATERNIONS
   Hestenes D., 2012, CLIFFORD ALGEBRA GEO
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   JUTTLER B, 1994, COMPUT GRAPH, V18, P315, DOI 10.1016/0097-8493(94)90033-7
   Kavan L., GEOMETRIC SKINNING A
   Klawitter D., 2010, DOCUMENTATION KINEMA
   Lechuga-Gutierrez L., 2016, LNCS, V9719
   Li HB, 2001, GEOMETRIC COMPUTING WITH CLIFFORD ALGEBRAS, P27
   PRAUTZCH H, 2002, BEZIER B SPLINE TECH
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Sepulveda-Cervantes G., 2015, APPL MATH INFORM SCI, V9, P113
NR 19
TC 6
Z9 6
U1 1
U2 40
PD JUN
PY 2018
VL 104
BP 72
EP 84
DI 10.1016/j.robot.2018.02.015
UT WOS:000430891900006
DA 2023-11-16
ER

PT C
AU Sedghi, M
   Ahmadi, A
   Eskandari, E
   Heydari, R
AF Sedghi, Maryam
   Ahmadi, Arash
   Eskandari, Elahe
   Heydari, Ramiyar
GP IEEE
TI A Performance Evaluation of Probabilistic Vs. Deterministic Spiking
   Neural Network
SO 2014 22nd Iranian Conference on Electrical Engineering (ICEE)
SE Iranian Conference on Electrical Engineering
DT Proceedings Paper
CT 22nd Iranian Conference on Electrical Engineering (ICEE)
CY MAY 20-22, 2014
CL Shahid Beheshti Univ, Tehran, IRAN
HO Shahid Beheshti Univ
DE Probabilistic Spiking Neural Network (PSNN); Spiking Neural Network
   (SNN); Back Propagation Algorithm
ID NEURONS; MODEL
AB this paper aims to present a comparison between probabilistic and deterministic spiking neural network for a back Propagation classification algorithm. To have a fair comparison, neuron models and structures are considered identical in both of the networks. The networks are trained and tested with the Iris database. According to the simulation results, the probabilistic network converges faster than the deterministic one, where it is also more sensitive to the input variations. The simulation results show a precision of 90% 88% for the probabilistic and the deterministic networks correspondingly, which is in consistence with the similar results for linear neural networks.
C1 [Sedghi, Maryam; Ahmadi, Arash; Eskandari, Elahe; Heydari, Ramiyar] Razi Univ, Dept Elect Engn, Kermanshah, Iran.
RP Sedghi, M (corresponding author), Razi Univ, Dept Elect Engn, Kermanshah, Iran.
CR Bishop CM, 2006, PATTERN RECOGNITIONA
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Gerstner W., 2002, SPIKING NEURON MODEL
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Hsieh Hung-Yi, HARDWARE FR IN PRESS
   Izhikevich EM, 2001, NEURAL NETWORKS, V14, P883, DOI 10.1016/S0893-6080(01)00078-8
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM., 2007, DYNAMICAL SYSTEMS NE, DOI [DOI 10.1017/S0143385704000173, 10.7551/mitpress/2526.001.0001]
   Kasabov N, 2010, NEURAL NETWORKS, V23, P16, DOI 10.1016/j.neunet.2009.08.010
   LIPPMANN RP, 1989, IEEE COMMUN MAG, V27, P47, DOI 10.1109/35.41401
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Schliebs S, 2010, INT J NEURAL SYST, V20, P481, DOI 10.1142/S0129065710002565
   Soleimani H., 2012, IEEE T CIRCUITS SY 1, V59
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   Wade JJ, 2010, IEEE T NEURAL NETWOR, V21, P1817, DOI 10.1109/TNN.2010.2074212
   Wu QX, 2006, NEUROCOMPUTING, V69, P1912, DOI 10.1016/j.neucom.2005.11.023
   Yegnanarayana B., 1999, ARTIFICIAL NEURAL NE
NR 18
TC 0
Z9 0
U1 0
U2 0
PY 2014
BP 274
EP 278
UT WOS:000392812600057
DA 2023-11-16
ER

PT J
AU Lin, XH
   Wang, XW
   Hao, ZJ
AF Lin, Xianghong
   Wang, Xiangwen
   Hao, Zhanjun
TI Supervised learning in multilayer spiking neural networks with inner
   products of spike trains
SO NEUROCOMPUTING
DT Article
DE Spiking neural networks; Supervised learning; Inner products of spike
   trains; Multilayer feedforward network; Backpropagation algorithm
ID ERROR-BACKPROPAGATION; GRADIENT DESCENT; CLASSIFICATION; ALGORITHM;
   NEURONS; SPACE; MODEL
AB Recent advances in neurosciences have revealed that neural information in the brain is encoded through precisely timed spike trains, not only through the neural firing rate. This paper presents a new supervised, multi-spike learning algorithm for multilayer spiking neural networks, which can implement the complex spatio-temporal pattern learning of spike trains. The proposed algorithm firstly defines inner product operators to mathematically describe and manipulate spike trains, and then solves the problems of error function construction and backpropagation among multiple output spikes during learning. The algorithm is successfully applied to different temporal tasks, such as learning sequences of spikes and nonlinear pattern classification problems. The experimental results show that the proposed algorithm has higher learning accuracy and efficiency than the Multi-ReSuMe learning algorithm. It is effective for solving complex spatio-temporal pattern learning problems. (C) 2016 Elsevier B.V. All rights reserved.
C1 [Lin, Xianghong; Wang, Xiangwen; Hao, Zhanjun] Northwest Normal Univ, Sch Comp Sci & Engn, Lanzhou 730070, Peoples R China.
RP Wang, XW (corresponding author), Northwest Normal Univ, Sch Comp Sci & Engn, Lanzhou 730070, Peoples R China.
EM wangxiangwen2@163.com
CR Almási AD, 2016, NEUROCOMPUTING, V174, P31, DOI 10.1016/j.neucom.2015.02.092
   [Anonymous], 2009, NEURAL NETWORKS LEAR
   Bohte Sander M., 2004, Natural Computing, V3, P195, DOI 10.1023/B:NACO.0000027755.02868.60
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Booij O, 2005, INFORM PROCESS LETT, V95, P552, DOI 10.1016/j.ipl.2005.05.023
   Brunel N, 2004, NEURON, V43, P745, DOI 10.1016/S0896-6273(04)00528-8
   Caporale N, 2008, ANNU REV NEUROSCI, V31, P25, DOI 10.1146/annurev.neuro.31.060407.125639
   Carnell A., 2005, P ESANN, P363
   Cash S, 1999, NEURON, V22, P383, DOI 10.1016/S0896-6273(00)81098-3
   Chauvin Y., 1995, BACKPROPAGATION THEO
   Fang HJ, 2012, CHINESE J CHEM ENG, V20, P1219, DOI 10.1016/S1004-9541(12)60611-9
   Florian RV, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0040233
   Gerstner W., 2002, SPIKING NEURON MODEL
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Guediche S., 2014, CEREB CORTEX
   Gütig R, 2003, J NEUROSCI, V23, P3697
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   KNUDSEN EI, 1994, J NEUROSCI, V14, P3985
   Knudsen EI, 2002, NATURE, V417, P322, DOI 10.1038/417322a
   Kuriscak E, 2015, NEUROCOMPUTING, V152, P27, DOI 10.1016/j.neucom.2014.11.022
   Legenstein R, 2005, NEURAL COMPUT, V17, P2337, DOI 10.1162/0899766054796888
   Lin Xiang-hong, 2009, Acta Electronica Sinica, V37, P1270
   [蔺想红 Lin Xianghong], 2015, [电子学报, Acta Electronica Sinica], V43, P577
   Maass W, 1996, NEURAL COMPUT, V8, P1, DOI 10.1162/neco.1996.8.1.1
   Maass W, 1997, NEURAL COMPUT, V9, P279, DOI 10.1162/neco.1997.9.2.279
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   McKennoch S, 2006, IEEE IJCNN, P3970
   McKennoch S, 2009, NEURAL COMPUT, V21, P9, DOI 10.1162/neco.2008.09-07-610
   Memmesheimer RM, 2014, NEURON, V82, P925, DOI 10.1016/j.neuron.2014.03.026
   Mohemmed A, 2013, NEUROCOMPUTING, V107, P3, DOI 10.1016/j.neucom.2012.08.034
   Mohemmed A, 2012, INT J NEURAL SYST, V22, DOI 10.1142/S0129065712500128
   Olshausen BA, 2003, J COGNITIVE NEUROSCI, V15, P154, DOI 10.1162/089892903321107891
   Paiva ARC, 2009, NEURAL COMPUT, V21, P424, DOI 10.1162/neco.2008.09-07-614
   Park IM, 2013, IEEE SIGNAL PROC MAG, V30, P149, DOI 10.1109/MSP.2013.2251072
   Park IM, 2012, NEURAL COMPUT, V24, P2223, DOI 10.1162/NECO_a_00309
   Ponulak F, 2008, INT J APPL MATH COMP, V18, P117, DOI 10.2478/v10006-008-0011-1
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Sporea I, 2013, NEURAL COMPUT, V25, P473, DOI 10.1162/NECO_a_00396
   Wade JJ, 2010, IEEE T NEURAL NETWOR, V21, P1817, DOI 10.1109/TNN.2010.2074212
   Whalley K, 2013, NAT REV NEUROSCI, V14, DOI 10.1038/nrn3532
   Xin JG, 2001, IEEE IJCNN, P1772, DOI 10.1109/IJCNN.2001.938430
   Xu Y, 2013, NEURAL NETWORKS, V43, P99, DOI 10.1016/j.neunet.2013.02.003
   Yu Q, 2014, NEUROCOMPUTING, V138, P3, DOI 10.1016/j.neucom.2013.06.052
   Yu Q, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0078318
NR 46
TC 34
Z9 34
U1 1
U2 31
PD MAY 10
PY 2017
VL 237
BP 59
EP 70
DI 10.1016/j.neucom.2016.08.087
UT WOS:000397356700006
DA 2023-11-16
ER

PT C
AU Kiselev, M
AF Kiselev, Mikhail
BE Kryzhanovsky, B
   DuninBarkowski, W
   Redko, V
   Tiumentsev, Y
TI Chaotic Spiking Neural Network Connectivity Configuration Leading to
   Memory Mechanism Formation
SO ADVANCES IN NEURAL COMPUTATION, MACHINE LEARNING, AND COGNITIVE RESEARCH
   III
SE Studies in Computational Intelligence
DT Proceedings Paper
CT 21st International Conference on Neuroinformatics
CY OCT 07-11, 2019
CL Dolgoprudny, RUSSIA
DE Spiking neural network; Liquid state machine; Chaotic neural network;
   Synaptic plasticity; Neural network self-organization; Memory mechanism
AB Chaotic spiking neural network serves as a main component (a "liquid") in liquid state machines (LSM) - a very promising approach to application of neural networks to online analysis of dynamic data streams. The LSM ability to recognize complex dynamic patterns is based on "memory" of its liquid component - prolonged reaction of its neural network to input stimuli. A generalization of LSM called self-organizing LSM (LSM including spiking neural network with synaptic plasticity switched on) is studied. It is demonstrated that memory appears in such networks under certain locality conditions on their connectivity. Genetic algorithm is utilized to determine parameters of neuron model, synaptic plasticity rule and connectivity optimal from point of view of memory characteristics.
C1 [Kiselev, Mikhail] Chuvash State Univ, Cheboksary, Russia.
RP Kiselev, M (corresponding author), Chuvash State Univ, Cheboksary, Russia.
EM mkiselev@chuvsu.ru
CR Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324
   Fiebig F, 2017, J NEUROSCI, V37, P83, DOI 10.1523/JNEUROSCI.1989-16.2016
   Kiselev M., 2011, P ICANNGA 2011 PART, P120
   Kiselev M., 2019, P IJCNN2019
   Kiselev M, 2016, IEEE IJCNN, P1355, DOI 10.1109/IJCNN.2016.7727355
   Kiselev M, 2013, LECT NOTES COMPUT SC, V7902, P510, DOI 10.1007/978-3-642-38679-4_51
   Kowatsch T, 2011, ADVANCED TECHNOLOGIES MANAGEMENT FOR RETAILING: FRAMEWORKS AND CASES, P270, DOI 10.4018/978-1-60960-738-8.ch014
   Lansner A, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0073776
   Seeholzer A, 2019, PLOS COMPUT BIOL, V15, DOI 10.1371/journal.pcbi.1006928
   Szatmary B., PLOS COMPUT BIOL
NR 10
TC 0
Z9 1
U1 1
U2 2
PY 2020
VL 856
BP 398
EP 404
DI 10.1007/978-3-030-30425-6_47
UT WOS:000547332800047
DA 2023-11-16
ER

PT C
AU Rostro-Gonzalez, H
   Garreau, G
   Andreou, A
   Georgiou, J
   Barron-Zambrano, JH
   Torres-Huitzil, C
AF Rostro-Gonzalez, Horacio
   Garreau, Guillaume
   Andreou, Andreas
   Georgiou, Julius
   Barron-Zambrano, Jose H.
   Torres-Huitzil, Cesar
GP IEEE
TI An FPGA-based approach for parameter estimation in spiking neural
   networks
SO 2012 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS (ISCAS 2012)
SE IEEE International Symposium on Circuits and Systems
DT Proceedings Paper
CT IEEE International Symposium on Circuits and Systems
CY MAY 20-23, 2012
CL Seoul, SOUTH KOREA
ID DYNAMICS
AB We present an FPGA-based approach for estimating the delayed synaptic weights of spiking neural networks. Our approach makes explicit use of the fact that reverse engineering of a spiking neural network can be cast as a linear programming problem, whereby the objective function is based on the network spiking activity. The solution is obtained by employing the widely used simplex algorithm. Numerical results on a Xilinx Spartan 3 FPGA board show that the present approach can be used to reproduce a desired output from the observed network spiking activity.
C1 [Rostro-Gonzalez, Horacio; Garreau, Guillaume; Andreou, Andreas; Georgiou, Julius] Univ Cyprus, KIOS Res Ctr, CY-1678 Nicosia, Cyprus.
RP Rostro-Gonzalez, H (corresponding author), Univ Cyprus, KIOS Res Ctr, CY-1678 Nicosia, Cyprus.
EM hrosgonz@ucy.ac.cy; julio@ucy.ac.cy; jhbarronz@tamps.cinvestav.mx;
   ctorres@tamps.cinvestav.mx
CR Bayliss S, 2006, 2006 IEEE International Conference on Field Programmable Technology, Proceedings, P49, DOI 10.1109/FPT.2006.270294
   Bixby R. E., 1992, J COMPUTING, V4
   Cassidy A, 2007, 2007 IEEE BIOMEDICAL CIRCUITS AND SYSTEMS CONFERENCE, P75, DOI 10.1109/BIOCAS.2007.4463312
   Cessac B, 2008, J MATH BIOL, V56, P311, DOI 10.1007/s00285-007-0117-3
   Cessac B, 2011, J MATH BIOL, V62, P863, DOI 10.1007/s00285-010-0358-4
   Gerstner W., 2002, SPIKING NEURON MODEL
   Maass W, 1997, NETWORK-COMP NEURAL, V8, P355, DOI 10.1088/0954-898X/8/4/002
   Maass W, 1997, NEURAL COMPUT, V9, P279, DOI 10.1162/neco.1997.9.2.279
   Maass W, 2003, PULSED NEURAL NETWOR
   Rostro-Gonzalez H, 2011, J PHYSIOL-PARIS, V105, P91, DOI 10.1016/j.jphysparis.2011.09.004
   Russell A, 2011, IEEE INT SYMP CIRC S, P669
   Síma J, 2005, NEURAL COMPUT, V17, P2635, DOI 10.1162/089976605774320601
   Soula H, 2006, NEURAL COMPUT, V18, P60, DOI 10.1162/089976606774841567
NR 13
TC 0
Z9 0
U1 0
U2 2
PY 2012
UT WOS:000316903703024
DA 2023-11-16
ER

PT C
AU Popov, AV
   Sayarkin, KS
   Zhilenkov, AA
AF Popov, Alexey V.
   Sayarkin, Konstantin S.
   Zhilenkov, Anton A.
GP IEEE
TI The Scalable Spiking Neural Network Automatic Generation in MATLAB
   Focused on the Hardware Implementation
SO PROCEEDINGS OF THE 2018 IEEE CONFERENCE OF RUSSIAN YOUNG RESEARCHERS IN
   ELECTRICAL AND ELECTRONIC ENGINEERING (EICONRUS)
DT Proceedings Paper
CT IEEE Conference of Russian Young Researchers in Electrical and
   Electronic Engineering (ElConRus)
CY JAN 29-FEB 01, 2018
CL Saint Petersburg Electrotechn Univ LETI, RUSSIA
HO Saint Petersburg Electrotechn Univ LETI
DE spiking neural network; neuron; MATLAB; Simulink; robotics
AB In article development of the program in the environment of MATLAB is considered. The program allows to generate spiking neural networks with the given scale. In article the problem of use of dynamic memory in the generated spiking neural network for a possibility of direct transfer on microprocessor or microcontroller systems is considered. Examples of practical implementation of the program are given. Results of work of the program are presented in the form of the generated spiking neural networks of the given scale. This program will be used for development, realization and programming of intelligent autonomous navigation systems for robotic objects control.
C1 [Popov, Alexey V.; Sayarkin, Konstantin S.; Zhilenkov, Anton A.] Peter Great St Petersburg Polytech Univ, Inst Comp Sci & Technol, High Sch Cyberphys Syst & Control, St Petersburg, Russia.
RP Zhilenkov, AA (corresponding author), Peter Great St Petersburg Polytech Univ, Inst Comp Sci & Technol, High Sch Cyberphys Syst & Control, St Petersburg, Russia.
EM zhilenkovanton@gmail.com
CR Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Booij O, 2005, INFORM PROCESS LETT, V95, P552, DOI 10.1016/j.ipl.2005.05.023
   Buzsáki G, 2004, SCIENCE, V304, P1926, DOI 10.1126/science.1099745
   Cohen-Cory S, 2002, SCIENCE, V298, P770, DOI 10.1126/science.1075510
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Lisitsa D., 2017, 2017 IEEE C RUSS YOU
   Long L., 2010, P AIAA INF AER 2010, DOI 10.2514/6.2010-3540
   Nyrkov A., 2017, ADV SYSTEMS CONTROL, P387
   Nyrkov AP, 2017, J PHYS CONF SER, V803, DOI 10.1088/1742-6596/803/1/012108
   Zhilenkov A., 2017, Solid State Phenomena, V265, P627, DOI 10.4028/www.scientific.net/SSP.265.627
   Zhilenkov Anton, 2016, Vibroengineering Procedia. 22nd International Conference on Vibroengineering, P17
   Zhilenkov  A., 2017, 2017 IEEE C RUSS YOU
   Zhilenkov AA, 2017, IOP C SER EARTH ENV, V87, DOI 10.1088/1755-1315/87/8/082059
NR 13
TC 6
Z9 6
U1 0
U2 0
PY 2018
BP 962
EP 965
UT WOS:000450337100227
DA 2023-11-16
ER

PT C
AU Jin, YC
   Wen, RJ
   Sendhoff, B
AF Jin, Yaochu
   Wen, Ruojing
   Sendhoff, Bernhard
BE MarquesDeSa, J
   Alexandre, LA
   Duch, W
   Mandic, DP
TI Evolutionary multi-objective optimization of spiking neural networks
SO ARTIFICIAL NEURAL NETWORKS - ICANN 2007, PT 1, PROCEEDINGS
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 17th International Conference on Artificial Neural Networks (ICANN 2007)
CY SEP 09-13, 2007
CL Oporto, PORTUGAL
ID NEURONS
AB Evolutionary multi-objective optimization of spiking neural networks for solving classification problems is studied in this paper. By means of a Paretobased multi-objective genetic algorithm, we are able to optimize both classification performance and connectivity of spiking neural networks with the latency coding. During optimization, the connectivity between two neurons, i.e., whether two neurons are connected, and if connected, both weight and delay between the two neurons, are evolved. We minimize the the classification error in percentage or the root mean square error for optimizing performance, and minimize the number of connections or the sum of delays for connectivity to investigate the influence of the objectives on the performance and connectivity of spiking neural networks. Simulation results on two benchmarks show that Pareto-based evolutionary optimization of spiking neural networks is able to offer a deeper insight into the properties of the spiking neural networks and the problem at hand.
C1 [Jin, Yaochu; Sendhoff, Bernhard] Honda Res Inst Europe, Carl Legien Str 30, D-63073 Offenbach, Germany.
   [Wen, Ruojing] Univ Karlsruhe, Dept Comp Sci, D-76131 Karlsruhe, Germany.
RP Jin, YC (corresponding author), Honda Res Inst Europe, Carl Legien Str 30, D-63073 Offenbach, Germany.
CR [Anonymous], 2006, MULTI OBJECTIVE MACH
   [Anonymous], 2000, PARALLEL PROBLEM SOL, DOI DOI 10.1007/3-540-45356-3_
   [Anonymous], PROBENL SET NEURAL N
   [Anonymous], INT C NEUR NETW SAN
   Belatreche A, 2007, SOFT COMPUT, V11, P239, DOI [10.1007/s00500-006-0065-7, 10.1007/S00500-006-0065-7]
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   BOHTE SM, 2005, NAT COMPUT, V3, P195
   Booij O, 2005, INFORM PROCESS LETT, V95, P552, DOI 10.1016/j.ipl.2005.05.023
   DEB K, 2001, WIL INT S SYS OPT, P1
   GERSTNER W, 1995, PHYS REV E, V51, P738, DOI 10.1103/PhysRevE.51.738
   Goldberg D. E., 1989, GENETIC ALGORITHMS S
   Hagras H, 2004, IEEE INT CONF ROBOT, P4620, DOI 10.1109/ROBOT.2004.1302446
   JIN Y, 2007, IEEE T SYSTEMS MAN C
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Pavlidis NG, 2005, IEEE IJCNN, P2190
   Reed R., 1999, NEURAL SMITHING SUPE, DOI DOI 10.7551/MITPRESS/4937.001.000
   Schrauwen B, 2004, IEEE IJCNN, P471, DOI 10.1109/IJCNN.2004.1379954
   SCHRAUWEN B, 2006, IJCNN, P3463
   SCHWEFEL H, 1994, EVOLUTION OPTIMUM SE
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   Tsodyks M, 1998, NEURAL COMPUT, V10, P821, DOI 10.1162/089976698300017502
   Yao X, 1999, P IEEE, V87, P1423, DOI 10.1109/5.784219
NR 22
TC 16
Z9 20
U1 0
U2 4
PY 2007
VL 4668
BP 370
EP +
PN I
UT WOS:000250338200038
DA 2023-11-16
ER

PT C
AU Panuku, LN
   Sekhar, CC
AF Panuku, Lakshmi Narayana
   Sekhar, C. Chandra
BE MarquesDeSa, J
   Alexandre, LA
   Duch, W
   Mandic, DP
TI Clustering of nonlinearly separable data using spiking neural networks
SO ARTIFICIAL NEURAL NETWORKS - ICANN 2007, PT 1, PROCEEDINGS
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 17th International Conference on Artificial Neural Networks (ICANN 2007)
CY SEP 09-13, 2007
CL Oporto, PORTUGAL
ID NEURONS
AB In this paper, we study the clustering capabilities of spiking neural networks. We first study the working of spiking neural networks for clustering linearly separable data. Also, a biological interpretation has been given to the delay selection in spiking neural networks. We show that by varying the firing threshold of spiking neurons during the training, nonlinearly separable data like the ring data can be clustered. When a multi-layer spiking neural network is trained for clustering, subclusters are formed in the hidden layer and these subclusters are combined in the output layer, resulting in hierarchical clustering of the data. A spiking neural network with a hidden layer is generally trained by modifying the weights of the connections to the nodes in the hidden layer and the output layer simultaneously. We propose a two-stage learning method for training a spiking neural network model for clustering. In the proposed method, the weights for the connections to the nodes in the hidden layer are learnt first, and then the weights for the connections to the nodes in the output layer are learnt. We show that the proposed two-stage learning method can cluster complex data such as the interlocking cluster data, without using lateral connections.
C1 [Panuku, Lakshmi Narayana; Sekhar, C. Chandra] Indian Inst Technol, Dept Comp Sci & Engn, Madras 600036, Tamil Nadu, India.
RP Panuku, LN (corresponding author), Indian Inst Technol, Dept Comp Sci & Engn, Madras 600036, Tamil Nadu, India.
EM panuku@cs.iitm.ernet.in; chandra@cs.iitm.ernet.in
CR [Anonymous], 1988, ALGORITHMS CLUSTERIN
   [Anonymous], 2001, DEGRUYTER SERIES NON
   Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   Gerstner W, 1996, NATURE, V383, P76, DOI 10.1038/383076a0
   Gerstner W., 2002, SPIKING NEURON MODEL
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   Maass W, 1997, NEURAL COMPUT, V9, P279, DOI 10.1162/neco.1997.9.2.279
   Maass W., 1999, PULSED NEURAL NETWOR
   Natschlager T, 1998, NETWORK-COMP NEURAL, V9, P319, DOI 10.1088/0954-898X/9/3/003
   YANG X, 2004, INT C INT SYST, P491
NR 10
TC 1
Z9 1
U1 0
U2 0
PY 2007
VL 4668
BP 390
EP +
PN I
UT WOS:000250338200040
DA 2023-11-16
ER

PT J
AU Hirose, T
   Asai, T
   Amemiya, Y
AF Hirose, T.
   Asai, T.
   Amemiya, Y.
TI Pulsed neural networks consisting of single-flux-quantum spiking neurons
SO PHYSICA C-SUPERCONDUCTIVITY AND ITS APPLICATIONS
DT Article; Proceedings Paper
CT 19th International Symposium on Superconductivity
CY OCT 30-NOV 01, 2006
CL Nagoya, JAPAN
DE single-flux-quantum; spiking neuron device; integrate-and-fire neuron;
   pulsed neural network
ID SYNCHRONIZATION
AB An inhibitory pulsed neural network was developed for brain-like information processing, by using single-flux-quantum (SFQ) circuits. It consists of spiking neuron devices that are coupled to each other through all-to-all inhibitory connections. The network selects neural activity. The operation of the neural network was confirmed by computer simulation. SFQ neuron devices can imitate the operation of the inhibition phenomenon of neural networks. (C) 2007 Elsevier B.V. All rights reserved.
C1 Hokkaido Univ, Dept Elect Engn, Sapporo, Hokkaido 0600814, Japan.
RP Hirose, T (corresponding author), Hokkaido Univ, Dept Elect Engn, Kita 14,Nishi 9, Sapporo, Hokkaido 0600814, Japan.
EM hirose@sapiens-ei.eng.hokudai.ac.jp
CR Fries P, 1997, P NATL ACAD SCI USA, V94, P12699, DOI 10.1073/pnas.94.23.12699
   Fukai T, 2001, BIOL CYBERN, V85, P107, DOI 10.1007/PL00007998
   Fukai T, 1996, BIOL CYBERN, V75, P453, DOI 10.1007/s004220050310
   Hirose T, 2006, PHYSICA C, V445, P1020, DOI 10.1016/j.physc.2006.05.093
   MAAS WG, 2001, PULSED NEURAL NETWOR, P133
NR 5
TC 15
Z9 17
U1 0
U2 11
PD OCT 1
PY 2007
VL 463
BP 1072
EP 1075
DI 10.1016/j.physc.2007.02.043
UT WOS:000250396000245
DA 2023-11-16
ER

PT J
AU Jia, ZY
   Ji, JY
   Zhou, XL
   Zhou, YH
AF Jia, Ziyu
   Ji, Junyu
   Zhou, Xinliang
   Zhou, Yuhan
TI Hybrid spiking neural network for sleep electroencephalogram signals
SO SCIENCE CHINA-INFORMATION SCIENCES
DT Article
DE spiking neural network; electroencephalogram signals; sleep staging
AB Sleep staging is important for assessing sleep quality. So far, many scholars have tried to achieve automatic sleep staging by using neural networks. However, most researchers only perform sleep staging based on artificial neural networks and their variant models, which can not fully mine and model the bio-electrical signals. In this paper, we propose a new hybrid spiking neural network (HSNN) model for automatic sleep staging. Specifically, we use a spiking neural network to classify sleep EEG signals. In addition, we adopt a hybrid macro/micro back propagation algorithm, aiming to overcome the limitations of existing error back propagation methods for spiking neural network. In order to verify the effectiveness of HSNN, we evaluate it on the public sleep dataset ISRUC-SLEEP (Institute of Systems and Robotics, University of Coimbra-Sleep). The results show that the proposed method achieves satisfactory performance on ISRUC-SLEEP.
C1 [Jia, Ziyu; Ji, Junyu; Zhou, Xinliang; Zhou, Yuhan] Beijing Jiaotong Univ, Sch Comp & Informat Technol, Beijing 100044, Peoples R China.
   [Jia, Ziyu] Natl Univ Singapore, Sch Comp, Singapore 117417, Singapore.
RP Jia, ZY; Zhou, XL (corresponding author), Beijing Jiaotong Univ, Sch Comp & Informat Technol, Beijing 100044, Peoples R China.; Jia, ZY (corresponding author), Natl Univ Singapore, Sch Comp, Singapore 117417, Singapore.
EM ziyujia@bjtu.edu.cn; xlzhou@bjtu.edu.cn
CR Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Cai XY, 2020, IEEE INT C BIOINFORM, P976, DOI [10.1109/bibm49941.2020.9313459, 10.1109/BIBM49941.2020.9313459]
   Chambon S, 2018, IEEE T NEUR SYS REH, V26, P758, DOI 10.1109/TNSRE.2018.2813138
   Chriskos P, 2020, IEEE T NEUR NET LEAR, V31, P113, DOI 10.1109/TNNLS.2019.2899781
   Huh D, 2018, ADV NEUR IN, V31
   Phan H, 2019, IEEE T NEUR SYS REH, V27, P400, DOI 10.1109/TNSRE.2019.2896659
   Jeon Y, 2019, IEEE ACCESS, V7, P96495, DOI 10.1109/ACCESS.2019.2928129
   Jia Z., 2021, ARXIV210513864
   Jia ZY, 2021, IEEE T NEUR SYS REH, V29, P1977, DOI 10.1109/TNSRE.2021.3110665
   Jia ZY, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1324
   [贾子钰 Jia Ziyu], 2020, [浙江大学学报. 工学版, Journal of Zhejiang University. Engineering Science], V54, P1899
   Jin WY, 2019, SCI CHINA TECHNOL SC, V62, P2113, DOI 10.1007/s11431-018-9423-x
   Jin Y., 2018, ADV NEURAL INFORM PR, P7005, DOI DOI 10.48550/ARXIV.1805.07866
   Khalighi S, 2016, COMPUT METH PROG BIO, V124, P180, DOI 10.1016/j.cmpb.2015.10.013
   Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508
   Liu YH, 2001, J COMPUT NEUROSCI, V10, P25, DOI 10.1023/A:1008916026143
   Maind S. B., 2014, INT J RECENT INNOVAT, V2, P96, DOI [10.17762/ijritcc.v2i1.2920, DOI 10.17762/IJRITCC.V2I1.2920]
   McCarley RW, 2007, SLEEP MED, V8, P302, DOI 10.1016/j.sleep.2007.03.005
   Michielli N, 2019, COMPUT BIOL MED, V106, P71, DOI 10.1016/j.compbiomed.2019.01.013
   Phan H, 2018, IEEE ENG MED BIO, P1452, DOI 10.1109/EMBC.2018.8512480
   Schrauwen B, 2003, IEEE IJCNN, P2825
   SM I.N., 2019, 2019 IEEE 10 INT C A, P1
   [宿云 Su Yun], 2015, [科学通报, Chinese Science Bulletin], V60, P1002
   Supratak A, 2017, IEEE T NEUR SYS REH, V25, P1998, DOI 10.1109/TNSRE.2017.2721116
   Zhang ZM, 2017, IEEE T GEOSCI REMOTE, V55, P7177, DOI 10.1109/TGRS.2017.2743222
   [赵文瑞 Zhao Wenrui], 2020, [中国科学. 生命科学, Scientia Sinica Vitae], V50, P270
   Ziyu Jia, 2020, IEEE Transactions on Artificial Intelligence, V1, P248, DOI 10.1109/TAI.2021.3060350
NR 27
TC 13
Z9 14
U1 9
U2 28
PD APR
PY 2022
VL 65
IS 4
AR 140403
DI 10.1007/s11432-021-3380-1
UT WOS:000771626400002
DA 2023-11-16
ER

PT C
AU Frick, A
   Iannella, N
AF Frick, Adam
   Iannella, Nicolangelo
BE Lintas, A
   Rovetta, S
   Verschure, PFMJ
   Villa, AEP
TI Stochasticity, Spike Timing, and a Layered Architecture for Finding
   Iterative Roots
SO ARTIFICIAL NEURAL NETWORKS AND MACHINE LEARNING - ICANN 2017, PT I
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 26th International Conference on Artificial Neural Networks (ICANN)
CY SEP 11-14, 2017
CL Alghero, ITALY
DE Spiking neural network; Functional equations; Iterative root; Artificial
   intelligence
ID FUNCTION APPROXIMATION; NEURAL-NETWORK; NEURONS
C1 [Frick, Adam] Univ Adelaide, Sch Elect & Elect Engn, Adelaide, SA, Australia.
   [Iannella, Nicolangelo] Univ Nottingham, Sch Math Sci, Nottingham, England.
RP Iannella, N (corresponding author), Univ Nottingham, Sch Math Sci, Nottingham, England.
EM adam.frick@adelaide.edu.au; nicolangelo.iannella@nottingham.ac.uk
CR Iannella N, 2005, INFORM PROCESS LETT, V95, P545, DOI 10.1016/j.ipl.2005.05.022
   Iannella N, 2001, NEURAL NETWORKS, V14, P933, DOI 10.1016/S0893-6080(01)00080-6
   Kindermann L, 2002, ICONIP'02: PROCEEDINGS OF THE 9TH INTERNATIONAL CONFERENCE ON NEURAL INFORMATION PROCESSING, P2655
   Kindermann L, 1998, ICONIP'98: THE FIFTH INTERNATIONAL CONFERENCE ON NEURAL INFORMATION PROCESSING JOINTLY WITH JNNS'98: THE 1998 ANNUAL CONFERENCE OF THE JAPANESE NEURAL NETWORK SOCIETY - PROCEEDINGS, VOLS 1-3, P713
   Maass W, 1997, NEURAL COMPUT, V9, P279, DOI 10.1162/neco.1997.9.2.279
   Sanger TD, 1998, NEURAL COMPUT, V10, P1567, DOI 10.1162/089976698300017313
NR 6
TC 0
Z9 0
U1 0
U2 0
PY 2017
VL 10613
BP 429
EP 430
DI 10.1007/978-3-319-68600-4_57
PN I
UT WOS:000449802500057
DA 2023-11-16
ER

PT C
AU Reljan-Delaney, M
   Wall, J
AF Reljan-Delaney, Mirela
   Wall, Julie
GP IEEE
TI Solving the Linearly Inseparable XOR Problem with Spiking Neural
   Networks
SO 2017 COMPUTING CONFERENCE
DT Proceedings Paper
CT Computing Conference
CY JUL 18-20, 2017
CL United Kingdon, London, ENGLAND
DE Spiking Neural Networks; Receptive Fields; XOR; Boolean Logic; Leaky
   Fire and Integrate Model
ID FIRE NEURONS; SYNAPSES
AB Spiking Neural Networks (SNN) are third generation neural networks and are considered to be the most biologically plausible so far. As a relative newcomer to the field of artificial learning, SNNs are still exploring their own capabilities, as well as dealing with the singular challenges that arise from attempting to be computationally applicable and biologically accurate. This paper explores the possibility of a different approach to solving linearly inseparable problems by using networks of spiking neurons. To this end two experiments were conducted. The first experiment was an attempt in creating a spiking neural network that would mimic the functionality of logic gates. The second experiment relied on the addition of receptive fields in order to filter the input. This paper demonstrates that a network of spiking neurons utilizing receptive fields or routing can successfully solve the XOR linearly inseparable problem.
C1 [Reljan-Delaney, Mirela; Wall, Julie] Univ East London, Sch Architecture Comp & Engn, London, England.
RP Reljan-Delaney, M (corresponding author), Univ East London, Sch Architecture Comp & Engn, London, England.
EM u1307319@uel.ac.uk; J.Wall@uel.ac.uk
CR Alonso J.-M., 2009, SCHOLARPEDIA, V4, P5393, DOI [10.4249/scholarpedia.5393, DOI 10.4249/SCHOLARPEDIA.5393]
   [Anonymous], 1957, PERCEPTRON PERCEIVIN
   Bohte Sander M., 2004, Natural Computing, V3, P195, DOI 10.1023/B:NACO.0000027755.02868.60
   Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   Connors BW, 2004, ANNU REV NEUROSCI, V27, P393, DOI 10.1146/annurev.neuro.26.041002.131128
   Glackin C, 2011, NEURAL NETWORKS, V24, P247, DOI 10.1016/j.neunet.2010.11.008
   Gruning A, 2014, ESANN
   Hartline HK, 1938, AM J PHYSIOL, V121, P400, DOI 10.1152/ajplegacy.1938.121.2.400
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V116, P449, DOI 10.1113/jphysiol.1952.sp004717
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V116, P473, DOI 10.1113/jphysiol.1952.sp004718
   Konig P, 1996, TRENDS NEUROSCI, V19, P130, DOI 10.1016/S0166-2236(96)80019-1
   Legenstein R, 2005, NEURAL COMPUT, V17, P2337, DOI 10.1162/0899766054796888
   Miller C. D. D., 2011, MATH IDEAS
   MINSKY M, 1969, PERCEPTIONS
   Paugam-Moisy H., 2012, HDB NATURAL COMPUTIN, V1, P1, DOI [DOI 10.1007/978-3-540-92910-9_10, 10.1007/978-3-540-92910-9_10]
   Ponulak F., 2006, PROC EPFL LATSIS S D, P119
   Smith LS, 2004, IEEE T NEURAL NETWOR, V15, P1125, DOI 10.1109/TNN.2004.832831
   STEIN RB, 1967, PROC R SOC SER B-BIO, V167, P64, DOI 10.1098/rspb.1967.0013
   Stromatias E., 2011, ARXIV11092788 ARXIV
   Tam N. D., 2013, INT J COMPUTER INFOR, V2
   Vogels TP, 2005, J NEUROSCI, V25, P10786, DOI 10.1523/JNEUROSCI.3508-05.2005
   Zohar O, 2011, J NEUROSCI, V31, P9192, DOI 10.1523/JNEUROSCI.6193-10.2011
NR 22
TC 9
Z9 9
U1 0
U2 3
PY 2017
BP 701
EP 705
UT WOS:000426944400089
DA 2023-11-16
ER

PT J
AU Hazan, H
   Saunders, DJ
   Khan, H
   Patel, D
   Sanghavi, DT
   Siegelmann, HT
   Kozma, R
AF Hazan, Hananel
   Saunders, Daniel J.
   Khan, Hassaan
   Patel, Devdhar
   Sanghavi, Darpan T.
   Siegelmann, Hava T.
   Kozma, Robert
TI BindsNET: A Machine Learning-Oriented Spiking Neural Networks Library in
   Python
SO FRONTIERS IN NEUROINFORMATICS
DT Article
DE GPU-computing; spiking Network; PyTorch; machine learning; python
   (programming language); reinforcement learning (RL)
ID NEURONS; TOOL
AB The development of spiking neural network simulation software is a critical component enabling the modeling of neural systems and the development of biologically inspired algorithms. Existing software frameworks support a wide range of neural functionality, software abstraction levels, and hardware devices, yet are typically not suitable for rapid prototyping or application to problems in the domain of machine learning. In this paper, we describe a new Python package for the simulation of spiking neural networks, specifically geared toward machine learning and reinforcement learning. Our software, called BindsNET(1), enables rapid building and simulation of spiking networks and features user-friendly, concise syntax. BindsNET is built on the PyTorch deep neural networks library, facilitating the implementation of spiking neural networks on fast CPU and GPU computational platforms. Moreover, the BindsNET framework can be adjusted to utilize other existing computing and hardware backends; e.g., TensorFlow and SpiNNaker. We provide an interface with the OpenAl gym library, allowing for training and evaluation of spiking networks on reinforcement learning environments. We argue that this package facilitates the use of spiking networks for large-scale machine learning problems and show some simple examples by using BindsNET in practice.
C1 [Hazan, Hananel; Saunders, Daniel J.; Khan, Hassaan; Patel, Devdhar; Sanghavi, Darpan T.; Siegelmann, Hava T.; Kozma, Robert] Univ Massachusetts, Coll Comp & Informat Sci, Biol Inspired Neural & Dynam Syst Lab, Amherst, MA 01003 USA.
RP Hazan, H; Saunders, DJ (corresponding author), Univ Massachusetts, Coll Comp & Informat Sci, Biol Inspired Neural & Dynam Syst Lab, Amherst, MA 01003 USA.
EM hananel@hazan.org.il; djsaunde@cs.umass.edu
CR Abadi M., 2016, TENSORFLOW LARGE SCA
   Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   Al-Rfou R., 2016, ARXIVEPRINTSABS16050
   [Anonymous], 2015, P WORKSHOP MACHINE L
   Bekolay T, 2014, FRONT NEUROINFORM, V7, DOI 10.3389/fninf.2013.00048
   Bengio Y., 2015, CORRABS150204156
   Beyeler M., 2015, 2015 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2015.7280424
   Brockman Greg, 2016, OPENAI GYM
   Bruna J., 2013, SPECTRAL NETWORKS LO
   Carnevale NT., 2006, NEURON BOOK, DOI DOI 10.1017/CBO9780511541612
   Chen T., 2015, CORRABS151201274
   Chetlur S., 2014, CORRABS14100759
   Cornelis H, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0029018
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Davison Andrew P, 2008, Front Neuroinform, V2, P11, DOI 10.3389/neuro.11.011.2008
   Diehl P.U., 2015, 2015 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2015.7280696
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Ferré P, 2018, FRONT COMPUT NEUROSC, V12, DOI 10.3389/fncom.2018.00024
   Fidjeland AK, 2009, IEEE INT CONF ASAP, P137, DOI 10.1109/ASAP.2009.24
   Florian RV, 2007, NEURAL COMPUT, V19, P1468, DOI 10.1162/neco.2007.19.6.1468
   Goodman DFM, 2009, FRONT NEUROSCI-SWITZ, V3, P192, DOI 10.3389/neuro.01.026.2009
   Hazan H, 2018, IEEE IJCNN, P493
   HEBB D. O., 1949
   Hines Michael L, 2009, Front Neuroinform, V3, P1, DOI 10.3389/neuro.11.001.2009
   Huh D., 2017, GRADIENT DESCENT SPI
   Hunsberger E., 2015, CORRABS151008829
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Jia Y., 2014, PROC 22 ACM INT C MU, DOI [DOI 10.1145/2647868.2654889, 10.1145/2647868.2654889]
   Kasabov NK, 2014, NEURAL NETWORKS, V52, P62, DOI 10.1016/j.neunet.2014.01.006
   Kheradpisheh S. R., 2016, CORRABS161101421
   Kistler W. M., 2002, SPIKING NEURON MODEL
   Klikauer T, 2016, TRIPLEC-COMMUN CAPIT, V14, P260
   Krizhevsky Alex, 2009, LEARNING MULTIPLE LA
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508
   Maass W, 1996, NEURAL COMPUT, V8, P1, DOI 10.1162/neco.1996.8.1.1
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Marblestone AH, 2016, FRONT COMPUT NEUROSC, V10, DOI 10.3389/fncom.2016.00094
   Marc-Oliver G., 2007, SCHOLARPEDIA, DOI [DOI 10.4249/SCHOLARPEDIA.1430, 10.4249/scholarpedia.1430]
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   Mostafa H, 2018, IEEE T NEUR NET LEAR, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Mozafari M, 2018, IEEE T NEUR NET LEAR, V29, P6178, DOI 10.1109/TNNLS.2018.2826721
   Paszke Adam, 2017, ADV NEURAL INFORM PR
   Plana LA, 2011, ACM J EMERG TECH COM, V7, DOI 10.1145/2043643.2043647
   Rueckauer B, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351295
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Saunders D. J., 2018, IEEE INNS INT JOINT, P4906
   Stewart T. C., 2012, TECHNICAL REPORT
   Stimberg M., 2018, BIORXIV
   Stimberg M, 2014, FRONT NEUROINFORM, V8, DOI 10.3389/fninf.2014.00006
   Stork D. G., 1989, IJCNN: International Joint Conference on Neural Networks (Cat. No.89CH2765-6), P241, DOI 10.1109/IJCNN.1989.118705
   Thorpe S, 1998, COMPUTATIONAL NEUROSCIENCE: TRENDS IN RESEARCH, P113
   Tikidji-Hamburyan RA, 2017, FRONT NEUROINFORM, V11, DOI 10.3389/fninf.2017.00046
   Vitay J, 2015, FRONT NEUROINFORM, V9, DOI 10.3389/fninf.2015.00019
   Wall J, 2013, FRONT COMPUT NEUROSC, V7, DOI 10.3389/fncom.2013.00182
   Wang JX, 2018, NAT NEUROSCI, V21, P860, DOI 10.1038/s41593-018-0147-8
   Welling M, 2016, CORRABS160208323
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Xiao H., 2017, CORRABS170807747
   Yavuz E, 2016, SCI REP-UK, V6, DOI 10.1038/srep18854
NR 60
TC 120
Z9 123
U1 2
U2 42
PD DEC 12
PY 2018
VL 12
AR 89
DI 10.3389/fninf.2018.00089
UT WOS:000453235300001
DA 2023-11-16
ER

PT C
AU Azimirad, V
   Sani, MF
   Ramezanlou, MT
AF Azimirad, Vahid
   Sani, Mohammad Fattahi
   Ramezanlou, Mohammad Tayefe
GP IEEE
TI Unsupervised Learning of Target Attraction for Robots through Spike
   Timing Dependent Plasticity
SO 2017 IEEE 4TH INTERNATIONAL CONFERENCE ON KNOWLEDGE-BASED ENGINEERING
   AND INNOVATION (KBEI)
DT Proceedings Paper
CT IEEE 4th International Conference on Knowledge-Based Engineering and
   Innovation (KBEI)
CY DEC 22, 2017
CL Iran Univ Sci & Technol, Tehran, IRAN
HO Iran Univ Sci & Technol
DE Unsupervised Learning; Spike Timing Dependent Plasticity; STDP; Mobile
   Robot; Spiking Neural Networks; SNN
ID NEURAL-NETWORKS; MODEL
AB In this paper, unsupervised learning of robots through Spiking Neural Network (SNN) is studied. Izhikevich model is used for building spiking neural network and kinematic model of the robots is considered for body modeling. Learning mechanism is based on Spike Timing Dependent Plasticity (STDP) in which variation of synapses between neurons depends on spike timing. The reciprocal architecture of network as well as timing regulation of algorithm result in tuning of spiking neural network which makes robot to be attracted to the target. It is supposed that the robot has two sensors for target detection. Signals from sensors are conducted into neural network, so that the output of network (spikes of motor neurons) derive the actuators. Neurons that fire together make special pathways from sensory neurons to motor neurons, and other synapses which are not involved, are weakened slowly. The algorithm of proposed method is illustrated and we designed two sets of simulation studies in order to test the effectiveness of the proposed method: the learning of target attraction for 1) mobile robot, 2) one-DOF robotic arm.
C1 [Azimirad, Vahid; Sani, Mohammad Fattahi; Ramezanlou, Mohammad Tayefe] Univ Tabriz, Fac Engn Emerging Technol, Tabriz, Iran.
RP Azimirad, V (corresponding author), Univ Tabriz, Fac Engn Emerging Technol, Tabriz, Iran.
EM azimirad@tabrizu.ac.ir; m.fattahi93@ms.tabrizu.ac.ir;
   mohammad_tayeferamezanloo95@ms.tabrizu.ac.ir
CR [Anonymous], 2001, THEORETICAL NEUROSCI
   [Anonymous], ARXIV150206096
   Arena P., 2010, PROC INT JOINT C NEU, P1, DOI [10.1109/IJCNN.2010.5596513ieeexplore.ieee.org, DOI 10.1109/IJCNN.2010.5596542, 10.1109/IJCNN.2010.5596542]
   Batllori R, 2011, PROCEDIA COMPUT SCI, V6, DOI 10.1016/j.procs.2011.08.060
   Bouganis A., 2010, NEUR NETW IJCNN 2010, P1, DOI DOI 10.1109/IJCNN.2010.5596525
   Carrillo RR, 2008, BIOSYSTEMS, V94, P18, DOI 10.1016/j.biosystems.2008.05.008
   Chadderdon GL, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0047251
   Cheng L., 2017, INT J BIOSEN BIOELEC, V2
   Clawson TS, 2016, IEEE DECIS CONTR P, P3381, DOI 10.1109/CDC.2016.7798778
   Dura-Bernal S, 2015, FRONT NEUROROBOTICS, V9, DOI 10.3389/fnbot.2015.00013
   Dura-Bernal S, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00028
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Hwu T., 2017, IEEE T COGNITIVE DEV
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2007, CEREB CORTEX, V17, P2443, DOI 10.1093/cercor/bhl152
   Korayem MH, 2012, ROBOTICA, V30, P53, DOI 10.1017/S0263574711000336
   Luque NR, 2011, INT J NEURAL SYST, V21, P385, DOI 10.1142/S0129065711002900
   Mantovani R. G., 2013, USE SPIKING NEURAL N
   Mazumder P, 2016, INTEGRATION, V54, P109, DOI 10.1016/j.vlsi.2016.01.002
   Neymotin SA, 2013, NEURAL COMPUT, V25, P3263, DOI 10.1162/NECO_a_00521
   Nichols E, 2013, IEEE T CYBERNETICS, V43, P115, DOI 10.1109/TSMCB.2012.2200674
   Sarim M., 2016, ASME 2016 DYN SYST C
   Shim MS, 2017, IEEE IJCNN, P3098, DOI 10.1109/IJCNN.2017.7966242
   Spuler M., 2015, P INT JOINT C NEURAL, V2015, DOI [10.1109/IJCNN.2015.7280521, DOI 10.1109/IJCNN.2015.7280521]
   Trhan P, 2010, COMPUT INFORM, V29, P823
   Wang XQ, 2008, NEUROCOMPUTING, V71, P655, DOI 10.1016/j.neucom.2007.08.025
   Wang XQ, 2014, NEUROCOMPUTING, V134, P230, DOI 10.1016/j.neucom.2013.07.055
   Wang XQ, 2009, 2009 INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND COMPUTATIONAL INTELLIGENCE, VOL I, PROCEEDINGS, P194, DOI 10.1109/AICI.2009.448
   Wang XQ, 2008, ICNC 2008: FOURTH INTERNATIONAL CONFERENCE ON NATURAL COMPUTATION, VOL 4, PROCEEDINGS, P125, DOI 10.1109/ICNC.2008.718
   Zennir M.N., 2015, AIAI WORKSHOPS, P2
   Zhang X, 2013, IEEE DECIS CONTR P, P6798, DOI 10.1109/CDC.2013.6760966
   Zhang X., 2017, INDIRECT TRAINING AL
NR 32
TC 3
Z9 3
U1 0
U2 0
PY 2017
BP 428
EP 433
UT WOS:000435249700077
DA 2023-11-16
ER

PT J
AU Maass, W
AF Maass, W
TI Networks of spiking neurons: The third generation of neural network
   models
SO NEURAL NETWORKS
DT Article
DE spiking neuron; integrate-and-fire neutron; computational complexity;
   sigmoidal neural nets; lower bounds
ID PATTERN-RECOGNITION; COMPUTATIONAL POWER; ACTION-POTENTIALS; CORTEX;
   REPRESENTATION; RELIABILITY; INFORMATION; DIMENSION; CIRCUITS; CODE
AB The computational power of formal models for networks of spiking neurons is compared with that of other neural network models based on McCulloch Pitts neurons (i.e., threshold gates), respectively, sigmoidal gates. In particular it is shown that networks of spiking neurons are, with regard to the number of neurons that are needed, computationally more powerful than these other neural network models. A concrete biologically relevant function is exhibited which can be computed by a single spiking neuron (for biologically reasonable values of ifs parameters), but which requires hundreds of hidden units on a sigmoidal neural net. On the other hand, it is known that any function that can be computed by a small sigmoidal neural net can also be computed by a small network of spiking neurons. This article does not assume prior knowledge about spiking neurons, and it contains an extensive list of references to the currently available literature on computations in networks of spiking neurons and relevant results from neurobiology. (C) 1997 Elsevier Science Ltd. All rights reserved.
C1 Graz Univ Technol, Inst Theoret Comp Sci, A-8010 Graz, Austria.
RP Maass, W (corresponding author), Graz Univ Technol, Inst Theoret Comp Sci, Klosterwiesgasse 32-2, A-8010 Graz, Austria.
EM maass@igi.tu-graz.ac.at
CR ABELES M, 1993, J NEUROPHYSIOL, V70, P1629, DOI 10.1152/jn.1993.70.4.1629
   Abeles M., 1991, CORTICONICS NEURAL C
   AERTSEN A, 1993, BRAIN THEORY SPATIOT
   Aityan S. K., 1993, Neural, Parallel & Scientific Computations, V1, P3
   [Anonymous], 1995, FDN CELLULAR NEUROPH
   [Anonymous], HDB BRAIN THEORY NEU
   Arbib, 1995, HDB BRAIN THEORY NEU
   BAIR W, 1994, P S DYN NEUR PROC WA
   Beeman D., 1995, BOOK GENESIS EXPLORI
   BIALEK W, 1992, TRENDS NEUROSCI, V15, P428, DOI 10.1016/0166-2236(92)90005-S
   BIENENSTOCK E, 1995, NETWORK-COMP NEURAL, V6, P179, DOI 10.1088/0954-898X/6/2/004
   Churchland P. S., 1993, The Computational Brain
   CRAIR MC, 1990, ADV NEURAL INFORMATI, V2, P109
   DASGUPTA B, 1993, ADV NEURAL INFORMATI, V5, P615
   DEYONG MR, 1992, IEEE T NEURAL NETWOR, V3, P363, DOI 10.1109/72.129409
   DOUGLAS RJ, 1995, SCIENCE, V269, P981, DOI 10.1126/science.7638624
   FERSTER D, 1995, SCIENCE, V270, P756, DOI 10.1126/science.270.5237.756
   GERSTNER W, 1993, BIOL CYBERN, V68, P363, DOI 10.1007/BF00201861
   GERSTNER W, 1995, PHYS REV E, V51, P738, DOI 10.1103/PhysRevE.51.738
   Gerstner W, 1991, ADV NEURAL INFORMATI, P84
   GERSTNER W, 1994, ADV NEURAL INFORMATI, V6, P463
   GOLDBERG PW, 1995, MACH LEARN, V18, P131, DOI 10.1007/BF00993408
   HERRMANN M, IN PRESS NORDITA PRE
   HOPFIELD JJ, 1995, P NATL ACAD SCI USA, V92, P6655, DOI 10.1073/pnas.92.15.6655
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   Horiuchi T., 1991, ADV NEURAL INFORMATI, V3, P406
   Jahnke A., 1996, Proceedings of the Fifth International Conference on Microelectronics for Neural Networks and Fuzzy Systems. MicroNeuro'96, P232, DOI 10.1109/MNNFS.1996.493796
   JIU CT, 1996, P 7 AUSTR C NEUR NET, P212
   JUDD KT, 1993, NEURAL NETWORKS, V6, P203, DOI 10.1016/0893-6080(93)90017-Q
   KARPINSKI M, IN PRESS J COMPUTER
   Kempter R, 1996, ADV NEUR IN, V8, P124
   Koch C., 1992, SINGLE NEURON COMPUT, P315, DOI DOI 10.1016/B978-0-12-484815-3.50019-0
   Koiran P, 1996, IEEE CONF COMP COMPL, P81, DOI 10.1109/CCC.1996.507671
   KRUGER J, 1988, J NEUROPHYSIOL, V60, P798, DOI 10.1152/jn.1988.60.2.798
   Lapique L., 1907, J PHYSL PATHOL GEN, V9, P620, DOI DOI 10.1007/S00422-007-0189-6
   LESHNO M, 1993, NEURAL NETWORKS, V6, P861, DOI 10.1016/S0893-6080(05)80131-5
   Lestienne R, 1996, BIOL CYBERN, V74, P55, DOI 10.1007/BF00199137
   MAAAS W, 1997, P 10 C COMP LEARN TH
   Maass W, 1996, NEURAL COMPUT, V8, P1, DOI 10.1162/neco.1996.8.1.1
   Maass W, 1997, NEURAL COMPUT, V9, P279, DOI 10.1162/neco.1997.9.2.279
   MAASS W, 1991, PROCEEDINGS - 32ND ANNUAL SYMPOSIUM ON FOUNDATIONS OF COMPUTER SCIENCE, P767, DOI 10.1109/SFCS.1991.185447
   Maass W, 1996, ADV NEUR IN, V8, P211
   MAASS W, 1995, HDB BRAIN THEORY NEU, P1000
   MAASS W, 1997, ADV NEURAL INFORMATI, V9
   MAASS W, 1995, P INT C ART NEUR NET, P515
   MAASS W, 1995, P 7 IT WORKSH NEUR N, P99
   MAASS W, 1995, ADV NEURAL INFORMATI, V7, P183
   Mahowald M., 1994, ANALOG VLSI SYSTEM S
   Mahowald M., 1992, THESIS CALTECH
   MAINEN ZF, 1995, SCIENCE, V268, P1503, DOI 10.1126/science.7770778
   MEAD C, 1989, ANOLOG VLSI NEURAL S
   MEADOR JL, 1991, IEEE T NEURAL NETWOR, V2, P101, DOI 10.1109/72.80295
   MURRAY A, 1994, ANALOGUE NEURAL VLSI
   NORTHMORE DP, 1996, P 5 ANN C COMP NEUR
   PERRETT DI, 1982, EXP BRAIN RES, V47, P329
   PRATT GA, 1989, THESIS MIT CAMBRIDGE
   Rieke F., 1996, SPIKES EXPLORING NEU
   RITZ R, 1994, BIOL CYBERN, V71, P349, DOI 10.1007/BF00239622
   ROLLS ET, 1994, BEHAV PROCESS, V33, P113, DOI 10.1016/0376-6357(94)90062-0
   ROLLS ET, 1994, P ROY SOC B-BIOL SCI, V257, P9, DOI 10.1098/rspb.1994.0087
   SEJNOWSKI TJ, 1995, NATURE, V376, P21, DOI 10.1038/376021a0
   SHASTRI L, 1993, BEHAV BRAIN SCI, V16, P417, DOI 10.1017/S0140525X00030910
   Shawe-Taylor J., 1991, Connection Science, V3, P317, DOI 10.1080/09540099108946589
   Shepherd G. M., 1990, The Synaptic Organization of the Brain
   Shepherd GM., 1994, NEUROBIOLOGY
   SOFTKY W, 1994, NEUROSCIENCE, V58, P13, DOI 10.1016/0306-4522(94)90154-6
   Sontag ED, 1997, NEURAL COMPUT, V9, P337, DOI 10.1162/neco.1997.9.2.337
   Stevens CF, 1996, ADV NEUR IN, V8, P75
   TAYLOR JG, 1993, MATH APPROACHES NEUR, P341
   THORPE SJ, 1989, CONNECTIONISM IN PERSPECTIVE, P63
   Tuckwell H.C., 1988, INTRO THEORETICAL NE, DOI [10.1017/CBO9780511623271, DOI 10.1017/CBO9780511623271]
   Tuckwell H.C., 1988, INTRO THEORETICAL NE, V2
   Valiant L. G., 1994, CIRCUITS MIND
   ZAGHLOUL ME, 1994, SILICON IMPLEMENTATI
   ZHAO J, 1995, THESIS U LONDON LOND
NR 75
TC 1580
Z9 1644
U1 19
U2 186
PD DEC
PY 1997
VL 10
IS 9
BP 1659
EP 1671
DI 10.1016/S0893-6080(97)00011-7
UT WOS:000071201100011
DA 2023-11-16
ER

PT C
AU Madrenas, J
   Zapata, M
   Fernández, D
   Sánchez-Chiva, JM
   Valle, J
   Mata-Hernández, D
   Oltra, JA
   Cosp-Vilella, J
   Sato, S
AF Madrenas, Jordi
   Zapata, Mireya
   Fernandez, Daniel
   Maria Sanchez-Chiva, Josep
   Valle, Juan
   Mata-Hernandez, Diana
   Angel Oltra, Josep
   Cosp-Vilella, Jordi
   Sato, Shigeo
GP IEEE
TI Towards Efficient and Adaptive Cyber Physical Spiking Neural Integrated
   Systems
SO 2020 27TH IEEE INTERNATIONAL CONFERENCE ON ELECTRONICS, CIRCUITS AND
   SYSTEMS (ICECS)
SE IEEE International Conference on Electronics Circuits and Systems
DT Proceedings Paper
CT 27th IEEE International Conference on Electronics, Circuits and Systems
   (IEEE ICECS)
CY NOV 23-25, 2020
CL ELECTR NETWORK
DE Sensor Neural Computing; Integration; Integrated Sensors; MEMS;
   CMOS-MEMS; Spiking Neural Networks; SNN; Cyber Physical Neural Systems
ID NETWORK
AB This work introduces multi-sensor integration combined with an efficient and adaptive Spiking Neural Network (SNN) emulation architecture for local intelligent processing. For this purpose, we propose CMOS-MEMS with on-chip conditioning electronics together with spike processing by means of a real-time bioinspired and model-programmable SIMD multiprocessor. System integration considerations and results in the MEMS and processor developments are provided.
C1 [Madrenas, Jordi; Maria Sanchez-Chiva, Josep; Valle, Juan; Mata-Hernandez, Diana; Angel Oltra, Josep; Cosp-Vilella, Jordi] Univ Politecn Cataluna, Dept Elect Engn, Barcelona, Catalunya, Spain.
   [Zapata, Mireya] Univ Tecnol Indoamer, Res Ctr Mechatron & Interact Syst, Quito, Ecuador.
   [Fernandez, Daniel] IFAE BIST, Inst Fis Altes Energies, Bellaterra, Catalunya, Spain.
   [Sato, Shigeo] Tohoku Univ, Res Inst Elect Commun, Sendai, Miyagi, Japan.
RP Zapata, M (corresponding author), Univ Tecnol Indoamer, Res Ctr Mechatron & Interact Syst, Quito, Ecuador.
EM mireyazapata@uti.edu.ec; dfernandez@ifae.es; josepmsch@gmail.com;
   diana.mata@upc.edu; josep.angel.oltra@upc.edu
CR Bol David, 2015, 2015 IEEE SOI-3D-Subthreshold Microelectronics Technology Unified Conference (S3S). Proceedings, P1, DOI 10.1109/S3S.2015.7333500
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   IEEE, 2018, INT ROADM DEV SYST 2
   Jordan S, 2014, TSENSORS SUMMIT TRIL, P1
   Kasabov N, 2014, NEUROCOMPUTING, V134, P269, DOI 10.1016/j.neucom.2013.09.049
   Lee EA, 2008, ISORC 2008: 11TH IEEE SYMPOSIUM ON OBJECT/COMPONENT/SERVICE-ORIENTED REAL-TIME DISTRIBUTED COMPUTING - PROCEEDINGS, P363, DOI 10.1109/ISORC.2008.25
   Mahowald M., 1992, VLSI ANALOGS NEURONA
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Qu HW, 2016, MICROMACHINES-BASEL, V7, DOI 10.3390/mi7010014
   Samu D, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003557
   Sánchez G, 2013, IEEE INT SYMP CIRC S, P1624, DOI 10.1109/ISCAS.2013.6572173
   Sripad A, 2018, NEURAL NETWORKS, V97, P28, DOI 10.1016/j.neunet.2017.09.011
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
NR 13
TC 2
Z9 2
U1 0
U2 1
PY 2020
DI 10.1109/icecs49266.2020.9294982
UT WOS:000612696300197
DA 2023-11-16
ER

PT C
AU Stentiford, R
   Knowles, TC
   Feldotto, B
   Ergene, D
   Morin, FO
   Pearson, MJ
AF Stentiford, Rachael
   Knowles, Thomas C.
   Feldotto, Benedikt
   Ergene, Deniz
   Morin, Fabrice O.
   Pearson, Martin J.
BE Hunt, A
   Vouloutsi, V
   Moses, K
   Quinn, R
   Mura, A
   Prescott, T
   Verschure, PFMJ
TI Integrating Spiking Neural Networks and Deep Learning Algorithms on the
   Neurorobotics Platform
SO BIOMIMETIC AND BIOHYBRID SYSTEMS, LIVING MACHINES 2022
SE Lecture Notes in Artificial Intelligence
DT Proceedings Paper
CT 11th International Conference on Biomimetic and Biohybrid Systems
   (Living Machines)
CY JUL 19-22, 2022
CL Case Western Reserve Univ, ELECTR NETWORK
HO Case Western Reserve Univ
DE Neurorobotics platform; Predictive coding; Spiking neural network;
   pyNEST; NRP; WhiskEye; Head direction
ID HEAD-DIRECTION SIGNAL; CELL-ACTIVITY
AB We present a neurorobotic model that can associate self motion (odometry) with vision to correct for drift in a spiking neural network model of head direction based closely on known rodent neurophysiology. We use a deep predictive coding network to learn the generative model of representations of head direction from the spiking neural network to views of naturalistic scenery from a simulated mobile robot. This model has been deployed onto the Neurorobotics Platform of the Human Brain Project which allows full closed loop experiments with spiking neural network models simulated using NEST, a biomimetic robot platform called WhiskEye in Gazebo robot simulator, and a Deep Predictive Coding network implemented in Tensorflow.
C1 [Stentiford, Rachael; Knowles, Thomas C.; Pearson, Martin J.] Univ West England, Bristol Robot Lab, Bristol, Avon, England.
   [Feldotto, Benedikt; Ergene, Deniz; Morin, Fabrice O.] Tech Univ Munich, Dept Informat, D-85748 Munich, Germany.
RP Stentiford, R (corresponding author), Univ West England, Bristol Robot Lab, Bristol, Avon, England.
EM rachael.stentiford@uwe.ac.uk; tom.knowles@uwe.ac.uk; feldotto@in.tum.de;
   ergene@in.tum.de; morinf@in.tum.de; martin.pearson@uwe.ac.uk
CR Aljalbout E, 2020, NEURAL PROCESS LETT, V51, P2751, DOI 10.1007/s11063-020-10224-9
   Bassett JP, 2001, J NEUROSCI, V21, P5740, DOI 10.1523/JNEUROSCI.21-15-05740.2001
   Blair HT, 1999, J NEUROSCI, V19, P6673
   Falotico E, 2017, FRONT NEUROROBOTICS, V11, DOI 10.3389/fnbot.2017.00002
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   Goodridge JP, 1998, BEHAV NEUROSCI, V112, P749, DOI 10.1037/0735-7044.112.4.749
   GOODRIDGE JP, 1995, BEHAV NEUROSCI, V109, P49, DOI 10.1037/0735-7044.109.1.49
   Hunt A, 2017, FRONT NEUROROBOTICS, V11, DOI 10.3389/fnbot.2017.00018
   Knowles T.C., 2021, P AUT ROB SYST 22 AN, P408, DOI [10.1007/978-3-030-89177-0_43, DOI 10.1007/978-3-030-89177-0_43]
   Koenig N., 2004, 2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566), P2149
   Marc-Oliver G., 2007, SCHOLARPEDIA, DOI [DOI 10.4249/SCHOLARPEDIA.1430, 10.4249/scholarpedia.1430]
   nc3rs, THE 3RS
   Pearson MJ, 2021, FRONT ROBOT AI, V8, DOI 10.3389/frobt.2021.732023
   Prescott TJ, 1999, ADAPT BEHAV, V7, P99, DOI 10.1177/105971239900700105
   Stackman RW, 2003, J NEUROPHYSIOL, V90, P2862, DOI 10.1152/jn.00346.2003
   Stanford Artificial Intelligence Laboratory, ROB OP SYST
   Szczecinski NS, 2017, BIOL CYBERN, V111, P105, DOI 10.1007/s00422-017-0711-4
   TAUBE JS, 1995, J NEUROPHYSIOL, V74, P1953, DOI 10.1152/jn.1995.74.5.1953
   Yoder RM, 2015, J NEUROSCI, V35, P1354, DOI 10.1523/JNEUROSCI.1418-14.2015
   Yoder RM, 2014, FRONT INTEGR NEUROSC, V8, DOI 10.3389/fnint.2014.00032
   Yoder RM, 2009, J NEUROSCI, V29, P1061, DOI 10.1523/JNEUROSCI.1679-08.2009
NR 21
TC 0
Z9 0
U1 2
U2 2
PY 2022
VL 13548
BP 68
EP 79
DI 10.1007/978-3-031-20470-8_7
UT WOS:000913320600007
DA 2023-11-16
ER

PT C
AU Yang, X
   Liu, G
   Hong, HY
   Lv, XH
   Qiao, TT
   Chen, XY
AF Yang, Xu
   Liu, Guo
   Hong, Hongyan
   Lv, Xiaohe
   Qiao, Tingting
   Chen, Xiaoya
GP Assoc Comp Machinery
TI Visualization Implementation of a Parallelized Research Platform for
   Spiking Neural Network Study
SO ICDLT 2019: 2019 3RD INTERNATIONAL CONFERENCE ON DEEP LEARNING
   TECHNOLOGIES
DT Proceedings Paper
CT 3rd International Conference on Deep Learning Technologies (ICDLT)
CY JUL 05-07, 2019
CL Xiamen, PEOPLES R CHINA
DE Artificial intelligence; Spiking neural network; Visualization;
   Parallelized simulating
AB Artificial intelligence has become one of the most popular research fields, and has been widely used in intelligent control, pattern recognition, computer vision, speech recognition, signal processing and other fields and achieved great success. Artificial neural network is the most widely used and successful technology in artificial intelligence field, which cannot be separated from the support and promotion of many open source frameworks. As the third generation of artificial neural network, spiking neural network is more suitable for the simulation of biological neural network than the traditional neural network, so it is also expected in the field of artificial intelligence. The lack of an efficient simulation platform is limiting the progress of research on impulsive neural networks. In this paper, we present our effort at constructing a visualized parallelized research platform for spiking neural network study.
C1 [Yang, Xu; Liu, Guo; Qiao, Tingting; Chen, Xiaoya] Beijing Inst Technol, Sch Comp Sci & Technol, Beijing, Peoples R China.
   [Hong, Hongyan; Lv, Xiaohe] China Inst Marine Technol & Econ, South Xue Yuan St, Beijing, Peoples R China.
RP Yang, X (corresponding author), Beijing Inst Technol, Sch Comp Sci & Technol, Beijing, Peoples R China.
EM yangxu@tsinghua.edu.cn; joseph04912654@gmail.com;
   hhy@sinoshipbuilding.com; lxh@sinoshipbuilding.com;
   3220180845@bit.edu.cn; 3220180788@bit.edu.cn
CR Amir A, 2013, IEEE IJCNN
   [Anonymous], 2013, INTEGRATED MEMBRANE, DOI DOI 10.1515/9783110285666
   [Anonymous], 1995, APPL MECH MAT
   [Anonymous], FRONTIERS NEUROINFOR
   [Anonymous], 2013, 2013 INT JOINT C NEU
   Delorme A, 1999, NEUROCOMPUTING, V26-7, P989, DOI 10.1016/S0925-2312(99)00095-8
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Schemmel J, 2012, IEEE INT SYMP CIRC S
NR 8
TC 71
Z9 76
U1 7
U2 316
PY 2019
BP 1
EP 5
DI 10.1145/3342999.3343000
UT WOS:000506805100001
DA 2023-11-16
ER

PT C
AU Han, B
   Sengupta, A
   Roy, K
AF Han, Bing
   Sengupta, Abhronil
   Roy, Kaushik
GP IEEE
TI On the Energy Benefits of Spiking Deep Neural Networks: A Case Study
   (Special Session Paper)
SO 2016 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUL 24-29, 2016
CL Vancouver, CANADA
AB Deep learning neural networks have achieved success in a large number of visual processing tasks and are currently utilized for many real-world applications like image search and speech recognition among others. However, in spite of achieving high accuracy in such classification problems, they involve significant computational resources. Over the past few years, artificial neural network models have evolved into the biologically realistic and event-driven spiking neural networks. Recent research efforts have been directed at developing mechanisms to convert traditional deep artificial nets to spiking nets where the neurons communicate by means of spikes. However, there have been limited studies providing insights on the specific power, area and energy benefits offered by deep spiking neural nets in comparison to their non-spiking counterparts. In this paper, we perform a case study for a hardware implementation of a spiking/non-spiking deep net on the MNIST dataset and clearly outline the design prospects involved in implementing neural computing platforms in the spiking mode of operation.
C1 [Han, Bing; Sengupta, Abhronil; Roy, Kaushik] Purdue Univ, Sch Elect & Comp Engn, W Lafayette, IN 47907 USA.
RP Han, B (corresponding author), Purdue Univ, Sch Elect & Comp Engn, W Lafayette, IN 47907 USA.
EM han183@purdue.edu
CR Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   [Anonymous], 2011, P 28 INT C INT C MAC
   [Anonymous], 2013, INT C LEARN REPR ICL
   [Anonymous], 2011, FRONTIERS NEUROSCIEN
   [Anonymous], 2012, PREDICTION CANDIDATE
   Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Ciresan DC, 2010, NEURAL COMPUT, V22, P3207, DOI 10.1162/NECO_a_00052
   Diehl P., 2015, FRONTIERS COMPUTATIO
   Diehl P.U., 2015, 2015 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2015.7280696
   Khan MM, 2008, IEEE IJCNN, P2849, DOI 10.1109/IJCNN.2008.4634199
   Khodagholy D, 2015, NAT NEUROSCI, V18, P310, DOI 10.1038/nn.3905
   Kim S, 2011, 2011 11TH INTERNATIONAL CONFERENCE ON CONTROL, AUTOMATION AND SYSTEMS (ICCAS), P1
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Ramasubramanian SG, 2014, I SYMPOS LOW POWER E, P15, DOI 10.1145/2627369.2627625
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
NR 17
TC 27
Z9 27
U1 0
U2 4
PY 2016
BP 971
EP 976
UT WOS:000399925501020
DA 2023-11-16
ER

PT C
AU Kulkarni, SR
   Babu, AV
   Rajendran, B
AF Kulkarni, Shruti R.
   Babu, Anakha, V
   Rajendran, Bipin
BE Pimenidis, E
   Jayne, C
TI Acceleration of Convolutional Networks Using Nanoscale Memristive
   Devices
SO ENGINEERING APPLICATIONS OF NEURAL NETWORKS, EANN 2018
SE Communications in Computer and Information Science
DT Proceedings Paper
CT 19th International Conference on Engineering Applications of Neural
   Networks (EANN)
CY SEP 03-05, 2018
CL Univ W England, Bristol, ENGLAND
HO Univ W England
DE Spiking neural networks; Artificial neural networks; Non-volatile memory
   devices; Programming variability; Memristors
ID NEURAL-NETWORKS; SPIKING
AB We discuss a convolutional neural network for handwritten digit classification and its hardware acceleration as an inference engine using nanoscale memristive devices in the spike domain. We study the impact of device programming variability on the spiking neural network's (SNN) inference accuracy and benchmark its performance with an equivalent artificial neural network (ANN). We demonstrate optimization strategies to implement these networks with memristive devices with an on-off ratio as low as 10 and only 32 levels of resolution. Further, close to baseline accuracies can be maintained for the networks even if such memristive devices are used to duplicate the pre-determined kernel weights to enable parallel execution of the convolution operation.
C1 [Kulkarni, Shruti R.; Babu, Anakha, V; Rajendran, Bipin] NJIT, Dept Elect & Comp Engn, Newark, NJ 07102 USA.
RP Rajendran, B (corresponding author), NJIT, Dept Elect & Comp Engn, Newark, NJ 07102 USA.
EM srk68@njit.edu; av442@njit.edu; bipin@njit.edu
CR Abbott LF, 1999, BRAIN RES BULL, V50, P303, DOI 10.1016/S0361-9230(99)00161-6
   Anwani N, 2015, IEEE IJCNN
   Babu AV, 2017, IEEE I C ELECT CIRC, P214, DOI 10.1109/ICECS.2017.8292067
   Boybat I., 2017, ARXIV
   Burr GW, 2015, 2015 IEEE INTERNATIONAL ELECTRON DEVICES MEETING (IEDM), DOI 10.1109/iedm.2015.7409625
   Burr GW, 2017, ADV PHYS-X, V2, P89, DOI 10.1080/23746149.2016.1259585
   Calderon A., 2003, P INT C COMPUTATIONA
   Chen PY, 2015, ICCAD-IEEE ACM INT, P194, DOI 10.1109/ICCAD.2015.7372570
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Garbin D, 2015, IEEE T ELECTRON DEV, V62, P2494, DOI 10.1109/TED.2015.2440102
   Gokmen T, 2017, Arxiv, DOI arXiv:1705.08014
   Han B, 2016, IEEE IJCNN, P971
   HUBEL DH, 1968, J PHYSIOL-LONDON, V195, P215, DOI 10.1113/jphysiol.1968.sp008455
   Jackson BL, 2013, ACM J EMERG TECH COM, V9, DOI 10.1145/2463585.2463588
   Jo SH, 2010, NANO LETT, V10, P1297, DOI 10.1021/nl904092h
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
   Kulkarni SR, 2018, NEURAL NETWORKS, V103, P118, DOI 10.1016/j.neunet.2018.03.019
   Kulkarni SR, 2017, IEEE I C ELECT CIRC, P128, DOI 10.1109/ICECS.2017.8292015
   Kuzum D, 2013, NANOTECHNOLOGY, V24, DOI 10.1088/0957-4484/24/38/382001
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lim S, 2017, Arxiv, DOI arXiv:1707.06381
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Panwar N, 2017, IEEE ELECTR DEVICE L, V38, P740, DOI 10.1109/LED.2017.2696023
   Qiao N, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00141
   Rajendran B, 2016, IEEE J EM SEL TOP C, V6, P198, DOI 10.1109/JETCAS.2016.2533298
   Sangkil Kim, 2015, 2015 IEEE MTT-S International Microwave Symposium (IMS2015), P1, DOI 10.1109/MWSYM.2015.7166723
   Schreiber S, 2003, NEUROCOMPUTING, V52-4, P925, DOI 10.1016/S0925-2312(02)00838-X
   Song LH, 2017, INT S HIGH PERF COMP, P541, DOI 10.1109/HPCA.2017.55
   Stromatias E, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00222
   Suri M, 2011, 2011 IEEE INTERNATIONAL ELECTRON DEVICES MEETING (IEDM)
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Wang B, 2016, FRONT CELL NEUROSCI, V10, DOI 10.3389/fncel.2016.00239
   Yakopcic C, 2017, IEEE IJCNN, P1696, DOI 10.1109/IJCNN.2017.7966055
   Yakopcic C, 2016, IEEE IJCNN, P963, DOI 10.1109/IJCNN.2016.7727302
NR 35
TC 1
Z9 1
U1 0
U2 0
PY 2018
VL 893
BP 240
EP 251
DI 10.1007/978-3-319-98204-5_20
UT WOS:000926164500020
DA 2023-11-16
ER

PT J
AU Lee, C
   Sarwar, SS
   Panda, P
   Srinivasan, G
   Roy, K
AF Lee, Chankyu
   Sarwar, Syed Shakib
   Panda, Priyadarshini
   Srinivasan, Gopalakrishnan
   Roy, Kaushik
TI Enabling Spike-Based Backpropagation for Training Deep Neural Network
   Architectures
SO FRONTIERS IN NEUROSCIENCE
DT Article
DE spiking neural network; convolutional neural network; spike-based
   learning rule; gradient descent backpropagation; leaky integrate and
   fire neuron
ID MODEL
AB Spiking Neural Networks (SNNs) have recently emerged as a prominent neural computing paradigm. However, the typical shallow SNN architectures have limited capacity for expressing complex representations while training deep SNNs using input spikes has not been successful so far. Diverse methods have been proposed to get around this issue such as converting off-the-shelf trained deep Artificial Neural Networks (ANNs) to SNNs. However, the ANN-SNN conversion scheme fails to capture the temporal dynamics of a spiking system. On the other hand, it is still a difficult problem to directly train deep SNNs using input spike events due to the discontinuous, non-differentiable nature of the spike generation function. To overcome this problem, we propose an approximate derivative method that accounts for the leaky behavior of LIF neurons. This method enables training deep convolutional SNNs directly (with input spike events) using spike-based backpropagation. Our experiments show the effectiveness of the proposed spike-based learning on deep networks (VGG and Residual architectures) by achieving the best classification accuracies in MNIST, SVHN, and CIFAR-10 datasets compared to other SNNs trained with a spike-based learning. Moreover, we analyze sparse event-based computations to demonstrate the efficacy of the proposed SNN training method for inference operation in the spiking domain.
C1 [Lee, Chankyu; Sarwar, Syed Shakib; Panda, Priyadarshini; Srinivasan, Gopalakrishnan; Roy, Kaushik] Purdue Univ, Sch Elect & Comp Engn, Nanoelect Res Lab, W Lafayette, IN 47907 USA.
RP Lee, C; Sarwar, SS (corresponding author), Purdue Univ, Sch Elect & Comp Engn, Nanoelect Res Lab, W Lafayette, IN 47907 USA.
EM lee2216@purdue.edu; sarwar@purdue.edu
CR Ankit A, 2017, DES AUT CON, DOI 10.1145/3061639.3062311
   [Anonymous], 2018, ARXIV180905793
   [Anonymous], 2015, ARXIV PREPRINT ARXIV
   [Anonymous], 2020, 738385 BIORXIV, DOI DOI 10.1101/738385
   [Anonymous], 2016, ARXIV161101421
   [Anonymous], 2015, INT J COMPUT VISION, DOI DOI 10.1007/s11263-014-0788-3
   [Anonymous], 2016, CONVOLUTIONAL NETWOR
   [Anonymous], ADV NEURAL INFORM PR
   [Anonymous], ADV NEURAL INFORM PR
   [Anonymous], 2001, THEORETICAL NEUROSCI
   [Anonymous], 2016, ARXIV161103000
   [Anonymous], 2015, ARXIV151104484
   [Anonymous], 2019, CORR
   [Anonymous], 2016 IEEE INT C REB
   Bengio Yoshua, 2013, ESTIMATING PROPAGATI
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS)
   Brader JM, 2007, NEURAL COMPUT, V19, P2881, DOI 10.1162/neco.2007.19.11.2881
   Brette R, 2005, J NEUROPHYSIOL, V94, P3637, DOI 10.1152/jn.00686.2005
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Diehl P.U., 2015, 2015 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2015.7280696
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Furber SB, 2013, IEEE T COMPUT, V62, P2454, DOI 10.1109/TC.2012.142
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V116, P449, DOI 10.1113/jphysiol.1952.sp004717
   Huang XW, 2015, ACTA POLYM SIN, P1133
   Hunsberger Eric, 2015, COMPUT SCI
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Jin Y., 2018, ADV NEURAL INFORM PR
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90
   Kaiming He, 2015, 2015 IEEE International Conference on Computer Vision (ICCV). Proceedings, P1026, DOI 10.1109/ICCV.2015.123
   Kappel D, 2018, ENEURO, V5, DOI 10.1523/ENEURO.0301-17.2018
   Kappel D, 2015, PLOS COMPUT BIOL, V11, DOI 10.1371/journal.pcbi.1004485
   Kingma DP., 2017, ARXIV
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee C, 2019, IEEE T COGN DEV SYST, V11, P384, DOI 10.1109/TCDS.2018.2833071
   Lee C, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00435
   Lee JM, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00191
   Lichtsteiner P, 2008, IEEE J SOLID-ST CIRC, V43, P566, DOI 10.1109/JSSC.2007.914337
   Lillicrap TP, 2019, CURR OPIN NEUROBIOL, V55, P82, DOI 10.1016/j.conb.2019.01.011
   Lin DD, 2016, PR MACH LEARN RES, V48
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Mostafa Hesham, 2018, IEEE Trans Neural Netw Learn Syst, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Neftci EO, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00324
   Netzer Y., 2011, NEURAL INFORM PROCES, P5
   Orchard G, 2015, FRONT NEUROSCI-SWITZ, V9, DOI [10.3389/fhins.2015.00437, 10.3389/fnins.2015.00437]
   Panda P, 2016, IEEE IJCNN, P299, DOI 10.1109/IJCNN.2016.7727212
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Rumelhart D.E., 1985, PARALLEL DISTRIBUTED
   Sarwar SS, 2018, IEEE J EM SEL TOP C, V8, P796, DOI 10.1109/JETCAS.2018.2835809
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Shrestha S. B., 2018, P 32 INT C NEUR INF, P1419, DOI DOI 10.5555/3326943.3327073
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Simonyan K., 2015, ARXIV
   Srinivasan G, 2018, ACM J EMERG TECH COM, V14, DOI 10.1145/3266229
   Srinivasan G, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00524
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Tavanaei A, 2017, IEEE IJCNN, P2023, DOI 10.1109/IJCNN.2017.7966099
   WERBOS PJ, 1990, P IEEE, V78, P1550, DOI 10.1109/5.58337
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Zhao B, 2015, IEEE T NEUR NET LEAR, V26, P1963, DOI 10.1109/TNNLS.2014.2362542
NR 62
TC 165
Z9 171
U1 6
U2 49
PD FEB 28
PY 2020
VL 14
AR 119
DI 10.3389/fnins.2020.00119
UT WOS:000525049300001
HC Y
HP N
DA 2023-11-16
ER

PT J
AU Zhang, HL
   Gang, C
   Xu, C
   Gong, GL
   Lu, HX
AF Zhang, Huilin
   Gang, Chen
   Xu, Chen
   Gong, Guoliang
   Lu, Huaxiang
TI Brain-Inspired Spiking Neural Network Using Superconducting Devices
SO IEEE TRANSACTIONS ON EMERGING TOPICS IN COMPUTATIONAL INTELLIGENCE
DT Article
DE Neurons; Synapses; Biological neural networks; Feature extraction;
   Training; Threshold voltage; Power demand; Spiking neural network (SNN);
   brain-inspired computing; superconducting devices; Josephson junction
   (JJ); quantum phase-slip junction (QPSJ); magnetic order
AB Based on recent research in artificial neural networks, researchers have focused on topics from brain-like computing based on the Von Neumann architecture to brain-inspired computing based on the integration of storage and calculation due to the large energy dissipation. Inspired by biological neural networks, people use superconducting devices, which have nonlinear dynamic characteristics, to construct synapse and neuron circuits. However, it is challenging to build large-scale superconducting spiking neural networks due to fan-ins and fan-outs of superconducting devices. In this paper, we present a criterion to scale up the superconducting neural network. Then, we present a three-layer fully connected superconducting spiking neural network construction scheme. Inspired by the biological neural network, we present a new training method based on frequency coding. After only 100 training iterations with one grayscale image of a digit, the learned three-layer fully connected superconducting spiking neural network has a recognition accuracy rate of 86.1% on the digit class. Moreover, the power dissipation of one spiking event in a superconducting synapse is $23.2{\bm{\ zJ}}$, which shows that the network has outstandingly high efficiency over existing biologically inspired neural networks.
C1 [Zhang, Huilin; Gang, Chen; Xu, Chen; Gong, Guoliang; Lu, Huaxiang] Chinese Acad Sci, Inst Semicond, Beijing 100083, Peoples R China.
   [Zhang, Huilin] Univ Chinese Acad Sci, Coll Mat Sci & Optoelect Technol, Beijing 100089, Peoples R China.
   [Gang, Chen; Gong, Guoliang; Lu, Huaxiang] Univ Chinese Acad Sci, Beijing 100089, Peoples R China.
   [Lu, Huaxiang] Chinese Acad Sci, Ctr Excellence Brain Sci & Intelligence Technol, Shanghai 200031, Peoples R China.
   [Lu, Huaxiang] Semicond Neural Network Intelligent Percept & Com, Beijing 100083, Peoples R China.
RP Lu, HX (corresponding author), Chinese Acad Sci, Inst Semicond, Beijing 100083, Peoples R China.; Lu, HX (corresponding author), Univ Chinese Acad Sci, Beijing 100089, Peoples R China.; Lu, HX (corresponding author), Chinese Acad Sci, Ctr Excellence Brain Sci & Intelligence Technol, Shanghai 200031, Peoples R China.
EM hlzhang@semi.ac.cn; chengang08@semi.ac.cn; chenxu@semi.ac.cn;
   gongmianjie@semi.ac.cn; luhx@semi.ac.cn
CR Ahmad AS, 2017, 2017 COMPUTING CONFERENCE, P135, DOI 10.1109/SAI.2017.8252094
   Andersson A, 2015, PHYS REV B, V91, DOI 10.1103/PhysRevB.91.134504
   Chen WD, 2021, SIGNAL PROCESS, V184, DOI 10.1016/j.sigpro.2020.107943
   Cheng R, 2018, J APPL PHYS, V124, DOI 10.1063/1.5042421
   Chu D, 2021, NEURAL NETWORKS, V135, P192, DOI 10.1016/j.neunet.2020.12.012
   Constantino NGN, 2018, NANOMATERIALS-BASEL, V8, DOI 10.3390/nano8060442
   Da Li, 2018, IOP Conference Series: Materials Science and Engineering, V439, DOI 10.1088/1757-899X/439/3/032083
   Daniels MW, 2020, PHYS REV APPL, V13, DOI 10.1103/PhysRevApplied.13.034016
   Furber S, 2016, J NEURAL ENG, V13, DOI 10.1088/1741-2560/13/5/051001
   Goteti US, 2015, ELECTRON LETT, V51, DOI 10.1049/el.2015.0904
   Goteti US, 2019, IEEE T APPL SUPERCON, V29, DOI 10.1109/TASC.2019.2904695
   Jeong H, 2019, J PHYS D APPL PHYS, V52, DOI 10.1088/1361-6463/aae223
   Jia L, 2019, IOP C SER EARTH ENV, V252, DOI 10.1088/1755-1315/252/2/022046
   JOSEPHSON BD, 1962, PHYS LETT, V1, P251, DOI 10.1016/0031-9163(62)91369-0
   Katam NK, 2018, IEEE T APPL SUPERCON, V28, DOI 10.1109/TASC.2018.2797262
   Kerr D, 2018, IEEE T NEUR NET LEAR, V29, P5356, DOI 10.1109/TNNLS.2018.2797994
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   MacGregor TH, 2018, COMPLEX VAR ELLIPTIC, V63, P1529, DOI 10.1080/17476933.2017.1385069
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Mooij JE, 2015, NEW J PHYS, V17, DOI 10.1088/1367-2630/17/3/033006
   Mooij JE, 2006, NAT PHYS, V2, P169, DOI 10.1038/nphys234
   Rajendran B, 2016, IEEE J EM SEL TOP C, V6, P198, DOI 10.1109/JETCAS.2016.2533298
   Ran Cheng, 2019, IEEE Transactions on Applied Superconductivity, V29, DOI 10.1109/TASC.2019.2892111
   Rathi N, 2021, IEEE TETCI, V5, P143, DOI 10.1109/TETCI.2018.2872014
   Satpathy SK, 2019, IEEE J SOLID-ST CIRC, V54, P1074, DOI 10.1109/JSSC.2018.2886350
   Schneider ML, 2018, SCI ADV, V4, DOI 10.1126/sciadv.1701329
   Srinivasan G, 2016, SCI REP-UK, V6, DOI 10.1038/srep29545
   Toomey E, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00933
   Tzelepi M, 2021, IEEE TETCI, V5, P191, DOI 10.1109/TETCI.2019.2897815
   Wang XS, 2021, IEEE TETCI, V5, P298, DOI 10.1109/TETCI.2018.2877154
   Wang ZR, 2017, NAT MATER, V16, P101, DOI [10.1038/nmat4756, 10.1038/NMAT4756]
   Wester R, 2015, IEEE SOFTWARE, V32, P37, DOI 10.1109/MS.2015.53
   Wijesinghe P, 2018, IEEE TETCI, V2, P345, DOI 10.1109/TETCI.2018.2829924
   Xue WH, 2020, CHINESE PHYS B, V29, DOI 10.1088/1674-1056/ab75da
NR 34
TC 6
Z9 6
U1 2
U2 10
PD FEB
PY 2023
VL 7
IS 1
BP 271
EP 277
DI 10.1109/TETCI.2021.3089328
EA JUL 2021
UT WOS:000732278200001
DA 2023-11-16
ER

PT C
AU Lin, XH
   Du, PG
AF Lin, Xianghong
   Du, Pangao
BE Farkas, I
   Masulli, P
   Wermter, S
TI Spike-Train Level Unsupervised Learning Algorithm for Deep Spiking
   Belief Networks
SO ARTIFICIAL NEURAL NETWORKS AND MACHINE LEARNING, ICANN 2020, PT II
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 29th International Conference on Artificial Neural Networks (ICANN)
CY SEP 15-18, 2020
CL Bratislava, SLOVAKIA
DE Deep spiking belief networks; Unsupervised learning; Spike neural
   machines; Reconstruction error
ID NEURAL-NETWORKS
AB Deep spiking belief network (DSBN) uses unsupervised layer-wise pre-training method to train the network weights, it is stacked with the spike neural machine (SNM) modules. However, the synaptic weights of SNMs are difficult to pre-training through simple and effective approach for spike-train driven networks. This paper proposes a new algorithm that uses unsupervised multi-spike learning rule to train SNMs, which can implement the complex spatio-temporal pattern learning of spike trains. The spike signals first propagate in the forward direction, and then are reconstructed in the reverse direction, and the synaptic weights are adjusted according to the reconstruction error. The algorithm is successfully applied to spike train patterns, the module parameters are analyzed, such as the neuron number and learning rate in the SNMs. In addition, the low reconstruction errors of DSBNs are shown by the experimental results.
C1 [Lin, Xianghong; Du, Pangao] Northwest Normal Univ, Coll Comp Sci & Engn, Lanzhou 730070, Peoples R China.
RP Lin, XH (corresponding author), Northwest Normal Univ, Coll Comp Sci & Engn, Lanzhou 730070, Peoples R China.
EM linxh@nwnu.edu.cn
CR Fatahi M, 2018, BIOL INSPIR COGN ARC, V24, P59, DOI 10.1016/j.bica.2018.04.009
   Gerstner W., 2002, SPIKING NEURON MODEL
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Jo T, 2015, SCI REP-UK, V5, DOI 10.1038/srep17573
   Kang SY, 2013, INT CONF ACOUST SPEE, P8012, DOI 10.1109/ICASSP.2013.6639225
   Kasabov N, 2014, NEUROCOMPUTING, V134, P269, DOI 10.1016/j.neucom.2013.09.049
   Kuremoto T, 2014, NEUROCOMPUTING, V137, P47, DOI 10.1016/j.neucom.2013.03.047
   Lee H, 2011, COMMUN ACM, V54, P95, DOI 10.1145/2001269.2001295
   Lin XH, 2018, LECT NOTES COMPUT SC, V11139, P222, DOI 10.1007/978-3-030-01418-6_22
   Lin XH, 2017, NEUROCOMPUTING, V237, P59, DOI 10.1016/j.neucom.2016.08.087
   Mleczko WK, 2015, COMM COM INF SC, V538, P400, DOI 10.1007/978-3-319-24770-0_35
   Mohemmed A, 2012, INT J NEURAL SYST, V22, DOI 10.1142/S0129065712500128
   Neftci E, 2014, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00272
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   Paiva ARC, 2010, STATISTICAL SIGNAL PROCESSING FOR NEUROSCIENCE AND NEUROTECHNOLOGY, P265, DOI 10.1016/B978-0-12-375027-3.00008-9
   Paiva ARC, 2009, NEURAL COMPUT, V21, P424, DOI 10.1162/neco.2008.09-07-614
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Tavanaei A., 2015, MACH LEARN SIGN PROC, P1
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Wang XW, 2020, NEURAL NETWORKS, V125, P258, DOI 10.1016/j.neunet.2020.02.011
   Yu QF, 2013, PLOS ONE, V8, DOI [10.1371/journal.pone.0059627, 10.1371/journal.pone.0078318]
NR 22
TC 0
Z9 0
U1 0
U2 3
PY 2020
VL 12397
BP 634
EP 645
DI 10.1007/978-3-030-61616-8_51
UT WOS:000713797800051
DA 2023-11-16
ER

PT J
AU Widmer, S
   Kossel, M
   Cherubini, G
   Wozniak, S
   Francese, PA
   Stanojevic, A
   Brändli, M
   Frick, K
   Pantazi, A
AF Widmer, Sandro
   Kossel, Marcel
   Cherubini, Giovanni
   Wozniak, Stanislaw
   Francese, Pier Andrea
   Stanojevic, Ana
   Brandli, Matthias
   Frick, Klaus
   Pantazi, Angeliki
TI Design of Time-Encoded Spiking Neural Networks in 7-nm CMOS Technology
SO IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS II-EXPRESS BRIEFS
DT Article
DE Neurons; Encoding; Network topology; Iris; Biological neural networks;
   Silicon; Signal resolution; Spiking neural networks (SNNs); artificial
   neural networks (ANNs); synchronous time-to-first-spike (sTTFS)
   encoding; time-to-first-spike (TTFS) encoding
AB In biologically inspired spiking neural networks (SNNs) neurons communicate by short pulses, called spikes. SNNs have the potential to be more power efficient than artificial neural networks (ANNs), thanks to the fewer computational steps required by the spike transmission and processing, as compared to the multiply-and-accumulate (MAC) operations with wide bit-vectors usually adopted in ANNs. We present the design of two types of SNNs with integrate-and-fire dynamics and single-spike per neuron operation, where neural communication is based on synchronous time-to-first-spike (sTTFS) and time-to-first-spike (TTFS) encoding schemes. In the considered time-encoded SNNs, the information is carried by the timing of the spikes with respect to a reference time. In 7nm CMOS technology both designs are synthesized as VHDL-based random-logic-macros (RLMs) and compared to an equivalent ANN design in terms of power consumption, latency and silicon area, using the Iris data set for inference. A cost function expressed as a product of energy consumption and silicon area is introduced to compare the three network designs. With respect to this cost function, it turns out that the SNN-TTFS implemented for the considered classification task outperforms the ANN used as baseline model.
C1 [Widmer, Sandro; Kossel, Marcel; Cherubini, Giovanni; Wozniak, Stanislaw; Francese, Pier Andrea; Stanojevic, Ana; Brandli, Matthias; Pantazi, Angeliki] IBM Res GmbH, Hybrid Cloud Res, CH-8803 Ruschlikon, Switzerland.
   [Widmer, Sandro] OST Eastern Switzerland Univ Appl Sci, ICE Inst Comput Engn, CH-9000 Gallen, Switzerland.
   [Widmer, Sandro] RhySearch, Ultraprecis Mfg Lab, CH-9471 Buchs, Switzerland.
   [Frick, Klaus] OST Eastern Switzerland Univ Appl Sci, ICE Inst Comput Engn, CH-9471 Buchs, Switzerland.
RP Kossel, M (corresponding author), IBM Res GmbH, Hybrid Cloud Res, CH-8803 Ruschlikon, Switzerland.
EM sandro.widmer@rhysearch.ch; mko@zurich.ibm.com;
   giovanni.cherubini@ibm.com; stw@zurich.ibm.com; pfr@zurich.ibm.com;
   ans@zurich.ibm.com; mbr@zurich.ibm.com; klaus.frick@ost.ch;
   agp@zurich.ibm.com
CR Bouvier M, 2019, ACM J EMERG TECH COM, V15, DOI 10.1145/3304103
   Chien CH, 2018, IEEE T EMERG TOP COM, V6, P135, DOI 10.1109/TETC.2015.2424593
   Comsa JM, 2020, INT CONF ACOUST SPEE, P8529, DOI [10.1109/icassp40776.2020.9053856, 10.1109/ICASSP40776.2020.9053856]
   Gollisch T, 2008, SCIENCE, V319, P1108, DOI 10.1126/science.1149639
   Johansson RS, 2004, NAT NEUROSCI, V7, P170, DOI 10.1038/nn1177
   Kubke MF, 2002, J NEUROSCI, V22, P7671
   Mesquida T, 2016, 2016 12TH CONFERENCE ON PH.D. RESEARCH IN MICROELECTRONICS AND ELECTRONICS (PRIME)
   Mostafa H, 2018, IEEE T NEUR NET LEAR, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Rueckauer B, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351295
   Stanojevic A, 2022, IEEE IMAGE PROC, P1901, DOI 10.1109/ICIP46576.2022.9897692
NR 10
TC 1
Z9 1
U1 0
U2 0
PD SEP
PY 2023
VL 70
IS 9
BP 3639
EP 3643
DI 10.1109/TCSII.2023.3277784
UT WOS:001066636600083
DA 2023-11-16
ER

PT J
AU Parhi, KK
   Unnikrishnan, NK
AF Parhi, Keshab K.
   Unnikrishnan, Nanda K.
TI Brain-Inspired Computing: Models and Architectures
SO IEEE OPEN JOURNAL OF CIRCUITS AND SYSTEMS
DT Article
DE Perceptron; multi-layer perceptron; convolutional neural network;
   recurrent neural network; Hopfield neural network; Boltzmann machines;
   hyperdimensional computing; spiking neural networks; graph neural
   networks
ID SPIKING NEURAL-NETWORKS; RECEPTIVE-FIELDS; FUNCTIONAL ARCHITECTURE;
   DEEP; CLASSIFICATION; ORGANIZATION; PERCEPTRONS; NEURONS; NETS
AB With an exponential increase in the amount of data collected per day, the fields of artificial intelligence and machine learning continue to progress at a rapid pace with respect to algorithms, models, applications, and hardware. In particular, deep neural networks have revolutionized these fields by providing unprecedented human-like performance in solving many real-world problems such as image or speech recognition. There is also significant research aimed at unraveling the principles of computation in large biological neural networks and, in particular, biologically plausible spiking neural networks. This article presents an overview of the brain-inspired computing models starting with the development of the perceptron and multi-layer perceptron followed by convolutional neural networks (CNNs) and recurrent neural networks (RNNs). This article also briefly reviews other neural network models such as Hopfield neural networks and Boltzmann machines. Other models such as spiking neural networks (SNNs) and hyperdimensional computing are then briefly reviewed. Recent advances in these neural networks and graph related neural networks are then described.
C1 [Parhi, Keshab K.; Unnikrishnan, Nanda K.] Univ Minnesota, Dept Elect & Comp Engn, Minneapolis, MN 55455 USA.
RP Parhi, KK (corresponding author), Univ Minnesota, Dept Elect & Comp Engn, Minneapolis, MN 55455 USA.
EM parhi@umn.edu
CR Abadi M, 2016, ACM SIGPLAN NOTICES, V51, P1, DOI [10.1145/2951913.2976746, 10.1145/3022670.2976746]
   Ambrogio S, 2018, NATURE, V558, P60, DOI 10.1038/s41586-018-0180-5
   [Anonymous], 2016, NIPS
   [Anonymous], 2018, WP499 XIL
   [Anonymous], 2007, ADV NEURAL INFORM PR
   [Anonymous], 1970, INFORM CONTROL
   [Anonymous], 1997, NEURAL COMPUT, DOI 10.1162/neco.1997.9.8.1735
   [Anonymous], 2017, WP08437001V02 NVIDIA
   [Anonymous], 1986, SERIAL ORDER PARALLE
   [Anonymous], 1960, 4315 CORN U NEWS SER
   Anwani N, 2015, IEEE IJCNN
   Auten A, 2020, DES AUT CON, DOI 10.1109/dac18072.2020.9218751
   BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   BOURLARD H, 1988, BIOL CYBERN, V59, P291, DOI 10.1007/BF00332918
   Bruna J., 2013, SPECTRAL NETWORKS LO
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Caporale N, 2008, ANNU REV NEUROSCI, V31, P25, DOI 10.1146/annurev.neuro.31.060407.125639
   Chung J., 2014, NIPS 2014 WORKSH DEE
   Ciresan DC, 2010, NEURAL COMPUT, V22, P3207, DOI 10.1162/NECO_a_00052
   Cybenko G., 1989, Mathematics of Control, Signals, and Systems, V2, P303, DOI 10.1007/BF02551274
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Defferrard M, 2016, ADV NEUR IN, V29
   Delbruck T, 2019, CONF REC ASILOMAR C, P500, DOI [10.1109/ieeeconf44664.2019.9048865, 10.1109/IEEECONF44664.2019.9048865]
   Deng CH, 2018, 2018 51ST ANNUAL IEEE/ACM INTERNATIONAL SYMPOSIUM ON MICROARCHITECTURE (MICRO), P189, DOI 10.1109/MICRO.2018.00024
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng L, 2020, P IEEE, V108, P485, DOI 10.1109/JPROC.2020.2976475
   Drenick R, 1981, SYSTEM MODELING OPTI, P762, DOI [DOI 10.1007/BFB0006203, 10.1007/BFb0006203]
   Dreyfus Stuart, 1962, J MATH ANAL APPL, V5, P30
   Drozdzal M, 2016, LECT NOTES COMPUT SC, V10008, P179, DOI 10.1007/978-3-319-46976-8_19
   Edalat A, NEURAL NETWORKS THEI
   ELMAN JL, 1990, COGNITIVE SCI, V14, P179, DOI 10.1207/s15516709cog1402_1
   Franco J, 2021, PHYSIOTHER THEOR PR, V37, P1419, DOI 10.1080/09593985.2019.1709234
   FUKUSHIMA K, 1980, BIOL CYBERN, V36, P193, DOI 10.1007/BF00344251
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   Gallego G, 2022, IEEE T PATTERN ANAL, V44, P154, DOI 10.1109/TPAMI.2020.3008413
   Gama F., 2020, GRAPH FILTERS GRAPH
   Gamboa N., 2020, ARXIV PREPRINT ARXIV
   Ge LL, 2020, IEEE CIRC SYST MAG, V20, P30, DOI 10.1109/MCAS.2020.2988388
   Gers FA, 2000, NEURAL COMPUT, V12, P2451, DOI 10.1162/089976600300015015
   Gers FA, 2000, IEEE IJCNN, P189, DOI 10.1109/IJCNN.2000.861302
   Gerstner W, 2018, SCHOLARPEDIA, V3, P1343
   Gómez-Bombarelli R, 2018, ACS CENTRAL SCI, V4, P268, DOI 10.1021/acscentsci.7b00572
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Gori M, 2005, IEEE IJCNN, P729
   Graves A, 2012, STUD COMPUT INTELL, V385, P5
   Graves A, 2009, IEEE T PATTERN ANAL, V31, P855, DOI 10.1109/TPAMI.2008.137
   Gulshan V, 2016, JAMA-J AM MED ASSOC, V316, P2402, DOI 10.1001/jama.2016.17216
   Hamilton WL, 2017, ADV NEUR IN, V30
   Hamilton WL, 2017, ARXIV
   Han S, 2016, CONF PROC INT SYMP C, P243, DOI 10.1109/ISCA.2016.30
   Hay J. C., 1960, VG1196G5 CORN U LIB
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   HEBB D. O., 1949
   Henaff M., 2015, ARXIV PREPRINT ARXIV
   Hinton G., 1986, PARALLEL DISTRIBUTED, V1
   Hinton G.E., 2007, SCHOLARPEDIA, V2, P1668, DOI [DOI 10.4249/SCHOLARPEDIA.1668, 10.4249/scholarpedia.1668]
   Hochreiter S., 1991, THESIS TU MUNCH I C
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V116, P449, DOI 10.1113/jphysiol.1952.sp004717
   HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554
   HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8
   Howard A. G., 2017, ABS170404861 CORR
   HUBEL DH, 1968, J PHYSIOL-LONDON, V195, P215, DOI 10.1113/jphysiol.1968.sp008455
   HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837
   HUBEL DH, 1959, J PHYSIOL-LONDON, V148, P574, DOI 10.1113/jphysiol.1959.sp006308
   Jang H, 2019, IEEE SIGNAL PROC MAG, V36, P64, DOI 10.1109/MSP.2019.2935234
   Jia X, 2016, IEEE ENG MED BIO, P639, DOI 10.1109/EMBC.2016.7590783
   Jiang YH, 2019, ADV NEUR IN, V32
   Jouppi NP, 2017, 44TH ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA 2017), P1, DOI 10.1145/3079856.3080246
   Kanerva Pentti, 1988, SPARSE DISTRIBUTED M
   Kasabov NK, 2014, NEURAL NETWORKS, V52, P62, DOI 10.1016/j.neunet.2014.01.006
   Kingma Diederik, 2014, P 2014 INT C LEARNIN
   Kipf T., 2017, INT C LEARN REPR
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
   Krizhevsky Alex, 2009, LEARNING MULTIPLE LA, V3, P6
   Kulkarni SR, 2018, NEURAL NETWORKS, V103, P118, DOI 10.1016/j.neunet.2018.03.019
   Lapique L., 1907, J PHYSL PATHOL GEN, V9, P620, DOI DOI 10.1007/S00422-007-0189-6
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Linnainmaa S., 1976, BIT (Nordisk Tidskrift for Informationsbehandling), V16, P146, DOI 10.1007/BF01931367
   Litjens G, 2019, JACC-CARDIOVASC IMAG, V12, P1549, DOI 10.1016/j.jcmg.2019.06.009
   LITTLE W A, 1974, Mathematical Biosciences, V19, P101, DOI 10.1016/0025-5564(74)90031-5
   Liu Y, 2019, IEEE J EM SEL TOP C, V9, P465, DOI 10.1109/JETCAS.2019.2934939
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   McCulloch WS., 1943, B MATH BIOPHYS, V5, P115, DOI DOI 10.1007/BF02478259
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Minsky M., 2017, PERCEPTRONS INTRO CO
   Monti F, 2017, PROC CVPR IEEE, P5425, DOI 10.1109/CVPR.2017.576
   Morris RGM, 1999, BRAIN RES BULL, V50, P437, DOI 10.1016/S0361-9230(99)00182-3
   Nair V., 2010, PROC 27 INT C INT C
   Niepert M, 2016, PR MACH LEARN RES, V48
   Olazaran M, 1996, SOC STUD SCI, V26, P611, DOI 10.1177/030631296026003005
   Olmstead K, 2017, NEARLY HALF AM USE D
   Paszke A, 2019, ADV NEUR IN, V32
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Rahimi A, 2017, IEEE T CIRCUITS-I, V64, P2508, DOI 10.1109/TCSI.2017.2705051
   Raina Rajat, 2009, LARGE SCALE DEEP UNS
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   ROSENBLATT F, 1958, PSYCHOL REV, V65, P386, DOI 10.1037/h0042519
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Simonyan K., 2015, 3 INT C LEARNING REP, P1
   Smolensky P, 1986, P 1986 PARALLEL DIST, P194
   Srinivasan G, 2020, INT CONF ACOUST SPEE, P8549, DOI [10.1109/ICASSP40776.2020.9053914, 10.1109/icassp40776.2020.9053914]
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Srivastava R. K., 2015, SEV SAV STAT CAST
   Srivastava RK, 2015, ADV NEUR IN, V28
   Steinkraus D, 2005, PROC INT CONF DOC, P1115, DOI 10.1109/ICDAR.2005.251
   Sze V, 2017, P IEEE, V105, P2295, DOI 10.1109/JPROC.2017.2761740
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Taherkhani A, 2020, IEEE T COGN DEV SYST, V12, P427, DOI 10.1109/TCDS.2019.2909355
   Tong Geng, 2020, 2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO), P922, DOI 10.1109/MICRO50266.2020.00079
   Unnikrishnan N, 2020, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS45731.2020.9181242
   Van Veen F, 2019, NEURAL NETWORK ZOO
   Vaswani A, 2017, ADV NEUR IN, V30
   Velickovic P., 2018, GRAPH ATTENTION NETW, P1
   Vincent P, 2008, APPL COMPUTATIONAL M, P1096, DOI [10.1145/1390156.1390294, DOI 10.1145/1390156.1390294]
   Vinyals O, 2019, NATURE, V575, P350, DOI 10.1038/s41586-019-1724-z
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Xu K, 2015, PR MACH LEARN RES, V37, P2048
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
   Zhou J, 2018, ARTIF CELL NANOMED B, V46, pS1016, DOI 10.1080/21691401.2018.1442841
   Zoph B., 2017, INT C LEARNING REPRE, P1
NR 130
TC 13
Z9 13
U1 2
U2 7
PY 2020
VL 1
BP 185
EP 204
DI 10.1109/OJCAS.2020.3032092
UT WOS:000723367800018
DA 2023-11-16
ER

PT C
AU Dytckov, S
   Purohit, SS
   Daneshtalab, M
   Plosila, J
   Tenhunen, H
AF Dytckov, Sergei
   Purohit, Sushri Sunita
   Daneshtalab, Masoud
   Plosila, Juha
   Tenhunen, Hannu
BE Smari, WW
TI Exploring NoC jitter effect on simulation of spiking neural networks
SO 2014 INTERNATIONAL CONFERENCE ON HIGH PERFORMANCE COMPUTING & SIMULATION
   (HPCS)
DT Proceedings Paper
CT International Conference on High Performance Computing & Simulation
   (HPCS)
CY JUL 21-25, 2014
CL Bologna, ITALY
DE spiking neural networks; self-organizing maps; network-on-chip
ID NEURONS
AB The major bottleneck in simulation of large-scale neural networks is the communication problem due to one-to-many neuron connectivity. Network-on-Chip concept has been proposed to address the problem. This work explores the drawback that is introduced by interconnection networks - a delay jitter. The preliminary experiment is held in the spiking neural network simulator introducing variable communicational delay to the simulation. The performance degradation is reported.
C1 [Dytckov, Sergei; Purohit, Sushri Sunita; Daneshtalab, Masoud; Plosila, Juha; Tenhunen, Hannu] Univ Turku, Dept Informat Technol, SF-20500 Turku, Finland.
   [Daneshtalab, Masoud; Tenhunen, Hannu] Royal Inst Technol, Dept Elect Syst, Stockholm, Sweden.
RP Dytckov, S (corresponding author), Univ Turku, Dept Informat Technol, SF-20500 Turku, Finland.
EM serdyt@utu.fi; susupu@utu.fi; masdan@utu.fi; juplos@utu.fi;
   hannu.tenhunen@utu.fi
CR Bao Jian, 2010, 2010 2nd International Workshop on Education Technology and Computer Science (ETCS), P86, DOI 10.1109/ETCS.2010.448
   Carrillo S, 2013, IEEE T PARALL DISTR, V24, P2451, DOI 10.1109/TPDS.2012.289
   Chen SH, 2004, ADV INFO PROC, P203
   Daneshtalab M, 2012, IEEE T COMPUT AID D, V31, P146, DOI 10.1109/TCAD.2011.2160348
   Fisher RA, 1936, ANN EUGENIC, V7, P179, DOI 10.1111/j.1469-1809.1936.tb02137.x
   Furber SB, 2013, IEEE T COMPUT, V62, P2454, DOI 10.1109/TC.2012.142
   Gerstner W., 2002, SPIKING NEURON MODEL
   Goodman Dan, 2008, Front Neuroinform, V2, P5, DOI 10.3389/neuro.11.005.2008
   Grafova L., 2013, ARTIFICIAL NEURAL NE
   Grossberg S, 2001, NEURAL NETWORKS, V14, P587, DOI 10.1016/S0893-6080(01)00102-2
   HOEHFELD M, 1992, IEEE T NEURAL NETWOR, V3, P602, DOI 10.1109/72.143374
   Hollmen J., 1996, THESIS HELS U TECH H
   Kohonen T., SCHOLARPEDIA, V2, P1568
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Navaridas J, 2010, PROCEEDINGS OF THE 2010 COMPUTING FRONTIERS CONFERENCE (CF 2010), P11, DOI 10.1145/1787275.1787278
   Palesi M, 2014, ROUTING ALGORITHMS N
   Pande S, 2013, PARALLEL COMPUT, V39, P357, DOI 10.1016/j.parco.2013.04.010
   Pheil T., 2012, FRONTIERS NEUROSCIEN, V6
   Plagianakos VP, 2000, IEEE IJCNN, P161, DOI 10.1109/IJCNN.2000.861451
   Plana LA, 2007, IEEE DES TEST COMPUT, V24, P454, DOI 10.1109/MDT.2007.149
   Roclin D., 2013, NEUR NETW IJCNN 2013, P1
   Rumbell T., NEURAL NETWORK UNPUB
   Schemmel J, 2008, IEEE IJCNN, P431, DOI 10.1109/IJCNN.2008.4633828
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Stromatias E., 2011, THESIS U LIVERPOOL L
   Trappenberg T., 2010, FUNDAMENTALS COMPUTA
NR 26
TC 0
Z9 0
U1 0
U2 0
PY 2014
BP 693
EP 696
UT WOS:000361141700094
DA 2023-11-16
ER

PT C
AU Goda, A
   Matsui, C
   Takeuchi, K
AF Goda, Akira
   Matsui, Chihiro
   Takeuchi, Ken
GP IEEE
TI Inter Spike Interval and Stochasticity Engineering of Floating Gate
   Technology-based Neurons for Spiking Neural Network Hardware
SO 6TH IEEE ELECTRON DEVICES TECHNOLOGY AND MANUFACTURING CONFERENCE (EDTM
   2022)
DT Proceedings Paper
CT 6th IEEE Electron Devices Technology and Manufacturing Conference (EDTM)
CY MAR 06-09, 2022
CL ELECTR NETWORK
DE stochastic neuron; floating gate; spiking neural networks; inter spike
   interval
ID MODEL
AB An analytical model has been developed for a stochastic spiking neuron with the floating gate (FG) technology. The electron injection statistics have been applied to the leaky-integrate-and-fire (LIF) neuron model for the first time and inter spike interval (ISI) stochasticity has been quantified. The model has demonstrated low voltage operations below 2V and the ability to fine-tune ISI mean values by 6 orders of magnitude and the degree of ISI stochasticity ranging from 0% to 60%. With the strong interests in the stochastic neurons in spiking neural network (SNN), the ISI stochasticity and tunability of the FG-based neuron have potential to realize efficient and robust spiking neural network hardware.
C1 [Goda, Akira; Matsui, Chihiro; Takeuchi, Ken] Univ Tokyo, Tokyo, Japan.
RP Goda, A (corresponding author), Univ Tokyo, Tokyo, Japan.
EM goda@co-design.t.u-tokyo.ac.jp
CR Agrawal A, 2020, IEEE T VLSI SYST, V28, P2481, DOI 10.1109/TVLSI.2020.2991679
   Chakraborty I, 2020, APPL PHYS REV, V7, DOI 10.1063/1.5113536
   Chatterjee D, 2019, IEEE ELECTR DEVICE L, V40, P1301, DOI 10.1109/LED.2019.2924259
   Chavan T, 2020, IEEE T ELECTRON DEV, V67, P2614, DOI 10.1109/TED.2020.2985167
   Compagnoni CM, 2008, IEEE T ELECTRON DEV, V55, P3192, DOI 10.1109/TED.2008.2003332
   Jeong DS, 2018, J APPL PHYS, V124, DOI 10.1063/1.5042243
   Kornijcuk V., 2016, FRONT NEUROSCI, V10, P1
   Pouget A, 2000, NAT REV NEUROSCI, V1, P125, DOI 10.1038/35039062
   SCHUEGRAF KF, 1994, IEEE T ELECTRON DEV, V41, P761, DOI 10.1109/16.285029
   Tuma T, 2016, NAT NANOTECHNOL, V11, P693, DOI [10.1038/NNANO.2016.70, 10.1038/nnano.2016.70]
   Zambrano D, 2019, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00987
NR 11
TC 2
Z9 2
U1 0
U2 0
PY 2022
BP 129
EP 131
DI 10.1109/EDTM53872.2022.9798349
UT WOS:000852566800042
DA 2023-11-16
ER

PT J
AU Dora, S
   Suresh, S
   Sundararajan, N
AF Dora, Shirin
   Suresh, Sundaram
   Sundararajan, Narasimhan
TI Online Meta-neuron based Learning Algorithm for a spiking neural
   classifier
SO INFORMATION SCIENCES
DT Article
DE Spiking neural networks; Evolving architecture; Meta-neuron; Pattern
   classification; Online learning
ID NETWORKS; PLASTICITY; ASTROCYTES
AB This paper presents a new spiking neural network architecture with a meta-neuron which envelopes all the pre-and postsynaptic neurons in the network. The concept of the meta neuron is inspired by the role of astrocytes in modulating synaptic plasticity in biological neural networks. The meta-neuron utilizes the global information stored in the network (synaptic weights) and the local information present in the input spike pattern to determine a weight sensitivity modulation factor for a given synapse. Based on the weight sensitivity modulation factor and the postsynaptic potential of a neuron, the meta-neuron based learning rule updates the synaptic weights in the network to produce precise shifts in the spike times of the postsynaptic neurons. Using this learning rule, an Online Meta neuron based Learning Algorithm (OMLA) is presented for an evolving spiking neural classifier. The learning algorithm employs heuristic learning strategies for learning each input spike pattern. It can choose to add a neuron, update the network parameters or delete a spike pattern depending on the spike times of the output neurons. OMLA employs a meta neuron with memory that stores only those spike patterns which are used to add a neuron to the network. These spike patterns (spike patterns in meta-neuron memory) are used as representative of past information stored in the network during subsequent neuron additions. The performance of OMLA has been compared with both the existing online learning and batch learning algorithms for spiking neural networks using the UCI machine learning benchmark data sets. The statistical comparison clearly indicates that the OMLA performs better than other existing online learning algorithms for spiking neural networks. Since, OMLA uses both, the global as well as the local information in the network, it is also able to perform better than other batch learning algorithms. (C) 2017 Elsevier Inc. All rights reserved.
C1 [Dora, Shirin; Suresh, Sundaram; Sundararajan, Narasimhan] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore, Singapore.
RP Suresh, S (corresponding author), Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore, Singapore.
EM ssundaram@ntu.edu.sg
CR [Anonymous], 2013, INT JOINT C NEUR NET, DOI DOI 10.1109/AGILE.2013.7
   [Anonymous], 2000, NEURAL ADAPTIVE SYST
   Araque A, 1999, TRENDS NEUROSCI, V22, P208, DOI 10.1016/S0166-2236(98)01349-6
   Babu GS, 2013, IEEE T NEUR NET LEAR, V24, P194, DOI 10.1109/TNNLS.2012.2226748
   BIENENSTOCK EL, 1982, J NEUROSCI, V2, P32, DOI 10.1523/jneurosci.02-01-00032.1982
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Bushong EA, 2002, J NEUROSCI, V22, P183, DOI 10.1523/JNEUROSCI.22-01-00183.2002
   Citri A, 2008, NEUROPSYCHOPHARMACOL, V33, P18, DOI 10.1038/sj.npp.1301559
   Dan Y, 2006, PHYSIOL REV, V86, P1033, DOI 10.1152/physrev.00030.2005
   Dora S, 2016, NEUROCOMPUTING, V171, P1216, DOI 10.1016/j.neucom.2015.07.086
   Dora S, 2015, APPL SOFT COMPUT, V36, P255, DOI 10.1016/j.asoc.2015.06.062
   DUNN OJ, 1959, ANN MATH STAT, V30, P192, DOI 10.1214/aoms/1177706374
   DUNN OJ, 1961, J AM STAT ASSOC, V56, P52, DOI 10.2307/2282330
   Fisher RA, 1921, METRON, V1, P3, DOI DOI 10.1093/BIOMET/9.1-2.22
   Florian RV, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0040233
   Gerstner W, 2002, SPIKING NEURON MODEL, DOI [10.1017/CBO9780511815706, DOI 10.1017/CBO9780511815706]
   Goldberg J, 2002, TRENDS NEUROSCI, V25, P433, DOI 10.1016/S0166-2236(02)02200-2
   Halassa MM, 2007, J NEUROSCI, V27, P6473, DOI 10.1523/JNEUROSCI.1419-07.2007
   Kasabov N, 2015, INFORM SCIENCES, V294, P565, DOI 10.1016/j.ins.2014.06.028
   Kasabov N, 2013, NEURAL NETWORKS, V41, P188, DOI 10.1016/j.neunet.2012.11.014
   Lichman M., 2013, UCI MACHINE LLEARNIN
   Maass W, 1997, NEURAL COMPUT, V9, P279, DOI 10.1162/neco.1997.9.2.279
   Maass W., 1999, TECHNICAL REPORT
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   Perea G, 2009, TRENDS NEUROSCI, V32, P421, DOI 10.1016/j.tins.2009.05.001
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32
   Song T, 2016, INFORM SCIENCES, V372, P380, DOI 10.1016/j.ins.2016.08.055
   Sporea I, 2013, NEURAL COMPUT, V25, P473, DOI 10.1162/NECO_a_00396
   Suresh S, 2003, AEROSP SCI TECHNOL, V7, P595, DOI 10.1016/S1270-9638(03)00053-1
   Suresh S, 2010, NEUROCOMPUTING, V73, P3012, DOI 10.1016/j.neucom.2010.07.003
   Thorpe S, 1998, COMPUTATIONAL NEUROSCIENCE: TRENDS IN RESEARCH, P113
   Van Rullen R, 2001, NEURAL COMPUT, V13, P1255, DOI 10.1162/08997660152002852
   Wade JJ, 2010, IEEE T NEURAL NETWOR, V21, P1817, DOI 10.1109/TNN.2010.2074212
   Wang JL, 2014, NEUROCOMPUTING, V144, P526, DOI 10.1016/j.neucom.2014.04.017
   Wang YH, 2016, ADV NEUR IN, V29, P253
   Wysoski SG, 2010, NEURAL NETWORKS, V23, P819, DOI 10.1016/j.neunet.2010.04.009
NR 37
TC 11
Z9 11
U1 1
U2 25
PD NOV
PY 2017
VL 414
BP 19
EP 32
DI 10.1016/j.ins.2017.05.050
UT WOS:000406574300002
DA 2023-11-16
ER

PT C
AU Zheng, DH
   Lin, XH
   Wang, XW
AF Zheng, Donghao
   Lin, Xianghong
   Wang, Xiangwen
GP IEEE
TI Image Segmentation Method Based on Spiking Neural Network with Adaptive
   Synaptic Weights
SO 2019 IEEE 4TH INTERNATIONAL CONFERENCE ON SIGNAL AND IMAGE PROCESSING
   (ICSIP 2019)
DT Proceedings Paper
CT 4th IEEE International Conference on Signal and Image Processing (ICSIP)
CY JUL 19-21, 2019
CL SE Univ, Wuxi, PEOPLES R CHINA
HO SE Univ
DE spiking neural network; time-to-first-spike encoding; image
   segmentation; synaptic weight adaptation
AB As the third-generation artificial neural network, the spiking neural network is characterized by using an accurate neural model to simulate and transmit spikes in biological neurons. In this paper, we present a new image segmentation method based on spiking neural network with adaptive synaptic weights. Using the time-to-first-spike encoding strategy and the excitability of neurons in a receptive field, we sort the first spike time of neurons. If the neurons in the receptive field are more excitatory than the center neuron, the connection weights are positive numbers. Otherwise, the center neuron of the field inhibits the pixel information input, the connection weights are inhibitory. According to the weight rules of the neurons in the receptive field, we implement synaptic weight adaptation based on spike firing times. In the experiment, maximum Shannon entropy is used to evaluate image segmentation results. The segmentation results of different noise images are shown to verify the effectiveness of the proposed algorithm. Experimental results show that the algorithm has a good performance in noisy image processing.
C1 [Zheng, Donghao; Lin, Xianghong; Wang, Xiangwen] Northwest Normal Univ, Coll Comp Sci & Engn, Lanzhou, Gansu, Peoples R China.
RP Lin, XH (corresponding author), Northwest Normal Univ, Coll Comp Sci & Engn, Lanzhou, Gansu, Peoples R China.
EM zhengdh.dd@foxmail.com; linxh@nwnu.edu.cn; wangxw2015@nwnu.edu.cn
CR Leñero-Bardallo JA, 2010, IEEE T CIRCUITS-I, V57, P2632, DOI 10.1109/TCSI.2010.2046971
   Chaturvedi S, 2012, INDIAN J UROL, V28, P21, DOI 10.4103/0970-1591.94949
   Chaturvedi S, 2013, INT CONF EMERG TR, P191, DOI 10.1109/ICETET.2013.54
   Curto C, 2017, B AM MATH SOC, V54, P63, DOI 10.1090/bull/1554
   Grigonis R, 2017, J PHYSIOL-LONDON, V595, P5843, DOI 10.1113/JP274434
   Jose J. T., 2015, ADV INTELLIGENT INFO, P107, DOI DOI 10.1007/978-3-319-11218-3_11
   Kasabov N. K., 2018, TIME SPACE SPIKING N, V7
   Li XK, 2008, ROUTL EXPLOR ENVIRON, P248
   LIN X., 2015, COMPUTER ENG, P43
   Lin X. H., 2008, ACTA ELECT SINICA, V8, P005
   Meftah B, 2010, NEURAL PROCESS LETT, V32, P131, DOI 10.1007/s11063-010-9149-6
   Naidu M. S. R., 2018, Alexandria Engineering Journal, V57, P1643, DOI 10.1016/j.aej.2017.05.024
   Ranganath H. S., 2018, GSTF J COMPUTING JOC, V1
   Schumann CL, 2019, AIDS BEHAV, V23, P5, DOI 10.1007/s10461-017-1727-4
   Thorpe S, 1996, NATURE, V381, P520, DOI 10.1038/381520a0
   Thorpe SJ, 2012, LECT NOTES COMPUT SC, V7583, P516, DOI 10.1007/978-3-642-33863-2_53
   Thorpe SJ, 1997, ADV NEUR IN, V9, P901
   Vala M., 2013, INT J ADV RES COMPUT, V2, P387, DOI DOI 10.1007/S11548-009-0389-8
   Wang X. W., 2014, P 3 INT C MULT TECHN, P275
   Yang Z, 2019, ARCH COMPUT METHOD E, V26, P491, DOI 10.1007/s11831-018-9253-8
   Zhan Yunjun, 2010, Proceedings of the 2010 Second International Conference on Multimedia and Information Technology (MMIT 2010), P200, DOI 10.1109/MMIT.2010.24
NR 21
TC 4
Z9 4
U1 0
U2 7
PY 2019
BP 1043
EP 1049
DI 10.1109/siprocess.2019.8868851
UT WOS:000557898200204
DA 2023-11-16
ER

PT C
AU Sheik, S
   Chicca, E
   Indiveri, G
AF Sheik, Sadique
   Chicca, Elisabetta
   Indiveri, Giacomo
GP IEEE
TI Exploiting Device Mismatch in Neuromorphic VLSI Systems to Implement
   Axonal Delays
SO 2012 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUN 10-15, 2012
CL Brisbane, AUSTRALIA
ID TIME-DELAY; DYNAMICS; SPIKING; SENSOR
AB Axonal delays are used in neural computation to implement faithful models of biological neural systems, and in spiking neural networks models to solve computationally demanding tasks. While there is an increasing number of software simulations of spiking neural networks that make use of axonal delays, only a small fraction of currently existing hardware neuromorphic systems supports them. In this paper we demonstrate a strategy to implement temporal delays in hardware spiking neural networks distributed across multiple Very Large Scale Integration (VLSI) chips. This is achieved by exploiting the inherent device mismatch present in the analog circuits that implement silicon neurons and synapses inside the chips, and the digital communication infrastructure used to configure the network topology and transmit the spikes across chips. We present an example of a recurrent VLSI spiking neural network that employs axonal delays and demonstrate how the proposed strategy efficiently implements them in hardware.
C1 [Sheik, Sadique; Chicca, Elisabetta; Indiveri, Giacomo] Univ Zurich, Inst Neuroinformat, CH-8006 Zurich, Switzerland.
RP Sheik, S (corresponding author), Univ Zurich, Inst Neuroinformat, CH-8006 Zurich, Switzerland.
EM sadique@ini.phys.ethz.ch
CR [Anonymous], P 45 ANN C INF SCI S
   [Anonymous], VISION CHIPS IMPLEME
   Bartolozzi C, 2007, NEURAL COMPUT, V19, P2581, DOI 10.1162/neco.2007.19.10.2581
   Bartolozzi C, 2009, NEUROCOMPUTING, V72, P726, DOI 10.1016/j.neucom.2008.05.016
   Boahen KA, 2004, IEEE T CIRCUITS-I, V51, P1269, DOI 10.1109/TCSI.2004.830703
   Boahen KA, 2000, IEEE T CIRCUITS-II, V47, P416, DOI 10.1109/82.842110
   BORG J, 1978, J PHYSIOL-LONDON, V277, P143, DOI 10.1113/jphysiol.1978.sp012266
   Cameron K, 2008, IEEE T NEURAL NETWOR, V19, P899, DOI 10.1109/TNN.2007.914192
   CARR CE, 1988, P NATL ACAD SCI USA, V85, P8311, DOI 10.1073/pnas.85.21.8311
   Deiss S., 1994, ADDRESS EVENT ASYNCH
   Dhamala M, 2004, PHYS REV LETT, V92, DOI 10.1103/PhysRevLett.92.028101
   ELIAS JG, 1993, NEURAL COMPUT, V5, P648, DOI 10.1162/neco.1993.5.4.648
   Elias JG, 1998, PULSED NEURAL NETWORKS, P135
   Ermentrout GB, 1998, P NATL ACAD SCI USA, V95, P1259, DOI 10.1073/pnas.95.3.1259
   Harrison RR, 1999, AUTON ROBOT, V7, P211, DOI 10.1023/A:1008916202887
   Indiveri GC, 1999, IEEE T CIRCUITS-II, V46, P1337, DOI 10.1109/82.803473
   Indiveri G, 2011, FRONT ARTIF INTEL AP, V234, P305, DOI 10.3233/978-1-60750-972-1-305
   Izhikevich EM, 2004, CEREB CORTEX, V14, P933, DOI 10.1093/cercor/bhh053
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Liu S.C., 2002, ANALOG VLSI CIRCUITS
   Liu SC, 1999, ANALOG INTEGR CIRC S, V18, P243, DOI 10.1023/A:1008319623922
   Neftci E, 2010, BIOMED CIRC SYST C, P262, DOI 10.1109/BIOCAS.2010.5709621
   Rubinov M, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1002038
   Scholze S, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00117
   Serrano-Gotarredona R., 2005, ADV NEURAL INFORM PR, V15
   Sheik S, 2012, FRONT NEUROSCI-SWITZ, V6, DOI 10.3389/fnins.2012.00017
   Silver R, 2007, J NEUROSCI, V27, P11807, DOI 10.1523/JNEUROSCI.3575-07.2007
   Wang RC, 2011, IEEE INT SYMP CIRC S, P869
   Wang YX, 2010, NEURAL COMPUT, V22, P2086, DOI 10.1162/neco.2010.06-09-1030
NR 29
TC 5
Z9 5
U1 1
U2 1
PY 2012
UT WOS:000309341301136
DA 2023-11-16
ER

PT C
AU Johnson, C
   Venayagamoorthy, GK
   Mitra, P
AF Johnson, Cameron
   Venayagamoorthy, Ganesh K.
   Mitra, Pinaki
GP IEEE
TI Online Identification of Generator Dynamics in a Multimachine Power
   System with a Spiking Neural Network
SO IJCNN: 2009 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS, VOLS 1- 6
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks
CY JUN 14-19, 2009
CL Atlanta, GA
AB This paper presents the application of a spiking neural network for online identification of generator dynamics in a multimachine power system. An integrate and fire model of a spiking neuron is used in this paper where the information is communicated through the interspike intervals. A network of spiking neurons is trained online based on a gradient descent algorithm. Speed and terminal voltage deviations of a generator in the IEEE 10-machine 39-bus New England power system are predicted one time step ahead by a spiking neural network. Two different training conditions are considered, namely, forced and natural perturbations. The simulation results show that a spiking neural network can successfully estimate the speed and terminal voltage deviations for both small and large perturbations applied to a power network.
C1 [Johnson, Cameron; Venayagamoorthy, Ganesh K.; Mitra, Pinaki] Missouri Univ Sci & Technol, Real Time Power & Intelligence Syst Lab, Rolla, MO 65401 USA.
RP Johnson, C (corresponding author), Missouri Univ Sci & Technol, Real Time Power & Intelligence Syst Lab, Rolla, MO 65401 USA.
EM cameron.e.johnson@gmail.com; gkumar@ieee.org; pm33d@mst.edu
CR AZMY M, 2004, JET P GENERATION TRA, V151, P681
   HAN D, 2007, 2007 IEEE INT C CONT, P2396
   IANNELLA N, 1999, P 1999 IEEE SIGN PRO, P139
   MISHRA D, 2007, INT J COMPUTERS SYST, V8, P29
   Park JW, 2005, IEEE T IND ELECTRON, V52, P1685, DOI 10.1109/TIE.2005.858703
   Rowcliffe P, 2008, IEEE T NEURAL NETWOR, V19, P1626, DOI 10.1109/TNN.2008.2000999
   Singh S, 2002, ART NEUR NETW ENG C, P485
   Stankovic AM, 2003, IEEE T POWER SYST, V18, P1478, DOI 10.1109/TPWRS.2003.818704
   Venayagamoorthy GK, 2007, NEURAL NETWORKS, V20, P404, DOI 10.1016/j.neunet.2007.04.021
NR 9
TC 0
Z9 0
U1 0
U2 0
PY 2009
BP 3307
EP 3312
UT WOS:000280591601225
DA 2023-11-16
ER

PT C
AU Lechuga-Gutiérrez, L
   Bayro-Corrochano, E
AF Lechuga-Gutierrez, Luis
   Bayro-Corrochano, Eduardo
BE Cheng, L
   Liu, Q
   Ronzhin, A
TI Quaternion Spike Neural Networks
SO ADVANCES IN NEURAL NETWORKS - ISNN 2016
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 13th International Symposium on Neural Networks (ISNN)
CY JUL 06-08, 2016
CL Saint Petersburg, RUSSIA
DE Spike neural networks; Quaternion; Spikeprop
AB This work presents a new type of Spike Neural Networks (SNN) developed in the quaternion algebra framework. This new neural structure based on SNN is developed using the quaternion algebra. The training algorithm was extended adjusting the weights according to the quaternion multiplication rule, which allows accurate results with a decreased network complexity with respect to the real SNN. The experimental part shows a good performance for robot manipulator control.
C1 [Lechuga-Gutierrez, Luis; Bayro-Corrochano, Eduardo] Ctr Invest & Estudios Avanzados, Dept Control Automat, Unidad Guagalajara, Zapopan, Jalisco, Mexico.
RP Bayro-Corrochano, E (corresponding author), Ctr Invest & Estudios Avanzados, Dept Control Automat, Unidad Guagalajara, Zapopan, Jalisco, Mexico.
EM lrlechuga@gdl.cinvestav.mx; edb@gdl.cinvestav.mx
CR Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Delorme A, 1999, NEUROCOMPUTING, V26-7, P989, DOI 10.1016/S0925-2312(99)00095-8
   Eduardo B.C., 2010, TGEOMETRIC COMPUTING
   Haykin S., 1999, NEURAL NETWORKS COMP
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   WULFRAM G, 2002, SPIKING NEURON MODEL
NR 6
TC 2
Z9 2
U1 0
U2 8
PY 2016
VL 9719
BP 640
EP 646
DI 10.1007/978-3-319-40663-3_73
UT WOS:000386324900073
DA 2023-11-16
ER

PT J
AU Izhikevich, EM
AF Izhikevich, EM
TI Which model to use for cortical spiking neurons?
SO IEEE TRANSACTIONS ON NEURAL NETWORKS
DT Article
DE chaos; Hodgkin-Huxley; pulse-coupled neural network (PCNN); quadratic
   integrate-and-fire (I&F); spike-timing
ID NEURAL EXCITABILITY; NEOCORTICAL NEURONS; DYNAMICS; NETWORKS;
   OSCILLATIONS
AB We discuss the biological plausibility and computational efficiency of some of the most useful models of spiking and bursting neurons. We compare their applicability to large-scale simulations of cortical neural networks.
C1 Inst Neurosci, San Diego, CA 92121 USA.
RP Izhikevich, EM (corresponding author), Inst Neurosci, San Diego, CA 92121 USA.
EM Eugene.Izhikevich@nsi.edu
CR [Anonymous], 1996, NEURAL COMPUT
   CONNORS BW, 1990, TRENDS NEUROSCI, V13, P99, DOI 10.1016/0166-2236(90)90185-D
   ERMENTROUT GB, 1986, SIAM J APPL MATH, V46, P233, DOI 10.1137/0146017
   FITZHUGH R, 1961, BIOPHYS J, V1, P445, DOI 10.1016/S0006-3495(61)86902-6
   Gerstner W., 2002, SPIKING NEURON MODEL
   Gibson JR, 1999, NATURE, V402, P75, DOI 10.1038/47035
   Gray CM, 1996, SCIENCE, V274, P109, DOI 10.1126/science.274.5284.109
   HODGKIN AL, 1948, J PHYSIOL-LONDON, V107, P165, DOI 10.1113/jphysiol.1948.sp004260
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Hoppensteadt FC, 1997, WEAKLY CONNECTED NEU
   Izhikevich EM, 2000, INT J BIFURCAT CHAOS, V10, P1171, DOI 10.1142/S0218127400000840
   Izhikevich EM, 2001, NEURAL NETWORKS, V14, P883, DOI 10.1016/S0893-6080(01)00078-8
   Izhikevich EM, 1999, IEEE T NEURAL NETWOR, V10, P499, DOI 10.1109/72.761707
   Izhikevich EM, 2004, CEREB CORTEX, V14, P933, DOI 10.1093/cercor/bhh053
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2003, TRENDS NEUROSCI, V26, P161, DOI 10.1016/S0166-2236(03)00034-1
   IZHIKEVICH EM, IN PRESS IEEE T NEUR
   IZHIKEVICH EM, IN PRESS DYNAMICAL S
   Latham PE, 2000, J NEUROPHYSIOL, V83, P808, DOI 10.1152/jn.2000.83.2.808
   Lismont L, 1997, THEOR DEC C, V20, P3
   MORRIS C, 1981, BIOPHYS J, V35, P193, DOI 10.1016/S0006-3495(81)84782-0
   Rinzel J., 1989, METHODS NEURONAL MOD, P251
   ROSE RM, 1989, PROC R SOC SER B-BIO, V237, P267, DOI 10.1098/rspb.1989.0049
   Smith GD, 2000, J NEUROPHYSIOL, V83, P588, DOI 10.1152/jn.2000.83.1.588
   Wilson HR, 1999, J THEOR BIOL, V200, P375, DOI 10.1006/jtbi.1999.1002
NR 25
TC 1578
Z9 1662
U1 42
U2 213
PD SEP
PY 2004
VL 15
IS 5
BP 1063
EP 1070
DI 10.1109/TNN.2004.832719
UT WOS:000223798400013
DA 2023-11-16
ER

PT C
AU Sadovsky, E
   Jakubec, M
   Jarina, R
AF Sadovsky, Erik
   Jakubec, Maros
   Jarina, Roman
BE Pidanic, J
TI Speech Command Recognition Based on Convolutional Spiking Neural
   Networks
SO 2023 33RD INTERNATIONAL CONFERENCE RADIOELEKTRONIKA, RADIOELEKTRONIKA
DT Proceedings Paper
CT 33rd International Conference on Radioelektronika (RADIOELEKTRONIKA)
CY APR 19-20, 2023
CL Pardubice, CZECH REPUBLIC
DE spiking neural network; spiking speech commands; command recognition;
   convolutional spiking neural network
AB This article presents a new technique for speech recognition that combines Convolutional Neural Networks (CNNs) with Spiking Neural Networks (SNNs) to create an SNN-CNN model. The model is tested on the Google Speech Command Dataset and achieves an accuracy of 72.03%, which is similar to the current state-of-the-art speech recognition methods. The study also compares the performance of the SNN-CNN model with other SNN models that use Multi-Layer Perceptrons (MLPs) and traditional Artificial Neural Networks (ANNs). The results show that the CNN-based SNNs outperform both MLPs and ANNs, demonstrating the superiority of the proposed model. The approach presented in this study can potentially be applied to other speech recognition tasks and could lead to further improvements in the field.
C1 [Sadovsky, Erik; Jakubec, Maros; Jarina, Roman] Univ Zilina, Dept Multimedia & Informat Commun Technol, FEIT, Zilina, Slovakia.
RP Sadovsky, E (corresponding author), Univ Zilina, Dept Multimedia & Informat Commun Technol, FEIT, Zilina, Slovakia.
EM erik.sadovsky@uniza.sk
CR Bellec G, 2018, Arxiv, DOI arXiv:1803.09574
   Bittar A, 2022, FRONT NEUROSCI-SWITZ, V16, DOI 10.3389/fnins.2022.865897
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Caporale N, 2008, ANNU REV NEUROSCI, V31, P25, DOI 10.1146/annurev.neuro.31.060407.125639
   Cramer B, 2022, IEEE T NEUR NET LEAR, V33, P2744, DOI 10.1109/TNNLS.2020.3044364
   Diehl PU, 2015, IEEE IJCNN
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Holmberg M., 2005, INTERSPEECH 2005, P1253, DOI [10.21437/Interspeech.2005-480, DOI 10.21437/INTERSPEECH.2005-480]
   Johnson C, 2009, NEURAL NETWORKS, V22, P833, DOI 10.1016/j.neunet.2009.06.033
   Kröger BJ, 2009, SPEECH COMMUN, V51, P793, DOI 10.1016/j.specom.2008.08.002
   Liaw JS, 1996, HIPPOCAMPUS, V6, P591
   Loiselle S, 2005, IEEE IJCNN, P2076
   Markowska-Kaczmar U, 2015, SOFT COMPUT, V19, P3465, DOI 10.1007/s00500-014-1515-2
   Näger C, 2002, NEUROCOMPUTING, V44, P937, DOI 10.1016/S0925-2312(02)00494-0
   Nahmias MA, 2013, IEEE J SEL TOP QUANT, V19, DOI 10.1109/JSTQE.2013.2257700
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Pan LQ, 2017, INT J NEURAL SYST, V27, DOI 10.1142/S0129065717500423
   Perez-Nieves N, 2021, NAT COMMUN, V12, DOI 10.1038/s41467-021-26022-3
   Rybakov O, 2020, INTERSPEECH, P2277, DOI 10.21437/Interspeech.2020-1003
   Shrestha SB, 2018, ADV NEUR IN, V31
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Tavanaei A, 2019, NEUROCOMPUTING, V330, P39, DOI 10.1016/j.neucom.2018.11.014
   Warden P, 2018, Arxiv, DOI arXiv:1804.03209
   Wu JB, 2023, IEEE T NEUR NET LEAR, V34, P446, DOI 10.1109/TNNLS.2021.3095724
   Wu JB, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00199
   Wu JB, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00836
   Wu YJ, 2019, AAAI CONF ARTIF INTE, P1311
   Yin BJ, 2021, Arxiv, DOI arXiv:2103.12593
   Zhang WR, 2019, ADV NEUR IN, V32
   Zhang Y, 2015, IEEE T NEUR NET LEAR, V26, P2635, DOI 10.1109/TNNLS.2015.2388544
   Zhang YD, 2018, Arxiv, DOI [arXiv:1711.07128, DOI 10.48550/ARXIV.1711.07128]
NR 31
TC 1
Z9 1
U1 4
U2 4
PY 2023
DI 10.1109/RADIOELEKTRONIKA57919.2023.10109082
UT WOS:000990505700055
DA 2023-11-16
ER

PT C
AU Krunglevicius, D
AF Krunglevicius, Dalius
BE Madani, K
   Kacprzyk, J
   Filipe, J
TI NEURAL PROCESSING OF LONG LASTING SEQUENCES OF TEMPORAL CODES <i>Model
   of Artificial Neural Network based on a Spike Timing</i>-<i>dependant
   Learning Rule</i>
SO NCTA 2011: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON NEURAL
   COMPUTATION THEORY AND APPLICATIONS
DT Proceedings Paper
CT International Conference on Neural Computation Theory and Applications
CY OCT 24-26, 2011
CL Univ Paris Est Creteil, Paris, FRANCE
HO Univ Paris Est Creteil
DE Artificial neural networks; Spike timing-dependent plasticity; STDP;
   Hebbian learning; Temporal coding; Neuroscience
ID SYNAPTIC PLASTICITY; VISUAL-CORTEX; PATTERNS; CELLS; RESPONSES; NEURONS;
   STDP
AB It has been demonstrated, that spike-timing-dependent plasticity (STDP) learning rule can be applied to train neuron to become selective to a spatiotemporal spike pattern. In this paper, we propose a model of neural network that is capable of memorizing prolonged sequences of different spike patterns and learn aggregated data in a larger temporal window.
C1 [Krunglevicius, Dalius] Vilnius Univ, Fac Math & Informat, Naugarduko 24, LT-03225 Vilnius, Lithuania.
RP Krunglevicius, D (corresponding author), Vilnius Univ, Fac Math & Informat, Naugarduko 24, LT-03225 Vilnius, Lithuania.
EM dalius.krunglevicius@gmail.com
CR Abbott LF, 2000, NAT NEUROSCI, V3, P1178, DOI 10.1038/81453
   Abraham WC, 2003, PHILOS T R SOC B, V358, P735, DOI 10.1098/rstb.2002.1222
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   BIENENSTOCK EL, 1982, J NEUROSCI, V2, P32, DOI 10.1523/jneurosci.02-01-00032.1982
   Caporale N, 2008, ANNU REV NEUROSCI, V31, P25, DOI 10.1146/annurev.neuro.31.060407.125639
   Cardin JA, 2009, NATURE, V459, P663, DOI 10.1038/nature08002
   Carpenter G. A., 2009, CASCNSTR2009008
   Feldman DE, 2000, NEURON, V27, P45, DOI 10.1016/S0896-6273(00)00008-8
   Fellous JM, 2004, J NEUROSCI, V24, P2989, DOI 10.1523/JNEUROSCI.4649-03.2004
   Gerstner W, 1996, NATURE, V383, P76, DOI 10.1038/383076a0
   Gerstner W., 2002, SPIKING NEURON MODEL
   Guyonneau R, 2005, NEURAL COMPUT, V17, P859, DOI 10.1162/0899766053429390
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Kayser C, 2009, NEURON, V61, P597, DOI 10.1016/j.neuron.2009.01.008
   Masquelier T, 2008, PLOS ONE, V3, DOI 10.1371/journal.pone.0001377
   Masquelier T, 2009, NEURAL COMPUT, V21, P1259, DOI 10.1162/neco.2008.06-08-804
   Pfister JP, 2006, J NEUROSCI, V26, P9673, DOI 10.1523/JNEUROSCI.1425-06.2006
   Prut Y, 1998, J NEUROPHYSIOL, V79, P2857, DOI 10.1152/jn.1998.79.6.2857
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Van Rullen R, 2001, NEURAL COMPUT, V13, P1255, DOI 10.1162/08997660152002852
   VanRullen R, 2005, TRENDS NEUROSCI, V28, P1, DOI 10.1016/j.tins.2004.10.010
   Woodin MA, 2003, NEURON, V39, P807, DOI 10.1016/S0896-6273(03)00507-5
NR 22
TC 0
Z9 0
U1 0
U2 1
PY 2011
BP 196
EP 204
UT WOS:000392350600032
DA 2023-11-16
ER

PT C
AU Dominguez-Morales, JP
   Liu, Q
   James, R
   Gutierrez-Galan, D
   Jimenez-Fernandez, A
   Davidson, S
   Furber, S
AF Dominguez-Morales, Juan P.
   Liu, Qian
   James, Robert
   Gutierrez-Galan, Daniel
   Jimenez-Fernandez, Angel
   Davidson, Simon
   Furber, Steve
GP IEEE
TI Deep Spiking Neural Network model for time-variant signals
   classification: a real-time speech recognition approach
SO 2018 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUL 08-13, 2018
CL Rio de Janeiro, BRAZIL
DE speech recognition; audio processing; Spiking Neural Networks;
   Convolutional Neural Networks; neuromorphic hardware; deep learning
ID FPGA
AB Speech recognition has become an important task to improve the human-machine interface. Taking into account the limitations of current automatic speech recognition systems, like non-real time cloud-based solutions or power demand, recent interest for neural networks and bio-inspired systems has motivated the implementation of new techniques.
   Among them, a combination of spiking neural networks and neuromorphic auditory sensors offer an alternative to carry out the human-like speech processing task. In this approach, a spiking convolutional neural network model was implemented, in which the weights of connections were calculated by training a convolutional neural network with specific activation functions, using firing rate-based static images with the spiking information obtained from a neuromorphic cochlea.
   The system was trained and tested with a large dataset that contains "left" and "right" speech commands, achieving 89.90% accuracy. A novel spiking neural network model has been proposed to adapt the network that has been trained with static images to a non-static processing approach, making it possible to classify audio signals and time series in real time.
C1 [Dominguez-Morales, Juan P.; Gutierrez-Galan, Daniel; Jimenez-Fernandez, Angel] Univ Seville, Robot & Comp Technol Lab, Seville, Spain.
   [Liu, Qian; James, Robert; Davidson, Simon; Furber, Steve] Univ Manchester, Sch Comp Sci, Adv Processor Technol Grp, Manchester, Lancs, England.
RP Dominguez-Morales, JP (corresponding author), Univ Seville, Robot & Comp Technol Lab, Seville, Spain.
EM jpdominguez@atc.us.es
CR [Anonymous], 2016, ARXIV161101421
   [Anonymous], 2016, WAV2LETTER END TO EN
   Berner R, 2007, IEEE INT SYMP CIRC S, P2451, DOI 10.1109/ISCAS.2007.378616
   Chowdhury GG, 2003, ANNU REV INFORM SCI, V37, P51, DOI 10.1002/aris.1440370103
   Cooper Robert S, 2004, US Patent, Patent No. [6,757,362, 6757362]
   Diehl PU, 2014, IEEE IJCNN, P4288, DOI 10.1109/IJCNN.2014.6889876
   Domínguez JI, 2017, CONT INTERAMER REL, P1
   Dominguez-Morales JP, 2018, IEEE T BIOMED CIRC S, V12, P24, DOI 10.1109/TBCAS.2017.2751545
   Dominguez-Morales JP, 2017, NEUROCOMPUTING, V237, P418, DOI 10.1016/j.neucom.2016.12.046
   Farabet C, 2012, FRONT NEUROSCI-SWITZ, V6, DOI 10.3389/fnins.2012.00032
   FURUI S, 1986, IEEE T ACOUST SPEECH, V34, P52, DOI 10.1109/TASSP.1986.1164788
   Gambin Isabel, 2010, Proceedings of the 2010 17th IEEE International Conference on Electronics, Circuits and Systems (ICECS 2010), P946, DOI 10.1109/ICECS.2010.5724669
   Gazdar G., 1989, NATURAL LANGUAGE PRO
   HAGAN MT, 1994, IEEE T NEURAL NETWOR, V5, P989, DOI 10.1109/72.329697
   Hamilton TJ, 2008, IEEE T BIOMED CIRC S, V2, P30, DOI 10.1109/TBCAS.2008.921602
   Hu ZH, 2017, LECT NOTES COMPUT SC, V10635, P92, DOI 10.1007/978-3-319-70096-0_10
   Huang X., 2001, SPOKEN LANGUAGE PROC, V95
   Iakymchuk T, 2014, IEEE INT SYMP CIRC S, P1556, DOI 10.1109/ISCAS.2014.6865445
   Iakymchuk T, 2015, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-015-0059-4
   Jiménez-Fernández A, 2017, IEEE T NEUR NET LEAR, V28, P804, DOI 10.1109/TNNLS.2016.2583223
   Leong MP, 2003, EURASIP J APPL SIG P, V2003, P629, DOI 10.1155/S1110865703303038
   Liu Q., 2017, ARXIV170603609
   Liu Q, 2016, LECT NOTES COMPUT SC, V9950, P405, DOI 10.1007/978-3-319-46681-1_49
   Liu SC, 2010, IEEE INT SYMP CIRC S, P2027, DOI 10.1109/ISCAS.2010.5537164
   McGuire P, 2002, 2002 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-3, PROCEEDINGS, P1082, DOI 10.1109/IRDS.2002.1043875
   Mishra A, 2002, IEEE T CIRCUITS-II, V49, P54, DOI 10.1109/82.996059
   Moraitis T, 2017, IEEE IJCNN, P1823, DOI 10.1109/IJCNN.2017.7966072
   Oord A. v. d., 2016, ARXIV160903499
   Painkras E., IEEE J SOLID STATE C, V48, P1943
   Pavlidis NG, 2005, IEEE IJCNN, P2190
   Dominguez-Morales JP, 2016, LECT NOTES COMPUT SC, V9886, P45, DOI 10.1007/978-3-319-44778-0_6
   Perez-Pena F., 2017, CIRC SYST ISCAS 2017, P1, DOI [10.1109/ISCAS.2017.8050808, DOI 10.1109/ISCAS.2017.8050808]
   Plana L., SPI O LIB FPGA DESIG
   Russakovsky O, 2015, PROC CVPR IEEE, P2121, DOI 10.1109/CVPR.2015.7298824
   Summerfield C. D., 1992, ICASSP-92: 1992 IEEE International Conference on Acoustics, Speech and Signal Processing (Cat. No.92CH3103-9), P673, DOI 10.1109/ICASSP.1992.226506
   Wen B, 2009, IEEE T BIOMED CIRC S, V3, P444, DOI 10.1109/TBCAS.2009.2027127
   Williams G., 1997, CONFIDENCE MEASURES
NR 37
TC 0
Z9 0
U1 0
U2 2
PY 2018
UT WOS:000585967404052
DA 2023-11-16
ER

PT C
AU Wijekoon, JHB
   Dudek, P
AF Wijekoon, Jayawan H. B.
   Dudek, Piotr
GP IEEE
TI Heterogeneous Neurons and Plastic Synapses in a Reconfigurable Cortical
   Neural Network IC
SO 2012 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS (ISCAS 2012)
SE IEEE International Symposium on Circuits and Systems
DT Proceedings Paper
CT IEEE International Symposium on Circuits and Systems
CY MAY 20-23, 2012
CL Seoul, SOUTH KOREA
ID SYNAPTIC PLASTICITY; SPIKING
AB This paper presents an analogue VLSI circuit intended to be used in a neural network architecture that closely resembles the small-scale laminar micro-circuits of the neocortex. The Cortical Neural Layer (CNL) chip comprises of 120 reconfigurable cortical neurons and 7,560 synapses. The neurons can be configured to produce regular spiking, fast spiking, chattering, intrinsically bursting, and other complex activity patterns. The synaptic circuits include inhibitory/ excitatory, facilitating/depressing and spike-time dependent plasticity (STDP) dynamics. The connectivity of the neural network can be configured using off-chip spike-routing and on-chip axonal arbor connections. A pre-synaptic spike can be sent to a group of crossbar synapses simultaneously, reducing latency in the pre-synaptic spike routing, enabling a high degree of connectivity of the neural network. The device is fabricated in a 0.35 mu m CMOS technology and on-chip neural dynamics are experimentally verified.
C1 [Wijekoon, Jayawan H. B.; Dudek, Piotr] Univ Manchester, Sch Elect & Elect Engn, Manchester M13 9PL, Lancs, England.
RP Wijekoon, JHB (corresponding author), Univ Manchester, Sch Elect & Elect Engn, Manchester M13 9PL, Lancs, England.
EM jayawan@ieee.org; p.dudek@manchester.ac.uk
CR Abbott LF, 2000, NAT NEUROSCI, V3, P1178, DOI 10.1038/81453
   Abbott LF, 1997, SCIENCE, V275, P220, DOI 10.1126/science.275.5297.221
   Giulioni M., 2008, 2008 15th IEEE International Conference on Electronics, Circuits and Systems (ICECS 2008), P678, DOI 10.1109/ICECS.2008.4674944
   Indiveri G., 2003, ADV NEURAL INFORM PR
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Markram H, 2006, NAT REV NEUROSCI, V7, P153, DOI 10.1038/nrn1848
   Merolla P., 2011, PROCEEDINGS OF THE I
   Schemmel J, 2008, IEEE IJCNN, P431, DOI 10.1109/IJCNN.2008.4633828
   Tsodyks M, 2002, TRENDS NEUROSCI, V25, P599, DOI 10.1016/S0166-2236(02)02294-4
   Vogelstein RJ, 2007, IEEE T NEURAL NETWOR, V18, P253, DOI 10.1109/TNN.2006.883007
   Wijekoon JHB, 2008, NEURAL NETWORKS, V21, P524, DOI 10.1016/j.neunet.2007.12.037
NR 11
TC 0
Z9 0
U1 0
U2 0
PY 2012
BP 2417
EP 2420
UT WOS:000316903702155
DA 2023-11-16
ER

PT C
AU Yan, YL
   Chu, HM
   Chen, X
   Jin, Y
   Huan, YX
   Zheng, LR
   Zou, Z
AF Yan, Yulong
   Chu, Haoming
   Chen, Xin
   Jin, Yi
   Huan, Yuxiang
   Zheng, Lirong
   Zou, Zhuo
GP IEEE
TI Graph-Based Spatio-Temporal Backpropagation for Training Spiking Neural
   Networks
SO 2021 IEEE 3RD INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE
   CIRCUITS AND SYSTEMS (AICAS)
DT Proceedings Paper
CT IEEE 3rd International Conference on Artificial Intelligence Circuits
   and Systems (AICAS)
CY JUN 06-09, 2021
CL ELECTR NETWORK
DE spiking neural network (SNN); spike sparsity; graph-based
   spatio-temporal backpropagation (G-STBP); leaky integrate-and-fire
   (LIF); recurrent network
AB Dedicated hardware for spiking neural networks (SNN) reduces energy consumption with spike-driven computing. This paper proposes a graph-based spatio-temporal backpropagation (G-STBP) to train SNN, aiming to enhance spike sparsity for energy efficiency, while ensuring the accuracy. A differentiable leaky integrate-and-fire (LIF) model is suggested to establish the backpropagation path. The sparse regularization is proposed to reduce the spike firing rate with a guaranteed accuracy. G-STBP enables training in any network topologies thanks to graph representation. A recurrent network is demonstrated with spike-sparse rank order coding. The experimental result on rank order coded MNIST shows that the recurrent SNN trained by G-STBP achieves the accuracy of 97.3% using 392 spikes per inference.
C1 [Yan, Yulong; Chu, Haoming; Chen, Xin; Jin, Yi; Huan, Yuxiang; Zheng, Lirong; Zou, Zhuo] Fudan Univ, Sch Informat Sci & Technol, State Key Lab ASIC & Syst, Shanghai, Peoples R China.
RP Yan, YL (corresponding author), Fudan Univ, Sch Informat Sci & Technol, State Key Lab ASIC & Syst, Shanghai, Peoples R China.
EM ylyan17@fudan.edu.cn; hmchu19@fudan.edu.cn; xin_chen20@fudan.edu.cn;
   yijin18@fudan.edu.cn; yxhuan@fudan.edu.cn; lrzheng@fudan.edu.cn;
   zhuo@fudan.edu.cn
CR Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   Bear M.F., 2020, NEUROSCIENCE EXPLORI
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Diehl PU, 2015, IEEE IJCNN
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Dragoi G, 2020, CURR OPIN NEUROBIOL, V64, P111, DOI 10.1016/j.conb.2020.03.003
   King DB, 2015, ACS SYM SER, V1214, P1
   Pei J, 2019, NATURE, V572, P106, DOI 10.1038/s41586-019-1424-8
   Tang H, 2020, NEUROCOMPUTING, V407, P300, DOI 10.1016/j.neucom.2020.05.031
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
NR 10
TC 8
Z9 8
U1 3
U2 10
PY 2021
DI 10.1109/AICAS51828.2021.9458461
UT WOS:000722241000027
DA 2023-11-16
ER

PT J
AU Reid, D
   Hussain, AJ
   Tawfik, H
AF Reid, David
   Hussain, Abir Jaafar
   Tawfik, Hissam
TI Financial Time Series Prediction Using Spiking Neural Networks
SO PLOS ONE
DT Article
ID NONSTATIONARY; ALGORITHMS; MODEL
AB In this paper a novel application of a particular type of spiking neural network, a Polychronous Spiking Network, was used for financial time series prediction. It is argued that the inherent temporal capabilities of this type of network are suited to non-stationary data such as this. The performance of the spiking neural network was benchmarked against three systems: two "traditional", rate-encoded, neural networks; a Multi-Layer Perceptron neural network and a Dynamic Ridge Polynomial neural network, and a standard Linear Predictor Coefficients model. For this comparison three non-stationary and noisy time series were used: IBM stock data; US/Euro exchange rate data, and the price of Brent crude oil. The experiments demonstrated favourable prediction results for the Spiking Neural Network in terms of Annualised Return and prediction error for 5-Step ahead predictions. These results were also supported by other relevant metrics such as Maximum Drawdown and Signal-To-Noise ratio. This work demonstrated the applicability of the Polychronous Spiking Network to financial data forecasting and this in turn indicates the potential of using such networks over traditional systems in difficult to manage non-stationary environments.
C1 [Reid, David; Hussain, Abir Jaafar; Tawfik, Hissam] Liverpool Hope Univ, Dept Math & Comp Sci, Liverpool, Merseyside, England.
   [Reid, David; Hussain, Abir Jaafar; Tawfik, Hissam] Liverpool John Moores Univ, Sch Comp & Math Sci, Liverpool L3 5UX, Merseyside, England.
RP Reid, D (corresponding author), Liverpool Hope Univ, Dept Math & Comp Sci, Liverpool, Merseyside, England.
EM reidd@hope.ac.uk
CR Abraham A, 2001, LECT NOTES COMPUT SC, V2074, P337
   Abu-Mostafa YS, 2001, IEEE T NEURAL NETWOR, V12, P653, DOI 10.1109/TNN.2001.935079
   Allen F, 1999, J FINANC ECON, V51, P245, DOI 10.1016/S0304-405X(98)00052-X
   [Anonymous], TIME SERIES DATA LIB
   [Anonymous], DERIVATIVES USE TRAD, DOI DOI 10.1002/F0R.935
   [Anonymous], WILEY SERIES PROBABI
   [Anonymous], 1998, PULSED NEURAL NETWOR
   Araújo RD, 2009, NEUROCOMPUTING, V72, P2507, DOI 10.1016/j.neucom.2008.11.008
   Boer K, 2007, COMPUT INTELL, V23, P142, DOI 10.1111/j.1467-8640.2007.00302.x
   Box G. E., 1994, TIME SERIES ANAL FOR
   Box G. E. P., 1970, Time series analysis, forecasting and control
   Cao LJ, 2003, IEEE T NEURAL NETWOR, V14, P1506, DOI 10.1109/TNN.2003.820556
   Conlisk J, 1996, J ECON LIT, V34, P669
   Czanner G, 2005, P 46 IEEE C DEC CONT, P5812
   Daley D.J., 2002, INTRO THEORY POINT P, VI
   Deco G, 1997, NEURAL NETWORKS, V10, P401, DOI 10.1016/S0893-6080(96)00108-6
   Dunis C., 1999, Neural Network World, V9, P193
   Edelman G. M., 1987, NEURAL DARWINISM THE
   Eden UT, 2004, NEURAL COMPUT, V16, P971, DOI 10.1162/089976604773135069
   Feng HM, 2011, EXPERT SYST APPL, V38, P8285, DOI 10.1016/j.eswa.2011.01.009
   Ganatr A, 2010, INT J COMPUTER THEOR, V2, P1793
   Gerstner W, 1996, NEURAL COMPUT, V8, P1653, DOI 10.1162/neco.1996.8.8.1653
   Ghazali R, 2008, NEURAL COMPUT APPL, V17, P311, DOI 10.1007/s00521-007-0132-8
   Ghazali R, 2009, NEUROCOMPUTING, V72, P2359, DOI 10.1016/j.neucom.2008.12.005
   GILES CL, 1987, APPL OPTICS, V26, P4972, DOI 10.1364/AO.26.004972
   Glackin C, 2009, THESIS U ULSTER
   HAYKIN S, 1995, IEEE T SIGNAL PROCES, V43, P526, DOI 10.1109/78.348134
   Hellstrom T, 1997, IMATOM199707 MAL U C
   Horvatic D, 2011, EPL-EUROPHYS LETT, V94, DOI 10.1209/0295-5075/94/18007
   Hussain AJ, 2006, IEEE INT C INN INF T
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2009, INT J BIFURCAT CHAOS, V19, P1733, DOI 10.1142/S0218127409023809
   Jiang H, 2012, EXPERT SYST APPL, V39, P2256, DOI 10.1016/j.eswa.2011.07.100
   Johnson C, 2010, IEEE WORLD C COMP IN
   Kaastra I, 1996, NEUROCOMPUTING, V10, P215, DOI 10.1016/0925-2312(95)00039-9
   Kantelhardt JW, 2002, PHYSICA A, V316, P87, DOI 10.1016/S0378-4371(02)01383-3
   Kenett Dror Y., 2012, International Journal of Modern Physics: Conference Series, V16, P13, DOI 10.1142/S201019451200774X
   Kenett DY, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0019378
   Kim TY, 2004, NEUROCOMPUTING, V61, P439, DOI 10.1016/j.neucom.2004.04.002
   Knowles AC, 2005, WORKSH FOR FIN MARK
   Lawrence S, 2000, IEEE IJCNN, P114, DOI 10.1109/IJCNN.2000.857823
   Legenstein R, 2005, NEURAL COMPUT, V17, P2337, DOI 10.1162/0899766054796888
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Magdon-Ismail M, 1998, P IEEE, V86, P2184, DOI 10.1109/5.726786
   Montgomery D. C., 2014, APPL STAT PROBABILIT
   Natschlager T., 2002, SPECIAL ISSUE FDN IN, P39, DOI [DOI 10.1017/CBO9781107415324.004, 10.1017/CBO9781107415324.004]
   Nessler B., 2010, ADV NEURAL INFORM PR, V22, P1357
   Pao Y.-H., 1989, ADAPTIVE PATTERN REC
   Plummer EA, 2000, THESIS U WYOMING
   Schwaerzel R, 1996, THESIS U TEXAS
   Sharma V., 2010, P 2010 INT JOINT C N, P1
   Thomas J, 1999, P GECCO 2000 WORKSH
   Thomason MR, 1998, J COMPUTATIONAL INTE, V7, P36
   Victor JD, 1997, NETWORK-COMP NEURAL, V8, P127, DOI 10.1088/0954-898X/8/2/003
   Wall JA, 2012, IEEE T NEUR NET LEAR, V23, P574, DOI 10.1109/TNNLS.2011.2178317
   White H., 1988, IEEE International Conference on Neural Networks (IEEE Cat. No.88CH2632-8), P451, DOI 10.1109/ICNN.1988.23959
   Wong C, 2012, NEURAL COMPUTING APP
   Yao JT, 2000, NEUROCOMPUTING, V34, P79, DOI 10.1016/S0925-2312(00)00300-3
   ySitte J, 2000, IEEE T SYSTEMS MAN C, V30
NR 60
TC 14
Z9 15
U1 0
U2 27
PD AUG 29
PY 2014
VL 9
IS 8
AR e103656
DI 10.1371/journal.pone.0103656
UT WOS:000341127500013
DA 2023-11-16
ER

PT C
AU Bidin, J
   Amin, MKM
AF Bidin, Junaidi
   Amin, Muhamad Kamal M.
BE Xiao, TY
   Zhang, L
   Fei, M
TI Towards a Biological More Plausible Artificial Neural Networks
SO ASIASIM 2012, PT II
SE Communications in Computer and Information Science
DT Proceedings Paper
CT Asia Simulation Conference/International Conference on System Simulation
   and Scientific Computing (AsiaSim and ICSC 2012)
CY OCT 27-30, 2012
CL Shanghai, PEOPLES R CHINA
DE Spiking Neural Network(SNN); Self-organized
AB This paper presents a simulation of a biological more plausible neural network system. The system modeled a Spiking Neural Network for self-organized architecture. Recently, Spiking Neural Networks have been much considered in an attempt to achieve a more biologically realistic neural network which was coined as the third generation Artificial Neural Networks. Spiking neurons with delays to encode the information is suggested. Thus, each output node will produce a different timing which enables competitive learning. The suggested mechanism is designed and analyzed to perform self-organizing learning and preserve the inputs topology. The simulation results show that the model is feasible to perform a self-organized unsupervised learning. The mechanism is further assessed in real-world dataset for data clustering problem.
C1 [Bidin, Junaidi; Amin, Muhamad Kamal M.] Univ Technol Malaysia Kuala Lumpur Campus, Malaysia Japan Int Inst Technol, Grad Sch Elect Syst Engn, Kuala Lumpur, Malaysia.
RP Bidin, J (corresponding author), Univ Technol Malaysia Kuala Lumpur Campus, Malaysia Japan Int Inst Technol, Grad Sch Elect Syst Engn, Kuala Lumpur, Malaysia.
EM junaidi.bidin@gmail.com; m_kamal@ic.utm.my
CR Gerstner W., 2002, SPIKING NEURON MODEL
   Kohonen T, 2001, SELF ORG MAPS, P501, DOI [10.1007/978-3-642-56927-2, DOI 10.1007/978-3-642-56927-2]
   Maass W, 1998, PULSED NEURAL NETWORKS, P55
   PANCHEV C, 2001, P WORLD C NEUR
   Ruf B, 1998, IEEE T NEURAL NETWOR, V9, P575, DOI 10.1109/72.668899
   Sala D. M., 1998, Australian Journal of Intelligent Information Processing Systems, V5, P161
NR 6
TC 0
Z9 0
U1 0
U2 1
PY 2012
VL 324
BP 169
EP 176
UT WOS:000313765600020
DA 2023-11-16
ER

PT J
AU Zambrano, D
   Nusselder, R
   Scholte, HS
   Bohté, SM
AF Zambrano, Davide
   Nusselder, Roeland
   Scholte, H. Steven
   Bohte, Sander M.
TI Sparse Computation in Adaptive Spiking Neural Networks
SO FRONTIERS IN NEUROSCIENCE
DT Article
DE spiking neural networks; neural coding; adaptive spiking neurons;
   attention; deep neural networks
ID VISUAL-CORTEX; ATTENTION; CODES
AB Artificial Neural Networks (ANNs) are bio-inspired models of neural computation that have proven highly effective. Still, ANNs lack a natural notion of time, and neural units in ANNs exchange analog values in a frame-based manner, a computationally and energetically inefficient form of communication. This contrasts sharply with biological neurons that communicate sparingly and efficiently using isomorphic binary spikes. While Spiking Neural Networks (SNNs) can be constructed by replacing the units of an ANN with spiking neurons (Cao et al., 2015; Diehl et al., 2015) to obtain reasonable performance, these SNNs use Poisson spiking mechanisms with exceedingly high firing rates compared to their biological counterparts. Here we show how spiking neurons that employ a form of neural coding can be used to construct SNNs that match high-performance ANNs and match or exceed state-of-the-art in SNNs on important benchmarks, while requiring firing rates compatible with biological findings. For this, we use spike-based coding based on the firing rate limiting adaptation phenomenon observed in biological spiking neurons. This phenomenon can be captured in fast adapting spiking neuron models, for which we derive the effective transfer function. Neural units in ANNs trained with this transfer function can be substituted directly with adaptive spiking neurons, and the resulting Adaptive SNNs (AdSNNs) can carry out competitive classification in deep neural networks without further modifications. Adaptive spike-based coding additionally allows for the dynamic control of neural coding precision: we show empirically how a simple model of arousal in AdSNNs further halves the average required firing rate and this notion naturally extends to other forms of attention as studied in neuroscience. AdSNNs thus hold promise as a novel and sparsely active model for neural computation that naturally fits to temporally continuous and asynchronous applications.
C1 [Zambrano, Davide; Nusselder, Roeland; Bohte, Sander M.] CWI, Machine Learning Grp, Amsterdam, Netherlands.
   [Scholte, H. Steven] Univ Amsterdam, Fac Social & Behav Sci, Programme Grp Brain & Cognit, Amsterdam, Netherlands.
   [Bohte, Sander M.] Univ Amsterdam, Fac Sci, Swammerdam Inst Life Sci, Amsterdam, Netherlands.
RP Bohté, SM (corresponding author), CWI, Machine Learning Grp, Amsterdam, Netherlands.; Bohté, SM (corresponding author), Univ Amsterdam, Fac Sci, Swammerdam Inst Life Sci, Amsterdam, Netherlands.
CR Abbott LF, 2004, NATURE, V431, P796, DOI 10.1038/nature03010
   [Anonymous], ARXIV160902053
   [Anonymous], 2015, ARXIV PREPRINT ARXIV
   [Anonymous], PROC CVPR IEEE
   [Anonymous], 2015, INT J COMPUT VISION, DOI DOI 10.1007/s11263-014-0788-3
   [Anonymous], 2015, INT JOINT C NEUR NET
   [Anonymous], 2012, ADV NEURAL INFORM PR
   [Anonymous], 2018, P 2018 IEEE INT S CI
   [Anonymous], 2016, ARXIV161105141
   [Anonymous], NEURONALDYNAMICS SIN
   Attwell D, 2001, J CEREBR BLOOD F MET, V21, P1133, DOI 10.1097/00004647-200110000-00001
   Boerlin M, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1001080
   Borovykh A., 2017, STATISTIC, V1050, P16
   Courbariaux M., 2016, C NEUR INF PROC SYST
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Denève S, 2016, NAT NEUROSCI, V19, P375, DOI 10.1038/nn.4243
   Esser SK, 2016, P NATL ACAD SCI USA, V113, P11441, DOI 10.1073/pnas.1604850113
   Fairhall AL, 2001, NATURE, V412, P787, DOI 10.1038/35090500
   Friston KJ, 2010, NAT REV NEUROSCI, V11, P127, DOI 10.1038/nrn2787
   Furber SB, 2013, IEEE T COMPUT, V62, P2454, DOI 10.1109/TC.2012.142
   Gerstner W., 2002, SPIKING NEURON MODEL
   GORMAN RP, 1988, NEURAL NETWORKS, V1, P75, DOI 10.1016/0893-6080(88)90023-8
   Guo P., 2018, 2018 INT JOINT C NEU, P1
   Harczos T, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00660
   Hengen KB, 2013, NEURON, V80, P335, DOI 10.1016/j.neuron.2013.08.038
   Hochreiter S, 1998, INT J UNCERTAIN FUZZ, V6, P107, DOI 10.1142/S0218488598000094
   King DB, 2015, ACS SYM SER, V1214, P1
   Krizhevsky Alex, 2009, LEARNING MULTIPLE LA
   Larkum ME, 2009, SCIENCE, V325, P756, DOI 10.1126/science.1171958
   Lazar AA, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL VI, PROCEEDINGS, P709
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Mensi S, 2016, PLOS COMPUT BIOL, V12, DOI 10.1371/journal.pcbi.1004761
   Pozzi I, 2018, LECT NOTES COMPUT SC, V11139, P284, DOI 10.1007/978-3-030-01418-6_28
   Pozzorini C, 2013, NAT NEUROSCI, V16, P942, DOI 10.1038/nn.3431
   Rieke F., 1997, SPIKES EXPLORING NEU
   Roelfsema PR, 1998, NATURE, V395, P376, DOI 10.1038/26475
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Saproo S, 2010, J NEUROPHYSIOL, V104, P885, DOI 10.1152/jn.00369.2010
   Simonyan K., 2015, ARXIV
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Yoon YC, 2017, IEEE T NEUR NET LEAR, V28, P1192, DOI 10.1109/TNNLS.2016.2526029
NR 42
TC 24
Z9 24
U1 2
U2 13
PD JAN 8
PY 2019
VL 12
AR 987
DI 10.3389/fnins.2018.00987
UT WOS:000455138400001
DA 2023-11-16
ER

PT C
AU Dzienkowski, BJ
   Markowska-Kaczmar, U
AF Dzienkowski, Bartlomiej Jozef
   Markowska-Kaczmar, Urszula
BE Jedrzejowicz, P
   Nguyen, NT
   Howlett, RJ
   Jain, LC
TI Biologically Inspired Agent System Based on Spiking Neural Network
SO AGENT AND MULTI-AGENT SYSTEMS: TECHNOLOGIES AND APPLICATIONS, PT II,
   PROCEEDINGS
SE Lecture Notes in Artificial Intelligence
DT Proceedings Paper
CT 4th KES International Symposium on Agent and Multi-Agent Systems
CY JUN 23-25, 2010
CL Gdynia Maritime Univ, Gdynia, POLAND
HO Gdynia Maritime Univ
DE pulsed neurons; spiking neural network; genetic algorithm; 3D
   environtrient; body structure; encoding method
AB The paper presents an architecture of a biologically inspired agent. Its physical body is described first. The agent's movement is directly controlled by Spiking Neural Network. To achieve this goal, the network is trained by a genetic algorithm. The agents move in a 3D physical environment. Their main goal is to effectively translocate themselves using a virtual body structure and muscles. This approach is inspired by a biological assumptions, where the neural network receives signals from sensors and directly controls the muscles. The application of Spiking Neural Network needs a suitable signal encoding method, which is also described. The system is flexible and it allows to create agents with various body structures and different neural controllers. Experiments presented in the paper refer to a simple snake-like creature. The effectiveness of controllers based on a standard threshold network and the spiking one are compared.
C1 [Dzienkowski, Bartlomiej Jozef; Markowska-Kaczmar, Urszula] Wroclaw Univ Technol, PL-50370 Wroclaw, Poland.
RP Dzienkowski, BJ (corresponding author), Wroclaw Univ Technol, PL-50370 Wroclaw, Poland.
EM urszula.markowska-kaczmar@pwr.wroc.pl
CR BOTHE SM, 2003, THESIS CTR COMPUTER
   Loiselle S, 2005, IEEE IJCNN, P2076
   Maass W., 1999, PULSED NEURAL NETWOR
   Masuta Hiroyuki, 2008, SICE 2008 - 47th Annual Conference of the Society of Instrument and Control Engineers of Japan, P2001, DOI 10.1109/SICE.2008.4654990
   Sims K., 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P15, DOI 10.1145/192161.192167
   WIKLENDT L, 2008, 8 INT C HYBR INT SYS, P144
NR 6
TC 1
Z9 1
U1 0
U2 2
PY 2010
VL 6071
BP 110
EP 119
UT WOS:000281485700012
DA 2023-11-16
ER

PT J
AU Bayro-Corrochano, E
   Solis-Gamboa, S
   Altamirano-Escobedo, G
   Lechuga-Gutierres, L
   Lisarraga-Rodriguez, J
AF Bayro-Corrochano, Eduardo
   Solis-Gamboa, Samuel
   Altamirano-Escobedo, Guillermo
   Lechuga-Gutierres, Luis
   Lisarraga-Rodriguez, Jorge
TI Quaternion Spiking and Quaternion Quantum Neural Networks: Theory and
   Applications
SO INTERNATIONAL JOURNAL OF NEURAL SYSTEMS
DT Article
DE Quaternion algebra; networks; quaternion spiking neural networks;
   quantum computing; quantum neural networks; quaternion quantum neural
   networks
ID BACKPROPAGATION; MODEL
AB Biological evidence shows that there are neural networks specialized for recognition of signals and patterns acting as associative memories. The spiking neural networks are another kind which receive input from a broad range of other brain areas to produce output that selects particular cognitive or motor actions to perform. An important contribution of this work is to consider the geometric processing in the modeling of feed-forward neural networks. Since quaternions are well suited to represent 3D rotations, it is then well justified to extend real-valued neural networks to quaternion-valued neural networks for task of perception and control of robot manipulators. This work presents the quaternion spiking neural networks which are able to control robots, where the examples confirm that these artificial neurons have the capacity to adapt on-line the robot to reach the desired position. Also, we present the quaternionic quantum neural networks for pattern recognition using just one quaternion neuron. In the experimental analysis, we show the excellent performance of both quaternion neural networks.
C1 [Bayro-Corrochano, Eduardo; Solis-Gamboa, Samuel; Altamirano-Escobedo, Guillermo; Lechuga-Gutierres, Luis; Lisarraga-Rodriguez, Jorge] CINVESTAV, Dept Elect Engn & Comp Sci, Guadalajara, Jalisco, Mexico.
RP Bayro-Corrochano, E (corresponding author), CINVESTAV, Dept Elect Engn & Comp Sci, Guadalajara, Jalisco, Mexico.
EM edb@gdl.cinvestav.mx
CR Abiyev RH, 2012, IEEE ASME INT C ADV, P1030, DOI 10.1109/AIM.2012.6265983
   Adeli H, 2009, NEURAL NETWORKS, V22, P1018, DOI 10.1016/j.neunet.2009.05.003
   Altaiskar M. V., ARXIVQUANTPH0107012
   [Anonymous], 2009, COMPLEX VALUED NEURA
   [Anonymous], 2012, STUDIES COMPUTATIONA
   Antonietti A, 2018, INT J NEURAL SYST, V28, DOI 10.1142/S012906571850020X
   Arena P, 1997, NEURAL NETWORKS, V10, P335, DOI 10.1016/S0893-6080(96)00048-2
   ARENA P, 1996, IEICE T FUND EA, V0079, P00001
   Bayro-Corrochano E., 1996, Proceedings of the Fourteenth International Conference Applied Informatics, P271
   BayroCorrochano E, 2010, GEOMETRIC COMPUTING: FOR WAVELET TRANSFORMS, ROBOT VISION, LEARNING, CONTROL AND ACTION, P1, DOI 10.1007/978-1-84882-929-9
   BayroCorrochano E, 1996, IEEE IJCNN, P120, DOI 10.1109/ICNN.1996.548877
   Bernert M, 2019, INT J NEURAL SYST, V29, DOI 10.1142/S0129065718500594
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Chen MQ, 2018, INT J NEURAL SYST, V28, DOI 10.1142/S0129065718500193
   Chuang I., 2001, QUANTUM COMPUTATION
   Galán-Prado F, 2019, INT J NEURAL SYST, V29, DOI 10.1142/S0129065719500047
   Geminiani A, 2018, INT J NEURAL SYST, V28, DOI 10.1142/S0129065717500174
   Gerstner W., 2002, SPIKING NEURON MODEL
   Ghosh-Dastidar S, 2007, INTEGR COMPUT-AID E, V14, P187
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Hamilton WR., 1853, LECT QUATERNIONS
   Haykin S, 2005, NEURAL NETWORKS
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Hu RH, 2019, INT J NEURAL SYST, V29, DOI 10.1142/S0129065719500060
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Kouda N., 2014, SYST COMP JAPAN, V3, P641
   Lechuga-Gutiérrez L, 2016, LECT NOTES COMPUT SC, V9719, P640, DOI 10.1007/978-3-319-40663-3_73
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maojun Cao, 2014, International Journal of Computer and Information Technology, V3, P83
   Mohemmed A, 2011, IFIP ADV INF COMM TE, V363, P219
   Natschlager T, 1998, NETWORK-COMP NEURAL, V9, P319, DOI 10.1088/0954-898X/9/3/003
   Nitta T, 1997, NEURAL NETWORKS, V10, P1391, DOI 10.1016/S0893-6080(97)00036-1
   Pan LQ, 2017, INT J NEURAL SYST, V27, DOI 10.1142/S0129065717500423
   PELLIONISZ A, 1985, NEUROSCIENCE, V16, P245, DOI 10.1016/0306-4522(85)90001-6
   Sagheer A, 2019, ENTROPY-SWITZ, V21, DOI 10.3390/e21080763
   Wu TF, 2018, INT J NEURAL SYST, V28, DOI 10.1142/S0129065718500132
   Zhang GX, 2014, INT J NEURAL SYST, V24, DOI 10.1142/S0129065714400061
NR 38
TC 11
Z9 11
U1 11
U2 74
PD FEB
PY 2021
VL 31
IS 2
AR 2050059
DI 10.1142/S0129065720500598
UT WOS:000610183500004
DA 2023-11-16
ER

PT J
AU Cao, ZQ
   Cheng, L
   Zhou, C
   Gu, N
   Wang, X
   Tan, M
AF Cao, Zhiqiang
   Cheng, Long
   Zhou, Chao
   Gu, Nong
   Wang, Xu
   Tan, Min
TI Spiking neural network-based target tracking control for autonomous
   mobile robots
SO NEURAL COMPUTING & APPLICATIONS
DT Article
DE Spiking neural network; Autonomous mobile robot; Spike trains; Target
   tracking; Hebbian learning
ID DYNAMIC-SYSTEM; BEHAVIOR
AB In this paper, a target tracking controller based on spiking neural network is proposed for autonomous robots. This controller encodes the preprocessed environmental and target information provided by CCD cameras, encoders and ultrasonic sensors into spike trains, which are integrated by a three-layer spiking neural network (SNN). The outputs of SNN are generated based on the competition between the forward/backward neuron pair corresponding to each motor, with the weights evolved by the Hebbian learning. The application to target tracking of a mobile robot in unknown environment verifies the validity of the proposed controller.
C1 [Cao, Zhiqiang; Cheng, Long; Zhou, Chao; Wang, Xu; Tan, Min] Chinese Acad Sci, Inst Automat, State Key Lab Management & Control Complex Syst, Beijing 100190, Peoples R China.
   [Gu, Nong] Deakin Univ, Ctr Intelligent Syst Res, Waurn Ponds, Vic, Australia.
RP Cao, ZQ (corresponding author), Chinese Acad Sci, Inst Automat, State Key Lab Management & Control Complex Syst, Beijing 100190, Peoples R China.
EM zqcao@compsys.ia.ac.cn
CR Alnajjar F, 2006, IEEE C ROB AUT MECH
   Alnajjar F, 2008, IEEE IJCNN, P2200, DOI 10.1109/IJCNN.2008.4634102
   Astudillo L, 2013, INT J INNOV COMPUT I, V9, P2007
   Balch T, 1998, IEEE T ROBOTIC AUTOM, V14, P926, DOI 10.1109/70.736776
   Bellis S, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON FIELD-PROGRAMMABLE TECHNOLOGY, PROCEEDINGS, P449, DOI 10.1109/FPT.2004.1393322
   Berglund T, 2010, IEEE T AUTOM SCI ENG, V7, P167, DOI 10.1109/TASE.2009.2015886
   Brader JM, 2007, NEURAL COMPUT, V19, P2881, DOI 10.1162/neco.2007.19.11.2881
   Brock O, 1999, ICRA '99: IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND AUTOMATION, VOLS 1-4, PROCEEDINGS, P341, DOI 10.1109/ROBOT.1999.770002
   Bugeja MK, 2009, IEEE T SYST MAN CY B, V39, P129, DOI 10.1109/TSMCB.2008.2002851
   Chadli M, 2013, J FRANKLIN I, V350, P2627, DOI 10.1016/j.jfranklin.2012.09.010
   Cui MY, 2013, AUTOMATICA, V49, P770, DOI 10.1016/j.automatica.2012.11.013
   Floreano D., 2001, LNCS, P38
   Hagras H, 2004, IEEE INT CONF ROBOT, P4620, DOI 10.1109/ROBOT.2004.1302446
   HELGADOTTIR LI, 2013, 6 INT IEEE EMBS C NE, P891
   Hwang CL, 2008, IEEE T FUZZY SYST, V16, P97, DOI 10.1109/TFUZZ.2006.889935
   Jafar FA, 2011, INT J INNOV COMPUT I, V7, P1341
   Kempter R, 1999, PHYS REV E, V59, P4498, DOI 10.1103/PhysRevE.59.4498
   KHATIB O, 1986, INT J ROBOT RES, V5, P90, DOI 10.1177/027836498600500106
   Koyama S, 2008, NEURAL COMPUT, V20, P1776, DOI 10.1162/neco.2008.06-07-540
   Lovelace JJ, 2008, NEURAL COMPUT, V20, P65, DOI 10.1162/neco.2008.20.1.65
   Maass W, 1996, NEURAL COMPUT, V8, P1, DOI 10.1162/neco.1996.8.1.1
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maass W., 1999, PULSED NEURAL NETWOR
   Pawlas Z, 2008, NEURAL COMPUT, V20, P1325, DOI 10.1162/neco.2007.01-07-442
   Qu H, 2009, IEEE T NEURAL NETWOR, V20, P1724, DOI 10.1109/TNN.2009.2029858
   Roggen D, 2003, 2003 NASA/DOD CONFERENCE ON EVOLVABLE HARDWARE, P189
   Salaris P, 2010, IEEE T ROBOT, V26, P269, DOI 10.1109/TRO.2009.2039379
   SASAKI H, 2009, IEEE WORKSH ROB INT, P73
   Sasaki H., 2006, 2006 SICE ICASE INT, P4214
   Shen QK, 2014, IEEE T FUZZY SYST, V22, P494, DOI 10.1109/TFUZZ.2013.2260757
   Tsourveloudis NC, 2001, IEEE T ROBOTIC AUTOM, V17, P490, DOI 10.1109/70.954761
   Ulrich I., 2000, Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065), P2505, DOI 10.1109/ROBOT.2000.846405
   Wang XQ, 2008, NEUROCOMPUTING, V71, P655, DOI 10.1016/j.neucom.2007.08.025
   Wang XQ, 2012, LECT NOTES COMPUT SC, V7666, P652, DOI 10.1007/978-3-642-34478-7_79
   Willms AR, 2008, IEEE T SYST MAN CY B, V38, P884, DOI 10.1109/TSMCB.2008.921002
   Willms AR, 2006, IEEE T SYST MAN CY B, V36, P755, DOI 10.1109/TSMCB.2005.862724
   YUAN Y, 2008, IEEE RSJ INT C INT R, P255
   Zhou Y, 2008, IEEE T SYST MAN CY B, V38, P963, DOI 10.1109/TSMCB.2008.922053
   Zhuang Y, 2011, INT J INNOV COMPUT I, V7, P1765
NR 39
TC 33
Z9 34
U1 4
U2 44
PD NOV
PY 2015
VL 26
IS 8
BP 1839
EP 1847
DI 10.1007/s00521-015-1848-5
UT WOS:000361397500005
DA 2023-11-16
ER

PT C
AU Chaturvedi, S
   Khurshid, AA
   Dorle, SS
AF Chaturvedi, Soni
   Khurshid, A. A.
   Dorle, S. S.
GP IEEE
TI Reconfiguration of Spiking Neural Network for Optimization with
   Applications to Image Processing
SO 2013 SIXTH INTERNATIONAL CONFERENCE ON EMERGING TRENDS IN ENGINEERING
   AND TECHNOLOGY (ICETET 2013)
SE International Conference on Emerging Trends in Engineering and
   Technology
DT Proceedings Paper
CT 6th International Conference on Emerging Trends in Engineering and
   Technology (ICETET)
CY DEC 16-18, 2013
CL Nagpur, INDIA
DE SNN; MSE; MAE; PSNR; LIF model
AB This paper depicts the restructuring of different models of third generation of Artificial neural network, that is, the spiking neural networks for image processing applications. The proposed work aims towards implementation of a novel algorithm using different models of Spiking Neural Networks which will improve upon the optimization results in the field of image processing. In this paper, we focus on various evaluation parameters like mean square error, mean absolute error peak signal to noise ratios as well as enhance the output using ANN as wellas Leaky Integrate and firing Model of Spiking Neural Networks.
C1 [Chaturvedi, Soni] PIET, E&C Engn, Nagpur, Maharashtra, India.
   [Khurshid, A. A.] RCOEM, EN, Nagpur, Maharashtra, India.
   [Dorle, S. S.] GHRCE, EN, Nagpur, Maharashtra, India.
RP Chaturvedi, S (corresponding author), PIET, E&C Engn, Nagpur, Maharashtra, India.
EM soni2569@gmail.com; aakhurshid@gmail.com; sanjay.dorle@raisoni.net
CR [Anonymous], 2012, INT J COMPUTER SCI E
   [Anonymous], 2012, REV AN PATT REC US N
   Bishop C. M., 1999, MIT ENCY COGNITIVE S
   Chaturvedi Soni, 2012, WORLD C APPL COMP WC
   Chaturvedi Soni, 2011, 4 INT C EM TRENDS EN
   Chaturvedi Soni, 2012, INT J VIDEO IMAGE PR, V12, P08
   Gupta A., 2007, P IEEE NEUR NET WORK
   Meftah B., 2012, SCI, V427, P525
   Patra P. K., 2010, INT J COMPUTER APPL, V9, P19
   Schliebs S, 2009, NEURAL NETWORKS, V22, P623, DOI 10.1016/j.neunet.2009.06.038
   Vazquez R. A., 2010, P 7 INT C EL ENG COM
   Zviagintsev A, 2006, J NEURAL ENG, V3, P35, DOI 10.1088/1741-2560/3/1/004
NR 12
TC 4
Z9 4
U1 0
U2 1
PY 2013
BP 191
EP 192
DI 10.1109/ICETET.2013.54
UT WOS:000353455300055
DA 2023-11-16
ER

PT J
AU Chakraborty, B
   She, XY
   Mukhopadhyay, S
AF Chakraborty, Biswadeep
   She, Xueyuan
   Mukhopadhyay, Saibal
TI A Fully Spiking Hybrid Neural Network for Energy-Efficient Object
   Detection
SO IEEE TRANSACTIONS ON IMAGE PROCESSING
DT Article
DE Biological neural networks; Object detection; Training; Neurons;
   Detectors; Standards; Feature extraction; Spiking neural networks; leaky
   integrate and fire; uncertainty estimation; generalization; object
   detection
ID POTENTIATION; INFERENCE; TAIL
AB This paper proposes a Fully Spiking Hybrid Neural Network (FSHNN) for energy-efficient and robust object detection in resource-constrained platforms. The network architecture is based on a Spiking Convolutional Neural Network using leaky-integrate-fire neuron models. The model combines unsupervised Spike Time-Dependent Plasticity (STDP) learning with back-propagation (STBP) learning methods and also uses Monte Carlo Dropout to get an estimate of the uncertainty error. FSHNN provides better accuracy compared to DNN based object detectors while being more energy-efficient. It also outperforms these object detectors, when subjected to noisy input data and less labeled training data with a lower uncertainty error.
C1 [Chakraborty, Biswadeep; She, Xueyuan; Mukhopadhyay, Saibal] Georgia Inst Technol, Dept Elect & Comp Engn, Atlanta, GA 30332 USA.
RP Chakraborty, B (corresponding author), Georgia Inst Technol, Dept Elect & Comp Engn, Atlanta, GA 30332 USA.
EM biswadeep@gatech.edu; xshe@gatech.edu; saibal@ece.gatech.edu
CR [Anonymous], 2016, PROC AUSTRALAS TRANS
   BLISS TVP, 1973, J PHYSIOL-LONDON, V232, P331, DOI 10.1113/jphysiol.1973.sp010273
   BLISS TVP, 1993, NATURE, V361, P31, DOI 10.1038/361031a0
   Carrillo S, 2012, NEURAL NETWORKS, V33, P42, DOI 10.1016/j.neunet.2012.04.004
   CHEN Z, 2020, P ADV NEUR INF PROC, V33
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Diehl P.U., 2015, 2015 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2015.7280696
   Diehl PU, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON REBOOTING COMPUTING (ICRC)
   Donahue J, 2014, PR MACH LEARN RES, V32
   Erdogdu M. A., 2020, P ADV NEUR INF PROC, V33
   Gal Y, 2016, PR MACH LEARN RES, V48
   GERSTNER W, 1993, BIOL CYBERN, V69, P503, DOI 10.1007/BF01185422
   Gurbuzbalaban M., 2020, ARXIV200604740
   HILL BM, 1975, ANN STAT, V3, P1163, DOI 10.1214/aos/1176343247
   Horowitz M, 2014, ISSCC DIG TECH PAP I, V57, P10, DOI 10.1109/ISSCC.2014.6757323
   Hosang J, 2016, IEEE T PATTERN ANAL, V38, P814, DOI 10.1109/TPAMI.2015.2465908
   Hu Y., 2016, NEURAL SYST COMPUT P
   Huang K., 2020, ARXIV200206262
   Huang SY, 2014, J NEUROSCI, V34, P7575, DOI 10.1523/JNEUROSCI.0983-14.2014
   Jacot Arthur, 2018, NEURIPS, DOI DOI 10.48550/ARXIV.1806.07572
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90
   Kappel D., 2015, ADV NEURAL INFORM PR, V28, P370
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Lansdell B. J., 2019, SPIKING ALLOWS NEURO
   Lee C, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00119
   Leen TK, 2012, NEURAL NETWORKS, V32, P219, DOI 10.1016/j.neunet.2012.02.006
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Masci J, 2011, LECT NOTES COMPUT SC, V6791, P52, DOI 10.1007/978-3-642-21735-7_7
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Miller D, 2019, IEEE INT CONF ROBOT, P2348, DOI 10.1109/icra.2019.8793821
   Miller D, 2018, IEEE INT CONF ROBOT, P3243
   Mittnik S, 1996, APPL MATH LETT, V9, P53, DOI 10.1016/0893-9659(96)00031-6
   Mohammadi M, 2015, METRIKA, V78, P549, DOI 10.1007/s00184-014-0515-7
   Moreno-Bote R, 2015, SCI REP-UK, V5, DOI 10.1038/srep17531
   Nessler B, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003037
   Panda P, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00653
   Panda P, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00126
   Paulauskas V., 2011, ARXIV11041242
   PICKANDS J, 1975, ANN STAT, V3, P119
   Rathi Nitin, 2020, INT C LEARN REPR
   Redmon J, 2018, Arxiv, DOI arXiv:1804.02767
   Royo-Miquel J., ARXIV210605624, V2021
   Rueckauer B., 2016, ARXIV161204052
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   sekli U.  Sim, 2020, ARXIV200609313
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   SHE X, 2020, PROC INT JOINT C NEU, P1
   Simsekli U., 2019, ARXIV190106053
   Stone, 2018, COMPUT NEUROSCI META
   Tavanaei A, 2018, 2018 INT JOINT C NEU, DOI DOI 10.1109/IJCNN.2018.8489104
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   TROSTEN DJ, 2019, P SCAND C IM AN, P197
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Wang R., 2020, ARXIV200805700
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Yosinski J, 2014, ADV NEUR IN, V27
   Yousefzadeh A, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00665
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
NR 60
TC 11
Z9 11
U1 7
U2 31
PY 2021
VL 30
BP 9014
EP 9029
DI 10.1109/TIP.2021.3122092
UT WOS:000714202000004
DA 2023-11-16
ER

PT J
AU Leone, G
   Raffo, L
   Meloni, P
AF Leone, Gianluca
   Raffo, Luigi
   Meloni, Paolo
TI On-FPGA Spiking Neural Networks for End-to-End Neural Decoding
SO IEEE ACCESS
DT Article
DE Decoding; Task analysis; Neurons; Field programmable gate arrays;
   Real-time systems; Biological system modeling; Hardware; Neural
   decoding; spike detection; spiking neural network; FPGA; low-power
ID ALGORITHMS; SELECTION
AB In the last decades, deep learning neural decoding algorithms have gained momentum in the field of neural interfaces and neural processing systems. However, to be deployed on low-budget portable devices while maintaining real-time operability, these models must withstand strict computational and power limitations. This work presents a spike decoding system implemented on a low-end Zynq-7010 FPGA, which includes a multiplier-less spike detection pipeline and a spiking-neural-network-based decoder mapped in the programmable logic. We tested the system on two publicly available datasets and achieved comparable results with state-of-the-art neural decoders that use more complex deep learning models. The system required 7.36 times fewer parameters than the smallest architecture tested on the same dataset. Moreover, by exploiting the spike sparsity property of the neural signal, the total amount of computations is reduced by about 90% during a test carried out on real recorded data. The low computational complexity of the chosen spike detection setup, combined with the power efficiency of spiking neural networks, makes this prototype a well-suited choice for low-power real-time neural decoding at the edge.
C1 [Leone, Gianluca; Raffo, Luigi; Meloni, Paolo] Univ Cagliari, Dipartimento Ingn Elettr & Elettron, I-09123 Cagliari, Italy.
RP Meloni, P (corresponding author), Univ Cagliari, Dipartimento Ingn Elettr & Elettron, I-09123 Cagliari, Italy.
EM paolo.meloni@unica.it
CR Agrawal Mradul, 2016, 2016 IEEE 21st International Mixed-Signal Testing Workshop (IMSTW). Proceedings, P1, DOI 10.1109/IMS3TW.2016.7524225
   Ahmadi N, 2021, J NEURAL ENG, V18, DOI 10.1088/1741-2552/abde8a
   Ahmadi N, 2019, I IEEE EMBS C NEUR E, P415, DOI [10.1109/NER.2019.8717045, 10.1109/ner.2019.8717045]
   Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   [Anonymous], UT ARR
   [Anonymous], 2022, LAV SOFTW FRAM
   Brochier T, 2018, SCI DATA, V5, DOI 10.1038/sdata.2018.55
   Busia Paola, 2022, 2022 IEEE Biomedical Circuits and Systems Conference (BioCAS), P640, DOI 10.1109/BioCAS54905.2022.9948637
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Du Y, 2023, PROG ARTIF INTELL, V12, P199, DOI 10.1007/s13748-022-00278-2
   Fan Jing, 2006, Conf Proc IEEE Eng Med Biol Soc, V2006, P5472
   Fang HJ, 2010, NEURAL COMPUT, V22, P1060, DOI 10.1162/neco.2009.10-08-885
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   Irmak H, 2021, J LOW POWER ELECT AP, V11, DOI 10.3390/jlpea11030032
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   KAISER JF, 1990, INT CONF ACOUST SPEE, P381, DOI 10.1109/ICASSP.1990.115702
   Khan SM, 2020, IEEE REV BIOMED ENG, V13, P248, DOI 10.1109/RBME.2019.2950897
   Leone G, 2022, IEEE ACCESS, V10, P76780, DOI 10.1109/ACCESS.2022.3192826
   Leone G, 2020, IEEE ACCESS, V8, P218145, DOI 10.1109/ACCESS.2020.3042034
   McCrimmon CM, 2017, IEEE T BIO-MED ENG, V64, P2313, DOI 10.1109/TBME.2017.2667579
   Moses DA, 2021, NEW ENGL J MED, V385, P217, DOI 10.1056/NEJMoa2027540
   Nurmikko A, 2020, NEURON, V108, P259, DOI 10.1016/j.neuron.2020.10.015
   Obeid I, 2004, IEEE T BIO-MED ENG, V51, P905, DOI 10.1109/TBME.2004.826683
   Petrini FM, 2019, ANN NEUROL, V85, P137, DOI 10.1002/ana.25384
   Quiroga RQ, 2012, CURR BIOL, V22, pR45, DOI 10.1016/j.cub.2011.11.005
   Saggese G, 2021, ELECTRONICS-SWITZ, V10, DOI 10.3390/electronics10243068
   Shrestha SB, 2018, ADV NEUR IN, V31
   Sliwowski M, 2022, J NEURAL ENG, V19, DOI 10.1088/1741-2552/ac5d69
   Slutzky MW, 2019, NEUROSCIENTIST, V25, P139, DOI 10.1177/1073858418775355
   Sussillo D, 2012, J NEURAL ENG, V9, DOI 10.1088/1741-2560/9/2/026027
   Vogels TP, 2005, J NEUROSCI, V25, P10786, DOI 10.1523/JNEUROSCI.3508-05.2005
   Wang XW, 2020, NEURAL NETWORKS, V125, P258, DOI 10.1016/j.neunet.2020.02.011
   Wu XL, 2022, J NEURAL ENG, V19, DOI 10.1088/1741-2552/ac65b1
   Xunguang Ma, 2019, 2019 IEEE 11th International Conference on Advanced Infocomm Technology (ICAIT), P223, DOI 10.1109/ICAIT.2019.8935935
   Yang SH, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21196372
   Yang YZ, 2023, POLYM BULL, V80, P1817, DOI [10.1007/s00289-022-04158-6, 10.1109/ICSICT55466.2022.9963420]
   Zhang Z, 2021, J NEUROSCI METH, V354, DOI 10.1016/j.jneumeth.2021.109103
NR 37
TC 0
Z9 0
U1 5
U2 5
PY 2023
VL 11
BP 41387
EP 41399
DI 10.1109/ACCESS.2023.3269598
UT WOS:000981875700001
DA 2023-11-16
ER

PT C
AU Comsa, JM
   Potempa, K
   Versari, L
   Fischbacher, T
   Gesmundo, A
   Alakuijala, J
AF Comsa, Julia M.
   Potempa, Krzysztof
   Versari, Luca
   Fischbacher, Thomas
   Gesmundo, Andrea
   Alakuijala, Jyrki
GP IEEE
TI TEMPORAL CODING IN SPIKING NEURAL NETWORKS WITH ALPHA SYNAPTIC FUNCTION
SO 2020 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL
   PROCESSING
SE International Conference on Acoustics Speech and Signal Processing
   ICASSP
DT Proceedings Paper
CT IEEE International Conference on Acoustics, Speech, and Signal
   Processing (ICASSP)
CY MAY 04-08, 2020
CL Barcelona, SPAIN
DE spiking networks; temporal coding; backpropagation; image
   classification; machine learning
ID BACKPROPAGATION
AB We propose a spiking neural network model that encodes information in the relative timing of individual neuron spikes and performs classification using the first output neuron to spike. This temporal coding scheme allows the supervised training of the network with backpropagation, using locally exact derivatives of the postsynaptic with respect to presynaptic spike times. The network uses a biologically-inspired alpha synaptic transfer function and trainable synchronisation pulses as temporal references. We successfully train the network on the MNIST dataset encoded in time. Our spiking neural network outperforms comparable spiking models and achieves similar accuracy to a fully connected conventional network. During training, our network displays a speed-accuracy trade-off, with either slow and highly-accurate or very fast but less accurate classification. The results demonstrate the computational power of spiking networks with biological characteristics that encode information in the timing of individual neurons. Our code is publicly available.
C1 [Comsa, Julia M.; Potempa, Krzysztof; Versari, Luca; Fischbacher, Thomas; Gesmundo, Andrea; Alakuijala, Jyrki] Google Res, Zurich, Switzerland.
RP Comsa, JM (corresponding author), Google Res, Zurich, Switzerland.
CR [Anonymous], 2017, TRAINING SPIKING NEU
   [Anonymous], 2001, HDB BIOL PHYS
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Booij O, 2005, INFORM PROCESS LETT, V95, P552, DOI 10.1016/j.ipl.2005.05.023
   Brader JM, 2007, NEURAL COMPUT, V19, P2881, DOI 10.1162/neco.2007.19.11.2881
   Corless RM, 1996, ADV COMPUT MATH, V5, P329, DOI 10.1007/BF02124750
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Diehl PU, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON REBOOTING COMPUTING (ICRC)
   FRANK K, 1959, IRE T MED ELECTRON, V6, P85, DOI 10.1109/IRET-ME.1959.5007923
   Glorot X., 2010, P 13 INT C ARTIFICIA, P249
   Gollisch T, 2008, SCIENCE, V319, P1108, DOI 10.1126/science.1149639
   Golovin D, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1487, DOI 10.1145/3097983.3098043
   Johansson RS, 2004, NAT NEUROSCI, V7, P170, DOI 10.1038/nn1177
   Lambert J.H., 1758, ACTA HELVETICA PHYS, V3, P128
   Maziarz K., 2018, EVOLUTIONARY NEURAL
   McKennoch S, 2006, IEEE IJCNN, P3970
   Mostafa Hesham, 2018, IEEE Trans Neural Netw Learn Syst, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Neftci EO, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00324
   OConnor P., 2016, ARXIV160208323
   Querlioz D, 2013, IEEE T NANOTECHNOL, V12, P288, DOI 10.1109/TNANO.2013.2250995
   RALL W, 1967, J NEUROPHYSIOL, V30, P1138, DOI 10.1152/jn.1967.30.5.1138
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Shanahan M, 2008, CONSCIOUS COGN, V17, P288, DOI 10.1016/j.concog.2006.12.005
   Sterratt D., 2011, PRINCIPLES COMPUTATI, P172, DOI 10.1017/CBO9780511975899.008
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Tavanaei A, 2019, NEUROCOMPUTING, V330, P39, DOI 10.1016/j.neucom.2018.11.014
   THORPE SJ, 1989, CONNECTIONISM IN PERSPECTIVE, P63
NR 27
TC 64
Z9 65
U1 3
U2 10
PY 2020
BP 8529
EP 8533
DI 10.1109/icassp40776.2020.9053856
UT WOS:000615970408160
DA 2023-11-16
ER

PT J
AU Zhang, XH
   Zhang, GX
   Paul, P
   Zhang, JQ
   Wu, TB
   Fan, SH
   Xiong, XZ
AF Zhang, Xihai
   Zhang, Gexiang
   Paul, Pirthwineel
   Zhang, Jinquan
   Wu, Tianbao
   Fan, Songhai
   Xiong, Xingzhong
TI Dissolved Gas Analysis for Transformer Fault Based on Learning Spiking
   Neural P System with Belief AdaBoost
SO INTERNATIONAL JOURNAL OF UNCONVENTIONAL COMPUTING
DT Article
DE Spiking neural P system; learning spiking neural P system; belief
   adaBoost; transformer fault diagnosis; dissolved gas analysis
ID FUZZY-LOGIC; DIAGNOSIS; NETWORKS; OIL
AB This paper proposes a bio-inspired learning approach, fault diagnosis method based on learning spiking neural P system with belief AdaBoost, for oil-immersed power transformer. The learning spiking neural P system is used for identification of the fault in the transformer under the framework of ensemble learning. To test the robustness of learning spiking neural P system with belief AdaBoost, the experiment is required to repeat many times to get average accuracy. The results of experiment show that the learning spiking neural P system with belief AdaBoost is effective in diagnosing faults in transformer for thermal and electrical fault situations with dissolved gas data and is superior to other methods, like Improved Three-Ratio Method, Back-Propagation Neural Network (BPNN), Support Vector Machine (SVM), Deep Belief Network (DBN), Learning Spiking Neural P system (LSN P system), in terms of the correctness of diagnosis results.
C1 [Zhang, Xihai; Zhang, Gexiang; Paul, Pirthwineel; Zhang, Jinquan] Southwest Jiaotong Univ, Sch Elect Engn, Chengdu 611756, Peoples R China.
   [Wu, Tianbao; Fan, Songhai] State Grid Sichuan Elect Power Res Inst, Chengdu 610041, Peoples R China.
   [Xiong, Xingzhong] Sichuan Univ Sci & Engn, Articial Intelligence Key Lab Sichuan Prov, Yibin 644000, Peoples R China.
RP Zhang, GX (corresponding author), Southwest Jiaotong Univ, Sch Elect Engn, Chengdu 611756, Peoples R China.
EM zhgxdylan@126.com
CR Aghaei J, 2010, EUR T ELECTR POWER, V20, P630, DOI 10.1002/etep.343
   Aizpurua JI, 2018, IEEE T DIELECT EL IN, V25, P494, DOI 10.1109/TDEI.2018.006766
   Aman B., 2019, J MEMBRANE COMPUTING, V1, P233
   [Anonymous], 2014, 7222014 DLT
   [Anonymous], 2012, C57912011 IEEE STD
   [Anonymous], 2017, REAL LIFE APPL MEMBR
   Bacha K, 2012, ELECTR POW SYST RES, V83, P73, DOI 10.1016/j.epsr.2011.09.012
   Dai JJ, 2017, IEEE T DIELECT EL IN, V24, P2828, DOI 10.1109/TDEI.2017.006727
   Díaz-Pernil D, 2019, J MEMBRANE COMPUT, V1, P58, DOI 10.1007/s41965-018-00002-x
   Duval M, 2001, IEEE ELECTR INSUL M, V17, P31, DOI 10.1109/57.917529
   Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   HALSTEAD WD, 1973, J I PETROL, V59, P239
   Ionescu M, 2006, FUND INFORM, V71, P279
   Juayong RAB, 2020, J MEMBRANE COMPUT, V2, P59, DOI 10.1007/s41965-020-00034-2
   Khan SA, 2015, IEEE T DIELECT EL IN, V22, P590, DOI 10.1109/TDEI.2014.004478
   Li JZ, 2016, IEEE T DIELECT EL IN, V23, P1198, DOI 10.1109/TDEI.2015.005277
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Meira M., 2018, IET GENER TRANSM DIS, V13, P5441
   Miranda V, 2012, IEEE T POWER DELIVER, V27, P1350, DOI 10.1109/TPWRD.2012.2188143
   Mirowski P, 2012, IEEE T POWER DELIVER, V27, P1791, DOI 10.1109/TPWRD.2012.2197868
   Ou MH, 2019, ENERGIES, V12, DOI 10.3390/en12060995
   Pan LQ, 2019, J MEMBRANE COMPUT, V1, P1, DOI 10.1007/s41965-019-00010-5
   Päun G, 2000, J COMPUT SYST SCI, V61, P108, DOI 10.1006/jcss.1999.1693
   Peng H, 2013, INFORM SCIENCES, V235, P106, DOI 10.1016/j.ins.2012.07.015
   ROGERS RR, 1978, IEEE T ELECTR INSUL, V13, P349, DOI 10.1109/TEI.1978.298141
   Rong HN, 2019, COMPLEXITY, V2019, DOI 10.1155/2019/2635714
   SCHAPIRE RE, 1990, MACH LEARN, V5, P197, DOI 10.1007/BF00116037
   Taha IBM, 2017, IET GENER TRANSM DIS, V11, P943, DOI 10.1049/iet-gtd.2016.0886
   Wang HF, 2020, INT J UNCONV COMPUT, V15, P37
   Wang J, 2013, IEEE T FUZZY SYST, V21, P209, DOI 10.1109/TFUZZ.2012.2208974
   Wang T, 2015, IEEE T POWER SYST, V30, P1182, DOI 10.1109/TPWRS.2014.2347699
   Wang XY, 2016, INTEGR COMPUT-AID E, V23, P15, DOI 10.3233/ICA-150503
   Wang ZQ, 2000, IEEE T NEURAL NETWOR, V11, P47, DOI 10.1109/72.822509
   Wu TF, 2016, THEOR COMPUT SCI, V623, P180, DOI 10.1016/j.tcs.2015.12.038
   Yan Chao, 2015, Journal of Applied Sciences - Electronics and Information Engineering, V33, P203, DOI 10.3969/j.issn.0255-8297.2015.02.010
   Zhang GX, 2014, INFORM SCIENCES, V279, P528, DOI 10.1016/j.ins.2014.04.007
   Zhang GX, 2013, APPL SOFT COMPUT, V13, P1528, DOI 10.1016/j.asoc.2012.05.032
   Zhang Xihai, 2019, PRE P AS BRANCH INT, V11, P18
   Zhang Y, 2014, INT J NEURAL SYST, V24, DOI 10.1142/S0129065714500038
   Zhu J, 2009, STAT INTERFACE, V2, P349
   Zou A., 2019, CLUSTER COMPUT, V22, P1
NR 42
TC 11
Z9 12
U1 1
U2 17
PY 2021
VL 16
IS 2-3
SI SI
BP 239
EP 258
UT WOS:000613543300008
DA 2023-11-16
ER

PT C
AU Mireille, EA
   Pierre, T
   Marius, BI
AF Mireille, El-Assal
   Pierre, Tirilly
   Marius, Bilasco Ioan
GP IEEE
TI A Study On the Effects of Pre-processing On Spatio-temporal Action
   Recognition Using Spiking Neural Networks Trained with STDP
SO 2021 INTERNATIONAL CONFERENCE ON CONTENT-BASED MULTIMEDIA INDEXING
   (CBMI)
SE International Workshop on Content-Based Multimedia Indexing
DT Proceedings Paper
CT 18th International Conference on Content-Based Multimedia Indexing (IEEE
   CBMI)
CY JUN 28-30, 2021
CL ELECTR NETWORK
DE spiking neural networks; STDP; pre-processing; action recognition;
   temporal fusion; optical flow; SVM; sequence preparation;
   spatio-temporal features
AB There has been an increasing interest in spiking neural networks in recent years. SNNs are seen as hypothetical solutions for the bottlenecks of ANNs in pattern recognition, such as energy efficiency [1]. But current methods such as ANN-to-SNN conversion and back-propagation do not take full advantage of these networks, and unsupervised methods have not yet reached a success comparable to advanced artificial neural networks. It is important to study the behavior of SNNs trained with unsupervised learning methods such as spike-timing dependent plasticity (STDP) on video classification tasks, including mechanisms to model motion information using spikes, as this information is critical for video understanding. This paper presents multiple methods of transposing temporal information into a static format, and then transforming the visual information into spikes using latency coding. These methods are paired with two types of temporal fusion known as early and late fusion, and are used to help the spiking neural network in capturing the spatio-temporal features from videos. In this paper, we rely on the network architecture of a convolutional spiking neural network trained with STDP, and we test the performance of this network when challenged with action recognition tasks. Understanding how a spiking neural network responds to different methods of movement extraction and representation can help reduce the performance gap between SNNs and ANNs. In this paper we show the effect of the similarity in the shape and speed of certain actions on action recognition with spiking neural networks, we also highlight the effectiveness of some methods compared to others.
C1 [Mireille, El-Assal; Pierre, Tirilly; Marius, Bilasco Ioan] Univ Lille, Ctr Rech Informat Signal & Automat Lille, CNRS, Cent Lille,UMR 9189,CRIStAL, F-59000 Lille, France.
RP Mireille, EA (corresponding author), Univ Lille, Ctr Rech Informat Signal & Automat Lille, CNRS, Cent Lille,UMR 9189,CRIStAL, F-59000 Lille, France.
EM mireille.elassal2@univ-lille.fr; pierre.tirilly@univ-lille.fr;
   marius.bilasco@univ-lille.fr
CR Berlin S J, 2020, APPL ARTIF INTELL, V3, P1
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Bing ZS, 2019, FRONT NEUROROBOTICS, V13, DOI 10.3389/fnbot.2019.00018
   Burkitt AN, 2006, BIOL CYBERN, V95, P1, DOI 10.1007/s00422-006-0068-6
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Falez P., 2019, THESIS U LILLE
   Falez P, 2019, PATTERN RECOGN, V93, P418, DOI 10.1016/j.patcog.2019.04.016
   Falez Pierre, 2018, 2018 INT JOINT C NEU, P8, DOI DOI 10.1109/IJCNN.2018.8489410
   Falez Pierre, 2019, 2019 INT JOINT C NEU, P1
   Farnebäck G, 2003, LECT NOTES COMPUT SC, V2749, P363, DOI 10.1007/3-540-45103-x_50
   Gavrilov AV, 2016, INT CONF ACT PROB EL, P455, DOI 10.1109/APEIE.2016.7806372
   Ghosh-Dastidar S, 2009, ADV INTEL SOFT COMPU, V61, P167
   Gorelick L, 2007, IEEE T PATTERN ANAL, V29, P2247, DOI 10.1109/TPAMI.2007.70711
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Kugele A, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00439
   Lee C, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00119
   Lee C, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00435
   Meng Y, 2010, IEEE IJCNN
   Mohemmed A, 2012, INT J NEURAL SYST, V22, DOI 10.1142/S0129065712500128
   Peng M, 2017, FRONT PSYCHOL, V8, DOI 10.3389/fpsyg.2017.01745
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Schüldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462
   Simonyan K, 2014, ADV NEUR IN, V27
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Wang H, 2013, INT J COMPUT VISION, V103, P60, DOI 10.1007/s11263-012-0594-8
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
NR 28
TC 0
Z9 0
U1 0
U2 0
PY 2021
BP 112
EP 117
DI 10.1109/CBMI50038.2021.9461922
UT WOS:000713450200020
DA 2023-11-16
ER

PT J
AU Lin, XH
   Hu, J
   Zheng, DH
   Hu, TD
   Wang, XW
AF Lin, Xianghong
   Hu, Jia
   Zheng, Donghao
   Hu, Tiandou
   Wang, Xiangwen
TI AN ONLINE SUPERVISED LEARNING ALGORITHM BASED ON FEEDBACK ALIGNMENT FOR
   MULTILAYER SPIKING NEURAL NETWORKS
SO PROCEEDINGS OF THE ROMANIAN ACADEMY SERIES A-MATHEMATICS PHYSICS
   TECHNICAL SCIENCES INFORMATION SCIENCE
DT Article
DE spiking neural network; supervised learning; online learning; feedback
   alignment
ID CLASSIFICATION
AB The feedback alignment provides a biologically plausible learning mechanism, which can directly transmit error signals with a random weight matrix to multiple layers of a neural network. This paper proposes an online supervised learning algorithm based on the feedback alignment mechanism for multilayer spiking neural networks, named Multi-OSLFA, which can support real-time learning for the spatio-temporal pattern of spike trains. The online learning rule is represented by the kernel function of spike trains and adjusts the synaptic weights when the output neuron fires a spike during the miming process of spiking neural networks. The Multi-OSLFA algorithm is successfully applied to spike train learning tasks and nonlinear pattern classification problems on two UCI datasets. Simulation results indicate that the proposed algorithm can improve learning accuracy in comparison with other supervised learning algorithms. It shows that the proposed learning algorithm is effective for solving spatio-temporal pattern learning problems.
C1 [Lin, Xianghong; Hu, Jia; Zheng, Donghao; Hu, Tiandou; Wang, Xiangwen] Northwest Normal Univ, Coll Comp Sci & Engn, Lanzhou, Peoples R China.
RP Lin, XH (corresponding author), Northwest Normal Univ, Coll Comp Sci & Engn, Lanzhou, Peoples R China.
EM linxh@nwnu.edu.cn
CR Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Carnell A., 2005, P ESANN, P363
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Glackin C, 2008, LECT NOTES COMPUT SC, V5164, P258, DOI 10.1007/978-3-540-87559-8_27
   Grafton B, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00525
   GROSSBERG S, 1987, COGNITIVE SCI, V11, P23, DOI 10.1111/j.1551-6708.1987.tb00862.x
   Gütig R, 2003, J NEUROSCI, V23, P3697
   Jeyasothy A, 2019, IEEE T NEUR NET LEAR, V30, P1231, DOI 10.1109/TNNLS.2018.2868874
   Lillicrap TP, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms13276
   Lin XH, 2021, COMPUT INTEL NEUROSC, V2021, DOI 10.1155/2021/8592824
   Lin XH, 2017, NEUROCOMPUTING, V237, P59, DOI 10.1016/j.neucom.2016.08.087
   Lobo JL, 2020, NEURAL NETWORKS, V121, P88, DOI 10.1016/j.neunet.2019.09.004
   Park IM, 2012, NEURAL COMPUT, V24, P2223, DOI 10.1162/NECO_a_00309
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Sporea I, 2013, NEURAL COMPUT, V25, P473, DOI 10.1162/NECO_a_00396
   Tan C, 2021, NEUROCOMPUTING, V434, P137, DOI 10.1016/j.neucom.2020.12.098
   Wade JJ, 2010, IEEE T NEURAL NETWOR, V21, P1817, DOI 10.1109/TNN.2010.2074212
   Wang JL, 2014, NEUROCOMPUTING, V144, P526, DOI 10.1016/j.neucom.2014.04.017
   Wang XW, 2020, NEURAL NETWORKS, V125, P258, DOI 10.1016/j.neunet.2020.02.011
   Wang XW, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00252
   Xianghong Lin, 2015, Intelligent Computing Theories and Methodologies. 11th International Conference, ICIC 2015. Proceedings: LNCS 9225, P106, DOI 10.1007/978-3-319-22180-9_11
   Xu Y, 2013, NEURAL NETWORKS, V43, P99, DOI 10.1016/j.neunet.2013.02.003
NR 22
TC 0
Z9 0
U1 2
U2 4
PD APR-JUN
PY 2022
VL 23
IS 2
BP 187
EP 196
UT WOS:000817811400010
DA 2023-11-16
ER

PT J
AU Barton, A
   Volna, E
   Kotyrba, M
   Jarusek, R
AF Barton, Adam
   Volna, Eva
   Kotyrba, Martin
   Jarusek, Robert
TI Proposal of a Control Algorithm for Multiagent Cooperation Using Spiking
   Neural Networks
SO IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
DT Article
DE Neurons; Mathematical model; Computational modeling; Biological neural
   networks; Biological system modeling; Heuristic algorithms; Proposals;
   Izhikevich model of a spiking neuron; multiagent system (MAS); spiking
   neural network (SNN); spike timing-dependent plasticity (STDP)
AB The study deals with the issue of using spiking neural networks (SNNs) in multiagent systems. The research objective is a proposal of a control algorithm for the cooperation of a group of agents using SNNs, application of the Izhikevich model, and plasticity depending on the timing of action potentials. The proposed method has been verified and experimentally tested, proving numerous advantages over second-generation networks. The advantages and the application in real systems are described in the research conclusions.
C1 [Barton, Adam; Volna, Eva; Kotyrba, Martin; Jarusek, Robert] Univ Ostrava, Dept Informat & Comp, Prirodovedecka Fak, Ostrava 70103, Czech Republic.
RP Kotyrba, M (corresponding author), Univ Ostrava, Dept Informat & Comp, Prirodovedecka Fak, Ostrava 70103, Czech Republic.
EM martin.kotyrba@osu.cz
CR Abusnaina Ahmed A., 2014, International Journal of Digital Content Technology and its Applications, V8, P14
   Aggarwal, 2020, ARXIV 201203485
   [Anonymous], 2018, ARXIV180202627
   Barton-Gooden A, 2019, PSYCHOL HEALTH MED, V24, P470, DOI 10.1080/13548506.2018.1533985
   Bouvier M, 2019, ACM J EMERG TECH COM, V15, DOI 10.1145/3304103
   Camuñas-Mesa LA, 2014, BIOMED CIRC SYST C, P516, DOI 10.1109/BioCAS.2014.6981776
   Deng L, 2020, NEURAL NETWORKS, V121, P294, DOI 10.1016/j.neunet.2019.09.005
   Doran JE, 1997, KNOWL ENG REV, V12, P309, DOI 10.1017/S0269888997003111
   Durfee E. H., 1989, IEEE Transactions on Knowledge and Data Engineering, V1, P63, DOI 10.1109/69.43404
   Evans, 1997, TCSCS199706 DEP COMP
   Farabet C, 2012, FRONT NEUROSCI-SWITZ, V6, DOI 10.3389/fnins.2012.00032
   Friedrich J, 2016, J NEUROSCI, V36, P1529, DOI 10.1523/JNEUROSCI.2854-15.2016
   Gerstner W, 1996, NATURE, V383, P76, DOI 10.1038/383076a0
   Gerstner W., 2002, SPIKING NEURON MODEL
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Jimenez-Romero C, 2017, NEURAL COMPUT APPL, V28, pS755, DOI 10.1007/s00521-016-2398-1
   Kotyrba, 2017, 23 INT C SOFT COMP, P223
   Kumarasinghe, 2015, INT J COMPUT APPL, V130, P33
   Latham PE, 2000, J NEUROPHYSIOL, V83, P808, DOI 10.1152/jn.2000.83.2.808
   Mahmoud, 2000, SOFTWARE AGENTS CHAR, P1
   Neil D, 2016, P 31 ANN ACM S APPL
   Ramezanlou Mohammad Tayefe, 2020, Proceedings of the 10th International Conference on Computer and Knowledge Engineering (ICCKE 2020), P544, DOI 10.1109/ICCKE50421.2020.9303687
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Sebag M., 2010, ADV NEURAL INFORM PR, P379
   Stöckl C, 2021, NAT MACH INTELL, V3, DOI 10.1038/s42256-021-00311-4
   Vassiliades V, 2011, IEEE T NEURAL NETWOR, V22, P639, DOI 10.1109/TNN.2011.2111384
   Vitanza A, 2015, J FRANKLIN I, V352, P3122, DOI 10.1016/j.jfranklin.2015.04.014
   Yu C, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON AGENTS (ICA), P40, DOI 10.1109/AGENTS.2017.8015298
   Zhou ZJ, 2020, IFAC PAPERSONLINE, V53, P8112, DOI 10.1016/j.ifacol.2020.12.2281
NR 29
TC 3
Z9 3
U1 3
U2 14
PD APR
PY 2023
VL 34
IS 4
BP 2016
EP 2027
DI 10.1109/TNNLS.2021.3105800
EA AUG 2021
UT WOS:000732331700001
DA 2023-11-16
ER

PT C
AU Sasaki, T
   Nakano, H
AF Sasaki, Tomoyuki
   Nakano, Hidehiro
GP IEEE
TI An Optimizer based on Spiking Neural-oscillator Networks
SO 2021 IEEE INTERNATIONAL CONFERENCE ON SYSTEMS, MAN, AND CYBERNETICS
   (SMC)
SE IEEE International Conference on Systems Man and Cybernetics Conference
   Proceedings
DT Proceedings Paper
CT IEEE International Conference on Systems, Man, and Cybernetics (SMC)
CY OCT 17-20, 2021
CL ELECTR NETWORK
AB Spiking neural networks (SNNs) are one of the human brain-inspired neural networks, which have been applied to various applications. SNNs can be implemented on hardware and can realize low-costs, low-power consumption and high-parallelism computation. There, however, are few researches to solve continuous optimization problems using SNNs. Swarm intelligence (SI) algorithm is a population-based metaheuristics, which is effective in solving those problems. SI algorithm requires many search agents to search for good quality of solutions. Since SNNs can provide high-parallelism computation, SNNs and SI algorithm are well suited. In this paper, we propose a new swarm intelligence algorithm called an optimizer based on spiking neural-oscillator networks (OSNN). In OSNN, each particle consists of plural spiking neural-oscillators which search one-dimensional search space. These spiking neural-oscillators are networked (i.e. SNNs), networks of which affect search dynamics and performances of OSNN. We performed numerical experiments to investigate basic search dynamics of OSNN and evaluated search performances of OSNN compared to OPRC and PSO. As the results, network topology of OSNN affects spike timing for spiking neural-oscillator, search dynamics and search performances. In addition, OSNN shows better search performances than OPRC and PSO.
C1 [Sasaki, Tomoyuki] Shonan Inst Technol, 1-1-25 Tsujido Nishikaigan, Fujisawa, Kanagawa 2518511, Japan.
   [Nakano, Hidehiro] Tokyo City Univ, Setagaya Ku, 1-28-1 Tamazutsumi, Tokyo 1588557, Japan.
RP Sasaki, T (corresponding author), Shonan Inst Technol, 1-1-25 Tsujido Nishikaigan, Fujisawa, Kanagawa 2518511, Japan.
EM sasaki@info.shonan-it.ac.jp; hnakano@tcu.ac.jp
CR Fang Y, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00855
   Fang Y, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON REBOOTING COMPUTING (ICRC), P16
   Gerstner W., 2002, SPIKING NEURON MODEL
   Haessig G, 2018, IEEE T BIOMED CIRC S, V12, P860, DOI 10.1109/TBCAS.2018.2834558
   Jonke Z, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00118
   Kennedy J., 1995, 1995 IEEE International Conference on Neural Networks Proceedings (Cat. No.95CH35828), P1942, DOI 10.1109/ICNN.1995.488968
   Lianqing Ji, 2013, 2013 IEEE International Wireless Symposium (IWS), DOI 10.1109/IEEE-IWS.2013.6616806
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Nakano H, 2002, IEEE T NEURAL NETWOR, V13, P92, DOI 10.1109/72.977276
   Shi YH, 1998, IEEE C EVOL COMPUTAT, P69, DOI 10.1109/ICEC.1998.699146
   STEIN RB, 1965, BIOPHYS J, V5, P173, DOI 10.1016/S0006-3495(65)86709-1
   Wang Z, 2017, IEEE ELECTR DEVICE L, V38, P1614, DOI 10.1109/LED.2017.2754138
   WELCH BL, 1947, BIOMETRIKA, V34, P28, DOI 10.1093/biomet/34.1-2.28
   Wu YJ, 2019, AAAI CONF ARTIF INTE, P1311
   Yamanaka Y, 2016, IEICE NONLINEAR THEO, V7, P557, DOI 10.1587/nolta.7.557
NR 15
TC 1
Z9 1
U1 0
U2 1
PY 2021
BP 2924
EP 2929
DI 10.1109/SMC52423.2021.9658829
UT WOS:000800532002143
DA 2023-11-16
ER

PT C
AU Schrauwen, B
   Van Campenhout, J
AF Schrauwen, Benjamin
   Van Campenhout, Jan
GP IEEE
TI Backpropagation for population-temporal coded spiking neural networks
SO 2006 IEEE INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORK PROCEEDINGS,
   VOLS 1-10
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT IEEE International Joint Conference on Neural Network
CY JUL 16-21, 2006
CL Vancouver, CANADA
ID NEURONS; FEEDFORWARD; SPIKES
AB Supervised learning rules for spiking neural networks are currently only able to use time-to-first-spike coding and are plagued by very irregular learning curves due to their inability to model spike creation and deletion by weight changes. This paper presents a new learning rule for spiking neurons that uses the general population-temporal coding model. It is inspired by learning rules for locally recurrent analog neural networks. As a result we have a very fast learning rule that is able to operate on a wide class of decoding schemes.
C1 [Schrauwen, Benjamin; Van Campenhout, Jan] Univ Ghent, Dept Elect & Informat Syst, Ghent, Belgium.
RP Schrauwen, B (corresponding author), Univ Ghent, Dept Elect & Informat Syst, Ghent, Belgium.
EM Benjamin.Schrauwen@UGent.be
CR Anderson CH., 2003, NEURAL ENG
   BELATRECHE A, 2003, P 6 INT C COMP INT N
   Bi GQ, 2001, ANNU REV NEUROSCI, V24, P139, DOI 10.1146/annurev.neuro.24.1.139
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   BOHTE SM, 2005, E0505 SEN CWI
   Booij O, 2005, INFORM PROCESS LETT, V95, P552, DOI 10.1016/j.ipl.2005.05.023
   Campolucci P, 1999, IEEE T NEURAL NETWOR, V10, P253, DOI 10.1109/72.750549
   CORWIN EM, 1994, IEEE T NEURAL NETWOR, V5, P507, DOI 10.1109/72.286926
   Deco G, 1999, NEURAL COMPUT, V11, P919, DOI 10.1162/089976699300016502
   FRASCONI P, 1992, NEURAL COMPUT, V4, P120, DOI 10.1162/neco.1992.4.1.120
   Gerstner W., 2002, SPIKING NEURON MODEL
   Igel C., 2000, P 2 INT ICSC S NEUR, P115
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Maass W, 1997, ADV NEUR IN, V9, P211
   Maass W, 2001, PULSED NEURAL NETWOR
   RIEDMILLER M, 1993, 1993 IEEE INTERNATIONAL CONFERENCE ON NEURAL NETWORKS, VOLS 1-3, P586, DOI 10.1109/ICNN.1993.298623
   Riedmiller M, 1994, RPROP DESCRIPTION IM
   Schrauwen B, 2003, IEEE IJCNN, P2825
   SCHRAWEN B, 2004, P INT JOINT C NEUR N
   Smithies R, 2004, NEURAL COMPUT, V16, P139, DOI 10.1162/08997660460734038
   TSOI AC, 1994, IEEE T NEURAL NETWOR, V5, P229, DOI 10.1109/72.279187
   UNNIKRISHNAN KP, 1994, NEURAL COMPUT, V6, P469, DOI 10.1162/neco.1994.6.3.469
   Upegui A, 2005, MICROPROCESS MICROSY, V29, P211, DOI 10.1016/j.micpro.2004.08.012
   Xin JG, 2001, IEEE IJCNN, P1772, DOI 10.1109/IJCNN.2001.938430
   YU X, 1994, P IEEE INT C NEURAL, V1, P526
NR 25
TC 11
Z9 12
U1 0
U2 1
PY 2006
BP 1797
EP +
UT WOS:000245125903029
DA 2023-11-16
ER

PT J
AU Comsa, IM
   Potempa, K
   Versari, L
   Fischbacher, T
   Gesmundo, A
   Alakuijala, J
AF Comsa, Iulia-Maria
   Potempa, Krzysztof
   Versari, Luca
   Fischbacher, Thomas
   Gesmundo, Andrea
   Alakuijala, Jyrki
TI Temporal Coding in Spiking Neural Networks With Alpha Synaptic Function:
   Learning With Backpropagation
SO IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
DT Article
DE Neurons; Encoding; Biological information theory; Timing; Biological
   system modeling; Biological neural networks; Backpropagation;
   Backpropagation; biological neural networks; feedforward neural
   networks; image classification; supervised learning
ID ERROR-BACKPROPAGATION; GRADIENT DESCENT; RATE-CODE; OSCILLATIONS;
   INFORMATION; INTEGRATION; INHIBITION; ALGORITHM
AB The timing of individual neuronal spikes is essential for biological brains to make fast responses to sensory stimuli. However, conventional artificial neural networks lack the intrinsic temporal coding ability present in biological networks. We propose a spiking neural network model that encodes information in the relative timing of individual spikes. In classification tasks, the output of the network is indicated by the first neuron to spike in the output layer. This temporal coding scheme allows the supervised training of the network with backpropagation, using locally exact derivatives of the postsynaptic spike times with respect to presynaptic spike times. The network operates using a biologically plausible synaptic transfer function. In addition, we use trainable pulses that provide bias, add flexibility during training, and exploit the decayed part of the synaptic function. We show that such networks can be successfully trained on multiple data sets encoded in time, including MNIST. Our model outperforms comparable spiking models on MNIST and achieves similar quality to fully connected conventional networks with the same architecture. The spiking network spontaneously discovers two operating modes, mirroring the accuracy-speed tradeoff observed in human decision-making: a highly accurate but slow regime, and a fast but slightly lower accuracy regime. These results demonstrate the computational power of spiking networks with biological characteristics that encode information in the timing of individual neurons. By studying temporal coding in spiking networks, we aim to create building blocks toward energy-efficient, state-based biologically inspired neural architectures. We provide open-source code for the model.
C1 [Comsa, Iulia-Maria; Potempa, Krzysztof; Versari, Luca; Fischbacher, Thomas; Gesmundo, Andrea; Alakuijala, Jyrki] Google Res Zurich, CH-8002 Zurich, Switzerland.
   [Potempa, Krzysztof] GSA Capital, London, England.
RP Comsa, IM (corresponding author), Google Res Zurich, CH-8002 Zurich, Switzerland.
EM iuliacomsa@google.com; krzysztof.potempa@gmail.com; veluca@google.com;
   tfish@google.com; agesmundo@google.com; jyrki@google.com
CR Ahissar E, 1998, NEURAL COMPUT, V10, P597, DOI 10.1162/089976698300017683
   Ahmed FYH, 2013, MATH PROBL ENG, V2013, DOI 10.1155/2013/257085
   [Anonymous], 2019, CORR
   [Anonymous], 2001, HDB BIOL PHYS
   Bellec G., 2018, ADV NEURAL INFORM PR
   Bengio Y., 2014, COMPUTING RES RE APR, P1
   Bengio Yoshua, 2015, ARXIV150204156
   Berry MJ, 1997, P NATL ACAD SCI USA, V94, P5411, DOI 10.1073/pnas.94.10.5411
   BLAKEMOR.C, 1970, NATURE, V228, P37, DOI 10.1038/228037a0
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Booij O, 2005, INFORM PROCESS LETT, V95, P552, DOI 10.1016/j.ipl.2005.05.023
   Brader JM, 2007, NEURAL COMPUT, V19, P2881, DOI 10.1162/neco.2007.19.11.2881
   BURKE RE, 1967, J NEUROPHYSIOL, V30, P1114, DOI 10.1152/jn.1967.30.5.1114
   Buzsaki Gyorgy, 2006, RHYTHMS BRAIN, DOI [10.1093/acprof:oso/9780195301069.001.0001, DOI 10.1093/ACPROF:OSO/9780195301069.001.0001]
   Comsa JM, 2020, INT CONF ACOUST SPEE, P8529, DOI [10.1109/icassp40776.2020.9053856, 10.1109/ICASSP40776.2020.9053856]
   Corless RM, 1996, ADV COMPUT MATH, V5, P329, DOI 10.1007/BF02124750
   CRICK F, 1989, NATURE, V337, P129, DOI 10.1038/337129a0
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Diehl PU, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON REBOOTING COMPUTING (ICRC)
   Diesmann M, 1999, NATURE, V402, P529, DOI 10.1038/990101
   Dua D., 2017, UCI MACHINE LEARNING
   ENGEL AK, 1992, TRENDS NEUROSCI, V15, P218, DOI 10.1016/0166-2236(92)90039-B
   Erhan D., 2009, U MONTREAL, V1341, P1
   Eroglu C, 2010, NATURE, V468, P223, DOI 10.1038/nature09612
   Esser S. K., 2015, ADV NEURAL INFORM PR, P1117
   Fisher RA, 1936, ANN EUGENIC, V7, P179, DOI 10.1111/j.1469-1809.1936.tb02137.x
   Florian RV, 2007, NEURAL COMPUT, V19, P1468, DOI 10.1162/neco.2007.19.6.1468
   Florian RV, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0040233
   FRANK K, 1959, IRE T MED ELECTRON, V6, P85, DOI 10.1109/IRET-ME.1959.5007923
   Gamez D, 2010, CONSCIOUS COGN, V19, P294, DOI 10.1016/j.concog.2009.11.001
   Glorot X., 2010, P 13 INT C ARTIFICIA, P249
   Gollisch T, 2008, SCIENCE, V319, P1108, DOI 10.1126/science.1149639
   Golovin D, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1487, DOI 10.1145/3097983.3098043
   Gray CM, 1999, NEURON, V24, P31, DOI 10.1016/S0896-6273(00)80820-X
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Hassabis D, 2017, NEURON, V95, P245, DOI 10.1016/j.neuron.2017.06.011
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Hong CF, 2020, IEEE T NEUR NET LEAR, V31, P1285, DOI 10.1109/TNNLS.2019.2919662
   Hunsberger E., 2016, TRAINING SPIKING DEE
   Huxter J, 2003, NATURE, V425, P828, DOI 10.1038/nature02058
   Johansson RS, 2004, NAT NEUROSCI, V7, P170, DOI 10.1038/nn1177
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   King DB, 2015, ACS SYM SER, V1214, P1
   Klimesch W, 2007, BRAIN RES REV, V53, P63, DOI 10.1016/j.brainresrev.2006.06.003
   Klimesch W, 2012, TRENDS COGN SCI, V16, P606, DOI 10.1016/j.tics.2012.10.007
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
   Lambert J.H., 1758, ACTA HELVETICA PHYS, V3, P128
   Lestienne R, 2001, PROG NEUROBIOL, V65, P545, DOI 10.1016/S0301-0082(01)00019-3
   Liao QL, 2016, AAAI CONF ARTIF INTE, P1837
   Lillicrap TP, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms13276
   Maziarz K., 2018, ARXIV181109828
   McKennoch S, 2006, IEEE IJCNN, P3970
   Mehta MR, 2002, NATURE, V417, P741, DOI 10.1038/nature00807
   MOISEFF A, 1981, J NEUROSCI, V1, P40, DOI 10.1523/JNEUROSCI.01-01-00040.1981
   Mordvintsev Alexander, 2015, GOOGLE RES BLOG, V2015, P3
   Mostafa H, 2018, IEEE T NEUR NET LEAR, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Mozafari M, 2018, IEEE T NEUR NET LEAR, V29, P6178, DOI 10.1109/TNNLS.2018.2826721
   NEFTCI E, 2017, PROC IEEE INT S CIRC, P1
   Newman EA, 2003, TRENDS NEUROSCI, V26, P536, DOI 10.1016/S0166-2236(03)00237-6
   OConnor P., 2016, ARXIV160208323
   Olah C., 2017, DISTILL, DOI [10.23915/distill.00007, DOI 10.23915/DISTILL.00007]
   Palva J. M., 2016, MULTIMODAL OSCILLATI, P51
   Pavlidis NG, 2005, IEEE IJCNN, P2190
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Querlioz D, 2013, IEEE T NANOTECHNOL, V12, P288, DOI 10.1109/TNANO.2013.2250995
   RALL W, 1967, J NEUROPHYSIOL, V30, P1138, DOI 10.1152/jn.1967.30.5.1138
   Reinagel P, 2000, J NEUROSCI, V20, P5392, DOI 10.1523/JNEUROSCI.20-14-05392.2000
   Rueckauer B, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351295
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Schrauwen B, 2004, IEEE IJCNN, P471, DOI 10.1109/IJCNN.2004.1379954
   Sejnowski, 2017, P 32 C NEUR INF PROC, P1433
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Shanahan M, 2008, CONSCIOUS COGN, V17, P288, DOI 10.1016/j.concog.2006.12.005
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Speelpenning B., 1980, THESIS ILLINOIS U CH
   Sporea I, 2013, NEURAL COMPUT, V25, P473, DOI 10.1162/NECO_a_00396
   Sterratt D., 2011, PRINCIPLES COMPUTATI, P172, DOI 10.1017/CBO9780511975899.008
   Sussillo D, 2009, NEURON, V63, P544, DOI 10.1016/j.neuron.2009.07.018
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Tavanaei A, 2019, NEUROCOMPUTING, V330, P39, DOI 10.1016/j.neucom.2018.11.014
   THORPE SJ, 1989, CONNECTIONISM PERSPE, V2016, P1
   Vandeginste B., 1990, J CHEMOMETR, V4, P191, DOI DOI 10.1002/CEM.1180040210
   Wang JL, 2017, IEEE T NEUR NET LEAR, V28, P30, DOI 10.1109/TNNLS.2015.2501322
   Wang XJ, 1996, J NEUROSCI, V16, P6402
   Wang Y., 2018, IEEE T NEUR NET LEAR
   Ward LM, 2003, TRENDS COGN SCI, V7, P553, DOI 10.1016/j.tics.2003.10.012
   Whittington JCR, 2017, NEURAL COMPUT, V29, P1229, DOI 10.1162/NECO_a_00949
   WOLBERG WH, 1990, P NATL ACAD SCI USA, V87, P9193, DOI 10.1073/pnas.87.23.9193
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Xu Y, 2013, NEURAL NETWORKS, V43, P99, DOI 10.1016/j.neunet.2013.02.003
   Zenke F, 2018, NEURAL COMPUT, V30, P1514, DOI 10.1162/neco_a_01086
NR 94
TC 20
Z9 20
U1 5
U2 26
PD OCT
PY 2022
VL 33
IS 10
BP 5939
EP 5952
DI 10.1109/TNNLS.2021.3071976
EA APR 2021
UT WOS:000732884600001
DA 2023-11-16
ER

PT C
AU Wang, XW
   Lin, XH
   Zhao, JC
   Ma, HF
AF Wang, Xiangwen
   Lin, Xianghong
   Zhao, Jichang
   Ma, Huifang
BE Huang, DS
   Jo, KH
TI Supervised Learning Algorithm for Spiking Neurons Based on Nonlinear
   Inner Products of Spike Trains
SO INTELLIGENT COMPUTING THEORIES AND APPLICATION, ICIC 2016, PT II
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 12th International Conference on Intelligent Computing (ICIC)
CY AUG 02-05, 2016
CL Lanzhou, PEOPLES R CHINA
DE Spiking neural networks; Supervised learning; Nonlinear inner products
   of spike trains; Widrow-Hoff rule
ID NEURAL-NETWORKS; RESUME
AB Spiking neural networks are shown to be suitable tools for the processing of spatio-temporal information. However, due to their intricately discontinuous and implicit nonlinear mechanisms, the formulation of efficient supervised learning algorithms for spiking neural networks is difficult, which has become an important problem in the research area. This paper presents a new supervised, multi-spike learning algorithm for spiking neurons, which can implement the complex spatio-temporal pattern learning of spike trains. The proposed algorithm firstly defines nonlinear inner products operators to mathematically describe and manipulate spike trains, and then derive the learning rule from the common Widrow-Hoff rule with the nonlinear inner products of spike trains. The algorithm is successfully applied to learn sequences of spikes. The experimental results show that the proposed algorithm is effective for solving complex spatio-temporal pattern learning problems.
C1 [Wang, Xiangwen; Lin, Xianghong; Zhao, Jichang; Ma, Huifang] Northwest Normal Univ, Sch Comp Sci & Engn, Lanzhou 730070, Peoples R China.
RP Lin, XH (corresponding author), Northwest Normal Univ, Sch Comp Sci & Engn, Lanzhou 730070, Peoples R China.
EM linxh@nwnu.edu.cn
CR Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Gerstner W., 2002, SPIKING NEURON MODEL
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Gütig R, 2003, J NEUROSCI, V23, P3697
   Kasinski A., 2006, International Journal of Applied Mathematics and Computer Science, V16, P101
   [蔺想红 Lin Xianghong], 2015, [电子学报, Acta Electronica Sinica], V43, P577
   Mohemmed A, 2013, NEUROCOMPUTING, V107, P3, DOI 10.1016/j.neucom.2012.08.034
   Mohemmed A, 2012, INT J NEURAL SYST, V22, DOI 10.1142/S0129065712500128
   Paiva A.R.C., 2010, STAT SIGNAL PROCESSI, V8
   Park IM, 2012, NEURAL COMPUT, V24, P2223, DOI 10.1162/NECO_a_00309
   Ponulak F, 2008, INT J APPL MATH COMP, V18, P117, DOI 10.2478/v10006-008-0011-1
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Quiroga RQ, 2013, PRINCIPLES OF NEURAL CODING, P1, DOI 10.1201/b14756
   Schreiber S, 2003, NEUROCOMPUTING, V52-4, P925, DOI 10.1016/S0925-2312(02)00838-X
   Shrestha SB, 2015, NEURAL NETWORKS, V63, P185, DOI 10.1016/j.neunet.2014.12.001
   Sporea I, 2013, NEURAL COMPUT, V25, P473, DOI 10.1162/NECO_a_00396
   Taherkhani A., 2015, NEURAL NETWORKS IJCN, P1
   Taherkhani A, 2015, IEEE T NEUR NET LEAR, V26, P3137, DOI 10.1109/TNNLS.2015.2404938
   Xu Y, 2013, NEURAL NETWORKS, V43, P99, DOI 10.1016/j.neunet.2013.02.003
   Yu Q, 2014, NEUROCOMPUTING, V138, P3, DOI 10.1016/j.neucom.2013.06.052
   Yu Q, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0078318
NR 22
TC 4
Z9 5
U1 0
U2 2
PY 2016
VL 9772
BP 95
EP 104
DI 10.1007/978-3-319-42294-7_8
UT WOS:000387430400008
DA 2023-11-16
ER

PT C
AU Huh, D
   Sejnowski, TJ
AF Huh, Dongsung
   Sejnowski, Terrence J.
BE Bengio, S
   Wallach, H
   Larochelle, H
   Grauman, K
   CesaBianchi, N
   Garnett, R
TI Gradient Descent for Spiking Neural Networks
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)
SE Advances in Neural Information Processing Systems
DT Proceedings Paper
CT 32nd Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-08, 2018
CL Montreal, CANADA
ID ERROR-BACKPROPAGATION; RULE
AB Most large-scale network models use neurons with static nonlinearities that produce analog output, despite the fact that information processing in the brain is predominantly carried out by dynamic neurons that produce discrete pulses called spikes. Research in spike-based computation has been impeded by the lack of efficient supervised learning algorithm for spiking neural networks. Here, we present a gradient descent method for optimizing spiking network models by introducing a differentiable formulation of spiking dynamics and deriving the exact gradient calculation. For demonstration, we trained recurrent spiking networks on two dynamic tasks: one that requires optimizing fast (approximate to millisecond) spike-based interactions for efficient encoding of information, and a delayed-memory task over extended duration (approximate to second). The results show that the gradient descent approach indeed optimizes networks dynamics on the time scale of individual spikes as well as on behavioral time scales. In conclusion, our method yields a general purpose supervised learning algorithm for spiking neural networks, which can facilitate further investigations on spike-based computations.
C1 [Huh, Dongsung; Sejnowski, Terrence J.] Salk Inst Biol Studies, La Jolla, CA 92037 USA.
RP Huh, D (corresponding author), Salk Inst Biol Studies, La Jolla, CA 92037 USA.
EM huh@salk.edu; terry@salk.edu
CR Abbott LF, 2016, NAT NEUROSCI, V19, P350, DOI 10.1038/nn.4241
   [Anonymous], 2018, ARXIV180202627
   [Anonymous], 2016, ARXIV160208323
   [Anonymous], ARXIV170511146
   Boerlin M, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003258
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Booij O, 2005, INFORM PROCESS LETT, V95, P552, DOI 10.1016/j.ipl.2005.05.023
   Brea J, 2013, J NEUROSCI, V33, P9565, DOI 10.1523/JNEUROSCI.4098-12.2013
   Brendel W., 2017, ARXIV170303777
   Denève S, 2016, NAT NEUROSCI, V19, P375, DOI 10.1038/nn.4243
   DIEHL PU, 2015, IEEE IJCNN
   Ermentrout B., 2008, SCHOLARPEDIA, V3, P1398, DOI [10.4249/scholarpedia.1398, DOI 10.4249/SCHOLARPEDIA.1398]
   Florian RV, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0040233
   Frémaux N, 2016, FRONT NEURAL CIRCUIT, V9, DOI 10.3389/fncir.2015.00085
   Gardner B, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0161335
   Gardner B, 2015, NEURAL COMPUT, V27, P2548, DOI 10.1162/NECO_a_00790
   Gütig R, 2016, SCIENCE, V351, DOI 10.1126/science.aab4113
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Hunsberger Eric, 2015, COMPUT SCI
   Izhikevich EM, 2007, CEREB CORTEX, V17, P2443, DOI 10.1093/cercor/bhl152
   King DB, 2015, ACS SYM SER, V1214, P1
   Lajoie G, 2013, PHYS REV E, V87, DOI 10.1103/PhysRevE.87.052901
   Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508
   Legenstein R, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.1000180
   Lillicrap TP, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms13276
   McKennoch S, 2009, NEURAL COMPUT, V21, P9, DOI 10.1162/neco.2008.09-07-610
   Memmesheimer RM, 2014, NEURON, V82, P925, DOI 10.1016/j.neuron.2014.03.026
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   Pfister JP, 2006, NEURAL COMPUT, V18, P1318, DOI 10.1162/neco.2006.18.6.1318
   Pontryagin L. S., 1974, SELECTED WORKS
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Rezende Danilo J, 2011, ADV NEURAL INFORM PR, P136
   Rueckauer B., 2016, ARXIV161204052
   Schrauwen B, 2004, IEEE IJCNN, P471, DOI 10.1109/IJCNN.2004.1379954
   Sporea I, 2013, NEURAL COMPUT, V25, P473, DOI 10.1162/NECO_a_00396
   Tino P, 2006, NEURAL COMPUT, V18, P591, DOI 10.1162/089976606775623360
   Urbanczik R, 2009, NEURAL COMPUT, V21, P340, DOI 10.1162/neco.2008.09-07-605
   VanRullen R, 2005, TRENDS NEUROSCI, V28, P1, DOI 10.1016/j.tins.2004.10.010
   Xin JG, 2001, IEEE IJCNN, P1772, DOI 10.1109/IJCNN.2001.938430
NR 39
TC 69
Z9 70
U1 1
U2 7
PY 2018
VL 31
UT WOS:000461823301042
DA 2023-11-16
ER

PT C
AU Amin, HH
   Deabes, W
   Bouazza, K
AF Amin, Hesham H.
   Deabes, Wael
   Bouazza, Kheireddine
GP IEEE
TI Hybrid Spiking Neural Model for Clustering Smart Environment Activities
SO 2017 IEEE 15TH INTERNATIONAL CONFERENCE ON INDUSTRIAL INFORMATICS
   (INDIN)
SE IEEE International Conference on Industrial Informatics INDIN
DT Proceedings Paper
CT 15th IEEE International Conference on Industrial Informatics (INDIN)
CY JUL 24-26, 2017
CL Univ Appl Sci Emden Leer, Emden, GERMANY
HO Univ Appl Sci Emden Leer
DE recurrent spiking neural network; adaptive threshold neuron; smart
   environment
ID ACTIVITY RECOGNITION; NEURONS
AB The need for the internet of things technologies becomes a state-of-the-art in this era. Human beings do many activities during their daily life which, in certain cases, should to be recognized and understood. Intelligent systems are considered to be the most advanced methods to analyze such these complex tasks. Spiking neural network is one of the most powerful intelligent techniques that has the ability to solve such these problems. In this paper, a hybrid spiking neural network model is proposed for clustering user's activities which are recognized in a smart environment. The model is composed of both recurrent and adaptive spiking neural networks. The results show that the proposed hybrid spiking neural model is able to do the clustering of users' activities in a distinguishing way.
C1 [Amin, Hesham H.; Deabes, Wael; Bouazza, Kheireddine] Umm Al Qura Univ, Univ Coll, Dept Comp Sci, Mecca, Saudi Arabia.
   [Amin, Hesham H.] Aswan Univ, Comp & Syst Dept, Fac Engn, Aswan, Egypt.
   [Deabes, Wael] Mansoura Univ, Comp & Syst Dept, Fac Engn, Mansoura, Egypt.
RP Amin, HH (corresponding author), Umm Al Qura Univ, Univ Coll, Dept Comp Sci, Mecca, Saudi Arabia.; Amin, HH (corresponding author), Aswan Univ, Comp & Syst Dept, Fac Engn, Aswan, Egypt.
EM hhabuelhasan@uqu.edu.sa
CR Amin H. H., 2011, SPIKING NEURAL NETWO
   Amin H. H., 2017, 9 INT C UB FUT NETW
   Amin HH, 2005, IEICE T INF SYST, VE88D, P1893, DOI 10.1093/ietisy/e88-d.8.1893
   [Anonymous], SENSORS
   Bourobou STM, 2015, SENSORS-BASEL, V15, P11953, DOI 10.3390/s150511953
   Brodu N., 2007, NEURAL NETWORKS 2007
   Cook D, 2013, KNOWL INF SYST, V36, P537, DOI 10.1007/s10115-013-0665-3
   Cook DJ, 2013, COMPUTER, V46, P62, DOI 10.1109/MC.2012.328
   Fontaine B, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003560
   Gerstner W., 2002, SPIKING NEURON MODEL
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Jakkula V., 2009, ADV INTELL ENV
   Lukosevicius M, 2009, COMPUT SCI REV, V3, P127, DOI 10.1016/j.cosrev.2009.03.005
   Maass W, 2004, J COMPUT SYST SCI, V69, P593, DOI 10.1016/j jcss.2004.04.001
   Maass W., 1996, NEURAL COMPUT
   Mehr H. D., 2016, SMART GRID C FAIR IC, P1
   Nguyen L. T., 2015, UBICOMP ISWC 15, P1463
   Trabelsi D, 2013, IEEE T AUTOM SCI ENG, V10, P829, DOI 10.1109/TASE.2013.2256349
   van Kasteren Tim, 2007, 3rd IET International Conference on Intelligent Environments, IE 07, P209
   Zheng D., 2015, INT J SMART HOME, V9, P207, DOI [10.14257/ijsh.2015.9.2.19, DOI 10.14257/IJSH.2015.9.2.19]
   Zhong LL, 2010, INT CONF COMP SCI, P295, DOI 10.1109/ICCSIT.2010.5564569
NR 21
TC 2
Z9 2
U1 0
U2 2
PY 2017
BP 206
EP 211
UT WOS:000427453200029
DA 2023-11-16
ER

PT C
AU Chernyshev, A
AF Chernyshev, Alexey
BE Samsonovich, AV
   Klimov, VV
   Rybina, GV
TI Bayesian Optimization of Spiking Neural Network Parameters to Solving
   the Time Series Classification Task
SO BIOLOGICALLY INSPIRED COGNITIVE ARCHITECTURES (BICA) FOR YOUNG
   SCIENTISTS
SE Advances in Intelligent Systems and Computing
DT Proceedings Paper
CT 1st International Early Research Career Enhancement School on
   Biologically Inspired Cognitive Architectures (FIERCES on BICA)
CY APR 21-24, 2016
CL Moscow, RUSSIA
DE Spiking neural network; Bayesian optimization; Time series
   classification
AB This study contains the application of spiking neural networks to time series classification task. Because of the lack of mathematical framework for such biologically inspired neural networks, this study tries to solve hyperparameter optimization task with the help of surrogate models. To define classification task quality metric that measures separability index based on Fisher's discriminant ratio is used.
C1 [Chernyshev, Alexey] Bauman Moscow State Tech Univ, Moscow, Russia.
RP Chernyshev, A (corresponding author), Bauman Moscow State Tech Univ, Moscow, Russia.
EM alexey.chernushev@gmail.com
CR [Anonymous], J NEURAL NETWORKS, DOI DOI 10.1016/0893-6080(89)90020-8
   [Anonymous], 2014, ARXIV14107172
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Büsing L, 2010, NEURAL COMPUT, V22, P1272, DOI 10.1162/neco.2009.01-09-947
   Chrol-Cannon J., 2014, CORRELATION RESERVOI
   Gerstner W., 2002, SPIKING NEURON MODEL
   Haselsteiner E, 2000, IEEE T REHABIL ENG, V8, P457, DOI 10.1109/86.895948
   Hennequin G, 2010, FRONT COMPUT NEUROSC, V4, DOI 10.3389/fncom.2010.00143
   Hyvärinen A, 2000, NEURAL NETWORKS, V13, P411, DOI 10.1016/S0893-6080(00)00026-5
   Jaeger H, 2007, NEURAL NETWORKS, V20, P287, DOI 10.1016/j.neunet.2007.04.001
   Jones DR, 1998, J GLOBAL OPTIM, V13, P455, DOI 10.1023/A:1008306431147
   Lukosevicius M, 2009, COMPUT SCI REV, V3, P127, DOI 10.1016/j.cosrev.2009.03.005
   Paiva ARC, 2010, NEURAL COMPUT APPL, V19, P405, DOI 10.1007/s00521-009-0307-6
   Shawe-Taylor J, 2004, KERNEL METHODS PATTE
   Siegelmann HT, 1997, IEEE T SYST MAN CY B, V27, P208, DOI 10.1109/3477.558801
   Wang SG, 2011, EXPERT SYST APPL, V38, P8696, DOI 10.1016/j.eswa.2011.01.077
   Wiskott L, 2002, NEURAL COMPUT, V14, P715, DOI 10.1162/089976602317318938
   Zhang J., 2004, KERNEL FISHER DISCRI
NR 18
TC 3
Z9 3
U1 0
U2 4
PY 2016
VL 449
BP 39
EP 45
DI 10.1007/978-3-319-32554-5_6
UT WOS:000386313100006
DA 2023-11-16
ER

PT J
AU Brette, R
   Goodman, DFM
AF Brette, Romain
   Goodman, Dan F. M.
TI Vectorized Algorithms for Spiking Neural Network Simulation
SO NEURAL COMPUTATION
DT Article
ID SYNAPTIC CONDUCTANCES; COMPUTATION; NEURONS; PLASTICITY; MODELS; TOOLS
AB High-level languages (Matlab, Python) are popular in neuroscience because they are flexible and accelerate development. However, for simulating spiking neural networks, the cost of interpretation is a bottleneck. We describe a set of algorithms to simulate large spiking neural networks efficiently with high-level languages using vector-based operations. These algorithms constitute the core of Brian, a spiking neural network simulator written in the Python language. Vectorized simulation makes it possible to combine the flexibility of high-level languages with the computational efficiency usually associated with compiled languages.
C1 [Brette, Romain] CNRS, Lab Psychol Percept, F-75006 Paris, France.
   Univ Paris 05, F-75006 Paris, France.
   Ecole Normale Super, Dept Etud Cognit, F-75230 Paris 05, France.
RP Brette, R (corresponding author), CNRS, Lab Psychol Percept, F-75006 Paris, France.
EM romain.brette@ens.fr; dan.goodman@ens.fr
CR [Anonymous], 1998, BOOK GENESIS EXPLORI, DOI DOI 10.1007/978-1-4612-1634-63
   Brette R, 2007, J COMPUT NEUROSCI, V23, P349, DOI 10.1007/s10827-007-0038-6
   Cannon RC, 2007, NEUROINFORMATICS, V5, P127, DOI 10.1007/s12021-007-0004-5
   Carnevale N.T., 2006, NEURON BOOK, DOI DOI 10.1017/CBO9780511541612
   D'Haene M, 2010, NEURAL COMPUT, V22, P1468, DOI 10.1162/neco.2010.07-09-1070
   Davison Andrew P, 2008, Front Neuroinform, V2, P11, DOI 10.3389/neuro.11.011.2008
   Deneve S, 2008, NEURAL COMPUT, V20, P91, DOI 10.1162/neco.2008.20.1.91
   Destexhe A, 1994, J Comput Neurosci, V1, P195, DOI 10.1007/BF00961734
   DESTEXHE A, 1994, NEURAL COMPUT, V6, P14, DOI 10.1162/neco.1994.6.1.14
   DJURFELDT M, 2007, 1 INCF WORKSH LARG S
   Eppler Jochen Martin, 2008, Front Neuroinform, V2, P12, DOI 10.3389/neuro.11.012.2008
   Garny A, 2008, PHILOS T R SOC A, V366, P3017, DOI 10.1098/rsta.2008.0094
   Gerstner W., 2002, SPIKING NEURON MODEL
   Giugliano M, 2000, NEURAL COMPUT, V12, P903, DOI 10.1162/089976600300015646
   Giugliano M, 1999, NEURAL COMPUT, V11, P1413, DOI 10.1162/089976699300016296
   Goddard NH, 2001, PHILOS T ROY SOC B, V356, P1209, DOI 10.1098/rstb.2001.0910
   Goodman Dan, 2008, Front Neuroinform, V2, P5, DOI 10.3389/neuro.11.005.2008
   Goodman DFM, 2009, FRONT NEUROSCI-SWITZ, V3, P192, DOI 10.3389/neuro.01.026.2009
   Hanuschkin A., 2010, FRONTIERS NEUROINFOR, V4, P12
   HINES M, 1984, INT J BIOMED COMPUT, V15, P69, DOI 10.1016/0020-7101(84)90008-4
   Hines Michael L, 2009, Front Neuroinform, V3, P1, DOI 10.3389/neuro.11.001.2009
   Hines ML, 2000, NEURAL COMPUT, V12, P995, DOI 10.1162/089976600300015475
   Hirsch M W., 1974, DIFF EQUAT+
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Jahnke A, 1998, PULSED NEURAL NETWORKS, P237
   Kohn J, 1998, NEURAL COMPUT, V10, P1639, DOI 10.1162/089976698300017061
   Loebel A, 2002, J COMPUT NEUROSCI, V13, P111, DOI 10.1023/A:1020110223441
   Lytton WW, 1996, NEURAL COMPUT, V8, P501, DOI 10.1162/neco.1996.8.3.501
   Marc-Oliver G., 2007, SCHOLARPEDIA, DOI [DOI 10.4249/SCHOLARPEDIA.1430, 10.4249/scholarpedia.1430]
   Markram H, 1998, P NATL ACAD SCI USA, V95, P5323, DOI 10.1073/pnas.95.9.5323
   Mongillo G, 2008, SCIENCE, V319, P1543, DOI 10.1126/science.1150769
   Morrison A, 2005, NEURAL COMPUT, V17, P1776, DOI 10.1162/0899766054026648
   Morrison A, 2008, BIOL CYBERN, V98, P459, DOI 10.1007/s00422-008-0233-1
   Morrison A, 2007, NEURAL COMPUT, V19, P1437, DOI 10.1162/neco.2007.19.6.1437
   Morrison A, 2007, NEURAL COMPUT, V19, P47, DOI 10.1162/neco.2007.19.1.47
   Morse T., 2007, SCHOLARPEDIA, V2, P3036
   Owens JD, 2007, COMPUT GRAPH FORUM, V26, P80, DOI 10.1111/j.1467-8659.2007.01012.x
   Platkiewicz J, 2010, PLOS COMPUT BIOL, V6, DOI 10.1371/journal.pcbi.1000850
   Plesser HE, 2009, NEURAL COMPUT, V21, P353, DOI 10.1162/neco.2008.03-08-731
   PROTOPAPAS AD, 1998, METHODS NEURONAL MOD
   Rossant Cyrille, 2010, Front Neuroinform, V4, P2, DOI 10.3389/neuro.11.002.2010
   Rotter S, 1999, BIOL CYBERN, V81, P381, DOI 10.1007/s004220050570
   Sanchez-Montanez M. A., 2001, STRATEGIES OPTIMIZAT
   Schutter E. D., 2008, PLOS COMPUT BIOL, V4
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Tsodyks M, 1998, NEURAL COMPUT, V10, P821, DOI 10.1162/089976698300017502
   Tsodyks MV, 1997, P NATL ACAD SCI USA, V94, P719, DOI 10.1073/pnas.94.2.719
NR 47
TC 22
Z9 24
U1 0
U2 13
PD JUN
PY 2011
VL 23
IS 6
BP 1503
EP 1535
DI 10.1162/NECO_a_00123
UT WOS:000290300400004
DA 2023-11-16
ER

PT S
AU Drewes, R
   Maciokas, J
   Louis, SJ
   Goodman, P
AF Drewes, R
   Maciokas, J
   Louis, SJ
   Goodman, P
BE Deb, K
   Poli, R
   Banzhaf, W
   Beyer, HG
   Burke, E
   Darwen, P
   Dasgupta, D
   Floreano, D
   Foster, O
   Harman, M
   Holland, O
   Lanzi, PL
   Spector, L
   Tettamanzi, A
   Thierens, D
   Tyrrell, A
TI An evolutionary autonomous agent with visual cortex and recurrent
   spiking columnar neural network
SO GENETIC AND EVOLUTIONARY COMPUTATION - GECCO 2004, PT 1, PROCEEDINGS
SE LECTURE NOTES IN COMPUTER SCIENCE
DT Article; Proceedings Paper
CT 6th Annual Genetic and Evolutionary Computation Conference (GECCO 2004)
CY JUN 26-30, 2004
CL Seattle, WA
AB Spiking neural networks are computationally more powerful than conventional artificial neural networks [1]. Although this fact should make them especially desirable for use in evolutionary autonomous agent research, several factors have limited their application. This work demonstrates an evolutionary agent with a sizeable recurrent spiking neural network containing a biologically motivated columnar visual cortex. This model is instantiated in spiking neural network simulation software and challenged with a dynamic image recognition and memory task. We use a genetic algorithm to evolve generations of this brain model that instinctively perform progressively better on the task. This early work builds a foundation for determining which features of biological neural networks are important for evolving capable dynamic cognitive agents.
C1 Univ Nevada, Brain Computat Lab, Reno, NV 89557 USA.
   Univ Nevada, Evolutionary Comp Syst Lab, Reno, NV 89557 USA.
RP Drewes, R (corresponding author), Univ Nevada, Brain Computat Lab, Reno, NV 89557 USA.
CR Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   MILLER E, 1996, J NEUROSCI      0815, P5154
NR 2
TC 1
Z9 1
U1 0
U2 2
PY 2004
VL 3102
BP 257
EP 258
PN 1
UT WOS:000225101200025
DA 2023-11-16
ER

PT J
AU Pan, LQ
   Paun, G
   Zhang, GX
   Neri, F
AF Pan, Linqiang
   Paun, Gheorghe
   Zhang, Gexiang
   Neri, Ferrante
TI Spiking Neural <i>P</i> Systems with Communication on Request
SO INTERNATIONAL JOURNAL OF NEURAL SYSTEMS
DT Article
DE Bio-inspired computing; membrane computing; P system; artificial neural
   network; spiking neural network
ID NETWORK; NEURONS; POWER
AB Spiking Neural P Systems are Neural System models characterized by the fact that each neuron mimics a biological cell and the communication between neurons is based on spikes. In the Spiking Neural P systems investigated so far, the application of evolution rules depends on the contents of a neuron (checked by means of a regular expression). In these P systems, a specified number of spikes are consumed and a specified number of spikes are produced, and then sent to each of the neurons linked by a synapse to the evolving neuron.
   In the present work, a novel communication strategy among neurons of Spiking Neural P Systems is proposed. In the resulting models, called Spiking Neural P Systems with Communication on Request, the spikes are requested from neighboring neurons, depending on the contents of the neuron (still checked by means of a regular expression). Unlike the traditional Spiking Neural P systems, no spikes are consumed or created: the spikes are only moved along synapses and replicated (when two or more neurons request the contents of the same neuron).
   The Spiking Neural P Systems with Communication on Request are proved to be computationally universal, that is, equivalent with Turing machines as long as two types of spikes are used. Following this work, further research questions are listed to be open problems.
C1 [Pan, Linqiang] Huazhong Univ Sci & Technol, Sch Automat, Educ Minist China, Key Lab Image Informat Proc & Intelligent Control, Wuhan 430074, Hubei, Peoples R China.
   [Pan, Linqiang] Zhengzhou Univ Light Ind, Zhengzhou 450002, Henan, Peoples R China.
   [Paun, Gheorghe] Romanian Acad, Inst Math, POB 1-764, RO-014700 Bucharest, Romania.
   [Zhang, Gexiang] Xihua Univ, Robot Res Ctr, Chengdu 610039, Sichuan, Peoples R China.
   [Zhang, Gexiang] Xihua Univ, Minist Educ, Key Lab Fluid & Power Machinery, Chengdu 610039, Sichuan, Peoples R China.
   [Zhang, Gexiang] Southwest Jiaotong Univ, Sch Elect Engn, Chengdu 610031, Sichuan, Peoples R China.
   [Neri, Ferrante] De Montfort Univ, Ctr Computat Intelligence, Sch Comp Sci & Informat, Gateway, Leicester LE1 9BH, Leics, England.
RP Zhang, GX (corresponding author), Xihua Univ, Robot Res Ctr, Chengdu 610039, Sichuan, Peoples R China.; Zhang, GX (corresponding author), Xihua Univ, Minist Educ, Key Lab Fluid & Power Machinery, Chengdu 610039, Sichuan, Peoples R China.; Zhang, GX (corresponding author), Southwest Jiaotong Univ, Sch Elect Engn, Chengdu 610031, Sichuan, Peoples R China.
EM lqpan@mail.hust.edu.cn; gpaun@us.es; zhgxdylan@126.com; fneri@dmu.ac.uk
CR Ahmadlou M, 2010, INTEGR COMPUT-AID E, V17, P197, DOI 10.3233/ICA-2010-0345
   [Anonymous], 1997, HDB FORMAL LANGUAGES, DOI DOI 10.1007/978-3-662-07675-0
   [Anonymous], 2017, REAL LIFE APPL MEMBR
   Cabarle FGC, 2016, NEURAL COMPUT APPL, V27, P1337, DOI 10.1007/s00521-015-1937-5
   Cavaliere M, 2009, THEOR COMPUT SCI, V410, P2352, DOI 10.1016/j.tcs.2009.02.031
   Cavaliere Matteo, 2008, Natural Computing, V7, P453, DOI 10.1007/s11047-008-9086-8
   Csuhaj-Varju E, 1994, GRAMMAR SYSTEMS GRAM
   García-Arnau M, 2009, INT J UNCONV COMPUT, V5, P411
   Garrido JA, 2016, INT J NEURAL SYST, V26, DOI 10.1142/S0129065716500209
   Geminiani A., 2017, INT J NEURAL SYST
   Ghosh-Dastidar S, 2007, INTEGR COMPUT-AID E, V14, P187
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Guo LL, 2017, INT J NEURAL SYST, V27, DOI 10.1142/S0129065717500022
   Ibarra OH, 2007, THEOR COMPUT SCI, V372, P196, DOI 10.1016/j.tcs.2006.11.025
   Iliya S, 2016, INT J NEURAL SYST, V26, DOI 10.1142/S0129065716500234
   Ionescu M, 2006, LECT NOTES COMPUT SC, V4287, P1
   Ionescu M, 2006, FUND INFORM, V71, P279
   Ishdorj TO, 2010, THEOR COMPUT SCI, V411, P2345, DOI 10.1016/j.tcs.2010.01.019
   Knieling S, 2016, INT J NEURAL SYST, V26, DOI 10.1142/S0129065715500422
   Korec I, 1996, THEOR COMPUT SCI, V168, P267, DOI 10.1016/S0304-3975(96)00080-1
   Leporati A, 2009, INT J UNCONV COMPUT, V5, P459
   Liu XY, 2017, NEURAL PROCESS LETT, V46, P171, DOI 10.1007/s11063-016-9577-z
   Luo C, 2015, INT J NEURAL SYST, V25, DOI 10.1142/S0129065715500276
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Minsky ML., 1967, COMPUTATION FINITE I
   Morro A., 2017, IEEE T NEURAL NETW L
   Muhl G, 2006, DISTRIBUTED EVENT BA
   Nobukawa S, 2016, INT J NEURAL SYST, V26, DOI 10.1142/S0129065715500409
   Pan L., 2016, B INT MEMBRANE COMPU, V1, P63
   Pan LQ, 2011, SCI CHINA INFORM SCI, V54, P1596, DOI 10.1007/s11432-011-4303-y
   Paun A, 2002, NEW GENERAT COMPUT, V20, P295, DOI 10.1007/BF03037362
   Paun A, 2007, BIOSYSTEMS, V90, P48, DOI 10.1016/j.biosystems.2006.06.006
   Paun G, 2002, MEMBRANE COMPUTING I
   Paun Gh, 2010, OXFORD HDB MEMBRANE
   Peng H, 2016, INT J NEURAL SYST, V26, DOI 10.1142/S0129065716500040
   Peng H, 2013, INFORM SCIENCES, V235, P106, DOI 10.1016/j.ins.2012.07.015
   Rigos A, 2016, INTEGR COMPUT-AID E, V23, P141, DOI 10.3233/ICA-150507
   Rosselló JL, 2016, INT J NEURAL SYST, V26, DOI 10.1142/S0129065715500367
   Rosselló JL, 2014, INT J NEURAL SYST, V24, DOI 10.1142/S0129065714300034
   Savin C, 2010, PLOS COMPUT BIOL, V6, DOI 10.1371/journal.pcbi.1000757
   Song T, 2016, NEUROCOMPUTING, V193, P193, DOI 10.1016/j.neucom.2016.02.023
   Song T, 2013, INFORM SCIENCES, V219, P197, DOI 10.1016/j.ins.2012.07.023
   Wang J, 2013, IEEE T FUZZY SYST, V21, P209, DOI 10.1109/TFUZZ.2012.2208974
   Wang J, 2010, NEURAL COMPUT, V22, P2615, DOI 10.1162/NECO_a_00022
   Wang NM, 2015, ENG APPL ARTIF INTEL, V41, P249, DOI 10.1016/j.engappai.2015.01.018
   Wang T, 2015, INT J COMPUT COMMUN, V10, P904
   Wang T, 2015, IEEE T POWER SYST, V30, P1182, DOI 10.1109/TPWRS.2014.2347699
   Wang X., 2015, INTEGRATED COMPUTER, V23, P15
   Wang ZZ, 2014, INT J NEURAL SYST, V24, DOI 10.1142/S0129065714400048
   Wu Tingfang, 2018, IEEE Trans Neural Netw Learn Syst, V29, P3349, DOI 10.1109/TNNLS.2017.2726119
   Wu TF, 2016, THEOR COMPUT SCI, V623, P180, DOI 10.1016/j.tcs.2015.12.038
   Yu Y, 2017, FUND INFORM, V150, P231, DOI 10.3233/FI-2017-1467
   ZANDRON C, 2001, THESIS
   Zeinali Y, 2017, INTEGR COMPUT-AID E, V24, P105, DOI 10.3233/ICA-170540
   Zhang GX, 2014, INT J NEURAL SYST, V24, DOI 10.1142/S0129065714400061
   Zhang GX, 2013, APPL SOFT COMPUT, V13, P1528, DOI 10.1016/j.asoc.2012.05.032
   Zhang X., 2017, INT J NEURAL SYST
NR 58
TC 156
Z9 157
U1 5
U2 63
PD DEC
PY 2017
VL 27
IS 8
AR 1750042
DI 10.1142/S0129065717500423
UT WOS:000414271800003
DA 2023-11-16
ER

PT C
AU Zuters, J
AF Zuters, Janis
BE Barzdins, J
   Kirikova, M
TI The Role of Random Spikes and Concurrent Input Layers in Spiking Neural
   Networks
SO DATABASES AND INFORMATION SYSTEMS
DT Proceedings Paper
CT 9th International Baltic Conference on Databases and Information Systems
   (Baltic DBandIS)
CY JUL 05-07, 2010
CL Riga, LATVIA
DE Spiking Neural Networks; Unsupervised Learning; Randomness
AB In spiking neural networks (SNNs), unlike traditional artificial neural networks, signals are propagated by a 'pulse code' instead of a 'rate code'. This results in incorporating the time dimension into the network and thus theoretically ensures a higher computational power. The different principle of operation makes learning in SNNs complicated. In this paper, a Hebbian-learning based learning algorithm, introduced by the author earlier, is further developed through applying random spikes in order to guarantee frequent firings of neurons for each layer. In addition, the impact of concurrent input layers to the learning ability is shown. The introduced ideas have been illustrated with experimental results.
C1 [Zuters, Janis] Univ Latvia, Fac Comp, Raina Bulvaris 19, LV-1586 Riga, Latvia.
RP Zuters, J (corresponding author), Univ Latvia, Fac Comp, Raina Bulvaris 19, LV-1586 Riga, Latvia.
EM janis.zuters@lu.lv
CR [Anonymous], PULSED NEURAL NETWOR
   Burkitt AN, 2006, BIOL CYBERN, V95, P1, DOI 10.1007/s00422-006-0068-6
   Fausett L, 1993, FUNDAMENTALS NEURAL
   Hebb D., 2005, ORG BEHAV NEUROPSYCH
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Kempter R, 1999, PHYS REV E, V59, P4498, DOI 10.1103/PhysRevE.59.4498
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maass W., 1999, PULSED NEURAL NETWOR
   Salerno M, 2010, IEEE MEDITERR ELECT, P1039, DOI 10.1109/MELCON.2010.5475902
   Seung HS, 2003, NEURON, V40, P1063, DOI 10.1016/S0896-6273(03)00761-X
   Zuters J, 2009, FRONT ARTIF INTEL AP, V187, P131, DOI 10.3233/978-1-58603-939-4-131
NR 11
TC 0
Z9 0
U1 0
U2 0
PY 2010
BP 229
EP 242
UT WOS:000396178700015
DA 2023-11-16
ER

PT C
AU Xiang, YD
   Meng, JY
   Ma, D
AF Xiang, Yande
   Meng, Jianyi
   Ma, De
GP IEEE
TI A Load Balanced Mapping for Spiking Neural Network
SO 2017 2ND INTERNATIONAL CONFERENCE ON IMAGE, VISION AND COMPUTING (ICIVC
   2017)
DT Proceedings Paper
CT 2nd International Conference on Image, Vision and Computing (ICIVC)
CY JUN 02-04, 2017
CL Chengdu, PEOPLES R CHINA
DE neural mapping; NoC; spiking neural network (SNN); execution time
ID ON-CHIP
AB Network-on-Chip (NoC) provides a scalable and packet-based inter-connected architecture for spiking neural networks (SNNs). However, existing neural mapping strategies just distribute all neurons in a population to an on-chip network core or nearby cores sequentially. The neurons within a population take a huge time cost for handling spikes, which results to uneven workload distribution among different on chip network nodes. This paper presents a NoC-based SNN mapping that makes workload balance among different nodes, aiming to accelerate application execution time. The experimental results show that proposed mapping strategy reduces application execution time by average 24%.
C1 [Xiang, Yande] Zhejiang Univ, Coll Informat Sci & Elect Engn, Hangzhou, Zhejiang, Peoples R China.
   [Meng, Jianyi] Fudan Univ, Dept Microelect, State Key Lab ASIC & Syst, Shanghai, Peoples R China.
   [Ma, De] Hangzhou Dianzi Univ, MOE Key Lab RF Circuits & Syst, Hangzhou, Zhejiang, Peoples R China.
RP Xiang, YD (corresponding author), Zhejiang Univ, Coll Informat Sci & Elect Engn, Hangzhou, Zhejiang, Peoples R China.
EM 11531003@zju.edu.cn; mengjy@fudan.edu.cn; made@hdu.edu.cn
CR Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   [Anonymous], ACM T ARCHIT CODE OP
   [Anonymous], NORCHIP
   [Anonymous], ADV INTELLIGENT COMP
   [Anonymous], 2013, DAC, DOI DOI 10.1145/2463209.2488734
   [Anonymous], ICS3PP
   [Anonymous], IEEE 8 INT C ASIC
   [Anonymous], IJCNN
   Ascia G, 2004, INTERNATIONAL CONFERENCE ON HARDWARE/SOFTWARE CODESIGN AND SYSTEM SYNTHESIS, P182
   Cao S., 2016, MICROPROCESS MICROSY, P1
   Castilhos G, 2016, J SYST ARCHITECT, V63, P80, DOI 10.1016/j.sysarc.2016.01.005
   Hu W., 2016, J SYST ARCHIT, pl
   Murali S, 2005, ASIA S PACIF DES AUT, P27, DOI 10.1145/1120725.1120737
   Neil D, 2014, IEEE T VLSI SYST, V22, P2621, DOI 10.1109/TVLSI.2013.2294916
   Painkras E, 2013, IEEE J SOLID-ST CIRC, V48, P1943, DOI 10.1109/JSSC.2013.2259038
   Pande S, 2013, NEURAL PROCESS LETT, V38, P131, DOI 10.1007/s11063-012-9274-5
   Srinivasan K, 2005, I CONF VLSI DESIGN, P623, DOI 10.1109/ICVD.2005.113
NR 17
TC 1
Z9 1
U1 2
U2 2
PY 2017
BP 899
EP 903
UT WOS:000414298700177
DA 2023-11-16
ER

PT J
AU Zhang, L
   Lai, QX
   Chen, Y
AF Zhang, Lei
   Lai, Qianxi
   Chen, Yong
TI Configurable Neural Phase Shifter With Spike-Timing-Dependent Plasticity
SO IEEE ELECTRON DEVICE LETTERS
DT Article
DE Configurable neural phase shifter (NPS); spike neuromorphic circuits;
   spike-timing-dependent plasticity (STDP)
ID NEURONS; TRANSISTOR; SYNAPSES; STDP; VLSI
AB A configurable neural phase shifter (NPS) is fabricated for spike neuromorphic circuits by integrating an ionic/Si hybrid synaptic transistor with a conventional MOSFET. NPS can mimic a neural network to process temporal-code pulses by shifting their temporal phases Delta t ranged between 9.0 and 24.5 ms, which falls in the range observed in biological neural networks. Delta t can also be configured by following the synaptic learning rule of spike-timing-dependent plasticity. The device can significantly simplify the structures and reduce the power consumption of neuromorphic circuits to emulate neural networks for spike signal processing, memory, and learning.
C1 [Zhang, Lei; Lai, Qianxi; Chen, Yong] Univ Calif Los Angeles, Mech & Aerosp Engn Dept, Los Angeles, CA 90095 USA.
   [Zhang, Lei; Lai, Qianxi; Chen, Yong] Univ Calif Los Angeles, Calif NanoSyst Inst, Los Angeles, CA 90095 USA.
RP Zhang, L (corresponding author), Univ Calif Los Angeles, Mech & Aerosp Engn Dept, Los Angeles, CA 90095 USA.
EM zhanglei@seas.ucla.edu; qlai@ucla.edu; yongchen@seas.ucla.edu
CR [Anonymous], 1989, ANALOG VLSI NEURAL S
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Bofill-i-Petit A, 2004, IEEE T NEURAL NETWOR, V15, P1296, DOI 10.1109/TNN.2004.832842
   Cameron K, 2005, IEEE T NEURAL NETWOR, V16, P1626, DOI 10.1109/TNN.2005.852238
   Diorio C, 1996, IEEE T ELECTRON DEV, V43, P1972, DOI 10.1109/16.543035
   Gergel-Hackett N, 2009, IEEE ELECTR DEVICE L, V30, P706, DOI 10.1109/LED.2009.2021418
   Gerstner W., 2002, SPIKING NEURON MODEL
   Hsiao KJ, 2009, IEEE J SOLID-ST CIRC, V44, P2478, DOI 10.1109/JSSC.2009.2024804
   Huang F, 1997, APPL PHYS LETT, V71, P2415, DOI 10.1063/1.120078
   Ikegaya Y, 2004, SCIENCE, V304, P559, DOI 10.1126/science.1093173
   Indiveri G, 2006, IEEE T NEURAL NETWOR, V17, P211, DOI 10.1109/TNN.2005.860850
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Jo SH, 2010, NANO LETT, V10, P1297, DOI 10.1021/nl904092h
   Lai QX, 2008, NANO LETT, V8, P876, DOI 10.1021/nl073112y
   Rabaey J. M., 2003, DIGITAL INTEGRATED C
   Strukov DB, 2008, NATURE, V453, P80, DOI 10.1038/nature06932
NR 16
TC 11
Z9 14
U1 2
U2 28
PD JUL
PY 2010
VL 31
IS 7
BP 716
EP 718
DI 10.1109/LED.2010.2049558
UT WOS:000281833100028
DA 2023-11-16
ER

PT C
AU Lee, JJ
   Chen, JH
   Zhang, WR
   Li, P
AF Lee, Jeong-Jun
   Chen, Jianhao
   Zhang, Wenrui
   Li, Peng
GP IEEE
TI Systolic-Array Spiking Neural Accelerators with Dynamic Heterogeneous
   Voltage Regulation
SO 2021 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUL 18-22, 2021
CL ELECTR NETWORK
DE spiking neural networks; power delivery networks; systolic arrays;
   voltage regulation
ID IMPLEMENTATION
AB Spiking neural networks (SNNs) have emerged as a new generation of neural networks, presenting a brain-inspired event-driven model with advantages in spatiotemporal information processing. Due to the need for high power consumption of compute-intensive neural accelerators, adequate power delivery network (PDN) design is a key requirement to ensure power efficiency and integrity. However, PDN design for SNN accelerators has not been extensively studied despite its great potential benefit in energy efficiency.
   In this paper, we present the first study on dynamic heterogeneous voltage regulation (HVR) for spiking neural accelerators to maximize system energy efficiency while ensuring power integrity. We propose a novel sparse-workload-aware dynamic PDN control policy, which enables high energy efficiency of sparse spiking computation on a systolic array. By exploring sparse inputs and all-or-none nature of spiking computations for PDN control, we explore different types of PDNs to accelerate spiking convolutional neural networks (S-CNNs) trained with the dynamic vision sensor (DVS) gesture dataset. Furthermore, we demonstrate various power gating schemes to further optimize the proposed PDN architecture, which leads to a more than a three-fold reduction in total energy overhead for spiking neural computations on systolic array-based accelerators.
C1 [Lee, Jeong-Jun; Zhang, Wenrui; Li, Peng] Univ Calif Santa Barbara, Elect & Comp Engn, Santa Barbara, CA 93106 USA.
   [Chen, Jianhao] Texas A&M Univ, Elect & Comp Engn, College Stn, TX USA.
RP Lee, JJ (corresponding author), Univ Calif Santa Barbara, Elect & Comp Engn, Santa Barbara, CA 93106 USA.
EM jeong-jun@ucsb.edu; chenjh@tamu.edu; wenruizhang@ucsb.edu; lip@ucsb.edu
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   Afifi A, 2009, 2009 EUROPEAN CONFERENCE ON CIRCUIT THEORY AND DESIGN, VOLS 1 AND 2, P563, DOI 10.1109/ECCTD.2009.5275035
   Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   Amir A., 2017, P IEEE C COMP VIS PA, P7243, DOI DOI 10.1109/CVPR.2017.781
   Bellec G., 2018, ADV NEURAL INFORM PR
   Burkitt AN, 2006, BIOL CYBERN, V95, P1, DOI 10.1007/s00422-006-0068-6
   Chen JH, 2020, PR IEEE COMP DESIGN, P486, DOI 10.1109/ICCD50377.2020.00088
   Chen K, 2012, DES AUT TEST EUROPE, P33
   Chen L, 2014, NEURAL COMPUT APPL, V25, P393, DOI 10.1007/s00521-013-1501-0
   Chen TS, 2014, ACM SIGPLAN NOTICES, V49, P269, DOI 10.1145/2541940.2541967
   CHEN YH, 1952, IEEE J SOLID-ST CIRC, V52, P127
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Du ZD, 2015, 2015 ACM/IEEE 42ND ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA), P92, DOI 10.1145/2749469.2750389
   Gerstner W., 2002, SPIKING NEURON MODEL
   Gokhale V, 2014, IEEE COMPUT SOC CONF, P696, DOI 10.1109/CVPRW.2014.106
   JIN Y, 2018, ARXIV PREPRINT ARXIV
   Jin YYZ, 2016, IEEE INT SYMP NANO, P55, DOI 10.1145/2950067.2950100
   Kim W, 2008, INT S HIGH PERF COMP, P115
   Kung HT, 2019, TWENTY-FOURTH INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS (ASPLOS XXIV), P821, DOI 10.1145/3297858.3304028
   Lee JJ, 2020, PR IEEE COMP DESIGN, P57, DOI 10.1109/ICCD50377.2020.00027
   Lee JM, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00191
   Li HR, 2017, DES AUT TEST EUROPE, P1265, DOI 10.23919/DATE.2017.7927185
   LYM S, 2020, ARXIV PREPRINT ARXIV
   Peemen M, 2013, 2013 IEEE 31ST INTERNATIONAL CONFERENCE ON COMPUTER DESIGN (ICCD), P13, DOI 10.1109/ICCD.2013.6657019
   Riad J, 2020, INT SYM QUAL ELECT, P190, DOI [10.1109/ISQED48828.2020.9136985, 10.1109/isqed48828.2020.9136985]
   Samajdar Ananda, 2018, ARXIV PREPRINT ARXIV
   Wang SQ, 2020, J COMPUT SCI TECH-CH, V35, P475, DOI 10.1007/s11390-020-9686-z
   Wu X, 2019, NEURAL NETWORKS, V113, P72, DOI 10.1016/j.neunet.2019.01.010
   Zhan X, 2019, IEEE T VLSI SYST, V27, P2641, DOI 10.1109/TVLSI.2019.2923911
   ZHANG W, 2019, ADV NEURAL INFORM PR, P7802
   Zyuban V, 2015, IBM J RES DEV, V59, DOI 10.1147/JRD.2014.2380200
NR 31
TC 0
Z9 0
U1 0
U2 3
PY 2021
DI 10.1109/IJCNN52387.2021.9534037
UT WOS:000722581705115
DA 2023-11-16
ER

PT C
AU Zyarah, AM
   Soures, N
   Kudithipudi, D
AF Zyarah, Abdullah M.
   Soures, Nicholas
   Kudithipudi, Dhireesha
GP IEEE
TI On-Device Learning in Memristor Spiking Neural Networks
SO 2018 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS (ISCAS)
SE IEEE International Symposium on Circuits and Systems
DT Proceedings Paper
CT IEEE International Symposium on Circuits and Systems (ISCAS)
CY MAY 27-30, 2018
CL Florence, ITALY
AB In this paper, a memristor spiking neuron and synaptic trace circuits for efficient on device learning are presented. A key feature of these circuits is the use of memristors to emulate the membrane potential of spiking neurons, as opposed to the conventional use of a capacitor. The circuits are designed in IBM 65nm technology node and validated on a small-scale spiking neural network. It was observed that a 3x3 spiking neural network consumes 19.1 mu W of power at 100 MHz.
C1 [Zyarah, Abdullah M.; Soures, Nicholas; Kudithipudi, Dhireesha] Rochester Inst Technol, NanoComp Res Lab, Rochester, NY 14623 USA.
RP Zyarah, AM (corresponding author), Rochester Inst Technol, NanoComp Res Lab, Rochester, NY 14623 USA.
CR Aamir SA, 2016, PROC EUR SOLID-STATE, P71
   Chakraborti P, 2016, ELEC COMP C, P1958, DOI 10.1109/ECTC.2016.270
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Ebong IE, 2012, P IEEE, V100, P2050, DOI 10.1109/JPROC.2011.2173089
   Jang JW, 2016, IEEE T ELECTRON DEV, V63, P2610, DOI 10.1109/TED.2016.2549359
   Kornijcuk V, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00212
   Kvatinsky S, 2013, IEEE T CIRCUITS-I, V60, P211, DOI 10.1109/TCSI.2012.2215714
   Snider GS, 2008, 2008 IEEE INTERNATIONAL SYMPOSIUM ON NANOSCALE ARCHITECTURES, P85, DOI 10.1109/NANOARCH.2008.4585796
   Soudry D, 2015, IEEE T NEUR NET LEAR, V26, P2408, DOI 10.1109/TNNLS.2014.2383395
   Strukov DB, 2008, NATURE, V453, P80, DOI 10.1038/nature06932
   van Schaik A., 2010, 2010 IEEE International Symposium on Circuits and Systems. ISCAS 2010, P4253, DOI 10.1109/ISCAS.2010.5537564
   Watt Alanna J, 2010, Front Synaptic Neurosci, V2, P5, DOI 10.3389/fnsyn.2010.00005
   Wijekoon JHB, 2008, NEURAL NETWORKS, V21, P524, DOI 10.1016/j.neunet.2007.12.037
   Zyarah AM, 2017, IEEE INT SYMP CIRC S
NR 14
TC 3
Z9 3
U1 1
U2 2
PY 2018
DI 10.1109/ISCAS.2018.8351813
UT WOS:000451218704041
DA 2023-11-16
ER

PT J
AU Pugavko, MM
   Maslennikov, OV
   Nekorkin, VI
AF Pugavko, Mechislav M.
   Maslennikov, Oleg V.
   Nekorkin, Vladimir I.
TI Multitask computation through dynamics in recurrent spiking neural
   networks
SO SCIENTIFIC REPORTS
DT Article
ID WORKING-MEMORY; MODEL; TIME
AB In this work, inspired by cognitive neuroscience experiments, we propose recurrent spiking neural networks trained to perform multiple target tasks. These models are designed by considering neurocognitive activity as computational processes through dynamics. Trained by input-output examples, these spiking neural networks are reverse engineered to find the dynamic mechanisms that are fundamental to their performance. We show that considering multitasking and spiking within one system provides insightful ideas on the principles of neural computation.
C1 [Pugavko, Mechislav M.; Maslennikov, Oleg V.; Nekorkin, Vladimir I.] Russian Acad Sci, Inst Appl Phys, Nizhnii Novgorod 603950, Russia.
RP Maslennikov, OV (corresponding author), Russian Acad Sci, Inst Appl Phys, Nizhnii Novgorod 603950, Russia.
EM olmaov@ipfran.ru
CR Abbott LF, 2016, NAT NEUROSCI, V19, P350, DOI 10.1038/nn.4241
   Anokhin KV, 2021, ZH VYSSH NERV DEYAT+, V71, P39, DOI 10.31857/S0044467721010032
   Barak O, 2017, CURR OPIN NEUROBIOL, V46, P1, DOI 10.1016/j.conb.2017.06.003
   Brette R, 2005, J NEUROPHYSIOL, V94, P3637, DOI 10.1152/jn.00686.2005
   BRITTEN KH, 1992, J NEUROSCI, V12, P4745
   Brücke C, 2008, EUR J NEUROSCI, V27, P2322, DOI 10.1111/j.1460-9568.2008.06203.x
   Brücke C, 2013, NEUROIMAGE, V75, P36, DOI 10.1016/j.neuroimage.2013.02.038
   Chaisangmongkon W, 2017, NEURON, V93, P1504, DOI 10.1016/j.neuron.2017.03.002
   Cichy RM, 2019, TRENDS COGN SCI, V23, P305, DOI 10.1016/j.tics.2019.01.009
   Dubreuil A, 2022, NAT NEUROSCI, V25, P783, DOI 10.1038/s41593-022-01088-4
   FUNAHASHI S, 1989, J NEUROPHYSIOL, V61, P331, DOI 10.1152/jn.1989.61.2.331
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Hassabis D, 2017, NEURON, V95, P245, DOI 10.1016/j.neuron.2017.06.011
   Hotelling H, 1933, J EDUC PSYCHOL, V24, P417, DOI 10.1037/h0071325
   Kingma DP., 2017, ARXIV
   Kobak D, 2016, ELIFE, V5, DOI 10.7554/eLife.10989
   Lloyd S., 1957, IEEE T INFORM THEOR, V18, P11
   Lukosevicius M, 2009, COMPUT SCI REV, V3, P127, DOI 10.1016/j.cosrev.2009.03.005
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Mante V, 2013, NATURE, V503, P78, DOI 10.1038/nature12742
   Marblestone AH, 2016, FRONT COMPUT NEUROSC, V10, DOI 10.3389/fncom.2016.00094
   Maslennikov OV, 2021, IZV VUZ-PRIKL NELINE, V29, P799, DOI 10.18500/0869-6632-2021-29-5-799-811
   Maslennikov O. V., 2022, PHYS-USP+, V192, P1089
   Maslennikov OV, 2020, NONLINEAR DYNAM, V101, P1093, DOI 10.1007/s11071-020-05787-0
   Maslennikova OV, 2019, CHAOS, V29, DOI 10.1063/1.5119895
   Murray JD, 2017, P NATL ACAD SCI USA, V114, P394, DOI 10.1073/pnas.1619449114
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Nicola W, 2017, NAT COMMUN, V8, DOI 10.1038/s41467-017-01827-3
   Paszke A., 2019, ADV NEURAL INFORM PR
   Pehle Christian-Gernot, 2021, Zenodo
   Pugavko M., 2021, IZV VUZ RADIOFIZ+, V64, P817, DOI [10.52452/00213462_2021_64_10_817, DOI 10.52452/00213462_2021_64_10_817]
   Pugavko MM, 2020, COMMUN NONLINEAR SCI, V90, DOI 10.1016/j.cnsns.2020.105399
   Pyle R, 2017, PHYS REV LETT, V118, DOI 10.1103/PhysRevLett.118.018103
   Rabinovich MI, 2012, COMPUT NEUROSCI-MIT, P1
   Ramezanian-Panahi M, 2022, FRONT ARTIF INTELL, V5, DOI 10.3389/frai.2022.807406
   Yang GR, 2021, CURR OPIN NEUROBIOL, V70, P182, DOI 10.1016/j.conb.2021.10.015
   Romo R, 1999, NATURE, V399, P470, DOI 10.1038/20939
   Shen JR, 2021, NEURAL COMPUT, V33, P2971, DOI 10.1162/neco_a_01432
   Sporns O, 2005, PLOS COMPUT BIOL, V1, P245, DOI 10.1371/journal.pcbi.0010042
   Steinhaus H, 1956, B ACAD POL SCI, V4, P801, DOI DOI 10.1126/SCIENCE.3287615
   Sussillo D, 2014, CURR OPIN NEUROBIOL, V25, P156, DOI 10.1016/j.conb.2014.01.008
   Sussillo D, 2013, NEURAL COMPUT, V25, P626, DOI 10.1162/NECO_a_00409
   Taherkhani A, 2020, NEURAL NETWORKS, V122, P253, DOI 10.1016/j.neunet.2019.09.036
   Vyas S, 2020, ANNU REV NEUROSCI, V43, P249, DOI 10.1146/annurev-neuro-092619-094115
   Wang XW, 2020, NEURAL NETWORKS, V125, P258, DOI 10.1016/j.neunet.2020.02.011
   Xue XH, 2022, COGN COMPUT, DOI 10.1007/s12559-022-09994-2
   Yang GR, 2020, NEURON, V107, P1048, DOI 10.1016/j.neuron.2020.09.005
   Yang GR, 2019, CURR OPIN BEHAV SCI, V29, P134, DOI 10.1016/j.cobeha.2019.07.001
   Yang GR, 2019, NAT NEUROSCI, V22, P297, DOI 10.1038/s41593-018-0310-2
   Zenke F, 2018, NEURAL COMPUT, V30, P1514, DOI 10.1162/neco_a_01086
   Zhang WR, 2019, ADV NEUR IN, V32
   Zhang XH, 2021, ISCIENCE, V24, DOI 10.1016/j.isci.2021.102919
   Zhang XX, 2019, ELIFE, V8, DOI 10.7554/eLife.43191
NR 53
TC 0
Z9 0
U1 3
U2 4
PD MAR 10
PY 2023
VL 13
IS 1
AR 3997
DI 10.1038/s41598-023-31110-z
UT WOS:000989359600005
DA 2023-11-16
ER

PT J
AU Mayr, C
   Schüffny, R
AF Mayr, C
   Schüffny, R
TI Applying spiking neural nets to noise shaping
SO IEICE TRANSACTIONS ON INFORMATION AND SYSTEMS
DT Article
DE neural noise shaping; pulse-coupled neural nets; neural net optimization
   with genetic algorithm; spike pulse processing; spiking neural net
   analog-digital conversion
AB In recent years, there has been an increased focus on the mechanics of information transmission in spiking neural networks. Especially the Noise Shaping properties of these networks and their similarity to Delta-Sigma Modulators has received a lot of attention. However, very little of the research done in this area has focused on the effect the weights in these networks have on the Noise Shaping properties and on post-processing of the network output signal. This paper concerns itself with the various modes of network operation and beneficial as well as detrimental effects which the systematic generation of network weights can effect. Also, a method for post-processing of the spiking output signal is introduced, bringing the output signal more in line with conventional Delta-Sigma Modulators. Relevancy of this research to industrial application of neural nets as building blocks of oversampled A/D converters is shown. Also, further points of contention are listed, which must be thoroughly researched to add to the above mentioned applicability of spiking neural nets.
C1 Dresden Univ Technol, Lehrstuhl Hochparallele VLSI Syst & Neuromikroele, D-8027 Dresden, Germany.
RP Mayr, C (corresponding author), Dresden Univ Technol, Lehrstuhl Hochparallele VLSI Syst & Neuromikroele, D-8027 Dresden, Germany.
EM mayr@iee.et.tu-dresden.de
CR BLACK WC, 1980, IEEE J SOLID-ST CIRC, V15, P1022, DOI 10.1109/JSSC.1980.1051512
   CHIPPERFIELD A, 1996, GENETIC ALGORITHM TO
   Coello CAC, 2001, LECT NOTES COMPUT SC, V1993, P21
   Gabbiani F, 1999, J EXP BIOL, V202, P1267
   Galton I, 1995, IEEE T CIRCUITS-II, V42, P773, DOI 10.1109/82.476175
   GESTNER W, 1999, C PUBICATION IEE, V470, P7
   MARIENBORG JT, 2002, ISCAS 02, V5, P26
   Norsworthy S., 1996, DELTA SIGMA DATA CON
   Stanley KO, 2002, P 2002 C EV COMP CEC
NR 9
TC 2
Z9 2
U1 0
U2 2
PD AUG
PY 2005
VL E88D
IS 8
BP 1885
EP 1892
DI 10.1093/ietisy/e88-d.8.1885
UT WOS:000231440500015
DA 2023-11-16
ER

PT J
AU Yang, RH
   Song, AG
   Yuan, WJ
AF Yang, Renhuan
   Song, Aiguo
   Yuan, Wujie
TI ENHANCEMENT OF SPIKE SYNCHRONY IN HINDMARSH-ROSE NEURAL NETWORKS BY
   RANDOMLY REWIRING CONNECTIONS
SO MODERN PHYSICS LETTERS B
DT Article
DE Spike synchrony; Hindmarsh-Rose neural networks; connection rewiring;
   synchronization
ID STOCHASTIC RESONANCE; FIRE NEURON; LEAKY INTEGRATE; MODEL; NOISE
AB Spike synchrony of the neural system is thought to have very dichotomous roles. On the one hand, it is ubiquitously present in the healthy brain and is thought to underlie feature binding during information processing. On the other hand, large scale synchronization is an underlying mechanism of epileptic seizures. In this paper, we investigate the spike synchrony of Hindmarsh-Rose (HR) neural networks. Our focus is the influence of the network connections on the spike synchrony of the neural networks. The simulations show that desynchronization in the nearest-neighbor coupled network evolves into accurate synchronization with connection-rewiring probability p increasing. We uncover a phenomenon of enhancement of spike synchrony by randomly rewiring connections. With connection strength c and average connection number m increasing spike synchrony is enhanced but it is not the whole story. Furthermore, the possible mechanism behind such synchronization is also addressed.
C1 [Yang, Renhuan; Song, Aiguo] Southeast Univ, Dept Instrument Sci & Engn, Nanjing 210096, Peoples R China.
   [Yuan, Wujie] Huaibei Coal Ind Teachers Coll, Dept Phys & Elect Informat, Huaibei 235000, Anhui, Peoples R China.
RP Song, AG (corresponding author), Southeast Univ, Dept Instrument Sci & Engn, Nanjing 210096, Peoples R China.
EM yangli357616338@126.com; a.g.song@seu.edu.cn; yuanwj2005@163.com
CR Aguirre C, 2002, LECT NOTES COMPUT SC, V2415, P27
   Battogtokh D, 2007, MOD PHYS LETT B, V21, P2033, DOI 10.1142/S0217984907014395
   Bucolo M, 2002, CHAOS SOLITON FRACT, V14, P1059, DOI 10.1016/S0960-0779(02)00042-5
   COLLINS JJ, 1995, NATURE, V376, P236, DOI 10.1038/376236a0
   Deransart C, 2003, EPILEPSIA, V44, P1513, DOI 10.1111/j.0013-9580.2003.26603.x
   Dhamala M, 2004, PHYS REV LETT, V92, DOI 10.1103/PhysRevLett.92.028101
   Dhamala M, 2002, PHYS LETT A, V292, P269, DOI 10.1016/S0375-9601(01)00790-3
   ECKHORN R, 1988, BIOL CYBERN, V60, P121, DOI 10.1007/BF00202899
   Ferri R, 2004, CLIN NEUROPHYSIOL, V115, P1202, DOI 10.1016/j.clinph.2003.12.014
   Gong YB, 2006, PHYS REV E, V73, DOI 10.1103/PhysRevE.73.046137
   González-Miranda JM, 2003, CHAOS, V13, P845, DOI 10.1063/1.1594851
   HINDMARSH JL, 1982, NATURE, V296, P162, DOI 10.1038/296162a0
   Huerta R, 1998, EUROPHYS LETT, V43, P719, DOI 10.1209/epl/i1998-00423-y
   KAASPETERSEN C, 1987, CHAOS BIOL SYSTEMS
   Kang YM, 2005, CHAOS SOLITON FRACT, V25, P165, DOI 10.1016/j.chaos.2004.09.123
   Kwon O, 2002, PHYS LETT A, V298, P319, DOI 10.1016/S0375-9601(02)00575-3
   Li QS, 2005, CHEM PHYS LETT, V416, P33, DOI 10.1016/j.cplett.2005.09.038
   Lin M, 2005, PHYS REV E, V71, DOI 10.1103/PhysRevE.71.016133
   Lindner B, 2001, PHYS REV LETT, V86, P2934, DOI 10.1103/PhysRevLett.86.2934
   Meng J, 2008, MOD PHYS LETT B, V22, P181, DOI 10.1142/S0217984908014596
   Patel A, 2005, NEURAL NETWORKS, V18, P467, DOI 10.1016/j.neunet.2005.06.031
   Shimokawa T, 1999, PHYS REV E, V59, P3427, DOI 10.1103/PhysRevE.59.3427
   Shimokawa T, 1999, PHYS REV E, V59, P3461, DOI 10.1103/PhysRevE.59.3461
   Stocks NG, 2001, PHYS REV E, V64, DOI 10.1103/PhysRevE.64.030902
   STROGATZ SH, 1988, PHYS REV LETT, V61, P2380, DOI 10.1103/PhysRevLett.61.2380
   Wang MS, 2006, CHEMPHYSCHEM, V7, P579, DOI 10.1002/cphc.200500499
   Wang QY, 2007, PHYSICA A, V374, P869, DOI 10.1016/j.physa.2006.08.062
   Watts DJ, 1998, NATURE, V393, P440, DOI 10.1038/30918
   Wu Y, 2005, CHAOS SOLITON FRACT, V23, P1605, DOI 10.1016/j.chaos.2004.06.077
   Xie Y, 2004, CHAOS SOLITON FRACT, V22, P151, DOI 10.1016/j.chaos.2004.01.001
   Zheng YH, 2008, PHYSICA A, V387, P3719, DOI 10.1016/j.physa.2008.02.039
NR 31
TC 6
Z9 6
U1 0
U2 18
PD MAY 10
PY 2009
VL 23
IS 11
BP 1405
EP 1414
DI 10.1142/S0217984909019533
UT WOS:000266353500004
DA 2023-11-16
ER

PT J
AU Guo, L
   Zhang, W
   Zhang, JL
AF Guo, Lei
   Zhang, Wei
   Zhang, Jialei
TI Neural information coding on small-world spiking neuronal networks
   modulated by spike-timing-dependent plasticity under external noise
   stimulation
SO CLUSTER COMPUTING-THE JOURNAL OF NETWORKS SOFTWARE TOOLS AND
   APPLICATIONS
DT Article
DE Spike-timing-dependent plasticity; Small-world network; Rate coding;
   Temporal coding
ID SYNCHRONIZATION; INHIBITION; EXCITATION; INTEGRATE; MODEL
AB Neural information coding is the fundamental of information processing mechanism in biological neural network. The study of neural information coding can help to understand the function of information processing in biological neural network and lay the theoretical foundation for improving bionic ability. As the abstract of a large number of real complex systems, small-world networks have the properties of biological neural networks. However, the neural information coding based on the small-world topology is rarely studied and the information transmission mechanism among the neurons is mostly excitatory regulation mechanism of spike-timing-dependent plasticity (STDP). In this paper, the small-world network is constructed and its properties are analyzed; the small-world spiking neural network based on the more complete STDP including excitatory synapse and inhibitory synapse is constructed; from the angle of firing rate of neurons and the temporal structure of the spike train, the properties of information coding on the small-world spiking neural network under the stimulations of white Gauss noise and impulse noise are analyzed respectively. Our experimental results indicate that under the same stimulation, the responses of the mean rate coding and ISI coding of the small-world network are both enhanced with the increase of stimulation intensity; under different stimulations, the mean rate coding and ISI coding of the small-world network show respective specificity.
C1 [Guo, Lei; Zhang, Wei; Zhang, Jialei] Hebei Univ Technol, State Key Lab Reliabil & Intelligence Elect Equip, Tianjin 300130, Peoples R China.
RP Guo, L (corresponding author), Hebei Univ Technol, State Key Lab Reliabil & Intelligence Elect Equip, Tianjin 300130, Peoples R China.
EM zhangwei19920106@163.com
CR Effenberger F, 2015, PLOS COMPUT BIOL, V11, DOI 10.1371/journal.pcbi.1004420
   George D, 2005, NEUROCOMPUTING, V65, P415, DOI 10.1016/j.neucom.2004.10.038
   Goudar V, 2015, J NEUROPHYSIOL, V113, P509, DOI 10.1152/jn.00568.2014
   [郭磊 Guo Lei], 2015, [电工技术学报, Transactions of China Electrotechnical Society], V30, P31
   Guo L, 2014, BIO-MED MATER ENG, V24, P1063, DOI 10.3233/BME-130904
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Kleberg FI, 2014, FRONT COMPUT NEUROSC, V8, DOI 10.3389/fncom.2014.00053
   Laurent G, 1996, J NEUROSCI, V16, P3837
   Litwin-Kumar A, 2014, NAT COMMUN, V5, DOI 10.1038/ncomms6319
   Mikkelsen K, 2013, PHYS REV LETT, V110, DOI 10.1103/PhysRevLett.110.208101
   Panzeri S, 2017, NEURON, V93, P491, DOI 10.1016/j.neuron.2016.12.036
   Parker D, 2010, PHILOS T R SOC B, V365, P2315, DOI 10.1098/rstb.2010.0043
   Perrinet L, 2001, NEUROCOMPUTING, V38, P817, DOI 10.1016/S0925-2312(01)00460-X
   Qu Haibo, 2016, Sheng Wu Yi Xue Gong Cheng Xue Za Zhi, V33, P931
   Rumbell T, 2014, IEEE T NEUR NET LEAR, V25, P894, DOI 10.1109/TNNLS.2013.2283140
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Srinivasa N, 2014, FRONT COMPUT NEUROSC, V8, DOI 10.3389/fncom.2014.00159
   Tsubo Y, 2012, PLOS COMPUT BIOL, V8, DOI 10.1371/journal.pcbi.1002461
   van Rossum MCW, 2002, J NEUROSCI, V22, P1956, DOI 10.1523/JNEUROSCI.22-05-01956.2002
   Vogels TP, 2011, SCIENCE, V334, P1569, DOI 10.1126/science.1211095
   Wang ML, 2015, ACTA PHYS SINICA, V64, P416
   Wang R, 2017, CHEM GEOL, V451, P116, DOI 10.1016/j.chemgeo.2017.01.010
   Wei Y, 2014, J NEUROSCI, V34, P15804, DOI 10.1523/JNEUROSCI.3929-12.2014
   Yang Y, 2016, J NEUROSCI, V36, P11999, DOI 10.1523/JNEUROSCI.1475-16.2016
   Yu HT, 2015, PHYSICA A, V419, P307, DOI 10.1016/j.physa.2014.10.031
   Yu HT, 2013, NEUROCOMPUTING, V99, P178, DOI 10.1016/j.neucom.2012.03.019
   Yu K, 2013, COGN NEURODYNAMICS, V7, P237, DOI 10.1007/s11571-012-9233-x
   Zemanová L, 2008, PRAMANA-J PHYS, V70, P1087, DOI 10.1007/s12043-008-0113-1
NR 28
TC 3
Z9 4
U1 1
U2 19
PD MAY
PY 2019
VL 22
SU 3
BP S5217
EP S5231
DI 10.1007/s10586-017-1188-6
UT WOS:000499872200009
DA 2023-11-16
ER

PT J
AU Takada, K
   Tateno, K
AF Takada, Kensuke
   Tateno, Katsumi
TI Real-time computation of a large-scaled entorhinal-hippocambal spiking
   neural network using GPU acceleration
SO IEICE NONLINEAR THEORY AND ITS APPLICATIONS
DT Article
DE spiking neural network; GPU; hippocampus; place cell
ID SPATIAL MAP; MODEL; CA3
AB This study investigates the real-time computation of a large-scaled spiking neural network using graphic processing units. A randomly coupled network comprising several hundred thousand spiking neurons was computed in real-time. We also developed an entorhinal-hippocampal neural network consisting of approximately 50,000 spiking neurons and implemented a mechanism to form place cells in the hippocampal network through the entorhinal cortex based on the direction of motion and velocity of a mobile robot. In an experiment using a real mobile robot, we confirmed that place cells were formed in the hippocampus while the robot moved through a square open field.
C1 [Takada, Kensuke; Tateno, Katsumi] Kyushu Inst Technol, Dept Human Intelligence Syst, Wakamatsu Ku, 2-4 Hibikino, Kitakyushu, Fukuoka 8080196, Japan.
   [Tateno, Katsumi] Kyushu Inst Technol, Res Ctr Neuromorph AI Hardware, Wakamatsu Ku, 2-4 Hibikino, Kitakyushu, Fukuoka 8080196, Japan.
RP Takada, K (corresponding author), Kyushu Inst Technol, Dept Human Intelligence Syst, Wakamatsu Ku, 2-4 Hibikino, Kitakyushu, Fukuoka 8080196, Japan.
EM takada.kensuke587@mail.kyutech.jp
CR BOSS BD, 1987, BRAIN RES, V406, P280, DOI 10.1016/0006-8993(87)90793-1
   Bradski GR, 1998, FOURTH IEEE WORKSHOP ON APPLICATIONS OF COMPUTER VISION - WACV'98, PROCEEDINGS, P214, DOI 10.1109/ACV.1998.732882
   Fujita K, 2018, COMPUTING, V100, P907, DOI 10.1007/s00607-018-0590-0
   Guanella A, 2007, INT J NEURAL SYST, V17, P231, DOI 10.1142/S0129065707001093
   Guzman SJ, 2016, SCIENCE, V353, P1117, DOI 10.1126/science.aaf1836
   Hafting T, 2005, NATURE, V436, P801, DOI 10.1038/nature03721
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Igarashi J, 2019, FRONT NEUROINFORM, V13, DOI 10.3389/fninf.2019.00071
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   OKEEFE J, 1971, BRAIN RES, V34, P171, DOI 10.1016/0006-8993(71)90358-1
   Yamazaki T, 2013, NEURAL NETWORKS, V47, P103, DOI 10.1016/j.neunet.2013.01.019
NR 11
TC 0
Z9 0
U1 0
U2 0
PY 2022
VL 13
IS 2
BP 349
EP 354
DI 10.1587/nolta.13.349
UT WOS:000778784900028
DA 2023-11-16
ER

PT C
AU Zuters, J
AF Zuters, Janis
BE Haav, HM
   Kalja, A
TI Spiking Neural Networks to Detect Temporal Patterns
SO DATABASES AND INFORMATION SYSTEMS V
SE Frontiers in Artificial Intelligence and Applications
DT Proceedings Paper
CT 8th International Baltic Conference on Databases and Information Systems
CY JUN 02-05, 2008
CL Tallinn, ESTONIA
DE spiking neural networks; unsupervised learning; adaptive activation;
   temporal patterns
AB Spiking neural networks (SNNs) are more powerful than their non-spiking predecessors as they can encode temporal information in their signals, but they also need different and biologically more plausible rules for synaptic plasticity. In this paper, an unsupervised learning algorithm is introduced for a spiking neural network. The algorithm is based on the Hebbian rule, with the addition of the principle of adapting a layer activation level so as to guarantee frequent firings of neurons for each layer. The proposed algorithm has been illustrated with experimental results related to the detection of temporal patterns in an input stream.
C1 Univ Latvia, Dept Comp, LV-1050 Riga, Latvia.
RP Zuters, J (corresponding author), Univ Latvia, Dept Comp, Raina Bulvaris 19, LV-1050 Riga, Latvia.
EM janis.zuters@lu.lv
CR [Anonymous], PULSED NEURAL NETWOR
   Fausett L., 1994, FUNDAMENTALS NEURAL
   Haykin S., 1999, NEURAL NETWORKS COMP
   HEBB D. O., 1949
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maass W., 1999, PULSED NEURAL NETWOR
   Roberts PD, 2002, BIOL CYBERN, V87, P392, DOI 10.1007/s00422-002-0361-y
NR 7
TC 2
Z9 2
U1 0
U2 0
PY 2009
VL 187
BP 131
EP 142
DI 10.3233/978-1-58603-939-4-131
UT WOS:000273295400011
DA 2023-11-16
ER

PT J
AU Volinski, A
   Zaidel, Y
   Shalumov, A
   DeWolf, T
   Supic, L
   Tsur, EE
AF Volinski, Alex
   Zaidel, Yuval
   Shalumov, Albert
   DeWolf, Travis
   Supic, Lazar
   Tsur, Elishai Ezra
TI Data-driven artificial and spiking neural networks for inverse
   kinematics in neurorobotics
SO PATTERNS
DT Article
ID CLUSTER NEWTON METHOD
AB Inverse kinematics is fundamental for computational motion planning. It is used to derive an appropriate state in a robot's configuration space, given a target position in task space. In this work, we investigate the performance of fully connected and residual artificial neural networks as well as recurrent, learning-based, and deep spiking neural networks for conventional and geometrically constrained inverse kinematics. We show that while highly parameterized data-driven neural networks with tens to hundreds of thousands of parameters exhibit sub-ms inference time and sub-mm accuracy, learning-based spiking architectures can provide reasonably good results with merely a few thousand neurons. Moreover, we show that spiking neural networks can perform well in geometrically constrained task space, even when configured to an energy conserved spiking rate, demonstrating their robustness. Neural networks were evaluated on NVIDIA's Xavier and Intel's neuromorphic Loihi chip.
C1 [Volinski, Alex; Zaidel, Yuval; Shalumov, Albert; Tsur, Elishai Ezra] Open Univ Israel, Neurobiomorph Engn Lab, Raanana, Israel.
   [DeWolf, Travis] Appl Brain Res, Waterloo, ON, Canada.
   [Supic, Lazar] Accenture Labs, San Francisco, CA USA.
RP Tsur, EE (corresponding author), Open Univ Israel, Neurobiomorph Engn Lab, Raanana, Israel.
EM elishai@nbel-lab.com
CR Almusawi A.R., 2016, NEW ARTIFICIAL NEURA
   [Anonymous], 2021, NVIDIA
   Aoki Y, 2014, SIAM J SCI COMPUT, V36, pB14, DOI 10.1137/120885462
   Bekolay T, 2014, FRONT NEUROINFORM, V7, DOI 10.3389/fninf.2013.00048
   Bing ZS, 2018, FRONT NEUROROBOTICS, V12, DOI 10.3389/fnbot.2018.00035
   Burkitt AN, 2006, BIOL CYBERN, V95, P1, DOI 10.1007/s00422-006-0068-6
   Buss S. R., 2004, IEEE J ROBOT AUTOM, V17, P16
   Cardiff M, 2008, COMPUT GEOSCI-UK, V34, P1480, DOI 10.1016/j.cageo.2008.01.013
   Chen DW, 2020, ENTROPY-SWITZ, V22, DOI 10.3390/e22020193
   Csiszar A, 2017, I C MECH MACH VIS PR, P372
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   DeWolf T, 2021, SCI ROBOT, V6, DOI 10.1126/scirobotics.abk3268
   DeWolf T, 2016, P ROY SOC B-BIOL SCI, V283, DOI 10.1098/rspb.2016.2134
   Dietrich A, 2015, INT J ROBOT RES, V34, P1385, DOI 10.1177/0278364914566516
   Duka AV, 2014, PROC TECH, V12, P20, DOI 10.1016/j.protcy.2013.12.451
   Eliasmith C., 2003, NEURAL ENG COMPUTATI
   Gaudreau P, 2015, J COMPUT APPL MATH, V283, P122, DOI 10.1016/j.cam.2015.01.014
   GROCHOW K, 2004, STYLE BASED INVERSE
   Hagras HA, 2004, IEEE T FUZZY SYST, V12, P524, DOI 10.1109/TFUZZ.2004.832538
   Hazan A, 2021, FRONT NEUROSCI-SWITZ, V15, DOI 10.3389/fnins.2021.627221
   Hunsberger E., 2016, ARXIV 1611 05141
   Hyun CM, 2021, MED IMAGE ANAL, V69, DOI 10.1016/j.media.2021.101967
   Lanfranco AR, 2004, ANN SURG, V239, P14, DOI 10.1097/01.sla.0000103020.19595.7d
   Li Yaotong, 1993, NEURAL NETWORKS ROBO, V202, DOI [10.1007/978-1-4615-3180-7_6, DOI 10.1007/978-1-4615-3180-7_6]
   Lin CK, 2018, COMPUTER, V51, P52, DOI 10.1109/MC.2018.157113521
   Liu XY, 2013, WATER RESOUR RES, V49, P6587, DOI 10.1002/wrcr.20489
   Lynch K. M., 2017, MODERN ROBOTICS
   Martin A., 2016, S OPERATING SYSTEMS
   Martin G., 2020, ARXIV
   Misra D., 2019, ARXIV 190808681
   Nishida SI, 2009, ACTA ASTRONAUT, V65, P95, DOI 10.1016/j.actaastro.2009.01.041
   Ramachandran P., 2017, ARXIV 1710 05941
   Ranjan JAK, 2020, J SUPERCOMPUT, V76, P6545, DOI 10.1007/s11227-019-02881-y
   Rasmussen D, 2019, NEUROINFORMATICS, V17, P611, DOI 10.1007/s12021-019-09424-z
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707
   Tsur E. E., 2021, NEUROMORPHIC ENG SCI, DOI 10.1201/9781003143499
   Tsur EE, 2020, NEUROCOMPUTING, V374, P54, DOI 10.1016/j.neucom.2019.09.072
   Voelker A.R., 2015, SOLUTION DYNAMICS PR
   Voruganti H K, 2020, SN APPL SCI, V2, P1
   Wang XQ, 2021, MEASUREMENT, V171, DOI 10.1016/j.measurement.2020.108821
   Zaidel Y, 2021, FRONT NEUROROBOTICS, V15, DOI 10.3389/fnbot.2021.631159
NR 41
TC 6
Z9 6
U1 2
U2 10
PD JAN 14
PY 2022
VL 3
IS 1
DI 10.1016/j.patter.2021.100391
EA JAN 2022
UT WOS:000784409700011
DA 2023-11-16
ER

PT J
AU Zheng, YJ
   Jia, SS
   Yu, ZF
   Huang, TJ
   Liu, JK
   Tian, YH
AF Zheng, Yajing
   Jia, Shanshan
   Yu, Zhaofei
   Huang, Tiejun
   Liu, Jian K.
   Tian, Yonghong
TI Probabilistic inference of binary Markov random fields in spiking neural
   networks through mean-field approximation
SO NEURAL NETWORKS
DT Article
DE Probabilistic inference; Markov Random Fields (MRFs); Spiking Neural
   Networks (SNNs); Recurrent Neural Networks (RNNs); Mean-field
   approximation
ID BAYESIAN-INFERENCE; BELIEF PROPAGATION; UNCERTAINTY; CONFIDENCE; VISION
AB Recent studies have suggested that the cognitive process of the human brain is realized as probabilistic inference and can be further modeled by probabilistic graphical models like Markov random fields. Nevertheless, it remains unclear how probabilistic inference can be implemented by a network of spiking neurons in the brain. Previous studies have tried to relate the inference equation of binary Markov random fields to the dynamic equation of spiking neural networks through belief propagation algorithm and reparameterization, but they are valid only for Markov random fields with limited network structure. In this paper, we propose a spiking neural network model that can implement inference of arbitrary binary Markov random fields. Specifically, we design a spiking recurrent neural network and prove that its neuronal dynamics are mathematically equivalent to the inference process of Markov random fields by adopting mean-field theory. Furthermore, our mean-field approach unifies previous works. Theoretical analysis and experimental results, together with the application to image denoising, demonstrate that our proposed spiking neural network can get comparable results to that of mean-field inference. (c) 2020 Elsevier Ltd. All rights reserved.
C1 [Zheng, Yajing; Jia, Shanshan; Yu, Zhaofei; Huang, Tiejun; Tian, Yonghong] Peking Univ, Dept Comp Sci & Technol, Natl Engn Lab Video Technol, Beijing 100871, Peoples R China.
   [Zheng, Yajing; Jia, Shanshan; Yu, Zhaofei; Huang, Tiejun; Tian, Yonghong] Peng Cheng Lab, Shenzhen 518055, Peoples R China.
   [Liu, Jian K.] Univ Leicester, Dept Neurosci Psychol & Behav, Ctr Syst Neurosci, Leicester LE1 7HA, Leics, England.
RP Yu, ZF; Tian, YH (corresponding author), Peking Univ, Dept Comp Sci & Technol, Natl Engn Lab Video Technol, Beijing 100871, Peoples R China.
EM yuzf12@pku.edu.cn; yhtian@pku.edu.cn
CR [Anonymous], 2007, 2007 IEEE C COMPUTER
   [Anonymous], 2016, ADV NEURAL INFORM PR
   [Anonymous], 2001, THEORETICAL NEUROSCI
   [Anonymous], 2005, ADV NEURAL INF PROCE
   Bays PM, 2007, J PHYSIOL-LONDON, V578, P387, DOI 10.1113/jphysiol.2006.120121
   Buonomano DV, 2009, NAT REV NEUROSCI, V10, P113, DOI 10.1038/nrn2558
   Chater N, 2006, TRENDS COGN SCI, V10, P292, DOI 10.1016/j.tics.2006.05.008
   Deneve S, 2008, NEURAL COMPUT, V20, P91, DOI 10.1162/neco.2008.20.1.91
   Dong JW, 2015, IEEE T MED IMAGING, V34, P531, DOI 10.1109/TMI.2014.2361764
   Doya K., 2007, BAYESIAN BRAIN PROBA
   Felzenszwalb PF, 2006, INT J COMPUT VISION, V70, P41, DOI 10.1007/s11263-006-7899-4
   Fischl B, 2002, NEURON, V33, P341, DOI 10.1016/S0896-6273(02)00569-X
   Guo S., 2017, IEEE T CYBERNET, V49, P133, DOI [10.1109/TCYB.2017.2768554, DOI 10.1109/TCYB.2017.2768554]
   Jampani V, 2015, COMPUT VIS IMAGE UND, V136, P32, DOI 10.1016/j.cviu.2015.03.002
   Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178
   Kersten D, 2004, ANNU REV PSYCHOL, V55, P271, DOI 10.1146/annurev.psych.55.090902.142005
   Knill D., 1996, PERCEPTION BAYESIAN
   Koller D., 2009, PROBABILISTIC GRAPHI
   Körding KP, 2004, NATURE, V427, P244, DOI 10.1038/nature02169
   Litvak S, 2009, NEURAL COMPUT, V21, P3010, DOI 10.1162/neco.2009.05-08-783
   Liu JK, 2009, J NEUROSCI, V29, P13172, DOI 10.1523/JNEUROSCI.2358-09.2009
   Ma WJ, 2014, ANNU REV NEUROSCI, V37, P205, DOI 10.1146/annurev-neuro-071013-014017
   Meyniel F, 2015, NEURON, V88, P78, DOI 10.1016/j.neuron.2015.09.039
   Ming YS, 2010, NEURAL COMPUT, V22, P2161, DOI 10.1162/NECO_a_00005-Ming
   Ott Thomas, 2007, ADV NEURAL INFORM PR, P1057
   Pouget A, 2016, NAT NEUROSCI, V19, P366, DOI 10.1038/nn.4240
   Pouget A, 2013, NAT NEUROSCI, V16, P1170, DOI 10.1038/nn.3495
   Probst D, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00013
   Rao RPN, 2004, NEURAL COMPUT, V16, P1, DOI 10.1162/08997660460733976
   Shi ZH, 2013, TRENDS COGN SCI, V17, P556, DOI 10.1016/j.tics.2013.09.009
   Steimer A, 2009, NEURAL COMPUT, V21, P2502, DOI 10.1162/neco.2009.08-08-837
   Vasta R, 2016, CURR ALZHEIMER RES, V13, P566, DOI 10.2174/1567205013666160120151457
   Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001
   Yu Z., 2020, ARXIV200104064
   Yu ZF, 2018, IEEE T NEUR NET LEAR, V29, P5761, DOI 10.1109/TNNLS.2018.2805813
   Yu ZF, 2017, APPL MATH COMPUT, V301, P193, DOI 10.1016/j.amc.2016.12.025
   Yuille A, 2006, TRENDS COGN SCI, V10, P301, DOI 10.1016/j.tics.2006.05.002
NR 37
TC 5
Z9 5
U1 5
U2 26
PD JUN
PY 2020
VL 126
BP 42
EP 51
DI 10.1016/j.neunet.2020.03.003
UT WOS:000536450900005
DA 2023-11-16
ER

PT C
AU Masumori, A
   Sinapayen, L
   Maruyama, N
   Mita, T
   Bakkum, D
   Frey, U
   Takahashi, H
   Ikegami, T
AF Masumori, Atsushi
   Sinapayen, Lana
   Maruyama, Norihiro
   Mita, Takeshi
   Bakkum, Douglas
   Frey, Urs
   Takahashi, Hirokazu
   Ikegami, Takashi
BE Ikegami, T
   Virgo, N
   Witkowski, O
   Oka, M
   Suzuki, R
   Iizuka, H
TI Autonomous Regulation of Self and Non-Self by Stimulation Avoidance in
   Embodied Neural Networks
SO 2018 CONFERENCE ON ARTIFICIAL LIFE (ALIFE 2018)
DT Proceedings Paper
CT Conference on Artificial Life (ALIFE)
CY JUL 23-27, 2018
CL Tokyo, JAPAN
ID TIMING-DEPENDENT PLASTICITY; SPIKING NEURONS; DYNAMICS; MODEL;
   ADAPTATION; PATTERNS
AB Our previous study showed that embodied cultured neural networks and spiking neural networks with spike-timing dependent plasticity (STDP) can learn a behavior as they avoid stimulation from outside. In a sense, the embodied neural network can autonomously change their activity to avoid external stimuli. In this paper, as a result of our experiments using cultured neurons, we find that there is also a second property allowing the network to avoid stimulation: if the network cannot learn to avoid the external stimuli, it tends to decrease the stimulus-evoked spikes as if to ignore the input neurons. We also show such a behavior is reproduced by spiking neural networks with asymmetric-STDP. We consider that these properties can be regarded as autonomous regulation of self and non-self for the network.
C1 [Masumori, Atsushi; Sinapayen, Lana; Maruyama, Norihiro; Mita, Takeshi; Takahashi, Hirokazu; Ikegami, Takashi] Univ Tokyo, Tokyo, Japan.
   [Bakkum, Douglas; Frey, Urs] Swiss Fed Inst Technol, Basel, Switzerland.
   [Frey, Urs] MaxWell Biosyst AG, Basel, Switzerland.
RP Masumori, A (corresponding author), Univ Tokyo, Tokyo, Japan.
EM masumori@sacral.c.u-tokyo.ac.jp
CR [Anonymous], COM ADAP SY
   Ashby W.R., 1960, DESIGN BRAIN
   Bakkum DJ, 2008, J NEURAL ENG, V5, P310, DOI 10.1088/1741-2560/5/3/004
   Brette R, 2007, J COMPUT NEUROSCI, V23, P349, DOI 10.1007/s10827-007-0038-6
   Canepari M, 1997, BIOL CYBERN, V77, P153, DOI 10.1007/s004220050376
   Caporale N, 2008, ANNU REV NEUROSCI, V31, P25, DOI 10.1146/annurev.neuro.31.060407.125639
   Dan Y, 2006, PHYSIOL REV, V86, P1033, DOI 10.1152/physrev.00030.2005
   Di Paolo EA, 2008, BIOSYSTEMS, V91, P409, DOI 10.1016/j.biosystems.2007.05.016
   Eytan D, 2006, J NEUROSCI, V26, P8465, DOI 10.1523/JNEUROSCI.1627-06.2006
   Frey U, 2010, IEEE J SOLID-ST CIRC, V45, P467, DOI 10.1109/JSSC.2009.2035196
   Friston K, 2016, NEUROSCI BIOBEHAV R, V68, P862, DOI 10.1016/j.neubiorev.2016.06.022
   Hebb D.O., 1949, ORG BEHAV NEUROPSYCH
   Iizuka H, 2007, ADAPT BEHAV, V15, P359, DOI 10.1177/1059712307084687
   Ikegami T, 2008, BIOSYSTEMS, V91, P388, DOI 10.1016/j.biosystems.2007.05.014
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Madhavan R, 2007, PHYS BIOL, V4, P181, DOI 10.1088/1478-3975/4/3/005
   Marom S, 2002, Q REV BIOPHYS, V35, P63, DOI 10.1017/S0033583501003742
   Masumori A., 2017, 2 INT S SWARM BEH BI
   Masumori A, 2015, ECAL 2015: THE THIRTEENTH EUROPEAN CONFERENCE ON ARTIFICIAL LIFE, P373, DOI 10.7551/978-0-262-33027-5-ch067
   Masumori A, 2017, FOURTEENTH EUROPEAN CONFERENCE ON ARTIFICIAL LIFE (ECAL 2017), P275
   Potter SM, 2001, J NEUROSCI METH, V110, P17, DOI 10.1016/S0165-0270(01)00412-5
   SEJNOWSKI TJ, 1988, SCIENCE, V241, P1299, DOI 10.1126/science.3045969
   Shahaf G, 2001, J NEUROSCI, V21, P8782, DOI 10.1523/JNEUROSCI.21-22-08782.2001
   Sinapayen L, 2015, 13 EUR C ART LIF ECA
   Sinapayen L, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0170388
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Whitmire CJ, 2016, NEURON, V92, P298, DOI 10.1016/j.neuron.2016.09.046
NR 29
TC 5
Z9 5
U1 0
U2 0
PY 2018
BP 163
EP 170
UT WOS:000494826700036
DA 2023-11-16
ER

PT C
AU Zuters, J
AF Zuters, Janis
BE Haav, HM
   Kalja, A
TI Learning with adaptive layer activation in spiking neural networks
SO DATABASES AND INFORMATION SYSTEMS
DT Proceedings Paper
CT 8th International Baltic Conference on Databases and Information Systems
CY JUN 02-05, 2008
CL Tallinn, ESTONIA
DE spiking neural networks; unsupervised learning; adaptive activation;
   temporal patterns
AB A spiking neural network (SNN) is a neural model in which the communication between two neurons is performed by a "pulse code", i.e., the process of computation considers the timings of the signals (spikes) while ignoring their strength. It has been shown theoretically that a neural network which is composed of spiking neurons is able to learn a large class of target functions. In this paper, an unsupervised learning algorithm is introduced for a spiking neural network. The algorithm is based on the Hebbian rule, with the addition of the principle of adapting a layer activation level so as to guarantee frequent firings of neurons for each layer. The proposed algorithm has been illustrated with experimental results related to the detection of temporal patterns in an input stream.
C1 Latvian State Univ, Dept Comp, LV-1050 Riga, Latvia.
RP Zuters, J (corresponding author), Latvian State Univ, Dept Comp, Raina Bulvaris 19, LV-1050 Riga, Latvia.
EM janis.zuters@lu.lv
CR [Anonymous], PULSED NEURAL NETWOR
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Fausett L., 1994, FUNDAMENTALS NEURAL
   Haykin S., 1999, NEURAL NETWORKS COMP
   HEBB D. O., 1949
   Kempter R, 1999, PHYS REV E, V59, P4498, DOI 10.1103/PhysRevE.59.4498
   KRUGER J, 1988, J NEUROPHYSIOL, V60, P798, DOI 10.1152/jn.1988.60.2.798
   MAAS W, 1999, PULSED NEURAL NETWOR
   Roberts PD, 2002, BIOL CYBERN, V87, P392, DOI 10.1007/s00422-002-0361-y
NR 9
TC 0
Z9 0
U1 0
U2 0
PY 2008
BP 117
EP 128
UT WOS:000258025100010
DA 2023-11-16
ER

PT C
AU Matenczuk, K
   Kozina, A
   Markowska, A
   Czerniachowska, K
   Kaczmarczyk, K
   Golec, P
   Hernes, M
   Lutoslawski, K
   Kozierkiewicz, A
   Pietranik, M
   Rot, A
   Dyvak, M
AF Matenczuk, Karolina
   Kozina, Agata
   Markowska, Aleksandra
   Czerniachowska, Kateryna
   Kaczmarczyk, Klaudia
   Golec, Pawel
   Hernes, Marcin
   Lutoslawski, Krzysztof
   Kozierkiewicz, Adrianna
   Pietranik, Marcin
   Rot, Artur
   Dyvak, Mykola
BE Watrobski, J
   Salabun, W
   Toro, C
   Zanni-Merk, C
   Howlett, RJ
   Jain, LC
TI Financial Time Series Forecasting: Comparison of Traditional and Spiking
   Neural Networks
SO KNOWLEDGE-BASED AND INTELLIGENT INFORMATION & ENGINEERING SYSTEMS (KSE
   2021)
SE Procedia Computer Science
DT Proceedings Paper
CT 25th KES International Conference on Knowledge-Based and Intelligent
   Information & Engineering Systems (KES)
CY SEP 08-10, 2021
CL Szczecin, POLAND
DE Financial forecasting; time series; neural networks
AB One of the most common applications of neural networks itself is data prediction models, for example, future stock market prices, calculated based on historical data. Spiking neural networks are one of the emerging architectures showing great potential in solving complex problems in complicated information environments. However, to the best of our knowledge, the spiking neural networks have not been successfully applied in stock market data prediction. The values of exchange-traded funds (ETF), due to their flexibility and simplicity, can be a good application of such a tool. Therefore, the following article provides the results of a comprehensive experimental comparison of different spiking neural networks in predicting ETF values. The main goal was to check if the spiking neural networks obtain better or worse results of forecasting than traditional neural networks. The secondary goal was a comparison of different spiking neural network architectures between themselves to judge which one is the most applicable to the given problem (C) 2021 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license (https://crativecommons.org/licenses/by-nc-nd/4.0) Peer-review under responsibility of the scientific committee of KES International.
C1 [Matenczuk, Karolina; Kozina, Agata; Markowska, Aleksandra; Czerniachowska, Kateryna; Kaczmarczyk, Klaudia; Golec, Pawel; Hernes, Marcin; Lutoslawski, Krzysztof; Rot, Artur] Wroclaw Univ Econ & Business, Komandorska 118-120, PL-53345 Wroclaw, Poland.
   [Kozierkiewicz, Adrianna; Pietranik, Marcin] Univ Sci & Technol, Wybrzeze Wyspianskiego 27, PL-50370 Wroclaw, Poland.
   [Dyvak, Mykola] West Ukrainian Natl Univ, 11 Lvivska Str, UA-46009 Ternopol, Ukraine.
RP Markowska, A (corresponding author), Wroclaw Univ Econ & Business, Komandorska 118-120, PL-53345 Wroclaw, Poland.
EM aleksandra.markowska@ue.wroc.pl
CR Abedin MZ, 2019, INT J FINANC ECON, V24, P474, DOI 10.1002/ijfe.1675
   Almomani A, 2019, CLUSTER COMPUT, V22, P419, DOI 10.1007/s10586-018-02891-0
   Aziz Norshakirah, 2020, 2020 International Conference on Computational Intelligence (ICCI), P173, DOI 10.1109/ICCI51257.2020.9247665
   Bae K, 2020, J FINANC ECON, V138, P222, DOI 10.1016/j.jfineco.2019.02.012
   Cao J, 2019, PHYSICA A, V519, P127, DOI 10.1016/j.physa.2018.11.061
   Chatigny P, 2021, INT J APPROX REASON, V132, P70, DOI 10.1016/j.ijar.2020.12.002
   Chou JS, 2020, IEEE ACCESS, V8, P14798, DOI 10.1109/ACCESS.2020.2965598
   Huang YS, 2021, NEUROCOMPUTING, V425, P207, DOI 10.1016/j.neucom.2020.04.086
   Jothimani D, 2019, LECT NOTES ARTIF INT, V11927, P283, DOI 10.1007/978-3-030-34885-4_22
   Kraemer P.M., 2020, PSYCHOL PERSPECTIVES, P3
   Lotfi Rezaabad A., 2020, INT C NEUR SYST, P1, DOI DOI 10.1145/3407197.3407211
   Mohammed SA, 2020, COMMUN STAT-THEOR M, V49, P178, DOI 10.1080/03610926.2018.1535073
   Morro A, 2018, IEEE T NEUR NET LEAR, V29, P1371, DOI 10.1109/TNNLS.2017.2657601
   Owoc M, 2017, FED CONF COMPUT SCI, P1123, DOI 10.15439/2017F383
   Pai Nikhitha, 2020, 2020 Fourth International Conference on I-SMAC (IoT in Social, Mobile, Analytics and Cloud) (I-SMAC), P672, DOI 10.1109/I-SMAC49090.2020.9243376
   Pearson M. J., 2005, Proceedings. 2005 International Conference on Field Programmable Logic and Applications (IEEE Cat. No.05EX1155), P582
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Salabun W, 2016, LECT NOTES ARTIF INT, V9692, P321, DOI 10.1007/978-3-319-39378-0_28
   Selvamuthu D, 2019, FINANC INNOV, V5, DOI 10.1186/s40854-019-0131-7
   Sun H, 2020, COMPUT ECON, V55, P451, DOI 10.1007/s10614-019-09896-w
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Yan X, 2021, NEURAL COMPUT APPL, V33, P257, DOI 10.1007/s00521-020-04992-7
NR 22
TC 2
Z9 2
U1 1
U2 9
PY 2021
VL 192
BP 5023
EP 5029
DI 10.1016/j.procs.2021.09.280
UT WOS:000720289005009
DA 2023-11-16
ER

PT C
AU Zhao, Z
   Wang, Y
   Li, C
   Cui, XX
   Huang, R
AF Zhao, Zhao
   Wang, Yuan
   Li, Cheng
   Cui, Xiaoxin
   Huang, Ru
GP IEEE
TI A Sparse Event-Driven Unsupervised Learning Network with Adaptive
   Exponential Integrate-and-Fire Model
SO 17TH IEEE INTERNATIONAL CONFERENCE ON IC DESIGN AND TECHNOLOGY (ICICDT
   2019)
DT Proceedings Paper
CT 17th IEEE International Conference on IC Design and Technology (ICICDT)
CY JUN 17-19, 2019
CL Suzhou, PEOPLES R CHINA
DE spiking neural network (SNN); regular spiking (RS) generator; leaky
   integrate-and-fire (LIF); frequency adaptation; energy-efficient;
   low-cost
ID SPIKING; SYNAPSES; NEURONS
AB A spiking neural network (SNN) with the energy-efficient and low-cost processor is presented in this paper, which is based on mechanism with increased biological plausibility, i.e., frequency adaptive neural model instead of Poisson-spiking neural model. Leaky integrate-and-fire (LIF) neurons and regular spiking (RS) neurons are used to generate input spikes with frequency adaption in our spiking neural networks ( SNNs), which enable high energy efficiency and good accuracy of our SNNs for pattern recognition. In the unsupervised learning mode, our SNNs with different spiking generators (LIF generator and RS generator) achieved 96.33% and 95.95% (more than 99% on the specific classes) accuracy on the MNIST benchmark. In addition, the SNNs attained high accuracy and reduced hardware resources due to fewer synapses used in our SNNs.
C1 [Wang, Yuan; Cui, Xiaoxin] Peking Univ, Inst Microelect, Beijing 100871, Peoples R China.
   Peking Univ, Key Lab Microelect Devices & Circuits, MoE, Beijing 100871, Peoples R China.
RP Wang, Y; Cui, XX (corresponding author), Peking Univ, Inst Microelect, Beijing 100871, Peoples R China.
EM wangyuan@pku.edu.cn; cuixx@pku.edu.cn
CR Bamford SA, 2012, IEEE T BIOMED CIRC S, V6, P385, DOI 10.1109/TBCAS.2012.2184285
   Brette R, 2005, J NEUROPHYSIOL, V94, P3637, DOI 10.1152/jn.00686.2005
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Indiveri G, 2006, IEEE T NEURAL NETWOR, V17, P211, DOI 10.1109/TNN.2005.860850
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Mihalas S, 2009, NEURAL COMPUT, V21, P704, DOI 10.1162/neco.2008.12-07-680
   Panda P, 2018, IEEE J EM SEL TOP C, V8, P51, DOI 10.1109/JETCAS.2017.2769684
   Park J, 2019, ISSCC DIG TECH PAP I, V62, P140, DOI 10.1109/ISSCC.2019.8662398
   Sanger TD, 2013, I IEEE EMBS C NEUR E, P1418, DOI 10.1109/NER.2013.6696209
   Tomás P, 2008, IEEE SIGNAL PROC LET, V15, P357, DOI 10.1109/LSP.2008.919994
   Wang J., 2017, 2017 IEEE BIOM CIRC, P1
   Whatmough PN, 2017, ISSCC DIG TECH PAP I, P242, DOI 10.1109/ISSCC.2017.7870351
   Yu T, 2010, IEEE T BIOMED CIRC S, V4, P139, DOI 10.1109/TBCAS.2010.2048566
NR 15
TC 0
Z9 0
U1 0
U2 0
PY 2019
DI 10.1109/icicdt.2019.8790832
UT WOS:000502601900003
DA 2023-11-16
ER

PT C
AU Kuroda, K
   Fujiwara, K
   Ikeguchi, T
AF Kuroda, Kaori
   Fujiwara, Kantaro
   Ikeguchi, Tohru
BE Huang, T
   Zeng, Z
   Li, C
   Leung, CS
TI Identification of Neural Network Structure from Multiple Spike Sequences
SO NEURAL INFORMATION PROCESSING, ICONIP 2012, PT II
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 19th International Conference on Neural Information Processing (ICONIP)
CY NOV 11-15, 2012
CL Doha, QATAR
DE Spike time metric; Multiple spike sequences; Direction of connectivity;
   Partialization analysis; Point processes
AB In this paper, we propose a new estimation method of direction of the connectivity between neurons in neural network only from multiple spike sequences. The proposed method is based on the spike time metric, or a statistical measure to quantify a degree of dissimilarity between two spike sequences, and the partialization analysis. To resolve this issue, we modify the definition of the conventional cost in the spike time metric. Then, the proposed method can effectively estimate direction of connectivity between neurons. To check the validity, we applied the proposed method to multiple spike sequences that are produced by a mathematical neural network model. As a result, our method can estimate the neural network structure and the direction of couplings with high accuracy.
C1 [Kuroda, Kaori; Fujiwara, Kantaro; Ikeguchi, Tohru] Saitama Univ, Grad Sch Sci & Engn, Sakura Ku, Saitama 3388570, Japan.
RP Kuroda, K (corresponding author), Saitama Univ, Grad Sch Sci & Engn, Sakura Ku, 255 Shimo Ohkubo, Saitama 3388570, Japan.
EM kuroda@nls.ics.saitama-u.ac.jp; kantaro@mail.saitama-u.ac.jp;
   tohru@mail.saitama-u.ac.jp
CR Eichler M, 2003, BIOL CYBERN, V89, P289, DOI 10.1007/s00422-003-0400-3
   Frenzel S, 2007, PHYS REV LETT, V99, DOI 10.1103/PhysRevLett.99.204101
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Kuroda K, 2011, PHYSICA A, V390, P4002, DOI 10.1016/j.physa.2011.06.026
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Schelter B, 2006, PHYS REV LETT, V96, DOI 10.1103/PhysRevLett.96.208103
   Smirnov D, 2007, CHAOS, V17, DOI 10.1063/1.2430639
   Victor JD, 1997, NETWORK-COMP NEURAL, V8, P127, DOI 10.1088/0954-898X/8/2/003
   Watts DJ, 1998, NATURE, V393, P440, DOI 10.1038/30918
NR 9
TC 0
Z9 0
U1 0
U2 0
PY 2012
VL 7664
BP 184
EP 191
UT WOS:000345088900023
DA 2023-11-16
ER

PT C
AU Hunter, JD
   Wu, JH
   Milton, JG
AF Hunter, John D.
   Wu, Jianhong
   Milton, John G.
GP IEEE
TI Clustering Neural Spike Trains with Transient Responses
SO 47TH IEEE CONFERENCE ON DECISION AND CONTROL, 2008 (CDC 2008)
SE IEEE Conference on Decision and Control
DT Proceedings Paper
CT 47th IEEE Conference on Decision and Control
CY DEC 09-11, 2008
CL IEEE Control Syst Soc (CSS), Cancun, MEXICO
HO IEEE Control Syst Soc (CSS)
ID PATTERNS; MODEL; STIMULATION; RELIABILITY; POPULATION; DYNAMICS;
   NEURONS; ART
AB The detection of transient responses, i.e. nonstationarities, that arise in a varying and small fraction of the total number of neural spike trains recorded from chronically implanted multielectrode grids becomes increasingly difficult as the number of electrodes grows. This paper presents a novel application of an unsupervised neural network for clustering neural spike trains with transient responses. This network is constructed by incorporating projective clustering into an adaptive resonance type neural network (ART) architecture resulting in a PART neural network. Since comparisons are made between inputs and learned patterns using only a subset of the total number of available dimensions, PART neural networks are ideally suited to the detection of transients. We show that PART neural networks are an effective tool for clustering neural spike trains that is easily implemented, computationally inexpensive, and well suited for detecting neural responses to dynamic environmental stimuli.
C1 [Hunter, John D.] Tradelink, 200 W Jackson Blvd, Chicago, IL 60606 USA.
   [Wu, Jianhong] Univ York, Lab Ind & Appl Math, Toronto, ON M3P 1P3, Canada.
   [Milton, John G.] Claremont Coll, Joint Sci Dept, Claremont, CA 91711 USA.
RP Hunter, JD (corresponding author), Tradelink, 200 W Jackson Blvd, Chicago, IL 60606 USA.
EM jdh2358@gmail.com; wujh@mathstat.yorku.ca; jmilton@jsd.claremont.edu
CR Aggarwal CC, 1999, SIGMOD RECORD, VOL 28, NO 2 - JUNE 1999, P61, DOI 10.1145/304181.304188
   Aggarwal CC, 2000, SIGMOD REC, V29, P70, DOI 10.1145/335191.335383
   Beierholm U, 2001, J NEUROPHYSIOL, V86, P1858, DOI 10.1152/jn.2001.86.4.1858
   Branch DW, 2000, IEEE T BIO-MED ENG, V47, P290, DOI 10.1109/10.827289
   Cao YQ, 2004, IEEE T NEURAL NETWOR, V15, P245, DOI 10.1109/TNN.2004.824261
   Cao YQ, 2002, NEURAL NETWORKS, V15, P105, DOI 10.1016/S0893-6080(01)00108-3
   CARPENTER GA, 1991, NEURAL NETWORKS, V4, P759, DOI 10.1016/0893-6080(91)90056-B
   CARPENTER GA, 1987, COMPUT VISION GRAPH, V37, P54, DOI 10.1016/S0734-189X(87)80014-2
   Chapin JK, 1999, NAT NEUROSCI, V2, P664, DOI 10.1038/10223
   CHURCH PJ, 1994, J NEUROPHYSIOL, V72, P1794, DOI 10.1152/jn.1994.72.4.1794
   Fellous JM, 2004, J NEUROSCI, V24, P2989, DOI 10.1523/JNEUROSCI.4649-03.2004
   Fofonoff TA, 2004, IEEE T BIO-MED ENG, V51, P890, DOI 10.1109/TBME.2004.826679
   Foss J, 1997, PHYS REV E, V55, P4536, DOI 10.1103/PhysRevE.55.4536
   FROMHERZ P, 1995, PHYS REV LETT, V75, P1670, DOI 10.1103/PhysRevLett.75.1670
   Harris KD, 2003, NATURE, V424, P552, DOI 10.1038/nature01834
   Hinneburg A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P506
   Hunter JD, 1998, J NEUROPHYSIOL, V80, P1427, DOI 10.1152/jn.1998.80.3.1427
   Hunter JD, 2003, J NEUROPHYSIOL, V90, P387, DOI 10.1152/jn.00074.2003
   Hunter JD, 2001, J NEUROSCI, V21, P5781, DOI 10.1523/JNEUROSCI.21-15-05781.2001
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Jain AK, 1999, ACM COMPUT SURV, V31, P264, DOI 10.1145/331499.331504
   Jirsch JD, 2006, BRAIN, V129, P1593, DOI 10.1093/brain/awl085
   KNIGHT BW, 1972, J GEN PHYSIOL, V59, P734, DOI 10.1085/jgp.59.6.734
   MAINEN ZF, 1995, AIDS RES HUM RETRO S, V268, P1503
   MIDDLEBROOKS JC, 1994, AIDS RES HUM RETRO S, V264, P842
   NORDHAUSEN CT, 1994, BRAIN RES, V637, P27, DOI 10.1016/0006-8993(94)91213-0
   NOVAK JL, 1988, J NEUROSCI METH, V23, P149, DOI 10.1016/0165-0270(88)90187-2
   Pancrazio Joseph J, 2006, Neuromodulation, V9, P1, DOI 10.1111/j.1525-1403.2006.00036.x
   Paninski L, 2004, J NEUROSCI, V24, P8551, DOI 10.1523/JNEUROSCI.0919-04.2004
   Procopiuc C. M., 2002, ACM SIGMOD INT C MAN, P418
   Reich DS, 2001, J NEUROPHYSIOL, V85, P1039, DOI 10.1152/jn.2001.85.3.1039
   RESCIGNO A, 1970, B MATH BIOPHYS, V32, P337, DOI 10.1007/BF02476873
   RICHMOND BJ, 1990, J NEUROPHYSIOL, V64, P370, DOI 10.1152/jn.1990.64.2.370
   Segev R, 2002, PHYS REV LETT, V88, DOI 10.1103/PhysRevLett.88.118102
   Shoham S, 2005, IEEE T BIO-MED ENG, V52, P1312, DOI 10.1109/TBME.2005.847542
   SINGER W, 1993, ANNU REV PHYSIOL, V55, P349, DOI 10.1146/annurev.physiol.55.1.349
   Steriade M., 2003, NEURONAL SUBSTRATES
   Tiesinga PHE, 2002, PHYS REV E, V65, DOI 10.1103/PhysRevE.65.041913
   Toups JV, 2006, NEUROCOMPUTING, V69, P1362, DOI 10.1016/j.neucom.2005.12.107
   Victor JD, 1998, J NEUROPHYSIOL, V80, P554, DOI 10.1152/jn.1998.80.2.554
NR 41
TC 4
Z9 4
U1 0
U2 1
PY 2008
BP 2000
EP 2005
DI 10.1109/CDC.2008.4738729
UT WOS:000307311602021
DA 2023-11-16
ER

PT C
AU Wu, QX
   McGinnity, TM
   Maguire, L
   Valderrama-Gonzalez, GD
   Dempster, P
AF Wu, QingXiang
   McGinnity, T. M.
   Maguire, Liam
   Valderrama-Gonzalez, G. D.
   Dempster, Patrick
BE Huang, DS
   Zhao, ZM
   Bevilacqua, V
   Figueroa, JC
TI Colour Image Segmentation Based on a Spiking Neural Network Model
   Inspired by the Visual System
SO ADVANCED INTELLIGENT COMPUTING THEORIES AND APPLICATIONS
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 6th International Conference on Intelligent Computing
CY AUG 18-21, 2010
CL Changsha, PEOPLES R CHINA
DE Spiking neural networks; image segmentation; visual system; visual image
AB The human visual system demonstrates powerful image processing functionalities Inspired by the visual system a spiking neural network is pro posed to segment visual images The network is constructed in the two parts The first part is a spiking neural network which is composed of photon recep tors cone and rod cells and ON/OFF ganglion cells Colour features can be extracted and passed through different ON/OFF pathways The second part is a BP neural network which is trained to recognize the colour features and seg ment the visual image The network has been successfully applied to segment leukocytes from blood smeared images
C1 [Wu, QingXiang; McGinnity, T. M.; Maguire, Liam; Valderrama-Gonzalez, G. D.; Dempster, Patrick] Univ Ulster, Intelligent Syst Res Ctr, Derry BT48 7JL, North Ireland.
RP Wu, QX (corresponding author), Univ Ulster, Intelligent Syst Res Ctr, Magee Campus, Derry BT48 7JL, North Ireland.
CR Dayan P., 2001, THEORETICAL NEUROSCI
   Demb JB, 2007, NEURON, V55, P179, DOI 10.1016/j.neuron.2007.07.001
   Gerstner W., 2002, SPIKING NEURON MODEL
   Jessell T. M, 1981, PRINCIPLES NEURAL SC
   Koch Christof, 1999, P1
   Masland RH, 2001, NAT NEUROSCI, V4, P877, DOI 10.1038/nn0901-877
   Nelson R, 2003, VISUAL NEUROSCIENCES, P260
   Taylor WR, 2003, TRENDS NEUROSCI, V26, P379, DOI 10.1016/S0166-2236(03)00167-X
   Wässle H, 2004, NAT REV NEUROSCI, V5, P747, DOI 10.1038/nrn1497
   Wu QX, 2008, NEUROCOMPUTING, V71, P2055, DOI 10.1016/j.neucom.2007.10.020
   Wu QX, 2008, NEURAL NETWORKS, V21, P1318, DOI 10.1016/j.neunet.2008.05.014
NR 11
TC 9
Z9 9
U1 0
U2 2
PY 2010
VL 6215
BP 49
EP 57
UT WOS:000284034500007
DA 2023-11-16
ER

PT C
AU Lin, XH
   Wang, XW
   Cui, WB
AF Lin, Xianghong
   Wang, Xiangwen
   Cui, Wenbo
BE Huang, DS
   Bevilacqua, V
   Premaratne, P
TI An Automatic Image Segmentation Algorithm Based on Spiking Neural
   Network Model
SO INTELLIGENT COMPUTING THEORY
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 10th International Conference on Intelligent Computing (ICIC)
CY AUG 03-06, 2014
CL Taiyuan, PEOPLES R CHINA
DE spiking neural network; image segmentation; genetic algorithm;
   segmentation threshold; maximum Shannon entropy
AB Inspired by the structure and behavior of the human visual system, an automatic image segmentation algorithm based on a spiking neural network model is proposed. At first, the image pixel values are encoded into the timing of spikes of neurons using the time-to-first-spike coding strategy. Then the segmentation model of spiking neural networks is applied to generate the matrix of spike timing for the visual image. Finally, using the maximum Shannon entropy as the fitness function of genetic algorithm, the evolved segmentation threshold is obtained to segment the visual image. The experimental results show that the method can obtain the optimum segmentation threshold, and achieve satisfactory segmentation results for different images.
C1 [Lin, Xianghong; Wang, Xiangwen] Northwest Normal Univ, Sch Comp Sci & Engn, Lanzhou 730070, Peoples R China.
   [Cui, Wenbo] China Telecom, Linyi Branch Corp, Linyi 276000, Peoples R China.
RP Lin, XH (corresponding author), Northwest Normal Univ, Sch Comp Sci & Engn, Lanzhou 730070, Peoples R China.
EM linxh@nwnu.edu.cn
CR Barbieri AL, 2011, PHYSICA A, V390, P512, DOI 10.1016/j.physa.2010.10.015
   Bohte Sander M., 2004, Natural Computing, V3, P195, DOI 10.1023/B:NACO.0000027755.02868.60
   Borisyuk R, 2009, NEURAL NETWORKS, V22, P707, DOI 10.1016/j.neunet.2009.06.047
   Buhmann JM, 2005, NEURAL COMPUT, V17, P1010, DOI 10.1162/0899766053491913
   Burkitt AN, 2006, BIOL CYBERN, V95, P1, DOI 10.1007/s00422-006-0068-6
   Chaturvedi S., 2012, INT J WISDOM BASED C, V2, P21
   Cui Wen-bo, 2012, Computer Engineering, V38, P196, DOI 10.3969/j.issn.1000-3428.2012.24.046
   Finger H., 2013, FRONTIERS COMPUTATIO, V7
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   HOLLAND JH, 1962, J ACM, V9, P297, DOI 10.1145/321127.321128
   Izhikevich E.M., 2013, BMC NEUROSCIENCE S1, V14, pO6
   Meftah B, 2010, NEURAL PROCESS LETT, V32, P131, DOI 10.1007/s11063-010-9149-6
   Paugam-Moisy H., 2012, HDB NATURAL COMPUTIN, V1, P1, DOI [DOI 10.1007/978-3-540-92910-9_10, 10.1007/978-3-540-92910-9_10]
   Sivanandam S., 2008, INTRO GENETIC ALGORI, P15
   Thorpe S, 1996, NATURE, V381, P520, DOI 10.1038/381520a0
   Wu QX, 2008, NEUROCOMPUTING, V71, P2055, DOI 10.1016/j.neucom.2007.10.020
   Wu QX, 2010, LECT NOTES COMPUT SC, V6215, P49
   Wysoski S.G., 2011, COMPUTATIONAL MODELI, P384
NR 18
TC 4
Z9 6
U1 0
U2 8
PY 2014
VL 8588
BP 248
EP 258
UT WOS:000345518700027
DA 2023-11-16
ER

PT C
AU Dhilipan, A
   Preethi, J
AF Dhilipan, A.
   Preethi, J.
GP IEEE
TI PATTERN RECOGNITION USING SPIKING NEURAL NETWOKS WITH TEMPORAL ENCODING
   AND LEARNING
SO PROCEEDINGS OF 2015 IEEE 9TH INTERNATIONAL CONFERENCE ON INTELLIGENT
   SYSTEMS AND CONTROL (ISCO)
DT Proceedings Paper
CT IEEE 9th International Conference on Intelligent Systems and Control
   (ISCO)
CY JAN 09-10, 2015
CL Coimbatore, INDIA
DE Pattern Recognition; Spiking Neural Networks; Temporal Encoding;
   Readout; Supervised Learning
ID CLASSIFICATION; ALGORITHM; STRATEGY; NEURONS
AB Pattern Recognition plays important role in several activities like speech recognition, face recognitions, character recognition. Patterns are recognized using Spiking Neural Networks. Commonly neural networks are used in analytical decision making process and cognitive process. Spiking neural network with leaky integrate fire neurons are used to recognize the patterns. Biologically inspired supervised learning is used to recognize the pattern. Temporal encoding, learning, and readout process is carried out during classifying the patterns. Spiking neural networks process different inputs and produce the accurate and fast recognition of the particular pattern. In this paper iris data set has to be taken to classifying those patterns. By using a temporal encoding learning process are used in iris data set which is effectively and efficiently recognizing the pattern.
C1 [Dhilipan, A.; Preethi, J.] Anna Univ, Reg Ctr, Dept CSE, Coimbatore, Tamil Nadu, India.
RP Dhilipan, A (corresponding author), Anna Univ, Reg Ctr, Dept CSE, Coimbatore, Tamil Nadu, India.
EM adhilipbe@gmail.com; Preethi17j@yahoo.com
CR [Anonymous], 2008, VLSI IEEE T BIOMEDIC
   Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   Bohtea Sander M., ERROR BACKPROPAGATIO, P17
   Delorme A., 1999, NEUROCOMPUTING, V24, P26
   Eurich CW, 2000, NEURAL COMPUT, V12, P1519, DOI 10.1162/089976600300015240
   Fallahnezhad M, 2011, EXPERT SYST APPL, V38, P386, DOI 10.1016/j.eswa.2010.06.077
   Froemke RC, 2002, NATURE, V416, P433, DOI 10.1038/416433a
   Ghosh-Dastidar S, 2007, INTEGR COMPUT-AID E, V14, P187
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Hansel C, 1997, EUR J NEUROSCI, V9, P2309, DOI 10.1111/j.1460-9568.1997.tb01648.x
   HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   Kempter R, 1999, ADV NEUR IN, V11, P125
   Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7
   Qiang Yu a, PATTERN RECOGNITION, P10
   Qiang Yu a, BRAIN INSPIRED SPIKI, V138, P3
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Tan KC, 2009, EXPERT SYST APPL, V36, P8616, DOI 10.1016/j.eswa.2008.10.013
   Tang H. J., 2012, IEEE MTT S INT MICR, P1, DOI [10.1109/MWSYM.2012.6259786, DOI 10.1109/MWSYM.2012.6259786]
   TREVES A, 1991, NETWORK-COMP NEURAL, V2, P371, DOI 10.1088/0954-898X/2/4/004
   TREVES A, 1990, PHYS REV A, V42, P2418, DOI 10.1103/PhysRevA.42.2418
   Tsukada M, 2005, BIOL CYBERN, V92, P139, DOI 10.1007/s00422-004-0523-1
   Tsukada Minoru, SPATIOTEMPORAL LEARN, P139
   Wade JJ, 2010, IEEE T NEURAL NETWOR, V21, P1817, DOI 10.1109/TNN.2010.2074212
   Wysoski Simei Gomes, FAST ADAPTIVE NETWOR, P2563
NR 25
TC 0
Z9 0
U1 0
U2 0
PY 2015
UT WOS:000380456900008
DA 2023-11-16
ER

PT J
AU Schrauwen, B
   D'Haene, M
   Verstraeten, D
   Van Campenhout, J
AF Schrauwen, Benjamin
   D'Haene, Michiel
   Verstraeten, David
   Van Campenhout, Jan
TI Compact hardware liquid state machines on FPGA for real-time speech
   recognition
SO NEURAL NETWORKS
DT Article; Proceedings Paper
CT International Joint Conference on Neural Networks
CY AUG 12-17, 2007
CL Orlando, FL
DE liquid state machine; spiking neural network; FPGA; speech recognition
ID SPIKING NEURAL-NETWORKS; NEURONS
AB Hardware implementations of Spiking Neural Networks are numerous because they are well suited for implementation in digital and analog hardware, and outperform classic neural networks. This work presents an application driven digital hardware exploration where we implement real-time, isolated digit speech recognition using a Liquid State Machine. The Liquid State Machine is a recurrent neural network of spiking neurons where only the output layer is trained. First we test two existing hardware architectures which we improve and extend, but that appears to be too fast and thus area consuming for this application. Next, we present a scalable, serialized architecture that allows a very compact implementation of spiking neural networks that is still fast enough for real-time processing. All architectures Support leaky integrate-and-fire membranes with exponential synaptic models. This work shows that there is actually a large hardware design space of Spiking Neural Network hardware that can be explored. Existing architectures have only spanned part of it. (C) 2007 Elsevier Ltd. All rights reserved.
C1 [Schrauwen, Benjamin; D'Haene, Michiel; Verstraeten, David; Van Campenhout, Jan] Univ Ghent, Dept Elect & Informat Syst, B-9000 Ghent, Belgium.
RP Schrauwen, B (corresponding author), Univ Ghent, Dept Elect & Informat Syst, Sint Pietersnieuwstr 41, B-9000 Ghent, Belgium.
EM Benjamin.Schrauwen@UGent.be
CR ANGUITA D, 2001, P EUR S ART NEUR NET, P45
   [Anonymous], 2005, ADV NEURAL INFORM PR
   [Anonymous], 2002, 159 GMD GERM NAT RES
   [Anonymous], ARTIFICIAL LIFE
   Aybay I., 1996, Neural Network World, V6, P11
   Bellis S, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON FIELD-PROGRAMMABLE TECHNOLOGY, PROCEEDINGS, P449, DOI 10.1109/FPT.2004.1393322
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   BURR JB, 1991, NEURAL NETWORKS CONC, V2, P237
   de Garis H, 2000, APPL MATH COMPUT, V111, P163, DOI 10.1016/S0096-3003(99)00161-7
   Delorme A, 2001, NEURAL NETWORKS, V14, P795, DOI 10.1016/S0893-6080(01)00049-1
   FLOREANO D, 2004, ARTIF LIFE, V11, P121
   Gerstner W., 2002, SPIKING NEURON MODEL
   GIRAU B, 2006, P ESANN, P173
   Girau Bernard, 2000, P 2 ICSC S NEUR COMP
   Glackin B, 2005, LECT NOTES COMPUT SC, V3512, P552
   Grassmann C, 2002, ANN IEEE SYM FIELD P, P277, DOI 10.1109/FPGA.2002.1106683
   HARTMANN G, 1997, P 6 INT C MICR NEUR, P130
   Hellmich HH, 2005, IEEE IJCNN, P3261
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Jaeger H, 2004, SCIENCE, V304, P78, DOI 10.1126/science.1091277
   JAHNKE A, 1995, P WORLD C NEUR NETW, P460
   JAHNKE A, 1997, P 1997 INT C ART NEU, P1187
   JAHNKE A, 1996, 5 INT C MICR NEUR NE, P232
   LEGENSTEIN R, 2005, NEW DIRECTIONS STAT
   Lyon R. F., 1982, Proceedings of ICASSP 82. IEEE International Conference on Acoustics, Speech and Signal Processing, P1282
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Maass W, 1998, PULSED NEURAL NETWORKS, P321
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maass W, 1997, ADV NEUR IN, V9, P211
   Maass W, 2001, PULSED NEURAL NETWOR
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   Mehrtash N, 2003, IEEE T NEURAL NETWOR, V14, P980, DOI 10.1109/TNN.2003.816060
   MORELAND P, 1997, HDB NEURAL COMPUTATI
   Nielsen J, 2003, IROS 2003: PROCEEDINGS OF THE 2003 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-4, P1363
   Omondi AR, 2006, FPGA IMPLEMENTATIONS OF NEURAL NETWORKS, P1, DOI 10.1007/0-387-28487-7_1
   Pearson M. J., 2005, Proceedings. 2005 International Conference on Field Programmable Logic and Applications (IEEE Cat. No.05EX1155), P582
   PREIS R, 2001, P PDPTA 2001, V1, P463
   Roggen D, 2003, 2003 NASA/DOD CONFERENCE ON EVOLVABLE HARDWARE, P189
   ROS E, 2005, P 8 INT WORK C ART N, P471
   ROTH U, 1995, P IWANN 9K, P720
   Schæfer M, 2002, NEUROCOMPUTING, V48, P647, DOI 10.1016/S0925-2312(01)00633-6
   SCHOENAUER T, 1998, NEURONAL NETWORKS AP, P101
   SCHOENAUER T, 1998, VIDYNN 98, P87
   Schrauwen B, 2004, IEEE IJCNN, P471, DOI 10.1109/IJCNN.2004.1379954
   Schrauwen B, 2003, IEEE IJCNN, P2825
   SCHRAUWEN B, 2007, P INT JOINT C NEUR N
   SCHRAUWEN B, 2006, P ESANN, P623
   Schrauwen B, 2006, IEEE IJCNN, P1797
   SMITH L, 1998, NEUROMORPHIC SYSTEMS
   Softky WR, 1996, NEURAL NETWORKS, V9, P15, DOI 10.1016/0893-6080(95)00012-7
   TRIESCH J, 2004, ADV NEURAL INFORM PR, V17
   Upegui A, 2005, MICROPROCESS MICROSY, V29, P211, DOI 10.1016/j.micpro.2004.08.012
   Verstraeten D, 2007, NEURAL NETWORKS, V20, P391, DOI 10.1016/j.neunet.2007.04.003
   Verstraeten D, 2005, INFORM PROCESS LETT, V95, P521, DOI 10.1016/j.ipl.2005.05.019
   VERSTRAETEN D, 2004, THESIS GHENT U
   VERSTRAETEN D, 2006, P WORLD C COMP INT, P1050
   Waldemark J, 2000, Int J Neural Syst, V10, P171, DOI 10.1016/S0129-0657(00)00015-6
   Zhu JH, 2003, LECT NOTES COMPUT SC, V2778, P1062
   [No title captured]
NR 59
TC 97
Z9 102
U1 3
U2 29
PD MAR-APR
PY 2008
VL 21
IS 2-3
BP 511
EP 523
DI 10.1016/j.neunet.2007.12.009
UT WOS:000255238800043
DA 2023-11-16
ER

PT C
AU Humaidi, AJ
   Kadhim, TM
AF Humaidi, Amjad J.
   Kadhim, Thaer M.
GP IEEE
TI Recognition of Arabic Characters using Spiking Neural Networks
SO 2017 INTERNATIONAL CONFERENCE ON CURRENT TRENDS IN COMPUTER, ELECTRICAL,
   ELECTRONICS AND COMMUNICATION (CTCEEC)
DT Proceedings Paper
CT International Conference on Current Trends in Computer, Electrical,
   Electronics and Communication (CTCEEC)
CY SEP 08-09, 2017
CL Mysore, INDIA
DE Spiking Neural Network (SNN); Remote Supervised Method (ReSuMe);
   Artificial Neural Networks(AAN)
ID MODEL
AB Text Recognition is one of the active and challenging areas of research in the pattern recognition field. It covers many applications like automatic number plate recognition, bank cheques, aid of reading for blind, and handwritten document conversion into form of structural text.
   In the present work, the spiking neural network (SNN) model is utilized for the identification of Arabic characters in a character set. The structure of considered neural network consists of two layers with Izhekevich neurons. Remote Supervised Method (ReSuMe) has been used as a learning rule for training.
   Also, the recognition of Arabic characters has been considered based on spike Neural Networks. The network could successfully recognize character set consisting of 28 isolating Arabic characters.
C1 [Humaidi, Amjad J.; Kadhim, Thaer M.] Univ Technol Baghdad, Control & Syst Engn, Baghdad, Iraq.
RP Humaidi, AJ (corresponding author), Univ Technol Baghdad, Control & Syst Engn, Baghdad, Iraq.
EM 601116@uotechnology.edu.iq; cse.61012@uotechnology.edu.iq
CR [Anonymous], THESIS
   BAIG AR, 2004, EUR S ART NEUR NETW, P561
   Brette R., 2006, J COMPUTER NEUROSCIE
   Buonomano DV, 1999, NEURAL COMPUT, V11, P103, DOI 10.1162/089976699300016836
   Dan Y, 2004, NEURON, V44, P23, DOI 10.1016/j.neuron.2004.09.007
   Delorme A, 2003, NETWORK-COMP NEURAL, V14, P613, DOI 10.1088/0954-898X/14/4/301
   GUPTA A, 2007, INT JOINT C NEUR NET
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Mohemmed A, 2011, IFIP ADV INF COMM TE, V363, P219
   Ponulak F., 2006, THESIS
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
NR 13
TC 1
Z9 1
U1 0
U2 0
PY 2017
BP 7
EP 11
UT WOS:000454738900002
DA 2023-11-16
ER

PT C
AU Wang, YC
   Wang, XB
   Qu, H
   Zhang, Y
   Chen, Y
   Luo, XL
AF Wang, Yuchen
   Wang, Xiaobin
   Qu, Hong
   Zhang, Ya
   Chen, Yi
   Luo, Xiaoling
GP IEEE
TI Bio-inspired Model Based on Global-Local Hybrid Learning in Spiking
   Neural Network
SO 2021 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUL 18-22, 2021
CL ELECTR NETWORK
DE deep neural networks; spiking neural networks; hybrid learning; STDP
AB Bringing machines up to human-level visual processing capabilities is an attractive research topic for decades. Deep neural networks (DNNs), inspired by the hierarchical structure of the human primary visual cortex at a macroscopic level, have achieved state-of-the-art performance in many applications. However, their practical applications remain limited due to the requisition of massive computing resources. Spiking neural networks (SNNs) simulate the spike-based information process of the biological neural system from the microscopic view and hold greater potential to ultra-low-power computations. In this paper, we imitate the human visual system from both the micro and macro scales and make the following contributions: (1) Inspired by the lateral effect between real neurons, we propose a GlobalLocal Hybrid Spike-Timing-Dependent Plasticity (GLHSTDP) algorithm that combines STDP with lateral synaptic learning mechanism, to train the spiking neural network. (2) We construct a deep spiking neural network (DSNN) to mimic the visual information processing mechanism in the human brain. Experimental results demonstrate that the proposed DSNN model equipped with the proposed learning algorithm works in a totally spikebased manner and achieve competitive accuracies on both the Caltech 101 and the MNIST datasets.
C1 [Wang, Yuchen; Wang, Xiaobin; Qu, Hong; Zhang, Ya; Chen, Yi; Luo, Xiaoling] Univ Elect Sci & Technol China, Comp Sci & Engn, Chengdu, Peoples R China.
RP Wang, YC (corresponding author), Univ Elect Sci & Technol China, Comp Sci & Engn, Chengdu, Peoples R China.
EM yuchenwang@std.uestc.edu.cn; xbwang@uestc.edu.cn; hongqu@uestc.edu.cn;
   yazhang@std.uestc.edu.cn; chenyi@std.uestc.edu.cn;
   luoxiaoling@std.uestc.edu.cn
CR Attwell D, 2001, J CEREBR BLOOD F MET, V21, P1133, DOI 10.1097/00004647-200110000-00001
   Boucsein C, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00032
   Cadieu CF, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003963
   Caporale N, 2008, ANNU REV NEUROSCI, V31, P25, DOI 10.1146/annurev.neuro.31.060407.125639
   Conrad M, 2017, INT EL DEVICES MEET
   COOK ND, 2018, THE BRAIN CODE MECHA, V2
   Cook PB, 1998, NAT NEUROSCI, V1, P714, DOI 10.1038/3714
   DiCarlo JJ, 2012, NEURON, V73, P415, DOI 10.1016/j.neuron.2012.01.010
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Felleman D., 1991, CEREB CORTEX
   Haak KV, 2018, CORTEX, V98, P73, DOI 10.1016/j.cortex.2017.03.020
   He Kaiming, 2015, IEEE I CONF COMP VIS, P1026, DOI DOI 10.1109/ICCV.2015.123
   Hunt Jonathan J, 2011, Neural Syst Circuits, V1, P3, DOI 10.1186/2042-1001-1-3
   Kasabov N. K., 2019, TIME SPACE SPIKING N, DOI DOI 10.1007/978-3-662-57715-8
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Lee C, 2019, IEEE T COGN DEV SYST, V11, P384, DOI 10.1109/TCDS.2018.2833071
   Lee C, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00435
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Martin KAC, 2014, NAT COMMUN, V5, DOI 10.1038/ncomms6252
   Mozafari M, 2018, IEEE T NEUR NET LEAR, V29, P6178, DOI 10.1109/TNNLS.2018.2826721
   Sheth BR, 2016, FRONT INTEGR NEUROSC, V10, DOI 10.3389/fnint.2016.00037
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tavanaei A, 2019, NEUROCOMPUTING, V330, P39, DOI 10.1016/j.neucom.2018.11.014
   Tavanaei A, 2017, IEEE IJCNN, P2023, DOI 10.1109/IJCNN.2017.7966099
   Wurtz R. H., 2000, PRINCIPLES NEURAL SC, V4, P523
   ZHANG M, 2010, IEEE TRANSACTIONS ON, V10, P151
   ZHANG M, 2019, PROCEEDINGS OF THE A, V33, P1327
   Zhang ML, 2020, IEEE J-STSP, V14, P592, DOI 10.1109/JSTSP.2020.2983547
   Zylberberg J, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1002250
NR 29
TC 3
Z9 3
U1 0
U2 0
PY 2021
DI 10.1109/IJCNN52387.2021.9534061
UT WOS:000722581706013
DA 2023-11-16
ER

PT C
AU Krithivasan, S
   Sen, S
   Rathi, N
   Roy, K
   Raghunathan, A
AF Krithivasan, Sarada
   Sen, Sanchari
   Rathi, Nitin
   Roy, Kaushik
   Raghunathan, Anand
GP ACM
TI Efficiency Attacks on Spiking Neural Networks
SO PROCEEDINGS OF THE 59TH ACM/IEEE DESIGN AUTOMATION CONFERENCE, DAC 2022
DT Proceedings Paper
CT 59th ACM/IEEE Design Automation Conference (DAC) - From Chips to Systems
   - Learn Today, Create Tomorrow
CY JUL 10-14, 2022
CL San Francisco, CA
AB Spiking Neural Networks are a class of artificial neural networks that process information as discrete spikes. The time and energy consumed in SNN implementations is strongly dependent on the number of spikes processed. We explore this sensitivity from an adversarial perspective and propose SpikeAttack, a completely new class of attacks on SNNs. SpikeAttack impacts the efficiency of SNNs via imperceptible perturbations that increase the overall spiking activity of the network, leading to increased time and energy consumption. Across four SNN benchmarks, SpikeAttackresults in 1.7x-2.5X increase in spike activity, leading to increases of 1.6x-2.3x and 1.4x-2.2x in latency and energy consumption, respectively.
C1 [Krithivasan, Sarada; Rathi, Nitin; Roy, Kaushik; Raghunathan, Anand] Purdue Univ, Sch Elect & Comp Engn, W Lafayette, IN 47907 USA.
   [Sen, Sanchari] IBM TJ Watson Res Ctr, Yorktown Hts, NY USA.
RP Krithivasan, S (corresponding author), Purdue Univ, Sch Elect & Comp Engn, W Lafayette, IN 47907 USA.
EM skrithiv@purdue.edu; sanchari.sen@us.ibm.com; rathi2@purdue.edu;
   kaushik@purdue.edu; raghunathan@purdue.edu
CR Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16
   Carlini N, 2017, Arxiv, DOI arXiv:1608.04644
   Choi J, 2018, Arxiv, DOI arXiv:1805.06085
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Hannun A, 2014, Arxiv, DOI arXiv:1412.5567
   Hong S., 2020, ARXIV
   Krithivasan S, 2020, IEEE T COMPUT AID D, V39, P4129, DOI 10.1109/TCAD.2020.3013077
   Krizhevsky Alex, CIFAR 10 CANADIAN I, P2
   Liang L, 2020, Arxiv, DOI arXiv:2001.01587
   Marchisio A, 2021, Arxiv, DOI arXiv:2107.00415
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Rathi Nitin, 2020, INT C LEARN REPR
   Samangouei P, 2018, Arxiv, DOI arXiv:1805.06605
   Sen S, 2017, DES AUT TEST EUROPE, P193, DOI 10.23919/DATE.2017.7926981
   Sharmin S, 2019, Arxiv, DOI arXiv:1905.02704
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
NR 17
TC 0
Z9 0
U1 0
U2 0
PY 2022
BP 373
EP 378
DI 10.1145/3489517.3530443
UT WOS:001041471300063
DA 2023-11-16
ER

PT J
AU van Gerven, M
   Bohte, S
AF van Gerven, Marcel
   Bohte, Sander
TI Editorial: Artificial Neural Networks as Models of Neural Information
   Processing
SO FRONTIERS IN COMPUTATIONAL NEUROSCIENCE
DT Editorial Material
DE neural networks; artificial intelligence; computational neuroscience;
   rate coding; spiking neural networks
C1 [van Gerven, Marcel] Radboud Univ Nijmegen, Donders Inst Brain Cognit & Behav, Dept Artificial Intelligence, Nijmegen, Netherlands.
   [Bohte, Sander] Ctr Wiskunde & Informat, Dept Machine Learning, Amsterdam, Netherlands.
RP van Gerven, M (corresponding author), Radboud Univ Nijmegen, Donders Inst Brain Cognit & Behav, Dept Artificial Intelligence, Nijmegen, Netherlands.
EM m.vangerven@donders.ru.nl
CR Brette R, 2015, FRONT SYST NEUROSCI, V9, DOI 10.3389/fnsys.2015.00151
   CRICK F, 1989, NATURE, V337, P129, DOI 10.1038/337129a0
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
NR 3
TC 36
Z9 36
U1 1
U2 4
PD DEC 19
PY 2017
VL 11
AR 114
DI 10.3389/fncom.2017.00114
UT WOS:000418280300001
DA 2023-11-16
ER

PT C
AU Leigh, AJ
   Heidarpur, M
   Mirhassani, M
AF Leigh, Alexander J.
   Heidarpur, Moslem
   Mirhassani, Mitra
GP IEEE
TI Selective Input Sparsity in Spiking Neural Networks for Pattern
   Classification
SO 2022 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS (ISCAS 22)
SE IEEE International Symposium on Circuits and Systems
DT Proceedings Paper
CT IEEE International Symposium on Circuits and Systems (ISCAS)
CY MAY 28-JUN 01, 2022
CL Austin, TX
DE Spiking Neural Networks; Pattern Recognition; Sparse Spiking Neural
   Networks
AB The concept of input sparsity in Spiking Neural Networks for pattern recognition is introduced and explored with the goals of reductions in network inference time and size, leading to lower resource requirements in hardware implementations. A method is proposed by which selective input sparsity can be inferred from the training set to reduce the size of the network before training and decrease the network inference time. This method also requires no additional pre-processing steps during the testing phase, making it an excellent candidate for edge applications. For a basic fully connected spiking neural network trained to solve the MNIST handwritten digits, selective input sparsity is applied and the network size is reduced by 58.16% and a 41.07% decrease in the network's inference time is observed without notable accuracy hinderance. In the case of the Fashion MNIST dataset, selective input sparsity reduced the network size by 55.99% and reduced the network's inference time by 59.05%.
C1 [Leigh, Alexander J.; Heidarpur, Moslem; Mirhassani, Mitra] Univ Windsor, Elect & Comp Engn, Windsor, ON, Canada.
RP Leigh, AJ (corresponding author), Univ Windsor, Elect & Comp Engn, Windsor, ON, Canada.
CR [Anonymous], 1982, IEEE J SOLID-ST CIRC, V17, P442
   [Anonymous], TF KERAS LOSSES SPAR
   Camuñas-Mesa LA, 2019, IEEE I C ELECT CIRC, P73, DOI [10.1109/icecs46596.2019.8964964, 10.1109/ICECS46596.2019.8964964]
   Davidson S, 2021, FRONT NEUROSCI-SWITZ, V15, DOI 10.3389/fnins.2021.651141
   FERREIRA J, 2018, 2018 IEEE 17 INT S N
   Frenkel C, 2019, IEEE T BIOMED CIRC S, V13, P145, DOI 10.1109/TBCAS.2018.2880425
   Guo HH, 2021, ADV DIFFER EQU-NY, V2021, DOI 10.1186/s13662-020-03162-2
   Han JH, 2020, TSINGHUA SCI TECHNOL, V25, P479, DOI 10.26599/TST.2019.9010019
   Kurková V, 2019, IEEE T NEUR NET LEAR, V30, P2746, DOI 10.1109/TNNLS.2018.2888517
   LeCun Y., 1998, MNIST DATABASE HANDW
   Lee C, 2019, IEEE T COGN DEV SYST, V11, P384, DOI 10.1109/TCDS.2018.2833071
   Lu S, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00535
   Luo W., 2020, IEEE ACCESS, V8
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Saxena V, 2021, 2021 INTERNATIONAL CONFERENCE ON SUSTAINABLE ENERGY AND FUTURE ELECTRIC TRANSPORTATION (SEFET), DOI 10.1109/SeFet48154.2021.9375821
   Simard PY, 2003, PROC INT CONF DOC, P958
   Xiao H., 2017, ARXIV170807747
   Zhang AS, 2022, INT J ADV MANUF TECH, V118, P3963, DOI 10.1007/s00170-021-08049-4
   Zhao Z., 2019, 2019 INT C IC DES TE, P1
NR 19
TC 1
Z9 1
U1 0
U2 1
PY 2022
BP 799
EP 803
DI 10.1109/ISCAS48785.2022.9937618
UT WOS:000946638601001
DA 2023-11-16
ER

PT J
AU Fu, CT
   Xiang, SY
   Han, YA
   Song, ZW
   Hao, Y
AF Fu, Chentao
   Xiang, Shuiying
   Han, Yanan
   Song, Ziwei
   Hao, Yue
TI Multilayer Photonic Spiking Neural Networks: Generalized Supervised
   Learning Algorithm and Network Optimization
SO PHOTONICS
DT Article
DE photonic spiking neural network; multilayer spiking neural network;
   supervised learning; vertical-cavity surface-emitting lasers; spike
   timing dependent plasticity
ID PLASTICITY; BACKPROPAGATION; CLASSIFICATION
AB We propose a generalized supervised learning algorithm for multilayer photonic spiking neural networks (SNNs) by combining the spike-timing dependent plasticity (STDP) rule and the gradient descent mechanism. A vertical-cavity surface-emitting laser with an embedded saturable absorber (VCSEL-SA) is employed as a photonic leaky-integrate-and-fire (LIF) neuron. The temporal coding strategy is employed to transform information into the precise firing time. With the modified supervised learning algorithm, the trained multilayer photonic SNN successfully solves the XOR problem and performs well on the Iris and Wisconsin breast cancer datasets. This indicates that a generalized supervised learning algorithm is realized for multilayer photonic SNN. In addition, network optimization is performed by considering different network sizes.
C1 [Fu, Chentao; Xiang, Shuiying; Han, Yanan; Song, Ziwei] Xidian Univ, State Key Lab Integrated Serv Networks, Xian 710071, Peoples R China.
   [Xiang, Shuiying; Hao, Yue] Xidian Univ, Sch Microelect, State Key Discipline Lab Wide Bandgap Semicond Te, Xian 710071, Peoples R China.
RP Xiang, SY (corresponding author), Xidian Univ, State Key Lab Integrated Serv Networks, Xian 710071, Peoples R China.; Xiang, SY (corresponding author), Xidian Univ, Sch Microelect, State Key Discipline Lab Wide Bandgap Semicond Te, Xian 710071, Peoples R China.
EM 19011210238@stu.xidian.edu.cn; syxiang@xidian.edu.cn;
   20011110214@stu.xidian.edu.cn; 21011110011@stu.xidian.edu.cn;
   yhao@xidian.edu.cn
CR Abbott LF, 2000, NAT NEUROSCI, V3, P1178, DOI 10.1038/81453
   Bellec G, 2018, ADV NEUR IN, V31
   Bellec G, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-17236-y
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Caporale N, 2008, ANNU REV NEUROSCI, V31, P25, DOI 10.1146/annurev.neuro.31.060407.125639
   Dayan P., 2001, THEORETICAL NEUROSCI
   DeFelipe J, 2012, FRONT NEUROANAT, V6, DOI [10.3389/fnana.2012.00022, 10.3389/fnsyn.2012.00002, 10.3389/fnana.2012.00005]
   Deng T, 2017, IEEE J SEL TOP QUANT, V23, DOI 10.1109/JSTQE.2017.2685140
   Dora S, 2016, NEUROCOMPUTING, V171, P1216, DOI 10.1016/j.neucom.2015.07.086
   Escobar MJ, 2009, INT J COMPUT VISION, V82, P284, DOI 10.1007/s11263-008-0201-1
   Esser SK, 2016, P NATL ACAD SCI USA, V113, P11441, DOI 10.1073/pnas.1604850113
   Feldmann J, 2019, NATURE, V569, P208, DOI 10.1038/s41586-019-1157-8
   Fisher RA, 1936, ANN EUGENIC, V7, P179, DOI 10.1111/j.1469-1809.1936.tb02137.x
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   Han YN, 2021, PHOTONICS RES, V9, pB119, DOI 10.1364/PRJ.413742
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   Hussain I, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-70136-5
   Kaiser J, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00424
   Kasabov N, 2014, NEUROCOMPUTING, V134, P269, DOI 10.1016/j.neucom.2013.09.049
   London M, 2010, NATURE, V466, P123, DOI 10.1038/nature09086
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Masuda N, 2002, PHYS REV LETT, V88, DOI 10.1103/PhysRevLett.88.248101
   Minsky M., 2017, PERCEPTRONS INTRO CO
   Mostafa Hesham, 2018, IEEE Trans Neural Netw Learn Syst, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Nahmias MA, 2013, IEEE J SEL TOP QUANT, V19, DOI 10.1109/JSTQE.2013.2257700
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Pammi VA, 2020, IEEE J SEL TOP QUANT, V26, DOI 10.1109/JSTQE.2019.2929187
   Peng HT, 2020, IEEE J SEL TOP QUANT, V26, DOI 10.1109/JSTQE.2019.2927582
   Prechelt L, 1998, NEURAL NETWORKS, V11, P761, DOI 10.1016/S0893-6080(98)00010-0
   Robertson J, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-62945-5
   Robertson J, 2020, IEEE J SEL TOP QUANT, V26, DOI 10.1109/JSTQE.2019.2931215
   Saleh AY, 2017, INT J COMPUT VIS ROB, V7, P20, DOI [10.1504/IJCVR.2017.081231, DOI 10.1504/IJCVR.2017.081231]
   Tavanaei A, 2019, NEUROCOMPUTING, V330, P39, DOI 10.1016/j.neucom.2018.11.014
   Tavanaei A, 2017, LECT NOTES COMPUT SC, V10639, P899, DOI 10.1007/978-3-319-70136-3_95
   Wade JJ, 2010, IEEE T NEURAL NETWOR, V21, P1817, DOI 10.1109/TNN.2010.2074212
   Wang JL, 2014, NEUROCOMPUTING, V144, P526, DOI 10.1016/j.neucom.2014.04.017
   WOLBERG WH, 1990, P NATL ACAD SCI USA, V87, P9193, DOI 10.1073/pnas.87.23.9193
   Wysoski SG, 2010, NEURAL NETWORKS, V23, P819, DOI 10.1016/j.neunet.2010.04.009
   Xiang SY, 2021, IEEE J SEL TOP QUANT, V27, DOI 10.1109/JSTQE.2020.3005589
   Xiang SY, 2021, IEEE T NEUR NET LEAR, V32, P2494, DOI 10.1109/TNNLS.2020.3006263
   Xiang SY, 2020, OPT LETT, V45, P1104, DOI 10.1364/OL.383942
   Xiang SY, 2019, IEEE J SEL TOP QUANT, V25, DOI 10.1109/JSTQE.2019.2911565
   Xu Y, 2013, NEURAL NETWORKS, V43, P99, DOI 10.1016/j.neunet.2013.02.003
   Zhang YH, 2021, PHOTONICS RES, V9, pB201, DOI 10.1364/PRJ.412141
   Zhao A, 2022, J LIGHTWAVE TECHNOL, V40, P751, DOI 10.1109/JLT.2021.3123653
NR 48
TC 0
Z9 0
U1 5
U2 28
PD APR
PY 2022
VL 9
IS 4
AR 217
DI 10.3390/photonics9040217
UT WOS:000786009500001
DA 2023-11-16
ER

PT J
AU Ji, MC
   Wang, ZL
   Yan, R
   Liu, QJ
   Xu, S
   Tang, HJ
AF Ji, Mingcheng
   Wang, Ziling
   Yan, Rui
   Liu, Qingjie
   Xu, Shu
   Tang, Huajin
TI SCTN: event-based object tracking with energy-efficient deep
   convolutional spiking neural networks (vol 17, 1123698, 2023)
SO FRONTIERS IN NEUROSCIENCE
DT Correction
DE spiking neural networks; event cameras; object tracking; exponential
   IoU; event-based tracking dataset
C1 [Ji, Mingcheng; Wang, Ziling; Tang, Huajin] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou, Peoples R China.
   [Yan, Rui] Zhejiang Univ Technol, Coll Comp Sci, Hangzhou, Peoples R China.
   [Liu, Qingjie; Xu, Shu] China Nanhu Acad Elect & Informat Technol, Machine Intelligence Lab, Jiaxing, Peoples R China.
   [Tang, Huajin] Zhejiang Lab, Hangzhou, Peoples R China.
RP Tang, HJ (corresponding author), Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou, Peoples R China.; Xu, S (corresponding author), China Nanhu Acad Elect & Informat Technol, Machine Intelligence Lab, Jiaxing, Peoples R China.; Tang, HJ (corresponding author), Zhejiang Lab, Hangzhou, Peoples R China.
EM xushu@cnaeit.com; htang@zju.edu.cn
CR Ji MC, 2023, FRONT NEUROSCI-SWITZ, V17, DOI 10.3389/fnins.2023.1123698
NR 1
TC 0
Z9 0
U1 6
U2 6
PD MAY 16
PY 2023
VL 17
AR 1204334
DI 10.3389/fnins.2023.1204334
UT WOS:000997668600001
DA 2023-11-16
ER

PT J
AU Lin, ZT
   Shen, JC
   Ma, D
   Meng, JY
AF Lin, Zhitao
   Shen, Juncheng
   Ma, De
   Meng, Jianyi
TI Quantisation and pooling method for low-inference-latency spiking neural
   networks
SO ELECTRONICS LETTERS
DT Article
DE neural nets; object recognition; real-time recognition tasks; CIFAR10;
   MNIST; spiking neurons; convolutional layers; pooling function;
   retraining; layer-wise quantisation method; DNN; deep neural network;
   SNN; low-inference-latency spiking neural networks; pooling method
AB Spiking neural network (SNN) that converted from conventional deep neural network (DNN) has shown great potential as a solution for fast and efficient recognition. A layer-wise quantisation method based on retraining is proposed to quantise the activation of DNN, which reduces the number of time steps required by converted SNN to achieve minimal accuracy loss. Pooling function is incorporated into convolutional layers to reduce at most 20% of spiking neurons. The converted SNNs achieved 99.15% accuracy on MNIST and 82.9% on CIFAR10 by only seven time steps, and only 10-40% of spikes need to be processed compared with networks using traditional algorithms. The experimental results show that the proposed methods are able to build hardware-friendly SNNs with ultra-low-inference latency.
C1 [Lin, Zhitao; Shen, Juncheng] Zhejiang Univ, Inst VLSI Design, Hangzhou, Zhejiang, Peoples R China.
   [Ma, De] Zhejiang Univ, Coll Comp Sci, Hangzhou, Zhejiang, Peoples R China.
   [Meng, Jianyi] Fudan Univ, State Key Lab ASIC & Syst, Shanghai, Peoples R China.
RP Ma, D (corresponding author), Zhejiang Univ, Coll Comp Sci, Hangzhou, Zhejiang, Peoples R China.
EM madehd@163.com
CR Aggarwal A, 2015, ELECTRON LETT, V51, DOI 10.1049/el.2014.4187
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Diehl P.U., 2015, 2015 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2015.7280696
   Neil D, 2016, P 31 ANN ACM S APPL
   Rueckauer B., 2016, ARXIV161204052, P1
   Wang S, 2016, ELECTRON LETT, V52, P1673, DOI 10.1049/el.2016.2645
NR 6
TC 9
Z9 9
U1 0
U2 14
PD SEP 28
PY 2017
VL 53
IS 20
BP 1347
EP 1348
DI 10.1049/el.2017.2219
UT WOS:000411812200006
DA 2023-11-16
ER

PT C
AU Wu, QX
   McGinnity, TM
   Maguire, L
   Ghani, A
   Condell, J
AF Wu, Qingxiang
   McGinnity, T. M.
   Maguire, Liam
   Ghani, Arfan
   Condell, Joan
BE Huang, DS
   Jo, KH
   Lee, HH
   Bevilacqua, V
   Kang, HJ
TI Spiking Neural Network Performs Discrete Cosine Transform for Visual
   Images
SO EMERGING INTELLIGENT COMPUTING TECHNOLOGY AND APPLICATIONS: WITH ASPECTS
   OF ARTIFICIAL INTELLIGENCE
SE Lecture Notes in Artificial Intelligence
DT Proceedings Paper
CT 5th International Conference on Intelligent Computing
CY SEP 16-19, 2009
CL Ulsan, SOUTH KOREA
DE spiking neural networks; visual system; discrete cosine transform;
   visual image
AB The human visual system demonstrates powerful image processing functionalities. Inspired by the principles from neuroscience, a spiking neural network is proposed to perform the discrete cosine transform for visual images. The structure and the properties of the network are detailed in this paper. Simulation results show that the network is able to perform the discrete cosine transform for visual images. Based on this mechanism, the key features can be extracted in ON/OFF neuron arrays. These key features can be used to reconstruct the visual images. The network can be used to explain how the spiking neuron-based system can perform key feature extraction. The differences between the discrete cosine transform and the spiking neural network transform are discussed.
C1 [Wu, Qingxiang; McGinnity, T. M.; Maguire, Liam; Ghani, Arfan; Condell, Joan] Univ Ulster, Intelligent Syst Res Ctr, Derry BT48 7JL, North Ireland.
RP Wu, QX (corresponding author), Univ Ulster, Intelligent Syst Res Ctr, Magee Campus, Derry BT48 7JL, North Ireland.
EM q.wu@ulster.ac.uk; tm.mcginnity@ulster.ac.uk; lp.maguire@ulster.ac.uk;
   a.ghani@ulster.ac.uk; j.condell@ulster.ac.uk
CR AHMED N, 1974, IEEE T COMPUT, VC 23, P90, DOI 10.1109/T-C.1974.223784
   Dayan P., 2001, THEORETICAL NEUROSCI
   Demb JB, 2007, NEURON, V55, P179, DOI 10.1016/j.neuron.2007.07.001
   Gerstner W., 2002, SPIKING NEURON MODEL
   Jessell T. M, 1981, PRINCIPLES NEURAL SC
   Koch Christof, 1999, P1
   Masland RH, 2001, NAT NEUROSCI, V4, P877, DOI 10.1038/nn0901-877
   MULLER E, 2003, HDKIP0322 U HEID
   Nelson R, 2003, VISUAL NEUROSCIENCES, P260
   Taylor WR, 2003, TRENDS NEUROSCI, V26, P379, DOI 10.1016/S0166-2236(03)00167-X
   Wässle H, 2004, NAT REV NEUROSCI, V5, P747, DOI 10.1038/nrn1497
   Wu QX, 2007, STUD COMPUT INTELL, V35, P171
   Wu QX, 2005, LECT NOTES COMPUT SC, V3610, P420
NR 13
TC 4
Z9 4
U1 0
U2 1
PY 2009
VL 5755
BP 21
EP 29
UT WOS:000270543000003
DA 2023-11-16
ER

PT C
AU Matsuda, S
AF Matsuda, Satoshi
GP IEEE
TI <i>BPSpike</i>: a backpropagation learning for all parameters in spiking
   neural networks with multiple layers and multiple spikes
SO 2016 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUL 24-29, 2016
CL Vancouver, CANADA
AB By further extending SpikeProp, we propose a back-propagation learning algorithm, which adjusts all the parameters, synaptic weights, synaptic delays, synaptic time constants, and neurons' thresholds, for spiking neural networks with multiple layers and multiple spiking neurons.
C1 [Matsuda, Satoshi] Nihon Univ, Dept Math Informat Engn, Narashino, Chiba 2758575, Japan.
RP Matsuda, S (corresponding author), Nihon Univ, Dept Math Informat Engn, Narashino, Chiba 2758575, Japan.
EM matsuda.satoshi@nihon-u.ac.jp
CR [Anonymous], P 15 PRORISC WORKSH
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Florian RV, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0040233
   Gerstner W., 2002, SPIKING NEURON MODEL
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Mohemmed A, 2013, NEUROCOMPUTING, V107, P3, DOI 10.1016/j.neucom.2012.08.034
   Mohemmed A, 2012, INT J NEURAL SYST, V22, DOI 10.1142/S0129065712500128
   Ponulak F., 2006, THESIS
   Xu Y, 2013, NEURAL NETWORKS, V43, P99, DOI 10.1016/j.neunet.2013.02.003
NR 10
TC 9
Z9 9
U1 0
U2 2
PY 2016
BP 293
EP 298
UT WOS:000399925500039
DA 2023-11-16
ER

PT J
AU Hasan, MM
   Holleman, J
AF Hasan, Md Munir
   Holleman, Jeremy
TI Fast Simulation of Analog Spiking Neural Network With Device
   Non-Idealites
SO IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS II-EXPRESS BRIEFS
DT Article
DE Spiking neuron; neuromorphic engineering; phase plane; silicon neuron
ID NEURONS; SYNAPSES
AB We present a method for spiking neural network simulation with hardware realistic device non-idealities present in the neuron and synapse circuits as an alternative to time-consuming spice simulation. Neuromorphic machine learning algorithms for spiking neural networks are often simulated with an ideal mathematical description of spiking neurons and synapses. However, silicon implementations of spiking neurons differ significantly from their ideal mathematical models because of device non-idealities and restrictions in the range of device operation. These non-idealities affect the performance of chip implementations of spiking networks. The dynamical system phase plane of the neuron and synapse circuit is used to create a compact representation of the neuron dynamics that captures device non-idealities for simulation of spiking neural networks. The proposed method would allow hardware-aware optimization of neuromorphic algorithms using standard machine learning tools and provide simulated network prediction close to what would be expected from a chip implementation.
C1 [Hasan, Md Munir; Holleman, Jeremy] Univ North Carolina Charlotte, Dept Elect & Comp Engn, Charlotte, NC 28223 USA.
RP Hasan, MM (corresponding author), Univ North Carolina Charlotte, Dept Elect & Comp Engn, Charlotte, NC 28223 USA.
EM mhasan13@uncc.edu; jhollem3@uncc.edu
CR Arthur JV, 2011, IEEE T CIRCUITS-I, V58, P1034, DOI 10.1109/TCSI.2010.2089556
   Bolme D, 2017, IEEE INT SYMP CIRC S
   Brette R, 2007, J COMPUT NEUROSCI, V23, P349, DOI 10.1007/s10827-007-0038-6
   Chicca E, 2003, IEEE T NEURAL NETWOR, V14, P1297, DOI 10.1109/TNN.2003.816367
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Gao PR, 2012, IEEE T CIRCUITS-I, V59, P2383, DOI 10.1109/TCSI.2012.2188956
   github, 2023, SKILLBRIDGE SEAMLESS
   Hasan MM, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351829
   Hasan MM, 2020, MIDWEST SYMP CIRCUIT, P452, DOI 10.1109/mwscas48704.2020.9184447
   Hasan MM, 2021, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS51556.2021.9401477
   Hasan MM, 2020, MIDWEST SYMP CIRCUIT, P460, DOI [10.1109/MWSCAS48704.2020.9184611, 10.1109/mwscas48704.2020.9184611]
   Hasler P, 2005, IEEE T CIRCUITS-I, V52, P834, DOI 10.1109/TCSI.2005.846663
   Indiveri G, 2006, IEEE T NEURAL NETWOR, V17, P211, DOI 10.1109/TNN.2005.860850
   LeCun Y., 1998, MNIST DATABASE HANDW
   Lungu IA, 2019, IEEE J EM SEL TOP C, V9, P690, DOI 10.1109/JETCAS.2019.2951062
   Sarpeshkar R., 2010, ULTRA LOW POWER BIOE
   SHEU BJ, 1987, IEEE J SOLID-ST CIRC, V22, P558, DOI 10.1109/JSSC.1987.1052773
   Stimberg M, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-019-54957-7
   Stimberg M, 2019, ELIFE, V8, DOI 10.7554/eLife.47314
   Tsividis Y., 1999, OPERATION MODELING M
NR 20
TC 0
Z9 0
U1 0
U2 0
PD SEP
PY 2023
VL 70
IS 9
BP 3574
EP 3578
DI 10.1109/TCSII.2023.3263048
UT WOS:001066636600070
DA 2023-11-16
ER

PT J
AU Mirsu, R
   Micut, S
   Caleanu, C
   Mirsu, DB
AF Mirsu, Radu
   Micut, Sebastian
   Caleanu, Catalin
   Mirsu, Diana B.
TI Optimized Simulation Framework for Spiking Neural Networks using GPU's
SO ADVANCES IN ELECTRICAL AND COMPUTER ENGINEERING
DT Article
DE artificial intelligence; biological neural networks; GPU computing;
   parallel processing; spiking neural networks
ID NEURONS; MODEL
AB This paper presents a hardware accelerated model of a spiking neural network implemented in CUDA C. It does a short description of the mathematical model for the neural network and presents the implementation on the GPU. Additionally, it presents three methods of further accelerating the model by eliminating excess kernel launch overhead time, efficiently using shared memory and overlapping computation with data transfer. Finally, the implementation is benchmarked against an existing C++ equivalent model.
C1 [Mirsu, Radu] Politehn Univ Timisoara, Timisoara 300006, Romania.
RP Mirsu, R (corresponding author), Politehn Univ Timisoara, Timisoara 300006, Romania.
EM radu.mirsu@etc.upt.ro; sebastian.micut@etc.upt.ro;
   catalin.caleanu@etc.upt.ro; betinaiovan@yahoo.com
CR ABBOTT LF, 1990, LECT NOTES PHYS, V368, P5
   [Anonymous], 2010, PROGRAMMING MASSIVEL
   [Anonymous], P SUP REN NV US
   Ayinde O, 2002, PATTERN RECOGN, V35, P1275, DOI 10.1016/S0031-3203(01)00120-0
   Bhuiyan A., 2010, IEEE INT S PAR DISTR
   BHUIYAN MA, 2009, IEEE P CIMSVP TN MAR, P29
   Bogdanov I, 2009, MATH COMPUT SCI ENG, P533
   Gerstner W., 2002, SPIKING NEURON MODEL
   Herlihy M., 2010, ART MULTIPROCESSOR P
   Izhikevich EM, 2001, NEURAL NETWORKS, V14, P883, DOI 10.1016/S0893-6080(01)00078-8
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich Eugene M, 2004, IEEE T NEURAL NETWOR
   KAMARAINEN JK, 2002, P IAPR WORKSH MACH V, P228
   KYRKI V, 2001, P INT C ADV CONC INT, P45
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Martinez D., 2003, P NATO ADV RES WORKS, P209
   Mirsu R, 2011, 17 INT C SOFT COMP M
   Mirsu R., 2010, 9 INT S EL TEL ISETC, P369
   Mirsu R, 2009, REC ADV COMPUT ENG, P318
   Nageswaran JM, 2009, NEURAL NETWORKS, V22, P791, DOI 10.1016/j.neunet.2009.06.028
   NVIDIA CUDATM, 2010, NVIDIA CUDA C PROGR
   WILLS SA, 2004, THESIS U CAMBRIDGE
NR 23
TC 4
Z9 4
U1 0
U2 11
PY 2012
VL 12
IS 2
BP 61
EP 68
DI 10.4316/AECE.2012.02011
UT WOS:000305608000011
DA 2023-11-16
ER

PT J
AU Zhang, ML
   Luo, XL
   Chen, Y
   Wu, JB
   Belatreche, A
   Pan, ZH
   Qu, H
   Li, HZ
AF Zhang, Malu
   Luo, Xiaoling
   Chen, Yi
   Wu, Jibin
   Belatreche, Ammar
   Pan, Zihan
   Qu, Hong
   Li, Haizhou
TI An Efficient Threshold-Driven Aggregate-Label Learning Algorithm for
   Multimodal Information Processing
SO IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING
DT Article
DE Neurons; Signal processing algorithms; Biomembranes; Biological neural
   networks; Computational modeling; Membrane potentials; Spiking neurons;
   spiking neural networks; aggregate-label learning; synaptic plasticity;
   multimodal information
ID SPIKING NEURAL-NETWORKS; RESUME
AB The aggregate-label learning paradigm tackles the long-standing temporary credit assignment (TCA) problem in neuroscience and machine learning, enabling spiking neural networks to learn multimodal sensory clues with delayed feedback signals. However, the existing aggregate-label learning algorithms only work for single spiking neurons, and with low learning efficiency, which limit their real-world applicability. To address these limitations, in this article, we first propose an efficient threshold-driven plasticity algorithm for spiking neurons, namely ETDP. It enables spiking neurons to generate the desired number of spikes that match the magnitude of delayed feedback signals and to learn useful multimodal sensory clues embedded within spontaneous spiking activities. Furthermore, we extend the ETDP algorithm to support multi-layer spiking neural networks (SNNs), which significantly improves the applicability of aggregate-label learning algorithms. We also validate the multi-layer ETDP learning algorithm in a multimodal computation framework for audio-visual pattern recognition. Experimental results on both synthetic and realistic datasets show significant improvements in the learning efficiency and model capacity over the existing aggregate-label learning algorithms. It, therefore, provides many opportunities for solving real-world multimodal pattern recognition tasks with spiking neural networks.
C1 [Zhang, Malu; Luo, Xiaoling; Chen, Yi; Qu, Hong] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 610054, Peoples R China.
   [Zhang, Malu; Wu, Jibin; Pan, Zihan; Li, Haizhou] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 119077, Singapore.
   [Belatreche, Ammar] Northumbria Univ, Dept Comp & Informat Sci, Fac Engn & Environm, Newcastle Upon Tyne NE1 8ST, Tyne & Wear, England.
RP Wu, JB (corresponding author), Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 119077, Singapore.
EM maluzhang@nus.edu.sg; luoxiaoling@std.uestc.edu.cn;
   chenyi@std.uestc.edu.cn; jibin.wu@u.nus.edu;
   ammar.belatreche@northumbria.ac.uk; panzihan@u.nus.edu;
   hongqu@uestc.edu.cn; haizhou.li@nus.edu.sg
CR Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   Calvert GA, 2001, CEREB CORTEX, V11, P1110, DOI 10.1093/cercor/11.12.1110
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Florian RV, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0040233
   Gerstner W., 2002, SPIKING NEURON MODEL
   Gu PJ, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1366
   Gütig R, 2016, SCIENCE, V351, DOI 10.1126/science.aab4113
   Gütig R, 2009, PLOS BIOL, V7, DOI 10.1371/journal.pbio.1000141
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Hong C., 2019, IEEE T NEUR NET LEAR, V31, P1285
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   Hu J, 2013, NEURAL COMPUT, V25, P450, DOI 10.1162/NECO_a_00395
   Kasabov N, 2013, NEURAL NETWORKS, V41, P188, DOI 10.1016/j.neunet.2012.11.014
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Li G., 2019, ARXIV190701167
   Luo X., 2019, FRONT NEUROSCI, V13, P1
   Memmesheimer RM, 2014, NEURON, V82, P925, DOI 10.1016/j.neuron.2014.03.026
   Mohemmed A, 2012, INT J NEURAL SYST, V22, DOI 10.1142/S0129065712500128
   Neil D, 2016, IEEE INT SYMP CIRC S, P2282, DOI 10.1109/ISCAS.2016.7539039
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Rathi N, 2019, IEEE T COMPUT AID D, V38, P668, DOI 10.1109/TCAD.2018.2819366
   Sporea I, 2013, NEURAL COMPUT, V25, P473, DOI 10.1162/NECO_a_00396
   Taherkhani A, 2015, IEEE T NEUR NET LEAR, V26, P3137, DOI 10.1109/TNNLS.2015.2404938
   Tavanaei A, 2017, LECT NOTES COMPUT SC, V10639, P899, DOI 10.1007/978-3-319-70136-3_95
   Tavanaei A, 2017, NEUROCOMPUTING, V240, P191, DOI 10.1016/j.neucom.2017.01.088
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   van Rossum MCW, 2001, NEURAL COMPUT, V13, P751, DOI 10.1162/089976601300014321
   Victor JD, 1997, NETWORK-COMP NEURAL, V8, P127, DOI 10.1088/0954-898X/8/2/003
   von Kriegstein K, 2005, J COGNITIVE NEUROSCI, V17, P367, DOI 10.1162/0898929053279577
   Wang JL, 2017, IEEE T NEUR NET LEAR, V28, P30, DOI 10.1109/TNNLS.2015.2501322
   Wu J., 2018, FRONT NEUROSCI, V12, P1
   Wu JY, 2019, 2019 5TH INTERNATIONAL CONFERENCE ON EVENT-BASED CONTROL, COMMUNICATION, AND SIGNAL PROCESSING (EBCCSP), DOI 10.1109/ebccsp.2019.8836892
   Wu YJ, 2019, AAAI CONF ARTIF INTE, P1311
   Wysoski SG, 2010, NEURAL NETWORKS, V23, P819, DOI 10.1016/j.neunet.2010.04.009
   Xiao R, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1445
   Xu GL, 2013, ZOOKEYS, P1, DOI 10.3897/zookeys.260.3770
   Xu Y, 2013, NEURAL COMPUT, V25, P1472, DOI 10.1162/NECO_a_00450
   Xu Y, 2013, NEURAL NETWORKS, V43, P99, DOI 10.1016/j.neunet.2013.02.003
   Yu Q, 2019, IEEE T CYBERNETICS, V49, P2178, DOI 10.1109/TCYB.2018.2821692
   Yu Q, 2017, LECT NOTES COMPUT SC, V10639, P759, DOI 10.1007/978-3-319-70136-3_80
   Yu Q, 2013, IEEE T NEUR NET LEAR, V24, P1539, DOI 10.1109/TNNLS.2013.2245677
   Zhang ML, 2019, AAAI CONF ARTIF INTE, P1327
   Zhang ML, 2019, IEEE T NEUR NET LEAR, V30, P123, DOI 10.1109/TNNLS.2018.2833077
   Zhang ML, 2018, IEEE T COGN DEV SYST, V10, P151, DOI 10.1109/TCDS.2017.2651943
   Zhao B, 2015, IEEE T NEUR NET LEAR, V26, P1963, DOI 10.1109/TNNLS.2014.2362542
NR 46
TC 22
Z9 23
U1 0
U2 15
PD MAR
PY 2020
VL 14
IS 3
BP 592
EP 602
DI 10.1109/JSTSP.2020.2983547
UT WOS:000543960100011
DA 2023-11-16
ER

PT C
AU Yang, J
   Wu, QY
   Huang, MQ
   Luo, T
AF Yang Jing
   Wu Qingyuan
   Huang Maiqi
   Luo Ting
GP IOP
TI Real time human motion recognition via spiking neural network
SO 2018 3RD ASIA CONFERENCE ON POWER AND ELECTRICAL ENGINEERING (ACPEE
   2018)
SE IOP Conference Series-Materials Science and Engineering
DT Proceedings Paper
CT 3rd Asia Conference on Power and Electrical Engineering (ACPEE)
CY MAR 22-24, 2018
CL Kitakyushu, JAPAN
AB Real time human action recognition is to recognize the human motion type based on skeleton movement in real time and is always a challenging task. In this paper, a novel method is proposed to accomplish the classification by using Spiking neural network (SNN) which is biology oriented neural network dealing with precise timing spikes. First, a new temporal encoding scheme is used to encode the real time motion capture data into a series of spikes and the according type of the motion is represented by a spike time. Second, a two-layered spiking neural network is initiated and trained through a gradient descent learning algorithm. The experimental results show that this method achieves a good learning precision and generalization.
C1 [Yang Jing; Wu Qingyuan; Huang Maiqi; Luo Ting] Beijing Normal Univ, Zhuhai Campus, Zhuhai, Guangdong, Peoples R China.
RP Yang, J (corresponding author), Beijing Normal Univ, Zhuhai Campus, Zhuhai, Guangdong, Peoples R China.
EM yangjing@bnuz.edu.cn; wuqingyuan@bnuz.edu.cn; huangmaiqi@outlook.com;
   Gloria_LuoTing@163.com
CR Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Li WB, 2010, 2010 THE 3RD INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND INDUSTRIAL APPLICATION (PACIIA2010), VOL I, P9, DOI 10.1109/cvprw.2010.5543273
   Maass W, 1997, ADV NEUR IN, V9, P211
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Wang Z., 2014, ENV SYST RES, V3, DOI [http://dx.doi.org/10.1186/2193-2697-3-5, DOI 10.1186/2193-2697-3-5]
   Xu Y, 2013, NEURAL NETWORKS, V43, P99, DOI 10.1016/j.neunet.2013.02.003
   Zhang S, 2010, OPT LASER ENG, V48, P149, DOI 10.1016/j.optlaseng.2009.03.008
NR 7
TC 4
Z9 4
U1 0
U2 0
PY 2018
VL 366
AR 012042
DI 10.1088/1757-899X/366/1/012042
UT WOS:000446123600042
DA 2023-11-16
ER

PT J
AU Dora, S
   Suresh, S
   Sundararajan, N
AF Dora, S.
   Suresh, S.
   Sundararajan, N.
TI A sequential learning algorithm for a spiking neural classifier
SO APPLIED SOFT COMPUTING
DT Article
DE Spiking neural network; Sequential learning; Pattern classification;
   2-Dimensional coding
ID GRADIENT DESCENT; NETWORK; NEURONS; PREDICTION; BRAIN; MODEL
AB This paper presents a biologically inspired, sequential learning spiking neural classifier (SLSNC) for pattern classification problems. It consists of a two layered neural network and a separate decision block which estimates the predicted class label. Inspired by observations in the neuroscience literature, the input layer employs a new neuron model which converts real valued stimuli into spikes with varying amplitudes and firing times. The intermediate layer neurons are modeled as integrate-and-fire spiking neurons. The decision block identifies that intermediate neuron which fires first and returns the class label associated with that neuron as the predicted class label. The sequential learning algorithm for the spiking neural network automatically determines the network structure from the training samples and adapts its synaptic weights by long term potentiation and long term depression. Performance of SLSNC has been evaluated using a number of benchmark classification problems and the results have been compared with other well-known spiking neural network classifiers in the literature as well as with the standard support vector machine (SVM) with a Gaussian kernel and the fast learning Extreme Learning Machine (ELM) classifiers. The results clearly indicate that the described spiking neural network produces similar or better generalization performance with a smaller network. (C) 2015 Elsevier B.V. All rights reserved.
C1 [Dora, S.; Suresh, S.] Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
   [Sundararajan, N.] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
RP Suresh, S (corresponding author), Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
EM ssundaram@ntu.edu.sg
CR Abbott LF, 1999, BRAIN RES BULL, V50, P303, DOI 10.1016/S0361-9230(99)00161-6
   Alle H, 2006, SCIENCE, V311, P1290, DOI 10.1126/science.1119055
   [Anonymous], 2013, INT JOINT C NEUR NET, DOI DOI 10.1109/AGILE.2013.7
   Babu GS, 2012, NEUROCOMPUTING, V81, P86, DOI 10.1016/j.neucom.2011.12.001
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   Booij O, 2005, INFORM PROCESS LETT, V95, P552, DOI 10.1016/j.ipl.2005.05.023
   Capecci E, 2015, NEURAL NETWORKS, V68, P62, DOI 10.1016/j.neunet.2015.03.009
   Conover WJ, 1999, PRACTICAL NONPARAMET, V350
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   DiCaprio RA, 2003, J NEUROPHYSIOL, V89, P1826, DOI 10.1152/jn.00978.2002
   Dora S., 2015, NEUROCOMPUTING
   Dora S., 2015, P INT JOINT C NEUR N, P2178
   Dora S, 2014, IEEE IJCNN, P2415, DOI 10.1109/IJCNN.2014.6889775
   Florian RV, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0040233
   Ghosh-Dastidar S, 2007, INTEGR COMPUT-AID E, V14, P187
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   Gütig R, 2014, CURR OPIN NEUROBIOL, V25, P134, DOI 10.1016/j.conb.2014.01.004
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Huang GB, 2004, IEEE IJCNN, P985
   Huang GB, 2012, IEEE T SYST MAN CY B, V42, P513, DOI 10.1109/TSMCB.2011.2168604
   Kasabov N, 2014, NEUROCOMPUTING, V134, P269, DOI 10.1016/j.neucom.2013.09.049
   Kasabov N, 2013, NEURAL NETWORKS, V41, P188, DOI 10.1016/j.neunet.2012.11.014
   Kasabov NK, 2014, NEURAL NETWORKS, V52, P62, DOI 10.1016/j.neunet.2014.01.006
   Lapique L., 1907, J PHYSL PATHOL GEN, V9, P620, DOI DOI 10.1007/S00422-007-0189-6
   Lichman M., 2013, UCI MACHINE LEARNING
   Maass W, 1997, NEURAL COMPUT, V9, P279, DOI 10.1162/neco.1997.9.2.279
   Maass W, 1996, ADV NEUR IN, V8, P211
   MAASS W, 1999, TR1999037 TU GRAZ I
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   McKennoch S, 2006, IEEE IJCNN, P3970
   Mohemmed A, 2012, INT J NEURAL SYST, V22, DOI 10.1142/S0129065712500128
   Omkar SN, 2002, ICONIP'02: PROCEEDINGS OF THE 9TH INTERNATIONAL CONFERENCE ON NEURAL INFORMATION PROCESSING, P1827, DOI 10.1109/ICONIP.2002.1198989
   Paugam-Moisy H., 2012, HDB NATURAL COMPUTIN, V1, P1, DOI [DOI 10.1007/978-3-540-92910-9_10, 10.1007/978-3-540-92910-9_10]
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Shrestha SB, 2015, NEURAL NETWORKS, V63, P185, DOI 10.1016/j.neunet.2014.12.001
   Shu YS, 2006, NATURE, V441, P761, DOI 10.1038/nature04720
   Silva SM, 2005, PROCEEDINGS OF THE 2005 INTERNATIONAL CONFERENCE ON NEURAL NETWORKS AND BRAIN, VOLS 1-3, P1354
   Sporea I, 2013, NEURAL COMPUT, V25, P473, DOI 10.1162/NECO_a_00396
   Subramanian Kartick, 2013, IEEE Transactions on Fuzzy Systems, V21, P1080, DOI 10.1109/TFUZZ.2013.2242894
   Suresh S, 2003, AEROSP SCI TECHNOL, V7, P595, DOI 10.1016/S1270-9638(03)00053-1
   Suresh S, 2010, ENG APPL ARTIF INTEL, V23, P1149, DOI 10.1016/j.engappai.2010.06.009
   Victor JD, 1996, J NEUROPHYSIOL, V76, P1310, DOI 10.1152/jn.1996.76.2.1310
   Wade JJ, 2010, IEEE T NEURAL NETWOR, V21, P1817, DOI 10.1109/TNN.2010.2074212
   Wysoski SG, 2008, NEUROCOMPUTING, V71, P2563, DOI 10.1016/j.neucom.2007.12.038
   Wysoski SG, 2006, LECT NOTES COMPUT SC, V4131, P61
   Wysoski SG, 2010, NEURAL NETWORKS, V23, P819, DOI 10.1016/j.neunet.2010.04.009
   Xu Y, 2013, NEURAL NETWORKS, V43, P99, DOI 10.1016/j.neunet.2013.02.003
   Yu Q, 2014, NEUROCOMPUTING, V138, P3, DOI 10.1016/j.neucom.2013.06.052
NR 49
TC 10
Z9 11
U1 1
U2 31
PD NOV
PY 2015
VL 36
BP 255
EP 268
DI 10.1016/j.asoc.2015.06.062
UT WOS:000360424700022
DA 2023-11-16
ER

PT C
AU Kozemiako, VP
   Kolesnytskyj, OK
   Lischenko, TS
   Wojcik, W
   Sulemenov, A
AF Kozemiako, V. P.
   Kolesnytskyj, O. K.
   Lischenko, T. S.
   Wojcik, W.
   Sulemenov, A.
BE Romaniuk, RS
   Wojcik, W
TI Optoelectronic Spiking Neural Network
SO OPTICAL FIBERS AND THEIR APPLICATIONS 2012
SE Proceedings of SPIE
DT Proceedings Paper
CT 14th Conference on Optical Fibers and Their Applications / Workshop on
   Optical Fiber Technology
CY OCT 09-12, 2012
CL UMCS OFT Lab, Lublin, POLAND
HO UMCS OFT Lab
DE neural network; spiking neuron; optoelectronics; spatial light
   modulator; bispin-device
AB Proposed by the authors compact optoelectronic implementation of a spiking neural network uses a matrix of semiconductor lasers. Also proposed the implementation of neural element on bispin-device, which is able to manage by a range of lasers in the array. Described the principles of the network in the training and operation mode. To modify the connections weights during the training mode used the optically controlled transparant with memory. Network parameters evaluated achievable with current technological capabilities.
C1 [Kozemiako, V. P.; Kolesnytskyj, O. K.; Lischenko, T. S.] Vinnytsia Natl Tech Univ, Vinnytsia, Ukraine.
RP Kozemiako, VP (corresponding author), Vinnytsia Natl Tech Univ, 95 Khmelnytske Sh, Vinnytsia, Ukraine.
EM kafedra_lot@ukr.net
CR [Anonymous], PRINC OPTR ADV HIGH
   Bardachenko V. F., 2005, TIMER NEURAL ELEMENT
   Bardachenko V. F., 2003, YCIM, V6, P73
   Chicca E, 2003, IEEE T NEURAL NETWOR, V14, P1297, DOI 10.1109/TNN.2003.816367
   Galushkin A. I., 2000, NEUROCOMPUTERS
   Knab O. D., 1989, ELECT IND, V8, P3
   Kolesnytskyj K, 2010, OPTICAL MEMORY NEURA, V19, P154
   Kolesnytskyj OK, 2000, P SOC PHOTO-OPT INS, V4425, P417
   KOLESNYTSKYJ OK, 2006, INFORM TEKHNOLOGII T, P54
   KOTESNYTSKY OK, 2005, VYMIROVALNA TA OBCHY, P134
   Maass W, 2001, PULSED NEURAL NETWOR
   Modha D, DARPA SYNAPSE PROGRA
   Smolarz A, 2010, PRZ ELEKTROTECHNICZN, V86, P287
   STEMPKOVSKI A, 2003, ELEKT NAUKA TEKHNOLO, P14
   Vasilev A.A, 1987, SPATIAL LIGHT MODULA
NR 15
TC 1
Z9 1
U1 0
U2 5
PY 2013
VL 8698
AR 86980M
DI 10.1117/12.2019340
UT WOS:000320559700021
DA 2023-11-16
ER

PT J
AU Suetake, K
   Ikegawa, S
   Saiin, R
   Sawada, Y
AF Suetake, Kazuma
   Ikegawa, Shin-ichi
   Saiin, Ryuji
   Sawada, Yoshihide
TI S3NN: Time step reduction of spiking surrogate gradients for training
   energy efficient single-step spiking neural networks
SO NEURAL NETWORKS
DT Article
DE Spiking neural network; Binary neural network; Surrogate gradient;
   Energy efficiency; Single -time step
ID BACKPROPAGATION; HARDWARE
AB As the scales of neural networks increase, techniques that enable them to run with low computa-tional cost and energy efficiency are required. From such demands, various efficient neural network paradigms, such as spiking neural networks (SNNs) or binary neural networks (BNNs), have been proposed. However, they have sticky drawbacks, such as degraded inference accuracy and latency. To solve these problems, we propose a single-step spiking neural network (S3NN), an energy-efficient neural network with low computational cost and high precision. The proposed S3NN processes the information between hidden layers by spikes as SNNs. Nevertheless, it has no temporal dimension so that there is no latency within training and inference phases as BNNs. Thus, the proposed S3NN has a lower computational cost than SNNs that require time-series processing. However, S3NN cannot adopt naive backpropagation algorithms due to the non-differentiability nature of spikes. We deduce a suitable neuron model by reducing the surrogate gradient for multi-time step SNNs to a single -time step. We experimentally demonstrated that the obtained surrogate gradient allows S3NN to be trained appropriately. We also showed that the proposed S3NN could achieve comparable accuracy to full-precision networks while being highly energy-efficient. (c) 2022 The Author(s). Published by Elsevier Ltd. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).
C1 [Suetake, Kazuma; Saiin, Ryuji] AISIN SOFTWARE, Anjo, Aichi, Japan.
   [Ikegawa, Shin-ichi; Sawada, Yoshihide] AISIN, Tokyo Res Ctr, Tokyo, Japan.
RP Sawada, Y (corresponding author), AISIN, Tokyo Res Ctr, Tokyo, Japan.
EM kazuma.suetake@aisintsoftrare.com; shinichi.ikegara@aisin.co.jp;
   ryuji.saiin@aisintsoftrare.com; yoshihide.sawada@aisin.co.jp
CR Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   Bengio Y, 2013, Arxiv, DOI arXiv:1308.3432
   Bing Han, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13555, DOI 10.1109/CVPR42600.2020.01357
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Chen TL, 2021, IEEE COMPUT SOC CONF, P4614, DOI 10.1109/CVPRW53098.2021.00520
   Cheng X, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1519
   Chowdhury S.S., 2021, ARXIV
   Courbariaux M., 2015, ADV NEURAL INF PROCE, V2, P3123, DOI [DOI 10.1109/TWC.2016.2633262, DOI 10.5555/2969442.2969588]
   Cubuk ED, 2020, IEEE COMPUT SOC CONF, P3008, DOI 10.1109/CVPRW50498.2020.00359
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Deng S., 2020, INT C LEARNING REPRE
   Diehl P.U., 2015, 2015 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2015.7280696
   Ding JH, 2021, Arxiv, DOI arXiv:2105.11654
   Esser SK, 2016, P NATL ACAD SCI USA, V113, P11441, DOI 10.1073/pnas.1604850113
   Fang W, 2021, INCORPORATING LEARNA, P2661
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Horowitz M, 2014, ISSCC DIG TECH PAP I, V57, P10, DOI 10.1109/ISSCC.2014.6757323
   Hubara I, 2016, ADV NEUR IN, V29
   Kheradpisheh SR, 2022, NEURAL PROCESS LETT, V54, P1255, DOI 10.1007/s11063-021-10680-x
   Kim Y, 2022, AAAI CONF ARTIF INTE, P1192
   Kim Y, 2022, Arxiv, DOI arXiv:2201.10355
   Kim Y, 2021, FRONT NEUROSCI-SWITZ, V15, DOI 10.3389/fnins.2021.773954
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Kundu S., 2021, ARXIV
   Le Y., 2015, CS 231N, V7, P3
   Lee C, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00119
   Li Y., 2021, INT C MACHINE LEARNI, V139, P6316
   Li Y, 2021, Arxiv, DOI arXiv:2105.12917
   Loshchilov I., 2016, ARXIV
   Lu S, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00535
   Ma CX, 2021, IEEE IJCNN, DOI 10.1109/IJCNN52387.2021.9534390
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maguire LP, 2007, NEUROCOMPUTING, V71, P13, DOI 10.1016/j.neucom.2006.11.029
   McCulloch Warren S., 1943, BULL MATH BIOPHYS, V5, P115, DOI 10.1007/BF02478259
   Misra J, 2010, NEUROCOMPUTING, V74, P239, DOI 10.1016/j.neucom.2010.03.021
   Na BYG, 2022, Arxiv, DOI arXiv:2201.12738
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Neil D, 2016, P 31 ANN ACM S APPL
   Qiao GC, 2020, NEUROCOMPUTING, V409, P351, DOI 10.1016/j.neucom.2020.06.084
   Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32
   Rathi N., 2019, INT C LEARNING REPRE
   Rathi N, 2020, Arxiv, DOI arXiv:2008.03658
   Roy D, 2019, 2019 IEEE INTERNATIONAL CONFERENCE ON COGNITIVE COMPUTING (IEEE ICCC 2019), P50, DOI 10.1109/ICCC.2019.00020
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Salakhutdinov Ruslan, 2009, AISTATS
   Severa W, 2019, NAT MACH INTELL, V1, P86, DOI 10.1038/s42256-018-0015-y
   Shekhovtsov A., 2021, P DAGM GERM C PATT R, P111
   Shen G., 2021, ARXIV
   Shen MZ, 2020, INT CONF ACOUST SPEE, P4197, DOI 10.1109/icassp40776.2020.9054599
   Shi YT, 2020, IEEE ACCESS, V8, P98562, DOI 10.1109/ACCESS.2020.2995886
   Shrestha SB, 2018, ADV NEUR IN, V31
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   van Rossum MCW, 2001, NEURAL COMPUT, V13, P751, DOI 10.1162/089976601300014321
   Wang PS, 2020, AAAI CONF ARTIF INTE, V34, P12192
   Wang XW, 2020, NEURAL NETWORKS, V125, P258, DOI 10.1016/j.neunet.2020.02.011
   Wang ZW, 2019, PROC CVPR IEEE, P568, DOI 10.1109/CVPR.2019.00066
   WERBOS PJ, 1990, P IEEE, V78, P1550, DOI 10.1109/5.58337
   Xiao H, 2017, Arxiv, DOI arXiv:1708.07747
   Xu Z., 2019, 30 BRIT MACHINE VISI
   Yan YL, 2021, 2021 IEEE 3RD INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE CIRCUITS AND SYSTEMS (AICAS), DOI 10.1109/AICAS51828.2021.9458461
   Yan ZL, 2021, AAAI CONF ARTIF INTE, V35, P10577
   Yang Y., 2021, P 38 INT C MACHINE L, V139, P11852
   Yin Pengcheng, 2019, INT C LEARNING REPRE
   Yuan CY, 2022, Arxiv, DOI arXiv:2110.06804
   Zenke F, 2018, NEURAL COMPUT, V30, P1514, DOI 10.1162/neco_a_01086
   Zhang W, 2020, ARXIV PREPRINT ARXIV
   Zhang WR, 2019, ADV NEUR IN, V32
   Zheng HL, 2021, AAAI CONF ARTIF INTE, V35, P11062
NR 71
TC 0
Z9 0
U1 2
U2 6
PD FEB
PY 2023
VL 159
BP 208
EP 219
DI 10.1016/j.neunet.2022.12.008
UT WOS:000991735700001
DA 2023-11-16
ER

PT J
AU Andreeva, NV
   Ryndin, EA
   Gerasimova, MI
AF Andreeva, N. V.
   Ryndin, E. A.
   Gerasimova, M. I.
TI Memristive Logic Design of Multifunctional Spiking Neural Network with
   Unsupervised Learning
SO BIONANOSCIENCE
DT Article
DE Neuromorphic; Multilevel resistive switching; Spiking neural networks
ID EXTRACTION
AB We report a prospective approach to neural network modeling based on implementation of metal-oxide heterostructures with non-volatile memory behavior and multilevel resistive switching. These structures could be used as artificial synapses in neural networks, providing a modulation of synaptic strength in the range of seven orders of magnitude. Together with leaky integrated-and-fire neurons, it allows to organize a feed-forward spiking neural network with embedded spike-timing-dependent plasticity mechanism at the hardware level. The results of computer simulation demonstrate an ability of reconstructed networks based on electronic multilevel synapses to unsupervised learning and processing of asynchronous stream of spikes. Following the results of computer simulation, the biologically inspired circuit design for artificial neural network with memristive synapses was developed. The idea underlying the circuit design is based on the analogue approach and implies the hybrid CMOS-neurons/memristive synapses network, where operational amplifiers used as elements of current control through the memristive devices.
C1 [Andreeva, N. V.; Ryndin, E. A.; Gerasimova, M. I.] St Petersburg Electrotech Univ, St Petersburg 197376, Russia.
RP Andreeva, NV (corresponding author), St Petersburg Electrotech Univ, St Petersburg 197376, Russia.
EM nvandr@gmail.com
CR Andreeva N, 2018, AIP ADV, V8, DOI 10.1063/1.5019570
   Basheer IA, 2000, J MICROBIOL METH, V43, P3, DOI 10.1016/S0167-7012(00)00201-3
   Bayat FM, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-04482-4
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Bichler O, 2012, NEURAL NETWORKS, V32, P339, DOI 10.1016/j.neunet.2012.02.022
   Burr GW, 2017, ADV PHYS-X, V2, P89, DOI 10.1080/23746149.2016.1259585
   ChenB ChenEQ, 2018, SCI ADV, P9, DOI 1186/s13287-018-0972-4
   Chicca E, 2014, P IEEE, V102, P1367, DOI 10.1109/JPROC.2014.2313954
   Emelyanov AV, 2020, NANOTECHNOLOGY, V31, DOI 10.1088/1361-6528/ab4a6d
   Fernandes C., 2015, THESIS
   Gibson S, 2010, IEEE T NEUR SYS REH, V18, P469, DOI 10.1109/TNSRE.2010.2051683
   Gismatulin AA, 2019, APPL PHYS LETT, V115, DOI 10.1063/1.5127039
   Gokmen T, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00333
   Guo YL, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00812
   Gupta I, 2019, FARADAY DISCUSS, V213, P511, DOI 10.1039/c8fd00130h
   Gupta I, 2018, IEEE T BIOMED CIRC S, V12, P351, DOI 10.1109/TBCAS.2018.2797939
   Jeong DS, 2018, ADV MATER, V30, DOI 10.1002/adma.201704729
   Jiang H, 2016, SCI REP-UK, V6, DOI 10.1038/srep28525
   Juzekaeva E., 2018, ADV MAT TECHNOLOGIES
   Kim CH, 2019, NANOTECHNOLOGY, V30, DOI 10.1088/1361-6528/aae975
   Kiselev M., 2019, P IJCNN 2019
   Kiselev M, 2016, IEEE IJCNN, P1355, DOI 10.1109/IJCNN.2016.7727355
   Kornijcuk V, 2019, ADV INTELL SYST-GER, V1, DOI 10.1002/aisy.201900030
   Kuzum D, 2013, NANOTECHNOLOGY, V24, DOI 10.1088/0957-4484/24/38/382001
   Li H, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON RECENT TRENDS IN ELECTRONICS, INFORMATION & COMMUNICATION TECHNOLOGY (RTEICT), P1, DOI 10.1109/RTEICT.2016.7807769
   Malenka RC, 2004, NEURON, V44, P5, DOI 10.1016/j.neuron.2004.09.012
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   Mikhaylov A, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00358
   Minnekhanov AA, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-47263-9
   Pedretti G, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-05480-0
   Pi S, 2019, NAT NANOTECHNOL, V14, P35, DOI 10.1038/s41565-018-0302-0
   Potok T, 2018, ACM J EMERGING TECHN, V14, P2
   Prezioso M, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-07757-y
   Sinev AE, 2018, PROCEEDINGS OF THE 2018 IEEE INTERNATIONAL CONFERENCE ON ELECTRICAL ENGINEERING AND PHOTONICS (EEXPOLYTECH), P189, DOI 10.1109/EExPolytech.2018.8564403
   Wang ZR, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-05677-5
   Xia QF, 2019, NAT MATER, V18, P309, DOI 10.1038/s41563-019-0291-x
   Yang JJS, 2013, NAT NANOTECHNOL, V8, P13, DOI [10.1038/nnano.2012.240, 10.1038/NNANO.2012.240]
NR 37
TC 9
Z9 12
U1 1
U2 13
PD DEC
PY 2020
VL 10
IS 4
BP 824
EP 833
DI 10.1007/s12668-020-00778-2
EA AUG 2020
UT WOS:000559316300001
DA 2023-11-16
ER

PT C
AU Gutiérrez-Naranjo, MA
   Pérez-Jiménez, MJ
AF Gutierrez-Naranjo, Miguel A.
   Perez-Jimenez, Mario J.
BE Corne, DW
   Frisco, P
   Paun, G
   Rozenberg, G
   Salomaa, A
TI Hebbian Learning from Spiking Neural P Systems View
SO MEMBRANE COMPUTING
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 9th International Workshop on Membrane Computing
CY JUL 28-31, 2008
CL Edinburgh, SCOTLAND
ID CAT VISUAL-CORTEX; NEURONS; SYNCHRONIZATION; HIPPOCAMPUS; RESPONSES
AB Spiking neural P systems and artificial neural networks are computational devices which share a biological inspiration based on the flow of information among neurons. In this paper we present a first model for Hebbian learning in the framework of spiking neural P systems by using concepts borrowed from neuroscience and artificial neural network theory.
C1 [Gutierrez-Naranjo, Miguel A.; Perez-Jimenez, Mario J.] Univ Seville, Dept Comp Sci & Artificial Intelligence, Res Grp Nat Comp, E-41012 Seville, Spain.
RP Gutiérrez-Naranjo, MA (corresponding author), Univ Seville, Dept Comp Sci & Artificial Intelligence, Res Grp Nat Comp, Avda Reina Mercedes S-N, E-41012 Seville, Spain.
EM magutier@us.es; marper@us.es
CR Adrian ED, 1926, J PHYSIOL-LONDON, V61, P49, DOI 10.1113/jphysiol.1926.sp002273
   ADRIAN ED, 1926, BASIS SENSATION
   [Anonymous], P 5 BRAINST WEEK MEM
   BLISS TVP, 1993, NATURE, V361, P31, DOI 10.1038/361031a0
   DEBANNE D, 1994, P NATL ACAD SCI USA, V91, P1148, DOI 10.1073/pnas.91.3.1148
   ENGEL AK, 1991, SCIENCE, V252, P1177, DOI 10.1126/science.252.5009.1177
   FREUND R, 2007, P INT WORKSH AUT CEL, P64
   Gerstner W, 1996, NATURE, V383, P76, DOI 10.1038/383076a0
   Gerstner W., 2002, SPIKING NEURON MODEL
   GRAY CM, 1989, NATURE, V338, P334, DOI 10.1038/338334a0
   GUTIERREZNARANJ.M, 2008, 6 BRAINST WEEK MEMBR, P211
   Haykin S., 1994, COMPREHENSIVE FDN NE
   Hebb D., 2005, ORG BEHAV NEUROPSYCH
   HUBEL DH, 1959, J PHYSIOL-LONDON, V148, P574, DOI 10.1113/jphysiol.1959.sp006308
   Ionescu M, 2006, FUND INFORM, V71, P279
   Markram H, 1996, NATURE, V382, P807, DOI 10.1038/382807a0
   Markram H., 2007, SOC NEUR ABSTR, V21
   MOUNTCASTLE VB, 1957, J NEUROPHYSIOL, V20, P408, DOI 10.1152/jn.1957.20.4.408
   Natschlager T, 1998, NETWORK-COMP NEURAL, V9, P319, DOI 10.1088/0954-898X/9/3/003
   Ramon, 1909, HISTOLOGIE SYSTEME N
   Thorpe S, 1996, NATURE, V381, P520, DOI 10.1038/381520a0
NR 21
TC 10
Z9 10
U1 0
U2 2
PY 2009
VL 5391
BP 217
EP 230
UT WOS:000264882300016
DA 2023-11-16
ER

PT C
AU Cordone, L
   Miramond, B
   Thierion, P
AF Cordone, Loic
   Miramond, Benoit
   Thierion, Philippe
GP IEEE
TI Object Detection with Spiking Neural Networks on Automotive Event Data
SO 2022 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT IEEE International Conference on Fuzzy Systems (FUZZ-IEEE) / IEEE World
   Congress on Computational Intelligence (IEEE WCCI) / International Joint
   Conference on Neural Networks (IJCNN) / IEEE Congress on Evolutionary
   Computation (IEEE CEC)
CY JUL 18-23, 2022
CL Padua, ITALY
DE spiking neural networks; event cameras; object detection; SSD
AB Automotive embedded algorithms have very high constraints in terms of latency, accuracy and power consumption. In this work, we propose to train spiking neural networks (SNNs) directly on data coming from event cameras to design fast and efficient automotive embedded applications. Indeed, SNNs are more biologically realistic neural networks where neurons communicate using discrete and asynchronous spikes, a naturally energy-efficient and hardware friendly operating mode. Event data, which are binary and sparse in space and time, are therefore the ideal input for spiking neural networks. But to date, their performance was insufficient for automotive real-world problems, such as detecting complex objects in an uncontrolled environment. To address this issue, we took advantage of the latest advancements in matter of spike backpropagation - surrogate gradient learning, parametric LIF, SpikingJelly framework and of our new voxel cube event encoding to train 4 different SNNs based on popular deep learning networks: SqueezeNet, VGG, MobileNet, and DenseNet. As a result, we managed to increase the size and the complexity of SNNs usually considered in the literature. In this paper, we conducted experiments on two automotive event datasets, establishing new state-of-the-art classification results for spiking neural networks. Based on these results, we combined our SNNs with SSD to propose the first spiking neural networks capable of performing object detection on the complex GEN1 Automotive Detection event dataset.
C1 [Cordone, Loic] Renault, Sophia Antipolis, France.
   [Cordone, Loic; Miramond, Benoit] Univ Cote Azur, LEAT, CNRS UMR 7248, Sophia Antipolis, France.
   [Thierion, Philippe] Renault, Software Factory, Sophia Antipolis, France.
RP Cordone, L (corresponding author), Renault, Sophia Antipolis, France.
EM loic.cordone@renault.com; benoit.miramond@univ-cotedazur.fr;
   philippe.thierion@renault.com
CR Abderrahmane N, 2020, NEURAL NETWORKS, V121, P366, DOI 10.1016/j.neunet.2019.09.024
   Bardow P, 2016, PROC CVPR IEEE, P884, DOI 10.1109/CVPR.2016.102
   Bekolay T, 2014, FRONT NEUROINFORM, V7, DOI 10.3389/fninf.2013.00048
   Cannici Marco, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12365), P136, DOI 10.1007/978-3-030-58565-5_9
   Cannici M., 2019, IEEE CVPR EV BAS VIS
   Cordone L, 2021, IEEE IJCNN, DOI 10.1109/IJCNN52387.2021.9533514
   de Tournemire P., 2020, ARXIV200108499
   Delorme A, 1999, NEUROCOMPUTING, V26-7, P989, DOI 10.1016/S0925-2312(99)00095-8
   Fang W., 2020, SPIKINGJELLY
   Fang Wei, 2021, INT C COMP VIS
   Fang Wei, 2021, ADV NEURAL INFORM PR
   Howard Andrew G., 2017, MOBILENETS EFFICIENT
   Huang G., 2017, INT C COMP VIS THEOR
   Iandola Forrest N., 2016, P IEEE C COMPUTER VI
   Khacef L, 2018, IEEE IJCNN
   Kim SY, 2020, AAAI CONF ARTIF INTE, V34, P11278
   Kugele A., 2021, DAGM GERM C PATT REC, P297, DOI [10.1007/978-3-030-92659-5_19, DOI 10.1007/978-3-030-92659-5_19]
   Lagorce X, 2017, IEEE T PATTERN ANAL, V39, P1346, DOI 10.1109/TPAMI.2016.2574707
   Li Yuheng, 2021, IEEE INT C COMP VIS
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lin Tsung-Yi, 2017, INT C COMP VIS
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Maqueda AI, 2018, PROC CVPR IEEE, P5419, DOI 10.1109/CVPR.2018.00568
   Messikommer Nico, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P415, DOI 10.1007/978-3-030-58598-3_25
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Orchard G., 2021, ARXIV211103746
   Pellegrini T, 2021, IEEE W SP LANG TECH, P97, DOI 10.1109/SLT48900.2021.9383587
   Prophesee, 2020, EV PREPR TUT EV CUB
   Safa A., 2022, ARXIV211100791
   Shrestha S. B., 2018, ADV NEURAL INFORM PR
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sironi A, 2018, PROC CVPR IEEE, P1731, DOI 10.1109/CVPR.2018.00186
   Tournemire P. D., 2020, ADV NEURAL INFORM PR, P1
   Toyoizumi T., 2005, ADV NEURAL INFORM PR
   Xiao R, 2020, IEEE T NEUR NET LEAR, V31, P3649, DOI 10.1109/TNNLS.2019.2945630
NR 35
TC 5
Z9 5
U1 7
U2 9
PY 2022
DI 10.1109/IJCNN55064.2022.9892618
UT WOS:000867070906002
DA 2023-11-16
ER

PT J
AU Zhang, ZX
   Liu, Q
AF Zhang, Zhixuan
   Liu, Qi
TI Spike-Event-Driven Deep Spiking Neural Network With Temporal Encoding
SO IEEE SIGNAL PROCESSING LETTERS
DT Article
DE Neurons; Encoding; Feature extraction; Computational modeling; Task
   analysis; Image coding; Biological neural networks; Image
   classification; multi-scale; speech recognition; spike-event-driven;
   spiking neural network
AB Feature extractionplays an important role before pattern recognition takes place. The existing artificial neural networks (ANNs), however, ignoreto learn and represent temporal information, instead of only utilizing spatial information for recognition. Moreover, the substantial computational and energy costs resulted from the conventional ANN-based classifiers, limit their uses in mobile and embedded applications. In this work, we develop a sparse temporal encoding method which exploits both spatial and temporal information. On the basis of spike-timing-dependent plasticity and multi-scale structure, the resulting temporal feature representation integrates with a temporal spiking neural network (SNN) classifier to achieve high efficiency of parallel computing for feature extraction. Experimental evaluation on four benchmark datasets from image classification and speech recognition tasks show the proposed SNN model yielding state-of-the-art accuracy.
C1 [Zhang, Zhixuan] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 610054, Peoples R China.
   [Liu, Qi] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 117583, Singapore.
RP Liu, Q (corresponding author), Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 117583, Singapore.
EM zhangzhixuan77@gmail.com; elelqi@nus.edu.sg
CR Abdollahi M, 2011, BIOMED CIRC SYST C, P269, DOI 10.1109/BioCAS.2011.6107779
   [Anonymous], 2019, PROC IEEE INT JOINT
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Darabkh KA, 2018, COMPUT APPL ENG EDUC, V26, P285, DOI 10.1002/cae.21884
   Dennis J, 2013, INT CONF ACOUST SPEE, P803, DOI 10.1109/ICASSP.2013.6637759
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Doddington G., 1993, TIDIGITS LDC93S10 LI, DOI [10.35111/72xz-6x59, DOI 10.35111/72XZ-6X59]
   He H, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00650
   Hussain S, 2014, IEEE INT SYMP CIRC S, P2640, DOI 10.1109/ISCAS.2014.6865715
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012
   Masquelier T, 2007, PLOS COMPUT BIOL, V3, P247, DOI 10.1371/journal.pcbi.0030031
   Mozafari M, 2019, PATTERN RECOGN, V94, P87, DOI 10.1016/j.patcog.2019.05.015
   Mozafari M, 2018, IEEE T NEUR NET LEAR, V29, P6178, DOI 10.1109/TNNLS.2018.2826721
   Nakamura S., 2000, LREC
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   Pan ZH, 2020, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.01420
   Querlioz D, 2013, IEEE T NANOTECHNOL, V12, P288, DOI 10.1109/TNANO.2013.2250995
   Sailor HB, 2016, IEEE-ACM T AUDIO SPE, V24, P2341, DOI 10.1109/TASLP.2016.2607341
   Shrestha S. B., 2018, ARXIV181008646, V31
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tavanaei A, 2017, NEUROCOMPUTING, V240, P191, DOI 10.1016/j.neucom.2017.01.088
   Wu JB, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00836
   Xiao R, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1445
   Xiao R, 2017, COMM COM INF SC, V710, P584, DOI 10.1007/978-981-10-5230-9_57
   Yin X, 2018, IEEE T IMAGE PROCESS, V27, P964, DOI 10.1109/TIP.2017.2765830
   Yu Q, 2013, IEEE T NEUR NET LEAR, V24, P1539, DOI 10.1109/TNNLS.2013.2245677
   Zhang KP, 2016, IEEE SIGNAL PROC LET, V23, P1499, DOI 10.1109/LSP.2016.2603342
   Zhang M., 2020, ARXIV
   Zhang ML, 2019, AAAI CONF ARTIF INTE, P1327
   Zhang ML, 2019, IEEE T NEUR NET LEAR, V30, P123, DOI 10.1109/TNNLS.2018.2833077
   Zhang ML, 2018, IEEE T COGN DEV SYST, V10, P151, DOI 10.1109/TCDS.2017.2651943
   Zhang Y, 2015, IEEE T NEUR NET LEAR, V26, P2635, DOI 10.1109/TNNLS.2015.2388544
   Zhao B, 2015, IEEE T NEUR NET LEAR, V26, P1963, DOI 10.1109/TNNLS.2014.2362542
   Zhou Y, 2020, IEEE SIGNAL PROC LET, V27, P1310, DOI 10.1109/LSP.2020.3010163
NR 36
TC 4
Z9 4
U1 2
U2 22
PY 2021
VL 28
BP 484
EP 488
DI 10.1109/LSP.2021.3059172
UT WOS:000629722500002
DA 2023-11-16
ER

PT C
AU Kerr, D
   McGinnity, M
   Coleman, S
   Wu, QX
   Clogenson, M
AF Kerr, Dermot
   McGinnity, Martin
   Coleman, Sonya
   Wu, Qingxiang
   Clogenson, Marine
BE Madani, K
   Kacprzyk, J
   Filipe, J
TI SPIKING HIERARCHICAL NEURAL NETWORK FOR CORNER DETECTION
SO NCTA 2011: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON NEURAL
   COMPUTATION THEORY AND APPLICATIONS
DT Proceedings Paper
CT International Conference on Neural Computation Theory and Applications
CY OCT 24-26, 2011
CL Univ Paris Est Creteil, Paris, FRANCE
HO Univ Paris Est Creteil
DE Spiking neural network; Corner detection
AB To enable fast reliable feature matching or tracking in scenes, features need to be discrete and meaningful, and hence corner detection is often used for this purpose. We present a new approach to corner detection inspired by the structure and behaviour of the human visual system, which uses spiking neural networks. Standard digital images are processed and converted to spikes in a manner similar to the processing that is performed in the retina. The spiking neural network performs edge and corner detection using receptive fields that are able to detect edges and corners of various orientations. The locations where neurons emit a spike indicate the positions of detected features. Results are presented using synthetic and real images.
C1 [Kerr, Dermot; McGinnity, Martin; Coleman, Sonya; Wu, Qingxiang] Univ Ulster, Intelligent Syst Res Ctr, Magee BT48 7JL, Derry, North Ireland.
   [Clogenson, Marine] CPE Lyon, F-69616 Villeurbanne, France.
RP Kerr, D (corresponding author), Univ Ulster, Intelligent Syst Res Ctr, Magee BT48 7JL, Derry, North Ireland.
EM d.kerr@ulster.ac.uk; tm.mcginnity@ulster.ac.uk; sa.coleman@ulster.ac.uk;
   marine.clogenson@cpe.fr
CR [Anonymous], 2010, 2010 INT JOINT C NEU
   [Anonymous], 1977, INT JOINT C ART INT
   Benyettou A., 2010, NEURAL PROCESSING LE, V32
   Chevallier S., 2006, ESANN 06
   Escobar MJ, 2009, INT J COMPUT VISION, V82, P284, DOI 10.1007/s11263-008-0201-1
   Gerstner W., 2002, SPIKING NEURON MODEL
   Goodman DFM, 2009, FRONT NEUROSCI-SWITZ, V3, P192, DOI 10.3389/neuro.01.026.2009
   Harris C., 1988, ALV VIS C MANCH UK, V4, P147, DOI DOI 10.5244/C.2.23
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Hugues E., 2002, BMVC
   Izhikevich E. M., IEEE T NEURAL NETWOR, V15, P5
   O'Connor DH, 2009, NATURE, V461, P923, DOI 10.1038/nature08539
   SHAPLEY RM, 1973, J PHYSIOL-LONDON, V229, P165, DOI 10.1113/jphysiol.1973.sp010133
   Shen F., 2001, P 3 INT C INF COMM S
   Smith SM, 1997, INT J COMPUT VISION, V23, P45, DOI 10.1023/A:1007963824710
   VanRullen R, 2002, VISION RES, V42, P2593, DOI 10.1016/S0042-6989(02)00298-5
   Wu Q. X., 2007, P ICIC
   Wu Q. X., 2007, 7 INT WORKSH INF PRO
NR 18
TC 8
Z9 8
U1 0
U2 0
PY 2011
BP 230
EP 235
UT WOS:000392350600037
DA 2023-11-16
ER

PT C
AU Yousefi, A
   Dibazar, AA
   Berger, TW
AF Yousefi, Ali
   Dibazar, Alireza A.
   Berger, Theodore W.
GP IEEE
TI Synaptic Dynamics: Linear Model and Adaptation Algorithm
SO 2012 ANNUAL INTERNATIONAL CONFERENCE OF THE IEEE ENGINEERING IN MEDICINE
   AND BIOLOGY SOCIETY (EMBC)
SE IEEE Engineering in Medicine and Biology Society Conference Proceedings
DT Proceedings Paper
CT 34th Annual International Conference of the IEEE
   Engineering-in-Medicine-and-Biology-Society (EMBS)
CY AUG 28-SEP 01, 2012
CL San Diego, CA
ID NEURAL-NETWORKS
AB Linear model for synapse temporal dynamics and learning algorithm for synaptic adaptation in spiking neural networks are presented. The proposed linear model substantially simplifies analysis and training of spiking neural networks, meanwhile accurately models facilitation and depression dynamics in synapse. The learning rule is biologically plausible and is capable of simultaneously adjusting both of LTP and STP parameters of individual synapses in a network. To prove efficiency of the system, a small size spiking neural network is trained for generating different spike and bursting patterns of cortical neurons. The simulation results revealed that the linear model of synaptic dynamics along with the proposed STDP based learning algorithm can provide a practical tool for simulating and training very large scale spiking neural circuitry comprising of significant number of synapses and neurons.
C1 [Yousefi, Ali; Dibazar, Alireza A.] Univ So Calif, Neural Dynam Lab, Los Angeles, CA 90089 USA.
RP Yousefi, A (corresponding author), Univ So Calif, Neural Dynam Lab, Los Angeles, CA 90089 USA.
EM ayousefi@usc.edu
CR Abbott LF, 2004, NATURE, V431, P796, DOI 10.1038/nature03010
   Bohte S.M., 2003, THESIS
   Dittman JS, 2000, J NEUROSCI, V20, P1374
   Gerstner W., 2006, SPIKING NEURON MODEL
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   Gutig Robert, 2006, NATURE NEUROSCIENCE
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Kreuz T, 2007, J NEUROSCI METH, V165, P151, DOI 10.1016/j.jneumeth.2007.05.031
   Liaw JS, 1999, NEUROCOMPUTING, V26-7, P199, DOI 10.1016/S0925-2312(99)00063-6
   Liaw JS, 1996, HIPPOCAMPUS, V6, P591
   Mongillo G, 2008, SCIENCE, V319, P1543, DOI 10.1126/science.1150769
   Morrison A, 2008, BIOL CYBERN, V98, P459, DOI 10.1007/s00422-008-0233-1
   Natschläger T, 2001, NETWORK-COMP NEURAL, V12, P75, DOI 10.1088/0954-898X/12/1/305
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Tsodyks M, 1998, NEURAL COMPUT, V10, P821, DOI 10.1162/089976698300017502
   Yousefi A., 2011, IJCNN 2011
NR 16
TC 2
Z9 2
U1 0
U2 2
PY 2012
BP 1362
EP 1365
UT WOS:000313296501155
DA 2023-11-16
ER

PT C
AU Haessig, G
   Galluppi, F
   Lagorce, X
   Benosman, R
AF Haessig, Germain
   Galluppi, Francesco
   Lagorce, Xavier
   Benosman, Ryad
GP IEEE
TI Neuromorphic networks on the SpiNNaker platform
SO 2019 IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE CIRCUITS
   AND SYSTEMS (AICAS 2019)
DT Proceedings Paper
CT 1st IEEE International Conference on Artificial Intelligence Circuits
   and Systems (AICAS)
CY MAR 18-20, 2019
CL Hsinchu, TAIWAN
ID HANDSHAKING; FPGAS; LINKS
AB This paper describes spike-based neural networks for optical flow and stereo estimation from Dynamic Vision Sensors data. These methods combine the Asynchronous Time-based Image Sensor with the SpiNNaker platform. The sensor generates spikes with sub-millisecond resolution in response to scene illumination changes. These spike are processed by a spiking neural network running on SpiNNaker with a 1 millisecond resolution to accurately determine the order and time difference of spikes from neighboring pixels, and therefore infer the velocity, direction or depth. The spiking neural networks are a variant of the Barlow-Levick method for optical flow estimation, and Marr& Poggio for the stereo matching.
C1 [Haessig, Germain] Unvers Zurich, Inst Neuroinformat, Zurich, Switzerland.
   [Haessig, Germain] Swiss Fed Inst Technol, Zurich, Switzerland.
   [Haessig, Germain; Galluppi, Francesco; Lagorce, Xavier; Benosman, Ryad] Sorbonne Univ, Inst Vis, Paris, France.
   [Benosman, Ryad] Univ Pittsburgh, Med Ctr, Pittsburgh, PA USA.
RP Haessig, G (corresponding author), Unvers Zurich, Inst Neuroinformat, Zurich, Switzerland.; Haessig, G (corresponding author), Swiss Fed Inst Technol, Zurich, Switzerland.; Haessig, G (corresponding author), Sorbonne Univ, Inst Vis, Paris, France.
EM germain@ini.uzh.ch
CR [Anonymous], 2014, P 20 EURO WIRELESS C
   BARLOW HB, 1965, J PHYSIOL-LONDON, V178, P477, DOI 10.1113/jphysiol.1965.sp007638
   Berner R, 2007, IEEE INT SYMP CIRC S, P2451, DOI 10.1109/ISCAS.2007.378616
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   Giulioni M, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00035
   HAESSIG G., 2018, IEEE T BIOMED CIRC S, V99, P1
   INC, 2011, TECH REP
   MARR D, 1976, SCIENCE, V194, P283, DOI 10.1126/science.968482
   Navaridas J, 2013, PARALLEL COMPUT, V39, P693, DOI 10.1016/j.parco.2013.09.001
   Osswald M, 2017, SCI REP-UK, V7, DOI 10.1038/srep40703
   PLANA L. A, 2013, TECH REP
   Posch C, 2011, IEEE J SOLID-ST CIRC, V46, P259, DOI 10.1109/JSSC.2010.2085952
   Rogister P, 2012, IEEE T NEUR NET LEAR, V23, P347, DOI 10.1109/TNNLS.2011.2180025
   TEMPLE S, 2011, TECH REP
   Yousefzadeh A, 2017, IEEE T BIOMED CIRC S, V11, P1133, DOI 10.1109/TBCAS.2017.2717341
   Yousefzadeh A, 2016, IEEE T CIRCUITS-II, V63, P763, DOI 10.1109/TCSII.2016.2531092
NR 16
TC 4
Z9 4
U1 0
U2 2
PY 2019
BP 86
EP 91
DI 10.1109/aicas.2019.8771512
UT WOS:000493095400020
DA 2023-11-16
ER

PT C
AU Rajput, G
   Raut, G
   Khan, S
   Gupta, N
   Behor, A
   Vishvakarma, SK
AF Rajput, Gunjan
   Raut, Gopal
   Khan, Sajid
   Gupta, Neha
   Behor, Ankur
   Vishvakarma, Santosh Kumar
GP IEEE
TI ASIC Implementation Of Biologically Inspired Spiking Neural Network
SO 2019 9TH INTERNATIONAL CONFERENCE ON EMERGING TRENDS IN ENGINEERING AND
   TECHNOLOGY: SIGNAL AND INFORMATION PROCESSING (ICETET-SIP-19)
SE International Conference on Emerging Trends in Engineering and
   Technology
DT Proceedings Paper
CT 9th International Conference on Emerging Trends in Engineering and
   Technology - Signal and Information Processing (ICETET-SIP)
CY NOV 01-02, 2019
CL GH Raisoni Coll Engn, Nagpur, INDIA
HO GH Raisoni Coll Engn
DE Action potential; refractory period; spiking neural network; synaptic
   weight
AB This paper presents the biologically inspired spiking neural network which can mimic the operation of our biological neural cell using CMOS implementation. Hardware implementation of a spiking neural network (SNN) shows more reliability and it comprises less area with high speed, which is much more than the neural network which has software counterpart. A body biasing technique has been used for the implementation of potassium and sodium ions in the neural cell. A fast neuron is required for the applications such as in image processing, data mining, supercomputing etc. Structural forms of spiking neural network and with synaptic functions are shown along with the simulation results. The main advantage of the proposed circuit is that it has a compact design, low power, and delay. In the proposed circuit power is decreased by 33.95 percent and delay is reduced by manifold. Power delay product (PDP) is approximately increased by 18x as a comparison with that of the reference circuit.
C1 [Rajput, Gunjan; Raut, Gopal; Khan, Sajid; Gupta, Neha; Behor, Ankur; Vishvakarma, Santosh Kumar] Indian Inst Technol, Discipline Elect Engn, Indore 453552, MP, India.
RP Rajput, G (corresponding author), Indian Inst Technol, Discipline Elect Engn, Indore 453552, MP, India.
EM gunjan.rajput17@yahoo.com; skvishvakarma@iiti.ac.in
CR ABBOTT LF, 1990, LECT NOTES PHYS, V368, P5
   [Anonymous], 2018, INTRO THEORY NEURAL, DOI DOI 10.1201/9780429499661
   [Anonymous], 2000, PRINCIPLES NEURAL SC, DOI DOI 10.1007/SPRINGERREFERENCE_183113
   Eckhorn R, 1999, IEEE T NEURAL NETWOR, V10, P464, DOI 10.1109/72.761705
   Gurney K., 2014, INTRO NEURAL NETWORK
   Ham FM., 2001, PRINCIPLES NEUROCOMP
   Haykin S, 1994, NEURAL NETWORKS, V2
   Hebb D. O., 1961, ORG BEHAV
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V116, P449, DOI 10.1113/jphysiol.1952.sp004717
   KOSTYUK PG, 1981, NEUROSCIENCE, V6, P2423, DOI 10.1016/0306-4522(81)90088-9
   Le Masson S, 1999, IEEE T BIO-MED ENG, V46, P638, DOI 10.1109/10.764940
   MURRAY AF, 1991, IEEE T NEURAL NETWOR, V2, P193, DOI 10.1109/72.80329
   Nicholls J. G., 2001, NEURON BRAIN, V271
   Ota Y, 1999, IEEE T NEURAL NETWOR, V10, P539, DOI 10.1109/72.761710
   Park J, 2017, IEEE T ELECTRON DEV, V64, P2438, DOI 10.1109/TED.2017.2685519
   Schemmel J, 2006, IEEE IJCNN, P1
   Schmitt O., 1937, P AM PHYS SOC 49 ANN
   Waldrop MM, 2016, NATURE, V530, P144, DOI 10.1038/530144a
   Wilamowski BM, 1996, IEEE IJCNN, P986, DOI 10.1109/ICNN.1996.549031
NR 19
TC 0
Z9 0
U1 0
U2 2
PY 2019
AR 26
DI 10.1109/icetet-sip-1946815.2019.9092079
UT WOS:000627713800015
DA 2023-11-16
ER

PT J
AU Zhang, CF
   Xiao, ZG
   Sheng, ZW
AF Zhang, Changfan
   Xiao, Zunguang
   Sheng, Zhenwen
TI A bearing fault diagnosis method based on a convolutional spiking neural
   network with spatial-temporal feature-extraction capability
SO TRANSPORTATION SAFETY AND ENVIRONMENT
DT Article
DE fault diagnosis; spiking neural network (SNN); convolutional neural
   network (CNN); surrogate gradient method
AB Convolutional neural networks (CNNs) are widely used in the field of fault diagnosis due to their strong feature-extraction capability. However, in each timestep, CNNs only consider the current input and ignore any cyclicity in time, therefore producing difficulties in mining temporal features from the data. In this work, the third-generation neural network-the spiking neural network (SNN)-is utilized in bearing fault diagnosis. SNNs incorporate temporal concepts and utilize discrete spike sequences in communication, making them more biologically explanatory. Inspired by the classic CNN LeNet-5 framework, a bearing fault diagnosis method based on a convolutional SNN is proposed. In this method, the spiking convolutional network and the spiking classifier network are constructed by using the integrate-and-fire (IF) and leaky-integrate-and-fire (LIF) model, respectively, and end-to-end training is conducted on the overall model using a surrogate gradient method. The signals are adaptively encoded into spikes in the spiking neuron layer. In addition, the network utilizes max-pooling, which is consistent with the spatial-temporal characteristics of SNNs. Combined with the spiking convolutional layers, the network fully extracts the spatial-temporal features from the bearing vibration signals. Experimental validations and comparisons are conducted on bearings. The results show that the proposed method achieves high accuracy and takes fewer time steps.
C1 [Zhang, Changfan; Xiao, Zunguang] Hunan Univ Technol, Coll Railway Transportat, Zhuzhou 412007, Hunan, Peoples R China.
   [Sheng, Zhenwen] Shandong Xiehe Univ, Jinan 250109, Shandong, Peoples R China.
RP Sheng, ZW (corresponding author), Shandong Xiehe Univ, Jinan 250109, Shandong, Peoples R China.
EM shengzhenwen@sdxiehe.edu.cn
CR Bai RX, 2021, MEASUREMENT, V184, DOI 10.1016/j.measurement.2021.109885
   Cao Y, 2022, IEEE T INTELL TRANSP, V23, P12074, DOI 10.1109/TITS.2021.3109632
   Chen LT, 2020, J MANUF SYST, V54, P1, DOI 10.1016/j.jmsy.2019.11.008
   Cheng X, 2020, Arxiv, DOI arXiv:2010.03140
   Comsa IM, 2022, IEEE T NEUR NET LEAR, V33, P5939, DOI 10.1109/TNNLS.2021.3071976
   Cramer B, 2022, IEEE T NEUR NET LEAR, V33, P2744, DOI 10.1109/TNNLS.2020.3044364
   Deng L, 2020, NEURAL NETWORKS, V121, P294, DOI 10.1016/j.neunet.2019.09.005
   Fang W, 2021, INCORPORATING LEARNA, P2661
   Hao SJ, 2020, MEASUREMENT, V159, DOI 10.1016/j.measurement.2020.107802
   Hao YZ, 2020, NEURAL NETWORKS, V121, P387, DOI 10.1016/j.neunet.2019.09.007
   Iyer LR, 2020, IEEE IJCNN, DOI 10.1109/ijcnn48605.2020.9207474
   Kaiser J, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00424
   Kheradpisheh SR, 2020, INT J NEURAL SYST, V30, DOI 10.1142/S0129065720500276
   Liu YH, 2001, J COMPUT NEUROSCI, V10, P25, DOI 10.1023/A:1008916026143
   Lotfi Rezaabad A., 2020, INT C NEUR SYST, P1, DOI DOI 10.1145/3407197.3407211
   Maass W, 1997, NEURAL COMPUT, V9, P279, DOI 10.1162/neco.1997.9.2.279
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Magar R, 2021, IEEE ACCESS, V9, P25189, DOI 10.1109/ACCESS.2021.3056944
   Mao SA, 2013, COMPUT ELECTR ENG, V39, P863, DOI 10.1016/j.compeleceng.2013.03.004
   Mostafa H, 2018, IEEE T NEUR NET LEAR, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Mozafari M, 2019, PATTERN RECOGN, V94, P87, DOI 10.1016/j.patcog.2019.05.015
   Muramatsu N., 2021, ARXIV PREPRINT
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Niu GX, 2021, NEUROCOMPUTING, V445, P26, DOI 10.1016/j.neucom.2021.02.078
   Roy K, 2019, NATURE, V575, P607, DOI 10.1038/s41586-019-1677-2
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Shen J., 2020, 2020 INT JOINT C NEU, P1
   Sun YK, 2022, ACCIDENT ANAL PREV, V166, DOI 10.1016/j.aap.2021.106549
   Taherkhani A, 2020, NEURAL NETWORKS, V122, P253, DOI 10.1016/j.neunet.2019.09.036
   Wang B, 2020, NEUROCOMPUTING, V379, P117, DOI 10.1016/j.neucom.2019.10.064
   Wang TX, 2021, NEUROCOMPUTING, V425, P96, DOI 10.1016/j.neucom.2020.10.100
   Wang XW, 2020, NEURAL NETWORKS, V125, P258, DOI 10.1016/j.neunet.2020.02.011
   Wen L, 2018, IEEE T IND ELECTRON, V65, P5990, DOI 10.1109/TIE.2017.2774777
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Xing YN, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.590164
   Xu F, 2018, APPL SOFT COMPUT, V73, P898, DOI 10.1016/j.asoc.2018.09.037
   Xu Y, 2021, MEASUREMENT, V169, DOI 10.1016/j.measurement.2020.108502
   Yang ZB, 2020, APPL SOFT COMPUT, V97, DOI 10.1016/j.asoc.2020.106829
   Yao DC, 2020, MEASUREMENT, V159, DOI 10.1016/j.measurement.2020.107756
   Yin B, 2020, IEEE INTERNET THINGS, V7, P8748, DOI [10.1109/JIOT.2020.2996562, 10.1145/3407197.3407225]
   Zenke F, 2018, NEURAL COMPUT, V30, P1514, DOI 10.1162/neco_a_01086
   Zhang KY, 2020, J MANUF SYST, V55, P273, DOI 10.1016/j.jmsy.2020.04.016
   Zhang T., 2021, IEEE T NEUR NET LEAR
   Zhou JL, 2012, OPT ENG, V51, DOI 10.1117/1.OE.51.11.117201
   Zuo L, 2021, J MANUF SYST, V61, P714, DOI 10.1016/j.jmsy.2020.07.003
NR 46
TC 1
Z9 1
U1 4
U2 4
PD MAR 23
PY 2023
VL 5
IS 2
AR tdac050
DI 10.1093/tse/tdac050
UT WOS:000971527500001
DA 2023-11-16
ER

PT J
AU Wang, XQ
   Hou, ZG
   Zou, A
   Tan, M
   Cheng, L
AF Wang, Xiuqing
   Hou, Zeng-Guang
   Zou, Anmin
   Tan, Min
   Cheng, Long
TI A behavior controller based on spiking neural networks for mobile robots
SO NEUROCOMPUTING
DT Article; Proceedings Paper
CT 4th International Symposium on Neural Networks (ISNN 2007)
CY JUN 03-07, 2007
CL Nanjing, PEOPLES R CHINA
DE spiking neural networks; mobile robot; obstacle avoidance; Hebbian
   learning; ultrasonic data
ID FUZZY CONTROLLER
AB Spiking neural networks (SNNs), as the third generation of artificial neural networks, have unique advantages and are good candidates for robot controllers. A behavior controller based on a spiking neural network is designed for mobile robots to avoid obstacles using ultrasonic sensory signals. Detailed structure and implementation of the controller are discussed. In the controller the integrated-and-firing model is used and the SNN is trained by the Hebbian learning algorithm. Under the framework of SNNs, fewer neurons are employed in the controller than those of the classical neural networks (NNs). Experimental results show that the proposed controller is effective and is easy to implement. (c) 2007 Elsevier B.V. All rights reserved.
C1 [Wang, Xiuqing; Hou, Zeng-Guang; Zou, Anmin; Tan, Min; Cheng, Long] Chinese Acad Sci, Inst Automat, Key Lab Complex Syst & Intelligence Sci, Beijing 100080, Peoples R China.
   [Wang, Xiuqing] Hebei Normal Univ, Vocat & Tech Inst, Shijiazhuang 050031, Peoples R China.
RP Hou, ZG (corresponding author), Chinese Acad Sci, Inst Automat, Key Lab Complex Syst & Intelligence Sci, POB 2728, Beijing 100080, Peoples R China.
EM zengguang.hou@ia.ac.cn
CR ALAMDARI ARS, 2005, T ENG COMPUT, V6, P49
   Belatreche A, 2007, SOFT COMPUT, V11, P239, DOI [10.1007/s00500-006-0065-7, 10.1007/S00500-006-0065-7]
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   Bonarini A, 2003, FUZZY SET SYST, V134, P101, DOI 10.1016/S0165-0114(02)00232-4
   Burgsteiner H, 2005, LECT NOTES ARTIF INT, V3533, P121
   Di Paolo EA, 2002, ADAPT BEHAV, V10, P243, DOI 10.1177/1059712302010003006
   Floreano D, 2005, ARTIF LIFE, V11, P121, DOI 10.1162/1064546053278900
   Floreano D., 2001, LNCS, P38
   Floreano D, 2006, INT J INTELL SYST, V21, P1005, DOI 10.1002/int.20173
   Florian RV, 2006, LECT NOTES COMPUT SC, V4095, P570
   GERSTNER W, 1995, PHYS REV E, V51, P738, DOI 10.1103/PhysRevE.51.738
   GERSTNER W, 1993, BIOL CYBERN, V69, P503, DOI 10.1007/BF01185422
   GODJAVEC J, 2000, FUZZY LOGIC TECHNIQU, P97
   Gupta A, 2000, SCIENCE, V287, P273, DOI 10.1126/science.287.5451.273
   Hagras H, 2004, IEEE INT CONF ROBOT, P4620, DOI 10.1109/ROBOT.2004.1302446
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   Kasinski A., 2006, International Journal of Applied Mathematics and Computer Science, V16, P101
   Kempter R, 1999, PHYS REV E, V59, P4498, DOI 10.1103/PhysRevE.59.4498
   KUBOTA N, 2006, P 2006 IEEE WORLD C, P530
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Maass W., 1997, ADV NEURAL INFORM PR, V9
   Maass W., 1999, PULSED NEURAL NETWOR
   NADASDY Z, 1998, THESIS RUTGERS U
   Natschlager T, 1998, NETWORK-COMP NEURAL, V9, P319, DOI 10.1088/0954-898X/9/3/003
   NELSON AL, 2003, THESIS N CAROLINA ST
   Nolfi S., 1994, P 4 INT WORKSH ART L
   Roberts PD, 1999, J COMPUT NEUROSCI, V7, P235, DOI 10.1023/A:1008910918445
   RUF B, 1998, THESIS TU GRAZ AUSTR
   Rusu P, 2003, IEEE T INSTRUM MEAS, V52, P1335, DOI 10.1109/TIM.2003.816846
   SCHMITT BM, 1997, P 7 INT C ART NEUR N, P361
   SENN W, 1997, P 7 INT C ART NEUR N, P121
   SOULA H, 2005, SPRING S DEV ROBOTIC, P6
   Thongchai S, 2002, P AMER CONTR CONF, V1-6, P995, DOI 10.1109/ACC.2002.1023148
   Thongchai S, 2000, PROCEEDINGS OF THE 2000 IEEE INTERNATIONAL CONFERENCE ON CONTROL APPLICATIONS, P425, DOI 10.1109/CCA.2000.897461
   VREEKEN J, 2002, UUCS2003008 UTR U I
   Ye C, 2003, IEEE T SYST MAN CY B, V33, P17, DOI 10.1109/TSMCB.2003.808179
NR 37
TC 56
Z9 64
U1 1
U2 23
PD JAN
PY 2008
VL 71
IS 4-6
BP 655
EP 666
DI 10.1016/j.neucom.2007.08.025
UT WOS:000253663800025
DA 2023-11-16
ER

PT C
AU Narduzzi, S
   Bigdeli, SA
   Liu, SC
   Dunbar, LA
AF Narduzzi, Simon
   Bigdeli, Siavash A.
   Liu, Shih-Chii
   Dunbar, L. Andrea
GP IEEE
TI OPTIMIZING THE CONSUMPTION OF SPIKING NEURAL NETWORKS WITH ACTIVITY
   REGULARIZATION
SO 2022 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)
SE International Conference on Acoustics Speech and Signal Processing
   ICASSP
DT Proceedings Paper
CT 47th IEEE International Conference on Acoustics, Speech and Signal
   Processing (ICASSP)
CY MAY 22-27, 2022
CL Singapore, SINGAPORE
DE Sparsity; Regularization; Spiking Neural Networks; Deep Neural Networks
AB Reducing energy consumption is a critical point for neural network models running on edge devices. In this regard, reducing the number of multiply-accumulate (MAC) operations of Deep Neural Networks (DNNs) running on edge hardware accelerators will reduce the energy consumption during inference. Spiking Neural Networks (SNNs) are an example of bio-inspired techniques that can further save energy by using binary activations, and avoid consuming energy when not spiking. The networks can be configured for equivalent accuracy on a task through DNN-to-SNN conversion frameworks but their conversion is based on rate coding therefore the synaptic operations can be high. In this work, we look into different techniques to enforce sparsity on the neural network activation maps and compare the effect of different training regularizers on the efficiency of the optimized DNNs and SNNs.
C1 [Narduzzi, Simon; Bigdeli, Siavash A.; Dunbar, L. Andrea] CSEM, Neuchatel, Switzerland.
   [Narduzzi, Simon; Liu, Shih-Chii] Univ Zurich, Inst Neuroinformat, Zurich, Switzerland.
   [Narduzzi, Simon; Liu, Shih-Chii] Swiss Fed Inst Technol, Zurich, Switzerland.
RP Narduzzi, S (corresponding author), CSEM, Neuchatel, Switzerland.; Narduzzi, S (corresponding author), Univ Zurich, Inst Neuroinformat, Zurich, Switzerland.; Narduzzi, S (corresponding author), Swiss Fed Inst Technol, Zurich, Switzerland.
CR Georgiadis G, 2019, PROC CVPR IEEE, P7078, DOI 10.1109/CVPR.2019.00725
   Gou JP, 2021, INT J COMPUT VISION, V129, P1789, DOI 10.1007/s11263-021-01453-z
   Han S, 2016, CONF PROC INT SYMP C, P243, DOI 10.1109/ISCA.2016.30
   Hoefler Torsten, 2021, ARXIV
   Kim D, 2018, IEEE DES TEST, V35, P39, DOI 10.1109/MDAT.2017.2741463
   Kurtz Mark, 2020, P MACHINE LEARNING R
   Liang TL, 2021, Arxiv, DOI arXiv:2101.09671
   Liu Z, 2017, IEEE I CONF COMP VIS, P2755, DOI 10.1109/ICCV.2017.298
   Louizos Christos, 2017, ARXIV, DOI DOI 10.48550/ARXIV.1712.01312
   Mirsadeghi M, 2021, NEUROCOMPUTING, V427, P131, DOI 10.1016/j.neucom.2020.11.052
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Neil D, 2016, P 31 ANN ACM S APPL
   Parashar A, 2017, 44TH ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA 2017), P27, DOI 10.1145/3079856.3080254
   Pellegrini T, 2021, IEEE W SP LANG TECH, P97, DOI 10.1109/SLT48900.2021.9383587
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Sorbaro M, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00662
   Yang HR, 2020, Arxiv, DOI arXiv:1908.09979
   Yousefzadeh A, 2021, Arxiv, DOI arXiv:2107.07305
   Yousefzadeh A, 2019, IEEE J EM SEL TOP C, V9, P668, DOI 10.1109/JETCAS.2019.2951121
   Zhao JH, 2022, IEEE T NEUR NET LEAR, V33, P4096, DOI 10.1109/TNNLS.2021.3055825
NR 20
TC 1
Z9 1
U1 0
U2 3
PY 2022
BP 61
EP 65
DI 10.1109/ICASSP43922.2022.9746375
UT WOS:000864187900013
DA 2023-11-16
ER

PT J
AU Minkovich, K
   Thibeault, CM
   O'Brien, MJ
   Nogin, A
   Cho, Y
   Srinivasa, N
AF Minkovich, Kirill
   Thibeault, Corey M.
   O'Brien, Michael John
   Nogin, Aleksey
   Cho, Youngkwan
   Srinivasa, Narayan
TI HRLSim: A High Performance Spiking Neural Network Simulator for GPGPU
   Clusters
SO IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
DT Article
DE Distributed graphical processing units (GPUs) programming; general
   purpose GPU (GPGPU); large scale; neuron simulation; spike
   timing-dependent plasticity (STDP); spiking neural simulation
ID MODEL; CONNECTIVITY; PLASTICITY
AB Modeling of large-scale spiking neural models is an important tool in the quest to understand brain function and subsequently create real-world applications. This paper describes a spiking neural network simulator environment called HRL Spiking Simulator (HRLSim). This simulator is suitable for implementation on a cluster of general purpose graphical processing units (GPGPUs). Novel aspects of HRLSim are described and an analysis of its performance is provided for various configurations of the cluster. With the advent of inexpensive GPGPU cards and compute power, HRLSim offers an affordable and scalable tool for design, real-time simulation, and analysis of large-scale spiking neural networks.
C1 [Minkovich, Kirill; Thibeault, Corey M.; O'Brien, Michael John; Nogin, Aleksey; Cho, Youngkwan; Srinivasa, Narayan] HRL Labs LLC, Ctr Neural & Emergent Syst, Informat & Syst Sci Dept, Malibu, CA 90265 USA.
   [Thibeault, Corey M.] Univ Nevada, Dept Elect & Biomed Engn, Reno, NV 89557 USA.
   [O'Brien, Michael John] Univ Calif Los Angeles, Los Angeles, CA 90095 USA.
RP Minkovich, K (corresponding author), Mentor Graph Corp, Fremont, CA 94538 USA.
EM kirill.minkovich@gmail.com; cmthibeault@hrl.com; mjobrien@hrl.com;
   anogin@hrl.com; ykcho@hrl.com; nsrinivasa@hrl.com
CR Achard S, 2006, J NEUROSCI, V26, P63, DOI 10.1523/JNEUROSCI.3874-05.2006
   [Anonymous], BIOM SCI ENG C BSEC
   [Anonymous], 2011, FRONT NEUROINFORM
   [Anonymous], 1998, BOOK GENESIS EXPLORI, DOI DOI 10.1007/978-1-4612-1634-63
   [Anonymous], 2010, MVAPICH MPI INFINIBA
   [Anonymous], 2003, PYTHON BOOST DOCUMEN
   [Anonymous], 2010 INT JOINT C NEU
   Boahen KA, 2000, IEEE T CIRCUITS-II, V47, P416, DOI 10.1109/82.842110
   Bower JM, 2002, HDB BRAIN THEORY NEU, V475-478
   Brette R, 2007, J COMPUT NEUROSCI, V23, P349, DOI 10.1007/s10827-007-0038-6
   Burkitt AN, 2006, BIOL CYBERN, V95, P1, DOI 10.1007/s00422-006-0068-6
   Buzsaki G., 2006, RHYTHMS BRAIN, DOI 10.1093/acprof:oso/9780195301069.001.0001
   Davison Andrew P, 2008, Front Neuroinform, V2, P11, DOI 10.3389/neuro.11.011.2008
   de Camargo RY, 2011, CONCURR COMP-PRACT E, V23, P556, DOI 10.1002/cpe.1665
   Fan Z, 2004, P 2004 ACMIEEE C SUP, V2004, P47, DOI DOI 10.1109/SC.2004.26
   Fidjeland Andreas K, 2010, 2010 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2010.5596678
   Han B, 2010, APPL OPTICS, V49, pB83, DOI 10.1364/AO.49.000B83
   Hickey J, 2006, LECT NOTES COMPUT SC, V3922, P63
   Hines ML, 2008, J NEUROSCI METH, V169, P425, DOI 10.1016/j.jneumeth.2007.09.010
   Hines ML, 1997, NEURAL COMPUT, V9, P1179, DOI 10.1162/neco.1997.9.6.1179
   Igarashi J, 2011, NEURAL NETWORKS, V24, P950, DOI 10.1016/j.neunet.2011.06.008
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2007, DYNAMICAL SYSTEMS NE
   Khronos, 2009, OPENCL OP STAND PAR
   Kunkel S, 2012, FRONT NEUROINFORM, V5, DOI 10.3389/fninf.2011.00035
   Marc-Oliver G., 2007, SCHOLARPEDIA, DOI [DOI 10.4249/SCHOLARPEDIA.1430, 10.4249/scholarpedia.1430]
   Merolla PA, 2007, IEEE T CIRCUITS-I, V54, P301, DOI 10.1109/TCSI.2006.887474
   Michel O., 1998, Virtual Worlds. First International Conference, VW'98. Proceedings, P254
   Migliore M, 2006, J COMPUT NEUROSCI, V21, P119, DOI 10.1007/s10827-006-7949-5
   Minkovich K, 2012, IEEE T NEUR NET LEAR, V23, P889, DOI 10.1109/TNNLS.2012.2191795
   Morrison A, 2005, NEURAL COMPUT, V17, P1776, DOI 10.1162/0899766054026648
   Morrison A, 2007, NEURAL COMPUT, V19, P1437, DOI 10.1162/neco.2007.19.6.1437
   Morrison A, 2007, NEURAL COMPUT, V19, P47, DOI 10.1162/neco.2007.19.1.47
   Nageswaran JM, 2009, NEURAL NETWORKS, V22, P791, DOI 10.1016/j.neunet.2009.06.028
   Nere A., 2011, Proceedings of the 25th IEEE International Parallel & Distributed Processing Symposium (IPDPS 2011), P906, DOI 10.1109/IPDPS.2011.88
   O'Brien MJ, 2013, NEURAL COMPUT, V25, P123, DOI 10.1162/NECO_a_00387
   Pecevski Dejan, 2009, Front Neuroinform, V3, P11
   Plesser HE, 2007, LECT NOTES COMPUT SC, V4641, P672
   Pope A., 2008, FS0804 AAAI
   Richmond P, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0018539
   Schemmel J, 2006, IEEE IJCNN, P1
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Srinivasa N, 2012, IEEE T NEUR NET LEAR, V23, P1526, DOI 10.1109/TNNLS.2012.2207738
   Srinivasa N, 2012, IEEE PULSE, V3, P51, DOI 10.1109/MPUL.2011.2175639
   Thibeault C. M., 2011, P 3 BICOB MAR
   Thibeault CM, 2013, FRONT COMPUT NEUROSC, V7, DOI 10.3389/fncom.2013.00077
   Tiesel Jan-Phillip, 2009, Proceedings 2009 International Joint Conference on Neural Networks (IJCNN 2009 - Atlanta), P3118, DOI 10.1109/IJCNN.2009.5178688
   Wilson C., 2001, P INT PERSP LOW FERT, P1
   Wilson E. C., 2001, THESIS U NEVADA RENO
   Wilt BA, 2009, ANNU REV NEUROSCI, V32, P435, DOI 10.1146/annurev.neuro.051508.135540
NR 51
TC 40
Z9 44
U1 1
U2 14
PD FEB
PY 2014
VL 25
IS 2
BP 316
EP 331
DI 10.1109/TNNLS.2013.2276056
UT WOS:000330040800006
DA 2023-11-16
ER

PT S
AU Amin, HH
   Fujii, RH
AF Amin, HH
   Fujii, RH
BE Wang, L
   Chen, K
   Ong, YS
TI Learning algorithm for spiking neural networks
SO ADVANCES IN NATURAL COMPUTATION, PT 1, PROCEEDINGS
SE Lecture Notes in Computer Science
DT Article; Proceedings Paper
CT 1st International Conference on Natural Computation (ICNC 2005)
CY AUG 27-29, 2005
CL Changsha, PEOPLES R CHINA
ID NEURONS
AB Spiking Neural Networks (SNNs) use inter-spike time coding to process input data. In this paper, a new learning algorithm for SNNs that uses the inter-spike times within a spike train is introduced. The learning algorithm utilizes the spatio-temporal pattern produced by the spike train input mapping unit and adjusts synaptic weights during learning. The approach was applied to classification problems.
C1 Univ Aizu, Aizu Wakamatsu, Fukushima, Japan.
RP Amin, HH (corresponding author), Univ Aizu, Aizu Wakamatsu, Fukushima, Japan.
EM d8042201@u-aizu.ac.jp; fujii@u-aizu.ac.jp
CR AMIN HH, 2004, P 12 EUR S ART NEUR, P355
   [Anonymous], 2003, ADV NEURAL INFORM PR
   Bohte S, 2000, P EUR S ART NEUR NET, P419
   Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   Dayan P., 2001, THEORETICAL NEUROSCI
   Gerstner W., 2002, SPIKING NEURON MODEL
   Haykin S., 1999, NEURAL NETWORKS COMP
   HESHAM H, 2004, P 2004 INT JOINT C N, P477
   Hopfield IJ, 2000, P NATL ACAD SCI USA, V97, P13919, DOI 10.1073/pnas.250483697
   Hopfield JJ, 2001, P NATL ACAD SCI USA, V98, P1282, DOI 10.1073/pnas.031567098
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maass W., 1999, PULSED NEURAL NETWOR
   MAASS W, 2003, COMPUTATIONAL NEUROS, pCH18
   Ruf B, 1998, IEEE T NEURAL NETWOR, V9, P575, DOI 10.1109/72.668899
   RUF B, 1997, THESIS TU GRAZ AUSTR, pCH8
NR 15
TC 0
Z9 0
U1 0
U2 3
PY 2005
VL 3610
BP 456
EP 465
UT WOS:000232222400058
DA 2023-11-16
ER

PT C
AU Lu, C
   Han, HG
   Qiao, JF
   Yang, CL
AF Lu Chao
   Han Honggui
   Qiao Junfei
   Yang Cuili
BE Chen, J
   Zhao, Q
TI Design of a Self-organizing Recurrent RBF Neural Network Based on
   Spiking Mechanism
SO PROCEEDINGS OF THE 35TH CHINESE CONTROL CONFERENCE 2016
SE Chinese Control Conference
DT Proceedings Paper
CT 35th Chinese Control Conference (CCC)
CY JUL 27-29, 2016
CL Chengdu, PEOPLES R CHINA
DE Self-organizing; spiking mechanism; recurrent radial basis function;
   time-series prediction
ID SEQUENTIAL LEARNING ALGORITHM; FUNCTION APPROXIMATION; PREDICTION;
   OPTIMIZATION
AB Based on the systemic investigation of recurrent neural network, a self-organizing recurrent radial basis function (SR-RBF) neural network which based on the spiking mechanism and improved Levenberg-Marquardt (LM) algorithm is proposed in this paper. The hidden neuron in the recurrent radial basis function (RRBF) can be added or pruned by computing the spiking strength of the connections between hidden and output neurons of RRBF neural network. Meanwhile, to ensure the accuracy of SR-RBF neural network, the parameters are trained by improved LM algorithm. The SR-RBF neural network is used for approximating the time-series prediction and classical non-linear functions. Finally, comparisons with other methods demonstrate that the SR-RBF neural network is more effective in terms of accuracy, generalization, and network structure.
C1 [Lu Chao; Han Honggui; Qiao Junfei; Yang Cuili] Beijing Univ Technol, Coll Elect & Control Engn, Beijing 100124, Peoples R China.
   [Lu Chao; Han Honggui; Qiao Junfei; Yang Cuili] Beijing Key Lab Computat Intelligence & Intellige, Beijing 100124, Peoples R China.
RP Lu, C (corresponding author), Beijing Univ Technol, Coll Elect & Control Engn, Beijing 100124, Peoples R China.; Lu, C (corresponding author), Beijing Key Lab Computat Intelligence & Intellige, Beijing 100124, Peoples R China.
EM luchaobs@emails.bjut.edu.cn
CR Alexandridis A, 2013, IEEE T NEUR NET LEAR, V24, P219, DOI 10.1109/TNNLS.2012.2227794
   Bachtiar LR, 2013, NEURAL COMPUT, V25, P259, DOI 10.1162/NECO_a_00386
   Bortman M, 2009, IEEE T NEURAL NETWOR, V20, P1039, DOI 10.1109/TNN.2009.2019270
   Delgado M, 2005, PATTERN RECOGN, V38, P1444, DOI 10.1016/j.patcog.2004.03.026
   El-Sousy FFM, 2014, APPL SOFT COMPUT, V21, P509, DOI 10.1016/j.asoc.2014.02.027
   Fan JY, 2012, MATH COMPUT, V81, P447
   Han HG, 2016, APPL SOFT COMPUT, V38, P477, DOI 10.1016/j.asoc.2015.09.051
   Han HG, 2011, NEURAL NETWORKS, V24, P717, DOI 10.1016/j.neunet.2011.04.006
   Han HG, 2014, IEEE IJCNN, P3775, DOI 10.1109/IJCNN.2014.6889473
   Hsu CF, 2014, NEUROCOMPUTING, V136, P170, DOI 10.1016/j.neucom.2014.01.015
   Hsu CF, 2008, ENG APPL ARTIF INTEL, V21, P1153, DOI 10.1016/j.engappai.2007.12.003
   Huang GB, 2005, IEEE T NEURAL NETWOR, V16, P57, DOI 10.1109/TNN.2004.836241
   Huang GB, 2004, IEEE T SYST MAN CY B, V34, P2284, DOI 10.1109/TSMCB.2004.834428
   Leng G, 2006, IEEE T FUZZY SYST, V14, P755, DOI 10.1109/TFUZZ.2006.877361
   Li SY, 2006, NEUROCOMPUTING, V69, P523, DOI 10.1016/j.neucom.2005.01.008
   Liang NY, 2006, IEEE T NEURAL NETWOR, V17, P1411, DOI 10.1109/TNN.2006.880583
   Phan MC, 2013, IEEE T NEUR NET LEAR, V24, P1709, DOI 10.1109/TNNLS.2013.2258470
   Park DC, 2013, INFORM SCIENCES, V237, P18, DOI 10.1016/j.ins.2009.10.005
   Sheng C., 1991, IEEE T NEURAL NETWOR, V2, P302
   Song Q, 2008, IEEE T NEURAL NETWOR, V19, P1841, DOI 10.1109/TNN.2008.2001923
   Vukovic N, 2013, NEURAL NETWORKS, V46, P210, DOI 10.1016/j.neunet.2013.06.004
   Wang N, 2009, NEUROCOMPUTING, V72, P3818, DOI 10.1016/j.neucom.2009.05.006
   Yao JJ, 2012, AASRI PROC, V1, P183, DOI 10.1016/j.aasri.2012.06.029
   Zemouri R, 2003, ENG APPL ARTIF INTEL, V16, P453, DOI 10.1016/S0952-1976(03)00063-0
   Zheng PS, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1002848
   [No title captured]
NR 26
TC 4
Z9 4
U1 0
U2 2
PY 2016
BP 3624
EP 3629
UT WOS:000400282200020
DA 2023-11-16
ER

PT J
AU Ajayan, A
   James, AP
AF Ajayan, A.
   James, A. P.
TI Edge to quantum: hybrid quantum-spiking neural network image classifier
SO NEUROMORPHIC COMPUTING AND ENGINEERING
DT Article
DE quantum; edge; hybrid; spiking; neural; networks
AB The extreme parallelism property warrant convergence of neural networks with that of quantum computing. As the size of the network grows, the classical implementation of neural networks becomes computationally expensive and not feasible. In this paper, we propose a hybrid image classifier model using spiking neural networks (SNN) and quantum circuits that combines dynamic behaviour of SNN with the extreme parallelism offered by quantum computing. The proposed model outperforms models in comparison with spiking neural network in classical computing, and hybrid convolution neural network-quantum circuit models in terms of various performance parameters. The proposed hybrid SNN-QC model achieves an accuracy of 99.9% in comparison with CNN-QC model accuracy of 96.3%, and SNN model of accuracy 91.2% in MNIST classification task. The tests on KMNIST and CIFAR-1O also showed improvements.
C1 [Ajayan, A.; James, A. P.] Digital Univ Kerala, IIITMK, Trivandrum, Kerala, India.
RP James, AP (corresponding author), Digital Univ Kerala, IIITMK, Trivandrum, Kerala, India.
EM apj@ieee.org
CR Abel S, 2020, Arxiv, DOI arXiv:2003.07374
   Beer K, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-14454-2
   Bi GQ, 2002, PHYSIOL BEHAV, V77, P551, DOI 10.1016/S0031-9384(02)00933-2
   Brette R, 2007, J COMPUT NEUROSCI, V23, P349, DOI 10.1007/s10827-007-0038-6
   Ciliberto C, 2018, P ROY SOC A-MATH PHY, V474, DOI 10.1098/rspa.2017.0551
   de Lima Marquezino F, 2019, PRIMER QUANTUM COMPU, P57
   Du YX, 2020, Arxiv, DOI arXiv:2007.12369
   Endo S, 2020, Arxiv, DOI arXiv:2011.01382
   Guo YM, 2016, NEUROCOMPUTING, V187, P27, DOI 10.1016/j.neucom.2015.09.116
   Hallgren S., 2002, P 34 ANN ACM S THEOR, P653
   Hegade NN, 2021, Arxiv, DOI [arXiv:1712.07326, 10.48550/arXiv.1712.07326, DOI 10.48550/ARXIV.1712.07326]
   Lee C, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00119
   Liu JH, 2021, Arxiv, DOI arXiv:1911.02998
   Ma L, 2019, ISPRS J PHOTOGRAMM, V152, P166, DOI 10.1016/j.isprsjprs.2019.04.015
   Mittal S, 2020, NEURAL COMPUT APPL, V32, P1109, DOI 10.1007/s00521-018-3761-1
   Peng TY, 2020, PHYS REV LETT, V125, DOI 10.1103/PhysRevLett.125.150504
   Rawat W, 2017, NEURAL COMPUT, V29, P2352, DOI [10.1162/NECO_a_00990, 10.1162/neco_a_00990]
   Resch S, 2019, Arxiv, DOI arXiv:1905.07240
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Skatchkovsky N., 2020, ARXIV, DOI DOI 10.48550/ARXIV.2010.14217
   Taherkhani A, 2020, NEURAL NETWORKS, V122, P253, DOI 10.1016/j.neunet.2019.09.036
   Xia RX, 2020, Arxiv, DOI arXiv:1912.06184
NR 22
TC 5
Z9 5
U1 0
U2 0
PD DEC
PY 2021
VL 1
IS 2
AR 024001
DI 10.1088/2634-4386/ac1cec
UT WOS:001062922400001
DA 2023-11-16
ER

PT C
AU Çelik, G
   Talu, MF
AF Celik, Gaffari
   Talu, M. Fatih
GP IEEE
TI Spiking Neural Network Applications
SO 2017 INTERNATIONAL ARTIFICIAL INTELLIGENCE AND DATA PROCESSING SYMPOSIUM
   (IDAP)
DT Proceedings Paper
CT 2017 International Artificial Intelligence and Data Processing Symposium
   (IDAP)
CY SEP 16-17, 2017
CL Malatya, TURKEY
DE Artificial Neural Networks; Spiking Neural Network; Delay Time;
   Synapses; BackPropagation; Population Coding; Gaussian Receptive Fields
ID GRADIENT DESCENT; NEURONS
AB Spiking Neural Network (SNN) are 3rd Generation Artificial Neural Networks (ANN) models. The fact that time information is processed in the form of spikes and there are multiple synapses between cells (neurons) are the most important features that distinguish SNN from previous generations. In this study, artificial learning systems which can learn by using basic logical operators such as AND, OR, XOR have been developed in order to understand SNN structure. In SNN, we tried to find optimal values for these parameters by examining the effect of the number of connections between cells and delays between connections to learning success.
C1 [Celik, Gaffari] Ibrahim Cecen Univ Agri, Meslek Yuksekokulu, Bilgisayar Programciligi, Agri, Turkey.
   [Talu, M. Fatih] Inonu Univ, Muhendisl Fak, Bilgisayar Muhendisligi, Malatya, Turkey.
RP Çelik, G (corresponding author), Ibrahim Cecen Univ Agri, Meslek Yuksekokulu, Bilgisayar Programciligi, Agri, Turkey.
EM gcelik@agri.edu.tr; fatih.talu@inonu.edu.tr
CR [Anonymous], 2011, YAPAY ZEKA UYGULAMAL
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Booij O, 2005, INFORM PROCESS LETT, V95, P552, DOI 10.1016/j.ipl.2005.05.023
   Boudjleal M., 2013, ARTIF INTELL, P525
   Dora S, 2016, NEUROCOMPUTING, V171, P1216, DOI 10.1016/j.neucom.2015.07.086
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Kabalci E., 2017, YAPAY SINIR AGLARI
   Nuno-Maganda M, 2009, 2009 5TH SOUTHERN CONFERENCE ON PROGRAMMABLE LOGIC, PROCEEDINGS, P83, DOI 10.1109/SPL.2009.4914919
   Oniz Y., 2014, DEV NEW LEARNING ALG
   Ponulak F, 2011, ACTA NEUROBIOL EXP, V71, P409
   Ruf B., 1998, COMPUTING LEARNING S
   Silva M, 2017, J VOICE, V31, P24, DOI 10.1016/j.jvoice.2016.02.019
   Sinanoglu G., 2012, CLASSIFICATION USING
   Sun QY, 2017, NEUROCOMPUTING, V228, P119, DOI 10.1016/j.neucom.2016.09.093
   Xu Y, 2013, NEURAL NETWORKS, V43, P99, DOI 10.1016/j.neunet.2013.02.003
   Zhang ZM, 2015, NEUROCOMPUTING, V151, P985, DOI 10.1016/j.neucom.2014.03.086
NR 16
TC 0
Z9 0
U1 0
U2 1
PY 2017
UT WOS:000426868700106
DA 2023-11-16
ER

PT C
AU Park, S
   Kim, S
   Choe, H
   Yoon, S
AF Park, Seongsik
   Kim, Seijoon
   Choe, Hyeokjun
   Yoon, Sungroh
GP ACM
TI Fast and Efficient Information Transmission with Burst Spikes in Deep
   Spiking Neural Networks
SO PROCEEDINGS OF THE 2019 56TH ACM/EDAC/IEEE DESIGN AUTOMATION CONFERENCE
   (DAC)
DT Proceedings Paper
CT 56th ACM/EDAC/IEEE Design Automation Conference (DAC)
CY JUN 02-06, 2019
CL Las Vegas, NV
ID COMMUNICATION; UNIT
AB Spiking neural networks (SNNs) are considered as one of the most promising artificial neural networks due to their energy-efficient computing capability. Recently, conversion of a trained deep neural network to an SNN has improved the accuracy of deep SNNs. However, most of the previous studies have not achieved satisfactory results in terms of inference speed and energy efficiency. In this paper, we propose a fast and energy-efficient information transmission method with burst spikes and hybrid neural coding scheme in deep SNNs. Our experimental results showed the proposed methods can improve inference energy efficiency and shorten the latency.
C1 [Yoon, Sungroh] Seoul Natl Univ, Dept Elect & Comp Engn, ASRI, INMC, Seoul 08826, South Korea.
   Seoul Natl Univ, Inst Engn Res, Seoul 08826, South Korea.
RP Yoon, S (corresponding author), Seoul Natl Univ, Dept Elect & Comp Engn, ASRI, INMC, Seoul 08826, South Korea.
EM sryoon@snu.ac.kr
CR Adrian ED, 1926, J PHYSIOL-LONDON, V61, P49, DOI 10.1113/jphysiol.1926.sp002273
   [Anonymous], 2016, ICLR
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   CONNORS BW, 1990, TRENDS NEUROSCI, V13, P99, DOI 10.1016/0166-2236(90)90185-D
   Delorme A, 2001, NEURAL NETWORKS, V14, P795, DOI 10.1016/S0893-6080(01)00049-1
   Diehl P.U., 2015, 2015 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2015.7280696
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   Gautrais J, 1998, BIOSYSTEMS, V48, P57, DOI 10.1016/S0303-2647(98)00050-1
   Han S, 2016, CONF PROC INT SYMP C, P243, DOI 10.1109/ISCA.2016.30
   Izhikevich EM, 2003, TRENDS NEUROSCI, V26, P161, DOI 10.1016/S0166-2236(03)00034-1
   Kayser C, 2009, NEURON, V61, P597, DOI 10.1016/j.neuron.2009.01.008
   Kim J., 2018, NEUROCOMPUTING
   Krahe R, 2004, NAT REV NEUROSCI, V5, P13, DOI 10.1038/nrn1296
   Lee JM, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00191
   Lisman JE, 1997, TRENDS NEUROSCI, V20, P38, DOI 10.1016/S0166-2236(96)10070-9
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Mochizuki Y, 2016, J NEUROSCI, V36, P5736, DOI 10.1523/JNEUROSCI.0230-16.2016
   Moradi S., 2018, J PHYS D, V52
   PARK S, 2018, AAAI, P3909, DOI DOI 10.1145/3197231.3197253
   Park S., 2018, ARXIV180507978
   Park S., 2016, NIPS WORKSH COMP SPI
   Rueckauer B., 2016, ARXIV161204052
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   THORPE SJ, 1990, PARALLEL PROCESSING IN NEURAL SYSTEMS AND COMPUTERS, P91
NR 26
TC 29
Z9 29
U1 0
U2 4
PY 2019
DI 10.1145/3316781.3317822
UT WOS:000482058200053
DA 2023-11-16
ER

PT C
AU Sichtig, H
AF Sichtig, Heike
GP IEEE
TI Building smart machines by utilizing spiking neural networks
SO 2007 IEEE SYMPOSIUM ON COMPUTATIONAL INTELLIGENCE IN BIOINFORMATICS AND
   COMPUTATIONAL BIOLOGY
DT Proceedings Paper
CT IEEE Symposium on Computational Intelligence in Bioinformatics and
   Computational Biology
CY APR 01-05, 2007
CL Honolulu, HI
ID SYNAPSES
AB In this paper we survey the current state of the art in spiking neural networks research and outline our approach to building smart machines. A thorough understanding of the history, open questions, and limitations of these networks can help the research community to gain a better grip on this new technology and to bridge the missing gaps. It is necessary to look at various aspects of spiking neural networks, such as the different modeling approaches, encoding schemes, simulators and learning techniques in order to efficiently make use of these networks. One paramount characteristic of spiking neural networks is the precise timing of inputs and outputs. As a dynamic system itself, it naturally lends itself to solving problems in the continuous domain such as time series analysis. This will be the focal point of our efforts to develop a smart machine utilizing spiking neural networks.
C1 SUNY Binghamton, Dept Bioengn, Binghamton, NY 13902 USA.
RP Sichtig, H (corresponding author), SUNY Binghamton, Dept Bioengn, Binghamton, NY 13902 USA.
EM hsichtig@binghamton.edu
CR [Anonymous], 1997, HOW MIND WORKS
   Bohte SM, 2005, INFORM PROCESS LETT, V95, P519, DOI 10.1016/j.ipl.2005.05.018
   DREWES R, 2005, LINUX J
   FODOR JA, 1983, MODULARITY MIND ESSA, V60, P976
   GERSTNER W, 2002, BIOL CYBERNETICS HEB
   Gerstner W., 2002, SPIKING NEURON MODEL
   GROOVER M, 2002, AMYGDALA SPIKING NEU
   GROUP CPR, 1986, PARALLEL DISTRIBUTED, V1
   Hawkins J, 2004, INTELLIGENCE
   Hopfield IJ, 2000, P NATL ACAD SCI USA, V97, P13919, DOI 10.1073/pnas.250483697
   Hopfield JJ, 2001, P NATL ACAD SCI USA, V98, P1282, DOI 10.1073/pnas.031567098
   IZHIKEVICH E, 2007, IEEE T NEURAL NETWOR
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Jin DZ, 2004, PHYS REV E, V69, DOI 10.1103/PhysRevE.69.021905
   MAAS W, 2000, PULSED NEURAL NETWOR
   Maass W, 2002, NEURAL NETWORKS, V15, P155, DOI 10.1016/S0893-6080(01)00144-7
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   MARIAN I, 2001, P 12 DAAAM INT S INT
   Markram H, 1998, P NATL ACAD SCI USA, V95, P5323, DOI 10.1073/pnas.95.9.5323
   MOUNTCASTLE V, 1978, ORGANIZING PRINCIPLE, P117
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   SOHN J, 1999, P INT JOINT C NEUR N, V4
   Storck J, 2001, NEURAL NETWORKS, V14, P275, DOI 10.1016/S0893-6080(00)00101-5
   Uttal W.R., 2001, NEW PHRENOLOGY LIMIT
   WALKER M, 1997, 15 IMACS WORLD C SCI
   WATTS L, 1994, ADV NEURAL INFORM PR, V6, P27934
   WATTS L, 2007, SPIKE NEURALOG
   [No title captured]
NR 28
TC 1
Z9 1
U1 0
U2 0
PY 2007
BP 346
EP 350
UT WOS:000248516200047
DA 2023-11-16
ER

PT J
AU Liu, YC
   Qian, K
   Hu, SG
   An, K
   Xu, S
   Zhan, XT
   Wang, JJ
   Guo, R
   Wu, YC
   Chen, TP
   Yu, Q
   Liu, Y
AF Liu, Yanchen
   Qian, Kun
   Hu, Shaogang
   An, Kun
   Xu, Sheng
   Zhan, Xitong
   Wang, J. J.
   Guo, Rui
   Wu, Yuancong
   Chen, Tu-Pei
   Yu, Qi
   Liu, Yang
TI Application of Deep Compression Technique in Spiking Neural Network Chip
SO IEEE TRANSACTIONS ON BIOMEDICAL CIRCUITS AND SYSTEMS
DT Article
DE Deep compression; network-on-chip; neuron; spiking neural network;
   synapse
ID ON-CHIP; DESIGN; SYSTEM
AB In this paper, a reconfigurable and scalable spiking neural network processor, containing 192 neurons and 6144 synapses, is developed. By using deep compression technique in spiking neural network chip, the amount of physical synapses can be reduced to 1/16 of that needed in the original network, while the accuracy is maintained. This compression technique can greatly reduce the number of SRAMs inside the chip as well as the power consumption of the chip. This design achieves throughput per unit area of 1.1 GSOP/(s. mm(2)) at 1.2 V, and energy consumed per SOP of 35 pJ. A 2-layer fully-connected spiking neural network is mapped to the chip, and thus the chip is able to realize handwritten digit recognition on MNIST with an accuracy of 91.2%.
C1 [Liu, Yanchen; Qian, Kun; Hu, Shaogang; Zhan, Xitong; Wang, J. J.; Guo, Rui; Wu, Yuancong] Univ Elect Sci & Technol China, Chengdu 610054, Peoples R China.
   [An, Kun] Omnivis Thchnol Inc, Shanghai 201210, Peoples R China.
   [Xu, Sheng] Cambricon Informat Technol Co, Shanghai, Peoples R China.
   [Chen, Tu-Pei] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
   [Yu, Qi; Liu, Yang] Univ Elect Sci & Technol China, State Key Lab Elect Thin Films & Intergrated, Chengdu 610054, Peoples R China.
RP Liu, YC (corresponding author), Univ Elect Sci & Technol China, Chengdu 610054, Peoples R China.
EM liuyanchen256@gmail.com; qiankun96214@outlook.com; sghu@uestc.edu.cn;
   kingann93@163.com; xusheng193@gmail.com; zhanxitong@foxmail.com;
   pyzxwjj@gmail.com; guorui_za@hotmail.com; wycong0416@gmail.com;
   echentp@ntu.edu.sg; qiyu@uestc.edu.cn; yliu1975@uestc.edu.cn
CR Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   [Anonymous], 2016, INT C LEARNING REPRE
   [Anonymous], 2013, ARXIV13124461
   [Anonymous], 2014, THESIS
   [Anonymous], 2016, ARXIV160704381
   Anwar S, 2017, ACM J EMERG TECH COM, V13, DOI 10.1145/3005348
   Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Bobda C., 2005, Proceedings. 2005 International Conference on Field Programmable Logic and Applications (IEEE Cat. No.05EX1155), P153
   Carrillo S., 2012, 2012 Sixth IEEE/ACM International Symposium on Networks-on-Chip (NoCS), P83, DOI 10.1109/NOCS.2012.17
   Carrillo S, 2013, IEEE T PARALL DISTR, V24, P2451, DOI 10.1109/TPDS.2012.289
   Cassidy AS, 2013, NEURAL NETWORKS, V45, P4, DOI 10.1016/j.neunet.2013.05.011
   Chen GK, 2019, IEEE J SOLID-ST CIRC, V54, P992, DOI 10.1109/JSSC.2018.2884901
   Choudhary Swadesh, 2012, Artificial Neural Networks and Machine Learning - ICANN 2012. Proceedings of the 22nd International Conference on Artificial Neural Networks, P121, DOI 10.1007/978-3-642-33269-2_16
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   de Garis H, 2010, NEUROCOMPUTING, V74, P3, DOI 10.1016/j.neucom.2010.08.004
   DENNARD RH, 1974, IEEE J SOLID-ST CIRC, VSC 9, P256, DOI 10.1109/JSSC.1974.1050511
   Eliasmith C, 2014, CURR OPIN NEUROBIOL, V25, P1, DOI 10.1016/j.conb.2013.09.009
   Frenkel C, 2019, IEEE T BIOMED CIRC S, V13, P999, DOI 10.1109/TBCAS.2019.2928793
   Frenkel C, 2019, IEEE T BIOMED CIRC S, V13, P145, DOI 10.1109/TBCAS.2018.2880425
   Han SY, 2016, IEEE ICC, DOI 10.1109/ICC.2016.7511104
   Han S, 2017, FPGA'17: PROCEEDINGS OF THE 2017 ACM/SIGDA INTERNATIONAL SYMPOSIUM ON FIELD-PROGRAMMABLE GATE ARRAYS, P75, DOI 10.1145/3020078.3021745
   Hassibi Babak, 1992, ADV NEURAL INFORM PR, V5, P2, DOI DOI 10.5555/645753.668069
   Huang HT, 2019, IEEE T NEUR NET LEAR, V30, P1497, DOI 10.1109/TNNLS.2018.2869974
   Hubara I., 2016, ADV NEURAL INFORM PR, P4107
   Hubara I, 2018, J MACH LEARN RES, V18
   Kong DY, 2018, IEEE ACCESS, V6, P68773, DOI 10.1109/ACCESS.2018.2880033
   LeCun Y., 1998, MNIST DATABASE HANDW
   Li MQ, 2018, IEEE T COMPUT AID D, V37, P941, DOI 10.1109/TCAD.2017.2740306
   Liu GY, 2018, APPL PHYS LETT, V113, DOI 10.1063/1.5040413
   Majer M., 2005, Proceedings. 19th IEEE International Parallel and Distributed Processing Symposium
   Mayr C, 2016, IEEE T BIOMED CIRC S, V10, P243, DOI 10.1109/TBCAS.2014.2379294
   MEAD C, 1990, P IEEE, V78, P1629, DOI 10.1109/5.58356
   Merolla P, 2014, IEEE T CIRCUITS-I, V61, P820, DOI 10.1109/TCSI.2013.2284184
   Molchanov Pavlo, 2016, ARXIV161106440
   Noack M, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00010
   Novikov A, 2015, ADV NEUR IN, V28
   Painkras E, 2013, IEEE J SOLID-ST CIRC, V48, P1943, DOI 10.1109/JSSC.2013.2259038
   Qiao N, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00141
   Rathi N, 2019, IEEE T COMPUT AID D, V38, P668, DOI 10.1109/TCAD.2018.2819366
   Sarikaya D, 2017, IEEE T MED IMAGING, V36, P1542, DOI 10.1109/TMI.2017.2665671
   Schemmel J, 2010, IEEE INT SYMP CIRC S, P1947, DOI 10.1109/ISCAS.2010.5536970
   Shafique M, 2017, IEEE DES TEST, V34, P8, DOI 10.1109/MDAT.2016.2633408
   Taylor MB, 2013, IEEE MICRO, V33, P8, DOI 10.1109/MM.2013.90
   Taylor MB, 2012, DES AUT CON, P1131
   Wang JJ, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-30768-0
   Yang CG, 2019, IEEE T NEUR NET LEAR, V30, P777, DOI 10.1109/TNNLS.2018.2852711
NR 46
TC 7
Z9 7
U1 4
U2 32
PD APR
PY 2020
VL 14
IS 2
BP 274
EP 282
DI 10.1109/TBCAS.2019.2952714
UT WOS:000522957500012
DA 2023-11-16
ER

PT S
AU Hulea, M
AF Hulea, Mircea
BE Espinosa, HEP
TI Bioinspired Control Method Based on Spiking Neural Networks and SMA
   Actuator Wires for LASER Spot Tracking
SO NATURE-INSPIRED COMPUTING FOR CONTROL SYSTEMS
SE Studies in Systems Decision and Control
DT Article; Book Chapter
DE Laser spot tracking; Spiking neural networks; Analogue bio-inspired
   neuron; Shape memory alloy actuator wires; Photosensitive panel
ID NEURONS; MODEL
AB This chapter presents a new biologically inspired technique for automatically compensating the light spot deviation from the normal position for laser spot trackers. The method is based on hardware implementation of the spiking neural networks which provides fast response due to real time operation and ability to learn unsupervised when they are stimulated by concurrent events. For increasing the biological plausibility of the method, the spiking neural network controls the contraction of shape memory alloy (SMA) actuator wires that operates as the muscular fibres. These SMA wires are the most suitable actuators for being controlled by the electronic spiking neurons because the contraction force increases naturally with the spiking frequency. From our knowledge the laser spot tracking using spiking neural networks was not performed previously. Moreover, other original ideas represent the use of analogue implementation of the spiking neural networks for real time operation as well as the SMA actuator wires for more biological plausibility. To validate this method we implemented in hardware a spiking neural network structure that processes the input from a one dimensional photodiode array and controls a positioning system based on SMA actuator wires. The results show that the spiking neural network is able to detect the one-dimensional spot motion and to adapt the response time by Hebbian learning mechanisms to the spot wandering amplitude. Moreover, by driving two antagonistic SMA actuator wires the system is able to track the laser spot with low response time and acceptable precision. These results are encouraging to develop bio-inspired low power spot tracking system for enhancing the receiving accuracy in free space optical communications or for enhancing the efficacy of the photovoltaic systems. Moreover, the light tracking principle based on spiking neural networks and SMA wires can be successfully used in implementation of the light tracking mechanism of an artificial eye.
C1 [Hulea, Mircea] Gheorghe Asachi Tech Univ Iasi, Dept Comp Sci & Engn, Fac Automat Control & Comp Engn, Iasi, Romania.
RP Hulea, M (corresponding author), Gheorghe Asachi Tech Univ Iasi, Dept Comp Sci & Engn, Fac Automat Control & Comp Engn, Iasi, Romania.
EM mhulea@tuiasi.ro
CR Andrianesis K, 2015, J INTELL ROBOT SYST, V78, P257, DOI 10.1007/s10846-014-0061-6
   [Anonymous], 2009, AFRICON 2009 AFRICON
   [Anonymous], TECHN CHAR FLEX ACT
   [Anonymous], 1998, PULSED NEURAL NETWOR
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Dan Y, 2004, NEURON, V44, P23, DOI 10.1016/j.neuron.2004.09.007
   Dönmez B, 2010, TURK J ELECTR ENG CO, V18, P899, DOI 10.3906/elk-0906-14
   Duh FB, 2004, IEEE T SYST MAN CY B, V34, P16, DOI 10.1109/TSMCB.2003.810953
   Haibin S., 2010, MAXIMUM POWER POINT, P353
   Hines ML, 2008, J COMPUT NEUROSCI, V25, P439, DOI 10.1007/s10827-008-0087-5
   Hines ML, 2008, J COMPUT NEUROSCI, V25, P203, DOI 10.1007/s10827-007-0073-3
   Hopfield JJ, 2001, P NATL ACAD SCI USA, V98, P1282, DOI 10.1073/pnas.031567098
   Hulea M., 2010, MEMOIRS SCI, VXXXIII, P129
   Hulea M., 2011, MATH MODEL BIOL INSP, P282
   Hulea M., 2008, CEAI J, V10, P32
   Hulea M, 2014, INT CONF SYST THEO, P163, DOI 10.1109/ICSTCC.2014.6982409
   Hulea M, 2012, EUR SIGNAL PR CONF, P1708
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Jacob V, 2007, J NEUROSCI, V27, P1271, DOI 10.1523/JNEUROSCI.4264-06.2007
   Jimenez-Fernandez A, 2012, SENSORS-BASEL, V12, P3831, DOI 10.3390/s120403831
   Jolivet R, 2004, J NEUROPHYSIOL, V92, P959, DOI 10.1152/jn.00190.2004
   Joshi P, 2005, NEURAL COMPUT, V17, P1715, DOI 10.1162/0899766054026684
   Kistler V. M., 2002, SPIKING NEURONS MODE
   Kobayashi R, 2009, FRONT COMPUT NEUROSC, V3, DOI 10.3389/neuro.10.009.2009
   Lovelace JJ, 2008, NEURAL COMPUT, V20, P65, DOI 10.1162/neco.2008.20.1.65
   Maass W, 2004, MATH COMP BIOL SER, P575
   OReylli R., 2000, COMPUTATIONAL EXPLOR
   Rast A., 2009, UNIVERSAL ABSTRACT T, P2611
   Swiercz W, 2006, IEEE T NEURAL NETWOR, V17, P94, DOI 10.1109/TNN.2005.860834
   Teh Y, 2008, THESIS
   Wong Y., 1998, DATA FUSION TRACKING, P1024
NR 32
TC 7
Z9 7
U1 0
U2 1
PY 2016
VL 40
BP 13
EP 38
DI 10.1007/978-3-319-26230-7_2
D2 10.1007/978-3-319-26230-7
UT WOS:000384741500003
DA 2023-11-16
ER

PT J
AU Aoki, T
   Aoyagi, T
AF Aoki, Takaaki
   Aoyagi, Toshio
TI Synchrony-Induced Attractor Transition in Cortical Neural Networks
   Organized by Spike-Timing Dependent Plasticity
SO JOURNAL OF ROBOTICS AND MECHATRONICS
DT Article
DE spike-timing dependent plasticity; neural synchrony; neural network;
   cortical neuron model
AB Recent studies have shown that synchronous neural activity in the cortex area occurs related to behavior or recognition of animals, which suggests that such neural activity involves in information processing. Functions enabled by synchronous firing, however, are still unknown. Results reporting that a transition between recall states of associative memory is induced by external synchronous spikes in a neural network formed by spike-timing dependent plasticity indicate the possibility of a function of synchronous neural activity as a transition signal, requiring further examination using detailed cortical neuron models [1]. We introduced a mathematical model of pyramidal and fast-spiking cortical neurons based on Hodgkin-Huxley, and confirmed the transition between recall states through synchronous spike inputs in detailed neuron models.
C1 [Aoki, Takaaki; Aoyagi, Toshio] Japan Sci & Technol Corp, CREST, Kyoto 6068501, Japan.
   [Aoyagi, Toshio] Kyoto Univ, Grad Sch Informat, Dept Appl Anal & Complex Dynam Syst, Kyoto 6068501, Japan.
RP Aoki, T (corresponding author), Japan Sci & Technol Corp, CREST, Kyoto 6068501, Japan.
EM aoki@acs.i.kyoto-u.ac.jp; aoyagi@acs.i.kyoto-u.ac.jp
CR Aoyagi T, 2003, NEURAL COMPUT, V15, P1035, DOI 10.1162/089976603765202659
   Aoyagi T., 2007, NEURAL COMPUT, V19
   Averbeck BB, 2004, TRENDS NEUROSCI, V27, P225, DOI 10.1016/j.tins.2004.02.006
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Brody CD, 2003, NEURON, V37, P843, DOI 10.1016/S0896-6273(03)00120-X
   Debanne D, 1998, J PHYSIOL-LONDON, V507, P237, DOI 10.1111/j.1469-7793.1998.237bu.x
   Diesmann M, 1999, NATURE, V402, P529, DOI 10.1038/990101
   Engel AK, 2001, NAT REV NEUROSCI, V2, P704, DOI 10.1038/35094565
   Ermentrout GB, 2001, NEURON, V29, P33, DOI 10.1016/S0896-6273(01)00178-7
   Flannery, 2002, NUMERICAL RECIPES C
   Fries P, 2001, SCIENCE, V291, P1560, DOI 10.1126/science.1055465
   GRAY CM, 1989, NATURE, V338, P334, DOI 10.1038/338334a0
   Hebb D.O., 1949, ORG BEHAV NEUROPSYCH
   HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554
   Lee D, 2003, J NEUROSCI, V23, P6798
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   Mushiake H, 2006, NEURON, V50, P631, DOI 10.1016/j.neuron.2006.03.045
   Nomura M, 2003, NEURAL COMPUT, V15, P2179, DOI 10.1162/089976603322297340
   Riehle A, 1997, SCIENCE, V278, P1950, DOI 10.1126/science.278.5345.1950
   Rubin J, 2001, PHYS REV LETT, V86, P364, DOI 10.1103/PhysRevLett.86.364
   Sakamoto K., 2005, TECHNICAL REPORT IEI, V8, P105
   Salinas E, 2001, NAT REV NEUROSCI, V2, P539, DOI 10.1038/35086012
   Shadlen MN, 1999, NEURON, V24, P67, DOI 10.1016/S0896-6273(00)80822-3
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Yoshimura Y, 2005, NATURE, V433, P868, DOI 10.1038/nature03252
   Yoshimura Y, 2005, NAT NEUROSCI, V8, P1552, DOI 10.1038/nn1565
   Zhang LI, 1998, NATURE, V395, P37, DOI 10.1038/25665
NR 27
TC 0
Z9 0
U1 0
U2 0
PD AUG
PY 2007
VL 19
IS 4
SI SI
BP 409
EP 415
DI 10.20965/jrm.2007.p0409
UT WOS:000448872400009
DA 2023-11-16
ER

PT C
AU Dorogyy, Y
   Kolisnichenko, V
AF Dorogyy, Yaroslav
   Kolisnichenko, Vadym
GP IEEE
TI Unsupervised Pre-Training with Spiking Neural Networks in
   Semi-Supervised Learning
SO 2018 IEEE FIRST INTERNATIONAL CONFERENCE ON SYSTEM ANALYSIS &
   INTELLIGENT COMPUTING (SAIC)
DT Proceedings Paper
CT 1st IEEE International Conference on System Analysis & Intelligent
   Computing (SAIC)
CY OCT 08-12, 2018
CL Kyiv, UKRAINE
DE semi-supervised learning; unsupervised pretraining; spiking neural
   networks
AB Semi-supervised learning appeared when people understood that ignoring the possibility of getting benefit from unlabeled data during supervised learning with little labeled data is not wise. It is difficult to overestimate the importance of this as much more unlabeled data exists than labeled and much easier it can be collected. Many semi-supervised learning methods were developed over the past decades. A new approach is presented in this paper, which is based on using spiking neural networks in the pre-training phase. Spiking neural networks are biologically plausible neural networks, which try to simulate the behavior of neurons and processes which occur in biological neural networks. Most of the learning rules used in spiking neural networks are unsupervised as unsupervised learning is thought to be a major drive for developmental plasticity in the brain. It is considered that it is hard for the brain to do supervised learning, things like doing math or classification. In a nutshell, the proposed method is a combination of the advantages of both spiking neuron networks (unsupervised learning) and classical artificial neural networks (supervised learning). We showed that such approach may increase the accuracy of the classifier when a small amount of labeled data is given.
C1 [Dorogyy, Yaroslav; Kolisnichenko, Vadym] Natl Tech Univ Ukraine, Igor Sikorsky Kyiv Polytech Inst, Dept Automat & Control Tech Syst, Kiev, Ukraine.
RP Dorogyy, Y (corresponding author), Natl Tech Univ Ukraine, Igor Sikorsky Kyiv Polytech Inst, Dept Automat & Control Tech Syst, Kiev, Ukraine.
EM cisco.rna@gmail.com; vadym.kolisnichenko@gmail.com
CR Bekolay T., 2011, THESIS
   Bengio Yoshua, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P437, DOI 10.1007/978-3-642-35289-8_26
   Bray LCJ, 2013, FRONT NEUROROBOTICS, V7, DOI 10.3389/fnbot.2013.00008
   Davies S. E., 2013, THESIS
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Dorogyy Y, 2016, 2016 13TH INTERNATIONAL CONFERENCE ON MODERN PROBLEMS OF RADIO ENGINEERING, TELECOMMUNICATIONS AND COMPUTER SCIENCE (TCSET), P124, DOI 10.1109/TCSET.2016.7451989
   Erhan D, 2010, J MACH LEARN RES, V11, P625
   Rasmus A., 2015, P 28 INT C NEUR INF, V28, P3546
   Sang N., 2010, EMBEDDED VISUAL SYST, P103
   Weston J., 2008, P 25 INT C MACHINE L, P1168, DOI DOI 10.1145/1390156.1390303
   Zeng Y., 2017, NEURAL EVOLUTIONARY
NR 11
TC 0
Z9 0
U1 0
U2 0
PY 2018
BP 177
EP 180
UT WOS:000454679400035
DA 2023-11-16
ER

PT J
AU Han, CS
   Lee, KM
AF Han, Chan Sik
   Lee, Keon Myung
TI A Survey on Spiking Neural Networks
SO INTERNATIONAL JOURNAL OF FUZZY LOGIC AND INTELLIGENT SYSTEMS
DT Article
DE Spiking neural network; Deep learning; Neural network; Machine learning;
   Learning algorithms
ID NEURONS; MODEL
AB Spiking neural networks (SNNs) have attracted attention as the third generation of neural networks for their promising characteristics of energy-efficiency and biological plausibility. The diversity of spiking neuron models and architectures have made various learning algorithms developed. This paper provides a gentle survey of SNNs to give an overview of what they are and how they are trained. It first presents how biological neurons works and how they are mathematically modelled specially in differential equations. Next it categorizes the learning algorithms of SNNs into groups and presents how their representative algorithms work. Then it briefly describe the neuromorphic hardware on which SNNs run.
C1 [Han, Chan Sik; Lee, Keon Myung] Chungbuk Natl Univ, Dept Comp Sci, Cheongju, South Korea.
RP Lee, KM (corresponding author), Chungbuk Natl Univ, Dept Comp Sci, Cheongju, South Korea.
EM kmlee@cbnu.ac.k
CR Asghar MS, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21134462
   Azghadi M. R., 2012, P 2012 INT JOINT C N, P1, DOI [10.1109/IJCNN.2012.6252778, DOI 10.1109/IJCNN.2012.6252778]
   Bekolay T., 2013, P ANN M COGN SCI SOC, V35
   Bing Han, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13555, DOI 10.1109/CVPR42600.2020.01357
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Bouvier M, 2019, ACM J EMERG TECH COM, V15, DOI 10.1145/3304103
   Brette R, 2007, J COMPUT NEUROSCI, V23, P349, DOI 10.1007/s10827-007-0038-6
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Chauhan N, 2020, INT J FUZZY LOG INTE, V20, P255, DOI 10.5391/IJFIS.2020.20.4.255
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Deng L, 2023, IEEE T NEUR NET LEAR, V34, P2791, DOI 10.1109/TNNLS.2021.3109064
   Deng S, 2021, OPTIMAL CONVERSION C
   Diehl PU, 2015, IEEE IJCNN
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Ding J., 2021, OPTIMAL ANNSNN CONVE
   Eliasmith C., 2003, NEURAL ENG COMPUTATI
   Eliasmith C, 2012, SCIENCE, V338, P1202, DOI 10.1126/science.1225266
   Erdenebayar U, 2020, INT J FUZZY LOG INTE, V20, P181, DOI 10.5391/IJFIS.2020.20.3.181
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   GERSTNER W, 1995, PHYS REV E, V51, P738, DOI 10.1103/PhysRevE.51.738
   Gerstner W., 2002, SPIKING NEURON MODEL
   Han JK, 2021, SCI ADV, V7, DOI 10.1126/sciadv.abg8836
   Heeger D., 2000, POISSON MODEL SPIKE
   Herculano-Houzel S, 2009, FRONT HUM NEUROSCI, V3, DOI 10.3389/neuro.09.031.2009
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Hunsberger Eric, 2015, COMPUT SCI
   Ikegaya Y, 2004, SCIENCE, V304, P559, DOI 10.1126/science.1093173
   Izhikevich E.M., 2006, SCHOLARPEDIA, V1, P1349
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   JAHN R, 1994, ANNU REV NEUROSCI, V17, P219, DOI 10.1146/annurev.ne.17.030194.001251
   Jang H, 2019, IEEE SIGNAL PROC MAG, V36, P64, DOI 10.1109/MSP.2019.2935234
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Kim KI, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10114010
   Lee C., 2019, ENABLING SPIKE BASED
   Lee C, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00119
   Lee KM, 2020, CONCURR COMP-PRACT E, V32, DOI 10.1002/cpe.5480
   Lisman John, 2010, Front Synaptic Neurosci, V2, P140, DOI 10.3389/fnsyn.2010.00140
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   MacNeil D, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0022885
   Malenka RC, 2004, NEURON, V44, P5, DOI 10.1016/j.neuron.2004.09.012
   Patel K., SPIKING NEURAL NETWO, V2021
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Ponulak F., 2005, RESUME NEW SUPERVISE
   Ponulak F, 2011, ACTA NEUROBIOL EXP, V71, P409
   Pouyanfar S, 2019, ACM COMPUT SURV, V51, DOI 10.1145/3234150
   Rathi N., 2020, DIET SNN DIRECT INPU
   Rathi Nitin, 2020, ENABLING DEEP SPIKIN
   Rieke F., 1999, SPIKES EXPLORING NEU
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Rueckauer B., 2016, ARXIV161204052
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   SCHUETZE SM, 1983, TRENDS NEUROSCI, V6, P164, DOI 10.1016/0166-2236(83)90078-4
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Severa W., 2018, WHETSTONE METHOD TRA
   STEIN RB, 1967, BIOPHYS J, V7, P37, DOI 10.1016/S0006-3495(67)86574-3
   Suedof Thomas C., 2008, V184, P1
   Takuya S, 2021, PROC IEEE COOL CHIPS, DOI 10.1109/COOLCHIPS52128.2021.9410323
   Tao CL, 2018, J NEUROSCI, V38, P1493, DOI 10.1523/JNEUROSCI.1548-17.2017
   Tavanaei A, 2019, NEUROCOMPUTING, V330, P39, DOI 10.1016/j.neucom.2018.11.014
   Wu S, 2002, NEURAL COMPUT, V14, P999, DOI 10.1162/089976602753633367
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
NR 62
TC 2
Z9 2
U1 2
U2 18
PD DEC 25
PY 2021
VL 21
IS 4
BP 317
EP 337
DI 10.5391/IJFIS.2021.21.4.317
UT WOS:000739841000001
DA 2023-11-16
ER

PT J
AU Xiang, YD
   Meng, JY
AF Xiang, Yande
   Meng, Jianyi
TI A Cross-layer based mapping for spiking neural network onto network on
   chip
SO INTERNATIONAL JOURNAL OF PARALLEL EMERGENT AND DISTRIBUTED SYSTEMS
DT Article
DE Cross-layer; neural mapping; network-on-chip (NoC); spiking neural
   networks (SNNs)
ID ON-CHIP; SYSTEM; MODEL
AB Network-on-Chip provides a packet-based and scalable inter-connected structure for spiking neural networks. However, existing neural mapping methods just distribute all neurons of a population into an on-chip network core or nearby cores sequentially. As there is no connection among population, the population based mapping degrades inter-neuron communicating performance between different cores. This paper presents a Cross-LAyer based neural MaPping method that maps synaptic connected neurons belonging to adjacent layers into the same on-chip network node. In order to adapt to various input patterns, the strategy also takes input spike rate into consideration and remap neurons for improving mapping efficiency. The method helps to reduce inter-core communication cost. The experimental results demonstrate the efficient results of the proposed mapping strategy in the aspect of spike transfer latency as well as dynamic energy cost improvement. In the applications of handwritten digits and edge extraction, in which the type of interconnection among neurons is different, the neural mapping algorithm reduces spike average transfer latency by maximum 42.83%, and reduces dynamic energy by maximum 36.29%.
C1 [Xiang, Yande] Zhejiang Univ, Coll Informat Sci & Elect Engn, Hangzhou, Zhejiang, Peoples R China.
   [Meng, Jianyi] Fudan Univ, Dept Microelect, State Key Lab ASIC & Syst, Shanghai, Peoples R China.
RP Meng, JY (corresponding author), Fudan Univ, Dept Microelect, State Key Lab ASIC & Syst, Shanghai, Peoples R China.
EM mengjy@fudan.edu.cn
CR Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   [Anonymous], NORCHIP
   [Anonymous], ACM T ARCHIT CODE OP
   [Anonymous], 2010, 2010 IEEE INT S PARA
   [Anonymous], 2013, DAC, DOI DOI 10.1145/2463209.2488734
   [Anonymous], NATURAL COMPUT
   [Anonymous], ENERGY EFFICIENT FAU
   [Anonymous], 2012, 2012 INT JOINT C NEU
   Ascia G, 2004, INTERNATIONAL CONFERENCE ON HARDWARE/SOFTWARE CODESIGN AND SYSTEM SYNTHESIS, P182
   Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Bi GQ, 2001, ANNU REV NEUROSCI, V24, P139, DOI 10.1146/annurev.neuro.24.1.139
   Bjerregaard T, 2006, ACM COMPUT SURV, V38, P1, DOI 10.1145/1132952.1132953
   Cao S, 2016, MICROPROCESS MICROSY, V46, P149, DOI 10.1016/j.micpro.2016.03.010
   Carrillo S, 2013, IEEE T PARALL DISTR, V24, P2451, DOI 10.1109/TPDS.2012.289
   Castilhos G, 2016, J SYST ARCHITECT, V63, P80, DOI 10.1016/j.sysarc.2016.01.005
   Catania V, 2016, ACM T MODEL COMPUT S, V27, DOI 10.1145/2953878
   Dong YP, 2009, 2009 IEEE 8TH INTERNATIONAL CONFERENCE ON ASIC, VOLS 1 AND 2, PROCEEDINGS, P891, DOI 10.1109/ASICON.2009.5351550
   Duraisamy K, 2017, IEEE T VLSI SYST, V25, P1126, DOI 10.1109/TVLSI.2016.2612647
   Fidjeland AK, 2009, IEEE INT CONF ASAP, P137, DOI 10.1109/ASAP.2009.24
   Furber SB, 2013, IEEE T COMPUT, V62, P2454, DOI 10.1109/TC.2012.142
   Gerstner W., 2002, SPIKING NEURON MODEL
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Hu W, 2016, J SYST ARCHITECT, V70, P48, DOI 10.1016/j.sysarc.2016.04.006
   Indiveri G, 2015, 2015 IEEE INTERNATIONAL ELECTRON DEVICES MEETING (IEDM)
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Ji Y, 2016, LECT NOTES COMPUT SC, V10048, P38, DOI 10.1007/978-3-319-49583-5_3
   Mahowald M., 1994, ANALOG VLSI SYSTEM S
   Mandelli M, 2015, INT SYM QUAL ELECT, P387
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Murali S, 2005, ASIA S PACIF DES AUT, P27, DOI 10.1145/1120725.1120737
   Neil D, 2014, IEEE T VLSI SYST, V22, P2621, DOI 10.1109/TVLSI.2013.2294916
   Painkras E, 2013, IEEE J SOLID-ST CIRC, V48, P1943, DOI 10.1109/JSSC.2013.2259038
   Pande S, 2013, PARALLEL COMPUT, V39, P357, DOI 10.1016/j.parco.2013.04.010
   Pande S, 2013, NEURAL PROCESS LETT, V38, P131, DOI 10.1007/s11063-012-9274-5
   Rodopoulos D, 2014, 2014 INTERNATIONAL CONFERENCE ON EMBEDDED COMPUTER SYSTEMS: ARCHITECTURES, MODELING, AND SIMULATION (SAMOS XIV), P367, DOI 10.1109/SAMOS.2014.6893235
   Singh AK, 2010, PROCEDIA COMPUT SCI, V1, P1013, DOI 10.1016/j.procs.2010.04.113
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Srinivasan K, 2005, I CONF VLSI DESIGN, P623, DOI 10.1109/ICVD.2005.113
   Taha TM, 2014, IEEE INT SOC CONF, P383, DOI 10.1109/SOCC.2014.6948959
   Vainbrand D, 2011, MICROPROCESS MICROSY, V35, P152, DOI 10.1016/j.micpro.2010.08.005
   Wu QX, 2007, LECT NOTES ARTIF INT, V4682, P26
   Wu QX, 2013, NEUROCOMPUTING, V116, P3, DOI 10.1016/j.neucom.2012.01.046
NR 43
TC 2
Z9 3
U1 1
U2 3
PY 2018
VL 33
IS 5
SI SI
BP 526
EP 544
DI 10.1080/17445760.2017.1399206
UT WOS:000438453400009
DA 2023-11-16
ER

PT C
AU Kvasnicka, V
AF Kvasnicka, V
BE Kurkova, V
   Steele, NC
   Neruda, R
   Karny, M
TI A simulation of spiking neurons by sigmoid neurons
SO ARTIFICIAL NEURAL NETS AND GENETIC ALGORITHMS
SE SPRINGER COMPUTER SCIENCE
DT Proceedings Paper
CT International Conference on Artificial Neural Networks and Genetic
   Algorithms (ICANNGA)
CY APR 22-25, 2001
CL PRAGUE, CZECH REPUBLIC
AB A simple feed-forward neural networks with sigmoid neurons are studied as a potential effective simulation device of neural networks with spiking neurons. A back-propagation method of calculation of partial derivatives could not be immediately used for spiking neurons. In particular, an adaptation process of the studied neural networks explicitly distinguishes a "time" of activities. We suggested a simple generalization of the back-propagation method such that instead of an original acyclic neural network we consider its unfolded tree form with a root corresponding to the output neuron. We have formulated a conjecture that the presented type of feed-forward neural networks is a universal approximator of any deterministic training pattern specified by binary spiking vectors.
C1 Slovak Univ Technol Bratislava, Dept Math, Bratislava 81237, Slovakia.
RP Kvasnicka, V (corresponding author), Slovak Univ Technol Bratislava, Dept Math, Bratislava 81237, Slovakia.
EM kvasnic@cvt.stuba.sk
CR [Anonymous], 2000, HYBRID NEURAL SYSTEM
   [Anonymous], 1999, NEURAL NETWORKS COMP, DOI DOI 10.1142/S0129065794000372
   MAAS W, 1999, PULSED NEURAL NETWOR
   Rieke F., 1999, SPIKES EXPLORING NEU
NR 4
TC 0
Z9 0
U1 0
U2 2
PY 2001
BP 31
EP 34
UT WOS:000170132600006
DA 2023-11-16
ER

PT C
AU Chen, YL
   Lu, CC
   Juang, KC
   Tang, KT
AF Chen, Yi-Lun
   Lu, Chih-Cheng
   Juang, Kai-Cheung
   Tang, Kea-Tiong
GP IEEE
TI Conversion of Artificial Neural Network to Spiking Neural Network for
   Hardware Implementation
SO 2019 IEEE INTERNATIONAL CONFERENCE ON CONSUMER ELECTRONICS - TAIWAN
   (ICCE-TW)
SE IEEE International Conference on Consumer Electronics-Taiwan
DT Proceedings Paper
CT IEEE International Conference on Consumer Electronics-Taiwan (IEEE
   ICCE-TW)
CY MAY 20-22, 2019
CL Ilan, TAIWAN
DE Edge Computing; Spiking Neural Network; Deep Artificial Neural Network
AB Spiking neural networks (SNNs) are potentially an efficient way to reduce the computation load as well as the power consumption on edge devices because of the sparsely activated neurons and event-driven behavior. In this paper, a continuous-valued artificial neural network (ANN) with fully connections is equivalently converted into spiking operations and the parameters are quantized to low resolution. With the proposed method, data bandwidth can be reduced and the algorithm is proved to be more useful and hardware-amenable on FPGAs. From the simulation results, the ANN with 8- and 4-bit weights received accuracy drop of 0.3% and 0.6%, respectively. The conversion of the quantized ANN to SNN received acceptable error drop within 0.15%.
C1 [Chen, Yi-Lun; Lu, Chih-Cheng; Juang, Kai-Cheung; Tang, Kea-Tiong] Ind Technol Res Inst, Zhudong Township, Taiwan.
   [Tang, Kea-Tiong] Natl Tsing Hua Univ, Dept Elect Engn, Hsinchu, Taiwan.
RP Chen, YL (corresponding author), Ind Technol Res Inst, Zhudong Township, Taiwan.
CR Chen YH, 2016, ISSCC DIG TECH PAP I, V59, P262, DOI 10.1109/ISSCC.2016.7418007
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
   Lin SC, 2018, ACM SIGPLAN NOTICES, V53, P751, DOI [10.1145/3296957.3173191, 10.1145/3173162.3173191]
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Zhou S., 2018, ARXIV160606160
NR 6
TC 0
Z9 0
U1 1
U2 1
PY 2019
DI 10.1109/icce-tw46550.2019.8991758
UT WOS:000712323500076
DA 2023-11-16
ER

PT J
AU Wu, GL
   Liang, DC
   Luan, ST
   Wang, J
AF Wu, Guanlin
   Liang, Dongchen
   Luan, Shaotong
   Wang, Ji
TI Training Spiking Neural Networks for Reinforcement Learning Tasks With
   Temporal Coding Method
SO FRONTIERS IN NEUROSCIENCE
DT Article
DE spiking neural networks; reinforcement learning; temporal coding; fully
   differentiable; asynchronous processing
AB Recent years witness an increasing demand for using spiking neural networks (SNNs) to implement artificial intelligent systems. There is a demand of combining SNNs with reinforcement learning architectures to find an effective training method. Recently, temporal coding method has been proposed to train spiking neural networks while preserving the asynchronous nature of spiking neurons to preserve the asynchronous nature of SNNs. We propose a training method that enables temporal coding method in RL tasks. To tackle the problem of high sparsity of spikes, we introduce a self-incremental variable to push each spiking neuron to fire, which makes SNNs fully differentiable. In addition, an encoding method is proposed to solve the problem of information loss of temporal-coded inputs. The experimental results show that the SNNs trained by our proposed method can achieve comparable performance of the state-of-the-art artificial neural networks in benchmark tasks of reinforcement learning.
C1 [Wu, Guanlin; Liang, Dongchen] Acad Mil Sci, Beijing, Peoples R China.
   [Luan, Shaotong] Nanhu Lab, Jiaxing, Peoples R China.
   [Wang, Ji] Natl Univ Def Technol, Coll Syst Engn, Changsha, Peoples R China.
RP Liang, DC (corresponding author), Acad Mil Sci, Beijing, Peoples R China.
EM liangdongchen@foxmail.com
CR AMARI S, 1993, NEUROCOMPUTING, V5, P185, DOI 10.1016/0925-2312(93)90006-O
   BARTO AG, 1983, IEEE T SYST MAN CYB, V13, P834, DOI 10.1109/TSMC.1983.6313077
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Comsa JM, 2020, INT CONF ACOUST SPEE, P8529, DOI [10.1109/icassp40776.2020.9053856, 10.1109/ICASSP40776.2020.9053856]
   Degris T., 2012, P 29 INT C MACH LEAR
   Fan J., 2020, LEARNING DYNAMICS CO, P486, DOI DOI 10.48550/ARXIV.1901.00137
   Kim J, 2020, ADV NEURAL INFORM PR, V33
   Kingma DP., 2017, ARXIV
   Li Q, 2020, ADV NEURAL INFORM PR, P4894
   MEAD C, 1990, P IEEE, V78, P1629, DOI 10.1109/5.58356
   Mostafa H, 2018, IEEE T NEUR NET LEAR, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Patel D, 2019, NEURAL NETWORKS, V120, P108, DOI 10.1016/j.neunet.2019.08.009
   Rosenfeld B, 2019, IEEE INT WORK SIGN P, DOI 10.1109/spawc.2019.8815546
   Schulman J, 2017, Arxiv, DOI arXiv:1707.06347
   Tan W., 2021, AAAI C ARTIFICIAL IN
   Tang G., 2020, 4 C ROBOT LEARNING C
   Tang GZ, 2020, Arxiv, DOI arXiv:2003.01157
   Xiong H, 2020, ADV NEURAL INFORM PR, P33
   Zhang W, 2020, ADV NEURAL INFORM PR, V33, P12022, DOI DOI 10.48550/ARXIV.2002.10085
   Zhang WR, 2019, ADV NEUR IN, V32
NR 21
TC 1
Z9 1
U1 4
U2 15
PD AUG 17
PY 2022
VL 16
AR 877701
DI 10.3389/fnins.2022.877701
UT WOS:000847942700001
DA 2023-11-16
ER

PT C
AU Ltaief, M
   Bezine, H
   Alimi, AM
AF Ltaief, Mahmoud
   Bezine, Hala
   Alimi, Adel M.
GP IEEE
TI A spiking motor-model for online handwriting movements generation
SO PROCEEDINGS OF 2016 15TH INTERNATIONAL CONFERENCE ON FRONTIERS IN
   HANDWRITING RECOGNITION (ICFHR)
SE International Conference on Handwriting Recognition
DT Proceedings Paper
CT 15th International Conference on Frontiers in Handwriting Recognition
   (ICFHR)
CY OCT 23-26, 2016
CL Shenzhen, PEOPLES R CHINA
DE handwriting generation models; Beta-elliptic model; spiking neural
   network
ID KINEMATIC THEORY
AB A spiking neural network model for handwriting movement generation is proposed, in which curvilinear velocity signals are modeled with Beta profiles. In the trajectory domain each Beta profile is replaced by an elliptic arc to fit the initial stroke. The spiking neural network architecture is constituted by an input layer which uploads the Beta-elliptic parameters as input, hidden layer and the output layer where script coordinates X(t) and Y(t) are generated. A separate timing network prepares the initial state of the network. This latter involves the time-index starting time of each simple stroke for an appropriate handwriting movement signal. The experiments showed that our spiking neural network model could be applied for the both cases of Latin and Arabic handwriting scripts. Similarity degree is measured between original scripts and generated scripts to evaluate our model. New ways are proposed for the application of the spiking neural network model such as: generation of complex handwriting movements, signature verification and shape recognition.
C1 [Ltaief, Mahmoud; Bezine, Hala; Alimi, Adel M.] Natl Sch Engineers Sfax, Res Grp Intelligent Machines REGIM, Sfax, Tunisia.
RP Ltaief, M (corresponding author), Natl Sch Engineers Sfax, Res Grp Intelligent Machines REGIM, Sfax, Tunisia.
EM mahmoud.ltaief@ieee.org; hala.bezine@ieee.org; Adel.Alimi@ieee.org
CR Alimi A. M., 2003, TASK Quarterly, V7, P23
   Alimi M. A., 1997, ICNN, P1
   [Anonymous], DYNAMIC PATTERNS COM
   [Anonymous], 2001, HDB BIOL PHYS
   Bezine H, 2004, NINTH INTERNATIONAL WORKSHOP ON FRONTIERS IN HANDWRITING RECOGNITION, PROCEEDINGS, P515, DOI 10.1109/IWFHR.2004.45
   Bezine H., 2011, INK ENABLED HANDWRIT, P213
   Bezine H, 2007, INT J PATTERN RECOGN, V21, P5, DOI 10.1142/S0218001407005272
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Gangadhar G, 2008, NEURAL COMPUT, V20, P2491, DOI 10.1162/neco.2008.03-07-498
   Gangadhar G, 2007, INT J DOC ANAL RECOG, V10, P69, DOI 10.1007/s10032-007-0046-0
   Kalveram KT, 1998, MOTOR BEHAVIOR AND HUMAN SKILL, P127
   Li YH, 2007, PATTERN RECOGN LETT, V28, P278, DOI 10.1016/j.patrec.2006.07.009
   Ltaief M, 2012, INT C FRONT HANDWR R, P799
   Ltaief M., 2012, J INTELLIGENT LEARNI, V4, P256
   Martín-Albo D, 2014, INT CONF FRONT HAND, P543, DOI 10.1109/ICFHR.2014.97
   Njah S, 2012, INT CONF FRONT HAND, P308, DOI 10.1109/ICFHR.2012.230
   Plamondon R, 2003, BIOL CYBERN, V89, P126, DOI 10.1007/s00422-003-0407-9
   PLAMONDON R, 1989, IEEE T SYST MAN CYB, V19, P1060, DOI 10.1109/21.44021
   Plamondon R., 2012, PATTERN RECOGNITION
   Plamondon R., 2006, HUM MOV SCI
   Rusu A, 2004, NINTH INTERNATIONAL WORKSHOP ON FRONTIERS IN HANDWRITING RECOGNITION, PROCEEDINGS, P226, DOI 10.1109/IWFHR.2004.54
   Schomaker L, 1998, ELECTRON COMMUN ENG, V10, P93, DOI 10.1049/ecej:19980302
   Schomaker L. R. B., 1991, THESIS
   Teulings H. L., 1986, EXPT PROTOCOL CURSIV
   Van Galen GP, 1998, ACTA PSYCHOL, V100, P195, DOI 10.1016/S0001-6918(98)00034-1
   Wada Y., 1994, NIPS, V6, P727
NR 26
TC 1
Z9 1
U1 0
U2 1
PY 2016
BP 477
EP 482
DI 10.1109/ICFHR.2016.87
UT WOS:000400052400085
DA 2023-11-16
ER

PT C
AU Webb, A
   Davies, S
   Lester, D
AF Webb, Andrew
   Davies, Sergio
   Lester, David
BE Lu, BL
   Zhang, LQ
   Kwok, J
TI Spiking Neural PID Controllers
SO NEURAL INFORMATION PROCESSING, PT III
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 18th International Conference on Neural Information Processing (ICONIP
   2011)
CY NOV 13-17, 2011
CL Shanghai, PEOPLES R CHINA
DE SpiNNaker; neural networks; PID controllers
ID NEURONS; SYSTEMS; MODEL
AB A PID controller is a simple and general-purpose way of providing responsive control of dynamic systems with reduced overshoot and oscillation. Spiking neural networks offer some advantages for dynamic systems control, including an ability to adapt, but it is not obvious how to alter such a control network's parameters to shape its response curve. In this paper we present a spiking neural PID controller: a small network of neurons that mimics a PID controller by using the membrane recovery variable in Izhikevich's simple model of spiking neurons to approximate derivative and integral functions.
C1 [Webb, Andrew; Davies, Sergio; Lester, David] Univ Manchester, Sch Comp Sci, Manchester, Lancs, England.
RP Webb, A (corresponding author), Univ Manchester, Sch Comp Sci, Oxford Rd, Manchester, Lancs, England.
EM webb@cs.man.ac.uk; daviess@cs.man.ac.uk; dlester@cs.man.ac.uk
CR Dayan P., 2001, THEORETICAL NEUROSCI
   Furber S, 2008, STUD COMPUT INTELL, V115, P763, DOI 10.1098/rsif.2006.0177
   Gerstner W., 2002, SPIKING NEURON MODEL
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Izhikevich E.M., 2006, DYNAMICAL SYSTEMS NE
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   JIN X, 2008, EFFICIENT MODELLING, P2812
   RAST AD, 2009, P 2009 INT JOINT C N, P3378
   Rast AD, 2010, PROCEEDINGS OF THE 2010 COMPUTING FRONTIERS CONFERENCE (CF 2010), P21, DOI 10.1145/1787275.1787279
   Shu HL, 2000, COMPUT CHEM ENG, V24, P859, DOI 10.1016/S0098-1354(00)00340-9
NR 11
TC 7
Z9 7
U1 0
U2 0
PY 2011
VL 7064
BP 259
EP 267
PN III
UT WOS:000307328500028
DA 2023-11-16
ER

PT J
AU Lee, JS
   Sim, D
AF Lee, Jongseok
   Sim, Donggyu
TI Semi-Supervised Learning for Spiking Neural Networks Based on
   Spike-Timing-Dependent Plasticity
SO IEEE ACCESS
DT Article
DE Neurons; Synapses; Supervised learning; Unsupervised learning;
   Biological neural networks; Brain modeling; Training; Spiking neural
   network; semi-supervised learning; spike-timing dependent plasticity;
   image classification
AB In this study, we propose a semi-supervised learning method for spiking neural networks based on spike-timing-dependent plasticity (STDP). The spiking neural network structure of the proposed method incorporates teacher neurons and synapses, which serve the same purpose as real-life teachers, who ensure that the actions of their students do not transcend social norms. In the first stage of the proposed learning method, STDP-based supervised learning is applied. In the second stage, STDP-based unsupervised learning is conducted in the absence of any input signal to the teacher neuron. The proposed learning method classified handwritten characters with higher accuracy than the existing method. On the MNIST dataset, the proposed method was approximately 5%, 1%, and 3% more accurate than the conventional algorithm on 100, 400, and 1600 excitatory neurons, respectively.
C1 [Lee, Jongseok; Sim, Donggyu] Kwangwoon Univ, Dept Comp Engn, Seoul 139701, South Korea.
RP Sim, D (corresponding author), Kwangwoon Univ, Dept Comp Engn, Seoul 139701, South Korea.
EM dgsim@kw.ac.kr
CR Beyeler M, 2013, NEURAL NETWORKS, V48, P109, DOI 10.1016/j.neunet.2013.07.012
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   BLISS TVP, 1993, NATURE, V361, P31, DOI 10.1038/361031a0
   Brader JM, 2007, NEURAL COMPUT, V19, P2881, DOI 10.1162/neco.2007.19.11.2881
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Chua JJE, 2010, J CELL SCI, V123, P819, DOI 10.1242/jcs.052696
   COOMBS JS, 1955, J PHYSIOL-LONDON, V130, P326, DOI 10.1113/jphysiol.1955.sp005412
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281
   Drake F.L, 1995, PYTHON TUTORIAL
   Habenschuss S., 2012, ADV NEURAL INFORM PR, P773
   Hao YZ, 2020, NEURAL NETWORKS, V121, P387, DOI 10.1016/j.neunet.2019.09.007
   Hazan H., 2018, 2018 INT JOINT C NEU, P1
   Heeger D., 2000, HANDOUT U STANDFORD, V5, P76
   Hunsberger E, 2015, Arxiv, DOI arXiv:1510.08829
   Kang J, 2010, EUR J NEUROSCI, V31, P1006, DOI 10.1111/j.1460-9568.2010.07145.x
   Kheradpisheh SR, 2016, NEUROCOMPUTING, V205, P382, DOI 10.1016/j.neucom.2016.04.029
   Koch C., 1998, METHODS NEURONAL MOD
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
   Krizhevsky Alex, 2009, LEARNING MULTIPLE LA
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Morrison A, 2007, NEURAL COMPUT, V19, P1437, DOI 10.1162/neco.2007.19.6.1437
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   Querlioz D, 2013, IEEE T NANOTECHNOL, V12, P288, DOI 10.1109/TNANO.2013.2250995
   Ross Helen E., 1996, WEBER TACTILE SENSES
   Salakhutdinov R., 2007, P 24 INT C MACH LEAR, P791, DOI DOI 10.1145/1273496
   Shi SS, 2019, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2019.00086
   Shi XJ, 2015, Arxiv, DOI [arXiv:1506.04214, 10.48550/arXiv.1506.04214]
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Tavanaei A, 2019, NEUROCOMPUTING, V330, P39, DOI 10.1016/j.neucom.2018.11.014
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Xiao H, 2017, Arxiv, DOI arXiv:1708.07747
   Zhang W, 2003, NAT REV NEUROSCI, V4, P885, DOI 10.1038/nrn1248
NR 36
TC 1
Z9 1
U1 1
U2 1
PY 2023
VL 11
BP 35140
EP 35149
DI 10.1109/ACCESS.2023.3264435
UT WOS:000973507400001
DA 2023-11-16
ER

PT J
AU García, P
   Suárez, CP
   Rodríguez, J
   Rodríguez, M
AF García, P
   Suárez, CP
   Rodríguez, J
   Rodríguez, M
TI Unsupervised classification of neural spikes with a hybrid multilayer
   artificial neural network
SO JOURNAL OF NEUROSCIENCE METHODS
DT Article
DE spike discriminator; neural network; multiple-unit recording; Kohonen
   network; Sanger network
ID CENTRAL NERVOUS-SYSTEM; SOFTWARE-BASED SYSTEM; WAVEFORM CLASSIFICATION;
   DOPAMINERGIC-NEURONS; NEUROELECTRIC DATA; CELL-ACTIVITY; REAL-TIME;
   RECORDINGS; ELECTROPHYSIOLOGY; IMPLEMENTATION
AB The understanding of the brain structure and function and its computational style is one of the biggest challenges both in Neuroscience and Neural Computation. In order to reach this and to test the predictions of neural network modeling, it is necessary to observe the activity of neural populations. In this paper we propose a hybrid modular computational system for the spike classification of multiunits recordings. It works with no knowledge about the waveform, and it consists of two moduli: a Preprocessing (Segmentation) module, which performs the detection and centering of spike vectors using programmed computation; and a Processing (Classification) module, which implements the general approach of neural classification: feature extraction, clustering and discrimination, by means of a hybrid unsupervised multilayer artificial neural network (HUMANN). The operations of this artificial neural network on the spike vectors are: (i) compression with a Sanger Layer from 70 points vector to five principal component vector; (ii) their waveform is analyzed by a Kohonen layer; (iii) the electrical noise and overlapping spikes are rejected by a previously unreported artificial neural network named Tolerance layer; and (iv) finally the spikes are labeled into spike classes by a Labeling layer. Each layer of the system has a specific unsupervised learning rule that progressively modifies itself until the performance of the layer has been automatically optimized. The procedure showed a high sensitivity and specificity also when working with signals containing four spike types. (C) 1998 Elsevier Science B.V. All rights reserved.
C1 Univ La Laguna, Dept Physiol, E-38207 La Laguna, Canary Islands, Spain.
   Univ La Laguna, Dept Stat Operating Res & Computat, E-38207 La Laguna, Canary Islands, Spain.
   Univ Las Palmas de Gran Canaria, Dept Comp Sci & Syst, Las Palmas Gran Canaria, Canary Islands, Spain.
RP Rodríguez, M (corresponding author), Univ La Laguna, Dept Physiol, E-38207 La Laguna, Canary Islands, Spain.
CR ABELES M, 1977, P IEEE, V65, P762, DOI 10.1109/PROC.1977.10559
   Abeles M., 1982, LOCAL CORTICAL CIRCU, DOI DOI 10.1007/978-3-642-81708-3
   [Anonymous], 1991, CORTICONICS
   ATIYA AF, 1992, IEEE T BIO-MED ENG, V39, P723, DOI 10.1109/10.142647
   BANKMAN IN, 1993, AUTOMATED RECOGNITIO, P69
   BERGMAN H, 1992, J NEUROSCI METH, V41, P187, DOI 10.1016/0165-0270(92)90084-Q
   CASTELLANO MA, 1991, BRAIN RES BULL, V27, P213, DOI 10.1016/0361-9230(91)90070-Z
   CASTELLANO MA, 1993, NEUROSCI LETT, V162, P1, DOI 10.1016/0304-3940(93)90545-V
   CHIODO LA, 1988, NEUROSCI BIOBEHAV R, V12, P49, DOI 10.1016/S0149-7634(88)80073-3
   Eggermont JS., 1990, CORRELATIVE BRAIN TH
   FREEMAN JA, 1971, J APPL PHYSIOL, V31, P939, DOI 10.1152/jappl.1971.31.6.939
   GARCIABAEZ P, 1996, NEUROCIENCIA COMPUTA, P283
   GARCIABAEZ P, 1994, P 5 INT S BIOM ENG, P194
   GARCIABAEZ P, 1996, T 14 C SOC ESP ING B, P205
   GERSTEIN GL, 1985, J NEUROSCI, V5, P881
   Glaser E., 1971, ADV BIOMED ENG, V1, P77
   Gonzalez C, 1997, J NEUROSCI METH, V72, P189, DOI 10.1016/S0165-0270(96)02202-9
   GRACE AA, 1984, J NEUROSCI, V4, P2877
   GRACE AA, 1983, NEUROSCIENCE, V10, P301, DOI 10.1016/0306-4522(83)90135-5
   GROVES PM, 1978, INTERACTIONS PUTATIV, P191
   JANSEN RF, 1990, J NEUROSCI METH, V35, P203, DOI 10.1016/0165-0270(90)90125-Y
   JANSEN RF, 1992, J NEUROSCI METH, V42, P123
   JOFFROY M, 1975, J PHYSIOL-PARIS, V70, P239
   KOHONEN T, 1990, P IEEE, V78, P1464, DOI 10.1109/5.58325
   Kohonen T., 1989, SELF ORG ASSOCIATIVE, V3rd
   KOHONEN T, 1993, IEEE 1993 INT C NEUR, P1147
   KRUGER J, 1991, NEURONAL COOPERATIVI, P105
   Lemon, 1984, IBRO HDB SERIES
   MACNICHOL EF, 1955, REV SCI INSTRUM, V26, P1176, DOI 10.1063/1.1715216
   MARIONPOLL F, 1991, J NEUROSCI METH, V37, P1, DOI 10.1016/0165-0270(91)90015-R
   MCNAUGHTON BL, 1983, J NEUROSCI METH, V8, P391, DOI 10.1016/0165-0270(83)90097-3
   MILLAR J, 1983, J NEUROSCI METH, V7, P157, DOI 10.1016/0165-0270(83)90078-X
   Paxinos G., 1986, RAT BRAIN STEREOTAXI
   Rodriguez M, 1995, BRAIN RES, V703, P201, DOI 10.1016/0006-8993(95)01098-X
   SABAH NH, 1977, MED BIOL ENG COMPUT, V15, P205, DOI 10.1007/BF02442966
   SALGANICOFF M, 1988, J NEUROSCI METH, V25, P181, DOI 10.1016/0165-0270(88)90132-X
   SANGER TD, 1989, NEURAL NETWORKS, V2, P459, DOI 10.1016/0893-6080(89)90044-0
   SARNA MF, 1988, J NEUROSCI METH, V25, P189, DOI 10.1016/0165-0270(88)90133-1
   SCHMIDT EM, 1984, J NEUROSCI METH, V12, P1, DOI 10.1016/0165-0270(84)90042-6
   SCHMIDT EM, 1984, J NEUROSCI METH, V12, P95, DOI 10.1016/0165-0270(84)90009-8
   SHAH S, 1994, IEEE ENG MED BIO NOV, P743
   SUAREZARAUJO CP, 1996, NEUROCIENCIA COMPUTA, P251
   VIBERT JF, 1979, ELECTROEN CLIN NEURO, V47, P172, DOI 10.1016/0013-4694(79)90219-0
   YANG XW, 1988, IEEE T BIO-MED ENG, V35, P806, DOI 10.1109/10.7287
NR 44
TC 20
Z9 23
U1 0
U2 5
PD JUL 1
PY 1998
VL 82
IS 1
BP 59
EP 73
DI 10.1016/S0165-0270(98)00035-1
UT WOS:000075114300007
DA 2023-11-16
ER

PT C
AU Lin, XH
   Shen, FQ
   Liu, K
AF Lin, Xianghong
   Shen, Fanqi
   Liu, Kun
BE Huang, DS
   Bevilacqua, V
   Premaratne, P
   Gupta, P
TI An Evolutionary Algorithm for Autonomous Agents with Spiking Neural
   Networks
SO INTELLIGENT COMPUTING THEORIES AND APPLICATION, ICIC 2017, PT I
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 13th International Conference on Intelligent Computing (ICIC)
CY AUG 07-10, 2017
CL Liverpool, ENGLAND
DE Spiking neural network; Evolutionary algorithm; Autonomous agent;
   Genetic operator
AB Inspired by the evolution of biological brains, the study of neurally-driven evolved autonomous agents has received more and more attention. In this paper, we propose an evolutionary algorithm for neurally-driven autonomous agents, each agent is controlled by a spiking neural network, and the network receives the sensory inputs and processes the motor outputs through the encoded spike information. The controlling spiking neural networks of autonomous agents are developed by the evolutionary algorithms that apply some of genetic operators and selection to a population of agents that undergo evolution. The corresponding food gathering experiment results show that the autonomous agents appear intelligent behaviours for the simulation environment. Additionally, the parameters of networks and agents play an important role in the evolutionary process.
C1 [Lin, Xianghong; Shen, Fanqi; Liu, Kun] Northwest Normal Univ, Coll Comp Sci & Engn, Lanzhou 730070, Gansu, Peoples R China.
RP Lin, XH (corresponding author), Northwest Normal Univ, Coll Comp Sci & Engn, Lanzhou 730070, Gansu, Peoples R China.
EM linxh@nwnu.edu.cn
CR Batllori R, 2011, PROCEDIA COMPUT SCI, V6, DOI 10.1016/j.procs.2011.08.060
   Bodyanskiy Y, 2014, APPL SOFT COMPUT, V14, P252, DOI 10.1016/j.asoc.2013.05.020
   Bredeche N, 2012, MATH COMP MODEL DYN, V18, P101, DOI 10.1080/13873954.2011.601425
   Crepinsek M, 2013, ACM COMPUT SURV, V45, DOI 10.1145/2480741.2480752
   Floreano D, 1998, NEURAL NETWORKS, V11, P1461, DOI 10.1016/S0893-6080(98)00082-3
   Gerstner W., 2002, SPIKING NEURON MODEL
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Kasabov N, 2016, NEURAL NETWORKS, V78, P1, DOI 10.1016/j.neunet.2015.09.011
   Keinan A, 2006, ARTIF LIFE, V12, P333, DOI 10.1162/artl.2006.12.3.333
   Rekabdar B., 2016, NEURAL COMPUT APPL, P1
   Rekabdar B, 2016, NEURAL PROCESS LETT, V43, P327, DOI 10.1007/s11063-015-9436-3
   Schliebs S, 2013, EVOL SYST-GER, V4, P87, DOI 10.1007/s12530-013-9074-9
   Seth AK, 2015, CURR BIOL, V25, pR110, DOI 10.1016/j.cub.2014.12.043
   Tettamanzi A. G., 2013, SOFT COMPUTING INTEG
   Wysoski SG, 2010, NEURAL NETWORKS, V23, P819, DOI 10.1016/j.neunet.2010.04.009
NR 15
TC 0
Z9 0
U1 0
U2 2
PY 2017
VL 10361
BP 37
EP 47
DI 10.1007/978-3-319-63309-1_4
UT WOS:000441208400004
DA 2023-11-16
ER

PT J
AU Liu, H
   Song, YD
   Xue, FZ
   Li, XM
AF Liu, Hui
   Song, Yongduan
   Xue, Fangzheng
   Li, Xiumin
TI Effects of bursting dynamic features on the generation of
   multi-clustered structure of neural network with symmetric
   spike-timing-dependent plasticity learning rule
SO CHAOS
DT Article
ID INFORMATION
AB In this paper, the generation of multi-clustered structure of self-organized neural network with different neuronal firing patterns, i.e., bursting or spiking, has been investigated. The initially all-to-all-connected spiking neural network or bursting neural network can be self-organized into clustered structure through the symmetric spike-timing-dependent plasticity learning for both bursting and spiking neurons. However, the time consumption of this clustering procedure of the burst-based self-organized neural network (BSON) is much shorter than the spike-based self-organized neural network (SSON). Our results show that the BSON network has more obvious small-world properties, i.e., higher clustering coefficient and smaller shortest path length than the SSON network. Also, the results of larger structure entropy and activity entropy of the BSON network demonstrate that this network has higher topological complexity and dynamical diversity, which benefits for enhancing information transmission of neural circuits. Hence, we conclude that the burst firing can significantly enhance the efficiency of clustering procedure and the emergent clustered structure renders the whole network more synchronous and therefore more sensitive to weak input. This result is further confirmed from its improved performance on stochastic resonance. Therefore, we believe that the multi-clustered neural network which self-organized from the bursting dynamics has high efficiency in information processing. (C) 2015 AIP Publishing LLC.
C1 [Liu, Hui; Song, Yongduan; Xue, Fangzheng; Li, Xiumin] Chongqing Univ, Minist Educ, Key Lab Dependable Serv Comp Cyber Phys Soc, Chongqing 400044, Peoples R China.
   [Liu, Hui; Song, Yongduan; Xue, Fangzheng; Li, Xiumin] Chongqing Univ, Coll Automat, Chongqing 400044, Peoples R China.
RP Li, XM (corresponding author), Chongqing Univ, Minist Educ, Key Lab Dependable Serv Comp Cyber Phys Soc, Chongqing 400044, Peoples R China.
EM xmli@cqu.edu.cn
CR Abbott LF, 2000, NAT NEUROSCI, V3, P1178, DOI 10.1038/81453
   Caporale N, 2008, ANNU REV NEUROSCI, V31, P25, DOI 10.1146/annurev.neuro.31.060407.125639
   Fox JJ, 2001, CHAOS, V11, P809, DOI 10.1063/1.1414882
   Gao ZK, 2015, PHYSICA A, V417, P7, DOI 10.1016/j.physa.2014.09.017
   Hebb D.O., 2005, ORG BEHAV NEUROPSYCH
   Hilgetag CC, 2000, PHILOS T ROY SOC B, V355, P91, DOI 10.1098/rstb.2000.0551
   Izhikevich E.M., 2007, COMPUTATIONAL NEUROS
   Izhikevich EM, 2000, INT J BIFURCAT CHAOS, V10, P1171, DOI 10.1142/S0218127400000840
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2003, TRENDS NEUROSCI, V26, P161, DOI 10.1016/S0166-2236(03)00034-1
   Kaiser M, 2004, PHYS REV E, V69, DOI 10.1103/PhysRevE.69.036103
   Kaiser M, 2004, NEUROCOMPUTING, V58, P297, DOI 10.1016/j.neucom.2004.01.059
   Kepecs A, 2003, NETWORK-COMP NEURAL, V14, P103, DOI 10.1088/0954-898X/14/1/306
   Landis F, 2010, NEURAL COMPUT, V22, P273, DOI 10.1162/neco.2009.12-08-926
   Li XM, 2012, CHAOS, V22, DOI 10.1063/1.3701946
   Li XM, 2010, NEW J PHYS, V12, DOI 10.1088/1367-2630/12/8/083045
   Li XM, 2009, CHAOS, V19, DOI 10.1063/1.3076394
   Masuda N, 2007, J COMPUT NEUROSCI, V22, P327, DOI 10.1007/s10827-007-0022-1
   Newman MEJ, 2003, SIAM REV, V45, P167, DOI 10.1137/S003614450342480
   Nishiyama M, 2000, NATURE, V408, P584, DOI 10.1038/35046067
   Perc M, 2007, PHYS REV E, V76, DOI 10.1103/PhysRevE.76.066203
   Prado TD, 2014, PHYS REV E, V90, DOI 10.1103/PhysRevE.90.032818
   Shew WL, 2011, J NEUROSCI, V31, P55, DOI 10.1523/JNEUROSCI.4637-10.2011
   Wang HX, 2011, CHAOS SOLITON FRACT, V44, P667, DOI 10.1016/j.chaos.2011.06.003
   Wang QY, 2009, CHAOS, V19, DOI 10.1063/1.3133126
   Wang SJ, 2012, NEW J PHYS, V14, DOI 10.1088/1367-2630/14/2/023005
   Wang Sheng-Jun, 2011, Front Comput Neurosci, V5, P30, DOI 10.3389/fncom.2011.00030
   Woodin MA, 2003, NEURON, V39, P807, DOI 10.1016/S0896-6273(03)00507-5
   Xiao Fan Wang, 2003, IEEE Circuits and Systems Magazine, V3, P6, DOI 10.1109/MCAS.2003.1228503
   Yu HT, 2014, CHAOS, V24, DOI 10.1063/1.4893773
   Zamora-López G, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00083
   Zhou CS, 2007, NEW J PHYS, V9, DOI 10.1088/1367-2630/9/6/178
NR 32
TC 0
Z9 0
U1 2
U2 25
PD NOV
PY 2015
VL 25
IS 11
AR 113108
DI 10.1063/1.4935281
UT WOS:000365766200008
DA 2023-11-16
ER

PT J
AU Wu, TF
   Bîlbîe, FD
   Paun, A
   Pan, LQ
   Neri, F
AF Wu, Tingfang
   Bilbie, Florin-Daniel
   Paun, Andrei
   Pan, Linqiang
   Neri, Ferrante
TI Simplified and Yet Turing Universal Spiking Neural P Systems with
   Communication on Request
SO INTERNATIONAL JOURNAL OF NEURAL SYSTEMS
DT Article
DE Bio-inspired computing; membrane computing; spiking neural network;
   spiking neural P system; computation power
ID COMPUTATIONAL POWER; NETWORKS; NEURONS; RECOGNITION; ALGORITHM;
   CLASSIFICATION; IMPLEMENTATION; MACHINES; DESIGN; SPIKES
AB Spiking neural P systems are a class of third generation neural networks belonging to the framework of membrane computing. Spiking neural P systems with communication on request (SNQ P systems) are a type of spiking neural P system where the spikes are requested from neighboring neurons. SNQ P systems have previously been proved to be universal (computationally equivalent to Turing machines) when two types of spikes are considered. This paper studies a simplified version of SNQ P systems, i.e. SNQ P systems with one type of spike. It is proved that one type of spike is enough to guarantee the Turing universality of SNQ P systems. Theoretical results are shown in the cases of the SNQ P system used in both generating and accepting modes. Furthermore, the influence of the number of unbounded neurons (the number of spikes in a neuron is not bounded) on the computation power of SNQ P systems with one type of spike is investigated. It is found that SNQ P systems functioning as number generating devices with one type of spike and four unbounded neurons are Turing universal.
C1 [Wu, Tingfang; Pan, Linqiang] Huazhong Univ Sci & Technol, Sch Automat, Educ Minist China, Key Lab Image Informat Proc & Intelligent Control, Wuhan 430074, Hubei, Peoples R China.
   [Bilbie, Florin-Daniel; Paun, Andrei] Univ Bucharest, Fac Math & Comp Sci, Dept Comp Sci, Str Acad 14,Sect 1, Bucharest 010014, Romania.
   [Paun, Andrei] Natl Inst Res & Dev Biol Sci, Dept Bioinformat, Splaiul Independentei 296,Sect 6, Bucharest, Romania.
   [Pan, Linqiang] Zhengzhou Univ Light Ind, Sch Elect & Informat Engn, Zhengzhou 450002, Henan, Peoples R China.
   [Neri, Ferrante] De Montfort Univ, Sch Comp Sci & Informat, Ctr Computat Intelligence, Leicester LE1 9BH, Leics, England.
RP Pan, LQ (corresponding author), Huazhong Univ Sci & Technol, Sch Automat, Educ Minist China, Key Lab Image Informat Proc & Intelligent Control, Wuhan 430074, Hubei, Peoples R China.; Pan, LQ (corresponding author), Zhengzhou Univ Light Ind, Sch Elect & Informat Engn, Zhengzhou 450002, Henan, Peoples R China.
EM lqpan@mail.hust.edu.cn
CR Ahmadlou M, 2010, INTEGR COMPUT-AID E, V17, P197, DOI 10.3233/ICA-2010-0345
   [Anonymous], 1997, HDB FORMAL LANGUAGES, DOI DOI 10.1007/978-3-662-07675-0
   Appeltant L, 2011, NAT COMMUN, V2, DOI 10.1038/ncomms1476
   Bilbie F.-D., 2017, P 18 INT C MEMBR COM
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Booij O, 2005, INFORM PROCESS LETT, V95, P552, DOI 10.1016/j.ipl.2005.05.023
   Cavaliere M, 2009, THEOR COMPUT SCI, V410, P2352, DOI 10.1016/j.tcs.2009.02.031
   Cavaliere Matteo, 2008, Natural Computing, V7, P453, DOI 10.1007/s11047-008-9086-8
   Chen HM, 2007, FUND INFORM, V75, P141
   Csuhaj-Varju E, 1994, GRAMMAR SYSTEMS GRAM
   D'Haene M, 2014, NEURAL COMPUT, V26, P1055, DOI 10.1162/NECO_a_00587
   Diaz C, 2016, NEUROCOMPUTING, V189, P130, DOI 10.1016/j.neucom.2015.12.086
   Díaz-Pernil D, 2013, NEUROCOMPUTING, V115, P81, DOI 10.1016/j.neucom.2012.12.032
   Freestone DR, 2017, INT J NEURAL SYST, V27, DOI 10.1142/S0129065716500453
   García-Arnau M, 2009, INT J UNCONV COMPUT, V5, P411
   Garrido JA, 2016, INT J NEURAL SYST, V26, DOI 10.1142/S0129065716500209
   Geminiani A, 2018, INT J NEURAL SYST, V28, DOI 10.1142/S0129065717500174
   Gentiletti D, 2017, INT J NEURAL SYST, V27, DOI 10.1142/S0129065717500046
   Ghosh-Dastidar S, 2007, INTEGR COMPUT-AID E, V14, P187
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Guo LL, 2017, INT J NEURAL SYST, V27, DOI 10.1142/S0129065717500022
   Gupta A, 2007, IEEE IJCNN, P53, DOI 10.1109/IJCNN.2007.4370930
   Hirschauer TJ, 2015, J MED SYST, V39, DOI 10.1007/s10916-015-0353-9
   Hopcroft John E, 2001, INTRO AUTOMATA THEOR, V32, P60
   Huynh HT, 2008, INT J NEURAL SYST, V18, P433, DOI 10.1142/S0129065708001695
   Iliya S, 2016, INT J NEURAL SYST, V26, DOI 10.1142/S0129065716500234
   Ionescu M, 2006, FUND INFORM, V71, P279
   Ionescu M, 2008, COMPUT INFORM, V27, P515
   Ishdorj TO, 2010, THEOR COMPUT SCI, V411, P2345, DOI 10.1016/j.tcs.2010.01.019
   Ishdorj Tseren-Onolt, 2008, Natural Computing, V7, P519, DOI 10.1007/s11047-008-9081-0
   Knieling S, 2016, INT J NEURAL SYST, V26, DOI 10.1142/S0129065715500422
   Korec I, 1996, THEOR COMPUT SCI, V168, P267, DOI 10.1016/S0304-3975(96)00080-1
   Leporati A, 2009, INT J UNCONV COMPUT, V5, P459
   Luo C, 2015, INT J NEURAL SYST, V25, DOI 10.1142/S0129065715500276
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Minsky ML., 1967, COMPUTATION FINITE I
   Mohemmed A, 2012, INT J NEURAL SYST, V22, DOI 10.1142/S0129065712500128
   Montani F, 2017, INT J NEURAL SYST, V27, DOI 10.1142/S012906571650009X
   Morro A, 2018, IEEE T NEUR NET LEAR, V29, P1371, DOI 10.1109/TNNLS.2017.2657601
   Nichols E, 2010, INT J NEURAL SYST, V20, P501, DOI 10.1142/S0129065710002577
   Pan LQ, 2017, INT J NEURAL SYST, V27, DOI 10.1142/S0129065717500423
   Pan LQ, 2011, SCI CHINA INFORM SCI, V54, P1596, DOI 10.1007/s11432-011-4303-y
   Paun A, 2007, BIOSYSTEMS, V90, P48, DOI 10.1016/j.biosystems.2006.06.006
   Paun G, 1995, LECT NOTES COMPUT SC, V944, P429
   Päun G, 2000, J COMPUT SYST SCI, V61, P108, DOI 10.1006/jcss.1999.1693
   Paun Gh, 2010, OXFORD HDB MEMBRANE
   Paun Gh., 2012, MEMBRANE COMPUTING I
   Paun G, 2006, INT J FOUND COMPUT S, V17, P975, DOI 10.1142/S0129054106004212
   Peng H, 2013, INFORM SCIENCES, V235, P106, DOI 10.1016/j.ins.2012.07.015
   Pigou L, 2015, LECT NOTES COMPUT SC, V8925, P572, DOI 10.1007/978-3-319-16178-5_40
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Rigos A, 2016, INTEGR COMPUT-AID E, V23, P141, DOI 10.3233/ICA-150507
   Rosselló JL, 2016, INT J NEURAL SYST, V26, DOI 10.1142/S0129065715500367
   Rosselló JL, 2014, INT J NEURAL SYST, V24, DOI 10.1142/S0129065714300034
   Schliebs S, 2010, INT J NEURAL SYST, V20, P481, DOI 10.1142/S0129065710002565
   Schrauwen B, 2008, NEURAL NETWORKS, V21, P511, DOI 10.1016/j.neunet.2007.12.009
   Shapero S, 2014, INT J NEURAL SYST, V24, DOI 10.1142/S0129065714400012
   SIEGELMANN HT, 1995, J COMPUT SYST SCI, V50, P132, DOI 10.1006/jcss.1995.1013
   Song T, 2016, INFORM SCIENCES, V372, P380, DOI 10.1016/j.ins.2016.08.055
   Song T, 2012, IEEE T NANOBIOSCI, V11, P352, DOI 10.1109/TNB.2012.2208122
   Song T, 2013, INFORM SCIENCES, V219, P197, DOI 10.1016/j.ins.2012.07.023
   Strain TJ, 2010, INT J NEURAL SYST, V20, P463, DOI 10.1142/S0129065710002553
   Turing AM, 1937, P LOND MATH SOC, V42, P230, DOI 10.1112/plms/s2-42.1.230
   Verstraeten D., 2005, P 16 ANN PRORISC WOR, P454
   Wang J, 2010, NEURAL COMPUT, V22, P2615, DOI 10.1162/NECO_a_00022
   Wang NM, 2015, ENG APPL ARTIF INTEL, V41, P249, DOI 10.1016/j.engappai.2015.01.018
   Wang T, 2015, IEEE T POWER SYST, V30, P1182, DOI 10.1109/TPWRS.2014.2347699
   Wang ZZ, 2014, INT J NEURAL SYST, V24, DOI 10.1142/S0129065714400048
   Wu TF, 2018, IEEE T NEUR NET LEAR, V29, P3349, DOI 10.1109/TNNLS.2017.2726119
   Xu ZH, 2014, FUND INFORM, V134, P183, DOI 10.3233/FI-2014-1098
   Yousif Yahya A.Q., 2016, J F INTELL LEARN SYS, V8, P77
   Zeinali Y, 2017, INTEGR COMPUT-AID E, V24, P105, DOI 10.3233/ICA-170540
   Zhang GX, 2014, INT J NEURAL SYST, V24, DOI 10.1142/S0129065714400061
NR 74
TC 90
Z9 90
U1 3
U2 38
PD OCT
PY 2018
VL 28
IS 8
AR 1850013
DI 10.1142/S0129065718500132
UT WOS:000442776900004
DA 2023-11-16
ER

PT C
AU Alomar, ML
   Canals, V
   Morro, A
   Oliver, A
   Rossello, JL
AF Alomar, Miquel L.
   Canals, Vincent
   Morro, Antoni
   Oliver, Antoni
   Rossello, Josep L.
GP IEEE
TI Stochastic Hardware Implementation of Liquid State Machines
SO 2016 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUL 24-29, 2016
CL Vancouver, CANADA
DE Spiking Neural Networks; Recurrent Neural Networks; Liquid State
   Machines; Stochastic Computing; Field Programmable Gate Array
ID PATTERN-RECOGNITION; COMPACT HARDWARE; SPIKING NEURONS; NETWORK
AB The hardware implementation of neural network models allows to efficiently exploit their inherent parallelism. Here, we focus on the Liquid State Machine (LSM) methodology to build recurrent Spiking Neural Networks (SNN), particularly suited to process time-dependent signals. We propose a low cost hardware implementation of LSM networks based on the use of stochastic computing (SC) concepts. The functionality of the present approach is demonstrated for a time-series prediction task.
C1 [Alomar, Miquel L.; Canals, Vincent; Morro, Antoni; Oliver, Antoni; Rossello, Josep L.] Univ Balear Isl, Phys Dept, Elect Engn Grp, Palma de Mallorca, Spain.
RP Canals, V (corresponding author), Univ Balear Isl, Phys Dept, Elect Engn Grp, Palma de Mallorca, Spain.
EM v.canals@uib.es
CR Alomar ML, 2016, COMPUT INTEL NEUROSC, V2016, DOI 10.1155/2016/3917892
   Alomar ML, 2015, IEEE T CIRCUITS-II, V62, P977, DOI 10.1109/TCSII.2015.2458071
   Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   Burgsteiner H, 2007, APPL INTELL, V26, P99, DOI 10.1007/s10489-006-0007-1
   Friedrich J, 2014, INT J NEURAL SYST, V24, DOI 10.1142/S0129065714500026
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Goodman E, 2006, IEEE IJCNN, P3848
   Grzyb Beata J., 2009, Proceedings 2009 International Joint Conference on Neural Networks (IJCNN 2009 - Atlanta), P1011, DOI 10.1109/IJCNN.2009.5179025
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   Koch C., 2004, COMPUTATIONAL NEUROS
   Larger L, 2012, OPT EXPRESS, V20, P3241, DOI 10.1364/OE.20.003241
   London M, 2010, NATURE, V466, P123, DOI 10.1038/nature09086
   Lukosevicius M, 2009, COMPUT SCI REV, V3, P127, DOI 10.1016/j.cosrev.2009.03.005
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Pape L, 2008, STUD COMPUT INTELL, V83, P191, DOI 10.1007/978-3-540-75398-8_9
   Rodan A, 2011, IEEE T NEURAL NETWOR, V22, P131, DOI 10.1109/TNN.2010.2089641
   Rosselló JL, 2008, IEICE ELECTRON EXPR, V5, P921, DOI 10.1587/elex.5.921
   Rosselló JL, 2014, INT J NEURAL SYST, V24, DOI 10.1142/S0129065714300034
   Rosselló JL, 2012, INT J NEURAL SYST, V22, DOI 10.1142/S0129065712500141
   Rossello JL, 2009, INT J NEURAL SYST, V19, P465, DOI 10.1142/S0129065709002166
   SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P243, DOI 10.1162/neco.1992.4.2.243
   Schrauwen B, 2008, NEURAL NETWORKS, V21, P511, DOI 10.1016/j.neunet.2007.12.009
   Schrauwen B, 2007, IEEE IJCNN, P1097, DOI 10.1109/IJCNN.2007.4371111
   Siqueira H, 2014, INT J NEURAL SYST, V24, DOI 10.1142/S0129065714300095
   Vandoorne K, 2008, OPT EXPRESS, V16, P11182, DOI 10.1364/OE.16.011182
   Verstraeten D, 2007, NEURAL NETWORKS, V20, P391, DOI 10.1016/j.neunet.2007.04.003
   Verstraeten D, 2006, IEEE IJCNN, P1050
   Wojcik GM, 2004, NEUROCOMPUTING, V58, P245, DOI 10.1016/j.neucom.2004.01.051
   Wysoski SG, 2008, NEUROCOMPUTING, V71, P2563, DOI 10.1016/j.neucom.2007.12.038
   Zuppicich A., 2008, ADV NEUR PROC 15 I 1, P1129
NR 32
TC 8
Z9 8
U1 0
U2 3
PY 2016
BP 1128
EP 1133
UT WOS:000399925501041
DA 2023-11-16
ER

PT J
AU Zhang, Y
   Wang, ZR
   Zhu, JD
   Yang, YC
   Rao, MY
   Song, WH
   Zhuo, Y
   Zhang, XM
   Cui, ML
   Shen, LL
   Huang, R
   Yang, JJ
AF Zhang, Yang
   Wang, Zhongrui
   Zhu, Jiadi
   Yang, Yuchao
   Rao, Mingyi
   Song, Wenhao
   Zhuo, Ye
   Zhang, Xumeng
   Cui, Menglin
   Shen, Linlin
   Huang, Ru
   Joshua Yang, J.
TI Brain-inspired computing with memristors: Challenges in devices,
   circuits, and systems
SO APPLIED PHYSICS REVIEWS
DT Review
ID RESISTIVE-SWITCHING MEMORY; PHASE-CHANGE MEMORY; SPIKING NEURAL-NETWORK;
   RANDOM-ACCESS MEMORY; SYNAPSE DEVICE; CONDUCTANCE LINEARITY;
   FEATURE-EXTRACTION; CROSSBAR ARRAYS; COMPACT MODEL; MECHANISMS
AB This article provides a review of current development and challenges in brain-inspired computing with memristors. We review the mechanisms of various memristive devices that can mimic synaptic and neuronal functionalities and survey the progress of memristive spiking and artificial neural networks. Different architectures are compared, including spiking neural networks, fully connected artificial neural networks, convolutional neural networks, and Hopfield recurrent neural networks. Challenges and strategies for nanoelectronic brain-inspired computing systems, including device variations, training, and testing algorithms, are also discussed.
C1 [Zhang, Yang; Shen, Linlin] Shenzhen Univ, Sch Comp Sci & Software Engn, Shenzhen 518060, Guangdong, Peoples R China.
   [Zhang, Yang; Wang, Zhongrui; Rao, Mingyi; Song, Wenhao; Zhuo, Ye; Zhang, Xumeng; Joshua Yang, J.] Univ Massachusetts, Dept Elect & Comp Engn, Amherst, MA 01003 USA.
   [Zhu, Jiadi; Yang, Yuchao; Huang, Ru] Peking Univ, Inst Microelect, Key Lab Microelect Devices & Circuits MOE, Beijing 100871, Peoples R China.
   [Zhang, Xumeng] Chinese Acad Sci, Inst Microelect, Key Lab Microelect Device & Integrated Technol, Beijing 100029, Peoples R China.
   [Zhang, Xumeng] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
   [Cui, Menglin] Univ Nottingham, Sch Comp Sci, Ningbo 315100, Zhejiang, Peoples R China.
RP Yang, JJ (corresponding author), Univ Massachusetts, Dept Elect & Comp Engn, Amherst, MA 01003 USA.
EM jjyang@umass.edu
CR Aamir SA, 2018, IEEE T BIOMED CIRC S, V12, P1027, DOI 10.1109/TBCAS.2018.2848203
   Abdalla H, 2011, IEEE INT SYMP CIRC S, P1832, DOI 10.1109/ISCAS.2011.5937942
   Ahn CY, 2015, NANO LETT, V15, P6809, DOI 10.1021/acs.nanolett.5b02661
   Akinaga H, 2010, P IEEE, V98, P2237, DOI 10.1109/JPROC.2010.2070830
   Alibart F, 2013, NAT COMMUN, V4, DOI 10.1038/ncomms3072
   Ambrogio S, 2018, NATURE, V558, P60, DOI 10.1038/s41586-018-0180-5
   [Anonymous], 2008, SCIDAC REV
   [Anonymous], 2017, EUR J TRANSL MYOL, V27, p7X, DOI DOI 10.1002/ADFM.201604740
   [Anonymous], 2010, PHYS LETT, DOI DOI 10.1063/1.3524521
   [Anonymous], 2018, BNJ M, DOI DOI 10.1002/ADFM.201705783
   [Anonymous], 2009, IEEE SPECTRUM
   [Anonymous], 2019, BIOL INSPIRED ALTERN
   [Anonymous], 2018, BNJ M, DOI DOI 10.1002/ADFM.201704862
   [Anonymous], 2018, DIABETES CARE S1, V4, pS4, DOI [10.2337/dc18-Srev01, DOI 10.1002/AELM.201800223]
   Aono M, 2010, P IEEE, V98, P2228, DOI 10.1109/JPROC.2010.2061830
   Arndt B, 2017, ADV FUNCT MATER, V27, DOI 10.1002/adfm.201702282
   Baek K, 2017, NANOSCALE, V9, P582, DOI 10.1039/c6nr06293h
   Baeumer C, 2019, NANO LETT, V19, P54, DOI 10.1021/acs.nanolett.8b03023
   Bayat FM, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-04482-4
   Berzina T, 2009, J APPL PHYS, V105, DOI 10.1063/1.3153944
   Bessonov AA, 2015, NAT MATER, V14, P199, DOI [10.1038/nmat4135, 10.1038/NMAT4135]
   Bi GQ, 2001, ANNU REV NEUROSCI, V24, P139, DOI 10.1146/annurev.neuro.24.1.139
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Bocquet M, 2014, IEEE T ELECTRON DEV, V61, P674, DOI 10.1109/TED.2013.2296793
   Bocquet M, 2011, APPL PHYS LETT, V98, DOI 10.1063/1.3605591
   Bohnstingl T, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00483
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Boniardi M, 2009, J APPL PHYS, V105, DOI 10.1063/1.3109063
   Burr GW, 2015, IEEE T ELECTRON DEV, V62, P3498, DOI 10.1109/TED.2015.2439635
   Chanthbouala A, 2012, NAT MATER, V11, P860, DOI [10.1038/NMAT3415, 10.1038/nmat3415]
   Chen JY, 2015, ADV MATER, V27, P5028, DOI 10.1002/adma.201502758
   Chen JY, 2013, NANO LETT, V13, P3671, DOI 10.1021/nl4015638
   Chen PY, 2015, IEEE T ELECTRON DEV, V62, P4022, DOI 10.1109/TED.2015.2492421
   Cheng YT, 2006, INT CONF NANO MICRO, P1
   Choi BJ, 2016, ADV FUNCT MATER, V26, P5290, DOI 10.1002/adfm.201600680
   Choi SJ, 2011, APPL PHYS A-MATER, V102, P1019, DOI 10.1007/s00339-011-6282-7
   Choi S, 2017, NANO LETT, V17, P3113, DOI 10.1021/acs.nanolett.7b00552
   Chu M, 2015, IEEE T IND ELECTRON, V62, P2410, DOI 10.1109/TIE.2014.2356439
   CHUA LO, 1971, IEEE T CIRCUITS SYST, VCT18, P507, DOI 10.1109/TCT.1971.1083337
   Covi E, 2016, IEEE INT SYMP CIRC S, P393, DOI 10.1109/ISCAS.2016.7527253
   Dai YT, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-017-02527-8
   Dongale TD, 2015, ELECTRON MATER LETT, V11, P944, DOI 10.1007/s13391-015-4180-4
   Ebong IE, 2012, P IEEE, V100, P2050, DOI 10.1109/JPROC.2011.2173089
   Eryilmaz S. B., 2013, IEEE INT EL DEV M IE, P25
   Eryilmaz SB, 2014, FRONT NEUROSCI-SWITZ, V8, DOI 10.3389/fnins.2014.00205
   Fong SW, 2017, IEEE T ELECTRON DEV, V64, P4374, DOI 10.1109/TED.2017.2746342
   Gao LG, 2016, IEEE ELECTR DEVICE L, V37, P870, DOI 10.1109/LED.2016.2573140
   Garcia V, 2014, NAT COMMUN, V5, DOI 10.1038/ncomms5289
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   González-Cordero G, 2016, SEMICOND SCI TECH, V31, DOI 10.1088/0268-1242/31/11/115013
   Graves A., 2013, PREPRINT ARXIV 1308
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947
   Guan XM, 2012, IEEE ELECTR DEVICE L, V33, P1405, DOI 10.1109/LED.2012.2210856
   Guan XM, 2012, IEEE T ELECTRON DEV, V59, P1172, DOI 10.1109/TED.2012.2184545
   Guo X, 2007, APPL PHYS LETT, V91, DOI 10.1063/1.2793686
   He HK, 2018, SMALL, V14, DOI 10.1002/smll.201800079
   Hecht-Nielsen R., 1989, IJCNN: International Joint Conference on Neural Networks (Cat. No.89CH2765-6), P593, DOI 10.1109/IJCNN.1989.118638
   Herpers A, 2014, ADV MATER, V26, P2730, DOI 10.1002/adma.201304054
   HICKMOTT TW, 1962, J APPL PHYS, V33, P2669, DOI 10.1063/1.1702530
   Higgins I, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0180174
   HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554
   Hu M, 2018, ADV MATER, V30, DOI 10.1002/adma.201705914
   Hu M, 2011, ASIA S PACIF DES AUT, DOI 10.1109/ASPDAC.2011.5722193
   Hu SG, 2015, NAT COMMUN, V6, DOI 10.1038/ncomms8522
   Hu XF, 2017, NEUROCOMPUTING, V223, P129, DOI 10.1016/j.neucom.2016.10.028
   Ielmini D, 2016, IEEE INT SYMP CIRC S, P1386, DOI 10.1109/ISCAS.2016.7527508
   Ielmini D, 2016, SEMICOND SCI TECH, V31, DOI 10.1088/0268-1242/31/6/063002
   Ielmini D, 2011, PHASE TRANSIT, V84, P570, DOI 10.1080/01411594.2011.561478
   Inoue IH, 2008, PHYS REV B, V77, DOI 10.1103/PhysRevB.77.035105
   Jaiswal A, 2017, IEEE T ELECTRON DEV, V64, P1818, DOI 10.1109/TED.2017.2671353
   James AP, 2017, IEEE T BIOMED CIRC S, V11, P640, DOI 10.1109/TBCAS.2016.2641983
   Jeong DS, 2012, REP PROG PHYS, V75, DOI 10.1088/0034-4885/75/7/076502
   Jeyasingh R, 2014, NANO LETT, V14, P3419, DOI 10.1021/nl500940z
   Ji X, 2019, AIP CONF PROC, V2073, DOI 10.1063/1.5090748
   Jiang H, 2018, NAT ELECTRON, V1, P548, DOI 10.1038/s41928-018-0146-5
   Jiang H, 2016, SCI REP-UK, V6, DOI 10.1038/srep19547
   Jiang ZZ, 2016, IEEE T ELECTRON DEV, V63, P1884, DOI 10.1109/TED.2016.2545412
   Jo SH, 2010, NANO LETT, V10, P1297, DOI 10.1021/nl904092h
   Jo SH, 2009, NANO LETT, V9, P870, DOI 10.1021/nl8037689
   Junsangsri P, 2013, IEEE T NANOTECHNOL, V12, P71, DOI 10.1109/TNANO.2012.2229715
   Kang K, 2015, 2015 8TH INTERNATIONAL CONFERENCE ON SIGNAL PROCESSING, IMAGE PROCESSING AND PATTERN RECOGNITION (SIP), P17, DOI 10.1109/SIP.2015.12
   Kasabov NK, 2014, NEURAL NETWORKS, V52, P62, DOI 10.1016/j.neunet.2014.01.006
   Kim DJ, 2012, NANO LETT, V12, P5697, DOI 10.1021/nl302912t
   Kim KM, 2011, NANOTECHNOLOGY, V22, DOI 10.1088/0957-4484/22/25/254002
   Kim S, 2013, SCI REP-UK, V3, DOI 10.1038/srep01680
   Krestinskaya O, 2019, IEEE T CIRCUITS-I, V66, P719, DOI 10.1109/TCSI.2018.2866510
   Krishnan K, 2016, ADV MATER, V28, P640, DOI 10.1002/adma.201504202
   Kumar S, 2017, NATURE, V548, P318, DOI 10.1038/nature23307
   Kumar S, 2016, ACS NANO, V10, P11205, DOI 10.1021/acsnano.6b06275
   Kvatinsky S, 2015, IEEE T CIRCUITS-II, V62, P786, DOI 10.1109/TCSII.2015.2433536
   Kvatinsky S, 2013, IEEE T CIRCUITS-I, V60, P211, DOI 10.1109/TCSI.2012.2215714
   Kwon DH, 2010, NAT NANOTECHNOL, V5, P148, DOI [10.1038/nnano.2009.456, 10.1038/NNANO.2009.456]
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lee BS, 2009, SCIENCE, V326, P980, DOI 10.1126/science.1177483
   Lee J, 2018, ADV MATER, V30, DOI 10.1002/adma.201702770
   Lee MJ, 2011, NAT MATER, V10, P625, DOI [10.1038/NMAT3070, 10.1038/nmat3070]
   Lee TH, 2011, PHYS REV LETT, V107, DOI 10.1103/PhysRevLett.107.145702
   Li C, 2019, NAT MACH INTELL, V1, P49, DOI 10.1038/s42256-018-0001-4
   Li C, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351877
   Li C, 2017, NAT COMMUN, V8, DOI 10.1038/ncomms14242
   Li WQ, 2016, APPL PHYS LETT, V108, DOI 10.1063/1.4945982
   Lim S, 2018, IEEE ELECTR DEVICE L, V39, P312, DOI 10.1109/LED.2018.2789425
   Lin J., 2016, IEEE INT EL DEV M IE, P34
   Lin KL, 2011, J APPL PHYS, V109, DOI 10.1063/1.3567915
   Liu CY, 2012, JPN J APPL PHYS, V51, DOI 10.1143/JJAP.51.041101
   Liu KQ, 2019, FARADAY DISCUSS, V213, P41, DOI 10.1039/c8fd00113h
   Liu Q, 2010, ACS NANO, V4, P6162, DOI 10.1021/nn1017582
   Locatelli N, 2014, NAT MATER, V13, P11, DOI [10.1038/NMAT3823, 10.1038/nmat3823]
   Loke D, 2012, SCIENCE, V336, P1566, DOI 10.1126/science.1221561
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Midya R, 2019, ADV ELECTRON MATER, V5, DOI 10.1002/aelm.201900060
   Mikolov T, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2, P1045
   Milo V., 2017, 2017 IEEE INT EL DEV, P11
   Moon K, 2017, IEEE ELECTR DEVICE L, V38, P1023, DOI 10.1109/LED.2017.2721638
   Nardi F, 2012, IEEE T ELECTRON DEV, V59, P2461, DOI 10.1109/TED.2012.2202319
   Niu DM, 2010, DES AUT CON, P877
   Noé P, 2018, SEMICOND SCI TECH, V33, DOI 10.1088/1361-6641/aa7c25
   Oh S, 2017, IEEE ELECTR DEVICE L, V38, P732, DOI 10.1109/LED.2017.2698083
   Ohno T, 2011, NAT MATER, V10, P591, DOI [10.1038/NMAT3054, 10.1038/nmat3054]
   Onofrio N, 2015, NAT MATER, V14, P440, DOI [10.1038/nmat4221, 10.1038/NMAT4221]
   Pan F, 2014, MAT SCI ENG R, V83, P1, DOI 10.1016/j.mser.2014.06.002
   Pantazi A, 2016, NANOTECHNOLOGY, V27, DOI 10.1088/0957-4484/27/35/355205
   Park S, 2015, SCI REP-UK, V5, DOI 10.1038/srep10123
   Park S, 2011, PHYS STATUS SOLIDI-R, V5, P409, DOI 10.1002/pssr.201105317
   Pei J, 2019, NATURE, V572, P106, DOI 10.1038/s41586-019-1424-8
   Pi S, 2019, NAT NANOTECHNOL, V14, P35, DOI 10.1038/s41565-018-0302-0
   Pickett MD, 2013, NAT MATER, V12, P114, DOI [10.1038/nmat3510, 10.1038/NMAT3510]
   Pickett MD, 2009, J APPL PHYS, V106, DOI 10.1063/1.3236506
   Pirovano A, 2004, IEEE T ELECTRON DEV, V51, P714, DOI 10.1109/TED.2004.825805
   Pokorny C, 2020, CEREB CORTEX, V30, P952, DOI 10.1093/cercor/bhz140
   Prezioso M, 2015, NATURE, V521, P61, DOI 10.1038/nature14441
   Querlioz D, 2013, IEEE T NANOTECHNOL, V12, P288, DOI 10.1109/TNANO.2013.2250995
   Rao F, 2017, SCIENCE, V358, P1423, DOI 10.1126/science.aao3212
   Raoux S, 2008, IBM J RES DEV, V52, P465, DOI 10.1147/rd.524.0465
   Russo U, 2009, IEEE T ELECTRON DEV, V56, P186, DOI 10.1109/TED.2008.2010583
   Salinga M, 2018, NAT MATER, V17, P681, DOI 10.1038/s41563-018-0110-9
   Salinga M, 2013, NAT COMMUN, V4, DOI 10.1038/ncomms3371
   Sarwat SG, 2017, MATER SCI TECH-LOND, V33, P1890, DOI 10.1080/02670836.2017.1341723
   Schemmel J, 2017, IEEE IJCNN, P2217, DOI 10.1109/IJCNN.2017.7966124
   Schindler C, 2009, APPL PHYS LETT, V94, DOI 10.1063/1.3077310
   Serb A, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms12611
   Shao XL, 2016, NANOSCALE, V8, P16455, DOI 10.1039/c6nr02800d
   Sheridan PM, 2016, IEEE T NEUR NET LEAR, V27, P2327, DOI 10.1109/TNNLS.2015.2482220
   Shi YZ, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-03156-5
   Shima H, 2007, APPL PHYS LETT, V91, DOI 10.1063/1.2753101
   SIMMONS JG, 1967, PROC R SOC LON SER-A, V301, P77, DOI 10.1098/rspa.1967.0191
   Truong SN, 2018, IEEE T NANOTECHNOL, V17, P482, DOI 10.1109/TNANO.2018.2815624
   Truong SN, 2014, NANOSCALE RES LETT, V9, P1, DOI 10.1186/1556-276X-9-629
   Truong SN, 2014, J SEMICOND TECH SCI, V14, P356, DOI 10.5573/JSTS.2014.14.3.356
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Soudry D., 2013, HEBBIAN LEARNING RUL
   STEINBUCH K, 1961, KYBERNETIK, V1, P36, DOI 10.1007/BF00293853
   Strachan JP, 2013, IEEE T ELECTRON DEV, V60, P2194, DOI 10.1109/TED.2013.2264476
   Strachan JP, 2010, ADV MATER, V22, P3573, DOI 10.1002/adma.201000186
   Strukov DB, 2008, NATURE, V453, P80, DOI 10.1038/nature06932
   Strukov DB, 2009, APPL PHYS A-MATER, V94, P515, DOI 10.1007/s00339-008-4975-3
   Sun WH, 2019, NAT COMMUN, V10, DOI 10.1038/s41467-019-08494-6
   Sung C, 2018, NANOTECHNOLOGY, V29, DOI 10.1088/1361-6528/aaa733
   Suri M, 2012, J APPL PHYS, V112, DOI 10.1063/1.4749411
   Tamura T, 2007, J PHYS CONF SER, V61, P1157, DOI 10.1088/1742-6596/61/1/229
   Tan ZH, 2016, ADV MATER, V28, P377, DOI 10.1002/adma.201503575
   Tang JS, 2019, ADV MATER, V31, DOI 10.1002/adma.201902761
   Terabe K, 2005, NATURE, V433, P47, DOI 10.1038/nature03190
   Tian H, 2017, ACS NANO, V11, P12247, DOI 10.1021/acsnano.7b05726
   Tian XZ, 2014, ADV MATER, V26, P3649, DOI 10.1002/adma.201400127
   Torrejon J, 2017, NATURE, V547, P428, DOI 10.1038/nature23011
   Tuma T, 2016, NAT NANOTECHNOL, V11, P693, DOI [10.1038/NNANO.2016.70, 10.1038/nnano.2016.70]
   Valov I, 2017, SEMICOND SCI TECH, V32, DOI 10.1088/1361-6641/aa78cd
   Valov I, 2012, NAT MATER, V11, P530, DOI [10.1038/nmat3307, 10.1038/NMAT3307]
   Valov I, 2011, NANOTECHNOLOGY, V22, DOI 10.1088/0957-4484/22/25/254003
   van ME, 2017, PEDIAT PHYS THER S3, V29, pS73, DOI DOI 10.1002/ADMA.201700951
   van ME, 2017, PEDIAT PHYS THER S3, V29, pS73, DOI DOI 10.1002/ADMA.201606927
   van ME, 2017, PEDIAT PHYS THER S3, V29, pS73, DOI DOI 10.1002/ADMA.201701153
   van ME, 2017, PEDIAT PHYS THER S3, V29, pS73, DOI DOI 10.1002/ADMA.201701333
   van ME, 2017, PEDIAT PHYS THER S3, V29, pS73, DOI DOI 10.1002/ADMA.201703232
   Wang ZQ, 2012, ADV FUNCT MATER, V22, P2759, DOI 10.1002/adfm.201103148
   Wang ZF, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-03243-7
   Wang ZR, 2019, NAT MACH INTELL, V1, P434, DOI 10.1038/s42256-019-0089-1
   Wang ZR, 2019, NAT ELECTRON, V2, P115, DOI 10.1038/s41928-019-0221-6
   Wang ZR, 2017, NAT MATER, V16, P101, DOI [10.1038/nmat4756, 10.1038/NMAT4756]
   Wang ZL, 2018, IEEE T CIRCUITS-I, V65, P2210, DOI 10.1109/TCSI.2017.2780826
   Wang ZW, 2016, NANOSCALE, V8, P14015, DOI 10.1039/c6nr00476h
   Waser R, 2009, ADV MATER, V21, P2632, DOI 10.1002/adma.200900375
   Wedig A, 2016, NAT NANOTECHNOL, V11, P67, DOI [10.1038/NNANO.2015.221, 10.1038/nnano.2015.221]
   Williams RS, 2013, IEEE INT SYMP CIRC S, P217, DOI 10.1109/ISCAS.2013.6571821
   Wong HSP, 2012, P IEEE, V100, P1951, DOI 10.1109/JPROC.2012.2190369
   Wong HSP, 2010, P IEEE, V98, P2201, DOI 10.1109/JPROC.2010.2070050
   Wuttig M., 2006, OPTICAL DATA STORAGE
   Wutting R., 2009, PHASE CHANGE MAT SCI
   Xia QF, 2019, NAT MATER, V18, P309, DOI 10.1038/s41563-019-0291-x
   Xie XD, 2018, NEUROCOMPUTING, V284, P10, DOI 10.1016/j.neucom.2018.01.024
   Xiong F, 2011, SCIENCE, V332, P568, DOI 10.1126/science.1201938
   Yakopcic C, 2017, IEEE IJCNN, P1696, DOI 10.1109/IJCNN.2017.7966055
   Yakopcic C, 2013, IEEE T COMPUT AID D, V32, P1201, DOI 10.1109/TCAD.2013.2252057
   Yang JJ, 2008, NAT NANOTECHNOL, V3, P429, DOI 10.1038/nnano.2008.160
   Yang JJ, 2017, NAT MATER, V16, P396, DOI 10.1038/nmat4870
   Yang JJS, 2013, NAT NANOTECHNOL, V8, P13, DOI [10.1038/nnano.2012.240, 10.1038/NNANO.2012.240]
   Yang JJ, 2012, MRS BULL, V37, P131, DOI 10.1557/mrs.2011.356
   Yang JJ, 2009, NANOTECHNOLOGY, V20, DOI 10.1088/0957-4484/20/21/215201
   Yang YC, 2015, ADV MATER, V27, P7720, DOI 10.1002/adma.201503202
   Yang YC, 2014, NAT COMMUN, V5, DOI 10.1038/ncomms5232
   Yang YC, 2012, NAT COMMUN, V3, DOI 10.1038/ncomms1737
   Yao P, 2017, NAT COMMUN, V8, DOI 10.1038/ncomms15199
   Yoon JH, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-017-02572-3
   Yu SM, 2018, P IEEE, V106, P260, DOI 10.1109/JPROC.2018.2790840
   Yu SM, 2016, INT EL DEVICES MEET
   Yu SM, 2011, IEEE T ELECTRON DEV, V58, P2729, DOI 10.1109/TED.2011.2147791
   Yu SM, 2011, APPL PHYS LETT, V98, DOI 10.1063/1.3564883
   Zeng XF, 2018, NEURAL COMPUT APPL, V30, P503, DOI 10.1007/s00521-016-2700-2
   Zhang W, 2019, NAT REV MATER, V4, P150, DOI 10.1038/s41578-018-0076-x
   Zhang XM, 2018, IEEE ELECTR DEVICE L, V39, P308, DOI 10.1109/LED.2017.2782752
   Zhang Y, 2019, NEUROCOMPUTING, V355, P48, DOI 10.1016/j.neucom.2019.04.031
   Zhang Y, 2021, IEEE T CYBERNETICS, V51, P1875, DOI 10.1109/TCYB.2019.2912205
   Zhang Y, 2018, IEEE T CIRCUITS-I, V65, P677, DOI 10.1109/TCSI.2017.2729787
   Zhang Y, 2017, IEEE T CIRCUITS-II, V64, P767, DOI 10.1109/TCSII.2016.2605069
   Zhang Y, 2017, IEEE T ELECTRON DEV, V64, P1806, DOI 10.1109/TED.2017.2671433
   Zhang Y, 2015, IEEE T CIRCUITS-I, V62, P1402, DOI 10.1109/TCSI.2015.2407436
   Zhao XL, 2018, ADV MATER, V30, DOI [10.1002/adma.201705193, 10.1002/adma.201705189]
   Zhou Y, 2015, P IEEE, V103, P1289, DOI 10.1109/JPROC.2015.2431914
   Zhu JD, 2018, ADV MATER, V30, DOI 10.1002/adma.201800195
   Zidan MA, 2017, IEEE T NANOTECHNOL, V16, P721, DOI 10.1109/TNANO.2017.2710158
   2019, LANCET DIGITAL HLTH, V1, DOI DOI 10.1002/AISY.201900084
NR 222
TC 182
Z9 184
U1 87
U2 564
PD MAR
PY 2020
VL 7
IS 1
AR 011308
DI 10.1063/1.5124027
UT WOS:000515505400003
HC Y
HP N
DA 2023-11-16
ER

PT C
AU Fujii, RH
   Ichishita, T
AF Fujii, Robert H.
   Ichishita, Taiki
BE Wani, MA
   Kantardzic, MM
   Li, T
   Liu, Y
   Kurgan, L
   Ye, J
   Ogihara, M
   Sagiroglu, S
   Chen, XW
   Peterson, L
   Hafeez, K
TI Effect of synaptic weight assignment on spiking neuron response
SO ICMLA 2007: SIXTH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND
   APPLICATIONS, PROCEEDINGS
DT Proceedings Paper
CT 6th International Conference on Machine Learning and Applications
CY DEC 13-15, 2007
CL Cincinnati, OH
AB Three synaptic weight assignment schemes were proposed and their effect on the behavior of a spiking neuron was analyzed To evaluate the proposed synaptic assignment schemes, a feed-forward Spiking Neural Network that can learn to recognize temporal sequences was proposed This spiking neural network uses two of the proposed synaptic weight assignment schemes in conjunction with a spiking neuron model that uses a simplified linear soma potential function. The robustness and reliability of the proposed spiking neural network system were shown by the high (approximately 99%) temporal sequence recognition rate achieved during testing. Practical hardware implementation issues were also discussed.
C1 [Fujii, Robert H.; Ichishita, Taiki] Univ Aizu, Aizu Wakamatsu, Fukushima, Japan.
RP Fujii, RH (corresponding author), Univ Aizu, Aizu Wakamatsu, Fukushima, Japan.
EM fujii@u-aizu.ac.jp
CR Amin H, 2004, IEEE IJCNN, P477, DOI 10.1109/IJCNN.2004.1379956
   AMIN HH, 2005, 48 IEEE INT MIDW S C, P683
   Gerstner W., 2002, SPIKING NEURON MODEL
   GILES CL, 1998, ADAPTIVE PROCESSING
   Haykin S., 1999, NEURAL NETWORKS COMP
   Maass W, 1996, NEURAL COMPUT, V8, P1, DOI 10.1162/neco.1996.8.1.1
   Maass W., 1999, PULSED NEURAL NETWOR
   THORPE SJ, 1990, PARALLEL PROCESSING IN NEURAL SYSTEMS AND COMPUTERS, P91
   WANG DL, 1990, P IEEE, V78, P1536, DOI 10.1109/5.58329
NR 9
TC 0
Z9 0
U1 0
U2 1
PY 2007
BP 217
EP 222
DI 10.1109/ICMLA.2007.16
UT WOS:000252793400036
DA 2023-11-16
ER

PT C
AU Nekhaev, D
   Demin, V
AF Nekhaev, Dmitry
   Demin, Vyacheslav
BE Kryzhanovsky, B
   DuninBarkowski, W
   Redko, V
   Tiumentsev, Y
TI Competitive Maximization of Neuronal Activity in Convolutional Recurrent
   Spiking Neural Networks
SO ADVANCES IN NEURAL COMPUTATION, MACHINE LEARNING, AND COGNITIVE RESEARCH
   III
SE Studies in Computational Intelligence
DT Proceedings Paper
CT 21st International Conference on Neuroinformatics
CY OCT 07-11, 2019
CL Dolgoprudny, RUSSIA
DE Spiking neural networks; Local learning rules
AB Spiking neural networks (SNNs) are the promising algorithm for specific neurochip hardware real-time solutions. SNNs are believed to be highly energy and computationally efficient. We focus on developing local learning rules that are capable to provide both supervised and unsupervised learning. We suppose that each neuron in a biological neural network tends to maximize its activity in competition with other neurons. This principle was put at the basis of SNN learning algorithm called FEELING. Here we introduce efficient Convolutional Recurrent Spiking Neural Network architecture that uses FEELING rules and provides better results than fully connected SNN on MNIST benchmark having 55 times less learnable weight parameters.
C1 [Nekhaev, Dmitry; Demin, Vyacheslav] Natl Res Ctr, Kurchatov Inst, Moscow, Russia.
RP Nekhaev, D (corresponding author), Natl Res Ctr, Kurchatov Inst, Moscow, Russia.
EM nekhaev.d.v@yandex.ru
CR Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Demin V, 2018, FRONT NEUROINFORM, V12, DOI 10.3389/fninf.2018.00079
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee JM, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00191
   Maass W., 1999, PULSED NEURAL NETWOR, P275
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Nekhaev D, 2017, PROCEDIA COMPUT SCI, V119, P174, DOI 10.1016/j.procs.2017.11.174
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   Zhao B, 2015, IEEE T NEUR NET LEAR, V26, P1963, DOI 10.1109/TNNLS.2014.2362542
NR 11
TC 1
Z9 1
U1 0
U2 4
PY 2020
VL 856
BP 255
EP 262
DI 10.1007/978-3-030-30425-6_30
UT WOS:000547332800030
DA 2023-11-16
ER

PT J
AU Masumori, A
   Ikegami, T
AF Masumori, Atsushi
   Ikegami, Takashi
TI Spiking neural networks produce informational closure by stimulus
   avoidance
SO BIOSYSTEMS
DT Article
DE Stimulus avoidance; Informational closure; Spiking neural networks;
   Spike-timing dependent plasticity; Autopoiesis; Autonomy; Homeostasis
ID SELF; MODEL
AB The concept of Learning by Stimulus Avoidance (LSA) has been proposed in recent literature, and the methods of avoiding stimuli: action, prediction, and separation appear to align well with the formation of Bertschinger's informational closure. In this study, we provide experimental evidence demonstrating that spiking neural networks, which avoid stimuli, can indeed facilitate the emergence of informational closure. The established link between LSA and informational closure lays the foundation for further exploration of autopoietic relationships and the self-organization of closure within neural networks.
C1 [Masumori, Atsushi] Univ Tokyo, Tokyo, Japan.
   Alternat Machine Inc, Tokyo, Japan.
RP Masumori, A (corresponding author), Univ Tokyo, Tokyo, Japan.
EM masumori@sacral.c.u-tokyo.ac.jp
CR Bertschinger N., 2006, P 7 GERM WORKSH ART, P26
   Bruineberg J, 2022, BEHAV BRAIN SCI, V45, DOI 10.1017/S0140525X21002351
   Cassenaer S, 2007, NATURE, V448, P709, DOI 10.1038/nature05973
   Chang AYC, 2020, FRONT PSYCHOL, V11, DOI 10.3389/fpsyg.2020.01504
   Clark A, 1998, ANALYSIS, V58, P7, DOI 10.1111/1467-8284.00096
   Drew PJ, 2006, P NATL ACAD SCI USA, V103, P8876, DOI 10.1073/pnas.0600676103
   Friston K, 2013, J R SOC INTERFACE, V10, DOI 10.1098/rsif.2013.0475
   Friston KJ, 2012, FRONT PSYCHOL, V3, DOI 10.3389/fpsyg.2012.00130
   Friston KJ, 2010, NAT REV NEUROSCI, V11, P127, DOI 10.1038/nrn2787
   Frith CD, 2000, PHILOS T R SOC B, V355, P1771, DOI 10.1098/rstb.2000.0734
   Froese T, 2013, BEHAV BRAIN SCI, V36, P213, DOI 10.1017/S0140525X12002348
   Gallagher S, 2000, TRENDS COGN SCI, V4, P14, DOI 10.1016/S1364-6613(99)01417-5
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   LUISI PL, 1989, ORIGINS LIFE EVOL B, V19, P633, DOI 10.1007/BF01808123
   Masumori A, 2021, FRONT ROBOT AI, V7, DOI 10.3389/frobt.2020.532375
   Masumori A, 2020, ARTIF LIFE, V26, P130, DOI 10.1162/artl_a_00314
   Masumori A, 2019, 2019 IEEE SYMPOSIUM SERIES ON COMPUTATIONAL INTELLIGENCE (IEEE SSCI 2019), P271, DOI 10.1109/SSCI44817.2019.9003066
   Masumori A, 2018, 2018 CONFERENCE ON ARTIFICIAL LIFE (ALIFE 2018), P163
   Mongillo G, 2008, SCIENCE, V319, P1543, DOI 10.1126/science.1150769
   Schreiber T, 2000, PHYS REV LETT, V85, P461, DOI 10.1103/PhysRevLett.85.461
   Sethi AK, 2012, FRONT PSYCHOL, V3, DOI 10.3389/fpsyg.2011.00395
   Sinapayen L, 2015, ECAL 2015: THE THIRTEENTH EUROPEAN CONFERENCE ON ARTIFICIAL LIFE, P175, DOI 10.7551/978-0-262-33027-5-ch037
   Sinapayen L, 2020, FRONT COMPUT NEUROSC, V13, DOI 10.3389/fncom.2019.00088
   Sinapayen L, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0170388
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Suzuki K, 2022, BEHAV BRAIN SCI, V45, DOI 10.1017/S0140525X22000206
   Varela F. J., 1979, PRINCIPLES BIOL AUTO
   WALDE P, 1994, J AM CHEM SOC, V116, P11649, DOI 10.1021/ja00105a004
NR 29
TC 1
Z9 1
U1 0
U2 0
PD OCT
PY 2023
VL 232
AR 104972
DI 10.1016/j.biosystems.2023.104972
UT WOS:001067070800001
DA 2023-11-16
ER

PT J
AU Ghosh-Dastidar, S
   Adeli, H
AF Ghosh-Dastidar, Samanwoy
   Adeli, Hojjat
TI SPIKING NEURAL NETWORKS
SO INTERNATIONAL JOURNAL OF NEURAL SYSTEMS
DT Review
DE Spiking neuron; Spiking Neural Network; learning algorithm; information
   encoding; unsupervised learning; supervised learning
ID GRADIENT LEARNING ALGORITHM; WAVELET-CHAOS METHODOLOGY; WORK ZONE
   CAPACITY; FUZZY-LOGIC MODEL; INCIDENT-DETECTION; COST OPTIMIZATION;
   GENETIC ALGORITHM; TRAFFIC FLOW; STRUCTURAL OPTIMIZATION;
   ERROR-BACKPROPAGATION
AB Most current Artificial Neural Network (ANN) models are based on highly simplified brain dynamics. They have been used as powerful computational tools to solve complex pattern recognition, function estimation, and classification problems. ANNs have been evolving towards more powerful and more biologically realistic models. In the past decade, Spiking Neural Networks (SNNs) have been developed which comprise of spiking neurons. Information transfer in these neurons mimics the information transfer in biological neurons, i.e., via the precise timing of spikes or a sequence of spikes. To facilitate learning in such networks, new learning algorithms based on varying degrees of biological plausibility have also been developed recently. Addition of the temporal dimension for information encoding in SNNs yields new insight into the dynamics of the human brain and could result in compact representations of large neural networks. As such, SNNs have great potential for solving complicated time-dependent pattern recognition problems because of their inherent dynamic representation. This article presents a state-of-the-art review of the development of spiking neurons and SNNs, and provides insight into their evolution as the third generation neural networks.
C1 [Ghosh-Dastidar, Samanwoy; Adeli, Hojjat] Ohio State Univ, Dept Biomed Engn, Columbus, OH 43210 USA.
   [Adeli, Hojjat] Ohio State Univ, Dept Biomed Informat Civil & Environm Engn, Columbus, OH 43210 USA.
   [Adeli, Hojjat] Ohio State Univ, Dept Geodet Sci Elect & Comp Engn, Columbus, OH 43210 USA.
   [Adeli, Hojjat] Ohio State Univ, Dept Neurol Surg & Neurosci, Columbus, OH 43210 USA.
RP Ghosh-Dastidar, S (corresponding author), Ohio State Univ, Dept Biomed Engn, Columbus, OH 43210 USA.
CR ABBOTT LF, 1990, STAT MECH NEURAL NET
   ADELI H, 1995, NEURAL NETWORKS, V8, P769, DOI 10.1016/0893-6080(95)00026-V
   Adeli H, 2000, COMPUT-AIDED CIV INF, V15, P251, DOI 10.1111/0885-9507.00189
   Adeli H, 2000, J TRANSP ENG, V126, P464, DOI 10.1061/(ASCE)0733-947X(2000)126:6(464)
   Adeli H, 2006, J STRUCT ENG, V132, P102, DOI 10.1061/(ASCE)0733-9445(2006)132:1(102)
   Adeli H, 2005, J ALZHEIMERS DIS, V7, P187
   Adeli H, 2005, CLIN EEG NEUROSCI, V36, P131, DOI 10.1177/155005940503600303
   ADELI H, 1995, J AEROSPACE ENG, V8, P156, DOI 10.1061/(ASCE)0893-1321(1995)8:3(156)
   ADELI H, 1995, J STRUCT ENG, V121, P1588, DOI 10.1061/(ASCE)0733-9445(1995)121:11(1588)
   Adeli H, 2004, J STRUCT ENG, V130, P128, DOI 10.1061/(ASCE)0733-9445(2004)130:1(128)
   Adeli H, 2003, J TRANSP ENG, V129, P484, DOI 10.1061/(ASCE)0733-947X(2003)129:5(484)
   Adeli H, 2001, COMPUT-AIDED CIV INF, V16, P126, DOI 10.1111/0885-9507.00219
   Adeli H, 2001, COMMUN NUMER METH EN, V17, P771, DOI 10.1002/cnm.448
   Adeli H, 1998, J CONSTR ENG M, V124, P18, DOI 10.1061/(ASCE)0733-9364(1998)124:1(18)
   ADELI H, 1994, APPL MATH COMPUT, V62, P81, DOI 10.1016/0096-3003(94)90134-1
   Adeli H, 1997, J STRUCT ENG-ASCE, V123, P1535, DOI 10.1061/(ASCE)0733-9445(1997)123:11(1535)
   ADELI H, 1993, INT J SUPERCOMPUT AP, V7, P155, DOI 10.1177/109434209300700206
   ADELI H, 1995, J STRUCT ENG-ASCE, V121, P1205, DOI 10.1061/(ASCE)0733-9445(1995)121:8(1205)
   ADELI H, NEURAL NETW IN PRESS, V22
   ADELI H, 1995, MACTINE LEARNING NEU
   Adeli H, 2008, NEUROSCI LETT, V444, P190, DOI 10.1016/j.neulet.2008.08.008
   Adeli H, 2007, IEEE T BIO-MED ENG, V54, P205, DOI 10.1109/TBME.2006.886855
   Ahmadkhanlou F, 2005, ENG APPL ARTIF INTEL, V18, P65, DOI 10.1016/j.engappai.2004.08.025
   [Anonymous], 1998, NEUROCOMPUTING DESIG
   [Anonymous], 1996, NEURAL NETWORK FUNDA
   [Anonymous], 2005, WAVELETS INTELLIGENT
   Banchs RE, 2007, INTEGR COMPUT-AID E, V14, P213
   Belatreche A, 2003, P IEEE CYB INT CHALL, P39
   Belatreche A, 2007, SOFT COMPUT, V11, P239, DOI [10.1007/s00500-006-0065-7, 10.1007/S00500-006-0065-7]
   Bénédic Y, 2008, INT J NEURAL SYST, V18, P293, DOI 10.1142/S0129065708001609
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   Booij O, 2005, INFORM PROCESS LETT, V95, P552, DOI 10.1016/j.ipl.2005.05.023
   Bourbakis N, 2007, INT J NEURAL SYST, V17, P1, DOI 10.1142/S0129065707000920
   Carden EP, 2008, COMPUT-AIDED CIV INF, V23, P360, DOI 10.1111/j.1467-8667.2008.00543.x
   Chakravarthy VS, 2008, INT J NEURAL SYST, V18, P157, DOI 10.1142/S0129065708001464
   Chao LC, 2007, COMPUT-AIDED CIV INF, V22, P449, DOI 10.1111/j.1467-8667.2007.00500.x
   Christodoulou MA, 2008, INT J NEURAL SYST, V18, P371, DOI 10.1142/S0129065708001658
   Dharia A, 2003, ENG APPL ARTIF INTEL, V16, P607, DOI 10.1016/j.engappai.2003.09.011
   Ermentrout B, 1996, NEURAL COMPUT, V8, P979, DOI 10.1162/neco.1996.8.5.979
   ERMENTROUT GB, 1986, SIAM J APPL MATH, V46, P233, DOI 10.1137/0146017
   Fyfe C, 2008, INT J NEURAL SYST, V18, P481, DOI 10.1142/S0129065708001749
   GERSTNER W, 1995, PHYS REV E, V51, P738, DOI 10.1103/PhysRevE.51.738
   Gerstner W, 1996, NATURE, V383, P76, DOI 10.1038/383076a0
   Gerstner W., 2002, SPIKING NEURON MODEL
   Ghosh-Dastidar S, 2006, J TRANSP ENG, V132, P331, DOI 10.1061/(ASCE)0733-947X(2006)132:4(331)
   Ghosh-Dastidar S, 2003, COMPUT-AIDED CIV INF, V18, P325, DOI 10.1111/1467-8667.t01-1-00311
   Ghosh-Dastidar S, 2008, IEEE T BIO-MED ENG, V55, P512, DOI 10.1109/TBME.2007.905490
   Ghosh-Dastidar S, 2007, IEEE T BIO-MED ENG, V54, P1545, DOI 10.1109/TBME.2007.891945
   Ghosh-Dastidar S, 2007, INTEGR COMPUT-AID E, V14, P187
   GHOSHDASTIDAR S, NEURAL NETW IN PRESS, V22
   Gueorguieva N, 2006, J EXP THEOR ARTIF IN, V18, P73, DOI 10.1080/09528130600552888
   Gutkin B, 2003, J PHYSIOLOGY-PARIS, V97, P209, DOI 10.1016/j.jphysparis.2003.09.005
   Gutkin BS, 1998, NEURAL COMPUT, V10, P1047, DOI 10.1162/089976698300017331
   Hagras H, 2004, IEEE INT CONF ROBOT, P4620, DOI 10.1109/ROBOT.2004.1302446
   Hille B, 1992, IONIC CHANNELS EXCIT
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   Hoppensteadt FC, 1997, WEAKLY CONNECTED NEU
   HUNG SL, 1994, IEEE T NEURAL NETWOR, V5, P900, DOI 10.1109/72.329686
   HUNG SL, 1994, NEUROCOMPUTING, V6, P45, DOI 10.1016/0925-2312(94)90033-7
   HUNG SL, 1993, NEUROCOMPUTING, V5, P287, DOI 10.1016/0925-2312(93)90042-2
   Huynh HT, 2008, INT J NEURAL SYST, V18, P433, DOI 10.1142/S0129065708001695
   Iglesias J, 2008, INT J NEURAL SYST, V18, P267, DOI 10.1142/S0129065708001580
   Izhikevich EM, 2001, NEURAL NETWORKS, V14, P883, DOI 10.1016/S0893-6080(01)00078-8
   Izhikevich EM, 2004, CEREB CORTEX, V14, P933, DOI 10.1093/cercor/bhh053
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2007, CEREB CORTEX, V17, P2443, DOI 10.1093/cercor/bhl152
   JAEGER H, 2001, TRGMD148 GERM NAT RE
   Jaegera H, 2007, NEURAL NETWORKS, V20, P335, DOI 10.1016/j.neunet.2007.04.016
   Jiang XM, 2008, INT J NUMER METH ENG, V75, P770, DOI 10.1002/nme.2274
   Jiang XM, 2008, INT J NUMER METH ENG, V74, P1045, DOI 10.1002/nme.2195
   Jiang X, 2007, INT J NUMER METH ENG, V71, P606, DOI 10.1002/nme.1964
   Jiang XM, 2005, J TRANSP ENG, V131, P771, DOI 10.1061/(ASCE)0733-947X(2005)131:10(771)
   Jiang XM, 2005, COMPUT-AIDED CIV INF, V20, P316, DOI 10.1111/j.1467-8667.2005.00399.x
   Jiang XM, 2004, COMPUT-AIDED CIV INF, V19, P324, DOI 10.1111/j.1467-8667.2004.00360.x
   Jiang XM, 2003, INTEGR COMPUT-AID E, V10, P287
   Jin YC, 2007, LECT NOTES COMPUT SC, V4668, P370
   Jorgensen TD, 2008, INT J NEURAL SYST, V18, P389, DOI 10.1142/S012906570800166X
   Kaiser F, 2007, LECT NOTES COMPUT SC, V4668, P380
   Kandel E. R., 2000, PRINCIPLES NEURAL SC, V4
   Karim A, 2003, J TRANSP ENG, V129, P494, DOI 10.1061/(ASCE)0733-947X(2003)129:5(494)
   Karim A, 2003, J TRANSP ENG, V129, P57, DOI 10.1061/(ASCE)0733-947X(2003)129:1(57)
   Karim A, 2002, J TRANSP ENG, V128, P232, DOI 10.1061/(ASCE)0733-947X(2002)128:3(232)
   Karim A, 2002, J TRANSP ENG-ASCE, V128, P21, DOI 10.1061/(ASCE)0733-947X(2002)128:1(21)
   Kasinski A, 2005, LECT NOTES COMPUT SC, V3696, P145, DOI 10.1007/11550822_24
   Katada N, 2007, INT J NEURAL SYST, V17, P161, DOI 10.1142/S0129065707001032
   Kawata S, 2008, INT J NEURAL SYST, V18, P173, DOI 10.1142/S0129065708001488
   KEPLER TB, 1992, BIOL CYBERN, V66, P381, DOI 10.1007/BF00197717
   Kim HJ, 2005, COMPUT-AIDED CIV INF, V20, P7, DOI 10.1111/j.1467-8667.2005.00373.x
   Kistler WM, 1997, NEURAL COMPUT, V9, P1015, DOI 10.1162/neco.1997.9.5.1015
   Kobayashi M, 2008, INT J NEURAL SYST, V18, P147, DOI 10.1142/S0129065708001452
   Lee H, 2007, INTEGR COMPUT-AID E, V14, P161
   Legenstein R, 2005, NEURAL COMPUT, V17, P2337, DOI 10.1162/0899766054796888
   Liu HB, 2007, INT J NEURAL SYST, V17, P459, DOI 10.1142/S0129065707001299
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Maass W, 1997, NETWORK-COMP NEURAL, V8, P355, DOI 10.1088/0954-898X/8/4/002
   Maass W, 1996, NEURAL COMPUT, V8, P1, DOI 10.1162/neco.1996.8.1.1
   Maass W, 1997, NEURAL COMPUT, V9, P279, DOI 10.1162/neco.1997.9.2.279
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maass W, 1997, ADV NEUR IN, V9, P211
   Maass W, 1998, COMPUTATIONAL NEUROSCIENCE: TRENDS IN RESEARCH, P221
   Maass W, 1998, NEUROMORPHIC SYSTEMS, P21
   Mayorga RV, 2007, INT J NEURAL SYST, V17, P353, DOI 10.1142/S0129065707001202
   McKennoch S, 2006, IEEE IJCNN, P3970
   McKennoch S, 2009, NEURAL COMPUT, V21, P9, DOI 10.1162/neco.2008.09-07-610
   MOORE S, 2002, THESIS U BATH
   Natschläger T, 1999, NEUROCOMPUTING, V26-7, P463, DOI 10.1016/S0925-2312(99)00052-1
   Natschlager T, 1998, NETWORK-COMP NEURAL, V9, P319, DOI 10.1088/0954-898X/9/3/003
   NATSCHLAGER T, 2001, SPRINGER SERIES STUD, V78
   Nitta T, 2008, INT J NEURAL SYST, V18, P123, DOI 10.1142/S0129065708001439
   Nomura Y, 2007, COMPUT-AIDED CIV INF, V22, P306, DOI 10.1111/j.1467-8667.2007.00487.x
   Panakkat A, 2007, INT J NEURAL SYST, V17, P13, DOI 10.1142/S0129065707000890
   Panakkat A, 2009, COMPUT-AIDED CIV INF, V24, P280, DOI 10.1111/j.1467-8667.2009.00595.x
   Pande A, 2008, COMPUT-AIDED CIV INF, V23, P549, DOI 10.1111/j.1467-8667.2008.00559.x
   Panuku LN, 2007, LECT NOTES COMPUT SC, V4668, P390
   PARK HS, 1995, COMPUT STRUCT, V57, P391, DOI 10.1016/0045-7949(95)00047-K
   Park HS, 1997, J STRUCT ENG, V123, P880, DOI 10.1061/(ASCE)0733-9445(1997)123:7(880)
   PAUGAMMOISY H, 2009, HDB NATURAL COMPUTIN
   Pavlidis NG, 2005, IEEE IJCNN, P2190
   Pedrycz W, 2008, INT J NEURAL SYST, V18, P279, DOI 10.1142/S0129065708001592
   Perusich K, 2008, INTEGR COMPUT-AID E, V15, P197
   Rao VSH, 2008, INT J NEURAL SYST, V18, P165, DOI 10.1142/S0129065708001476
   Rinzel J, 1989, METHODS NEURAL MODEL
   Ruiz-Pinales J, 2008, INT J NEURAL SYST, V18, P419, DOI 10.1142/S0129065708001683
   Rumelhart D.E., 1987, LEARNING INTERNAL RE, P318
   Sabourin C, 2007, INTEGR COMPUT-AID E, V14, P173
   Samant A, 2001, COMPUT-AIDED CIV INF, V16, P239, DOI 10.1111/0885-9507.00229
   Sarma KC, 2000, J STRUCT ENG-ASCE, V126, P1339, DOI 10.1061/(ASCE)0733-9445(2000)126:11(1339)
   Sarma KC, 2000, J STRUCT ENG-ASCE, V126, P596, DOI 10.1061/(ASCE)0733-9445(2000)126:5(596)
   Sarma KC, 2002, INT J NUMER METH ENG, V55, P1451, DOI 10.1002/nme.549
   Sarma KC, 2001, COMPUT-AIDED CIV INF, V16, P295, DOI 10.1111/0885-9507.00234
   Schäfer AM, 2007, INT J NEURAL SYST, V17, P253, DOI 10.1142/S0129065707001111
   Schrauwen B, 2004, IEEE IJCNN, P471, DOI 10.1109/IJCNN.2004.1379954
   Schrauwen B, 2007, LECT NOTES COMPUT SC, V4668, P471
   SEJNOWSKI TJ, 1986, PARALLEL DISTRIBUTED, V2, P372
   Senouci AB, 2001, J CONSTR ENG M, V127, P28, DOI 10.1061/(ASCE)0733-9364(2001)127:1(28)
   Silva SM, 2005, PROCEEDINGS OF THE 2005 INTERNATIONAL CONFERENCE ON NEURAL NETWORKS AND BRAIN, VOLS 1-3, P1354
   Sirca GF, 2005, J STRUCT ENG, V131, P380, DOI 10.1061/(ASCE)0733-9445(2005)131:3(380)
   Sirca GF, 2001, J STRUCT ENG, V127, P1276, DOI 10.1061/(ASCE)0733-9445(2001)127:11(1276)
   Smith DE, 2007, SEX HEALTH, V4, P141, DOI 10.1071/SH06060
   Stathopoulos A, 2008, COMPUT-AIDED CIV INF, V23, P521, DOI 10.1111/j.1467-8667.2008.00558.x
   Suresh S, 2008, INT J NEURAL SYST, V18, P219, DOI 10.1142/S0129065708001543
   Tashakori A, 2002, J CONSTR STEEL RES, V58, P1545, DOI 10.1016/S0143-974X(01)00105-5
   Tsapatsoulis N, 2007, INT J NEURAL SYST, V17, P289, DOI 10.1142/S0129065707001147
   Villaverde I, 2007, INTEGR COMPUT-AID E, V14, P355
   Vlahogianni EI, 2008, COMPUT-AIDED CIV INF, V23, P536, DOI 10.1111/j.1467-8667.2008.00554.x
   Wersing H, 2007, INT J NEURAL SYST, V17, P219, DOI 10.1142/S0129065707001081
   Xin JG, 2001, IEEE IJCNN, P1772, DOI 10.1109/IJCNN.2001.938430
   Zhou ZQ, 2003, COMPUT-AIDED CIV INF, V18, P379, DOI 10.1111/1467-8667.t01-1-00315
   Zou W, 2008, INT J NEURAL SYST, V18, P195, DOI 10.1142/S012906570800152X
NR 151
TC 528
Z9 544
U1 27
U2 341
PD AUG
PY 2009
VL 19
IS 4
BP 295
EP 308
DI 10.1142/S0129065709002002
UT WOS:000269527600006
DA 2023-11-16
ER

PT J
AU Asgari, H
   Maybodi, BMN
   Sandamirskaya, Y
AF Asgari, Hajar
   Maybodi, Babak Mazloom-Nezhad
   Sandamirskaya, Yulia
TI Digital multiplier-less implementation of high-precision SDSP and
   synaptic strength-based STDP
SO INTERNATIONAL JOURNAL OF CIRCUIT THEORY AND APPLICATIONS
DT Article
DE FPGA; neuromorphic engineering; plastic synapse; spiking neural network
   (SNN)
ID HIPPOCAMPAL-NEURONS; SPIKING NEURONS; NEURAL-NETWORK; REALIZATION;
   PLASTICITY; MODEL; ARCHITECTURE; EFFICIENT; PAIR
AB Spiking neural networks (SNNs) can achieve lower latency and higher efficiency compared with traditional neural networks if they are implemented in dedicated neuromorphic hardware. In both biological and artificial spiking neuronal systems, synaptic modifications are the main mechanism for learning. Plastic synapses are thus the core component of neuromorphic hardware with on-chip learning capability. Recently, several research groups have designed hardware architectures for modeling plasticity in SNNs for various applications. Following these research efforts, this paper proposes multiplier-less digital neuromorphic circuits for two plasticity learning rules: the spike-driven synaptic plasticity (SDSP) and synaptic strength-based spike timing-dependent plasticity (SSSTDP). The proposed architectures have increased the precision of the plastic synaptic weights and are suitable for spiking neural network architectures with more precise calculations. The proposed models are validated in MATLAB simulations and physical implementations on a field-programmable gate array (FPGA).
C1 [Asgari, Hajar; Maybodi, Babak Mazloom-Nezhad] Shahid Beheshti Univ, Dept Elect Engn, Tehran, Iran.
   [Sandamirskaya, Yulia] Univ Zurich UZH, Inst Neuroinformat, Zurich, Switzerland.
   [Sandamirskaya, Yulia] Swiss Fed Inst Technol, Zurich, Switzerland.
RP Maybodi, BMN (corresponding author), Shahid Beheshti Univ, Dept Elect Engn, Tehran, Iran.
EM b-mazloom@sbu.ac.ir
CR Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   Amiri M, 2019, INT J CIRC THEOR APP, V47, P483, DOI 10.1002/cta.2596
   [Anonymous], 2019, MEMORY DRIVEN MIXED
   ASGARI H, 2019, ARXIV190609835
   ASGARI H, 2020, 2 IEEE INT C ART INT
   Azghadi MR, 2017, IEEE T BIOMED CIRC S, V11, P434, DOI 10.1109/TBCAS.2016.2618351
   Bartolozzi C, 2007, NEURAL COMPUT, V19, P2581, DOI 10.1162/neco.2007.19.10.2581
   Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Brader JM, 2007, NEURAL COMPUT, V19, P2881, DOI 10.1162/neco.2007.19.11.2881
   Brown AD, 2015, IEEE T COMPUT, V64, P1769, DOI 10.1109/TC.2014.2329686
   Cassidy A, 2011, IEEE INT SYMP CIRC S, P673
   Cruz-Albrecht JM, 2012, IEEE T BIOMED CIRC S, V6, P246, DOI 10.1109/TBCAS.2011.2174152
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   DEFELIPE J, 2012, FRONT NEUROANAT, V6
   Frenkel C, 2019, IEEE T BIOMED CIRC S, V13, P145, DOI 10.1109/TBCAS.2018.2880425
   Frenkel C, 2017, IEEE INT SYMP CIRC S, P17
   Friedmann S, 2017, IEEE T BIOMED CIRC S, V11, P128, DOI 10.1109/TBCAS.2016.2579164
   Galluppi F, 2015, FRONT NEUROSCI-SWITZ, V8, DOI 10.3389/fnins.2014.00429
   Gomar S, 2014, IEEE T CIRCUITS-I, V61, P1206, DOI 10.1109/TCSI.2013.2286030
   GU J, 2018, PROJECTION CONVOLUTI
   Hayati M, 2016, IEEE T CIRCUITS-II, V63, P463, DOI 10.1109/TCSII.2015.2505258
   Hayati M, 2016, IEEE T BIOMED CIRC S, V10, P518, DOI 10.1109/TBCAS.2015.2450837
   Hayati M, 2015, IEEE T CIRCUITS-I, V62, P1805, DOI 10.1109/TCSI.2015.2423794
   Indiveri G, 2006, IEEE T NEURAL NETWOR, V17, P211, DOI 10.1109/TNN.2005.860850
   INDIVERI G, 2011, FRONT NEUROSCI SWITZ, V5
   Jokar E, 2017, IEEE T CIRCUITS-II, V64, P832, DOI 10.1109/TCSII.2016.2621823
   Karimi G, 2018, INT J CIRC THEOR APP, V46, P965, DOI 10.1002/cta.2457
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Kim H, 2019, INT S HIGH PERF COMP, P661, DOI 10.1109/HPCA.2019.00017
   Kistler WM, 2000, NEURAL COMPUT, V12, P385, DOI 10.1162/089976600300015844
   Komorowski RW, 2009, J NEUROSCI, V29, P9918, DOI 10.1523/JNEUROSCI.1378-09.2009
   Lammie C, 2019, IEEE T CIRCUITS-I, V66, P1558, DOI 10.1109/TCSI.2018.2881753
   Lin J., 2019, DEFENSIVE QUANTIZATI
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Moradi S, 2018, IEEE T BIOMED CIRC S, V12, P106, DOI 10.1109/TBCAS.2017.2759700
   Moradi S, 2014, IEEE T BIOMED CIRC S, V8, P98, DOI 10.1109/TBCAS.2013.2255873
   Nawrocki RA, 2016, IEEE T ELECTRON DEV, V63, P3819, DOI 10.1109/TED.2016.2598413
   Nouri M, 2018, IEEE T CIRCUITS-II, V65, P804, DOI 10.1109/TCSII.2017.2750214
   Qiao N, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00141
   Rahimian E, 2018, IEEE T BIOMED CIRC S, V12, P47, DOI 10.1109/TBCAS.2017.2753541
   Raudies F, 2014, FRONT SYST NEUROSCI, V8, DOI 10.3389/fnsys.2014.00178
   Sandamirskaya Y, 2013, NEW IDEAS PSYCHOL, V31, P322, DOI 10.1016/j.newideapsych.2013.01.002
   Schumann CL, 2019, AIDS BEHAV, V23, P5, DOI 10.1007/s10461-017-1727-4
   Soleimani H, 2018, IEEE T CIRCUITS-II, V65, P91, DOI 10.1109/TCSII.2017.2697826
   Soleimani H, 2012, IEEE T CIRCUITS-I, V59, P2991, DOI 10.1109/TCSI.2012.2206463
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Wang RF, 2016, BIOTECHNOL BIOFUELS, V9, DOI 10.1186/s13068-016-0500-7
   WIEDEMANN S, 2019, IEEE T NEUR NET LEAR, P1
   Ziegler M, 2015, IEEE T BIOMED CIRC S, V9, P197, DOI 10.1109/TBCAS.2015.2410811
NR 51
TC 2
Z9 2
U1 1
U2 9
PD MAY
PY 2020
VL 48
IS 5
BP 724
EP 738
DI 10.1002/cta.2753
EA FEB 2020
UT WOS:000516554700001
DA 2023-11-16
ER

PT J
AU Ryndin, EA
   Andreeva, NV
   Luchinin, VV
   Goncharov, KS
   Raiimzhonov, VS
AF Ryndin, E. A.
   Andreeva, N. V.
   Luchinin, V. V.
   Goncharov, K. S.
   Raiimzhonov, V. S.
TI Neuromorphic Functional Modules of a Spiking Neural Network
SO NANOBIOTECHNOLOGY REPORTS
DT Article
DE spiking neural network; artificial neuron; functional analogue of
   memristor
AB In the current era, the design and development of artificial neural networks exploiting the architecture of the human brain have evolved rapidly. Artificial neural networks (ANN) effectively solve a wide range of common artificial-intelligence tasks involving data classification and recognition, prediction, forecasting and adaptive control of the behavior of an object. The biologically inspired underlying principles of ANN operation have certain advantages over the conventional von Neumann architecture including unsupervised learning, architectural flexibility and adaptability to environmental change and high performance under significantly reduced power consumption due to large parallel and asynchronous data processing. In this paper, we present a circuit design of main functional blocks (neurons and synapses) intended for the hardware implementation of a perceptron-based feedforward spiking neural network. As the third generation of artificial neural networks, spiking neural networks perform data processing utilizing spikes, which are discrete events (or functions) that take place at points in time. Neurons in spiking neural networks initiate precisely timed spikes and communicate with each other via spikes transmitted through synaptic connections or synapses with adaptable scalable weight. One of the prospective approaches to emulating synaptic behavior in hardware implemented spiking neural networks is to use nonvolatile-memory devices with analog conduction modulation (or memristive structures). Here we propose a circuit design for functional analogues of memristive structures to mimic synaptic plasticity, pre- and postsynaptic neurons which could be used for developing a circuit design of spiking-neural-network architectures with different training algorithms including the spike-timing-dependent-plasticity learning rule. Two different circuits of an electrical synapse are developed. The first one is an analog synapse with a photoresistor optocoupler used to ensure the tunable conductivity for synaptic-plasticity emulation. While the second one is a digital synapse, in which the synaptic weight is stored in a digital code with its direct conversion into conductivity (without a digital-to-analog converter and photoresistor optocoupler). The results of prototyping the developed circuits for electronic analogs of synapses, pre- and postsynaptic neurons and the study of transient processes are presented. The developed approach could provide a basis for the ASIC (application-specific integrated circuit) design of spiking neural networks based on CMOS (complementary metal-oxide-semiconductor) design technology.
C1 [Ryndin, E. A.; Andreeva, N. V.; Luchinin, V. V.; Goncharov, K. S.; Raiimzhonov, V. S.] St Petersburg Electrotech Univ LETI, St Petersburg 197376, Russia.
RP Ryndin, EA (corresponding author), St Petersburg Electrotech Univ LETI, St Petersburg 197376, Russia.
EM rynenator@gmail.com
CR Adam G.C., 2018, NAT COMMUN, V9, P1
   Andreeva N, 2018, AIP ADV, V8, DOI 10.1063/1.5019570
   Andreeva NV, 2020, BIONANOSCIENCE, V10, P824, DOI 10.1007/s12668-020-00778-2
   Andreeva N. V., 2020, ELEKTRONIKA NTV NO, V9, P1, DOI [10.22184/1992-4178.2020.200.9.72.82, DOI 10.22184/1992-4178.2020.200.9.72.82]
   Basheer IA, 2000, J MICROBIOL METH, V43, P3, DOI 10.1016/S0167-7012(00)00201-3
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Bichler O, 2012, NEURAL NETWORKS, V32, P339, DOI 10.1016/j.neunet.2012.02.022
   Burr GW, 2017, ADV PHYS-X, V2, P89, DOI 10.1080/23746149.2016.1259585
   Chicca E, 2014, P IEEE, V102, P1367, DOI 10.1109/JPROC.2014.2313954
   Choi BJ, 2016, ADV FUNCT MATER, V26, P5290, DOI 10.1002/adfm.201600680
   Gokmen T, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00333
   Goux L., 2012, 2012 IEEE Symposium on VLSI Technology, P159, DOI 10.1109/VLSIT.2012.6242510
   Guo YL, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00812
   Gupta I, 2019, FARADAY DISCUSS, V213, P511, DOI 10.1039/c8fd00130h
   Hansen M, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-27033-9
   Kornijcuk V, 2019, ADV INTELL SYST-GER, V1, DOI 10.1002/aisy.201900030
   Kuzum D, 2013, NANOTECHNOLOGY, V24, DOI 10.1088/0957-4484/24/38/382001
   Malenka RC, 2004, NEURON, V44, P5, DOI 10.1016/j.neuron.2004.09.012
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   [Петров А. Petrov A.], 2016, [Наноиндустрия, Nanoindustriya], P94
   Pi S, 2019, NAT NANOTECHNOL, V14, P35, DOI 10.1038/s41565-018-0302-0
   Potok TE, 2018, ACM J EMERG TECH COM, V14, DOI 10.1145/3178454
   Rambidi N. G., 2002, NANOI MIKROSISTEMNAY, P21
   Wang W, 2018, SCI ADV, V4, DOI 10.1126/sciadv.aat4752
   Xia QF, 2019, NAT MATER, V18, P309, DOI 10.1038/s41563-019-0291-x
   Yang JJS, 2013, NAT NANOTECHNOL, V8, P13, DOI [10.1038/nnano.2012.240, 10.1038/NNANO.2012.240]
NR 26
TC 0
Z9 0
U1 0
U2 0
PD DEC
PY 2022
VL 17
IS SUPPL 1
SU 1
BP S80
EP S90
DI 10.1134/S2635167622070175
UT WOS:000943157200016
DA 2023-11-16
ER

PT J
AU Tsang, IJ
   Corradi, F
   Sifalakis, M
   Van Leekwijck, W
   Latré, S
AF Tsang, Ing Jyh
   Corradi, Federico
   Sifalakis, Manolis
   Van Leekwijck, Werner
   Latre, Steven
TI Radar-Based Hand Gesture Recognition Using Spiking Neural Networks
SO ELECTRONICS
DT Article
DE radar; liquid state machine; spiking neural network; hand gesture
   recognition
ID HUMAN-COMPUTER-INTERACTION
AB We propose a spiking neural network (SNN) approach for radar-based hand gesture recognition (HGR), using frequency modulated continuous wave (FMCW) millimeter-wave radar. After pre-processing the range-Doppler or micro-Doppler radar signal, we use a signal-to-spike conversion scheme that encodes radar Doppler maps into spike trains. The spike trains are fed into a spiking recurrent neural network, a liquid state machine (LSM). The readout spike signal from the SNN is then used as input for different classifiers for comparison, including logistic regression, random forest, and support vector machine (SVM). Using liquid state machines of less than 1000 neurons, we achieve better than state-of-the-art results on two publicly available reference datasets, reaching over 98% accuracy on 10-fold cross-validation for both data sets.
C1 [Tsang, Ing Jyh; Van Leekwijck, Werner; Latre, Steven] Univ Antwerp, IDLab Dept Comp Sci, IMEC, Sint Pietersvliet 7, B-2000 Antwerp, Belgium.
   [Corradi, Federico; Sifalakis, Manolis] IMEC Nederland, Holst Ctr High Tech Campus 31, NL-5656 AE Eindhoven, Netherlands.
RP Tsang, IJ (corresponding author), Univ Antwerp, IDLab Dept Comp Sci, IMEC, Sint Pietersvliet 7, B-2000 Antwerp, Belgium.
EM ingjyh.tsang@uantwerpen.be; federico.corradi@imec.nl;
   manolis.sifalakis@imec.nl; werner.vanleekwijck@uantwerpen.be;
   steven.latre@uantwerpen.be
CR Ahmed S, 2021, REMOTE SENS-BASEL, V13, DOI 10.3390/rs13030527
   Ahuja K, 2021, CHI '21: PROCEEDINGS OF THE 2021 CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, DOI 10.1145/3411764.3445138
   Amir A, 2017, PROC CVPR IEEE, P7388, DOI 10.1109/CVPR.2017.781
   Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Cheng L, 2021, IEEE T COGN DEV SYST, V13, P151, DOI 10.1109/TCDS.2019.2918228
   Corradi F, 2019, IEEE IJCNN
   Donati E, 2018, BIOMED CIRC SYST C, P455
   Fardet T., 2020, COMPUT SYST NEUROSCI
   Fioranelli H.F., 2020, MICRODOPPLER RADAR I
   George AM, 2020, IEEE IJCNN, DOI 10.1109/ijcnn48605.2020.9206681
   Gonzalez RC, 2018, DIGITAL IMAGE PROCES
   Gu CZ, 2017, IEEE SENSOR LETT, V1, DOI 10.1109/LSENS.2017.2696520
   Gurbuz S, 2020, DEEP NEURAL NETWORK
   Hazan H, 2012, EXPERT SYST APPL, V39, P1597, DOI 10.1016/j.eswa.2011.06.052
   Hu YH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00405
   Jankiraman M, 2018, ARTECH HSE RADAR LIB, P1
   KITTLER J, 1985, IEEE T SYST MAN CYB, V15, P652, DOI 10.1109/TSMC.1985.6313443
   KITTLER J, 1986, PATTERN RECOGN, V19, P41, DOI 10.1016/0031-3203(86)90030-0
   Klikauer T, 2016, TRIPLEC-COMMUN CAPIT, V14, P260
   Lien JM, 2016, ACM T GRAPHIC, V35, DOI [10.1145/2897824.2925953, 10.1145/9999997.9999999]
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Maass W, 2000, NEURAL COMPUT, V12, P1743, DOI 10.1162/089976600300015123
   Maass W, 2004, J COMPUT SYST SCI, V69, P593, DOI 10.1016/j jcss.2004.04.001
   Maass W., 2011, COMPUTABILITY CONTEX, P275, DOI DOI 10.1142/9781848162778_0008
   Maqueda AI, 2015, COMPUT VIS IMAGE UND, V141, P126, DOI 10.1016/j.cviu.2015.07.009
   Marc-Oliver G., 2007, SCHOLARPEDIA, DOI [DOI 10.4249/SCHOLARPEDIA.1430, 10.4249/scholarpedia.1430]
   Massa R., 2020, ARXIV200609985, DOI DOI 10.1109/IJCNN48605.2020.9207109
   Meier K, 2015, 2015 IEEE INTERNATIONAL ELECTRON DEVICES MEETING (IEDM)
   Min YC, 2020, PROC CVPR IEEE, P5760, DOI 10.1109/CVPR42600.2020.00580
   Moin A, 2021, NAT ELECTRON, V4, P54, DOI 10.1038/s41928-020-00510-8
   Molchanov P, 2016, PROC CVPR IEEE, P4207, DOI 10.1109/CVPR.2016.456
   Molchanov P, 2015, IEEE RAD CONF, P1491, DOI 10.1109/RADAR.2015.7131232
   Nasr I, 2016, IEEE J SOLID-ST CIRC, V51, P2066, DOI 10.1109/JSSC.2016.2585621
   Neckar A, 2019, P IEEE, V107, P144, DOI 10.1109/JPROC.2018.2881432
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Qiao N, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00141
   Ritchie M, 2020, ELECTRON LETT, V56, P568, DOI 10.1049/el.2019.4153
   Ritchie M, 2019, IEEE RAD CONF, DOI 10.1109/radar.2019.8835782
   Samuelsson A., 2020, BUILD ADV HAND GESTU
   Seifert D., 2021, VERGE
   Sezgin M, 2004, J ELECTRON IMAGING, V13, P146, DOI 10.1117/1.1631315
   Tsodyks M, 2000, J NEUROSCI, V20
   Wang SW, 2016, UIST 2016: PROCEEDINGS OF THE 29TH ANNUAL SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P851, DOI 10.1145/2984511.2984565
   Wiggers K., 2019, GOOGLE UNVEILS PIXEL
NR 44
TC 13
Z9 13
U1 6
U2 26
PD JUN
PY 2021
VL 10
IS 12
AR 1405
DI 10.3390/electronics10121405
UT WOS:000666505600001
DA 2023-11-16
ER

PT C
AU Han, M
   Wang, Y
   Dong, J
   Liu, H
   Wu, J
   Qu, G
AF Han, Ming
   Wang, Ye
   Dong, Jian
   Liu, Heng
   Wu, Jin
   Qu, Gang
GP ACM
TI ATC: Approximate Temporal Coding for Efficient Implementations of
   Spiking Neural Networks
SO PROCEEDINGS OF THE GREAT LAKES SYMPOSIUM ON VLSI 2023, GLSVLSI 2023
DT Proceedings Paper
CT 33rd Great Lakes Symposium on VLSI (GLSVLSI)
CY JUN 05-07, 2023
CL Knoxville, TN
DE approximate computing; spiking neural network; temporal coding; pruning
ID BACKPROPAGATION
AB Spiking Neural Networks (SNN) update their neurons' states, the most energy consuming action, only after receiving or firing spikes for energy efficiency. So reducing the number of spikes would lead to more efficient SNN implementations. We propose an approximate temporal coding (ATC) for this purpose. Because the reduction of spikes leads to more synapses being used rarely, we develop a pruning method for further energy improvement. Experimental results validate the efficiency of ATC and the pruning method. On the MNIST dataset, for example, 61% of the spikes are reduced, leading to 60% energy saving without any accuracy loss.
C1 [Han, Ming; Wang, Ye; Dong, Jian; Liu, Heng; Wu, Jin] Harbin Inst Technol, Harbin, Peoples R China.
   [Qu, Gang] Univ Maryland, College Pk, MD USA.
RP Dong, J (corresponding author), Harbin Inst Technol, Harbin, Peoples R China.
EM minghan@hit.edu.cn; yewang@hit.edu.cn; dan@hit.edu.cn;
   hengliu@stu.hit.edu.cn; jwu0212@gmail.com; gangqu@umd.edu
CR Comsa JM, 2020, INT CONF ACOUST SPEE, P8529, DOI [10.1109/icassp40776.2020.9053856, 10.1109/ICASSP40776.2020.9053856]
   Kim S, 2020, AAAI CONF ARTIF INTE, V34, P11270
   Kim S, 2020, INT S HIGH PERF COMP, P663, DOI 10.1109/HPCA47549.2020.00060
   Kim Y, 2022, INT CONF ACOUST SPEE, P71, DOI 10.1109/ICASSP43922.2022.9747906
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Mirsadeghi M, 2021, NEUROCOMPUTING, V427, P131, DOI 10.1016/j.neucom.2020.11.052
   Mitchell J. Parker, 2020, ICONS 2020: International Conference on Neuromorphic Systems 2020, DOI 10.1145/3407197.3407216
   Putra RVW, 2022, PROCEEDINGS OF THE 59TH ACM/IEEE DESIGN AUTOMATION CONFERENCE, DAC 2022, P151, DOI 10.1145/3489517.3530657
   Putra RVW, 2021, DES AUT CON, P379, DOI 10.1109/DAC18074.2021.9586332
   Putra RVW, 2020, IEEE T COMPUT AID D, V39, P3601, DOI 10.1109/TCAD.2020.3013049
   Rathi N, 2019, IEEE T COMPUT AID D, V38, P668, DOI 10.1109/TCAD.2018.2819366
   Sen S, 2017, DES AUT TEST EUROPE, P193, DOI 10.23919/DATE.2017.7926981
   Shahbaz M, 2023, INT J FINANC ECON, V28, P1308, DOI 10.1002/ijfe.2478
   Tavanaei A, 2019, NEUROCOMPUTING, V330, P39, DOI 10.1016/j.neucom.2018.11.014
   Wang Y, 2021, J PARALLEL DISTR COM, V158, P164, DOI 10.1016/j.jpdc.2021.08.003
   Wunderlich TC, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-91786-z
   Xiao H, 2017, Arxiv, DOI arXiv:1708.07747
   Yavuz E, 2016, SCI REP-UK, V6, DOI 10.1038/srep18854
   Zhang ZX, 2021, IEEE SIGNAL PROC LET, V28, P484, DOI 10.1109/LSP.2021.3059172
   Zhao JH, 2022, IEEE T NEUR NET LEAR, V33, P4096, DOI 10.1109/TNNLS.2021.3055825
   Zheng N, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351516
NR 23
TC 0
Z9 0
U1 2
U2 2
PY 2023
BP 527
EP 532
DI 10.1145/3583781.3590201
UT WOS:001042307500097
DA 2023-11-16
ER

PT J
AU Silva, RG
AF Silva, Rui G.
TI Condition monitoring of the cutting process using a self-organizing
   spiking neural network map
SO JOURNAL OF INTELLIGENT MANUFACTURING
DT Article
DE Spiking neuron networks; Unsupervised learning; Condition monitoring;
   Tool wear
ID TOOL; SYSTEM; COMPUTATION; OPERATIONS; SIGNALS
AB This paper presents a new approach to sensor based condition monitoring using a self-organizing spiking neuron network map. Experimental evidence suggests that biological neural networks, which communicate through spikes, use the timing of these spikes to encode and compute information in a more efficient way. The paper introduces the basis of a simplified version of the Self-Organizing neural architecture based on Spiking Neurons. The fundamental steps for the development of this computational model are presented as well as some experimental evidence of its performance. It is shown that this computational architecture has a greater potential to unveil embedded information in tool wear monitoring data sets and that faster learning occurs if compared to traditional sigmoidal neural networks.
C1 Univ Lusiada, VN Famalicao Largo Tinoco Sousa, P-4760108 Vila Nova De Famalicao, Portugal.
RP Silva, RG (corresponding author), Univ Lusiada, VN Famalicao Largo Tinoco Sousa, P-4760108 Vila Nova De Famalicao, Portugal.
EM rsilva@fam.ulusiada.pt
CR Aliustaoglu C, 2009, MECH SYST SIGNAL PR, V23, P539, DOI 10.1016/j.ymssp.2008.02.010
   [Anonymous], 1977, 3685 ISO
   [Anonymous], PULSED NEURAL NETWOR
   Balazinski M, 2002, ENG APPL ARTIF INTEL, V15, P73, DOI 10.1016/S0952-1976(02)00004-0
   Bugmann G, 1997, BIOSYSTEMS, V40, P11, DOI 10.1016/0303-2647(96)01625-5
   Byrne G., 1995, ANN CIRP, V44, P541
   Choudhury SK, 1999, INT J MACH TOOL MANU, V39, P489, DOI 10.1016/S0890-6955(98)00032-7
   Cybenko G., 1989, Mathematics of Control, Signals, and Systems, V2, P303, DOI 10.1007/BF02551274
   Dimla DE, 1997, INT J MACH TOOL MANU, V37, P1219, DOI 10.1016/S0890-6955(97)00020-5
   Dimla DE, 2000, INT J MACH TOOL MANU, V40, P1073, DOI 10.1016/S0890-6955(99)00122-4
   GERSTNER W, 2002, HDB BRAIN THEORY NEU, P577
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8
   Jantunen E, 2002, INT J MACH TOOL MANU, V42, P997, DOI 10.1016/S0890-6955(02)00040-8
   KOHONEN T, 1990, P IEEE, V78, P1464, DOI 10.1109/5.58325
   Lennox B, 2001, J PROCESS CONTR, V11, P497, DOI 10.1016/S0959-1524(00)00027-5
   Maass W, 1999, INFORM COMPUT, V148, P202, DOI 10.1006/inco.1998.2743
   Maass W., 1999, PULSED NEURAL NETWOR
   MAASS W, 1997, NEURAL NETWORKS, V10, P1656
   Natschläger T, 2002, THEOR COMPUT SCI, V287, P251, DOI 10.1016/S0304-3975(02)00099-3
   Parlos AG, 2000, NEURAL NETWORKS, V13, P765, DOI 10.1016/S0893-6080(00)00048-4
   Salgado DR, 2007, INT J MACH TOOL MANU, V47, P2140, DOI 10.1016/j.ijmachtools.2007.04.013
   Silva RG, 1998, MECH SYST SIGNAL PR, V12, P319, DOI 10.1006/mssp.1997.0123
   Silva RG, 2000, MECH SYST SIGNAL PR, V14, P287, DOI 10.1006/mssp.1999.1286
   SILVA RG, 1995, 8 INT C COND MON DIA
   Sun J, 2006, INT J MACH TOOL MANU, V46, P218, DOI 10.1016/j.ijmachtools.2005.04.005
   WARNECKE A, 1990, WINT ANN M ASME DALL, P43
NR 27
TC 13
Z9 13
U1 0
U2 12
PD DEC
PY 2010
VL 21
IS 6
BP 823
EP 829
DI 10.1007/s10845-009-0258-x
UT WOS:000284509700015
DA 2023-11-16
ER

PT J
AU Nevarez, Y
   Rotermund, D
   Pawelzik, KR
   Garcia-Ortiz, A
AF Nevarez, Yarib
   Rotermund, David
   Pawelzik, Klaus R.
   Garcia-Ortiz, Alberto
TI Accelerating Spike-by-Spike Neural Networks on FPGA With Hybrid Custom
   Floating-Point and Logarithmic Dot-Product Approximation
SO IEEE ACCESS
DT Article
DE Artificial intelligence; spiking neural networks; approximate computing;
   logarithmic; parameterisable floating-point; optimization; hardware
   accelerator; embedded systems; FPGA
ID ON-CHIP
AB Spiking neural networks (SNNs) represent a promising alternative to conventional neural networks. In particular, the so-called Spike-by-Spike (SbS) neural networks provide exceptional noise robustness and reduced complexity. However, deep SbS networks require a memory footprint and a computational cost unsuitable for embedded applications. To address this problem, this work exploits the intrinsic error resilience of neural networks to improve performance and to reduce hardware complexity. More precisely, we design a vector dot-product hardware unit based on approximate computing with configurable quality using hybrid custom floating-point and logarithmic number representation. This approach reduces computational latency, memory footprint, and power dissipation while preserving inference accuracy. To demonstrate our approach, we address a design exploration flow using high-level synthesis and a Xilinx SoC-FPGA. The proposed design reduces 20.5x computational latency and 8x weight memory footprint, with less than 0.5% of accuracy degradation on a handwritten digit recognition task.
C1 [Nevarez, Yarib; Garcia-Ortiz, Alberto] Univ Bremen, Inst Electrodynam & Microelect, D-28359 Bremen, Germany.
   [Rotermund, David; Pawelzik, Klaus R.] Univ Bremen, Inst Theoret Phys, D-28359 Bremen, Germany.
RP Nevarez, Y (corresponding author), Univ Bremen, Inst Electrodynam & Microelect, D-28359 Bremen, Germany.
EM nevarez@item.uni-bremen.de
CR Abderrahmane N, 2020, NEURAL NETWORKS, V121, P366, DOI 10.1016/j.neunet.2019.09.024
   Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   Amunts K, 2019, PLOS BIOL, V17, DOI 10.1371/journal.pbio.3000344
   [Anonymous], ADV NEURAL INFORM PR
   [Anonymous], 2015, ARXIV 151000149
   [Anonymous], 2016, INT C LEARNING REPRE
   [Anonymous], 2015, P 52 ANN DESIGN AUTO
   Bellec G., 2017, ARXIV171105136
   Bouvier M, 2019, ACM J EMERG TECH COM, V15, DOI 10.1145/3304103
   Buesing L, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1002211
   Chen GK, 2019, IEEE J SOLID-ST CIRC, V54, P992, DOI 10.1109/JSSC.2018.2884901
   Dapello Joel, 2020, NEURIPS, V33, P13073, DOI DOI 10.1101/2020.06.16.154542
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Dayan P., 2001, THEORETICAL NEUROSCI, DOI DOI 10.1016/j.neuron.2013.01.033
   Du ZD, 2014, ASIA S PACIF DES AUT, P201, DOI 10.1109/ASPDAC.2014.6742890
   Ernst U, 2007, NEURAL COMPUT, V19, P1313, DOI 10.1162/neco.2007.19.5.1313
   Gupta V., 2011, 2011 International Symposium on Low Power Electronics and Design (ISLPED 2011), P409, DOI 10.1109/ISLPED.2011.5993675
   Han J, 2013, PROC EUR TEST SYMP
   Hassibi Babak, 1992, NIPS
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hrica J., 2012, APPL NOTE VIVADO DES
   Hubara I, 2018, J MACH LEARN RES, V18
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Jerry M, 2017, SYMP VLSI CIRCUITS, pT186
   Kim Y, 2013, ICCAD-IEEE ACM INT, P130, DOI 10.1109/ICCAD.2013.6691108
   LeCun Y., 1998, MNIST DATABASE HANDW
   LeCun Y., 1989, ADV IN NEURAL INFORM, P598
   Liu Z., ARXIV181005270
   LiWan Matthew Zeiler, 2013, P 30 INT C MACH LEAR, P1058
   Lotric U, 2012, NEUROCOMPUTING, V96, P57, DOI 10.1016/j.neucom.2011.09.039
   McDonnell MD, 2011, NAT REV NEUROSCI, V12, P415, DOI 10.1038/nrn3061
   Mittal S, 2016, ACM COMPUT SURV, V48, DOI 10.1145/2893356
   Molchanov Pavlo, 2016, ARXIV161106440
   Moons B, 2016, SYMP VLSI CIRCUITS
   Mrazek V., 2016, P 35 INT C COMP AID, P1
   Neftci EO, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00241
   Nevarez Y., 2020, P 9 INT C MOD CIRC S, P1
   Painkras E, 2013, IEEE J SOLID-ST CIRC, V48, P1943, DOI 10.1109/JSSC.2013.2259038
   Park J, 2010, IEEE T VLSI SYST, V18, P787, DOI 10.1109/TVLSI.2009.2016839
   Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32
   Rathi N, 2019, IEEE T COMPUT AID D, V38, P668, DOI 10.1109/TCAD.2018.2819366
   Rotermund D., 2018, BIORXIV PREPRINT, DOI [10.1101/500280, DOI 10.1101/500280]
   Rotermund D., 2019, BIOL PLAUSIBLE LEARN
   Rotermund D, 2019, FRONT COMPUT NEUROSC, V13, DOI 10.3389/fncom.2019.00055
   Roy K, 2019, NATURE, V575, P607, DOI 10.1038/s41586-019-1677-2
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sarwar SS, 2016, DES AUT TEST EUROPE, P145
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Sen S, 2017, DES AUT TEST EUROPE, P193, DOI 10.23919/DATE.2017.7926981
   Sheik S, 2016, IEEE INT SYMP CIRC S, P2090, DOI 10.1109/ISCAS.2016.7538991
   Srinivasan G, 2016, SCI REP-UK, V6, DOI 10.1038/srep29545
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Sun XY, 2018, DES AUT TEST EUROPE, P1423, DOI 10.23919/DATE.2018.8342235
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Whatmough PN, 2017, ISSCC DIG TECH PAP I, P242, DOI 10.1109/ISSCC.2017.7870351
   Xilinx U., 2015, ZYNQ 7000 ALL PROGRA
   Young AR, 2019, IEEE ACCESS, V7, P135606, DOI 10.1109/ACCESS.2019.2941772
   Zhang M, 2018, CHINESE J ELECTRON, V27, P667, DOI 10.1049/cje.2018.05.006
NR 58
TC 1
Z9 1
U1 1
U2 11
PY 2021
VL 9
BP 80603
EP 80620
DI 10.1109/ACCESS.2021.3085216
UT WOS:000673883400001
DA 2023-11-16
ER

PT C
AU Schnell, B
   Garner, PN
AF Schnell, Bastian
   Garner, Philip N.
GP Int Speech Commun Assoc
TI A Neural Model to Predict Parameters for a Generalized Command Response
   Model of Intonation
SO 19TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION
   ASSOCIATION (INTERSPEECH 2018), VOLS 1-6: SPEECH RESEARCH FOR EMERGING
   MARKETS IN MULTILINGUAL SOCIETIES
SE Interspeech
DT Proceedings Paper
CT 19th Annual Conference of the
   International-Speech-Communication-Association (INTERSPEECH 2018)
CY AUG 02-SEP 06, 2018
CL Hyderabad, INDIA
DE Speech synthesis; prosody modelling; recurrent neural network; Fujisaki
   model
ID SPIKE; ALGORITHMS; NETWORKS
AB The Generalised Command Response (GCR) model is a time local model of intonation that has been shown to lend itself to (cross -language) transfer of emphasis. In order to generalise the model to longer prosodic sequences, we show that it can be driven by a recurrent neural network emulating a spiking neural network. We show that a loss function for error backpropagation can be formulated analogously to that of the Spike Pattern Association Neuron (SPAN) method for spiking networks. The resulting system is able to generate prosody comparable to a state-of-the-art deep neural network implementation, but potentially retaining the transfer capabilities of the GCR model.
C1 [Schnell, Bastian; Garner, Philip N.] Idiap Res Inst, Martigny, Switzerland.
   [Schnell, Bastian] Ecole Polytech Fed Lausanne, Lausanne, Switzerland.
RP Schnell, B (corresponding author), Idiap Res Inst, Martigny, Switzerland.; Schnell, B (corresponding author), Ecole Polytech Fed Lausanne, Lausanne, Switzerland.
EM bastian.schnell@idiap.ch; phil.garner@idiap.ch
CR [Anonymous], 2014, SEM STRUCT STAT TRAN
   [Anonymous], 2016, 9 ISCA SPEECH SYNTH
   [Anonymous], 1992, P ICSLP
   [Anonymous], 2008, P BLIZZ CHALL WORKSH
   Dauwels J., 2008, ADV NEURAL INFORM PR, V20, P361
   Fujisaki H., 1998, 3 ESCA COCOSDA WORKS
   Gerazov B, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P1601
   Gers FA, 2003, J MACH LEARN RES, V3, P115, DOI 10.1162/153244303768966139
   Hirst D., 2000, PROSODY THEORY EXPT, V14, P51, DOI 10.1007/978-94-015-9413-4_4
   Hojo N, 2017, INTERSPEECH, P1074, DOI 10.21437/Interspeech.2017-719
   Honnet P.-E., 2016, 9 ISCA SPEECH SYNTH
   Honnet PE, 2015, INT CONF ACOUST SPEE, P4744, DOI 10.1109/ICASSP.2015.7178871
   Hunter JD, 2003, J NEUROPHYSIOL, V90, P387, DOI 10.1152/jn.00074.2003
   Kameoka H, 2015, IEEE-ACM T AUDIO SPE, V23, P1042, DOI 10.1109/TASLP.2015.2418576
   Kingma D. P., 2015, INT C LEARNING REPRE
   Kraft Sebastian, 2014, LIN AUD C KARLSR DE
   MALLAT SG, 1993, IEEE T SIGNAL PROCES, V41, P3397, DOI 10.1109/78.258082
   Mohemmed A, 2013, NEUROCOMPUTING, V107, P3, DOI 10.1016/j.neucom.2012.08.034
   Morise M, 2016, SPEECH COMMUN, V84, P57, DOI 10.1016/j.specom.2016.09.001
   Morise M, 2016, IEICE T INF SYST, VE99D, P1877, DOI 10.1587/transinf.2015EDP7457
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Quiroga RQ, 2002, PHYS REV E, V66, DOI 10.1103/PhysRevE.66.041904
   Ronanki S, 2017, INTERSPEECH, P1133, DOI 10.21437/Interspeech.2017-628
   Rusu CV, 2014, NEURAL COMPUT, V26, P306, DOI 10.1162/NECO_a_00545
   Schreiber S, 2003, NEUROCOMPUTING, V52-4, P925, DOI 10.1016/S0925-2312(02)00838-X
   Taylor P, 2000, J ACOUST SOC AM, V107, P1697, DOI 10.1121/1.428453
   Tokuda K, 2000, INT CONF ACOUST SPEE, P1315, DOI 10.1109/ICASSP.2000.861820
   van Rossum MCW, 2001, NEURAL COMPUT, V13, P751, DOI 10.1162/089976601300014321
   van Santen JPH, 2000, TEXT SPEECH LANG TEC, V15, P269
   Victor JD, 1997, NETWORK-COMP NEURAL, V8, P127, DOI 10.1088/0954-898X/8/2/003
   2005, SPEECH COMMUN, V46, P348, DOI DOI 10.1016/J.SPECOM.2005.04.008
NR 31
TC 3
Z9 3
U1 0
U2 1
PY 2018
BP 3147
EP 3151
DI 10.21437/Interspeech.2018-1904
UT WOS:000465363900656
DA 2023-11-16
ER

PT J
AU Lee, ST
   Lim, S
   Bae, JH
   Kwon, D
   Kim, HS
   Park, BG
   Lee, JH
AF Lee, Sung-Tae
   Lim, Suhwan
   Bae, Jong-Ho
   Kwon, Dongseok
   Kim, Hyeong-Su
   Park, Byung-Gook
   Lee, Jong-Ho
TI Pruning for Hardware-Based Deep Spiking Neural Networks Using Gated
   Schottky Diode as Synaptic Devices
SO JOURNAL OF NANOSCIENCE AND NANOTECHNOLOGY
DT Article
DE Neuromorphic; Deep Spiking Neural Networks; Pruning; Synaptic Device;
   Gated Schottky Diode
AB Deep learning represents state-of-the-art results in various machine learning tasks, but for applications that require real-time inference, the high computational cost of deep neural networks becomes a bottleneck for the efficiency. To overcome the high computational cost of deep neural networks, spiking neural networks (SNN) have been proposed. Herein, we propose a hardware implementation of the SNN with gated Schottky diodes as synaptic devices. In addition, we apply L1 regularization for connection pruning of the deep spiking neural networks using gated Schottky diodes as synaptic devices. Applying L1 regularization eliminates the need for a re-training procedure because it prunes the weights based on the cost function. The compressed hardware-based SNN is energy efficient while achieving a classification accuracy of 97.85% which is comparable to 98.13% of the software deep neural networks (DNN).
C1 [Lee, Sung-Tae; Lim, Suhwan; Bae, Jong-Ho; Kwon, Dongseok; Kim, Hyeong-Su; Park, Byung-Gook; Lee, Jong-Ho] Seoul Natl Univ, Dept Elect & Comp Engn, Seoul 08826, South Korea.
   [Lee, Sung-Tae; Lim, Suhwan; Bae, Jong-Ho; Kwon, Dongseok; Kim, Hyeong-Su; Park, Byung-Gook; Lee, Jong-Ho] Seoul Natl Univ, ISRC Interuniv Semicond Res Ctr, Seoul 08826, South Korea.
RP Lee, JH (corresponding author), Seoul Natl Univ, Dept Elect & Comp Engn, Seoul 08826, South Korea.
CR Aghasi A, 2017, ADV NEURAL INFORM PR, P3177
   [Anonymous], 2012, ADV NEURAL INFORM PR
   Bae JH, 2019, IEEE J ELECTRON DEVI, V7, P335, DOI 10.1109/JEDS.2019.2898674
   Bae JH, 2017, IEEE ELECTR DEVICE L, V38, P1153, DOI 10.1109/LED.2017.2713460
   Choi KB, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00704
   Hu BT, 2014, ADV NEUR IN, V27
   Jeong DS, 2016, ADV ELECTRON MATER, V2, DOI 10.1002/aelm.201600090
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lee CY, 2015, JMLR WORKSH CONF PRO, V38, P562
   Lee JM, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00191
   Lee ST, 2018, 2018 IEEE SYMPOSIUM ON VLSI TECHNOLOGY, P169, DOI 10.1109/VLSIT.2018.8510667
   MEAD C, 1990, P IEEE, V78, P1629, DOI 10.1109/5.58356
   Prezioso M, 2015, NATURE, V521, P61, DOI 10.1038/nature14441
   Rathi N, 2018, IEEE T COMPUT AID D, V38, P668
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Wan X, 2016, IEEE ELECTR DEVICE L, V37, P299, DOI 10.1109/LED.2016.2517080
   Wang DT, 2016, IEEE ELECTR DEVICE L, V37, P878, DOI 10.1109/LED.2016.2570279
   Wang IT, 2016, NANOTECHNOLOGY, V27, DOI 10.1088/0957-4484/27/36/365204
NR 19
TC 3
Z9 3
U1 0
U2 25
PD NOV
PY 2020
VL 20
IS 11
BP 6603
EP 6608
DI 10.1166/jnn.2020.18772
UT WOS:000554982500003
DA 2023-11-16
ER

PT J
AU Rossello, JL
   Canals, V
   Morro, A
   Verd, J
AF Rossello, Josep L.
   Canals, Vincent
   Morro, Antoni
   Verd, Jaume
TI CHAOS-BASED MIXED SIGNAL IMPLEMENTATION OF SPIKING NEURONS
SO INTERNATIONAL JOURNAL OF NEURAL SYSTEMS
DT Article
DE Spiking Neural Networks; mixed signal neural networks; chaotic circuits
ID NEURAL-NETWORKS
AB A new design of Spiking Neural Networks is proposed and fabricated using a 0.35 mu m CMOS technology. The architecture is based on the use of both digital and analog circuitry. The digital circuitry is dedicated to the inter-neuron communication while the analog part implements the internal non-linear behavior associated to spiking neurons. The main advantages of the proposed system are the small area of integration with respect to digital solutions, its implementation using a standard CMOS process only and the reliability of the inter-neuron communication.
C1 [Rossello, Josep L.; Canals, Vincent; Morro, Antoni; Verd, Jaume] Univ Illes Balears, Dept Phys, Balears 07122, Spain.
RP Rossello, JL (corresponding author), Univ Illes Balears, Dept Phys, Campus UIB,Cra Valldemossa Km 7-5,Ed MO Palma de, Balears 07122, Spain.
EM j.rossello@uib.es
CR Alnajjar F, 2008, IEEE IJCNN, P2207, DOI 10.1109/IJCNN.2008.4634103
   Alnajjar F, 2008, IEEE IJCNN, P2200, DOI 10.1109/IJCNN.2008.4634102
   Caignet F, 2001, P IEEE, V89, P556, DOI 10.1109/5.920583
   Dowrick T, 2007, IEEE IJCNN, P715, DOI 10.1109/IJCNN.2007.4371045
   Ghosh-Dastidar S, 2007, INTEGR COMPUT-AID E, V14, P187
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Gupta A, 2007, IEEE IJCNN, P53, DOI 10.1109/IJCNN.2007.4370930
   Hsu CC, 1996, IEEE T NEURAL NETWOR, V7, P1339, DOI 10.1109/72.548163
   Iglesias J, 2008, INT J NEURAL SYST, V18, P267, DOI 10.1142/S0129065708001580
   Indiveri G, 2006, IEEE T NEURAL NETWOR, V17, P211, DOI 10.1109/TNN.2005.860850
   LANGLOIS N, 2000, P INT JOINT C NEUR N, V4, P485
   Loiselle S, 2005, IEEE IJCNN, P2076
   Malaka R., 2000, P INT JOINT C NEUR N, V6, P486, DOI 10.1109/IJCNN.2000.859442
   Pearson M. J., 2005, Proceedings. 2005 International Conference on Field Programmable Logic and Applications (IEEE Cat. No.05EX1155), P582
   Pearson MJ, 2007, IEEE T NEURAL NETWOR, V18, P1472, DOI 10.1109/TNN.2007.891203
   Renaud S, 2007, IEEE INT SYMP CIRC S, P3355, DOI 10.1109/ISCAS.2007.378286
   ROSENSTEIN MT, 1993, PHYSICA D, V65, P117, DOI 10.1016/0167-2789(93)90009-P
   Rosselló JL, 2008, IEICE ELECTRON EXPR, V5, P1042, DOI 10.1587/elex.5.1042
   Rosselló JL, 2008, IEICE ELECTRON EXPR, V5, P921, DOI 10.1587/elex.5.921
   Sala DM, 1999, IEEE T NEURAL NETWOR, V10, P953, DOI 10.1109/72.774270
   Torikai H, 2007, IEEE IJCNN, P2676
   XIN J, 2003, INT JOINT C NEUR NET, V2, P1189
NR 23
TC 31
Z9 31
U1 0
U2 4
PD DEC
PY 2009
VL 19
IS 6
BP 465
EP 471
DI 10.1142/S0129065709002166
UT WOS:000273174000007
DA 2023-11-16
ER

PT J
AU Stanojevic, A
   Wozniak, S
   Bellec, G
   Cherubini, G
   Pantazi, A
   Gerstner, W
AF Stanojevic, Ana
   Wozniak, Stanislaw
   Bellec, Guillaume
   Cherubini, Giovanni
   Pantazi, Angeliki
   Gerstner, Wulfram
TI An exact mapping from ReLU networks to spiking neural networks
SO NEURAL NETWORKS
DT Article
DE Spiking neural network; ReLU network; Temporal coding; Single -spike
   network; Deep network conversion
ID ERROR-BACKPROPAGATION
AB Deep spiking neural networks (SNNs) offer the promise of low-power artificial intelligence. However, training deep SNNs from scratch or converting deep artificial neural networks to SNNs without loss of performance has been a challenge. Here we propose an exact mapping from a network with Rectified Linear Units (ReLUs) to an SNN that fires exactly one spike per neuron. For our constructive proof, we assume that an arbitrary multi-layer ReLU network with or without convolutional layers, batch normalization and max pooling layers was trained to high performance on some training set. Furthermore, we assume that we have access to a representative example of input data used during training and to the exact parameters (weights and biases) of the trained ReLU network. The mapping from deep ReLU networks to SNNs causes zero percent drop in accuracy on CIFAR10, CIFAR100 and the ImageNet-like data sets Places365 and PASS. More generally our work shows that an arbitrary deep ReLU network can be replaced by an energy-efficient single-spike neural network without any loss of performance. (c) 2023 Published by Elsevier Ltd.
C1 [Stanojevic, Ana; Wozniak, Stanislaw; Cherubini, Giovanni; Pantazi, Angeliki] IBM Res Europe Zurich, Ruschlikon, Switzerland.
   [Stanojevic, Ana; Bellec, Guillaume; Gerstner, Wulfram] Ecole Polytech Fed Lausanne, Sch Life Sci, Lausanne, Switzerland.
   [Stanojevic, Ana; Bellec, Guillaume; Gerstner, Wulfram] Lausanne EPFL, Sch Comp & Commun Sci, Lausanne, Switzerland.
   [Stanojevic, Ana] Saumerstr 4, CH-8803 Ruschlikon, Switzerland.
RP Stanojevic, A (corresponding author), Saumerstr 4, CH-8803 Ruschlikon, Switzerland.
EM ans@zurich.ibm.com
CR [Anonymous], 2022, APPL VGG16
   Asano Y. M., 2021, NEURIPS TRACK DATASE
   Attwell D, 2001, J CEREBR BLOOD F MET, V21, P1133, DOI 10.1097/00004647-200110000-00001
   Bellec G, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-17236-y
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Boroumand A, 2021, INT CONFER PARA, P159, DOI 10.1109/PACT52795.2021.00019
   Brown Tom, 2020, NEURIPS, V1, P3
   Bu T., 2022, ICLR
   Burr GW, 2017, ADV PHYS-X, V2, P89, DOI 10.1080/23746149.2016.1259585
   Comsa JM, 2020, INT CONF ACOUST SPEE, P8529, DOI [10.1109/icassp40776.2020.9053856, 10.1109/ICASSP40776.2020.9053856]
   Davies M, 2021, P IEEE, V109, P911, DOI 10.1109/JPROC.2021.3067593
   Diehl PU, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON REBOOTING COMPUTING (ICRC)
   Furui S, 2012, IEEE SIGNAL PROC MAG, V29, P16, DOI 10.1109/MSP.2012.2209906
   Howard AG, 2017, Arxiv, DOI arXiv:1704.04861
   Gallego G, 2022, IEEE T PATTERN ANAL, V44, P154, DOI 10.1109/TPAMI.2020.3008413
   Gardner B, 2015, NEURAL COMPUT, V27, P2548, DOI 10.1162/NECO_a_00790
   Gerstner W, 1998, PULSED NEURAL NETWORKS, P3
   Gerstner W, 2002, SPIKING NEURON MODEL, DOI [10.1017/CBO9780511815706, DOI 10.1017/CBO9780511815706]
   Göltz J, 2021, NAT MACH INTELL, V3, P823, DOI 10.1038/s42256-021-00388-x
   Gollisch T, 2008, SCIENCE, V319, P1108, DOI 10.1126/science.1149639
   Goltz J., 2020, P NEUR COMP EL WORKS, P1
   Hubara I, 2016, ADV NEUR IN, V29
   Huh D, 2018, ADV NEUR IN, V31
   Hung CP, 2005, SCIENCE, V310, P863, DOI 10.1126/science.1117593
   Jiang Ziheng, 2018, ACM SYSML
   Johansson RS, 2004, NAT NEUROSCI, V7, P170, DOI 10.1038/nn1177
   Kheradpisheh SR, 2020, INT J NEURAL SYST, V30, DOI 10.1142/S0129065720500276
   Kingma D. P., 2015, 3 INT C LEARNING REP
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Kubke MF, 2002, J NEUROSCI, V22, P7671
   Lennie P, 2003, CURR BIOL, V13, P493, DOI 10.1016/S0960-9822(03)00135-0
   Marc-Oliver G., 2007, SCHOLARPEDIA, DOI [DOI 10.4249/SCHOLARPEDIA.1430, 10.4249/scholarpedia.1430]
   Mirsadeghi M, 2021, NEUROCOMPUTING, V427, P131, DOI 10.1016/j.neucom.2020.11.052
   Mostafa H, 2018, IEEE T NEUR NET LEAR, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   OPTICAN LM, 1987, J NEUROPHYSIOL, V57, P162, DOI 10.1152/jn.1987.57.1.162
   Park S, 2020, DES AUT CON, DOI [10.1109/dac18072.2020.9218689, 10.1007/s00779-020-01476-2]
   Patterson D, 2021, Arxiv, DOI [arXiv:2104.10350, 10.48550/ARXIV.2104.10350]
   Pillow JW, 2008, NATURE, V454, P995, DOI 10.1038/nature07140
   Rieke F., 1996, SPIKES EXPLORING NEU
   Rueckauer B, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351295
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sebastian A, 2018, J APPL PHYS, V124, DOI 10.1063/1.5042413
   Sorbaro M, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00662
   Stanojevic A, 2023, Arxiv, DOI arXiv:2306.08744
   Stanojevic A, 2022, IEEE IMAGE PROC, P1901, DOI 10.1109/ICIP46576.2022.9897692
   Stöckl C, 2021, NAT MACH INTELL, V3, DOI 10.1038/s42256-021-00311-4
   Strubell E, 2020, AAAI CONF ARTIF INTE, V34, P13693
   Tan MX, 2019, PR MACH LEARN RES, V97
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Thorpe S, 1996, NATURE, V381, P520, DOI 10.1038/381520a0
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   Wang XF, 2020, IEEE COMMUN SURV TUT, V22, P869, DOI 10.1109/COMST.2020.2970550
   Widmer S, 2023, IEEE T CIRCUITS-II, V70, P3639, DOI 10.1109/TCSII.2023.3277784
   Wozniak S, 2020, NAT MACH INTELL, V2, P325, DOI 10.1038/s42256-020-0187-0
   Xiao H, 2017, Arxiv, DOI arXiv:1708.07747
   Yamins DLK, 2016, NAT NEUROSCI, V19, P356, DOI 10.1038/nn.4244
   Yan ZL, 2021, AAAI CONF ARTIF INTE, V35, P10577
   Yang K., 2022, P MACHINE LEARNING R, V162, P25313
   Yang KY, 2022, PR MACH LEARN RES
   Zenke F, 2021, NEURAL COMPUT, V33, P899, DOI 10.1162/neco_a_01367
   Zenke F, 2018, NEURAL COMPUT, V30, P1514, DOI 10.1162/neco_a_01086
   Zhang L, 2019, AAAI CONF ARTIF INTE, P1319
   Zhang ML, 2022, IEEE T NEUR NET LEAR, V33, P1947, DOI 10.1109/TNNLS.2021.3110991
   Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009
NR 66
TC 0
Z9 0
U1 2
U2 2
PD NOV
PY 2023
VL 168
BP 74
EP 88
DI 10.1016/j.neunet.2023.09.011
EA SEP 2023
UT WOS:001083547800001
DA 2023-11-16
ER

PT J
AU Thomas, PJ
AF Thomas, Peter J.
TI Commentary on Structured chaos shapes spike-response noise entropy in
   balanced neural networks, by Lajoie, Thivierge, and Shea-Brown
SO FRONTIERS IN COMPUTATIONAL NEUROSCIENCE
DT Editorial Material
DE spike-response noise entroy; balanced neural networks; information
   theory; chaos; mutual information
ID SYSTEM
C1 Case Western Reserve Univ, Dept Math Appl Math & Stat, Cleveland, OH 44106 USA.
RP Thomas, PJ (corresponding author), Case Western Reserve Univ, Dept Math Appl Math & Stat, Cleveland, OH 44106 USA.
EM pjthomas@case.edu
CR Andersen P., 2006, HIPPOCAMPUS BOOK
   Brown E, 2004, NEURAL COMPUT, V16, P673, DOI 10.1162/089976604322860668
   Carnevale N.T., 2006, NEURON BOOK, DOI DOI 10.1017/CBO9780511541612
   Chiel HJ, 1997, TRENDS NEUROSCI, V20, P553, DOI 10.1016/S0166-2236(97)01149-1
   CHURCHLAND P.S., 1992, COMPUTATIONAL BRAIN, DOI 10.7551/mitpress/2010.001.0001
   Ermentrout B, 1996, NEURAL COMPUT, V8, P979, DOI 10.1162/neco.1996.8.5.979
   Ermentrout B., 2008, SCHOLARPEDIA, V3, P1398, DOI [10.4249/scholarpedia.1398, DOI 10.4249/SCHOLARPEDIA.1398]
   ERMENTROUT GB, 1986, SIAM J APPL MATH, V46, P233, DOI 10.1137/0146017
   Lajoie G, 2014, FRONT COMPUT NEUROSC, V8, DOI 10.3389/fncom.2014.00123
   Lajoie G, 2013, PHYS REV E, V87, DOI 10.1103/PhysRevE.87.052901
   Monteforte M, 2010, PHYS REV LETT, V105, DOI 10.1103/PhysRevLett.105.268104
   Wolf F, 2014, CURR OPIN NEUROBIOL, V25, P228, DOI 10.1016/j.conb.2014.01.017
NR 12
TC 0
Z9 0
U1 0
U2 4
PD MAR 10
PY 2015
VL 9
AR 23
DI 10.3389/fncom.2015.00023
UT WOS:000352462500001
DA 2023-11-16
ER

PT C
AU Maass, W
AF Maass, W
BE Silva, FL
   Principe, JC
   Almeida, LB
TI Analog computations with temporal coding in networks of spiking neurons
SO SPATIOTEMPORAL MODELS IN BIOLOGICAL AND ARTIFICIAL SYSTEMS
SE FRONTIERS IN ARTIFICIAL INTELLIGENCE AND APPLICATIONS
DT Proceedings Paper
CT Sintra Workshop on Spatiotemporal Models in Biological and Artificial
   Systems
CY NOV 06-08, 1996
CL SINTRA, PORTUGAL
AB We show that theoretical models for networks of spiking neurons carl simulate with temporal coding arbitrary sigmoidal neural nets. Furthermore it is shown that networks of spiking neurons with temporal coding have a strictly larger computational power than sigmoidal neural nets with the same number of units.
RP Maass, W (corresponding author), GRAZ TECH UNIV,INST THEORET COMP SCI,KLOSTERWIESGASSE 32-2,A-8010 GRAZ,AUSTRIA.
NR 0
TC 0
Z9 0
U1 0
U2 1
PY 1997
VL 37
BP 97
EP 104
UT WOS:A1997BG92H00013
DA 2023-11-16
ER

PT J
AU Haessig, G
   Cassidy, A
   Alvarez, R
   Benosman, R
   Orchard, G
AF Haessig, Germain
   Cassidy, Andrew
   Alvarez, Rodrigo
   Benosman, Ryad
   Orchard, Garrick
TI Spiking Optical Flow for Event-Based Sensors Using IBM's TrueNorth
   Neurosynaptic System
SO IEEE TRANSACTIONS ON BIOMEDICAL CIRCUITS AND SYSTEMS
DT Article
DE Event-based imaging; neuromorphic hardware; neuromorphic vision; optical
   flow; spiking neural network
AB This paper describes a fully spike-based neural network for optical flow estimation from dynamic vision sensor data. A low power embedded implementation of the method, which combines the asynchronous time-based image sensor with IBM's TrueNorth Neurosynaptic System, is presented. The sensor generates spikes with submillisecond resolution in response to scene illumination changes. These spike are processed by a spiking neural network running on TrueNorth with a 1-ms resolution to accurately determine the order and time difference of spikes from neighbouring pixels, and therefore infer the velocity. The spiking neural network is a variant of the Barlow Levick method for optical flow estimation. The system is evaluated on two recordings for which ground truth motion is available, and achieves an average endpoint error of 11% at an estimated power budget of under 80 mW for the sensor and computation.
C1 [Haessig, Germain; Benosman, Ryad] Sorbonne Univ, Inst Vis, Vis & Nat Computat Team, F-75012 Paris, France.
   [Cassidy, Andrew; Alvarez, Rodrigo] IBM Res, Austin, TX 78758 USA.
   [Orchard, Garrick] Natl Univ Singapore, Temasek Labs, Singapore 117456, Singapore.
   [Orchard, Garrick] Natl Univ Singapore, Singapore Inst Neurotechnol, Singapore 117456, Singapore.
RP Haessig, G (corresponding author), Sorbonne Univ, Inst Vis, Vis & Nat Computat Team, F-75012 Paris, France.
EM germain.haessig@inserm.fr; andrewca@us.ibm.com; arodrigo@us.ibm.com;
   ryad.benosman@upmc.fr; garrickorchard@gmail.com
CR Alvarez L, 1999, LECT NOTES COMPUT SC, V1682, P235
   Amir A, 2013, IEEE IJCNN
   [Anonymous], 1986, VLSI SIGNAL PROCESSI
   [Anonymous], NASATM84352 AM RES C
   [Anonymous], BIOINSPIRED OPTIC FL
   [Anonymous], CORR
   [Anonymous], 2013, 2013 INT JOINT C NEU
   [Anonymous], SPIRAL OPTICAL FLOW
   [Anonymous], PIPE OPTICAL FLOW RE
   [Anonymous], LAB SCENE OPTICAL FL
   Baker S, 2011, INT J COMPUT VISION, V92, P1, DOI 10.1007/s11263-010-0390-2
   BARDOW P, 2016, PROC CVPR IEEE, P884, DOI DOI 10.1109/CVPR.2016.102
   BARLOW HB, 1965, J PHYSIOL-LONDON, V178, P477, DOI 10.1113/jphysiol.1965.sp007638
   Barranco F, 2014, P IEEE, V102, P1537, DOI 10.1109/JPROC.2014.2347207
   BARRON JL, 1994, INT J COMPUT VISION, V12, P43, DOI 10.1007/BF01420984
   Benosman R, 2012, NEURAL NETWORKS, V27, P32, DOI 10.1016/j.neunet.2011.11.001
   Boahen KA, 2000, IEEE T CIRCUITS-II, V47, P416, DOI 10.1109/82.842110
   Brosch T, 2015, P 9 EAI INT C BIOINS, P551, DOI [10.4108/eai.3-12-2015.2262447, DOI 10.4108/EAI.3-12-2015.2262447]
   DELBRUCK T, 1993, IEEE T NEURAL NETWOR, V4, P529, DOI 10.1109/72.217194
   Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316
   Giulioni M, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00035
   HEEGER DJ, 1987, J OPT SOC AM A, V4, P1455, DOI 10.1364/JOSAA.4.001455
   Honegger D, 2013, IEEE INT CONF ROBOT, P1736, DOI 10.1109/ICRA.2013.6630805
   Joesch M, 2010, NATURE, V468, P300, DOI 10.1038/nature09545
   LETTVIN JY, 1959, P IRE, V47, P1940, DOI 10.1109/JRPROC.1959.287207
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Orchard G, 2014, P IEEE, V102, P1520, DOI 10.1109/JPROC.2014.2346763
   Orchard G, 2013, BIOMED CIRC SYST C, P298, DOI 10.1109/BioCAS.2013.6679698
   Posch C, 2014, P IEEE, V102, P1470, DOI 10.1109/JPROC.2014.2346153
   Posch C, 2011, IEEE J SOLID-ST CIRC, V46, P259, DOI 10.1109/JSSC.2010.2085952
   Sutton M.A., 1983, IMAGE VISION COMPUT, V1, P133, DOI DOI 10.1016/0262-8856(83)90064-1
NR 31
TC 43
Z9 47
U1 4
U2 20
PD AUG
PY 2018
VL 12
IS 4
BP 860
EP 870
DI 10.1109/TBCAS.2018.2834558
UT WOS:000442350600014
DA 2023-11-16
ER

PT C
AU Kubota, N
   Tang, DL
   Obo, T
   Wakisaka, S
AF Kubota, Naoyuki
   Tang, Dalai
   Obo, Takenori
   Wakisaka, Shiho
GP IEEE
TI Localization of Human Based on Fuzzy Spiking Neural Network in
   Informationally Structured Space
SO 2010 IEEE INTERNATIONAL CONFERENCE ON FUZZY SYSTEMS (FUZZ-IEEE 2010)
SE IEEE International Conference on Fuzzy Systems
DT Proceedings Paper
CT 2010 IEEE World Congress on Computational Intelligence
CY JUL 18-23, 2010
CL Barcelona, SPAIN
DE Human Localization; Sensor Networks; Robot Partners; Neural Networks;
   Fuzzy Theory
AB This paper proposes a human localization method in informationally structured space based on sensor network. First, we explain informationally structured space, robot partners, and sensor networks developed in this study. Next, we apply a fuzzy spiking neural network to extract a person from the measured data by the sensor network. Furthermore, we propose a learning method of fuzzy spiking neural network based on the time series of measured data. Finally, we discuss the effectiveness of the proposed methods through experimental results in a living room.
C1 [Kubota, Naoyuki; Tang, Dalai; Obo, Takenori; Wakisaka, Shiho] Tokyo Metropolitan Univ, Dept Syst Design, Tokyo 158, Japan.
RP Kubota, N (corresponding author), Tokyo Metropolitan Univ, Dept Syst Design, 6-6 Asahigaoka, Tokyo 158, Japan.
EM kubota@tmu.ac.jp; tang@sd.tmu.ac.jp; oobo-takenori@sd.tmu.ac.jp;
   wakisaka-shiho@sd.tmu.ac.jp
CR Anderson J.A., 1988, NEUROCOMPUTING
   [Anonymous], 1997, NEURO FUZZY SOFT COM
   Attygalle S, 2008, 2008 VIRTUAL REHABILITATION, P168, DOI 10.1109/ICVR.2008.4625155
   Corderio C., 2006, AD HOC  SENSOR NETWO
   Duff M, 2008, IEEE ENG MED BIO, P4150, DOI 10.1109/IEMBS.2008.4650123
   Gerstner W, 1998, PULSED NEURAL NETWORKS, P3
   GERSTNER W, 1999, PULSED NEURAL NETWOR, P3
   Khemapech I., 2005, P 6 ANN POSTGRADUATE
   KIM JH, 2004, P 2 INT C AUT ROB AG, P1
   Kubota N, 2005, INFORM SCIENCES, V171, P403, DOI 10.1016/j.ins.2004.09.012
   Kubota Naoyuki, 2009, RO-MAN 2009 - The 18th IEEE International Symposium on Robot and Human Interactive Communication, P346, DOI 10.1109/ROMAN.2009.5326082
   Kubota N., 2005, P 6 INT S ADV INT SY, P577
   Kubota N., 2010, WCCI 2010 IN PRESS
   Kubota N., 2006, PROC JOINT 3RDINT C, P737
   Kubota N., 2006, P SICE ICCAS 2006, P4861
   Kubota N., 2004, P 2 INT C AUT ROB AG, P164
   Kubota N, 2006, INT J COMPUT SCI NET, V6, P19
   Kubota N, 2007, IEEE T IND ELECTRON, V54, P866, DOI 10.1109/TIE.2007.891644
   Maass W., 1999, PULSED NEURAL NETWOR
   Satomi M., 2009, HIER GROW NEUR GAS I
   Szekeres S.F., 1987, FRAMEWORK COGNITIVE, P87
NR 21
TC 5
Z9 5
U1 0
U2 1
PY 2010
UT WOS:000287453602115
DA 2023-11-16
ER

PT J
AU Makarov, VA
   Panetsos, F
   de Feo, O
AF Makarov, VA
   Panetsos, F
   de Feo, O
TI A method for determining neural connectivity and inferring the
   underlying network dynamics using extracellular spike recordings
SO JOURNAL OF NEUROSCIENCE METHODS
DT Article
DE neural circuits; spike trains; connectivity identification; network
   modeling
ID DIRECTED TRANSFER-FUNCTION; CAUSAL RELATIONS; MODEL; IDENTIFICATION;
   SYSTEMS; TRAINS
AB In the present paper we propose a novel method for the identification and modeling of neural networks using extracellular spike recordings. We create a deterministic model of the effective network, whose dynamic behavior fits experimental data. The network obtained by our method includes explicit mathematical models of each of the spiking neurons and a description of the effective connectivity between them. Such a model allows us to study the properties of the neuron ensemble independently from the original data. It also permits to infer properties of the ensemble that cannot be directly obtained from the observed spike trains. The performance of the method is tested with spike trains artificially generated by a number of different neural networks. (c) 2004 Elsevier B.V. All rights reserved.
C1 Univ Complutense Madrid, Sch Opt, Dept Math Appl, Neurosci Lab, Madrid, Spain.
   Swiss Fed Inst Technol, IC, LANOS, Nonlinear Syst Lab, CH-1015 Lausanne, Switzerland.
RP Makarov, VA (corresponding author), Univ Complutense Madrid, Sch Opt, Dept Math Appl, Neurosci Lab, Avda Arcos Jalon S-N, Madrid, Spain.
EM vmakarov@opt.ucm.es
CR AERTSEN A, 1991, NONLIN SYST, V2, P281
   BRILLINGER DR, 1976, BIOL CYBERN, V22, P213, DOI 10.1007/BF00365087
   CIZEK P, 2001, SONDERFORSCHUNGSBERE, V373, P25
   Dahlhaus R, 1997, J NEUROSCI METH, V77, P93, DOI 10.1016/S0165-0270(97)00100-3
   Eichler M, 2003, BIOL CYBERN, V89, P289, DOI 10.1007/s00422-003-0400-3
   Ermentrout B, 1996, NEURAL COMPUT, V8, P979, DOI 10.1162/neco.1996.8.5.979
   ERMENTROUT GB, 1982, LECT NOTES BIOMATH, P45
   FITZHUGH R, 1961, BIOPHYS J, V1, P445, DOI 10.1016/S0006-3495(61)86902-6
   GERSTNER W, 1995, PHYS REV E, V51, P738, DOI 10.1103/PhysRevE.51.738
   GETTING PA, 1989, METHODS NEURONAL MOD, P171
   GRANGER CWJ, 1969, ECONOMETRICA, V37, P424, DOI 10.2307/1912791
   Harris KD, 2000, J NEUROPHYSIOL, V84, P401, DOI 10.1152/jn.2000.84.1.401
   HINDMARSH JL, 1984, PROC R SOC SER B-BIO, V221, P87, DOI 10.1098/rspb.1984.0024
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Horwitz B, 2003, NEUROIMAGE, V19, P466, DOI 10.1016/S1053-8119(03)00112-5
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Jolivet R, 2004, J NEUROPHYSIOL, V92, P959, DOI 10.1152/jn.00190.2004
   Kaminski M, 2001, BIOL CYBERN, V85, P145, DOI 10.1007/s004220000235
   Kandel E. R., 2000, PRINCIPLES NEURAL SC, V4
   Korzeniewska A, 2003, J NEUROSCI METH, V125, P195, DOI 10.1016/S0165-0270(03)00052-9
   MAKAROV VA, 2004, IDENTIFICATION NEURA
   MOORE GP, 1966, ANNU REV PHYSIOL, V28, P493, DOI 10.1146/annurev.ph.28.030166.002425
   Motulasky H., 2003, THEORY, DOI [10.5206/mase/10847, DOI 10.1007/BFB0067700]
   Paninski L, 2004, NETWORK-COMP NEURAL, V15, P243, DOI 10.1088/0954-898X/15/4/002
   Paninski L, 2003, NEUROCOMPUTING, V52-4, P877, DOI 10.1016/S0925-2312(02)00819-6
   Pavlov AN, 2001, PHYS REV E, V63, DOI 10.1103/PhysRevE.63.036205
   PERKEL DH, 1967, BIOPHYS J, V7, P419, DOI 10.1016/S0006-3495(67)86597-4
   PERKEL DH, 1967, BIOPHYS J, V7, P391, DOI 10.1016/S0006-3495(67)86596-2
   Pillow JW, 2003, NEUROCOMPUTING, V52-4, P109, DOI 10.1016/S0925-2312(02)00822-6
   PLANT RE, 1981, J MATH BIOL, V11, P15, DOI 10.1007/BF00275821
   Racicot DM, 1997, PHYSICA D, V104, P184, DOI 10.1016/S0167-2789(97)00296-0
   ROSENBERG JR, 1989, PROG BIOPHYS MOL BIO, V53, P1, DOI 10.1016/0079-6107(89)90004-7
   Sameshima K, 1999, J NEUROSCI METH, V94, P93, DOI 10.1016/S0165-0270(99)00128-4
   SAUER T, 1994, PHYS REV LETT, V72, P3811, DOI 10.1103/PhysRevLett.72.3811
   Segundo JP, 2003, INT J BIFURCAT CHAOS, V13, P2035, DOI 10.1142/S0218127403007886
   STEIN RB, 1967, BIOPHYS J, V7, P37, DOI 10.1016/S0006-3495(67)86574-3
   VANVREESWIJK C, 1994, J COMPUT NEUROSCI, V1, P303
NR 38
TC 49
Z9 50
U1 0
U2 4
PD JUN 15
PY 2005
VL 144
IS 2
BP 265
EP 279
DI 10.1016/j.jneumeth.2004.11.013
UT WOS:000229704000016
DA 2023-11-16
ER

PT C
AU Soures, NM
   Kudithipudi, D
   Jacobs-Gedrim, RB
   Agarwal, S
   Marinella, M
AF Soures, N. M.
   Kudithipudi, D.
   Jacobs-Gedrim, R. B.
   Agarwal, S.
   Marinella, M.
BE Roozeboom, F
   Timans, PJ
   Kakushima, K
   Jagannathan, H
   Karim, Z
   Gusev, EP
   DeGendt, S
TI Enabling On-Device Learning with Deep Spiking Neural Networks for Speech
   Recognition
SO SILICON COMPATIBLE MATERIALS, PROCESSES, AND TECHNOLOGIES FOR ADVANCED
   INTEGRATED CIRCUITS AND EMERGING APPLICATIONS 8
SE ECS Transactions
DT Proceedings Paper
CT Symposium on Silicon Compatible Materials, Processes, and Technologies
   for Advanced Integrated Circuits and Emerging Applications held as part
   of the 233rd Meeting of The Electrochemical-Society (ECS)
CY MAY 13-17, 2018
CL Seattle, WA
AB Spiking recurrent neural networks are gaining traction in solving complex temporal tasks. In general, spiking neural networks are resilient and computationally powerful. These intrinsic properties make them attractive for learning on edge devices. In this work, we propose a semi-supervised deep spiking neural network (deep-liquid state machine) that can be deployed on embedded devices. We demonstrate a high-level memristor based neuromorphic architecture for the proposed deep spiking network. An experimental TiN-TaOx-TaTiN memristor device stack model is used for analyzing the overall architecture performance. An accuracy of 95.83 +/- 1.74% is achieved for a speech recognition task, on the standard TIMIT dataset. To study the robustness of the proposed architecture, the accuracy of the network was tested with and without noise in the memristor devices.
C1 [Soures, N. M.; Kudithipudi, D.] Rochester Inst Technol, Dept Comp Engn, Rochester, NY 14623 USA.
   [Jacobs-Gedrim, R. B.; Agarwal, S.; Marinella, M.] Sandia Natl Labs, Ctr Comp Res, Albuquerque, NM 87123 USA.
RP Soures, NM (corresponding author), Rochester Inst Technol, Dept Comp Engn, Rochester, NY 14623 USA.
CR Agarwal S, 2016, IEEE IJCNN, P929, DOI 10.1109/IJCNN.2016.7727298
   Deng ZD, 2007, IEEE T NEURAL NETWOR, V18, P1364, DOI 10.1109/TNN.2007.894082
   Garofolo J.S., 1993, 93 NASA STIRECON
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hu M, 2011, ASIA S PACIF DES AUT, DOI 10.1109/ASPDAC.2011.5722193
   Jaeger H, 2001, 14834 GMD GERM NAT R, V148, P34
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Maass W, 2004, J COMPUT SYST SCI, V69, P593, DOI 10.1016/j jcss.2004.04.001
   Markram Henry, 1998, P NATL ACAD SCI US P, V95, P9
   Norton D, 2010, NEUROCOMPUTING, V73, P2893, DOI 10.1016/j.neucom.2010.08.005
   Renart A, 2003, NEURON, V38, P473, DOI 10.1016/S0896-6273(03)00255-1
   Soures N., 2017, CIRC SYST MWSCAS 201
   Tseng Y. H., 2010, IEDM, P636
   Zyarah AM, 2017, IEEE INT SYMP CIRC S
NR 14
TC 1
Z9 1
U1 0
U2 5
PY 2018
VL 85
IS 6
BP 127
EP 137
DI 10.1149/08506.0127ecst
UT WOS:000542940500015
DA 2023-11-16
ER

PT C
AU Alemi, A
   Machens, CK
   Denève, S
   Slotine, JJ
AF Alemi, Aireza
   Machens, Christian K.
   Deneve, Sophie
   Slotine, Jean-Jacques
GP AAAI
TI Learning Nonlinear Dynamics in Efficient, Balanced Spiking Networks
   Using Local Plasticity Rules
SO THIRTY-SECOND AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE / THIRTIETH
   INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE / EIGHTH
   AAAI SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE
SE AAAI Conference on Artificial Intelligence
DT Proceedings Paper
CT 32nd AAAI Conference on Artificial Intelligence / 30th Innovative
   Applications of Artificial Intelligence Conference / 8th AAAI Symposium
   on Educational Advances in Artificial Intelligence
CY FEB 02-07, 2018
CL New Orleans, LA
AB The brain uses spikes in neural circuits to perform many dynamical computations. The computations are performed with properties such as spiking efficiency, i.e. minimal number of spikes, and robustness to noise. A major obstacle for learning computations in artificial spiking neural networks with such desired biological properties is due to lack of our understanding of how biological spiking neural networks learn computations.
   Here, we consider the credit assignment problem, i.e. determining the local contribution of each synapse to the network's global output error, for learning nonlinear dynamical computations in a spiking network with the desired properties of biological networks. We approach this problem by fusing the theory of efficient, balanced neural networks (EBN) with nonlinear adaptive control theory to propose a local learning rule. Locality of learning rules are ensured by feeding back into the network its own error, resulting in a learning rule depending solely on presynaptic inputs and error feedbacks. The spiking efficiency and robustness of the network are guaranteed by maintaining a tight excitatory/inhibitory balance, ensuring that each spike represents a local projection of the global output error and minimizes a loss function. The resulting networks can learn to implement complex dynamics with very small numbers of neurons and spikes, exhibit the same spike train variability as observed experimentally, and are extremely robust to noise and neuronal loss.
C1 [Alemi, Aireza; Deneve, Sophie] Ecole Normale Super, Paris, France.
   [Alemi, Aireza] Univ Calif Davis, Davis, CA 95616 USA.
   [Machens, Christian K.] Champalimaud Ctr, Lisbon, Portugal.
   [Slotine, Jean-Jacques] MIT, Cambridge, MA 02139 USA.
RP Alemi, A (corresponding author), Ecole Normale Super, Paris, France.; Alemi, A (corresponding author), Univ Calif Davis, Davis, CA 95616 USA.
EM alemi@ucdavis.edu
CR Abbott LF, 2016, NAT NEUROSCI, V19, P350, DOI 10.1038/nn.4241
   [Anonymous], 2017, ARXIV170206463
   [Anonymous], LEARNING REPRESENT S
   Barrett DGT, 2016, ELIFE, V5, DOI 10.7554/eLife.12454
   Boahen K, 2017, COMPUT SCI ENG, V19, P14, DOI 10.1109/MCSE.2017.33
   Boerlin M, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003258
   Bourdoukan R., 2012, ADV NEURAL INFORM PR
   Bourdoukan R, 2015, ADV NEURAL INFORM PR, V28, P982
   Chalk M, 2016, ELIFE, V5, DOI 10.7554/eLife.13824
   Cheah CC, 2006, INT J ROBOT RES, V25, P283, DOI 10.1177/0278364906063830
   Den`eve S., 2016, NATURE NEUROSCIENCE
   DeWolf T, 2016, P ROY SOC B-BIOL SCI, V283, DOI 10.1098/rspb.2016.2134
   Eliasmith C., 2004, NEURAL ENG COMPUTATI
   Eliasmith C., 2005, UNIFIED APPROACH BUI, P1
   Laughlin SB, 1998, NAT NEUROSCI, V1, P36, DOI 10.1038/236
   Memmesheimer RM, 2014, NEURON, V82, P925, DOI 10.1016/j.neuron.2014.03.026
   Poirazi P, 2003, NEURON, V37, P977, DOI 10.1016/S0896-6273(03)00148-X
   Poirazi P, 2003, NEURON, V37, P989, DOI 10.1016/S0896-6273(03)00149-1
   SANNER RM, 1992, IEEE T NEURAL NETWOR, V3, P837, DOI 10.1109/72.165588
   Schuman CD., 2017, ARXIV
   Slotine J.-J.E., 1991, APPL NONLINEAR CONTR
   SLOTINE JJE, 1986, INT J CONTROL, V43, P1631, DOI 10.1080/00207178608933564
   Sussillo D, 2009, NEURON, V63, P544, DOI 10.1016/j.neuron.2009.07.018
   Thalmeier D, 2016, PLOS COMPUT BIOL, V12, DOI 10.1371/journal.pcbi.1004895
   Vertechi P., 2014, ADV NEURAL INFORM PR
   Voelker A.R., 2017, ARXIV170808133
   Voelker AR, 2017, IEEE INT SYMP CIRC S, P2086
   Wang W, 2006, NEUROCOMPUTING, V69, P2320, DOI 10.1016/j.neucom.2005.04.012
   Werbos P.J., 1990, P IEEE
   Wolpert D. M., 2000, NATURE NEUROSCIENCE
NR 30
TC 11
Z9 11
U1 0
U2 0
PY 2018
BP 588
EP 595
UT WOS:000485488900073
DA 2023-11-16
ER

PT B
AU Adeli, H
   Ghosh-Dastidar, S
   Dadmehr, N
AF Adeli, Hojjat
   Ghosh-Dastidar, Samanwoy
   Dadmehr, Nahid
BA Adeli, H
   GhoshDastidar, S
BF Adeli, H
   GhoshDastidar, S
TI Spiking Neural Networks: Spiking Neurons and Learning Algorithms
SO AUTOMATED EEG-BASED DIAGNOSIS OF NEUROLOGICAL DISORDERS: INVENTING THE
   FUTURE OF NEUROLOGY
DT Article; Book Chapter
RP Adeli, H (corresponding author), Ohio State Univ, Columbus, OH 43210 USA.
NR 0
TC 1
Z9 1
U1 0
U2 4
PY 2010
BP 241
EP 269
UT WOS:000285137300013
DA 2023-11-16
ER

PT J
AU Uchida, H
   Oishi, Y
   Saito, T
AF Uchida, Hiroaki
   Oishi, Yuya
   Saito, Toshimichi
TI A SIMPLE DIGITAL SPIKING NEURAL NETWORK: SYNCHRONIZATION AND SPIKE-TRAIN
   APPROXIMATION
SO DISCRETE AND CONTINUOUS DYNAMICAL SYSTEMS-SERIES S
DT Article
DE Neural networks; Digital spiking neurons; Synchronization; Stability
AB This paper studies synchronization phenomena of spike-trains and approximation of target spike-trains in a simple network of digital spiking neurons. Repeating integrate-and-fire behavior between a periodic base signal and constant firing threshold, the neurons can generate various spike-trains. Connecting multiple neurons by cross-firing with delay, the network is constructed. The network can exhibit multi-phase synchronization of various spike-trains. Stability of the synchronization phenomena can be guaranteed theoretically. Applying a simple winner-take-all switching, the network can approximate target spike-trains automatically. In order to evaluate the approximation performance, we present two metrics: spike-position error and spike missing rate. Using the metrics, approximation capability of the network is investigated in typical target signals. Presenting an FPGA based hardware prototype, typical synchronization phenomenon and spike-train approximation are confirmed experimentally.
C1 [Uchida, Hiroaki; Oishi, Yuya; Saito, Toshimichi] Hosei Univ, Fac Sci & Engn, Dept Elect & Elect Engn, Koganei, Tokyo 1848584, Japan.
RP Saito, T (corresponding author), Hosei Univ, Fac Sci & Engn, Dept Elect & Elect Engn, Koganei, Tokyo 1848584, Japan.
EM atsuken5926@gmail.com; yuya.oishi.0411@gmail.com; tsaito@hosei.ac.jp
CR [Anonymous], 2006, NONLINEAR DYNAMICS P
   Appeltant L, 2011, NAT COMMUN, V2, DOI 10.1038/ncomms1476
   Campbell SR, 1999, NEURAL COMPUT, V11, P1595, DOI 10.1162/089976699300016160
   Iguchi T, 2010, IEICE T FUND ELECTR, VE93A, P1486, DOI 10.1587/transfun.E93.A.1486
   Lasota A., 1994, CHAOS FRACTALS NOISE
   Lee G, 2001, NEURAL NETWORKS, V14, P115, DOI 10.1016/S0893-6080(00)00083-6
   Lozano A, 2016, SCI REP-UK, V6, DOI 10.1038/srep23622
   Matoba A, 2013, IEICE T FUND ELECTR, VE96A, P1808, DOI 10.1587/transfun.E96.A.1808
   MAY RM, 1976, NATURE, V261, P459, DOI 10.1038/261459a0
   Ott E., 2002, CHAOS DYNAMICAL SYST, V2nd edn
   PEREZ R, 1982, PHYS LETT A, V90, P441, DOI 10.1016/0375-9601(82)90391-7
   Sato R, 2017, NEUROCOMPUTING, V248, P19, DOI 10.1016/j.neucom.2016.10.084
   Schüle M, 2012, CHAOS, V22, DOI 10.1063/1.4771662
   Tanaka G, 2019, NEURAL NETWORKS, V115, P100, DOI 10.1016/j.neunet.2019.03.005
   Torikai H, 2004, IEEE T NEURAL NETWOR, V15, P337, DOI 10.1109/TNN.2004.824403
   Torikai H, 2002, INT J BIFURCAT CHAOS, V12, P1207, DOI 10.1142/S0218127402005054
   Torikai H, 2001, IEEE T CIRCUITS-I, V48, P1198, DOI 10.1109/81.956014
   Torikai H, 2008, NEURAL NETWORKS, V21, P140, DOI 10.1016/j.neunet.2007.12.045
   Torikai H, 2006, IEEE T CIRCUITS-II, V53, P734, DOI 10.1109/TCSII.2006.876381
   Uchida H, 2019, IEICE T FUND ELECTR, VE102A, P235, DOI 10.1587/transfun.E102.A.235
   Wada M, 2002, PHYS LETT A, V306, P110, DOI 10.1016/S0375-9601(01)00610-7
   WANG DL, 1995, IEEE T NEURAL NETWOR, V6, P283, DOI 10.1109/72.363423
   Wolfram Stephen, 2002, NEW KIND SCI
   Yamaoka H, 2016, IEICE T FUND ELECTR, VE99A, P1806, DOI 10.1587/transfun.E99.A.1806
NR 24
TC 2
Z9 2
U1 1
U2 17
PD APR
PY 2021
VL 14
IS 4
SI SI
BP 1479
EP 1494
DI 10.3934/dcdss.2020374
UT WOS:000613645400017
DA 2023-11-16
ER

PT J
AU Liu, XY
   Ren, QQ
AF Liu, Xiyu
   Ren, Qianqian
TI Spiking Neural Membrane Computing Models
SO PROCESSES
DT Article
DE membrane computing; spiking neural P systems; artificial neural
   networks; spiking neural membrane computing models; Turing universality
ID P SYSTEMS; FAULT-DIAGNOSIS; RULES; POWER; POLARIZATIONS; ALGORITHM
AB As third-generation neural network models, spiking neural P systems (SNP) have distributed parallel computing capabilities with good performance. In recent years, artificial neural networks have received widespread attention due to their powerful information processing capabilities, which is an effective combination of a class of biological neural networks and mathematical models. However, SNP systems have some shortcomings in numerical calculations. In order to improve the incompletion of current SNP systems in dealing with certain real data technology in this paper, we use neural network structure and data processing methods for reference. Combining them with membrane computing, spiking neural membrane computing models (SNMC models) are proposed. In SNMC models, the state of each neuron is a real number, and the neuron contains the input unit and the threshold unit. Additionally, there is a new style of rules for neurons with time delay. The way of consuming spikes is controlled by a nonlinear production function, and the produced spike is determined based on a comparison between the value calculated by the production function and the critical value. In addition, the Turing universality of the SNMC model as a number generator and acceptor is proved.
C1 [Liu, Xiyu; Ren, Qianqian] Shandong Normal Univ, Acad Management Sci, Sch Business, Jinan 250358, Peoples R China.
RP Liu, XY (corresponding author), Shandong Normal Univ, Acad Management Sci, Sch Business, Jinan 250358, Peoples R China.
EM xyliu@sdnu.edu.cn; Rqq_517@163.com
CR Aman B, 2020, INT J NEURAL SYST, V30, DOI 10.1142/S0129065720500665
   Bibi A, 2019, COMPLEXITY, DOI 10.1155/2019/7313414
   Cabarle FGC, 2019, INFORM SCIENCES, V501, P30, DOI 10.1016/j.ins.2019.05.070
   Cabarle FGC, 2018, IEEE T NANOBIOSCI, V17, P560, DOI 10.1109/TNB.2018.2879345
   Cabarle FGC, 2017, IEEE T NANOBIOSCI, V16, P792, DOI 10.1109/TNB.2017.2762580
   Cabarle FGC, 2015, NEURAL COMPUT APPL, V26, P1905, DOI 10.1007/s00521-015-1857-4
   Carandang JPA, 2017, ROM J INF SCI TECH, V20
   Chen ZH, 2018, NEURAL COMPUT APPL, V29, P695, DOI 10.1007/s00521-016-2489-z
   Dong JP, 2021, INT J UNCONV COMPUT, V16, P201
   Frias T, 2018, NEUROCOMPUTING, V319, P176, DOI 10.1016/j.neucom.2018.08.076
   Gou XT, 2021, INT J UNCONV COMPUT, V16, P121
   Guo P, 2019, KNOWL-BASED SYST, V170, P20, DOI 10.1016/j.knosys.2019.01.013
   Ionescu M, 2006, FUND INFORM, V71, P279
   Jiang SX, 2020, COMPLEXITY, V2020, DOI 10.1155/2020/8742308
   Jimenez ZB, 2019, J MEMBRANE COMPUT, V1, P145, DOI 10.1007/s41965-019-00020-3
   Kong DT, 2020, PROCESSES, V8, DOI 10.3390/pr8091132
   Li B, 2021, SIGNAL PROCESS, V178, DOI 10.1016/j.sigpro.2020.107793
   Li B, 2021, INT J NEURAL SYST, V31, DOI 10.1142/S0129065720500501
   Li B, 2020, KNOWL-BASED SYST, V196, DOI 10.1016/j.knosys.2020.105794
   Liu W, 2020, COMPLEXITY, V2020, DOI 10.1155/2020/2462647
   Ma TM, 2019, IEEE ACCESS, V7, P177562, DOI 10.1109/ACCESS.2019.2958895
   Pan LQ, 2012, NEURAL PROCESS LETT, V35, P13, DOI 10.1007/s11063-011-9201-1
   Pan LQ, 2011, SCI CHINA INFORM SCI, V54, P1596, DOI 10.1007/s11432-011-4303-y
   Pan LQ, 2009, INT J COMPUT COMMUN, V4, P273, DOI 10.15837/ijccc.2009.3.2435
   Peng H, 2020, INT J NEURAL SYST, V30, DOI 10.1142/S0129065720500082
   Peng H, 2020, KNOWL-BASED SYST, V188, DOI 10.1016/j.knosys.2019.105064
   Peng H, 2019, KNOWL-BASED SYST, V163, P875, DOI 10.1016/j.knosys.2018.10.016
   Peng H, 2019, IEEE T NEUR NET LEAR, V30, P1672, DOI 10.1109/TNNLS.2018.2872999
   Peng H, 2018, IEEE T SMART GRID, V9, P4777, DOI 10.1109/TSG.2017.2670602
   Peng H, 2017, NEURAL NETWORKS, V95, P66, DOI 10.1016/j.neunet.2017.08.003
   RamachandranPillai R, 2020, ARAB J SCI ENG, V45, P2513, DOI 10.1007/s13369-019-04153-6
   Rodríguez-Chavarría D, 2020, NEURAL PROCESS LETT, V52, P1583, DOI 10.1007/s11063-020-10324-6
   Song T, 2019, IEEE T NANOBIOSCI, V18, P176, DOI 10.1109/TNB.2019.2896981
   Song T, 2019, NEURAL PROCESS LETT, V50, P1485, DOI 10.1007/s11063-018-9947-9
   Song T, 2016, IEEE T NANOBIOSCI, V15, P666, DOI 10.1109/TNB.2016.2598879
   Song XX, 2020, NEUROCOMPUTING, V378, P1, DOI 10.1016/j.neucom.2019.06.104
   Song XX, 2018, BIOSYSTEMS, V169, P13, DOI 10.1016/j.biosystems.2018.05.004
   Wang HF, 2018, INT J COMPUT COMMUN, V13, P574
   Wang J., 2009, P 10 WORKSH MEMBR CO, P514
   Wang J, 2019, ENG APPL ARTIF INTEL, V82, P102, DOI 10.1016/j.engappai.2019.03.014
   Wang T, 2020, ENG APPL ARTIF INTEL, V92, DOI 10.1016/j.engappai.2020.103680
   Wang X, 2016, SCI REP-UK, V6, DOI 10.1038/srep27624
   Wu TF, 2020, NEUROCOMPUTING, V414, P255, DOI 10.1016/j.neucom.2020.07.051
   Wu TF, 2020, NEUROCOMPUTING, V401, P392, DOI 10.1016/j.neucom.2020.03.095
   Wu TF, 2021, IEEE T NEUR NET LEAR, V32, P2443, DOI 10.1109/TNNLS.2020.3005538
   Wu TF, 2018, INT J NEURAL SYST, V28, DOI 10.1142/S0129065718500132
   Xue J, 2021, INFORM FUSION, V65, P84, DOI 10.1016/j.inffus.2020.08.016
   Yang Q, 2020, THEOR COMPUT SCI, V801, P143, DOI 10.1016/j.tcs.2019.08.034
   Zein M, 2019, EXPERT SYST APPL, V121, P204, DOI 10.1016/j.eswa.2018.12.029
   Zhang GX, 2021, INT J NEURAL SYST, V31, DOI 10.1142/S0129065720500550
   Zhu M, 2021, INT J NEURAL SYST, V31, DOI 10.1142/S0129065720500549
NR 51
TC 4
Z9 4
U1 2
U2 38
PD MAY
PY 2021
VL 9
IS 5
AR 733
DI 10.3390/pr9050733
UT WOS:000654470000001
DA 2023-11-16
ER

PT C
AU Phusalculkajom, W
   Hendriks, J
   Moraal, J
   Dollevoet, R
   Li, Z
   Núñez, A
AF Phusalculkajom, Wassamon
   Hendriks, Jurjen
   Moraal, Jan
   Dollevoet, Rolf
   Li, Zili
   Nunez, Alfredo
GP IEEE
TI A multiple spiking neural network architecture based on fuzzy intervals
   for anomaly detection: a case study of rail defects
SO 2022 IEEE INTERNATIONAL CONFERENCE ON FUZZY SYSTEMS (FUZZ-IEEE)
SE IEEE International Fuzzy Systems Conference Proceedings
DT Proceedings Paper
CT IEEE International Conference on Fuzzy Systems (FUZZ-IEEE) / IEEE World
   Congress on Computational Intelligence (IEEE WCCI) / International Joint
   Conference on Neural Networks (IJCNN) / IEEE Congress on Evolutionary
   Computation (IEEE CEC)
CY JUL 18-23, 2022
CL Padua, ITALY
DE spiking neural network; parameter uncertainty; prediction interval;
   interpretability; anomaly detection
ID CONDITION-BASED MAINTENANCE; NEURONS; SYSTEMS; BOUNDS
AB In this paper, a fuzzy interval-based method is proposed for solving the problem of rail defect detection relying on an on-board measurement system and a multiple spiking neural network architecture. Instead of outputting binary values (defect or not defect), all data will belong to both classes with different spreads that are given by two fuzzy intervals. The multiple spiking neural networks are used to capture different sources of uncertainties. In this paper, we consider uncertainties in the parameters of spiking neural networks during the training phase. The proposed method comprises two steps. In the first step, multiple sets of the firing times for both classes are obtained from multiple spiking neural networks. In the second step, the obtained multiple sets of firing times are fuzzy numbers and they are used to construct fuzzy intervals. The proposed method is showcased with the problem of rail defect detection. The numerical analysis indicates that the fuzzy intervals are suitable to make use of the information provided by the multiple spike neural networks. Finally, with the proposed method, we improve the interpretability of the decision making regarding the detection of anomalies.
C1 [Phusalculkajom, Wassamon; Hendriks, Jurjen; Moraal, Jan; Dollevoet, Rolf; Li, Zili; Nunez, Alfredo] Delft Univ Technol, Railway Engn, Delft, Netherlands.
RP Phusalculkajom, W (corresponding author), Delft Univ Technol, Railway Engn, Delft, Netherlands.
EM W.Phusalculkajorn@tudelft.n1; J.M.Hendriks@tudelft.nl;
   J.Moraal@tudelft.nl; R.PB.J.Dollevoet@tudelft.nk; Z.Li@tudelft.nl;
   A.A.Nunezvicencio@tudelft.nl
CR Abdar M, 2021, INFORM FUSION, V76, P243, DOI 10.1016/j.inffus.2021.05.008
   Babugia R., 1998, FUZZY MODELING CONTR, V1st
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Booij O, 2005, INFORM PROCESS LETT, V95, P552, DOI 10.1016/j.ipl.2005.05.023
   Cartagena O, 2021, IEEE ACCESS, V9, P23357, DOI 10.1109/ACCESS.2021.3056003
   Cicirello A, 2022, MECH SYST SIGNAL PR, V170, DOI 10.1016/j.ymssp.2021.108619
   Deng ZM, 2017, MECH SYST SIGNAL PR, V84, P699, DOI 10.1016/j.ymssp.2016.09.001
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   Gray A, 2022, MECH SYST SIGNAL PR, V165, DOI 10.1016/j.ymssp.2021.108210
   Ho SL, 2001, IEEE T ELECTRON PA M, V24, P323, DOI 10.1109/6104.980042
   Jamshidi A, 2018, TRANSPORT RES C-EMER, V95, P185, DOI 10.1016/j.trc.2018.07.007
   Jamshidi A, 2017, J INFRASTRUCT SYST, V23, DOI 10.1061/(ASCE)IS.1943-555X.0000357
   Jeyasothy A, 2019, IEEE T NEUR NET LEAR, V30, P1231, DOI 10.1109/TNNLS.2018.2868874
   Kavousi-Fard A, 2017, IEEE T IND ELECTRON, V64, P5295, DOI 10.1109/TIE.2017.2677345
   Khodayar M, 2019, IEEE T SMART GRID, V10, P3974, DOI 10.1109/TSG.2018.2847223
   Khosravi A, 2013, IEEE T SUSTAIN ENERG, V4, P849, DOI 10.1109/TSTE.2013.2253140
   Khosravi A, 2011, IEEE T NEURAL NETWOR, V22, P1341, DOI 10.1109/TNN.2011.2162110
   Leite D., 2012, INTERVAL APPROACH EV, P271
   Leite D, 2013, NEURAL NETWORKS, V38, P1, DOI 10.1016/j.neunet.2012.10.006
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maciel Leandro, 2015, 2015 Annual Conference of the North American Fuzzy Information Processing Society (NAFIPS) held jointly with 2015 5th World Conference on Soft Computing (WConSC), P1, DOI 10.1109/NAFIPS-WConSC.2015.7284142
   Maciel L, 2023, NEURAL COMPUT APPL, V35, P7149, DOI 10.1007/s00521-021-06263-5
   Marín LG, 2019, EXPERT SYST APPL, V119, P128, DOI 10.1016/j.eswa.2018.10.043
   Mendel J.M., 2001, UNCERTAIN RULE BASED
   Molodova M, 2015, P I MECH ENG F-J RAI, V229, P841, DOI 10.1177/0954409714523583
   Núñez A, 2019, IEEE T IND INFORM, V15, P1496, DOI 10.1109/TII.2018.2847736
   Pang JY, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18040967
   Phusakulkajorn W., SPIKING NEURAL NETWO
   Sáez D, 2015, IEEE T SMART GRID, V6, P548, DOI 10.1109/TSG.2014.2377178
   Shao HD, 2018, MECH SYST SIGNAL PR, V100, P743, DOI 10.1016/j.ymssp.2017.08.002
   Skrjanc I, 2005, AUTOMATICA, V41, P327, DOI 10.1016/j.automatica.2004.09.010
   Skrjanc I, 2011, APPL MATH MODEL, V35, P4083, DOI 10.1016/j.apm.2011.02.033
   Skrjanc I, 2009, CHEMOMETR INTELL LAB, V96, P182, DOI 10.1016/j.chemolab.2009.01.009
   Su Z, 2019, TRANSPORT RES C-EMER, V105, P359, DOI 10.1016/j.trc.2019.05.045
   Su Z, 2017, TRANSPORT RES C-EMER, V84, P92, DOI 10.1016/j.trc.2017.08.018
   Tan C, 2021, NEUROCOMPUTING, V434, P137, DOI 10.1016/j.neucom.2020.12.098
   Turkson RE, 2021, NEURAL PROCESS LETT, V53, P2649, DOI 10.1007/s11063-021-10514-w
   Valencia F, 2016, IEEE T SMART GRID, V7, P1486, DOI 10.1109/TSG.2015.2463079
   Wade JJ, 2010, IEEE T NEURAL NETWOR, V21, P1817, DOI 10.1109/TNN.2010.2074212
   Wang LQ, 2020, APPL MATH MODEL, V82, P449, DOI 10.1016/j.apm.2020.01.059
   Wei ZL, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17102236
   Yan ZL, 2021, BIOMED SIGNAL PROCES, V63, DOI 10.1016/j.bspc.2020.102170
NR 42
TC 0
Z9 0
U1 0
U2 3
PY 2022
DI 10.1109/FUZZ-IEEE55066.2022.9882864
UT WOS:000861288500142
DA 2023-11-16
ER

PT J
AU Gamez, D
   Fidjeland, AK
   Lazdins, E
AF Gamez, D.
   Fidjeland, A. K.
   Lazdins, E.
TI iSpike: a spiking neural interface for the iCub robot
SO BIOINSPIRATION & BIOMIMETICS
DT Article
ID 2-DIMENSIONAL LIMB MOVEMENTS; MUSCLE-SPINDLE FEEDBACK; RETINAL
   GANGLION-CELLS; POPULATION; HUMANS; MODEL; SKIN; INFORMATION; AFFERENTS;
   RECEPTORS
AB This paper presents iSpike: a C++ library that interfaces between spiking neural network simulators and the iCub humanoid robot. It uses a biologically inspired approach to convert the robot's sensory information into spikes that are passed to the neural network simulator, and it decodes output spikes from the network into motor signals that are sent to control the robot. Applications of iSpike range from embodied models of the brain to the development of intelligent robots using biologically inspired spiking neural networks. iSpike is an open source library that is available for free download under the terms of the GPL.
C1 [Gamez, D.; Fidjeland, A. K.; Lazdins, E.] Univ London Imperial Coll Sci Technol & Med, Dept Comp, London SW7 2AZ, England.
RP Gamez, D (corresponding author), Univ London Imperial Coll Sci Technol & Med, Dept Comp, 180 Queens Gate, London SW7 2AZ, England.
EM andreas.fidjeland@imperial.ac.uk
CR Adrian ED, 1926, J PHYSIOL-LONDON, V61, P49, DOI 10.1113/jphysiol.1926.sp002273
   Anand R, 2009, SELF-DEFENSE IN INTERNATIONAL RELATIONS, P1, DOI 10.1057/9780230245747
   Andreou A., 1994, MIDW S CIRC SYST, P97
   Bear Mark, 2007, NEUROSCIENCE EXPLORI, V3rd
   Bendor D, 2007, NAT NEUROSCI, V10, P763, DOI 10.1038/nn1888
   Bergenheim M, 2000, EXP BRAIN RES, V134, P301, DOI 10.1007/s002210000471
   BIALEK W, 1991, SCIENCE, V252, P1854, DOI 10.1126/science.2063199
   Bolduc M, 1998, COMPUT VIS IMAGE UND, V69, P170, DOI 10.1006/cviu.1997.0560
   Bouganis A., 2010, P IEEE INT JOINT C N, P4104
   Cannata Giorgio, 2008, 2008 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems (MFI 2008), P434, DOI 10.1109/MFI.2008.4648033
   Clark A., 2008, SUPERSIZING MIND EMB
   Collins DF, 1996, J PHYSIOL-LONDON, V496, P857, DOI 10.1113/jphysiol.1996.sp021733
   Collins DF, 2005, J NEUROPHYSIOL, V94, P1699, DOI 10.1152/jn.00191.2005
   Cozzi L, 2006, BIOL CYBERN, V94, P335, DOI 10.1007/s00422-006-0051-2
   Delbrück T, 2004, VISION RES, V44, P2083, DOI 10.1016/j.visres.2004.03.021
   DeMarse TB, 2001, AUTON ROBOT, V11, P305, DOI 10.1023/A:1012407611130
   Deneve S, 1999, NAT NEUROSCI, V2, P740, DOI 10.1038/11205
   EDIN BB, 1995, J PHYSIOL-LONDON, V487, P243, DOI 10.1113/jphysiol.1995.sp020875
   ENROTHCUGELL C, 1966, J PHYSIOL-LONDON, V187, P517, DOI 10.1113/jphysiol.1966.sp008107
   FERRELL WR, 1987, J PHYSIOL-LONDON, V386, P63, DOI 10.1113/jphysiol.1987.sp016522
   Fidjeland AK, 2010, P IEEE WORLD C COMP, P536
   Gamez D, 2006, P IEEE 5 CHAPT C ADV, P85
   Gamez D, 2011, CONSCIOUS COGN, V20, P1403, DOI 10.1016/j.concog.2011.05.016
   Gamez D, 2010, CONSCIOUS COGN, V19, P294, DOI 10.1016/j.concog.2009.11.001
   Gautrais J, 1998, BIOSYSTEMS, V48, P57, DOI 10.1016/S0303-2647(98)00050-1
   GEORGOPOULOS AP, 1992, SCIENCE, V256, P1692, DOI 10.1126/science.256.5064.1692
   GEORGOPOULOS AP, 1986, SCIENCE, V233, P1416, DOI 10.1126/science.3749885
   Gollisch T, 2008, SCIENCE, V319, P1108, DOI 10.1126/science.1149639
   Grill-Spector K, 2004, ANNU REV NEUROSCI, V27, P649, DOI 10.1146/annurev.neuro.27.070203.144220
   Grillner S, 2005, TRENDS NEUROSCI, V28, P364, DOI 10.1016/j.tins.2005.05.004
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Hoshi T, 2006, IEEE INT CONF ROBOT, P3463, DOI 10.1109/ROBOT.2006.1642231
   Ijspeert AJ, 2007, SCIENCE, V315, P1416, DOI 10.1126/science.1138353
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2008, P NATL ACAD SCI USA, V105, P3593, DOI 10.1073/pnas.0712231105
   Javaheri M, 2006, ANN ACAD MED SINGAP, V35, P137
   Jones KE, 2001, J PHYSIOL-LONDON, V536, P635, DOI 10.1111/j.1469-7793.2001.0635c.xd
   Krichmar JL, 2005, P NATL ACAD SCI USA, V102, P2111, DOI 10.1073/pnas.0409792102
   Kuramoto Y., 1975, International Symposium on Mathematical Problems in Theoretical Physics, P420, DOI 10.1007/BFb0013365
   Lichtsteiner P, 2008, IEEE J SOLID-ST CIRC, V43, P566, DOI 10.1109/JSSC.2007.914337
   Linares-Barranco A, 2007, IEEE INT SYMP CIRC S, P1192, DOI 10.1109/ISCAS.2007.378265
   Liu JD, 2006, INT J AUTOM COMPUT, V3, P336, DOI 10.1007/s11633-006-0336-x
   Lyon R. F., 1982, Proceedings of ICASSP 82. IEEE International Conference on Acoustics, Speech and Signal Processing, P1282
   Maass W., 1999, PULSED NEURAL NETWOR
   MACEFIELD G, 1990, J PHYSIOL-LONDON, V429, P113, DOI 10.1113/jphysiol.1990.sp018247
   Marques H., 2010, INT C HUM ROB, P391, DOI DOI 10.1109/ICHR.2010.5686344
   Martinoia S, 2004, NEUROCOMPUTING, V58, P1065, DOI 10.1016/j.neucom.2004.01.167
   Masquelier T, 2007, PLOS COMPUT BIOL, V3, P247, DOI 10.1371/journal.pcbi.0030031
   Metta G., 2008, P 8 WORKSHOP PERFORM, P50, DOI DOI 10.1145/1774674.1774683
   Metta G, 2008, INT J ADV ROBOT SYST, V3, P1
   Nageswaran JM, 2009, NEURAL NETWORKS, V22, P791, DOI 10.1016/j.neunet.2009.06.028
   Novellino A, 2007, Comput Intell Neurosci, P12725, DOI 10.1155/2007/12725
   O'Regan JK, 2001, BEHAV BRAIN SCI, V24, P939, DOI 10.1017/S0140525X01000115
   Ribot-Ciscar E, 2003, EXP BRAIN RES, V149, P512, DOI 10.1007/s00221-003-1384-x
   Rizzo JF, 2011, J NEURO-OPHTHALMOL, V31, P160, DOI 10.1097/WNO.0b013e31821eb79e
   Roll JP, 2000, EXP BRAIN RES, V134, P311, DOI 10.1007/s002210000472
   Roll JP, 2004, EXP BRAIN RES, V157, P359, DOI 10.1007/s00221-004-1853-x
   Rossant C, 2011, FRONT NEUROSCI-SWITZ, V5, DOI [10.3389/fnins.2011.00009, 10.3389/fninf.2011.00009]
   SCHWARTZ EL, 1980, VISION RES, V20, P645, DOI 10.1016/0042-6989(80)90090-5
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Van Rullen R, 2001, NEURAL COMPUT, V13, P1255, DOI 10.1162/08997660152002852
   Wessel R, 1996, J NEUROPHYSIOL, V75, P2280, DOI 10.1152/jn.1996.75.6.2280
NR 62
TC 27
Z9 27
U1 0
U2 8
PD JUN
PY 2012
VL 7
IS 2
SI SI
AR 025008
DI 10.1088/1748-3182/7/2/025008
UT WOS:000304411000009
DA 2023-11-16
ER

PT J
AU Wiklendt, L
   Chalup, S
   Middleton, R
AF Wiklendt, Lukasz
   Chalup, Stephan
   Middleton, Rick
TI A small spiking neural network with LQR control applied to the acrobot
SO NEURAL COMPUTING & APPLICATIONS
DT Article
DE Spiking neural networks; Acrobot; LQR; Evolution
ID CIRCUITS
AB This paper presents the results of a computer simulation which, combined a small network of spiking neurons with linear quadratic regulator (LQR) control to solve the acrobot swing-up and balance task. To our knowledge, this task has not been previously solved with spiking neural networks. Input to the network was drawn from the state of the acrobot, and output was torque, either directly applied to the actuated joint, or via the switching of an LQR controller designed for balance. The neural network's weights were tuned using a (mu + lambda)-evolution strategy without recombination, and neurons' parameters, were chosen to roughly approximate biological neurons.
C1 [Wiklendt, Lukasz; Chalup, Stephan; Middleton, Rick] Univ Newcastle, Sch Elect Engn & Comp Sci, Callaghan, NSW 2308, Australia.
RP Wiklendt, L (corresponding author), Univ Newcastle, Sch Elect Engn & Comp Sci, Callaghan, NSW 2308, Australia.
EM lukasz.wiklendt@studentmail.newcastle.edu.au
CR ANDERSON M, 1971, LINEAR OPTIMAL CONTR
   Beyer H.-G., 2001, NAT COMP SER
   Boone G, 1997, IEEE INT CONF ROBOT, P3281, DOI 10.1109/ROBOT.1997.606789
   Coulom R, 2004, EUR S ART NEUR NETW
   Federici D, 2005, IEEE C EVOL COMPUTAT, P543
   Floreano D, 2006, INT J INTELL SYST, V21, P1005, DOI 10.1002/int.20173
   FRENCH RLB, 2002, ICSAB, P335
   GERSTNER W, 2001, MAASS BISHOP, V15, P3
   Gerstner W., 2002, SPIKING NEURON MODEL
   Joshi P, 2005, NEURAL COMPUT, V17, P1715, DOI 10.1162/0899766054026684
   Kandel E. R., 2000, PRINCIPLES NEURAL SC, V4
   Kawada K, 2005, 2005 IEEE INTERNATIONAL SYMPOSIUM ON COMPUTATIONAL INTELLIGENCE IN ROBOTICS AND AUTOMATION, PROCEEDINGS, P261, DOI 10.1109/CIRA.2005.1554287
   Lai X, 1999, IEE P-CONTR THEOR AP, V146, P505, DOI 10.1049/ip-cta:19990749
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maass W, 2001, PULSED NEURAL NETWOR
   Markram H, 2006, NAT REV NEUROSCI, V7, P153, DOI 10.1038/nrn1848
   NAM TK, 2002, P 41 SICE ANN C SICE
   PRESS WH, 2001, NUMERICAL RECIPES C, P710
   SPONG MW, 1995, IEEE CONTR SYST MAG, V15, P49, DOI 10.1109/37.341864
   Xu X, 2002, PROCEEDINGS OF THE 2002 IEEE INTERNATIONAL SYMPOSIUM ON INTELLIGENT CONTROL, P758, DOI 10.1109/ISIC.2002.1157857
   Yoshimoto J, 2005, ARTIF LIFE ROBOT, V9, P67, DOI 10.1007/s10015-004-0340-6
NR 21
TC 15
Z9 15
U1 0
U2 11
PD MAY
PY 2009
VL 18
IS 4
BP 369
EP 375
DI 10.1007/s00521-008-0187-1
UT WOS:000264550900006
DA 2023-11-16
ER

PT C
AU Matsuda, S
AF Matsuda, Satoshi
BE Hirose, A
   Ozawa, S
   Doya, K
   Ikeda, K
   Lee, M
   Liu, D
TI BPSpike II: A New Backpropagation Learning Algorithm for Spiking Neural
   Networks
SO NEURAL INFORMATION PROCESSING, ICONIP 2016, PT II
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 23rd International Conference on Neural Information Processing (ICONIP)
CY OCT 16-21, 2016
CL Kyoto, JAPAN
DE Spiking neural networks; Learning algorithm; Backpropagation
AB Using gradient descent, we propose a new backpropagation learning algorithm for spiking neural networks with multi-layers, multi-synapses between neurons, and multi-spiking neurons. It adjusts synaptic weights, delays, and time constants, and neurons' thresholds in output and hidden layers. It guarantees convergence to minimum error point, and unlike SpikeProp and its extensions, does not need a one-to-one correspondence between actual and desired spikes in advance. So, it is stably and widely applicable to practical problems.
C1 [Matsuda, Satoshi] Nihon Univ, 1-2-1 Izumi Cho, Narashino, Chiba 2758575, Japan.
RP Matsuda, S (corresponding author), Nihon Univ, 1-2-1 Izumi Cho, Narashino, Chiba 2758575, Japan.
EM matsuda.satoshi@nihon-u.ac.jp
CR Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Florian RV, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0040233
   Gerstner W., 2002, SPIKING NEURON MODEL
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Matsuda S., 2016, IJCNN 2016
   Mohemmed A, 2012, INT J NEURAL SYST, V22, DOI 10.1142/S0129065712500128
   Ponulak F., 2006, THESIS
   Schrauwen B., 2004, PROC 15 PRORISC WORK
   Xu Y, 2013, NEURAL NETWORKS, V43, P99, DOI 10.1016/j.neunet.2013.02.003
NR 10
TC 1
Z9 1
U1 1
U2 4
PY 2016
VL 9948
BP 56
EP 65
DI 10.1007/978-3-319-46672-9_7
UT WOS:000389805500007
DA 2023-11-16
ER

PT C
AU Karia, V
   Zohora, FT
   Soures, N
   Kudithipudi, D
AF Karia, Vedant
   Zohora, Fatima Tuz
   Soures, Nicholas
   Kudithipudi, Dhireesha
GP IEEE
TI SCOLAR: A Spiking Digital Accelerator with Dual Fixed Point for
   Continual Learning
SO 2022 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS (ISCAS 22)
SE IEEE International Symposium on Circuits and Systems
DT Proceedings Paper
CT IEEE International Symposium on Circuits and Systems (ISCAS)
CY MAY 28-JUN 01, 2022
CL Austin, TX
DE Metaplasticity; Continual learning; Spiking Neural Network; neural
   network architecture; low precision
ID NEURAL-NETWORKS
AB Spiking neural network models when deployed in dynamic environments, catastrophically forget previously learned tasks. In this paper, we propose a reconfigurable spiking digital accelerator, which uses activity-dependent metaplasticity to mitigate catastrophic forgetting. The proposed accelerator has a custom low precision dual fixed point representation for network parameters. The custom precision leads to lower quantization error and higher accuracy. We evaluate the proposed accelerator on split-MNIST continual learning benchmark. Analysis shows that representing network parameters with 8-bit dual fixed point numbers reduces the memory footprint compared to 16-bit fixed point numbers, while maintaining comparable continual learning ability.
C1 [Karia, Vedant; Zohora, Fatima Tuz; Soures, Nicholas; Kudithipudi, Dhireesha] Univ Texas San Antonio, Neuromorph AI Lab, San Antonio, TX 78249 USA.
RP Karia, V (corresponding author), Univ Texas San Antonio, Neuromorph AI Lab, San Antonio, TX 78249 USA.
CR Abraham WC, 1996, TRENDS NEUROSCI, V19, P126, DOI 10.1016/S0166-2236(96)80018-X
   De Lange Matthias, 2019, ARXIV PREPRINT ARXIV
   Ewe CT, 2004, LECT NOTES COMPUT SC, V3203, P200
   Hadsell Raia, 2020, TRENDS COGNITIVE SCI
   Han B, 2016, IEEE IJCNN, P971
   Horowitz M, 2014, ISSCC DIG TECH PAP I, V57, P10, DOI 10.1109/ISSCC.2014.6757323
   Hsu Yen-Chang, 2018, ARXIV181012488
   Kuhn HG, 1996, J NEUROSCI, V16, P2027
   Laborieux A., 2020, ARXIV200303533
   Langroudi H. F., 2021, P IEEE CVF C COMP VI, P3100
   Langroudi H. F., 2021, ARXIV210402233
   LeCun Yann, 1989, ADV NEURAL INFORM PR, V2, P1, DOI DOI 10.1111/DSU.12130
   MCCLELLAND JL, 1995, PSYCHOL REV, V102, P419, DOI 10.1037/0033-295X.102.3.419
   McCloskey M, 1989, PSYCHOL LEARN MOTIV, V24, P109, DOI 10.1016/S0079-7421(08)60536-8
   Neftci EO, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00324
   Nilsson P, 2014, 2014 NORCHIP
   Parisi GI, 2019, NEURAL NETWORKS, V113, P54, DOI 10.1016/j.neunet.2019.01.012
   Roy K, 2019, NATURE, V575, P607, DOI 10.1038/s41586-019-1677-2
   Samajdar Ananda, 2018, SCALE SIM SYSTOLIC C
   Soures N., 2021, THEOR FDN CONT LEARN
   van de Ven Gido M., 2019, ARXIV190407734
   Zohora F. T., METAPLASTICITY MULTI
NR 22
TC 1
Z9 1
U1 1
U2 1
PY 2022
BP 1372
EP 1376
DI 10.1109/ISCAS48785.2022.9937294
UT WOS:000946638601123
DA 2023-11-16
ER

PT B
AU Schliebs, S
   Kasabov, N
AF Schliebs, Stefan
   Kasabov, Nikola
BE Kasabov, N
TI Computational Modeling with Spiking Neural Networks
SO SPRINGER HANDBOOK OF BIO-/NEUROINFORMATICS
DT Article; Book Chapter
ID RABBIT FOLLOWING STIMULATION; TIMING DEPENDENT PLASTICITY; LONG-LASTING
   POTENTIATION; SYNAPTIC-TRANSMISSION; DENTATE AREA; NEURONS; INTEGRATE;
   REINFORCEMENT; DYNAMICS; PATTERNS
AB This chapter reviews recent developments in the area of spiking neural networks (SNN) and summarizes the main contributions to this research field. We give background information about the functioning of biological neurons, discuss the most important mathematical neural models along with neural encoding techniques, learning algorithms, and applications of spiking neurons. As a specific application, the functioning of the evolving spiking neural network (eSNN) classification method is presented in detail and the principles of numerous eSNN based applications are highlighted and discussed.
C1 [Schliebs, Stefan] Auckland Univ Technol, Sch Comp & Math Sci, Auckland 1142, New Zealand.
   [Kasabov, Nikola] Auckland Univ Technol, KEDRI Knowledge Engn & Discovery Res Inst, Auckland, New Zealand.
RP Schliebs, S (corresponding author), Auckland Univ Technol, Sch Comp & Math Sci, Auckland 1142, New Zealand.
EM stefan.schliebs@aut.ac.nz; nkasabov@aut.ac.nz
CR Abbott LF, 1999, BRAIN RES BULL, V50, P303, DOI 10.1016/S0361-9230(99)00161-6
   Adrian ED, 1926, J PHYSIOL-LONDON, V61, P49, DOI 10.1113/jphysiol.1926.sp002273
   Bell CC, 1997, NATURE, V387, P278, DOI 10.1038/387278a0
   Bi GQ, 2001, ANNU REV NEUROSCI, V24, P139, DOI 10.1146/annurev.neuro.24.1.139
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   BLISS TVP, 1973, J PHYSIOL-LONDON, V232, P357, DOI 10.1113/jphysiol.1973.sp010274
   BLISS TVP, 1973, J PHYSIOL-LONDON, V232, P331, DOI 10.1113/jphysiol.1973.sp010273
   Bohte S., 2000, SPIKEPROP BACKPROPAG, P419
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Bower J. M., 1995, THE BOOK OF GENESIS
   Bower J. M., 2007, SCHOLARPEDIA, V2, P1383, DOI [10.4249/scholarpedia.1383, DOI 10.4249/SCHOLARPEDIA.1383]
   Brunel N, 2001, PHYS REV LETT, V86, P2186, DOI 10.1103/PhysRevLett.86.2186
   Brunel N, 2007, BIOL CYBERN, V97, P337, DOI 10.1007/s00422-007-0190-0
   Burkitt AN, 2006, BIOL CYBERN, V95, P97, DOI 10.1007/s00422-006-0082-8
   Burkitt AN, 2006, BIOL CYBERN, V95, P1, DOI 10.1007/s00422-006-0068-6
   Carnevale N.T., 2006, NEURON BOOK, DOI DOI 10.1017/CBO9780511541612
   Cocu N, 2005, J BIOGEOGR, V32, P615, DOI 10.1111/j.1365-2699.2005.01190.x
   de Sousa H. C., 2002, 7 BRAZ S NEUR NETW S
   Delorme A, 1999, NEUROCOMPUTING, V26-7, P989, DOI 10.1016/S0925-2312(99)00095-8
   Delorme A, 2003, NETWORK-COMP NEURAL, V14, P613, DOI 10.1088/0954-898X/14/4/301
   Delorme A, 2001, NEUROCOMPUTING, V38, P539, DOI 10.1016/S0925-2312(01)00403-9
   Delorme A, 2001, NEURAL NETWORKS, V14, P795, DOI 10.1016/S0893-6080(01)00049-1
   Egger V, 1999, NAT NEUROSCI, V2, P1098, DOI 10.1038/16026
   Floreano D., 2001, LNCS, P38
   Floreano D, 2006, INT J INTELL SYST, V21, P1005, DOI 10.1002/int.20173
   Florian RV, 2007, NEURAL COMPUT, V19, P1468, DOI 10.1162/neco.2007.19.6.1468
   Florian RV, 2005, Seventh International Symposium on Symbolic and Numeric Algorithms for Scientific Computing, Proceedings, P299
   Gerstner W, 2002, BIOL CYBERN, V87, P404, DOI 10.1007/s00422-002-0353-y
   Gerstner W, 2000, NEURAL COMPUT, V12, P43, DOI 10.1162/089976600300015899
   Gerstner W, 1996, NEURAL COMPUT, V8, P1653, DOI 10.1162/neco.1996.8.8.1653
   Gerstner W., 2002, SPIKING NEURON MODEL
   Glackin B, 2005, LECT NOTES COMPUT SC, V3512, P552
   Hebb D., 2005, ORG BEHAV NEUROPSYCH
   Hines ML, 2001, NEUROSCIENTIST, V7, P123, DOI 10.1177/107385840100700207
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Iannella N, 2005, INFORM PROCESS LETT, V95, P545, DOI 10.1016/j.ipl.2005.05.022
   Iglesias J, 2005, BIOSYSTEMS, V79, P11, DOI 10.1016/j.biosystems.2004.09.016
   Iglesias J, 2007, BIOSYSTEMS, V89, P287, DOI 10.1016/j.biosystems.2006.05.020
   Iglesias J, 2006, LECT NOTES COMPUT SC, V4131, P953
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM., 2007, DYNAMICAL SYSTEMS NE, DOI [DOI 10.1017/S0143385704000173, 10.7551/mitpress/2526.001.0001]
   Izhikevich EM, 2008, P NATL ACAD SCI USA, V105, P3593, DOI 10.1073/pnas.0712231105
   Kandel E. R., 2000, PRINCIPLES NEURAL SC, V4
   Kasabov N., 2006, EVOLVING CONNECTIONI
   Kasinski A., 2006, International Journal of Applied Mathematics and Computer Science, V16, P101
   Kasinski A, 2005, LECT NOTES COMPUT SC, V3696, P145, DOI 10.1007/11550822_24
   Kempter R, 1999, PHYS REV E, V59, P4498, DOI 10.1103/PhysRevE.59.4498
   Kistler WM, 1998, PHYSICA D, V114, P273, DOI 10.1016/S0167-2789(97)00195-4
   Kistler WM, 2002, BIOL CYBERN, V87, P416, DOI 10.1007/s00422-002-0359-5
   KNIGHT BW, 1972, J GEN PHYSIOL, V59, P734, DOI 10.1085/jgp.59.6.734
   Knoblauch A, 2005, INFORM PROCESS LETT, V95, P537, DOI 10.1016/j.ipl.2005.05.021
   Lapique L., 1907, J PHYSL PATHOL GEN, V9, P620, DOI DOI 10.1007/S00422-007-0189-6
   Lestienne R, 1996, BIOL CYBERN, V74, P55, DOI 10.1007/BF00199137
   Loiselle S, 2005, IEEE IJCNN, P2076
   Maass W, 1998, PULSED NEURAL NETWORKS, P55
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Maass W, 1996, NEURAL COMPUT, V8, P1, DOI 10.1162/neco.1996.8.1.1
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maass W., 1999, PULSED NEURAL NETWOR
   Maguire LP, 2007, NEUROCOMPUTING, V71, P13, DOI 10.1016/j.neucom.2006.11.029
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   Masquelier T, 2008, PLOS ONE, V3, DOI 10.1371/journal.pone.0001377
   Meffin H, 2004, J COMPUT NEUROSCI, V16, P159, DOI 10.1023/B:JCNS.0000014108.03012.81
   NATSCHLAGER T, 2003, NEUROSCIENCE DATABAS, P123, DOI DOI 10.1007/978-1-4615-1079-6_9
   Natschlager T., 2002, SPECIAL ISSUE FDN IN, P39, DOI [DOI 10.1017/CBO9781107415324.004, 10.1017/CBO9781107415324.004]
   Nawrot MP, 2009, FRONT NEURAL CIRCUIT, V3, DOI 10.3389/neuro.04.001.2009
   Nelson M., 1995, BOOK GENESIS, P27
   Perrinet L, 2001, NEUROCOMPUTING, V38, P817, DOI 10.1016/S0925-2312(01)00460-X
   Ponulak F., 2006, P ESANN, P629
   PONULAK F, 2008, APPL MATH COMPUT SCI, V18, P117, DOI DOI 10.2478/V10006-008-0011-1
   Ponulak F., 2005, RESUME NEW SUPERVISE
   Rabiner L.R., 1993, FUNDAMENTALS SPEECH
   Rieke F., 1999, SPIKES EXPLORING NEU
   Riul A, 2004, SENSOR ACTUAT B-CHEM, V98, P77, DOI 10.1016/j.snb.2003.09.025
   Schliebs Stefan, 2009, Proceedings 2009 International Joint Conference on Neural Networks (IJCNN 2009 - Atlanta), P2833, DOI 10.1109/IJCNN.2009.5179049
   Schliebs S, 2009, NEURAL NETWORKS, V22, P623, DOI 10.1016/j.neunet.2009.06.038
   Schrauwen B., 2004, ROC 15 PRORISC WORKS, P104
   Seung HS, 2003, NEURON, V40, P1063, DOI 10.1016/S0896-6273(03)00761-X
   Soltic S, 2008, IEEE IJCNN, P2091, DOI 10.1109/IJCNN.2008.4634085
   Thorpe S, 1996, NATURE, V381, P520, DOI 10.1038/381520a0
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   Thorpe S. J., 1997, ESANN 1997 5 EUR S A
   Thorpe S. J., 1998, RANK ORDER CODING CO, P113
   Thorpe SJ, 2004, NEUROCOMPUTING, V58, P857, DOI 10.1016/j.neucom.2004.01.138
   THORPE SJ, 1990, PARALLEL PROCESSING IN NEURAL SYSTEMS AND COMPUTERS, P91
   Thorpe SJ, 1997, ADV NEUR IN, V9, P901
   Tino P, 2006, NEURAL COMPUT, V18, P591, DOI 10.1162/089976606775623360
   TOVEE MJ, 1993, J NEUROPHYSIOL, V70, P640, DOI 10.1152/jn.1993.70.2.640
   Van Rullen R, 1998, BIOSYSTEMS, V48, P229, DOI 10.1016/S0303-2647(98)00070-7
   Van Rullen R, 2001, NEURAL COMPUT, V13, P1255, DOI 10.1162/08997660152002852
   Verstraeten D, 2007, NEURAL NETWORKS, V20, P391, DOI 10.1016/j.neunet.2007.04.003
   Verstraeten D., 2005, ESANN, V5, P435
   Villa AEP, 1999, P NATL ACAD SCI USA, V96, P1106, DOI 10.1073/pnas.96.3.1106
   von der Malsburg C, 1981, CORRELATION THEORY B, V81-2
   Wang XQ, 2008, NEUROCOMPUTING, V71, P655, DOI 10.1016/j.neucom.2007.08.025
   Watts M. J., 2006, P 2006 INT JOINT C N, P3506
   Worner SP, 2002, N Z PLANT PROTECT-SE, V55, P312, DOI 10.30843/nzpp.2002.55.3897
   Wysoski S. G., 2008, THESIS AUCKLAND U AU
   Wysoski SG, 2008, NEUROCOMPUTING, V71, P2563, DOI 10.1016/j.neucom.2007.12.038
   Wysoski SG, 2008, LECT NOTES COMPUT SC, V4985, P406
   Wysoski SG, 2007, LECT NOTES COMPUT SC, V4669, P758
   Wysoski SG, 2006, LECT NOTES COMPUT SC, V4179, P1133
   Wysoski SG, 2006, LECT NOTES COMPUT SC, V4131, P61
   Xie XH, 2004, PHYS REV E, V69, DOI 10.1103/PhysRevE.69.041909
   Xin JG, 2001, IEEE IJCNN, P1772, DOI 10.1109/IJCNN.2001.938430
   Zuppicich A, 2009, LECT NOTES COMPUT SC, V5506, P1129, DOI 10.1007/978-3-642-02490-0_137
NR 107
TC 8
Z9 8
U1 0
U2 1
PY 2014
BP 625
EP 646
D2 10.1007/978-3-642-30574-0
UT WOS:000401725400040
DA 2023-11-16
ER

PT J
AU Lee, JH
   Delbruck, T
   Pfeiffer, M
AF Lee, Jun Haeng
   Delbruck, Tobi
   Pfeiffer, Michael
TI Training Deep Spiking Neural Networks Using Backpropagation
SO FRONTIERS IN NEUROSCIENCE
DT Article
DE spiking neural network; deep neural network; backpropagation;
   neuromorphic; DVS; MNIST; N-MNIST
AB Deep spiking neural networks (SNNs) hold the potential for improving the latency and energy efficiency of deep neural networks through data-driven event-based computation. However, training such networks is difficult due to the non-differentiable nature of spike events. In this paper, we introduce a novel technique, which treats the membrane potentials of spiking neurons as differentiable signals, where discontinuities at spike times are considered as noise. This enables an error backpropagation mechanism for deep SNNs that follows the same principles as in conventional deep networks, but works directly on spike signals and membrane potentials. Compared with previous methods relying on indirect training and conversion, our technique has the potential to capture the statistics of spikes more precisely. We evaluate the proposed framework on artificially generated events from the original MNIST handwritten digit benchmark, and also on the N-MNIST benchmark recorded with an event-based dynamic vision sensor, in which the proposed method reduces the error rate by a factor of more than three compared to the best previous SNN, and also achieves a higher accuracy than a conventional convolutional neural network (CNN) trained and tested on the same data. We demonstrate in the context of the MNIST task that thanks to their event-driven operation, deep SNNs (both fully connected and convolutional) trained with our method achieve accuracy equivalent with conventional neural networks. In the N-MNIST example, equivalent accuracy is achieved with about five times fewer computational operations.
C1 [Lee, Jun Haeng] Samsung Elect, Samsung Adv Inst Technol, Suwon, South Korea.
   [Lee, Jun Haeng; Delbruck, Tobi; Pfeiffer, Michael] Univ Zurich, Inst Neuroinformat, Zurich, Switzerland.
   [Lee, Jun Haeng; Delbruck, Tobi; Pfeiffer, Michael] ETH, Zurich, Switzerland.
RP Lee, JH (corresponding author), Samsung Elect, Samsung Adv Inst Technol, Suwon, South Korea.; Lee, JH (corresponding author), Univ Zurich, Inst Neuroinformat, Zurich, Switzerland.; Lee, JH (corresponding author), ETH, Zurich, Switzerland.
EM junhaeng.lee@gmail.com
CR Afshar S., 2014, ARXIV14112821
   [Anonymous], 2015, ARXIV150905936
   Camuñas-Mesa L, 2012, IEEE J SOLID-ST CIRC, V47, P504, DOI 10.1109/JSSC.2011.2167409
   Cohen GK, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00184
   Diehl PU, 2015, IEEE IJCNN
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Esser S. K., 2015, ADV NEURAL INFORM PR, P1117
   Esser SK, 2016, P NATL ACAD SCI USA, V113, P11441, DOI 10.1073/pnas.1604850113
   Furber SB, 2013, IEEE T COMPUT, V62, P2454, DOI 10.1109/TC.2012.142
   Garbin D, 2014, INT EL DEVICES MEET
   Gerstner W, 2002, SPIKING NEURON MODEL, DOI [10.1017/CBO9780511815706, DOI 10.1017/CBO9780511815706]
   Glorot X., 2010, P 13 INT C ARTIFICIA, P249
   Goodfellow I. J., 2013, ICML, P2356, DOI DOI 10.48550/ARXIV.1302.4389
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   Hunsberger Eric, 2015, COMPUT SCI
   Indiveri G., 2015, P EL DEV M IEDM 2015, p4.2.1, DOI [DOI 10.1109/IEDM.2015.7409623, 10.1109/iedm.2015.7409623]
   Ioffe S., 2015, PR MACH LEARN RES, P448
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90
   Kingma DP., 2017, ARXIV
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lichtsteiner Patrick, 2008, IEEE Journal of Solid-State Circuits, V43, P566, DOI 10.1109/JSSC.2007.914337
   LiWan Matthew Zeiler, 2013, P 30 INT C MACH LEAR, P1058
   Loosli G, 2007, LARGE SCALE KERNEL M
   Maass W, 2004, J COMPUT SYST SCI, V69, P593, DOI 10.1016/j jcss.2004.04.001
   Masquelier T, 2007, PLOS COMPUT BIOL, V3, P247, DOI 10.1371/journal.pcbi.0030031
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Nageswaran Jayram Moorkanikara, 2009, 2009 IEEE International Symposium on Circuits and Systems - ISCAS 2009, P1917, DOI 10.1109/ISCAS.2009.5118157
   Neftci E, 2014, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00272
   Neil D, 2016, IEEE INT SYMP CIRC S, P2282, DOI 10.1109/ISCAS.2016.7539039
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   OConnor P., 2016, ARXIV160208323
   Orchard G, 2015, FRONT NEUROSCI-SWITZ, V9, DOI [10.3389/fhins.2015.00437, 10.3389/fnins.2015.00437]
   Oster M, 2009, NEURAL COMPUT, V21, P2437, DOI 10.1162/neco.2009.07-08-829
   Rozell CJ, 2008, NEURAL COMPUT, V20, P2526, DOI 10.1162/neco.2008.03-07-486
   RUMELHART DE, 1985, COGNITIVE SCI, V9, P75, DOI 10.1207/s15516709cog0901_5
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707
   Schmidhuber J., 2015, SCHOLARPEDIA, V10, P32832, DOI DOI 10.4249/SCHOLARPEDIA.32832
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Vincent Pascal, 2008, P 25 INT C MACH LEAR, P1096, DOI 10.1145/1390156.1390294
NR 39
TC 306
Z9 308
U1 4
U2 76
PD NOV 8
PY 2016
VL 10
AR 508
DI 10.3389/fnins.2016.00508
UT WOS:000387056100001
DA 2023-11-16
ER

PT C
AU Zhang, CW
   Liu, LJ
AF Zhang Chun-wei
   Liu Liu-jiang
TI A New Supervised Spiking Neural Network
SO ICICTA: 2009 SECOND INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTATION
   TECHNOLOGY AND AUTOMATION, VOL I, PROCEEDINGS
DT Proceedings Paper
CT 2nd Internationl Conference on Intelligent Computation Technology and
   Automation
CY OCT 10-11, 2009
CL Changsha, PEOPLES R CHINA
DE spking neural network; Iris data; classification
ID NEURONS
AB A more computational spiking neural network, PTSNN, was proposed. In PTSNN, the synaptic connection weights between neurons were set to one. Network runs through modulating the PSP location in timeline of each neuron by adapting their accepted time make the network spike at the right time so that meet the requirement of classification. The weight modulating of PTSNN is determined by the error of actual spike time and expectation time as thus avoid calculating the derivative of error function which is often used in other SNNs. The PTSNN has more computational advantage. We perform experiments for the classical his dataset problem with less neurons compare to other neuron networks and the results show that it is capable to classify data set on non-linearly problem with convergence accuracy comparable to traditional sigmoidal network and other spiking neural networks. The proposed network is promise in classification problems.
C1 [Zhang Chun-wei; Liu Liu-jiang] Tongji Univ, Coll Mech Engn, Shanghai 200092, Peoples R China.
RP Zhang, CW (corresponding author), Tongji Univ, Coll Mech Engn, Shanghai 200092, Peoples R China.
EM 666zhangchunwei@tongji.edu.cn
CR AMIN HH, 2005, IEEE INT 48 MIDW S C, P683
   [Anonymous], 2003, ADV NEURAL INFORM PR
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   Fang Hui-juan, 2008, Journal of Applied Sciences, V26, P638
   GERSTNER W, 1995, PHYS REV E, V51, P738, DOI 10.1103/PhysRevE.51.738
   Rowcliffe P, 2008, IEEE T NEURAL NETWOR, V19, P1626, DOI 10.1109/TNN.2008.2000999
   Wang XQ, 2008, NEUROCOMPUTING, V71, P655, DOI 10.1016/j.neucom.2007.08.025
NR 8
TC 0
Z9 0
U1 0
U2 1
PY 2009
BP 23
EP 26
DI 10.1109/ICICTA.2009.13
UT WOS:000275861900006
DA 2023-11-16
ER

PT C
AU Pabian, M
   Rzepka, D
   Pawlak, M
AF Pabian, Mateusz
   Rzepka, Dominik
   Pawlak, Miroslaw
GP IEEE
TI SUPERVISED TRAINING OF SIAMESE SPIKING NEURAL NETWORKS WITH EARTH
   MOVER'S DISTANCE
SO 2022 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)
SE International Conference on Acoustics Speech and Signal Processing
   ICASSP
DT Proceedings Paper
CT 47th IEEE International Conference on Acoustics, Speech and Signal
   Processing (ICASSP)
CY MAY 22-27, 2022
CL Singapore, SINGAPORE
DE spiking neural networks; siamese neural networks; event-based computing;
   sparse coding
AB This study adapts the highly-versatile siamese neural network model to the event data domain. We introduce a supervised training framework for optimizing Earth Mover's Distance (EMD) between spike trains with spiking neural networks (SNN). We train this model on images of the MNIST dataset converted into spiking domain with novel conversion schemes. The quality of the siamese embeddings of input images was evaluated by measuring the classifier performance for different dataset coding types. The models achieved performance similar to existing SNN-based approaches (F1-score of up to 0.9386) while using only about 15% of hidden layer neurons to classify each example. Furthermore, models which did not employ a sparse neural code were about 45% slower than their sparse counterparts. These properties make the model suitable for low energy consumption and low prediction latency applications.
C1 [Pabian, Mateusz; Rzepka, Dominik; Pawlak, Miroslaw] AGH Univ Sci & Technol, Dept Measurement & Elect, Krakow, Poland.
RP Pabian, M (corresponding author), AGH Univ Sci & Technol, Dept Measurement & Elect, Krakow, Poland.
CR Adrian ED, 1926, J PHYSIOL-LONDON, V61, P151, DOI 10.1113/jphysiol.1926.sp002281
   Bredin H, 2017, INT CONF ACOUST SPEE, P5430, DOI 10.1109/ICASSP.2017.7953194
   Cohen S., 1999, THESIS
   Dayan P., 2001, THEORETICAL NEUROSCI
   Diehl P.U., 2015, 2015 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2015.7280696
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Dunnhofer M, 2020, MED IMAGE ANAL, V60, DOI 10.1016/j.media.2019.101631
   Gollisch T, 2008, SCIENCE, V319, P1108, DOI 10.1126/science.1149639
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Harper NS, 2004, NATURE, V430, P682, DOI 10.1038/nature02768
   Hermans Alexander, 2017, ARXIV170307737
   Koch G., 2015, ICML DEEP LEARNING W
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lin XH, 2017, NEUROCOMPUTING, V237, P59, DOI 10.1016/j.neucom.2016.08.087
   Liu XL, 2017, IEEE I CONF COMP VIS, P1040, DOI 10.1109/ICCV.2017.118
   Luo YH, 2021, LECT NOTES COMPUT SC, V12895, P182, DOI 10.1007/978-3-030-86383-8_15
   Mostafa H, 2018, IEEE T NEUR NET LEAR, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Neftci E, 2014, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00272
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Rzepka D, 2018, IEEE ACCESS, V6, P35001, DOI 10.1109/ACCESS.2018.2839186
   Satuvuori E, 2018, J NEUROSCI METH, V299, P22, DOI 10.1016/j.jneumeth.2018.02.009
   Schreiber S, 2003, NEUROCOMPUTING, V52-4, P925, DOI 10.1016/S0925-2312(02)00838-X
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Sihn D, 2019, FRONT COMPUT NEUROSC, V13, DOI 10.3389/fncom.2019.00082
   Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26, DOI DOI 10.1007/S12654-012-0173-1
   van Rossum MCW, 2001, NEURAL COMPUT, V13, P751, DOI 10.1162/089976601300014321
   Victor JD, 1997, NETWORK-COMP NEURAL, V8, P127, DOI 10.1088/0954-898X/8/2/003
   Xing YN, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.590164
   Zenke F, 2018, NEURAL COMPUT, V30, P1514, DOI 10.1162/neco_a_01086
NR 30
TC 1
Z9 1
U1 1
U2 1
PY 2022
BP 4233
EP 4237
DI 10.1109/ICASSP43922.2022.9746630
UT WOS:000864187904104
DA 2023-11-16
ER

PT C
AU Zheng, S
   Li, W
   Qian, L
   He, C
   Li, X
AF Zheng, Shengjie
   Li, Wenyi
   Qian, Lang
   He, Chenggang
   Li, Xiaojian
BE Pimenidis, E
   Angelov, P
   Jayne, C
   Papaleonidas, A
   Aydin, M
TI A Spiking Neural Network Based on Neural Manifold for Augmenting
   Intracortical Brain-Computer Interface Data
SO ARTIFICIAL NEURAL NETWORKS AND MACHINE LEARNING - ICANN 2022, PT III
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 31st International Conference on Artificial Neural Networks (ICANN)
CY SEP 06-09, 2022
CL Univ W England, Bristol, ENGLAND
HO Univ W England
DE Brain-computer interface; Spiking neural network; Brain-inspired
   intelligence; Data augmentation
AB Brain-computer interfaces (BCIs), transform neural signals in the brain into instructions to control external devices. However, obtaining sufficient training data is difficult as well as limited. With the advent of advanced machine learning methods, the capability of brain-computer interfaces has been enhanced like never before, however, these methods require a large amount of data for training and thus require data augmentation of the limited data available. Here, we use spiking neural networks (SNN) as data generators. It is touted as the next-generation neural network and is considered as one of the algorithms oriented to general artificial intelligence because it borrows the neural information processing from biological neurons. We use the SNN to generate neural spike information that is bio-interpretable and conforms to the intrinsic patterns in the original neural data. Experiments show that the model can directly synthesize new spike trains, which in turn improves the generalization ability of the BCI decoder. Both the input and output of the spiking neural model are spike information, which is a brain-inspired intelligence approach that can be better integrated with BCI in the future.
C1 [Zheng, Shengjie; Li, Wenyi] Univ Chinese Acad Sci, Beijing, Peoples R China.
   [Zheng, Shengjie; Li, Wenyi; He, Chenggang; Li, Xiaojian] Chinese Acad Sci, Shenzhen Inst Adv Technol, Brain Cognit & Brain Dis Inst,CAS Key Lab Brain C, Shenzhen Hong Kong Inst Brain Sci,Shenzhen Fundam, Shenzhen, Peoples R China.
   [Qian, Lang] Tsinghua Univ, Tsinghua Shenzhen Int Grad Sch, Shenzhen, Peoples R China.
RP Li, X (corresponding author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Brain Cognit & Brain Dis Inst,CAS Key Lab Brain C, Shenzhen Hong Kong Inst Brain Sci,Shenzhen Fundam, Shenzhen, Peoples R China.
EM zhengshengjie20@mails.ucas.edu.cn; wy.li@siat.ac.cn;
   ql20@mails.tsinghua.edu.cn; xj.li@siat.ac.cn
CR Bouton CE, 2016, NATURE, V533, P247, DOI 10.1038/nature17435
   Cunningham JP, 2014, NAT NEUROSCI, V17, P1500, DOI 10.1038/nn.3776
   Even-Chen N, 2020, NAT BIOMED ENG, V4, P984, DOI 10.1038/s41551-020-0595-9
   Gallego JA, 2020, NAT NEUROSCI, V23, P260, DOI 10.1038/s41593-019-0555-4
   Gallego JA, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-06560-z
   Gallego JA, 2017, NEURON, V94, P978, DOI 10.1016/j.neuron.2017.05.025
   Gilja V, 2012, NAT NEUROSCI, V15, P1752, DOI 10.1038/nn.3265
   Glaser Joshua I, 2020, eNeuro, V7, DOI 10.1523/ENEURO.0506-19.2020
   Hochberg LR, 2012, NATURE, V485, P372, DOI 10.1038/nature11076
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Singanamalla SKR, 2021, FRONT NEUROSCI-SWITZ, V15, DOI 10.3389/fnins.2021.651762
   Taherkhani A, 2020, NEURAL NETWORKS, V122, P253, DOI 10.1016/j.neunet.2019.09.036
   Trautmann EM, 2019, NEURON, V103, P292, DOI 10.1016/j.neuron.2019.05.003
   Wen SX, 2021, NAT BIOMED ENG, DOI 10.1038/s41551-021-00811-z
NR 14
TC 1
Z9 1
U1 4
U2 5
PY 2022
VL 13531
BP 519
EP 530
DI 10.1007/978-3-031-15934-3_43
UT WOS:000866212600042
DA 2023-11-16
ER

PT C
AU Yin, S
   Venkataramanaiah, SK
   Chen, GK
   Krishnamurthy, R
   Cao, Y
   Chakrabarti, C
   Seo, JS
AF Yin, Shihui
   Venkataramanaiah, Shreyas K.
   Chen, Gregory K.
   Krishnamurthy, Ram
   Cao, Yu
   Chakrabarti, Chaitali
   Seo, Jae-Sun
GP IEEE
TI Algorithm and Hardware Design of Discrete-Time Spiking Neural Networks
   Based on Back Propagation with Binary Activations
SO 2017 IEEE BIOMEDICAL CIRCUITS AND SYSTEMS CONFERENCE (BIOCAS)
SE Biomedical Circuits and Systems Conference
DT Proceedings Paper
CT IEEE Biomedical Circuits and Systems Conference (BioCAS)
CY OCT 19-21, 2017
CL Torino, ITALY
DE Spiking neural networks; back propagation; neuromorphic hardware;
   straight-through estimator
AB We present a new back propagation based training algorithm for discrete-time spiking neural networks (SNN). Inspired by recent deep learning algorithms on binarized neural networks, binary activation with a straight-through gradient estimator is used to model the leaky integrate-fire spiking neuron, overcoming the difficulty in training SNNs using back propagation. Two SNN training algorithms are proposed: (1) SNN with discontinuous integration, which is suitable for rate-coded input spikes, and (2) SNN with continuous integration, which is more general and can handle input spikes with temporal information. Neuromorphic hardware designed in 28nm CMOS exploits the spike sparsity and demonstrates high classification accuracy (>98% on MNIST) and low energy (51.4-773 nJ/image).
C1 [Yin, Shihui; Venkataramanaiah, Shreyas K.; Cao, Yu; Chakrabarti, Chaitali; Seo, Jae-Sun] Arizona State Univ, Sch Elect Comp & Energy Engn, Tempe, AZ 85281 USA.
   [Chen, Gregory K.; Krishnamurthy, Ram] Intel Corp, Circuits Res Lab, Hillsboro, OR USA.
RP Yin, S (corresponding author), Arizona State Univ, Sch Elect Comp & Energy Engn, Tempe, AZ 85281 USA.
EM syin11@asu.edu
CR Bengio Y, 2013, Arxiv, DOI arXiv:1308.3432
   Diehl PU, 2015, IEEE IJCNN
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Esser SK, 2015, ADV NEUR IN, V28
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hubara I., 2016, ADV NEURAL INFORM PR, P4107, DOI DOI 10.5555/3157382.3157557
   Hunsberger E, 2016, Arxiv, DOI [arXiv:1611.05141, 10.13140/RG.2.2.10967.06566]
   Kheradpisheh SR, 2017, Arxiv, DOI arXiv:1611.01421
   Kingma DP., 2017, ARXIV
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Moons B, 2016, SYMP VLSI CIRCUITS
   Mostafa H, 2017, Arxiv, DOI arXiv:1606.08165
   Orchard G, 2015, FRONT NEUROSCI-SWITZ, V9, DOI [10.3389/fhins.2015.00437, 10.3389/fnins.2015.00437]
   Rueckauer Bodo, 2016, ARXIV, DOI DOI 10.3389/FNINS.2017.00682
   Whatmough PN, 2017, ISSCC DIG TECH PAP I, P242, DOI 10.1109/ISSCC.2017.7870351
   Xiong W, 2017, Arxiv, DOI arXiv:1609.03528
NR 19
TC 0
Z9 0
U1 0
U2 0
PY 2017
UT WOS:000903671600182
DA 2023-11-16
ER

PT C
AU Chevallier, S
   Tarroux, P
AF Chevallier, Sylvain
   Tarroux, Philippe
BE Gasteratos, A
   Vincze, M
   Tsotsos, JK
TI Covert attention with a spiking neural network
SO COMPUTER VISION SYSTEMS, PROCEEDINGS
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 6th International Conference on Computer Vision Systems
CY MAY 12-15, 2008
CL Santorini, GREECE
DE spiking neurons; precise spike-timing; covert attention; saliency
ID NEURONS; INTEGRATE; SPIKENET; MODELS
AB We propose an implementation of covert attention mechanisms with spiking neurons. Spiking neural models describe the activity of a neuron with precise spike-timing rather than firing rate. We investigate the interests offered by such a temporal code for low-level vision and early attentional process. This paper describes a spiking neural network which achieves saliency extraction and stable attentional focus of a moving stimulus. Experimental results obtained using real visual scene illustrate the robustness and the quickness of this approach.
C1 [Chevallier, Sylvain] Univ Paris 11, Paris, France.
   [Chevallier, Sylvain; Tarroux, Philippe] CNRS, LIMSI, UPR 3251, Orsay, France.
   [Tarroux, Philippe] Ecole Normale Super, Paris, France.
RP Chevallier, S (corresponding author), Univ Paris 11, Paris, France.
EM sylvain.chevallier@limsi.fr; philippe.tarroux@limsi.fr
CR Brette R, 2006, NEURAL COMPUT, V18, P2004, DOI 10.1162/neco.2006.18.8.2004
   Brunel N, 2000, J COMPUT NEUROSCI, V8, P183, DOI 10.1023/A:1008925309027
   Chevallier S., 2006, P ESANN, P209
   CHEVALLIER S, 2008, P EUR S ART NEUR NET
   Cios KJ, 2004, NEUROCOMPUTING, V61, P99, DOI 10.1016/j.neucom.2004.03.007
   Delorme A, 1999, NEUROCOMPUTING, V26-7, P989, DOI 10.1016/S0925-2312(99)00095-8
   Fecteau JH, 2006, TRENDS COGN SCI, V10, P382, DOI 10.1016/j.tics.2006.06.011
   Gerstner W., 2002, SPIKING NEURON MODEL
   Heinke D, 2005, STUD COGN, P273
   Itti L, 2001, NAT REV NEUROSCI, V2, P194, DOI 10.1038/35058500
   ITTI L, 2005, MODELS BOTTOM ATTENT, P576
   Jaeger H, 2007, NEURAL NETWORKS, V20, P287, DOI 10.1016/j.neunet.2007.04.001
   Kadir T, 2001, INT J COMPUT VISION, V45, P83, DOI 10.1023/A:1012460413855
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maass W., 1999, PULSED NEURAL NETWOR
   Maass W, 2007, LECT NOTES COMPUT SC, V4497, P507
   O'Regan JK, 2001, BEHAV BRAIN SCI, V24, P939, DOI 10.1017/S0140525X01000115
   Perrinet Laurent, 2004, Natural Computing, V3, P159, DOI 10.1023/B:NACO.0000027753.27593.a7
   RUDOLPH M, 2006, NEURAL COMPUTATION
   Schmid C, 2000, INT J COMPUT VISION, V37, P151, DOI 10.1023/A:1008199403446
   Thorpe S, 1996, NATURE, V381, P520, DOI 10.1038/381520a0
   Thorpe SJ, 2004, NEUROCOMPUTING, V58, P857, DOI 10.1016/j.neucom.2004.01.138
   TREISMAN AM, 1980, COGNITIVE PSYCHOL, V12, P97, DOI 10.1016/0010-0285(80)90005-5
   TSOTSOS JK, 1992, INT J COMPUT VISION, V7, P127, DOI 10.1007/BF00128132
   Wolfe JM., 2000, SEEING, P335
NR 26
TC 7
Z9 7
U1 0
U2 1
PY 2008
VL 5008
BP 56
EP 65
UT WOS:000255865700006
DA 2023-11-16
ER

PT C
AU Liu, Y
   Chen, JW
   Chen, LJ
AF Liu, Yan
   Chen, Jiawei
   Chen, Liujun
GP ACM
TI Recognizing Sound Signals Through Spiking Neurons and
   Spike-timing-dependent Plasticity
SO 2019 2ND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND PATTERN
   RECOGNITION (AIPR 2019)
DT Proceedings Paper
CT 2nd International Conference on Artificial Intelligence and Pattern
   Recognition (AIPR)
CY AUG 16-18, 2019
CL Beijing, PEOPLES R CHINA
DE Sound Signals Recognition; Spiking Neural Networks; Hodgkin-Huxley
   Neuron; Spike-timing-dependent Plasticity
ID MODEL; NETWORKS
AB Spiking Neural Networks (SNNs) are regarded as brain-inspired neural networks. Most SNNs described spiking neurons with the leaky integrate-and-fire model, which does not incorporate biological properties of real neurons. In this paper, a model motivated by the human auditory pathway is proposed to explore the possible sound signals recognition mechanism based on the biological dynamic properties of Hodgkin-Huxley (HH) neurons and the spike-timing-dependent-plasticity (STDP) rule of synapses. The first mechanism is that HH neurons have the property of frequency selective response. They only respond to their characteristic frequencies in burst spike trains, which makes the recognition of sound intensity based on the dynamic neurons become possible. The second mechanism is that according to the STDP rule, a synaptic connection structure is formed, and the frequency and the intensity information of input signals are stored in the synaptic delay times. Finally, the neural networks recognize sound signals with spatiotemporal firing patterns.
C1 [Liu, Yan; Chen, Jiawei; Chen, Liujun] Beijing Normal Univ, Sch Syst Sci, Beijing, Peoples R China.
RP Liu, Y (corresponding author), Beijing Normal Univ, Sch Syst Sci, Beijing, Peoples R China.
EM bnuliuyan@bnu.edu.cn; chenjiawei@bnu.edu.cn; chenlj@bnu.edu.cn
CR Abarbanel HDI, 2006, PHYS REV LETT, V96, DOI 10.1103/PhysRevLett.96.148104
   Bear Mark, 2007, NEUROSCIENCE EXPLORI, V3rd
   Caporale N, 2008, ANNU REV NEUROSCI, V31, P25, DOI 10.1146/annurev.neuro.31.060407.125639
   DESTEXHE A, 1994, NEURAL COMPUT, V6, P14, DOI 10.1162/neco.1994.6.1.14
   Gerstner W., 2002, SPIKING NEURON MODEL
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2010, DYNAMICAL SYSTEMS NE
   Lee C, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00435
   LI JY, 2018, PLOS ONE, V13, DOI DOI 10.1186/S11671-017-2412-2
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Näger C, 2002, NEUROCOMPUTING, V44, P937, DOI 10.1016/S0925-2312(02)00494-0
   Nandakumar SR, 2018, IEEE NANOTECHNOL MAG, V12, P19, DOI 10.1109/MNANO.2018.2845078
   Seung HS, 2003, NEURON, V40, P1063, DOI 10.1016/S0896-6273(03)00761-X
NR 16
TC 1
Z9 1
U1 1
U2 4
PY 2019
BP 112
EP 115
DI 10.1145/3357254.3357264
UT WOS:000555682100024
DA 2023-11-16
ER

PT J
AU Peng, HT
   Angelatos, G
   de Lima, TF
   Nahmias, MA
   Tait, AN
   Abbaslou, S
   Shastri, BJ
   Prucnal, PR
AF Peng, Hsuan-Tung
   Angelatos, Gerasimos
   de Lima, Thomas Ferreira
   Nahmias, Mitchell A.
   Tait, Alexander N.
   Abbaslou, Siamak
   Shastri, Bhavin J.
   Prucnal, Paul R.
TI Temporal Information Processing With an Integrated Laser Neuron
SO IEEE JOURNAL OF SELECTED TOPICS IN QUANTUM ELECTRONICS
DT Article
DE Neuromorphic photonics; photonic integrated circuits; photonic neural
   networks; excitable lasers; spiking neural networks
ID EXCITABILITY
AB Spiking neural networks enable efficient information processing in real-time. Excitable lasers can exhibit ultrafast spiking dynamics, and when preceded by a photodetector in an O/E/O link, can process optical spikes at different wavelengths and thus be interconnected in large neural networks. Here, we experimentally demonstrate and numerically simulate the spiking dynamics of a laser neuron fabricated in a photonic integrated circuit. Our spiking laser neuron is shown to perform coincidence detection with nanosecond time resolution, and we observe refractory periods in the order of 0.1 ns. We propose a method to implement XOR classification using our laser neurons, and simulations of the resultant dynamics indicate robust tolerance to timing jitter.
C1 [Peng, Hsuan-Tung; Angelatos, Gerasimos; de Lima, Thomas Ferreira; Nahmias, Mitchell A.; Tait, Alexander N.; Abbaslou, Siamak; Shastri, Bhavin J.; Prucnal, Paul R.] Princeton Univ, Dept Elect Engn, Princeton, NJ 08544 USA.
   [Tait, Alexander N.] NIST, Boulder, CO 80305 USA.
   [Shastri, Bhavin J.] Queens Univ, Dept Phys Engn Phys & Astron, Kingston, ON K17 3N6, Canada.
RP Peng, HT (corresponding author), Princeton Univ, Dept Elect Engn, Princeton, NJ 08544 USA.
EM hpeng@princeton.edu; ga4@princeton.edu; tlima@princeton.edu;
   mnahmias@princeton.edu; atait@ieee.org; siamaka@princeton.edu;
   shastri@ieee.org; prucnal@princeton.edu
CR Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   [Anonymous], HDB BRAIN THEORY NEU
   [Anonymous], J APPL MATH
   Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Berry MJ, 1998, J NEUROSCI, V18, P2200
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   de Lima TF, 2019, J LIGHTWAVE TECHNOL, V37, P1515, DOI 10.1109/JLT.2019.2903474
   Dubbeldam JLA, 1999, PHYS REV E, V60, P6580, DOI 10.1103/PhysRevE.60.6580
   Fok MP, 2012, OPT LETT, V37, P3309, DOI 10.1364/OL.37.003309
   Gelens L, 2010, PHYS REV A, V82, DOI 10.1103/PhysRevA.82.063841
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Hasler J, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00118
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   Hurtado A, 2015, APPL PHYS LETT, V107, DOI 10.1063/1.4937730
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Ma PY, 2018, OPT LETT, V43, P3802, DOI 10.1364/OL.43.003802
   Maass W, 1998, PULSED NEURAL NETWORKS, P55
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Nahmias M. A., 2018, P IEEE PHOT C IPC SE, P1
   Nahmias MA, 2016, APPL PHYS LETT, V108, DOI 10.1063/1.4945368
   Peng HT, 2018, IEEE J SEL TOP QUANT, V24, DOI 10.1109/JSTQE.2018.2840448
   Prucnal PR, 2016, ADV OPT PHOTONICS, V8, P228, DOI 10.1364/AOP.8.000228
   Prucnal PR, 2017, NEUROMORPHIC PHOTONICS, P1
   Ren YQ, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ELECTRONICS AND COMMUNICATION ENGINEERING (ICECE 2018), P1, DOI 10.1109/ICECOME.2018.8644931
   Romeira B, 2016, SCI REP-UK, V6, DOI 10.1038/srep19510
   Romeira B, 2013, OPT EXPRESS, V21, P20931, DOI 10.1364/OE.21.020931
   Selmi F, 2015, OPT LETT, V40, P5690, DOI 10.1364/OL.40.005690
   Selmi F, 2014, PHYS REV LETT, V112, DOI 10.1103/PhysRevLett.112.183902
   Shastri BJ, 2016, SCI REP-UK, V6, DOI 10.1038/srep19126
   Tait AN, 2019, PHYS REV APPL, V11, DOI 10.1103/PhysRevApplied.11.064043
   Tsukada M, 2005, BIOL CYBERN, V92, P139, DOI 10.1007/s00422-004-0523-1
   Vandoorne K, 2014, NAT COMMUN, V5, DOI 10.1038/ncomms4541
   Wünsche HJ, 2002, PHYS REV LETT, V88, DOI 10.1103/PhysRevLett.88.023901
   YAMADA M, 1993, IEEE J QUANTUM ELECT, V29, P1330, DOI 10.1109/3.236146
NR 34
TC 35
Z9 38
U1 1
U2 74
PD JAN-FEB
PY 2020
VL 26
IS 1
AR 5100209
DI 10.1109/JSTQE.2019.2927582
UT WOS:000484191900001
DA 2023-11-16
ER

PT C
AU Wang, X
   Wu, Q
   Lin, X
   Zhuo, Z
AF Wang, Xuan
   Wu, QingXiang
   Lin, XiaoJin
   Zhuo, ZhiQiang
BE Huang, DS
   Bevilacqua, V
   Premaratne, P
TI Finding Specific Person Using Spiking Neural Network Based on Texture
   Features
SO INTELLIGENT COMPUTING THEORY
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 10th International Conference on Intelligent Computing (ICIC)
CY AUG 03-06, 2014
CL Taiyuan, PEOPLES R CHINA
DE Texture feature; Gabor filters; Spike neural network; Support Vector
   Machine; BP neural network
ID MODEL
AB This paper has proposed a new texture-based method to find a specific person. A spike neural network is constructed to extract texture features. The network is constructed using integrate-and-fire neuron model, and it has behaviors similar to the Gabor filters. The person image is divided into three parts: head, torso and leg. Three parts of texture features are extracted by means of this network. BP neural network and multi-class Support Vector Machine are used as classifiers. CUDA technology is applied in the experiment, which greatly reduces the computation time.
C1 [Wang, Xuan; Wu, QingXiang; Lin, XiaoJin; Zhuo, ZhiQiang] Fujian Normal Univ, Key Lab Optoelect Sci & Technol Med, Minist Educ, Coll Photon & Elect Engn, Fuzhou 350007, Peoples R China.
RP Wu, Q (corresponding author), Fujian Normal Univ, Key Lab Optoelect Sci & Technol Med, Minist Educ, Coll Photon & Elect Engn, Fuzhou 350007, Peoples R China.
EM xwang197806@sina.com; qxwu@fjnu.edu.cn
CR DAUGMAN JG, 1985, J OPT SOC AM A, V2, P1160, DOI 10.1364/JOSAA.2.001160
   Destexhe A, 1997, NEURAL COMPUT, V9, P503, DOI 10.1162/neco.1997.9.3.503
   Elmir Y., 2009, P 5 SCI EL TECHN INF
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Manjunath BS, 1996, IEEE T PATTERN ANAL, V18, P837, DOI 10.1109/34.531803
   Van Rullen R, 1998, BIOSYSTEMS, V48, P229, DOI 10.1016/S0303-2647(98)00070-7
   Wang L, 2003, IEEE T PATTERN ANAL, V25, P1505, DOI 10.1109/TPAMI.2003.1251144
   Wiskott L., 1995, FACE RECOGNITION GEN
   Wu QX, 2007, LECT NOTES ARTIF INT, V4682, P26
   Wu QX, 2013, NEUROCOMPUTING, V116, P3, DOI 10.1016/j.neucom.2012.01.046
   Xie E., 2011, 2011 4 INT C IM SIGN, V3, P1246
NR 11
TC 1
Z9 1
U1 0
U2 0
PY 2014
VL 8588
BP 488
EP 494
UT WOS:000345518700054
DA 2023-11-16
ER

PT J
AU Hu, TD
   Lin, XH
   Wang, XW
   Du, PA
AF Hu, Tiandou
   Lin, Xianghong
   Wang, Xiangwen
   Du, Pangao
TI Supervised learning algorithm based on spike optimization mechanism for
   multilayer spiking neural networks
SO INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS
DT Article
DE Spiking neural networks; Supervised learning; Spike train; Spike
   optimization mechanism
ID BACKPROPAGATION; PLASTICITY; NEURONS
AB Supervised learning is one of the significant research contents in spiking neural networks (SNNs). Aiming at enhancing the performance and efficiency of supervised learning algorithms for multilayer SNNs, this paper proposes a spike optimization mechanism to select optimal presynaptic spikes for computing the change amount of synaptic weights during the learning process. The proposed spike optimization mechanism comprehensively considers the correlation between the desired and actual output spikes of the network. The synaptic weight adjustment is determined by the presynaptic spikes within an optimized time interval, which makes the network output spikes similar to the desired output spikes as much as possible. The spike optimization mechanism is applied to two representative supervised learning algorithms of multilayer SNNs (Multi-STIP and Multi-ReSuMe) to improve their learning performance. The spike train learning results show that the improved algorithms can achieve higher learning accuracy and require fewer learning epochs than the original algorithms. In addition, the spike optimization mechanism can shorten the running time of the algorithms. It indicates that the learning algorithms based on spike optimization are very efficient for learning spatio-temporal spike patterns.
C1 [Hu, Tiandou; Lin, Xianghong; Wang, Xiangwen; Du, Pangao] Northwest Normal Univ, Coll Comp Sci & Engn, Lanzhou 730070, Peoples R China.
RP Lin, XH (corresponding author), Northwest Normal Univ, Coll Comp Sci & Engn, Lanzhou 730070, Peoples R China.
EM linxh@nwnu.edu.cn
CR Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Caporale N, 2008, ANNU REV NEUROSCI, V31, P25, DOI 10.1146/annurev.neuro.31.060407.125639
   Cho MW, 2021, J KOREAN PHYS SOC, V79, P328, DOI 10.1007/s40042-021-00254-4
   Comsa IM, 2022, IEEE T NEUR NET LEAR, V33, P5939, DOI 10.1109/TNNLS.2021.3071976
   DeFelipe J, 2012, FRONT NEUROANAT, V6, DOI [10.3389/fnana.2012.00022, 10.3389/fnsyn.2012.00002, 10.3389/fnana.2012.00005]
   Gerstner W., 2002, SPIKING NEURON MODEL, DOI [DOI 10.1017/CBO9780511815706, 10.1017/cbo9780511815706]
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Guo LL, 2017, INT J NEURAL SYST, V27, DOI 10.1142/S0129065717500022
   Hao YZ, 2020, NEURAL NETWORKS, V121, P387, DOI 10.1016/j.neunet.2019.09.007
   Kheradpisheh SR, 2020, INT J NEURAL SYST, V30, DOI 10.1142/S0129065720500276
   Kumarasinghe K, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-81805-4
   Lan YW, 2020, CURR BIOINFORM, V15, P854, DOI 10.2174/1574893615999200425230713
   Lin X, 2018, SPIKING NEURAL NETWO
   Lin XH, 2017, NEUROCOMPUTING, V237, P59, DOI 10.1016/j.neucom.2016.08.087
   [蔺想红 Lin Xianghong], 2016, [电子学报, Acta Electronica Sinica], V44, P2877
   Lin XH, 2016, LECT NOTES ARTIF INT, V9773, P44, DOI 10.1007/978-3-319-42297-8_5
   [蔺想红 Lin Xianghong], 2015, [电子学报, Acta Electronica Sinica], V43, P577
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Mohemmed A, 2012, INT J NEURAL SYST, V22, DOI 10.1142/S0129065712500128
   Mostafa Hesham, 2018, IEEE Trans Neural Netw Learn Syst, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Schreiber S, 2003, NEUROCOMPUTING, V52-4, P925, DOI 10.1016/S0925-2312(02)00838-X
   Shrestha SB, 2018, IEEE T NEUR NET LEAR, V29, P3126, DOI 10.1109/TNNLS.2017.2713125
   Skatchkovsky N, 2021, IEEE COMMUN LETT, V25, P1741, DOI 10.1109/LCOMM.2021.3050242
   Sporea I, 2013, NEURAL COMPUT, V25, P473, DOI 10.1162/NECO_a_00396
   Taherkhani A, 2020, NEURAL NETWORKS, V122, P253, DOI 10.1016/j.neunet.2019.09.036
   Taherkhani A, 2018, IEEE T NEUR NET LEAR, V29, P5394, DOI 10.1109/TNNLS.2018.2797801
   Tavanaei A, 2019, NEUROCOMPUTING, V330, P39, DOI 10.1016/j.neucom.2018.11.014
   Wang XW, 2020, NEURAL NETWORKS, V125, P258, DOI 10.1016/j.neunet.2020.02.011
   Wang XW, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00252
   Wang XW, 2016, LECT NOTES COMPUT SC, V9772, P95, DOI 10.1007/978-3-319-42294-7_8
   Xianghong Lin, 2015, Intelligent Computing Theories and Methodologies. 11th International Conference, ICIC 2015. Proceedings: LNCS 9225, P106, DOI 10.1007/978-3-319-22180-9_11
   Xiao R, 2020, INT C NEUR INF PROC, V1333, P481
   Xu Y, 2019, NEURAL NETWORKS, V116, P11, DOI 10.1016/j.neunet.2019.03.017
   Xu Y, 2013, NEURAL NETWORKS, V43, P99, DOI 10.1016/j.neunet.2013.02.003
   Zhang ML, 2020, NEUROCOMPUTING, V409, P103, DOI 10.1016/j.neucom.2020.03.079
   Zhao JH, 2018, NEURAL NETWORKS, V103, P19, DOI 10.1016/j.neunet.2018.03.007
   Zheng N, 2018, IEEE T NEUR NET LEAR, V29, P4287, DOI 10.1109/TNNLS.2017.2761335
NR 38
TC 8
Z9 8
U1 3
U2 25
PD JUL
PY 2022
VL 13
IS 7
BP 1981
EP 1995
DI 10.1007/s13042-021-01500-8
EA JAN 2022
UT WOS:000746362000001
DA 2023-11-16
ER

PT C
AU Tupitsy, AN
   Pavlov, AN
   Makarov, VA
AF Tupitsy, Anatoly N.
   Pavlov, Alexey N.
   Makarov, Valeri A.
BE Tuchin, VV
   Wang, LV
   Duncan, DD
TI Separation of extracellular spikes with wavelets and neural networks
SO DYNAMICS AND FLUCTUATIONS IN BIOMEDICAL PHOTONICS VI
SE Proceedings of SPIE
DT Proceedings Paper
CT Conference on Dynamics and Fluctuations in Biomedical Photonics VI
CY JAN 24-26, 2009
CL San Jose, CA
DE Extracellular recordings; neurons; spikes; wavelet-analysis
AB We propose a novel method for automatic classification of neural spikes recorded extracellularly. The method makes use of the wavelet multiscale spike decomposition and identification of the most discriminative features by artificial neural networks. We demonstrate the efficiency of the method on semi-simulated data and using in-vivo recordings. Advantage of the proposed approach over existing techniques is shown.
C1 [Tupitsy, Anatoly N.; Pavlov, Alexey N.] Saratov NG Chernyshevskii State Univ, Dept Phys, Radiophys & Nonlinear Dynam Chair, Astrakhanskaya Str 83, Saratov 410026, Russia.
   [Makarov, Valeri A.] Univ Complutense, Dept Matemat Aplicada, FCC Matemat, Madrid 28040, Spain.
RP Pavlov, AN (corresponding author), Saratov NG Chernyshevskii State Univ, Dept Phys, Radiophys & Nonlinear Dynam Chair, Astrakhanskaya Str 83, Saratov 410026, Russia.
EM pavlov@chaos.ssu.runnet.ru
CR [Anonymous], NUMERICAL RECIPES C
   Callan R., 1999, ESSENCE NEURAL NETWO
   Daubechies I., 1992, 10 LECT WAVELETS
   Harris KD, 2000, J NEUROPHYSIOL, V84, P401, DOI 10.1152/jn.2000.84.1.401
   Haykin S., 1999, NEURAL NETWORKS COMP
   HOPFIELD JJ, 1985, BIOL CYBERN, V52, P141
   KAISER G, 1994, FRIENDLY GUIDE WAVEL
   KOHONEN T, 1989, SELFORGANIZATION ASS
   Letelier JC, 2000, J NEUROSCI METH, V101, P93, DOI 10.1016/S0165-0270(00)00250-8
   Pavlov Alexey, 2007, Natural Computing, V6, P269, DOI 10.1007/s11047-006-9014-8
   Rojas R., 1996, NERUAL NETWORKS SYST
NR 11
TC 0
Z9 0
U1 0
U2 0
PY 2009
VL 7176
AR 71760M
DI 10.1117/12.808378
UT WOS:000285714400017
DA 2023-11-16
ER

PT C
AU Luke, R
   McAlpine, D
AF Luke, Robert
   McAlpine, David
GP IEEE
TI A SPIKING NEURAL NETWORK APPROACH TO AUDITORY SOURCE LATERALISATION
SO 2019 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)
SE International Conference on Acoustics Speech and Signal Processing
   ICASSP
DT Proceedings Paper
CT 44th IEEE International Conference on Acoustics, Speech and Signal
   Processing (ICASSP)
CY MAY 12-17, 2019
CL Brighton, ENGLAND
DE spiking neural networks; binaural localisation algorithms; acoustic
   source localisation; machine learning
ID SOUND LOCALIZATION
AB A novel approach to multi-microphone acoustic source localisation based on spiking neural networks is presented. We demonstrate that a two microphone system connected to a spiking neural network can be used to localise acoustic sources based purely on inter microphone timing differences, with no need for manually configured delay lines. A two sensor example is provided which includes 1) a front end which converts the acoustic signal to a series of spikes, 2) a hidden layer of spiking neurons, 3) an output layer of spiking neurons which represents the location of the acoustic source. We present details on training the network, and evaluation of its performance in quiet and noisy conditions. The system is trained on two locations, and we show that the lateralisation accuracy is 100% when presented with previously unseen data in quiet conditions. We also demonstrate the network generalises to modulation rates and background noise on which it was not trained.
C1 [Luke, Robert; McAlpine, David] Macquarie Univ, Dept Linguist, Australian Hearing Hub, Sydney, NSW, Australia.
   [Luke, Robert] Bion Inst, East Melbourne, Australia.
RP Luke, R (corresponding author), Macquarie Univ, Dept Linguist, Australian Hearing Hub, Sydney, NSW, Australia.; Luke, R (corresponding author), Bion Inst, East Melbourne, Australia.
CR Chan V, 2007, IEEE T CIRCUITS-I, V54, P48, DOI 10.1109/TCSI.2006.887979
   Courtois G., 2014, AUDIO ENG SOC CONVEN
   Goodman D., 2010, ADV NEURAL INFORM PR, ppp 784
   Grothe B, 2010, PHYSIOL REV, V90, P983, DOI 10.1152/physrev.00026.2009
   JEFFRESS LA, 1948, J COMP PHYSIOL PSYCH, V41, P35, DOI 10.1037/h0061495
   Luke R, 2015, HEARING RES, V324, P37, DOI 10.1016/j.heares.2015.02.006
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   McAlpine D, 2003, TRENDS NEUROSCI, V26, P347, DOI 10.1016/S0166-2236(03)00140-1
   Rudnicki M, 2015, CELL TISSUE RES, V361, P159, DOI 10.1007/s00441-015-2202-z
   Wall JA, 2012, IEEE T NEUR NET LEAR, V23, P574, DOI 10.1109/TNNLS.2011.2178317
   Wall Julie A, 2007, P IR SIGN SYST C ISS, P19
   Zenke F, 2018, NEURAL COMPUT, V30, P1514, DOI 10.1162/neco_a_01086
   Zilany MSA, 2014, J ACOUST SOC AM, V135, P283, DOI 10.1121/1.4837815
NR 13
TC 2
Z9 2
U1 0
U2 2
PY 2019
BP 1488
EP 1492
UT WOS:000482554001144
DA 2023-11-16
ER

PT J
AU Stewart, RD
   Gurney, KN
AF Stewart, Robert D.
   Gurney, Kevin N.
TI Spiking neural network simulation: memory-optimal synaptic event
   scheduling
SO JOURNAL OF COMPUTATIONAL NEUROSCIENCE
DT Article
DE Spiking neural network; Synaptic event; Izhikevich
ID DRIVEN SIMULATION; NEURONS; INTEGRATION; MODEL
AB Spiking neural network simulations incorporating variable transmission delays require synaptic events to be scheduled prior to delivery. Conventional methods have memory requirements that scale with the total number of synapses in a network. We introduce novel scheduling algorithms for both discrete and continuous event delivery, where the memory requirement scales instead with the number of neurons. Superior algorithmic performance is demonstrated using large-scale, benchmarking network simulations.
C1 [Stewart, Robert D.; Gurney, Kevin N.] Univ Sheffield, Dept Psychol, Sheffield S10 2TP, S Yorkshire, England.
RP Stewart, RD (corresponding author), MRC Anat Neuropharmacol Unit, Mansfield Rd, Oxford OX1 3TH, England.
EM Robert.Stewart@pharm.ox.ac.uk
CR Brette R, 2007, J COMPUT NEUROSCI, V23, P349, DOI 10.1007/s10827-007-0038-6
   BROWN R, 1988, COMMUN ACM, V31, P1220, DOI 10.1145/63039.63045
   Claverol ET, 2002, NEUROCOMPUTING, V47, P277, DOI 10.1016/S0925-2312(01)00629-4
   Destexhe A, 2009, J COMPUT NEUROSCI, V27, P493, DOI 10.1007/s10827-009-0164-4
   Goodman Dan, 2008, Front Neuroinform, V2, P5, DOI 10.3389/neuro.11.005.2008
   Humphries MD, 2009, NEURAL NETWORKS, V22, P1174, DOI 10.1016/j.neunet.2009.07.018
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2007, DYNAMICAL SYSTEMS NE
   Izhikevich EM, 2008, P NATL ACAD SCI USA, V105, P3593, DOI 10.1073/pnas.0712231105
   Mattia M, 2000, NEURAL COMPUT, V12, P2305, DOI 10.1162/089976600300014953
   Morrison A, 2005, NEURAL COMPUT, V17, P1776, DOI 10.1162/0899766054026648
   Morrison A, 2007, NEURAL COMPUT, V19, P47, DOI 10.1162/neco.2007.19.1.47
   Nageswaran JM, 2009, NEURAL NETWORKS, V22, P791, DOI 10.1016/j.neunet.2009.06.028
   Ros E, 2006, NEURAL COMPUT, V18, P2959, DOI 10.1162/neco.2006.18.12.2959
   RUSHTON WAH, 1951, J PHYSIOL-LONDON, V115, P101, DOI 10.1113/jphysiol.1951.sp004655
   Stewart RD, 2009, J COMPUT NEUROSCI, V27, P115, DOI 10.1007/s10827-008-0131-5
   Vogels TP, 2005, J NEUROSCI, V25, P10786, DOI 10.1523/JNEUROSCI.3508-05.2005
NR 17
TC 1
Z9 1
U1 2
U2 7
PD JUN
PY 2011
VL 30
IS 3
BP 721
EP 728
DI 10.1007/s10827-010-0288-6
UT WOS:000291253400012
DA 2023-11-16
ER

PT C
AU Sakemi, Y
   Morino, K
   Morie, T
   Hosomi, T
   Aihara, K
AF Sakemi, Yusuke
   Morino, Kai
   Morie, Takashi
   Hosomi, Takeo
   Aihara, Kazuyuki
GP IEEE
TI A Spiking Neural Network with Resistively Coupled Synapses Using
   Time-to-First-Spike Coding Towards Efficient Charge-Domain Computing
SO 2022 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS (ISCAS 22)
SE IEEE International Symposium on Circuits and Systems
DT Proceedings Paper
CT IEEE International Symposium on Circuits and Systems (ISCAS)
CY MAY 28-JUN 01, 2022
CL Austin, TX
DE spiking neural networks; in-memory computing; analog memory;
   neuromorcphic engineering
AB Spiking neural networks (SNNs) are expected to be energy efficient when implemented on dedicated hardware. However, fully exploiting SNN's characteristics such as eventdriven communications challenges on circuit designers and manufacturers. In this paper, inspired by the recent success of an artificial neural network (ANN) based system, known as chargedomain computing (CDC), we propose a novel framework for SNNs called "RC-Spike." As CDC, RC-Spike uses a two-phase system: input spikes are received in the accumulation phase, and a neuron produces a spike in the spike generation phase. In RCSpike, synaptic currents are accumulated with resistively coupled synapses, with which circuit implementation can be simplified compared with CDC circuits. Because of this resistive coupling effect, a neuron in RC-Spike does not compute an exact dot product. However, RC-Spike can be successfully trained in the framework of SNNs, and we show that the learning performance of RC-Spike is as high as ANNs on the MNIST and FashionMNIST datasets.
C1 [Sakemi, Yusuke] Chiba Inst Technol, Res Ctr Math Engn, Narashino, Chiba, Japan.
   [Morino, Kai] Kyushu Univ, Interdisciplinary Grad Sch Engn Sci, Fukuoka, Japan.
   [Morie, Takashi] Kyushu Inst Technol, Grad Sch Life Sci & Syst Engn, Kitakyushu, Fukuoka, Japan.
   [Hosomi, Takeo] NEC Corp Ltd, Data Sci Res Lab, Kawasaki, Kanagawa, Japan.
   [Aihara, Kazuyuki] Univ Tokyo, Int Res Ctr Neurointelligence WPI IRCN, Inst Adv Study, Tokyo, Japan.
RP Sakemi, Y (corresponding author), Chiba Inst Technol, Res Ctr Math Engn, Narashino, Chiba, Japan.
CR Ankit A, 2019, TWENTY-FOURTH INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS (ASPLOS XXIV), P715, DOI 10.1145/3297858.3304049
   [Anonymous], 2020, 2020 57 ACM IEEE DES
   Bavandpour M, 2019, IEEE T CIRCUITS-II, V66, P1512, DOI 10.1109/TCSII.2019.2891688
   Bhattacharjee A., 2021, IEEE T COMPUTER AIDE
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Bouvier M, 2019, ACM J EMERG TECH COM, V15, DOI 10.1145/3304103
   Cai FX, 2019, NAT ELECTRON, V2, P290, DOI 10.1038/s41928-019-0270-x
   Comsa IM, 2022, IEEE T NEUR NET LEAR, V33, P5939, DOI 10.1109/TNNLS.2021.3071976
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Frenkel C., 2021, ARXIV210601288
   Göltz J, 2021, NAT MACH INTELL, V3, P823, DOI 10.1038/s42256-021-00388-x
   Hayakawa Y., 2015, VLSI TECHNOLOGY VLSI, pT14, DOI DOI 10.1109/VLSIC.2015.7231381
   Huh D, 2018, ADV NEUR IN, V31
   Jin Y., 2018, ADV NEURAL INFORM PR, P7005, DOI DOI 10.48550/ARXIV.1805.07866
   Kheradpisheh SR, 2020, INT J NEURAL SYST, V30, DOI 10.1142/S0129065720500276
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Liu TY, 2013, ISSCC DIG TECH PAP I, V56, P210, DOI 10.1109/ISSCC.2013.6487703
   Long Y, 2019, DES AUT TEST EUROPE, P1769, DOI [10.23919/date.2019.8715178, 10.23919/DATE.2019.8715178]
   Marinella MJ, 2018, IEEE J EM SEL TOP C, V8, P86, DOI 10.1109/JETCAS.2018.2796379
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Moradi S, 2018, IEEE T BIOMED CIRC S, V12, P106, DOI 10.1109/TBCAS.2017.2759700
   Mostafa H, 2018, IEEE T NEUR NET LEAR, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Park S, 2020, DES AUT CON, DOI [10.1109/dac18072.2020.9218689, 10.1007/s00779-020-01476-2]
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Qiao N, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00141
   Roy K, 2020, DES AUT CON, DOI 10.1109/dac18072.2020.9218505
   Roy K, 2019, NATURE, V575, P607, DOI 10.1038/s41586-019-1677-2
   Saito D, 2020, IEEE T ELECTRON DEV, V67, P4616, DOI 10.1109/TED.2020.3025986
   Sakemi Y., 2021, ARXIV210610382
   Sakemi Y., 2021, IEEE T NEURAL NETWOR, P1
   Schumann CL, 2019, AIDS BEHAV, V23, P5, DOI 10.1007/s10461-017-1727-4
   Sebastian A, 2020, NAT NANOTECHNOL, V15, P529, DOI 10.1038/s41565-020-0655-z
   Stöckl C, 2021, NAT MACH INTELL, V3, DOI 10.1038/s42256-021-00311-4
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Valavi H, 2019, IEEE J SOLID-ST CIRC, V54, P1789, DOI 10.1109/JSSC.2019.2899730
   Verma Naveen, 2019, IEEE Solid-State Circuits Magazine, V11, P43, DOI 10.1109/MSSC.2019.2922889
   Xiao H., 2017, ARXIV170807747
   Yamaguchi M., 2019, ARXIV190207707
   Yamaguchi M, 2021, IEEE ACCESS, V9, P2644, DOI 10.1109/ACCESS.2020.3047619
   Zhang L, 2019, AAAI CONF ARTIF INTE, P1319
   Zhang ML, 2022, IEEE T NEUR NET LEAR, V33, P1947, DOI 10.1109/TNNLS.2021.3110991
   Zhou KJ, 2021, IEEE J SOLID-ST CIRC, V56, P988, DOI 10.1109/JSSC.2020.3025756
NR 42
TC 0
Z9 0
U1 3
U2 4
PY 2022
BP 2152
EP 2156
DI 10.1109/ISCAS48785.2022.9937662
UT WOS:000946638602074
DA 2023-11-16
ER

PT C
AU George, AM
   Banerjee, D
   Dey, S
   Mukherjee, A
   Balamurali, P
AF George, Arun M.
   Banerjee, Dighanchal
   Dey, Sounak
   Mukherjee, Arijit
   Balamurali, P.
GP IEEE
TI A Reservoir-based Convolutional Spiking Neural Network for Gesture
   Recognition from DVS Input
SO 2020 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN) held as part
   of the IEEE World Congress on Computational Intelligence (IEEE WCCI)
CY JUL 19-24, 2020
CL ELECTR NETWORK
ID COMPUTATION; FRAMEWORK
AB Mammalian neural circuits respond to different sensory stimuli by firing spikes at particular times. Closely mimicking this phenomenon, the evolving 3rd generation neural networks, known as Spiking Neural Networks (SNNs), are found to be capable of memorizing and learning from the spatio-temporal spike patterns. This makes SNN applicable in identification of human actions and gestures, especially in the robotics domain. The paradigm is also suited for Neuromorphic Systems leading to less energy intensive applications. In this work, we present a novel spiking neural network constituting multiple convolutional layers and a reservoir layer to extract spatial and temporal features respectively from human gesture videos captured with DVS camera. We achieved more than 95% Top-3 accuracy on IBM DVS dataset and we claim that the performance of our network is better in terms of accuracy vs. learning parameters ratio when compared to other networks.
C1 [George, Arun M.; Banerjee, Dighanchal; Dey, Sounak; Mukherjee, Arijit; Balamurali, P.] TCS Res & Innovat, Kolkata, India.
RP George, AM (corresponding author), TCS Res & Innovat, Kolkata, India.
EM arunm.george@tcs.com; dighanchal.b@tcs.com; sounak.d@tcs.com;
   mukherjee.arijit@tcs.com; balamurali.p@tcs.com
CR Amir A, 2017, PROC CVPR IEEE, P7388, DOI 10.1109/CVPR.2017.781
   [Anonymous], 1949, ORG BEHAV
   Baccouche Moez, 2011, Human Behavior Unterstanding. Proceedings Second International Workshop, HBU 2011, P29, DOI 10.1007/978-3-642-25446-8_4
   Bellec G., 2018, ADV NEURAL INFORM PR
   Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Camuñas-Mesa L, 2005, P SOC PHOTO-OPT INS, V5839, P160, DOI 10.1117/12.607709
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   DeFelipe J, 2012, FRONT NEUROANAT, V6, DOI [10.3389/fnana.2012.00022, 10.3389/fnsyn.2012.00002, 10.3389/fnana.2012.00005]
   Dey S., 2019, NEURAL INFORM PROCES
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Furber S, 2016, J NEURAL ENG, V13, DOI 10.1088/1741-2560/13/5/051001
   GROSSBERG S, 1973, STUD APPL MATH, V52, P213
   Hazan H, 2018, FRONT NEUROINFORM, V12, DOI 10.3389/fninf.2018.00089
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Jaeger H, 2004, SCIENCE, V304, P78, DOI 10.1126/science.1091277
   Jonke Z, 2017, J NEUROSCI, V37, P8511, DOI 10.1523/JNEUROSCI.2078-16.2017
   Lee C, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00435
   Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508
   Louis L, 1907, J PHYSL PATHOL GENET, V9, P620
   Maass W, 2000, NEURAL COMPUT, V12, P2519, DOI 10.1162/089976600300014827
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maro J.-M., 2018, ARXIV181107802
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Oster M, 2009, NEURAL COMPUT, V21, P2437, DOI 10.1162/neco.2009.07-08-829
   Panda P, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00126
   Park J, 2014, BIOMED CIRC SYST C, P675, DOI 10.1109/BioCAS.2014.6981816
   Ponulak F, 2011, ACTA NEUROBIOL EXP, V71, P409
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Shi XJ, 2015, ADV NEUR IN, V28
   Shrestha SB, 2018, ADV NEUR IN, V31
   Simonyan K, 2014, ADV NEUR IN, V27
   Soures N, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00686
   Suri RE, 2004, BIOL CYBERN, V90, P400, DOI 10.1007/s00422-004-0487-1
   Sussillo D, 2009, NEURON, V63, P544, DOI 10.1016/j.neuron.2009.07.018
   Zhang AG, 2017, CHIN AUTOM CONGR, P6189, DOI 10.1109/CAC.2017.8243892
   Zidan MA, 2018, NAT ELECTRON, V1, P22, DOI 10.1038/s41928-017-0006-8
NR 41
TC 13
Z9 13
U1 0
U2 0
PY 2020
DI 10.1109/ijcnn48605.2020.9206681
UT WOS:000626021400090
DA 2023-11-16
ER

PT J
AU Kasabov, N
AF Kasabov, Nikola
TI To spike or not to spike: A probabilistic spiking neuron model
SO NEURAL NETWORKS
DT Article
DE Spiking neural networks; Probabilistic modelling; Quantum inspired
   evolutionary algorithm; Classification; Computational neurogenetic
   models
ID NETWORKS
AB Spiking neural networks (SNN) are promising artificial neural network (ANN) models as they utilise information representation as trains of spikes, that adds new dimensions of time, frequency and phase to the structure and the functionality of ANN. The current SNN models though are deterministic, that restricts their applications for large scale engineering and cognitive modelling of stochastic processes. This paper proposes a novel probabilistic spiking neuron model (pSNM) and suggests ways of building pSNN for a wide range of applications including classification, string pattern recognition and associative memory. It also extends previously published computational neurogenetic models. (C) 2009 Elsevier Ltd. All rights reserved.
C1 Auckland Univ Technol, KEDRI, Auckland, New Zealand.
RP Kasabov, N (corresponding author), Auckland Univ Technol, KEDRI, Auckland, New Zealand.
EM nkasabov@aut.ac.nz
CR [Anonymous], 2007, EVOLVING CONNECTIONI
   [Anonymous], 1991, CORTICONICS
   Benuskova L., 2007, TOP BIOMED ENG
   Brette R, 2007, J COMPUT NEUROSCI, V23, P349, DOI 10.1007/s10827-007-0038-6
   DEFOINPLATEL M, 2009, IEEE T EVOLUTIONARY
   GERSTNER W, 1995, PHYS REV E, V51, P738, DOI 10.1103/PhysRevE.51.738
   Gerstner W., 2002, SPIKING NEURON MODEL
   Hebb D., 2005, ORG BEHAV NEUROPSYCH
   Hirvensalo M, 2004, QUANTUM COMPUTING
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Huguenard JR, 2000, P NATL ACAD SCI USA, V97, P9349, DOI 10.1073/pnas.97.17.9349
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2007, DYNAMICAL SYSTEMS NE
   Izhikevich EM, 2008, P NATL ACAD SCI USA, V105, P3593, DOI 10.1073/pnas.0712231105
   Kasabov N, 2007, STUD COMPUT INTELL, V63, P193
   Kasabov N, 2009, LECT NOTES COMPUT SC, V5506, P3, DOI 10.1007/978-3-642-02490-0_1
   Kasabov Nikola, 2009, Natural Computing, V8, P199, DOI 10.1007/s11047-008-9066-z
   KATSUMATA S, 2009, LNCS, V5506
   Kistler WM, 1997, NEURAL COMPUT, V9, P1015, DOI 10.1162/neco.1997.9.5.1015
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Maass W, 1999, NEURAL COMPUT, V11, P903, DOI 10.1162/089976699300016494
   Maass W., 1999, PULSED NEURAL NETWOR
   SAKMANN B, 1983, CONFORMATIONAL TRANS
   Schliebs S, 2009, NEURAL NETWORKS, V22, P623, DOI 10.1016/j.neunet.2009.06.038
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   WYSOSKI S, 2009, NEURAL NETW IN PRESS
NR 26
TC 81
Z9 85
U1 3
U2 15
PD JAN
PY 2010
VL 23
IS 1
BP 16
EP 19
DI 10.1016/j.neunet.2009.08.010
UT WOS:000273126500003
DA 2023-11-16
ER

PT J
AU Lan, YW
   Li, Q
AF Lan, Yawen
   Li, Qiang
TI Supervised Learning in Spiking Neural Networks with Synaptic Delay
   Plasticity: An Overview
SO CURRENT BIOINFORMATICS
DT Review
DE Action potentials; spike-timing; spiking neural networks; biological
   neural networks; supervised learning; synaptic delay plasticity
ID PATTERN-RECOGNITION; CLASSIFICATION; BACKPROPAGATION; IMPLEMENTATION;
   DISTURBANCES; PRECISION; ALGORITHM; RESUME; TRAINS; MODEL
AB Throughout the central nervous system (CNS), the information communicated between neurons is mainly implemented by the action potentials (or spikes). Although the spike-timing based neuronal codes have significant computational advantages over rate encoding scheme, the exact spike timing-based learning mechanism in the brain remains an open question. To close this gap, many weight-based supervised learning algorithms have been proposed for spiking neural networks. However, it is insufficient to consider only synaptic weight plasticity, and biological evidence suggest that the synaptic delay plasticity also plays an important role in the learning progress in biological neural networks. Recently, many learning algorithms have been proposed to consider both the synaptic weight plasticity and synaptic delay plasticity. The goal of this paper is to give an overview of the existing synaptic delay-based learning algorithms in spiking neural networks. We described the typical learning algorithms and reported the experimental results. Finally, we discussed the properties and limitations of each algorithm and made a comparison among them.
C1 [Lan, Yawen; Li, Qiang] Southwest Univ Sci & Technol, Sch Informat Engn, Mianyang 621010, Sichuan, Peoples R China.
RP Li, Q (corresponding author), Southwest Univ Sci & Technol, Sch Informat Engn, Mianyang 621010, Sichuan, Peoples R China.
EM liqiangsir@swust.edu.cn
CR Abdel-Hamid O, 2014, IEEE-ACM T AUDIO SPE, V22, P1533, DOI 10.1109/TASLP.2014.2339736
   Bair W, 1996, NEURAL COMPUT, V8, P1185, DOI 10.1162/neco.1996.8.6.1185
   Bermak A, 2004, INT J ROBOT AUTOM, V19, P197, DOI 10.2316/Journal.206.2004.4.206-2715
   Berry MJ, 1998, ADV NEUR IN, V10, P110
   Bi GQ, 2001, ANNU REV NEUROSCI, V24, P139, DOI 10.1146/annurev.neuro.24.1.139
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Bullock TH, 1968, NEUROSCI RES PROGRAM
   Cariani PA, 2004, IEEE T NEURAL NETWOR, V15, P1100, DOI 10.1109/TNN.2004.833305
   Che Y., 2019, IEEE T NEURAL NETW L
   Comsa I. M., 2019, ARXIV190713223
   Devalle F, 2018, PHYS REV E, V98, DOI 10.1103/PhysRevE.98.042214
   Diehl PU, 2015, IEEE IJCNN
   Emre O., 2019, ARXIV190109948
   Esser S. K., 2015, ADV NEURAL INFORM PR, P1117
   Esser SK, 2016, P NATL ACAD SCI USA, V113, P11441, DOI 10.1073/pnas.1604850113
   Esteva A, 2019, NAT MED, V25, P24, DOI 10.1038/s41591-018-0316-z
   Esteva A, 2017, NATURE, V542, P115, DOI 10.1038/nature21056
   Feldmann J, 2019, NATURE, V569, P208, DOI 10.1038/s41586-019-1157-8
   Florian RV, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0040233
   Gao G, 13 INT COMP C WAV AC, P139
   Gautrais J, 1998, BIOSYSTEMS, V48, P57, DOI 10.1016/S0303-2647(98)00050-1
   Gerstner W., 2002, SPIKING NEURON MODEL, DOI [DOI 10.1017/CBO9780511815706, 10.1017/cbo9780511815706]
   Ghaeini R., 2018, P C N AM ASS CHAPT A, VI, P1460, DOI DOI 10.18653/V1/N18-1132
   Ghosh-Dastidar S, 2007, INTEGR COMPUT-AID E, V14, P187
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   Gilson M, 2012, NEURAL COMPUT, V24, P2251, DOI 10.1162/NECO_a_00331
   Glackin B, 2010, FRONT COMPUT NEUROSC, V4, DOI 10.3389/fncom.2010.00018
   Gollisch T, 2008, SCIENCE, V319, P1108, DOI 10.1126/science.1149639
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   Hunsberger Eric, 2015, COMPUT SCI
   Ito M, 2000, BRAIN RES, V886, P237, DOI 10.1016/S0006-8993(00)03142-5
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Jin Y., 2018, ADV NEURAL INFORM PR, P7005, DOI DOI 10.48550/ARXIV.1805.07866
   Kempter R, 1999, PHYS REV E, V59, P4498, DOI 10.1103/PhysRevE.59.4498
   Kheradpisheh S. Reza, 2019, ARXIV191009495
   KNUDSEN EI, 1994, J NEUROSCI, V14, P3985
   Kumarasinghe K, 2020, NEURAL NETWORKS, V121, P169, DOI 10.1016/j.neunet.2019.08.029
   Lam MWY, 2019, INT CONF ACOUST SPEE, P7235, DOI 10.1109/ICASSP.2019.8683660
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lee JM, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00191
   Legenstein R, 2005, NEURAL COMPUT, V17, P2337, DOI 10.1162/0899766054796888
   Li G., 2019, ARXIV190701167
   Liu Q., 2017, ARXIV170603609
   McKennoch S, 2006, IEEE IJCNN, P3970
   Meister M, 1999, NEURON, V22, P435, DOI 10.1016/S0896-6273(00)80700-X
   Mohapatra S., 2019, ARXIV190302080
   Mohemmed A, 2012, INT J NEURAL SYST, V22, DOI 10.1142/S0129065712500128
   Nadasdy Z, 2009, FRONT SYST NEUROSCI, V3, DOI 10.3389/neuro.06.006.2009
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   Panda P, 2016, IEEE IJCNN, P299, DOI 10.1109/IJCNN.2016.7727212
   Park S, 2019, PROCEEDINGS OF THE 2019 56TH ACM/EDAC/IEEE DESIGN AUTOMATION CONFERENCE (DAC), DOI 10.1145/3316781.3317822
   Patiño-Saucedo A, 2020, NEURAL NETWORKS, V121, P319, DOI 10.1016/j.neunet.2019.09.008
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Qiu Q, P INT C NEUR SYST, P10
   Reinagel P, 2000, J NEUROSCI, V20, P5392, DOI 10.1523/JNEUROSCI.20-14-05392.2000
   Rotermund D, 2019, FRONT COMPUT NEUROSC, V13, DOI 10.3389/fncom.2019.00055
   Roy K., 2019, ARXIV190306379 2
   Rueckauer B, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351295
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Russell BC, 2008, INT J COMPUT VISION, V77, P157, DOI 10.1007/s11263-007-0090-8
   Schreiber S, 2003, NEUROCOMPUTING, V52-4, P925, DOI 10.1016/S0925-2312(02)00838-X
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Shrestha SB., 2018, ADV NEURAL INFORM PR, V31, P1412
   Shrestha SB, 2018, IEEE T NEUR NET LEAR, V29, P3126, DOI 10.1109/TNNLS.2017.2713125
   Shrestha SB, 2017, NEURAL NETWORKS, V96, P33, DOI 10.1016/j.neunet.2017.08.010
   Singer W, 1999, CURR OPIN NEUROBIOL, V9, P189, DOI 10.1016/S0959-4388(99)80026-9
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Sporea I, 2013, NEURAL COMPUT, V25, P473, DOI 10.1162/NECO_a_00396
   Steuber V, 2004, J COMPUT NEUROSCI, V17, P149, DOI 10.1023/B:JCNS.0000037678.26155.b5
   TAHERKHANI A, INT C NEUR INF PROC, DOI DOI 10.1007/978-3-319-26535-3_22
   Taherkhani A, 2018, IEEE T NEUR NET LEAR, V29, P5394, DOI 10.1109/TNNLS.2018.2797801
   Taherkhani A, 2015, IEEE T NEUR NET LEAR, V26, P3137, DOI 10.1109/TNNLS.2015.2404938
   Tang H, INT S NEUR NETW, P361
   Tavanaei A, 2019, NEUROCOMPUTING, V330, P39, DOI 10.1016/j.neucom.2018.11.014
   Thach W. T., 1996, Behavioral and Brain Sciences, V19, P411
   Uzzell VJ, 2004, J NEUROPHYSIOL, V92, P780, DOI 10.1152/jn.01171.2003
   Vaila R., 2019, ARXIV190312272
   Nguyen VA, 2012, IEEE T NEUR NET LEAR, V23, P971, DOI 10.1109/TNNLS.2012.2191419
   Wang SX, 2019, APPL ENERG, V235, P1126, DOI 10.1016/j.apenergy.2018.09.160
   Wang WW, 2012, IEEE T NEUR NET LEAR, V23, P1574, DOI 10.1109/TNNLS.2012.2208477
   Wang XW, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00252
   Wu JY, 2019, 2019 5TH INTERNATIONAL CONFERENCE ON EVENT-BASED CONTROL, COMMUNICATION, AND SIGNAL PROCESSING (EBCCSP), DOI 10.1109/ebccsp.2019.8836892
   Wu X, 2019, NEURAL NETWORKS, V113, P72, DOI 10.1016/j.neunet.2019.01.010
   Xie X, P 18 AS PAC S INT EV, V1, P171
   Xu B, 2013, SCI CHINA CHEM, V56, P222, DOI 10.1007/s11426-012-4710-y
   Xu Y, 2013, NEURAL COMPUT, V25, P1472, DOI 10.1162/NECO_a_00450
   Xu Y, 2013, NEURAL NETWORKS, V43, P99, DOI 10.1016/j.neunet.2013.02.003
   Young T, 2018, IEEE COMPUT INTELL M, V13, P55, DOI 10.1109/MCI.2018.2840738
   Yu Q, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0078318
   Zarrella Guido, INT JOINT C NEUR NET, P4278
   Zenke F, 2018, NEURAL COMPUT, V30, P1514, DOI 10.1162/neco_a_01086
   Zhang M, INT C NEUR INF PROC, P797
   Zhang ML, 2019, AAAI CONF ARTIF INTE, P1327
   Zhang ML, 2019, IEEE T NEUR NET LEAR, V30, P123, DOI 10.1109/TNNLS.2018.2833077
   Zhang ML, 2018, IEEE T COGN DEV SYST, V10, P151, DOI 10.1109/TCDS.2017.2651943
   Zhang ML, 2017, NEUROCOMPUTING, V219, P333, DOI 10.1016/j.neucom.2016.09.044
   Zhang YQ, 2018, SENS IMAGING, V19, DOI 10.1007/s11220-018-0192-0
   Zhao R, 2019, MECH SYST SIGNAL PR, V115, P213, DOI 10.1016/j.ymssp.2018.05.050
NR 101
TC 4
Z9 4
U1 2
U2 38
PY 2020
VL 15
IS 8
BP 854
EP 865
DI 10.2174/1574893615999200425230713
UT WOS:000604954400006
DA 2023-11-16
ER

PT C
AU Shiomi, Y
   Torikai, H
AF Shiomi, Yuta
   Torikai, Hiroyuki
GP IEEE
TI A novel hardware-efficient ergodic sequential logic spiking neural
   network and reproductions of biologically plausible spatio-temporal
   phenomena towards development of neural prosthetic device
SO 2023 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS, IJCNN
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUN 18-23, 2023
CL Broadbeach, AUSTRALIA
DE Neural prosthesis; spiking neural network; non-linear dynamics; chimera
   state; ergodic sequential logic; field programmable gate array (FPGA)
AB An ergodic sequential logic (SL) neuron model is designed and it is shown that the model can reproduce various nonlinear responses of neurons. Using the neuron model, a novel ergodic SL spiking neural network is presented. It is shown that the network can reproduce biologically plausible spatio-temporal phenomena, (e.g., fundamental synchronization phenomenon and complicated chimera phenomenon) which are typically observed in the brain. In addition, the network is implemented by a field programmable gate array and it is shown that the presented network is more hardware-efficient compared to a commonly used digital processor spiking neural network. Furthermore, it is discussed that the presented network will contribute to develop a small and low power brain prosthetic device.
C1 [Shiomi, Yuta; Torikai, Hiroyuki] Hosei Univ, Grad Sch Sci & Engn, Tokyo, Japan.
RP Shiomi, Y (corresponding author), Hosei Univ, Grad Sch Sci & Engn, Tokyo, Japan.
EM yuta.shiomi.3i@stu.hosei.ac.jp; torikai@hosei.ac.jp
CR Buccelli S, 2019, ISCIENCE, V19, P402, DOI 10.1016/j.isci.2019.07.046
   Buzsáki G, 2012, ANNU REV NEUROSCI, V35, P203, DOI 10.1146/annurev-neuro-062111-150444
   Eshraghian JK, 2020, IEEE INSTRU MEAS MAG, V23, P21, DOI 10.1109/MIM.2020.8979519
   Hampson RE, 2018, J NEURAL ENG, V15, DOI 10.1088/1741-2552/aaaed7
   Izhikevich E. M., 2006, DYNAMICAL SYSTEMS NE, DOI DOI 10.7551/MITPRESS/2526.001.0001
   Kohno T, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00273
   Komaki S., 2021, IEEE T CIRCUITS SY 2
   LYON RF, 1988, IEEE T ACOUST SPEECH, V36, P1119, DOI 10.1109/29.1639
   Matsubara T., 2015, IEEE T NEURAL NETWOR, V27, P836
   Matsubara T, 2013, IEEE T NEUR NET LEAR, V24, P736, DOI 10.1109/TNNLS.2012.2230643
   MEAD C, 1990, P IEEE, V78, P1629, DOI 10.1109/5.58356
   Mosbacher Y, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-63934-4
   Panaggio MJ, 2015, NONLINEARITY, V28, pR67, DOI 10.1088/0951-7715/28/3/R67
   Rao RPN, 2019, CURR OPIN NEUROBIOL, V55, P142, DOI 10.1016/j.conb.2019.03.008
   Schumann CL, 2019, AIDS BEHAV, V23, P5, DOI 10.1007/s10461-017-1727-4
   Serb A, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-58831-9
   Shirafuji S., 2022, IEEE ACCESS
   Suzuki H., 2022, P IEEE ISCAS
   Takeda K, 2020, IEEE T CIRCUITS-I, V67, P1989, DOI 10.1109/TCSI.2020.2971786
   Timofeev I, 2012, SLEEP AND BRAIN ACTIVITY, P1, DOI 10.1016/B978-0-12-384995-3.00001-0
   Tsakalos KA, 2022, IEEE T CIRCUITS-I, V69, P4128, DOI 10.1109/TCSI.2022.3187376
   Wang ZH, 2020, FRONT PHYSIOL, V11, DOI 10.3389/fphys.2020.00724
   Zeng Fan-Gang, 2008, IEEE Rev Biomed Eng, V1, P115, DOI 10.1109/RBME.2008.2008250
NR 23
TC 0
Z9 0
U1 0
U2 0
PY 2023
DI 10.1109/IJCNN54540.2023.10191905
UT WOS:001046198706097
DA 2023-11-16
ER

PT J
AU Trhan, P
AF Trhan, Peter
TI THE APPLICATION OF SPIKING NEURAL NETWORKS IN AUTONOMOUS ROBOT CONTROL
SO COMPUTING AND INFORMATICS
DT Article
DE Spiking neural network; genetic algorithm; population; navigation
   system; mobile robot; trajectory control
AB Artificial neural networks have a wide range of applications nowadays in which they ale used for intelligent information processing This paper deals with an application of spiking neural networks in autonomous mobile robot control The topology of the implemented spiking neural networks was developed through a modified genetic algorithm and through the process of autonomous interaction with the scene environment Since the genetic algorithm did not use a crossover operator we adapted the mutation operator adding a constraint that prevented creation of a new generation of population with weak individuals in comparison with the previous generation of population The paper proposes a parallel combination of both left and right local spiking neural network as well as a practical implementation of this pi position in the form of an intelligent navigation system in an autonomous mobile robot This design enhances the Implemented navigation system with a new cognitive property of intelligent information processing using a spiking neural network Having been adapted to the scene environment, the navigation system was able to make light decisions, change its direction and refrain from collision with the scene walls
C1 Univ Matej Bel, Fac Nat Sci, Dept Comp Sci, Banska Bystrica 97401, Slovakia.
RP Trhan, P (corresponding author), Univ Matej Bel, Fac Nat Sci, Dept Comp Sci, Tajovskeho 40, Banska Bystrica 97401, Slovakia.
CR *AN LAB, AN LAB PUBL
   BALOG T, PECULIARITIES INSECT
   *ECL, ECL OP DEV PLATF
   Floreano D., 2008, HDB ROBOTICS
   FLOREANO D, 2001, DD, P38
   Floreano D, 2006, INT J INTELL SYST, V21, P1005, DOI 10.1002/int.20173
   Gerstner W., 2002, SPIKING NEURON MODEL
   Jacobs RA, 1991, NEURAL COMPUT, V3, P79, DOI 10.1162/neco.1991.3.1.79
   JORDAN MI, 2001, GRAPHICAL MODELS FDN, P254
   Kistler WM, 2000, NEURAL COMPUT, V12, P385, DOI 10.1162/089976600300015844
   KVASNICKA V, 2002, COGNITIVE SCI
   KVASNICKA V, STU 2000
   *LAB INT SYST, EC POL FDRAL LAUS
   *LEGO, LEGO MINDST NXT
   *LEJOS, JAV LEGO MINDST
   *MATHWORKS INC, 2008, GEN ALG DIR SEARCH T
   Nolfi S., 2000, EVOLUTIONARY ROBOTIC
   PECHLAT J, INSECT VISION
   *PLANET AL, INTR EHW
   Rekeczky C, 2003, PROCEEDINGS OF THE 2003 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL III, P774
   Rubin J, 2001, PHYS REV LETT, V86, P364, DOI 10.1103/PhysRevLett.86.364
   *SDN, DOWNL JMF 2 1 LE SOF
   TRHAN P, 2007, TECHN COMP PRAG 2007
   TRHAN P, 2007, P 9 INT C INF DRUZB, P77
   TRHAN P, 2007, APPL ELECT 2007, P223
   WEISHAUPL T, 2004, INT S PAR ARCH ALG N
   ZIEMKE T, 2000, INT SER COMPUTAT INT, P355
   ZUFFEREY JC, 2005, THESIS EPF LAUSANNE
NR 28
TC 6
Z9 6
U1 0
U2 9
PY 2010
VL 29
IS 5
BP 823
EP 847
UT WOS:000284193200007
DA 2023-11-16
ER

PT C
AU Mokhtar, M
   Halliday, DM
   Tyrrell, AM
AF Mokhtar, Maizura
   Halliday, David M.
   Tyrrell, Andy M.
BE Hornby, GS
   Sekanina, L
   Haddow, PC
TI Hippocampus-Inspired Spiking Neural Network on FPGA
SO EVOLVABLE SYSTEMS: FROM BIOLOGY TO HARDWARE, PROCEEDINGS
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 8th International Conference on Evolvable Systems
CY SEP 21-24, 2008
CL Prague, CZECH REPUBLIC
DE Bio-inspired Hardware; Spiking Neural Network; FPGA
AB There is now a great interest in exploiting the capabilities of hardware devices; especially the reconfigurable properties of FPGA to implement and study the dynamics of neural networks, specifically a biological inspired neural network. FPGA has a much greater appeal than other modes of implementation because ail FPGA implementation offers the flexibility of simple software (re-)configuration in comparison to other digital ASICs. In this paper a hippocampus-inspired spiking neural network is implemented oil all FPGA, in order to take advantage of these two characteristics as well as to achieve autonomy for a neuro-controller device.
C1 [Mokhtar, Maizura; Halliday, David M.; Tyrrell, Andy M.] Univ York, Dept Elect, Intelligence Syst Grp, York YO10 5DD, N Yorkshire, England.
RP Mokhtar, M (corresponding author), Univ York, Dept Elect, Intelligence Syst Grp, York YO10 5DD, N Yorkshire, England.
EM mm520@ohm.york.ac.uk; dh20@ohm.york.ac.uk; amt@ohm.york.ac.uk
CR ARLEO A, 2000, THESIS SWISS FEDERAL
   Blum KI, 1996, NEURAL COMPUT, V8, P85, DOI 10.1162/neco.1996.8.1.85
   DANIEL J, 1998, SYNAPTIC ORG BRAIN, P417
   Degris T, 2004, FROM ANIMALS TO ANIMATS 8, P255
   Di Paolo EA, 2002, ADAPT BEHAV, V10, P243, DOI 10.1177/1059712302010003006
   Floreano D, 2000, NEURAL NETWORKS, V13, P431, DOI 10.1016/S0893-6080(00)00032-0
   Gerstner W., 2002, SPIKING NEURON MODEL
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Mokhtar M, 2007, IEEE IJCNN, P813, DOI 10.1109/IJCNN.2007.4371062
   Mokhtar M, 2007, IEEE IJCNN, P795, DOI 10.1109/IJCNN.2007.4371059
   TRAUB RD, 1991, J NEUROPHYSIOL, V66, P635, DOI 10.1152/jn.1991.66.2.635
NR 11
TC 14
Z9 14
U1 0
U2 4
PY 2008
VL 5216
BP 362
EP 371
UT WOS:000260884000032
DA 2023-11-16
ER

PT C
AU Lee, K
   Choi, S
   Lew, D
   Park, J
AF Lee, Kyungchul
   Choi, Sunghyun
   Lew, Dongwoo
   Park, Jongsun
GP IEEE
TI Optimization Techniques for Conversion of Quantization Aware Trained
   Deep Neural Networks to Lightweight Spiking Neural Networks
SO 2021 36TH INTERNATIONAL TECHNICAL CONFERENCE ON CIRCUITS/SYSTEMS,
   COMPUTERS AND COMMUNICATIONS (ITC-CSCC)
DT Proceedings Paper
CT 36th International Technical Conference on Circuits/Systems, Computers
   and Communications (ITC-CSCC)
CY JUN 28-30, 2021
CL Jeju, SOUTH KOREA
DE spiking neural networks; ANN-SNN conversion; quantization aware training
AB In this paper, we present spiking neural network (SNN) conversion technique optimized for converting low bit-width artificial neural networks (ANN) trained with quantization aware training (QAT). Conventional conversion technique suffers significant accuracy drop on QAT ANNs due to different activation function used for QAT ANNs. To minimize such accuracy drop of the conventional conversion, the proposed technique uses Spike-Norm Skip, which selectively applies threshold balancing. In addition, subtraction based reset is used to further reduce accuracy degradation. The proposed conversion technique achieves an accuracy of 89.92% (0.68% drop) with a 5-bit weight on CIFAR-10 using VGG-16.
C1 [Lee, Kyungchul; Choi, Sunghyun; Lew, Dongwoo; Park, Jongsun] Korea Univ, Seoul, South Korea.
RP Lee, K (corresponding author), Korea Univ, Seoul, South Korea.
EM lkc1225@korea.ac.kr; cshy926@korea.ac.kr; wwe9712@korea.ac.kr;
   jongsun@korea.ac.kr
CR Chen JS, 2019, P IEEE, V107, P1655, DOI 10.1109/JPROC.2019.2921977
   Diehl PU, 2015, IEEE IJCNN
   Diehl PU, 2016, IEEE IJCNN, P4278, DOI 10.1109/IJCNN.2016.7727758
   Jacob B, 2018, PROC CVPR IEEE, P2704, DOI 10.1109/CVPR.2018.00286
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
NR 5
TC 0
Z9 0
U1 0
U2 3
PY 2021
DI 10.1109/ITC-CSCC52171.2021.9501427
UT WOS:000849981000029
DA 2023-11-16
ER

PT C
AU Kirkland, P
   Di Caterina, G
   Soraghan, J
   Matich, G
AF Kirkland, Paul
   Di Caterina, Gaetano
   Soraghan, John
   Matich, George
GP IEEE
TI SpikeSEG: Spiking Segmentation via STDP Saliency Mapping
SO 2020 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN) held as part
   of the IEEE World Congress on Computational Intelligence (IEEE WCCI)
CY JUL 19-24, 2020
CL ELECTR NETWORK
DE Spiking Neural Network; SNN; Convolution; STDP; Segmentation
ID FEATURES
AB Taking inspiration from the structure and behaviour of the human visual system and using the Transposed Convolution and Saliency Mapping methods of Convolutional Neural Networks (CNN), a spiking event-based image segmentation algorithm, SpikeSEG is proposed. The approach makes use of both spike-based imaging and spike-based processing, where the images are either standard images converted to spiking images or they are generated directly from a neuromorphic event driven sensor, and then processed using a spiking fully convolutional neural network. The spiking segmentation method uses the spike activations through time within the network to trace back any outputs from saliency maps, to the exact pixel location. This not only gives exact pixel locations for spiking segmentation, but with low latency and computational overhead. SpikeSEG is the first spiking event-based segmentation network and over three experiment test achieves promising results with 96% accuracy overall and a 74% mean intersection over union for the segmentation, all within an event by event-based framework.
C1 [Kirkland, Paul; Di Caterina, Gaetano; Soraghan, John] Univ Strathclyde, Neuromorph Sensing & Proc Lab, Glasgow, Lanark, Scotland.
   [Matich, George] Leonardo, London, England.
RP Kirkland, P (corresponding author), Univ Strathclyde, Neuromorph Sensing & Proc Lab, Glasgow, Lanark, Scotland.
EM paul.kirkland@starth.ac.uk; gaetano.di-caterina@strath.ac.uk;
   j.soraghan@strath.ac.uk; george.matich@leonardocompany.com
CR [Anonymous], 2018, ECCV
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Bichler O, 2012, NEURAL NETWORKS, V32, P339, DOI 10.1016/j.neunet.2012.02.022
   Brandli C, 2014, IEEE J SOLID-ST CIRC, V49, P2333, DOI 10.1109/JSSC.2014.2342715
   Burbank KS, 2015, PLOS COMPUT BIOL, V11, DOI 10.1371/journal.pcbi.1004566
   Cannici M., 2019, P IEEE C COMP VIS PA
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Carlson D, 2013, 2013 9TH IEEE INTERNATIONAL CONFERENCE ON DISTRIBUTED COMPUTING IN SENSOR SYSTEMS (IEEE DCOSS 2013), P1, DOI 10.1109/DCOSS.2013.73
   Delbruck T, 2007, IEEE INT SYMP CIRC S, P845, DOI 10.1109/ISCAS.2007.378038
   Desai NS, 2003, J PHYSIOL-PARIS, V97, P391, DOI 10.1016/j.jphysparis.2004.01.005
   Falez Pierre, 2019, 2019 INT JOINT C NEU, P1
   Gallego G, 2022, IEEE T PATTERN ANAL, V44, P154, DOI 10.1109/TPAMI.2020.3008413
   Glover A, 2017, IEEE INT C INT ROBOT, P3769, DOI 10.1109/IROS.2017.8206226
   Hao YZ, 2020, NEURAL NETWORKS, V121, P387, DOI 10.1016/j.neunet.2019.09.007
   Hunsberger Eric, 2015, COMPUT SCI
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Kim S., 2019, ARXIV190306530
   Kirkland P, 2019, LECT NOTES COMPUT SC, V11727, P724, DOI 10.1007/978-3-030-30487-4_56
   Lichtsteiner P, 2008, IEEE J SOLID-ST CIRC, V43, P566, DOI 10.1109/JSSC.2007.914337
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Linares-Barranco Alejandro, 2019, ARXIV190507419
   Liu Q., TECH REP
   Masquelier T, 2007, PLOS COMPUT BIOL, V3, P247, DOI 10.1371/journal.pcbi.0030031
   Masquelier T, 2018, FRONT COMPUT NEUROSC, V12, DOI 10.3389/fncom.2018.00074
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   Orchard G, 2015, FRONT NEUROSCI-SWITZ, V9, DOI [10.3389/fhins.2015.00437, 10.3389/fnins.2015.00437]
   Panda P., 2017, TECH REP
   Panda P, 2016, IEEE IJCNN, P299, DOI 10.1109/IJCNN.2016.7727212
   Park J, 2014, BIOMED CIRC SYST C, P675, DOI 10.1109/BioCAS.2014.6981816
   Paugam-Moisy H., 2012, HDB NATURAL COMPUTIN, V1, P1, DOI [DOI 10.1007/978-3-540-92910-9_10, 10.1007/978-3-540-92910-9_10]
   Poudel R. P. K., 2018, BMVC
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Shelhamer E, 2017, IEEE T PATTERN ANAL, V39, P640, DOI 10.1109/TPAMI.2016.2572683
   Siam M, 2018, IEEE COMPUT SOC CONF, P700, DOI 10.1109/CVPRW.2018.00101
   Simonyan Karen, 2014, ICLR WORKSH
   Thorpe SJ, 2012, LECT NOTES COMPUT SC, V7583, P516, DOI 10.1007/978-3-642-33863-2_53
   Wunderlich T, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00260
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zhang W, 2003, NAT REV NEUROSCI, V4, P885, DOI 10.1038/nrn1248
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhou BL, 2017, PROC CVPR IEEE, P5122, DOI 10.1109/CVPR.2017.544
NR 41
TC 9
Z9 11
U1 0
U2 1
PY 2020
DI 10.1109/ijcnn48605.2020.9207075
UT WOS:000626021403110
DA 2023-11-16
ER

PT J
AU Nascimben, M
   Rimondini, L
AF Nascimben, Mauro
   Rimondini, Lia
TI Molecular Toxicity Virtual Screening Applying a Quantized Computational
   SNN-Based Framework
SO MOLECULES
DT Article
DE machine learning; in silico toxicity prediction; molecular fingerprints;
   spiking neural networks
ID RANDOM FOREST; MACHINE; MODEL; NEURONS; CLASSIFICATION; OPTIMIZATION;
   REGRESSION; NETWORKS; NMDA; AMPA
AB Spiking neural networks are biologically inspired machine learning algorithms attracting researchers' attention for their applicability to alternative energy-efficient hardware other than traditional computers. In the current work, spiking neural networks have been tested in a quantitative structure-activity analysis targeting the toxicity of molecules. Multiple public-domain databases of compounds have been evaluated with spiking neural networks, achieving accuracies compatible with high-quality frameworks presented in the previous literature. The numerical experiments also included an analysis of hyperparameters and tested the spiking neural networks on molecular fingerprints of different lengths. Proposing alternatives to traditional software and hardware for time- and resource-consuming tasks, such as those found in chemoinformatics, may open the door to new research and improvements in the field.
C1 [Nascimben, Mauro; Rimondini, Lia] Univ Piemonte Orientale, Ctr Autoimmune & Allerg Dis CAAD, Dept Hlth Sci, I-28100 Novara, Italy.
   [Nascimben, Mauro] Enginsoft SpA, I-35129 Padua, Italy.
RP Nascimben, M (corresponding author), Univ Piemonte Orientale, Ctr Autoimmune & Allerg Dis CAAD, Dept Hlth Sci, I-28100 Novara, Italy.; Nascimben, M (corresponding author), Enginsoft SpA, I-35129 Padua, Italy.
EM m.nascimben@enginsoft.com
CR Ancuceanu R, 2019, ONCOL LETT, V17, P4188, DOI 10.3892/ol.2019.10068
   [Anonymous], MOLVS MOL VALIDATION
   Atz K, 2021, NAT MACH INTELL, V3, P1023, DOI 10.1038/s42256-021-00418-8
   Bajusz D, 2015, J CHEMINFORMATICS, V7, DOI 10.1186/s13321-015-0069-3
   Banerjee P, 2016, J CHEMINFORMATICS, V8, DOI 10.1186/s13321-016-0162-2
   Brunel N, 2007, BIOL CYBERN, V97, P337, DOI 10.1007/s00422-007-0190-0
   Burkitt AN, 2006, BIOL CYBERN, V95, P1, DOI 10.1007/s00422-006-0068-6
   Carlson KD, 2014, FRONT NEUROSCI-SWITZ, V8, DOI 10.3389/fnins.2014.00010
   Chavan S, 2014, INT J MOL SCI, V15, P18162, DOI 10.3390/ijms151018162
   Chen DL, 2020, AAAI CONF ARTIF INTE, V34, P3438
   Chen D, 2021, J PHYS CHEM LETT, V12, P10793, DOI 10.1021/acs.jpclett.1c03058
   Cherkasov A, 2014, J MED CHEM, V57, P4977, DOI 10.1021/jm4004285
   Czerminski R, 2001, QUANT STRUCT-ACT REL, V20, P227, DOI 10.1002/1521-3838(200110)20:3<227::AID-QSAR227>3.0.CO;2-Y
   Dai HJ, 2016, PR MACH LEARN RES, V48
   Dayan P, 2005, THEORETICAL NEUROSCI
   Daylight Toolkit, 2007, SMARTS A LANG DESCR
   Doucet JP, 2007, CURR COMPUT-AID DRUG, V3, P263, DOI 10.2174/157340907782799372
   Drwal MN, 2015, FRONT ENV SCI-SWITZ, V3, DOI 10.3389/fenvs.2015.00054
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   Durant JL, 2002, J CHEM INF COMP SCI, V42, P1273, DOI 10.1021/ci010132r
   Eshraghian JK, 2021, ARXIV
   FUKUTOME H, 1963, KYBERNETIK, V2, P28, DOI 10.1007/BF00292107
   Fung V, 2021, NPJ COMPUT MATER, V7, DOI 10.1038/s41524-021-00554-0
   Gayvert KM, 2016, CELL CHEM BIOL, V23, P1294, DOI 10.1016/j.chembiol.2016.07.023
   Gerum RC, 2021, NEURAL COMPUT, V33, P2827, DOI 10.1162/neco_a_01424
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Goh GB, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P302, DOI 10.1145/3219819.3219838
   Goh GB, 2017, J COMPUT CHEM, V38, P1291, DOI 10.1002/jcc.24764
   Goh GB, 2017, ARXIV
   Guo WZ, 2022, IEEE T NEUR NET LEAR, V33, P3988, DOI 10.1109/TNNLS.2021.3055421
   Hassabis D, 2017, NEURON, V95, P245, DOI 10.1016/j.neuron.2017.06.011
   Hinton G., 2012, NEURAL NETWORKS MACH
   Hu SS, 2020, IEEE J BIOMED HEALTH, V24, P3020, DOI 10.1109/JBHI.2020.2977009
   Idakwo G, 2020, J CHEMINFORMATICS, V12, DOI 10.1186/s13321-020-00468-x
   Jeon W, 2019, BIOINFORMATICS, V35, P4979, DOI 10.1093/bioinformatics/btz307
   Jiang J, 2021, J CHEM INF MODEL, V61, P1691, DOI 10.1021/acs.jcim.0c01294
   Jin WG, 2018, PR MACH LEARN RES, V80
   Judson RS, 2010, ENVIRON HEALTH PERSP, V118, P485, DOI 10.1289/ehp.0901392
   Kaiser F, 2007, LECT NOTES COMPUT SC, V4668, P380
   Karpov P, 2020, J CHEMINFORMATICS, V12, DOI 10.1186/s13321-020-00423-w
   Kingma DP., 2017, ARXIV
   Konovalov DA, 2007, J CHEM INF MODEL, V47, P1648, DOI 10.1021/ci700100f
   KROGH A, 1992, ADV NEUR IN, V4, P950
   Kuhn M, 2016, NUCLEIC ACIDS RES, V44, pD1075, DOI 10.1093/nar/gkv1075
   Landrum Greg, 2023, Zenodo, DOI 10.5281/ZENODO.7541264
   Lim S., 2021, 2020 25 INT C PATTER, P3146, DOI DOI 10.1109/ICPR48806.2021.9412555
   Loshchilov I, 2018, FIXING WEIGHT DECAY
   Maass W, 2001, PULSED NEURAL NETWOR
   Manna Davide L., 2022, Neuromorphic Computing and Engineering, DOI 10.1088/2634-4386/ac999b
   Markovic D, 2020, NAT REV PHYS, V2, P499, DOI 10.1038/s42254-020-0208-2
   Mauri A, 2006, MATCH-COMMUN MATH CO, V56, P237
   Mayr A, 2016, FRONT ENV SCI-SWITZ, V3, DOI 10.3389/fenvs.2015.00080
   Mitchell JBO, 2014, WIRES COMPUT MOL SCI, V4, P468, DOI 10.1002/wcms.1183
   Moreno-Bote R, 2005, NEUROCOMPUTING, V65, P441, DOI 10.1016/j.neucom.2004.10.016
   Moriwaki H, 2018, J CHEMINFORMATICS, V10, DOI 10.1186/s13321-018-0258-y
   Motamedi F, 2022, BIOINFORMATICS, V38, P469, DOI 10.1093/bioinformatics/btab659
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Neves BJ, 2018, FRONT PHARMACOL, V9, DOI 10.3389/fphar.2018.01275
   Oono K., 2019, ARXIV
   Oyedotun OK, 2023, APPL INTELL, V53, P15621, DOI 10.1007/s10489-022-04230-8
   Paszke A., 2019, ADV NEURAL INFORM PR
   Paul A., 2018, ARXIV
   Peng YZ, 2020, METHODS, V179, P55, DOI 10.1016/j.ymeth.2020.05.013
   Polishchuk PG, 2009, J CHEM INF MODEL, V49, P2481, DOI 10.1021/ci900203n
   Rahmani N, 2020, STRUCT CHEM, V31, P2129, DOI 10.1007/s11224-020-01543-7
   Rao A, 2022, NAT MACH INTELL, V4, P467, DOI 10.1038/s42256-022-00480-w
   Rao VR, 2007, TRENDS NEUROSCI, V30, P284, DOI 10.1016/j.tins.2007.03.012
   Richard AM, 2021, CHEM RES TOXICOL, V34, P189, DOI 10.1021/acs.chemrestox.0c00264
   Rogers D, 2010, J CHEM INF MODEL, V50, P742, DOI 10.1021/ci100050t
   ROSENBLATT F, 1958, PSYCHOL REV, V65, P386, DOI 10.1037/h0042519
   Roy K, 2019, NATURE, V575, P607, DOI 10.1038/s41586-019-1677-2
   Puentes PR, 2021, PLOS ONE, V16, DOI 10.1371/journal.pone.0241728
   Russell A, 2010, IEEE T NEURAL NETWOR, V21, P1950, DOI 10.1109/TNN.2010.2083685
   Singh H, 2015, BIOL DIRECT, V10, DOI 10.1186/s13062-015-0046-9
   Sutskever Ilya, 2013, INT C MACH LEARN, P1139
   Svetnik V, 2003, J CHEM INF COMP SCI, V43, P1947, DOI 10.1021/ci034160g
   Triolascarya K., 2022, J RESTI REKAYASA SIS, V6, P632, DOI [10.29207/resti.v6i4.4273, DOI 10.29207/RESTI.V6I4.4273]
   Wigh DS, 2022, WIRES COMPUT MOL SCI, V12, DOI 10.1002/wcms.1603
   Wu ZQ, 2018, CHEM SCI, V9, P513, DOI 10.1039/c7sc02664a
   Wu ZX, 2021, BRIEF BIOINFORM, V22, DOI 10.1093/bib/bbaa321
   Xu K, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8350934
   Xu YJ, 2017, J CHEM INF MODEL, V57, P2672, DOI 10.1021/acs.jcim.7b00244
   Yang K, 2019, J CHEM INF MODEL, V59, P3370, DOI 10.1021/acs.jcim.9b00237
   Yang Q, 2022, CHEMPHYSCHEM, V23, DOI 10.1002/cphc.202200255
   Ying C., 2021, ADV NEURAL INFORM PR, V34, P28877
   Young AR, 2019, IEEE ACCESS, V7, P135606, DOI 10.1109/ACCESS.2019.2941772
   Zeiler M.D., 2012, ADADELTA ADAPTIVE LE
   Zhang J, 2019, ARXIV
   Zhang J, 2019, J CHEM INF MODEL, V59, P4150, DOI 10.1021/acs.jcim.9b00633
   Zou XQ, 2021, SCI CHINA INFORM SCI, V64, DOI 10.1007/s11432-020-3227-1
NR 90
TC 1
Z9 1
U1 3
U2 3
PD FEB
PY 2023
VL 28
IS 3
AR 1342
DI 10.3390/molecules28031342
UT WOS:000932970200001
DA 2023-11-16
ER

PT C
AU Tamura, S
   Nishitani, Y
   Hosokawa, C
   Miyoshi, T
   Sawai, H
   Mizuno-Matsumoto, Y
   Chen, YW
AF Tamura, Shinichi
   Nishitani, Yoshi
   Hosokawa, Chie
   Miyoshi, Tomomitsu
   Sawai, Hajime
   Mizuno-Matsumoto, Yuko
   Chen, Yen-Wei
BE Wang, Y
   An, J
   Wang, L
   Li, Q
   Yan, G
   Chang, Q
TI Multiplex communication by BP learning in neural network
SO 2016 9TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING,
   BIOMEDICAL ENGINEERING AND INFORMATICS (CISP-BMEI 2016)
DT Proceedings Paper
CT 9th International Congress on Image and Signal Processing, BioMedical
   Engineering and Informatics (CISP-BMEI)
CY OCT 15-17, 2016
CL Datong, PEOPLES R CHINA
DE Neural network; BP learning; Multiplex communication; Fluctuation of
   neuron
AB It is a mystery that neural network composed of neurons with fluctuating characteristics can transmit information well reliably. In this paper, we show, in a simulation using a 9x9 2D mesh neural network, 9 to 1 multiplex communication is possible with 99% correct rate. Neurons are modeled by integrate and fire model without leak. Spikes spreads from transmitting neuron groups, propagated as spike waves, and received by receiving neurons. Then, the receiving neurons classify from which neuron group the spike waves come by back propagation neural network (BPN) method.
C1 [Tamura, Shinichi] NBL Technovator Co Ltd, Sennan, Japan.
   [Tamura, Shinichi; Nishitani, Yoshi] Osaka Univ, Grad Sch Med, Dept Radiol, Suita, Osaka, Japan.
   [Hosokawa, Chie] AIST, Biomed Res Inst, Ikeda, Osaka, Japan.
   [Miyoshi, Tomomitsu] Osaka Univ, Grad Sch Med, Dept Integrat Physiol, Suita, Osaka, Japan.
   [Sawai, Hajime] Osaka Prefecture Univ, Coll Hlth & Human Sci, Habikino, Japan.
   [Mizuno-Matsumoto, Yuko] Univ Hyogo, Grad Sch Appl Informat, Kobe, Hyogo, Japan.
   [Chen, Yen-Wei] Ritsumeikan Univ, Coll Informat Sci & Engn, Kusatsu, Japan.
RP Tamura, S (corresponding author), NBL Technovator Co Ltd, Sennan, Japan.; Tamura, S (corresponding author), Osaka Univ, Grad Sch Med, Dept Radiol, Suita, Osaka, Japan.
CR Gerstner W., 2002, SINGLE NEURONS POPUL
   Kamimura T., 2015, AUTOMATION CONTROL I, V3, P63, DOI DOI 10.11648/J.ACIS.20150305.11
   Mizuno-Matsumoto Y, 2001, BRAIN TOPOGR, V13, P269, DOI 10.1023/A:1011176612377
   Mizuno-Matsumoto Y, 1999, IEEE T BIO-MED ENG, V46, P271, DOI 10.1109/10.748980
   Nishitani Y, 2012, COMPUT INTEL NEUROSC, V2012, DOI 10.1155/2012/862579
   Tamura Shinichi, 2016, COMPUTATIONAL INTELL, V2016
   Tamura Shinichi, 2016, COMPUT INTEL NEUROSC, V2016
NR 7
TC 1
Z9 1
U1 0
U2 1
PY 2016
BP 825
EP 828
UT WOS:000405706400152
DA 2023-11-16
ER

PT C
AU Pozzi, I
   Nusselder, R
   Zambrano, D
   Bohté, S
AF Pozzi, Isabella
   Nusselder, Roeland
   Zambrano, Davide
   Bohte, Sander
BE Kurkova, V
   Manolopoulos, Y
   Hammer, B
   Iliadis, L
   Maglogiannis, I
TI Gating Sensory Noise in a Spiking Subtractive LSTM
SO ARTIFICIAL NEURAL NETWORKS AND MACHINE LEARNING - ICANN 2018, PT I
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 27th International Conference on Artificial Neural Networks (ICANN)
CY OCT 04-07, 2018
CL Rhodes, GREECE
DE Spiking neurons; LSTM; Recurrent neural networks; Supervised learning;
   Reinforcement learning
AB Spiking neural networks are being investigated both as biologically plausible models of neural computation and also as a potentially more efficient type of neural network. Recurrent neural networks in the form of networks of gating memory cells have been central in state-of-the-art solutions in problem domains that involve sequence recognition or generation. Here, we design an analog Long Short-Term Memory (LSTM) cell where its neurons can be substituted with efficient spiking neurons, where we use subtractive gating (following the subLSTM in [1]) instead of multiplicative gating. Subtractive gating allows for a less sensitive gating mechanism, critical when using spiking neurons. By using fast adapting spiking neurons with a smoothed Rectified Linear Unit (ReLU)-like effective activation function, we show that then an accurate conversion from an analog subLSTM to a continuous-time spiking subLSTM is possible. This architecture results in memory networks that compute very efficiently, with low average firing rates comparable to those in biological neurons, while operating in continuous time.
C1 [Pozzi, Isabella; Nusselder, Roeland; Zambrano, Davide; Bohte, Sander] Ctr Wiskunde & Informat, Amsterdam, Netherlands.
RP Pozzi, I (corresponding author), Ctr Wiskunde & Informat, Amsterdam, Netherlands.
EM isabella.pozzi@cwi.nl
CR [Anonymous], ARXIV160902053
   [Anonymous], 2012, ADV NEURAL INFORM PR
   Attwell D, 2001, J CEREBR BLOOD F MET, V21, P1133, DOI 10.1097/00004647-200110000-00001
   Bakker B, 2002, ADV NEUR IN, V14, P1475
   Cho K., 2014, ARXIV14061078, V1406, P1078
   Costa RP, 2017, ADV NEUR IN, V30
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Denève S, 2016, NAT NEUROSCI, V19, P375, DOI 10.1038/nn.4243
   Diehl P.U., 2015, 2015 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2015.7280696
   Esser SK, 2016, P NATL ACAD SCI USA, V113, P11441, DOI 10.1073/pnas.1604850113
   Gers FA, 2003, J MACH LEARN RES, V3, P115, DOI 10.1162/153244303768966139
   Greff K, 2017, IEEE T NEUR NET LEAR, V28, P2222, DOI 10.1109/TNNLS.2016.2582924
   Harmon M., 1996, MULTIPLAYER RESIDUAL, P45433
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hunsberger Eric, 2015, COMPUT SCI
   Jozefowicz R, 2015, PR MACH LEARN RES, V37, P2342
   Neil D., 2016, LEARNING BE EFFICIEN
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   Rombouts J., 2012, ADV NEURAL INFORM PR, P1871
   Shrestha A, 2017, SPIKE BASED LONG SHO
NR 20
TC 3
Z9 3
U1 0
U2 4
PY 2018
VL 11139
BP 284
EP 293
DI 10.1007/978-3-030-01418-6_28
UT WOS:000463336400028
DA 2023-11-16
ER

PT C
AU Johnson, C
   Venayagamoorthy, GK
AF Johnson, Cameron
   Venayagamoorthy, G. K.
GP IEEE
TI Encoding Real Values into Polychronous Spiking Networks
SO 2010 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS IJCNN 2010
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT World Congress on Computational Intelligence (WCCI 2010)
CY 2010
CL Barcelona, SPAIN
AB Spiking neural networks show promising capability in handling the same kind of scaling up of problems as living brains, due to their more faithful similarity to biological neural networks. The big challenge of dealing with spiking neural networks is getting data into and out of them, which requires proper encoding and decoding methods. Presented in this paper is an adaptation of Izhikevich's model of a polychronous spiking network and an encoding scheme for real valued data. Data is chosen arbitrarily to cover the range of the encoding scheme in order to best demonstrate the network's response to different inputs. Preliminary results show that the network is able to recognize distinct input values and respond to them with unique spiking patterns.
C1 [Johnson, Cameron; Venayagamoorthy, G. K.] Real Time Power & Intelligent Syst lab, Rolla, MO 65409 USA.
RP Johnson, C (corresponding author), Real Time Power & Intelligent Syst lab, Rolla, MO 65409 USA.
EM cameron.e.johnson@gmail.com; gkumar@ieee.org
CR Gerstner W., 2002, SPIKING NEURON MODEL
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   Iannella N, 2001, NEURAL NETWORKS, V14, P933, DOI 10.1016/S0893-6080(01)00080-6
   Izhikevich E., NEURAL COMPUTATION, V18, P245
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Johnson C, 2009, NEURAL NETWORKS, V22, P833, DOI 10.1016/j.neunet.2009.06.033
   Kozma R., 1999, IJCNN'99. International Joint Conference on Neural Networks. Proceedings (Cat. No.99CH36339), P52, DOI 10.1109/IJCNN.1999.831455
   Kozma R, 2000, IEEE IJCNN, P33, DOI 10.1109/IJCNN.2000.861431
   Principe JC, 2001, P IEEE, V89, P1030, DOI 10.1109/5.939813
   Zhang J, 2008, 2008 7TH WORLD CONGRESS ON INTELLIGENT CONTROL AND AUTOMATION, VOLS 1-23, P1537, DOI 10.1109/WCICA.2008.4593148
NR 11
TC 0
Z9 0
U1 0
U2 0
PY 2010
UT WOS:000287421402062
DA 2023-11-16
ER

PT J
AU Cachi, PG
   Ventura, S
   Cios, KJ
AF Cachi, Paolo G.
   Ventura, Sebastian
   Cios, Krzysztof J.
TI Improving Spiking Neural Network Performance with Auxiliary Learning
SO MACHINE LEARNING AND KNOWLEDGE EXTRACTION
DT Article
DE auxiliary learning; spiking neural networks; implicit differentiation
ID CLASSIFICATION; LOIHI
AB The use of back propagation through the time learning rule enabled the supervised training of deep spiking neural networks to process temporal neuromorphic data. However, their performance is still below non-spiking neural networks. Previous work pointed out that one of the main causes is the limited number of neuromorphic data currently available, which are also difficult to generate. With the goal of overcoming this problem, we explore the usage of auxiliary learning as a means of helping spiking neural networks to identify more general features. Tests are performed on neuromorphic DVS-CIFAR10 and DVS128-Gesture datasets. The results indicate that training with auxiliary learning tasks improves their accuracy, albeit slightly. Different scenarios, including manual and automatic combination losses using implicit differentiation, are explored to analyze the usage of auxiliary tasks.
C1 [Cachi, Paolo G.; Cios, Krzysztof J.] Virginia Commonwealth Univ, Dept Comp Sci, Richmond, VA 23220 USA.
   [Ventura, Sebastian] Univ Cordoba, Dept Comp & Artificial Intelligence, Cordoba 14071, Spain.
   [Cachi, Paolo G.] 601 West Main St, Richmond, VA 23220 USA.
RP Cios, KJ (corresponding author), Virginia Commonwealth Univ, Dept Comp Sci, Richmond, VA 23220 USA.
EM pcachi@vcu.edu; sventura@uco.es; kcios@vcu.edu
CR Alzubaidi L, 2021, J BIG DATA-GER, V8, DOI 10.1186/s40537-021-00444-8
   Amir A, 2017, PROC CVPR IEEE, P7388, DOI 10.1109/CVPR.2017.781
   Shrestha SB, 2018, Arxiv, DOI arXiv:1810.08646
   Buettner K, 2021, IEEE COMP SOC ANN, P138, DOI 10.1109/ISVLSI51109.2021.00035
   Ceolini E, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00637
   Davies M, 2021, P IEEE, V109, P911, DOI 10.1109/JPROC.2021.3067593
   Du YS, 2020, Arxiv, DOI [arXiv:1812.02224, DOI 10.48550/ARXIV.1812.02224]
   Emmert-Streib F, 2020, FRONT ARTIF INTELL, V3, DOI 10.3389/frai.2020.00004
   Eshraghian JK, 2023, Arxiv, DOI [arXiv:2109.12894, DOI 10.48550/ARXIV.2109.12894]
   Fang W., 2020, P IEEE CVF INT C COM
   Fang W., 2020, SPIKINGJELLY
   Frémaux N, 2010, J NEUROSCI, V30, P13326, DOI 10.1523/JNEUROSCI.6249-09.2010
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   Gerstner W, 2002, BIOL CYBERN, V87, P404, DOI 10.1007/s00422-002-0353-y
   Gerstner W, 2002, SPIKING NEURON MODEL, DOI [10.1017/CBO9780511815706, DOI 10.1017/CBO9780511815706]
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Guo YF, 2022, PROC CVPR IEEE, P326, DOI 10.1109/CVPR52688.2022.00042
   Hajizada E., 2022, P INT C NEUR SYST, DOI [10.1145/3546790.3546791, DOI 10.1145/3546790.3546791]
   HEBB D. O., 1949
   Kaiser J, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00424
   Kandel E. R., 2013, PRINCIPLES NEURAL SC, V5th
   Kasinski A., 2006, International Journal of Applied Mathematics and Computer Science, V16, P101
   Khalifa NE, 2022, ARTIF INTELL REV, V55, P2351, DOI 10.1007/s10462-021-10066-4
   KONORSKI J., 1948, Conditioned reflexes and neuron organization.
   Kugele A, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00439
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508
   Legenstein R, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.1000180
   Li HM, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00309
   Li Yan, 2021, arXiv
   Li YH, 2022, LECT NOTES COMPUT SC, V13667, P631, DOI 10.1007/978-3-031-20071-7_37
   Liu S, 2019, ADV NEUR IN, V32
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Meng Q., 2023, P IEEE ICV C COMP VI
   Mohammadi M, 2022, NEUROMORPH COMPUT EN, V2, DOI 10.1088/2634-4386/ac94f3
   Mozer M.C., 1989, COMPLEX SYST, V3, P348
   Na B., 2022, P INT C MACH LEARN B
   Navon A, 2021, Arxiv, DOI arXiv:2007.02693
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Orchard G., 2021, P 2021 IEEE WORKSH S
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Schröder F, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P2971
   Shen H., 2023, P ICASSP 2023 2023 I, P1, DOI [10.1109/ICASSP49357.2023.10096958, DOI 10.1109/ICASSP49357.2023.10096958]
   Shi B., 2020, ADV NEURAL INF PROCE, P7148, DOI DOI 10.48550/ARXIV.2010.08244
   Shorten C, 2019, J BIG DATA-GER, V6, DOI 10.1186/s40537-019-0197-0
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Smith J.D., 2020, P INT C NEUR SYST, DOI [10.1145/3407197.3407202, DOI 10.1145/3407197.3407202]
   Standley Trevor Scott, 2020, INT C MACHINE LEARNI, P9120
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Wang Z., 2018, P IEEE CVF C COMP VI
   Wu YJ, 2019, AAAI CONF ARTIF INTE, P1311
   Yin B., 2020, P INT C MEAS SYST
   Zheng H., 2020, P AAAI C ART INT NEW
   Zhou Z., 2023, P 11 INT C LEARN REP
NR 54
TC 0
Z9 0
U1 0
U2 0
PD SEP
PY 2023
VL 5
IS 3
BP 1010
EP 1022
DI 10.3390/make5030052
UT WOS:001073593100001
DA 2023-11-16
ER

PT C
AU Machado, P
   Cosma, G
   McGinnity, TM
AF Machado, Pedro
   Cosma, Georgina
   McGinnity, T. Martin
BE Tetko, IV
   Kurkova, V
   Karpov, P
   Theis, F
TI NatCSNN: A Convolutional Spiking Neural Network for Recognition of
   Objects Extracted from Natural Images
SO ARTIFICIAL NEURAL NETWORKS AND MACHINE LEARNING - ICANN 2019:
   THEORETICAL NEURAL COMPUTATION, PT I
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 28th International Conference on Artificial Neural Networks (ICANN)
CY SEP 17-19, 2019
CL Tech Univ Munchen, Klinikum Rechts Isar, Munich, GERMANY
HO Tech Univ Munchen, Klinikum Rechts Isar
DE SNN; CSNN; Bio-inspired neural networks; Object classification;
   Unsupervised learning; Supervised learning; ReSuMe; STDP
ID MODEL
AB Biological image processing is performed by complex neural networks composed of thousands of neurons interconnected via thousands of synapses, some of which are excitatory and others inhibitory. Spiking neural models are distinguished from classical neurons by being biological plausible and exhibiting the same dynamics as those observed in biological neurons. This paper proposes a Natural Convolutional Neural Network (NatCSNN) which is a 3-layer bio-inspired Convolutional Spiking Neural Network (CSNN), for classifying objects extracted from natural images. A two-stage training algorithm is proposed using unsupervised Spike Timing Dependent Plasticity (STDP) learning (phase 1) and ReSuMe supervised learning (phase 2). The NatCSNN was trained and tested on the CIFAR-10 dataset and achieved an average testing accuracy of 84.7% which is an improvement over the 2-layer neural networks previously applied to this dataset.
C1 [Machado, Pedro; Cosma, Georgina; McGinnity, T. Martin] Nottingham Trent Univ, Sch Sci & Technol, Computat Neurosci & Cognit Robot Grp, Nottingham, England.
RP Machado, P (corresponding author), Nottingham Trent Univ, Sch Sci & Technol, Computat Neurosci & Cognit Robot Grp, Nottingham, England.
EM pedro.baptistamachado@ntu.ac.uk; georgina.cosma@ntu.ac.uk;
   martin.mcginnity@ntu.ac.uk
CR [Anonymous], 2010, MNIST HANDWRITTEN DI
   Boedecker J, 2014, 2014 IEEE SYMPOSIUM ON ADAPTIVE DYNAMIC PROGRAMMING AND REINFORCEMENT LEARNING (ADPRL), P1
   Chen YR, 2018, INTEGRATION, V61, P49, DOI 10.1016/j.vlsi.2017.11.001
   Coates A, 2011, PROC INT CONF DOC, P440, DOI 10.1109/ICDAR.2011.95
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   Hu Y, 2018, SPIKING DEEP RESIDUA
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Kobayashi R, 2009, FRONT COMPUT NEUROSC, V3, DOI 10.3389/neuro.10.009.2009
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Krizhevsky A., 2010, CONVOLUTIONAL DEEP B, V40, P1
   Kulkarni SR, 2018, NEURAL NETWORKS, V103, P118, DOI 10.1016/j.neunet.2018.03.019
   Lee C, 2019, IEEE T COGN DEV SYST, V11, P384, DOI 10.1109/TCDS.2018.2833071
   Ling J., 2001, POWER HUMAN BRAIN PH
   Linssen C., 2018, ZENODO, V2, P1430, DOI [10.4249/scholarpedia.1430, DOI 10.4249/SCHOLARPEDIA.1430]
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   SENGUPTA A, 2018, FRONT NEUROSCI, V13, P1
   Srivastava R.K., 2015, TRAINING VERY DEEP N, P1, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]
   Tavanaei A, 2018, 2018 INT JOINT C NEU, DOI DOI 10.1109/IJCNN.2018.8489104
   Wang ZY, 2018, ADV MATER SCI ENG, V2018, DOI 10.1155/2018/3463298
NR 20
TC 3
Z9 3
U1 1
U2 2
PY 2019
VL 11727
BP 351
EP 362
DI 10.1007/978-3-030-30487-4_28
UT WOS:000546494000028
DA 2023-11-16
ER

PT C
AU Hulea, M
   Vieriu, G
   Bârleanu, A
AF Hulea, Mircea
   Vieriu, George
   Barleanu, Alexandru
BE Petre, E
   Brezovan, M
TI A method to implement wireless communication between areas of electronic
   neurons
SO 2016 20TH INTERNATIONAL CONFERENCE ON SYSTEM THEORY, CONTROL AND
   COMPUTING (ICSTCC)
SE International Conference on System Theory Control and Computing
DT Proceedings Paper
CT 20th International Conference on System Theory, Control and Computing
   (ICSTCC)
CY OCT 13-15, 2016
CL Sinaia, ROMANIA
DE electronic spiking neuron; wireless synapse; neural areas; synaptic
   configuration
ID MODEL
AB Spiking neural networks are designed for modeling the natural neural tissue physiology in order to increase the biological plausibility of the artificial neural structures. The best option to obtain real time response of spiking neural networks is to implement the artificial neurons in analogue hardware. Despite the fast operation, a disadvantage of analogue spiking neural networks is the difficulty in prototyping of high complexity artificial neural areas because the connections between neurons, as well as, between neural areas should be hardwired. A solution that allows fast reconfiguration of spiking neural networks during prototyping represents wireless communication that also ensures communication at significant distance between artificial neural areas.
   This paper presents a method to implement wireless communication in RF range between areas of electronic neurons. Also, we tested using a microcontroller how this method affects the normal operation of the electronic neural network taking into account that wireless synapses implies conversion of voltage levels to digital values, data transmission delay and restoration of voltage levels.
   The results showed that the difference between the energies generated by the neural network with and without modeled wireless synapses is not significant and the delay induced by the digital data transmission is reduced by the neural network activity.
C1 [Hulea, Mircea; Vieriu, George; Barleanu, Alexandru] TUIASI, Fac Automat Control & Comp Engn, Dept Comp Engn, Iasi, Romania.
RP Hulea, M (corresponding author), TUIASI, Fac Automat Control & Comp Engn, Dept Comp Engn, Iasi, Romania.
EM mhulea@tuiasi.ro
CR Gerstner W., 2002, SPIKING NEURONS MODE, p102 
   Hulea M., 2009, RO Patent, Patent No. [A 2009 00531 (22), 200900531]
   Hulea M., 2011, P 15 C SYST THEOR CO, P282
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Jannson Tomasz, 2009, Proceedings of the SPIE - The International Society for Optical Engineering, V7347, DOI 10.1117/12.817691
   Jannson T., 2008, P SPIE INT SOC OPTIC, V6964
   Jolivet R, 2004, J NEUROPHYSIOL, V92, P959, DOI 10.1152/jn.00190.2004
   Kim C., 2012, AN114 TEX INSTR, V114
   Kobayashi R, 2009, FRONT COMPUT NEUROSC, V3, DOI 10.3389/neuro.10.009.2009
   Lovelace JJ, 2008, NEURAL COMPUT, V20, P65, DOI 10.1162/neco.2008.20.1.65
   Maass W, 1998, PULSED NEURAL NETWORKS, P55
   Mudumbai R., IEEE T WIRELESS COMM, V6, p1754 
   O'Reylli R. C., 2000, COMPUTATIONAL EXPLOR, P32
   Sigg Stephan, 2011, SERIES LECT NOTES I, V104, P273
   Timis M, 2010, CARPATHIAN J ELECT C, V3, P124
   Whitlock JR, 2006, SCIENCE, V313, P1093, DOI 10.1126/science.1128134
NR 16
TC 1
Z9 1
U1 0
U2 2
PY 2016
BP 520
EP 525
UT WOS:000391609900088
DA 2023-11-16
ER

PT C
AU Faraji, MM
   Shouraki, SB
   Iranmehr, E
AF Faraji, Mohammad Mandi
   Shouraki, Saeed Bagheri
   Iranmehr, Ensieh
GP IEEE
TI Spiking Neural Network for Sound localization Using Microphone Array
SO 2015 23RD IRANIAN CONFERENCE ON ELECTRICAL ENGINEERING (ICEE)
SE Iranian Conference on Electrical Engineering
DT Proceedings Paper
CT 23rd Iranian Conference on Electrical Engineering
CY MAY 10-14, 2015
CL Sharif Univ of Technol, Tehran, IRAN
HO Sharif Univ of Technol
DE Sound Source Localization; Spiking Neural Network; Microphone Array;
   FPGA Implementation
AB Sound source localization is very useful in various fields of engineering applications. Due to remarkable ability of humans for sound source localization, this paper describes a simple biological inspired model based on spiking neural network for localizing sound source. In this paper, a simple method using analog and digital combinational circuits is proposed for generating spikes. Because of simplicity of the proposed generating spikes method, in this paper, microphone array is utilized instead of using two microphones in order to increase accuracy. Then, a neural structure based on spiking neural network is proposed which works by means of microphone's signals. This structure is designed in way that can be implemented on Field Programming Gate Array (FPGA) properly. Simulation results show that implementing of this model for different types of microphone array is not only very simple, but also shows high accuracy of localizing sound source.
C1 [Faraji, Mohammad Mandi; Shouraki, Saeed Bagheri; Iranmehr, Ensieh] Sharif Univ Technol, Dept Elect Engn, Tehran, Iran.
RP Faraji, MM (corresponding author), Sharif Univ Technol, Dept Elect Engn, Tehran, Iran.
EM mmfaraji@ee.sharif.edu; bagheri-s@sharif.ir; eiranmehr@ee.sharif.edu
CR [Anonymous], 2013, PHD DISSERTATION, DOI DOI 10.1109/ICME.2013.6607563
   Babagli B., 2012, VEHICULAR 2012 1 INT
   Chakraborty R., 2014 IEEE INT C AC S, P619
   FILLINGER A, 2007, IPSN 07
   Huo J, 2008, IEEE IJCNN, P155, DOI 10.1109/IJCNN.2008.4633782
   Izák R, 1999, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON MICROELECTRONICS FOR NEURAL, FUZZY AND BIO-INSPIRED SYSTEMS, MICORNEURO'99, P103, DOI 10.1109/MN.1999.758852
   JEFFRESS LA, 1948, J COMP PHYSIOL PSYCH, V61, P468
   Jindong Liu, 2009, Proceedings 2009 International Joint Conference on Neural Networks (IJCNN 2009 - Atlanta), P1855, DOI 10.1109/IJCNN.2009.5178672
   Lazzaro J, 1989, NEURAL COMPUT, V1, P47, DOI 10.1162/neco.1989.1.1.47
   Liu JD, 2010, NEUROCOMPUTING, V74, P129, DOI 10.1016/j.neucom.2009.10.030
   Weng Y, 2011, INT J DISTRIB SENS N, DOI 10.1155/2011/172902
   Zhang Ch., 2008, IEEE T MULTIMEDIA, V10
NR 12
TC 8
Z9 8
U1 0
U2 1
PY 2015
BP 1260
EP 1265
UT WOS:000380559800234
DA 2023-11-16
ER

PT C
AU Vasu, MC
   Izquierdo, EJ
AF Vasu, Madhavun Candadai
   Izquierdo, Eduardo J.
BE Lintas, A
   Rovetta, S
   Verschure, PFMJ
   Villa, AEP
TI Information Bottleneck in Control Tasks with Recurrent Spiking Neural
   Networks
SO ARTIFICIAL NEURAL NETWORKS AND MACHINE LEARNING - ICANN 2017, PT I
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 26th International Conference on Artificial Neural Networks (ICANN)
CY SEP 11-14, 2017
CL Alghero, ITALY
DE Spiking neurons; Evolutionary neural networks; Recurrent networks;
   Information theory; Information bottleneck
AB The nervous system encodes continuous information from the environment in the form of discrete spikes, and then decodes these to produce smooth motor actions. Understanding how spikes integrate, represent, and process information to produce behavior is one of the greatest challenges in neuroscience. Information theory has the potential to help us address this challenge. Informational analyses of deep and feed-forward artificial neural networks solving static input-output tasks, have led to the proposal of the Information Bottleneck principle, which states that deeper layers encode more relevant yet minimal information about the inputs. Such an analyses on networks that are recurrent, spiking, and perform control tasks is relatively unexplored. Here, we present results from a Mutual Information analysis of a recurrent spiking neural network that was evolved to perform the classic pole-balancing task. Our results show that these networks deviate from the Information Bottleneck principle prescribed for feed-forward networks.
C1 [Vasu, Madhavun Candadai; Izquierdo, Eduardo J.] Indiana Univ, Sch Informat & Comp, Cognit Sci, Bloomington, IN 47405 USA.
RP Vasu, MC (corresponding author), Indiana Univ, Sch Informat & Comp, Cognit Sci, Bloomington, IN 47405 USA.
EM madcanda@indiana.edu; edizquie@indiana.edu
CR [Anonymous], 2017, ARXIV170300548
   [Anonymous], 2001, P 37 ALL C COMM CONT
   BARTO AG, 1984, BEHAV PROCESS, V9, P89
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Berger T., 1971, ENCY TELECOMMUNICATI
   Borst A, 1999, NAT NEUROSCI, V2, P947, DOI 10.1038/14731
   De Jong K. A., 2006, EVOLUTIONARY COMPUTA
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Mnih V., 2013, PLAYING ATARI DEEP R
   Onat A., 1998, Q LEARNING RECURRENT
   Pasemann F, 1997, LECT NOTES COMPUT SC, V1240, P1279, DOI 10.1007/BFb0032588
   Salimans T., 2017, ARXIV170303864
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Simoncelli EP, 2001, ANNU REV NEUROSCI, V24, P1193, DOI 10.1146/annurev.neuro.24.1.1193
   Tishby N, 2015, 2015 IEEE INFORMATION THEORY WORKSHOP (ITW)
NR 16
TC 1
Z9 1
U1 0
U2 0
PY 2017
VL 10613
BP 236
EP 244
DI 10.1007/978-3-319-68600-4_28
PN I
UT WOS:000449802500028
DA 2023-11-16
ER

PT C
AU Stromatias, E
   Marsland, JS
AF Stromatias, Evangelos
   Marsland, John S.
GP IEEE
TI Supervised learning in Spiking Neural Networks with Limited Precision:
   SNN/LP
SO 2015 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUL 12-17, 2015
CL Killarney, IRELAND
DE Spiking neural networks; Supervised learning; Genetic algorithms;
   Limited synaptic precision; Temporal coding; Hardware implementation
ID NEURONS
AB A new supervised learning algorithm, SNNILP, is proposed for Spiking Neural Networks. This novel algorithm uses limited precision for both synaptic weights and synaptic delays; 3 bits in each case. Also a genetic algorithm is used for the supervised training. The results are comparable or better than previously published work. The results are applicable to the realization of large-scale hardware neural networks. One of the trained networks is implemented in programmable hardware.
C1 [Stromatias, Evangelos] Univ Manchester, Sch Comp Sci, Oxford Rd, Manchester, Lancs, England.
   [Marsland, John S.] Univ Liverpool, Dept Elect Engn & Elect, Liverpool L69 3GJ, Merseyside, England.
RP Stromatias, E (corresponding author), Univ Manchester, Sch Comp Sci, Oxford Rd, Manchester, Lancs, England.
EM stromate@cs.man.ac.uk; marsland@liv.ac.uk
CR [Anonymous], P 15 PRORISC WORKSH
   Bao Jian, 2010, 2010 2nd International Workshop on Education Technology and Computer Science (ETCS), P86, DOI 10.1109/ETCS.2010.448
   Belatreche A, 2003, P IEEE CYB INT CHALL, P39
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   Booij O, 2005, INFORM PROCESS LETT, V95, P552, DOI 10.1016/j.ipl.2005.05.023
   Gerstner W., 2002, SPIKING NEURON MODEL
   Ghosh-Dastidar S, 2007, INTEGR COMPUT-AID E, V14, P187
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   Haupt RL, 2003, PRACTICAL GENETIC AL
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   HOHFELD M, 1992, NEUROCOMPUTING, V4, P291, DOI 10.1016/0925-2312(92)90014-G
   Kistler WM, 1997, NEURAL COMPUT, V9, P1015, DOI 10.1162/neco.1997.9.5.1015
   Lichman M., 2013, UCI MACHINE LEARNING
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   MCKENNOCH S, 2006, NEUR NETW 2006 IJCNN, P3970
   Meftah B, 2010, NEURAL PROCESS LETT, V32, P131, DOI 10.1007/s11063-010-9149-6
   Murtagh F., 1991, NEUROCOMPUTING, V2, P183, DOI [DOI 10.1016/0925-2312(91)90023-5, 10.1016/0925-2312(91)90023-5]
   Plagianakos VP, 2000, IEEE IJCNN, P161, DOI 10.1109/IJCNN.2000.861451
   Rawlins, 1991, FDN GENETIC ALGORITH, P69, DOI DOI 10.1016/B978-0-08-050684-5.50008-2
   Smith Ian C., 2009, EXPERIENCES RUNNING
   Stromatias E., 2011, ARXIV E PRINTS
NR 22
TC 0
Z9 0
U1 0
U2 2
PY 2015
UT WOS:000370730603030
DA 2023-11-16
ER

PT J
AU Lajoie, G
   Thivierge, JP
   Shea-Brown, E
AF Lajoie, Guillaume
   Thivierge, Jean-Philippe
   Shea-Brown, Eric
TI Structured chaos shapes spike-response noise entrophy in balanced neural
   networks
SO FRONTIERS IN COMPUTATIONAL NEUROSCIENCE
DT Article
DE neural variability; chaotic networks; neural excitability; network
   dynamics; spiking stimulus responses
ID CORTICAL CIRCUITS; POPULATION; PATTERNS; INFORMATION; CORTEX; STATE
AB Large networks of sparsely coupled, excitatory and inhibitory cells occur throughout the brain. For many models of these networks, a striking feature is that their dynamics are chaotic and thus, are sensitive to small perturbations. How does this chaos manifest in the neural code? Specifically, how variable are the spike patterns that such a network produces in response to an input signal? To answer this, we derive a bound for a general measure of variability- spike-train entropy. This leads to important insights on the variability of multi-cell spike pattern distributions in large recurrent networks of spiking neurons responding to fluctuating inputs. The analysis is based on results from random dynamical systems theory and is complemented by detailed numerical simulations. We find that the spike pattern entropy is an order of magnitude lower than what would be extrapolated from single cells. This holds despite the fact that network coupling becomes vanishingly sparse as network size grows-a phenomenon that depends on "extensive chaos," as previously discovered for balanced networks without stimulus drive. Moreover, we show how spike pattern entropy is controlled by temporal features of the inputs. our findings provide insight into how neural networks may encode stimuli in the presence of inherently chaotic dynamics.
C1 [Lajoie, Guillaume] Max Planck Inst Dynam & Self Org, Nonlinear Dynam Dept, D-37077 Gottingen, Germany.
   [Lajoie, Guillaume] Max Planck Inst Dynam & Self Org, Bernstein Ctr Computat Neurosci, D-37077 Gottingen, Germany.
   [Lajoie, Guillaume; Shea-Brown, Eric] Univ Washington, Dept Appl Math, Seattle, WA 98195 USA.
   [Thivierge, Jean-Philippe] Univ Ottawa, Sch Psychol, Ottawa, ON K1N 6N5, Canada.
   [Thivierge, Jean-Philippe] Univ Ottawa, Ctr Neural Dynam, Ottawa, ON, Canada.
   [Shea-Brown, Eric] Univ Washington, Dept Physiol & Biophys, Seattle, WA 98195 USA.
RP Lajoie, G (corresponding author), Max Planck Inst Dynam & Self Org, Nonlinear Dynam Dept, Fassberg 17, D-37077 Gottingen, Germany.
EM glajoie@nld.ds.mpg.de
CR Abbott LF, 1999, NEURAL COMPUT, V11, P91, DOI 10.1162/089976699300016827
   Averbeck BB, 2006, NAT REV NEUROSCI, V7, P358, DOI 10.1038/nrn1888
   Buonomano DV, 2009, NAT REV NEUROSCI, V10, P113, DOI 10.1038/nrn2558
   Ecker AS, 2011, J NEUROSCI, V31, P14272, DOI 10.1523/JNEUROSCI.2539-11.2011
   Ermentrout B, 1996, NEURAL COMPUT, V8, P979, DOI 10.1162/neco.1996.8.5.979
   Greven A., 2003, ENTROPY
   Hu Y, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003469
   Kifer Y., 1986, ERGODIC THEORY RANDO, DOI [10.1007/978-1-4684-9175-3, DOI 10.1007/978-1-4684-9175-3]
   KUNITA H., 1990, STOCHASTIC FLOWS STO, V24
   Laje R, 2013, NAT NEUROSCI, V16, P925, DOI 10.1038/nn.3405
   Lajoie G, 2013, PHYS REV E, V87, DOI 10.1103/PhysRevE.87.052901
   LEDRAPPIER F, 1988, PROBAB THEORY REL, V80, P217, DOI 10.1007/BF00356103
   Lin KK, 2009, J NONLINEAR SCI, V19, P497, DOI 10.1007/s00332-009-9042-5
   Lin KK, 2009, J COMPUT NEUROSCI, V27, P135, DOI 10.1007/s10827-008-0133-3
   Lindner B, 2003, NEURAL COMPUT, V15, P1761, DOI 10.1162/08997660360675035
   London M, 2010, NATURE, V466, P123, DOI 10.1038/nature09086
   Luccioli S, 2012, PHYS REV LETT, V109, DOI 10.1103/PhysRevLett.109.138103
   Marre O, 2009, J NEUROSCI, V29, P14596, DOI 10.1523/JNEUROSCI.0753-09.2009
   MOLGEDEY L, 1992, PHYS REV LETT, V69, P3717, DOI 10.1103/PhysRevLett.69.3717
   Monteforte M, 2010, PHYS REV LETT, V105, DOI 10.1103/PhysRevLett.105.268104
   Rajan K, 2010, PHYS REV E, V82, DOI 10.1103/PhysRevE.82.011903
   Reinagel P, 2000, J NEUROSCI, V20, P5392, DOI 10.1523/JNEUROSCI.20-14-05392.2000
   Rieke F., 1996, SPIKES EXPLORING NEU
   Ritt J, 2003, PHYS REV E, V68, DOI 10.1103/PhysRevE.68.041915
   Ruelle, 1989, CHAOTIC EVOLUTION ST, DOI [10.1017/CBO9780511608773, DOI 10.1017/CBO9780511608773]
   Schneidman E, 2006, NATURE, V440, P1007, DOI 10.1038/nature04701
   Strong SP, 1998, PHYS REV LETT, V80, P197, DOI 10.1103/PhysRevLett.80.197
   Sun Y, 2010, J COMPUT NEUROSCI, V28, P247, DOI 10.1007/s10827-009-0202-2
   Sussillo D, 2009, NEURON, V63, P544, DOI 10.1016/j.neuron.2009.07.018
   Tiesinga P, 2008, NAT REV NEUROSCI, V9, P97, DOI 10.1038/nrn2315
   van Vreeswijk C, 1998, NEURAL COMPUT, V10, P1321, DOI 10.1162/089976698300017214
   Yang Y, 2008, NAT NEUROSCI, V11, P1262, DOI 10.1038/nn.2211
   ZOHARY E, 1994, NATURE, V370, P140, DOI 10.1038/370140a0
NR 33
TC 11
Z9 11
U1 0
U2 7
PD OCT 2
PY 2014
VL 8
AR 123
DI 10.3389/fncom.2014.00123
UT WOS:000346787100001
DA 2023-11-16
ER

PT J
AU Sun, QY
   Wu, QX
   Wang, X
   Hou, L
AF Sun, Qi Yan
   Wu, Qing Xiang
   Wang, Xuan
   Hou, Lei
TI A spiking neural network for extraction of features in colour opponent
   visual pathways and FPGA implementation
SO NEUROCOMPUTING
DT Article
DE Spiking neural network; Colour opponent; Visual pathways; Spike lime
   dependent learning; FPGA implementation
ID INFORMATION-THEORY; MODEL; NEURONS; RECOGNITION
AB The human vision system is one of the most complex and the most superior systems that have been known to date. Inspired by the visual ventral pathways, a hierarchical spiking neural network is proposed to extract features from colour opponent visual pathways. The network is composed of multiple visual processing channels to simulate processing mechanism of the visual system. The firing rate map of each channel is recorded and further processed. Useful features are extracted from firing rate map with the characteristics of colour contrast sensitivity, orientation selective and illuminant constancy. These characteristics greatly enrich the expression of colour features and enhance the ability of object recognition. Simulation algorithm of the spiking neural network is based on integrated-and-fire neuron model and a set of receptive fields. Simulation results show that the network has the merit of capability of dealing with complex pattern recognition tasks. Furthermore, a SNN blockset is developed to implement the spiking neural network on FPGAs. It greatly speeds up the hardware modeling of the spiking neural network so that SNNs can be efficiently used in the hardware systems such as robots.
C1 [Sun, Qi Yan; Wu, Qing Xiang; Wang, Xuan; Hou, Lei] Fujian Normal Univ, Coll Photon & Elect Engn, Key Lab Optoelecron Sci & Technol Med, Fujian Prov Key Lab Photon Technol,Minist Educ, Fuzhou 350007, Fujian, Peoples R China.
   [Sun, Qi Yan] Fujian Agr & Forestry Univ, Fuzhou 350002, Fujian, Peoples R China.
RP Wu, QX (corresponding author), Fujian Normal Univ, Coll Photon & Elect Engn, Key Lab Optoelecron Sci & Technol Med, Fujian Prov Key Lab Photon Technol,Minist Educ, Fuzhou 350007, Fujian, Peoples R China.
EM sunqiyan99168@163.com; qxwu@fjnu.edu.cn
CR [Anonymous], ENCY NEUROSCI
   [Anonymous], NEURON DYN
   [Anonymous], BMC NEUROSCIENCE S1
   [Anonymous], HARDWARE SOFTWARE CO
   [Anonymous], XIL BLOCKS REF GUID
   [Anonymous], P IS T 1 EUR C COL G
   [Anonymous], COMPUT INTELL NEUROS
   [Anonymous], 2015, 2015 INT JOINT C NEU, DOI DOI 10.1109/IJCNN.2015.7280592
   [Anonymous], QUICK START GUID XIL
   Baladron J, 2015, NEURAL NETWORKS, V67, P1, DOI 10.1016/j.neunet.2015.03.002
   Chatterjee S, 2003, NATURE, V426, P668, DOI 10.1038/nature02163
   DEVALOIS RL, 1965, COLD SPRING HARB SYM, V30, P567, DOI 10.1101/SQB.1965.030.01.055
   FUKUSHIMA K, 1980, BIOL CYBERN, V36, P193, DOI 10.1007/BF00344251
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Glackin B, 2009, I C FIELD PROG LOGIC, P670, DOI 10.1109/FPL.2009.5272339
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Huang D.S., 1996, SYSTEMATIC THEORY NE
   Huang DS, 2008, IEEE T NEURAL NETWOR, V19, P2099, DOI 10.1109/TNN.2008.2004370
   Huang DS, 1999, INT J PATTERN RECOGN, V13, P1083, DOI 10.1142/S0218001499000604
   HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837
   Indiveri G, 2006, IEEE T NEURAL NETWOR, V17, P211, DOI 10.1109/TNN.2005.860850
   Johnson DH, 2010, IEEE T INFORM THEORY, V56, P653, DOI 10.1109/TIT.2009.2037047
   Komatsu H, 1998, CURR OPIN NEUROBIOL, V8, P503, DOI 10.1016/S0959-4388(98)80038-X
   LIVINGSTONE MS, 1984, J NEUROSCI, V4, P309
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Pearson MJ, 2007, IEEE T NEURAL NETWOR, V18, P1472, DOI 10.1109/TNN.2007.891203
   Quiroga RQ, 2009, NAT REV NEUROSCI, V10, P173, DOI 10.1038/nrn2578
   Ratnasingam S, 2013, IEEE IJCNN
   Riesenhuber M, 1999, NAT NEUROSCI, V2, P1019, DOI 10.1038/14819
   Rossello JL, 2009, INT J NEURAL SYST, V19, P465, DOI 10.1142/S0129065709002166
   Shang L, 2006, NEUROCOMPUTING, V69, P1782, DOI 10.1016/j.neucom.2005.11.004
   Shapley R, 2011, VISION RES, V51, P701, DOI 10.1016/j.visres.2011.02.012
   Thomas DB, 2009, ANN IEEE SYM FIELD P, P45, DOI 10.1109/FCCM.2009.46
   Tigaeru L, 2011, ADV ELECTR COMPUT EN, V11, P29, DOI 10.4316/AECE.2011.04005
   Van Essen D.C., 1995, INTRO NEURAL ELECT N, P45
   Wang XF, 2009, IEEE T KNOWL DATA EN, V21, P1515, DOI 10.1109/TKDE.2009.21
   Wu QX, 2008, NEUROCOMPUTING, V71, P2055, DOI 10.1016/j.neucom.2007.10.020
   Wu QX, 2013, NEUROCOMPUTING, V116, P3, DOI 10.1016/j.neucom.2012.01.046
   Wu QX, 2010, LECT NOTES COMPUT SC, V6215, P49
   Wysoski SG, 2008, NEUROCOMPUTING, V71, P2563, DOI 10.1016/j.neucom.2007.12.038
   [张悦 Zhang Yue], 2014, [高技术通讯, Chinese High Technology Letters], V24, P407
NR 41
TC 6
Z9 8
U1 2
U2 53
PD MAR 8
PY 2017
VL 228
SI SI
BP 119
EP 132
DI 10.1016/j.neucom.2016.09.093
UT WOS:000393017900014
DA 2023-11-16
ER

PT C
AU Martinelli, F
   Dellaferrera, G
   Mainar, P
   Cernak, M
AF Martinelli, Flavio
   Dellaferrera, Giorgia
   Mainar, Pablo
   Cernak, Milos
GP IEEE
TI SPIKING NEURAL NETWORKS TRAINED WITH BACKPROPAGATION FOR LOW POWER
   NEUROMORPHIC IMPLEMENTATION OF VOICE ACTIVITY DETECTION
SO 2020 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL
   PROCESSING
SE International Conference on Acoustics Speech and Signal Processing
   ICASSP
DT Proceedings Paper
CT IEEE International Conference on Acoustics, Speech, and Signal
   Processing (ICASSP)
CY MAY 04-08, 2020
CL Barcelona, SPAIN
DE Spiking Neural Networks; Voice Activity Detection; Power Efficiency;
   Backpropagation; Neuromorphic Microchips
ID OPTIMIZATION
AB Recent advances in Voice Activity Detection (VAD) are driven by artificial and Recurrent Neural Networks (RNNs), however, using a VAD system in battery-operated devices requires further power efficiency. This can be achieved by neuromorphic hardware, which enables Spiking Neural Networks (SNNs) to perform inference at very low energy consumption. Spiking networks are characterized by their ability to process information efficiently, in a sparse cascade of binary events in time called spikes. However, a big performance gap separates artificial from spiking networks, mostly due to a lack of powerful SNN training algorithms. To overcome this problem we exploit an SNN model that can be recast into a recurrent network and trained with known deep learning techniques. We describe a training procedure that achieves low spiking activity and apply pruning algorithms to remove up to 85% of the network connections with no performance loss. The model competes with state-of-the-art performance at a fraction of the power consumption comparing to other methods.
C1 [Martinelli, Flavio; Dellaferrera, Giorgia] Ecole Polytech Fed Lausanne EPFL, Lausanne, Switzerland.
   [Mainar, Pablo; Cernak, Milos] Logitech Europe SA, Lausanne, Switzerland.
RP Martinelli, F (corresponding author), Ecole Polytech Fed Lausanne EPFL, Lausanne, Switzerland.
CR Bellec G., 2018, ADV NEURAL INFORM PR
   Bellec G, 2018, INT C LEARN REPR
   Benyassine A, 1997, IEEE COMMUN MAG, V35, P64, DOI 10.1109/35.620527
   Cheng Yu, 2017, ARXIV171009282
   Cho M, 2019, ISSCC DIG TECH PAP I, V62, P278, DOI 10.1109/ISSCC.2019.8662540
   Dean D, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 3 AND 4, P3110
   Esser SK, 2016, P NATL ACAD SCI USA, V113, P11441, DOI 10.1073/pnas.1604850113
   Frankle J., 2019, TRAINABLE NEURAL NET, P05
   Frenkel C, 2019, IEEE T BIOMED CIRC S, V13, P145, DOI 10.1109/TBCAS.2018.2880425
   Gelly G, 2018, IEEE-ACM T AUDIO SPE, V26, P646, DOI 10.1109/TASLP.2017.2769220
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   Ghaemmaghami H, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P2292
   Jassim WA, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P5524, DOI 10.1109/ICASSP.2018.8461952
   King DB, 2015, ACS SYM SER, V1214, P1
   Li JY, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL I, PROCEEDINGS, P61
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Martinelli Flavio, 2020, SPIKING NEURAL NETWO
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Mostafa Hesham, 2018, IEEE Trans Neural Netw Learn Syst, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Ng T, 2012, 13TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2012 (INTERSPEECH 2012), VOLS 1-3, P1967
   Ramírez J, 2004, SPEECH COMMUN, V42, P271, DOI 10.1016/j.specom.2003.10.002
   Ramirez J., 2007, ROBUST SPEECH RECOGN, V6, P1, DOI DOI 10.5772/4740
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Shrestha SB, 2018, ADV NEUR IN, V31
   Silva DA, 2017, TELECOMMUN INF TECH, P37, DOI 10.1007/978-3-319-53753-5_4
   Sohn J, 1999, IEEE SIGNAL PROC LET, V6, P1, DOI 10.1109/97.736233
   Tavanaei A., 2018, CORR
   Van Segbroeck M, 2013, INTERSPEECH, P704
   Zenke F, 2018, NEURAL COMPUT, V30, P1514, DOI 10.1162/neco_a_01086
   Zenke Friedemann, COSYNE 2019 CASC POR
   ZUE V, 1990, SPEECH COMMUN, V9, P351, DOI 10.1016/0167-6393(90)90010-7
NR 32
TC 12
Z9 13
U1 0
U2 2
PY 2020
BP 8544
EP 8548
DI 10.1109/icassp40776.2020.9053412
UT WOS:000615970408163
DA 2023-11-16
ER

PT J
AU Zenke, F
   Ganguli, S
AF Zenke, Friedemann
   Ganguli, Surya
TI SuperSpike: Supervised Learning in Multilayer Spiking Neural Networks
SO NEURAL COMPUTATION
DT Article
ID MODEL; RULE; TRANSFORMATIONS; PLASTICITY; RESPONSES; SPEED; TIME
AB A vast majority of computation in the brain is performed by spiking neural networks. Despite the ubiquity of such spiking, we currently lack an understanding of how biological spiking neural circuits learn and compute in vivo, as well as how we can instantiate such capabilities in artificial spiking circuits in silico. Here we revisit the problem of supervised learning in temporally coding multilayer spiking neural networks. First, by using a surrogate gradient approach, we derive SuperSpike, a nonlinear voltage-based three-factor learning rule capable of training multilayer networks of deterministic integrate-and-fire neurons to perform nonlinear computations on spatiotemporal spike patterns. Second, inspired by recent results on feedback alignment, we compare the performance of our learning rule under different credit assignment strategies for propagating output errors to hidden units. Specifically, we test uniform, symmetric, and random feedback, finding that simpler tasks can be solved with any type of feedback, while more complex tasks require symmetric feedback. In summary, our results open the door to obtaining a better scientific understanding of learning and computation in spiking neural networks by advancing our ability to train them to solve nonlinear problems involving transformations between different spatiotemporal spike time patterns.
C1 [Zenke, Friedemann; Ganguli, Surya] Stanford Univ, Dept Appl Phys, Stanford, CA 94305 USA.
   [Zenke, Friedemann] Univ Oxford, Ctr Neural Circuits & Behav, Oxford OX1 3SR, England.
RP Ganguli, S (corresponding author), Stanford Univ, Dept Appl Phys, Stanford, CA 94305 USA.
EM sganguli@stanford.edu
CR Abbott LF, 2016, NAT NEUROSCI, V19, P350, DOI 10.1038/nn.4241
   Albers C, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0148948
   [Anonymous], 2016, LEARNING MACHINE RAN
   [Anonymous], 2010, FRONT SYNAPTIC NEURO
   [Anonymous], 2017, ARXIV170604698
   [Anonymous], 2016, ARXIV161203214
   [Anonymous], ARXIV170306043
   ARTOLA A, 1990, NATURE, V347, P69, DOI 10.1038/347069a0
   Auer P, 2008, NEURAL NETWORKS, V21, P786, DOI 10.1016/j.neunet.2007.12.036
   Banerjee A, 2016, NEURAL COMPUT, V28, P826, DOI 10.1162/NECO_a_00829
   Bengio Yoshua, 2013, ABS13083432 CORR
   Bohte SM, 2007, NEURAL COMPUT, V19, P371, DOI 10.1162/neco.2007.19.2.371
   Bohte SM, 2011, LECT NOTES COMPUT SC, V6791, P60, DOI 10.1007/978-3-642-21735-7_8
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Booij O, 2005, INFORM PROCESS LETT, V95, P552, DOI 10.1016/j.ipl.2005.05.023
   Brea J, 2013, J NEUROSCI, V33, P9565, DOI 10.1523/JNEUROSCI.4098-12.2013
   Brendel W., 2017, ARXIV170303777
   Carneiro Gustavo, 2015, ARXIV150800330
   CRICK F, 1989, NATURE, V337, P129, DOI 10.1038/337129a0
   de Montigny S, 2016, NEURAL COMPUT, V28, P2461, DOI 10.1162/NECO_a_00884
   Denève S, 2016, NAT NEUROSCI, V19, P375, DOI 10.1038/nn.4243
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Eliasmith C, 2012, SCIENCE, V338, P1202, DOI 10.1126/science.1225266
   Feldman DE, 2012, NEURON, V75, P556, DOI 10.1016/j.neuron.2012.08.001
   Florian RV, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0040233
   Frémaux N, 2016, FRONT NEURAL CIRCUIT, V9, DOI 10.3389/fncir.2015.00085
   Frémaux N, 2010, J NEUROSCI, V30, P13326, DOI 10.1523/JNEUROSCI.6249-09.2010
   Gardner B, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0161335
   Gardner B, 2015, NEURAL COMPUT, V27, P2548, DOI 10.1162/NECO_a_00790
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   Gilra A, 2017, ELIFE, V6, DOI 10.7554/eLife.28295
   Gollisch T, 2008, SCIENCE, V319, P1108, DOI 10.1126/science.1149639
   GROSSBERG S, 1987, COGNITIVE SCI, V11, P23, DOI 10.1111/j.1551-6708.1987.tb00862.x
   Guergiuev J., 2016, ARXIV161000161
   Guerguiev J, 2017, ELIFE, V6, DOI 10.7554/eLife.22901
   Gütig R, 2016, SCIENCE, V351, DOI 10.1126/science.aab4113
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Hinton G, 2012, LECT 6A OVERVIEW MIN
   Izhikevich EM, 2007, CEREB CORTEX, V17, P2443, DOI 10.1093/cercor/bhl152
   Kingma DP., 2017, ARXIV
   Kusmierz L, 2017, CURR OPIN NEUROBIOL, V46, P170, DOI 10.1016/j.conb.2017.08.020
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lillicrap TP, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms13276
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Marblestone AH, 2016, FRONT COMPUT NEUROSC, V10, DOI 10.3389/fncom.2016.00094
   McClure P, 2016, FRONT COMPUT NEUROSC, V10, DOI 10.3389/fncom.2016.00131
   McIntosh LT, 2016, ADV NEUR IN, V29
   McKennoch S, 2006, IEEE IJCNN, P3970
   Memmesheimer RM, 2014, NEURON, V82, P925, DOI 10.1016/j.neuron.2014.03.026
   Mohemmed A, 2012, INT J NEURAL SYST, V22, DOI 10.1142/S0129065712500128
   Neftci E., 2016, ARXIV161205596
   OJA E, 1982, J MATH BIOL, V15, P267, DOI 10.1007/BF00275687
   Pfister JP, 2006, NEURAL COMPUT, V18, P1318, DOI 10.1162/neco.2006.18.6.1318
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Rezende DJ, 2014, FRONT COMPUT NEUROSC, V8, DOI 10.3389/fncom.2014.00038
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Shrestha SB, 2015, NEURAL NETWORKS, V63, P185, DOI 10.1016/j.neunet.2014.12.001
   Shrestha SB, 2017, NEURAL NETWORKS, V86, P54, DOI 10.1016/j.neunet.2016.10.011
   Sporea I, 2013, NEURAL COMPUT, V25, P473, DOI 10.1162/NECO_a_00396
   Thalmeier D, 2016, PLOS COMPUT BIOL, V12, DOI 10.1371/journal.pcbi.1004895
   Thorpe S, 1996, NATURE, V381, P520, DOI 10.1038/381520a0
   Toyoizumi T., 2005, ADV NEURAL INFORM PR, V17, P1409
   Urbanczik R, 2009, NEURAL COMPUT, V21, P340, DOI 10.1162/neco.2008.09-07-605
   van Rossum MCW, 2001, NEURAL COMPUT, V13, P751, DOI 10.1162/089976601300014321
   Victor JD, 1997, NETWORK-COMP NEURAL, V8, P127, DOI 10.1088/0954-898X/8/2/003
   Williams RJ, 1989, NEURAL COMPUT, V1, P270, DOI 10.1162/neco.1989.1.2.270
   Yamins DLK, 2014, P NATL ACAD SCI USA, V111, P8619, DOI 10.1073/pnas.1403112111
   Zenke F., 2017, SSBM SUPERSPIKE BENC
   Zenke F., 2014, THESIS
   Zenke F, 2015, NAT COMMUN, V6, DOI 10.1038/ncomms7922
   Zenke F, 2014, FRONT NEUROINFORM, V8, DOI 10.3389/fninf.2014.00076
NR 71
TC 260
Z9 263
U1 11
U2 65
PD JUN
PY 2018
VL 30
IS 6
BP 1514
EP 1541
DI 10.1162/neco_a_01086
UT WOS:000432863200002
HC Y
HP N
DA 2023-11-16
ER

PT J
AU Liu, YD
   Wang, LM
AF Liu Yu-Dong
   Wang Lian-Ming
TI Application of memristor-based spiking neural network in image edge
   extraction
SO ACTA PHYSICA SINICA
DT Article
DE spiking neural network; memristor; synapse; image edge extraction
AB By simulating biological synapses with memristors according to the function and principle of biological visual system and by combining the memory characteristic of memristor with high-efficient processing ability in spiking neural network, a three-layer spiking neural network model for image edge extraction is constructed, in which the image edge information is represented by the variation of the memristor conductance. The edge extraction result obtained with this approach has the characteristics of continuity, smoothness, low false leak detection and edge positioning accuracy. Since the processing mechanism of this neural network conforms to the biological counterpart, it offers a new idea for the bionic implementation of biological visual system.
C1 [Liu Yu-Dong; Wang Lian-Ming] NE Normal Univ, Sch Phys, Changchun 130024, Peoples R China.
RP Wang, LM (corresponding author), NE Normal Univ, Sch Phys, Changchun 130024, Peoples R China.
EM wanglm703@nenu.edu.cn
CR Afifi A, 2009, 2009 EUROPEAN CONFERENCE ON CIRCUIT THEORY AND DESIGN, VOLS 1 AND 2, P563, DOI 10.1109/ECCTD.2009.5275035
   Egmont P M, 2012, PATTERN RECOGN, V35, P2279
   Fang XD, 2012, CHINESE PHYS B, V21, DOI 10.1088/1674-1056/21/9/098901
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Hong QH, 2013, ACTA PHYS SIN-CH ED, V62, DOI 10.7498/aps.62.230502
   Hu FW, 2013, ACTA PHYS SIN-CH ED, V62, DOI 10.7498/aps.62.218401
   Jessell T M, 2002, PRINCIPLES NEURAL SC, P533
   Jin QT, 2011, ACTA PHYS SIN-CH ED, V60, DOI 10.7498/aps.60.098701
   Kim JJ, 2002, NAT REV NEUROSCI, V3, P453, DOI 10.1038/nrn849
   Li C, 2012, ACTA PHYS SIN-CH ED, V61, DOI 10.7498/aps.61.070701
   Liang Y, 2013, ACTA PHYS SIN-CH ED, V62, DOI 10.7498/aps.62.158501
   Pershin YV, 2010, NEURAL NETWORKS, V23, P881, DOI 10.1016/j.neunet.2010.05.001
   Serrano-Gotarredona T, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00002
   Song DH, 2012, ACTA PHYS SIN-CH ED, V61, DOI 10.7498/aps.61.118101
   Strukov DB, 2008, NATURE, V453, P80, DOI 10.1038/nature06932
   Wang L M, 2008, J NE NORMAL U NAT SC, V40, P346
   Wang LD, 2012, INT J BIFURCAT CHAOS, V22, DOI 10.1142/S0218127412502057
   Zhou J, 2012, CHINESE PHYS B, V21, DOI 10.1088/1674-1056/21/4/048401
NR 18
TC 2
Z9 2
U1 3
U2 55
PD APR
PY 2014
VL 63
IS 8
AR 080503
DI 10.7498/aps.63.080503
UT WOS:000336090200005
DA 2023-11-16
ER

PT C
AU Chen, T
   Sun, GQ
   Wei, ZN
   Li, HJ
   Cheung, KW
   Sun, YH
AF Chen, Tong
   Sun, Guoqiang
   Wei, Zhinong
   Li, Huijie
   Cheung, Kwok W.
   Sun, Yonghui
BE Jia, Y
   Du, J
   Li, H
   Zhang, W
TI Photovoltaic System Power Generation Forecasting Based on Spiking Neural
   Network
SO PROCEEDINGS OF THE 2015 CHINESE INTELLIGENT SYSTEMS CONFERENCE, VOL 1
SE Lecture Notes in Electrical Engineering
DT Proceedings Paper
CT Chinese Intelligent Systems Conference (CISC)
CY 2015
CL Yangzhou, PEOPLES R CHINA
DE Photovoltaic system; Spiking neural network; Similar day selection
   algorithm; Power generation forecasting
ID NEURONS
AB A forecasting model based on Spiking neural network (SNN) was proposed to tackle with the problem of the forecasting of photovoltaic system (PVS) power generation. This neural network uses temporal encoding scheme with precise times of spikes, which is closer to the real biological neural system and has powerful computing ability. Considering the main influencing factors such as season types, weather types, sunshine intensity and temperature etc., this model use the method of grey correlation analysis to select similar days. The high accuracy and robust applicability of the proposed forecasting model are verified by the simulation using actual operating data of PVS.
C1 [Chen, Tong; Sun, Guoqiang; Wei, Zhinong; Sun, Yonghui] Hohai Univ, Coll Energy & Elect Engn, Nanjing 211100, Jiangsu, Peoples R China.
   [Li, Huijie] ALSTOM GRID Technol Ctr Co Ltd, Shanghai 201114, Peoples R China.
   [Cheung, Kwok W.] ALSTOM Grid Inc, Redmond, WA 98052 USA.
RP Chen, T (corresponding author), Hohai Univ, Coll Energy & Elect Engn, Nanjing 211100, Jiangsu, Peoples R China.
EM ntuchentong@163.com
CR Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   Ding Ming, 2011, Power System Technology, V35, P152
   Gerstner W., 2002, SPIKING NEURON MODEL
   Ghosh-Dastidar S, 2007, INTEGR COMPUT-AID E, V14, P187
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Natschlager T, 1998, NETWORK-COMP NEURAL, V9, P319, DOI 10.1088/0954-898X/9/3/003
   Rowcliffe P, 2008, IEEE T NEURAL NETWOR, V19, P1626, DOI 10.1109/TNN.2008.2000999
   Shi J, 2012, IEEE T IND APPL, V48, P1064, DOI 10.1109/TIA.2012.2190816
   Xuyang G, 2013, POWER SYST TECHNOL, V37, P1499
   Yi Danhui, 2008, DATA ANAL APPL EVIEW
   Yona A., 2008, 2008 IEEE POW EN SOC, P1, DOI DOI 10.1109/PES.2008.4596295
   Yuan Xiaoling, 2013, Proceedings of the CSEE, V33, P57
   Zhao Zhengming, 2011, Automation of Electric Power Systems, V35, P101
NR 14
TC 0
Z9 0
U1 0
U2 7
PY 2016
VL 359
BP 573
EP 581
DI 10.1007/978-3-662-48386-2_59
UT WOS:000377755300059
DA 2023-11-16
ER

PT J
AU Rosselló, JL
   Canals, V
   Morro, A
   Oliver, A
AF Rossello, Josep L.
   Canals, Vincent
   Morro, Antoni
   Oliver, Antoni
TI HARDWARE IMPLEMENTATION OF STOCHASTIC SPIKING NEURAL NETWORKS
SO INTERNATIONAL JOURNAL OF NEURAL SYSTEMS
DT Article
DE Stochastic spiking neural networks; neural networks; signal processing;
   hardware implementation; Gabor filters
ID TRAINING ALGORITHM; PATTERN-ANALYSIS; MODEL; NEURONS; FPGA;
   CLASSIFICATION; PREDICTION; FREQUENCY
AB Spiking Neural Networks, the last generation of Artificial Neural Networks, are characterized by its bio-inspired nature and by a higher computational capacity with respect to other neural models. In real biological neurons, stochastic processes represent an important mechanism of neural behavior and are responsible of its special arithmetic capabilities. In this work we present a simple hardware implementation of spiking neurons that considers this probabilistic nature. The advantage of the proposed implementation is that it is fully digital and therefore can be massively implemented in Field Programmable Gate Arrays. The high computational capabilities of the proposed model are demonstrated by the study of both feed-forward and recurrent networks that are able to implement high-speed signal filtering and to solve complex systems of linear equations.
C1 [Rossello, Josep L.; Canals, Vincent; Morro, Antoni; Oliver, Antoni] Univ Illes Balears, Dept Phys, Palma de Mallorca 07122, Balears, Spain.
RP Rosselló, JL (corresponding author), Univ Illes Balears, Dept Phys, Cra Valldemossa Km 7-5, Palma de Mallorca 07122, Balears, Spain.
EM j.rossello@uib.es
CR ADELI H, 1995, NEURAL NETWORKS, V8, P769, DOI 10.1016/0893-6080(95)00026-V
   Agnes EJ, 2012, PHYSICA A, V391, P843, DOI 10.1016/j.physa.2011.08.036
   [Anonymous], INT J ARTIF INTELL
   Arena P, 2009, IEEE T NEURAL NETWOR, V20, P202, DOI 10.1109/TNN.2008.2005134
   Cawley S, 2011, GENET PROGRAM EVOL M, V12, P257, DOI 10.1007/s10710-011-9130-9
   DAUGMAN JG, 1985, J OPT SOC AM A, V2, P1160, DOI 10.1364/JOSAA.2.001160
   Fu SY, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P1637, DOI 10.1109/IJCNN.2011.6033421
   Ghosh-Dastidar S, 2007, INTEGR COMPUT-AID E, V14, P187
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Hamed H. N. A., 2011, P INT JOINT C NEUR N, P2457
   Hishiki T, 2011, IEEE T NEURAL NETWOR, V22, P752, DOI 10.1109/TNN.2011.2116802
   Indiveri G, 2006, IEEE T NEURAL NETWOR, V17, P211, DOI 10.1109/TNN.2005.860850
   Jahangiri A, 2011, INT J NEURAL SYST, V21, P127, DOI 10.1142/S0129065711002705
   Johnston SP, 2010, INT J NEURAL SYST, V20, P447, DOI 10.1142/S0129065710002541
   JONES JP, 1987, J NEUROPHYSIOL, V58, P1233, DOI 10.1152/jn.1987.58.6.1233
   Kasabov NK, 2011, IEEE T AUTON MENT DE, V3, P300, DOI 10.1109/TAMD.2011.2159839
   Khashman A, 2008, INT J NEURAL SYST, V18, P453, DOI 10.1142/S0129065708001713
   Koch Christof, 1999, P1
   Luque NR, 2011, INT J NEURAL SYST, V21, P385, DOI 10.1142/S0129065711002900
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Memmesheimer RM, 2010, P NATL ACAD SCI USA, V107, P11092, DOI 10.1073/pnas.0909615107
   Natschläger T, 1999, NEUROCOMPUTING, V26-7, P463, DOI 10.1016/S0925-2312(99)00052-1
   Natschlager T, 1998, NETWORK-COMP NEURAL, V9, P319, DOI 10.1088/0954-898X/9/3/003
   Nichols E, 2010, INT J NEURAL SYST, V20, P501, DOI 10.1142/S0129065710002577
   Rasheed F, 2009, INT J NEURAL SYST, V19, P11, DOI 10.1142/S012906570900180X
   RIEKE F, 1999, EXPLORING NEURAL COD
   ROSENBLATT F, 1958, PSYCHOL REV, V65, P386, DOI 10.1037/h0042519
   Rosselló JL, 2008, IEICE ELECTRON EXPR, V5, P1042, DOI 10.1587/elex.5.1042
   Rossello JL, 2009, INT J NEURAL SYST, V19, P465, DOI 10.1142/S0129065709002166
   Schliebs S, 2010, INT J NEURAL SYST, V20, P481, DOI 10.1142/S0129065710002565
   Schrauwen B, 2008, NEURAL NETWORKS, V21, P511, DOI 10.1016/j.neunet.2007.12.009
   Soltic S, 2010, INT J NEURAL SYST, V20, P437, DOI 10.1142/S012906571000253X
   Strain TJ, 2010, INT J NEURAL SYST, V20, P463, DOI 10.1142/S0129065710002553
   Sun Q, 2011, IEEE T NEURAL NETWOR, V22, P858, DOI 10.1109/TNN.2011.2125986
   Theodoridis D, 2010, INT J NEURAL SYST, V20, P129, DOI 10.1142/S0129065710002310
   Tigaeru L, 2011, ADV ELECTR COMPUT EN, V11, P29, DOI 10.4316/AECE.2011.04005
   van Schaik A, 2001, NEURAL NETWORKS, V14, P617, DOI 10.1016/S0893-6080(01)00067-3
   Vidybida A, 2011, INT J NEURAL SYST, V21, P187, DOI 10.1142/S0129065711002742
   Wade JJ, 2010, IEEE T NEURAL NETWOR, V21, P1817, DOI 10.1109/TNN.2010.2074212
   Wu HY, 2002, INT C PATT RECOG, P107, DOI 10.1109/ICPR.2002.1044624
   Yamin HY, 2004, INT J ELEC POWER, V26, P571, DOI 10.1016/j.ijepes.2004.04.005
   Zhang R., PHYS REV E, V80
NR 42
TC 34
Z9 35
U1 2
U2 34
PD AUG
PY 2012
VL 22
IS 4
AR 1250014
DI 10.1142/S0129065712500141
UT WOS:000306838400004
DA 2023-11-16
ER

PT J
AU Iannella, N
   Back, AD
AF Iannella, N
   Back, AD
TI A spiking neural network architecture for nonlinear function
   approximation
SO NEURAL NETWORKS
DT Article
DE spiking neurons; integrate-and-fire units; delays; neural architecture;
   nonlinear function approximation
AB Multilayer perceptrons have received much attention in recent years due to their universal approximation capabilities. Normally, such models use real valued continuous signals, although they are loosely based on biological neuronal networks that encode signals using spike trains. Spiking neural networks are of interest both from a biological point of view and in terms of a method of robust signaling in particularly noisy or difficult environments. It is important to consider networks based on spike trains. A basic question that needs to be considered however, is what type of architecture can be used to provide universal function approximation capabilities in spiking networks? In this paper, we propose a spiking neural network architecture using both integrate-and-fire units as well as delays, that is capable of approximating a real valued function mapping to within a specified degree of accuracy. (C) 2001 Elsevier Science Ltd. All rights reserved.
C1 RIKEN, Brain Sci Inst, Wako, Saitama 3510198, Japan.
RP Iannella, N (corresponding author), RIKEN, Brain Sci Inst, 2-1 Hirosawa, Wako, Saitama 3510198, Japan.
EM angelo@postman.riken.go.jp
CR BIALEK W, 1991, SCIENCE, V252, P1854, DOI 10.1126/science.2063199
   Cybenko G., 1989, Mathematics of Control, Signals, and Systems, V2, P303, DOI 10.1007/BF02551274
   DeWeese M, 1996, ADV NEUR IN, V8, P281
   FUNAHASHI K, 1989, NEURAL NETWORKS, V2, P183, DOI 10.1016/0893-6080(89)90003-8
   HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8
   Maass W, 1997, NEURAL COMPUT, V9, P279, DOI 10.1162/neco.1997.9.2.279
   Rieke F., 1997, SPIKES EXPLORING NEU
   Ruf B, 1997, LECT NOTES COMPUT SC, V1240, P265, DOI 10.1007/BFb0032484
   Ruyter van Steveninck R. de, 1995, Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences, V348, P321, DOI 10.1098/rstb.1995.0071
   Sanger TD, 1998, NEURAL COMPUT, V10, P1567, DOI 10.1162/089976698300017313
NR 10
TC 28
Z9 30
U1 0
U2 14
PD JUL-SEP
PY 2001
VL 14
IS 6-7
SI SI
BP 933
EP 939
DI 10.1016/S0893-6080(01)00080-6
UT WOS:000171417300030
DA 2023-11-16
ER

PT J
AU Jin, DZ
AF Jin, DZ
TI Spiking neural network for recognizing spatiotemporal sequences of
   spikes
SO PHYSICAL REVIEW E
DT Article
ID INHIBITORY NEURONS; INTERNEURONS; ORGANIZATION; INFORMATION; NEOCORTEX;
   SYNCHRONY; STATES; CORTEX; MODEL; TIME
AB Sensory neurons in many brain areas spike with precise timing to stimuli with temporal structures, and encode temporally complex stimuli into spatiotemporal spikes. How the downstream neurons read out such neural code is an important unsolved problem. In this paper, we describe a decoding scheme using a spiking recurrent neural network. The network consists of excitatory neurons that form a synfire chain, and two globally inhibitory interneurons of different types that provide delayed feedforward and fast feedback inhibition, respectively. The network signals recognition of a specific spatiotemporal sequence when the last excitatory neuron down the synfire chain spikes, which happens if and only if that sequence was present in the input spike stream. The recognition scheme is invariant to variations in the intervals between input spikes within some range. The computation of the network can be mapped into that of a finite state machine. Our network provides a simple way to decode spatiotemporal spikes with diverse types of neurons.
C1 MIT, Howard Hughes Med Inst, Cambridge, MA 02139 USA.
   MIT, Dept Brain & Cognit Sci, Cambridge, MA 02139 USA.
RP Jin, DZ (corresponding author), MIT, Howard Hughes Med Inst, Cambridge, MA 02139 USA.
EM djin@mit.edu
CR Ahissar E, 1997, P NATL ACAD SCI USA, V94, P11633, DOI 10.1073/pnas.94.21.11633
   [Anonymous], 1991, CORTICONICS
   Berry MJ, 1997, P NATL ACAD SCI USA, V94, P5411, DOI 10.1073/pnas.94.10.5411
   BIALEK W, 1991, SCIENCE, V252, P1854, DOI 10.1126/science.2063199
   Braitenberg V, 1967, Prog Brain Res, V25, P334
   BUONOMANO DV, 1995, SCIENCE, V267, P1028, DOI 10.1126/science.7863330
   Carrasco RC, 2000, NEURAL COMPUT, V12, P2129, DOI 10.1162/089976600300015097
   deCharms RC, 1998, SCIENCE, V280, P1439, DOI 10.1126/science.280.5368.1439
   Erisir A, 1999, J NEUROPHYSIOL, V82, P2476, DOI 10.1152/jn.1999.82.5.2476
   Gibson JR, 1999, NATURE, V402, P75, DOI 10.1038/47035
   Gupta A, 2000, SCIENCE, V287, P273, DOI 10.1126/science.287.5451.273
   Hansel D, 2003, NEURAL COMPUT, V15, P1, DOI 10.1162/089976603321043685
   Hansel D, 2001, PHYS REV LETT, V86, P4175, DOI 10.1103/PhysRevLett.86.4175
   Hopfield JJ, 2001, P NATL ACAD SCI USA, V98, P1282, DOI 10.1073/pnas.031567098
   Jin DZ, 2002, PHYS REV LETT, V89, DOI 10.1103/PhysRevLett.89.208102
   Jurafsky Dan, 2000, SPEECH LANGUAGE PROC
   Laurent G, 1999, SCIENCE, V286, P723, DOI 10.1126/science.286.5440.723
   Lewicki MS, 1996, J NEUROSCI, V16, P6987
   LEWIS TJ, 2002, SPIKING NEURON MODEL
   RAUSCHECKER JP, 1995, SCIENCE, V268, P111, DOI 10.1126/science.7701330
   Shimegi S, 2000, J NEUROSCI, V20, P6241, DOI 10.1523/JNEUROSCI.20-16-06241.2000
   Sipser M., 2012, INTRO THEORY COMPUTA
   Stopfer M, 1999, NATURE, V402, P664, DOI 10.1038/45244
   TANK DW, 1987, P NATL ACAD SCI USA, V84, P1896, DOI 10.1073/pnas.84.7.1896
   TREISMAN M, 1963, PSYCHOL MONOGR, V77, P1, DOI 10.1037/h0093864
   Trussell LO, 1999, ANNU REV PHYSIOL, V61, P477, DOI 10.1146/annurev.physiol.61.1.477
   Wang XJ, 1996, J NEUROSCI, V16, P6402
NR 27
TC 25
Z9 25
U1 2
U2 15
PD FEB
PY 2004
VL 69
IS 2
AR 021905
DI 10.1103/PhysRevE.69.021905
PN 1
UT WOS:000220255400070
DA 2023-11-16
ER

PT J
AU Carrillo-Medina, JL
   Latorre, R
AF Luis Carrillo-Medina, Jose
   Latorre, Roberto
TI Implementing Signature Neural Networks with Spiking Neurons
SO FRONTIERS IN COMPUTATIONAL NEUROSCIENCE
DT Article
DE bioinspired ANNs; neural signatures; subcellular plasticity;
   multicoding; local contextualization; signature neural network; spiking
   neuron
ID WINNERLESS COMPETITION; PATTERN-ANALYSIS; MODEL; INFORMATION; DYNAMICS;
   SPIKES; GENERATION; PLASTICITY; PREDICTION; CORTEX
AB Spiking Neural Networks constitute the most promising approach to develop realistic Artificial Neural Networks (ANNs). Unlike traditional firing rate-based paradigms, information coding in spiking models is based on the precise timing of individual spikes. It has been demonstrated that spiking ANNs can be successfully and efficiently applied to multiple realistic problems solvable with traditional strategies (e.g., data classification or pattern recognition). In recent years, major breakthroughs in neuroscience research have discovered new relevant computational principles in different living neural systems. Could ANNs benefit from some of these recent findings providing novel elements of inspiration? This is an intriguing question for the research community and the development of spiking ANNs including novel bio-inspired information coding and processing strategies is gaining attention. From this perspective, in this work, we adapt the core concepts of the recently proposed Signature Neural Network paradigm-i.e., neural signatures to identify each unit in the network, local information contextualization during the processing, and multicoding strategies for information propagation regarding the origin and the content of the data to be employed in a spiking neural network. To the best of our knowledge, none of these mechanisms have been used yet in the context of ANNs of spiking neurons. This paper provides a proof-of-concept for their applicability in such networks. Computer simulations show that a simple network model like the discussed here exhibits complex self-organizing properties. The combination of multiple simultaneous encoding schemes allows the network to generate coexisting spatio-temporal patterns of activity encoding information in different spatio-temporal spaces. As a function of the network and/or intra-unit parameters shaping the corresponding encoding modality, different forms of competition among the evoked patterns can emerge even in the absence of inhibitory connections. These parameters also modulate the memory capabilities of the network. The dynamical modes observed in the different informational dimensions in a given moment are independent and they only depend on the parameters shaping the information processing in this dimension. In view of these results, we argue that plasticity mechanisms inside individual cells and multicoding strategies can provide additional computational properties to spiking neural networks, which could enhance their capacity and performance in a wide variety of real-world tasks.
C1 [Luis Carrillo-Medina, Jose] Univ Fuerzas Armadas ESPE, Dept Elect & Elect, Sangolqui, Ecuador.
   [Latorre, Roberto] Univ Autonoma Madrid, Escuela Politecn Super, Dept Ingn Informat, Grp Neurocomputac Biol, Madrid, Spain.
RP Latorre, R (corresponding author), Univ Autonoma Madrid, Escuela Politecn Super, Dept Ingn Informat, Grp Neurocomputac Biol, Madrid, Spain.
EM roberto.latorre@uam.es
CR [Anonymous], 1996, WAVELETS COMPUTER GR
   Aref W. G., 2000, Database and expert systems applications. 11th International Conference, DEXA 2000. Proceedings (Lecture Notes in Computer Science Vol.1873), P774
   Arena P, 2009, INT J CIRC THEOR APP, V37, P505, DOI 10.1002/cta.567
   Barth AL, 2012, TRENDS NEUROSCI, V35, P345, DOI 10.1016/j.tins.2012.03.008
   Belatreche A, 2007, SOFT COMPUT, V11, P239, DOI [10.1007/s00500-006-0065-7, 10.1007/S00500-006-0065-7]
   BIALEK W, 1991, SCIENCE, V252, P1854, DOI 10.1126/science.2063199
   Bishop C., 1995, NEURAL NETWORKS PATT
   Bohte Sander M., 2004, Natural Computing, V3, P195, DOI 10.1023/B:NACO.0000027755.02868.60
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   Brette R, 2007, J COMPUT NEUROSCI, V23, P349, DOI 10.1007/s10827-007-0038-6
   Brochini L, 2011, J NEUROSCI, V31, P12297, DOI 10.1523/JNEUROSCI.1568-11.2011
   Campos D, 2007, NEUROCOMPUTING, V70, P1792, DOI 10.1016/j.neucom.2006.10.118
   Carrillo-Medina JL, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00033
   Catoni O., 1998, SIAM Journal on Control and Optimization, V36, P1539, DOI 10.1137/S0363012996307813
   Cessac B, 2010, J PHYSIOL-PARIS, V104, P5, DOI 10.1016/j.jphysparis.2009.11.002
   Davis GW, 2006, ANNU REV NEUROSCI, V29, P307, DOI 10.1146/annurev.neuro.28.061604.135751
   Deco G, 1998, NETWORK-COMP NEURAL, V9, P303, DOI 10.1088/0954-898X/9/3/002
   Deco G, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.1000092
   Diehl PU, 2015, IEEE IJCNN
   Diesmann M, 1999, NATURE, V402, P529, DOI 10.1038/990101
   Elson RC, 1999, J NEUROPHYSIOL, V82, P115, DOI 10.1152/jn.1999.82.1.115
   Esser SK, 2016, P NATL ACAD SCI USA, V113, P11441, DOI 10.1073/pnas.1604850113
   Garcia L, 2005, J NEUROPHYSIOL, V94, P3662, DOI 10.1152/jn.00496.2005
   GERSTNER W, 1993, BIOL CYBERN, V69, P503, DOI 10.1007/BF01185422
   Gerstner W., 2002, TOPICS, DOI [10.1017/cbo9780511815706, DOI 10.1103/PHYSREVE.51.738]
   Gerstner Wulfram, 2002, SPIKING NEURON MODEL, DOI [10.1017/CBO9780511815706, DOI 10.1017/CB09780511815706, DOI 10.1017/CBO9780511815706]
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   HINDMARSH JL, 1984, PROC R SOC SER B-BIO, V221, P87, DOI 10.1098/rspb.1984.0024
   HUDGINS B, 1993, IEEE T BIO-MED ENG, V40, P82, DOI 10.1109/10.204774
   Izhikevich E. M., 2006, DYNAMICAL SYSTEMS NE, DOI DOI 10.7551/MITPRESS/2526.001.0001
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2008, P NATL ACAD SCI USA, V105, P3593, DOI 10.1073/pnas.0712231105
   Kampakis S, 2013, NEURAL NETWORKS, V43, P41, DOI 10.1016/j.neunet.2013.01.011
   Kandel E. R., 1991, PRINCIPLES NEURAL SC
   Kaping D, 2011, PLOS BIOL, V9, DOI 10.1371/journal.pbio.1001224
   Karlik B, 2003, IEEE T BIO-MED ENG, V50, P1255, DOI 10.1109/TBME.2003.818469
   Kayser C, 2009, NEURON, V61, P597, DOI 10.1016/j.neuron.2009.01.008
   Kepecs A, 2004, NEUROCOMPUTING, V58, P1, DOI 10.1016/j.neucom.2004.01.014
   Kepecs A, 2003, NETWORK-COMP NEURAL, V14, P103, DOI 10.1088/0954-898X/14/1/306
   Kiebel SJ, 2009, PLOS COMPUT BIOL, V5, DOI 10.1371/journal.pcbi.1000464
   Klausberger T, 2003, NATURE, V421, P844, DOI 10.1038/nature01374
   Komendantov AO, 1996, J THEOR BIOL, V183, P219, DOI 10.1006/jtbi.1996.0215
   Kube K, 2008, NEUROCOMPUTING, V71, P1694, DOI 10.1016/j.neucom.2007.03.013
   Laje R, 2013, NAT NEUROSCI, V16, P925, DOI 10.1038/nn.3405
   Latorre R, 2004, NEUROCOMPUTING, V58, P535, DOI 10.1016/j.neucom.2004.01.091
   Latorre R, 2002, LECT NOTES COMPUT SC, V2415, P160
   Latorre R, 2007, NEUROCOMPUTING, V70, P1797, DOI 10.1016/j.neucom.2006.10.059
   Latorre R, 2006, BIOL CYBERN, V95, P169, DOI 10.1007/s00422-006-0077-5
   Latorre R, 2013, FRONT NEURAL CIRCUIT, V7, DOI 10.3389/fncir.2013.00138
   Latorre R, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1002908
   Latorre R, 2011, IEEE T NEURAL NETWOR, V22, P8, DOI 10.1109/TNN.2010.2060495
   Lestienne R, 1996, BIOL CYBERN, V74, P55, DOI 10.1007/BF00199137
   Li GL, 2010, IEEE T NEUR SYS REH, V18, P185, DOI 10.1109/TNSRE.2009.2039619
   Liu Z, 1998, J NEUROSCI, V18, P2309
   Maass W, 1997, NEURAL COMPUT, V9, P279, DOI 10.1162/neco.1997.9.2.279
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maass W, 1996, ADV NEUR IN, V8, P211
   Maass W, 2001, PULSED NEURAL NETWOR
   MAINEN ZF, 1995, SCIENCE, V268, P1503, DOI 10.1126/science.7770778
   Mallat S., 1999, WAVELET TOUR SIGNAL, V2nd, P20, DOI [10.1016/B978-012466606-1/50004-0, DOI 10.1016/B978-012466606-1/50004-0]
   Marin B, 2014, PHYS REV E, V90, DOI 10.1103/PhysRevE.90.042718
   Meftah B, 2010, NEURAL PROCESS LETT, V32, P131, DOI 10.1007/s11063-010-9149-6
   Middleton JW, 2011, J NEUROSCI, V31, P2461, DOI 10.1523/JNEUROSCI.4672-10.2011
   Natschlager T, 1998, NETWORK-COMP NEURAL, V9, P319, DOI 10.1088/0954-898X/9/3/003
   O'Connor DH, 2010, NEURON, V67, P1048, DOI 10.1016/j.neuron.2010.08.026
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   Panzeri S, 2010, TRENDS NEUROSCI, V33, P111, DOI 10.1016/j.tins.2009.12.001
   Ponulak F, 2011, ACTA NEUROBIOL EXP, V71, P409
   Rabinovich MI, 2006, BIOL CYBERN, V95, P519, DOI 10.1007/s00422-006-0121-5
   Rabinovich MI, 2006, REV MOD PHYS, V78, P1213, DOI 10.1103/RevModPhys.78.1213
   Reinagel P, 2002, J NEUROSCI, V22, P6837
   Rieke F., 1999, SPIKES EXPLORING NEU
   Rodríguez FB, 2002, LECT NOTES COMPUT SC, V2415, P167
   Ruf B, 1998, IEEE T NEURAL NETWOR, V9, P575, DOI 10.1109/72.668899
   Rumbell T, 2014, IEEE T NEUR NET LEAR, V25, P894, DOI 10.1109/TNNLS.2013.2283140
   Saini A, 2013, INT J ELEC POWER, V46, P376, DOI 10.1016/j.ijepes.2012.10.018
   SARIDIS GN, 1982, IEEE T BIO-MED ENG, V29, P403, DOI 10.1109/TBME.1982.324954
   Sato TR, 2007, PLOS BIOL, V5, P1440, DOI 10.1371/journal.pbio.0050189
   Schmuker M, 2014, P NATL ACAD SCI USA, V111, P2081, DOI 10.1073/pnas.1303053111
   Seliger P, 2003, PHYS REV E, V67, DOI 10.1103/PhysRevE.67.011905
   Shoham S, 2006, J COMP PHYSIOL A, V192, P777, DOI 10.1007/s00359-006-0117-6
   Somogyi P, 2005, J PHYSIOL-LONDON, V562, P9, DOI 10.1113/jphysiol.2004.078915
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Sun R, 2001, IEEE INTELL SYST, V16, P67, DOI 10.1109/MIS.2001.1463065
   Szücs A, 2005, EUR J NEUROSCI, V21, P763, DOI 10.1111/j.1460-9568.2005.03894.x
   Szucs A, 2003, J NEUROPHYSIOL, V89, P1363, DOI 10.1152/jn.00732.2002
   Tabak J, 2010, J NEUROPHYSIOL, V103, P2208, DOI 10.1152/jn.00857.2009
   Taylor C, 1994, MACHINE LEARNING NEU, V13, P1, DOI DOI 10.1080/00401706.1995.10484383
   Tristán A, 2004, NEUROCOMPUTING, V58, P41, DOI 10.1016/j.neucom.2004.01.020
   Turrigiano GG, 2004, NAT REV NEUROSCI, V5, P97, DOI 10.1038/nrn1327
   Turrigiano G, 2007, CURR OPIN NEUROBIOL, V17, P318, DOI 10.1016/j.conb.2007.04.004
   VanRullen R, 2005, TRENDS NEUROSCI, V28, P1, DOI 10.1016/j.tins.2004.10.010
   Varona P, 2001, NEURAL NETWORKS, V14, P865, DOI 10.1016/S0893-6080(01)00046-6
   Varona P, 2001, BIOL CYBERN, V84, P91, DOI 10.1007/s004220000198
   Wang P, 2014, IEEE SYST J, V8, P63, DOI 10.1109/JSYST.2013.2265663
   Wiedemann UA, 2003, J NEUROPHYSIOL, V90, P3902, DOI 10.1152/jn.00284.2003
   Wörgötter F, 2005, NEURAL COMPUT, V17, P245, DOI 10.1162/0899766053011555
   Yu QF, 2013, PLOS ONE, V8, DOI [10.1371/journal.pone.0059627, 10.1371/journal.pone.0078318]
   Zeck GM, 2007, EUR J NEUROSCI, V26, P367, DOI 10.1111/j.1460-9568.2007.05670.x
   Zhang W, 2003, NAT REV NEUROSCI, V4, P885, DOI 10.1038/nrn1248
   ZIPSER D, 1993, J NEUROSCI, V13, P3406
NR 103
TC 4
Z9 4
U1 1
U2 17
PD DEC 20
PY 2016
VL 10
AR 132
DI 10.3389/fncom.2016.00132
UT WOS:000390062100001
DA 2023-11-16
ER

PT C
AU Dabbous, A
   Ibrahim, A
   Valle, M
   Bartolozzi, C
AF Dabbous, Ali
   Ibrahim, Ali
   Valle, Maurizio
   Bartolozzi, Chiara
GP IEEE
TI Touch Modality Classification using Spiking Neural Networks and
   Supervised-STDP Learning
SO 2021 28TH IEEE INTERNATIONAL CONFERENCE ON ELECTRONICS, CIRCUITS, AND
   SYSTEMS (IEEE ICECS 2021)
SE IEEE International Conference on Electronics Circuits and Systems
DT Proceedings Paper
CT 28th IEEE International Conference on Electronics, Circuits, and Systems
   (IEEE ICECS)
CY NOV 28-DEC 01, 2021
CL Dubai, U ARAB EMIRATES
DE SNN; STDP; Touch Modality; LIF; tactile sensors
AB Spiking Neural Networks and synaptic learning have recently emerged as viable techniques to solve classification problems characterized by high computational efficiency when implemented on low-power neuromorphic hardware. This paper presents the implementation of a Spiking Neural Network endowed with supervised Spike Timing Dependent Plasticity for touch modality classification (e.g. poke, press, grab, squeeze, push, and rolling a wheel). Results demonstrates the ability of the network to learn appropriate connectivity patterns for the classification. The proposed network achieves a total accuracy of 88:3% overcoming similar state-of-the-art solutions.
C1 [Dabbous, Ali; Ibrahim, Ali; Valle, Maurizio] Univ Genoa, Connected Objects Sensing Mat Integrated Circuits, DITEN, Genoa, Italy.
   [Dabbous, Ali; Bartolozzi, Chiara] Ist Italiano Tecnol IIT, Event Driven Percept Robot EDPR Lab, Genoa, Italy.
RP Dabbous, A (corresponding author), Univ Genoa, Connected Objects Sensing Mat Integrated Circuits, DITEN, Genoa, Italy.; Dabbous, A (corresponding author), Ist Italiano Tecnol IIT, Event Driven Percept Robot EDPR Lab, Genoa, Italy.
CR Alameh M, 2021, IEEE SENS J, V21, P9983, DOI 10.1109/JSEN.2021.3055565
   Dabbous A, 2021, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS51556.2021.9401749
   Decherchi S, 2011, IEEE T ROBOT, V27, P635, DOI 10.1109/TRO.2011.2130030
   Gastaldo P, 2015, ROBOT AUTON SYST, V63, P268, DOI 10.1016/j.robot.2014.09.022
   Hao YZ, 2020, NEURAL NETWORKS, V121, P387, DOI 10.1016/j.neunet.2019.09.007
   Hu ZH, 2017, LECT NOTES COMPUT SC, V10635, P92, DOI 10.1007/978-3-319-70096-0_10
   Illing B, 2019, NEURAL NETWORKS, V118, P90, DOI 10.1016/j.neunet.2019.06.001
   Iwata H, 2005, IEEE T IND ELECTRON, V52, P1468, DOI 10.1109/TIE.2005.858739
   Jamali N, 2010, IEEE INT CONF ROBOT, P2336, DOI 10.1109/ROBOT.2010.5509675
   Mazid AM, 2008, 2008 10TH INTERNATIONAL CONFERENCE ON CONTROL AUTOMATION ROBOTICS & VISION: ICARV 2008, VOLS 1-4, P1830, DOI 10.1109/ICARCV.2008.4795806
   Naya F., 1999, IEEE SMC'99 Conference Proceedings. 1999 IEEE International Conference on Systems, Man, and Cybernetics (Cat. No.99CH37028), P1030, DOI 10.1109/ICSMC.1999.825404
   Rongala UB, 2020, NEURAL NETWORKS, V123, P273, DOI 10.1016/j.neunet.2019.11.020
   Stiehl W. D., 2004, 2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566), P3015
   Stimberg M, 2019, ELIFE, V8, DOI 10.7554/eLife.47314
   Strain TJ, 2006, IEEE IJCNN, P3409
   Sun J., 2017, ROB SENS TOUCH SENS
   Talbot M, 2008, 2008 WORLD AUTOMATION CONGRESS PROCEEDINGS, VOLS 1-3, P433
   Taunyazov T, 2020, IEEE INT C INT ROBOT, P9890, DOI 10.1109/IROS45743.2020.9340693
   Tawil DS, 2012, INT J ROBOT RES, V31, P1627, DOI 10.1177/0278364912455441
   Wang JL, 2014, NEUROCOMPUTING, V144, P526, DOI 10.1016/j.neucom.2014.04.017
NR 20
TC 0
Z9 0
U1 0
U2 5
PY 2021
DI 10.1109/ICECS53924.2021.9665453
UT WOS:000784162400006
DA 2023-11-16
ER

PT J
AU Wen, J
   Zhang, H
   Wu, ZW
   Wang, Q
   Yu, HM
   Sun, W
   Liang, BZ
   He, CY
   Xiong, KY
   Pan, Y
   Zhang, Y
   Liu, ZZ
AF Wen, Jin
   Zhang, Hui
   Wu, Zhengwei
   Wang, Qian
   Yu, Huimin
   Sun, Wei
   Liang, Bozhi
   He, Chenyao
   Xiong, Keyu
   Pan, Yu
   Zhang, Ying
   Liu, Zhanzhi
TI All-optical spiking neural network and optical spike-time-dependent
   plasticity based on the self-pulsing effect within a micro-ring
   resonator
SO APPLIED OPTICS
DT Article
AB In this paper, we proposed an all-optical version of photonic spiking neurons and spike-time-dependent plasticity (STDP) based on the nonlinear optical effects within a micro-ring resonator. In this system, the self-pulsing effect was exploited to implement threshold control, and the equivalent pulse energy required for spiking, calculated by multiplying the input pulse power amplitude with its duration, was about 14.1 pJ. The positive performance of the neurons in the excitability and cascadability tests validated the feasibility of this scheme. Furthermore, two simulations were performed to demonstrate that such an all-optical spiking neural network incorporated with STDP could run stably on a stochastic topology. The essence of such an all-optical spiking neural network is a non-linear spiking dynamical system that combines the advantages of photonics and spiking neural networks (SNNs), promising access to the high speed and lower consumption inherent to optical systems. & COPY; 2023 Optica Publishing Group
C1 [Wen, Jin; Zhang, Hui; Wu, Zhengwei; Wang, Qian; Yu, Huimin; Sun, Wei; Liang, Bozhi; He, Chenyao; Xiong, Keyu; Pan, Yu; Zhang, Ying; Liu, Zhanzhi] Xian Shiyou Univ, Sch Sci, Xian 710065, Peoples R China.
   [Wen, Jin] Chinese Acad Sci, Xian Inst Opt & Precis Mech, State Key Lab Transient Opt & Photon, Xian 710119, Peoples R China.
RP Wen, J (corresponding author), Xian Shiyou Univ, Sch Sci, Xian 710065, Peoples R China.; Wen, J (corresponding author), Chinese Acad Sci, Xian Inst Opt & Precis Mech, State Key Lab Transient Opt & Photon, Xian 710119, Peoples R China.
EM wenjin@xsyu.edu.cn
CR Azhigulov D, 2020, OPT QUANT ELECTRON, V52, DOI 10.1007/s11082-020-02526-y
   Caporale N, 2008, ANNU REV NEUROSCI, V31, P25, DOI 10.1146/annurev.neuro.31.060407.125639
   Chakraborty I, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-31365-x
   Chatzidimitriou D, 2015, J APPL PHYS, V118, DOI 10.1063/1.4926501
   Chen SW, 2012, OPT EXPRESS, V20, P7454, DOI 10.1364/OE.20.007454
   Du YA, 2023, IET OPTOELECTRON, V17, P1, DOI 10.1049/ote2.12086
   Feldmann J, 2019, NATURE, V569, P208, DOI 10.1038/s41586-019-1157-8
   Fu ZL, 2022, OPT EXPRESS, V30, P44943, DOI 10.1364/OE.476110
   Gu QLL, 2018, FRONT COMPUT NEUROSC, V12, DOI 10.3389/fncom.2018.00047
   Huang YL, 2022, IEEE PHOTONICS J, V14, DOI 10.1109/JPHOT.2022.3163793
   Husen A, 2023, ACM COMPUT SURV, V55, DOI 10.1145/3524106
   Izhikevich EM, 2003, TRENDS NEUROSCI, V26, P161, DOI 10.1016/S0166-2236(03)00034-1
   Jha A, 2022, J LIGHTWAVE TECHNOL, V40, P2901, DOI 10.1109/JLT.2022.3146157
   Kumar A, 2021, OPT QUANT ELECTRON, V53, DOI 10.1007/s11082-021-03131-3
   Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508
   Liang ZM, 2023, T I MEAS CONTROL, V45, P761, DOI 10.1177/01423312221093147
   Liou JC, 2019, OPT QUANT ELECTRON, V51, DOI 10.1007/s11082-019-1784-1
   Malaguti S, 2011, PHYS REV A, V83, DOI 10.1103/PhysRevA.83.051802
   Mesaritakis C., 2020, OPTICAL FIBER COMMUN, P1
   Nahmias MA, 2013, IEEE PHOTON CONF, P93, DOI 10.1109/IPCon.2013.6656385
   Neishtadt AI, 2021, RUSS MATH SURV+, V76, P883, DOI 10.1070/RM10023
   O'Toole AJ, 2021, ANNU REV VIS SCI, V7, P543, DOI 10.1146/annurev-vision-093019-111701
   Radosavljevic M, 2022, IEEE SPECTRUM, V59, P32, DOI 10.1109/MSPEC.2022.9976473
   Rakshit JK, 2020, BRAZ J PHYS, V50, P582, DOI 10.1007/s13538-020-00767-6
   Saunders DJ, 2019, NEURAL NETWORKS, V119, P332, DOI 10.1016/j.neunet.2019.08.016
   Sebastian A, 2020, NAT NANOTECHNOL, V15, P529, DOI 10.1038/s41565-020-0655-z
   Shastri BJ, 2016, SCI REP-UK, V6, DOI 10.1038/srep19126
   Shastri BJ, 2014, OPT QUANT ELECTRON, V46, P1353, DOI 10.1007/s11082-014-9884-4
   Taherkhani A, 2020, NEURAL NETWORKS, V122, P253, DOI 10.1016/j.neunet.2019.09.036
   Thacker HD, 2010, ELEC COMP C, P240, DOI 10.1109/ECTC.2010.5490965
   Uesugi T, 2006, OPT EXPRESS, V14, P377, DOI 10.1364/OPEX.14.000377
   Van Vaerenbergh T, 2012, OPT EXPRESS, V20, P20292, DOI 10.1364/OE.20.020292
   Wang XW, 2020, NEURAL NETWORKS, V125, P258, DOI 10.1016/j.neunet.2020.02.011
   Windisch A, 2023, APPL MATH-CZECH, V68, P35, DOI 10.21136/AM.2022.0158-21
   Xiang JL, 2020, J LIGHTWAVE TECHNOL, V38, P4019, DOI 10.1109/JLT.2020.2986233
   Xiang SY, 2019, IEEE J SEL TOP QUANT, V25, DOI 10.1109/JSTQE.2019.2911565
   Xu L., 2022, IEEE J SEL TOP QUANT, V28, pC1
   Yacomotti AM, 2006, PHYS REV LETT, V97, DOI 10.1103/PhysRevLett.97.143904
   Yamazaki K, 2022, BRAIN SCI, V12, DOI 10.3390/brainsci12070863
NR 39
TC 0
Z9 0
U1 12
U2 12
PD JUL 10
PY 2023
VL 62
IS 20
BP 5459
EP 5466
DI 10.1364/AO.493466
UT WOS:001038818900003
DA 2023-11-16
ER

PT C
AU Schoenauer, T
   Mehrtash, N
   Jahnke, A
   Klar, H
AF Schoenauer, T
   Mehrtash, N
   Jahnke, A
   Klar, H
BE Lindblad, T
   Padgett, ML
   Kinser, J
TI MASPINN: Novel concepts for a neuro-accelerator for spiking neural
   networks
SO NINTH WORKSHOP ON VIRTUAL INTELLIGENCE/DYNAMIC NEURAL NETWORKS:
   ACADEMIC/INDUSTRIAL/NASA/DEFENSE TECHNICAL INTERCHANGE AND TUTORIALS
SE PROCEEDINGS OF THE SOCIETY OF PHOTO-OPTICAL INSTRUMENTATION ENGINEERS
   (SPIE)
DT Proceedings Paper
CT 9th Workshop on Virtual Intelligence/Dynamic Neural Networks
CY JUN 22-28, 1998
CL ROYAL INST TECHNOL, STOCKHOLM, SWEDEN
HO ROYAL INST TECHNOL
DE spiking neural networks; pulse-coded neural networks; neurochip;
   neurocomputer; neuro-accelerator
ID VISUAL-CORTEX; OSCILLATIONS; CAT
AB We present the basic architecture of a Memory Optimized Accelerator for Spiking Neural Networks (MASPINN). The accelerator architecture exploits two novel concepts for an efficient computation of spiking neural networks: weight caching and a compressed memory organization These concepts allow a further parallelization in processing and reduce bandwidth requirements on accelerator's components. Therefore, they pave the way to dedicated digital hardware for real-time computation of more complex networks of pulse-coded neurons in the order of 10(6)neurons. The programmable neuron model which the accelerator is based on is described extensively. This shall encourage a discussion and suggestions on features which would be desirable to add to the current model.
C1 Tech Univ Berlin, Inst Microelect, D-10623 Berlin, Germany.
RP Schoenauer, T (corresponding author), Tech Univ Berlin, Inst Microelect, Jebensstr 1,Sekr J13, D-10623 Berlin, Germany.
CR ECKHORN R, 1988, BIOL CYBERN, V60, P121, DOI 10.1007/BF00202899
   ECKHORN R, 1989, P ICNN, V1, P723
   FRANK G, 1995, P INT C NEUR NETW IC, V4, P2014
   GERSTNER W, 1998, PULSED NEURAL NETWOR
   GRAY CM, 1989, P NATL ACAD SCI USA, V86, P1698, DOI 10.1073/pnas.86.5.1698
   HARTMANN G, 1997, P 6 INT C MICR NEUR, P130
   JAHNKE A, 1998, PULSED NEURAL NETWOR
   JAHNKE A, 1996, MICRONEURO 96, P232
   LAZARRO JP, 1993, ADV NEURAL INFORMATI, V5, P820
   SCHOTT U, 1997, COMMUNICATION
   VONDERMALSBURG C, 1981, 812 MPI BIOPH CHEM
   WEITZEL L, 1997, COMPUTER ANAL IMAGES
NR 12
TC 2
Z9 2
U1 0
U2 0
PY 1999
VL 3728
BP 87
EP 96
DI 10.1117/12.343072
UT WOS:000080215800007
DA 2023-11-16
ER

PT C
AU Shirsavar, SR
   Dehaqani, MRA
AF Shirsavar, Shahriar Rezghi
   Dehaqani, Mohammad-Reza A.
BE Matthews, MB
TI Performance and Runtime Improvement of Spiking Convolutional Neural
   Networks
SO 2022 56TH ASILOMAR CONFERENCE ON SIGNALS, SYSTEMS, AND COMPUTERS
SE Conference Record of the Asilomar Conference on Signals Systems and
   Computers
DT Proceedings Paper
CT 56th Asilomar Conference on Signals, Systems, and Computers
CY OCT 31-NOV 02, 2022
CL ELECTR NETWORK
DE Spiking Neural Network; STDP; Digit Recognition; Temporal Coding;
   Machine Learning
AB Spiking neural networks (SNNs) have closer dynamics to the brain compared to the current deep neural networks. Their low power consumption and sample efficiency make these networks interesting. Recently, several spiking convolutional neural networks have been proposed. These networks aim to increase biological plausibility while creating powerful tools to be applied to machine learning tasks. Here, we suggest a network structure based on previous work to improve network runtime and accuracy. Improvements to the network include reducing training iterations to only once, effectively using principal component analysis (PCA) dimension reduction, weight quantization, timed outputs for classification, and better hyperparameter tuning. Furthermore, the preprocessing step is changed to allow the processing of colored images instead of only black and white to improve accuracy. The proposed structure significantly reduces the runtime and introduces an efficient approach to convolutional SNNs.
C1 [Shirsavar, Shahriar Rezghi; Dehaqani, Mohammad-Reza A.] Univ Tehran, Sch Elect & Comp Engn, Coll Engn, Tehran, Iran.
RP Shirsavar, SR (corresponding author), Univ Tehran, Sch Elect & Comp Engn, Coll Engn, Tehran, Iran.
EM shahriar.rezghi@ut.ac.ir; dehaqani@ut.ac.ir
CR Abbott LF, 1999, BRAIN RES BULL, V50, P303, DOI 10.1016/S0361-9230(99)00161-6
   Adrian ED, 1926, J PHYSIOL-LONDON, V61, P465, DOI 10.1113/jphysiol.1926.sp002308
   Akbarzadeh-Sherbaf K, 2020, NEUROCOMPUTING, V412, P129, DOI 10.1016/j.neucom.2020.05.044
   Akbarzadeh-Sherbaf K, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00698
   Dehaqani MRA, 2018, CEREB CORTEX, V28, P3046, DOI 10.1093/cercor/bhy141
   Dehaqani MRA, 2016, J NEUROPHYSIOL, V116, P587, DOI 10.1152/jn.00018.2016
   Falez P., 2020, IEEE IJCNN, P1
   Frémaux N, 2016, FRONT NEURAL CIRCUIT, V9, DOI 10.3389/fncir.2015.00085
   Gerstner W, 1996, NATURE, V383, P76, DOI 10.1038/383076a0
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huynh PK, 2022, Arxiv, DOI arXiv:2202.08897
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   Mozafari M, 2019, PATTERN RECOGN, V94, P87, DOI 10.1016/j.patcog.2019.05.015
   Ng AY., 2004, P 21 INT C MACHINE L, P78
   Thorpe S, 1998, COMPUTATIONAL NEUROSCIENCE: TRENDS IN RESEARCH, P113
NR 16
TC 0
Z9 0
U1 1
U2 1
PY 2022
BP 801
EP 805
DI 10.1109/IEEECONF56349.2022.10051999
UT WOS:000976687600146
DA 2023-11-16
ER

PT J
AU Lin, RX
   Dai, BZ
   Zhao, YK
   Chen, G
   Lu, HX
AF Lin, Ranxi
   Dai, Benzhe
   Zhao, Yingkai
   Chen, Gang
   Lu, Huaxiang
TI Constrain Bias Addition to Train Low-Latency Spiking Neural Networks
SO BRAIN SCIENCES
DT Article
DE spiking neural network; backpropagation; Sigma-Delta ADC; neural
   encoding; pattern recognition
AB In recent years, a third-generation neural network, namely, spiking neural network, has received plethora of attention in the broad areas of Machine learning and Artificial Intelligence. In this paper, a novel differential-based encoding method is proposed and new spike-based learning rules for backpropagation is derived by constraining the addition of bias voltage in spiking neurons. The proposed differential encoding method can effectively exploit the correlation between the data and improve the performance of the proposed model, and the new learning rule can take complete advantage of the modulation properties of bias on the spike firing threshold. We experiment with the proposed model on the environmental sound dataset RWCP and the image dataset MNIST and Fashion-MNIST, respectively, and assign various conditions to test the learning ability and robustness of the proposed model. The experimental results demonstrate that the proposed model achieves near-optimal results with a smaller time step by maintaining the highest accuracy and robustness with less training data. Among them, in MNIST dataset, compared with the original spiking neural network with the same network structure, we achieved a 0.39% accuracy improvement.
C1 [Lin, Ranxi; Dai, Benzhe; Zhao, Yingkai; Chen, Gang; Lu, Huaxiang] Chinese Acad Sci, Inst Semicond, Beijing 100083, Peoples R China.
   [Lin, Ranxi; Dai, Benzhe; Zhao, Yingkai; Lu, Huaxiang] Univ Chinese Acad Sci, Beijing 100089, Peoples R China.
   [Chen, Gang; Lu, Huaxiang] Semicond Neural Network Intelligent Percept & Comp, Beijing 100083, Peoples R China.
   [Lu, Huaxiang] Univ Chinese Acad Sci, Coll Microelect, Beijing 100049, Peoples R China.
   [Lu, Huaxiang] Univ Chinese Acad Sci, Mat & Optoelect Res Ctr, Beijing 200031, Peoples R China.
RP Chen, G (corresponding author), Chinese Acad Sci, Inst Semicond, Beijing 100083, Peoples R China.; Chen, G (corresponding author), Semicond Neural Network Intelligent Percept & Comp, Beijing 100083, Peoples R China.
EM chengang08@semi.ac.cn
CR Adsumilli C.B., 2005, P P ICASSP 05 IEEE I, VVolume 2
   Agarap AF, 2018, ARXIV PREPRINT ARXIV, DOI DOI 10.48550/ARXIV.1803.08375
   Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   Bing Han, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P388, DOI 10.1007/978-3-030-58607-2_23
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Cheng X, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1519
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Dennis J, 2013, INT CONF ACOUST SPEE, P803, DOI 10.1109/ICASSP.2013.6637759
   Diehl PU, 2015, IEEE IJCNN
   Fang W., 2020, SPIKINGJELLY
   Froemke Robert C, 2010, Front Synaptic Neurosci, V2, P19, DOI 10.3389/fnsyn.2010.00019
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Hu YF, 2023, IEEE T NEUR NET LEAR, V34, P5200, DOI 10.1109/TNNLS.2021.3119238
   Jin YYZ, 2018, ADV NEUR IN, V31
   Kim S, 2020, AAAI CONF ARTIF INTE, V34, P11270
   Kiselev M, 2016, IEEE IJCNN, P1355, DOI 10.1109/IJCNN.2016.7727355
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee C, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00119
   McFee Brian, 2015, P 14 PYTH SCI C
   Mirsadeghi M., 2021, ARXIV
   Mun S, 2012, EUR SIGNAL PR CONF, P1424
   Nakamura Satoshi, 2000, ACOUSTICAL SOUND DAT
   Naud R, 2018, P NATL ACAD SCI USA, V115, pE6329, DOI 10.1073/pnas.1720995115
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   O'Connor P, 2017, Arxiv, DOI arXiv:1706.04159
   OConnor Peter, 2019, 22 INT C ARTIFICIAL, V89
   Pan ZH, 2020, Arxiv, DOI arXiv:2007.03274
   Peterson DG, 2023, IEEE TETCI, V7, P89, DOI 10.1109/TETCI.2022.3174905
   Ponulak F, 2011, ACTA NEUROBIOL EXP, V71, P409
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Schorkhuber C., 2010, SOUND MUSIC COMPUTIN, P3
   Sharma V., 2010, P 2010 INT JOINT C N, P1
   Tang JX, 2022, NEUROCOMPUTING, V501, P499, DOI 10.1016/j.neucom.2022.06.036
   Wu H, 2021, AAAI CONF ARTIF INTE, V35, P10320
   Wu JB, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00836
   Wu YJ, 2019, AAAI CONF ARTIF INTE, P1311
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Xiao H., 2017, ARXIV170807747
   Yan YL, 2022, FRONT NEUROSCI-SWITZ, V16, DOI 10.3389/fnins.2022.760298
   Yoon YC, 2017, IEEE T NEUR NET LEAR, V28, P1192, DOI 10.1109/TNNLS.2016.2526029
   Yousefzadeh A, 2019, 2019 IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE CIRCUITS AND SYSTEMS (AICAS 2019), P81, DOI [10.1109/AICAS.2019.8771624, 10.1109/aicas.2019.8771624]
   Yu Qiang, 2021, IEEE Trans Neural Netw Learn Syst, V32, P625, DOI 10.1109/TNNLS.2020.2978764
   Yu Q, 2019, INT CONF ACOUST SPEE, P890, DOI 10.1109/ICASSP.2019.8682963
   Zang YL, 2021, J NEUROSCI, V41, P1850, DOI 10.1523/JNEUROSCI.1719-20.2020
   Zang YL, 2020, ELIFE, V9, DOI 10.7554/eLife.60692
   Zang YL, 2018, CELL REP, V24, P1536, DOI 10.1016/j.celrep.2018.07.011
   Zhang WR, 2019, ADV NEUR IN, V32
   Zhao DC, 2020, FRONT COMPUT NEUROSC, V14, DOI 10.3389/fncom.2020.576841
NR 50
TC 0
Z9 0
U1 5
U2 9
PD FEB
PY 2023
VL 13
IS 2
AR 319
DI 10.3390/brainsci13020319
UT WOS:000938421100001
DA 2023-11-16
ER

PT J
AU Mostafa, H
AF Mostafa, Hesham
TI Supervised Learning Based on Temporal Coding in Spiking Neural Networks
SO IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
DT Article
DE Backpropagation; spiking networks; supervised learning
AB Gradient descent training techniques are remarkably successful in training analog-valued artificial neural networks (ANNs). Such training techniques, however, do not transfer easily to spiking networks due to the spike generation hard nonlinearity and the discrete nature of spike communication. We show that in a feedforward spiking network that uses a temporal coding scheme where information is encoded in spike times instead of spike rates, the network input-output relation is differentiable almost everywhere. Moreover, this relation is piecewise linear after a transformation of variables. Methods for training ANNs thus carry directly to the training of such spiking networks as we show when training on the permutation invariant MNIST task. In contrast to rate-based spiking networks that are often used to approximate the behavior of ANNs, the networks we present spike much more sparsely and their behavior cannot be directly approximated by conventional ANNs. Our results highlight a new approach for controlling the behavior of spiking networks with realistic temporal dynamics, opening up the potential for using these networks to process spike patterns with complex temporal information.
C1 [Mostafa, Hesham] Univ Calif San Diego, Inst Neural Computat, Jacobs Sch Engn, Dept Bioengn, La Jolla, CA 92093 USA.
RP Mostafa, H (corresponding author), Univ Calif San Diego, Inst Neural Computat, Jacobs Sch Engn, Dept Bioengn, La Jolla, CA 92093 USA.
EM hmmostafa@ucsd.edu
CR [Anonymous], 2012, THEANO NEW FEATURES
   Ba L. J., 2014, ADV NEURAL INFORM PR, V2, P2654
   Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Bergstra J, 2010, P PYTH SCI COMP C SC, V4, P3
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Diehl P.U., 2015, 2015 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2015.7280696
   Fabre-Thorpe M, 1998, NEUROREPORT, V9, P303, DOI 10.1097/00001756-199801260-00023
   Gardner B, 2015, NEURAL COMPUT, V27, P2548, DOI 10.1162/NECO_a_00790
   Goodfellow I. J., 2013, P 30 INT C MACHINE L, P1319, DOI DOI 10.5555/3042817.3043084
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Hung CP, 2005, SCIENCE, V310, P863, DOI 10.1126/science.1117593
   Kingma DP., 2017, ARXIV
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508
   Martens J., 2010, P 27 INT C MACH LEA, P735
   Memmesheimer RM, 2014, NEURON, V82, P925, DOI 10.1016/j.neuron.2014.03.026
   MINSKY M, 1961, P IRE, V49, P8, DOI 10.1109/JRPROC.1961.287775
   Mozer M., 2015, COMPLEX SYST, V3, P349
   Nair V., 2010, ICML, P807
   Neil D, 2014, IEEE T VLSI SYST, V22, P2621, DOI 10.1109/TVLSI.2013.2294916
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   Pfister J. - P., 1986, NEURAL COMPUT, V18, P1309
   Qiao N, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00141
   Rumelhart D.E., 1986, PARALLEL DISTRIBUTED, V1, DOI [10.21236/ada164453, 10.1016/b978-1-4832-1446-7.50035-2]
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Sutskever Ilya, 2013, INT C MACH LEARN, P1139
   Thorpe S. J., 2016, NEURAL NETWORKS, V14, P715
NR 31
TC 126
Z9 126
U1 3
U2 9
PD JUL
PY 2018
VL 29
IS 7
BP 3227
EP 3235
DI 10.1109/TNNLS.2017.2726060
UT WOS:000436420400045
DA 2023-11-16
ER

PT C
AU Kim, J
   Kim, DS
AF Kim, Jeongho
   Kim, Dae-Shik
BE Huang, T
   Lv, J
   Sun, C
   Tuzikov, AV
TI Competitive Hyperparameter Balancing on Spiking Neural Network for a
   Fast, Accurate and Energy-Efficient Inference
SO ADVANCES IN NEURAL NETWORKS - ISNN 2018
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 15th International Symposium on Neural Networks (ISNN)
CY JUN 25-28, 2018
CL Minsk, BELGIUM
DE Spiking Neural Network; Spiking neuron; Evolutionary algorithm;
   Tournament Selection; Hyperparameter
AB The Spiking Neural Network (SNN) is currently considered as a next generation neural network model. However, its performance often lags that of classical Artificial Neural Networks. Although there has been a wide range of research to improve the accuracy of SNNs, their performance is determined not only by accuracy, but also by speed and energy efficiency. In this study, we analyzed the relationship between hyperparameters, accuracy, speed and energy of SNN, set a new criterion to estimate the comprehensive performance and applied the Neuroevolutionary algorithm to balance the hyperparameters without the need for manually setting them. The optimized model showed better performance in all terms of our criteria.
C1 [Kim, Jeongho; Kim, Dae-Shik] Korea Adv Inst Sci & Technol, Sch Elect Engn, 291 Daehak Ro, Daejeon, South Korea.
RP Kim, DS (corresponding author), Korea Adv Inst Sci & Technol, Sch Elect Engn, 291 Daehak Ro, Daejeon, South Korea.
EM causaljh@kaist.ac.kr; daeshik@kaist.ac.kr
CR [Anonymous], 2017, P INT C LEARN REPR T
   [Anonymous], 2016, ARXIV161101421
   [Anonymous], 2017, P 34 INT C MACH LEAR
   [Anonymous], 2016, 2016 IEE INT C REB C, DOI [10.1109/ICRC.2016.7738691, DOI 10.1109/ICRC.2016.7738691]
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Diehl P.U., 2015, 2015 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2015.7280696
   LeCun Y., 2010, MNIST HANDWRITTEN DI
   Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Robertson S., 2017, PYTORCH TUTORIAL
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Wiklendt L, 2009, NEURAL NETW WORLD, V19, P235
NR 13
TC 1
Z9 1
U1 0
U2 3
PY 2018
VL 10878
BP 44
EP 53
DI 10.1007/978-3-319-92537-0_6
UT WOS:000460424800079
DA 2023-11-16
ER

PT J
AU Yusoff, N
   Ahmad, FK
   ChePa, N
   Ab Aziz, A
AF Yusoff, N.
   Ahmad, F. Kabir
   ChePa, N.
   Ab Aziz, A.
TI LEARNING STIMULUS-STIMULUS ASSOCIATION IN SPATIOTEMPORAL NEURAL NETWORKS
SO JURNAL TEKNOLOGI
DT Article
DE Associative learning; stimulus-stimulus association; spatio-temporal
   neural networks; spike-timing dependent plasticity
AB We propose a stimulus-stimulus association learning by coupling firing rate and precise spike timing encoding for spatio-temporal neural networks. We simulate a generic recurrent network with random and sparse connectivity consisting of Izhikevich spiking neurons. The magnitude of weight adjustment in learning is dependent on pre-and postsynaptic spikes based on their spikes count and time correlation. As a result of learning, synchronisation of activity among inter-and intra-subpopulation neurons demonstrates association between two stimuli. The associations show in spill-over of activity between the two stimuli involved.
C1 [Yusoff, N.; Ahmad, F. Kabir; ChePa, N.; Ab Aziz, A.] Univ Utara Malaysia, Coll Arts & Sci, Sch Comp, Sintok 06010, Kedah, Malaysia.
RP Yusoff, N (corresponding author), Univ Utara Malaysia, Coll Arts & Sci, Sch Comp, Sintok 06010, Kedah, Malaysia.
EM nooraini@uum.edu.my
CR [Anonymous], 1991, ANATOMY CORTEX
   [Anonymous], 2001, BRAIN MIND BEHAV
   [Anonymous], 1991, CORTICONICS
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Brunel N, 2001, J COMPUT NEUROSCI, V11, P63, DOI 10.1023/A:1011204814320
   Brunel N, 2009, J COGNITIVE NEUROSCI, V21, P2300, DOI 10.1162/jocn.2008.21156
   CRICK F, 1989, NATURE, V337, P129, DOI 10.1038/337129a0
   Dayan P, 2005, THEORETICAL NEUROSCI
   Dominguez M, 2006, NEURAL COMPUT, V18, P2942, DOI 10.1162/neco.2006.18.12.2942
   ECKHORN R, 1988, BIOL CYBERN, V60, P121, DOI 10.1007/BF00202899
   Erickson CA, 1999, J NEUROSCI, V19, P10404
   Filippova MG, 2011, SPAN J PSYCHOL, V14, P20, DOI 10.5209/rev_SJOP.2011.v14.n1.2
   FUSTER JM, 1981, SCIENCE, V212, P952, DOI 10.1126/science.7233192
   Gerstner W., 2002, SPIKING NEURON MODEL
   Glackin C, 2008, LECT NOTES COMPUT SC, V5164, P258, DOI 10.1007/978-3-540-87559-8_27
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   Iakymchuk T, 2015, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-015-0059-4
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   KUBOTA K, 1971, J NEUROPHYSIOL, V34, P337, DOI 10.1152/jn.1971.34.3.337
   Litt RA, 2014, J MEM LANG, V71, P71, DOI 10.1016/j.jml.2013.10.005
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   MIYASHITA Y, 1988, NATURE, V331, P68, DOI 10.1038/331068a0
   Mongillo G, 2003, EUR J NEUROSCI, V18, P2011, DOI 10.1046/j.1460-9568.2003.02908.x
   Naya Y, 2003, J NEUROSCI, V23, P2861
   Paugam-Moisy H, 2008, NEUROCOMPUTING, V71, P1143, DOI 10.1016/j.neucom.2007.12.027
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Purves D., 2018, NEUROSCIENCE
   SAKAI K, 1991, NATURE, V354, P152, DOI 10.1038/354152a0
   SCHACTER DL, 1992, J COGNITIVE NEUROSCI, V4, P244, DOI 10.1162/jocn.1992.4.3.244
   Swiercz W, 2006, IEEE T NEURAL NETWOR, V17, P94, DOI 10.1109/TNN.2005.860834
   TULVING E, 1982, J EXP PSYCHOL LEARN, V8, P336, DOI 10.1037/0278-7393.8.4.336
   VanRullen R, 2005, TRENDS NEUROSCI, V28, P1, DOI 10.1016/j.tins.2004.10.010
   Wang JL, 2014, NEUROCOMPUTING, V144, P526, DOI 10.1016/j.neucom.2014.04.017
   WILSON FAW, 1993, SCIENCE, V260, P1955, DOI 10.1126/science.8316836
   ZIPSER D, 1988, NATURE, V331, P679, DOI 10.1038/331679a0
NR 37
TC 0
Z9 0
U1 0
U2 0
PY 2015
VL 77
IS 5
BP 101
EP 112
UT WOS:000218610200015
DA 2023-11-16
ER

PT C
AU Hazan, H
   Saunders, D
   Sanghavi, DT
   Siegelmann, H
   Kozma, R
AF Hazan, Hananel
   Saunders, Daniel
   Sanghavi, Darpan T.
   Siegelmann, Hava
   Kozma, Robert
GP IEEE
TI Unsupervised Learning with Sel-Organizing Spiking Neural Networks
SO 2018 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUL 08-13, 2018
CL Rio de Janeiro, BRAZIL
DE spiking neural networks; self-organizing map; clustering; classification
AB We present a system comprising a hybridization of self-organized map (SOM) properties with spiking neural networks (SNNs) that retain many of the features of SOMs. Networks are trained in an unsupervised manner to learn a self-organized lattice of filters via excitatory-inhibitory interactions among populations of neurons. We develop and test various inhibition strategies, such as growing with inter-neuron distance and two distinct levels of inhibition. The quality of the unsupervised learning algorithm is evaluated using examples with known labels. Several biologically-inspired classification tools are proposed and compared, including population-level confidence rating, and n-grams using spike motif algorithm. Using the optimal choice of parameters, our approach produces improvements over state-of-art spiking neural networks.
C1 [Hazan, Hananel; Saunders, Daniel; Sanghavi, Darpan T.; Siegelmann, Hava; Kozma, Robert] Univ Massachusetts, Coll Informat & Comp Sci, 140 Governors Dr, Amherst, MA 01003 USA.
RP Hazan, H (corresponding author), Univ Massachusetts, Coll Informat & Comp Sci, 140 Governors Dr, Amherst, MA 01003 USA.
EM hhazan@cs.umass.edu; djsaunde@cs.umass.edu; dsanghavi@cs.umass.edu;
   hava@cs.umass.edu; rkozma@cs.umass.edu
CR [Anonymous], 1974, NEW TOOLS PREDICTION
   [Anonymous], 2016, NAT METHODS, DOI DOI 10.1038/nmeth.3707
   [Anonymous], 2001, SPRINGER SERIES INFO
   Beyeler M., 2015, 2015 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2015.7280424
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Diehl P., 2015, FRONTIERS COMPUTATIO
   ERWIN E, 1995, NEURAL COMPUT, V7, P425, DOI 10.1162/neco.1995.7.3.425
   Gerstner W., 2002, SPIKING NEURON MODEL
   Gollisch T, 2008, SCIENCE, V319, P1108, DOI 10.1126/science.1149639
   Goodman D. F. M., 2009, FRONTIERS COMPUTATIO
   Kermany E, 2010, J NEUROSCI, V30, P9588, DOI 10.1523/JNEUROSCI.0661-10.2010
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Marc-Oliver G., 2007, SCHOLARPEDIA, DOI [DOI 10.4249/SCHOLARPEDIA.1430, 10.4249/scholarpedia.1430]
   Petersen RS, 2001, NEURON, V32, P503, DOI 10.1016/S0896-6273(01)00481-0
   Raichman N, 2008, J NEUROSCI METH, V170, P96, DOI 10.1016/j.jneumeth.2007.12.020
   Rumelhart D.E., 1987, LEARNING INTERNAL RE, P318
   Saunders D. J., 2018, IEEE INNS IJCNN2018
NR 17
TC 6
Z9 6
U1 0
U2 1
PY 2018
BP 493
EP 498
UT WOS:000585967400067
DA 2023-11-16
ER

PT C
AU Liu, JX
   Huo, H
   Hu, WT
   Fang, T
AF Liu, Jiaxing
   Huo, Hong
   Hu, Weitai
   Fang, Tao
GP ACM
TI Brain-inspired hierarchical spiking neural network using unsupervised
   STDP rule for image classification
SO PROCEEDINGS OF 2018 10TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING
   AND COMPUTING (ICMLC 2018)
DT Proceedings Paper
CT 10th International Conference on Machine Learning and Computing (ICMLC)
CY FEB 26-28, 2018
CL Univ Macau, Zhuhai, PEOPLES R CHINA
HO Univ Macau
DE Hierarchical spiking neural network; brain-inspired; unsupervised
   learning; STDP rule; image classification
ID TIMING-DEPENDENT PLASTICITY; OBJECT; FEATURES
AB The human brain has powerful unsupervised learning ability without needs of large amounts of labeled samples, so brain-inspired neural networks have gained great attention recently. In this paper, a hierarchical spiking neural network (HSNN) is proposed to simulate the visual information processing of the ventral pathway in human brain. In HSNN, signals are transmitted as spike sequences instead of numerical data in traditional neural networks. It has six layers and each layer is composed of spike neurons simulated by leaky-integrate-and-fire neuron model. Some biological mechanisms, such as lateral inhibitory, homeostasis (adaptive threshold) and refractory periods, are incorporated in the proposed HSNN. Six layers include one encoding layer, three convolutional layers, and two max pooling layers, through which complex features with shift invariance can be extracted. The unsupervised spike timing-dependent plasticity (STDP) rule is utilized to train HSNN. The classification experimental results on CIFAR and MNIST datasets have shown that the proposed HSNN outperforms the existing unsupervised spiking network models, which testifies neural mechanisms in the biological visual system are potential and powerful ways to improve the performance of SNNs.
C1 [Liu, Jiaxing] Shanghai Jiao Tong Univ, Dept Automat, Shanghai 200240, Peoples R China.
   [Liu, Jiaxing] Minist Educ, Key Lab Syst Control & Informat Proc, Shanghai 200240, Peoples R China.
RP Liu, JX (corresponding author), Shanghai Jiao Tong Univ, Dept Automat, Shanghai 200240, Peoples R China.; Liu, JX (corresponding author), Minist Educ, Key Lab Syst Control & Informat Proc, Shanghai 200240, Peoples R China.
EM liujiaxing@sjtu.edu.cn
CR Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Caporale N, 2008, ANNU REV NEUROSCI, V31, P25, DOI 10.1146/annurev.neuro.31.060407.125639
   DiCarlo JJ, 2007, TRENDS COGN SCI, V11, P333, DOI 10.1016/j.tics.2007.06.010
   DiCarlo JJ, 2012, NEURON, V73, P415, DOI 10.1016/j.neuron.2012.01.010
   Diehl P.U., 2015, 2015 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2015.7280696
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Huang SY, 2014, J NEUROSCI, V34, P7575, DOI 10.1523/JNEUROSCI.0983-14.2014
   Kheradpisheh SR, 2016, NEUROCOMPUTING, V205, P382, DOI 10.1016/j.neucom.2016.04.029
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
   Krizhevsky Alex, 2009, LEARNING MULTIPLE LA, V3, P6
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Morrison A, 2007, NEURAL COMPUT, V19, P1437, DOI 10.1162/neco.2007.19.6.1437
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   Querlioz D, 2013, IEEE T NANOTECHNOL, V12, P288, DOI 10.1109/TNANO.2013.2250995
   Riesenhuber M, 1999, NAT NEUROSCI, V2, P1019, DOI 10.1038/14819
   Tal D, 1997, NEURAL COMPUT, V9, P305, DOI 10.1162/neco.1997.9.2.305
   Yu AJ, 2002, NEURAL COMPUT, V14, P2857, DOI 10.1162/089976602760805313
   Zhao B, 2015, IEEE T NEUR NET LEAR, V26, P1963, DOI 10.1109/TNNLS.2014.2362542
NR 18
TC 5
Z9 6
U1 1
U2 9
PY 2018
BP 230
EP 235
DI 10.1145/3195106.3195115
UT WOS:000458148400043
DA 2023-11-16
ER

PT J
AU Wang, B
   Chen, LL
   Zhang, ZY
AF Wang, B.
   Chen, L. L.
   Zhang, Z. Y.
TI A novel method on the edge detection of infrared image
SO OPTIK
DT Article
DE Edge detection; Infrared image; Spiking neural network
ID SPIKING NEURAL-NETWORK
AB Infrared image processing is important for fault identification of high-voltage equipment. This paper studies the problem on the edge detection of infrared image. First a kind of spiking neural network is constructed, and by using the characteristics of the spiking neuron, a novel method is designed to achieve the edge detection of infrared image. Finally, some typical examples are included and corresponding experimental results show the effectiveness and advantage of the proposed method.
C1 [Wang, B.; Chen, L. L.; Zhang, Z. Y.] Xihua Univ, Sch Elect & Informat Engn, Chengdu 610096, Sichuan, Peoples R China.
   [Wang, B.] Univ Elect Sci & Technol China, Sch Appl Math, Chengdu 610054, Sichuan, Peoples R China.
RP Wang, B (corresponding author), Xihua Univ, Sch Elect & Informat Engn, Chengdu 610096, Sichuan, Peoples R China.
EM 3227594837@qq.com
CR Awadalla MHA, 2012, ALEX ENG J, V51, P27, DOI 10.1016/j.aej.2012.07.004
   Bellotto N, 2009, IEEE T SYST MAN CY B, V39, P167, DOI 10.1109/TSMCB.2008.2004050
   Biswas S, 2018, OPTIK, V168, P931, DOI 10.1016/j.ijleo.2018.05.011
   Chen WH, 2014, OPT LASER ENG, V55, P69, DOI 10.1016/j.optlaseng.2013.10.025
   Lin ZT, 2018, NEUROCOMPUTING, V275, P94, DOI 10.1016/j.neucom.2017.05.009
   Liu DQ, 2017, NEUROCOMPUTING, V249, P212, DOI 10.1016/j.neucom.2017.04.003
   Mattia M, 2000, NEURAL COMPUT, V12, P2305, DOI 10.1162/089976600300014953
   Miró-Amarante L, 2017, NEUROCOMPUTING, V226, P249, DOI 10.1016/j.neucom.2016.12.005
   Pham D. T., 2006, INTELLIGENT PRODUCTI, P319
   Wang B, 2019, OPTIK, V178, P1264, DOI 10.1016/j.ijleo.2018.09.165
   Wang B, 2018, OPTIK, V172, P873, DOI 10.1016/j.ijleo.2018.07.075
   Wang B, 2018, OPTIK, V163, P81, DOI 10.1016/j.ijleo.2018.02.062
   Wang B, 2018, ADV DIFFER EQU-NY, DOI 10.1186/s13662-018-1504-3
   Wang B, 2018, COMPLEXITY, DOI 10.1155/2018/3079108
   Wang B, 2019, P I MECH ENG I-J SYS, V233, P31, DOI 10.1177/0959651818770337
   Wang B, 2018, IEICE T FUND ELECTR, VE101A, P1132, DOI 10.1587/transfun.E101.A.1132
   Wang B, 2018, NONLINEAR DYNAM, V93, P2461, DOI 10.1007/s11071-018-4336-5
   Yuan S, 2018, OPT COMMUN, V410, P350, DOI 10.1016/j.optcom.2017.10.016
NR 18
TC 28
Z9 28
U1 1
U2 26
PY 2019
VL 180
BP 610
EP 614
DI 10.1016/j.ijleo.2018.11.113
UT WOS:000462810600073
DA 2023-11-16
ER

PT J
AU Touboul, JD
   Faugeras, OD
AF Touboul, Jonathan D.
   Faugeras, Olivier D.
TI A Markovian event-based framework for stochastic spiking neural networks
SO JOURNAL OF COMPUTATIONAL NEUROSCIENCE
DT Article
DE Stochastic network; Linear integrate-and-fire neurons; Event-based
   model; Event-based simulation
ID 1ST HITTING TIME; EXACT SIMULATION; FIRE MODELS; NEURONS
AB In spiking neural networks, the information is conveyed by the spike times, that depend on the intrinsic dynamics of each neuron, the input they receive and on the connections between neurons. In this article we study the Markovian nature of the sequence of spike times in stochastic neural networks, and in particular the ability to deduce from a spike train the next spike time, and therefore produce a description of the network activity only based on the spike times regardless of the membrane potential process. To study this question in a rigorous manner, we introduce and study an event-based description of networks of noisy integrate-and-fire neurons, i.e. that is based on the computation of the spike times. We show that the firing times of the neurons in the networks constitute a Markov chain, whose transition probability is related to the probability distribution of the interspike interval of the neurons in the network. In the cases where the Markovian model can be developed, the transition probability is explicitly derived in such classical cases of neural networks as the linear integrate-and-fire neuron models with excitatory and inhibitory interactions, for different types of synapses, possibly featuring noisy synaptic integration, transmission delays and absolute and relative refractory period. This covers most of the cases that have been investigated in the event-based description of spiking deterministic neural networks.
C1 [Touboul, Jonathan D.; Faugeras, Olivier D.] INRIA, NeuroMathComp Lab, Sophia Antipolis, France.
RP Touboul, JD (corresponding author), INRIA, NeuroMathComp Lab, Sophia Antipolis, France.
EM jonathan.touboul@sophia.inria.fr
CR [Anonymous], HDB BRAIN THEORY NEU
   [Anonymous], 1988, NONLINEAR STOCHASTIC
   [Anonymous], ADV NEURAL INFORM PR
   [Anonymous], THESIS G AUGUST U
   [Anonymous], 1962, J MATH KYOTO U, DOI DOI 10.1215/KJM/1250524936
   [Anonymous], 1987, BROWNIAN MOTION STOC
   [Anonymous], THESIS ECOLE POLYTEC
   [Anonymous], MARKOV PROCESSES REL
   [Anonymous], P 3 WSEAS INT C NEUR
   Asmussen S, 1998, J APPL PROBAB, V35, P783, DOI 10.1017/S0021900200016491
   Brette R, 2006, NEURAL COMPUT, V18, P2004, DOI 10.1162/neco.2006.18.8.2004
   Brette R, 2007, J COMPUT NEUROSCI, V23, P349, DOI 10.1007/s10827-007-0038-6
   Brette R, 2007, NEURAL COMPUT, V19, P2604, DOI 10.1162/neco.2007.19.10.2604
   Brunel N, 1999, NEURAL COMPUT, V11, P1621, DOI 10.1162/089976699300016179
   Cessac B, 2008, J MATH BIOL, V56, P311, DOI 10.1007/s00285-007-0117-3
   Cessac B, 2011, J MATH BIOL, V62, P863, DOI 10.1007/s00285-010-0358-4
   Claverol ET, 2002, NEUROCOMPUTING, V47, P277, DOI 10.1016/S0925-2312(01)00629-4
   COTTRELL M, 1992, STOCH PROC APPL, V40, P103, DOI 10.1016/0304-4149(92)90140-L
   Cottrell M, 2000, J APPL PROBAB, V37, P168, DOI 10.1017/S0021900200015321
   DAVIS MHA, 1984, J ROY STAT SOC B MET, V46, P353
   Delorme A, 2003, NETWORK-COMP NEURAL, V14, P613, DOI 10.1088/0954-898X/14/4/301
   Delorme A, 2001, NEURAL NETWORKS, V14, P795, DOI 10.1016/S0893-6080(01)00049-1
   Fabre-Thorpe M, 1998, NEUROREPORT, V9, P303, DOI 10.1097/00001756-199801260-00023
   Fricker C, 1994, ANN APPL PROBAB, V4, P1112, DOI 10.1214/aoap/1177004906
   Gerstner W, 2002, BIOL CYBERN, V87, P404, DOI 10.1007/s00422-002-0353-y
   Gerstner W., 2002, SPIKING NEURON MODEL
   Gobet E, 2000, STOCH PROC APPL, V87, P167, DOI 10.1016/S0304-4149(99)00109-X
   GOLDMAN M, 1971, ANN MATH STAT, V42, P2150, DOI 10.1214/aoms/1177693084
   Gromoll HC, 2008, MATH OPER RES, V33, P375, DOI 10.1287/moor.1070.0298
   Holden A.V., 1976, Lecture Notes in Biomathematics, V12, P1
   Izhikevich EM, 2008, P NATL ACAD SCI USA, V105, P3593, DOI 10.1073/pnas.0712231105
   Kandel E. R., 2000, PRINCIPLES NEURAL SC, V4
   Kloeden PE, 2011, NUMERICAL SOLUTION S
   LACHAL A, 1991, ANN I H POINCARE-PR, V27, P385
   Lachal A, 1996, COMMUN PUR APPL MATH, V49, P1299, DOI 10.1002/(SICI)1097-0312(199612)49:12<1299::AID-CPA4>3.0.CO;2-5
   Makino T, 2003, NEURAL COMPUT APPL, V11, P210, DOI 10.1007/s00521-003-0358-z
   Ricciardi L.M., 1977, DIFFUSION PROCESSES
   Rolls E. T., 2010, NOISY BRAIN STOCHAST
   Roxin A, 2005, PHYS REV LETT, V94, DOI 10.1103/PhysRevLett.94.238103
   Rudolph M, 2006, NEURAL COMPUT, V18, P2146, DOI 10.1162/neco.2006.18.9.2146
   Shadlen Michael N., 1994, Current Opinion in Neurobiology, V4, P569, DOI 10.1016/0959-4388(94)90059-0
   SOFTKY WR, 1993, J NEUROSCI, V13, P334
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   Tonnelier A, 2007, NEURAL COMPUT, V19, P3226, DOI 10.1162/neco.2007.19.12.3226
   Touboul J, 2008, ADV APPL PROBAB, V40, P501, DOI 10.1239/aap/1214950214
   Touboul J, 2007, J PHYSIOL-PARIS, V101, P78, DOI 10.1016/j.jphysparis.2007.10.008
   Turova T, 2000, BIOSYSTEMS, V58, P159, DOI 10.1016/S0303-2647(00)00119-2
NR 47
TC 6
Z9 6
U1 0
U2 9
PD NOV
PY 2011
VL 31
IS 3
BP 485
EP 507
DI 10.1007/s10827-011-0327-y
UT WOS:000297820900002
DA 2023-11-16
ER

PT C
AU Shen, JR
   Lin, K
   Wang, YM
   Pan, G
AF Shen, Jiangrong
   Lin, Kang
   Wang, Yueming
   Pan, Gang
GP IEEE
TI Character Recognition from Trajectory by Recurrent Spiking Neural
   Networks
SO 2017 39TH ANNUAL INTERNATIONAL CONFERENCE OF THE IEEE ENGINEERING IN
   MEDICINE AND BIOLOGY SOCIETY (EMBC)
SE IEEE Engineering in Medicine and Biology Society Conference Proceedings
DT Proceedings Paper
CT 39th Annual International Conference of the
   IEEE-Engineering-in-Medicine-and-Biology-Society (EMBC)
CY JUL 11-15, 2017
CL SOUTH KOREA
ID NEURONS
AB Spiking neural networks are biologically plausible and power-efficient on neuromorphic hardware, while recurrent neural networks have been proven to be efficient on time series data. However, how to use the recurrent property to improve the performance of spiking neural networks is still a problem. This paper proposes a recurrent spiking neural network for character recognition using trajectories. In the network, a new encoding method is designed, in which varying time ranges of input streams are used in different recurrent layers. This is able to improve the generalization ability of our model compared with general encoding methods. The experiments are conducted on four groups of the character data set from University of Edinburgh. The results show that our method can achieve a higher average recognition accuracy than existing methods.
C1 [Pan, Gang] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310027, Zhejiang, Peoples R China.
   [Shen, Jiangrong; Lin, Kang; Wang, Yueming] Zhejiang Univ, Qiushi Acad Adv Studies, Hangzhou 310027, Zhejiang, Peoples R China.
RP Wang, YM (corresponding author), Zhejiang Univ, Qiushi Acad Adv Studies, Hangzhou 310027, Zhejiang, Peoples R China.
EM qaas@zju.edu.cn
CR [Anonymous], 2014, ARXIV14021128CSSTAT
   [Anonymous], 2016, ARXIV161105141
   [Anonymous], 2011, ADV NEURAL INF PROCE
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Gardner B, 2015, NEURAL COMPUT, V27, P2548, DOI 10.1162/NECO_a_00790
   Gerstner W., 2002, SPIKING NEURON MODEL
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Kuroe Y., 2010, 2010 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2010.5596914
   McKennoch S, 2006, IEEE IJCNN, P3970
   Opper Manfred, 2001, NEURAL INFORM PROCES
   Ponulak F., 2005, RESUME NEW SUPERVISE
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Rezende Danilo J, 2011, ADV NEURAL INFORM PR, P136
   Tino P, 2006, NEURAL COMPUT, V18, P591, DOI 10.1162/089976606775623360
   Williams Ben H, UCI MACHINE LEARNING
   Yu QF, 2013, PLOS ONE, V8, DOI [10.1371/journal.pone.0059627, 10.1371/journal.pone.0078318]
NR 16
TC 6
Z9 6
U1 0
U2 2
PY 2017
BP 2900
EP 2903
UT WOS:000427085303084
DA 2023-11-16
ER

PT J
AU OGHALAI, JS
   STREET, WN
   RHODE, WS
AF OGHALAI, JS
   STREET, WN
   RHODE, WS
TI A NEURAL-NETWORK-BASED SPIKE DISCRIMINATOR
SO JOURNAL OF NEUROSCIENCE METHODS
DT Article
DE MULTIUNIT SPIKE TRAIN; SPIKE DISCRIMINATION; EXTRACELLULAR RECORDING;
   COCHLEAR NUCLEUS; NEURAL NETWORK; ART-2
ID DORSAL COCHLEAR NUCLEUS; UNSUPERVISED WAVEFORM CLASSIFICATION;
   SOFTWARE-BASED SYSTEM; NEUROELECTRIC DATA; AUDITORY MIDBRAIN; REAL-TIME;
   RECORDINGS; IMPLEMENTATION; RECOGNITION; PATTERNS
AB A software routine to reconstruct individual spike trains from multi-neuron, single-channel extracellular recordings was designed. Using a neural network algorithm that automatically clusters and sorts the spikes, the only user input needed is the threshold level for spike detection and the number of unit types present in the recording. Adaptive features are included in the algorithm to allow for tracking of spike trains during periods of amplitude variation and also to identify noise spikes. The routine will operate on-line during extracellular studies of the cochlear nucleus in cats.
C1 UNIV WISCONSIN, DEPT COMP SCI, MADISON, WI 53706 USA.
RP OGHALAI, JS (corresponding author), UNIV WISCONSIN, SCH MED, DEPT NEUROPHYSIOL, MADISON, WI 53706 USA.
CR ABELES M, 1977, P IEEE, V65, P762, DOI 10.1109/PROC.1977.10559
   AERTSEN AMHJ, 1989, J NEUROPHYSIOL, V61, P900, DOI 10.1152/jn.1989.61.5.900
   [Anonymous], 1986, PARALLEL DISTRIBUTED
   BERGMAN H, 1992, J NEUROSCI METH, V41, P187, DOI 10.1016/0165-0270(92)90084-Q
   CARPENTER GA, 1988, COMPUTER, V21, P77, DOI 10.1109/2.33
   CARPENTER GA, 1991, NEURAL NETWORKS, V4, P493, DOI 10.1016/0893-6080(91)90045-7
   CARPENTER GA, 1987, APPL OPTICS, V26, P4919, DOI 10.1364/AO.26.004919
   EGGERMONT JJ, 1991, J NEUROPHYSIOL, V66, P1549, DOI 10.1152/jn.1991.66.5.1549
   EPPING WJM, 1987, J NEUROPHYSIOL, V57, P1464, DOI 10.1152/jn.1987.57.5.1464
   GOCHIN PM, 1990, BRAIN RES, V510, P195, DOI 10.1016/0006-8993(90)91367-P
   GOCHIN PM, 1989, BRAIN RES, V497, P1, DOI 10.1016/0006-8993(89)90963-3
   JANSEN RF, 1992, J NEUROSCI METH, V41, P123, DOI 10.1016/0165-0270(92)90055-I
   JANSEN RF, 1990, J NEUROSCI METH, V35, P203, DOI 10.1016/0165-0270(90)90125-Y
   PERKEL DH, 1967, BIOPHYS J, V7, P419, DOI 10.1016/S0006-3495(67)86597-4
   PERKEL DH, 1967, BIOPHYS J, V7, P391, DOI 10.1016/S0006-3495(67)86596-2
   PERKEL DH, 1975, BRAIN RES, V100, P271, DOI 10.1016/0006-8993(75)90483-7
   REINIS S, 1992, J NEUROSCI METH, V43, P1, DOI 10.1016/0165-0270(92)90061-H
   SALGANICOFF M, 1988, J NEUROSCI METH, V25, P181, DOI 10.1016/0165-0270(88)90132-X
   SARNA MF, 1988, J NEUROSCI METH, V25, P189, DOI 10.1016/0165-0270(88)90133-1
   SCHMIDT EM, 1984, J NEUROSCI METH, V12, P1, DOI 10.1016/0165-0270(84)90042-6
   SCHMIDT EM, 1984, J NEUROSCI METH, V12, P95, DOI 10.1016/0165-0270(84)90009-8
   VOIGT HF, 1980, J NEUROPHYSIOL, V44, P76, DOI 10.1152/jn.1980.44.1.76
   WHEELER BC, 1982, IEEE T BIO-MED ENG, V29, P752, DOI 10.1109/TBME.1982.324870
   YAMADA S, 1992, J NEUROSCI METH, V43, P23, DOI 10.1016/0165-0270(92)90063-J
NR 24
TC 14
Z9 14
U1 0
U2 1
PD SEP
PY 1994
VL 54
IS 1
BP 9
EP 22
DI 10.1016/0165-0270(94)90155-4
UT WOS:A1994PH09600002
DA 2023-11-16
ER

PT C
AU Clogenson, M
   Kerr, D
   McGinnity, M
   Coleman, S
   Wu, QX
AF Clogenson, Marine
   Kerr, Dermot
   McGinnity, Martin
   Coleman, Sonya
   Wu, Qingxiang
BE Madani, K
   Kacprzyk, J
   Filipe, J
TI BIOLOGICALLY INSPIRED EDGE DETECTION USING SPIKING NEURAL NETWORKS AND
   HEXAGONAL IMAGES
SO NCTA 2011: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON NEURAL
   COMPUTATION THEORY AND APPLICATIONS
DT Proceedings Paper
CT International Conference on Neural Computation Theory and Applications
CY OCT 24-26, 2011
CL Univ Paris Est Creteil, Paris, FRANCE
HO Univ Paris Est Creteil
DE Spiking neural network; Edge detection; Multi-scale hexagonal receptive
   fields
ID NEURONS
AB Inspired by the structure and behaviour of the human visual system, we extend existing work using spiking neural networks for edge detection with a biologically plausible hexagonal pixel arrangement. Standard digital images are converted into a hexagonal pixel representation before being processed with a spiking neural network with scalable hexagonally shaped receptive fields. The performance is compared with different sized receptive fields implemented on standard rectangular images. Results illustrate that using hexagonal-shaped receptive fields provides improved performance over a range of scales compared with standard rectangular shaped receptive fields and images.
C1 [Clogenson, Marine] CPE Lyon, BP 82077, F-69616 Villeurbanne, France.
   [Kerr, Dermot; McGinnity, Martin; Coleman, Sonya; Wu, Qingxiang] Univ Ulster, Intelligent Syst Res Ctr, Magee BT48 7JL, Derry, North Ireland.
RP Clogenson, M (corresponding author), CPE Lyon, BP 82077, F-69616 Villeurbanne, France.
EM marine.clogenson@cpe.fr; d.kerr@ulster.ac.uk; tm.mcginnity@ulster.ac.uk;
   sa.coleman@ulster.ac.uk
CR ABDOU IE, 1979, P IEEE, V67, P753, DOI 10.1109/PROC.1979.11325
   Allen J. D, 2003, FILTER BANKS IMAGES
   Buhmann JM, 2005, NEURAL COMPUT, V17, P1010, DOI 10.1162/0899766053491913
   Chevallier S., 2009, P INT C NEUR COMP MA
   Chevallier S., 2006, P ESANN, P209
   Delorme A, 2003, NETWORK-COMP NEURAL, V14, P613, DOI 10.1088/0954-898X/14/4/301
   Egmont-Petersen M, 2002, PATTERN RECOGN, V35, P2279, DOI 10.1016/S0031-3203(01)00178-9
   Escobar MJ, 2009, INT J COMPUT VISION, V82, P284, DOI 10.1007/s11263-008-0201-1
   Gao Y., 2006, INT C SERIES, V1291, P229
   Hugues E, 2002, LECT NOTES COMPUT SC, V2525, P60
   Izhikevich E. M., 2004, IEEE T NN, V15
   Kunkle D.R., 2002, PULSED NEURAL NETWOR
   Lindeberg T., 2013, SCALE SPACE THEORY C, V256
   Masland RH, 2001, NAT NEUROSCI, V4, P877, DOI 10.1038/nn0901-877
   Meftah B, 2010, NEURAL PROCESS LETT, V32, P131, DOI 10.1007/s11063-010-9149-6
   Middleton L, 2001, IMAGE VISION COMPUT, V19, P1071, DOI 10.1016/S0262-8856(01)00067-1
   Paugam-Moisy H., 2009, HDB NATURAL COMPUTIN, V40
   Vitulli R, 2002, INT GEOSCI REMOTE SE, P979, DOI 10.1109/IGARSS.2002.1025749
   Wu QX, 2007, LECT NOTES ARTIF INT, V4682, P26
NR 19
TC 3
Z9 3
U1 0
U2 1
PY 2011
BP 381
EP 384
UT WOS:000392350600062
DA 2023-11-16
ER

PT J
AU Januario, FDP
   Carvalho, JRH
AF Pereira Januario, Francisco De Assis
   Hugh Carvalho, Jose Reginaldo
TI ReSNN-DCT: Methodology for Reduction of the Spiking Neural Network Using
   Discrete Cosine Transform and Elegant Pairing
SO IEEE ACCESS
DT Article
DE Neurons; Biological neural networks; Hardware; Biological system
   modeling; Mathematical models; Computational modeling; Field
   programmable gate arrays; Spiking neural network (SNN); Izhikevich
   model; discrete cosine transform (DCT); elegant pairing
AB In recent years, the use of artificial neural network applications to perform object classification and event prediction has increased, mainly from research about deep learning techniques running on hardware such as GPU and FPGA. The interest in the use of neural networks extends to embedded systems, due to the development of applications in smart mobile devices, such as cell phones, drones, autonomous cars and industrial robots. In this article, a methodology is proposed that allows to reduce a spiking neural network, applying the discrete cosine transform (DCT) and elegant pairing. The Izhikevich model was used as a basis for the architecture of the spiking neural network. The simulation results demonstrate the effectiveness of the methodology, showing the feasibility of reducing synapses, applying the DCT transform, and of reducing neurons from the intermediate layers, using the elegant pairing technique of the coefficients, and maintaining the accuracy of the response of the spiking neural network. The results also demonstrate the contribution of the proposed methodology to the scalability of the neural network, with the increase in the storage capacity of the coefficients of the SNN layers.
C1 [Pereira Januario, Francisco De Assis] Univ Fed Amazonas, Fac Technol FT, Dept Elect & Computat, Manaus, Amazonas, Brazil.
   [Hugh Carvalho, Jose Reginaldo] Fed Univ Amazonas UFAM, Comp Inst, Manaus, Amazonas, Brazil.
RP Januario, FDP (corresponding author), Univ Fed Amazonas, Fac Technol FT, Dept Elect & Computat, Manaus, Amazonas, Brazil.
CR Ahmed K., 2020, BIOMIMETICS, DOI [10.5772/INTECHOPEN.93435, DOI 10.5772/INTECHOPEN.93435]
   AHMED N, 1974, IEEE T COMPUT, VC 23, P90, DOI 10.1109/T-C.1974.223784
   Diniz P, DIGIT SIGNAL PROCESS, V2nd
   Garg I., 2020, ARXIV201001795
   Grout I, 2008, DIGITAL SYSTEMS DESIGN WITH FPGAS AND CPLDS, P1
   Han B, 2016, IEEE IJCNN, P971
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Lacey G., 2016, DEEP LEARNING FPGAS
   Lapique L., 1907, J PHYSL PATHOL GEN, V9, P620, DOI DOI 10.1007/S00422-007-0189-6
   Li SJ, 2011, IEEE IMAGE PROC, P1537, DOI 10.1109/ICIP.2011.6115738
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Mallorqui E.-G, 2017, DIGITAL SYSTEM SPIKI
   Ong S, 2017, SIGNAL PROCESS-IMAGE, V58, P1, DOI 10.1016/j.image.2017.06.002
   Reddaiah B, 2016, INT J COMPUT APPL, V149, P4
   REGAN KW, 1992, J COMPUT SYST SCI, V45, P285, DOI 10.1016/0022-0000(92)90027-G
   Singh N, 2014, TRAINING ALGORITHMS
   Solis-Rosas A., 2019, INT J COMPUT SCI SOF, V8, P2409
   Soniya, 2015, 2015 IEEE WORKSHOP C, P1
   Szudzik M., 2006, SPECIAL NKS 2006 WOL, P1
   Szudzik M. P, 2019, ROSENBERG STRONG PAI
   Wang C, 2017, IEEE T COMPUT AID D, V36, P513, DOI 10.1109/TCAD.2016.2587683
   Wang Y, 2016, IEEE INT SYMP CIRC S, P129, DOI 10.1109/ISCAS.2016.7527187
   Wu QX, 2009, LECT NOTES ARTIF INT, V5755, P21
   Yin S, 2017, PROC IEEE BIOMED CIR, P1
   Yu Q, 2015, IEEE ACM INT SYMP, P1159, DOI 10.1109/CCGrid.2015.114
   Zhang J, 2016, 2016 2ND INTERNATIONAL CONFERENCE ON INDUSTRIAL INFORMATICS - COMPUTING TECHNOLOGY, INTELLIGENT TECHNOLOGY, INDUSTRIAL INFORMATION INTEGRATION (ICIICII), P136, DOI [10.1109/ICIICII.2016.53, 10.1109/ICIICII.2016.0043]
NR 27
TC 0
Z9 0
U1 1
U2 6
PY 2022
VL 10
BP 64504
EP 64515
DI 10.1109/ACCESS.2022.3182719
UT WOS:000814541200001
DA 2023-11-16
ER

PT J
AU Mou, HM
   Luo, ZC
   Zhang, XZ
AF Mou, Hongming
   Luo, Zhaochu
   Zhang, Xiaozhong
TI A magnetic-field-driven neuristor for spiking neural networks
SO APPLIED PHYSICS LETTERS
DT Article
AB Artificial intelligence has been widely deployed in many fields with remarkable success. Among various artificial neural network structures in artificial intelligence, the spiking neural network, as the next-generation artificial neural network, closely mimics the natural neural networks. It contains the all-or-nothing and diverse periodic spiking, which is an analogy to the behavior of natural neurons. Artificial devices that perform the function of neurons are called neuristors. Most existing neuristors are driven by electrical signals, which suffer the problem of impedance mismatch between input and output neuristors. By exploiting the S-shape negative differential resistances element that is sensitive to the external magnetic field, we constructed a magnetic-field-driven neuristor. Magnetic fields can stimulate all-or nothing spiking, and its shape and frequency can be modulated through capacitances in the circuit. As magnetic fields serve as the information carrier, the cascading of our neuristors can get rid of the electrical impedance mismatch, promising a scalable hardware platform for spiking neural networks.
C1 [Mou, Hongming; Zhang, Xiaozhong] Tsinghua Univ, Sch Mat Sci & Engn, Key Lab Adv Mat MOE, Beijing, Peoples R China.
   [Luo, Zhaochu] Peking Univ, Sch Phys, State Key Lab Artificial Microstruct & Mesoscop Ph, Beijing, Peoples R China.
RP Zhang, XZ (corresponding author), Tsinghua Univ, Sch Mat Sci & Engn, Key Lab Adv Mat MOE, Beijing, Peoples R China.; Luo, ZC (corresponding author), Peking Univ, Sch Phys, State Key Lab Artificial Microstruct & Mesoscop Ph, Beijing, Peoples R China.
EM zhaochu.luo@pku.edu.cn; xzzhang@mail.tsinghua.edu.cn
CR Bahdanau D, 2016, Arxiv, DOI arXiv:1409.0473
   Bao L, 2019, ACS APPL MATER INTER, V11, P41482, DOI 10.1021/acsami.9b10072
   Bullock TH, 2005, SCIENCE, V310, P791, DOI 10.1126/science.1114394
   Cajal S.R., 1911, HISTOLOGIE SYSTEME N, V2, P891, DOI DOI 10.5962/BHL.TITLE.48637
   Duan QX, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-17215-3
   ESAKI L, 1958, PHYS REV, V109, P603, DOI 10.1103/PhysRev.109.603
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V116, P424, DOI 10.1113/jphysiol.1952.sp004716
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V116, P497, DOI 10.1113/jphysiol.1952.sp004719
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V116, P449, DOI 10.1113/jphysiol.1952.sp004717
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V116, P473, DOI 10.1113/jphysiol.1952.sp004718
   Jiang J, 2017, SMALL, V13, DOI 10.1002/smll.201700933
   Li XY, 2020, NAT NANOTECHNOL, V15, P776, DOI 10.1038/s41565-020-0722-5
   Luo ZC, 2017, ADV ELECTRON MATER, V3, DOI 10.1002/aelm.201700186
   NAGUMO J, 1962, P IRE, V50, P2061, DOI 10.1109/JRPROC.1962.288235
   NISHIZAWA JI, 1969, INT J ELECTRON, V26, P437, DOI 10.1080/00207216908938173
   Pickett MD, 2013, NAT MATER, V12, P114, DOI [10.1038/nmat3510, 10.1038/NMAT3510]
   Ramon Y Cajal S., 1888, Rev trimestral de histol normal y patolog Ano, VI, P1
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Tuma T, 2016, NAT NANOTECHNOL, V11, P693, DOI [10.1038/NNANO.2016.70, 10.1038/nnano.2016.70]
   Yi W, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-07052-w
   Yuan R, 2022, NAT COMMUN, V13, DOI 10.1038/s41467-022-31747-w
   Yuste R, 2015, FRONT NEUROANAT, V9, DOI 10.3389/fnana.2015.00018
   Zhang WQ, 2020, NAT ELECTRON, V3, P371, DOI 10.1038/s41928-020-0435-7
NR 27
TC 0
Z9 0
U1 6
U2 6
PD JUN 19
PY 2023
VL 122
IS 25
AR 250601
DI 10.1063/5.0158341
UT WOS:001018500000003
DA 2023-11-16
ER

PT C
AU Virgilio, GCD
   Sossa, H
   Antelis, JM
   Falcón, LE
AF Virgilio, Carlos D. G.
   Sossa, Humberto
   Antelis, Javier M.
   Falcon, Luis E.
BE CarrascoOchoa, JA
   MartinezTrinidad, JF
   OlveraLopez, JA
   Salas, J
TI Motor Imagery Task Classification in EEG Signals with Spiking Neural
   Network
SO PATTERN RECOGNITION, MCPR 2019
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 11th Mexican Conference on Pattern Recognition (MCPR)
CY JUN 26-29, 2019
CL Queretaro, MEXICO
DE EEG signals; Motor Imagery; Power Spectral Density; Wavelet
   Decomposition; Neural networks; Multi layer perceptron; Spiking Neural
   Network
ID MU
AB We report the development and evaluation of brain signal classifiers, specifically Spiking Neuron based classifiers. The proposal consists of two main stages: feature extraction and pattern classification. The EEG signals used represent four motor imagery tasks: Left Hand, Right Hand, Foot and Tongue movements. In addition, one more class was added: Rest. These EEG signals were obtained from a database provided by the Technological University of Graz. Feature extraction stage was carried out by applying two algorithms: Power Spectral Density and Wavelet Decomposition. The tested algorithms were: K-Nearest Neighbors, Multilayer Perceptron, Single Spiking Neuron and Spiking Neural Network. All of them were evaluated in the classification between two Motor Imagery tasks; all possible pairings were made with the 5 mental tasks (Rest, Left Hand, Right Hand, Tongue and Foot). In the end, a performance comparison was made between a Multilayer Perceptron and Spiking Neural Network.
C1 [Virgilio, Carlos D. G.; Sossa, Humberto] Inst Politecn Nacl, Ctr Invest Computac, Ave Juan de Dios Batiz & M Othon de Mendizabal, Mexico City 07738, DF, Mexico.
   [Sossa, Humberto; Antelis, Javier M.; Falcon, Luis E.] Tecnol Monterrey, Escuela Ingn & Ciencias, Ave Gen Ramon Corona 2514, Zapopan, Jalisco, Mexico.
RP Sossa, H (corresponding author), Inst Politecn Nacl, Ctr Invest Computac, Ave Juan de Dios Batiz & M Othon de Mendizabal, Mexico City 07738, DF, Mexico.; Sossa, H (corresponding author), Tecnol Monterrey, Escuela Ingn & Ciencias, Ave Gen Ramon Corona 2514, Zapopan, Jalisco, Mexico.
EM danielvg92@gmail.com; hsossa@cic.ipn.mx; mauricio.antelis@itesm.mx;
   luis.eduardo.falcon@tec.mx
CR Ahangi A, 2013, NEURAL COMPUT APPL, V23, P1319, DOI 10.1007/s00521-012-1074-3
   Antelis JM, 2018, BIOMED SIGNAL PROCES, V44, P12, DOI 10.1016/j.bspc.2018.03.010
   Asensio-Cubero J, 2013, BIOMED SIGNAL PROCES, V8, P772, DOI 10.1016/j.bspc.2013.07.004
   Belhadj S.A., 2016, EL ENG ICEE 2015 4 I, P3, DOI [10.1109/INTEE.2015.7416697, DOI 10.1109/INTEE.2015.7416697]
   Goutte C, 2005, LECT NOTES COMPUT SC, V3408, P345
   Han RX, 2013, APPL MECH MATER, V239-240, P974, DOI 10.4028/www.scientific.net/AMM.239-240.974
   Herman P, 2008, IEEE T NEUR SYS REH, V16, P317, DOI 10.1109/TNSRE.2008.926694
   Izhikevich E. M., 2007, DYNAMICAL SYSTEMS NE, DOI [10.1017/S0143385704000173, DOI 10.1017/S0143385704000173]
   Kennedy J, 1995, 1995 IEEE INTERNATIONAL CONFERENCE ON NEURAL NETWORKS PROCEEDINGS, VOLS 1-6, P1942, DOI 10.1109/icnn.1995.488968
   Ma YL, 2016, COMPUT MATH METHOD M, V2016, DOI 10.1155/2016/4941235
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   McFarland DJ, 2000, BRAIN TOPOGR, V12, P177, DOI 10.1023/A:1023437823106
   Mulder T, 2007, J NEURAL TRANSM, V114, P1265, DOI 10.1007/s00702-007-0763-z
   Pfurtscheller G, 2006, NEUROIMAGE, V31, P153, DOI 10.1016/j.neuroimage.2005.12.003
   Ponulak Filip, 2011, ALLEN 2011 INTRO SPI
   Gonzalez CDV, 2018, LECT NOTES ARTIF INT, V11288, P123, DOI 10.1007/978-3-030-04491-6_10
NR 16
TC 3
Z9 3
U1 1
U2 5
PY 2019
VL 11524
BP 14
EP 24
DI 10.1007/978-3-030-21077-9_2
UT WOS:000493816400002
DA 2023-11-16
ER

PT C
AU Luo, YL
   Lu, Q
   Liu, JX
   Fu, Q
   Harkin, J
   McDaid, L
   Martínez-Corral, J
   Biot-Marí, G
AF Luo, Yuling
   Lu, Qian
   Liu, Junxiu
   Fu, Qiang
   Harkin, Jim
   McDaid, Liam
   Martinez-Corral, Jordi
   Biot-Mari, Guillermo
GP Assoc Comp Machinery
TI Forest Fire Detection using Spiking Neural Networks
SO 2018 ACM INTERNATIONAL CONFERENCE ON COMPUTING FRONTIERS
DT Proceedings Paper
CT 15th ACM International Conference on Computing Frontiers
CY MAY 08-10, 2018
CL Ischia, ITALY
DE forest fire detection; spiking neural networks; detection model
ID MODEL
AB Forest fires is one of the main causes of environmental degradation and its detection and forecasting is challenging. A novel method of forest fire detection based on spiking neural networks is proposed in this paper. Data obtained from controlled experiments are used as input training samples and a detection model is established by considering the factors of temperature, humidity, carbon monoxide concentration, wind speed and wind direction. Experimental results show that the spiking neural network can achieve a detection accuracy of similar to 91%, and therefore provides a better power/accuracy trade-off against existing approaches.
C1 [Luo, Yuling; Lu, Qian; Liu, Junxiu; Fu, Qiang] Guangxi Normal Univ, Fac Elect Engn, Guilin 541004, Peoples R China.
   [Harkin, Jim; McDaid, Liam] Ulster Univ, Sch Comp Engn & Intelligent Syst, Derry BT48 7JL, Londonderry, North Ireland.
   [Martinez-Corral, Jordi; Biot-Mari, Guillermo] Senticnel, C Valencia 3, Valencia 46135, Spain.
RP Luo, YL (corresponding author), Guangxi Normal Univ, Fac Elect Engn, Guilin 541004, Peoples R China.
EM yuling0616@mailbox.gxnu.edu.cn; jg.harkin@ulster.ac.uk; jmc@ntforest.com
CR [Anonymous], 2010, P INT JOINT C NEUR N, DOI DOI 10.1109/IJCNN.2010.5596470
   [Anonymous], 2015, P INT JOINT C NEUR N
   Aslan YE, 2012, COMPUT ENVIRON URBAN, V36, P614, DOI 10.1016/j.compenvurbsys.2012.03.002
   Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Camuñas-Mesa LA, 2018, IEEE T NEUR NET LEAR, V29, P4223, DOI 10.1109/TNNLS.2017.2759326
   Chen YJ, 2008, IEEE IJCNN, P1615, DOI 10.1109/IJCNN.2008.4634013
   Chen Y, 2010, CONSUM COMM NETWORK, P1
   Chersi F, 2013, NEURAL NETWORKS, V41, P212, DOI 10.1016/j.neunet.2012.11.009
   Divya T. L., 2014, 2014 International Conference on Communications and Signal Processing (ICCSP), P312, DOI 10.1109/ICCSP.2014.6949852
   Habenschuss S, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003311
   Hu J, 2013, NEURAL COMPUT, V25, P450, DOI 10.1162/NECO_a_00395
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Kasabov Nikola, 2012, Artificial Neural Networks in Pattern Recognition. Proceedings of the 5th INNS IAPR TC 3 GIRPR Workshop, ANNPR 2012, P225, DOI 10.1007/978-3-642-33212-8_21
   Kasabov N, 2010, NEURAL NETWORKS, V23, P16, DOI 10.1016/j.neunet.2009.08.010
   Kasabov NK, 2014, NEURAL NETWORKS, V52, P62, DOI 10.1016/j.neunet.2014.01.006
   Kulkarni S, 2013, APPL SOFT COMPUT, V13, P3628, DOI 10.1016/j.asoc.2013.04.007
   [蔺想红 Lin Xianghong], 2015, [电子学报, Acta Electronica Sinica], V43, P577
   Liu Dan, 2016, STAT APPL, V5, P163, DOI DOI 10.12677/SA.2016.52016
   Liu JX, 2016, IEEE INT SYMP CIRC S, P1350, DOI 10.1109/ISCAS.2016.7527499
   Liu JX, 2015, IEEE INT SYMP CIRC S, P2700, DOI 10.1109/ISCAS.2015.7169243
   Marc-Oliver G., 2007, SCHOLARPEDIA, DOI [DOI 10.4249/SCHOLARPEDIA.1430, 10.4249/scholarpedia.1430]
   Oladunjoye M, 2015, INT JOINT C NEUR NET, P1
   Sakr G. E., 2010, 2010 IEEE/ASME International Conference on Advanced Intelligent Mechatronics (AIM 2010), P1311, DOI 10.1109/AIM.2010.5695809
   Schemmel J, 2006, IEEE IJCNN, P1
   Schliebs S, 2013, EVOL SYST-GER, V4, P87, DOI 10.1007/s12530-013-9074-9
   Serrano-Gotarredona T, 2013, IEEE CIRC SYST MAG, V13, P74, DOI 10.1109/MCAS.2013.2256271
   Szatmáry B, 2010, PLOS COMPUT BIOL, V6, DOI 10.1371/journal.pcbi.1000879
   Yu LY, 2005, I C WIREL COMM NETW, P1214
   Zhang JG, 2009, ICIEA: 2009 4TH IEEE CONFERENCE ON INDUSTRIAL ELECTRONICS AND APPLICATIONS, VOLS 1-6, P515
NR 31
TC 4
Z9 4
U1 0
U2 7
PY 2018
BP 371
EP 375
DI 10.1145/3203217.3203231
UT WOS:000455156500058
DA 2023-11-16
ER

PT C
AU Kiselev, MV
AF Kiselev, Mikhail V.
BE Cheng, L
   Liu, Q
   Ronzhin, A
TI Conversion from Rate Code to Temporal Code - Crucial Role of Inhibition
SO ADVANCES IN NEURAL NETWORKS - ISNN 2016
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 13th International Symposium on Neural Networks (ISNN)
CY JUL 06-08, 2016
CL Saint Petersburg, RUSSIA
DE Spiking neural network; Rate code; Temporal code; Polychronization;
   Chaotic neural network; Neural network self-organization
ID SPIKING NEURAL-NETWORKS
AB This study is an attempt to answer the question - what kind of spiking neural networks could efficiently transform rate-coded input signal into temporally coded form - specific activity of neuronal groups with strictly fixed temporal delays between spikes emitted by different neurons in every group. Since theoretical approach to the solution of this problem appears to be very hard or impossible the network configurations performing this task efficiently were found by means of genetic algorithm. Exploration of their structure showed that while excitatory neurons form the groups with stereotypical firing patterns, the inhibitory neurons of the network make these patterns specific for different rate-coded stimuli and, thus, play the key role in conversion of rate-coded input signal to temporal code.
C1 [Kiselev, Mikhail V.] Chuvash State Univ, Cheboksary, Russia.
RP Kiselev, MV (corresponding author), Chuvash State Univ, Cheboksary, Russia.
EM mkiselev@megaputer.ru
CR Butts DA, 2007, NATURE, V449, P92, DOI [10.1038/nature06105, 10.1038/natureO6105]
   Gardner B, 2015, NEURAL COMPUT, V27, P2548, DOI 10.1162/NECO_a_00790
   Guise M, 2014, NEURAL COMPUT, V26, P2052, DOI 10.1162/NECO_a_00620
   Ikegaya Y, 2004, SCIENCE, V304, P559, DOI 10.1126/science.1093173
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Kiselev M., 2016, P IJCNN 201 IN PRESS
   Kiselev M, 2013, LECT NOTES COMPUT SC, V7902, P510, DOI 10.1007/978-3-642-38679-4_51
   Kiselev MV, 2014, COMPUT INTEL NEUROSC, V2014, DOI 10.1155/2014/476580
   Martinez R, 2009, LECT NOTES COMPUT SC, V5769, P75, DOI 10.1007/978-3-642-04277-5_8
   Mehta MR, 2002, NATURE, V417, P741, DOI 10.1038/nature00807
   Szatmáry B, 2010, PLOS COMPUT BIOL, V6, DOI 10.1371/journal.pcbi.1000879
NR 11
TC 0
Z9 0
U1 0
U2 1
PY 2016
VL 9719
BP 665
EP 672
DI 10.1007/978-3-319-40663-3_76
UT WOS:000386324900076
DA 2023-11-16
ER

PT S
AU Bernhard, F
   Keriven, R
AF Bernhard, Fabrice
   Keriven, Renaud
BE Alexandrov, VN
   VanAlbada, GD
   Sloot, PMA
   Dongarra, J
TI Spiking neurons on GPUs
SO COMPUTATIONAL SCIENCE - ICCS 2006, PT 4, PROCEEDINGS
SE Lecture Notes in Computer Science
DT Article; Proceedings Paper
CT 6th International Conference on Computational Science (ICCS 2006)
CY MAY 28-31, 2006
CL Reading, ENGLAND
AB Simulating large networks of spiking neurons is a very common task in the areas of Neuroinformatics and Computational Neurosciences. These simulations are time-consuming but also often intrinsically parallel. The recent advent of powerful and programmable graphic cards seems to be a pertinent solution to the problem: they offer a cheap but efficient possibility to serve as very fast co-processors for the parallel computing that spiking neural networks need. We describe our implementation of three different problems on such a card: two image-segmentation algorithms using spiking neural networks and one multi-purpose spiking neural-network simulator. Using these examples we show the benefits, the challenges and the limits of such an implementation.
C1 ENPC, ENS, INRIA, Projet Odyssee, F-75005 Paris, France.
RP Bernhard, F (corresponding author), ENPC, ENS, INRIA, Projet Odyssee, 45 Rue Ulm, F-75005 Paris, France.
EM Fabrice.Bernhard@polytechnique.org
CR Buhmann JM, 2005, NEURAL COMPUT, V17, P1010, DOI 10.1162/0899766053491913
   Campbell SR, 1999, NEURAL COMPUT, V11, P1595, DOI 10.1162/089976699300016160
   GODDEKE D, 2005, 300 U DORTM FB MATH
   Lapique L., 1907, J PHYSL PATHOL GEN, V9, P620, DOI DOI 10.1007/S00422-007-0189-6
   OWENS JD, 2005, EUROGRAPHICS 2005
   PIETRAS K, GPU BASED MULTILAYER
   ROSENBLATT F, 1962, PRINCIPLES NEURAL DY
NR 7
TC 17
Z9 17
U1 0
U2 0
PY 2006
VL 3994
BP 236
EP 243
UT WOS:000238417500036
DA 2023-11-16
ER

PT C
AU Bagchi, S
   Bhat, SS
   Kumar, A
AF Bagchi, Samya
   Bhat, Srikrishna S.
   Kumar, Atul
GP IEEE
TI O(1) time Sorting Algorithms using Spiking Neurons
SO 2016 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUL 24-29, 2016
CL Vancouver, CANADA
DE spiking neural network; constant time sort; number comparison
ID NETWORK
AB This paper presents the design and implementation of two spiking neural network based sorting algorithms with time complexity of O(1). These algorithms are inspired from artificial neural network based sorting algorithms. A number comparator circuit using spiking network as a building block has been proposed and been used in the sorting algorithms to compare numbers. This comparator circuit is generic and can be used in other algorithms. Results show that both these algorithms take constant time to sort any count of numbers once the circuit is setup with sufficient input lines and input sequence of numbers is configured in the network.
C1 [Bagchi, Samya] SBALABS Pvt Ltd, 80 Ft Rd, Bangalore, Karnataka, India.
   [Bhat, Srikrishna S.] IIIT Bangalore, Hosur Rd, Bangalore, Karnataka, India.
   [Kumar, Atul] IBM Res, Manyata Tech Pk, Bangalore, Karnataka, India.
RP Bagchi, S (corresponding author), SBALABS Pvt Ltd, 80 Ft Rd, Bangalore, Karnataka, India.
EM samya.bagchi@iiitb.org; srikrishna.bhat@iiitb.org; kumar.atul@in.ibm.com
CR Beyeler M, 2015, IEEE IJCNN
   CHEN W, 1990, INT JOINT C NEUR NET
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Krause T.U., 2014, THESIS
   KWON TM, 1991, INT JOINT C NEUR NET
   LEIGHTON T, 1985, IEEE T COMPUT, V34, P344, DOI 10.1109/TC.1985.5009385
   Lin SS, 1997, NEUROCOMPUTING, V14, P289, DOI 10.1016/S0925-2312(96)00040-9
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Paugam-Moisy H., 2012, HDB NATURAL COMPUTIN, V1, P1, DOI [DOI 10.1007/978-3-540-92910-9_10, 10.1007/978-3-540-92910-9_10]
   Plana LA, 2007, IEEE DES TEST COMPUT, V24, P454, DOI 10.1109/MDT.2007.149
   SIU KY, 1990, P IEEE, V78, P1669, DOI 10.1109/5.58350
   TAKEFUJI Y, 1990, IEEE T CIRCUITS SYST, V37, P1425, DOI 10.1109/31.62417
   Tambouratzis T, 1999, IEEE T SYST MAN CY B, V29, P271, DOI 10.1109/3477.752799
NR 13
TC 0
Z9 0
U1 0
U2 1
PY 2016
BP 1037
EP 1043
UT WOS:000399925501029
DA 2023-11-16
ER

PT C
AU Zhang, Z
   Lin, XH
AF Zhang, Zhen
   Lin, Xianghong
BE Chakrabarti, S
   Paul, R
TI Supervised Learning Algorithm Based on Spike Back Reconstruction
SO 2022 IEEE 13TH ANNUAL UBIQUITOUS COMPUTING, ELECTRONICS & MOBILE
   COMMUNICATION CONFERENCE (UEMCON)
DT Proceedings Paper
CT IEEE 13th Annual Ubiquitous Computing, Electronics and Mobile
   Communication Conference (UEMCON)
CY OCT 26-29, 2022
CL ELECTR NETWORK
DE deep learning; spiking neural network; spike back reconstruction; local
   error
AB The error backpropagation mechanism adjusts the synaptic weights by feeding back the total error signal layer by layer in a neural network, which is unreasonable in biology. In this paper, the supervised learning algorithm based on spike back reconstruction is proposed, which gets rid of the concept of total error, and derives the local error signal of each layer of the network by generating the desired spike signal of each neuron. The reconstruction spike signal is directly fed back to the presynaptic local neurons, thus the training of the multi-layer neural network can be realized through local training mode with biological interpretation. We derive the spike back reconstruction learning algorithm for deep spiking neural networks, in which the learning rule is expressed by the inner product operation of spike trains. Additionally, the spike train learning experiments are applied to verify the effectiveness of the proposed algorithm, and the impact of relevant factors on the learning performance of the algorithm is also analyzed. Experimental results demonstrate that the learning ability of spike back reconstruction method is obviously superior to the traditional error backpropagation mechanism.
C1 [Zhang, Zhen; Lin, Xianghong] Northwest Normal Univ, Coll Comp Sci & Engn, Lanzhou, Peoples R China.
RP Zhang, Z (corresponding author), Northwest Normal Univ, Coll Comp Sci & Engn, Lanzhou, Peoples R China.
EM 2020211974@nwnu.edu.cn; linxh@nwnu.edu.cn
CR Akrout M, 2019, ADV NEUR IN, V32
   [Anonymous], 2016, ADV NEURAL INFORM PR
   Hecht-Nielsen R, 1992, NEURAL NETWORKS PERC, P65
   Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508
   Lillicrap TP, 2020, NAT REV NEUROSCI, V21, P335, DOI 10.1038/s41583-020-0277-3
   Lillicrap TP, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms13276
   Lin XH, 2017, NEUROCOMPUTING, V237, P59, DOI 10.1016/j.neucom.2016.08.087
   Park IM, 2013, IEEE SIGNAL PROC MAG, V30, P149, DOI 10.1109/MSP.2013.2251072
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Schreiber S, 2003, NEUROCOMPUTING, V52-4, P925, DOI 10.1016/S0925-2312(02)00838-X
   Shi C, 2021, IEEE T CIRCUITS-II, V68, P1581, DOI 10.1109/TCSII.2021.3063784
   Stork D. G., 1989, IJCNN: International Joint Conference on Neural Networks (Cat. No.89CH2765-6), P241, DOI 10.1109/IJCNN.1989.118705
   Wang XW, 2020, NEURAL NETWORKS, V125, P258, DOI 10.1016/j.neunet.2020.02.011
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Zhao DC, 2020, FRONT COMPUT NEUROSC, V14, DOI 10.3389/fncom.2020.576841
NR 15
TC 0
Z9 0
U1 0
U2 0
PY 2022
BP 1
EP 7
DI 10.1109/UEMCON54665.2022.9965701
UT WOS:000946108300002
DA 2023-11-16
ER

PT C
AU Saravanan, M
   Bablani, A
   Rangisetty, N
AF Saravanan, M.
   Bablani, Annushree
   Rangisetty, Navyasai
BE Singh, M
   Tyagi, V
   Gupta, PK
   Flusser, J
   Oren, T
TI Evolving Spiking Neural Network as a Classifier: An Experimental Review
SO ADVANCES IN COMPUTING AND DATA SCIENCES (ICACDS 2022), PT II
SE Communications in Computer and Information Science
DT Proceedings Paper
CT 6th International Conference on Advances in Computing and Data Sciences
   (ICACDS)
CY APR 22-23, 2022
CL G Pullaiah Coll Engn &Technol, Kurnool, INDIA
HO G Pullaiah Coll Engn &Technol
DE Evolving spiking neural network; Reservoir; Gaussian receptive field;
   Spatial-temporal data
ID NEURONS
AB The brain-inspired Spiking Neural Networks (SNNs) are considered as the third generation of neural networks for AI applications. The spiking neural network has been proved very efficient to predict and classify data with spatial and temporal information in the way of modeling the behavior and learning potential of the brain. The aim is to understand the working of the Evolving SNN (ESNN) as a classifier and how it is different from the existing neural models. Besides exploring the existing ESNN architecture, the results have been generated by tuning the various parameters of the ESNN model which may be contributing to provide better prediction accuracies. The tuned ESNN model is applied to various datasets and compared with the existing second-generation neural network model like LSTM. The results show comparable improvement in the classification accuracy using ESNN which concludes that the ESNN and its variants are the beginning of a new era of Neural Networks.
C1 [Saravanan, M.] Ericsson India Global Serv Pvt Ltd, Perungudi, India.
   [Bablani, Annushree; Rangisetty, Navyasai] Indian Inst Informat Technol, Sri City, India.
RP Saravanan, M (corresponding author), Ericsson India Global Serv Pvt Ltd, Perungudi, India.
EM m.saravanan@ericsson.com; annushree.bablani@iiits.in;
   navyasai.r17@iiits.in
CR Dhoble K., 2013, THESIS U TECHNOLOGY
   Hamed HNA, 2009, 2009 INTERNATIONAL CONFERENCE OF SOFT COMPUTING AND PATTERN RECOGNITION, P695, DOI 10.1109/SoCPaR.2009.139
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Kasabov N, 1998, ICONIP'98: THE FIFTH INTERNATIONAL CONFERENCE ON NEURAL INFORMATION PROCESSING JOINTLY WITH JNNS'98: THE 1998 ANNUAL CONFERENCE OF THE JAPANESE NEURAL NETWORK SOCIETY - PROCEEDINGS, VOLS 1-3, P1232
   Kasabov N., 2011, NEURAL NETWORKS
   Kasabov N., 2018, TIME SPACE SPIKING N, DOI 10.1007/978-3-662-57715-8
   Koelstra S, 2012, IEEE T AFFECT COMPUT, V3, P18, DOI 10.1109/T-AFFC.2011.15
   Lobo JL, 2018, NEURAL NETWORKS, V108, P1, DOI 10.1016/j.neunet.2018.07.014
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Perepu SK, 2020, IEEE GLOBE WORK, DOI 10.1109/GCWkshp50303.2020.9367494
   Schliebs S, 2011, LECT NOTES COMPUT SC, V7063, P160, DOI 10.1007/978-3-642-24958-7_19
   Schliebs S, 2010, LECT NOTES COMPUT SC, V6443, P163, DOI 10.1007/978-3-642-17537-4_21
   Schliebs S, 2009, LECT NOTES COMPUT SC, V5506, P1229, DOI 10.1007/978-3-642-02490-0_149
   Soltic S, 2008, IEEE IJCNN, P2091, DOI 10.1109/IJCNN.2008.4634085
   Soltic S, 2010, INT J NEURAL SYST, V20, P437, DOI 10.1142/S012906571000253X
   Thantharate A, 2019, 2019 IEEE 10TH ANNUAL UBIQUITOUS COMPUTING, ELECTRONICS & MOBILE COMMUNICATION CONFERENCE (UEMCON), P762
   Thantharate A, 2020, 2020 10TH ANNUAL COMPUTING AND COMMUNICATION WORKSHOP AND CONFERENCE (CCWC), P852, DOI [10.1109/CCWC47524.2020.9031158, 10.1109/ccwc47524.2020.9031158]
   Wysoski SG, 2008, NEUROCOMPUTING, V71, P2563, DOI 10.1016/j.neucom.2007.12.038
   Wysoski SG, 2008, LECT NOTES COMPUT SC, V4985, P406
   Wysoski SG, 2007, LECT NOTES COMPUT SC, V4669, P758
   Wysoski SG, 2006, LECT NOTES COMPUT SC, V4179, P1133
   Wysoski SG, 2006, LECT NOTES COMPUT SC, V4131, P61
NR 22
TC 0
Z9 0
U1 0
U2 0
PY 2022
VL 1614
BP 304
EP 315
DI 10.1007/978-3-031-12641-3_25
UT WOS:000889373500025
DA 2023-11-16
ER

PT J
AU Mo, LF
   Wang, MH
AF Mo, Lingfei
   Wang, Minghao
TI LogicSNN: A Unified Spiking Neural Networks Logical Operation Paradigm
SO ELECTRONICS
DT Article
DE spiking neural networks; logical operation; spike-timing-dependent
   plasticity
AB LogicSNN, a unified spiking neural networks (SNN) logical operation paradigm is proposed in this paper. First, we define the logical variables under the semantics of SNN. Then, we design the network structure of this paradigm and use spike-timing-dependent plasticity for training. According to this paradigm, six kinds of basic SNN binary logical operation modules and three kinds of combined logical networks based on these basic modules are implemented. Through these experiments, the rationality, cascading characteristics and the potential of building large-scale network of this paradigm are verified. This study fills in the blanks of the logical operation of SNN and provides a possible way to realize more complex machine learning capabilities.
C1 [Mo, Lingfei; Wang, Minghao] Southeast Univ, Sch Instrument Sci & Engn, FutureX Lab, Nanjing 210096, Peoples R China.
RP Mo, LF (corresponding author), Southeast Univ, Sch Instrument Sci & Engn, FutureX Lab, Nanjing 210096, Peoples R China.
EM lfmo@seu.edu.cn; diff@seu.edu.cn
CR Adonias G.L., 2019, P 4 WORKSH MOL COMM
   Adonias GL, 2020, FRONT COMPUT NEUROSC, V14, DOI 10.3389/fncom.2020.556628
   Adonias GL, 2020, IEEE T NANOBIOSCI, V19, P224, DOI 10.1109/TNB.2020.2975942
   Boole G., 1854, INVESTIGATION LAWS T, DOI DOI 10.1016/j.wasman.2010.04.010
   Brittin CA, 2021, NATURE, V591, P105, DOI 10.1038/s41586-021-03284-x
   Caporale N, 2008, ANNU REV NEUROSCI, V31, P25, DOI 10.1146/annurev.neuro.31.060407.125639
   Cook SJ, 2019, NATURE, V571, P63, DOI 10.1038/s41586-019-1352-7
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   Goldental A, 2014, FRONT COMPUT NEUROSC, V8, DOI 10.3389/fncom.2014.00052
   Goodman DFM, 2009, FRONT NEUROSCI-SWITZ, V3, P192, DOI 10.3389/neuro.01.026.2009
   Hebb D., 2005, ORG BEHAV NEUROPSYCH
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   McCulloch Warren S., 1943, BULL MATH BIOPHYS, V5, P115, DOI 10.1007/BF02478259
   Peizhi L., 2009, DIGITAL CIRCUIT LOGI
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Stimberg M, 2019, ELIFE, V8, DOI 10.7554/eLife.47314
   Su JW, 2019, IEEE T EVOLUT COMPUT, V23, P828, DOI 10.1109/TEVC.2019.2890858
   Vogels TP, 2005, J NEUROSCI, V25, P10786, DOI 10.1523/JNEUROSCI.3508-05.2005
   Wade JJ, 2010, IEEE T NEURAL NETWOR, V21, P1817, DOI 10.1109/TNN.2010.2074212
   Wang Z, 2020, COMPUT INTEL NEUROSC, V2020, DOI 10.1155/2020/2710561
NR 20
TC 4
Z9 5
U1 0
U2 9
PD SEP
PY 2021
VL 10
IS 17
AR 2123
DI 10.3390/electronics10172123
UT WOS:000694072400001
DA 2023-11-16
ER

PT C
AU Cabessa, J
   Horcholle-Bossavit, G
   Quenet, B
AF Cabessa, Jeremie
   Horcholle-Bossavit, Ginette
   Quenet, Brigitte
BE Lintas, A
   Rovetta, S
   Verschure, PFMJ
   Villa, AEP
TI Neural Computation with Spiking Neural Networks Composed of Synfire
   Rings
SO ARTIFICIAL NEURAL NETWORKS AND MACHINE LEARNING - ICANN 2017, PT I
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 26th International Conference on Artificial Neural Networks (ICANN)
CY SEP 11-14, 2017
CL Alghero, ITALY
DE Neural computation; Izhikevich spiking neurons; Synfire rings; Finite
   state automata
AB We show that any finite state automaton can be simulated by some neural network of Izhikevich spiking neurons composed of interconnected synfire rings. The construction turns out to be robust to the introduction of two kinds of synaptic noises. These considerations show that a biological paradigm of neural computation based on sustained activities of cell assemblies is indeed possible.
C1 [Cabessa, Jeremie] Univ Paris 2, Lab Econ Math LEMMA, Paris, France.
   [Horcholle-Bossavit, Ginette; Quenet, Brigitte] PSL Res Univ, ESPCI ParisTech, Equipe Stat Appl, Paris, France.
   [Horcholle-Bossavit, Ginette; Quenet, Brigitte] UPMC Univ Paris 6, Sorbonne Univ, Neurophysiol Resp Expt & Clin, INSERM,UMRS1158, Paris, France.
RP Cabessa, J (corresponding author), Univ Paris 2, Lab Econ Math LEMMA, Paris, France.
EM jeremie.cabessa@u-paris2.fr
CR Abbott LF, 2000, NAT NEUROSCI, V3, P1178, DOI 10.1038/81453
   Abeles M, 2004, SCIENCE, V304, P523, DOI 10.1126/science.1097725
   [Anonymous], STUDIES BRAIN FUNCTI
   [Anonymous], 1948, TECHNICAL REPORT
   Asai Y, 2008, NEURAL NETWORKS, V21, P799, DOI 10.1016/j.neunet.2008.06.014
   Asai Y, 2012, BRAIN RES, V1434, P17, DOI 10.1016/j.brainres.2011.10.012
   Cabessa J, 2017, 2016 INT JOINT C NEU
   Cabessa J, 2014, INT J NEURAL SYST, V24, DOI 10.1142/S0129065714500294
   Fourcaud-Trocmé N, 2003, J NEUROSCI, V23, P11628
   Horcholle-Bossavit G, 2009, BIOSYSTEMS, V97, P35, DOI 10.1016/j.biosystems.2009.04.002
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Kleene Stephen, 1956, REPRESENTATION EVENT, V34, P3, DOI DOI 10.1515/9781400882618-002
   Kobayashi R, 2009, FRONT COMPUT NEUROSC, V3, DOI 10.3389/neuro.10.009.2009
   McCulloch Warren S., 1943, BULL MATH BIOPHYS, V5, P115, DOI 10.1007/BF02478259
   Minsky ML., 1967, COMPUTATION FINITE I
   SIEGELMANN HT, 1994, THEOR COMPUT SCI, V131, P331, DOI 10.1016/0304-3975(94)90178-3
   SIEGELMANN HT, 1995, J COMPUT SYST SCI, V50, P132, DOI 10.1006/jcss.1995.1013
   Siegelmann HT, 1996, COMPUT INTELL, V12, P567, DOI 10.1111/j.1467-8640.1996.tb00277.x
   Zheng PS, 2014, FRONT COMPUT NEUROSC, V8, DOI 10.3389/fncom.2014.00066
NR 19
TC 1
Z9 1
U1 0
U2 1
PY 2017
VL 10613
BP 245
EP 253
DI 10.1007/978-3-319-68600-4_29
PN I
UT WOS:000449802500029
DA 2023-11-16
ER

PT C
AU Salerno, M
   Susi, G
   Cristini, A
AF Salerno, Mario
   Susi, Gianluca
   Cristini, Alessandro
BE Pellegrini, M
   Fred, A
   Filipe, J
   Gamboa, H
TI ACCURATE LATENCY CHARACTERIZATION FOR VERY LARGE ASYNCHRONOUS SPIKING
   NEURAL NETWORKS
SO BIOINFORMATICS 2011
DT Proceedings Paper
CT International Conference on Bioinformatics Models, Methods and
   Algorithms
CY JAN 26-29, 2011
CL Rome, ITALY
DE Neuron; Spiking Neural Network (SNN); Latency; Event-driven; Plasticity;
   Threshold; Neuromorphic; Neuronal group selection
AB The simulation problem of very large fully asynchronous Spiking Neural Networks is considered in this paper. To this purpose, a preliminary accurate analysis of the latency time is made, applying classical modelling methods to single neurons. The latency characterization is then used to propose a simplified model, able to simulate large neural networks. On this basis, networks, with up to 100,000 neurons for more than 100,000 spikes, can be simulated in a quite short time with a simple MATLAB program. Plasticity algorithms are also applied to emulate interesting global effects as the Neuronal Group Selection.
C1 [Salerno, Mario; Susi, Gianluca; Cristini, Alessandro] Univ Roma Tor Vergata, Inst Elect, Rome, Italy.
RP Salerno, M (corresponding author), Univ Roma Tor Vergata, Inst Elect, Rome, Italy.
EM gianluca.susi@uniroma2.it
CR Boudkkazi S, 2007, NEURON, V56, P1048, DOI 10.1016/j.neuron.2007.10.037
   Chua L. O., 1988, IEEE TRANS CIRCUITS, V35
   D'Haene M., 2009, NEURAL COMPUTATI APR, V21
   Edelman G. M., 1987, NEURAL DARWINISM THE
   FitzHugh R., 1955, B MATH BIOPHYSICS
   Gernstein G. L., 1964, BIOPHISICAL J, V4
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Izhikevich E. M., 2004, SPIKE TIMING DYNAMIC, V14
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2007, DYNAMICAL SYSTEMS NE
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Lapicque L., 1907, RECHERCHES QUANTITAT
   Maas W., 1997, ETWORKS SPIKING NEU
   RAMON S, 1909, HISTOLOGIE SYSTEME N, V1
   RAMON S, 1911, HISTOLOGIE SYSTEME N, V1
NR 15
TC 5
Z9 5
U1 0
U2 2
PY 2011
BP 116
EP +
UT WOS:000308455800020
DA 2023-11-16
ER

PT J
AU Nallathambi, A
   Sen, S
   Raghunathan, A
   Chandrachoodan, N
AF Nallathambi, Abinand
   Sen, Sanchari
   Raghunathan, Anand
   Chandrachoodan, Nitin
TI Probabilistic Spike Propagation for Efficient Hardware Implementation of
   Spiking Neural Networks
SO FRONTIERS IN NEUROSCIENCE
DT Article
DE spiking neural networks; hardware acceleration; energy efficiency;
   memory; probabilistic spike propagation
ID NEURONS
AB Spiking neural networks (SNNs) have gained considerable attention in recent years due to their ability to model temporal event streams, be trained using unsupervised learning rules, and be realized on low-power event-driven hardware. Notwithstanding the intrinsic desirable attributes of SNNs, there is a need to further optimize their computational efficiency to enable their deployment in highly resource-constrained systems. The complexity of evaluating an SNN is strongly correlated to the spiking activity in the network, and can be measured in terms of a fundamental unit of computation, viz. spike propagation along a synapse from a single source neuron to a single target neuron. We propose probabilistic spike propagation, an approach to optimize rate-coded SNNs by interpreting synaptic weights as probabilities, and utilizing these probabilities to regulate spike propagation. The approach results in 2.4-3.69x reduction in spikes propagated, leading to reduced time and energy consumption. We propose Probabilistic Spiking Neural Network Application Processor (P-SNNAP), a specialized SNN accelerator with support for probabilistic spike propagation. Our evaluations across a suite of benchmark SNNs demonstrate that probabilistic spike propagation results in 1.39-2x energy reduction with simultaneous speedups of 1.16-1.62x compared to the traditional model of SNN evaluation.
C1 [Nallathambi, Abinand; Raghunathan, Anand; Chandrachoodan, Nitin] Indian Inst Technol Madras, Dept Elect Engn, Chennai, India.
   [Sen, Sanchari; Raghunathan, Anand] Purdue Univ, Sch Elect & Comp Engn, W Lafayette, IN USA.
RP Nallathambi, A (corresponding author), Indian Inst Technol Madras, Dept Elect Engn, Chennai, India.
EM ee16s032@ee.iitm.ac.in
CR Afifi A, 2009, 2009 EUROPEAN CONFERENCE ON CIRCUIT THEORY AND DESIGN, VOLS 1 AND 2, P563, DOI 10.1109/ECCTD.2009.5275035
   Ahmed K, 2016, IEEE IJCNN, P4286, DOI 10.1109/IJCNN.2016.7727759
   Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   Al-Shedivat M, 2015, IEEE J EM SEL TOP C, V5, P242, DOI 10.1109/JETCAS.2015.2435512
   [Anonymous], 2012, 2012 INT JOINT C NEU, DOI DOI 10.1109/IJCNN.2012.6252807
   Asmussen S., 2007, STOCHASTIC SIMULATIO, V57
   Bezanson J, 2017, SIAM REV, V59, P65, DOI 10.1137/141000671
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Chakraborty I, 2019, PHYS REV APPL, V11, DOI 10.1103/PhysRevApplied.11.014063
   Chen MC, 2018, IEEE T MAGN, V54, DOI 10.1109/TMAG.2018.2845890
   Cheung K, 2016, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00516
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   de Lima TF, 2017, NANOPHOTONICS-BERLIN, V6, P577, DOI 10.1515/nanoph-2016-0139
   Deneve S, 2008, NEURAL COMPUT, V20, P91, DOI 10.1162/neco.2008.20.1.91
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   Hu YH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00405
   Kasabov N, 2010, NEURAL NETWORKS, V23, P16, DOI 10.1016/j.neunet.2009.08.010
   Krithivasan S, 2019, I SYMPOS LOW POWER E
   Kundu S, 2021, IEEE WINT CONF APPL, P3952, DOI 10.1109/WACV48630.2021.00400
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Neftci EO, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00241
   Neil D, 2014, IEEE T VLSI SYST, V22, P2621, DOI 10.1109/TVLSI.2013.2294916
   Orchard G, 2015, FRONT NEUROSCI-SWITZ, V9, DOI [10.3389/fhins.2015.00437, 10.3389/fnins.2015.00437]
   Park S, 2019, PROCEEDINGS OF THE 2019 56TH ACM/EDAC/IEEE DESIGN AUTOMATION CONFERENCE (DAC), DOI 10.1145/3316781.3317822
   Paulin M. G., 2014, ARXIV
   Pedram A, 2017, IEEE DES TEST, V34, P39, DOI 10.1109/MDAT.2016.2573586
   Rathi N, 2019, IEEE T COMPUT AID D, V38, P668, DOI 10.1109/TCAD.2018.2819366
   Roy A, 2017, I SYMPOS LOW POWER E
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Rueckauer B, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00176
   Sahu U., 2018, 2018 4 IEEE INT C EM, P1
   Sen S, 2017, DES AUT TEST EUROPE, P193, DOI 10.23919/DATE.2017.7926981
   Sengupta A, 2016, PHYS REV APPL, V6, DOI 10.1103/PhysRevApplied.6.064003
   Serrano-Gotarredona T, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00002
   Seung HS, 2003, NEURON, V40, P1063, DOI 10.1016/S0896-6273(03)00761-X
   Shanbhag NR, 2010, DES AUT CON, P859
   Smaragdos G, 2017, J NEURAL ENG, V14, DOI 10.1088/1741-2552/aa7fc5
   Smithson SC, 2016, 2016 IEEE INTERNATIONAL WORKSHOP ON SIGNAL PROCESSING SYSTEMS (SIPS), P309, DOI 10.1109/SiPS.2016.61
   Springenberg Jost Tobias, 2014, ARXIV14126806
   Srinivasan G, 2017, DES AUT TEST EUROPE, P530, DOI 10.23919/DATE.2017.7927045
   Thoziyoor S, 2008, CONF PROC INT SYMP C, P51, DOI 10.1109/ISCA.2008.16
   Vanarse A, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00115
   Xiang SY, 2019, IEEE J SEL TOP QUANT, V25, DOI 10.1109/JSTQE.2019.2911565
   Zhang DM, 2016, IEEE INT SYMP NANO, P173, DOI 10.1145/2950067.2950105
NR 45
TC 2
Z9 2
U1 2
U2 9
PD JUL 15
PY 2021
VL 15
AR 694402
DI 10.3389/fnins.2021.694402
UT WOS:000878824300001
DA 2023-11-16
ER

PT J
AU Zhang, AG
   Han, Y
   Niu, YZ
   Gao, YM
   Chen, ZZ
   Zhao, K
AF Zhang, Anguo
   Han, Ying
   Niu, Yuzhen
   Gao, Yueming
   Chen, Zhizhang
   Zhao, Kai
TI Self-Evolutionary Neuron Model for Fast-Response Spiking Neural Networks
SO IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS
DT Article
DE Neural networks; Computational modeling; Capacitors; Adaptation models;
   Capacitance; Neuromorphics; Biomembranes; Fast response network;
   self-evolutionary neuron model; spiking neural network (SNN); synaptic
   plasticity
ID PLASTICITY; SIMULATOR
AB We propose two simple and effective spiking neuron models to improve the response time of the conventional spiking neural network. The proposed neuron models adaptively tune the presynaptic input current depending on the input received from its presynapses and subsequent neuron firing events. We analyze and derive the firing activity homeostatic convergence of the proposed models. We experimentally verify and compare the models on MNIST handwritten digits and FashionMNIST classification tasks. We show that the proposed neuron models significantly increase the response speed to the input signal.
C1 [Zhang, Anguo] Fuzhou Univ, Coll Phys & Informat Engn, Fuzhou 350108, Fujian, Peoples R China.
   [Zhang, Anguo] Key Lab Med Instrumentat & Pharmaceut Technol Fuji, Fuzhou 350116, Peoples R China.
   [Zhang, Anguo] Ruijie Networks Co Ltd, Res Inst Ruijie, Fuzhou 350002, Fujian, Peoples R China.
   [Han, Ying] Xiamen Univ, Sch Publ Hlth, Xiamen 361102, Peoples R China.
   [Niu, Yuzhen] Fuzhou Univ, Coll Math & Comp Sci, Fujian Key Lab Network Comp & Intelligent Informat, Fuzhou 350108, Fujian, Peoples R China.
   [Niu, Yuzhen] Fuzhou Univ, Key Lab Spatial Data Min & Informat Sharing, Minist Educ, Fuzhou 350108, Fujian, Peoples R China.
   [Gao, Yueming] Fuzhou Univ, Coll Phys & informat Engn, Key Lab Med Instrumentat & Pharmaceut Technol Fuji, Fuzhou 350108, Fujian, Peoples R China.
   [Gao, Yueming] Fuzhou Univ, Key Lab Med Instrumentat & Pharmaceut Technol Fuji, Fuzhou 350108, Fujian, Peoples R China.
   [Chen, Zhizhang] Fuzhou Univ, Coll Phys & Informat Engn, Fuzhou 350108, Fujian, Peoples R China.
   [Zhao, Kai] Univ Macau, Fac Sci & Technol, Macau, Peoples R China.
RP Gao, YM (corresponding author), Fuzhou Univ, Coll Phys & informat Engn, Key Lab Med Instrumentat & Pharmaceut Technol Fuji, Fuzhou 350108, Fujian, Peoples R China.; Gao, YM (corresponding author), Fuzhou Univ, Key Lab Med Instrumentat & Pharmaceut Technol Fuji, Fuzhou 350108, Fujian, Peoples R China.; Chen, ZZ (corresponding author), Fuzhou Univ, Coll Phys & Informat Engn, Fuzhou 350108, Fujian, Peoples R China.
EM fzugym@gmail.com; zz.chen@ieee.org
CR Afifi A, 2009, 2009 EUROPEAN CONFERENCE ON CIRCUIT THEORY AND DESIGN, VOLS 1 AND 2, P563, DOI 10.1109/ECCTD.2009.5275035
   BELL AJ, 1995, NEURAL COMPUT, V7, P1129, DOI 10.1162/neco.1995.7.6.1129
   Bing Han, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13555, DOI 10.1109/CVPR42600.2020.01357
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Diehl PU, 2015, IEEE IJCNN
   Diesmann M, 1999, NATURE, V402, P529, DOI 10.1038/990101
   Dominguez-Morales J., 2018, 2018 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2018.8489381
   Fusi S, 2000, IEEE IJCNN, P121, DOI 10.1109/IJCNN.2000.861291
   Galarreta M, 1999, NATURE, V402, P72, DOI 10.1038/47029
   Holt GR, 1997, NEURAL COMPUT, V9, P1001, DOI 10.1162/neco.1997.9.5.1001
   Hong CF, 2020, IEEE T NEUR NET LEAR, V31, P1285, DOI 10.1109/TNNLS.2019.2919662
   Joshi Prashant, 2009, Proceedings 2009 International Joint Conference on Neural Networks (IJCNN 2009 - Atlanta), P1456, DOI 10.1109/IJCNN.2009.5178625
   Kasabov N, 2015, INFORM SCIENCES, V294, P565, DOI 10.1016/j.ins.2014.06.028
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Kim S, 2019, Arxiv, DOI arXiv:1903.06530
   Li C, 2018, 2018 THIRD IEEE/ACM SYMPOSIUM ON EDGE COMPUTING (SEC), P334, DOI 10.1109/SEC.2018.00035
   Li CG, 2013, IEEE T AUTON MENT DE, V5, P62, DOI 10.1109/TAMD.2012.2211101
   Li E, 2020, IEEE T WIREL COMMUN, V19, P447, DOI 10.1109/TWC.2019.2946140
   Liu Q, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00496
   Maass W, 1997, NEURAL COMPUT, V9, P279, DOI 10.1162/neco.1997.9.2.279
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maass W, 1997, ADV NEUR IN, V9, P211
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Minkovich K, 2014, IEEE T NEUR NET LEAR, V25, P316, DOI 10.1109/TNNLS.2013.2276056
   Naveros F, 2015, IEEE T NEUR NET LEAR, V26, P1567, DOI 10.1109/TNNLS.2014.2345844
   Neckar A, 2019, P IEEE, V107, P144, DOI 10.1109/JPROC.2018.2881432
   Neumann K., 2012, ESANN 2012, P555
   Nikouei SY, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON EDGE COMPUTING (IEEE EDGE), P125, DOI 10.1109/EDGE.2018.00025
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   Parhi KK, 2020, IEEE OPEN J CIRCUITS, V1, P185, DOI 10.1109/OJCAS.2020.3032092
   Park S, 2018, 2018 ACM SYMPOSIUM ON EYE TRACKING RESEARCH & APPLICATIONS (ETRA 2018), DOI 10.1145/3204493.3204545
   Petro B, 2020, IEEE T NEUR NET LEAR, V31, P358, DOI 10.1109/TNNLS.2019.2906158
   Roy S, 2017, IEEE T NEUR NET LEAR, V28, P900, DOI 10.1109/TNNLS.2016.2582517
   Rueckauer B, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351295
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Schemmel J, 2006, IEEE IJCNN, P1
   Spiess R, 2016, FRONT COMPUT NEUROSC, V10, DOI 10.3389/fncom.2016.00093
   Stromatias E, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00350
   Wang RC, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00014
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Xiao H, 2017, Arxiv, DOI arXiv:1708.07747
   Zambrano D, 2016, Arxiv, DOI arXiv:1609.02053
   Zhang AG, 2019, NEUROCOMPUTING, V365, P102, DOI 10.1016/j.neucom.2019.07.009
   Zhang AG, 2019, IEEE ACCESS, V7, P4927, DOI 10.1109/ACCESS.2018.2887354
   Zhang ML, 2018, IEEE T COGN DEV SYST, V10, P151, DOI 10.1109/TCDS.2017.2651943
   Zhang ML, 2017, NEUROCOMPUTING, V219, P333, DOI 10.1016/j.neucom.2016.09.044
NR 47
TC 5
Z9 5
U1 2
U2 7
PD DEC
PY 2022
VL 14
IS 4
BP 1766
EP 1777
DI 10.1109/TCDS.2021.3139444
UT WOS:000916821100039
DA 2023-11-16
ER

PT C
AU Zhou, J
   Xiong, J
AF Zhou, Jing
   Xiong, Jun
BE Li, Z
   Sun, J
TI Object detection and tracking method based on spiking neuron network
SO 2022 41ST CHINESE CONTROL CONFERENCE (CCC)
SE Chinese Control Conference
DT Proceedings Paper
CT 41st Chinese Control Conference (CCC)
CY JUL 25-27, 2022
CL Hefei, PEOPLES R CHINA
DE Object detection; Spiking neuron network; Object tracking;
   Spiking-SiamFC
AB Aiming at the problems of large parameter capacity and heavy dependence on acceleration framework for complex target detection and tracking model, the conversion method between artificial neural network and impulse neural network is studied in this work to implement target detection and tracking task based on the spiking neural network. Firstly, the convolution modules Alexnet, vgg11-5c and vgg16-8c of the full convolution twin network Siamese-FC are properly adjusted and cut to adapt with the characteristics and limitations of impulse neurons, and then the Norm-SiamFC network model is constructed and trained. Finally, the migration parameters are adopted to construct a spiking network Spiking-SiamFC for object detection and tracking, which achieves good tracking effect and occupies less resources, thus the model can be widely utilized in edge devices with limited resource storage capacity.
C1 [Zhou, Jing; Xiong, Jun] Jianghan Univ, Sch Artificial Intelligence, Wuhan 430056, Peoples R China.
RP Zhou, J (corresponding author), Jianghan Univ, Sch Artificial Intelligence, Wuhan 430056, Peoples R China.
EM zhj131@jhun.edu.cn
CR Berlin SJ, 2020, J INTELL FUZZY SYST, V39, P961, DOI 10.3233/JIFS-191914
   Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   [陈国军 Chen Guojun], 2021, [电子学报, Acta Electronica Sinica], V49, P331
   Diehl P.U., 2015, 2015 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2015.7280696
   Lei RZ, 2021, KDD '21: PROCEEDINGS OF THE 27TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P892, DOI 10.1145/3447548.3467356
   López-Sastre RJ, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19194062
   Luo YH, 2019, LECT NOTES COMPUT SC, V11901, P118, DOI 10.1007/978-3-030-34120-6_10
   Meerdink S., 2022, IEEE T GEOSCIENCE RE, V60, P1
   Sun X., 2022, PATTERN RECOGN, V124, P1
   Wang YX, 2021, IEEE T COGN DEV SYST, V13, P514, DOI 10.1109/TCDS.2020.2971655
   [徐宁 Xu Ning], 2020, [小型微型计算机系统, Journal of Chinese Computer Systems], V41, P2484
   Yi Y, 2018, MULTIMED TOOLS APPL, V77, P16267, DOI 10.1007/s11042-017-5198-4
   Yiming Zhou, 2020, Frontier Computing. Theory, Technologies and Applications (FC 2019). Lecture Notes in Electrical Engineering (LNEE 551), P913, DOI 10.1007/978-981-15-3250-4_117
   Zhang KH, 2016, IEEE T IMAGE PROCESS, V25, P1779, DOI 10.1109/TIP.2016.2531283
NR 15
TC 0
Z9 0
U1 5
U2 5
PY 2022
BP 6811
EP 6815
UT WOS:000932071606158
DA 2023-11-16
ER

PT C
AU Kasinski, A
   Pawlowski, J
   Ponulak, F
AF Kasinski, Andrzej
   Pawlowski, Juliusz
   Ponulak, Filip
BE Bolc, L
   Kulikowski, JL
   Wojciechowski, K
TI 'SNN3DViewer'-3D Visualization Tool for Spiking Neural Network Analysis
SO COMPUTER VISION AND GRAPHICS
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT International Conference on Computer Vision and Graphics (ICCVG 2008)
CY NOV 10-12, 2008
CL Warsaw, POLAND
DE data visualization; 3D graphics; spiking neural networks
AB We present a specialized visualization system dedicated to support analysis of dynamical processes in large spiking neural networks. The key features of the considered system are: clarity of visual representation, easy accessibility of multiple views and parameters related to the network analysis, advanced and flexible GUI and the system interoperability. In this paper we focus specifically on the implementation issues related to the network 3D representation, design of the graphical objects, visualisation functions and system performance.
C1 [Kasinski, Andrzej; Pawlowski, Juliusz; Ponulak, Filip] Poznan Univ Tech, Inst Control & Informat Engn, Poznan, Poland.
RP Kasinski, A (corresponding author), Poznan Univ Tech, Inst Control & Informat Engn, Poznan, Poland.
EM Andrzej.Kasinski@put.poznan.pl; Juliusz.Pawlowski@gmail.com;
   Filip.Ponulak@put.poznan.pl
CR Brette R, 2007, J COMPUT NEUROSCI, V23, P349, DOI 10.1007/s10827-007-0038-6
   Dunn F, 2002, 3D MATH PRIMER GRAPH
   EDDIE C, GUI SYSTEM MK 2
   Gerstner W., 2002, SPIKING NEURON MODEL
   Lengyel E., 2004, MATH 3D GAME PROGRAM
   Maass W., 1999, PULSED NEURAL NETWOR
   Ware C., 2019, INFORM VISUALIZATION
NR 7
TC 6
Z9 6
U1 0
U2 2
PY 2009
VL 5337
BP 469
EP 476
UT WOS:000271640000046
DA 2023-11-16
ER

PT C
AU Mohemmed, A
   Lu, GY
   Kasabov, N
AF Mohemmed, Ammar
   Lu, Guoyu
   Kasabov, Nikola
BE Huang, T
   Zeng, Z
   Li, C
   Leung, CS
TI Evaluating SPAN Incremental Learning for Handwritten Digit Recognition
SO NEURAL INFORMATION PROCESSING, ICONIP 2012, PT III
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 19th International Conference on Neural Information Processing (ICONIP)
CY NOV 11-15, 2012
CL Doha, QATAR
DE Spiking Neural Networks; Supervised Learning; Nuerocomputing;
   Spatiotemporal pattern recognition
ID SPIKING NEURONS; NEURAL-NETWORK
AB In a previous work [12, 11], the authors proposed SPAN: a learning algorithm based on temporal coding for Spiking Neural Network (SNN). The algorithm trains a neuron to associate target spike patterns to input spatio-temporal spike patterns. In this paper we present the details of experiment to evaluate the feasibility of SPAN learning on a real-world dataset: classifying images of handwritten digits. As spike encoding is an important issue in using SNN for practical applications, we discuss few methods for image conversion to spike patterns. The experiment yields encouraging results to consider the SPAN learning for practical temporal pattern recognition applications.
C1 [Mohemmed, Ammar; Kasabov, Nikola] Auckland Univ Technol, Knowledge Engn Discovery Res Inst, Auckland, New Zealand.
   [Kasabov, Nikola] ETH & Univ Zurich, Inst Neuroinformat, Zurich, Switzerland.
   [Lu, Guoyu] Univ Trento, Dept Informat Engn & Comp Sci, Trento, Italy.
RP Mohemmed, A (corresponding author), Auckland Univ Technol, Knowledge Engn Discovery Res Inst, Auckland, New Zealand.
EM amohemme@aut.ac.nz; nkasabov@aut.ac.nz
CR [Anonymous], IEEE WORLD C COMP IN
   [Anonymous], INT C PAR PROC NEUR
   Brader JM, 2007, NEURAL COMPUT, V19, P2881, DOI 10.1162/neco.2007.19.11.2881
   Delorme A, 1999, NEUROCOMPUTING, V26-7, P989, DOI 10.1016/S0925-2312(99)00095-8
   Delorme A, 2001, NEURAL NETWORKS, V14, P795, DOI 10.1016/S0893-6080(01)00049-1
   Floreano D, 2006, INT J INTELL SYST, V21, P1005, DOI 10.1002/int.20173
   Gerstner W., 2002, SPIKING NEURON MODEL
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   LeCun Y., 1998, MNIST DATABASE HANDW
   Lichtsteiner P., 2006, IEEE INT SOL STAT CI, P2060, DOI DOI 10.1109/ISSCC.2006.1696265
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Mohemmed A, 2011, IFIP ADV INF COMM TE, V363, P219
   Mohemmed A, 2012, INT J NEURAL SYST, V22, DOI 10.1142/S0129065712500128
   Moradi S, 2011, BIOMED CIRC SYST C, P277, DOI 10.1109/BioCAS.2011.6107781
   Nikolic D, 2009, PLOS BIOL, V7, DOI 10.1371/journal.pbio.1000260
   Panchev C, 2006, CONNECT SCI, V18, P1, DOI 10.1080/09540090500132385
   Uysal I, 2008, INT J AP MAT COM-POL, V18, P129, DOI 10.2478/v10006-008-0012-0
   Van Rullen R, 1998, BIOSYSTEMS, V48, P229, DOI 10.1016/S0303-2647(98)00070-7
   Wohrer A, 2009, J COMPUT NEUROSCI, V26, P219, DOI 10.1007/s10827-008-0108-4
   Wysoski SG, 2010, NEURAL NETWORKS, V23, P819, DOI 10.1016/j.neunet.2010.04.009
NR 20
TC 11
Z9 11
U1 0
U2 3
PY 2012
VL 7665
BP 670
EP 677
UT WOS:000345089800081
DA 2023-11-16
ER

PT J
AU Zhao, DC
   Li, Y
   Zeng, Y
   Wang, JH
   Zhang, Q
AF Zhao, Dongcheng
   Li, Yang
   Zeng, Yi
   Wang, Jihang
   Zhang, Qian
TI Spiking CapsNet: A spiking neural network with a biologically plausible
   routing rule between capsules
SO INFORMATION SCIENCES
DT Article
DE Spiking Neural Network; Capsual Neural Netowrk; Biologically Plausible
   Routing; Noise Robustness; Affine Transformation Robustness
AB Spiking neural network (SNN) has attracted much attention due to its powerful spatiotemporal information representation ability. Capsule Neural Network (CapsNet) does well in assembling and coupling features of different network layers. Here, we propose Spiking CapsNet by combining spiking neurons and capsule structures. In addition, we propose a more biologically plausible Spike Timing Dependent Plasticity routing mechanism. The coupling ability is further improved by fully considering the spatio-temporal relationship between spiking capsules of the low layer and the high layer. We have verified experiments on the MNIST, FashionMNIST, and CIFAR10 datasets. Our algorithm still shows comparable performance concerning other excellent SNNs with typical structures (convolutional, fully-connected) on these classification tasks. Our Spiking CapsNet combines SNN and CapsNet's strengths and shows strong robustness to noise and affine transformation. By adding different Salt-Pepper and Gaussian noise to the test dataset, the experimental results demonstrate that our algorithm is more resistant to noise than other approaches. As well, our Spiking CapsNet shows strong generalization to affine transformation on the AffNIST dataset. Our code is available at https://github.com/BrainCog-X/Brain-Cog. (C) 2022 The Author(s). Published by Elsevier Inc.
C1 [Zhao, Dongcheng; Li, Yang; Zeng, Yi; Wang, Jihang; Zhang, Qian] Chinese Acad Sci CASIA, Inst Automat, Beijing, Peoples R China.
   [Li, Yang; Zeng, Yi; Wang, Jihang; Zhang, Qian] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing, Peoples R China.
   [Zhao, Dongcheng; Li, Yang; Zeng, Yi; Wang, Jihang; Zhang, Qian] CASIA, Res Ctr Brain Inspired Intelligence, Beijing, Peoples R China.
   [Zeng, Yi] CASIA, Natl Lab Pattern Recognit, Beijing, Peoples R China.
   [Zeng, Yi] Chinese Acad Sci, Ctr Excellence Brain Sci & Intelligence Technol, Shanghai, Peoples R China.
RP Zeng, Y (corresponding author), Chinese Acad Sci CASIA, Inst Automat, Beijing, Peoples R China.; Zeng, Y (corresponding author), CASIA, Natl Lab Pattern Recognit, Beijing, Peoples R China.; Zeng, Y (corresponding author), Chinese Acad Sci, Ctr Excellence Brain Sci & Intelligence Technol, Shanghai, Peoples R China.
EM zhaodongcheng2016@ia.ac.cn; liyang2019@ia.ac.cn; yi.zeng@ia.ac.cn
CR Ahmed K, 2019, ADV NEUR IN, V32
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Chen ZH, 2018, Arxiv, DOI arXiv:1808.08692
   Cheng X, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1519
   Choi J, 2019, IEEE INT CONF COMP V, P1981, DOI 10.1109/ICCVW.2019.00247
   Chowdhury SS, 2021, NEUROCOMPUTING, V464, P83, DOI 10.1016/j.neucom.2021.07.091
   Ding XP, 2020, IEEE T IMAGE PROCESS, V29, P6789, DOI 10.1109/TIP.2020.2993931
   Dong YT, 2022, Arxiv, DOI [arXiv:2207.02727, 10.48550/arXiv.2207.02727, DOI 10.48550/ARXIV.2207.02727]
   Lenssen JE, 2018, Arxiv, DOI arXiv:1806.05086
   FANG H, 2021, FRONTIERS COMPUTATIO, V15, P8
   Ge CJ, 2017, INFORM SCIENCES, V399, P30, DOI 10.1016/j.ins.2017.03.006
   Hahn T., 2019, ADV NEUR IN, V32, P7658
   Hao YZ, 2020, NEURAL NETWORKS, V121, P387, DOI 10.1016/j.neunet.2019.09.007
   HEBB D. O., 1949
   Hinton G., 2012, BRAIN COGN SCI FALL
   Hinton G.E, 2018, P INT C LEARNING REP
   Tsai YHH, 2020, Arxiv, DOI [arXiv:2002.04764, DOI 10.48550/ARXIV.2002.04764]
   Jin Y., 2018, ADV NEURAL INFORM PR, P7005, DOI DOI 10.48550/ARXIV.1805.07866
   Kingma DP., 2017, ARXIV
   Lee C, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00435
   Li C, 2020, IEEE IJCNN, DOI 10.1109/ijcnn48605.2020.9207019
   Li HY, 2018, LECT NOTES COMPUT SC, V11215, P266, DOI 10.1007/978-3-030-01252-6_16
   Li Y., 2022, ARXIV
   Mazzia V, 2021, Arxiv, DOI arXiv:2101.12491
   Paszke Adam, 2017, AUTOMATIC DIFFERENTI
   Pei J, 2019, NATURE, V572, P106, DOI 10.1038/s41586-019-1424-8
   Ribeiro FD, 2020, AAAI CONF ARTIF INTE, V34, P3749
   Roy K, 2019, NATURE, V575, P607, DOI 10.1038/s41586-019-1677-2
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Sabour S, 2017, ADV NEUR IN, V30
   Shrestha A, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0220344
   Sun YQ, 2021, ISCIENCE, V24, DOI 10.1016/j.isci.2021.102880
   Tavanaei A, 2018, 2018 INT JOINT C NEU, DOI DOI 10.1109/IJCNN.2018.8489104
   Tripathy SJ, 2013, P NATL ACAD SCI USA, V110, P8248, DOI 10.1073/pnas.1221214110
   Wang D, 2018, 25TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2018), DOI 10.14722/ndss.2018.23142
   Wang Y, 2021, INFORM SCIENCES, V581, P932, DOI 10.1016/j.ins.2021.10.001
   Wu YJ, 2019, AAAI CONF ARTIF INTE, P1311
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Zhang AG, 2022, INFORM SCIENCES, V585, P543, DOI 10.1016/j.ins.2021.11.065
   Zhang AG, 2019, NEUROCOMPUTING, V365, P102, DOI 10.1016/j.neucom.2019.07.009
   Zhang ML, 2022, IEEE T NEUR NET LEAR, V33, P1947, DOI 10.1109/TNNLS.2021.3110991
   Zhang SF, 2021, CONCURR COMP-PRACT E, V33, DOI 10.1002/cpe.5281
   Zhang TL, 2018, AAAI CONF ARTIF INTE, P620
   Zhang TL, 2016, IEEE SYS MAN CYBERN, P2301, DOI 10.1109/SMC.2016.7844581
   Zhang W, 2020, ADV NEURAL INFORM PR, V33, P12022, DOI DOI 10.48550/ARXIV.2002.10085
   Zhao DC, 2020, FRONT COMPUT NEUROSC, V14, DOI 10.3389/fncom.2020.576841
   Zhao Dongcheng, 2022, NEURAL NETWORKS
   Zhao FF, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-75628-y
NR 48
TC 9
Z9 9
U1 4
U2 21
PD SEP
PY 2022
VL 610
BP 1
EP 13
DI 10.1016/j.ins.2022.07.152
EA AUG 2022
UT WOS:000848341500001
DA 2023-11-16
ER

PT J
AU Aghabarar, H
   Kiani, K
   Keshavarzi, P
AF Aghabarar, Hedyeh
   Kiani, Kourosh
   Keshavarzi, Parviz
TI Improvement of pattern recognition in spiking neural networks by
   modifying threshold parameter and using image inversion
SO MULTIMEDIA TOOLS AND APPLICATIONS
DT Article; Early Access
DE Spiking neural networks (SNNs); Current-based LIF neuron model;
   Threshold voltage parameter; Constant-current-LIF encoding
ID INTEGRATE-AND-FIRE; OBJECT; BACKPROPAGATION; DYNAMICS; REWARD
AB In recent years, spiking neural networks (SNNs) have gained popularity as a biologically plausible and energy-efficient alternative to artificial neural networks. Unlike non-spiking networks, SNNs use asynchronous, scattered pulses to communicate between spiking neurons, making them suitable for portable systems with limited hardware and energy resources. In this paper, we propose a feed-forward spiking neural network (SNN) with two spiking convolutional layers, one spiking classification layer, and a single non-spiking leaky integrator for readout. This network uses reverse pixel values and a type of time encoding known as constant-current-leaky integrate and fire (LIF) coding. Here, for the first time, the idea of adjusting the threshold voltage parameter of network neurons and inverting input images is introduced to improve recognition accuracy. We demonstrate that decreasing the threshold voltage parameter below its default voltage and optimizing input images by inverting pixel values can improve the recognition accuracy of the network. Using this approach, we test the network on the popular MNIST and Fashion-MNIST benchmarks, achieving test accuracies of 99.28% and 90.43%, respectively. We also test our approach on models from several studies and different datasets to prove its validity and generality. Results show that our proposed approach is effective in improving recognition accuracy compared to the initial accuracies of the models.
C1 [Aghabarar, Hedyeh; Kiani, Kourosh; Keshavarzi, Parviz] Semnan Univ, Fac Elect & Comp Engn, Semnan, Iran.
RP Kiani, K (corresponding author), Semnan Univ, Fac Elect & Comp Engn, Semnan, Iran.
EM h.aqabarar@semnan.ac.ir; Kourosh.kiani@semnan.ac.ir;
   pkeshavarzi@semnan.ac.ir
CR Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   Alom MZ, 2019, ELECTRONICS-SWITZ, V8, DOI 10.3390/electronics8030292
   Amiri R., 2018, P IEEE INT C COMM IC, P1
   Basu A, 2018, IEEE J EM SEL TOP C, V8, P6, DOI 10.1109/JETCAS.2018.2816339
   Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Bouwmans T, 2019, NEURAL NETWORKS, V117, P8, DOI 10.1016/j.neunet.2019.04.024
   Brette R, 2004, J MATH BIOL, V48, P38, DOI 10.1007/s00285-003-0223-9
   Burkitt AN, 2006, BIOL CYBERN, V95, P1, DOI 10.1007/s00422-006-0068-6
   Daw ND, 2006, CURR OPIN NEUROBIOL, V16, P199, DOI 10.1016/j.conb.2006.03.006
   Dayan P, 2002, NEURON, V36, P285, DOI 10.1016/S0896-6273(02)00963-7
   Detorakis G, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00583
   Fang W, 2021, INCORPORATING LEARNA, P2661
   Fatahi M, 2014, MNIST HANDWRITTEN DI, DOI [10.13140/2.1.4601.1681, DOI 10.13140/2.1.4601.1681]
   Frenkel C, 2019, PROC IEEE INT S CIRC, P1, DOI 10.1109/ISCAS.2019.8702793
   Frenkel C, 2019, IEEE T BIOMED CIRC S, V13, P999, DOI 10.1109/TBCAS.2019.2928793
   Frenkel C, 2019, IEEE T BIOMED CIRC S, V13, P145, DOI 10.1109/TBCAS.2018.2880425
   Frenkel C, 2017, BIOMED CIRC SYST C
   Frenkel C, 2017, IEEE INT SYMP CIRC S, P17
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   Gerstner W, 2002, SPIKING NEURON MODEL, DOI [10.1017/CBO9780511815706, DOI 10.1017/CBO9780511815706]
   Göltz J, 2021, NAT MACH INTELL, V3, P823, DOI 10.1038/s42256-021-00388-x
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   KEENER JP, 1981, SIAM J APPL MATH, V41, P503, DOI 10.1137/0141042
   Kheradpisheh SR, 2022, IEEE ACCESS, V10, P70769, DOI 10.1109/ACCESS.2022.3187033
   Kheradpisheh SR, 2022, NEURAL PROCESS LETT, V54, P1255, DOI 10.1007/s11063-021-10680-x
   Kheradpisheh SR, 2020, INT J NEURAL SYST, V30, DOI 10.1142/S0129065720500276
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Kheradpisheh SR, 2016, NEUROCOMPUTING, V205, P382, DOI 10.1016/j.neucom.2016.04.029
   Koch C., 1998, METHODS NEURONAL MOD
   Kornijcuk V, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00212
   Krizhevsky Alex, 2009, LEARNING MULTIPLE LA
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee C, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00435
   Lee JS, 2023, IEEE ACCESS, V11, P35140, DOI 10.1109/ACCESS.2023.3264435
   Liu WB, 2017, NEUROCOMPUTING, V234, P11, DOI 10.1016/j.neucom.2016.12.038
   Lobo JL, 2020, NEURAL NETWORKS, V121, P88, DOI 10.1016/j.neunet.2019.09.004
   Mirsadeghi M, 2021, NEUROCOMPUTING, V427, P131, DOI 10.1016/j.neucom.2020.11.052
   Mohamed SA., 2019, SCI, V15, P1392, DOI [10.11591/ijeecs.v15.i3.pp1392-1400, DOI 10.11591/IJEECS.V15.I3.PP1392-1400]
   Mozafari M, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00625
   Mozafari M, 2019, PATTERN RECOGN, V94, P87, DOI 10.1016/j.patcog.2019.05.015
   Mozafari M, 2018, IEEE T NEUR NET LEAR, V29, P6178, DOI 10.1109/TNNLS.2018.2826721
   Niv Y, 2009, J MATH PSYCHOL, V53, P139, DOI 10.1016/j.jmp.2008.12.005
   Pehle C, 2021, NORSE LIB DEEP LEARN
   Perez-Peña F, 2020, NEUROCOMPUTING, V371, P91, DOI 10.1016/j.neucom.2019.09.004
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Ponulak F, 2011, ACTA NEUROBIOL EXP, V71, P409
   Rafi TH, 2021, Preprints, DOI [10.20944/preprints202104.0202.v1, 10.20944/preprints202104.0202.v1, DOI 10.20944/PREPRINTS202104.0202.V1]
   Sanchez-Garcia M, 2023, BIOL CYBERN, V117, P95, DOI 10.1007/s00422-023-00956-x
   Schumann CL, 2019, AIDS BEHAV, V23, P5, DOI 10.1007/s10461-017-1727-4
   Seo JS, 2015, IEEE INT CONF VLSI, P49, DOI 10.1109/VLSI-SoC.2015.7314390
   Shour I, 2018, RECONFIG
   Srinivasan G, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00189
   Standage DI, 2005, IEEE IJCNN, P396
   Sze V, 2017, P IEEE, V105, P2295, DOI 10.1109/JPROC.2017.2761740
   Taherkhani A, 2020, NEURAL NETWORKS, V122, P253, DOI 10.1016/j.neunet.2019.09.036
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128
   Vaila R, 2022, IEEE TETCI, V6, P124, DOI 10.1109/TETCI.2020.3035164
   Van Rullen R, 2001, NEURAL COMPUT, V13, P1255, DOI 10.1162/08997660152002852
   Vazquez Roberto A., 2010, 2010 7th International Conference on Electrical Engineering, Computing Science and Automatic Control (CCE 2010) (Formerly known as ICEEE), P424, DOI 10.1109/ICEEE.2010.5608622
   Vigneron A., 2020, INT JOINT C NEURAL N, V2020, P1, DOI DOI 10.1109/IJCNN48605.2020.9207239
   Wang XW, 2020, NEURAL NETWORKS, V125, P258, DOI 10.1016/j.neunet.2020.02.011
   Xiao H, 2017, Arxiv, DOI arXiv:1708.07747
   Xiao M, 2021, ADV NEURAL INFORM PR, V34, P14516, DOI [10.48550/arXiv.2109.14247, DOI 10.48550/ARXIV.2109.14247]
   Xiao MQ, 2023, NEURAL NETWORKS, V161, P9, DOI 10.1016/j.neunet.2023.01.026
   Xu CQ, 2022, SYMMETRY-BASEL, V14, DOI 10.3390/sym14091933
   Zhang AG, 2022, IEEE T NEUR NET LEAR, V33, P1986, DOI 10.1109/TNNLS.2021.3084955
   Zhang W, 2020, ADV NEURAL INFORM PR, V33, P12022, DOI DOI 10.48550/ARXIV.2002.10085
NR 70
TC 0
Z9 0
U1 1
U2 1
PD 2023 JUL 25
PY 2023
DI 10.1007/s11042-023-16344-3
EA JUL 2023
UT WOS:001036798900005
DA 2023-11-16
ER

PT J
AU Zhe, S
   Micheletto, R
AF Zhe, Sun
   Micheletto, Ruggero
TI Noise influence on spike activation in a Hindmarsh-Rose small-world
   neural network
SO JOURNAL OF PHYSICS A-MATHEMATICAL AND THEORETICAL
DT Article
DE neural networks; noise; stochastic processes; small-world networks;
   discrete Fourier analysis
ID MODEL; SYNCHRONIZATION; ARRAY
AB We studied the role of noise in neural networks, especially focusing on its relation to the propagation of spike activity in a small sized system. We set up a source of information using a single neuron that is constantly spiking. This element called initiator x(o) feeds spikes to the rest of the network that is initially quiescent and subsequently reacts with vigorous spiking after a transitional period of time. We found that noise quickly suppresses the initiator's influence and favors spontaneous spike activity and, using a decibel representation of noise intensity, we established a linear relationship between noise amplitude and the interval from the initiator's first spike and the rest of the network activation. We studied the same process with networks of different sizes (number of neurons) and found that the initiator xo has a measurable influence on small networks, but as the network grows in size, spontaneous spiking emerges disrupting its effects on networks of more than about N = 100 neurons. This suggests that the mechanism of internal noise generation allows information transmission within a small neural neighborhood, but decays for bigger network domains. We also analyzed the Fourier spectrum of the whole network membrane potential and verified that noise provokes the reduction of main. and a peaks before transitioning into chaotic spiking. However, network size does not reproduce a similar phenomena; instead we recorded a reduction in peaks' amplitude, a better sharpness and definition of Fourier peaks, but not the evident degeneration to chaos observed with increasing external noise. This work aims to contribute to the understanding of the fundamental mechanisms of propagation of spontaneous spiking in neural networks and gives a quantitative assessment of how noise can be used to control and modulate this phenomenon in Hindmarsh-Rose (H-R) neural networks.
C1 [Zhe, Sun; Micheletto, Ruggero] Yokohama City Univ, 22-2 Seto,Kanazawa Ward, Yokohama, Kanagawa 232, Japan.
   [Zhe, Sun] Riken Brain Sci Inst, 2-1 Hirosawa, Wako, Saitama, Japan.
   [Micheletto, Ruggero] Harvard Univ, 64 Sidney St,Suite 170, Cambridge, MA 02139 USA.
RP Zhe, S (corresponding author), Yokohama City Univ, 22-2 Seto,Kanazawa Ward, Yokohama, Kanagawa 232, Japan.; Zhe, S (corresponding author), Riken Brain Sci Inst, 2-1 Hirosawa, Wako, Saitama, Japan.
EM n145301c@yokohama-cu.ac.jp
CR Amitay S, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0068928
   Balev S, 2012, P EUR C COMPL SYST 2, DOI [10.1007/978-3-319-00395-5, DOI 10.1007/978-3-319-00395-5]
   Baltanás JP, 2002, PHYS REV E, V65, DOI 10.1103/PhysRevE.65.041915
   Bassett DS, 2006, NEUROSCIENTIST, V12, P512, DOI 10.1177/1073858406293182
   Belykh T, 2005, INT J BIFURCAT CHAOS, V15, P3423, DOI 10.1142/S0218127405014143
   DeBolt D T, 2011, THESIS
   Destexhe A, 2012, SPR SER COMPUT NEURO, V8, P1, DOI 10.1007/978-0-387-79020-6
   HINDMARSH JL, 1984, PROC R SOC SER B-BIO, V221, P87, DOI 10.1098/rspb.1984.0024
   Huber MT, 2007, BIOSYSTEMS, V89, P38, DOI 10.1016/j.biosystems.2006.05.009
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2008, P NATL ACAD SCI USA, V105, P3593, DOI 10.1073/pnas.0712231105
   Jones E, 2011, SCIPY OPEN SOURCE SC
   Longtin A, 2003, INTERD APPL, V25, P149
   Masuda N, 2004, PHYS REV E, V69, DOI 10.1103/PhysRevE.69.031917
   Medvedev GS, 2012, BIOL CYBERN, V106, P67, DOI 10.1007/s00422-012-0481-y
   Newman MEJ, 1999, PHYS LETT A, V263, P341, DOI 10.1016/S0375-9601(99)00757-4
   Newman MEJ, 1999, PHYS REV E, V60, P7332, DOI 10.1103/PhysRevE.60.7332
   Newman MEJ, 2003, SIAM REV, V45, P167, DOI 10.1137/S003614450342480
   Ozer M, 2009, PHYS LETT A, V373, P964, DOI 10.1016/j.physleta.2009.01.034
   Palva S, 2007, TRENDS NEUROSCI, V30, P150, DOI 10.1016/j.tins.2007.02.001
   Sherwood WE, 2010, SIAM J APPL DYN SYST, V9, P659, DOI 10.1137/090773519
   Shilnikov A, 2008, INT J BIFURCAT CHAOS, V18, P2141, DOI 10.1142/S0218127408021634
   Simonotto E, 1997, PHYS REV LETT, V78, P1186, DOI 10.1103/PhysRevLett.78.1186
   Song S, 2014, NAT NEUROSCI, V3, P919
   Tang Y, 2013, PHYS REV E, V87, DOI 10.1103/PhysRevE.87.062920
   van der Walt S, 2011, COMPUT SCI ENG, V13, P22, DOI 10.1109/MCSE.2011.37
   Wang CN, 2010, COMMUN THEOR PHYS, V53, P382, DOI 10.1088/0253-6102/53/2/32
   Wang YQ, 2000, J PHYS SOC JPN, V69, P276, DOI 10.1143/JPSJ.69.276
   Watts DJ, 1998, NATURE, V393, P440, DOI 10.1038/30918
   Wolfe JM, 2000, NATURE, V406, P691, DOI 10.1038/35021132
   Wu Y, 2005, CHAOS SOLITON FRACT, V23, P1605, DOI 10.1016/j.chaos.2004.06.077
   Yan R, 2006, LECT NOTES COMPUT SC, V3972, P498
   Yu H J, 2005, ACTA BIOPHYS SIN, V4
   Yu HT, 2011, CHAOS, V21, DOI 10.1063/1.3565027
   Yu S, 2008, CEREB CORTEX, V18, P2891, DOI 10.1093/cercor/bhn047
   Zheng YH, 2008, PHYSICA A, V387, P3719, DOI 10.1016/j.physa.2008.02.039
NR 36
TC 2
Z9 2
U1 0
U2 8
PD JUL 15
PY 2016
VL 49
IS 28
AR 285601
DI 10.1088/1751-8113/49/28/285601
UT WOS:000377481500012
DA 2023-11-16
ER

PT C
AU Pan, L
   Christophe, F
   Mikkonen, T
   Li, Z
   Bhattacharyya, SS
AF Pan, Lei
   Christophe, Francois
   Mikkonen, Tommi
   Li, Zhu
   Bhattacharyya, Shuvra S.
GP IEEE
TI Simulating Spiking Neural Networks with Timed Dataflow Graphs
SO 2020 2ND IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE
   CIRCUITS AND SYSTEMS (AICAS 2020)
DT Proceedings Paper
CT 2nd IEEE International Conference on Artificial Intelligence Circuits
   and Systems (AICAS)
CY AUG 31-SEP 04, 2020
CL ELECTR NETWORK
DE Dataflow; model-based design; simulation; spiking neural networks
ID MODEL; TOOL
AB This article presents a novel approach for simulating Spiking Neural Networks (SNNs) that is based on timed dataflow graphs. Whereas conventional SNN simulators compute changes in spiking neuron variables at each time step, the proposed simulation approach focuses on evaluating spike timings. This focus on evaluating when a dataflow actor (spiking neuron) reaches a new spike contributes to making spike evaluation an event-driven computation. The resulting event-driven simulation approach avoids unnecessary computations at time steps that lie between spiking events. This optimization is achieved while avoiding the large overheads associated with lookup tables that are incurred in existing event-driven approaches. Our results show identical spiking behavior compared to simulation using a conventional (time-based) simulator while providing significant improvement in execution time.
C1 [Pan, Lei; Bhattacharyya, Shuvra S.] Univ Maryland, Dept ECE, College Pk, MD 20742 USA.
   [Pan, Lei; Bhattacharyya, Shuvra S.] Univ Maryland, UMIACS, College Pk, MD 20742 USA.
   [Christophe, Francois] HAMK Univ Appl Sci, HAMK Tech, Hameenlinna, Finland.
   [Mikkonen, Tommi] Univ Helsinki, Dept Comp Sci, Helsinki, Finland.
   [Li, Zhu] Univ Missouri, Dept Comp Sci & Elect Engn, Kansas City, MO 64110 USA.
RP Pan, L (corresponding author), Univ Maryland, Dept ECE, College Pk, MD 20742 USA.; Pan, L (corresponding author), Univ Maryland, UMIACS, College Pk, MD 20742 USA.
CR Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   [Anonymous], 2010, P WIRELESS INNOVATIO
   Bhattacharyya S. S., 2019, HDB SIGNAL PROCESSIN, V3rd
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Brette R, 2005, J NEUROPHYSIOL, V94, P3637, DOI 10.1152/jn.00686.2005
   Cheung K, 2016, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00516
   Eppler Jochen Martin, 2008, Front Neuroinform, V2, P12, DOI 10.3389/neuro.11.012.2008
   Geng J., 2018, P IEEE INT WORKSH FA, P1, DOI DOI 10.1145/3229543.3229544
   Goodman Dan, 2008, Front Neuroinform, V2, P5, DOI 10.3389/neuro.11.005.2008
   Hines ML, 2001, NEUROSCIENTIST, V7, P123, DOI 10.1177/107385840100700207
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Koch C., 1998, METHODS NEURONAL MOD
   LEE EA, 1995, P IEEE, V83, P773, DOI 10.1109/5.381846
   Li L, 2019, 2019 IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE CIRCUITS AND SYSTEMS (AICAS 2019), P112, DOI [10.1109/AICAS.2019.8771476, 10.1109/aicas.2019.8771476]
   Naveros F, 2017, FRONT NEUROINFORM, V11, DOI 10.3389/fninf.2017.00007
   Painkras E, 2013, IEEE J SOLID-ST CIRC, V48, P1943, DOI 10.1109/JSSC.2013.2259038
   Stimberg M, 2014, FRONT NEUROINFORM, V8, DOI 10.3389/fninf.2014.00006
NR 17
TC 0
Z9 0
U1 0
U2 0
PY 2020
BP 64
EP 68
UT WOS:000720328700014
DA 2023-11-16
ER

PT J
AU Song, T
   Zeng, XX
   Zheng, P
   Jiang, M
   Rodríguez-Patón, A
AF Song, Tao
   Zeng, Xiangxiang
   Zheng, Pan
   Jiang, Min
   Rodriguez-Paton, Alfonso
TI A Parallel Workflow Pattern Modeling Using Spiking Neural P Systems With
   Colored Spikes
SO IEEE TRANSACTIONS ON NANOBIOSCIENCE
DT Article
DE Membrane computing; spiking neural P system; Petri net; colored spike;
   workflow pattern
ID HIGH-SPEED TRAINS; SYNAPSES WORKING; NETWORKS; NEURONS; INFORMATION;
   RULES
AB Spiking neural P systems, otherwise known as named SN P systems, are bio-inspired parallel and distributed neural-like computing models. Due to the spiking behavior, SN P systems fall into the category of spiking neural networks, and are considered to be an auspicious candidate of the 3G of neural networks. It has been reported that SN P systems with colored spikes are computationally capable, and perform well in describing behaviors of complex systems. Nonetheless, some practical issue is open to be investigate, such as workflow and traffic flow modeling. In this paper, a parallel workflow pattern modeling using SN P systems with colored spikes is proposed. As results, 20 designs are constructed using SN P systems for 20 classical workflow patterns. The functioning processes that operate both sequentially and simultaneously in the workflow pattern are able to be modeled and simulated. SN P systems with colored spikes have some similarity with Petri nets, hence can be used to model workflow patterns. This will provide a novel neural- like modeling method for modeling traffic flow.
C1 [Song, Tao] China Univ Petr, Coll Comp & Commun Engn, Qingdao 266580, Peoples R China.
   [Zeng, Xiangxiang; Jiang, Min] Xiamen Univ, Sch Informat Sci & Technol, Xiamen 361005, Peoples R China.
   [Zheng, Pan] Univ Canterbury, Dept Accounting & Informat Syst, Christchurch 8041, New Zealand.
   [Rodriguez-Paton, Alfonso] Univ Politecn Madrid, Dept Inteligencia Artificial, Campus Montegancedo, E-28660 Madrid, Spain.
RP Zeng, XX (corresponding author), Xiamen Univ, Sch Informat Sci & Technol, Xiamen 361005, Peoples R China.
EM xzeng@xmu.edu.cn
CR [Anonymous], NANOCOMMUN NETW
   [Anonymous], 2004, OXFORD HDB COMPUTATI
   [Anonymous], 2010, SPIKING NEURAL P SYS
   Anthony M., 2009, NEURAL NETWORK LEARN
   Binder A., 2007, P 5 BRAINST WEEK MEM, P63
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Buibas M., 2017, Neural network learning and collaboration apparatus and methods, Patent No. [U. S. Patent 9 208 432, 9208432]
   Cabarle F. G. C., 2012, LECT NOTES COMPUTER, V7762
   Cavaliere M, 2009, THEOR COMPUT SCI, V410, P2352, DOI 10.1016/j.tcs.2009.02.031
   Chavez-Garcia RO, 2016, IEEE T INTELL TRANSP, V17, P525, DOI 10.1109/TITS.2015.2479925
   Chen HM, 2007, FUND INFORM, V75, P141
   Dayhoff JE, 2001, CANCER, V91, P1615, DOI 10.1002/1097-0142(20010415)91:8+<1615::AID-CNCR1175>3.0.CO;2-L
   Deco G, 1998, NETWORK-COMP NEURAL, V9, P303, DOI 10.1088/0954-898X/9/3/002
   Eurich CW, 2000, NEURAL COMPUT, V12, P1519, DOI 10.1162/089976600300015240
   Gao RZ, 2016, INFORM SCIENCES, V367, P449, DOI 10.1016/j.ins.2016.05.033
   Gerstner W., 2002, SPIKING NEURON MODEL
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Giroire F, 2018, IEEE T INTELL TRANSP, V19, P1166, DOI 10.1109/TITS.2017.2718737
   Hagan MT., 1997, NEURAL NETWORK DESIG
   Han S., 2015, C NEUR INF PROC SYST
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Ibarra OH, 2009, THEOR COMPUT SCI, V410, P2982, DOI 10.1016/j.tcs.2009.03.004
   IONESCU M, 2007, P 8 WORKSH MEMBR COM, P383
   Ionescu M, 2007, INT J UNCONV COMPUT, V3, P135
   Ionescu M, 2006, FUND INFORM, V71, P279
   Ionescu M, 2008, COMPUT INFORM, V27, P515
   Ishdorj TO, 2010, THEOR COMPUT SCI, V411, P2345, DOI 10.1016/j.tcs.2010.01.019
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Jo HJ, 2018, IEEE T INTELL TRANSP, V19, P1065, DOI 10.1109/TITS.2017.2712772
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
   Krogh A., 1995, Advances in Neural Information Processing Systems 7, P231
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Leporati Alberto, 2009, Natural Computing, V8, P681, DOI 10.1007/s11047-008-9091-y
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Macias-Ramos L., 2012, LECT NOTES COMPUTER, P228
   Metta VP, 2012, NEW MATH NAT COMPUT, V8, P283, DOI 10.1142/S1793005712500032
   Pan LQ, 2011, SCI CHINA INFORM SCI, V54, P1596, DOI 10.1007/s11432-011-4303-y
   Pan LQ, 2009, INT J COMPUT COMMUN, V4, P273, DOI 10.15837/ijccc.2009.3.2435
   Paum G, 2007, J UNIVERS COMPUT SCI, V13, P1707
   Paun A, 2007, BIOSYSTEMS, V90, P48, DOI 10.1016/j.biosystems.2006.06.006
   Paun Gh, 2010, OXFORD HDB MEMBRANE
   Peng H, 2013, INFORM SCIENCES, V235, P106, DOI 10.1016/j.ins.2012.07.015
   Rodríguez VD, 2018, IEEE T INTELL TRANSP, V19, P1227, DOI 10.1109/TITS.2017.2749966
   ROSENBLATT F, 1958, PSYCHOL REV, V65, P386, DOI 10.1037/h0042519
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Segev A, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0158590
   Sirmatel II, 2018, IEEE T INTELL TRANSP, V19, P1112, DOI 10.1109/TITS.2017.2716541
   Song D., IEEE T INTELL TRANSP
   Song T., IEEE T COGN DEV SYST
   Song T, 2016, INFORM SCIENCES, V372, P380, DOI 10.1016/j.ins.2016.08.055
   Song T, 2015, IEEE T NANOBIOSCI, V14, P465, DOI 10.1109/TNB.2015.2402311
   Song T, 2015, IEEE T NANOBIOSCI, V14, P38, DOI 10.1109/TNB.2014.2367506
   Song T, 2015, NEURAL PROCESS LETT, V42, P199, DOI 10.1007/s11063-014-9352-y
   Song T, 2014, NEURAL COMPUT APPL, V24, P1833, DOI 10.1007/s00521-013-1397-8
   Song T, 2013, IEEE T NANOBIOSCI, V12, P255, DOI 10.1109/TNB.2013.2271278
   Song T, 2013, INFORM SCIENCES, V219, P197, DOI 10.1016/j.ins.2012.07.023
   SPECHT DF, 1990, NEURAL NETWORKS, V3, P109, DOI 10.1016/0893-6080(90)90049-Q
   Tang HY, 2018, IEEE T INTELL TRANSP, V19, P1027, DOI 10.1109/TITS.2017.2710138
   Van der Aalst WMP, 2003, DISTRIB PARALLEL DAT, V14, P5, DOI 10.1023/A:1022883727209
   Wang J, 2010, NEURAL COMPUT, V22, P2615, DOI 10.1162/NECO_a_00022
   Wang T, 2015, IEEE T POWER SYST, V30, P1182, DOI 10.1109/TPWRS.2014.2347699
   Wu P, 2018, IEEE T INTELL TRANSP, V19, P1140, DOI 10.1109/TITS.2017.2717188
   Zeng XX, 2012, IEEE T NANOBIOSCI, V11, P366, DOI 10.1109/TNB.2012.2211034
   Zeng XX, 2009, FUND INFORM, V97, P275, DOI 10.3233/FI-2009-200
   Zhang XC, 2012, INT J ADV COMPUT SC, V3, P1
   Zhang XY, 2014, NEURAL COMPUT, V26, P2925, DOI 10.1162/NECO_a_00665
   Zhang XY, 2014, INFORM SCIENCES, V278, P285, DOI 10.1016/j.ins.2014.03.053
   Zhang XY, 2014, NEURAL COMPUT, V26, P974, DOI 10.1162/NECO_a_00580
   Zhang Y, 2014, INT J NEURAL SYST, V24, DOI 10.1142/S0129065714500038
   Zhao Z., IEEE T INTELL TRANSP
NR 70
TC 55
Z9 55
U1 1
U2 29
PD OCT
PY 2018
VL 17
IS 4
BP 474
EP 484
DI 10.1109/TNB.2018.2873221
UT WOS:000450357100014
DA 2023-11-16
ER

PT C
AU Wu, QX
   Cai, RT
   McGinnity, TM
   Maguire, L
   Harkin, J
AF Wu, QingXiang
   Cai, Rongtai
   McGinnity, T. M.
   Maguire, Liam
   Harkin, Jim
BE Qiu, PH
   Yiu, C
   Zhang, H
   Wen, XB
TI Remembering Key Features of Visual Images based on Spike Timing
   Dependent Plasticity of Spiking Neurons
SO PROCEEDINGS OF THE 2009 2ND INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL
   PROCESSING, VOLS 1-9
DT Proceedings Paper
CT 2nd International Congress on Image and Signal Processing
CY OCT 17-19, 2009
CL Tianjin, PEOPLES R CHINA
DE Spiking Neural networks; spike timing dependent plasticity; ON/OFF
   pathways; visual system
AB The brain has the powerful capability of remembering key features of images. Based on the principle of spike timing dependent plasticity of spiking neurons and the ON/OFF pathways in the visual system, a spiking neural network is proposed to remember key features of visual images. The simulation results show that the network is capable of remembering key features according to a learning rule based on spike timing dependent plasticity. The principle of the network can be used to explain how a spiking neuron-based system can store the key features of visual images. Furthermore, the network can be applied to spiking neuron based artificial intelligent systems to support the processing visual images.
C1 [Wu, QingXiang; Cai, Rongtai] Fujian Normal Univ, Sch Phys & OptoElect Technol, Fuzhou 350007, Fujian, Peoples R China.
   [McGinnity, T. M.; Maguire, Liam; Harkin, Jim] Univ Ulster, Intelligent Syst Res Ctr, Coleraine BT52 1SA, Londonderry, North Ireland.
RP Wu, QX (corresponding author), Fujian Normal Univ, Sch Phys & OptoElect Technol, Fuzhou 350007, Fujian, Peoples R China.
EM qxwu@fjnu.edu.cn; rtcai@fjnu.edu.cn; tm.mcginnity@ulster.ac.uk;
   lp.maguire@ulster.ac.uk; jg.harkin@ulster.ac.uk
CR AHMED N, 1974, IEEE T COMPUT, VC 23, P90, DOI 10.1109/T-C.1974.223784
   [Anonymous], 2000, J NEUROSCI
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Dayan P., 2001, THEORETICAL NEUROSCI
   Gerstner W., 2002, SPIKING NEURON MODEL
   Jessell T. M, 1981, PRINCIPLES NEURAL SC
   Koch Christof, 1999, P1
   MULLER E, 2003, HDKIP0322 U HEID
   Nelson R, 2003, VISUAL NEUROSCIENCES, P260
   Song S, 2001, NEURON, V32, P339, DOI 10.1016/S0896-6273(01)00451-2
   Wu QX, 2007, STUD COMPUT INTELL, V35, P171
   Wu QX, 2008, NEURAL NETWORKS, V21, P1318, DOI 10.1016/j.neunet.2008.05.014
NR 12
TC 1
Z9 1
U1 0
U2 0
PY 2009
BP 2168
EP 2172
UT WOS:000280804301106
DA 2023-11-16
ER

PT C
AU Guo, N
   Xiao, R
   Gao, SB
   Tang, HJ
AF Guo, Na
   Xiao, Rong
   Gao, Shaobing
   Tang, Huajin
GP IEEE
TI A Neurally Inspired Pattern Recognition Approach with Latency-Phase
   Encoding and Precise-Spike-Driven Rule in Spiking Neural Network
SO 2017 IEEE INTERNATIONAL CONFERENCE ON CYBERNETICS AND INTELLIGENT
   SYSTEMS (CIS) AND IEEE CONFERENCE ON ROBOTICS, AUTOMATION AND
   MECHATRONICS (RAM)
DT Proceedings Paper
CT IEEE International Conference on Cybernetics and Intelligent Systems
   (CIS) / IEEE Conference on Robotics, Automation and Mechatronics (RAM)
CY NOV 19-21, 2017
CL Ningbo, PEOPLES R CHINA
DE Spiking Neural Networks (SNNs); Pattern Recognition; Latency-Phase
   Encoding; Precise-Spike-Driven (PSD)
ID COLOR CONSTANCY; INFORMATION; CELLS; OSCILLATIONS
AB Recent years, several advances have been made in spiking neural networks (SNNs). In this paper, we introduce a novel approach that uses both the latency-phase encoding and the precise-spike-driven (PSD) learning rule for pattern recognition. The empirical results show that our proposed approach can perform quite competitive in comparison to other commonly used encoding and learning strategies in SNNs, when evaluating on the gray OCR dataset and the Address-Event-Representation (AER) motion dataset for image classification.
C1 [Guo, Na; Xiao, Rong; Gao, Shaobing; Tang, Huajin] Sichuan Univ, Coll Comp Sci, Chengdu, Sichuan, Peoples R China.
RP Gao, SB; Tang, HJ (corresponding author), Sichuan Univ, Coll Comp Sci, Chengdu, Sichuan, Peoples R China.
EM guona.work@foxmail.com; rxiao.@stu.scu.edu.cn; gaoshaobing@scu.edu.cn;
   htang@scu.edu.cn
CR [Anonymous], 1990, ADV NEURAL INFORM PR
   Baden T, 2011, CURR BIOL, V21, P1859, DOI 10.1016/j.cub.2011.09.042
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Brader JM, 2007, NEURAL COMPUT, V19, P2881, DOI 10.1162/neco.2007.19.11.2881
   Carr Catherine E., 1996, Advances in Psychology, V115, P27
   Chrobak JJ, 1998, J NEUROSCI, V18, P388
   Dreosti E, 2011, NAT NEUROSCI, V14, P951, DOI 10.1038/nn.2841
   Florian RV, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0040233
   Gao SB, 2017, J OPT SOC AM A, V34, P1448, DOI 10.1364/JOSAA.34.001448
   Gao SB, 2015, IEEE T PATTERN ANAL, V37, P1973, DOI 10.1109/TPAMI.2015.2396053
   Gao SB, 2014, LECT NOTES COMPUT SC, V8690, P158, DOI 10.1007/978-3-319-10605-2_11
   Gao SB, 2013, IEEE I CONF COMP VIS, P929, DOI 10.1109/ICCV.2013.119
   Gollisch T, 2008, SCIENCE, V319, P1108, DOI 10.1126/science.1149639
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Heiligenberg W., 1991, NEURAL NETS ELECT FI
   Hill AV, 1929, NATURE, V123, P9
   Hu J, 2013, 2013 IEEE SYMPOSIUM ON COMPUTATIONAL INTELLIGENCE, COGNITIVE ALGORITHMS, MIND, AND BRAIN (CCMB), P23, DOI 10.1109/CCMB.2013.6609161
   Jensen O, 2001, NEURAL COMPUT, V13, P2743, DOI 10.1162/089976601317098510
   Kayser C, 2009, NEURON, V61, P597, DOI 10.1016/j.neuron.2009.01.008
   Koepsell K, 2009, FRONT SYST NEUROSCI, V3, DOI 10.3389/neuro.06.004.2009
   LLINAS RR, 1991, P NATL ACAD SCI USA, V88, P897, DOI 10.1073/pnas.88.3.897
   Mohemmed A, 2012, INT J NEURAL SYST, V22, DOI 10.1142/S0129065712500128
   O'Keefe J, 2005, HIPPOCAMPUS, V15, P853, DOI 10.1002/hipo.20115
   Ponulak F., 2005, RESUME NEW SUPERVISE
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   SINGER W, 1995, ANNU REV NEUROSCI, V18, P555, DOI 10.1146/annurev.ne.18.030195.003011
   Tsodyks MV, 1996, HIPPOCAMPUS, V6, P271, DOI 10.1002/(SICI)1098-1063(1996)6:3<271::AID-HIPO5>3.3.CO;2-Q
   van Rossum MCW, 2001, NEURAL COMPUT, V13, P751, DOI 10.1162/089976601300014321
   Victor JD, 2000, BRAIN RES, V886, P33, DOI 10.1016/S0006-8993(00)02751-7
   WIDROW B, 1990, P IEEE, V78, P1415, DOI 10.1109/5.58323
   Yu Q, 2015, PROCEEDINGS OF THE 18TH ASIA PACIFIC SYMPOSIUM ON INTELLIGENT AND EVOLUTIONARY SYSTEMS, VOL 2, P65, DOI 10.1007/978-3-319-13356-0_6
   Yu Q, 2013, IEEE T NEUR NET LEAR, V24, P1539, DOI 10.1109/TNNLS.2013.2245677
   Zhang XS, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00151
   Zhang XS, 2016, IEEE T IMAGE PROCESS, V25, P1219, DOI 10.1109/TIP.2016.2516953
   Zhao B, 2015, IEEE T NEUR NET LEAR, V26, P1963, DOI 10.1109/TNNLS.2014.2362542
NR 35
TC 1
Z9 1
U1 0
U2 4
PY 2017
BP 484
EP 489
UT WOS:000425928100086
DA 2023-11-16
ER

PT C
AU Dorogyy, Y
   Kolisnichenko, V
AF Dorogyy, Yaroslav
   Kolisnichenko, Vadym
GP IEEE
TI Designing Spiking Neural Networks
SO 2016 13TH INTERNATIONAL CONFERENCE ON MODERN PROBLEMS OF RADIO
   ENGINEERING, TELECOMMUNICATIONS AND COMPUTER SCIENCE (TCSET)
DT Proceedings Paper
CT 13th International Conference on Modern Problems of Radio Engineering,
   Telecommunications and Computer Science (TCSET)
CY FEB 23-26, 2016
CL UKRAINE
DE Spkiking Neural Network; Spike Coding; STDP; Neurons Modeling;
   Biological Neuron Models
AB The problem of design is the most important part of complex systems building. This is also true for spiking neural networks. In this paper, the next steps of SNN design are described: coding, selecting neuron model and learning algorithm, creating network architecture. Software and hardware solutions for simulating these networks are also discussed. We propose a range of evolution directions, future studies on every step of the design. Our methods are based on detailed analysis of existing solutions and needs.
C1 [Dorogyy, Yaroslav; Kolisnichenko, Vadym] Natl Tech Univ Ukraine, Kyiv Polytech Inst, Prospect Peremogy Str 37, UA-03056 Kiev, Ukraine.
RP Dorogyy, Y (corresponding author), Natl Tech Univ Ukraine, Kyiv Polytech Inst, Prospect Peremogy Str 37, UA-03056 Kiev, Ukraine.
EM alt2600h@gmail.com
CR [Anonymous], 2014, ADV SEMINAR TU MUNIC
   [Anonymous], RESUME NEW SUPERVISE
   [Anonymous], NY TIMES AUG
   [Anonymous], NEUROMORPHIC ENG SYS
   [Anonymous], MICROSOFT RES
   [Anonymous], LECT NOTES COMPUTER
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Diehl P.U., 2015, 2015 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2015.7280696
   Iakymchuk T, 2015, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-015-0059-4
   Kasinski A, 2005, LECT NOTES COMPUT SC, V3696, P145, DOI 10.1007/11550822_24
   Ros E, 2006, NEURAL COMPUT, V18, P2959, DOI 10.1162/neco.2006.18.12.2959
NR 11
TC 4
Z9 4
U1 0
U2 6
PY 2016
BP 124
EP 127
UT WOS:000381804300029
DA 2023-11-16
ER

PT C
AU Galán-Prado, F
   Morán, A
   Font, J
   Roca, M
   Rosselló, JL
AF Galan-Prado, Fabio
   Moran, Alejandro
   Font, Joan
   Roca, Miquel
   Rossello, Josep L.
GP IEEE
TI Stochastic Radial Basis Neural Networks
SO 2019 IEEE 29TH INTERNATIONAL SYMPOSIUM ON POWER AND TIMING MODELING,
   OPTIMIZATION AND SIMULATION (PATMOS 2019)
SE International Symposium on Power and Timing Modeling Optimization and
   Simulation
DT Proceedings Paper
CT 29th IEEE International Symposium on Power and Timing Modeling,
   Optimization and Simulation (PATMOS)
CY JUL 01-03, 2019
CL GREECE
DE Neuromorphic Hardware; Spiking Neural Network; FPGA
ID SPIKING; MACHINES
AB Stochastic spiking Neural Networks (SNN) is a new neural modeling oriented to include the intrinsic stochastic processes present in the brain. One of the main advantages of this kind of modeling is that they can be easily implemented in a digital circuit, thus taking advantage of this mature technology. In this paper we propose a digital design for stochastic spiking neurons oriented to high-density hardware implementation. We compare the proposal with other neural models, comparing in terms of speed, area and precision. As is shown, the circuit proposal is able to provide competitive results when comparing with other works present in the literature.
C1 [Galan-Prado, Fabio; Moran, Alejandro; Font, Joan; Roca, Miquel; Rossello, Josep L.] Univ Illes Balears, Elect Engn Grp, Phys Dept, Palma De Mallorca 07122, Balears, Spain.
RP Rosselló, JL (corresponding author), Univ Illes Balears, Elect Engn Grp, Phys Dept, Palma De Mallorca 07122, Balears, Spain.
EM j.rossello@uib.es
CR Broomhead D. S., 1988, Complex Systems, V2, P321
   CHEN S, 1991, IEEE T NEURAL NETWOR, V2, P302, DOI 10.1109/72.80341
   Fisher RA, 1936, ANN EUGENIC, V7, P179, DOI 10.1111/j.1469-1809.1936.tb02137.x
   Galán-Prado F, 2019, INT J NEURAL SYST, V29, DOI 10.1142/S0129065719500047
   Ghosh-Dastidar S, 2009, ADV INTEL SOFT COMPU, V61, P167
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Kasabov N, 2010, NEURAL NETWORKS, V23, P16, DOI 10.1016/j.neunet.2009.08.010
   Koch C., 1998, BRADFORD BOOK, V2
   MacQueen J., 1967, P 5 BERKELEY S MATH
   Maguire LP, 2007, NEUROCOMPUTING, V71, P13, DOI 10.1016/j.neucom.2006.11.029
   Morro A, 2018, IEEE T NEUR NET LEAR, V29, P1371, DOI 10.1109/TNNLS.2017.2657601
   MUSAVI MT, 1992, NEURAL NETWORKS, V5, P595, DOI 10.1016/S0893-6080(05)80038-3
   Park J, 1991, NEURAL COMPUT, V3, P246, DOI 10.1162/neco.1991.3.2.246
   Rosselló JL, 2016, INT J NEURAL SYST, V26, DOI 10.1142/S0129065715500367
   Rosselló JL, 2014, INT J NEURAL SYST, V24, DOI 10.1142/S0129065714300034
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Sboev A, 2018, PROCEDIA COMPUT SCI, V123, P494, DOI 10.1016/j.procs.2018.01.075
   Scholkopf B, 1997, IEEE T SIGNAL PROCES, V45, P2758, DOI 10.1109/78.650102
   Schrauwen B, 2008, NEURAL NETWORKS, V21, P511, DOI 10.1016/j.neunet.2007.12.009
   Shukla A, 2017, IEEE IJCNN, P4657, DOI 10.1109/IJCNN.2017.7966447
   Steinmetz PN, 2000, J COMPUT NEUROSCI, V9, P133, DOI 10.1023/A:1008967807741
NR 21
TC 0
Z9 0
U1 0
U2 0
PY 2019
BP 145
EP 149
DI 10.1109/patmos.2019.8862129
UT WOS:000556107300025
DA 2023-11-16
ER

PT J
AU Wang, C
   Shanechi, MM
AF Wang, Chuanmeizhi
   Shanechi, Maryam M.
TI Estimating Multiscale Direct Causality Graphs in Neural Spike-Field
   Networks
SO IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING
DT Article
DE Causality; local field potentials (LFP); spikes; multiscale data; neural
   encoding
ID BRAIN-MACHINE INTERFACES; ENSEMBLE; INFORMATION; STIMULATION;
   POTENTIALS; SYSTEMS
AB Neural representations span various spatiotemporal scales of brain activity, from the spiking activity of single neurons to field activity measuring large-scale networks. The simultaneous analyses of spikes and fields to uncover causal interactions in multiscale networks could help understand neural mechanisms. However, assessing causality within spike-field networks is challenging as spikes are binary-valued with a fast time-scale while fields are continuous-valued with slower time-scales. Current causality measures are largely not applicable to mixed discrete-continuous network activity. Here, in this paper, we develop a novel multiscale causality estimation algorithm for spike-field networks. We construct a likelihood function comprised of point process models for spikes and linear Gaussian models for fields. For spikes, firing rates are modeled as a function of the history of both field signals and binary spike events within the network. For fields, to make their linear models consistent with biophysical findings, we use the history of field signals and the history of the latent log-firing rates of neurons as predictors. To resolve the challenge of estimating the network model parameters in the presence of latent firing rates, we develop a sequential maximum-likelihood parameter estimation procedure that extends to large networks. Once models are estimated, we compute directed information as our measure of multiscale causality and devise two statistical tests to assess its significance. Using extensive simulations, we show that the algorithm can accurately reconstruct the true causality graphs of random spike-field networks. Moreover, the algorithm is robust to the number of connections, connection strengths, or exact topology of the network. This multiscale causality estimation algorithm has important implications for studying neural mechanisms and for future neurotechnology design.
C1 [Wang, Chuanmeizhi] Univ Southern Calif Los Angeles, Ming Hsieh Dept Elect & Comp Engn, Los Angeles, CA 90089 USA.
   [Shanechi, Maryam M.] Univ Southern Calif Los Angeles, Neurosci Grad Program, Ming Hsieh Dept Elect & Comp Engn, Los Angeles, CA 90089 USA.
RP Shanechi, MM (corresponding author), Univ Southern Calif Los Angeles, Neurosci Grad Program, Ming Hsieh Dept Elect & Comp Engn, Los Angeles, CA 90089 USA.
EM shanechi@usc.edu
CR Abbaspourazad H, 2017, IEEE ENG MED BIO, P201, DOI 10.1109/EMBC.2017.8036797
   Agarwal R, 2016, NEURAL COMPUT, V28, P1356, DOI 10.1162/NECO_a_00847
   AKAIKE H, 1974, IEEE T AUTOMAT CONTR, VAC19, P716, DOI 10.1109/TAC.1974.1100705
   BENJAMINI Y, 1995, J R STAT SOC B, V57, P289, DOI 10.1111/j.2517-6161.1995.tb02031.x
   Brovelli A, 2004, P NATL ACAD SCI USA, V101, P9849, DOI 10.1073/pnas.0308538101
   Brown EN, 1998, J NEUROSCI, V18, P7411
   Bullmore ET, 2009, NAT REV NEUROSCI, V10, P186, DOI 10.1038/nrn2575
   Cai ZT, 2017, J NEUROPHYSIOL, V118, P1055, DOI 10.1152/jn.00086.2017
   Citi L, 2014, NEURAL COMPUT, V26, P237, DOI 10.1162/NECO_a_00548
   Cunningham JP, 2011, J NEUROPHYSIOL, V105, P1932, DOI 10.1152/jn.00503.2010
   Ehrens D, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00058
   Einevoll GT, 2013, NAT REV NEUROSCI, V14, P770, DOI 10.1038/nrn3599
   Gowda S, 2014, IEEE T NEUR SYS REH, V22, P911, DOI 10.1109/TNSRE.2014.2309673
   Graat I, 2017, INT REV PSYCHIATR, V29, P178, DOI 10.1080/09540261.2017.1282439
   GRANGER CWJ, 1980, J ECON DYN CONTROL, V2, P329, DOI 10.1016/0165-1889(80)90069-X
   Hsieh HL, 2019, J NEURAL ENG, V16, DOI 10.1088/1741-2552/aaeb1a
   Hsieh HL, 2018, PLOS COMPUT BIOL, V14, DOI 10.1371/journal.pcbi.1006168
   Hu M, 2016, NEUROIMAGE, V133, P457, DOI 10.1016/j.neuroimage.2016.03.030
   Jarvis MR, 2001, NEURAL COMPUT, V13, P717, DOI 10.1162/089976601300014312
   Kaminski M, 2001, BIOL CYBERN, V85, P145, DOI 10.1007/s004220000235
   Kass RE, 2001, NEURAL COMPUT, V13, P1713, DOI 10.1162/08997660152469314
   Kim S, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1001110
   Krumin M, 2010, COMPUT INTEL NEUROSC, P1
   Mayberg HS, 2003, BRIT MED BULL, V65, P193, DOI 10.1093/bmb/65.1.193
   Mazzoni A, 2015, PLOS COMPUT BIOL, V11, DOI 10.1371/journal.pcbi.1004584
   Millard DC, 2013, J NEURAL ENG, V10, DOI 10.1088/1741-2560/10/6/066011
   Moxon KA, 2015, NEURON, V86, P55, DOI 10.1016/j.neuron.2015.03.036
   Muller L, 2018, NAT REV NEUROSCI, V19, P255, DOI 10.1038/nrn.2018.20
   Nadeau C, 2003, MACH LEARN, V52, P239, DOI 10.1023/A:1024068626366
   Nedungadi AG, 2009, J COMPUT NEUROSCI, V27, P55, DOI 10.1007/s10827-008-0126-2
   Newey K., 1994, HDB ECONOMETRICS, V4, P2112, DOI DOI 10.1016/S1573-4412(05)80005-4
   Newman JP, 2015, ELIFE, V4, DOI 10.7554/eLife.07192
   Neyman J, 1933, PHILOS T R SOC LOND, V231, P289, DOI 10.1098/rsta.1933.0009
   Papadopoulou M, 2011, SCIENCE, V332, P721, DOI 10.1126/science.1201835
   Pereda E, 2005, PROG NEUROBIOL, V77, P1, DOI 10.1016/j.pneurobio.2005.10.003
   Pesaran B, 2018, NAT NEUROSCI, V21, P903, DOI 10.1038/s41593-018-0171-8
   Philiastides MG, 2006, IEEE T BIO-MED ENG, V53, P2602, DOI 10.1109/TBME.2006.885122
   Powers D. M., 2011, EVALUATION PRECISION, DOI 10.48550/arXiv.2010.16061
   Quinn CJ, 2015, IEEE T INFORM THEORY, V61, P6887, DOI 10.1109/TIT.2015.2478440
   Quinn CJ, 2011, J COMPUT NEUROSCI, V30, P17, DOI 10.1007/s10827-010-0247-2
   Ray S, 2015, CURR OPIN NEUROBIOL, V31, P111, DOI 10.1016/j.conb.2014.09.004
   Sadtler PT, 2014, NATURE, V512, P423, DOI 10.1038/nature13665
   Sameshima K, 1999, J NEUROSCI METH, V94, P93, DOI 10.1016/S0165-0270(99)00128-4
   Sani OG, 2018, NAT BIOTECHNOL, V36, P954, DOI 10.1038/nbt.4200
   Schreiber T, 2000, PHYS REV LETT, V85, P461, DOI 10.1103/PhysRevLett.85.461
   Shanechi MM, 2017, IEEE T NEUR SYS REH, V25, P1725, DOI 10.1109/TNSRE.2016.2639501
   Shanechi MM, 2017, NAT COMMUN, V8, DOI 10.1038/ncomms13825
   Shanechi MM, 2016, PLOS COMPUT BIOL, V12, DOI 10.1371/journal.pcbi.1004730
   Shanechi MM, 2013, IEEE T NEUR SYS REH, V21, P129, DOI 10.1109/TNSRE.2012.2221743
   Sheikhattar A, 2018, P NATL ACAD SCI USA, V115, pE3869, DOI 10.1073/pnas.1718154115
   So K, 2012, J NEURAL ENG, V9, DOI 10.1088/1741-2560/9/2/026004
   Stam Cornelis J, 2007, Nonlinear Biomed Phys, V1, P3, DOI 10.1186/1753-4631-1-3
   Truccolo W, 2005, J NEUROPHYSIOL, V93, P1074, DOI 10.1152/jn.00697.2004
   Truccolo Wilson, 2016, J Physiol Paris, V110, P336, DOI 10.1016/j.jphysparis.2017.02.004
   Truccolo W, 2010, NAT NEUROSCI, V13, P105, DOI 10.1038/nn.2455
   Watts DJ, 1998, NATURE, V393, P440, DOI 10.1038/30918
   WEDDERBURN RWM, 1976, BIOMETRIKA, V63, P27, DOI 10.2307/2335080
   Wichmann T, 2016, NEUROTHERAPEUTICS, V13, P264, DOI 10.1007/s13311-016-0426-6
   Wilks SS, 1938, PSYCHOMETRIKA, V3, P23, DOI 10.1007/BF02287917
   Williams ZM, 2015, NAT NEUROSCI, V18, P618, DOI 10.1038/nn.4007
   Wong YT, 2016, NAT NEUROSCI, V19, P327, DOI 10.1038/nn.4210
   Yang YX, 2019, J NEURAL ENG, V16, DOI 10.1088/1741-2552/ab0ea4
   Yang YX, 2018, J NEURAL ENG, V15, DOI 10.1088/1741-2552/aad1a8
   Yang YX, 2016, J NEURAL ENG, V13, DOI 10.1088/1741-2560/13/6/066019
   Zhu LQ, 2003, NEURAL COMPUT, V15, P2359, DOI 10.1162/089976603322362392
   2007, SCIENCE, V316, P1609, DOI DOI 10.1126/SCIENCE.1139597
NR 66
TC 12
Z9 13
U1 0
U2 11
PD MAY
PY 2019
VL 27
IS 5
BP 857
EP 866
DI 10.1109/TNSRE.2019.2908156
UT WOS:000467572900008
DA 2023-11-16
ER

PT C
AU Chen, Y
   Zhang, SL
   Ren, SY
   Qu, H
AF Chen, Yi
   Zhang, Silin
   Ren, Shiyu
   Qu, Hong
GP IEEE
TI GRADUAL SURROGATE GRADIENT LEARNING IN DEEP SPIKING NEURAL NETWORKS
SO 2022 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)
SE International Conference on Acoustics Speech and Signal Processing
   ICASSP
DT Proceedings Paper
CT 47th IEEE International Conference on Acoustics, Speech and Signal
   Processing (ICASSP)
CY MAY 22-27, 2022
CL Singapore, SINGAPORE
DE Spiking Neural Networks; Spiking Neuron Model; Surrogate Gradient
AB Spiking Neural Network (SNN) is a promising solution for ultra-low-power hardware. Recent SNNs have reached the performance of Deep Neural Networks (DNNs) in dealing with many tasks. However, these methods often suffer from a long simulation time to achieve the accurate spike train information. In addition, these methods are contingent on a well-designed initialization to effectively transmit the gradient information. To address these issues, we propose the Internal Spiking Neuron Model (ISNM), which uses the synaptic current instead of spike trains as the carrier of information. In addition, we design a gradual surrogate gradient learning algorithm to ensure that SNNs effectively back-propagate gradient information in the early stage of training and more accurate gradient information in the later stage of training. The experiments on various network structures on CIFAR-10 and CIFAR-100 datasets show that the proposed method can exceed the performance of previous SNN methods within 5 time steps.
C1 [Chen, Yi; Zhang, Silin; Ren, Shiyu; Qu, Hong] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu, Peoples R China.
RP Qu, H (corresponding author), Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu, Peoples R China.
EM hongqu@uestc.edu.cn
CR [Anonymous], 2019, CORR
   Chen Y, 2021, AAAI CONF ARTIF INTE, V35, P7073
   Datta G, 2021, IEEE IJCNN, DOI 10.1109/IJCNN52387.2021.9534306
   Ding Jianhao, 2021, ARXIV210511654
   Fang WJ, 2021, IEEE INT SYMP INFO, P3261, DOI 10.1109/ISIT45174.2021.9517911
   Gerstner W., 2002, SPIKING NEURON MODEL
   Han S., 2016, P INT C LEARN REPR, DOI DOI 10.48550/ARXIV.1510.00149
   He K., 2016, P IEEE C COMP VIS PA, DOI DOI 10.1007/978-3-319-46493-0_38
   Kim J, 2018, NEUROCOMPUTING, V311, P373, DOI 10.1016/j.neucom.2018.05.087
   Kim Youngeun, 2020, ARXIV201001729
   Kingma DP., 2017, ARXIV
   Lu S, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00535
   Luo XL, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00559
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Papernot N, 2016, P IEEE S SECUR PRIV, P582, DOI 10.1109/SP.2016.41
   Park S, 2020, DES AUT CON, DOI [10.1109/dac18072.2020.9218689, 10.1007/s00779-020-01476-2]
   Qin HT, 2020, PROC CVPR IEEE, P2247, DOI 10.1109/CVPR42600.2020.00232
   Samaddar R, 2019, GLOBAL GOVERNANCE AND INDIA'S NORTH-EAST: LOGISTICS, INFRASTRUCTURE AND SOCIETY, P1
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Wozniak S, 2020, NAT MACH INTELL, V2, P325, DOI 10.1038/s42256-020-0187-0
   Wu YJ, 2019, AAAI CONF ARTIF INTE, P1311
   Zhang ML, 2019, IEEE T NEUR NET LEAR, V30, P123, DOI 10.1109/TNNLS.2018.2833077
   Zhang Malu, 2021, IEEE T NEURAL NETWOR
NR 24
TC 2
Z9 2
U1 1
U2 3
PY 2022
BP 8927
EP 8931
DI 10.1109/ICASSP43922.2022.9746774
UT WOS:000864187909048
DA 2023-11-16
ER

PT J
AU Shin, CW
   Kim, S
AF Shin, Chang-Woo
   Kim, Seunghwan
TI Hierarchical modularity of the functional neural network organized by
   spike timing dependent synaptic plasticity
SO INTERNATIONAL JOURNAL OF MODERN PHYSICS B
DT Article; Proceedings Paper
CT International Conference on Frontiers of Nonlinear and Complex Systems
CY MAY 24-26, 2006
CL Hong Kong, PEOPLES R CHINA
DE complex networks; hierarchical modularity; functional neural networks;
   STDP
AB We study the emergent functional neural network organized by synaptic reorganization by the spike timing dependent synaptic plasticity (STDP). We show that small-world and scale-free functional structures organized by STDP, in the case of synaptic balance, exhibit hierarchial modularity.
C1 [Shin, Chang-Woo; Kim, Seunghwan] Natl Core Res Ctr Syst Biodynam, Asia Pacific Ctr Theoret Phys, Pohang 790784, South Korea.
   [Shin, Chang-Woo; Kim, Seunghwan] Pohang Univ Sci & Technol, Dept Phys, Nonlinear Complex Syst Lab, Pohang 790784, South Korea.
RP Shin, CW (corresponding author), Natl Core Res Ctr Syst Biodynam, Asia Pacific Ctr Theoret Phys, San 31 Hyoja Dong, Pohang 790784, South Korea.
EM Shinc@postech.ac.kr; swan@postech.ac.kr
CR Albert R, 2002, REV MOD PHYS, V74, P47, DOI 10.1103/RevModPhys.74.47
   Barabási AL, 1999, SCIENCE, V286, P509, DOI 10.1126/science.286.5439.509
   Bell CC, 1997, NATURE, V387, P278, DOI 10.1038/387278a0
   Bi GQ, 2001, ANNU REV NEUROSCI, V24, P139, DOI 10.1146/annurev.neuro.24.1.139
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Debanne D, 1998, J PHYSIOL-LONDON, V507, P237, DOI 10.1111/j.1469-7793.1998.237bu.x
   Dorogovtsev SN, 2002, ADV PHYS, V51, P1079, DOI 10.1080/00018730110112519
   FIRZHUGH R, 1961, BIOPHYS J, V1, P445
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   Newman MEJ, 2003, SIAM REV, V45, P167, DOI 10.1137/S003614450342480
   Ravasz E, 2003, PHYS REV E, V67, DOI 10.1103/PhysRevE.67.026112
   Ravasz E, 2002, SCIENCE, V297, P1551, DOI 10.1126/science.1073374
   Rubin J, 2001, PHYS REV LETT, V86, P364, DOI 10.1103/PhysRevLett.86.364
   SHIN CW, 2006, IN PRESS PHYS REV E, V74
   van Rossum MCW, 2000, J NEUROSCI, V20, P8812
   Watts DJ, 1998, NATURE, V393, P440, DOI 10.1038/30918
NR 16
TC 0
Z9 0
U1 0
U2 2
PD SEP 30
PY 2007
VL 21
IS 23-24
BP 4123
EP 4128
UT WOS:000251997500041
DA 2023-11-16
ER

PT C
AU Knoblock, EJ
   Bahrami, HR
AF Knoblock, Eric J.
   Bahrami, Hamid R.
GP IEEE
TI Investigation of Spiking Neural Networks for Modulation Recognition
   using Spike-Timing-Dependent Plasticity
SO 2019 IEEE COGNITIVE COMMUNICATIONS FOR AEROSPACE APPLICATIONS WORKSHOP
   (CCAAW)
DT Proceedings Paper
CT IEEE Cognitive Communications for Aerospace Applications Workshop
   (CCAAW)
CY JUN 25-26, 2019
CL Cleveland, OH
DE machine learning; spiking neural networks; space communications;
   neuromorphic platforms; modulation recognition; spike-timing-dependent
   plasticity; CubeSats
AB Spiking neural networks (SNNs) operating on neuromorphic hardware can enable cognitive functionality with relatively low power consumption as compared to other artificial neural network implementations, making it ideally suited for resource-constrained space platforms such as CubeSats. The objective of this study is to investigate the implementation of a modulation recognition capability using SNNs, which may eventually be applied to neuromorphic hardware for implementation. This preliminary analysis uses a software simulation approach with an unsupervised learning algorithm based on spike-timing-dependent plasticity for classification of digital modulation constellation patterns. This modulation recognition capability can provide enhanced situational awareness for a space platform and facilitate additional high-level cognitive functionality that can be investigated in future studies.
C1 [Knoblock, Eric J.] Natl Aeronaut & Space Adm, Commun & Intelligent Syst Div, Cleveland, OH 44135 USA.
   [Bahrami, Hamid R.] Univ Akron, Dept Elect & Comp Engn, Akron, OH 44325 USA.
RP Knoblock, EJ (corresponding author), Natl Aeronaut & Space Adm, Commun & Intelligent Syst Div, Cleveland, OH 44135 USA.
EM eric.j.knoblock@nasa.gov; hrb@uakron.edu
CR Briones J., 2018, 1 NESC INT MULT DISC
   Coates A., 2011, NEURAL INFORM PROCES
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Gerstern W., 2002, SPIKING NEURON MODEL
   Higginbotham S., 2017, NEV SPAC GRANT NEV N
   Iakymchuk T, 2015, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-015-0059-4
   Khademian F., 2015, ADV COMPUT SCI INT J, V4, P133
   Lee JM, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00191
   Nair MV, 2017, NANO FUTURES, V1, DOI 10.1088/2399-1984/aa954a
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Schaire S., 2016, AIAA SPACEOPS 2016
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   West N. E., 2017, IEEE INT S DYN SPECT
NR 13
TC 1
Z9 1
U1 0
U2 0
PY 2019
DI 10.1109/ccaaw.2019.8904911
UT WOS:000520123400021
DA 2023-11-16
ER

PT J
AU Miri, F
   Miles, CI
   Lewis, HW
AF Miri, Fatemehossadat
   Miles, Carol, I
   Lewis, Harold W., III
TI Simulating a complete <i>Tritonia</i> escape swim network using a novel
   event-based spiking neural network algorithm
SO NEURAL COMPUTING & APPLICATIONS
DT Article
DE Event-based spiking neural network; Spiking neural network simulation;
   Tritonia escape swim; Spike response model
ID NEURONS; MECHANISMS; INITIATION
AB Tritonia has been studied in the laboratory by several studies, which have led to significant advances in identifying the biological components that participate in the Tritonia escape swim network. There are also studies, which have artificially reproduced the neuronal patterns of the Tritonia escape swim circuit. These studies simulated the interneurons of the swim central pattern generator (CPG) known as dorsal swim interneuron, ventral swim interneuron, and cerebral 2. In this research, other neurons that participate in the Tritonia escape swim network were simulated. In addition to the main CPG components, sensory, ramp, and dorsal/ventral flexion neurons are all included in the neural network (NN). The objective of the study was to artificially reconstruct a more representative image of the Tritonia escape swim NN, its neuronal activities, and synaptic connections. The network was simulated using a spiking neural network (SNN) simulator named Synapse, which has been implemented based on a novel event-based SNN algorithm. After tuning synaptic delays, weights, and membrane potential properties, the expected spike patterns were successfully reproduced for each involved neuron. The spike patterns from this study were validated using the laboratory recorded signals as well as the existing simulated patterns.
C1 [Miri, Fatemehossadat; Lewis, Harold W., III] SUNY Binghamton, Dept Syst Sci & Ind Engn, Watson Sch Engn & Appl Sci, Binghamton, NY 13902 USA.
   [Miles, Carol, I] SUNY Binghamton, Dept Biol Sci, Biochem Program, Binghamton, NY 13902 USA.
RP Miri, F (corresponding author), SUNY Binghamton, Dept Syst Sci & Ind Engn, Watson Sch Engn & Appl Sci, Binghamton, NY 13902 USA.
EM fmiri1@binghamton.edu
CR [Anonymous], 2009, CURRENT OPINION N, DOI DOI 10.4249/SCHOLARPEDIA.3638
   [Anonymous], 2001, HDB BIOL PHYS
   BALABAN PM, 1979, ACTA NEUROBIOL EXP, V39, P97
   Bichler O, 2012, NEURAL NETWORKS, V32, P339, DOI 10.1016/j.neunet.2012.02.022
   Calin-Jageman RJ, 2007, J NEUROPHYSIOL, V98, P2382, DOI 10.1152/jn.00572.2007
   Carlson KD, 2014, FRONT NEUROSCI-SWITZ, V8, DOI 10.3389/fnins.2014.00010
   Fickbohm DJ, 2000, J NEUROSCI, V20, P1622
   Frost WN, 2003, NEURON, V40, P991, DOI 10.1016/S0896-6273(03)00731-1
   Frost WN, 2001, AM ZOOL, V41, P952, DOI 10.1668/0003-1569(2001)041[0952:SINITD]2.0.CO;2
   GETTING PA, 1983, J NEUROPHYSIOL, V49, P1036, DOI 10.1152/jn.1983.49.4.1036
   GETTING PA, 1981, J NEUROPHYSIOL, V46, P65, DOI 10.1152/jn.1981.46.1.65
   GETTING PA, 1983, J NEUROPHYSIOL, V49, P1017, DOI 10.1152/jn.1983.49.4.1017
   GETTING PA, 1976, J COMP PHYSIOL, V110, P271, DOI 10.1007/BF00659144
   HUME RI, 1982, J NEUROPHYSIOL, V47, P75, DOI 10.1152/jn.1982.47.1.75
   Katz PS, 2004, J NEUROPHYSIOL, V92, P1904, DOI 10.1152/jn.00864.2003
   KATZ PS, 1995, J NEUROSCI, V15, P6035
   Lee AH, 2012, J NEUROSCI, V32, P15262, DOI 10.1523/JNEUROSCI.0160-12.2012
   Mongeluzi DL, 2000, LEARN MEMORY, V7, P43, DOI 10.1101/lm.7.1.43
   Sakurai A, 2009, J NEUROSCI, V29, P13115, DOI 10.1523/JNEUROSCI.3485-09.2009
   Wyeth R, 2012, TRITONIA DIOMEDEA ES
NR 20
TC 0
Z9 0
U1 1
U2 9
PD JAN
PY 2023
VL 35
IS 2
SI SI
BP 1733
EP 1748
DI 10.1007/s00521-022-07829-7
EA OCT 2022
UT WOS:000863974300001
DA 2023-11-16
ER

PT J
AU Yang, J
   Ji, XY
   Li, SB
   Hu, JJ
   Wang, Y
   Liu, TQ
AF Yang, Jing
   Ji, Xiaoyang
   Li, Shaobo
   Hu, Jianjun
   Wang, Yang
   Liu, Tingqing
TI Spiking Neural Network Robot Tactile Object Recognition Method with
   Regularization Constraints
SO JOURNAL OF ELECTRONICS & INFORMATION TECHNOLOGY
DT Article
DE Tactile perception; Spiking neural network; Recognition algorithm;
   Regularization method; Back propagation
ID LOIHI
AB It is important for the future development of intelligent robots to expand tactile perception ability, which determines the scope of application scenarios for robots. Tactile data collected by tactile sensors are the basis of robotics work, but these data have complex spatio-temporal properties. Spiking neural network has rich spatio-temporal dynamics and event-driven nature. It can better process spatio-temporal information and be applied to artificial intelligence chips to bring higher energy efficiency to robots. To solve the problem of backpropagation failure in the network training process caused by the discreteness of neuron spike activity in the spiking neural network, from the perspective of the dynamic system of the intelligent robot, the spiking activity approximation function is introduced to make the spiking neural network back-propagation gradient descent method effective. The over-fitting problem caused by the small amount of tactile data is alleviated by the regularization methods. Finally, the spiking neural network robot tactile object recognition algorithm SnnTd and SnnTdlc with regularization constraints are proposed. Compared with the classical methods TactileSGNet, Grid-based CNN, MLP and GCN, the SnnTd method tactile object recognition rate is improved by 5.00% over the best method TactileSGNet on EvTouch-Containers dataset, and the SnnTdlc method tactile object recognition rate is improved by 3.16% over the best method TactileSGNet on EvTouch-Objects dataset.
C1 [Yang, Jing; Ji, Xiaoyang; Li, Shaobo; Hu, Jianjun; Wang, Yang; Liu, Tingqing] Guizhou Univ, State Key Lab Publ Big Data, Guiyang 550025, Peoples R China.
   [Yang, Jing; Ji, Xiaoyang; Li, Shaobo; Wang, Yang; Liu, Tingqing] Guizhou Univ, Coll Comp Sci & Technol, Guiyang 550025, Peoples R China.
   [Yang, Jing; Li, Shaobo] Minist Educ, Key Lab Adv Mfg Technol, Guiyang 550025, Peoples R China.
   [Hu, Jianjun] Univ South Carolina, Dept Comp Sci & Engn, Columbia, SC 29208 USA.
RP Ji, XY (corresponding author), Guizhou Univ, State Key Lab Publ Big Data, Guiyang 550025, Peoples R China.; Ji, XY (corresponding author), Guizhou Univ, Coll Comp Sci & Technol, Guiyang 550025, Peoples R China.
EM gs.xyji20@gzu.edu.cn
CR Ahmadi R, 2012, IEEE SENS J, V12, P22, DOI 10.1109/JSEN.2011.2113394
   [Anonymous], 2019, P AAAI C ARTIFICIAL
   Barbier T, 2021, IEEE COMPUT SOC CONF, P1377, DOI 10.1109/CVPRW53098.2021.00152
   Bartolozzi C, 2017, IEEE INT C INT ROBOT, P166, DOI 10.1109/IROS.2017.8202153
   Bing Han, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13555, DOI 10.1109/CVPR42600.2020.01357
   [曹毅 Cao Yi], 2022, [华中科技大学学报. 自然科学版, Journal of Huazhong University of Science and Technology. Nature Science], V50, P40
   [程龙 Cheng Long], 2022, [控制与决策, Control and Decision], V37, P1409
   Cheng X, 2021, J ELECTRON INF TECHN, V43, P3662, DOI 10.11999/JEIT200755
   Dahiya R, 2019, P IEEE, V107, P2016, DOI 10.1109/JPROC.2019.2941366
   Dahiya RS, 2010, IEEE T ROBOT, V26, P1, DOI 10.1109/TRO.2009.2033627
   Davies M, 2021, P IEEE, V109, P911, DOI 10.1109/JPROC.2021.3067593
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Deng L, 2020, NEURAL NETWORKS, V121, P294, DOI 10.1016/j.neunet.2019.09.005
   Gal Y, 2016, PR MACH LEARN RES, V48
   Gu FQ, 2020, IEEE INT C INT ROBOT, P9876, DOI 10.1109/IROS45743.2020.9341421
   See HH, 2020, Arxiv, DOI arXiv:2005.04319
   [胡一凡 Hu Yifan], 2021, [控制与决策, Control and Decision], V36, P1
   Kim K, 2021, ADV SCI, V8, DOI 10.1002/advs.202002362
   Kingma DP., 2017, ARXIV
   Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508
   Li Q, 2020, IEEE T ROBOT, V36, P1619, DOI 10.1109/TRO.2020.3003230
   [李翔 Li Xiang], 2021, [计算机学报, Chinese Journal of Computers], V44, P2122
   Loshchilov I., 2016, ARXIV
   Paredes-Vallés F, 2020, IEEE T PATTERN ANAL, V42, P2051, DOI 10.1109/TPAMI.2019.2903179
   Sundaram S, 2019, NATURE, V569, P698, DOI 10.1038/s41586-019-1234-z
   Sushko V, 2021, IEEE COMPUT SOC CONF, P2596, DOI 10.1109/CVPRW53098.2021.00293
   Taunyazov T., 2020, ARXIV, DOI 10.48550/arXiv.2009.07083
   Taunyazov T, 2020, IEEE INT C INT ROBOT, P9890, DOI 10.1109/IROS45743.2020.9340693
   Wang CF, 2019, ADV INTELL SYST-GER, V1, DOI 10.1002/aisy.201900090
   Yan YC, 2021, SCI ROBOT, V6, DOI 10.1126/scirobotics.abc8801
   Yeh SK, 2021, IEEE SENS J, V21, P12563, DOI 10.1109/JSEN.2021.3060539
   [张铁林 Zhang Tielin], 2021, [计算机学报, Chinese Journal of Computers], V44, P1767
   Zhu ML, 2020, SCI ADV, V6, DOI 10.1126/sciadv.aaz8693
NR 33
TC 0
Z9 0
U1 0
U2 0
PD JUL
PY 2023
VL 45
IS 7
BP 2595
EP 2604
DI 10.11999/JEIT220711
UT WOS:001045092500002
DA 2023-11-16
ER

PT J
AU Ahmadi, A
AF Ahmadi, Arash
TI \Evolving Spiking Neural Networks for Control of Artificial Creatures
SO BRAIN-BROAD RESEARCH IN ARTIFICIAL INTELLIGENCE AND NEUROSCIENCE
DT Article
DE Spiking Neural Networks (SNN); Izhikevich Model; Genetic Algorithm (GA);
   artificial creature
AB To understand and analysis behavior of complicated and intelligent organisms, scientists apply bio-inspired concepts including evolution and learning to mathematical models and analyses. Researchers utilize these perceptions in different applications, searching for improved methods and approaches for modern computational systems. This paper presents a genetic algorithm based evolution framework in which Spiking Neural Network (SNN) of artificial creatures are evolved for higher chance of survival in a virtual environment. The artificial creatures are composed of randomly connected Izhikevich spiking reservoir neural networks using population activity rate coding. Inspired by biological neurons, the neuronal connections are considered with different axonal conduction delays. Simulations results prove that the evolutionary algorithm has the capability to find or synthesis artificial creatures which can survive in the environment successfully. Keywords: Spiking Neural Networks (SNN), Izhikevich Model, Genetic Algorithm
C1 [Ahmadi, Arash] Univ Southampton, Elect Syst Design Grp, Elect & Comp Sci, Southampton SO17 1BJ, Hants, England.
RP Ahmadi, A (corresponding author), Univ Southampton, Elect Syst Design Grp, Elect & Comp Sci, Southampton SO17 1BJ, Hants, England.
EM aa5@ecs.soton.ac.uk
CR [Anonymous], 1992, ARTIFICIAL NEURAL NE
   [Anonymous], 2008, SPRINGER HDB ROBOTIC, DOI [DOI 10.1007/978-3-540-30301-5_62, DOI 10.1007/978-3-540-30301-5_39]
   [Anonymous], 2005, NEURAL NETWORKS METH, DOI DOI 10.1007/3-540-28847-3
   Bäck T, 1993, EVOL COMPUT, V1, P1, DOI 10.1162/evco.1993.1.1.1
   Bailey JA, 2011, NEUROCOMPUTING, V74, P2392, DOI 10.1016/j.neucom.2011.04.001
   Brebisson AD, 2012, THESIS
   Brunel N, 2007, BIOL CYBERN, V97, P337, DOI 10.1007/s00422-007-0190-0
   Cliff D, 1992, ISSUES EVOLUTIONARY
   Davis L., 1991, HDB GENETIC ALGORITH
   Floreano D, 2001, EVOLUTION SPIKING NE
   Gerstner W., 2002, SPIKING NEURON MODEL
   Goldberg D. E., 1989, GENETIC ALGORITHMS S
   Hagras H, 2004, IEEE INT CONF ROBOT, P4620, DOI 10.1109/ROBOT.2004.1302446
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   HOLLAND J, 1998, EMERGENCE
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Jaeger, 2001, TECH REP 148
   Jin X., 2010, PARALLEL SIMULATION
   Judd J. S., 1990, NEURAL NETWORK DESIG
   Kasabov N, 2010, NEURAL NETWORKS, V23, P16, DOI 10.1016/j.neunet.2009.08.010
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Mitchell M., 1998, INTRO GENETIC ALGORI
   Moutarde F., 2008, ROBOT BEHAV LEARNING
   Nolfi S, 1999, AUTON ROBOT, V7, P89, DOI 10.1023/A:1008973931182
   Oros N, 2009, IEEE SYMP ART LIFE, P116, DOI 10.1109/ALIFE.2009.4937702
   Paugam-Moisy H, 2010, HDB NATURAL COMPUTIN
   Paugam-Moisy H, 2008, NEUROCOMPUTING, V71, P1143, DOI 10.1016/j.neucom.2007.12.027
   Poli R., 2008, FIELD GUIDE GENETIC
   Roberts PD, 2002, BIOL CYBERN, V87, P392, DOI 10.1007/s00422-002-0361-y
   Schrauwen B., 2007, P 15 EUR S ART NEUR, P471
   Soares G. E., 2010, Proceedings of the 2010 Eleventh Brazilian Symposium on Neural Networks (SBRN 2010), P43, DOI 10.1109/SBRN.2010.16
   Su Dan, 2010, THESIS
   Urbanczik R, 2009, NAT NEUROSCI, V12, P250, DOI 10.1038/nn.2264
   Vreeken J, 2003, SPIKING NEURAL NETWO
NR 35
TC 2
Z9 2
U1 0
U2 0
PD OCT
PY 2013
VL 4
IS 1-4
BP 5
EP 19
UT WOS:000422342200001
DA 2023-11-16
ER

PT C
AU Patil, S
   Zhou, KD
   Parker, AC
AF Patil, Sukanya
   Zhou, Kaidi
   Parker, Alice C.
GP IEEE
TI Neural Circuits for Touch-Induced Locomotion in <i>Caenorhabditis
   Elegans</i>
SO 2015 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUL 12-17, 2015
CL Killarney, IRELAND
AB In this paper, We demonstrate biomimetic neural circuits (CMOS circuits) responsible for touch induced-locomotion in the nematode Caenorhabditis elegans (c. elegans). Our circuits model the neural network responsible for touch-induced locomotion of C. elegans worm (Chalfie and Sulston, 1985 [1]). Most animals use action potentials (spikes) for information transfer across neurons. Our initial touch-sensitive neural circuit presented here uses spiking neurons from the BioRC library. However, C. elegans neurons communicate with each other through graded potentials instead of action potentials, requiring redesign of the BioRC neuromorphic circuits to mimic a non-spiking neural network representing motion in C. elegans. Each neuron designed uses basic circuits like spike-responsive or graded-potential excitatory or inhibitory synapses, a voltage adder (dendritic computations) and an axon hillock (for spiking motor neurons). A neuromorphic neural network was constructed in CMOS using our neural circuits to implement touch-induced locomotion. We simulated spiking and non-spiking neural networks to demonstrate how signals propagate from sensory neurons to motor neurons when either posterior or anterior gentle touch is applied to the worm. We have demonstrated similarity between simulation results and biological measurements.
C1 [Patil, Sukanya] Indian Inst Technol, Dept Elect Engn, Bombay 400076, Maharashtra, India.
   [Zhou, Kaidi; Parker, Alice C.] Univ So Calif, Dept Elect Engn, Los Angeles, CA 90089 USA.
RP Patil, S (corresponding author), Indian Inst Technol, Dept Elect Engn, Bombay 400076, Maharashtra, India.
CR [Anonymous], 1989, ANALOG VLSI NEURAL S
   Bhatla N., NEURAL CIRCUITS TOUC
   Cangelosi A, 1997, NEURAL PROCESS LETT, V6, P91, DOI 10.1023/A:1009615807222
   Chalfie E. S. Martin, 1985, J NEUROSCI, V5, P956
   Friesz A. K., 2007, BIOM ENG C
   Irizarry-Valle Y, 2014, IEEE INT SYMP CIRC S, P261, DOI 10.1109/ISCAS.2014.6865115
   Joshi A. C. P. Jonathan, 2009, IEEE NIH LIF SCI SYS, P133
   Lockery SR, 2009, NAT NEUROSCI, V12, P377, DOI 10.1038/nn0409-377
   PARKER AC, 2008, 51 IEEE MIDW S CIRC, P818
   Pirri JK, 2012, CURR OPIN NEUROBIOL, V22, P187, DOI 10.1016/j.conb.2011.12.007
NR 10
TC 0
Z9 0
U1 0
U2 4
PY 2015
UT WOS:000370730600113
DA 2023-11-16
ER

PT J
AU Stasenko, SV
   Kazantsev, VB
AF Stasenko, Sergey V. V.
   Kazantsev, Victor B. B.
TI Dynamic Image Representation in a Spiking Neural Network Supplied by
   Astrocytes
SO MATHEMATICS
DT Article
DE spiking neural network; neuron-glial interactions; astrocyte
ID SYNAPTIC-TRANSMISSION; SYNCHRONIZATION; MODULATION; OSCILLATIONS; GLIA
AB The mathematical model of the spiking neural network (SNN) supplied by astrocytes is investigated. The astrocytes are a specific type of brain cells which are not electrically excitable but induce chemical modulations of neuronal firing. We analyze how the astrocytes influence images encoded in the form of the dynamic spiking pattern of the SNN. Serving at a much slower time scale, the astrocytic network interacting with the spiking neurons can remarkably enhance the image representation quality. The spiking dynamics are affected by noise distorting the information image. We demonstrate that the activation of astrocytes can significantly suppress noise influence, improving the dynamic image representation by the SNN.
C1 [Stasenko, Sergey V. V.; Kazantsev, Victor B. B.] Moscow Inst Phys & Technol, Moscow 117303, Russia.
RP Stasenko, SV (corresponding author), Moscow Inst Phys & Technol, Moscow 117303, Russia.
EM stasenko@neuro.nnov.ru
CR Amiri M, 2013, J COMPUT NEUROSCI, V34, P489, DOI 10.1007/s10827-012-0432-6
   Amiri M, 2012, J THEOR BIOL, V292, P60, DOI 10.1016/j.jtbi.2011.09.013
   Angulo MC, 2004, J NEUROSCI, V24, P6920, DOI 10.1523/JNEUROSCI.0473-04.2004
   Araque A, 1999, TRENDS NEUROSCI, V22, P208, DOI 10.1016/S0166-2236(98)01349-6
   Araque A, 1998, EUR J NEUROSCI, V10, P2129, DOI 10.1046/j.1460-9568.1998.00221.x
   De Pittà M, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1002293
   Fiacco TA, 2004, J NEUROSCI, V24, P722, DOI 10.1523/JNEUROSCI.2859-03.2004
   González-Arias C, 2019, SPR SER COMPUT NEURO, P213, DOI 10.1007/978-3-030-00817-8_8
   Gordleeva S., 2022, ARXIV
   Gordleeva SY, 2012, FRONT COMPUT NEUROSC, V6, DOI 10.3389/fncom.2012.00092
   Gordleeva SY, 2021, FRONT CELL NEUROSCI, V15, DOI 10.3389/fncel.2021.631485
   Halassa MM, 2009, NEUROPHARMACOLOGY, V57, P343, DOI 10.1016/j.neuropharm.2009.06.031
   Haydon PG, 2001, NAT REV NEUROSCI, V2, P185, DOI 10.1038/35058528
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Hunter JD, 2007, COMPUT SCI ENG, V9, P90, DOI 10.1109/MCSE.2007.55
   Izhikevich E.M., 2007, COMPUTATIONAL NEUROS
   Jourdain P, 2007, NAT NEUROSCI, V10, P331, DOI 10.1038/nn1849
   Kastanenka KV, 2020, GLIA, V68, P5, DOI 10.1002/glia.23632
   Kazantsev VB, 2009, PHYS REV E, V79, DOI 10.1103/PhysRevE.79.010901
   Kol A, 2020, NAT NEUROSCI, V23, P1229, DOI 10.1038/s41593-020-0679-6
   Lazarevich IA, 2017, JETP LETT+, V105, P210, DOI 10.1134/S0021364017030092
   LeCun Y., 1998, MNIST DATABASE HANDW
   Manninen T, 2021, GLIA, V69, pE335
   Manninen T, 2018, FRONT COMPUT NEUROSC, V12, DOI 10.3389/fncom.2018.00014
   Martín ED, 2007, GLIA, V55, P36, DOI 10.1002/glia.20431
   Nadkarni S, 2004, PHYS BIOL, V1, P35, DOI 10.1088/1478-3967/1/1/004
   Nadkarni S, 2007, PHYS BIOL, V4, P1, DOI 10.1088/1478-3975/4/1/001
   Noriega-Prieto JA, 2021, NEUROCHEM RES, V46, P2580, DOI 10.1007/s11064-021-03317-x
   Oliveira JF, 2022, GLIA, V70, P1455, DOI 10.1002/glia.24178
   Pankratova EV, 2019, NONLINEAR DYNAM, V97, P647, DOI 10.1007/s11071-019-05004-7
   Perea G, 2009, TRENDS NEUROSCI, V32, P421, DOI 10.1016/j.tins.2009.05.001
   Postnov DE, 2007, BIOSYSTEMS, V89, P84, DOI 10.1016/j.biosystems.2006.04.012
   Riera J, 2011, J INTEGR NEUROSCI, V10, P439, DOI 10.1142/S0219635211002877
   Rusakov DA, 1998, J NEUROSCI, V18, P3158
   Santello M, 2019, NAT NEUROSCI, V22, P154, DOI 10.1038/s41593-018-0325-8
   Sara U., 2019, J COMPUT COMMUN, V7, P8, DOI DOI 10.4236/JCC.2019.73002
   Stasenko Sergey V., 2020, Procedia Computer Science, V169, P704, DOI 10.1016/j.procs.2020.02.175
   Stimberg M, 2019, ELIFE, V8, DOI 10.7554/eLife.47314
   Tsybina Y, 2022, NEURAL COMPUT APPL, V34, P9147, DOI 10.1007/s00521-022-06936-9
   Ullah G, 2006, CELL CALCIUM, V39, P197, DOI 10.1016/j.ceca.2005.10.009
   van Rossum G, 1995, CSR9526 CWI
   Verisokin AY, 2021, FRONT CELL NEUROSCI, V15, DOI 10.3389/fncel.2021.645068
   Volman V, 2007, NEURAL COMPUT, V19, P303, DOI 10.1162/neco.2007.19.2.303
   Wade JJ, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0029445
   Wallach G, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003964
   Wang XJ, 1999, J NEUROSCI, V19, P9587
   Wang Y, 2022, FEBS J, V289, P2202, DOI 10.1111/febs.15878
   Wang Z, 2002, IEEE SIGNAL PROC LET, V9, P81, DOI 10.1109/97.995823
   Wittenberg GM, 2002, HIPPOCAMPUS, V12, P637, DOI 10.1002/hipo.10102
   Xiao H., 2017, ARXIV170807747
   Yang Rundong, 2021, Edge Computing and IoT: Systems, Management and Security. First EAI International Conference, ICECI 2020. Proceedings. Lecture Notes of the Institute for Computer Sciences, Social Informatics and Telecommunications Engineering (LNICST 368), P86, DOI 10.1007/978-3-030-73429-9_6
NR 51
TC 5
Z9 5
U1 2
U2 3
PD FEB
PY 2023
VL 11
IS 3
AR 561
DI 10.3390/math11030561
UT WOS:000931061500001
HC Y
HP N
DA 2023-11-16
ER

PT J
AU Zhao, FF
   Zeng, Y
   Xu, B
AF Zhao, Feifei
   Zeng, Yi
   Xu, Bo
TI A brain-inspired decision-making spiking neural network and its
   application in unmanned aerial vehicle(Vol 12, 56, 2022)
SO FRONTIERS IN NEUROROBOTICS
DT Correction
DE spiking neural network; brain-inspired decision-making; dopamine
   regulation; multiple brain areas coordination; reinforcement learning;
   UAV autonomous learning
C1 [Zhao, Feifei; Zeng, Yi; Xu, Bo] Chinese Acad Sci, Inst Automat, Res Ctr Brain Inspired Intelligence, Beijing, Peoples R China.
   [Zhao, Feifei; Zeng, Yi; Xu, Bo] Univ Chinese Acad Sci, Beijing, Peoples R China.
   [Zeng, Yi] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing, Peoples R China.
   [Zeng, Yi; Xu, Bo] Chinese Acad Sci, Ctr Excellence Brain Sci & Intelligence Technol, Shanghai, Peoples R China.
RP Zeng, Y (corresponding author), Chinese Acad Sci, Inst Automat, Res Ctr Brain Inspired Intelligence, Beijing, Peoples R China.; Zeng, Y (corresponding author), Univ Chinese Acad Sci, Beijing, Peoples R China.; Zeng, Y (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing, Peoples R China.; Zeng, Y (corresponding author), Chinese Acad Sci, Ctr Excellence Brain Sci & Intelligence Technol, Shanghai, Peoples R China.
EM yi.zeng@ia.ac.cn
CR Zhao FF, 2018, FRONT NEUROROBOTICS, V12, DOI 10.3389/fnbot.2018.00056
NR 1
TC 0
Z9 0
U1 4
U2 8
PD NOV 29
PY 2022
VL 16
AR 1092428
DI 10.3389/fnbot.2022.1092428
UT WOS:000898543300001
DA 2023-11-16
ER

PT C
AU Masuta, H
   Kubota, N
AF Masuta, Hiroyuki
   Kubota, Naoyuki
GP IEEE
TI The perception for partner robot using spiking neural network in dynamic
   environment
SO 2008 PROCEEDINGS OF SICE ANNUAL CONFERENCE, VOLS 1-7
DT Proceedings Paper
CT Annual Conference of the SICE
CY AUG 20-22, 2008
CL Chofu, JAPAN
DE Partner Robot; Spiking Neural Network; Perceiving-Acting Cycle
ID ATTENTION; SYSTEM
AB This paper discusses a perceptual system using spiking neural network for a partner robot from the viewpoint of human visual perception. Recently, various types of robots equip various types of sensors for perceiving the environment. However, the robot is difficult to perceive the necessary information like a human. In this study, we emphasize the importance of human vision for the robot to realize flexible perception. Therefore, we propose a retinal model for a laser range finder, and the information extraction method using a spiking neural network based on perceiving-acting cycle. We apply the proposed method for a human tracking task. As an experimental result, we show the robot can directly perceive the necessary information by the attention mechanism for the flexible perception according to spatiotemporal context based on the spiking neural network.
C1 [Masuta, Hiroyuki; Kubota, Naoyuki] Tokyo Metropolitan Univ, Dept Syst Design, Tokyo 158, Japan.
RP Masuta, H (corresponding author), Tokyo Metropolitan Univ, Dept Syst Design, Tokyo 158, Japan.
EM masuta-hiroyki@sd.tmu.ac.jp; kubota@tmu.ac.jp
CR Ando N, 2005, 2005 IEEE/RSJ International Conference on Intelligent Robots and Systems, Vols 1-4, P3555, DOI 10.1109/IROS.2005.1545521
   [Anonymous], 1999, J CONSCIOUSNESS STUD
   BROOKS RA, 1986, IEEE T ROBOTIC AUTOM, V2, P14, DOI 10.1109/JRA.1986.1087032
   CURCIO CA, 1990, J COMP NEUROL, V292, P497, DOI 10.1002/cne.902920402
   Deguchi K, 2005, LECT NOTES COMPUT SC, V3704, P386, DOI 10.1007/11565123_37
   FOGEL DB, 2003, COMPUTATIONAL INTELL
   Fukuda T, 1999, P IEEE, V87, P1448, DOI 10.1109/5.784220
   FUKUSHIMA K, 1987, APPL OPTICS, V26, P4985, DOI 10.1364/AO.26.004985
   Gibson J.J., 1979, ECOLOGICAL APPROACH
   GIBSON JAMES J., 1966
   Imai M, 2003, IEEE T IND ELECTRON, V50, P636, DOI 10.1109/TIE.2003.814769
   Kubota N, 2005, INFORM SCIENCES, V171, P403, DOI 10.1016/j.ins.2004.09.012
   Kubota N, 2002, ICONIP'02: PROCEEDINGS OF THE 9TH INTERNATIONAL CONFERENCE ON NEURAL INFORMATION PROCESSING, P2103, DOI 10.1109/ICONIP.2002.1199047
   Kubota N, 2007, IEEE T IND ELECTRON, V54, P866, DOI 10.1109/TIE.2007.891644
   Maass W., 1999, PULSED NEURAL NETWOR
   MACDOUGALL HG, 2005, J APPL PHYSL
   MASUTA H, 2008, INT J FACTO IN PRESS, P1828
   Nakauchi Y, 2002, AUTON ROBOT, V12, P313, DOI 10.1023/A:1015273816637
   *NEDO, 2006, REP FY2004 RES EXP F
   OGGIER T, 2003, P SOC PHOTO-OPT INS, V5249, P634
   OHARA K, 2006, SICE ICASE INT JOINT, P2629
   RAO RPN, 2003, P ART INT SIM BEH
   Roberts ME, 2003, LECT NOTES COMPUT SC, V2878, P655
   Russell S, 1995, ARTIFICIAL INTELLIGE
   SATO E, 2007, IEEE T IND ELECT, V54
   XIE M, 2003, MACHINE PERCEPTION A, V54
NR 26
TC 0
Z9 0
U1 0
U2 1
PY 2008
BP 1919
EP 1924
UT WOS:000263966701093
DA 2023-11-16
ER

PT C
AU Su, F
   Xu, Z
AF Su, Fang
   Xu, Zhan
BA Anonymous
BF Anonymous
TI Prediction of Sea Spikes and Target Detection Using Echo State Network
SO INTERNATIONAL CONFERENCE ON ELECTRICAL AND ELECTRONIC ENGINEERING (EEE
   2014)
DT Proceedings Paper
CT International Conference on Electrical and Electronic Engineering (EEE)
CY APR 26-27, 2014
CL Hong Kong, PEOPLES R CHINA
DE Sea clutter; Sea spikes; Echo state network
ID CLUTTER
AB In this paper, neural network method is firstly introduced to predict sea spikes in sea clutter background. Here we use a novel recurrent neural network (RNN) architecture named echo state network (ESN). The prediction performance is tested using IPIX radar real data. Experiment results show that ESN can predict sea spikes, and can also detect the target exactly. This idea is important for increasing detection performance of radar and decreasing false alarm rates.
C1 [Su, Fang] Natl Univ Def Technol, Coll Sci, Changsha 410073, Hunan, Peoples R China.
   [Xu, Zhan] Natl Univ Def Technol, Sch Elect Sci & Engn, Changsha 410073, Hunan, Peoples R China.
RP Su, F (corresponding author), Natl Univ Def Technol, Coll Sci, Changsha 410073, Hunan, Peoples R China.
CR [Anonymous], 2001, ECHO STATE APPROACH
   Greco M, 2010, IET RADAR SONAR NAV, V4, P239, DOI 10.1049/iet-rsn.2009.0088
   Ishii K, 2004, OCEANS '04 MTS/IEEE TECHNO-OCEAN '04, VOLS 1- 2, CONFERENCE PROCEEDINGS, VOLS. 1-4, P1205
   Jaeger H, 2004, SCIENCE, V304, P78, DOI 10.1126/science.1091277
   Jaeger H., 2003, ADV NEURAL INFORM PR, V15, P609
   Posner F, 2003, RADAR CONF, P38, DOI 10.1109/NRC.2003.1203377
   WATTS S, 1987, IEEE T AERO ELEC SYS, V23, P40, DOI 10.1109/TAES.1987.313334
NR 7
TC 2
Z9 2
U1 0
U2 0
PY 2014
BP 221
EP 224
UT WOS:000351923100042
DA 2023-11-16
ER

PT J
AU Tanaka, H
   Morie, T
   Aihara, K
AF Tanaka, Hideki
   Morie, Takashi
   Aihara, Kazuyuki
TI A CMOS Spiking Neural Network Circuit with Symmetric/Asymmetric STDP
   Function
SO IEICE TRANSACTIONS ON FUNDAMENTALS OF ELECTRONICS COMMUNICATIONS AND
   COMPUTER SCIENCES
DT Article
DE spiking neuron model; associative memory; spike-timing dependent
   synaptic plasticity (STDP); LSI implementation
AB In this paper, we propose an analog CMOS circuit which achieves spiking neural networks with spike-timing dependent synaptic plasticity (STDP). In particular, we propose a STDP circuit with symmetric function for the first time, and also we demonstrate associative memory operation in a Hopfield-type feedback network with STDP learning. In our spiking neuron model, analog information expressing processing results is given by the relative timing of spike firing events. It is well known that a biological neuron changes its synaptic weights by STDP, which provides learning rules depending on relative timing between asynchronous spikes. Therefore, STDP can be used for spiking neural systems with learning function. The measurement results of fabricated chips using TSMC 0.25 mu m CMOS process technology demonstrate that our spiking neuron circuit can construct feedback networks and. update synaptic weights based on relative timing between asynchronous spikes by a symmetric or an asymmetric STDP circuits.
C1 [Tanaka, Hideki; Morie, Takashi] Kyushu Inst Technol, Grad Sch Life Sci & Syst Engn, Kitakyushu, Fukuoka 8080196, Japan.
   [Aihara, Kazuyuki] Univ Tokyo, Inst Ind Sci, Tokyo 1538505, Japan.
   [Aihara, Kazuyuki] Japan Sci & Technol Agcy, ERATO, Aihara Complex Modelling Project, Tokyo, Japan.
RP Tanaka, H (corresponding author), Kyushu Inst Technol, Grad Sch Life Sci & Syst Engn, Kitakyushu, Fukuoka 8080196, Japan.
EM tanaka-hideki@edu.brain.kyutech.ac.jp; morie@brain.kyutech.ac.jp
CR Bi G.Q., 1993, J NEUROSCI, V18, P10462
   Bofill-i-Petit A, 2004, IEEE T NEURAL NETWOR, V15, P1296, DOI 10.1109/TNN.2004.832842
   INDIVERI G, 2004, P EUR S ART NEUR NET, P405
   Maass W., 1999, PULSED NEURAL NETWOR
   MAASS W, 1997, T SOC COMPUT SIMUL, V14, P1659
   Morie T, 2004, IEICE T ELECTRON, VE87C, P1856
   Nishiyama M, 2000, NATURE, V408, P584, DOI 10.1038/35046067
   Sasaki K, 2006, IEICE T ELECTRON, VE89C, P1637, DOI 10.1093/ietele/e89-c.11.1637
   TANAKA H, 2006, INT S NONL THEOR ITS, P495
   TANAKA H, 2005, INT S NONL THEOR ITS, P313
   Tanaka H., 2007, INT C SERIES, V1301, P152
   TANAKA H, 2007, 15 IEEE INT WORKSH N, P313
   THORPE SJ, 1996, RAPID VISUAL PROCESS, P901
   Tsukada M, 2005, HIPPOCAMPUS, V15, P104, DOI 10.1002/hipo.20035
NR 14
TC 44
Z9 50
U1 7
U2 27
PD JUL
PY 2009
VL E92A
IS 7
BP 1690
EP 1698
DI 10.1587/transfun.E92.A.1690
UT WOS:000268506600018
DA 2023-11-16
ER

PT C
AU Ghosh-Dastidar, S
   Adeli, H
AF Ghosh-Dastidar, Samanwoy
   Adeli, Hojjat
BE Yu, W
   Sanchez, EN
TI Third Generation Neural Networks: Spiking Neural Networks
SO ADVANCES IN COMPUTATIONAL INTELLIGENCE
SE Advances in Intelligent and Soft Computing
DT Proceedings Paper
CT 2nd International Workshop on Advanced Computational Intelligence
CY OCT 08-09, 2009
CL Mexico City, MEXICO
ID GRADIENT LEARNING ALGORITHM; UPLIFT LOAD-CAPACITY; WORK ZONE CAPACITY;
   COMPUTATIONAL POWER; ALZHEIMERS-DISEASE; MODEL; OPTIMIZATION; EPILEPSY;
   NEURONS; BACKPROPAGATION
AB Artificial Neural Networks (ANNs) are based oil highly simplified brain dynamics and have been used as powerful computational tools to solve complex pattern recognition, function estimation, and classification problems. Throughout their development, ANNs have been evolving towards more powerful and more biologically realistic models. In the last decade, the third generation Spiking Neural Networks (SNNs) have been developed which comprise of spiking neurons. Information transfer in these neurons models the information transfer ill biological neurons, i.e., via the precise timing of spikes or a sequence of spikes. Addition of the temporal dimension for information encoding in SNNs yields new insight into the dynamics of the human bra,in and has the potential to result in compact representations of large neural networks. As Such, SNNs have great potential for solving complicated time-dependent pattern recognition problems defined by time series because of their inherent dynamic representation. This article presents all overview of the development of spiking neurons and SNNs within the context; of feed-forward networks, and provides insight into their potential for becoming the next generation neural networks.
C1 [Ghosh-Dastidar, Samanwoy] Ohio State Univ, Dept Biomed Engn, 470 Hitchcock Hall,2070 Neil Ave, Columbus, OH 43210 USA.
   [Adeli, Hojjat] Ohio State Univ, Dept Biomed Engn Biomed Informat, Columbus, OH 43210 USA.
   [Adeli, Hojjat] Ohio State Univ, Dept Civil & Environm Engn, Columbus, OH 43210 USA.
   [Adeli, Hojjat] Ohio State Univ, Dept Geodet Sci, Columbus, OH 43210 USA.
   [Adeli, Hojjat] Ohio State Univ, Dept Elect & Comp Engn, Columbus, OH 43210 USA.
   [Adeli, Hojjat] Ohio State Univ, Dept Neurosci, Columbus, OH 43210 USA.
RP Ghosh-Dastidar, S (corresponding author), Ohio State Univ, Dept Biomed Engn, 470 Hitchcock Hall,2070 Neil Ave, Columbus, OH 43210 USA.
CR ABBOTT LF, 1990, STAT MECH NEURAL NET
   ADELI H, 1995, NEURAL NETWORKS, V8, P769, DOI 10.1016/0893-6080(95)00026-V
   Adeli H, 2000, COMPUT-AIDED CIV INF, V15, P251, DOI 10.1111/0885-9507.00189
   Adeli H, 2000, J TRANSP ENG, V126, P464, DOI 10.1061/(ASCE)0733-947X(2000)126:6(464)
   Adeli H, 2006, J STRUCT ENG, V132, P102, DOI 10.1061/(ASCE)0733-9445(2006)132:1(102)
   Adeli H, 2005, J ALZHEIMERS DIS, V7, P187
   Adeli H, 2005, CLIN EEG NEUROSCI, V36, P131, DOI 10.1177/155005940503600303
   Adeli H, 2003, J TRANSP ENG, V129, P484, DOI 10.1061/(ASCE)0733-947X(2003)129:5(484)
   Adeli H, 2001, COMPUT-AIDED CIV INF, V16, P126, DOI 10.1111/0885-9507.00219
   Adeli H, 1998, J CONSTR ENG M, V124, P18, DOI 10.1061/(ASCE)0733-9364(1998)124:1(18)
   ADELI H, 1994, APPL MATH COMPUT, V62, P81, DOI 10.1016/0096-3003(94)90134-1
   Adeli H, 1997, J STRUCT ENG-ASCE, V123, P1535, DOI 10.1061/(ASCE)0733-9445(1997)123:11(1535)
   ADELI H, 1993, INT J SUPERCOMPUT AP, V7, P155, DOI 10.1177/109434209300700206
   ADELI H, 1995, J STRUCT ENG-ASCE, V121, P1205, DOI 10.1061/(ASCE)0733-9445(1995)121:8(1205)
   Adeli H., 1995, MACHINE LEARNING NEU
   [Anonymous], 1998, NEUROCOMPUTING DESIG
   [Anonymous], 1996, NEURAL NETWORK FUNDA
   [Anonymous], 2005, WAVELETS INTELLIGENT
   [Anonymous], INT C NEUR NETW SAN
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Booij O, 2005, INFORM PROCESS LETT, V95, P552, DOI 10.1016/j.ipl.2005.05.023
   Dharia A, 2003, ENG APPL ARTIF INTEL, V16, P607, DOI 10.1016/j.engappai.2003.09.011
   Ermentrout B, 1996, NEURAL COMPUT, V8, P979, DOI 10.1162/neco.1996.8.5.979
   ERMENTROUT GB, 1986, SIAM J APPL MATH, V46, P233, DOI 10.1137/0146017
   Fahlman S. E., 1988, P 1988 CONN MOD SUMM, P38
   GERSTNER W, 1995, PHYS REV E, V51, P738, DOI 10.1103/PhysRevE.51.738
   Gerstner W., 2002, SPIKING NEURON MODEL
   Ghosh-Dastidar S, 2006, J TRANSP ENG, V132, P331, DOI 10.1061/(ASCE)0733-947X(2006)132:4(331)
   Ghosh-Dastidar S, 2003, COMPUT-AIDED CIV INF, V18, P325, DOI 10.1111/1467-8667.t01-1-00311
   Ghosh-Dastidar S, 2008, IEEE T BIO-MED ENG, V55, P512, DOI 10.1109/TBME.2007.905490
   Ghosh-Dastidar S, 2007, IEEE T BIO-MED ENG, V54, P1545, DOI 10.1109/TBME.2007.891945
   Ghosh-Dastidar S, 2007, INTEGR COMPUT-AID E, V14, P187
   GHOSHDASTIDAR S, 2009, NEURAL NETW IN PRESS, V22
   HILL B, 1992, IONIC CHANNELS EXCIT
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Hoppensteadt FC, 1997, WEAKLY CONNECTED NEU
   HUNG SL, 1993, NEUROCOMPUTING, V5, P287, DOI 10.1016/0925-2312(93)90042-2
   Izhikevich EM, 2001, NEURAL NETWORKS, V14, P883, DOI 10.1016/S0893-6080(01)00078-8
   Izhikevich EM, 2007, CEREB CORTEX, V17, P2443, DOI 10.1093/cercor/bhl152
   Jiang XM, 2008, INT J NUMER METH ENG, V75, P770, DOI 10.1002/nme.2274
   Jiang XM, 2008, INT J NUMER METH ENG, V74, P1045, DOI 10.1002/nme.2195
   Jiang X, 2007, INT J NUMER METH ENG, V71, P606, DOI 10.1002/nme.1964
   Jiang XM, 2005, J TRANSP ENG, V131, P771, DOI 10.1061/(ASCE)0733-947X(2005)131:10(771)
   Kandel E. R., 2000, PRINCIPLES NEURAL SC, V4
   Karim A, 2003, J TRANSP ENG, V129, P494, DOI 10.1061/(ASCE)0733-947X(2003)129:5(494)
   Karim A, 2002, J TRANSP ENG-ASCE, V128, P21, DOI 10.1061/(ASCE)0733-947X(2002)128:1(21)
   KEPLER TB, 1992, BIOL CYBERN, V66, P381, DOI 10.1007/BF00197717
   Kistler WM, 1997, NEURAL COMPUT, V9, P1015, DOI 10.1162/neco.1997.9.5.1015
   Maass W, 1996, NEURAL COMPUT, V8, P1, DOI 10.1162/neco.1996.8.1.1
   Maass W, 1997, NEURAL COMPUT, V9, P279, DOI 10.1162/neco.1997.9.2.279
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maass W, 1997, ADV NEUR IN, V9, P211
   McKennoch S, 2006, IEEE IJCNN, P3970
   MOORE S, 2002, THESIS U BATH
   Natschlager T, 1998, NETWORK-COMP NEURAL, V9, P319, DOI 10.1088/0954-898X/9/3/003
   Panakkat A, 2007, INT J NEURAL SYST, V17, P13, DOI 10.1142/S0129065707000890
   Panakkat A, 2009, COMPUT-AIDED CIV INF, V24, P280, DOI 10.1111/j.1467-8667.2009.00595.x
   Park HS, 1997, J STRUCT ENG, V123, P880, DOI 10.1061/(ASCE)0733-9445(1997)123:7(880)
   Rinzel J, 1989, METHODS NEURAL MODEL
   Rumelhart D.E., 1987, LEARNING INTERNAL RE, P318
   Schrauwen B, 2004, IEEE IJCNN, P471, DOI 10.1109/IJCNN.2004.1379954
   SEJNOWSKI TJ, 1986, PARALLEL DISTRIBUTED, V2, P372
   Silva SM, 2005, PROCEEDINGS OF THE 2005 INTERNATIONAL CONFERENCE ON NEURAL NETWORKS AND BRAIN, VOLS 1-3, P1354
   Sirca GF, 2003, J STRUCT ENG-ASCE, V129, P562, DOI 10.1061/(ASCE)0733-9445(2003)129:4(562.2)
   Sirca GF, 2001, J STRUCT ENG, V127, P1276, DOI 10.1061/(ASCE)0733-9445(2001)127:11(1276)
   Xin JG, 2001, IEEE IJCNN, P1772, DOI 10.1109/IJCNN.2001.938430
NR 66
TC 40
Z9 40
U1 4
U2 18
PY 2009
VL 61
BP 167
EP +
UT WOS:000273977700017
DA 2023-11-16
ER

PT J
AU Aoki, T
   Aoyac, T
AF Aoki, T
   Aoyac, T
TI A possible role of incoming spike synchrony in associative memory model
   with STDP learning rule
SO PROGRESS OF THEORETICAL PHYSICS SUPPLEMENT
DT Article; Proceedings Paper
CT International Symposium on Oscillation, Chaos and Network Dynamics in
   Nonlinear Science
CY NOV 25-28, 2004
CL Kyoto Univ, Kyoto, JAPAN
HO Kyoto Univ
ID OSCILLATIONS; MODULATION
AB Although recent experiments suggested that a synchronous neural activity may play a significant functional role in cognitive processes, it is still unclear how the spike synchronization affects the network behavior in neural systems. In the first approach, we consider a specific problem: how the retrieval state of associative memory model of neural network is affected by incoming spike synchronization. Considering a simple network of Leaky-Integrate-and-Fire neurons, organizing through the learning rule by spike-timing dependent plasticity (STDP), we present that this network exhibits the nature of associative memory as a standard Hopfield model for asynchronous input, whereas a transition between the learned spike-trains can be triggered by a brief incoming spike-synchrony.
C1 Kyoto Univ, Dept Phys, Grad Sch Sci, Kyoto 6068502, Japan.
   Kyoto Univ, Grad Sch Informat, Dept Appl Anal & Complex Dynam Syst, Kyoto 6068501, Japan.
RP Aoki, T (corresponding author), Kyoto Univ, Dept Phys, Grad Sch Sci, Kyoto 6068502, Japan.
EM aoki@scphys.kyoto-u.ac.jp; aoyagi@acs.i.kyoto-u.ac.jp
CR Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Engel AK, 2001, NAT REV NEUROSCI, V2, P704, DOI 10.1038/35094565
   Fries P, 2001, SCIENCE, V291, P1560, DOI 10.1126/science.1055465
   GRAY CM, 1989, NATURE, V338, P334, DOI 10.1038/338334a0
   HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554
   Lee D, 2003, J NEUROSCI, V23, P6798
   Riehle A, 1997, SCIENCE, V278, P1950, DOI 10.1126/science.278.5345.1950
NR 7
TC 7
Z9 7
U1 0
U2 1
PY 2006
IS 161
BP 152
EP 155
UT WOS:000237233300015
DA 2023-11-16
ER

PT C
AU Belatreche, A
   Maguire, LP
   McGinnity, TM
AF Belatreche, A
   Maguire, LP
   McGinnity, TM
BE Ruan, D
   DHondt, P
   DeCock, M
   Nachtegael, M
   Kerre, EE
TI Pattern recognition with spiking neural networks and dynamic synapses
SO APPLIED COMPUTATIONAL INTELLIGENCE
DT Proceedings Paper
CT 6th International Conference on Fuzzy Logic and Intelligent Technologies
   in Nuclear Science
CY SEP 01-03, 2004
CL Blankenberge, BELGIUM
AB Spiking neural networks represent a more plausible model of real biological neurons where time is considered as an important feature for information representation and processing in the human brain. In this paper, we apply spiking neural networks with dynamic synapses for pattern recognition in multidimensional data. The neurons are based on the integrate and-fire model, and are connected using a biologically plausible model of dynamic synapses. Unlike the conventional synapse employed in artificial neural networks, which is considered as a static entity with a fixed weight, the dynamic synapse (weightless synapse) efficacy changes upon the arrival of input spikes, and depends on the temporal structure of the impinging spike train. The training of the free parameters of the spiking network is performed using an evolutionary strategy (ES) where real values are used to encode the dynamic synapse parameters, which underlie the learning process.. The results show that spiking neurons with dynamic synapses are capable of pattern recognition by means of spatio-temporal encoding.
C1 Univ Ulster, Fac Engn, Sch Comp & Intelligent Syst, Intelligeng Syst Engn Lab, Derry BT48 7JL, North Ireland.
RP Belatreche, A (corresponding author), Univ Ulster, Fac Engn, Sch Comp & Intelligent Syst, Intelligeng Syst Engn Lab, Magee Campus,Northland Rd, Derry BT48 7JL, North Ireland.
CR Bäck T, 1993, EVOL COMPUT, V1, P1, DOI 10.1162/evco.1993.1.1.1
   Belatreche A, 2003, PROCEEDINGS OF THE 7TH JOINT CONFERENCE ON INFORMATION SCIENCES, P1524
   Bohte S, 2000, P EUR S ART NEUR NET, P419
   Fuhrmann G, 2002, J NEUROPHYSIOL, V87, P140, DOI 10.1152/jn.00258.2001
   GERSTNER W, 1995, PHYS REV E, V51, P738, DOI 10.1103/PhysRevE.51.738
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   Maass W., 1996, Proceedings of the Seventh Australian Conference on Neural Networks (ACNN'96), P1
   Mangasarian O.L., 1990, CANC DIAGNOSIS VIA L
   Markram H, 1996, NATURE, V382, P807, DOI 10.1038/382807a0
   THORPE SJ, 1996, NATURE, P381
   Tsodyks M, 1998, NEURAL COMPUT, V10, P821, DOI 10.1162/089976698300017502
   Yao X, 1999, IEEE T EVOLUT COMPUT, V3, P82, DOI 10.1109/4235.771163
NR 12
TC 3
Z9 3
U1 0
U2 2
PY 2004
BP 205
EP 210
DI 10.1142/9789812702661_0040
UT WOS:000228784700037
DA 2023-11-16
ER

PT J
AU Nomura, O
   Sakemi, Y
   Hosomi, T
   Morie, T
AF Nomura, Osamu
   Sakemi, Yusuke
   Hosomi, Takeo
   Morie, Takashi
TI Robustness of Spiking Neural Networks Based on Time-to-First-Spike
   Encoding Against Adversarial Attacks
SO IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS II-EXPRESS BRIEFS
DT Article
DE Neurons; Timing; Robustness; Membrane potentials; Encoding; Perturbation
   methods; Image recognition; Spiking neural networks; time-to-first-spike
   encoding; adversarial attack
AB Spiking neural networks (SNNs) more closely mimic the human brain than artificial neural networks (ANNs). For SNNs, time-to-first-spike (TTFS) encoding, which represents the output values of neurons based on the timing of a single spike, has been proposed as a promising model to reduce power consumption. Adversarial attacks that can lead ANNs to misrecognize images have been reported in many studies. However, the characteristics of TTFS-based SNNs trained using a backpropagation algorithm against adversarial attacks have not yet been clarified. In particular, the dependence of the robustness against adversarial attacks on spike timings has not been investigated. In this brief, we investigated the robustness of SNNs against adversarial attacks and compared it with that of an ANN. We found that SNNs trained with the appropriate temporal penalty settings are more robust against adversarial images than ANNs.
C1 [Nomura, Osamu; Morie, Takashi] Kyushu Inst Technol, Grad Sch Life Sci & Syst Engn, Fukuoka 8080196, Japan.
   [Nomura, Osamu; Morie, Takashi] Kyushu Inst Technol, Res Ctr Neuromorph AI Hardware, Fukuoka 8080196, Japan.
   [Sakemi, Yusuke] Chiba Inst Technol, Res Ctr Math Engn, Chiba 2750016, Japan.
   [Hosomi, Takeo] NEC Corp Ltd, Digital Technol Dev Labs, Kawasaki, Kanagawa 2118666, Japan.
RP Nomura, O (corresponding author), Kyushu Inst Technol, Grad Sch Life Sci & Syst Engn, Fukuoka 8080196, Japan.; Nomura, O (corresponding author), Kyushu Inst Technol, Res Ctr Neuromorph AI Hardware, Fukuoka 8080196, Japan.
EM nomura@brain.kyutech.ac.jp
CR Auge D, 2021, NEURAL PROCESS LETT, V53, P4693, DOI 10.1007/s11063-021-10562-2
   Delacre M, 2017, INT REV SOC PSYCHOL, V30, P92, DOI 10.5334/irsp.82
   Goodfellow I. J., 2015, P 3 INT C LEARN REPR, P1
   Jia XJ, 2019, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2019.00624
   Liang L., 2020, IEEE T NEUR NET LEAR, DOI DOI 10.1109/TNNLS.2021.3106961
   Nanfa G., 2020, IEEE IJCNN, P1
   Papernot N, 2016, P IEEE S SECUR PRIV, P582, DOI 10.1109/SP.2016.41
   Picard D, 2021, Arxiv, DOI arXiv:2109.08203
   Richards BA, 2019, NAT NEUROSCI, V22, P1761, DOI 10.1038/s41593-019-0520-2
   Sakemi Y, 2023, IEEE T NEUR NET LEAR, V34, P394, DOI 10.1109/TNNLS.2021.3095068
   Sharmin Saima, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12374), P399, DOI 10.1007/978-3-030-58526-6_24
   Sharmin S., 2019, IEEE IJCNN, P1, DOI DOI 10.1109/ijcnn.2019.8851732
   Theagarajan R, 2019, PROC CVPR IEEE, P6981, DOI 10.1109/CVPR.2019.00715
   Thorpe SJ, 1997, ADV NEUR IN, V9, P901
   Warde-Farley D, 2016, NEURAL INF PROCESS S, P311
   Xu WL, 2018, 25TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2018), DOI 10.14722/ndss.2018.23198
NR 16
TC 5
Z9 5
U1 6
U2 16
PD SEP
PY 2022
VL 69
IS 9
BP 3640
EP 3644
DI 10.1109/TCSII.2022.3184313
UT WOS:000848263100007
DA 2023-11-16
ER

PT C
AU Han, FY
   Li, RM
   Qian, DW
AF Han, Fengyang
   Li, Runmin
   Qian, Dianwei
GP IEEE
TI Short-term Wind Speed Forecasting Model Based on Spiking Neural Network
SO 2018 INTERNATIONAL CONFERENCE ON ADVANCED MECHATRONIC SYSTEMS (ICAMECHS)
SE International Conference on Advanced Mechatronic Systems
DT Proceedings Paper
CT International Conference on Advanced Mechatronic Systems (ICAMechS)
CY AUG 30-SEP 02, 2018
CL Zhengzhou, PEOPLES R CHINA
DE wind speed prediction; spiking neural network; spike response model
   (SRM); SpikeProp algorithm
AB Short-term wind speed forecasting plays an important role in the daily power system operation. Therefore, this paper presents a novel model based on spiking neural network (SNN) used spike response model (SRM). Further, to achieve both smaller training errors and higher precision forecasting, the basic SpikeProp learning algorithm is improved by adaptively adjusting the learning rate and adding momentum items. Then, this paper selects the actual sampling data from a wind farm to verify the effectiveness and advantages of the proposed model.
C1 [Han, Fengyang; Li, Runmin; Qian, Dianwei] North China Elect Power Univ, Sch Control & Comp Engn, Beijing 102206, Peoples R China.
RP Han, FY (corresponding author), North China Elect Power Univ, Sch Control & Comp Engn, Beijing 102206, Peoples R China.
CR Gerstner W, 2009, ENCY NEUROSCIENCE, P277
   Kasabov NK, 2014, NEURAL NETWORKS, V52, P62, DOI 10.1016/j.neunet.2014.01.006
   Kou P, 2014, ENERG CONVERS MANAGE, V84, P649, DOI 10.1016/j.enconman.2014.04.051
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Okumus I, 2016, ENERG CONVERS MANAGE, V123, P362, DOI 10.1016/j.enconman.2016.06.053
   Quan H, 2014, IEEE T NEUR NET LEAR, V25, P303, DOI 10.1109/TNNLS.2013.2276053
   Schrauwen B, 2004, IEEE IJCNN, P471, DOI 10.1109/IJCNN.2004.1379954
   Sharma V., 2010, P 2010 INT JOINT C N, P1
   Xin JG, 2001, IEEE IJCNN, P1772, DOI 10.1109/IJCNN.2001.938430
   Yu Q, 2014, NEUROCOMPUTING, V138, P3, DOI 10.1016/j.neucom.2013.06.052
NR 10
TC 3
Z9 3
U1 0
U2 1
PY 2018
BP 359
EP 363
UT WOS:000454629700068
DA 2023-11-16
ER

PT J
AU Almomani, A
   Alauthman, M
   Alweshah, M
   Dorgham, O
   Albalas, F
AF Almomani, Ammar
   Alauthman, Mohammad
   Alweshah, Mohammed
   Dorgham, O.
   Albalas, Firas
TI A comparative study on spiking neural network encoding schema:
   implemented with cloud computing
SO CLUSTER COMPUTING-THE JOURNAL OF NETWORKS SOFTWARE TOOLS AND
   APPLICATIONS
DT Article
DE Spiking neural network; Encoding schema; Comprehensive study; Cloud
   computing
ID MODEL; COMPUTATION; NEURONS
AB Spiking neural networks (SNN) represents the third generation of neural network models, it differs significantly from the early neural network generation. The time is becoming the most important input. The presence and precise timing of spikes encapsulate have a meaning such as human brain behavior. However, deferent techniques are therefore required to submit a stimulus to the neural network to build the timing spike. The characteristics of these spikes are based on their firing time because of the stereotypical nature of the human brain. Neural networks (NN) as engineering tools Operate on analog quantities (analog input, analog output), SNN More powerful than classic NN Interesting to implement in hardware. But the Problem that is internally work with spike trains unequal analog signal, so this algorithm design to firstly convert analog function into spike trains which calling encoding (E) then Convert spike trains into analog function: which calling decoding (D), so to use spiking NN as engineering tool: communication problem must be solved using some international encoding algorithms. This paper discusses techniques of transforming data into a suitable form for SNN submission. We present a comparative study on SNN encoding schema that effect on SNN performance in hardware and software implementation, however, this is the first comprehensive study to discuss encoding algorithms in SNNs in details, which involved the advantages, disadvantages and when and where we can use and implements the encoding algorithms, with focusing on some examples implement SNN in cloud computing generally, and which algorithms still unused in the world of cloud computing to make the door open for new researcher.
C1 [Almomani, Ammar] Al Balqa Appl Univ, Al Huson Univ Coll, IT Dept, POB 50, Irbid, Jordan.
   [Alauthman, Mohammad] Zarqa Univ, Fac Informat Technol, Dept Comp Sci, Zarqa, Jordan.
   [Alweshah, Mohammed; Dorgham, O.] Al Balqa Appl Univ, Prince Abdullah Ben Ghazi Fac Informat Technol, Al Salt 19117, Jordan.
   [Albalas, Firas] Jordan Univ Sci & Technol, Dept Comp Sci, POB 3030, Irbid, Jordan.
RP Almomani, A (corresponding author), Al Balqa Appl Univ, Al Huson Univ Coll, IT Dept, POB 50, Irbid, Jordan.
EM ammarnav6@bau.edu.jo; malauthman@zu.edu.jo; weshah@bau.edu.jo;
   o.dorgham@bau.edu.jo; faalbalas@just.edu.jo
CR Ahn S, 2015, IEEE T CIRC SYST VID, V25, P422, DOI 10.1109/TCSVT.2014.2360031
   Alien R. A, 2017, Q DISADVANTAGES DIST
   Almomani A, 2018, INT J CLOUD APPL COM, V8, P96, DOI 10.4018/IJCAC.2018040105
   Andrew AM, 2003, KYBERNETES, V100, P100
   [Anonymous], 2011, INT WORKSHOP CRITICA
   [Anonymous], 2012, 2012 INT JOINT C NEU, DOI DOI 10.1109/VCIP.2012.6410775
   [Anonymous], 2014, NANOPHOTONIC INFORM
   Belatreche A, 2007, SOFT COMPUT, V11, P239, DOI [10.1007/s00500-006-0065-7, 10.1007/S00500-006-0065-7]
   Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   Booij O, 2005, INFORM PROCESS LETT, V95, P552, DOI 10.1016/j.ipl.2005.05.023
   Bouton C, 2017, J INTERN MED, V282, P37, DOI 10.1111/joim.12610
   Brody CD, 2003, NEURON, V37, P843, DOI 10.1016/S0896-6273(03)00120-X
   Chadha A., 2017, ARXIV171005112
   Cruz B, 2016, MCAFEE LABS THREATS
   Dayan P., 2001, THEORETICAL NEUROSCI
   Delbruck T, 2007, IEEE INT SYMP CIRC S, P845, DOI 10.1109/ISCAS.2007.378038
   Delorme A, 2001, NEUROCOMPUTING, V38, P539, DOI 10.1016/S0925-2312(01)00403-9
   Delorme A, 2001, NEURAL NETWORKS, V14, P795, DOI 10.1016/S0893-6080(01)00049-1
   Dhilipan A, 2014, INT J RES ADVENT TEC, V2, P121
   Dhoble K, 2013, SPATIO SPECTRO TEMPO
   Donachy S, 2015, THESIS
   Du DK, 2013, ANALOG INTEGR CIRC S, V75, P447, DOI 10.1007/s10470-013-0041-y
   Eurich CW, 2000, NEURAL COMPUT, V12, P1519, DOI 10.1162/089976600300015240
   Gabbiani F, 1999, J EXP BIOL, V202, P1267
   GABBIANI F, 1998, NETW COMPUT NEURAL S, V7, P61
   Gardner B, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0161335
   Gerstner W, 2001, MATH MODELL, V13, P23
   GERSTNER W, 1995, PHYS REV E, V51, P738, DOI 10.1103/PhysRevE.51.738
   Gerstner W., 2002, SPIKING NEURON MODEL
   Gross CG, 2002, NEUROSCIENTIST, V8, P512, DOI 10.1177/107385802237175
   Hamed H, 2010, AUST J INTEL INF PRO, V11, P23
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V116, P449, DOI 10.1113/jphysiol.1952.sp004717
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   Hough M., 1999, INT C ROB ART LIF
   Izhikevich E.M., 2008, SIAM REV, V50, P397
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Kasabov N, 2016, NEURAL NETWORKS, V78, P1, DOI 10.1016/j.neunet.2015.09.011
   Kasabov N, 2013, NEURAL NETWORKS, V41, P188, DOI 10.1016/j.neunet.2012.11.014
   Kiselev M, 2016, IEEE IJCNN, P1355, DOI 10.1109/IJCNN.2016.7727355
   Kistler WM, 1997, NEURAL COMPUT, V9, P1015, DOI 10.1162/neco.1997.9.5.1015
   Koch C, 1999, LINEAR STIMULUS ENCO
   Korkin M, 2000, SECOND NASA/DOD WORKSHOP ON EVOLVABLE HARDWARE, PROCEEDINGS, P173, DOI 10.1109/EH.2000.869355
   Lichtsteiner P., 2005, 2005 PhD Research in Microelectronics and Electronics (IEEE Cat. No.05EX1148), P206, DOI 10.1109/RME.2005.1542972
   Loiselle S, 2005, IEEE IJCNN, P2076
   Maass W, 1998, PULSED NEURAL NETWORKS, P55
   Maass W, 2001, PULSED NEURAL NETWOR
   Maguire LP, 2007, NEUROCOMPUTING, V71, P13, DOI 10.1016/j.neucom.2006.11.029
   Martens MB, 2017, J COMPUT NEUROSCI, V42, P87, DOI 10.1007/s10827-016-0629-1
   Martinelli E, 2006, SENSOR ACTUAT B-CHEM, V119, P234, DOI 10.1016/j.snb.2005.12.029
   Meftah B., 2013, ARTIF INTELL, P525
   Muhammad C, 2014, THESIS
   Olshausen BA, 2003, J COGNITIVE NEUROSCI, V15, P154, DOI 10.1162/089892903321107891
   Paugam-Moisy H., 2012, HDB NATURAL COMPUTIN, V1, P1, DOI [DOI 10.1007/978-3-540-92910-9_10, 10.1007/978-3-540-92910-9_10]
   Ray S, 2015, EXP BRAIN RES, V233, P459, DOI 10.1007/s00221-014-4127-2
   Schliebs S, 2009, NEURAL NETWORKS, V22, P623, DOI 10.1016/j.neunet.2009.06.038
   Schrauwen B, 2003, IEEE IJCNN, P2825
   Sen Bhattacharya B., 2006, WORKSH SCH COMP SCI
   Szatmáry B, 2010, PLOS COMPUT BIOL, V6, DOI 10.1371/journal.pcbi.1000879
   Thangamalar C, 2017, INT J ENG TECH, V3, P504
   Thorpe S, 1998, COMPUTATIONAL NEUROSCIENCE: TRENDS IN RESEARCH, P113
   Thorpe SJ, 2012, COMPUT NEUROSCI-MIT, P23
   Torikai Hiroyuki, 2009, Proceedings 2009 International Joint Conference on Neural Networks (IJCNN 2009 - Atlanta), P3132, DOI 10.1109/IJCNN.2009.5178837
   Valadez S, 2015, LECT NOTES COMPUT SC, V9116, P282, DOI 10.1007/978-3-319-19264-2_27
   van der Meer MAA, 2017, HIPPOCAMPUS, V27, P580, DOI 10.1002/hipo.22714
   Van Rullen R, 2001, NEURAL COMPUT, V13, P1255, DOI 10.1162/08997660152002852
   Wysoski SG, 2010, NEURAL NETWORKS, V23, P819, DOI 10.1016/j.neunet.2010.04.009
   Yu Q, 2017, INTEL SYST REF LIBR, V126, P19, DOI 10.1007/978-3-319-55310-8_2
   Yu Q, 2014, NEUROCOMPUTING, V138, P3, DOI 10.1016/j.neucom.2013.06.052
   Zuppicich A., 2008, ADV NEUR PROC 15 I 1, P1129
NR 70
TC 12
Z9 14
U1 0
U2 6
PD JUN
PY 2019
VL 22
IS 2
SI SI
BP 419
EP 433
DI 10.1007/s10586-018-02891-0
UT WOS:000467905600008
DA 2023-11-16
ER

PT C
AU Neil, D
   Liu, SC
AF Neil, Daniel
   Liu, Shih-Chii
GP IEEE
TI Effective Sensor Fusion with Event-Based Sensors and Deep Network
   Architectures
SO 2016 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS (ISCAS)
SE IEEE International Symposium on Circuits and Systems
DT Proceedings Paper
CT IEEE International Symposium on Circuits and Systems (ISCAS)
CY MAY 22-25, 2016
CL Montreal, CANADA
DE Event-Driven Sensors; Deep Networks; Recurrent Neural Networks; Dynamic
   Vision Sensor; Sensor Fusion
ID DRIVEN
AB The use of spiking neuromorphic sensors with state-of-art deep networks is currently an active area of research. Still relatively unexplored are the pre-processing steps needed to transform spikes from these sensors and the types of network architectures that can produce high-accuracy performance using these sensors. This paper discusses several methods for preprocessing the spiking data from these sensors for use with various deep network architectures. The outputs of these preprocessing methods are evaluated using different networks including a deep fusion network composed of Convolutional Neural Networks and Recurrent Neural Networks, to jointly solve a recognition task using the MNIST (visual) and TIDIGITS (audio) benchmark datasets. With only 1000 visual input spikes from a spiking hardware retina, the classification accuracy of 64.5% achieved by a particular trained fusion network increases to 98.31% when combined with inputs from a spiking hardware cochlea.
C1 [Neil, Daniel] Univ Zurich, Inst Neuroinformat, Winterthurerstr 190, CH-8057 Zurich, Switzerland.
   Swiss Fed Inst Technol, Winterthurerstr 190, CH-8057 Zurich, Switzerland.
RP Neil, D (corresponding author), Univ Zurich, Inst Neuroinformat, Winterthurerstr 190, CH-8057 Zurich, Switzerland.
CR Pérez-Carrasco JA, 2013, IEEE T PATTERN ANAL, V35, P2706, DOI 10.1109/TPAMI.2013.71
   CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851
   Cho K., 2014, ARXIV14061078, V1406, P1078
   Chollet F, KERAS
   Diehl PU, 2015, IEEE IJCNN
   Farabet C, 2012, FRONT NEUROSCI-SWITZ, V6, DOI 10.3389/fnins.2012.00032
   Kingma DP., 2017, ARXIV
   Kiselev I., 2016, 2016 IEEE INT S CIRC
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lichtsteiner Patrick, 2008, IEEE Journal of Solid-State Circuits, V43, P566, DOI 10.1109/JSSC.2007.914337
   Liu SC, 2014, IEEE T BIOMED CIRC S, V8, P453, DOI 10.1109/TBCAS.2013.2281834
   Neil D, 2014, IEEE T VLSI SYST, V22, P2621, DOI 10.1109/TVLSI.2013.2294916
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   Orchard G, 2015, FRONT NEUROSCI-SWITZ, V9, DOI [10.3389/fhins.2015.00437, 10.3389/fnins.2015.00437]
   Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131
   Stromatias E, 2015, IEEE INT SYMP CIRC S, P1901, DOI 10.1109/ISCAS.2015.7169034
   Stromatias E, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00222
   Zai AT, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00347
NR 18
TC 61
Z9 62
U1 0
U2 4
PY 2016
BP 2282
EP 2285
UT WOS:000390094702103
DA 2023-11-16
ER

PT J
AU Xiao, R
   Wan, Y
   Yang, BS
   Zhang, HB
   Tang, HJ
   Wong, DF
   Chen, BX
AF Xiao, Rong
   Wan, Yu
   Yang, Baosong
   Zhang, Haibo
   Tang, Huajin
   Wong, Derek F. F.
   Chen, Boxing
TI Towards Energy-Preserving Natural Language Understanding With Spiking
   Neural Networks
SO IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING
DT Article
DE Natural language processing; language model; natural language
   understanding; spiking neural network
AB neural networks have shown promising results in a variety of natural language understanding (NLU) tasks. Despite their successes, conventional neural-based NLU models are criticized for high energy consumption, making them laborious to be widely applied in low-power electronics, such as smartphones and intelligent terminals. In this paper, we introduce a potential direction to alleviate this bottleneck by proposing a spiking encoder. The core of our model is bi-directional spiking neural network (SNN) which transforms numeric values into discrete spiking signals and replaces massive multiplications with much cheaper additive operations. We examine our model on sentiment classification and machine translation tasks. Experimental results reveal that our model achieves comparable classification and translation accuracy to advanced TRANSFORMER baseline, whereas significantly reduces the required computational energy to 0.82%.
C1 [Xiao, Rong; Yang, Baosong; Zhang, Haibo; Chen, Boxing] Damo Acad, Alibaba Grp, Hangzhou 310000, Peoples R China.
   [Wan, Yu; Wong, Derek F. F.] Univ Macau, NLP CT Lab, Macau 999078, Peoples R China.
   [Tang, Huajin] Zhejiang Univ, Hangzhou 310027, Peoples R China.
RP Yang, BS (corresponding author), Damo Acad, Alibaba Grp, Hangzhou 310000, Peoples R China.; Tang, HJ (corresponding author), Zhejiang Univ, Hangzhou 310027, Peoples R China.
EM xiaorong.scu@gmail.com; nlp2ct.ywan@gmail.com; nlp2ct.baosong@gmail.com;
   dreamfly.zhang@gmail.com; huajin.tang@gmail.com; derekfw@umac.mo;
   boxing.cbx@alibaba-inc.com
CR Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   Chen HT, 2020, PROC CVPR IEEE, P1465, DOI 10.1109/CVPR42600.2020.00154
   Chen MX, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P76
   Chung J., 2014, ARXIV
   Conneau A., 2019, UNSUPERVISED CROSS L
   Deng L, 2020, NEURAL NETWORKS, V121, P294, DOI 10.1016/j.neunet.2019.09.005
   Devlin J., 2019, P C N AM CHAPT ASS C, P4171, DOI DOI 10.18653/V1/N19-1423
   Frenkel C, 2019, IEEE T BIOMED CIRC S, V13, P145, DOI 10.1109/TBCAS.2018.2880425
   Gerstner W., 2002, SPIKING NEURON MODEL
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   He WH, 2020, NEURAL NETWORKS, V132, P108, DOI 10.1016/j.neunet.2020.08.001
   He YH, 2017, IEEE I CONF COMP VIS, P1398, DOI 10.1109/ICCV.2017.155
   Hinton G, 2015, Arxiv, DOI arXiv:1503.02531
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Horowitz M, 2014, ISSCC DIG TECH PAP I, V57, P10, DOI 10.1109/ISSCC.2014.6757323
   Jiao Xiaoqi, 2020, FINDINGS ASS COMPUTA, P4163
   Jouppi N, 2017, PROC ACMIEEE INT S, P1
   Kim S, 2020, AAAI CONF ARTIF INTE, V34, P11270
   Kim Y., 2014, ASS COMPUTATIONAL LI, P1746, DOI 10.3115/v1/D14-1181
   Li ZH, 2018, PR MACH LEARN RES, V80
   Liang L, 2021, PROC IEEE T COMPUT A, P4782
   Maas A., 2011, P 49 ANN M ASS COMPU
   Ott M., 2018, P 3 C MACHINE TRANSL, P1, DOI 10.18653/v1/W18-6301
   Pennington J., 2014, P 2014 C EMP METH NA, V14, P1532, DOI 10.3115/v1/D14-1162
   Rathi N, 2020, Arxiv, DOI arXiv:2008.03658
   Sennrich R, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P1715
   Taherkhani A, 2020, NEURAL NETWORKS, V122, P253, DOI 10.1016/j.neunet.2019.09.036
   Tang RP, 2019, Arxiv, DOI arXiv:1903.12136
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Tay Y., 2021, INT C MACHINE LEARNI, P10183
   Vaswani A., 2017, ADV NEURAL INFORM PR, P6000, DOI 10.5555/3295222.3295349
   Wan Y, 2022, FINDINGS OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2022), P3969
   Wan Y, 2020, PROCEEDINGS OF THE 2020 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP), P1074
   Wu JY, 2019, 2019 5TH INTERNATIONAL CONFERENCE ON EVENT-BASED CONTROL, COMMUNICATION, AND SIGNAL PROCESSING (EBCCSP), DOI 10.1109/ebccsp.2019.8836892
   Wu YJ, 2019, AAAI CONF ARTIF INTE, P1311
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Yang ZY, 2019, Arxiv, DOI arXiv:1909.12942
   Yu Q, 2016, IEEE T NEUR NET LEAR, V27, P621, DOI 10.1109/TNNLS.2015.2416771
   Zhang A., 2020, DIVE DEEP LEARNING
NR 39
TC 1
Z9 1
U1 6
U2 19
PY 2023
VL 31
BP 439
EP 447
DI 10.1109/TASLP.2022.3221011
UT WOS:000896638000002
DA 2023-11-16
ER

PT C
AU Jouni, Z
   Rioufol, TP
   Wang, SQ
   Benlarbi-Delai, A
   Ferreira, PM
AF Jouni, Zalfa
   Rioufol, Theo P.
   Wang, Siqi
   Benlarbi-Delai, Aziz
   Ferreira, Pietro M.
GP IEEE
TI Jitter Noise Impact on Analog Spiking Neural Networks: STDP Limitations
SO 2023 36TH SBC/SBMICRO/IEEE/ACM SYMPOSIUM ON INTEGRATED CIRCUITS AND
   SYSTEMS DESIGN, SBCCI
DT Proceedings Paper
CT 36th SBC/SBMicro/IEEE/ACM Symposium on Integrated Circuits and Systems
   Design (SBCCI)
CY AUG 28-SEP 01, 2023
CL Rio de Janeiro, BRAZIL
DE Analog Neurons; STDP; Noise; Spiking Neural Networks; Temporal Learning
   Rule
AB Accurate spike timing of neurons is essential for the temporal learning rules in Spiking Neural Networks (SNNs), such as the spike-timing dependent plasticity (STDP). Implementing these rules in analog SNNs presents a challenge as it would require a reliable spike timing of analog neurons. This paper investigates the impact of jitter noise on the spike timing of various analog neuron models, and highlights trade-offs in training neurons using spike timing. Post-layout simulation results reveal that noise-induced period jitter significantly affects spike occurrence time. Its values are three orders of magnitude higher than the precise spike timing required for an accurate synaptic weight updates. Moreover, the spike timing becomes a random variable with a sigma/mu that exceeds 76%. This study shows that the spiking frequency is a more reliable measure of neuronal activity, as it achieves a sigma/mu of only 0.2%.
C1 [Jouni, Zalfa] Univ Paris Saclay, CNRS, Lab Genie Elect & Elect Paris, CentraleSupelec, Gif Sur Yvette, France.
   Sorbonne Univ, Lab Genie Elect & Elect Paris, CNRS, F-75252 Paris, France.
RP Jouni, Z (corresponding author), Univ Paris Saclay, CNRS, Lab Genie Elect & Elect Paris, CentraleSupelec, Gif Sur Yvette, France.
EM zalfa@ieee.org; maris@ieee.org
CR Abidi AA, 2006, IEEE J SOLID-ST CIRC, V41, P1803, DOI 10.1109/JSSC.2006.876206
   Besrour M, 2022, 2022 20TH IEEE INTERREGIONAL NEWCAS CONFERENCE (NEWCAS), P148, DOI 10.1109/NEWCAS52662.2022.9842088
   Schuman CD, 2017, Arxiv, DOI arXiv:1705.06963
   Danneville F, 2019, SOLID STATE ELECTRON, V153, P88, DOI 10.1016/j.sse.2019.01.002
   Danneville F, 2021, INT WORK CONTENT MUL, P135, DOI 10.1109/CBMI50038.2021.9461899
   Ferreira Pietro, 2021, Zenodo, DOI 10.5281/ZENODO.5618268
   Ferreira PM, 2021, ANALOG INTEGR CIRC S, V106, P261, DOI 10.1007/s10470-020-01729-3
   Gautam A, 2021, FRONT NEUROSCI-SWITZ, V15, DOI 10.3389/fnins.2021.741116
   Grasser T., 2020, NOISE NANOSCALE SEMI, DOI [10.1007/978-3-030-37500-3, DOI 10.1007/978-3-030-37500-3]
   Joo B, 2022, IEEE T CIRCUITS-I, V69, P3632, DOI 10.1109/TCSI.2022.3178989
   Moriya S, 2022, IEEE IJCNN, DOI 10.1109/IJCNN55064.2022.9891920
   Shrestha A, 2022, IEEE CIRC SYST MAG, V22, P6, DOI 10.1109/MCAS.2022.3166331
   Soupizet T., 2023, J INTEGR CIRC SYST, DOI [10.29292/jics.v18i1.663, DOI 10.29292/JICS.V18I1.663]
   Sun CY, 2022, IEEE T CIRCUITS-I, V69, P5147, DOI 10.1109/TCSI.2022.3204645
   Takaloo H, 2023, IEEE T CIRCUITS-II, V70, P6, DOI 10.1109/TCSII.2022.3203929
   Vohra S. K., 2022, ARXIV
   Weiss R, 2022, 2022 IEEE 65TH INTERNATIONAL MIDWEST SYMPOSIUM ON CIRCUITS AND SYSTEMS (MWSCAS 2022), DOI 10.1109/MWSCAS54063.2022.9859294
NR 17
TC 0
Z9 0
U1 0
U2 0
PY 2023
BP 107
EP +
DI 10.1109/SBCCI60457.2023.10261661
UT WOS:001086004500020
DA 2023-11-16
ER

PT J
AU Cho, MW
AF Cho, Myoung Won
TI Supervised learning in a spiking neural network
SO JOURNAL OF THE KOREAN PHYSICAL SOCIETY
DT Article
DE Spiking neural network; Neural network learning; Free energy and charge
   conservation principles
ID NEURONS
AB We introduce a method to train a bio-inspired neural network model, having the characteristics of spiking-timing-dependent interaction and learning, in a manner of supervised learning. We assume the spiking neural network model has the tendency to obey the charge conservation principle or the junction rule on a long (or the learning dynamics) time scale. The tendency makes the distribution of connectivities is determined depending on not only the incoming stimuli to input neurons but also the outgoing stimuli from output neurons as if a solution of the finite elementary method in a fluid system. We apply the learning method to several cases in simulations and find the adoption of the conservation principle exerts desired effects on the neural network learning. Finally, we discuss the significance and the drawbacks of the introduced method and compare it with the supervised learning method implemented by the artificial neural network model.
C1 [Cho, Myoung Won] Sungshin Womens Univ, Sch Biopharmaceut & Med Sci, Seoul 01133, South Korea.
RP Cho, MW (corresponding author), Sungshin Womens Univ, Sch Biopharmaceut & Med Sci, Seoul 01133, South Korea.
EM mwcho@sungshin.ac.kr
CR Abbott LF, 2000, NAT NEUROSCI, V3, P1178, DOI 10.1038/81453
   ACKLEY DH, 1985, COGNITIVE SCI, V9, P147
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Cho, 2016, NEW PHYS SAE MULLI, V66, P786, DOI [10.3938/NPSM.66.786, DOI 10.3938/NPSM.66.786]
   Cho MW, 2020, J KOREAN PHYS SOC, V77, P168, DOI 10.3938/jkps.77.168
   Cho Myoung Won, 2019, [New Physics: Sae Mulli, 새물리], V69, P874, DOI 10.3938/NPSM.69.874
   Cho MW, 2019, J KOREAN PHYS SOC, V75, P261, DOI 10.3938/jkps.75.261
   Cho M, 2018, J KOREAN PHYS SOC, V73, P1385, DOI 10.3938/jkps.73.1385
   Cho MW, 2017, J KOREAN PHYS SOC, V71, P222, DOI 10.3938/jkps.71.222
   Cho MW, 2016, EPL-EUROPHYS LETT, V115, DOI 10.1209/0295-5075/115/38001
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554
   Lee KM, 2008, J NEUROSCI, V28, P2242, DOI 10.1523/JNEUROSCI.3596-07.2008
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   Maunsell JHR, 1999, VISUAL NEUROSCI, V16, P1, DOI 10.1017/S0952523899156177
   Myoung Won Cho, 2015, New Physics: Sae Mulli, V65, P456, DOI 10.3938/NPSM.65.456
   Zhang LI, 1998, NATURE, V395, P37, DOI 10.1038/25665
NR 18
TC 3
Z9 4
U1 1
U2 4
PD AUG
PY 2021
VL 79
IS 3
BP 328
EP 335
DI 10.1007/s40042-021-00254-4
EA JUL 2021
UT WOS:000675046300001
DA 2023-11-16
ER

PT C
AU Uchida, H
   Saito, T
AF Uchida, Hiroaki
   Saito, Toshimichi
BE Cheng, L
   Leung, ACS
   Ozawa, S
TI A Ladder-Type Digital Spiking Neural Network
SO NEURAL INFORMATION PROCESSING (ICONIP 2018), PT I
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 25th International Conference on Neural Information Processing (ICONIP)
CY DEC 13-16, 2018
CL Siem Reap, CAMBODIA
DE Digital spiking neurons; Multi-phase synchronization; FPGA
AB This paper presents a ladder-type digital spiking neural network and its hardware implementation. Depending on the parameters, the network can exhibit multi-phase synchronization of periodic spike-trains. Applying a time dependent selection switching, the network can output a variety of periodic spike-trains consisting of any combination of desired inter-spike-intervals. The network is a digital dynamical system and is suitable for FPGA based hardware implementation. A test circuit is implemented in a FPGA board by the Verilog and typical phenomena are confirmed experimentally. These results will be developed into several applications including time-series approximation/prediction.
C1 [Uchida, Hiroaki; Saito, Toshimichi] Hosei Univ, Koganei, Tokyo 1848584, Japan.
RP Saito, T (corresponding author), Hosei Univ, Koganei, Tokyo 1848584, Japan.
EM tsaito@hosei.ac.jp
CR Antonik P, 2017, IEEE IJCNN, P2407, DOI 10.1109/IJCNN.2017.7966148
   Appeltant L, 2011, NAT COMMUN, V2, DOI 10.1038/ncomms1476
   Campbell SR, 1999, NEURAL COMPUT, V11, P1595, DOI 10.1162/089976699300016160
   Iguchi T, 2010, IEICE T FUND ELECTR, VE93A, P1486, DOI 10.1587/transfun.E93.A.1486
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM., 2007, DYNAMICAL SYSTEMS NE, DOI [DOI 10.1017/S0143385704000173, 10.7551/mitpress/2526.001.0001]
   Lozano A, 2016, SCI REP-UK, V6, DOI 10.1038/srep23622
   PEREZ R, 1982, PHYS LETT A, V90, P441, DOI 10.1016/0375-9601(82)90391-7
   Rulkov NF, 2001, IEEE T CIRCUITS-I, V48, P1436, DOI 10.1109/TCSI.2001.972850
   Saito T, 2017, IEICE NONLINEAR THEO, V8, P267, DOI 10.1587/nolta.8.267
   Torikai H, 1999, IEEE T CIRCUITS-I, V46, P1072, DOI 10.1109/81.788809
   Torikai H, 2008, NEURAL NETWORKS, V21, P140, DOI 10.1016/j.neunet.2007.12.045
   Torikai H, 2006, IEEE T CIRCUITS-II, V53, P734, DOI 10.1109/TCSII.2006.876381
   Uchida H, 2017, LECT NOTES COMPUT SC, V10639, P804, DOI 10.1007/978-3-319-70136-3_85
NR 14
TC 0
Z9 0
U1 0
U2 0
PY 2018
VL 11301
BP 555
EP 562
DI 10.1007/978-3-030-04167-0_50
UT WOS:000612952400050
DA 2023-11-16
ER

PT J
AU Antonov, DI
   Sviatov, KV
   Sukhov, S
AF Antonov, D. I.
   Sviatov, K. V.
   Sukhov, S.
TI Continuous learning of spiking networks trained with local rules
SO NEURAL NETWORKS
DT Article
DE Continual learning; Catastrophic forgetting; Spiking neural network;
   Spike-timing-dependent plasticity; Langevin dynamics
ID NEURAL-NETWORKS; NOISE; NEOCORTEX; SYSTEMS
AB Artificial neural networks (ANNs) experience catastrophic forgetting (CF) during sequential learning. In contrast, the brain can learn continuously without any signs of catastrophic forgetting. Spiking neural networks (SNNs) are the next generation of ANNs with many features borrowed from biological neural networks. Thus, SNNs potentially promise better resilience to CF. In this paper, we study the susceptibility of SNNs to CF and test several biologically inspired methods for mitigating catastrophic forgetting. SNNs are trained with biologically plausible local training rules based on spike-timing -dependent plasticity (STDP). Local training prohibits the direct use of CF prevention methods based on gradients of a global loss function. We developed and tested the method to determine the importance of synapses (weights) based on stochastic Langevin dynamics without the need for the gradients. Several other methods of catastrophic forgetting prevention adapted from analog neural networks were tested as well. The experiments were performed on freely available datasets in the SpykeTorch environment.(c) 2022 Elsevier Ltd. All rights reserved.
C1 [Antonov, D. I.; Sukhov, S.] Russian Acad Sci, Kotelnikov Inst Radio Engn & Elect, Ulyanovsk branch, 48-2 Goncharov Str, Ulyanovsk 432071, Russia.
   [Sviatov, K. V.] Ulyanovsk State Tech Univ, 32 Severny Venets, Ulyanovsk 432027, Russia.
RP Sukhov, S (corresponding author), Russian Acad Sci, Kotelnikov Inst Radio Engn & Elect, Ulyanovsk branch, 48-2 Goncharov Str, Ulyanovsk 432071, Russia.
EM d.antonov@ulireran.ru; k.svyatov@ulstu.ru; ssukhov@ulireran.ru
CR Aljundi R, 2019, Arxiv, DOI arXiv:1806.05421
   Aljundi R, 2019, ADV NEUR IN, V32
   Aljundi Rahaf, 2019, ADV NEURAL INFORM PR, V2
   Allred JM, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00007
   Atkinson C, 2018, Arxiv, DOI arXiv:1802.03875
   Atkinson C, 2021, NEUROCOMPUTING, V428, P291, DOI 10.1016/j.neucom.2020.11.050
   BISHOP CM, 1995, NEURAL COMPUT, V7, P108, DOI 10.1162/neco.1995.7.1.108
   Brzosko Z, 2017, ELIFE, V6, P1, DOI 10.7554/eLife.27756
   Chaudhry A., 2019, ARXIV
   Chaudhry A, 2021, AAAI CONF ARTIF INTE, V35, P6993
   Cohen G, 2017, Arxiv, DOI [arXiv:1702.05373, 10.48550/arXiv.1702.05373]
   d'Autume CD, 2019, ADV NEUR IN, V32
   Draelos TJ, 2017, IEEE IJCNN, P526, DOI 10.1109/IJCNN.2017.7965898
   Faisal AA, 2008, NAT REV NEUROSCI, V9, P292, DOI 10.1038/nrn2258
   Flesch T, 2018, P NATL ACAD SCI USA, V115, pE10313, DOI 10.1073/pnas.1800755115
   Frémaux N, 2016, FRONT NEURAL CIRCUIT, V9, DOI 10.3389/fncir.2015.00085
   French RM, 1999, TRENDS COGN SCI, V3, P128, DOI 10.1016/S1364-6613(99)01294-2
   Gautrais J, 1998, BIOSYSTEMS, V48, P57, DOI 10.1016/S0303-2647(98)00050-1
   Gerstner W., 2002, SPIKING NEURON MODEL
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Golden R., 2020, BIORXIV688622
   González OC, 2020, ELIFE, V9, DOI 10.7554/eLife.51005
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Hadsell R, 2020, TRENDS COGN SCI, V24, P1028, DOI 10.1016/j.tics.2020.09.004
   Hassabis D, 2017, NEURON, V95, P245, DOI 10.1016/j.neuron.2017.06.011
   Hasselmo ME, 2017, TRENDS COGN SCI, V21, P407, DOI 10.1016/j.tics.2017.04.001
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Holtmaat AJGD, 2005, NEURON, V45, P279, DOI 10.1016/j.neuron.2005.01.003
   HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837
   Isele D, 2018, Arxiv, DOI arXiv:1802.10269
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Legenstein R, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.1000180
   Louie K, 2001, NEURON, V29, P145, DOI 10.1016/S0896-6273(01)00186-6
   Maass W, 2014, P IEEE, V102, P860, DOI 10.1109/JPROC.2014.2310593
   MCCLELLAND JL, 1995, PSYCHOL REV, V102, P419, DOI 10.1037/0033-295X.102.3.419
   McDonnell MD, 2011, NAT REV NEUROSCI, V12, P415, DOI 10.1038/nrn3061
   Mozafari M., 2019, SPYKETORCH
   Mozafari M, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00625
   Mozafari M, 2019, PATTERN RECOGN, V94, P87, DOI 10.1016/j.patcog.2019.05.015
   Muñoz-Martín I, 2019, IEEE J EXPLOR SOLID-, V5, P58, DOI 10.1109/JXCDC.2019.2911135
   Ororbia A., 2019, ARXIV
   Parisi GI, 2019, NEURAL NETWORKS, V113, P54, DOI 10.1016/j.neunet.2019.01.012
   Peng J, 2021, Arxiv, DOI arXiv:1912.09091
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Ponulak F, 2011, ACTA NEUROBIOL EXP, V71, P409
   Rebuffi SA, 2017, PROC CVPR IEEE, P5533, DOI 10.1109/CVPR.2017.587
   Robins A., 1995, Connection Science, V7, P123, DOI 10.1080/09540099550039318
   Sacramento J, 2015, PLOS COMPUT BIOL, V11, DOI 10.1371/journal.pcbi.1004265
   Sanda P, 2017, PLOS COMPUT BIOL, V13, DOI 10.1371/journal.pcbi.1005705
   Schwarz J, 2018, PR MACH LEARN RES, V80
   Shin H, 2017, ADV NEUR IN, V30
   Su X, 2020, IEEE T NEUR NET LEAR, V31, P1884, DOI 10.1109/TNNLS.2019.2927369
   Sukhov S, 2020, NEUROCOMPUTING, V400, P73, DOI 10.1016/j.neucom.2020.03.024
   Takiyama K, 2012, PLOS COMPUT BIOL, V8, DOI 10.1371/journal.pcbi.1002348
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Thorpe S, 1996, NATURE, V381, P520, DOI 10.1038/381520a0
   Vaila R., 2020, INT C NEUROMORPHIC S, P1
   Vaila R., 2019, P INT C NEUR SYST, P1
   van de Ven GM, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-17866-2
   Wei Y, 2014, J NEUROSCI, V34, P15804, DOI 10.1523/JNEUROSCI.3929-12.2014
   Welling Max, 2011, P 28 INT C MACH LEAR, P681
   Wen JF, 2018, Arxiv, DOI arXiv:1812.00543
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Yang G, 2009, NATURE, V462, P920, DOI 10.1038/nature08577
   Yoon J, 2018, Arxiv, DOI arXiv:1708.01547
   Yousefzadeh A., 2017, 2017 IEEE INT S CIRC, P1
   Yu YG, 2014, J NEUROSCI, V34, P13701, DOI 10.1523/JNEUROSCI.1834-14.2014
   Zenke F, 2017, PR MACH LEARN RES, V70
NR 71
TC 1
Z9 1
U1 1
U2 12
PD NOV
PY 2022
VL 155
BP 512
EP 522
DI 10.1016/j.neunet.2022.09.003
EA SEP 2022
UT WOS:000867366000006
DA 2023-11-16
ER

PT C
AU Kiselev, M
AF Kiselev, Mikhail
BE Dobnikar, A
   Lotric, U
   Ster, B
TI Self-organized Short-Term Memory Mechanism in Spiking Neural Network
SO ADAPTIVE AND NATURAL COMPUTING ALGORITHMS, PT I
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 10th International Conference on Adaptive and Natural Computing
   Algorithms
CY APR 14-16, 2011
CL Ljubljana, SLOVENIA
DE spiking neural networks; spatio-temporal pattern recognition; short-term
   memory; neural network self-organization
AB The paper is devoted to implementation and exploration of evolutionary development of the short-term memory mechanism in spiking neural networks (SNN) starting from initial chaotic state. Short-term memory is defined here as a network ability to store information about recent stimuli in form of specific neuron activity patterns. Stable appearance of this effect was demonstrated for so called stabilizing SNN, the network model proposed by the author. In order to show the desired evolutionary behavior the network should have a specific topology determined by "horizontal" layers and "vertical" columns.
C1 Megaputer Intelligence Inc, Bloomington, IN 47404 USA.
RP Kiselev, M (corresponding author), Megaputer Intelligence Inc, 120 W 7th St,Suite 314, Bloomington, IN 47404 USA.
EM mkiselev@megaputer.ru
CR Gerstner W., 2002, SPIKING NEURON MODEL
   Jones EG, 2010, CEREB CORTEX, V20, P2261, DOI 10.1093/cercor/bhq127
   Kiselev M, 2005, NEUROCOMPUTER, V12, P16
   Kiselev M, 2009, NEUROCOMPUTER, V10, P3
   Kiselev M. V., 2009, Proceedings 2009 International Joint Conference on Neural Networks (IJCNN 2009 - Atlanta), P1633, DOI 10.1109/IJCNN.2009.5178580
   Kiselev MV, 2003, IEEE IJCNN, P2843
   Maass W, 2004, J COMPUT SYST SCI, V69, P593, DOI 10.1016/j jcss.2004.04.001
   Wysoski SG, 2008, LECT NOTES COMPUT SC, V4985, P406
NR 8
TC 3
Z9 3
U1 0
U2 0
PY 2011
VL 6593
BP 120
EP 129
UT WOS:000302389300013
DA 2023-11-16
ER

PT C
AU Sun, Q
   Schwartz, F
   Michel, J
   Rom, R
AF Sun, Qing
   Schwartz, Francois
   Michel, Jacques
   Rom, Rami
GP IEEE
TI Implantation Study of an Analog Spiking Neural Network in an
   Auto-Adaptive Pacemaker
SO 2008 JOINT IEEE NORTH-EAST WORKSHOP ON CIRCUITS AND SYSTEMS AND TAISA
   CONFERENCE
DT Proceedings Paper
CT Joint IEEE North-East Workshop on Circuits and Systems/TAISA Conference
CY JUN 22-25, 2008
CL Montreal, CANADA
ID CARDIAC RESYNCHRONIZATION THERAPY; DEVICE
AB The goals of this research are to develop an analog spiking neural network so as to improve the performance of biventricular pacemaker (CRT devices). Implantation in silicon uses the analogical neural network approach that requires the development of a technical solution satisfying the requirement of very low energy consumption. Targeting an alternative analog solution in 0.18 mu m CMOS technology, this paper presents a new approach in analog spiking neural network for the delay prediction by using a Hebbian learning algorithm.
C1 [Sun, Qing; Schwartz, Francois; Michel, Jacques] Univ Strasbourg, UMR 7163, Inst Elect Solide & Syst, Strasbourg, France.
   [Rom, Rami] AI Med Semicond Ltd, Or Akiv, Israel.
RP Sun, Q (corresponding author), Univ Strasbourg, UMR 7163, Inst Elect Solide & Syst, Strasbourg, France.
EM qing.sun@iness.c-strasbourg.fr
CR Anstotz F, 1998, NUCL INSTRUM METH A, V412, P123, DOI 10.1016/S0168-9002(98)00418-5
   ANTSAKLIS PJ, 1990, IEEE CONTROL SYS APR
   Braunschweig F, 2004, J CARDIOVASC ELECTR, V15, P94, DOI 10.1046/j.1540-8167.2004.03208.x
   ELLENBOGEN KA, 2004, DEVICE THERAPY CONGE, P47
   GHORBEL M, 2005, IEEE SSD 2005 C SOUS
   Hebb D., 2005, ORG BEHAV NEUROPSYCH
   KLOPF AH, 1988, PSYCHOBIOLOGY, V16, P85
   MICHEL J, 2004, IEEE ISIE 2004 C AJ
   O'Donnell D, 2005, PACE, V28, pS24, DOI 10.1111/j.1540-8159.2005.00070.x
   Rom R, 2005, PACE, V28, P1168, DOI 10.1111/j.1540-8159.2005.40007.x
   Rom R, 2007, IEEE T NEURAL NETWOR, V18, P542, DOI 10.1109/TNN.2006.890806
NR 11
TC 3
Z9 3
U1 0
U2 0
PY 2008
BP 41
EP +
DI 10.1109/NEWCAS.2008.4606316
UT WOS:000262463700011
DA 2023-11-16
ER

PT C
AU Kiselev, M
AF Kiselev, Mikhail
BE Rojas, I
   Joya, G
   Gabestany, J
TI Self-Organization Process in Large Spiking Neural Networks Leading to
   Formation of Working Memory Mechanism
SO ADVANCES IN COMPUTATIONAL INTELLIGENCE, PT I
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 12th International Work-Conference on Artificial Neural Networks, IWANN
   2013
CY JUN 12-14, 2013
CL Puerto de la Cruz, SPAIN
DE spiking neural network; working memory; neural network
   self-organization; polychronization
AB The subject of this work is evolutionary process in initially chaotic and homogenous spiking neural networks leading to formation of the neuron groups with partially synchronized activity (so called polychronous groups) which are not only capable of recognizing input patterns but also can keep information about pattern presentation in form of their specific activity for a long time. This result is demonstrated for very simple neuron coincidence detector and for standard synaptic plasticity model (STDP).
C1 Megaputer Intelligence Ltd, Moscow, Russia.
RP Kiselev, M (corresponding author), Megaputer Intelligence Ltd, Moscow, Russia.
EM mkiselev@megaputer.ru
CR Gerstner W., 2002, SPIKING NEURON MODEL
   Itskov V, 2011, FRONT COMPUT NEUROSC, V5, DOI 10.3389/fncom.2011.00040
   Kiselev M. V., 2009, Proceedings 2009 International Joint Conference on Neural Networks (IJCNN 2009 - Atlanta), P1633, DOI 10.1109/IJCNN.2009.5178580
   Kiselev M, 2011, LECT NOTES COMPUT SC, V6593, P120, DOI 10.1007/978-3-642-20282-7_13
   Mongillo G, 2008, SCIENCE, V319, P1543, DOI 10.1126/science.1150769
   Nolte J., 2009, HUMAN BRAIN INTRO IT
   Szatmáry B, 2010, PLOS COMPUT BIOL, V6, DOI 10.1371/journal.pcbi.1000879
   Wang XJ, 2001, TRENDS NEUROSCI, V24, P455, DOI 10.1016/S0166-2236(00)01868-3
   ZIPSER D, 1993, J NEUROSCI, V13, P3406
NR 9
TC 6
Z9 7
U1 0
U2 0
PY 2013
VL 7902
BP 510
EP 517
UT WOS:000324897700051
DA 2023-11-16
ER

PT J
AU Jimenez-Rodriguez, A
   Castillo, LF
   Bedia, MG
AF Jimenez-Rodriguez, Alejandro
   Castillo, Luis F.
   Bedia, Manuel Glez
TI Studying the mechanisms of the Somatic Marker Hypothesis in Spiking
   Neural Networks (SNN)
SO ADCAIJ-ADVANCES IN DISTRIBUTED COMPUTING AND ARTIFICIAL INTELLIGENCE
   JOURNAL
DT Article
DE Somatic Marker Hypothesis; Spiking Neural Networks; Decision Making;
   Emotions
AB In this paper, a mechanism of emotional bias in decision making is studied using Spiking Neural Networks to simulate the associative and recurrent networks involved. The results obtained are along the lines of those proposed by A. Damasio as part of the Somatic Marker Hypothesis, in particular, that, in absence of emotional input, the decision making is driven by the rational input alone. Appropriate representations for the Objective and Emotional Values are also suggested, provided a spike representation (code) of the information.
C1 [Jimenez-Rodriguez, Alejandro] Univ Autonoma Manizales, Grp Neuroaprendizaje, Lab Neurofisiol, Manizales, Colombia.
   [Castillo, Luis F.] Univ Autonoma Manizales, Grp Invest Ing Software, Manizales, Colombia.
   [Castillo, Luis F.] Univ Caldas, Grp Invest GITIR, Manizales, Colombia.
   [Castillo, Luis F.] Univ Zaragoza, Comp Sci Dept, Zaragoza, Spain.
RP Jimenez-Rodriguez, A (corresponding author), Univ Autonoma Manizales, Grp Neuroaprendizaje, Lab Neurofisiol, Manizales, Colombia.
CR Albert M., 1994, SCIENCE, V264, P1102
   [Anonymous], 2008, MEMORY ATTENTION DEC
   [Anonymous], RETURN PHINEAS GAGE
   [Anonymous], 2005, NEURON BOOK
   BECHARA Antonie, 2005, SOMATIC MARKER HYPOT, V52, P336
   Colombetti G, 2008, BRIT J PHILOS SCI, V59, P51, DOI 10.1093/bjps/axm045
   Damasio A.R., 1991, SOMATIC MARKERS GUID
   Dayan P., 2001, THEORETICAL NEUROSCI
   Dunn BD, 2006, NEUROSCI BIOBEHAV R, V30, P239, DOI 10.1016/j.neubiorev.2005.07.001
   Fellous J., 2005, WHO NEEDS EMOTIONS B
   FELLOUS Jean M., 2007, MODELS OF EMOTION
   Glimcher PW, 2009, NEUROECONOMICS: DECISION MAKING AND THE BRAIN, P1
   Jenkins J. M, 1994, DESCARTES ERROR EMOT
   LeDoux JE, 2000, ANNU REV NEUROSCI, V23, P155, DOI 10.1146/annurev.neuro.23.1.155
   Litt A, 2008, COGN SYST RES, V9, P252, DOI 10.1016/j.cogsys.2007.11.001
   MACMILLIAN Malcolm, 2009, PHINEAS GAGE INFORM
   Platt M, 2009, NEUROECONOMICS: DECISION MAKING AND THE BRAIN, P441, DOI 10.1016/B978-0-12-374176-9.00029-4
   THAGARD Paul, 2006, SPIKING PHINEAS GAGE
NR 18
TC 0
Z9 0
U1 0
U2 0
PY 2012
VL 1
IS 2
BP 35
EP 42
UT WOS:000422563000004
DA 2023-11-16
ER

PT C
AU Larisch, R
   Teichmann, M
   Hamker, FH
AF Larisch, Rene
   Teichmann, Michael
   Hamker, Fred H.
BE Kurkova, V
   Manolopoulos, Y
   Hammer, B
   Iliadis, L
   Maglogiannis, I
TI A Neural Spiking Approach Compared to Deep Feedforward Networks on
   Stepwise Pixel Erasement
SO ARTIFICIAL NEURAL NETWORKS AND MACHINE LEARNING - ICANN 2018, PT I
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 27th International Conference on Artificial Neural Networks (ICANN)
CY OCT 04-07, 2018
CL Rhodes, GREECE
DE STDP; Unsupervised learning; Deep convolutional networks
ID INHIBITION
AB In real world scenarios, objects are often partially occluded. This requires a robustness for object recognition against these perturbations. Convolutional networks have shown good performances in classification tasks. The learned convolutional filters seem similar to receptive fields of simple cells found in the primary visual cortex. Alternatively, spiking neural networks are more biological plausible. We developed a two layer spiking network, trained on natural scenes with a biologically plausible learning rule. It is compared to two deep convolutional neural networks using a classification task of stepwise pixel erasement on MNIST. In comparison to these networks the spiking approach achieves good accuracy and robustness.
C1 [Larisch, Rene; Teichmann, Michael; Hamker, Fred H.] Tech Univ Chemnitz, Dept Comp Sci, Str Nationen 62, D-09111 Chemnitz, Germany.
RP Larisch, R (corresponding author), Tech Univ Chemnitz, Dept Comp Sci, Str Nationen 62, D-09111 Chemnitz, Germany.
EM rene.larisch@informatik.tu-chemnitz.de;
   michael.teichmann@informatik.tu-chemnitz.de;
   fred.hamker@informatik.tu-chemnitz.de
CR [Anonymous], 2012, ARXIV12125701V1
   [Anonymous], 2015, CORR
   BEAULIEU C, 1992, CEREB CORTEX, V2, P295, DOI 10.1093/cercor/2.4.295
   Bengio Y., 2015, ARXIV170308245
   Cichy RM, 2016, SCI REP-UK, V6, DOI 10.1038/srep27755
   Clopath C, 2010, NAT NEUROSCI, V13, P344, DOI 10.1038/nn.2479
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837
   JONES JP, 1987, J NEUROPHYSIOL, V58, P1187, DOI 10.1152/jn.1987.58.6.1187
   Katzner S, 2011, J NEUROSCI, V31, P5931, DOI 10.1523/JNEUROSCI.5753-10.2011
   Kheradpisheh S. R., 2017, ARXIV161101421
   Kolankeh AK, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00035
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0
   Potjans TC, 2014, CEREB CORTEX, V24, P785, DOI 10.1093/cercor/bhs358
   Priebe NJ, 2008, NEURON, V57, P482, DOI 10.1016/j.neuron.2008.02.005
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Schmidhuber J., 2012, ARXIV12022745
   Tavanaei A, 2017, IEEE IJCNN, P2023, DOI 10.1109/IJCNN.2017.7966099
   Vitay J, 2015, FRONT NEUROINFORM, V9, DOI 10.3389/fninf.2015.00019
   Vogels TP, 2011, SCIENCE, V334, P1569, DOI 10.1126/science.1211095
   Wen HG, 2018, CEREB CORTEX, V28, P4136, DOI 10.1093/cercor/bhx268
NR 23
TC 2
Z9 2
U1 0
U2 2
PY 2018
VL 11139
BP 253
EP 262
DI 10.1007/978-3-030-01418-6_25
UT WOS:000463336400025
DA 2023-11-16
ER

PT J
AU Gandharava, S
   Ivans, RC
   Etcheverry, BR
   Cantley, KD
AF Gandharava, Sumedha
   Ivans, Robert C.
   Etcheverry, Benjamin R.
   Cantley, Kurtis D.
TI Neuron Circuit Failure and Pattern Learning in Electronic Spiking Neural
   Networks
SO ELECTRONICS
DT Article
DE spiking neural network; spike timing-dependent plasticity; memristor;
   spatio-temporal pattern recognition
ID ELECTRICAL CHARACTERISTICS; RADIATION
AB Biological neural networks demonstrate remarkable resilience and the ability to compensate for neuron losses over time. Thus, the effects of neural/synaptic losses in the brain go mostly unnoticed until the loss becomes profound. This study analyses the capacity of electronic spiking networks to compensate for the sudden, random neuron failure ("death") due to reliability degradation or other external factors such as exposure to ionizing radiation. Electronic spiking neural networks with memristive synapses are designed to learn spatio-temporal patterns representing 25 or 100-pixel characters. The change in the pattern learning ability of the neural networks is observed as the afferents (input layer neurons) in the network fail/die during network training. Spike-timing-dependent plasticity (STDP) learning behavior is implemented using shaped action potentials with a realistic, non-linear memristor model. This work focuses on three cases: (1) when only neurons participating in the pattern are affected, (2) when non-participating neurons (those that never present spatio-temporal patterns) are disabled, and (3) when random/non-selective neuron death occurs in the network (the most realistic scenario). Case 3 is further analyzed to compare what happens when neuron death occurs over time versus when multiple afferents fail simultaneously. Simulation results emphasize the importance of non-participating neurons during the learning process, concluding that non-participating afferents contribute to improving the learning ability and stability of the neural network. Instantaneous neuron death proves to be more detrimental for the network compared to when afferents fail over time. To a surprising degree, the electronic spiking neural networks can sometimes retain their pattern recognition capability even in the case of significant neuron death.
C1 [Gandharava, Sumedha; Ivans, Robert C.; Etcheverry, Benjamin R.; Cantley, Kurtis D.] Boise State Univ, Dept Elect & Comp Engn, Boise, ID 83725 USA.
RP Cantley, KD (corresponding author), Boise State Univ, Dept Elect & Comp Engn, Boise, ID 83725 USA.
EM sumedha248@gmail.com; robert.ivans@inl.gov;
   benetcheverry@u.boisestate.edu; kurtiscantley@boisestate.edu
CR Adhikari SP, 2012, IEEE T NEUR NET LEAR, V23, P1426, DOI 10.1109/TNNLS.2012.2204770
   Alemi A, 2018, AAAI CONF ARTIF INTE, P588
   Barnaby HJ, 2011, IEEE T NUCL SCI, V58, P2838, DOI 10.1109/TNS.2011.2168827
   Barrett DGT, 2016, ELIFE, V5, DOI 10.7554/eLife.12454
   Büchel J, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-02779-x
   BURGHARD RA, 1973, IEEE T NUCL SCI, VNS20, P300, DOI 10.1109/TNS.1973.4327411
   Cantley KD, 2017, IEEE IJCNN, P4633, DOI 10.1109/IJCNN.2017.7966444
   Cantley KD, 2012, IEEE T NEUR NET LEAR, V23, P565, DOI 10.1109/TNNLS.2012.2184801
   Cantley KD, 2011, IEEE T NANOTECHNOL, V10, P1066, DOI 10.1109/TNANO.2011.2105887
   Castelli V, 2019, FRONT MOL NEUROSCI, V12, DOI 10.3389/fnmol.2019.00132
   Dahl S.G., 2019, P INT C NEUROMORPHIC
   Dahl S. G., 2018, P INT C NEUR SYST IC, P1
   Dahl SG, 2021, SN APPL SCI, V3, DOI 10.1007/s42452-021-04553-0
   Dahl SG, 2019, MIDWEST SYMP CIRCUIT, P53, DOI [10.1109/mwscas.2019.8885288, 10.1109/MWSCAS.2019.8885288]
   DeIonno E, 2013, AEROSP CONF PROC
   Dodd PE, 2010, IEEE T NUCL SCI, V57, P1747, DOI 10.1109/TNS.2010.2042613
   Dutta S, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-07418-y
   Joglekar YN, 2009, EUR J PHYS, V30, P661, DOI 10.1088/0143-0807/30/4/001
   Keys AS, 2008, AIP CONF PROC, V969, P749, DOI 10.1063/1.2845040
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Leary MC, 2003, CEREBROVASC DIS, V16, P280, DOI 10.1159/000071128
   Li N, 2016, NATURE, V532, P459, DOI 10.1038/nature17643
   Marinella MJ, 2012, IEEE T NUCL SCI, V59, P2987, DOI 10.1109/TNS.2012.2224377
   McDonald NR, 2010, IEEE IJCNN
   Morrison JH, 1997, SCIENCE, V278, P412, DOI 10.1126/science.278.5337.412
   Paccagnella A, 2004, IEEE INTERNATIONAL ELECTRON DEVICES MEETING 2004, TECHNICAL DIGEST, P473, DOI 10.1109/IEDM.2004.1419192
   Prodromakis T, 2011, IEEE T ELECTRON DEV, V58, P3099, DOI 10.1109/TED.2011.2158004
   Rozenberg MJ, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-47348-5
   Strukov DB, 2008, NATURE, V453, P80, DOI 10.1038/nature06932
   TERRY RD, 1991, ANN NEUROL, V30, P572, DOI 10.1002/ana.410300410
   Tong WM, 2010, IEEE T NUCL SCI, V57, P1640, DOI 10.1109/TNS.2010.2045768
   Wozniak S, 2016, IEEE INT SYMP CIRC S, P365, DOI 10.1109/ISCAS.2016.7527246
   Wu XY, 2015, IEEE T CIRCUITS-II, V62, P1088, DOI 10.1109/TCSII.2015.2456372
   Yakopcic C, 2011, IEEE ELECTR DEVICE L, V32, P1436, DOI 10.1109/LED.2011.2163292
NR 34
TC 0
Z9 0
U1 4
U2 7
PD MAY
PY 2022
VL 11
IS 9
AR 1392
DI 10.3390/electronics11091392
UT WOS:000799272900001
DA 2023-11-16
ER

PT J
AU Masumori, A
   Sinapayen, L
   Maruyama, N
   Mita, T
   Bakkum, D
   Frey, U
   Takahashi, H
   Ikegami, T
AF Masumori, Atsushi
   Sinapayen, Lana
   Maruyama, Norihiro
   Mita, Takeshi
   Bakkum, Douglas
   Frey, Urs
   Takahashi, Hirokazu
   Ikegami, Takashi
TI Neural Autopoiesis: Organizing Self-Boundaries by Stimulus Avoidance in
   Biological and Artificial Neural Networks
SO ARTIFICIAL LIFE
DT Article
DE Autopoiesis; homeostasis; neural adaptation; cultured neurons; spiking
   neural networks; STDP
ID TIMING-DEPENDENT PLASTICITY; FREE-ENERGY PRINCIPLE; ACTIVE INFERENCE;
   SPIKING NEURONS; ADAPTATION; DYNAMICS; MODEL; PATTERNS; CORTEX; FIELD
AB Living organisms must actively maintain themselves in order to continue existing. Autopoiesis is a key concept in the study of living organisms, where the boundaries of the organism are not static but dynamically regulated by the system itself. To study the autonomous regulation of a self-boundary, we focus on neural homeodynamic responses to environmental changes using both biological and artificial neural networks. Previous studies showed that embodied cultured neural networks and spiking neural networks with spike-timing dependent plasticity (STDP) learn an action as they avoid stimulation from outside. In this article, as a result of our experiments using embodied cultured neurons, we find that there is also a second property allowing the network to avoid stimulation: If the agent cannot learn an action to avoid the external stimuli, it tends to decrease the stimulus-evoked spikes, as if to ignore the uncontrollable input. We also show such a behavior is reproduced by spiking neural networks with asymmetric STDP. We consider that these properties are to be regarded as autonomous regulation of self and nonself for the network, in which a controllable neuron is regarded as self, and an uncontrollable neuron is regarded as nonself. Finally, we introduce neural autopoiesis by proposing the principle of stimulus avoidance.
C1 [Masumori, Atsushi; Maruyama, Norihiro; Ikegami, Takashi] Univ Tokyo, Dept Gen Syst Sci, Grad Sch Arts & Sci, Tokyo, Japan.
   [Sinapayen, Lana] Tokyo Inst Technol, Sony Comp Sci Labs, Earth Life Sci Inst, Tokyo, Japan.
   [Mita, Takeshi; Takahashi, Hirokazu] Univ Tokyo, Dept Mechanoinformat, Grad Sch Informat Sci & Technol, Tokyo, Japan.
   [Bakkum, Douglas] Swiss Fed Inst Technol, Dept Biosyst Sci & Engn, Zurich, Switzerland.
   [Frey, Urs] MaxWell Biosyst AG, Zurich, Switzerland.
RP Masumori, A (corresponding author), Univ Tokyo, Dept Gen Syst Sci, Grad Sch Arts & Sci, Tokyo, Japan.
EM masumori@sacral.c.u-tokyo.ac.jp; lana.sinapayen@gmail.com;
   murayama@sacral.c.u-tokyo.ac.jp; takahashi@i.u-tokyo.ac.jp;
   djbakkum@gmail.com; urs.frey@mxwbio.com; mita@brain.imi.i.u-tokyo.ac.jp;
   Ikeg@sacral.c.u-tokyo.ac.jp
CR Ashby W.R., 1960, DESIGN BRAIN
   Bakkum DJ, 2008, J NEURAL ENG, V5, P310, DOI 10.1088/1741-2560/5/3/004
   Bakkum DJ, 2013, NAT COMMUN, V4, DOI 10.1038/ncomms3181
   Brette R, 2007, J COMPUT NEUROSCI, V23, P349, DOI 10.1007/s10827-007-0038-6
   Canepari M, 1997, BIOL CYBERN, V77, P153, DOI 10.1007/s004220050376
   Caporale N, 2008, ANNU REV NEUROSCI, V31, P25, DOI 10.1146/annurev.neuro.31.060407.125639
   Cassenaer S, 2007, NATURE, V448, P709, DOI 10.1038/nature05973
   Chung S, 2002, NEURON, V34, P437, DOI 10.1016/S0896-6273(02)00659-1
   Dan Y, 2006, PHYSIOL REV, V86, P1033, DOI 10.1152/physrev.00030.2005
   Di Paolo EA, 2000, FROM ANIM ANIMAT, P440
   Di Paolo EA, 2008, BIOSYSTEMS, V91, P409, DOI 10.1016/j.biosystems.2007.05.016
   Eytan D, 2006, J NEUROSCI, V26, P8465, DOI 10.1523/JNEUROSCI.1627-06.2006
   Frey U, 2010, IEEE J SOLID-ST CIRC, V45, P467, DOI 10.1109/JSSC.2009.2035196
   Friston K, 2016, NEUROSCI BIOBEHAV R, V68, P862, DOI 10.1016/j.neubiorev.2016.06.022
   Friston KJ, 2006, J PHYSIOL-PARIS, V100, P70, DOI 10.1016/j.jphysparis.2006.10.001
   Friston KJ, 2012, BIOL CYBERN, V106, P523, DOI 10.1007/s00422-012-0512-8
   Friston KJ, 2011, BIOL CYBERN, V104, P137, DOI 10.1007/s00422-011-0424-z
   Friston KJ, 2010, NAT REV NEUROSCI, V11, P127, DOI 10.1038/nrn2787
   HEBB D. O., 1949
   Iizuka H, 2007, ADAPT BEHAV, V15, P359, DOI 10.1177/1059712307084687
   Ikegami T, 2008, BIOSYSTEMS, V91, P388, DOI 10.1016/j.biosystems.2007.05.014
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   JERNE NK, 1974, ANN INST PASTEUR IMM, VC125, P373
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   MacQueen J, 1967, 5 BERK S MATH STAT P, DOI DOI 10.1007/S11665-016-2173-6
   Madhavan R, 2007, PHYS BIOL, V4, P181, DOI 10.1088/1478-3975/4/3/005
   Marom S, 2002, Q REV BIOPHYS, V35, P63, DOI 10.1017/S0033583501003742
   Masumori A., 2019, THESIS
   Masumori A, 2015, ECAL 2015: THE THIRTEENTH EUROPEAN CONFERENCE ON ARTIFICIAL LIFE, P373, DOI 10.7551/978-0-262-33027-5-ch067
   Masumori A, 2017, FOURTEENTH EUROPEAN CONFERENCE ON ARTIFICIAL LIFE (ECAL 2017), P275
   Maturana H. R., 2012, AUTOPOIESIS COGNITIO
   Mongillo G, 2008, SCIENCE, V319, P1543, DOI 10.1126/science.1150769
   Noda T, 2014, NEUROSCI RES, V79, P52, DOI 10.1016/j.neures.2013.11.002
   O'Regan JK, 2001, BEHAV BRAIN SCI, V24, P939, DOI 10.1017/S0140525X01000115
   Parras GG, 2017, NAT COMMUN, V8, DOI 10.1038/s41467-017-02038-6
   Potter SM, 2001, J NEUROSCI METH, V110, P17, DOI 10.1016/S0165-0270(01)00412-5
   Rao RPN, 1999, NAT NEUROSCI, V2, P79, DOI 10.1038/4580
   SEJNOWSKI TJ, 1988, SCIENCE, V241, P1299, DOI 10.1126/science.3045969
   Shahaf G, 2001, J NEUROSCI, V21, P8782, DOI 10.1523/JNEUROSCI.21-22-08782.2001
   Shiramatsu TI, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0082663
   Simons-Weidenmaier NS, 2006, BMC NEUROSCI, V7, DOI 10.1186/1471-2202-7-38
   Sinapayen L, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0170388
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Tajima S, 2017, P NATL ACAD SCI USA, V114, P9517, DOI 10.1073/pnas.1705981114
   Ulanovsky N, 2003, NAT NEUROSCI, V6, P391, DOI 10.1038/nn1032
   VARELA F G, 1974, Biosystems, V5, P187, DOI 10.1016/0303-2647(74)90031-8
   Wagenaar D, 2005, I IEEE EMBS C NEUR E, P518
   Wagenaar D. A., 2002, Journal of Neuroscience Methods, V120, P113, DOI 10.1016/S0165-0270(02)00149-8
   Whitmire CJ, 2016, NEURON, V92, P298, DOI 10.1016/j.neuron.2016.09.046
   Yada Y, 2017, NEUROSCIENCE, V343, P55, DOI 10.1016/j.neuroscience.2016.11.031
   Yada Y, 2016, FRONT SYST NEUROSCI, V10, DOI 10.3389/fnsys.2016.00028
NR 52
TC 4
Z9 4
U1 3
U2 10
PD APR
PY 2020
VL 26
IS 1
SI SI
BP 130
EP 151
DI 10.1162/artl_a_00314
UT WOS:000531417900009
DA 2023-11-16
ER

PT J
AU Zhou, ZH
   Yip, HM
   Tsimring, K
   Sur, M
   Tin, C
   Ip, JPK
AF Zhou, Zhanhong
   Yip, Hei Matthew
   Tsimring, Katya
   Sur, Mriganka
   Tin, Chung
   Ip, Jacque Pak Kan
TI Effective and efficient neural networks for spike inference from <i>in
   vivo</i> calcium imaging
SO CELL REPORTS METHODS
DT Article
ID CEREBELLAR GRANULE CELLS; VISUAL-CORTEX; DECONVOLUTION; INDICATOR;
   MEMBRANE
AB Calcium imaging provides advantages in monitoring large populations of neuronal activities simultaneously. However, it lacks the signal quality provided by neural spike recording in traditional electrophysiology. To address this issue, we developed a supervised data-driven approach to extract spike information from calcium signals. We propose the ENS2 (effective and efficient neural networks for spike inference from calcium signals) system for spike-rate and spike-event predictions using ?F/F-0 calcium inputs based on a U-Net deep neural network. When testing on a large, ground-truth public database, it consistently outperformed state-of-the-art algorithms in both spike-rate and spike-event predictions with reduced computational load. We further demonstrated that ENS2 can be applied to analyses of orientation selectivity in primary visual cortex neurons. We conclude that it would be a versatile inference system that may benefit diverse neuroscience studies.
C1 [Zhou, Zhanhong; Tin, Chung] City Univ Hong Kong, Dept Biomed Engn, Hong Kong, Peoples R China.
   [Yip, Hei Matthew; Ip, Jacque Pak Kan] Chinese Univ Hong Kong, Sch Biomed Sci, Hong Kong, Peoples R China.
   [Tsimring, Katya; Sur, Mriganka] MIT, Picower Inst Learning & Memory, Dept Brain & Cognit Sci, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
RP Tin, C (corresponding author), City Univ Hong Kong, Dept Biomed Engn, Hong Kong, Peoples R China.; Ip, JPK (corresponding author), Chinese Univ Hong Kong, Sch Biomed Sci, Hong Kong, Peoples R China.
EM jacqueip@cuhk.edu.hk; chungtin@cityu.edu.hk
CR Akerboom J, 2012, J NEUROSCI, V32, P13819, DOI 10.1523/JNEUROSCI.2601-12.2012
   Alcaraz JML, 2022, Arxiv, DOI arXiv:2208.09399
   Berens P, 2018, PLOS COMPUT BIOL, V14, DOI 10.1371/journal.pcbi.1006157
   Bethge P, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0179460
   Buzsáki G, 2004, NAT NEUROSCI, V7, P446, DOI 10.1038/nn1233
   Charles AS, 2019, bioRxiv, DOI [10.1101/726174, 10.1101/726174, DOI 10.1101/726174, 10.1101/726174v1, DOI 10.1101/726174V1]
   Chen TW, 2013, NATURE, V499, P295, DOI 10.1038/nature12354
   Dana H, 2016, ELIFE, V5, DOI 10.7554/eLife.12727
   de Vries SEJ, 2020, NAT NEUROSCI, V23, P138, DOI 10.1038/s41593-019-0550-9
   Deneux T, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms12190
   DENK W, 1990, SCIENCE, V248, P73, DOI 10.1126/science.2321027
   El-Boustani S, 2018, SCIENCE, V360, P1349, DOI 10.1126/science.aao0862
   Éltes T, 2019, J PHYSIOL-LONDON, V597, P2925, DOI 10.1113/JP277681
   Friedrich J, 2016, ADV NEUR IN, V29
   Friedrich J, 2017, PLOS COMPUT BIOL, V13, DOI 10.1371/journal.pcbi.1005423
   Giovannucci A, 2019, ELIFE, V8, DOI 10.7554/eLife.38173
   Giovannucci A, 2017, NAT NEUROSCI, V20, P727, DOI 10.1038/nn.4531
   Goodfellow IJ, 2014, ADV NEUR IN, V27, P2672
   Greenberg DS, 2008, NAT NEUROSCI, V11, P749, DOI 10.1038/nn.2140
   Grewe BF, 2010, NAT METHODS, V7, P399, DOI 10.1038/nmeth.1453
   Grienberger C, 2012, NEURON, V73, P862, DOI 10.1016/j.neuron.2012.02.011
   HAMILL OP, 1981, PFLUG ARCH EUR J PHY, V391, P85, DOI 10.1007/BF00656997
   Ho J., 2020, ADV NEURAL INF PROCE, V33, P6840, DOI DOI 10.48550/ARXIV.2006.11239
   Hoang H, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-74672-y
   Huang L., 2020, BIORXIV, DOI [DOI 10.1101/788802, 10.1101/788802v1, DOI 10.1101/788802V1]
   Jewell SW, 2020, BIOSTATISTICS, V21, P709, DOI 10.1093/biostatistics/kxy083
   Kerr JND, 2008, NAT REV NEUROSCI, V9, P195, DOI 10.1038/nrn2338
   Kerr JND, 2005, P NATL ACAD SCI USA, V102, P14063, DOI 10.1073/pnas.0506029102
   Khan AG, 2018, NAT NEUROSCI, V21, P851, DOI 10.1038/s41593-018-0143-z
   Kingma D. P, 2015, 3 INT C LEARN REPR I
   Knogler LD, 2017, CURR BIOL, V27, P1288, DOI 10.1016/j.cub.2017.03.029
   Kwan AC, 2012, CURR BIOL, V22, P1459, DOI 10.1016/j.cub.2012.06.007
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li X, 2020, bioRxiv, DOI [10.1101/2020.11.16.383984, 10.1101/2020.11.16.383984, DOI 10.1101/2020.11.16.383984]
   Mazurek M, 2014, FRONT NEURAL CIRCUIT, V8, DOI 10.3389/fncir.2014.00092
   NEHER E, 1976, NATURE, V260, P799, DOI 10.1038/260799a0
   Oñativia J, 2013, J NEURAL ENG, V10, DOI 10.1088/1741-2560/10/4/046017
   Pachitariu M, 2018, J NEUROSCI, V38, P7976, DOI 10.1523/JNEUROSCI.3339-17.2018
   Pnevmatikakis EA, 2019, CURR OPIN NEUROBIOL, V55, P15, DOI 10.1016/j.conb.2018.11.004
   Pnevmatikakis EA, 2016, NEURON, V89, P285, DOI 10.1016/j.neuron.2015.11.037
   Pnevmatikakis EA, 2013, CONF REC ASILOMAR C, P349, DOI 10.1109/ACSSC.2013.6810293
   Rahmati V, 2016, PLOS COMPUT BIOL, V12, DOI 10.1371/journal.pcbi.1004736
   Rikhye RV, 2015, J NEUROSCI, V35, P14661, DOI 10.1523/JNEUROSCI.1660-15.2015
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Rupprecht P, 2021, NAT NEUROSCI, V24, P1324, DOI 10.1038/s41593-021-00895-5
   Schoenfeld G., 2021, BIORXIV, DOI [10.1101/2021.01.21.427642, DOI 10.1101/2021.01.21.427642]
   Sebastian J, 2021, PLOS COMPUT BIOL, V17, DOI 10.1371/journal.pcbi.1007921
   Sebastian J, 2019, IEEE T SIGNAL PROCES, V67, P2923, DOI [10.1109/TSP.2019.2908913, 10.1109/tsp.2019.2908913]
   Sofroniew NJ, 2016, ELIFE, V5, DOI 10.7554/eLife.14472
   Spira ME, 2013, NAT NANOTECHNOL, V8, P83, DOI [10.1038/nnano.2012.265, 10.1038/NNANO.2012.265]
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Stosiek C, 2003, P NATL ACAD SCI USA, V100, P7319, DOI 10.1073/pnas.1232232100
   Stoyanov M, 2011, INT J UNCERTAIN QUAN, V1, P257, DOI 10.1615/Int.J.UncertaintyQuantification.2011003089
   Stringer C, 2019, CURR OPIN NEUROBIOL, V55, P22, DOI 10.1016/j.conb.2018.11.005
   Tada M, 2014, EUR J NEUROSCI, V39, P1720, DOI 10.1111/ejn.12476
   Tashiro Yusuke, 2021, ADV NEURAL INFORM PR, V34, P24804
   Theis L, 2016, NEURON, V90, P471, DOI 10.1016/j.neuron.2016.04.014
   Tsutsumi S, 2015, J NEUROSCI, V35, P843, DOI 10.1523/JNEUROSCI.2170-14.2015
   Ulyanov D, 2017, Arxiv, DOI arXiv:1607.08022
   van Rossum MCW, 2001, NEURAL COMPUT, V13, P751, DOI 10.1162/089976601300014321
   Vaswani A., 2017, ARXIV, DOI DOI 10.48550/ARXIV.1706.03762
   Victor JD, 1996, J NEUROPHYSIOL, V76, P1310, DOI 10.1152/jn.1996.76.2.1310
   Vogelstein JT, 2010, J NEUROPHYSIOL, V104, P3691, DOI 10.1152/jn.01073.2009
   Wagner MJ, 2017, NATURE, V544, P96, DOI 10.1038/nature21726
   Wilson NR, 2012, NATURE, V488, P343, DOI 10.1038/nature11347
   Yaksi E, 2006, NAT METHODS, V3, P377, DOI 10.1038/NMETH874
   Zhai XL, 2020, EXPERT SYST APPL, V158, DOI 10.1016/j.eswa.2020.113411
   Zhai XL, 2018, IEEE ACCESS, V6, P27465, DOI 10.1109/ACCESS.2018.2833841
   Zhai XL, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00379
   Zhou ZH, 2021, EXPERT SYST APPL, V174, DOI 10.1016/j.eswa.2021.114809
   2016, bioRxiv, DOI [10.1101/061507, 10.1101/061507, DOI 10.1101/061507]
NR 71
TC 0
Z9 0
U1 3
U2 3
PD MAY 22
PY 2023
VL 3
IS 5
AR 100462
DI 10.1016/j.crmeth.2023.100462
UT WOS:001015455500001
DA 2023-11-16
ER

PT C
AU Zou, CL
   Cui, XX
   Ge, JX
   Ma, HH
   Wang, XN
AF Zou, Chenglong
   Cui, Xiaoxin
   Ge, Jiexian
   Ma, Hanghang
   Wang, Xinan
GP IEEE
TI A Novel Conversion Method for Spiking Neural Network Using Median
   Quantization
SO 2020 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS (ISCAS)
SE IEEE International Symposium on Circuits and Systems
DT Proceedings Paper
CT IEEE International Symposium on Circuits and Systems (ISCAS)
CY OCT 10-21, 2020
CL ELECTR NETWORK
DE Artificial Neural Networks (ANNs); Spiking Neural Networks (SNNs);
   Dynamic Quantization; Neuromorphic Hardware
AB Artificial Neural Networks (ANNs) have achieved great success in the field of computer vision and language understanding. However, it is difficult to deploy these deep learning models on mobile devices because of its massive energy consumption and memory occupation. For another way, highly inspired from biological brain, spiking neural networks (SNNs), are often referred to as the 3-th generation of neural network for its potential superiority in cognitive learning and energy efficiency. Nevertheless, training a deep SNN remains a big challenge. In this paper, we propose a quantized training algorithm for ANNs to minimize spike approximation error, and provide two (temporally or spatially) rate-based conversion methods for SNNs, both of which can be easily mapped to specific neuromorphic platforms. Besides, this novel method can be generalized to various network architectures and adapted to dynamic quantization demand. Experimental results on MNIST and CIFAR-10 dataset demonstrate that the proposed deep spiking neural networks yield the state-of-the-art classification accuracy and need much less operations compared with their ANN counterparts. Our source code will be available upon request for the academic purpose.
C1 [Zou, Chenglong; Cui, Xiaoxin; Ge, Jiexian; Ma, Hanghang] Peking Univ, Key Lab Microelect Devices & Circuits, Inst Microelect, Beijing 100871, Peoples R China.
   [Zou, Chenglong; Wang, Xinan] Peking Univ, Key Lab Integrated Microsyst, Sch ECE, Shenzhen Grad Sch, Shenzhen 518055, Peoples R China.
RP Cui, XX (corresponding author), Peking Univ, Key Lab Microelect Devices & Circuits, Inst Microelect, Beijing 100871, Peoples R China.
EM cuixx@pku.edu.cn
CR Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   [Anonymous], 2019, CORR
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Esser SK, 2016, P NATL ACAD SCI USA, V113, P11441, DOI 10.1073/pnas.1604850113
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hubara I., 2016, ADV NEURAL INFORM PR, P4107, DOI DOI 10.5555/3157382.3157557
   Hunsberger E., 2016, TRAINING SPIKING DEE
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   King DB, 2015, ACS SYM SER, V1214, P1
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lin X., 2017, ARXIV171111294
   Mostafa H., 2017, 2017 IEEE INT S CIRC, P1, DOI [10.1109/ISCAS.2017.8050527, DOI 10.1109/ISCAS.2017.8050527]
   Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32
   Rueckauer B, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351295
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Springenberg Jost Tobias, 2014, ARXIV14126806
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Xu Y, 2017, 2017 IEEE SYMPOSIUM SERIES ON COMPUTATIONAL INTELLIGENCE (SSCI), P1219
   Yousefzadeh A, 2019, 2019 IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE CIRCUITS AND SYSTEMS (AICAS 2019), P81, DOI [10.1109/AICAS.2019.8771624, 10.1109/aicas.2019.8771624]
NR 21
TC 2
Z9 2
U1 0
U2 1
PY 2020
UT WOS:000706854700101
DA 2023-11-16
ER

PT J
AU Ponulak, F
AF Ponulak, Filip
TI Analysis of the ReSuMe learning process for spiking neural networks
SO INTERNATIONAL JOURNAL OF APPLIED MATHEMATICS AND COMPUTER SCIENCE
DT Article
DE supervised learning; spiking neural networks; parametric analysis;
   learning window
ID SYNAPTIC PLASTICITY
AB In this paper we perform an analysis of the learning process with the ReSuMe method and spiking neural networks (Ponulak, 2005; Ponulak, 2006b). We investigate how the particular parameters of the learning algorithm affect the process of learning. We consider the issue of speeding up the adaptation process, while maintaining the stability of the optimal solution. This is an important issue in many real-life tasks where the neural networks are applied and where the fast learning convergence is highly desirable.
C1 Poznan Tech Univ, Inst Control & Informat Engn, PL-60965 Poznan, Poland.
RP Ponulak, F (corresponding author), Poznan Tech Univ, Inst Control & Informat Engn, Ul Piotrowo 3A, PL-60965 Poznan, Poland.
EM Filip.Ponulak@put.poznan.pl
CR [Anonymous], 2006, P EPFL LATSIS S 2006
   [Anonymous], 1991, INTRO THEORY NEURAL
   [Anonymous], ARTIFICIAL NEURAL NE
   Bi GQ, 2002, BIOL CYBERN, V87, P319, DOI 10.1007/s00422-002-0349-7
   *CSIM, 2002, CSIM NEUR CIRC SIMUL
   Freeman J.A., 1991, NEURAL NETWORKS ALGO
   Gerstner W, 2002, BIOL CYBERN, V87, P404, DOI 10.1007/s00422-002-0353-y
   Gerstner W., 2002, SPIKING NEURON MODEL
   Kangas J, 1996, MATH COMPUT SIMULAT, V41, P3, DOI 10.1016/0378-4754(96)88223-1
   Kasinski A, 2005, LECT NOTES COMPUT SC, V3696, P145, DOI 10.1007/11550822_24
   KASINSKI A, 2006, P INT C ART INT SOFT, P57
   Kempter R, 1999, PHYS REV E, V59, P4498, DOI 10.1103/PhysRevE.59.4498
   KRAFT M, 2006, P 3 INT IFAC WORKSH, P301
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Maass W., 1999, PULSED NEURAL NETWOR
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   NATSCHLAEGER T, 2002, PROCESSING TELEMATIK, V8, P32
   Papik K., 1998, MED SCI MONITOR, V4, P538
   Ponulak F., 2006, RESUME PROOF CONVERG
   Ponulak F., 2006, PROC EPFL LATSIS S D, P119
   Ponulak F., 2006, THESIS POZNAN U TECH
   Ponulak F., 2005, RESUME NEW SUPERVISE
   Ponulak F, 2006, P EUR S ART NEUR NET, P623
   PONULAK F, 2005, P 3 INT S AD MOT AN
   VANHEMMEN JL, 2001, HDB BIOL PHYS, V4, P771
NR 25
TC 22
Z9 32
U1 1
U2 9
PY 2008
VL 18
IS 2
BP 117
EP 127
DI 10.2478/v10006-008-0011-1
UT WOS:000257330400002
DA 2023-11-16
ER

PT C
AU Schrauwen, B
   D'Haene, M
   Verstraeten, D
   Van Campenhout, J
AF Schrauwen, Benjamin
   D'Haene, Michiel
   Verstraeten, David
   Van Campenhout, Jan
GP IEEE
TI Compact hardware for real-time speech recognition using a liquid state
   machine
SO 2007 IEEE INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS, VOLS 1-6
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks
CY AUG 12-17, 2007
CL Orlando, FL
ID NETWORKS
AB Hardware implementations of Spiking Neural Networks are numerous because they are well suited for implementation in digital and analog hardware, and outperform classic neural networks. This work presents an application driven digital hardware exploration where we implement realtime, isolated digit speech recognition using a Liquid State Machine (a recurrent neural network of spiking neurons where only the output layer is trained). First we test two existing hardware architectures, but they appear to be too fast and thus area consuming for this application. Then we present a scalable, serialised architecture that allows a very compact implementation of spiking neural networks that is still fast enough for real-time processing. This work shows that there is actually a large hardware design space of Spiking Neural Network hardware that can be explored. Existing architectures only spanned part of it.
C1 [Schrauwen, Benjamin; D'Haene, Michiel; Verstraeten, David; Van Campenhout, Jan] Univ Ghent, Elect & Informat Syst Dept, Ghent, Belgium.
RP Schrauwen, B (corresponding author), Univ Ghent, Elect & Informat Syst Dept, Ghent, Belgium.
EM Benjamin.Schrauwen@UGent.be
CR [Anonymous], 2002, 159 GMD GERM NAT RES
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Delorme A, 2001, NEURAL NETWORKS, V14, P795, DOI 10.1016/S0893-6080(01)00049-1
   FLOREANO D, 2004, ARTIFICIAL LIFE
   GIRAU B, 2006, P ESANN, P173
   Hellmich HH, 2005, IEEE IJCNN, P3261
   Jaeger H, 2004, SCIENCE, V304, P78, DOI 10.1126/science.1091277
   Jahnke A., 1996, Proceedings of the Fifth International Conference on Microelectronics for Neural Networks and Fuzzy Systems. MicroNeuro'96, P232, DOI 10.1109/MNNFS.1996.493796
   LEGENSTEIN R, 2005, WHAT MAKES DYNAMICAL
   Lyon R. F., 1982, Proceedings of ICASSP 82. IEEE International Conference on Acoustics, Speech and Signal Processing, P1282
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   MAASS W, 1997, P NIPS, P211
   Pearson M. J., 2005, Proceedings. 2005 International Conference on Field Programmable Logic and Applications (IEEE Cat. No.05EX1155), P582
   Roggen D, 2003, 2003 NASA/DOD CONFERENCE ON EVOLVABLE HARDWARE, P189
   SCHOENAUER T, 1998, VIDYNN 98
   Schrauwen B, 2003, IEEE IJCNN, P2825
   SCHRAUWEN B, 2006, P ESANN, P623
   SCHRAUWEN B, 2006, P IJCNN
   SCHRAUWEN B, 2004, P IJCNN
   SCHURMANN F, 2004, P NIPS
   Upegui A, 2005, MICROPROCESS MICROSY, V29, P211, DOI 10.1016/j.micpro.2004.08.012
   Verstraeten D, 2005, INFORM PROCESS LETT, V95, P521, DOI 10.1016/j.ipl.2005.05.019
   VERSTRAETEN D, 2006, P IJCNN
   VERSTRAETEN D, 2006, UNPUB NEURAL NETWORK
   [No title captured]
NR 26
TC 6
Z9 6
U1 1
U2 3
PY 2007
BP 1097
EP 1102
DI 10.1109/IJCNN.2007.4371111
UT WOS:000254291101010
DA 2023-11-16
ER

PT C
AU Arya, AS
   Ravi, V
   Tejasviram, V
   Sengupta, N
   Kasabov, N
AF Arya, Amit Soni
   Ravi, Vadlamani
   Tejasviram, Vadali
   Sengupta, Neelava
   Kasabov, N.
GP IEEE
TI Cyber Fraud Detection using Evolving Spiking Neural Network
SO 2016 11TH INTERNATIONAL CONFERENCE ON INDUSTRIAL AND INFORMATION SYSTEMS
   (ICIIS)
SE International Conference on Industrial and Information Systems
DT Proceedings Paper
CT 11th International Conference on Industrial and Information Systems
   (ICIIS)
CY DEC 03-04, 2016
CL Roorkee, INDIA
ID CLASSIFICATION; NEURONS; MODEL
AB With the rapid growth of the internet, most of the businesses are now moving online. Since the internet is ubiquitous and can be accessed from anywhere websites are susceptible to attacks. One of such attack is phishing website attack. In which an attacker creates a duplicate copy of the website and tries to pose it as a legitimate to steal user's information. So it is the utmost need to detect such phishing websites. Machine learning techniques have been successfully applied to detect the phishing websites. The neural network is one of the efficient ways for detecting these phishing attacks.
   In our work, we have applied the spiking neural network approach to detect these phishing websites. The spiking neural network is biologically inspired by neuroscience literature, evolving spiking neural classifier for the pattern classification problem. We have compared it with various other machine learning techniques and we show that the evolving spiking neural network performs better than the existing machine learning techniques.
C1 [Arya, Amit Soni] Univ Hyderabad, Sch Comp & Informat Sci, Hyderabad, Andhra Pradesh, India.
   [Ravi, Vadlamani; Tejasviram, Vadali] IDRBT, Ctr Excellence Analyt, Castle Hills,Rd 1, Hyderabad 500057, Andhra Pradesh, India.
   [Sengupta, Neelava; Kasabov, N.] Auckland Univ Technol, KEDRI, AUT Tower,Level 7,Corner Rutland & Wakefield St, Auckland 1010, New Zealand.
RP Arya, AS (corresponding author), Univ Hyderabad, Sch Comp & Informat Sci, Hyderabad, Andhra Pradesh, India.
EM amitsoniuoh@uohyd.ac.in; vravi@idrbt.ac.in; vtejasviram@gmail.com;
   neelava.sengupta@aut.ac.nz; nkasabov@aut.ac.nz
CR Abbott LF, 1999, BRAIN RES BULL, V50, P303, DOI 10.1016/S0361-9230(99)00161-6
   [Anonymous], 2012 INT JOINT C NEU, DOI DOI 10.1109/IJCNN.2012.6252439
   Barto A, 1998, INTRO REINFORCEMENT, V1st
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   Brader JM, 2007, NEURAL COMPUT, V19, P2881, DOI 10.1162/neco.2007.19.11.2881
   Buesing L, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1002211
   Dora S, 2015, APPL SOFT COMPUT, V36, P255, DOI 10.1016/j.asoc.2015.06.062
   Gerstner W., 2002, SPIKING NEURON MODEL
   Ghosh-Dastidar S, 2007, INTEGR COMPUT-AID E, V14, P187
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Kasabov Nikola, 2012, Advances in Computational Intelligence. IEEE World Congress on Computational Intelligence (WCCI 2012). Plenary/Invited Lectures, P234, DOI 10.1007/978-3-642-30687-7_12
   Kasabov N, 2014, NEUROCOMPUTING, V134, P269, DOI 10.1016/j.neucom.2013.09.049
   Kasabov N, 2013, NEURAL NETWORKS, V41, P188, DOI 10.1016/j.neunet.2012.11.014
   Kubota N, 2005, INFORM SCIENCES, V171, P403, DOI 10.1016/j.ins.2004.09.012
   Maass W, 2000, NEURAL COMPUT, V12, P2519, DOI 10.1162/089976600300014827
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maass W, 2001, PULSED NEURAL NETWOR
   Masquelier T, 2009, NEURAL COMPUT, V21, P1259, DOI 10.1162/neco.2008.06-08-804
   Pandey M, 2013, LECT NOTES COMPUT SC, V8298, P559, DOI 10.1007/978-3-319-03756-1_50
   Paugam-Moisy H., 2012, HDB NATURAL COMPUTIN, V1, P1, DOI [DOI 10.1007/978-3-540-92910-9_10, 10.1007/978-3-540-92910-9_10]
   Schliebs S, 2013, EVOL SYST-GER, V4, P87, DOI 10.1007/s12530-013-9074-9
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   Thorpe S, 1998, COMPUTATIONAL NEUROSCIENCE: TRENDS IN RESEARCH, P113
   THORPE SJ, 1990, PARALLEL PROCESSING IN NEURAL SYSTEMS AND COMPUTERS, P91
   VanRullen R, 2005, TRENDS NEUROSCI, V28, P1, DOI 10.1016/j.tins.2004.10.010
   Vreeken Jilles, 2002, UUCS2003008
   Wysoski SG, 2006, LECT NOTES COMPUT SC, V4131, P61
   Xin JG, 2001, IEEE IJCNN, P1772, DOI 10.1109/IJCNN.2001.938430
NR 31
TC 3
Z9 3
U1 1
U2 3
PY 2016
BP 263
EP 268
UT WOS:000425231300053
DA 2023-11-16
ER

PT C
AU Feldbusch, F
   Kaiser, F
AF Feldbusch, F
   Kaiser, F
GP IEEE
TI Simulation of spiking neural nets with INSpiRE ME
SO INTERNATIONAL CONFERENCE ON SYSTEMS, MAN AND CYBERNETICS, VOL 1-4,
   PROCEEDINGS
SE IEEE International Conference on Systems Man and Cybernetics Conference
   Proceedings
DT Proceedings Paper
CT IEEE International Conference on Systems, Man and Cybernetics
CY OCT 10-12, 2005
CL Waikoloa, HI
DE spiking neural nets; event-driven simulation; spike response model
ID NETWORKS
AB While the usage of rate based neural nets for classification tasks is state of the art, this approach lacks one characteristic: time. However, many application patterns to be recognized are distributed over time. Although there exist some approaches introducing time into neural nets like the time delayed neural networks (TDNN), these approaches generally compute in cycles and are therefore insensitive to more subtle timescales. Thus, a more powerful approach is desirable. We examined spiking neural nets to tackle this task and developed an event-driven simulator for spiking neural nets called INSpiRE ME. The simulator uses the biologically inspired spike response model. To achieve a fast and powerful tool many design decisions had to be made, which will be explained in this paper
C1 Univ Karlsruhe, Dept Comp Sci, Karlsruhe, Germany.
RP Feldbusch, F (corresponding author), Univ Karlsruhe, Dept Comp Sci, Karlsruhe, Germany.
EM feldbus@ira.uka.de; fkaiser@ira.uka.de
CR [Anonymous], 1998, BOOK GENESIS EXPLORI, DOI DOI 10.1007/978-1-4612-1634-63
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   BULLOCK TH, 1968, P NATL ACAD SCI USA, V60, P1058, DOI 10.1073/pnas.60.4.1058
   Delorme A, 2003, NETWORK-COMP NEURAL, V14, P613, DOI 10.1088/0954-898X/14/4/301
   GERSTNER W, 1995, PHYS REV E, V51, P738, DOI 10.1103/PhysRevE.51.738
   Gerstner W., 2002, SPIKING NEURON MODEL
   Hines ML, 1997, NEURAL COMPUT, V9, P1179, DOI 10.1162/neco.1997.9.6.1179
   MAASS W, 1987, ADV NEURAL INFORM PR, V9, P211
   MAASS W, 2002, COMPUTATIONAL NEUROS
   Marian I, 2002, THESIS U COLL DUBLIN
   MOORE S, 2002, THESIS U BATH
   Schrauwen B, 2004, IEEE IJCNN, P471, DOI 10.1109/IJCNN.2004.1379954
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Stürzl W, 2000, PHYS REV LETT, V84, P5668, DOI 10.1103/PhysRevLett.84.5668
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
NR 15
TC 0
Z9 0
U1 0
U2 0
PY 2005
BP 999
EP 1004
UT WOS:000235210801001
DA 2023-11-16
ER

PT J
AU Chen, L
   Xiong, XZ
   Liu, J
AF Chen, Lu
   Xiong, Xingzhong
   Liu, Jun
TI A Survey of Intelligent Chip Design Research Based on Spiking Neural
   Networks
SO IEEE ACCESS
DT Article
DE Neurons; Biological system modeling; Biological neural networks;
   Computational modeling; Encoding; Hardware; Power demand; Neural
   networks; Stochastic processes; Spiking neural network; spiking neuron
   model; neural computing; stochastic computing; brain-like chip
ID HARDWARE IMPLEMENTATION; DATA-STORAGE; NEURONS; MODEL; SIMULATION;
   PLASTICITY; PROCESSOR; SYNAPSES; DYNAMICS; QUERIES
AB The traditional neural network Intelligent chip has the problem of high power consumption due to classical computing architecture, limiting the development of neural network Intelligent chips. Stochastic computing (SC) encodes binary numbers into stochastic pulse sequences in operation, taking advantage of low power consumption and high performance. The application of SC in spiking neural networks (SNNs) Intelligent chips is beneficial to solving the high power consumption of traditional neural network chips. This article first summarizes the basic elements of SNNs and the basic principles of SC. Then, we review the development trends of the stochastic computation-based neural network chips and existing SNN chips under research at home and abroad, respectively, and analyze the current problems. Finally, a review of SNN chips based on SC is highlighted. This paper aims to provide new research directions and to learn ideas for the field of SNN chips through systematic summaries.
C1 [Xiong, Xingzhong] Sichuan Univ Sci & Engn, Sch Automat & Informat Engn, Yibin 644000, Peoples R China.
   Artificial Intelligence Key Lab Sichuan Prov, Yibin 644000, Peoples R China.
RP Xiong, XZ (corresponding author), Sichuan Univ Sci & Engn, Sch Automat & Informat Engn, Yibin 644000, Peoples R China.
EM xzxiong@suse.edu.cn
CR Abadi Martin, 2016, arXiv
   Abbott LF, 1999, BRAIN RES BULL, V50, P303, DOI 10.1016/S0361-9230(99)00161-6
   Adewuyi EO, 2022, COMMUN BIOL, V5, DOI 10.1038/s42003-022-03607-2
   Agarwal S, 2016, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00484
   Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   Akopyan FA, 2016, PROCEEDINGS OF THE 2016 INTERNATIONAL SYMPOSIUM ON PHYSICAL DESIGN (ISPD'16), P59, DOI 10.1145/2872334.2878629
   Alaghi A, 2018, IEEE T COMPUT AID D, V37, P1515, DOI 10.1109/TCAD.2017.2778107
   Alaghi A, 2013, ACM T EMBED COMPUT S, V12, DOI 10.1145/2465787.2465794
   Alaghi A, 2013, 2013 IEEE 31ST INTERNATIONAL CONFERENCE ON COMPUTER DESIGN (ICCD), P39, DOI 10.1109/ICCD.2013.6657023
   Alawad M, 2017, IEEE INT CONF BIG DA, P311, DOI 10.1109/BigData.2017.8257939
   Amit DJ, 1997, NETWORK-COMP NEURAL, V8, P373, DOI 10.1088/0954-898X/8/4/003
   Ankit A, 2017, DES AUT CON, DOI 10.1145/3061639.3062311
   [Anonymous], 2012, P 26 ACM INT C SUPER
   [Anonymous], 2015, COMPUT SCI
   [Anonymous], 2014, P INT C COMP ARCH SY
   [Anonymous], 2012 INT JOINT C NEU, DOI DOI 10.1109/IJCNN.2012.6252439
   [Anonymous], 1980, SOC IND APPL MATH, DOI DOI 10.1137/1.9781611970364
   Ardakani A, 2017, IEEE T VLSI SYST, V25, P2688, DOI 10.1109/TVLSI.2017.2654298
   Azghadi MR, 2015, ACM J EMERG TECH COM, V12, DOI 10.1145/2658998
   Bao C, 2021, SENSOR ACTUAT B-CHEM, V332, DOI 10.1016/j.snb.2021.129527
   Batude P., 2015, 2015 Symposium on VLSI Technology, pT48, DOI 10.1109/VLSIT.2015.7223698
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Boahen KA, 2000, IEEE T CIRCUITS-II, V47, P416, DOI 10.1109/82.842110
   Boahen K, 2010, NEUROSCI RES, V68, pE31, DOI 10.1016/j.neures.2010.07.378
   Bouvier M., 2020, ARXIV
   Brette R, 2007, J COMPUT NEUROSCI, V23, P349, DOI 10.1007/s10827-007-0038-6
   Brown BD, 2001, IEEE T COMPUT, V50, P891, DOI 10.1109/12.954505
   Buonocore A, 2016, MATH BIOSCI ENG, V13, P483, DOI 10.3934/mbe.2016002
   Burr GW, 2015, 2015 IEEE INTERNATIONAL ELECTRON DEVICES MEETING (IEDM), DOI 10.1109/iedm.2015.7409625
   Campbell M. J., 1991, U.S. Patent, Patent No. [5021947-A, 5021947]
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Chang K.-T., 2017, 2017 IEEEACM INT S L, P1, DOI DOI 10.1109/ISLPED.2017.8009175
   Chaturvedi S, 2013, INT CONF EMERG TR, P191, DOI 10.1109/ICETET.2013.54
   Chen GK, 2019, IEEE J SOLID-ST CIRC, V54, P992, DOI 10.1109/JSSC.2018.2884901
   Chen M.-F., 2017, ARXIV
   Chen YH, 2016, ISSCC DIG TECH PAP I, V59, P262, DOI 10.1109/ISSCC.2016.7418007
   Chen YJ, 2014, INT SYMP MICROARCH, P609, DOI 10.1109/MICRO.2014.58
   [程龙 Cheng Long], 2018, [控制与决策, Control and Decision], V33, P923
   Chetlur S., 2014, ARXIV PREPRINT ARXIV
   Chien CH, 2018, IEEE T EMERG TOP COM, V6, P135, DOI 10.1109/TETC.2015.2424593
   Cong Jason, 2014, Artificial Neural Networks and Machine Learning - ICANN 2014. 24th International Conference on Artificial Neural Networks. Proceedings: LNCS 8681, P281, DOI 10.1007/978-3-319-11179-7_36
   COOK SA, 1969, T AM MATH SOC, V142, P291, DOI 10.2307/1995359
   Courbariaux M, 2015, ADV NEUR IN, V28
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Davies S, 2012, IEEE IJCNN
   de Garis H., 1993, Artificial Neural Nets and Genetic Algorithms. Proceedings of the International Conference, P441
   Deng L, 2020, P IEEE, V108, P485, DOI 10.1109/JPROC.2020.2976475
   Deng L, 2018, NEURAL NETWORKS, V100, P49, DOI 10.1016/j.neunet.2018.01.010
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Ding CW, 2017, 50TH ANNUAL IEEE/ACM INTERNATIONAL SYMPOSIUM ON MICROARCHITECTURE (MICRO), P395, DOI 10.1145/3123939.3124552
   Ehsan MA, 2018, IEEE T COMPUT AID D, V37, P1640, DOI 10.1109/TCAD.2017.2760506
   El-Harouni W, 2017, DES AUT TEST EUROPE, P1384, DOI 10.23919/DATE.2017.7927209
   Eryilmaz SB, 2015, 2015 IEEE INTERNATIONAL ELECTRON DEVICES MEETING (IEDM)
   Faraji SR, 2019, DES AUT TEST EUROPE, P1757, DOI [10.23919/date.2019.8714937, 10.23919/DATE.2019.8714937]
   Feng HF, 2001, NEURAL NETWORKS, V14, P955, DOI 10.1016/S0893-6080(01)00074-0
   FISCHL KD, 2017, 2017 51 ANN C INF, pNI587
   Frenkel C, 2019, IEEE T BIOMED CIRC S, V13, P145, DOI 10.1109/TBCAS.2018.2880425
   Furber S, 2016, J NEURAL ENG, V13, DOI 10.1088/1741-2560/13/5/051001
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   Garbin D, 2015, IEEE T ELECTRON DEV, V62, P2494, DOI 10.1109/TED.2015.2440102
   Gautrais J, 1998, BIOSYSTEMS, V48, P57, DOI 10.1016/S0303-2647(98)00050-1
   GERSTNER W, 1995, PHYS REV E, V51, P738, DOI 10.1103/PhysRevE.51.738
   Gerstner W., 2002, SPIKING NEURON MODEL
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Glackin B, 2005, LECT NOTES COMPUT SC, V3512, P552
   Han J, 2013, PROC EUR TEST SYMP
   Han S, 2016, CONF PROC INT SYMP C, P243, DOI 10.1109/ISCA.2016.30
   Hayes JP, 2015, DES AUT CON, DOI 10.1145/2744769.2747932
   Hirtzlin T, 2019, IEEE ACCESS, V7, P76394, DOI 10.1109/ACCESS.2019.2921104
   Hoai ALT, 2011, COMPUT OPTIM APPL, V50, P463, DOI 10.1007/s10589-011-9433-z
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V116, P449, DOI 10.1113/jphysiol.1952.sp004717
   Hojabr R, 2019, PROCEEDINGS OF THE 2019 56TH ACM/EDAC/IEEE DESIGN AUTOMATION CONFERENCE (DAC), DOI 10.1145/3316781.3317911
   Horio Y, 2005, NEUROCOMPUTING, V64, P447, DOI 10.1016/j.neucom.2004.09.001
   Horowitz M, 2014, ISSCC DIG TECH PAP I, V57, P10, DOI 10.1109/ISSCC.2014.6757323
   Howard Andrew G., 2017, MOBILENETS EFFICIENT
   Howard D, 2015, CONNECT SCI, V27, P397, DOI 10.1080/09540091.2015.1080225
   Hsu F, 2002, DEEP BLUE BUILDING C
   Huang TJ, 2017, INT J AUTOM COMPUT, V14, P520, DOI 10.1007/s11633-017-1082-y
   [黄铁军 Huang Tiejun], 2019, [计算机研究与发展, Journal of Computer Research and Development], V56, P1135
   Hubara I, 2018, J MACH LEARN RES, V18
   Imam N, 2012, INT SYMP ASYNCHRON C, P25, DOI 10.1109/ASYNC.2012.12
   Indiveri G, 2006, IEEE T NEURAL NETWOR, V17, P211, DOI 10.1109/TNN.2005.860850
   Indiveri G, 2015, 2015 IEEE INTERNATIONAL ELECTRON DEVICES MEETING (IEDM)
   Indiveri G, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00073
   Izhikevich E. M., 2006, DYNAMICAL SYSTEMS NE, DOI DOI 10.7551/MITPRESS/2526.001.0001
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Jayakumar H, 2016, ASIA S PACIF DES AUT, P298, DOI 10.1109/ASPDAC.2016.7428027
   Jenson D, 2016, ICCAD-IEEE ACM INT, DOI 10.1145/2966986.2966988
   Jerry M, 2017, S VLSI TECH, pT186
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Jo SH, 2010, NANO LETT, V10, P1297, DOI 10.1021/nl904092h
   Joseph S, 2002, LECT NOTES COMPUT SC, V2376, P202
   Joshi Prashant, 2009, Proceedings 2009 International Joint Conference on Neural Networks (IJCNN 2009 - Atlanta), P1456, DOI 10.1109/IJCNN.2009.5178625
   Kappel D., 2015, PROC INT C NEURAL IN, P1
   Kasabov N, 2013, NEURAL NETWORKS, V41, P188, DOI 10.1016/j.neunet.2012.11.014
   Kim D, 2016, CONF PROC INT SYMP C, P380, DOI 10.1109/ISCA.2016.41
   Kim KH, 2012, NANO LETT, V12, P389, DOI 10.1021/nl203687n
   Kim K, 2016, DES AUT CON, DOI 10.1145/2897937.2898011
   Kim K, 2015, INT SOC DESIGN CONF, P123, DOI 10.1109/ISOCC.2015.7401667
   Kim Y, 2013, ICCAD-IEEE ACM INT, P130, DOI 10.1109/ICCAD.2013.6691108
   Knag P, 2014, IEEE T NANOTECHNOL, V13, P283, DOI 10.1109/TNANO.2014.2300342
   Korn F., 1997, SIGMOD Record, V26, P289, DOI 10.1145/253262.253332
   Kuang ZB, 2019, CHIN CONT DECIS CONF, P3621, DOI [10.1109/ccdc.2019.8832952, 10.1109/CCDC.2019.8832952]
   Lavin A, 2016, PROC CVPR IEEE, P4013, DOI 10.1109/CVPR.2016.435
   Lee SJ, 2002, MOBILE NETW APPL, V7, P441, DOI 10.1023/A:1020756600187
   Li C, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351877
   Li H, 2006, IEICE T INF SYST, VE89D, P2572, DOI 10.1093/ietisy/e89-d.9.2572
   Li J, 2019, INTEGRATION, V65, P395, DOI 10.1016/j.vlsi.2017.11.002
   Li Z, 2019, IEEE T COMPUT AID D, V38, P1543, DOI 10.1109/TCAD.2018.2852752
   Li Z, 2017, DES AUT TEST EUROPE, P250, DOI 10.23919/DATE.2017.7926991
   Li Z, 2016, PR IEEE COMP DESIGN, P678, DOI 10.1109/ICCD.2016.7753357
   Liang J, 2010, IEEE T ELECTRON DEV, V57, P2531, DOI 10.1109/TED.2010.2062187
   Liang L., 2011, INRIA IRISA RENNES B, V32, P203
   Lin CK, 2018, COMPUTER, V51, P52, DOI 10.1109/MC.2018.157113521
   Lindholm E, 2008, IEEE MICRO, V28, P39, DOI 10.1109/MM.2008.31
   Liu JX, 2019, 2019 13TH INTERNATIONAL SYMPOSIUM ON THEORETICAL ASPECTS OF SOFTWARE ENGINEERING (TASE 2019), P81, DOI [10.1109/TASE.2019.00-16, 10.1109/TASE.2019.00018]
   Liu L, 1999, IEEE T KNOWL DATA EN, V11, P610, DOI 10.1109/69.790816
   Liu YH, 2001, J COMPUT NEUROSCI, V10, P25, DOI 10.1023/A:1008916026143
   Liu YD, 2021, IEEE T NEUR NET LEAR, V32, P2809, DOI 10.1109/TNNLS.2020.3009047
   Liu YD, 2018, IEEE J EM SEL TOP C, V8, P454, DOI 10.1109/JETCAS.2018.2852705
   Lu LQ, 2017, ANN IEEE SYM FIELD P, P101, DOI 10.1109/FCCM.2017.64
   Lu WY, 2017, INT S HIGH PERF COMP, P553, DOI 10.1109/HPCA.2017.29
   Lunglmayr M, 2020, IEEE T COMPUT, V69, P402, DOI 10.1109/TC.2019.2949779
   Ma D, 2017, J SYST ARCHITECT, V77, P43, DOI 10.1016/j.sysarc.2017.01.003
   Mahapatra N. R., 1999, XRDS CROSSROADS ACM, V5, P1
   Mansouri SA, 2012, INT J PROD ECON, V135, P24, DOI 10.1016/j.ijpe.2010.11.016
   Maor G, 2019, PR IEEE COMP DESIGN, P38, DOI 10.1109/ICCD46524.2019.00014
   Marukame T, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351298
   Mathieu M, 2013, ARXIV
   Maya S., 2000, Field-Programmable Logic and Applications. Roadmap to Reconfigurable Computing. 10th International Conference, FPL 2000. Proceedings (Lecture Notes in Computer Science Vol.1896), P270
   MEAD CA, 1989, ADVANCED RESEARCH IN VLSI : PROCEEDINGS OF THE DECENNIAL CALTECH CONFERENCE ON VLSI, P1
   Merolla P, 2011, IEEE CUST INTEGR CIR
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Mesaritakis C, 2016, SCI REP-UK, V6, DOI 10.1038/srep39317
   Michaelis C., 2021, ARXIV
   Midya R, 2019, ADV ELECTRON MATER, V5, DOI 10.1002/aelm.201900060
   Mikaitis M, 2018, P S COMP ARITHM, P37, DOI 10.1109/ARITH.2018.8464785
   Minsky M., 1969, PERCEPTRONS INTRO CO
   Misra J, 2010, NEUROCOMPUTING, V74, P239, DOI 10.1016/j.neucom.2010.03.021
   Molka Daniel, 2010, 2010 International Conference on Green Computing (Green Comp), P123, DOI 10.1109/GREENCOMP.2010.5598316
   Morro A, 2018, IEEE T NEUR NET LEAR, V29, P1371, DOI 10.1109/TNNLS.2017.2657601
   Mostafaei H, 2017, 2017 IEEE CONFERENCE ON NETWORK SOFTWARIZATION (IEEE NETSOFT)
   Mundy A., 2015, P INT JOINT C NEUR N, P1, DOI DOI 10.1109/IJCNN.2015.7280390
   Neftci E. O., 2015, ARXIV
   Neil D, 2014, IEEE T VLSI SYST, V22, P2621, DOI 10.1109/TVLSI.2013.2294916
   Niu LY, 2022, NEURAL PROCESS LETT, V54, P1055, DOI 10.1007/s11063-021-10669-6
   OConnor P., 2016, ARXIV
   Park J, 2017, IEEE T NEUR NET LEAR, V28, P2408, DOI 10.1109/TNNLS.2016.2572164
   Paszke A., 2017, P NIPS WORKSH LONG B, P1
   Pearson M. J., 2005, Proceedings. 2005 International Conference on Field Programmable Logic and Applications (IEEE Cat. No.05EX1155), P582
   Potuzak M, 2008, EARTH PLANET SC LETT, V270, P54, DOI 10.1016/j.epsl.2008.03.018
   Prezioso M, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-07757-y
   Prezioso M, 2015, NATURE, V521, P61, DOI 10.1038/nature14441
   Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32
   Rathi N., 2017, ARXIV
   Razzali N, 2009, 2009 IEEE 9TH MALAYSIA INTERNATIONAL CONFERENCE ON COMMUNICATIONS (MICC), P84, DOI 10.1109/MICC.2009.5431469
   Ren A, 2017, ACM SIGPLAN NOTICES, V52, P405, DOI 10.1145/3093336.3037746
   Ren A, 2017, OPER SYST REV, V51, P405, DOI 10.1145/3037697.3037746
   Rice KL, 2009, 2009 INTERNATIONAL CONFERENCE ON RECONFIGURABLE COMPUTING AND FPGAS, P451, DOI 10.1109/ReConFig.2009.77
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sakuma K, 2018, DEVICE CIRC SYST, P1
   Sangkil Kim, 2015, 2015 IEEE MTT-S International Microwave Symposium (IMS2015), P1, DOI 10.1109/MWSYM.2015.7166723
   Sartori J, 2011, PROCEEDINGS OF THE PROCEEDINGS OF THE 14TH INTERNATIONAL CONFERENCE ON COMPILERS, ARCHITECTURES AND SYNTHESIS FOR EMBEDDED SYSTEMS (CASES '11), P135
   Sato S, 2003, IEEE T NEURAL NETWOR, V14, P1122, DOI 10.1109/TNN.2003.816341
   Schemmel J, 2017, IEEE IJCNN, P2217, DOI 10.1109/IJCNN.2017.7966124
   Schemmel J, 2012, IEEE INT SYMP CIRC S
   Schemmel J, 2010, IEEE INT SYMP CIRC S, P1947, DOI 10.1109/ISCAS.2010.5536970
   Schemmel J, 2008, IEEE IJCNN, P431, DOI 10.1109/IJCNN.2008.4633828
   Se S, 2005, IEEE T ROBOT, V21, P364, DOI 10.1109/TRO.2004.839228
   Sen S, 2017, DES AUT TEST EUROPE, P193, DOI 10.23919/DATE.2017.7926981
   Sheik S, 2016, IEEE INT SYMP CIRC S, P2090, DOI 10.1109/ISCAS.2016.7538991
   Sickle K. V., 2009, PROC IEEE NAT AEROSP, P275
   Silver R, 2007, J NEUROSCI, V27, P11807, DOI 10.1523/JNEUROSCI.3575-07.2007
   Singh M., 2010, ACM T DES AUTOMAT EL, V16, P11
   Smithson SC, 2016, 2016 IEEE INTERNATIONAL WORKSHOP ON SIGNAL PROCESSING SYSTEMS (SIPS), P309, DOI 10.1109/SiPS.2016.61
   Soize C, 2012, CISM COURSES LECT, V539, P61
   Soula H., 2005, P AM ASS ART INT AAA, P1
   Srinivasan G, 2016, SCI REP-UK, V6, DOI 10.1038/srep29545
   STRASSEN V, 1969, NUMER MATH, V13, P354, DOI 10.1007/BF02165411
   Stromatias E, 2015, IEEE IJCNN
   Stromatias E, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00222
   Strukov DB, 2008, NATURE, V453, P80, DOI 10.1038/nature06932
   Suri M, 2011, 2011 IEEE INTERNATIONAL ELECTRON DEVICES MEETING (IEDM)
   Sze V, 2017, P IEEE, V105, P2295, DOI 10.1109/JPROC.2017.2761740
   Tang TQ, 2017, ASIA S PACIF DES AUT, P782, DOI 10.1109/ASPDAC.2017.7858419
   Thomas DB, 2009, ANN IEEE SYM FIELD P, P45, DOI 10.1109/FCCM.2009.46
   TOOM AL, 1963, DOKL AKAD NAUK SSSR+, V150, P496
   van Schaik A, 2001, NEURAL NETWORKS, V14, P617, DOI 10.1016/S0893-6080(01)00067-3
   VanRullen R, 2005, TRENDS NEUROSCI, V28, P1, DOI 10.1016/j.tins.2004.10.010
   Vasilache N., 2014, ARXIV
   Wang D., 2015, 15 NONV MEM TECHN S, P1
   Wang HF, 2018, 2018 ACM SYMPOSIUM ON EYE TRACKING RESEARCH & APPLICATIONS (ETRA 2018), DOI 10.1145/3204493.3204584
   Wang JL, 2014, NEUROCOMPUTING, V144, P526, DOI 10.1016/j.neucom.2014.04.017
   [王亚辉 Wang Yahui], 2016, [计算机科学, Computer Science], V43, P24
   Wang YM, 2015, IEEE COMPUT INTELL M, V10, P42, DOI 10.1109/MCI.2015.2405318
   Wemmie JA, 2002, NEURON, V34, P463, DOI 10.1016/S0896-6273(02)00661-X
   Whatmough PN, 2017, ISSCC DIG TECH PAP I, P242, DOI 10.1109/ISSCC.2017.7870351
   Wiart J, 2013, PROC EUR CONF ANTENN, P1979
   Wille R, 2016, ICCAD-IEEE ACM INT, DOI 10.1145/2966986.2980099
   Wu D, 2019, PROCEEDINGS OF THE 2019 56TH ACM/EDAC/IEEE DESIGN AUTOMATION CONFERENCE (DAC), DOI [10.1145/3316781.3317844, 10.1109/egrid48402.2019.9092701]
   Wu ZH, 2016, IEEE INTELL SYST, V31, P44, DOI 10.1109/MIS.2016.105
   Wysoski S. G., 2008, ADAPTIVE SPIKING NEU
   Wysoski SG, 2008, NEUROCOMPUTING, V71, P2563, DOI 10.1016/j.neucom.2007.12.038
   Xiang SY, 2021, IEEE J SEL TOP QUANT, V27, DOI 10.1109/JSTQE.2020.3005589
   Xiao SL, 2020, IEEE T BIOMED CIRC S, V14, P942, DOI 10.1109/TBCAS.2020.2995869
   Xiong HF, 2020, IEEE T CIRCUITS-II, V67, P2667, DOI 10.1109/TCSII.2020.2969691
   Xu Y, 2017, NEURAL NETWORKS, V93, P7, DOI 10.1016/j.neunet.2017.04.010
   Yakopcic C, 2020, DES AUT TEST EUROPE, P1079, DOI [10.23919/DATE48585.2020.9116227, 10.23919/date48585.2020.9116227]
   Yang YS, 2020, INT SOC DESIGN CONF, P218, DOI 10.1109/ISOCC50952.2020.9332961
   Yilma G., 2021, PROC 5 INT C ADV IMA, P13
   Yin SY, 2017, DES AUT CON, DOI 10.1145/3061639.3062232
   Yousefzadeh A, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351562
   Zell A., 1993, SNNS STUTTGART NEURA, P1
   Zhang D, 2008, IEEE T IND ELECTRON, V55, P551, DOI 10.1109/TIE.2007.911946
   Zhang H., 2015, COMPUT SCI, V43, P7
   Zhang XM, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-019-13827-6
   Zhang XM, 2017, IEEE ELECTR DEVICE L, V38, P1208, DOI 10.1109/LED.2017.2722463
   Zhang Z. F., 2009, TECHNOL ENG, V9, P2564
   Zhang ZD, 2020, IEEE T CIRCUITS-II, V67, P3342, DOI 10.1109/TCSII.2020.2993273
   Zhao CY, 2016, IEEE T MULTI-SCALE C, V2, P265, DOI 10.1109/TMSCS.2016.2607164
   Zyarah AM, 2017, IEEE INT SYMP CIRC S
   2015, EV NEUR SYST, P1, DOI DOI 10.1002/9781118927601
NR 225
TC 1
Z9 1
U1 9
U2 24
PY 2022
VL 10
BP 89663
EP 89686
DI 10.1109/ACCESS.2022.3200454
UT WOS:000848200200001
DA 2023-11-16
ER

PT C
AU Suszynski, P
   Wawrzynski, P
AF Suszynski, Piotr
   Wawrzynski, Pawel
GP IEEE
TI Learning population of spiking neural networks with perturbation of
   conductances
SO 2013 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY AUG 04-09, 2013
CL Dallas, TX
DE Spiking neural networks; learning
ID REINFORCEMENT
AB In this paper a method is presented for learning of spiking neural networks. It is based on perturbation of synaptic conductances. While this approach is known to be model-free, it is also known to be slow, because it applies improvement direction estimates with large variance. Two ideas are analysed to alleviate this problem: First, learning of many networks at the same time instead of one. Second, autocorrelation of perturbations in time. In the experimental study the method is validated on three learning tasks in which information is conveyed with frequency and spike timing.
C1 [Suszynski, Piotr; Wawrzynski, Pawel] Warsaw Univ Technol, Inst Control & Computat Engn, PL-00661 Warsaw, Poland.
RP Suszynski, P (corresponding author), Warsaw Univ Technol, Inst Control & Computat Engn, PL-00661 Warsaw, Poland.
EM p.suszynski@stud.elka.pw.edu.pl; p.wawrzynski@elka.pw.edu.pl
CR Barto A, 1998, INTRO REINFORCEMENT, V1st
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Dembo A, 1990, IEEE Trans Neural Netw, V1, P58, DOI 10.1109/72.80205
   Fiete IR, 2006, PHYS REV LETT, V97, DOI 10.1103/PhysRevLett.97.048104
   Florian RV, 2007, NEURAL COMPUT, V19, P1468, DOI 10.1162/neco.2007.19.6.1468
   Frank A., 2010, UCI MACHINE LEARNING
   Kushner H., 1997, STOCHASTIC APPROXIMA
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Seung HS, 2003, NEURON, V40, P1063, DOI 10.1016/S0896-6273(03)00761-X
   Werfel J, 2005, NEURAL COMPUT, V17, P2699, DOI 10.1162/089976605774320539
   Williams R., 1992, MACH LEARN, V8, P299
NR 11
TC 1
Z9 1
U1 0
U2 0
PY 2013
UT WOS:000349557200049
DA 2023-11-16
ER

PT J
AU Kasabov, NK
AF Kasabov, Nikola K.
TI Evolving connectionist systems for adaptive learning and knowledge
   discovery: Trends and directions
SO KNOWLEDGE-BASED SYSTEMS
DT Article
DE Knowledge-based systems; Neuro-fuzzy systems; Evolving connectionist
   systems; Evolving spiking neural networks; Computational neurogenetic
   systems; Quantum inspired spiking neural networks; Spatio-temporal
   pattern recognition
ID SPIKING NEURAL-NETWORKS; CLASSIFICATION; ARCHITECTURE
AB This paper follows the 25 years of development of methods and systems for knowledge-based neural network systems and more specifically the recent evolving connectionist systems (ECOS). ECOS combine the adaptive/evolving learning ability of neural networks and the approximate reasoning and linguistically meaningful explanation features of symbolic representation, such as fuzzy rules. This review paper presents the classical now hybrid expert systems and evolving neuro-fuzzy systems, along with new developments in spiking neural networks, neurogenetic systems, and quantum inspired systems, all discussed from the point of few of their adaptability, model interpretability and knowledge discovery. The paper discusses new directions for the integration of principles from neural networks, fuzzy systems, bio- and neuroinformatics, and nature in general. (C) 2015 Elsevier B.V. All rights reserved.
C1 Auckland Univ Technol, KEDRI, Auckland, New Zealand.
RP Kasabov, NK (corresponding author), Auckland Univ Technol, KEDRI, Auckland, New Zealand.
EM nkasabov@aut.ac.nz
CR AMARI S, 1990, P IEEE, V78, P1443, DOI 10.1109/5.58324
   AMARI S, 1967, IEEE TRANS ELECTRON, VEC16, P299, DOI 10.1109/PGEC.1967.264666
   [Anonymous], P 5 IFSA WORLD C SEO
   [Anonymous], 1992, P INT C FUZZ LOG NEU
   [Anonymous], 2007, EVOLVING CONNECTIONI
   [Anonymous], 2003, PERSP NEURAL COMP
   [Anonymous], 2002, EVOLVING RULE BASED
   Benuskova L., 2007, TOP BIOMED ENG
   Futschik ME, 2002, P WORLD C COMP INT W
   GERSTNER W, 1995, PHYS REV E, V51, P738, DOI 10.1103/PhysRevE.51.738
   Hebb D., 2005, ORG BEHAV NEUROPSYCH
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Kasabov N., 1998, Proceedings of the 5th International Conference on Soft Computing and Information/Intelligent Systems. Methodologies for the Conception, Design and Application of Soft Computing, P271
   Kasabov N, 2001, IEEE T SYST MAN CY B, V31, P902, DOI 10.1109/3477.969494
   KASABOV N, 1995, INTELL AUTOM SOFT CO, V1, P351
   Kasabov N., 1996, FDN NEURAL NETWORKS, P550
   Kasabov N. K., 1994, Fuzzy Logic in Artificial Intelligence. IJCAI '93 Workshop. Proceedings, P114
   Kasabov N. K., 1991, Computer Science and Informatics, V21, P26
   Kasabov N. K., 1993, Connection Science, V5, P275, DOI 10.1080/09540099308915702
   Kasabov N. K., 1993, Journal of Systems Engineering, V3, P15
   Kasabov N, 2015, INFORM SCIENCES, V294, P565, DOI 10.1016/j.ins.2014.06.028
   Kasabov N, 2014, NEUROCOMPUTING, V134, P269, DOI 10.1016/j.neucom.2013.09.049
   Kasabov N, 2013, NEURAL NETWORKS, V41, P188, DOI 10.1016/j.neunet.2012.11.014
   Kasabov Nikola, 2010, International Journal of Functional Informatics and Personalised Medicine, V3, P236
   Kasabov NK, 2014, NEURAL NETWORKS, V52, P62, DOI 10.1016/j.neunet.2014.01.006
   Kasabov NK, 2002, IEEE T FUZZY SYST, V10, P144, DOI 10.1109/91.995117
   Kasabov NK, 1997, INFORM SCIENCES, V101, P155, DOI 10.1016/S0020-0255(97)00007-8
   Platel MD, 2009, IEEE T EVOLUT COMPUT, V13, P1218, DOI 10.1109/TEVC.2008.2003010
   Soltic S, 2010, INT J NEURAL SYST, V20, P437, DOI 10.1142/S012906571000253X
   Song Q, 2006, NEURAL NETWORKS, V19, P1591, DOI 10.1016/j.neunet.2006.05.028
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   Verstraeten D, 2007, NEURAL NETWORKS, V20, P391, DOI 10.1016/j.neunet.2007.04.003
   Watts MJ, 2009, IEEE T SYST MAN CY C, V39, P253, DOI 10.1109/TSMCC.2008.2012254
   Widiputra H, 2011, LECT NOTES ARTIF INT, V6635, P161, DOI 10.1007/978-3-642-20847-8_14
   Wysoski SG, 2010, NEURAL NETWORKS, V23, P819, DOI 10.1016/j.neunet.2010.04.009
   Yagar RR., 1994, J INTELL FUZZY SYST, V2, P209, DOI [10.3233/IFS-1994-2301, DOI 10.3233/IFS-1994-2301]
   ZADEH LA, 1988, COMPUTER, V21, P83, DOI 10.1109/2.53
   ZADEH LA, 1965, INFORM CONTROL, V8, P338, DOI 10.1016/S0019-9958(65)90241-X
   [No title captured]
   [No title captured]
NR 42
TC 21
Z9 22
U1 1
U2 22
PD MAY
PY 2015
VL 80
SI SI
BP 24
EP 33
DI 10.1016/j.knosys.2014.12.032
UT WOS:000353853200004
DA 2023-11-16
ER

PT J
AU Farsa, EZ
   Nazari, S
   Gholami, M
AF Farsa, Edris Zaman
   Nazari, Soheila
   Gholami, Morteza
TI Function approximation by hardware spiking neural network
SO JOURNAL OF COMPUTATIONAL ELECTRONICS
DT Article
DE Spiking neural networks; Function approximation; Hardware implementation
ID MODEL
AB Spiking neural networks (SNN) represent a special class of artificial neural networks, where neu-ron models communicate by sequences of spikes. SNNs are often referred to as the third generation of neural networks that highly inspired from natural computing in the brain and recent advances in neuroscience. In this paper we implement biologically-inspired, hardware-realizable SNN architecture using integrate-and-fire units, which is capable of approximating a real-valued function. Based on the results of MATLAB simulations, hardware synthesis and FPGA implementation, it is demonstrated that the implemented hardware can approximate linear and nonlinear functions with low minimum relative error. This framework may represent a fundamental computational unit for the development of artificial SNN, opening new perspectives in pattern recognition tasks.
C1 [Farsa, Edris Zaman] Islamic Azad Univ, Kermanshah Sci & Res Branch, Dept Elect, Coll Engn, Kermanshah, Iran.
   [Nazari, Soheila] Amirkabir Univ Technol, Dept Elect Engn, Tehran, Iran.
   [Gholami, Morteza] Razi Univ, Dept Elect Engn, Kermanshah, Iran.
RP Farsa, EZ (corresponding author), Islamic Azad Univ, Kermanshah Sci & Res Branch, Dept Elect, Coll Engn, Kermanshah, Iran.
EM edriss.zamanfarsa@gmail.com
CR Azghadi MR, 2013, NEURAL NETWORKS, V45, P70, DOI 10.1016/j.neunet.2013.03.003
   Brette R, 2012, NETWORK-COMP NEURAL, V23, P167, DOI 10.3109/0954898X.2012.730170
   Cessac B, 2008, FRONT COMPUT NEUROSC, V2, DOI 10.3389/neuro.10.002.2008
   Courtine G, 2013, LANCET, V381, P515, DOI 10.1016/S0140-6736(12)62164-3
   de Garis H, 2010, NEUROCOMPUTING, V74, P3, DOI 10.1016/j.neucom.2010.08.004
   GERSTEIN GL, 1964, BIOPHYS J, V4, P41, DOI 10.1016/S0006-3495(64)86768-0
   GRAY CM, 1989, P NATL ACAD SCI USA, V86, P1698, DOI 10.1073/pnas.86.5.1698
   Haykin S., 1998, NEURAL NETWORKS COMP, V2nd
   Iannella N, 2001, NEURAL NETWORKS, V14, P933, DOI 10.1016/S0893-6080(01)00080-6
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Kestur S, 2012, ANN IEEE SYM FIELD P, P141, DOI 10.1109/FCCM.2012.33
   Kilts S., 2007, ADV FPGA DESIGN ARCH
   Kuon I, 2007, FOUND TRENDS ELECTRO, V2, P135, DOI 10.1561/1000000005
   Li WXY, 2013, IEEE T BIOMED CIRC S, V7, P489, DOI 10.1109/TBCAS.2012.2228261
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maass W., 2009, COMPUTABILITY CONTEX
   Maxfield C., 2004, DESIGN WARRIORS GUID
   Nazari S., 2015, NEUROCOMPUTING
   Nazari S., 2014, J COMPUT ELECTRON, P1
   Paugam-Moisy H., 2009, HDB NATURAL COMPUTIN, P40
   Rieke F., 1996, SPIKES EXPLORING NEU
   Shadlen Michael N., 1994, Current Opinion in Neurobiology, V4, P569, DOI 10.1016/0959-4388(94)90059-0
   Shayani H, 2008, LECT NOTES COMPUT SC, V5216, P273
   Thomas DB, 2009, ANN IEEE SYM FIELD P, P45, DOI 10.1109/FCCM.2009.46
   Thorpe S, 1996, NATURE, V381, P520, DOI 10.1038/381520a0
   Thorpe SJ, 2001, SCIENCE, V291, P260, DOI 10.1126/science.1058249
   Touboul J, 2008, BIOL CYBERN, V99, P319, DOI 10.1007/s00422-008-0267-4
   Trappenberg T., 2010, FUNDAMENTALS COMPUTA
   Vato A, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0091677
   Vato A, 2012, PLOS COMPUT BIOL, V8, DOI 10.1371/journal.pcbi.1002578
   Verstraeten D, 2007, NEURAL NETWORKS, V20, P391, DOI 10.1016/j.neunet.2007.04.003
   Wade J., 2010, THESIS U ULSTER
NR 33
TC 7
Z9 7
U1 3
U2 18
PD SEP
PY 2015
VL 14
IS 3
BP 707
EP 716
DI 10.1007/s10825-015-0709-x
UT WOS:000358655300007
DA 2023-11-16
ER

PT C
AU Um, KS
   Hwang, H
   Kim, H
   Heo, SW
AF Um, Kwi Seob
   Hwang, Heejae
   Kim, Hyungtak
   Heo, Seo Weon
GP IEEE
TI Analysis of the Effects of Decay Coefficient and Time Resolution in SNN
   Backpropagation
SO 2020 INTERNATIONAL CONFERENCE ON ELECTRONICS, INFORMATION, AND
   COMMUNICATION (ICEIC)
DT Proceedings Paper
CT 19th International Conference on Electronics, Information, and
   Communication (ICEIC)
CY JAN 19-22, 2020
CL Barcelona, SPAIN
DE spiking neural network; LIF neuron; backpropagation
ID NETWORKS
AB High-performance neural networks operating at low power use spiking neural network (SNN) that are biologically closer than traditional ANN. SNN, unlike ANN, receives a series of binary-coded spike trains as input and updates the membrane potential of the neuron and generates spikes over a period of time specified by the number of spike trains. The function that generates the spike corresponding to the activation function of the ANN is not differentiable, which makes it difficult to apply the backpropagation (BP) algorithm used in the ANN. In order to overcome this problem, studies using numerical approximation of derivatives have been carried out in various ways. However, research on the decay coefficient and the number of spike trains, which are characteristic of SNN neuron, are insufficient. In this paper, we analyze the distribution of spikes and discuss how the decay coefficient characteristics of neurons and the number of spike trains affect network performance.
C1 [Um, Kwi Seob; Hwang, Heejae; Kim, Hyungtak; Heo, Seo Weon] Hongik Univ, Seoul, South Korea.
RP Um, KS (corresponding author), Hongik Univ, Seoul, South Korea.
EM suafion@gmail.com; kokiiiy@naver.com; hkim@hongik.ac.kr;
   seoweon.heo@hongik.ac.kr
CR Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   [Anonymous], 2017, ARXIV170604698
   [Anonymous], 2017, ARXIV PREPRINT ARXIV
   Cheng Y, 2018, IEEE SIGNAL PROC MAG, V35, P126, DOI 10.1109/MSP.2017.2765695
   Deboleena Roy, 2019, FRONT NEUROSCI-SWITZ, V13
   Esser S. K., 2015, ADV NEURAL INFORM PR, P1117
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Mozafari M., 2018, IEEE T NEURAL NETWOR, V29, P1
NR 10
TC 0
Z9 0
U1 1
U2 1
PY 2020
UT WOS:000942592200062
DA 2023-11-16
ER

PT J
AU Jang, H
   Simeone, O
   Gardner, B
   Grüning, A
AF Jang, Hyeryung
   Simeone, Osvaldo
   Gardner, Brian
   Gruening, Andre
TI An Introduction to Probabilistic Spiking Neural Networks: Probabilistic
   Models, Learning Rules, and Applications
SO IEEE SIGNAL PROCESSING MAGAZINE
DT Article
DE Neural networks; Neurons; Biological system modeling; Artificial neural
   networks; Heuristic algorithms
ID NEURONS
AB Spiking neural networks (SNNs) are distributed trainable systems whose computing elements, or neurons, are characterized by internal analog dynamics and by digital and sparse synaptic communications. The sparsity of the synaptic spiking inputs and the corresponding event-driven nature of neural processing can be leveraged by energy-efficient hardware implementations, which can offer significant energy reductions as compared to conventional artificial neural networks (ANNs). The design of training algorithms for SNNs, however, lags behind hardware implementations: most existing training algorithms for SNNs have been designed either for biological plausibility or through conversion from pretrained ANNs via rate encoding.
C1 [Jang, Hyeryung] Kings Coll London, Dept Informat, London, England.
   [Simeone, Osvaldo] Kings Coll London, Dept Informat, Ctr Telecommun Res, Informat Engn, London, England.
   [Gardner, Brian] Univ Surrey, Dept Comp Sci, Guildford, Surrey, England.
   [Gruening, Andre] Univ Appl Sci, Math & Computat Intelligence, Stralsund, Germany.
RP Jang, H (corresponding author), Kings Coll London, Dept Informat, London, England.
EM hyeryung.jang@kcl.ac.uk; osvaldo.simeone@kcl.ac.uk;
   b.gardner@surrey.ac.uk; andre.gruening@hochschule-stralsund.de
CR [Anonymous], 2019, SURROGATE GRADIENT L
   Bellec G., 2018, ADV NEURAL INFORM PR
   BIENENSTOCK EL, 1982, J NEUROSCI, V2, P32, DOI 10.1523/jneurosci.02-01-00032.1982
   Blouw P., 2018, BENCHMARKING KEYWORD
   Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   Brea J, 2013, J NEUROSCI, V33, P9565, DOI 10.1523/JNEUROSCI.4098-12.2013
   Dau H. A., 2018, UCR TIME SERIES CLAS
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Dayan P., 2008, THEORETICAL NEUROSCI
   Eliasmith C., 2004, NEURAL ENG COMPUTATI
   Frémaux N, 2016, FRONT NEURAL CIRCUIT, V9, DOI 10.3389/fncir.2015.00085
   Gardner B, 2015, NEURAL COMPUT, V27, P2548, DOI 10.1162/NECO_a_00790
   Gerhard F, 2017, PLOS COMPUT BIOL, V13, DOI 10.1371/journal.pcbi.1005390
   Gerstner W., 2002, SPIKING NEURON MODEL
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Hinton GE, 2000, ADV NEUR IN, V12, P122
   HULL JJ, 1994, IEEE T PATTERN ANAL, V16, P550, DOI 10.1109/34.291440
   Jang H., 2019, INTRO SPIKING NEURAL
   Jang H, 2019, INT CONF ACOUST SPEE, P3382, DOI 10.1109/ICASSP.2019.8682960
   Kappel D, 2015, PLOS COMPUT BIOL, V11, DOI 10.1371/journal.pcbi.1004485
   Kasabov N, 2010, NEURAL NETWORKS, V23, P16, DOI 10.1016/j.neunet.2009.08.010
   Kingma Diederik P, 2013, ARXIV13126114
   Lee JM, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00191
   Leibfried F, 2015, NEURAL COMPUT, V27, P1686, DOI 10.1162/NECO_a_00758
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maass W, 2014, P IEEE, V102, P860, DOI 10.1109/JPROC.2014.2310593
   Mnih A, 2014, PR MACH LEARN RES, V32, P1791
   Mostafa H, 2018, NEURAL COMPUT, V30, P1542, DOI 10.1162/neco_a_01080
   NEAL RM, 1992, ARTIF INTELL, V56, P71, DOI 10.1016/0004-3702(92)90065-6
   OConnor P, 2016, DEEP SPIKING NETWORK
   Osogami T., 2017, BOLTZMANN MACHINES T
   Paugam-Moisy H., 2012, HDB NATURAL COMPUTIN, V1, P1, DOI [DOI 10.1007/978-3-540-92910-9_10, 10.1007/978-3-540-92910-9_10]
   Pillow JW, 2008, NATURE, V454, P995, DOI 10.1038/nature07140
   Rajendran B., 2019, LOW POWER NEUROMORPH
   Rezende DJ, 2014, FRONT COMPUT NEUROSC, V8, DOI 10.3389/fncom.2014.00038
   Rosenfeld B, 2019, IEEE INT WORK SIGN P, DOI 10.1109/spawc.2019.8815546
   Rueckauer B, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351295
   Simeone O, 2018, FOUND TRENDS SIGNAL, V12, P200, DOI 10.1561/2000000102
   Sutton RS, 2018, ADAPT COMPUT MACH LE, P1
   Watt Alanna J, 2010, Front Synaptic Neurosci, V2, P5, DOI 10.3389/fnsyn.2010.00005
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
NR 41
TC 41
Z9 42
U1 2
U2 19
PD NOV
PY 2019
VL 36
IS 6
BP 64
EP 77
DI 10.1109/MSP.2019.2935234
UT WOS:000497692800002
DA 2023-11-16
ER

PT J
AU Yang, WY
   Yang, J
   Wu, W
AF Yang, Wenyu
   Yang, Jie
   Wu, Wei
TI A Modified Spiking Neuron that Involves Derivative of the State Function
   at Firing Time
SO NEURAL PROCESSING LETTERS
DT Article
DE Spiking neuron; Firing time; Derivative of the state function
ID ERROR-BACKPROPAGATION; NETWORKS; MODEL
AB In usual spiking neural networks, the real world information is interpreted as spike time. A spiking neuron of the spiking neural network receives input vector of spike times, and activates a state function x(t) by increasing the time t until the value of x(t) reaches certain threshold value at a firing time t (a) . And t (a) is the output of the spiking neuron. In this paper we propose, and investigate the performance of, a modified spiking neuron, of which the output is a linear combination of the firing time t (a) and the derivative x'(t (a) ). The merit of the modified spiking neuron is shown by numerical experiments for solving some benchmark problems: The computational time of a modified spiking neuron is a little greater than that of a usual spiking neuron, but the accuracy of a modified spiking neuron is almost as good as a usual spiking neural network with a hidden layer.
C1 [Yang, Wenyu; Yang, Jie; Wu, Wei] Dalian Univ Technol, Sch Math Sci, Dalian, Peoples R China.
RP Wu, W (corresponding author), Dalian Univ Technol, Sch Math Sci, Dalian, Peoples R China.
EM wuweiw@dlut.edu.cn
CR Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   FUNAHASHI K, 1993, NEURAL NETWORKS, V6, P801, DOI 10.1016/S0893-6080(05)80125-X
   GERSTNER W, 1995, PHYS REV E, V51, P738, DOI 10.1103/PhysRevE.51.738
   Ghosh-Dastidar S, 2007, INTEGR COMPUT-AID E, V14, P187
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   Gonzalez-Nalda P, 2008, NEUROCOMPUTING, V71, P721, DOI 10.1016/j.neucom.2007.07.032
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Kistler WM, 1997, NEURAL COMPUT, V9, P1015, DOI 10.1162/neco.1997.9.5.1015
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maass W., 1999, PULSED NEURAL NETWOR
   Natschläger T, 1999, NEUROCOMPUTING, V26-7, P463, DOI 10.1016/S0925-2312(99)00052-1
   STEIN RB, 1965, BIOPHYS J, V5, P173, DOI 10.1016/S0006-3495(65)86709-1
   Wu QX, 2006, NEUROCOMPUTING, V69, P1912, DOI 10.1016/j.neucom.2005.11.023
   Wysoski SG, 2008, NEUROCOMPUTING, V71, P2563, DOI 10.1016/j.neucom.2007.12.038
   Wysoski SG, 2010, NEURAL NETWORKS, V23, P819, DOI 10.1016/j.neunet.2010.04.009
   Yang J, 2012, APPL MATH LETT, V25, P1118, DOI 10.1016/j.aml.2012.02.016
NR 18
TC 2
Z9 3
U1 1
U2 9
PD OCT
PY 2012
VL 36
IS 2
BP 135
EP 144
DI 10.1007/s11063-012-9226-0
UT WOS:000308442300003
DA 2023-11-16
ER

PT C
AU Opielka, P
   Starczewski, JT
   Wróbel, M
   Nieszporek, K
   Marchlewska, A
AF Opielka, Piotr
   Starczewski, Janusz T.
   Wrobel, Michal
   Nieszporek, Katarzyna
   Marchlewska, Alina
BE Rutkowski, L
   Scherer, R
   Korytkowski, M
   Pedrycz, W
   Tadeusiewicz, R
   Zurada, JM
TI Application of Spiking Neural Networks to Fashion Classification
SO ARTIFICIAL INTELLIGENCEAND SOFT COMPUTING, PT I
SE Lecture Notes in Artificial Intelligence
DT Proceedings Paper
CT 18th International Conference on Artificial Intelligence and Soft
   Computing (ICAISC)
CY JUN 16-20, 2019-2109
CL Zakopane, POLAND
AB In this paper, a model of spiking neural networks is studied. Such networks are commonly called as the third generation of artificial neural networks. The main difference between them and previous generation networks is that they are based on spiking neurons. This approach leads us to the need of using specific ways of coding inputs and outputs as well as original methods of learning. The paper considers evaluation of such a network with a Fashion-MNIST dataset that contains labeled images. The results of this experiment and its conclusion are also described in the paper.
C1 [Opielka, Piotr; Starczewski, Janusz T.; Wrobel, Michal; Nieszporek, Katarzyna] Czestochowa Tech Univ, Inst Computat Intelligence, Czestochowa, Poland.
   [Marchlewska, Alina] Univ Social Sci, Informat Technol Inst, Lodz, Poland.
   [Marchlewska, Alina] Clark Univ, Worcester, MA 01610 USA.
RP Opielka, P (corresponding author), Czestochowa Tech Univ, Inst Computat Intelligence, Czestochowa, Poland.
EM piotr.opielka@iisi.pcz.pl
CR [Anonymous], 2017, CORR ABS170807747
   Bartczuk L, 2016, INT J AP MAT COM-POL, V26, P603, DOI 10.1515/amcs-2016-0042
   Bawane P, 2018, MATER TODAY-PROC, V5, P360, DOI 10.1016/j.matpr.2017.11.093
   Bilski J, 2015, IEEE T PARALL DISTR, V26, P2561, DOI 10.1109/TPDS.2014.2357019
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Gabryel M, 2018, COMM COM INF SC, V920, P437, DOI 10.1007/978-3-319-99972-2_36
   Gavrilov AV, 2016, INT CONF ACT PROB EL, P455, DOI 10.1109/APEIE.2016.7806372
   Hazan H, 2018, FRONT NEUROINFORM, V12, DOI 10.3389/fninf.2018.00089
   Huh D., 2017, GRADIENT DESCENT SPI
   Isokawa T, 2018, J ARTIF INTELL SOFT, V8, P237, DOI 10.1515/jaiscr-2018-0015
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Kasinski A., 2006, International Journal of Applied Mathematics and Computer Science, V16, P101
   Ke Y, 2017, J ARTIF INTELL SOFT, V7, P229, DOI 10.1515/jaiscr-2017-0016
   Liu JB, 2018, J ARTIF INTELL SOFT, V8, P257, DOI 10.1515/jaiscr-2018-0016
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Ponulak F, 2011, ACTA NEUROBIOL EXP, V71, P409
   Pregowska A, 2016, NEUROCOMPUTING, V216, P756, DOI 10.1016/j.neucom.2016.08.034
   THORPE SJ, 1990, PARALLEL PROCESSING IN NEURAL SYSTEMS AND COMPUTERS, P91
   Wei RY, 2018, J ARTIF INTELL SOFT, V8, P269, DOI 10.1515/jaiscr-2018-0017
NR 19
TC 0
Z9 0
U1 0
U2 6
PY 2019
VL 11508
BP 172
EP 180
DI 10.1007/978-3-030-20912-4_17
UT WOS:000485150200017
DA 2023-11-16
ER

PT J
AU Leigh, AJ
   Heidarpur, M
   Mirhassani, M
AF Leigh, Alexander J. J.
   Heidarpur, Moslem
   Mirhassani, Mitra
TI Digital Hardware Implementations of Spiking Neural Networks With
   Selective Input Sparsity for Edge Inferences in Controlled Image
   Acquisition Environments
SO IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS II-EXPRESS BRIEFS
DT Article; Proceedings Paper
CT IEEE International Symposium on Circuits and Systems (ISCAS)
CY MAY 21-25, 2023
CL Monterey, CA
DE Hardware; Neurons; Random access memory; Training; Indexes; Membrane
   potentials; Biological neural networks; Spiking neural networks; sparse
   neural networks; pattern recognition; digital hardware
AB Digital hardware implementations of Spiking Neural Networks designed using Selective Input Sparsity (SIS) are proposed for edge inference applications in image classification where the image acquisition environment is controlled. These sparsely connected networks are well-suited to area-constrained applications as they require fewer neurons and synapses than baseline Fully Connected (FC) networks of analogous structures. The SIS networks were validated on FPGA, and baseline FC networks were also implemented on FGPA for comparison. The SIS networks require fewer hardware resources and make inferences faster than the baseline FC networks without substantial impact on the classification accuracy.
C1 [Leigh, Alexander J. J.; Heidarpur, Moslem; Mirhassani, Mitra] Univ Windsor, SHIELD Automot Cybersecur Ctr Excellence, Windsor, ON N9B 3P4, Canada.
RP Leigh, AJ (corresponding author), Univ Windsor, SHIELD Automot Cybersecur Ctr Excellence, Windsor, ON N9B 3P4, Canada.
EM leigh11@uwindsor.ca
CR An SHY, 2020, Arxiv, DOI arXiv:2008.10400
   Belabed T, 2021, IEEE ACCESS, V9, P89162, DOI 10.1109/ACCESS.2021.3090196
   Chen GK, 2019, IEEE J SOLID-ST CIRC, V54, P992, DOI 10.1109/JSSC.2018.2884901
   Chen Y., 2022, INT C MACH LEARN ICM, V162, P3701
   Clanuwat T, 2018, Arxiv, DOI arXiv:1812.01718
   Davidson S, 2021, FRONT NEUROSCI-SWITZ, V15, DOI 10.3389/fnins.2021.651141
   Kabir HD, 2022, Arxiv, DOI arXiv:2007.03347
   Nguyen DA, 2021, J LOW POWER ELECT AP, V11, DOI 10.3390/jlpea11020023
   Frenkel C, 2019, IEEE T BIOMED CIRC S, V13, P145, DOI 10.1109/TBCAS.2018.2880425
   Kuon I, 2007, IEEE T COMPUT AID D, V26, P203, DOI 10.1109/TCAD.2006.884574
   LeCun Y., 1998, MNIST DATABASE HANDW
   Leigh AJ, 2022, IEEE INT SYMP CIRC S, P799, DOI 10.1109/ISCAS48785.2022.9937618
   Liang MX, 2021, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS51556.2021.9401607
   LIU D, 1994, IEEE T CIRCUITS-II, V41, P295, DOI 10.1109/82.285706
   LIU DR, 1993, 1993 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS : PROCEEDINGS, VOLS 1-4 ( ISCAS 93 ), P2596, DOI 10.1109/ISCAS.1993.394297
   Lu S, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00535
   Luo W., 2020, IEEE ACCESS, V8
   Martinelli F, 2020, INT CONF ACOUST SPEE, P8544, DOI [10.1109/icassp40776.2020.9053412, 10.1109/ICASSP40776.2020.9053412]
   Neil D, 2014, IEEE T VLSI SYST, V22, P2621, DOI 10.1109/TVLSI.2013.2294916
   Reiners M, 2022, COMPUT OPER RES, V141, DOI 10.1016/j.cor.2021.105676
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Silva R. L., 2018, PRODUCT LIFECYCLE MA, V540
   Tanveer MS, 2020, Arxiv, DOI arXiv:2006.09042
   Wang Q, 2017, NEUROCOMPUTING, V221, P146, DOI 10.1016/j.neucom.2016.09.071
   Xiao H, 2017, Arxiv, DOI arXiv:1708.07747
   Zheng HL, 2021, IEEE T CIRCUITS-II, V68, P3172, DOI 10.1109/TCSII.2021.3090422
NR 26
TC 0
Z9 0
U1 4
U2 4
PD MAY
PY 2023
VL 70
IS 5
BP 1724
EP 1728
DI 10.1109/TCSII.2023.3260704
UT WOS:000988497300012
DA 2023-11-16
ER

PT J
AU Losh, M
   Llamocca, D
AF Losh, Michael
   Llamocca, Daniel
TI A Low-Power Spike-Like Neural Network Design
SO ELECTRONICS
DT Article
DE spiking neural networks; bit-serial architectures; FPGA
ID IMPLEMENTATION
AB Modern massively-parallel Graphics Processing Units (GPUs) and Machine Learning (ML) frameworks enable neural network implementations of unprecedented performance and sophistication. However, state-of-the-art GPU hardware platforms are extremely power-hungry, while microprocessors cannot achieve the performance requirements. Biologically-inspired Spiking Neural Networks (SNN) have inherent characteristics that lead to lower power consumption. We thus present a bit-serial SNN-like hardware architecture. By using counters, comparators, and an indexing scheme, the design effectively implements the sum-of-products inherent in neurons. In addition, we experimented with various strength-reduction methods to lower neural network resource usage. The proposed Spiking Hybrid Network (SHiNe), validated on an FPGA, has been found to achieve reasonable performance with a low resource utilization, with some trade-off with respect to hardware throughput and signal representation.
C1 [Losh, Michael; Llamocca, Daniel] Oakland Univ, Elect & Comp Engn Dept, Rochester, MI 48309 USA.
RP Llamocca, D (corresponding author), Oakland Univ, Elect & Comp Engn Dept, Rochester, MI 48309 USA.
EM mlosh@oakland.edu; llamocca@oakland.edu
CR Arbib MA, 2002, HDB BRAIN THEORY NEU
   BELYAEV M, 2019, ELECTRONICS-SWITZ, V8, DOI DOI 10.3390/electronics8101065
   Chakradhar S, 2010, CONF PROC INT SYMP C, P247, DOI 10.1145/1816038.1815993
   Chen YH, 2017, IEEE J SOLID-ST CIRC, V52, P127, DOI 10.1109/JSSC.2016.2616357
   Farabet C., 2010, P 2010 IEEE INT S CI
   Furui S, 2012, IEEE SIGNAL PROC MAG, V29, P16, DOI 10.1109/MSP.2012.2209906
   Glorot X., 2011, P 14 INT C ARTIFICIA, P315
   Gokhale V., 2014, P 2014 IEEE C COMP V
   Gomperts A, 2011, IEEE T IND INFORM, V7, P78, DOI 10.1109/TII.2010.2085006
   Hardieck M, 2019, PROCEEDINGS OF THE 2019 ACM/SIGDA INTERNATIONAL SYMPOSIUM ON FIELD-PROGRAMMABLE GATE ARRAYS (FPGA'19), P43, DOI 10.1145/3289602.3293905
   Hauswald J., 2015, P 2015 ACM IEEE 42 A
   Himavathi S, 2007, IEEE T NEURAL NETWOR, V18, P880, DOI 10.1109/TNN.2007.891626
   Iakymchuk T, 2012, 2012 7TH INTERNATIONAL WORKSHOP ON RECONFIGURABLE AND COMMUNICATION-CENTRIC SYSTEMS-ON-CHIP (RECOSOC)
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Llamocca D, 2017, J PARALLEL DISTR COM, V109, P178, DOI 10.1016/j.jpdc.2017.05.017
   Markidis S., 2018, P 2018 IEEE INT PAR
   Minsky M., 2017, PERCEPTRONS INTRO CO
   Misra J, 2010, NEUROCOMPUTING, V74, P239, DOI 10.1016/j.neucom.2010.03.021
   Nielsen MA., 2015, NEURAL NETWORKS DEEP
   Nurvitadhi E, 2017, FPGA'17: PROCEEDINGS OF THE 2017 ACM/SIGDA INTERNATIONAL SYMPOSIUM ON FIELD-PROGRAMMABLE GATE ARRAYS, P5, DOI 10.1145/3020078.3021740
   Pearson MJ, 2007, IEEE T NEURAL NETWOR, V18, P1472, DOI 10.1109/TNN.2007.891203
   Reagen B., 2016, P 2016 ACM IEEE 43 A
   Renteria-Cedano J, 2019, ELECTRONICS-SWITZ, V8, DOI 10.3390/electronics8070761
   Rice K., 2009, P 2019 INT C REC COM
   Song S., 2013, P 2013 IEEE 27 INT S
   Strigl D., 2010, P 2018 18 EUR C PAR
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Umuroglu Y, 2017, FPGA'17: PROCEEDINGS OF THE 2017 ACM/SIGDA INTERNATIONAL SYMPOSIUM ON FIELD-PROGRAMMABLE GATE ARRAYS, P65, DOI 10.1145/3020078.3021744
   Zhang Chen, 2015, P 2015 ACMSIGDA INT, P161, DOI 10.1145/2684746.2689060
NR 29
TC 5
Z9 5
U1 0
U2 3
PD DEC
PY 2019
VL 8
IS 12
AR 1479
DI 10.3390/electronics8121479
UT WOS:000506678200105
DA 2023-11-16
ER

PT C
AU Bagheri, A
   Simeone, O
   Rajendran, B
AF Bagheri, Alireza
   Simeone, Osvaldo
   Rajendran, Bipin
GP IEEE
TI Adversarial Training for Probabilistic Spiking Neural Networks
SO 2018 IEEE 19TH INTERNATIONAL WORKSHOP ON SIGNAL PROCESSING ADVANCES IN
   WIRELESS COMMUNICATIONS (SPAWC)
SE IEEE International Workshop on Signal Processing Advances in Wireless
   Communications
DT Proceedings Paper
CT IEEE 19th International Workshop on Signal Processing Advances in
   Wireless Communications (SPAWC)
CY JUN 25-28, 2018
CL Kalamata, GREECE
DE Spiking Neural Networks (SNNs); adversarial examples; adversarial
   training; Generalized Linear Model (GLM)
AB Classifiers trained using conventional empirical risk minimization or maximum likelihood methods are known to suffer dramatic performance degradations when tested over examples adversarially selected based on knowledge of the classifier's decision rule. Due to the prominence of Artificial Neural Networks (ANNs) as classifiers, their sensitivity to adversarial examples, as well as robust training schemes, have been recently the subject of intense investigation. In this paper, for the first time, the sensitivity of spiking neural networks (SNNs), or third-generation neural networks, to adversarial examples is studied. The study considers rate and time encoding, as well as rate and first-to-spike decoding. Furthermore, a robust training mechanism is proposed that is demonstrated to enhance the performance of SNNs under white-box attacks.
C1 [Bagheri, Alireza; Simeone, Osvaldo; Rajendran, Bipin] New Jersey Inst Technol, ECE Dept, Newark, NJ 07102 USA.
   [Simeone, Osvaldo] Kings Coll London, Dept Informat, London WC2R 2LS, England.
RP Bagheri, A (corresponding author), New Jersey Inst Technol, ECE Dept, Newark, NJ 07102 USA.
EM ab745@njit.edu; osvaldo.simeone@kcl.ac.uk; bipin@njit.edu
CR [Anonymous], 2016, ARXIV161101421
   [Anonymous], 2015, INT C LEARNING REPRE
   [Anonymous], 2018, ARXIV180202627
   [Anonymous], INTEL INVESTIGATES C
   [Anonymous], 2018, INT C LEARNING REPRE
   Bagheri Alireza, 2017, ARXIV171010704
   Fawzi A, 2017, IEEE SIGNAL PROC MAG, V34, P50, DOI 10.1109/MSP.2017.2740965
   Gardner B, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0161335
   Guo S., 2017, IEEE T MOBILE COMPUT, P1
   Kurakin A., 2016, INT C LEARN REPR
   Masquelier T, 2007, PLOS COMPUT BIOL, V3, P247, DOI 10.1371/journal.pcbi.0030031
   Paugam-Moisy H., 2012, HDB NATURAL COMPUTIN, V1, P1, DOI [DOI 10.1007/978-3-540-92910-9_10, 10.1007/978-3-540-92910-9_10]
   Pillow JW, 2008, NATURE, V454, P995, DOI 10.1038/nature07140
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Ranjan R, 2018, IEEE SIGNAL PROC MAG, V35, P66, DOI 10.1109/MSP.2017.2764116
   Rezende Danilo J, 2011, ADV NEURAL INFORM PR, P136
   Smith JE, 2017, IEEE MICRO, V37, P8
   Stromatias E, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00350
NR 18
TC 12
Z9 12
U1 0
U2 2
PY 2018
BP 261
EP 265
UT WOS:000451080200053
DA 2023-11-16
ER

PT J
AU Kasabov, N
AF Kasabov, Nikola
TI FROM MULTILAYER PERCEPTRONS AND NEUROFUZZY SYSTEMS TO DEEP LEARNING
   MACHINES: WHICH METHOD TO USE? - A SURVEY
SO INTERNATIONAL JOURNAL ON INFORMATION TECHNOLOGIES AND SECURITY
DT Article
DE artificial neural networks; Evolving Connectionist Systems (ECOS);
   neuro-fuzzy systems; Spiking Neural Networks (SNN); evolving spiking
   neural networks; NeuCube; Quantum inspired neural networks;
   spatio-temporal pattern recognition; data mining
ID SPIKING NEURAL-NETWORKS; FUZZY INFERENCE SYSTEM; PATTERN-RECOGNITION;
   MODEL; ALGORITHM; CLASSIFICATION; OPTIMIZATION; ARCHITECTURE;
   STRATEGIES; PREDICTION
AB Artificial neural networks have now a long history as major techniques in computational intelligence with a wide range of application for learning from data. There are many methods developed and applied so far, from multiplayer perceptrons (MLP) to the recent ones being deep neural networks and deep learning machines based on spiking neural networks. The paper addresses a main question for researchers and practitioners: Having data and a problem in hand, which method would be most suitable to create a model from the data and to efficiently solve the problem? In order to answer this question, the paper reviews the main features of the most popular neural network methods and then lists examples of applications already published and referenced. The methods include: simple MLP; hybrid systems; neuro-fuzzy systems; deep neural networks; spiking neural networks; quantum inspired evolutionary computation methods for network parameter optimization; deep learning neural networks and brain-like deep learning machines. The paper covers both methods and their numerous applications for data modelling, predictive systems, data mining, pattern recognition, across application areas of: engineering, health, robotics, security, finances, etc. It concludes with recommendations on which method would be more suitable to use, depending on the data and the problems in hand, in order to create efficient information technologies across application domains.
C1 [Kasabov, Nikola] Auckland Univ Technol, KEDRI, Auckland, New Zealand.
RP Kasabov, N (corresponding author), Auckland Univ Technol, KEDRI, Auckland, New Zealand.
EM nkasabov@aut.ac.nz
CR AMARI S, 1990, P IEEE, V78, P1443, DOI 10.1109/5.58324
   AMARI S, 1967, IEEE TRANS ELECTRON, VEC16, P299, DOI 10.1109/PGEC.1967.264666
   Ang KK, 2005, NEURAL COMPUT, V17, P205, DOI 10.1162/0899766052530857
   Angelov P, 2008, FUZZY SET SYST, V159, P3160, DOI 10.1016/j.fss.2008.06.019
   Angelov P, 2007, 2007 IEEE SYMPOSIUM ON COMPUTATIONAL INTELLIGENCE IN IMAGE AND SIGNAL PROCESSING, P220, DOI 10.1109/CIISP.2007.369172
   [Anonymous], 1987, ANAL FUZZY INFORM
   [Anonymous], P 5 IFSA WORLD C SEO
   [Anonymous], 2011, EU RP7 M CUR EVOSPIK
   [Anonymous], 1989, ANALOG VLSI NEURAL S
   [Anonymous], 2007, EVOLVING CONNECTIONI
   [Anonymous], IIZUKA 92 2 INT C FU
   Aznarte JL, 2007, FUZZY SET SYST, V158, P2734, DOI 10.1016/j.fss.2007.03.021
   Babu GS, 2013, APPL SOFT COMPUT, V13, P654, DOI 10.1016/j.asoc.2012.08.047
   Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Binas J, 2014, FRONT COMPUT NEUROSC, V8, DOI 10.3389/fncom.2014.00068
   Bordignon F, 2014, NEUROCOMPUTING, V127, P13, DOI 10.1016/j.neucom.2013.04.047
   Boubacar HA, 2008, NEURAL NETWORKS, V21, P1287, DOI 10.1016/j.neunet.2008.03.016
   Cetisli B, 2010, EXPERT SYST APPL, V37, P6093, DOI 10.1016/j.eswa.2010.02.108
   Chan Z. S. H., 2004, International Journal of Computational Intelligence and Applications, V4, P309, DOI 10.1142/S1469026804001331
   Chen Y., 2013, P 20 INT C NEUR INF, V8228, P70
   Corradi F, 2015, IEEE T BIOMED CIRC S, V9, P699, DOI 10.1109/TBCAS.2015.2479256
   Rubio JD, 2010, EVOL SYST-GER, V1, P173, DOI 10.1007/s12530-010-9015-9
   Rubio JD, 2009, IEEE T FUZZY SYST, V17, P1296, DOI 10.1109/TFUZZ.2009.2029569
   de Moraes R. M., 2012, P 2 BRAZ C FUZZ SYST, P672
   Deng D, 2003, NEUROCOMPUTING, V51, P87, DOI 10.1016/S0925-2312(02)00599-4
   Dovzan D, 2011, EVOL SYST-GER, V2, P15, DOI 10.1007/s12530-010-9025-7
   Filev D. P., 2007, ADV FUZZY CLUSTERING
   Furber SB, 2013, IEEE T COMPUT, V62, P2454, DOI 10.1109/TC.2012.142
   Futschik ME, 2002, PROCEEDINGS OF THE 2002 IEEE INTERNATIONAL CONFERENCE ON FUZZY SYSTEMS, VOL 1 & 2, P414, DOI 10.1109/FUZZ.2002.1005026
   Ghobakhlou A, 2003, INFORM SCIENCES, V156, P71, DOI 10.1016/S0020-0255(03)00165-8
   Goh HL, 2009, IEEE T NEURAL NETWOR, V20, P1302, DOI 10.1109/TNN.2009.2023213
   Hamed H, 2010, AUST J INTEL INF PRO, V11, P23
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   Huang GB, 2005, PROCEEDINGS OF THE IASTED INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE, P232
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Kadlec P., 2009, MEMET COMPUT, V1, P241, DOI DOI 10.1007/S12293-009-0017-8
   Kalhor A, 2013, APPL SOFT COMPUT, V13, P939, DOI 10.1016/j.asoc.2012.09.015
   Kasabov N, 2006, APPL SOFT COMPUT, V6, P307, DOI 10.1016/j.asoc.2005.01.006
   Kasabov N., 1998, Proceedings of the 5th International Conference on Soft Computing and Information/Intelligent Systems. Methodologies for the Conception, Design and Application of Soft Computing, P271
   Kasabov N, 2001, IEEE T SYST MAN CY B, V31, P902, DOI 10.1109/3477.969494
   Kasabov N., 2014, SPRINGER HDB BIO NEU
   KASABOV N, 2002, EVOLVING CONNECTIONI
   Kasabov N. K., 1993, Connection Science, V5, P275, DOI 10.1080/09540099308915702
   Kasabov N.K., 1996, FDN NEURAL NETWORKS, DOI DOI 10.1016/S0898-1221(97)84600-7
   Kasabov N, 2007, PATTERN RECOGN LETT, V28, P673, DOI 10.1016/j.patrec.2006.08.007
   Kasabov N, 2016, NEURAL NETWORKS, V78, P1, DOI 10.1016/j.neunet.2015.09.011
   Kasabov N, 2014, NEUROCOMPUTING, V134, P269, DOI 10.1016/j.neucom.2013.09.049
   Kasabov N, 2013, NEURAL NETWORKS, V41, P188, DOI 10.1016/j.neunet.2012.11.014
   Kasabov Nikola, 2010, International Journal of Functional Informatics and Personalised Medicine, V3, P236
   Kasabov NK, 2014, NEURAL NETWORKS, V52, P62, DOI 10.1016/j.neunet.2014.01.006
   Kasabov NK, 2002, IEEE T FUZZY SYST, V10, P144, DOI 10.1109/91.995117
   Kasabov NK, 1997, INFORM SCIENCES, V101, P155, DOI 10.1016/S0020-0255(97)00007-8
   Kim K, 2005, LECT NOTES ARTIF INT, V3613, P179
   Leite D, 2013, NEURAL NETWORKS, V38, P1, DOI 10.1016/j.neunet.2012.10.006
   Leite DF, 2009, 2009 IEEE WORKSHOP ON EVOLVING AND SELF-DEVELOPING INTELLIGENT SYSTEMS, P1, DOI 10.1109/ESDIS.2009.4938992
   Lim JS, 2009, IEEE T NEURAL NETWOR, V20, P522, DOI 10.1109/TNN.2008.2012031
   Liu F, 2007, NEURAL COMPUT, V19, P1656, DOI 10.1162/neco.2007.19.6.1656
   Lughofer ED, 2008, IEEE T FUZZY SYST, V16, P1393, DOI 10.1109/TFUZZ.2008.925908
   MAHOWALD MA, 1991, SCI AM, V264, P76, DOI 10.1038/scientificamerican0591-76
   Hernández JAM, 2009, IEEE T FUZZY SYST, V17, P1379, DOI 10.1109/TFUZZ.2009.2032364
   Minku F. L., 2006, P IEEE C EV COMP CAN, P1399
   Minku F.L., 2006, 2006 9 BRAZILIAN S N, P48
   Minku FL, 2008, NEURAL NETWORKS, V21, P1363, DOI 10.1016/j.neunet.2008.02.001
   Minku FL, 2005, IEEE C EVOL COMPUTAT, P1951
   Mohemmed A, 2013, NEUROCOMPUTING, V107, P3, DOI 10.1016/j.neucom.2012.08.034
   O'Hora B, 2006, IEEE IJCNN, P2932
   Ozawa S, 2005, NEURAL NETWORKS, V18, P575, DOI 10.1016/j.neunet.2005.06.016
   Platel MD, 2009, IEEE T EVOLUT COMPUT, V13, P1218, DOI 10.1109/TEVC.2008.2003010
   ROSENBLATT F, 1958, PSYCHOL REV, V65, P386, DOI 10.1037/h0042519
   Rubio J. de Jesus, 2010, EVOLVING INTELLIGENT
   Rumelhart DE, 1986, PARALLEL DISTRIBUTED, V1, DOI DOI 10.7551/MITPRESS/5236.001.0001
   Sawada J, 2016, SC '16: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE FOR HIGH PERFORMANCE COMPUTING, NETWORKING, STORAGE AND ANALYSIS, P130, DOI 10.1109/SC.2016.11
   Schliebs S, 2009, NEURAL NETWORKS, V22, P623, DOI 10.1016/j.neunet.2009.06.038
   Sengupta N, 2017, INFORM SCIENCES, V406, P133, DOI 10.1016/j.ins.2017.04.017
   Soltic S, 2010, INT J NEURAL SYST, V20, P437, DOI 10.1142/S012906571000253X
   Song HJ, 2010, IEEE T FUZZY SYST, V18, P233, DOI 10.1109/TFUZZ.2009.2038371
   Song Q, 2005, IEEE T FUZZY SYST, V13, P799, DOI 10.1109/TFUZZ.2005.859311
   Song Q, 2006, NEURAL NETWORKS, V19, P1591, DOI 10.1016/j.neunet.2006.05.028
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Subramanian Kartick, 2013, IEEE Transactions on Fuzzy Systems, V21, P1080, DOI 10.1109/TFUZZ.2013.2242894
   Subramanian K, 2012, APPL SOFT COMPUT, V12, P3603, DOI 10.1016/j.asoc.2012.06.012
   Suresh S, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P2507, DOI 10.1109/IJCNN.2011.6033545
   Tan J, 2010, IEEE T NEURAL NETWOR, V21, P985, DOI 10.1109/TNN.2010.2046747
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   Tonelli P., 2011, Proceedings 2011 IEEE Workshop on Evolving and Adaptive Intelligent Systems (EAIS 2011), P9, DOI 10.1109/EAIS.2011.5945909
   Tung SW, 2013, INFORM SCIENCES, V220, P124, DOI 10.1016/j.ins.2012.02.031
   Tung SW, 2011, IEEE T NEURAL NETWOR, V22, P1928, DOI 10.1109/TNN.2011.2167720
   Vajpai J, 2006, LECT NOTES COMPUT SC, V4232, P505
   Verstraeten D, 2007, NEURAL NETWORKS, V20, P391, DOI 10.1016/j.neunet.2007.04.003
   Watts MJ, 2009, IEEE T SYST MAN CY C, V39, P253, DOI 10.1109/TSMCC.2008.2012254
   Werbos P., 1990, P IEEE, V87, P10
   Widiputra H, 2011, CYBERNET SYST, V42, P100, DOI 10.1080/01969722.2011.541210
   Wysoski SG, 2010, NEURAL NETWORKS, V23, P819, DOI 10.1016/j.neunet.2010.04.009
   Yagar RR., 1994, J INTELL FUZZY SYST, V2, P209, DOI [10.3233/IFS-1994-2301, DOI 10.3233/IFS-1994-2301]
   Yamauchi K, 2007, IEICE T INF SYST, VE90D, P722, DOI 10.1093/ietisy/e90-d.4.722
   ZADEH LA, 1988, COMPUTER, V21, P83, DOI 10.1109/2.53
   ZADEH LA, 1965, INFORM CONTROL, V8, P338, DOI 10.1016/S0019-9958(65)90241-X
   Zanchettin C, 2010, INT J COMPUT INTELL, V9, P137, DOI 10.1142/S1469026810002823
   Zhao QL, 2010, LECT NOTES ARTIF INT, V6441, P1, DOI 10.1007/978-3-642-17313-4_1
NR 100
TC 11
Z9 11
U1 1
U2 15
PY 2017
VL 9
IS 2
BP 3
EP 24
UT WOS:000418154300001
DA 2023-11-16
ER

PT J
AU Guo, L
   Lv, H
   Huang, FR
   Shi, HY
AF Guo, Lei
   Lv, Huan
   Huang, Fengrong
   Shi, Hongyi
TI Research on Neural Information Coding of Spiking Neural Network Based on
   Synaptic Plasticity Under AC Electric Field Stimulation
SO INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE
DT Article
DE Spiking neural network; synaptic plasticity; AC electric field;
   inter-spike interval coding; average rate coding
ID MODEL
AB Neural information coding is helpful in understanding the working mechanism of the nervous system. Currently, most of the studies are based on the neural network which is based on excitatory synaptic plasticity. However, the inhibitory synaptic plasticity also plays an important role in the regulation of neural network. For presenting better biological authenticity, a spiking neural network was constructed based on the synaptic plasticity regulation mechanism in this study. The synaptic plasticity regulation mechanism contains excitatory and inhibitory synapses. The characteristics of neural information coding under AC electric field stimulation were studied from the perspective of time coding (inter-spike interval coding) and rate coding (average rate coding). The experimental results indicate that inter-spike intervals decrease and the firing rate of neurons increases under AC electric field stimulation. With the increase of the stimulation intensity, inter-spike intervals are decreased and the firing rate of neurons is increased. The neurons whose average firing rate increases can be raised as a neuron cluster to express the information. The results of this paper help us to understand the mechanism of information processing of the brain, and bring new ideas to the engineering applications such as neural computation and artificial intelligence.
C1 [Guo, Lei; Lv, Huan; Shi, Hongyi] Hebei Univ Technol, Sch Elect Engn, State Key Lab Reliabil & Intelligence Elect Equip, Tianjin 300130, Peoples R China.
   [Guo, Lei; Lv, Huan; Shi, Hongyi] Hebei Univ Technol, Sch Elect Engn, Key Lab Electromagnet Field & Elect Apparat Relia, Tianjin 300130, Peoples R China.
   [Huang, Fengrong] Hebei Univ Technol, Sch Mech Engn, Tianjin 300130, Peoples R China.
RP Guo, L (corresponding author), Hebei Univ Technol, Sch Elect Engn, State Key Lab Reliabil & Intelligence Elect Equip, Tianjin 300130, Peoples R China.; Guo, L (corresponding author), Hebei Univ Technol, Sch Elect Engn, Key Lab Electromagnet Field & Elect Apparat Relia, Tianjin 300130, Peoples R China.
EM guoshengrui@163.com; 1318053519@qq.com; 2015016@hebut.edu.cn;
   981217582@qq.com
CR [陈书仪 Chen Shuyi], 2017, [生命的化学, Chemistry of Life], V37, P607
   Chen Yunzhi, 2015, Sheng Wu Yi Xue Gong Cheng Xue Za Zhi, V32, P25
   Erkaymaz O, 2014, TURK J ELECTR ENG CO, V22, P708, DOI 10.3906/elk-1202-89
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Kotnik T, 2000, BIOELECTROMAGNETICS, V21, P385, DOI 10.1002/1521-186X(200007)21:5<385::AID-BEM7>3.0.CO;2-F
   Liu YD, 2014, ACTA PHYS SIN-CH ED, V63, DOI 10.7498/aps.63.080503
   Owusu E., 2014, THESIS
   Pinar C, 2017, NEUROSCI BIOBEHAV R, V80, P394, DOI 10.1016/j.neubiorev.2017.06.001
   Qin YM, 2018, COGN NEURODYNAMICS, V12, P509, DOI 10.1007/s11571-018-9492-2
   Vargas-Caballero M, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00201
   Wang ML, 2015, ACTA PHYS SIN-CH ED, V64, DOI 10.7498/aps.64.108701
   Wei XL, 2016, INT J MOD PHYS B, V30, DOI 10.1142/S0217979216501423
   Wei Y, 2014, J NEUROSCI, V34, P15804, DOI 10.1523/JNEUROSCI.3929-12.2014
   Yu Kai, 2014, Application Research of Computers, V31, P70, DOI 10.3969/j.issn.1001-3695.2014.01.016
   Yu Q, 2014, NEUROCOMPUTING, V138, P3, DOI 10.1016/j.neucom.2013.06.052
   Zhu YT, 2016, FRONT HUM NEUROSCI, V10, DOI 10.3389/fnhum.7016.00019
   [朱雅婷 Zhu Yating], 2015, [振动与冲击, Journal of Vibration and Shock], V34, P1
   Zhu YT, 2017, THESIS
NR 18
TC 2
Z9 3
U1 1
U2 22
PD JUN 30
PY 2019
VL 33
IS 7
AR 1959021
DI 10.1142/S0218001419590213
UT WOS:000470871900014
DA 2023-11-16
ER

PT C
AU Rocke, P
   McGinley, B
   Maher, J
   Morgan, F
   Harkin, J
AF Rocke, Patrick
   McGinley, Brian
   Maher, John
   Morgan, Fearghal
   Harkin, Jim
BE Hornby, GS
   Sekanina, L
   Haddow, PC
TI Investigating the Suitability of FPAAs for Evolved Hardware Spiking
   Neural Networks
SO EVOLVABLE SYSTEMS: FROM BIOLOGY TO HARDWARE, PROCEEDINGS
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 8th International Conference on Evolvable Systems
CY SEP 21-24, 2008
CL Prague, CZECH REPUBLIC
DE FPAA Hardware Evolution; Spiking Neural Networks; Analogue Neural
   Networks
AB This paper investigates the use of a network of cascaded Field Programmable Analogue Arrays (FPAAs) to implement an evolved, analogue, Spiking Neural Network (SNN) pole balance controller. The SNN hardware platform interfaces to a simulated pole balancing model for evaluation. Performance of the evolved analogue hardware controller is compared to that of a software-based SNN controller. The evolved hardware network displays an improved tolerance to changing environments compared with networks evolved solely in simulation. The paper goes on to discuss the suitability of low density FPAA devices for analogue-centric hardware neural network platforms. It concludes by outlining some possible directions which address the observed limitations of using FPAAs for ANNs.
C1 [Rocke, Patrick; McGinley, Brian; Maher, John; Morgan, Fearghal] NUI Galway, BIRC Res Grp, Dept Elect Engn, Galway, Ireland.
   [Harkin, Jim] Univ Ulster, Intelligent Syst Res Ctr, Fac Engn, Derry, North Ireland.
RP Rocke, P (corresponding author), NUI Galway, BIRC Res Grp, Dept Elect Engn, Galway, Ireland.
EM patrick.rocke@nuigalway.ie; brian.mcginley@nuigalway.ie;
   john.maher@nuigalway.ie; fearghal.morgan@nuigalway.ie;
   jg.harkin@ulster.ac.uk
CR AMARAL J, 2004, P 2004 NASA DOD C EV
   BELLIS S, 2004, P 2004 INT C FIELD P, V4, P6
   BERENSON D, EVOLVABLE HARDWARE, V2, P12
   Dong PX, 2006, IEEE IJCNN, P1518
   Gerstner W., 2002, SPIKING NEURON MODEL
   HARKIN J, 2007, IEEE SOFT COMP IND A
   Hereford J, 2004, 2004 NASA/DOD CONFERENCE ON EVOLVABLE HARDWARE, PROCEEDINGS, P161
   HOHMANN SG, 2002, P GEN EV COMP C 2002, P375
   Holland JH., 1975, ADAPTATION NATURAL A
   Kumagai T, 1999, INT J ADAPT CONTROL, V13, P261, DOI 10.1002/(SICI)1099-1115(199906)13:4<261::AID-ACS546>3.0.CO;2-N
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maher J, 2006, ANN IEEE SYM FIELD P, P321
   PASERO E, 2004, P IEEE INT JOINT C N, V4
   PEREZURIBE A, STRUCTURE ADAPTABLE, P1251
   Plante J, 2003, 2003 NASA/DOD CONFERENCE ON EVOLVABLE HARDWARE, P77
   ROCKE P, 2005, P 2005 INT C REC COM
   Rocke P, 2007, LECT NOTES COMPUT SC, V4419, P373
   SCHEMMEL J, 2004, P IEEE INT JOINT C N, V3
   Schemmel J, 2006, P 2006 INT JOINT C N
   SCHENIMEL J, 2001, P EV SYST BIOL HARDW
   TERRY M, 2006, 9 EUR C GEN PROGR EV
   Ventresca M, 2006, IEEE IJCNN, P4514
   FIELD PROGRAMMABLE A
NR 23
TC 13
Z9 13
U1 1
U2 3
PY 2008
VL 5216
BP 118
EP +
UT WOS:000260884000011
DA 2023-11-16
ER

PT C
AU El-Masry, M
   Anees, S
   Weigel, R
AF El-Masry, Moamen
   Anees, Sohaib
   Weigel, Robert
BE Becker, J
   Marshall, A
   Harbaum, T
   Ganguly, A
   Siddiqui, F
   McLaughlin, K
TI Spiking Neural Networks Design-Space Exploration Platform Supporting
   Online and Offline Learning
SO 2023 IEEE 36TH INTERNATIONAL SYSTEM-ON-CHIP CONFERENCE, SOCC
SE IEEE International SOC Conference
DT Proceedings Paper
CT 36th IEEE International System-on-Chip Conference (SOCC)
CY SEP 05-08, 2023
CL Santa Clara, CA
DE Spiking neural networks; neuromorphic; system-on-module; non-volatile
   memory; resistive-RAM; online learning; offline learning; STDP
ID NEURONS
AB As Spiking Neural Networks (SNNs) gain popularity, more SNN components are developed. SNN development requires a dedicated platform for testing and evaluating the various network components. This paper presents a novel system-on-module (SoM) for exploring spiking neural network hardware components based on a fully integrated platform. The platform is highly reconfigurable, allowing different modes of operation for SNN on/offline learning. The proposed fully integrated SoM platform was tested for its ability to perform learning recognition of random input patterns using the STDP learning algorithm. The results obtained using 28 nm CMOS technology demonstrate the feasibility of this approach.
C1 [El-Masry, Moamen; Anees, Sohaib] Infineon Technol AG, Neubiberg, Germany.
   [El-Masry, Moamen; Weigel, Robert] Univ Erlangen Nurnberg, Erlangen, Germany.
RP El-Masry, M (corresponding author), Infineon Technol AG, Neubiberg, Germany.; El-Masry, M (corresponding author), Univ Erlangen Nurnberg, Erlangen, Germany.
EM moamen.el-masry@infineon.com
CR Aamir SA, 2018, IEEE T BIOMED CIRC S, V12, P1027, DOI 10.1109/TBCAS.2018.2848203
   Bai KJ, 2021, IEEE T CIRCUITS-I, V68, P2850, DOI 10.1109/TCSI.2021.3071956
   Bartolozzi C, 2007, NEURAL COMPUT, V19, P2581, DOI 10.1162/neco.2007.19.10.2581
   Bofill-i-Petit A, 2004, IEEE T NEURAL NETWOR, V15, P1296, DOI 10.1109/TNN.2004.832842
   Caporale N, 2008, ANNU REV NEUROSCI, V31, P25, DOI 10.1146/annurev.neuro.31.060407.125639
   Christensen DV, 2022, NEUROMORPH COMPUT EN, V2, DOI 10.1088/2634-4386/ac4a83
   El-Masry M., 2023, 2023 IEEE 5 INT C AR, P1
   Indiveri G, 2006, IEEE T NEURAL NETWOR, V17, P211, DOI 10.1109/TNN.2005.860850
   Indiveri G, 2003, PROCEEDINGS OF THE 2003 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL IV, P820
   Indiveri G, 2013, NANOTECHNOLOGY, V24, DOI 10.1088/0957-4484/24/38/384010
   Indiveri G, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00073
   Joo B, 2022, IEEE T CIRCUITS-I, V69, P3632, DOI 10.1109/TCSI.2022.3178989
   Marche D, 2010, IEEE T CIRCUITS-I, V57, P31, DOI 10.1109/TCSI.2009.2019396
   Masquelier T, 2008, PLOS ONE, V3, DOI 10.1371/journal.pone.0001377
   Mostafa H, 2017, IEEE INT SYMP CIRC S
   Neckar A, 2019, P IEEE, V107, P144, DOI 10.1109/JPROC.2018.2881432
   Qiao N, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00141
   Rachmuth G, 2011, P NATL ACAD SCI USA, V108, pE1266, DOI 10.1073/pnas.1106161108
   Sangiovanni-Vincentelli A, 2001, IEEE DES TEST COMPUT, V18, P23, DOI 10.1109/54.970421
   Shen YK, 2021, INT SOC DESIGN CONF, P274, DOI 10.1109/ISOCC53507.2021.9613939
   Shi RZ, 2004, ADV NEUR IN, V16, P1003
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Tapson J, 2010, IEEE INT SYMP CIRC S, P1424, DOI 10.1109/ISCAS.2010.5537307
   Thakur CS, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00891
   Wu XY, 2015, IEEE T CIRCUITS-II, V62, P1088, DOI 10.1109/TCSII.2015.2456372
   Yang ZT, 2021, I SYMPOS LOW POWER E, DOI 10.1109/ISLPED52811.2021.9502497
NR 26
TC 0
Z9 0
U1 0
U2 0
PY 2023
BP 319
EP 323
DI 10.1109/SOCC58585.2023.10257044
UT WOS:001071170500055
DA 2023-11-16
ER

PT J
AU Huang, YL
   Wang, T
   Wang, J
   Peng, H
AF Huang, YuLei
   Wang, Tao
   Wang, Jun
   Peng, Hong
TI Reliability evaluation of distribution network based on fuzzy spiking
   neural P system with self-synapse
SO JOURNAL OF MEMBRANE COMPUTING
DT Article
DE Spiking neural P system; Self-synapse; Reliability assessment;
   Distribution network
ID FAULT-DIAGNOSIS
AB This paper proposes a fuzzy spiking neural P system with self-synapse (in short, FSNPSS) which is applied to the reliability assessment of distribution networks. The method maps the operation or fault states of the distribution network component and the load to the excited or resting states of neurons, and converts electrical relationships among components, loads and targeted systems into a synaptic connection relationship. Then, the occurrence probabilities of the states are transmitted by spikes, and reliability indices are computed by accumulating pulse values of the spikes. Finally, the feasibility and effectiveness of solving reliability assessment of distribution networks by membrane systems are verified in case studies.
C1 [Huang, YuLei; Wang, Tao; Wang, Jun] Xihua Univ, Sch Elect Engn & Elect Informat, Chengdu, Peoples R China.
   [Huang, YuLei; Wang, Tao; Wang, Jun] Xihua Univ, Minist Educ, Key Lab Fluid Power Machinery, Chengdu, Peoples R China.
   [Peng, Hong] Xihua Univ, Sch Comp & Software Engn, Chengdu, Peoples R China.
RP Huang, YL (corresponding author), Xihua Univ, Sch Elect Engn & Elect Informat, Chengdu, Peoples R China.; Huang, YL (corresponding author), Xihua Univ, Minist Educ, Key Lab Fluid Power Machinery, Chengdu, Peoples R China.
EM 2256856320@qq.com
CR ALLAN RN, 1991, IEEE T POWER SYST, V6, P813, DOI 10.1109/59.76730
   Bekkers JM, 2009, CURR BIOL, V19, pR296, DOI 10.1016/j.cub.2009.02.010
   Cabarle FGC, 2018, IEEE T NANOBIOSCI, V17, P560, DOI 10.1109/TNB.2018.2879345
   Cabarle FGC, 2017, IEEE T NANOBIOSCI, V16, P792, DOI 10.1109/TNB.2017.2762580
   Cabarle FGC, 2016, NEURAL COMPUT APPL, V27, P1337, DOI 10.1007/s00521-015-1937-5
   Cabarle FGC, 2015, NEURAL COMPUT APPL, V26, P1905, DOI 10.1007/s00521-015-1857-4
   de la Cruz RTA, 2019, J MEMBRANE COMPUT, V1, P161, DOI 10.1007/s41965-019-00021-2
   Gutierrez-Naranjo Miguel A., 2008, Membrane Computing. 9th International Workshop, WMC 2008. Revised Selected and Invited Papers, P217
   Heydt GT, 2010, IEEE T POWER SYST, V25, P2006, DOI 10.1109/TPWRS.2010.2045929
   Hou K, 2016, IEEE T POWER SYST, V31, P738, DOI 10.1109/TPWRS.2015.2392103
   Ionescu M, 2006, FUND INFORM, V71, P279
   Jimenez ZB, 2019, J MEMBRANE COMPUT, V1, P145, DOI 10.1007/s41965-019-00020-3
   Li W, 2014, IEEE PR SER POWER, P1, DOI 10.1002/9781118849972
   Liai G., 2016, INT J GRID DISTRIB, V1, P8
   Liu W, 2020, COMPLEXITY, V2020, DOI 10.1155/2020/2462647
   Pan LQ, 2012, NEURAL PROCESS LETT, V35, P13, DOI 10.1007/s11063-011-9201-1
   Pan LQ, 2012, NEURAL COMPUT, V24, P805, DOI 10.1162/NECO_a_00238
   Pan LQ, 2011, SCI CHINA INFORM SCI, V54, P1596, DOI 10.1007/s11432-011-4303-y
   Pan LQ, 2009, INT J COMPUT COMMUN, V4, P273, DOI 10.15837/ijccc.2009.3.2435
   Päun G, 2000, J COMPUT SYST SCI, V61, P108, DOI 10.1006/jcss.1999.1693
   Peng H, 2013, INFORM SCIENCES, V235, P106, DOI 10.1016/j.ins.2012.07.015
   Rong H., 2018, SPIKING NEURAL P SYS, P256
   Rozenberg G, 2010, OXFORD HDB MEMBRANE
   Song T, 2019, IEEE T NANOBIOSCI, V18, P176, DOI 10.1109/TNB.2019.2896981
   Tu M, 2014, CHINESE J ELECTRON, V23, P87
   Wang J, 2016, CHINESE J ELECTRON, V25, P320, DOI 10.1049/cje.2016.03.019
   Wang J, 2013, IEEE T FUZZY SYST, V21, P209, DOI 10.1109/TFUZZ.2012.2208974
   Wang T, 2015, INT J COMPUT COMMUN, V10, P904
   Wang T, 2019, IEEE ACCESS, V7, P12798, DOI 10.1109/ACCESS.2019.2892797
   Wang T, 2015, J COMPUT THEOR NANOS, V12, P1103, DOI 10.1166/jctn.2015.3857
   Wang T, 2015, IEEE T POWER SYST, V30, P1182, DOI 10.1109/TPWRS.2014.2347699
   Xiong G., 2013, MATH PROBL ENG, V1, P211
   Yin LP, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-07317-4
   Yu DC, 1999, IEEE T POWER SYST, V14, P426, DOI 10.1109/59.761860
   Zhang G., 2017, REAL LIFE APPL MEMBR, DOI [10.1007/978-3-319-55989-6, DOI 10.1007/978-3-319-55989-6]
   Zhu YL, 2008, MEX INT CONF ARTIF I, P271, DOI 10.1109/MICAI.2008.35
NR 36
TC 14
Z9 14
U1 1
U2 7
PD MAR
PY 2021
VL 3
IS 1
BP 51
EP 62
DI 10.1007/s41965-020-00035-1
UT WOS:000672025000005
DA 2023-11-16
ER

PT J
AU Issar, D
   Williamson, RC
   Khanna, SB
   Smith, MA
AF Issar, Deepa
   Williamson, Ryan C.
   Khanna, Sanjeev B.
   Smith, Matthew A.
TI A neural network for online spike classification that improves decoding
   accuracy
SO JOURNAL OF NEUROPHYSIOLOGY
DT Article
DE BCI; decoding; neural network; prefrontal cortex; spike-sorting
ID LATERAL PREFRONTAL CORTEX; NEURONS; RECORDINGS; MOVEMENT; INFORMATION;
   SELECTIVITY; POTENTIALS; STABILITY; ATTENTION
AB Separating neural signals from noise can improve brain-computer interface performance and stability. However, most algorithms for separating neural action potentials from noise are not suitable for use in real time and have shown mixed effects on decoding performance. With the goal of removing noise that impedes online decoding, we sought to automate the intuition of human spike-sorters to operate in real time with an easily tunable parameter governing the stringency with which spike waveforms are classified. We trained an artificial neural network with one hidden layer on neural waveforms that were hand-labeled as either spikes or noise. The network output was a likelihood metric for each waveform it classified, and we tuned the network's stringency by varying the minimum likelihood value for a waveform to be considered a spike. Using the network's labels to exclude noise waveforms, we decoded remembered target location during a memory-guided saccade task from electrode arrays implanted in prefrontal cortex of rhesus macaque monkeys. The network classified waveforms in real time, and its classifications were qualitatively similar to those of a human spike-sorter. Compared with decoding with threshold crossings, in most sessions we improved decoding performance by removing waveforms with low spike likelihood values. Furthermore, decoding with our network's classifications became more beneficial as time since array implantation increased. Our classifier serves as a feasible preprocessing step, with little risk of harm, that could be applied to both off-line neural data analyses and online decoding.
   NEW & NOTEWORTHY Although there are many spike-sorting methods that isolate well-defined single units, these methods typically involve human intervention and have inconsistent effects on decoding. We used human classified neural waveforms as training data to create an artificial neural network that could be tuned to separate spikes from noise that impaired decoding. We found that this network operated in real time and was suitable for both off-line data processing and online decoding.
C1 [Issar, Deepa; Khanna, Sanjeev B.] Univ Pittsburgh, Dept Bioengn, Pittsburgh, PA USA.
   [Issar, Deepa; Williamson, Ryan C.] Univ Pittsburgh, Sch Med, Pittsburgh, PA USA.
   [Williamson, Ryan C.] Carnegie Mellon Univ, Dept Machine Learning, Pittsburgh, PA 15213 USA.
   [Smith, Matthew A.] Carnegie Mellon Univ, Dept Biomed Engn, 4400 Fifth Ave,Rm 115, Pittsburgh, PA 15213 USA.
   [Williamson, Ryan C.; Smith, Matthew A.] Carnegie Mellon Neurosci Inst, Pittsburgh, PA USA.
   [Smith, Matthew A.] Univ Pittsburgh, Sch Med, Dept Ophthalmol, Pittsburgh, PA 15261 USA.
RP Smith, MA (corresponding author), Carnegie Mellon Univ, Dept Biomed Engn, 4400 Fifth Ave,Rm 115, Pittsburgh, PA 15213 USA.; Smith, MA (corresponding author), Carnegie Mellon Univ, Neurosci Inst, 4400 Fifth Ave,Rm 115, Pittsburgh, PA 15213 USA.
EM matt@smithlab.net
CR Abadi M., 2016, OSDI 16
   Anastassiou CA, 2015, J NEUROPHYSIOL, V114, P608, DOI 10.1152/jn.00628.2014
   Averbeck BB, 2006, NAT REV NEUROSCI, V7, P358, DOI 10.1038/nrn1888
   Bishop W, 2014, J NEURAL ENG, V11, DOI 10.1088/1741-2560/11/2/026001
   Boulay CB, 2016, J NEUROPHYSIOL, V115, P486, DOI 10.1152/jn.00788.2015
   Chandra R, 1997, IEEE T BIO-MED ENG, V44, P403, DOI 10.1109/10.568916
   Chase SM, 2012, J NEUROPHYSIOL, V108, P624, DOI 10.1152/jn.00371.2011
   Chaure FJ, 2018, J NEUROPHYSIOL, V120, P1859, DOI 10.1152/jn.00339.2018
   Chestek CA, 2011, J NEURAL ENG, V8, DOI 10.1088/1741-2560/8/4/045005
   Chollet F., 2015, KERAS
   Christie BP, 2015, J NEURAL ENG, V12, DOI 10.1088/1741-2560/12/1/016009
   Chung JE, 2017, NEURON, V95, P1381, DOI 10.1016/j.neuron.2017.08.030
   Cohen MR, 2011, NAT NEUROSCI, V14, P811, DOI 10.1038/nn.2842
   Collinger JL, 2013, LANCET, V381, P557, DOI 10.1016/S0140-6736(12)61816-9
   Dai J, 2019, J NEURAL ENG, V16, DOI 10.1088/1741-2552/ab0bfb
   Dickey AS, 2009, J NEUROPHYSIOL, V102, P1331, DOI 10.1152/jn.90920.2008
   Downey JE, 2018, J NEURAL ENG, V15, DOI 10.1088/1741-2552/aab7a0
   Flint RD, 2016, J NEUROSCI, V36, P3623, DOI 10.1523/JNEUROSCI.2339-15.2016
   Fraser GW, 2012, J NEUROPHYSIOL, V107, P1970, DOI 10.1152/jn.01012.2010
   Fraser GW, 2009, J NEURAL ENG, V6, DOI 10.1088/1741-2560/6/5/055004
   Gilja V, 2012, NAT NEUROSCI, V15, P1752, DOI 10.1038/nn.3265
   Golub MD, 2015, ELIFE, V4, DOI [10.7554/elife.10015, 10.7554/eLife.10015]
   Hochberg LR, 2006, NATURE, V442, P164, DOI 10.1038/nature04970
   Hochberg LR, 2012, NATURE, V485, P372, DOI 10.1038/nature11076
   Homer ML, 2013, ANNU REV BIOMED ENG, V15, P383, DOI 10.1146/annurev-bioeng-071910-124640
   Jia N, 2017, J NEURAL ENG, V14, DOI 10.1088/1741-2552/aa5a3e
   Kelly RC, 2007, J NEUROSCI, V27, P261, DOI 10.1523/JNEUROSCI.4906-06.2007
   Khanna SB, 2019, J NEUROSCI, V39, P4511, DOI 10.1523/JNEUROSCI.2329-18.2019
   Kim KH, 2000, IEEE T BIO-MED ENG, V47, P1406, DOI 10.1109/10.871415
   Kingma DP., 2017, ARXIV
   Kloosterman F, 2014, J NEUROPHYSIOL, V111, P217, DOI 10.1152/jn.01046.2012
   Kohn A, 2016, ANNU REV NEUROSCI, V39, P237, DOI 10.1146/annurev-neuro-070815-013851
   Koralek AC, 2012, NATURE, V483, P331, DOI 10.1038/nature10845
   Lee JH, 2017, ADV NEURAL INFORM PR
   Lewicki MS, 1998, NETWORK-COMP NEURAL, V9, pR53, DOI 10.1088/0954-898X/9/4/001
   Masse NY, 2014, J NEUROSCI METH, V236, P58, DOI 10.1016/j.jneumeth.2014.08.004
   Meyers EM, 2008, J NEUROPHYSIOL, V100, P1407, DOI 10.1152/jn.90248.2008
   Neto JP, 2016, J NEUROPHYSIOL, V116, P892, DOI 10.1152/jn.00103.2016
   Nuyujukian P, 2014, J NEURAL ENG, V11, DOI 10.1088/1741-2560/11/6/066003
   Oby ER, 2016, J NEURAL ENG, V13, DOI 10.1088/1741-2560/13/3/036009
   Pachitariu M, 2016, ADV NEUR IN, V29
   Parthasarathy A, 2017, NAT NEUROSCI, V20, P1770, DOI 10.1038/s41593-017-0003-2
   Pedreira C, 2012, J NEUROSCI METH, V211, P58, DOI 10.1016/j.jneumeth.2012.07.010
   Perge JA, 2014, J NEURAL ENG, V11, DOI 10.1088/1741-2560/11/4/046007
   Perge JA, 2013, J NEURAL ENG, V10, DOI 10.1088/1741-2560/10/3/036004
   Rácz M, 2020, J NEURAL ENG, V17, DOI 10.1088/1741-2552/ab4896
   Rey HG, 2015, BRAIN RES BULL, V119, P106, DOI 10.1016/j.brainresbull.2015.04.007
   Rizzuto DS, 2005, NAT NEUROSCI, V8, P415, DOI 10.1038/nn1424
   Rossant C, 2016, NAT NEUROSCI, V19, P634, DOI 10.1038/nn.4268
   Sadtler PT, 2015, J NEURAL ENG, V12, DOI 10.1088/1741-2560/12/1/016015
   Sadtler PT, 2014, NATURE, V512, P423, DOI 10.1038/nature13665
   Saif-ur-Rehman M, 2019, J NEURAL ENG, V16, DOI 10.1088/1741-2552/ab1e63
   Santhanam G, 2004, P ANN INT IEEE EMBS, V26, P4380
   Schafer RJ, 2011, SCIENCE, V332, P1568, DOI 10.1126/science.1199892
   Shoham S, 2003, J NEUROSCI METH, V127, P111, DOI 10.1016/S0165-0270(03)00120-1
   Snyder AC, 2015, J NEUROPHYSIOL, V114, P1468, DOI 10.1152/jn.00427.2015
   Snyder AC, 2015, NAT NEUROSCI, V18, P736, DOI 10.1038/nn.3979
   Spaak E, 2017, J NEUROSCI, V37, P6503, DOI 10.1523/JNEUROSCI.3364-16.2017
   Stark E, 2007, J NEUROSCI, V27, P8387, DOI 10.1523/JNEUROSCI.1321-07.2007
   Todorova S, 2014, J NEURAL ENG, V11, DOI 10.1088/1741-2560/11/5/056005
   Tolias AS, 2007, J NEUROPHYSIOL, V98, P3780, DOI 10.1152/jn.00260.2007
   Trautmann EM, 2019, NEURON, V103, P292, DOI 10.1016/j.neuron.2019.05.003
   Tremblay S, 2015, J NEUROSCI, V35, P9038, DOI 10.1523/JNEUROSCI.1041-15.2015
   Velliste M, 2008, NATURE, V453, P1098, DOI 10.1038/nature06996
   Ventura V, 2015, NEURAL COMPUT, V27, P1033, DOI 10.1162/NECO_a_00731
   Wang D, 2014, J NEURAL ENG, V11, DOI 10.1088/1741-2560/11/3/036009
   Ward MP, 2009, BRAIN RES, V1282, P183, DOI 10.1016/j.brainres.2009.05.052
   Wood F, 2004, IEEE T BIO-MED ENG, V51, P912, DOI 10.1109/TBME.2004.826677
NR 68
TC 7
Z9 7
U1 0
U2 15
PD APR
PY 2020
VL 123
IS 4
BP 1472
EP 1485
DI 10.1152/jn.00641.2019
UT WOS:000529327500018
DA 2023-11-16
ER

PT C
AU Alamdari, ARSA
AF Alamdari, Amir Reza Saffari Azar
BE Ardil, C
TI Unknown Environment Representation for Mobile Robot Using Spiking Neural
   Networks
SO PROCEEDINGS OF WORLD ACADEMY OF SCIENCE, ENGINEERING AND TECHNOLOGY, VOL
   6
DT Proceedings Paper
CT Conference of the World-Academy-of-Science-Engineering-and-Technology
CY JUN 24-26, 2005
CL Sydney, AUSTRALIA
DE Mobile Robot; Path Planning; Self-organization; Spiking Neural Networks
AB In this paper, a model of self-organizing spiking neural networks is introduced and applied to mobile robot environment representation and path planning problem. A network of spike-response-model neurons with a recurrent architecture is used to create robot's internal representation from surrounding environment. The overall activity of network simulates a self-organizing system with unsupervised learning. A modified A* algorithm is used to find the best path using this internal representation between starting and goal points. This method can be used with good performance for both known and unknown environments.
C1 Graz Univ Technol, Inst Theoret Comp Sci, A-8010 Graz, Austria.
RP Alamdari, ARSA (corresponding author), Graz Univ Technol, Inst Theoret Comp Sci, A-8010 Graz, Austria.
EM amir@ymer.org
CR CHOE Y, 2001, THESIS U TEXAS AUSTI
   Eckhorn R, 1990, NEURAL COMPUT, V2, P293, DOI 10.1162/neco.1990.2.3.293
   ECKHORN R, 1988, BIOL CYBERN, V60, P121, DOI 10.1007/BF00202899
   Eurich CW, 2000, NEUROCOMPUTING, V32, P741, DOI 10.1016/S0925-2312(00)00239-3
   Gerstner W., 2002, SPIKING NEURON MODEL
   Maass W, 1996, NEURAL COMPUT, V8, P1, DOI 10.1162/neco.1996.8.1.1
   PATEL AJ, GAME PROGRAMMING PAT
NR 7
TC 4
Z9 4
U1 0
U2 1
PY 2005
BP 49
EP 52
UT WOS:000261051500011
DA 2023-11-16
ER

PT C
AU Tezuka, T
   Claramunt, C
AF Tezuka, Taro
   Claramunt, Christophe
GP IEEE
TI Connectivity estimation of neural networks using a spike train kernel
SO 2015 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUL 12-17, 2015
CL Killarney, IRELAND
ID FUNCTIONAL CONNECTIVITY; CAUSAL RELATIONS; MODEL
AB Estimating the connectivity strength based on the signals observed at each node of a network is an important task in neural network analysis. One notable example is estimating the connectivity of a biological neural network using spikes (i.e., action potentials) observed at electrodes. The research presented in this paper introduces a novel method that estimates the underlying connectivity of a given neural network based on a similarity measure applied to spike trains. Specifically, we use a normalized positive definite kernel defined on spike trains to estimate network connectivity. The proposed method was evaluated in the context of synthetic and real data. The generation of synthetic data is based on a CERM (Coupled Escape-Rate Model), which is known to generate spike trains of various types by tuning a few parameters. We also analyzed real data recorded from the visual cortex of an anaesthetized cat. The results showed that our method provides an effective way of estimating connectivity when spike trains are the only observable information.
C1 [Tezuka, Taro] Univ Tsukuba, Tsukuba, Ibaraki, Japan.
   [Claramunt, Christophe] Naval Acad Res Inst, Brest, France.
RP Tezuka, T (corresponding author), Univ Tsukuba, Tsukuba, Ibaraki, Japan.
EM tezuka@slis.tsukuba.ac.jp; claramunt@ecole-navale.fr
CR [Anonymous], 2008, P NEUR INF PROC SYST
   Blanche Tim, 2009, MULTINEURON RECORDIN
   Blanche TJ, 2005, J NEUROPHYSIOL, V93, P2987, DOI 10.1152/jn.01023.2004
   Chi ZY, 2007, J NEUROPHYSIOL, V97, P1221, DOI 10.1152/jn.00448.2006
   Dauwels J, 2009, LECT NOTES COMPUT SC, V5506, P177, DOI 10.1007/978-3-642-02490-0_22
   Dubbs AJ, 2010, NEURAL COMPUT, V22, P2785, DOI 10.1162/NECO_a_00026
   Eichhorn J, 2004, ADV NEUR IN, V16, P1367
   Fisher N., 2010, ADV NEURAL INFORM PR, P595
   Friston KJ, 2003, NEUROIMAGE, V19, P1273, DOI 10.1016/S1053-8119(03)00202-7
   Garofalo M, 2009, PLOS ONE, V4, DOI 10.1371/journal.pone.0006482
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   GRANGER CWJ, 1969, ECONOMETRICA, V37, P424, DOI 10.2307/1912791
   Houghton Conor, 2011, UNDERSTANDING VISUAL
   Houghton Conor, 2013, MULTISCALE ANAL NONL
   Il Memming Park, 2013, IEEE Signal Processing Magazine, V30, P149, DOI 10.1109/MSP.2013.2251072
   Johnson DH, 2001, J COMPUT NEUROSCI, V10, P47, DOI 10.1023/A:1008968010214
   Kaminski M, 2001, BIOL CYBERN, V85, P145, DOI 10.1007/s004220000235
   KAMINSKI MJ, 1991, BIOL CYBERN, V65, P203, DOI 10.1007/BF00198091
   Kobayashi R, 2013, J COMPUT NEUROSCI, V35, P109, DOI 10.1007/s10827-013-0443-y
   Kreuz T, 2013, J NEUROPHYSIOL, V109, P1457, DOI 10.1152/jn.00873.2012
   Kung Sun Yuan, 2014, KERNEL METHODS MACHI
   Li Lin, 2014, COMPUTATIONAL INTELL
   Mohri M, 2012, FDN MACHINE LEARNING
   Naud R, 2011, NEURAL COMPUT, V23, P3016, DOI 10.1162/NECO_a_00208
   Okatan M, 2005, NEURAL COMPUT, V17, P1927, DOI 10.1162/0899766054322973
   Paiva ARC, 2010, STATISTICAL SIGNAL PROCESSING FOR NEUROSCIENCE AND NEUROTECHNOLOGY, P265, DOI 10.1016/B978-0-12-375027-3.00008-9
   Paiva ARC, 2009, NEURAL COMPUT, V21, P424, DOI 10.1162/neco.2008.09-07-614
   Park IM, 2012, NEURAL COMPUT, V24, P2223, DOI 10.1162/NECO_a_00309
   Quinn Christopher J., 2010, J COMPUTATIONAL NEUR
   Rusu CV, 2014, NEURAL COMPUT, V26, P306, DOI 10.1162/NECO_a_00545
   SALTON G, 1975, COMMUN ACM, V18, P613, DOI 10.1145/361219.361220
   Shawe-Taylor J, 2004, KERNEL METHODS PATTE
   Shpigelman L, 2005, NEURAL COMPUT, V17, P671, DOI 10.1162/0899766053019944
   Shpigelman Lavi, 2003, ADV NEURAL INFORM PR, V15, P125
   Tezuka Taro, 2014, P 39 INT C AC SPEECH, P6035
   van Rossum MCW, 2001, NEURAL COMPUT, V13, P751, DOI 10.1162/089976601300014321
   Vicente R, 2011, J COMPUT NEUROSCI, V30, P45, DOI 10.1007/s10827-010-0262-3
   Victor JD, 1996, J NEUROPHYSIOL, V76, P1310, DOI 10.1152/jn.1996.76.2.1310
   Victor Jonathan D., 2011, UNDERSTANDING VISUAL
   Wu W, 2011, J COMPUT NEUROSCI, V31, P725, DOI 10.1007/s10827-011-0336-x
NR 40
TC 0
Z9 0
U1 0
U2 2
PY 2015
UT WOS:000370730601010
DA 2023-11-16
ER

PT C
AU Sulaiman, MBG
   Juang, KC
   Lu, CC
AF Sulaiman, Muhammad Bintang Gemintang
   Juang, Kai-Cheung
   Lu, Chih-Cheng
GP IEEE
TI Weight Quantization in Spiking Neural Network for Hardware
   Implementation
SO 2020 IEEE INTERNATIONAL CONFERENCE ON CONSUMER ELECTRONICS - TAIWAN
   (ICCE-TAIWAN)
SE IEEE International Conference on Consumer Electronics-Taiwan
DT Proceedings Paper
CT 7th IEEE International Conference on Consumer Electronics - Taiwan
   (ICCE-Taiwan)
CY SEP 28-30, 2020
CL Taoyuan, TAIWAN
DE Spiking Neural Network; Hardware Implementation; Spike-Timing-Dependent
   Plasticity
AB Spiking Neural Network (SNN) is the third generation of Artificial Neural Network (ANN), which is potentially an efficient way to reduce the computation load as well as the power consumption on hardware due to the sparse activation and event-driven behavior of its neuron. In this paper, we have trained an SNN by using Spike Timing Dependent Plasticity in an unsupervised manner. The training process is optimized by using the quantization method on its weights, performed at the regular intervals during training. From the simulation results, the SNN with the 4-bit weights received an accuracy drop of 0.09%. The behavioral simulation result with the same bit-width weight quantization received an accuracy drop of 0.28%.
C1 [Sulaiman, Muhammad Bintang Gemintang; Juang, Kai-Cheung; Lu, Chih-Cheng] Ind Technol Res Inst, Zhudong Township, Taiwan.
RP Sulaiman, MBG (corresponding author), Ind Technol Res Inst, Zhudong Township, Taiwan.
CR Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Paugam-Moisy H., 2012, COMPUTING SPIKING NE, ppp 335
   Rathi N, 2019, IEEE T COMPUT AID D, V38, P668, DOI 10.1109/TCAD.2018.2819366
   Rojas R, 1996, NEURAL NETWORKS SYST
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Zhou S., 2018, ARXIV160606160V3
NR 6
TC 0
Z9 0
U1 0
U2 0
PY 2020
DI 10.1109/icce-taiwan49838.2020.9258272
UT WOS:000648532300246
DA 2023-11-16
ER

PT C
AU Sahni, L
   Chakraborty, D
   Ghosh, A
AF Sahni, Lakshay
   Chakraborty, Debasrita
   Ghosh, Ashish
GP AAAI
TI Implementation of Boolean AND and OR Logic Gates with Biologically
   Reasonable Time Constants in Spiking Neural Networks
SO THIRTY-THIRD AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE / THIRTY-FIRST
   INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE / NINTH
   AAAI SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE
SE AAAI Conference on Artificial Intelligence
DT Proceedings Paper
CT 33rd AAAI Conference on Artificial Intelligence / 31st Innovative
   Applications of Artificial Intelligence Conference / 9th AAAI Symposium
   on Educational Advances in Artificial Intelligence
CY JAN 27-FEB 01, 2019
CL Honolulu, HI
AB Latest developments in the field of power-efficient neural interface circuits provide an excellent platform for applications where power consumption is the primary concern. Developing neural networks to achieve pattern recognition on such hardware remains a daunting task owing to substantial computational complexity. We propose and demonstrate a Spiking Neural Network (SNN) with biologically reasonable time constants to implement basic Boolean Logic Gates. The same network can be further applied to more complex problem statements. We employ a frequency spike encoding for data representation in the model, and a simplified and computationally efficient model of a neuron with exponential synapses and Spike Timing Dependent Plasticity (STDP).
C1 [Sahni, Lakshay] Delhi Technol Univ, Dept Elect & Elect Engn, Delhi, India.
   [Chakraborty, Debasrita; Ghosh, Ashish] Indian Stat Inst, Machine Intelligence Unit, Kolkata, India.
RP Sahni, L (corresponding author), Delhi Technol Univ, Dept Elect & Elect Engn, Delhi, India.
EM layshaysahni2k14@dtu.ac.in; debasritac@gmail.com; ash@isical.ac.in
CR Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Bi GQ, 2001, ANNU REV NEUROSCI, V24, P139, DOI 10.1146/annurev.neuro.24.1.139
   Jug F., 2012, THESIS
   Ourdighi A, 2016, INT ARAB J INF TECHN, V13, P1032
   Petrovic C, 2013, SPIKING NEURAL NETWO
   Reljan-Delaney M, 2017, 2017 COMPUTING CONFERENCE, P701, DOI 10.1109/SAI.2017.8252173
   Stoliar P, 2017, ADV FUNCT MATER, V27, DOI 10.1002/adfm.201604740
NR 7
TC 2
Z9 2
U1 0
U2 6
PY 2019
BP 10021
EP 10022
UT WOS:000486572504148
DA 2023-11-16
ER

PT C
AU Ona, KM
   Charbonnier, B
   Hassan, K
AF Ona, Keshia Mekemeza
   Charbonnier, Benoit
   Hassan, Karim
GP IEEE
TI Design of an Integrated III-V on silicon semiconductor laser for spiking
   neural networks
SO IITC2021: 2021 IEEE INTERNATIONAL INTERCONNECT TECHNOLOGY CONFERENCE
   (IITC)
SE IEEE International Interconnect Technology Conference IITC
DT Proceedings Paper
CT IEEE International Interconnect Technology Conference (IITC)
CY JUL 06-09, 2021
CL ELECTR NETWORK
DE Spiking Neural Network; Semiconductor laser; Saturable absorber; Mode
   locking; III-V on SI process integration
ID DYNAMICS
AB We present the simulation of an individual spiking laser to be used as a photonic neuron for a spiking neural network scale-up. The neuron is a semiconductor laser optimized for pulsed operation through modification of the CEA Leti III-V on Si integrated continuous wave laser's heterostructure. Its full integration into a complete silicon photonics platform makes it ideal for large scale photonic SNN circuits.
C1 [Ona, Keshia Mekemeza; Charbonnier, Benoit; Hassan, Karim] Univ Grenoble Alpes, CEA Leti, F-38000 Grenoble, France.
RP Ona, KM (corresponding author), Univ Grenoble Alpes, CEA Leti, F-38000 Grenoble, France.
EM keshia.mekemezaona@cea.fr; benoit.charbonnier@cea.fr;
   karim.hassan@cea.fr
CR Aggarwal C.C., 2018, NEURAL NETWORKS DEEP, DOI [10.1007/978-3-319-94463-0, DOI 10.1007/978-3-319-94463-0]
   Bai B., 2020, SCI CHINA INFORM SCI, V63, P1
   Descos A, 2013, 39 EUR C EXH OPT COM, P1
   DONNELLY JP, 1977, SOLID STATE ELECTRON, V20, P727, DOI 10.1016/0038-1101(77)90052-1
   Duprez H, 2015, OPT EXPRESS, V23, P8489, DOI 10.1364/OE.23.008489
   JONES DJ, 1995, IEEE J QUANTUM ELECT, V31, P1051, DOI 10.1109/3.387042
   KARIN JR, 1994, APPL PHYS LETT, V64, P676, DOI 10.1063/1.111058
   Piprek J, 2002, IEEE J QUANTUM ELECT, V38, P1253, DOI 10.1109/JQE.2002.802441
   Prucnal PR, 2017, NEUROMORPHIC PHOTONICS, P1
   Shastri BJ, 2015, OPT EXPRESS, V23, P8029, DOI 10.1364/OE.23.008029
   Vreeken J, 2003, SPIKING NEURAL NETWO
   Zhang ZX, 2020, NONLINEAR DYNAM, V99, P1103, DOI 10.1007/s11071-019-05339-1
NR 12
TC 2
Z9 2
U1 1
U2 4
PY 2021
DI 10.1109/IITC51362.2021.9537482
UT WOS:000784773200037
DA 2023-11-16
ER

PT C
AU Abinaya, B
   Sophia, SSJ
AF Abinaya, B.
   Sophia, S. Sheeba Jeya
BE Karthigaikumar, P
   Arulmurugan, C
   Manojkumar, T
TI AN EVENT BASED CMOS QUAD BILATERAL COMBINATION WITH ASYNCHRONOUS SRAM
   ARCHITECTURE BASED NEURAL NETWORK USING LOW POWER
SO 2015 2ND INTERNATIONAL CONFERENCE ON ELECTRONICS AND COMMUNICATION
   SYSTEMS (ICECS)
DT Proceedings Paper
CT 2nd International Conference on Electronics and Communication Systems
   (ICECS)
CY FEB 26-27, 2015
CL Coimbatore, INDIA
DE Neural networks; analog; digital; synaptic weight; Static Random Access
   Memory (SRAM); Very Large Scale Integrated Circuit (VLSI); neural
   network; programmable synaptic memory; spiking network; spike frequency;
   spike voltage; quad bilateral technique and multiple threshold voltage
ID SPIKING; NEURONS
AB Neural networks have a wide range of applications in analog and digital signal processing. Once the network has been trained and the synaptic weight values stored in the SRAM, the VLSI device can be used in stand-alone mode to carry out neural computation in real-time. In existing system they implemented the neural network with programmable synaptic memory. Synaptic weight refers to the strength of a connection between two nodes. The design of neural network architecture is based on CMOS technology and the design performed in its architecture level. Commonly CMOS technology provides less noise during design. The proposed neural network consists of Synaptic weight and spiking network. In the existing system, they concentrated only in spike frequency and spike voltage. The proposed system focuses on reducing the complexity and increasing the efficiency and the network using CMOS technology with quad bilateral technique used in integrated circuit designed to reduce power consumption, by shutting off the current to block the circuits that are not in use and also multiple threshold voltages applied to the circuit which helps to reduce the delay/power. In addition to reducing stand-by or leakage power this approach eliminates critical path delay. The proposed design concentrates on reducing complexity; hence the power consumption will be reduced. And also efficiency will be increased due to less complexity. The proposed system is used in image processing & control system applications.
C1 [Abinaya, B.] NPR Coll Engn & Technol, VLSI design, Dindigul, Tamil Nadu, India.
   [Sophia, S. Sheeba Jeya] NPR Coll Engn & Technol, ECE, Dindigul, Tamil Nadu, India.
RP Abinaya, B (corresponding author), NPR Coll Engn & Technol, VLSI design, Dindigul, Tamil Nadu, India.
EM abinayaece30@gmail.com; sheebajeyasophia@gmail.com
CR [Anonymous], INT JOINT C NEUR NET
   Basu A, 2010, IEEE T BIOMED CIRC S, V4, P311, DOI 10.1109/TBCAS.2010.2055157
   Belatreche A, 2007, SOFT COMPUT, V11, P239, DOI [10.1007/s00500-006-0065-7, 10.1007/S00500-006-0065-7]
   Brader JM, 2007, NEURAL COMPUT, V19, P2881, DOI 10.1162/neco.2007.19.11.2881
   Brette R, 2007, J COMPUT NEUROSCI, V23, P349, DOI 10.1007/s10827-007-0038-6
   Chicca E, 2007, IEEE T CIRCUITS-I, V54, P981, DOI 10.1109/TCSI.2007.893509
   Imam N, 2012, INT SYMP ASYNCHRON C, P25, DOI 10.1109/ASYNC.2012.12
   Jin X, 2010, COMPUT SCI ENG, V12, P91, DOI 10.1109/MCSE.2010.112
   Leñero-Bardallo JA, 2008, IEEE T CIRCUITS-II, V55, P522, DOI 10.1109/TCSII.2007.916864
   Maass W, 2000, NEURAL COMPUT, V12, P1743, DOI 10.1162/089976600300015123
   Merolla PA, 2007, IEEE T CIRCUITS-I, V54, P301, DOI 10.1109/TCSI.2006.887474
   Millner S., ADV NEURAL INFORM PR
   Moradi S, 2011, BIOMED CIRC SYST C, P277, DOI 10.1109/BioCAS.2011.6107781
   Moradi Saber, 2014, IEEE T BIOMED CIRCUI, V8
   Pfeil T, 2012, FRONT NEUROSCI-SWITZ, V6, DOI 10.3389/fnins.2012.00090
   Ramakrishnan S, 2012, BIOMED CIRC SYST C, P400, DOI 10.1109/BioCAS.2012.6418412
   Rowcliffe P, 2008, IEEE T NEURAL NETWOR, V19, P1626, DOI 10.1109/TNN.2008.2000999
   Scholze S, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00117
   Serrano-Gotarredona R, 2009, IEEE T NEURAL NETWOR, V20, P1417, DOI 10.1109/TNN.2009.2023653
   Silver R, 2007, J NEUROSCI, V27, P11807, DOI 10.1523/JNEUROSCI.3575-07.2007
   Systems J. Lafferty, 2010, NIPS, V23, P1642
   Wang YX, 2006, IEEE INT SYMP CIRC S, P4531
   Wang YX, 2010, NEURAL COMPUT, V22, P2086, DOI 10.1162/neco.2010.06-09-1030
   Wijekoon JHB, 2008, NEURAL NETWORKS, V21, P524, DOI 10.1016/j.neunet.2007.12.037
   Yu T, 2012, BIOMED CIRC SYST C, P21, DOI 10.1109/BioCAS.2012.6418479
   Yu T, 2010, IEEE T BIOMED CIRC S, V4, P139, DOI 10.1109/TBCAS.2010.2048566
NR 26
TC 2
Z9 2
U1 0
U2 0
PY 2015
BP 995
EP 999
UT WOS:000380619600195
DA 2023-11-16
ER

PT C
AU Ghaemi, H
   Mirzaei, E
   Nouri, M
   Kheradpisheh, SR
AF Ghaemi, Hafez
   Mirzaei, Erfan
   Nouri, Mahbod
   Kheradpisheh, Saeed Reza
BE Nicosia, G
   Ojha, V
   LaMalfa, E
   LaMalfa, G
   Pardalos, P
   DiFatta, G
   Giuffrida, G
   Umeton, R
TI BioLCNet: Reward-Modulated Locally Connected Spiking Neural Networks
SO MACHINE LEARNING, OPTIMIZATION, AND DATA SCIENCE, LOD 2022, PT II
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 8th Annual International Conference on Machine Learning, Optimization
   and Data science (LOD)
CY SEP 18-22, 2022
CL ELECTR NETWORK
DE Spiking neural networks (SNN); Bio-plausible learning; Spike-timing
   dependent plasticity (STDP); Image classification
ID STDP
AB Brain-inspired computation and information processing alongside compatibility with neuromorphic hardware have made spiking neural networks (SNN) a promising method for solving learning tasks in machine learning (ML). Spiking neurons are only one of the requirements for building a bio-plausible learning model. Network architecture and learning rules are other important factors to consider when developing such artificial agents. In this work, inspired by the human visual pathway and the role of dopamine in learning, we propose a reward-modulated locally connected spiking neural network, BioLCNet, for visual learning tasks. To extract visual features from Poisson-distributed spike trains, we used local filters that are more analogous to the biological visual system compared to convolutional filters with weight sharing. In the decoding layer, we applied a spike population-based voting scheme to determine the decision of the network. We employed Spike-timing-dependent plasticity (STDP) for learning the visual features, and its reward-modulated variant (R-STDP) for training the decoder based on the reward or punishment feedback signal. For evaluation, we first assessed the robustness of our rewarding mechanism to varying target responses in a classical conditioning experiment. Afterwards, we evaluated the performance of our network on image classification tasks of MNIST and XOR MNIST datasets.
C1 [Ghaemi, Hafez] Politecn Torino, Turin, Italy.
   [Mirzaei, Erfan] Univ Tehran, Tehran, Iran.
   [Nouri, Mahbod] Univ Edinburgh, Edinburgh, Midlothian, Scotland.
   [Kheradpisheh, Saeed Reza] Shahid Beheshti Univ, Tehran, Iran.
RP Kheradpisheh, SR (corresponding author), Shahid Beheshti Univ, Tehran, Iran.
EM hafez.ghaemi@studenti.polito.it; erfunmirzaei@ut.ac.ir;
   m.nouri@sms.ed.ac.uk; skheradpisheh@sbu.ac.ir
CR Allred JM, 2016, IEEE IJCNN, P2492, DOI 10.1109/IJCNN.2016.7727509
   Bartunov S, 2018, ADV NEUR IN, V31
   Bellec G, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-17236-y
   Bing ZS, 2019, IEEE INT CONF ROBOT, P9645, DOI [10.1109/icra.2019.8793774, 10.1109/ICRA.2019.8793774]
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Carion Nicolas, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12346), P213, DOI 10.1007/978-3-030-58452-8_13
   CONNORS BW, 1990, TRENDS NEUROSCI, V13, P99, DOI 10.1016/0166-2236(90)90185-D
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Florian RV, 2007, NEURAL COMPUT, V19, P1468, DOI 10.1162/neco.2007.19.6.1468
   Frémaux N, 2016, FRONT NEURAL CIRCUIT, V9, DOI 10.3389/fncir.2015.00085
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Hazan H, 2018, FRONT NEUROINFORM, V12, DOI 10.3389/fninf.2018.00089
   HEBB D. O., 1949
   Illing B, 2019, NEURAL NETWORKS, V118, P90, DOI 10.1016/j.neunet.2019.06.001
   Izhikevich EM, 2007, CEREB CORTEX, V17, P2443, DOI 10.1093/cercor/bhl152
   Kheradpisheh SR, 2020, INT J NEURAL SYST, V30, DOI 10.1142/S0129065720500276
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Kheradpisheh SR, 2016, NEUROCOMPUTING, V205, P382, DOI 10.1016/j.neucom.2016.04.029
   LeCun Y., 2010, EMERGENCE COMPLEX LI
   LeCun Y., 1999, MNIST DATASET HANDWR
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lee C, 2019, IEEE T COGN DEV SYST, V11, P384, DOI 10.1109/TCDS.2018.2833071
   Liao QL, 2016, AAAI CONF ARTIF INTE, P1837
   Lillicrap TP, 2020, NAT REV NEUROSCI, V21, P335, DOI 10.1038/s41583-020-0277-3
   LOWEL S, 1992, SCIENCE, V255, P209, DOI 10.1126/science.1372754
   Mozafari M, 2019, PATTERN RECOGN, V94, P87, DOI 10.1016/j.patcog.2019.05.015
   Mozafari M, 2018, IEEE T NEUR NET LEAR, V29, P6178, DOI 10.1109/TNNLS.2018.2826721
   Paszke A., 2019, ADV NEURAL INFORM PR
   Poggio T, 2017, INT J AUTOM COMPUT, V14, P503, DOI 10.1007/s11633-017-1054-2
   Pogodin R., 2021, ADV NEURAL INFORM PR, V34, P13924
   Saunders D., 2020, 2020 INT JOINT C NEU, P1
   Saunders DJ, 2018, IEEE IJCNN
   Saunders DJ, 2019, NEURAL NETWORKS, V119, P332, DOI 10.1016/j.neunet.2019.08.016
   Schemmel J, 2010, IEEE INT SYMP CIRC S, P1947, DOI 10.1109/ISCAS.2010.5536970
   Schrimpf M., 2020, BIORXIV
   Schultz W, 1997, SCIENCE, V275, P1593, DOI 10.1126/science.275.5306.1593
   Sun S.-H., 2019, MULTIDIGIT MNIST FEW
   Sutton RS, 2018, ADAPT COMPUT MACH LE, P1
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Weidel P, 2021, FRONT COMPUT NEUROSC, V15, DOI 10.3389/fncom.2021.543872
NR 41
TC 0
Z9 0
U1 0
U2 0
PY 2023
VL 13811
BP 564
EP 578
DI 10.1007/978-3-031-25891-6_42
UT WOS:000995538200042
DA 2023-11-16
ER

PT C
AU Lobov, S
   Kazantsev, V
   Makarov, VA
AF Lobov, Sergey
   Kazantsev, Victor
   Makarov, Valeri A.
TI Spiking Neurons as Universal Building Blocks for Hybrid Systems
SO ADVANCED SCIENCE LETTERS
DT Proceedings Paper
CT International Conference on Control Theory and Its Application (CNTIA)
CY DEC 15-17, 2015
CL Melaka, MALAYSIA
DE Spiking Neuron; Neural Computation; Neuroanimat; Human-Machine
   Interface; Electromyography
ID NEURAL-NETWORKS
AB Spiking neural networks in-silico can closely resemble the architecture and dynamics of neural networks in-vivo and then mimic brain functions. However, their use for applied computations remains rather limited. In this work we report two successful cases of using networks of spiking neurons for controlling mobile robots. In the first case a neural network serves as a "brain" of an animat (a crocodile toy). We show that the network can learn from the environment and reproduce basic behaviors of advancing towards an object and "biting". In the second case spiking neurons are used in a human-robot interface allowing controlling a mobile robot by hand gestures. Sensory neurons detect myographic signals from a bracelet worn on a forearm. Then, the preprocessed output is classified according to hand gestures and the corresponding command is sent to the robot. Our results show that after 3-10 trials all users manage to control the robot fluently.
C1 [Lobov, Sergey; Kazantsev, Victor; Makarov, Valeri A.] Lobachevsky State Univ Nizhny Novgorod, Gagarin Ave 23, Nizhnii Novgorod 603950, Russia.
   [Makarov, Valeri A.] Univ Complutense Madrid, Appl Math Dept, Inst Matemat lnterdisciplinar, Avda Complutense S-N, E-28040 Madrid, Spain.
RP Makarov, VA (corresponding author), Lobachevsky State Univ Nizhny Novgorod, Gagarin Ave 23, Nizhnii Novgorod 603950, Russia.; Makarov, VA (corresponding author), Univ Complutense Madrid, Appl Math Dept, Inst Matemat lnterdisciplinar, Avda Complutense S-N, E-28040 Madrid, Spain.
CR Bakkum DJ, 2008, J NEURAL ENG, V5, P310, DOI 10.1088/1741-2560/5/3/004
   Bichler O, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P859, DOI 10.1109/IJCNN.2011.6033311
   DeMarse TB, 2001, AUTON ROBOT, V11, P305, DOI 10.1023/A:1012407611130
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   Goel P, 2006, LECT NOTES ARTIF INT, V4251, P825
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Kasabov N., 2012, NEURAL NETWORKS
   Loiselle S, 2005, IEEE IJCNN, P2076
   Meyer J.-A., 1991, P 1 INT C SIM AD BEH
   Morrison A, 2008, BIOL CYBERN, V98, P459, DOI 10.1007/s00422-008-0233-1
   PAUGAMMOISY H, 2009, HDB NATURAL COMPUTIN
   Pimashkin A., 2013, FRONTIERS NEURAL CIR
   Tsodyks M, 1998, NEURAL COMPUT, V10, P821, DOI 10.1162/089976698300017502
   Villacorta-Atienza JA, 2015, BIOL CYBERN, V109, P307, DOI 10.1007/s00422-015-0644-8
   Wagenaar DA, 2006, BMC NEUROSCI, V7, DOI 10.1186/1471-2202-7-11
   WANG GP, 2005, IEEE IJCNN, P416
NR 16
TC 3
Z9 4
U1 0
U2 1
PD OCT
PY 2016
VL 22
IS 10
BP 2633
EP 2637
DI 10.1166/asl.2016.7004
UT WOS:000399357900006
DA 2023-11-16
ER

PT J
AU Tapson, JC
   Cohen, GK
   Afshar, S
   Stiefel, KM
   Buskila, Y
   Wang, RM
   Hamilton, TJ
   van Schaik, A
AF Tapson, Jonathan C.
   Cohen, Greg K.
   Afshar, Saeed
   Stiefel, Klaus M.
   Buskila, Yossi
   Wang, Runchun Mark
   Hamilton, Tara J.
   van Schaik, Andre
TI Synthesis of neural networks for spatio-temporal spike pattern
   recognition and processing
SO FRONTIERS IN NEUROSCIENCE
DT Article
DE pseudoinverse solution; spatio-temporal spike pattern recognition;
   spiking network synthesis; kernel method; spike-time encoded information
ID NEURONS; COMPUTATION; MECHANISM; MODEL
AB The advent of large scale neural computational platforms has highlighted the lack of algorithms for synthesis of neural structures to perform predefined cognitive tasks. The Neural Engineering Framework (NEF) offers one such synthesis, but it is most effective for a spike rate representation of neural information, and it requires a large number of neurons to implement simple functions. We describe a neural network synthesis method that generates synaptic connectivity for neurons which process time-encoded neural signals, and which makes very sparse use of neurons. The method allows the user to specify-arbitrarily-neuronal characteristics such as axonal and dendritic delays, and synaptic transfer functions, and then solves for the optimal input-output relationship using computed dendritic weights. The method may be used for batch or online learning and has an extremely fast optimization process. We demonstrate its use in generating a network to recognize speech which is sparsely encoded as spike times.
C1 [Tapson, Jonathan C.; Cohen, Greg K.; Afshar, Saeed; Stiefel, Klaus M.; Buskila, Yossi; Wang, Runchun Mark; Hamilton, Tara J.; van Schaik, Andre] Univ Western Sydney, MARCS Inst, Kingswood, NSW, Australia.
RP Tapson, JC (corresponding author), Univ Western Sydney, Sch Comp Engn Math, Locked Bag 1797, Penrith, NSW 2751, Australia.
EM j.tapson@uws.edu.au
CR [Anonymous], 2013, T146 SPOKEN DIGITS C
   [Anonymous], 2006, P IEEE C ENG MED BIO, DOI [DOI 10.1109/IEMBS.2006.260925, 10.1109/IEMBS.2006.260925]
   Basu A, 2013, NEUROCOMPUTING, V102, P125, DOI 10.1016/j.neucom.2012.01.042
   Chechik G, 1999, NEURAL COMPUT, V11, P2061, DOI 10.1162/089976699300016089
   Choudhary S., 2012, INT C ART NEUR NETW
   Conradt J., 2012, P IEEE BIOM CIRC SYS, P91
   Drachman DA, 2005, NEUROLOGY, V64, P2004, DOI 10.1212/01.WNL.0000166914.38327.BB
   Eliasmith C., 2003, NEURAL ENG COMPUTATI
   Eliasmith C, 2012, SCIENCE, V338, P1202, DOI 10.1126/science.1225266
   Galluppi F., 2012, P INT JOINT C NEUR N
   Gütig R, 2009, PLOS BIOL, V7, DOI 10.1371/journal.pbio.1000141
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Häusser M, 2000, SCIENCE, V290, P739, DOI 10.1126/science.290.5492.739
   Hopfield IJ, 2000, P NATL ACAD SCI USA, V97, P13919, DOI 10.1073/pnas.250483697
   Hopfield J. J., 2013, MUS SILICIUM COMPETI
   Hopfield JJ, 2001, P NATL ACAD SCI USA, V98, P1282, DOI 10.1073/pnas.031567098
   Huang GB, 2008, NEUROCOMPUTING, V71, P3460, DOI 10.1016/j.neucom.2007.10.008
   Huang GB, 2006, NEUROCOMPUTING, V70, P489, DOI 10.1016/j.neucom.2005.12.126
   Huang GB, 2006, IEEE T NEURAL NETWOR, V17, P879, DOI 10.1109/TNN.2006.875977
   Hussain S, 2012, 2012 IEEE ASIA PACIFIC CONFERENCE ON CIRCUITS AND SYSTEMS (APCCAS), P304, DOI 10.1109/APCCAS.2012.6419032
   Indiveri G, 2001, ANALOG INTEGR CIRC S, V28, P279, DOI 10.1023/A:1011208127849
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Jaeger H., 2003, ADV NEURAL INFORM PR, V15, P609
   KAY SM, 1981, P IEEE, V69, P1380, DOI 10.1109/PROC.1981.12184
   Khan MM, 2008, IEEE IJCNN, P2849, DOI 10.1109/IJCNN.2008.4634199
   Levy WB, 1996, NEURAL COMPUT, V8, P531, DOI 10.1162/neco.1996.8.3.531
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Maass W, 2000, NEURAL COMPUT, V12, P1743, DOI 10.1162/089976600300015123
   Maass W, 2004, J COMPUT SYST SCI, V69, P593, DOI 10.1016/j jcss.2004.04.001
   Masquelier T, 2008, PLOS ONE, V3, DOI 10.1371/journal.pone.0001377
   Masquelier T, 2009, NEURAL COMPUT, V21, P1259, DOI 10.1162/neco.2008.06-08-804
   Masuda N, 2003, NEURAL COMPUT, V15, P103, DOI 10.1162/089976603321043711
   Mohemmed A, 2012, INT J NEURAL SYST, V22, DOI 10.1142/S0129065712500128
   Paolicelli RC, 2011, SCIENCE, V333, P1456, DOI 10.1126/science.1202529
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Rahimi A., 2008, ADV NEURAL INFORM PR, V2008, P1313
   Russell A, 2010, IEEE T NEURAL NETWOR, V21, P1950, DOI 10.1109/TNN.2010.2083685
   Saxe A. M., 2011, INT C MACH LEARN
   Schemmel J, 2010, IEEE INT SYMP CIRC S, P1947, DOI 10.1109/ISCAS.2010.5536970
   Tapson J, 2013, NEURAL NETWORKS, V45, P94, DOI 10.1016/j.neunet.2013.02.008
   Tapson J, 2009, NEURAL COMPUT, V21, P1554, DOI 10.1162/neco.2009.06-07-548
   Torben-Nielsen B, 2010, FRONT COMPUT NEUROSC, V4, DOI 10.3389/fncom.2010.00128
   Van Rullen R, 2001, NEURAL COMPUT, V13, P1255, DOI 10.1162/08997660152002852
   Wills S. A., 2001, RECOGNISING SPEECH B
   WILLS SA, 2004, THESIS U CAMBRIDGE
   Zamarreño-Ramos C, 2013, IEEE T BIOMED CIRC S, V7, P82, DOI 10.1109/TBCAS.2012.2195725
NR 46
TC 46
Z9 46
U1 1
U2 13
PY 2013
VL 7
AR 153
DI 10.3389/fnins.2013.00153
UT WOS:000346567300152
DA 2023-11-16
ER

PT C
AU Oniz, Y
   Kaynak, O
AF Oniz, Yesim
   Kaynak, Okyay
GP IEEE
TI Wheel Slip Regulation Using Fuzzy Spiking Neural Networks
SO 2016 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUL 24-29, 2016
CL Vancouver, CANADA
AB In this paper, a fuzzy spiking neural network structure is developed for the wheel slip regulation problem of an Antilock Braking System. Sliding mode control theory is utilized in the derivation of the update rules for the neural network's weights as well as the parameters of the fuzzy membership functions. Gaussian membership functions are used to convert the sensor readings into the neural networks inputs and the spike response model is employed to denote the effect of the incoming spikes on the postsynaptic membrane potential. The use of the Lyapunov stability method for the derivation of the parameter update rules leads to a stable system response even in the existence of external disturbances.
C1 [Oniz, Yesim] Istanbul Bilgi Univ, Dept Mechatron Engn, Istanbul, Turkey.
   [Kaynak, Okyay] Bogazici Univ, Dept Elect & Elect Engn, Istanbul, Turkey.
   [Kaynak, Okyay] Harbin Inst Technol, Harbin, Peoples R China.
RP Oniz, Y (corresponding author), Istanbul Bilgi Univ, Dept Mechatron Engn, Istanbul, Turkey.
EM yesim.oniz@bilgi.edu.tr; okyay.kaynak@boun.edu.tr
CR Abiyev R., 2012, P 2012 IEEE ASME INT, P1030
   [Anonymous], 1991, APPL NONLINEAR CONTR
   Batllori R, 2011, PROCEDIA COMPUT SCI, V6, DOI 10.1016/j.procs.2011.08.060
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Bouganis A., 2010, P 2010 INT JOINT C N, P1, DOI DOI 10.1109/IJCNN.2010.5596525
   Gamez D, 2012, BIOINSPIR BIOMIM, V7, DOI 10.1088/1748-3182/7/2/025008
   Ghosh-Dastidar S, 2007, INTEGR COMPUT-AID E, V14, P187
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Johnson C, 2009, NEURAL NETWORKS, V22, P833, DOI 10.1016/j.neunet.2009.06.033
   Juang CF, 2002, IEEE T FUZZY SYST, V10, P155, DOI 10.1109/91.995118
   Kasabov N., 2012, EVOLVING SPIKING NEU
   Kayacan E, 2012, IEEE T IND ELECTRON, V59, P3510, DOI 10.1109/TIE.2011.2182017
   Lian RJ, 2012, EXPERT SYST APPL, V39, P1545, DOI 10.1016/j.eswa.2011.08.052
   LIN CT, 1995, FUZZY SET SYST, V70, P183, DOI 10.1016/0165-0114(94)00216-T
   Lin D, 2010, FUZZY SET SYST, V161, P2066, DOI 10.1016/j.fss.2010.03.006
   Maass W, 1997, NEURAL COMPUT, V9, P279, DOI 10.1162/neco.1997.9.2.279
   Olsson H., 1996, THESIS
   Oniz Y, 2014, J FRANKLIN I, V351, P3269, DOI 10.1016/j.jfranklin.2014.03.002
   Oniz Y, 2009, IEEE T SYST MAN CY B, V39, P551, DOI 10.1109/TSMCB.2008.2007966
   Sadati N, 2008, NEUROCOMPUTING, V71, P2702, DOI 10.1016/j.neucom.2007.06.019
   Schrauwen B, 2003, IEEE IJCNN, P2825
   Shin JH, 2010, IEEE T NEURAL NETWOR, V21, P1697, DOI 10.1109/TNN.2010.2050600
   Utkin V. I., 1999, SLIDING MODE CONTROL, V9
   VanRullen R, 2005, TRENDS NEUROSCI, V28, P1, DOI 10.1016/j.tins.2004.10.010
   Zak S. H., 2003, SYSTEMS AND CONTROL, V388
NR 25
TC 0
Z9 0
U1 0
U2 0
PY 2016
BP 1029
EP 1036
UT WOS:000399925501028
DA 2023-11-16
ER

PT S
AU Zhang, GX
   Pérez-Jiménez, MJ
   Gheorghe, M
AF Zhang, Gexiang
   Perez-Jimenez, Mario J.
   Gheorghe, Marian
BA Zhang, G
   PerezJimenez, MJ
   Gheorghe, M
BF Zhang, G
   PerezJimenez, MJ
   Gheorghe, M
TI Electric Power System Fault Diagnosis with Membrane Systems
SO REAL-LIFE APPLICATIONS WITH MEMBRANE COMPUTING
SE Emergence Complexity and Computation
DT Article; Book Chapter
ID NEURAL P SYSTEMS; SECTION ESTIMATION; EXPERT-SYSTEM; SPIKING; NETWORK
AB Spiking Neural P systems (SN P systems, for short) are used in electric power systems fault diagnostics, by expanding their modeling capabilities with fuzzy theory concepts. The following variants of SN P systems are introduced and investigated: fuzzy reasoning spiking neural P systems with real numbers, weighted fuzzy reasoning spiking neural P systems and fuzzy reasoning spiking neural P systems with trapezoidal fuzzy numbers.
C1 [Zhang, Gexiang] Xihua Univ, Minist Educ, Key Lab Fluid & Power Machinery, Chengdu, Sichuan, Peoples R China.
   [Zhang, Gexiang] Xihua Univ, Robot Res Ctr, Chengdu, Sichuan, Peoples R China.
   [Perez-Jimenez, Mario J.] Univ Seville, Dept Comp Sci & Artificial Intelligence, Seville, Spain.
   [Gheorghe, Marian] Univ Bradford, Sch Elect Engn & Comp Sci, Bradford, W Yorkshire, England.
RP Zhang, GX (corresponding author), Xihua Univ, Minist Educ, Key Lab Fluid & Power Machinery, Chengdu, Sichuan, Peoples R China.; Zhang, GX (corresponding author), Xihua Univ, Robot Res Ctr, Chengdu, Sichuan, Peoples R China.
CR [Anonymous], 2009, P 2009 IEEE POW EN S
   Cardoso G, 2008, IEEE T POWER DELIVER, V23, P1335, DOI 10.1109/TPWRD.2008.916743
   Cavaliere M, 2009, THEOR COMPUT SCI, V410, P2352, DOI 10.1016/j.tcs.2009.02.031
   Chang CS, 1997, IEE P-GENER TRANSM D, V144, P406, DOI 10.1049/ip-gtd:19971278
   Chen H., 2006, ROM J INF SCI TECH, V9, P151
   Chen SM, 1996, IEEE T SYST MAN CY B, V26, P769, DOI 10.1109/3477.537318
   Chen WH, 2012, IEEE T POWER DELIVER, V27, P688, DOI 10.1109/TPWRD.2011.2178079
   Chen WH, 2011, IEEE T POWER DELIVER, V26, P963, DOI 10.1109/TPWRD.2010.2093585
   Chen WH, 2011, IEEE T POWER DELIVER, V26, P205, DOI 10.1109/TPWRD.2010.2061873
   Chien CF, 2002, IEEE T POWER DELIVER, V17, P785, DOI 10.1109/TPWRD.2002.1022804
   Davidson EM, 2006, IEEE T POWER SYST, V21, P559, DOI 10.1109/TPWRS.2006.873109
   Freund R, 2008, INT J FOUND COMPUT S, V19, P1223, DOI 10.1142/S0129054108006248
   Grzegorzewski P, 2005, FUZZY SET SYST, V153, P115, DOI 10.1016/j.fss.2004.02.015
   Guo WX, 2010, IEEE T POWER DELIVER, V25, P1393, DOI 10.1109/TPWRD.2010.2048344
   Hossack JA, 2003, IEEE T POWER SYST, V18, P639, DOI 10.1109/TPWRS.2003.810910
   Ionescu M, 2006, FUND INFORM, V71, P279
   Lee HJ, 2000, IEEE T POWER DELIVER, V15, P92, DOI 10.1109/61.847234
   LIN Sheng, 2010, POWER SYST PROTECT C, V38, P140
   Lin XN, 2010, IEEE T POWER DELIVER, V25, P1268, DOI 10.1109/TPWRD.2010.2044590
   Liu HC, 2013, IEEE T CYBERNETICS, V43, P1059, DOI 10.1109/TSMCB.2012.2223671
   Luo X, 2008, IEEE T POWER DELIVER, V23, P676, DOI 10.1109/TPWRD.2008.915809
   Ma DY, 2013, ENG APPL ARTIF INTEL, V26, P937, DOI 10.1016/j.engappai.2012.03.017
   Marks II R. J., 1994, IEEE IEEE TECHNOLOGY
   Pan LQ, 2010, THEOR COMPUT SCI, V411, P906, DOI 10.1016/j.tcs.2009.11.010
   Paun G, 2006, INT J FOUND COMPUT S, V17, P975, DOI 10.1142/S0129054106004212
   Peng H, 2013, INFORM SCIENCES, V235, P106, DOI 10.1016/j.ins.2012.07.015
   Shyi-Ming Chen, 1990, IEEE Transactions on Knowledge and Data Engineering, V2, P311, DOI 10.1109/69.60794
   Sun J, 2004, IEEE T POWER SYST, V19, P2053, DOI 10.1109/TPWRS.2004.836256
   Tang L., 2003, P CSEE, V23
   Tao Wang, 2010, 2010 IEEE Fifth International Conference on Bio-Inspired Computing: Theories and Applications (BIC-TA), P586, DOI 10.1109/BICTA.2010.5645191
   Thukaram D, 2005, IEEE T POWER DELIVER, V20, P710, DOI 10.1109/TPWRD.2005.844307
   Tu M, 2014, CHINESE J ELECTRON, V23, P87
   Wang J, 2013, IEEE T FUZZY SYST, V21, P209, DOI 10.1109/TFUZZ.2012.2208974
   Wang T, 2015, INT J COMPUT COMMUN, V10, P904
   Wang T, 2014, INT J COMPUT COMMUN, V9, P786, DOI 10.15837/ijccc.2014.6.1485
   WANG T., 2011, P WCICA CHIN, P255
   Wang T, 2015, J COMPUT THEOR NANOS, V12, P1103, DOI 10.1166/jctn.2015.3857
   Wang T, 2014, LECT NOTES COMPUT SC, V8961, P385, DOI 10.1007/978-3-319-14370-5_24
   Wang T, 2015, IEEE T POWER SYST, V30, P1182, DOI 10.1109/TPWRS.2014.2347699
   Wen FS, 1995, ELECTR POW SYST RES, V34, P165, DOI 10.1016/0378-7796(95)00974-6
   [吴双 WU Shuang], 2011, [电网技术, Power System Technology], V35, P79
   Xiong GJ, 2013, MATH PROBL ENG, V2013, DOI 10.1155/2013/815352
   Yang Jianwei, 2010, Proceedings of the CSEE, V30, P42
   Zhang GX, 2014, INT J NEURAL SYST, V24, DOI 10.1142/S0129065714400061
   Zhu YL, 2006, IEEE T POWER DELIVER, V21, P634, DOI 10.1109/TPWRD.2005.858774
NR 45
TC 0
Z9 0
U1 0
U2 4
PY 2017
VL 25
BP 159
EP 212
DI 10.1007/978-3-319-55989-6_5
D2 10.1007/978-3-319-55989-6
UT WOS:000418033500006
DA 2023-11-16
ER

PT C
AU Soures, N
   Hays, L
   Bohannon, E
   Zyarah, AM
   Kudithipudi, D
AF Soures, Nicholas
   Hays, Lydia
   Bohannon, Eric
   Zyarah, Abdullah M.
   Kudithipudi, Dhireesha
GP IEEE
TI On-Device STDP and Synaptic Normalization for Neuromemristive Spiking
   Neural Network
SO 2017 IEEE 60TH INTERNATIONAL MIDWEST SYMPOSIUM ON CIRCUITS AND SYSTEMS
   (MWSCAS)
SE Midwest Symposium on Circuits and Systems Conference Proceedings
DT Proceedings Paper
CT 60th IEEE International Midwest Symposium on Circuits and Systems
   (MWSCAS)
CY AUG 06-09, 2017
CL Tufts Univ, Medford Somerville Campus, Boston, MA
HO Tufts Univ, Medford Somerville Campus
DE Synaptic Normalization; Memristor; Neuromemristive Systems; Spiking
   Neural Network; STDP
ID NEURONS
AB Spiking Neural Networks offer low precision communication, robustness, and low power consumption and are attractive for autonomous applications. One of the well accepted learning rules for these networks is spike time dependent plasticity which is governed by the pre- and postsynaptic spike timings. To stabilize the plasticity and avoid saturation in these learning rules, synaptic normalization is used. In this work, we propose a circuit to efficiently realize synaptic normalization in a neuromemristive system and show how it improves the plasticity for unsupervised on-device learning. High-level modeling shows the efficacy of synaptic normalization in pattern recognition and feature extraction.
C1 [Soures, Nicholas; Hays, Lydia; Bohannon, Eric; Zyarah, Abdullah M.; Kudithipudi, Dhireesha] Rochester Inst Technol, NanoComp Res Lab, Rochester, NY 14623 USA.
RP Soures, N (corresponding author), Rochester Inst Technol, NanoComp Res Lab, Rochester, NY 14623 USA.
CR Afshar S, 2015, IEEE T BIOMED CIRC S, V9, P188, DOI 10.1109/TBCAS.2015.2416391
   [Anonymous], 2013, 2013 INT JOINT C NEU, DOI DOI 10.1109/IJCNN.2013.6706961
   Bi GQ, 2001, ANNU REV NEUROSCI, V24, P139, DOI 10.1146/annurev.neuro.24.1.139
   Bofill-i-Petit A, 2004, IEEE T NEURAL NETWOR, V15, P1296, DOI 10.1109/TNN.2004.832842
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Du ZD, 2015, PROCEEDINGS OF THE 48TH ANNUAL IEEE/ACM INTERNATIONAL SYMPOSIUM ON MICROARCHITECTURE (MICRO-48), P494, DOI 10.1145/2830772.2830789
   Jin YYZ, 2016, IEEE INT SYMP NANO, P55, DOI 10.1145/2950067.2950100
   Kim S, 2011, 2011 11TH INTERNATIONAL CONFERENCE ON CONTROL, AUTOMATION AND SYSTEMS (ICCAS), P1
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   McKinstry JL, 2013, FRONT NEUROROBOTICS, V7, DOI 10.3389/fnbot.2013.00010
   Qiao N, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00141
   Serrano-Gotarredona T, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00002
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Soures N., 2017, NEUR NETW IJCNN 201
   Zyarah A., 2017, CIRC SYST ISCAS 2017
NR 15
TC 2
Z9 2
U1 0
U2 1
PY 2017
BP 1081
EP 1084
UT WOS:000424694700271
DA 2023-11-16
ER

PT J
AU Tavanaei, A
   Maida, AS
AF Tavanaei, Amirhossein
   Maida, Anthony S.
TI A spiking network that learns to extract spike signatures from speech
   signals
SO NEUROCOMPUTING
DT Article
DE Spiking neural networks; STDP; Speech recognition; Neural model; Spike
   signatures; Speech signal coding
ID ISOLATED WORD RECOGNITION; DYNAMIC SYNAPSES; NEURONS; MODEL
AB Spiking neural networks (SNNs) with adaptive synapses reflect core properties of biological neural networks. Speech recognition, as an application involving audio coding and dynamic learning, provides a good test problem to study SNN functionality. We present a simple, novel, and efficient nonrecurrent SNN that learns to convert a speech signal into a spike train signature. The signature is distinguishable from signatures for other speech signals representing different words, thereby enabling digit recognition and discrimination in devices that use only spiking neurons. The method uses a small, nonrecurrent SNN consisting of Izhikevich neurons equipped with spike timing dependent plasticity (STDP) and biologically realistic synapses. This approach introduces an efficient and fast network without error-feedback training, although it does require supervised training. The new simulation results produce discriminative spike train patterns for spoken digits in which highly correlated spike trains belong to the same category and low correlated patterns belong to different categories. The proposed SNN is evaluated using a spoken digit recognition task where a subset of the Aurora speech dataset is used. The experimental results show that the network performs well in terms of accuracy rate and complexity. (C) 2017 Elsevier B.V. All rights reserved.
C1 [Tavanaei, Amirhossein; Maida, Anthony S.] Univ Louisiana Lafayette, Ctr Adv Comp Studies, 301 E Lewis, Lafayette, LA 70503 USA.
RP Tavanaei, A (corresponding author), Univ Louisiana Lafayette, Ctr Adv Comp Studies, 301 E Lewis, Lafayette, LA 70503 USA.
EM tavanaei@louisiana.edu
CR BIENENSTOCK EL, 1982, J NEUROSCI, V2, P32, DOI 10.1523/jneurosci.02-01-00032.1982
   Buck J. R, 1999, DISCRETE TIME SIGNAL, V2nd, P594
   Caporale N, 2008, ANNU REV NEUROSCI, V31, P25, DOI 10.1146/annurev.neuro.31.060407.125639
   Dao M, 2014, CONF REC ASILOMAR C, P106, DOI 10.1109/ACSSC.2014.7094407
   Dibazar AA, 2004, IEEE IJCNN, P3071, DOI 10.1109/IJCNN.2004.4620175
   Dibazar AA, 2003, IEEE IJCNN, P3146
   DODDINGTON GR, 1981, IEEE SPECTRUM, V18, P26, DOI 10.1109/MSPEC.1981.6369809
   Ghani A, 2008, LECT NOTES COMPUT SC, V5163, P513, DOI 10.1007/978-3-540-87536-9_53
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Kasabov N, 2013, NEURAL NETWORKS, V41, P188, DOI 10.1016/j.neunet.2012.11.014
   Kasinski A., 2006, International Journal of Applied Mathematics and Computer Science, V16, P101
   Lukosevicius M, 2009, COMPUT SCI REV, V3, P127, DOI 10.1016/j.cosrev.2009.03.005
   Markram Henry, 2011, Front Synaptic Neurosci, V3, P4, DOI 10.3389/fnsyn.2011.00004
   Masquelier T, 2007, PLOS COMPUT BIOL, V3, P247, DOI 10.1371/journal.pcbi.0030031
   Näger C, 2002, NEUROCOMPUTING, V44, P937, DOI 10.1016/S0925-2312(02)00494-0
   Namarvar HH, 2001, IEEE IJCNN, P2985, DOI 10.1109/IJCNN.2001.938853
   Panchev C, 2004, NEUROCOMPUTING, V58, P365, DOI 10.1016/j.neucom.2004.01.068
   Pearce D., 2000, P ASR2000 AUT SPEECH, P181
   Schafer PB, 2014, NEURAL COMPUT, V26, P523, DOI 10.1162/NECO_a_00557
   Schrauwen B, 2007, LECT NOTES COMPUT SC, V4668, P471
   Skowronski MD, 2007, NEURAL NETWORKS, V20, P414, DOI 10.1016/j.neunet.2007.04.006
   Storck J, 2001, NEURAL NETWORKS, V14, P275, DOI 10.1016/S0893-6080(00)00101-5
   Tavanaei A., 2011, 2011 International Symposium on Artificial Intelligence and Signal Processing (AISP), P138, DOI 10.1109/AISP.2011.5960989
   Tavanaei A, 2018, J SIGNAL PROCESS SYS, V90, P211, DOI 10.1007/s11265-016-1153-2
   Tavanaei A, 2016, IEEE IJCNN, P307, DOI 10.1109/IJCNN.2016.7727213
   Verstraeten D, 2005, INFORM PROCESS LETT, V95, P521, DOI 10.1016/j.ipl.2005.05.019
   Verstraeten D, 2006, IEEE IJCNN, P1050
   Victor JD, 2005, CURR OPIN NEUROBIOL, V15, P585, DOI 10.1016/j.conb.2005.08.002
   Victor JD, 1997, NETWORK-COMP NEURAL, V8, P127, DOI 10.1088/0954-898X/8/2/003
   Wade JJ, 2010, IEEE T NEURAL NETWOR, V21, P1817, DOI 10.1109/TNN.2010.2074212
   Wang JL, 2014, NEUROCOMPUTING, V144, P526, DOI 10.1016/j.neucom.2014.04.017
   Wysoski SG, 2008, NEUROCOMPUTING, V71, P2563, DOI 10.1016/j.neucom.2007.12.038
   Wysoski SG, 2010, NEURAL NETWORKS, V23, P819, DOI 10.1016/j.neunet.2010.04.009
   Zhang Y, 2015, IEEE T NEUR NET LEAR, V26, P2635, DOI 10.1109/TNNLS.2015.2388544
NR 34
TC 35
Z9 36
U1 1
U2 31
PD MAY 31
PY 2017
VL 240
BP 191
EP 199
DI 10.1016/j.neucom.2017.01.088
UT WOS:000397692500016
DA 2023-11-16
ER

PT C
AU Lew, D
   Park, J
AF Lew, Dongwoo
   Park, Jongsun
GP IEEE
TI Early Image Termination Technique During STDP Training of Spiking Neural
   Network
SO 2020 17TH INTERNATIONAL SOC DESIGN CONFERENCE (ISOCC 2020)
SE International SoC Design Conference
DT Proceedings Paper
CT 17th International SoC Design Conference (ISOCC)
CY OCT 21-24, 2020
CL Yeosu, SOUTH KOREA
DE Spking Neural Network (SNN); Spike Timing Dependant Plasticity (STDP)
AB Spiking Neural Network (SNN) is a breed of neural networks that seek to achieve low energy and power by more closely mimicking biological brains. SNNs are often trained using lightweight unsupervised learning such as Spike Time Dependent Plasticity (STDP). However, STDP is prone to redundant time steps during training since STDP cannot determine current image needs further training or not. To reduce redundant time steps and lower energy costs during STDP training, we propose a novel technique that terminates training upon an image preemptively. The proposed technique reduces time steps by 44% with accuracy drop of 0.91% on MNIST.
C1 [Lew, Dongwoo; Park, Jongsun] Korea Univ, Sch Elect Engn, Seoul, South Korea.
RP Lew, D (corresponding author), Korea Univ, Sch Elect Engn, Seoul, South Korea.
CR Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
NR 3
TC 1
Z9 1
U1 0
U2 1
PY 2020
BP 79
EP 80
DI 10.1109/ISOCC50952.2020.9333081
UT WOS:000680824100039
DA 2023-11-16
ER

PT C
AU Vincan, V
   Zoranovic, J
   Samardzic, N
   Dautovic, S
AF Vincan, Vladimir
   Zoranovic, Jovana
   Samardzic, Natasa
   Dautovic, Stanisa
GP IEEE
TI All-memristive Spiking Neural Network Circuit Simulator
SO 2022 11TH INTERNATIONAL CONFERENCE ON MODERN CIRCUITS AND SYSTEMS
   TECHNOLOGIES (MOCAST)
DT Proceedings Paper
CT 11th International Conference on Modern Circuits and Systems
   Technologies (MOCAST)
CY JUN 08-10, 2022
CL Bremen, GERMANY
DE spiking neural networks; memristors; volatility; LTspice; Simscape
ID SPICE MODEL
AB In this paper we present a circuit-level simulation test bed for an all-memristive spiking neural network (MSNN), composed of synapses and leaky integrate-and-fire (LIF) neuron circuits. As recently proposed, an all-memristive neural network can be designed using volatile diffusion memristors as part of the LW neuron, and non-volatile drift memristors as synaptic elements. The cognitive performances of our MSNN are demonstrated by the implementation of the spike timing dependent plasticity (STDP) learning rule. Starting from a circuit-level memristive neuron model which incorporates volatility, and a synaptic memristive array, a simple MSNN circuit simulator is designed and its performances are discussed.
C1 [Vincan, Vladimir; Zoranovic, Jovana; Samardzic, Natasa; Dautovic, Stanisa] Univ Novi Sad, Fac Tech Sci, Dept Power Elect & Telecommun Engn, Novi Sad, Serbia.
RP Vincan, V (corresponding author), Univ Novi Sad, Fac Tech Sci, Dept Power Elect & Telecommun Engn, Novi Sad, Serbia.
EM vladimirvincan@uns.ac.rs; jzoranovic@uns.ac.rs; nsamardzic@uns.ac.rs;
   dautovic@uns.ac.rs
CR Ascoli A, 2021, FRONT NEUROSCI-SWITZ, V15, DOI 10.3389/fnins.2021.651452
   Berdan R, 2014, IEEE ELECTR DEVICE L, V35, P135, DOI 10.1109/LED.2013.2291158
   Biolek Z, 2009, RADIOENGINEERING, V18, P210
   Schuman CD, 2017, Arxiv, DOI arXiv:1705.06963
   Dautovic S., 2021, P 28 IEEE INT C ELEC, DOI [10.1109/ICECS53924.2021.9665488, DOI 10.1109/ICECS53924.2021.9665488]
   Krestinskaya O, 2020, IEEE T NEUR NET LEAR, V31, P4, DOI 10.1109/TNNLS.2019.2899262
   Li QJ, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0120506
   Pantazi A, 2016, NANOTECHNOLOGY, V27, DOI 10.1088/0957-4484/27/35/355205
   Samardzic NM, 2022, ELECTRONICS-SWITZ, V11, DOI 10.3390/electronics11060894
   Sung C, 2018, J APPL PHYS, V124, DOI 10.1063/1.5037835
   Vincan V., LTSPICE SIMULATIONS
   Wang ZR, 2018, NAT ELECTRON, V1, P137, DOI 10.1038/s41928-018-0023-2
   Yu SM, 2018, P IEEE, V106, P260, DOI 10.1109/JPROC.2018.2790840
NR 13
TC 0
Z9 0
U1 4
U2 7
PY 2022
DI 10.1109/MOCAST54814.2022.9837753
UT WOS:000853078900077
DA 2023-11-16
ER

PT J
AU Long, LF
   Lugu, RK
   Xiong, X
   Liu, Q
   Peng, H
   Wang, J
   Orellana-Martín, D
   Pérez-Jiménez, MJ
AF Long, Lifan
   Lugu, Rikong
   Xiong, Xin
   Liu, Qian
   Peng, Hong
   Wang, Jun
   Orellana-Martin, David
   Perez-Jimenez, Mario J.
TI Echo spiking neural P systems
SO KNOWLEDGE-BASED SYSTEMS
DT Article
DE Nonlinear spiking neural P systems; Echo state network; Echo spiking
   neural P systems; Time-series forecasting
ID STATE NETWORKS
AB Nonlinear spiking neural P (NSNP) systems are distributed parallel neural-like computing models that abstract the nonlinear spiking mechanisms of biological neurons. Echo state network (ESN) is a new type of recurrent neural network (RNN) that can overcome the disadvantages of traditional RNN. Inspired by the structure of ESN, this study proposes a new variant of NSNP systems, called echo spiking neural P (ESNP) systems, or the ESNP model. An ESNP system is essentially a specialized NSNP system equipped with an input layer and a readout layer. The ESNP system can also be viewed as a variant of ESN, where a specialized NSNP system is used as its reservoir. Owing to the use of spiking neurons, the state equation of ESNP model is different from the state equation of the ESN model. The proposed ESNP model is a recurrent-like model, and we used time-series forecasting as a case study to prove its capabilities. Experimental results demonstrate the effectiveness of the proposed ESNP model for time-series forecasting. (C) 2022 Elsevier B.V. All rights reserved.
C1 [Long, Lifan; Lugu, Rikong; Xiong, Xin; Liu, Qian; Peng, Hong] Xihua Univ, Sch Comp & Software Engn, Chengdu 610039, Peoples R China.
   [Wang, Jun] Xihua Univ, Sch Elect Engn & Elect Informat, Chengdu 610039, Peoples R China.
   [Orellana-Martin, David; Perez-Jimenez, Mario J.] Univ Seville, Dept Comp Sci & Artificial Intelligence, Res Grp Nat Comp, Seville 41012, Spain.
RP Peng, H (corresponding author), Xihua Univ, Sch Comp & Software Engn, Chengdu 610039, Peoples R China.
EM ph.xhu@hotmail.com
CR Bai SJ, 2018, Arxiv, DOI [arXiv:1803.01271, DOI 10.48550/ARXIV.1803.01271]
   Cabarle FGC, 2021, INFORM COMPUT, V281, DOI 10.1016/j.ic.2021.104766
   Cabarle FGC, 2015, NEURAL COMPUT APPL, V26, P1905, DOI 10.1007/s00521-015-1857-4
   Cai YL, 2022, INFORM SCIENCES, V587, P473, DOI 10.1016/j.ins.2021.12.058
   de la Cruz RTA, 2021, J MEMBRANE COMPUT, V3, P10, DOI 10.1007/s41965-020-00067-7
   de la Cruz RTA, 2019, J MEMBRANE COMPUT, V1, P161, DOI 10.1007/s41965-019-00021-2
   Hu J, 2020, NEUROCOMPUTING, V383, P122, DOI 10.1016/j.neucom.2019.11.060
   Ionescu M, 2006, FUND INFORM, V71, P279
   Jaeger H, 2004, SCIENCE, V304, P78, DOI 10.1126/science.1091277
   Jaeger H, 2001, 14834 GMD GERM NAT R, V148, P34
   Jaegera H, 2007, NEURAL NETWORKS, V20, P335, DOI 10.1016/j.neunet.2007.04.016
   JANG JSR, 1993, IEEE T SYST MAN CYB, V23, P665, DOI 10.1109/21.256541
   Li B, 2021, SIGNAL PROCESS, V178, DOI 10.1016/j.sigpro.2020.107793
   Li B, 2020, KNOWL-BASED SYST, V196, DOI 10.1016/j.knosys.2020.105794
   Liu PH, 2020, KNOWL-BASED SYST, V203, DOI 10.1016/j.knosys.2020.106081
   Liu Q, 2023, IEEE T NEUR NET LEAR, V34, P6227, DOI 10.1109/TNNLS.2021.3134792
   Liu Q, 2022, KNOWL-BASED SYST, V235, DOI 10.1016/j.knosys.2021.107656
   Lukoevieius M., 2007, TECHNICAL REPORT
   Lv ZQ, 2021, J MEMBRANE COMPUT, V3, P270, DOI 10.1007/s41965-021-00089-9
   Orellana-Martín D, 2021, INT J NEURAL SYST, V31, DOI 10.1142/S0129065720500719
   Pan LQ, 2009, INT J COMPUT COMMUN, V4, P273, DOI 10.15837/ijccc.2009.3.2435
   Paum G, 2007, J UNIVERS COMPUT SCI, V13, P1707
   Päun G, 2000, J COMPUT SYST SCI, V61, P108, DOI 10.1006/jcss.1999.1693
   Paun Gh, 2010, OXFORD HDB MEMBRANE
   Peng H, 2021, COMPUT VIS IMAGE UND, V210, DOI 10.1016/j.cviu.2021.103228
   Peng H, 2020, NEURAL NETWORKS, V127, P110, DOI 10.1016/j.neunet.2020.04.014
   Peng H, 2020, INT J NEURAL SYST, V30, DOI 10.1142/S0129065720500082
   Peng H, 2020, KNOWL-BASED SYST, V188, DOI 10.1016/j.knosys.2019.105064
   Peng H, 2019, KNOWL-BASED SYST, V163, P875, DOI 10.1016/j.knosys.2018.10.016
   Peng H, 2019, IEEE T NEUR NET LEAR, V30, P1672, DOI 10.1109/TNNLS.2018.2872999
   Peng H, 2018, IEEE T SMART GRID, V9, P4777, DOI 10.1109/TSG.2017.2670602
   Peng H, 2017, NEURAL NETWORKS, V95, P66, DOI 10.1016/j.neunet.2017.08.003
   Perez-Hurtado I, 2022, INFORM SCIENCES, V587, P1, DOI 10.1016/j.ins.2021.12.003
   Pérez-Hurtado I, 2019, J MEMBRANE COMPUT, V1, P93, DOI 10.1007/s41965-019-00014-1
   Song XX, 2021, INFORM SCIENCES, V570, P383, DOI 10.1016/j.ins.2021.04.051
   Song XX, 2021, INT J NEURAL SYST, V31, DOI 10.1142/S0129065720500422
   Sun XC, 2017, KNOWL-BASED SYST, V130, P17, DOI 10.1016/j.knosys.2017.05.022
   Verlan S, 2020, J MEMBRANE COMPUT, V2, P355, DOI 10.1007/s41965-020-00050-2
   Wang B, 2020, NEUROCOMPUTING, V397, P11, DOI 10.1016/j.neucom.2020.01.111
   Wang T, 2015, IEEE T POWER SYST, V30, P1182, DOI 10.1109/TPWRS.2014.2347699
   Wu TF, 2018, IEEE T NEUR NET LEAR, V29, P3349, DOI 10.1109/TNNLS.2017.2726119
   Wu Z, 2022, IEEE T NEUR NET LEAR, V33, P1974, DOI 10.1109/TNNLS.2021.3098866
   Yang SC, 2018, IEEE T FUZZY SYST, V26, P3391, DOI 10.1109/TFUZZ.2018.2831640
   Yusof MH, 2016, INFORM SCIENCES, V364, P184, DOI 10.1016/j.ins.2015.11.017
   Zhang GX, 2020, ACM COMPUT SURV, V53, DOI 10.1145/3402456
   Zhang GX, 2014, INT J NEURAL SYST, V24, DOI 10.1142/S0129065714400061
NR 46
TC 10
Z9 10
U1 13
U2 46
PD OCT 11
PY 2022
VL 253
AR 109568
DI 10.1016/j.knosys.2022.109568
EA AUG 2022
UT WOS:000861208200015
DA 2023-11-16
ER

PT C
AU Li, YD
   Wu, AG
   Dong, N
   Du, LJ
   Chai, Y
AF Li, Yudi
   Wu, Aiguo
   Dong, Na
   Du, Lijia
   Chai, Yi
GP IEEE
TI Oscillation Characteristics of Feedforward Neural Networks
SO 2018 13TH WORLD CONGRESS ON INTELLIGENT CONTROL AND AUTOMATION (WCICA)
DT Proceedings Paper
CT 13th World Congress on Intelligent Control and Automation (WCICA)
CY JUL 04-08, 2018
CL Changsha, PEOPLES R CHINA
AB Based on the physiological characteristics of the acupuncture signal transduction pathway, this paper uses the Izhikevich model and the small-world neural network to establish a feedforward neural network model to study the influence of the noise of intensity and the connection probability between layers on the oscillation characteristics of the feedforward neural network when the neurons are in the types of regular spiking and burst spiking.
C1 [Li, Yudi; Wu, Aiguo; Dong, Na; Du, Lijia; Chai, Yi] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
RP Li, YD (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
EM liyudi925@163.com
CR [Anonymous], 2014, MATH PROBL ENG, DOI DOI 10.1155/2014/914689
   FitzHugu R., 1961, B MATH BIOPHYS, V17, P257
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Itoh K, 2007, EVID-BASED COMPL ALT, V4, P431, DOI 10.1093/ecam/nel092
   Izhikevich EM, 2000, INT J BIFURCAT CHAOS, V10, P1171, DOI 10.1142/S0218127400000840
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Komarov M. A., 2008, VERIETY SYNCHRONOUS, V18
   Manyakov NV, 2008, CHAOS, V18, DOI 10.1063/1.2949928
   [石霞 Shi Xia], 2010, [力学季刊, Chinese quarterly of mechanics], V31, P52
   Watts DJ, 1998, NATURE, V393, P440, DOI 10.1038/30918
NR 10
TC 0
Z9 0
U1 0
U2 0
PY 2018
BP 1074
EP 1079
UT WOS:000461361000184
DA 2023-11-16
ER

PT C
AU Wang, SQ
   Ferreira, PM
   Benlarbi-Delai, A
AF Wang, Siqi
   Ferreira, Pietro Maris
   Benlarbi-Delai, Aziz
GP IEEE
TI Behavioral Modeling of Nonlinear Power Amplifiers Using Spiking Neural
   Networks
SO 2022 20TH IEEE INTERREGIONAL NEWCAS CONFERENCE (NEWCAS)
DT Proceedings Paper
CT 20th IEEE Interregional NEWCAS Conference (IEEE NEWCAS)
CY JUN 19-22, 2022
CL Quebec City, CANADA
DE Device modeling; memory effects; nonlinear distortion; power amplifiers;
   spiking neurons
ID MEMORY POLYNOMIAL MODEL; DIGITAL PREDISTORTION; NEURONS
AB In this paper, we propose a novel way for power amplifiers (PA) modeling using spiking neurons. The rate of neurons firing spikes is a nonlinear function of its excitation current. Taking the firing rate as the output and the excitation current as the input of a one-layer spiking neuron network, we build up a PA behavioral model with low nonlinearity order to mimic its strong nonlinearity. The results of modeling two Doherty PA show that the proposed method can reach better performance but with lower computational complexity compared with traditional methods. This is the first time that the nonlinearity property of spiking neurons are used for processing such nonlinear signals. Future work is to develop a complete system for the training of the spiking neural networks and to explore the application of spiking neural networks on real-time PA linearization.
C1 [Wang, Siqi] Sorbonne Univ, Lab Genie Elect & Elect Paris, CNRS, F-75252 Paris, France.
   Univ Paris Saclay, Lab Genie Elect & Elect Paris, CNRS, CentraleSupelec, F-91192 Gif Sur Yvette, France.
RP Wang, SQ (corresponding author), Sorbonne Univ, Lab Genie Elect & Elect Paris, CNRS, F-75252 Paris, France.
CR Borwankar R, 2018, IEEE INT NEW CIRC, P134, DOI 10.1109/NEWCAS.2018.8585554
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Cappy A., 2020, ELECTRONICS ENG SERI
   Davidson S, 2021, FRONT NEUROSCI-SWITZ, V15, DOI 10.3389/fnins.2021.651141
   Ferreira PM, 2021, ANALOG INTEGR CIRC S, V106, P261, DOI 10.1007/s10470-020-01729-3
   Ferreira PM, 2019, 2019 32ND SYMPOSIUM ON INTEGRATED CIRCUITS AND SYSTEMS DESIGN (SBCCI 2019), DOI 10.1145/3338852.3339852
   Kim J, 2001, ELECTRON LETT, V37, P1417, DOI 10.1049/el:20010940
   Li XM, 2018, PHYSICA A, V491, P716, DOI 10.1016/j.physa.2017.08.053
   Liang L, 2023, IEEE T NEUR NET LEAR, V34, P2569, DOI 10.1109/TNNLS.2021.3106961
   Liu YJ, 2014, IEEE T MICROW THEORY, V62, P2604, DOI 10.1109/TMTT.2014.2360398
   Loyez C., 2021, PROC 19 IEEE INT NEW, P1
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Matsubara T, 2016, IEEE T NEUR NET LEAR, V27, P836, DOI 10.1109/TNNLS.2015.2425893
   Morgan DR, 2006, IEEE T SIGNAL PROCES, V54, P3852, DOI 10.1109/TSP.2006.879264
   Sharmin S., 2019, IEEE IJCNN, P1, DOI DOI 10.1109/ijcnn.2019.8851732
   Tehrani AS, 2010, IEEE T MICROW THEORY, V58, P1510, DOI 10.1109/TMTT.2010.2047920
   Wang SQ, 2019, IEEE MICROW WIREL CO, V29, P741, DOI 10.1109/LMWC.2019.2939911
   Wang SQ, 2018, IEEE T MICROW THEORY, V66, P3958, DOI 10.1109/TMTT.2018.2838126
   Wang SQ, 2018, IEEE T VEH TECHNOL, V67, P7326, DOI 10.1109/TVT.2018.2833283
   Wood J, 2014, ARTECH HSE MICROW LI, P1
   Yu C, 2019, IEEE T MICROW THEORY, V67, P2847, DOI 10.1109/TMTT.2019.2918450
   Zhang AS, 2022, INT J ADV MANUF TECH, V118, P3963, DOI 10.1007/s00170-021-08049-4
   Zhu AD, 2006, IEEE T MICROW THEORY, V54, P4323, DOI 10.1109/TMTT.2006.883243
   Zhu AD, 2015, IEEE T MICROW THEORY, V63, P737, DOI 10.1109/TMTT.2014.2387853
NR 24
TC 1
Z9 1
U1 2
U2 4
PY 2022
BP 495
EP 499
DI 10.1109/NEWCAS52662.2022.9842167
UT WOS:000855039400096
DA 2023-11-16
ER

PT J
AU Yu, CT
   Du, YK
   Chen, MF
   Wang, AL
   Wang, GA
   Li, ER
AF Yu, Chengting
   Du, Yangkai
   Chen, Mufeng
   Wang, Aili
   Wang, Gaoang
   Li, Erping
TI MAP-SNN: Mapping spike activities with multiplicity, adaptability, and
   plasticity into bio-plausible spiking neural networks
SO FRONTIERS IN NEUROSCIENCE
DT Article
DE spiking neural network (SNN); leaky integrate-and-fire (LIF) neuron;
   multiple spike pattern (MSP); spike frequency adaption (SFA); state-free
   synaptic response model (SFSRM); neuromorphic recognition;
   backpropagation (BP)
ID PROCESSOR; DYNAMICS
AB Spiking Neural Networks (SNNs) are considered more biologically realistic and power-efficient as they imitate the fundamental mechanism of the human brain. Backpropagation (BP) based SNN learning algorithms that utilize deep learning frameworks have achieved good performance. However, those BP-based algorithms partially ignore bio-interpretability. In modeling spike activity for biological plausible BP-based SNNs, we examine three properties: multiplicity, adaptability, and plasticity (MAP). Regarding multiplicity, we propose a Multiple-Spike Pattern (MSP) with multiple-spike transmission to improve model robustness in discrete time iterations. To realize adaptability, we adopt Spike Frequency Adaption (SFA) under MSP to reduce spike activities for enhanced efficiency. For plasticity, we propose a trainable state-free synapse that models spike response current to increase the diversity of spiking neurons for temporal feature extraction. The proposed SNN model achieves competitive performances on the N-MNIST and SHD neuromorphic datasets. In addition, experimental results demonstrate that the proposed three aspects are significant to iterative robustness, spike efficiency, and the capacity to extract spikes' temporal features. In summary, this study presents a realistic approach for bio-inspired spike activity with MAP, presenting a novel neuromorphic perspective for incorporating biological properties into spiking neural networks.
C1 [Yu, Chengting; Chen, Mufeng; Wang, Aili; Li, Erping] Zhejiang Univ, Coll Informat Sci & Elect Engn, Hangzhou, Peoples R China.
   [Yu, Chengting; Wang, Aili; Wang, Gaoang; Li, Erping] Zhejiang Univ, Univ Illinois, Urbana Champaign Inst, Haining, Peoples R China.
   [Du, Yangkai] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou, Peoples R China.
RP Wang, AL (corresponding author), Zhejiang Univ, Coll Informat Sci & Elect Engn, Hangzhou, Peoples R China.; Wang, AL (corresponding author), Zhejiang Univ, Univ Illinois, Urbana Champaign Inst, Haining, Peoples R China.
EM ailiwang@intl.zju.edu.cn
CR Adibi M, 2013, J NEUROSCI, V33, P14921, DOI 10.1523/JNEUROSCI.1313-13.2013
   Bellec G., 2018, ADV NEURAL INFORM PR
   Bellec G, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-17236-y
   Benda J, 2003, NEURAL COMPUT, V15, P2523, DOI 10.1162/089976603322385063
   Bing Han, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13555, DOI 10.1109/CVPR42600.2020.01357
   Cheng X, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1519
   Chowdhury S.S., 2021, ARXIV
   Cohen GK, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00184
   Cramer B, 2022, IEEE T NEUR NET LEAR, V33, P2744, DOI 10.1109/TNNLS.2020.3044364
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Dayan P, 2005, THEORETICAL NEUROSCI
   Diehl PU, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON REBOOTING COMPUTING (ICRC)
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Eshraghian JK, 2023, Arxiv, DOI [arXiv:2109.12894, DOI 10.48550/ARXIV.2109.12894]
   Fang HW, 2020, Arxiv, DOI [arXiv:2003.02944, 10.48550/arXiv.2003.02944]
   Fang HW, 2021, DES AUT CON, P361, DOI 10.1109/DAC18074.2021.9586133
   Gerstner W., 2002, SPIKING NEURON MODEL, DOI [DOI 10.1017/CBO9780511815706, 10.1017/cbo9780511815706]
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Glorot X., 2010, P 13 INT C ARTIFICIA, P249
   Gu PJ, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1366
   HODGKIN AL, 1952, PROC R SOC SER B-BIO, V140, P177, DOI 10.1098/rspb.1952.0054
   Hunsberger E, 2016, Arxiv, DOI [arXiv:1611.05141, 10.13140/RG.2.2.10967.06566]
   Ingrosso A, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0220547
   Izhikevich EM, 2004, CEREB CORTEX, V14, P933, DOI 10.1093/cercor/bhh053
   Jin Y., 2018, ADV NEURAL INFORM PR
   Lechner M, 2020, NAT MACH INTELL, V2, P642, DOI 10.1038/s42256-020-00237-3
   Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508
   Liu SC, 2010, CURR OPIN NEUROBIOL, V20, P288, DOI 10.1016/j.conb.2010.03.007
   Ma D, 2017, J SYST ARCHITECT, V77, P43, DOI 10.1016/j.sysarc.2017.01.003
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Masquelier T, 2018, FRONT COMPUT NEUROSC, V12, DOI 10.3389/fncom.2018.00074
   Muratore P, 2021, PLOS ONE, V16, DOI 10.1371/journal.pone.0247014
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Neil D., 2016, P 30 INT C NEUR INF, P3889, DOI DOI 10.48550/ARXIV.1610.09513
   Neil D, 2016, IEEE INT SYMP CIRC S, P2282, DOI 10.1109/ISCAS.2016.7539039
   Nessler B., 2009, ADV NEURAL INFORM PR
   Orchard G, 2015, FRONT NEUROSCI-SWITZ, V9, DOI [10.3389/fhins.2015.00437, 10.3389/fnins.2015.00437]
   Paszke A., 2019, ADV NEURAL INFORM PR
   Payeur A, 2021, NAT NEUROSCI, V24, P1010, DOI 10.1038/s41593-021-00857-x
   Pei J, 2019, NATURE, V572, P106, DOI 10.1038/s41586-019-1424-8
   Perez-Nieves N, 2021, NAT COMMUN, V12, DOI 10.1038/s41467-021-26022-3
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Rothman J. S., 2013, ENCY COMPUTATIONAL N, P1, DOI DOI 10.1007/978-1-4614-7320-6_240-1
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Rueckauer Bodo, 2016, ARXIV, DOI DOI 10.3389/FNINS.2017.00682
   Salaj D, 2021, ELIFE, V10, DOI 10.7554/eLife.65459
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Shrestha SB, 2018, ADV NEUR IN, V31
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Tavanaei A, 2016, IEEE IJCNN, P307, DOI 10.1109/IJCNN.2016.7727213
   Vanarse A, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00115
   Wozniak S, 2020, NAT MACH INTELL, V2, P325, DOI 10.1038/s42256-020-0187-0
   Wu YJ, 2019, AAAI CONF ARTIF INTE, P1311
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Xu C., 2021, ARXIV, DOI DOI 10.48550/ARXIV.2112.07426
   Xu CQ, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00104
   Yin B, 2020, IEEE INTERNET THINGS, V7, P8748, DOI [10.1109/JIOT.2020.2996562, 10.1145/3407197.3407225]
   Zenke F, 2021, NEURAL COMPUT, V33, P899, DOI 10.1162/neco_a_01367
   Zhang TL, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-86780-4
NR 60
TC 1
Z9 1
U1 3
U2 8
PD SEP 20
PY 2022
VL 16
AR 945037
DI 10.3389/fnins.2022.945037
UT WOS:000863759100001
DA 2023-11-16
ER

PT C
AU Dennis, J
   Dat, TH
   Li, HZ
AF Dennis, Jonathan
   Tran Huy Dat
   Li, Haizhou
GP IEEE
TI COMBINING ROBUST SPIKE CODING WITH SPIKING NEURAL NETWORKS FOR SOUND
   EVENT CLASSIFICATION
SO 2015 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL
   PROCESSING (ICASSP)
SE International Conference on Acoustics Speech and Signal Processing
   ICASSP
DT Proceedings Paper
CT 40th IEEE International Conference on Acoustics, Speech, and Signal
   Processing (ICASSP)
CY APR 19-24, 2015
CL Brisbane, AUSTRALIA
DE Neural spike coding; local spectrogram features; noise robust; sound
   event classification
ID HOUGH TRANSFORM; RECOGNITION; SPEECH; FEATURES; NOISE
AB This paper proposes a novel biologically inspired method for sound event classification which combines spike coding with a spiking neural network (SNN). Our spike coding extracts keypoints that represent the local maxima components of the sound spectrogram, and are encoded based on their local timefrequency information; hence both location and spectral information are being extracted. We then design a modified tempotron SNN that, unlike the original tempotron, allows the network to learn the temporal distributions of spike coding input, in an analogous way to the generalized Hough transform. The proposed method simultaneously enhances the sparsity of the sound event spectrogram, producing a representation which is robust against noise, as well as maximises the discriminability of the spike coding input in terms of its temporal information, which is important for sound event classification. Experimental results on a large dataset of 50 environment sound events show the superiority of both the spike coding versus the raw spectrogram and the SNN versus conventional cross- entropy neural networks.
C1 [Dennis, Jonathan; Tran Huy Dat; Li, Haizhou] ASTAR, Inst Infocomm Res, 1 Fusionopolis Way, Singapore 138632, Singapore.
RP Dennis, J (corresponding author), ASTAR, Inst Infocomm Res, 1 Fusionopolis Way, Singapore 138632, Singapore.
CR Allen JB, 1994, IEEE T SPEECH AUDI P, V2, P567, DOI 10.1109/89.326615
   BALLARD DH, 1981, PATTERN RECOGN, V13, P111, DOI 10.1016/0031-3203(81)90009-1
   Chu S, 2009, IEEE T AUDIO SPEECH, V17, P1142, DOI 10.1109/TASL.2009.2017438
   Cooke M, 2006, J ACOUST SOC AM, V119, P1562, DOI 10.1121/1.2166600
   Dennis J, 2013, PATTERN RECOGN LETT, V34, P1085, DOI 10.1016/j.patrec.2013.02.015
   Dennis J., 2013, P IEEE INT C AC SPEE
   Gerosa L., 2007, P EUR SIGN PROC C EU
   Ghoraani B, 2011, IEEE T AUDIO SPEECH, V19, P2197, DOI 10.1109/TASL.2011.2118753
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Leibe B, 2008, INT J COMPUT VISION, V77, P259, DOI 10.1007/s11263-007-0095-3
   Lyon RF, 2010, IEEE SIGNAL PROC MAG, V27, P131, DOI 10.1109/MSP.2010.937498
   Nakamura S., 2000, LREC
   VARGA A, 1993, SPEECH COMMUN, V12, P247, DOI 10.1016/0167-6393(93)90095-3
   Vinyals O., 2012, P ANN C INT SPEECH C
   Weninger F, 2011, INT CONF ACOUST SPEE, P337
NR 15
TC 4
Z9 4
U1 0
U2 4
PY 2015
BP 176
EP 180
UT WOS:000427402900036
DA 2023-11-16
ER

PT J
AU Wu, DY
   Yi, XP
   Huang, XW
AF Wu, Dengyu
   Yi, Xinping
   Huang, Xiaowei
TI A Little Energy Goes a Long Way: Build an Energy-Efficient, Accurate
   Spiking Neural Network From Convolutional Neural Network
SO FRONTIERS IN NEUROSCIENCE
DT Article
DE spiking neural network (SNN); spiking network conversion; deep learning;
   deep neural networks (DNNs); event-driven neural network
AB This article conforms to a recent trend of developing an energy-efficient Spiking Neural Network (SNN), which takes advantage of the sophisticated training regime of Convolutional Neural Network (CNN) and converts a well-trained CNN to an SNN. We observe that the existing CNN-to-SNN conversion algorithms may keep a certain amount of residual current in the spiking neurons in SNN, and the residual current may cause significant accuracy loss when inference time is short. To deal with this, we propose a unified framework to equalize the output of the convolutional or dense layer in CNN and the accumulated current in SNN, and maximally align the spiking rate of a neuron with its corresponding charge. This framework enables us to design a novel explicit current control (ECC) method for the CNN-to-SNN conversion which considers multiple objectives at the same time during the conversion, including accuracy, latency, and energy efficiency. We conduct an extensive set of experiments on different neural network architectures, e.g., VGG, ResNet, and DenseNet, to evaluate the resulting SNNs. The benchmark datasets include not only the image datasets such as CIFAR-10/100 and ImageNet but also the Dynamic Vision Sensor (DVS) image datasets such as DVS-CIFAR-10. The experimental results show the superior performance of our ECC method over the state-of-the-art.
C1 [Wu, Dengyu; Huang, Xiaowei] Univ Liverpool, Dept Comp Sci, Liverpool, England.
   [Yi, Xinping] Univ Liverpool, Dept Elect Engn & Elect, Liverpool, England.
RP Wu, DY (corresponding author), Univ Liverpool, Dept Comp Sci, Liverpool, England.
EM dengyu.wu@liverpool.ac.uk
CR Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   [Anonymous], 2012, PROC CICC
   Bing Han, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13555, DOI 10.1109/CVPR42600.2020.01357
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Deng S., 2021, INT C LEARN REPR
   Diehl PU, 2015, IEEE IJCNN
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Jacob B, 2018, PROC CVPR IEEE, P2704, DOI 10.1109/CVPR.2018.00286
   Ju XP, 2020, NEURAL COMPUT, V32, P182, DOI 10.1162/neco_a_01245
   Kobayashi R, 2009, FRONT COMPUT NEUROSC, V3, DOI 10.3389/neuro.10.009.2009
   Krizhevsky Alex, 2009, LEARNING MULTIPLE LA
   Kugele A, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00439
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Lee C, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00119
   Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508
   Li HM, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00309
   Li Y., 2021, INT C MACHINE LEARNI, V139, P6316
   Lin J., 2019, ARXIV PREPRINT ARXIV, DOI [10.48550/arXiv.1904.08444, DOI 10.48550/ARXIV.1904.08444]
   Lu S, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00535
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Parsa M, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00667
   Pei J, 2019, NATURE, V572, P106, DOI 10.1038/s41586-019-1424-8
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Rathi N, 2023, IEEE T NEUR NET LEAR, V34, P3174, DOI 10.1109/TNNLS.2021.3111897
   Rathi Nitin, 2020, INT C LEARN REPR
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Santurkar S, 2018, ADV NEUR IN, V31
   Schuster C, 2020, PUBLIC ADMIN REV, V80, P792, DOI 10.1111/puar.13246
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Simonyan K., 2014, ARXIV, DOI [DOI 10.48550/ARXIV.1409.1556, 10.48550/arXiv.1409.1556]
   Soures N, 2019, IEEE SIGNAL PROC MAG, V36, P78, DOI 10.1109/MSP.2019.2931479
   Sze V, 2017, P IEEE, V105, P2295, DOI 10.1109/JPROC.2017.2761740
   Wu H., 2021, P AAAI C ARTIFICIAL
   Yu H., 2020, P IEEECVF C COMPUTER, P680
NR 36
TC 3
Z9 3
U1 0
U2 10
PD MAY 26
PY 2022
VL 16
AR 759900
DI 10.3389/fnins.2022.759900
UT WOS:000808326700001
DA 2023-11-16
ER

PT C
AU Wang, XQ
   Hou, ZG
   Zeng, H
   Tan, M
   Wang, YJ
AF Wang Xiuqing
   Hou Zeng-Guang
   Zeng Hui
   Tan Min
   Wang Yongji
GP IEEE
TI A Comparison for Probabilistic Spiking Neuron Model and Spiking
   Integrated and Fired Neuron Model
SO 2014 33RD CHINESE CONTROL CONFERENCE (CCC)
SE Chinese Control Conference
DT Proceedings Paper
CT 33rd Chinese Control Conference (CCC)
CY JUL 28-30, 2014
CL Nanjing, PEOPLES R CHINA
DE Spiking neural network; Spiking integrated and fired neuron model;
   Probability spiking neuron model; Spiking time-delayed encoding; Spiking
   frequency encoding
AB As the third generation of artificial neural networks (ANNs), spiking neural networks (SNNs) have many advantages over the traditional ones. Selecting proper spiking neuron models for the design of SNNs is important. In this paper, schematic and training algorithm of spiking integrated and fired (IAF) neuron model and probability spiking neuron model (pSNM) are introduced. By comparing the classification results for mobile robots' corridor-scene-classifier based on IAF model and pSNM, and the control results of mobile robots' wall-following controller based on spiking IAF model and pSNM, the similarities and differences between the two models are discussed. The similar and different features of the two spiking neuron models are obtained. IAF model is more suitable for the design of mobile robots controller than that of pSNM. While pSNM has better noise robust than IAF model. Spiking IAF model and pSNM are suitable for different situations.
C1 [Wang Xiuqing] Hebei Normal Univ, Shijiazhuang 050031, Peoples R China.
   [Hou Zeng-Guang; Tan Min] Chinese Acad Sci, Inst Automat, State Key Lab Complex Syst & Intelligence Sci, Beijing 100090, Peoples R China.
   [Zeng Hui] Univ Sci & Technol Beijing, Sch Informat, Beijing 100083, Peoples R China.
   [Wang Yongji] Chinese Acad Sci, Inst Software, State Key Lab Comp Sci, Lab Internet Technol, Beijing 100190, Peoples R China.
RP Wang, XQ (corresponding author), Hebei Normal Univ, Shijiazhuang 050031, Peoples R China.
EM xqwang2013@163.com; zengguang.hou@ia.ac.cn; zenghui@163.com;
   min.tan@ia.ac.cn; ywang@itechs.iscas.ac.cn
CR Gerstner W., 2002, SPIKING NEURON MODEL
   Hebb D., 2005, ORG BEHAV NEUROPSYCH
   Kasabov N, 2010, NEURAL NETWORKS, V23, P16, DOI 10.1016/j.neunet.2009.08.010
   Kempter R, 1999, PHYS REV E, V59, P4498, DOI 10.1103/PhysRevE.59.4498
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   Wang X. Q., 2012, ADV MAT RES, V589, P1547
   Wang XQ, 2009, 2009 INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND COMPUTATIONAL INTELLIGENCE, VOL I, PROCEEDINGS, P194, DOI 10.1109/AICI.2009.448
   Wang XQ, 2008, ICNC 2008: FOURTH INTERNATIONAL CONFERENCE ON NATURAL COMPUTATION, VOL 4, PROCEEDINGS, P125, DOI 10.1109/ICNC.2008.718
   Xiuqing Wang, 2011, 2011 10th IEEE International Conference on Cognitive Informatics & Cognitive Computing (ICCI-CC 2011), P348, DOI 10.1109/COGINF.2011.6016164
NR 10
TC 0
Z9 0
U1 0
U2 1
PY 2014
BP 5059
EP 5064
UT WOS:000366482805032
DA 2023-11-16
ER

PT J
AU Tigaeru, L
   Bonteanu, G
AF Tigaeru, Liviu
   Bonteanu, Gabriel
TI A Neuron Model for FPGA Spiking Neuronal Network Implementation
SO ADVANCES IN ELECTRICAL AND COMPUTER ENGINEERING
DT Article
DE spiking neural network; neuromorphics; biological system modeling; field
   programmable gate arrays; very large scale integration
AB We propose a neuron model, able to reproduce the basic elements of the neuronal dynamics, optimized for digital implementation of Spiking Neural Networks. Its architecture is structured in two major blocks, a datapath and a control unit. The datapath consists of a membrane potential circuit, which emulates the neuronal dynamics at the soma level, and a synaptic circuit used to update the synaptic weight according to the spike timing dependent plasticity (STDP) mechanism. The proposed model is implemented into a Cyclone II-Altera FPGA device. Our results indicate the neuron model can be used to build up 1K Spiking Neural Networks on reconfigurable logic suport, to explore various network topologies.
C1 [Tigaeru, Liviu; Bonteanu, Gabriel] Gheorghe Asachi Tech Univ Iasi, Iasi 700506, Romania.
RP Tigaeru, L (corresponding author), Gheorghe Asachi Tech Univ Iasi, Iasi 700506, Romania.
EM ltigaeru@etti.tuiasi.ro; gbonteanu@etti.tuiasi.ro
CR Abbott LF, 1999, BRAIN RES BULL, V50, P303, DOI 10.1016/S0361-9230(99)00161-6
   [Anonymous], 2008, CYCLONE 2 DEVICE HDB
   [Anonymous], P 18 IFAC WORLD C
   Arena P, 2009, IEEE T NEURAL NETWOR, V20, P202, DOI 10.1109/TNN.2008.2005134
   Bailey JA, 2011, NEUROCOMPUTING, V74, P2392, DOI 10.1016/j.neucom.2011.04.001
   Chu P. P., 2006, RTL HARDWARE DESIGN
   Hebb D., 2005, ORG BEHAV NEUROPSYCH
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V116, P424, DOI 10.1113/jphysiol.1952.sp004716
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V116, P497, DOI 10.1113/jphysiol.1952.sp004719
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V116, P449, DOI 10.1113/jphysiol.1952.sp004717
   Hodgkin AL, 1952, J PHYSL, V116, P507
   Hynna KM, 2007, NEURAL COMPUT, V19, P327, DOI 10.1162/neco.2007.19.2.327
   Indiveri G, 2006, IEEE T NEURAL NETWOR, V17, P211, DOI 10.1109/TNN.2005.860850
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Koch Christof, 1999, P1
   Maass, 1997, 3 GENERATION NEURAL
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   Pearson MJ, 2007, IEEE T NEURAL NETWOR, V18, P1472, DOI 10.1109/TNN.2007.891203
   Ros E, 2006, IEEE T NEURAL NETWOR, V17, P1050, DOI 10.1109/TNN.2006.875980
   Wijekoon JHB, 2008, NEURAL NETWORKS, V21, P524, DOI 10.1016/j.neunet.2007.12.037
   SPARTAN 3E FPGA DEVI
   2008, QUARTUS 2 HDB
   VITEX 2 FPGA DEVICE
NR 23
TC 3
Z9 4
U1 0
U2 13
PY 2011
VL 11
IS 4
BP 29
EP 36
DI 10.4316/AECE.2011.04005
UT WOS:000297764500005
DA 2023-11-16
ER

PT C
AU Saggie, K
   Keinan, A
   Ruppin, E
AF Saggie, K
   Keinan, A
   Ruppin, E
BE DeSchutter, E
TI Spikes that count: rethinking spikiness in neurally embedded systems
SO COMPUTATIONAL NEUROSCIENCE: TRENDS IN RESEARCH 2004
DT Proceedings Paper
CT 12th Annual Computational Neuroscience Meeting (CNS 03)
CY JUL 05-09, 2003
CL Alicante, SPAIN
DE evolutionary computation; spiking; counting; neurocontroller analysis
AB Spiky neural networks are widely used in neural modeling, due to their biological relevance and high computational power. In this paper we investigate the usage of spiking dynamics in embedded artificial neural networks, that serve as a control mechanism for evolved autonomous agents performing a counting task. The synaptic weights and spiking dynamics are evolved using a genetic algorithm. We compare evolved spiky networks with evolved McCulloch-Pitts networks, while confronting new questions about the nature of "spikiness" and its contribution to the neurocontroller's processing. We show that in a memory-dependent task, network solutions that incorporate spiking dynamics can be less complex and easier to evolve than networks involving McCulloch-Pitts neurons. We identify and rigorously characterize two distinct properties of spiking dynamics in embedded agents: spikiness dynamic influence and spikiness functional contribution. (C) 2004 Elsevier B.V. All rights reserved.
C1 Tel Aviv Univ, Sch Comp Sci, IL-69978 Tel Aviv, Israel.
   Tel Aviv Univ, Sch Med, IL-69978 Tel Aviv, Israel.
RP Saggie, K (corresponding author), Tel Aviv Univ, Sch Comp Sci, IL-69978 Tel Aviv, Israel.
EM keren@cns.tau.ac.il; keinan@cns.tau.ac.il; ruppin@post.tau.ac.il
CR Aharonov R, 2003, NEURAL COMPUT, V15, P885, DOI 10.1162/08997660360581949
   Aharonov-Barki R, 2001, NEURAL COMPUT, V13, P691, DOI 10.1162/089976601300014529
   Bugmann G, 1997, BIOSYSTEMS, V40, P11, DOI 10.1016/0303-2647(96)01625-5
   Di Paolo EA, 2002, ADAPT BEHAV, V10, P243, DOI 10.1177/1059712302010003006
   FLOREANO D, 2001, EVOLUTIONARY ROBOTIC, V4
   Hartwell LH, 1999, NATURE, V402, pC47, DOI 10.1038/35011540
   KEINAN A, IN PRESS NEURAL COMP
   Maass W, 1999, INFORM COMPUT, V148, P202, DOI 10.1006/inco.1998.2743
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Mitchell M., 1998, INTRO GENETIC ALGORI
   Ruppin E, 2002, NAT REV NEUROSCI, V3, P132, DOI 10.1038/nrn729
NR 11
TC 0
Z9 0
U1 0
U2 0
PY 2004
BP 303
EP 311
DI 10.1016/j.neucom.2004.01.060
UT WOS:000223887200047
DA 2023-11-16
ER

PT C
AU Afrin, F
   Cantley, KD
AF Afrin, Farhana
   Cantley, Kurtis D.
GP IEEE
TI R(t)-based Spike-Timing-Dependent Plasticity in Memristive Neural
   Networks
SO 2023 IEEE WORKSHOP ON MICROELECTRONICS AND ELECTRON DEVICES, WMED
SE IEEE Workshop on Microelectronics and Electron Devices
DT Proceedings Paper
CT 20th Annual IEEE Workshop on Microelectronics and Electron Devices
   (WMED)
CY MAR 31, 2023
CL IEEE Electron Devices Soc, Boise Chapter, Boise, ID
HO IEEE Electron Devices Soc, Boise Chapter
DE Spike-Timing-Dependent Plasticity; R(t) element; memristor; Spiking
   Neural Network; spike triplet learning
ID TRIPLET-BASED STDP; MODEL; SYNAPSE; PAIR
AB Inspired by the human brain, neuromorphic computation should be extremely efficient at very large scales due to inherent parallelism, scalability, and fault and failure tolerance. Spike-Timing-Dependent Plasticity (STDP) is one of the most biologically plausible synaptic learning behaviors. The proposed generic model of time-varying resistance, or R(t) elements in this work can produce STDP in electronic spiking neural networks with memristive synapses that is very similar to that observed in biology. Both pair-based and triplet-based STDP is verified with the proposed generic R(t) model.
C1 [Afrin, Farhana; Cantley, Kurtis D.] Boise State Univ, Dept Elect & Comp Engn, Boise, ID 83725 USA.
RP Afrin, F (corresponding author), Boise State Univ, Dept Elect & Comp Engn, Boise, ID 83725 USA.
EM farhanaafrin@u.boisestate.edu; kurtiscantley@boisestate.edu
CR Cai WR, 2015, IEEE T BIOMED CIRC S, V9, P87, DOI 10.1109/TBCAS.2014.2318012
   Cantley K. D., 2011, 2011 IEEE 11th International Conference on Nanotechnology (IEEE-NANO), P421, DOI 10.1109/NANO.2011.6144430
   Dahl SG, 2021, SN APPL SCI, V3, DOI 10.1007/s42452-021-04553-0
   Gjorgjieva J, 2011, P NATL ACAD SCI USA, V108, P19383, DOI 10.1073/pnas.1105933108
   Gopalakrishnan R, 2015, IEEE INT SYMP CIRC S, P710, DOI 10.1109/ISCAS.2015.7168732
   Ivans RC, 2020, IEEE T NEUR NET LEAR, V31, P4206, DOI 10.1109/TNNLS.2019.2952768
   Krunglevicius D., 2016, ADV ARTIFICIAL NEURA, P1
   Lammie C, 2019, IEEE T CIRCUITS-I, V66, P1558, DOI 10.1109/TCSI.2018.2881753
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Pfister JP, 2006, J NEUROSCI, V26, P9673, DOI 10.1523/JNEUROSCI.1425-06.2006
   Prezioso M, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-07757-y
   Prezioso M, 2016, SCI REP-UK, V6, DOI 10.1038/srep21331
   Sjöström PJ, 2001, NEURON, V32, P1149, DOI 10.1016/s0896-6273(01)00542-6
   Wang W, 2018, SCI ADV, V4, DOI 10.1126/sciadv.aat4752
   Wang ZQ, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-15158-3
   Yang R, 2018, ADV FUNCT MATER, V28, DOI 10.1002/adfm.201704455
NR 16
TC 0
Z9 0
U1 1
U2 1
PY 2023
BP 26
EP 29
DI 10.1109/WMED58543.2023.10097441
UT WOS:000991977600006
DA 2023-11-16
ER

PT C
AU Kucik, AS
   Meoni, G
AF Kucik, Andrzej S.
   Meoni, Gabriele
GP IEEE Comp Soc
TI Investigating Spiking Neural Networks for Energy-Efficient On-Board AI
   Applications. A Case Study in Land Cover and Land Use Classification
SO 2021 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION
   WORKSHOPS, CVPRW 2021
SE IEEE Computer Society Conference on Computer Vision and Pattern
   Recognition Workshops
DT Proceedings Paper
CT IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CY JUN 19-25, 2021
CL ELECTR NETWORK
AB Spiking neural networks have been attracting the interest of researchers due to their potential energy efficiency. This feature makes them appealing for applications on board CubeSats or small Earth observation satellites, given their strict energy consumption requirements. However, the performance of spiking neural networks in terms of the accuracy on the space-scene classification datasets, and their benefits with respect to the energy efficiency still remain to be demonstrated. This work is a preliminary investigation on deploying spiking neural networks to land cover and land use classification problems. To train a spiking model, a VGG-16-based artificial neural network has been trained on EuroSAT RGB benchmark dataset. The parameters of this network were then used to initialise a spiking model, which was optimised by fine-tuning the connection weights and the synaptic filters parameters. By using the mean neuron activations, and the number of time-steps and their width as proxies for the energy consumption of the models, this study shows different trade-offs between accuracy, and energy efficiency, when comparing a spiking model to the deep learning approach. Moreover, some additional input data preprocessing strategies are investigated as a method of further enhancement of the energy benefits of the spiking models.
C1 [Kucik, Andrzej S.; Meoni, Gabriele] European Space Agcy, Paris, France.
RP Kucik, AS (corresponding author), European Space Agcy, Paris, France.
EM andrzej.kucik@esa.int; gabriele.meoni@esa.int
CR Abadi M., 2016, TENSORFLOW LARGE SCA
   Amodei D, 2018, OPENAI BLOG
   Applied Brain Research, KERASSPIKING EST MOD
   Applied Brain Research, KERASSPIKING
   Benelli G, 2018, IEEE INT CONF VLSI, P267, DOI 10.1109/VLSI-SoC.2018.8644728
   Bose P, 2016, IEEE T GEOSCI REMOTE, V54, P6563, DOI 10.1109/TGRS.2016.2586602
   Bouvier M, 2019, ACM J EMERG TECH COM, V15, DOI 10.1145/3304103
   Bouwmeester J, 2010, ACTA ASTRONAUT, V67, P854, DOI 10.1016/j.actaastro.2010.06.004
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Degnan B, 2016, IEEE T VLSI SYST, V24, P58, DOI 10.1109/TVLSI.2015.2392942
   Denby B, 2020, TWENTY-FIFTH INTERNATIONAL CONFERENCE ON ARCHITECTURAL SUPPORT FOR PROGRAMMING LANGUAGES AND OPERATING SYSTEMS (ASPLOS XXV), P939, DOI 10.1145/3373376.3378473
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Frenkel C, 2019, IEEE T BIOMED CIRC S, V13, P999, DOI 10.1109/TBCAS.2019.2928793
   Furano G, 2020, IEEE AERO EL SYS MAG, V35, P44, DOI 10.1109/MAES.2020.3008468
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   Furber SB, 2013, IEEE T COMPUT, V62, P2454, DOI 10.1109/TC.2012.142
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Giuffrida G, 2020, REMOTE SENS-BASEL, V12, DOI 10.3390/rs12142205
   Goyal Priya, 2021, SELF SUPERVISED PRET
   Han B, 2018, IEEE T MULTI-SCALE C, V4, P613, DOI 10.1109/TMSCS.2017.2737625
   HELBER P, 2019, IEEE J-STARS, V12, P2217, DOI [DOI 10.1109/JSTARS.2019.2918242, 10.1109/IGARSS.2018.8519248]
   Helber P, 2018, INT GEOSCI REMOTE SE, P204, DOI 10.1109/IGARSS.2018.8519248
   Hinton G., 2016, LECT C
   Höppner S, 2019, IEEE T CIRCUITS-I, V66, P2973, DOI 10.1109/TCSI.2019.2911898
   Huarcaya-Victoria Jeff, 2020, INT J MENT HEALTH AD, V20, P249, DOI [10.1007/s11469-020-00354-5, DOI 10.1007/S11469-020-00354-5]
   Ioffe S., 2015, PR MACH LEARN RES, P448
   Kheradpisheh SR, 2020, INT J NEURAL SYST, V30, DOI 10.1142/S0129065720500276
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Kothari V, 2020, PROCEEDINGS OF THE 21ST INTERNATIONAL WORKSHOP ON MOBILE COMPUTING SYSTEMS AND APPLICATIONS (HOTMOBILE'20), P45, DOI 10.1145/3376897.3377864
   Kucik Andrzej S., 2021, SNN SPACE
   Lin H, 2020, LANGUAGE MODELS ARE, V33, P1877, DOI DOI 10.48550/ARXIV.2005.14165
   Martin E, 2021, ISCIENCE, V24, DOI 10.1016/j.isci.2021.102222
   Maskey A, 2020, ENG APPL ARTIF INTEL, V96, DOI 10.1016/j.engappai.2020.103952
   Mayr C., 2019, ARXIV191102385
   Moloney D, 2014, IEEE HOT CHIP SYMP
   Panda P, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00653
   Reiter T, 2020, STRESS CHALLENGES AND IMMUNITY IN SPACE: FROM MECHANISMS TO MONITORING AND PREVENTIVE STRATEGIES, 2ND EDITION, P7, DOI 10.1007/978-3-030-16996-1_2
   Roffe Seth, 2021, NEUTRON INDUCED SING
   Sawada J, 2016, SC '16: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE FOR HIGH PERFORMANCE COMPUTING, NETWORKING, STORAGE AND ANALYSIS, P130, DOI 10.1109/SC.2016.11
   Schliebs S, 2013, EVOL SYST-GER, V4, P87, DOI 10.1007/s12530-013-9074-9
   Selva D, 2012, ACTA ASTRONAUT, V74, P50, DOI 10.1016/j.actaastro.2011.12.014
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Strubell E, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P3645
   Suresh S, 2017, INT CONF ADV COMPU, P9, DOI 10.1109/ICoAC.2017.8441181
   Tao XL, 2005, P SOC PHOTO-OPT INS, V5781, P127, DOI 10.1117/12.609117
NR 45
TC 7
Z9 7
U1 0
U2 1
PY 2021
BP 2020
EP 2030
DI 10.1109/CVPRW53098.2021.00230
UT WOS:000705890202015
DA 2023-11-16
ER

PT C
AU Zuo, L
   Chen, S
   Qu, H
   Zhang, ML
AF Zuo, Lin
   Chen, Shan
   Qu, Hong
   Zhang, Malu
BE Liu, D
   Xie, S
   Li, Y
   Zhao, D
   ElAlfy, ESM
TI A Fast Precise-Spike and Weight-Comparison Based Learning Approach for
   Evolving Spiking Neural Networks
SO NEURAL INFORMATION PROCESSING (ICONIP 2017), PT III
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 24th International Conference on Neural Information Processing (ICONIP)
CY NOV 14-18, 2017
CL Guangzhou, PEOPLES R CHINA
DE Evolving spiking neural networks; Precise spike; Weight comparison;
   Classification
ID NEURONS
AB Evolving spiking neural networks (ESNNs) evolve the output neurons dynamically based on the information presented in the incoming samples and the information stored in the network. In order to improve the learning efficiency of the existing algorithms for ESNNs, this paper presents a fast precise-spike and weight-comparison based learning algorithm, called PSWC. PSWC can dynamically add a new neuron or update the parameters of existing neurons according to the precise time of the incoming spikes and the similarities of the weights. The proposed algorithm is demonstrated on several standard data sets. The experimental results demonstrate that PSWC has a significant advantage in terms of speed performance and provides competitive results in classification accuracy compared with SpikeTemp and rank-order-based approach.
C1 [Zuo, Lin; Chen, Shan; Qu, Hong; Zhang, Malu] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 610054, Peoples R China.
RP Qu, H (corresponding author), Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 610054, Peoples R China.
EM hongqu@uestc.edu.cn
CR [Anonymous], 2015, IEEE 9 INT S INT SIG, DOI [DOI 10.1109/INTLEC.2015.7572411, DOI 10.1109/WISP.2015.7139183]
   [Anonymous], 2012 INT JOINT C NEU, DOI DOI 10.1109/IJCNN.2012.6252439
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Chan J, 2017, IEEE T COGN DEV SYST, V99, P1
   Dora S, 2016, NEUROCOMPUTING, V171, P1216, DOI 10.1016/j.neucom.2015.07.086
   Dora S, 2015, APPL SOFT COMPUT, V36, P255, DOI 10.1016/j.asoc.2015.06.062
   Dora S, 2014, IEEE IJCNN, P2415, DOI 10.1109/IJCNN.2014.6889775
   Kasabov N, 2001, IEEE T SYST MAN CY B, V31, P902, DOI 10.1109/3477.969494
   Kasabov N, 2013, NEURAL NETWORKS, V41, P188, DOI 10.1016/j.neunet.2012.11.014
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Soltic S, 2010, INT J NEURAL SYST, V20, P437, DOI 10.1142/S012906571000253X
   Thorpe S, 1998, COMPUTATIONAL NEUROSCIENCE: TRENDS IN RESEARCH, P113
   Wysoski SG, 2006, LECT NOTES COMPUT SC, V4131, P61
   Xie X., 2016, IEEE NEUR NET LEAR, V99, P1
   Zhang ML, 2017, NEUROCOMPUTING, V219, P333, DOI 10.1016/j.neucom.2016.09.044
NR 16
TC 1
Z9 1
U1 0
U2 0
PY 2017
VL 10636
BP 797
EP 804
DI 10.1007/978-3-319-70090-8_81
PN III
UT WOS:000576767300081
DA 2023-11-16
ER

PT S
AU Oster, M
   Whatley, AM
   Liu, SC
   Douglas, RJ
AF Oster, M
   Whatley, AM
   Liu, SC
   Douglas, RJ
BE Duch, W
   Kacprzyk, J
   Zadrozny, S
TI A hardware/software framework for real-time spiking systems
SO ARTIFICIAL NEURAL NETWORKS: BIOLOGICAL INSPIRATIONS - ICANN 2005, PT 1,
   PROCEEDINGS
SE Lecture Notes in Computer Science
DT Article; Proceedings Paper
CT 15th International Conference on Artificial Neural Networks (ICANN 2005)
CY SEP 11-15, 2005
CL Warsaw, POLAND
AB One focus of recent research in the field of biologically plausible neural networks is the investigation of higher-level functions such as learning, development and modulatory functions in spiking neural networks. It is desirable to explore these functions in physical neural network systems operating in real-time. We present a framework which supports such research by combining hardware spiking neurons implemented in analog VLSI (aVLSI) together with software agents. These agents are embedded in the spiking communication of the network and can change the parameters and connectivity of the network. This new approach incorporating feedback from active software agents to aVLSI hardware allows the exploration of a large variety of dynamic real-time spiking network models by adding the flexibility of software to the real-time performance of hardware.
C1 Univ Zurich, Inst Neuroinformat, CH-8057 Zurich, Switzerland.
RP Oster, M (corresponding author), Univ Zurich, Inst Neuroinformat, Winterthurerstr 190, CH-8057 Zurich, Switzerland.
EM mao@ini.phys.ethz.ch
CR Abbott LF, 2000, NAT NEUROSCI, V3, P1178, DOI 10.1038/81453
   DANTE V, 2001, 2001 TELL WORKSH NEU, P99
   Deiss SR, 1998, PULSED NEURAL NETWORKS, P157
   DOUGLAS R, 1995, ANNU REV NEUROSCI, V18, P255, DOI 10.1146/annurev.ne.18.030195.001351
   Douglas R., 1995, HDB BRAIN THEORY NEU, P282
   *IST, 2002, IST200134124
   LAZZARO J, 1993, IEEE T NEURAL NETWOR, V4, P523, DOI 10.1109/72.217193
   OSTER M, 2004, 11 IEEE INT C EL CIR
   VOGELSTEIN R, 2004, NEUROMORPHIC ENG, V1
NR 9
TC 11
Z9 11
U1 0
U2 3
PY 2005
VL 3696
BP 161
EP 166
UT WOS:000232193800026
DA 2023-11-16
ER

PT C
AU Amirsoleimani, A
   Ahmadi, M
   Ahmadi, A
AF Amirsoleimani, Amirali
   Ahmadi, Majid
   Ahmadi, Arash
GP IEEE
TI STDP-based Unsupervised Learning of Memristive Spiking Neural Network by
   Morris-Lecar Model
SO 2017 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY MAY 14-19, 2017
CL Anchorage, AK
DE Memristor; Morris-Lecar; Spike-Timing-Dependent-Plasticity (STDP);
   Spiking Neural Network (SNN); Learning; Neuron
AB These days, there is an increasing interest in implementation of spiking neural systems that can be used to perform complex computations or solve pattern recognition tasks like mammalian neocortex. In this paper, Morris-Lecar neuron neuron is utilized to implement bio-inspired memristive spiking neural network for unsupervised learning applications. The spike timing dependent plasticity learning mechanism has been applied as the learning scheme in the system. The memristive implementation of the Morris-Lecar neuron has been analyzed. Also the memristors are utilized as the synapses for the proposed system to reproduce long term potentiation and long term depression. The proposed platform is tested for pattern classification applications and the results are successfully confirmed its functionality.
C1 [Amirsoleimani, Amirali; Ahmadi, Majid; Ahmadi, Arash] Univ Windsor, Res Ctr Integrated Microsyst, Windsor, ON, Canada.
RP Amirsoleimani, A (corresponding author), Univ Windsor, Res Ctr Integrated Microsyst, Windsor, ON, Canada.
EM amirsol@uwindsor.ca; ahmadi@uwindsor.ca; aahmadi@uwindsor.ca
CR Amirsoleimani A, 2016, IEEE I C ELECT CIRC, P81, DOI 10.1109/ICECS.2016.7841137
   [Anonymous], CIRC THEOR DES ECCTD
   Biolek Z, 2009, RADIOENGINEERING, V18, P210
   Cantley KD, 2012, IEEE T NEUR NET LEAR, V23, P565, DOI 10.1109/TNNLS.2012.2184801
   CHUA LO, 1971, IEEE T CIRCUITS SYST, VCT18, P507, DOI 10.1109/TCT.1971.1083337
   FITZHUGH R, 1961, BIOPHYS J, V1, P445, DOI 10.1016/S0006-3495(61)86902-6
   Furber S, 2006, P AISB WORKSH GC5 AR, P29
   Gerstner W., 2002, SPIKING NEURON MODEL
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Indiveri G, 2013, NANOTECHNOLOGY, V24, DOI 10.1088/0957-4484/24/38/384010
   Izhikevich EM, 2001, NEURAL NETWORKS, V14, P883, DOI 10.1016/S0893-6080(01)00078-8
   MORRIS C, 1981, BIOPHYS J, V35, P193, DOI 10.1016/S0006-3495(81)84782-0
   NAGUMO J, 1962, P IRE, V50, P2061, DOI 10.1109/JRPROC.1962.288235
   Pearson MJ, 2007, IEEE T NEURAL NETWOR, V18, P1472, DOI 10.1109/TNN.2007.891203
   Sah MP, 2016, INT J BIFURCAT CHAOS, V26, DOI 10.1142/S0218127416300019
   Serrano-Gotarredona T, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00002
   Strukov DB, 2008, NATURE, V453, P80, DOI 10.1038/nature06932
   Teimoory M, 2014, IEEE I C ELECT CIRC, P562, DOI 10.1109/ICECS.2014.7050047
   Toulboul J, 2008, BIOL CYBERN, V99, P319
   Yenpo Ho, 2009, Proceedings of the 2009 IEEE/ACM International Conference on Computer-Aided Design (ICCAD 2009), P485
NR 20
TC 14
Z9 14
U1 0
U2 3
PY 2017
BP 3409
EP 3414
UT WOS:000426968703090
DA 2023-11-16
ER

PT C
AU Yang, X
   Yu, SM
   Liu, LY
   Liu, J
   Wu, NJ
AF Yang, Xu
   Yu, Shuangming
   Liu, Liyuan
   Liu, Jian
   Wu, Nanjian
BE Lu, H
   Tang, H
   Wang, Z
TI Efficient Reservoir Encoding Method for Near-Sensor Classification with
   Rate-Coding Based Spiking Convolutional Neural Networks
SO ADVANCES IN NEURAL NETWORKS - ISNN 2019, PT II
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 16th International Symposium on Neural Networks (ISNN)
CY JUL 10-12, 2019
CL Moscow, RUSSIA
DE Rate coding; Reservoir encoding; Near-sensor classification; Spiking
   neural networks
AB This paper proposes a general and efficient reservoir encoding method to encode information captured by spike-based and analog-based sensors into spike trains, which helps to realize near-sensor classification with rate-coding based spiking neural networks in real applications. The concept of reservoir is proposed to realize long-term residual information storage while encoding. This method has two configurable parameters, integration time and threshold, and they are determined optimal based on our analysis about encoding requirements. Trough different setting we proposed, reservoir encoding method can be configured as compression mode to compress sparse spike trains obtained from spike-based sensors, or conversion mode to convert pixel values captured by analog-based sensor into spike trains respectively. Verified on MNIST and SVHN dataset, the mapping relationship of information before and after encoding are linear, and the experimental results prove that rate-coding based spiking neural networks with our reservoir encoding method can realize high-accuracy and low-latency classification in two modes.
C1 [Yang, Xu; Yu, Shuangming; Liu, Liyuan; Liu, Jian; Wu, Nanjian] Chinese Acad Sci, Inst Semicond, State Key Lab Superlattices & Microstruct, Beijing 100083, Peoples R China.
   [Yang, Xu; Yu, Shuangming; Liu, Liyuan; Liu, Jian; Wu, Nanjian] Univ Chinese Acad Sci, Ctr Mat Sci & Optoelect Engn, Beijing 100049, Peoples R China.
   [Wu, Nanjian] Chinese Acad Sci, Ctr Excellence Brain Sci & Intelligence Technol, Beijing 100083, Peoples R China.
RP Liu, LY; Wu, NJ (corresponding author), Chinese Acad Sci, Inst Semicond, State Key Lab Superlattices & Microstruct, Beijing 100083, Peoples R China.; Liu, LY; Wu, NJ (corresponding author), Univ Chinese Acad Sci, Ctr Mat Sci & Optoelect Engn, Beijing 100049, Peoples R China.; Wu, NJ (corresponding author), Chinese Acad Sci, Ctr Excellence Brain Sci & Intelligence Technol, Beijing 100083, Peoples R China.
EM liuly@semi.ac.cn; nanjian@red.semi.ac.cn
CR Al Abbas T, 2016, INT EL DEVICES MEET
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Diehl P.U., 2015, 2015 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2015.7280696
   Hunsberger E., 2016, TRAINING SPIKING DEE
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Shi C, 2014, IEEE J SOLID-ST CIRC, V49, P2067, DOI 10.1109/JSSC.2014.2332134
   Wu NJ, 2018, SCI CHINA INFORM SCI, V61, DOI 10.1007/s11432-017-9303-0
   Xu Q, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1646
   Xu Y, 2017, 2017 IEEE SYMPOSIUM SERIES ON COMPUTATIONAL INTELLIGENCE (SSCI), P1219
   Yu Q, 2013, IEEE T NEUR NET LEAR, V24, P1539, DOI 10.1109/TNNLS.2013.2245677
NR 10
TC 0
Z9 0
U1 1
U2 5
PY 2019
VL 11555
BP 242
EP 251
DI 10.1007/978-3-030-22808-8_25
UT WOS:000611776800025
DA 2023-11-16
ER

PT J
AU Nobukawa, S
AF Nobukawa, Sou
TI Long-Tailed Characteristics of Neural Activity Induced by Structural
   Network Properties
SO FRONTIERS IN APPLIED MATHEMATICS AND STATISTICS
DT Article
DE excitatory postsynaptic potential; log-normal distribution; spiking
   neural network; stochastic resonance; synchronization
ID FUNCTIONAL CONNECTIVITY; INHIBITION BALANCE; BINOCULAR-RIVALRY; SYNAPTIC
   STRENGTH; NEURONAL NETWORK; BRAIN; ORGANIZATION; EXCITATION; DYNAMICS;
   MODEL
AB Over the past few decades, neuroscience studies have elucidated the structural/anatomical network characteristics in the brain and their associations with functional networks and the dynamics of neural activity. These studies have been carried out at multiple spatial-temporal scale levels, including spikes at the neural microcircuit level, neural activity at the intra-brain regional level, and neural interactions at the whole-brain network level. One of the structural and functional neural characteristics widely observed among large spatial-temporal scale ranges is long-tail distribution, typified as power-low distribution, gamma distribution, and log-normal distribution. In particular, long-tailed distributions found in excitatory postsynaptic potentials (EPSP) induce various types of neural dynamics and functions. We reviewed recent studies on neural dynamics produced by the structural long-tailed characteristics of brain neural networks. In particular, the spiking neural network with a log-normal EPSP distribution was first introduced for the essential factors to produce spontaneous activity and was extended and utilized for studies on the association of neural dynamics with the network topology depending on EPSP amplitude. Furthermore, the characteristics of the response to a steady stimulus and its dependence on E/I balance, which are widely observed under pathological conditions, were described by the spiking neural networks with EPSP long-tailed distribution. Moreover, this spiking neural network has been utilized in modeling studies of mutual interactions among local microcircuit circuits. In future studies, the implementation of more global brain network architectures in modeling studies might reveal the mechanisms by which brain dynamics and brain functions emerge from the whole brain network architecture.
C1 [Nobukawa, Sou] Chiba Inst Technol, Dept Comp Sci, Narashino, Japan.
   [Nobukawa, Sou] Natl Inst Mental Hlth, Natl Ctr Neurol & Psychiat, Dept Prevent Intervent Psychiat Disorders, Tokyo, Japan.
RP Nobukawa, S (corresponding author), Chiba Inst Technol, Dept Comp Sci, Narashino, Japan.; Nobukawa, S (corresponding author), Natl Inst Mental Hlth, Natl Ctr Neurol & Psychiat, Dept Prevent Intervent Psychiat Disorders, Tokyo, Japan.
EM nobukawa@cs.it-chiba.ac.jp
CR Ando M, 2022, FRONT AGING NEUROSCI, V14, DOI 10.3389/fnagi.2022.793298
   Ando M, 2021, FRONT NEUROSCI-SWITZ, V15, DOI 10.3389/fnins.2021.667614
   Asrican B, 2007, J NEUROSCI, V27, P14007, DOI 10.1523/JNEUROSCI.3587-07.2007
   Barral J, 2016, NAT NEUROSCI, V19, P1690, DOI 10.1038/nn.4415
   Bartos M, 2002, P NATL ACAD SCI USA, V99, P13222, DOI 10.1073/pnas.192233099
   Beggs JM, 2003, J NEUROSCI, V23, P11167
   Betzel RF, 2017, NEUROIMAGE, V160, P73, DOI 10.1016/j.neuroimage.2016.11.006
   Blake R, 2002, NAT REV NEUROSCI, V3, P13, DOI 10.1038/nrn701
   Börgers C, 2003, NEURAL COMPUT, V15, P509, DOI 10.1162/089976603321192059
   BORSELLINO A, 1972, KYBERNETIK, V10, P139, DOI 10.1007/BF00290512
   Bruining H, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-65500-4
   Bullmore ET, 2009, NAT REV NEUROSCI, V10, P186, DOI 10.1038/nrn2575
   Buzsáki G, 2014, NAT REV NEUROSCI, V15, P264, DOI 10.1038/nrn3687
   Chao HT, 2010, NATURE, V468, P263, DOI 10.1038/nature09582
   Chattopadhyaya Bidisha, 2012, Front Psychiatry, V3, P51, DOI 10.3389/fpsyt.2012.00051
   Chung MK, 2017, BRAIN CONNECT, V7, P331, DOI 10.1089/brain.2016.0481
   Cossell L, 2015, NATURE, V518, P399, DOI 10.1038/nature14182
   Destexhe A, 2009, J COMPUT NEUROSCI, V27, P493, DOI 10.1007/s10827-009-0164-4
   Fiser J, 2004, NATURE, V431, P573, DOI 10.1038/nature02907
   Foss-Feig JH, 2017, BIOL PSYCHIAT, V81, P848, DOI 10.1016/j.biopsych.2017.03.005
   GALAMBOS R, 1981, P NATL ACAD SCI-BIOL, V78, P2643, DOI 10.1073/pnas.78.4.2643
   Gavrilov N, 2018, FRONT CELL NEUROSCI, V12, DOI 10.3389/fncel.2018.00248
   Gibson JR, 2008, J NEUROPHYSIOL, V100, P2615, DOI 10.1152/jn.90752.2008
   Gilson M, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0025339
   Glausier JR, 2013, NEUROSCIENCE, V251, P90, DOI 10.1016/j.neuroscience.2012.04.044
   Goñi J, 2014, P NATL ACAD SCI USA, V111, P833, DOI 10.1073/pnas.1315529111
   Gu H, 2019, NEUROIMAGE, V185, P388, DOI 10.1016/j.neuroimage.2018.10.055
   Guo DQ, 2016, EPL-EUROPHYS LETT, V114, DOI 10.1209/0295-5075/114/30001
   Guo DQ, 2016, SCI REP-UK, V6, DOI 10.1038/srep26096
   Guo DQ, 2012, PHYS REV E, V85, DOI 10.1103/PhysRevE.85.061905
   Guo DQ, 2010, IEEE T NEURAL NETWOR, V21, P895, DOI 10.1109/TNN.2010.2044419
   Hagmann P, 2008, PLOS BIOL, V6, P1479, DOI 10.1371/journal.pbio.0060159
   Hagmann P, 2007, PLOS ONE, V2, DOI 10.1371/journal.pone.0000597
   Helmstaedter M, 2013, NATURE, V500, P168, DOI 10.1038/nature12346
   Hillyard Steven A., 1995, P665
   Hiratani N, 2016, FRONT NEURAL CIRCUIT, V10, DOI 10.3389/fncir.2016.00041
   Hiratani N, 2013, FRONT COMPUT NEUROSC, V6, DOI 10.3389/fncom.2012.00102
   Hromadka T, 2008, PLOS BIOL, V6, P124, DOI 10.1371/journal.pbio.0060016
   Izhikevich EM, 2007, DYNAMICAL SYSTEMS NE
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Kanamaru T, 2005, NEURAL COMPUT, V17, P1315, DOI 10.1162/0899766053630387
   Kanamaru T, 2017, NEURAL COMPUT, V29, P1696, DOI 10.1162/NECO_a_00965
   Klinshov VV, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0094292
   Kopsick JD, 2023, COGN COMPUT, V15, P1190, DOI 10.1007/s12559-021-09954-2
   Kusmierz L, 2020, PHYS REV LETT, V125, DOI 10.1103/PhysRevLett.125.028101
   Lefort S, 2009, NEURON, V61, P301, DOI 10.1016/j.neuron.2008.12.020
   LEHKY SR, 1995, P ROY SOC B-BIOL SCI, V259, P71, DOI 10.1098/rspb.1995.0011
   LEVELT WJM, 1967, BRIT J PSYCHOL, V58, P143, DOI 10.1111/j.2044-8295.1967.tb01068.x
   Linkenkaer-Hansen K, 2001, J NEUROSCI, V21, P1370
   Masquelier T, 2008, PLOS ONE, V3, DOI 10.1371/journal.pone.0001377
   McCormick DA, 1999, SCIENCE, V285, P541, DOI 10.1126/science.285.5427.541
   McNally JM, 2016, CURR OPIN PSYCHIATR, V29, P202, DOI 10.1097/YCO.0000000000000244
   Mikanmaa E, 2019, NEUROIMAGE, V190, P144, DOI 10.1016/j.neuroimage.2017.11.026
   Mizuseki K, 2014, PHILOS T R SOC B, V369, DOI 10.1098/rstb.2012.0530
   Mizuseki K, 2013, CELL REP, V4, P1010, DOI 10.1016/j.celrep.2013.07.039
   N??t?nen R., 1992, ATTENTION BRAIN FUNC, DOI 10.4324/9780429487354
   Nagao N, 2000, NEURAL PROCESS LETT, V12, P267, DOI 10.1023/A:1026511124944
   Nobukawa S., 2021, INT C NEURAL INFORM, P46
   Nobukawa S, 2022, COGN NEURODYNAMICS, V16, P871, DOI 10.1007/s11571-021-09757-z
   Nobukawa S, 2021, IEEE T NEUR NET LEAR, V32, P3525, DOI 10.1109/TNNLS.2020.3015208
   Nobukawa S, 2020, FRONT APPL MATH STAT, V6, DOI 10.3389/fams.2020.00019
   Nobukawa S, 2020, COGN NEURODYNAMICS, V14, P829, DOI 10.1007/s11571-020-09605-6
   Nobukawa S, 2019, J ARTIF INTELL SOFT, V9, P283, DOI 10.2478/jaiscr-2019-0009
   Nobukawa S, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-49286-8
   Nobukawa S, 2019, COGN NEURODYNAMICS, V13, P1, DOI 10.1007/s11571-018-9509-x
   Nobukawa S, 2019, NEUROIMAGE, V188, P357, DOI 10.1016/j.neuroimage.2018.12.008
   Nobukawa S, 2016, INT J NEURAL SYST, V26, DOI 10.1142/S0129065715500409
   O'Donnell BF, 2004, NEUROREPORT, V15, P1369, DOI 10.1097/01.wnr.0000127348.64681.b2
   O'Donnell Brian F, 2013, Suppl Clin Neurophysiol, V62, P101
   Oda Y, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0039955
   Omura Y, 2015, J NEUROSCI, V35, P14585, DOI 10.1523/JNEUROSCI.4944-14.2015
   Rabinovich MI, 2006, REV MOD PHYS, V78, P1213, DOI 10.1103/RevModPhys.78.1213
   Ramírez-Toraño F, 2021, CEREB CORTEX, V31, P1201, DOI 10.1093/cercor/bhaa286
   Rass O, 2010, BIPOLAR DISORD, V12, P793, DOI 10.1111/j.1399-5618.2010.00871.x
   Rojas DC, 2011, MOL AUTISM, V2, DOI 10.1186/2040-2392-2-11
   Sakata S, 2009, NEURON, V64, P404, DOI 10.1016/j.neuron.2009.09.020
   Samura T, 2015, COGN NEURODYNAMICS, V9, P265, DOI 10.1007/s11571-015-9329-1
   Seymour RA, 2020, MOL AUTISM, V11, DOI 10.1186/s13229-020-00357-y
   Shinomoto S, 2005, BIOSYSTEMS, V79, P67, DOI 10.1016/j.biosystems.2004.09.023
   SOFTKY WR, 1993, J NEUROSCI, V13, P334
   Song S, 2005, PLOS BIOL, V3, P507, DOI 10.1371/journal.pbio.0030068
   Song S, 2001, NEURON, V32, P339, DOI 10.1016/S0896-6273(01)00451-2
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Spencer KM, 2008, BIOL PSYCHIAT, V64, P369, DOI 10.1016/j.biopsych.2008.02.021
   Sporns O., 2016, NETWORKS BRAIN
   Teramae J, 2012, SCI REP-UK, V2, DOI 10.1038/srep00485
   van den Heuvel M, 2008, J NEUROSCI, V28, P10844, DOI 10.1523/JNEUROSCI.2964-08.2008
   Vogels TP, 2005, J NEUROSCI, V25, P10786, DOI 10.1523/JNEUROSCI.3508-05.2005
   Wagatsuma Nobuhiko, 2011, Front Comput Neurosci, V5, P31, DOI 10.3389/fncom.2011.00031
   WALKER P, 1975, PERCEPT PSYCHOPHYS, V18, P467, DOI 10.3758/BF03204122
   Watanabe K, 2016, LECT NOTES COMPUT SC, V9947, P115, DOI 10.1007/978-3-319-46687-3_12
   Whittington MA, 2000, INT J PSYCHOPHYSIOL, V38, P315, DOI 10.1016/S0167-8760(00)00173-2
   Wilson TW, 2007, BIOL PSYCHIAT, V62, P192, DOI 10.1016/j.biopsych.2006.07.002
   Yasumatsu N, 2008, J NEUROSCI, V28, P13592, DOI 10.1523/JNEUROSCI.0603-08.2008
   Zhigalov A, 2017, NETW NEUROSCI, V1, P143, DOI [10.1162/NETN_a_00008, 10.1162/netn_a_00008]
   Zhou TH, 2018, SCHIZOPHR RES, V201, P278, DOI 10.1016/j.schres.2018.05.027
NR 96
TC 1
Z9 1
U1 0
U2 0
PD MAY 23
PY 2022
VL 8
AR 905807
DI 10.3389/fams.2022.905807
UT WOS:000806608200001
DA 2023-11-16
ER

PT J
AU Saggie, K
   Keinan, A
   Ruppin, E
AF Saggie, K
   Keinan, A
   Ruppin, E
TI Spikes that count: rethinking spikiness in neurally embedded systems
SO NEUROCOMPUTING
DT Article
DE evolutionary computation; spiking; counting; neurocontroller analysis
AB Spiky neural networks are widely used in neural modeling, due to their biological relevance and high computational power. In this paper we investigate the usage of spiking dynamics in embedded artificial neural networks, that serve as a control mechanism for evolved autonomous agents performing a counting task. The synaptic weights and spiking dynamics are evolved using a genetic algorithm. We compare evolved spiky networks with evolved McCulloch-Pitts networks, while confronting new questions about the nature of "spikiness" and its contribution to the neurocontroller's processing. We show that in a memory-dependent task, network solutions that incorporate spiking dynamics can be less complex and easier to evolve than networks involving McCulloch-Pitts neurons. We identify and rigorously characterize two distinct properties of spiking dynamics in embedded agents: spikiness dynamic influence and spikiness functional contribution. (C) 2004 Elsevier B.V. All rights reserved.
C1 Tel Aviv Univ, Sch Comp Sci, IL-69978 Tel Aviv, Israel.
   Tel Aviv Univ, Sch Med, IL-69978 Tel Aviv, Israel.
RP Saggie, K (corresponding author), Tel Aviv Univ, Sch Comp Sci, IL-69978 Tel Aviv, Israel.
EM keren@cns.tau.ac.il; keinan@cns.tau.ac.il; ruppin@post.tau.ac.il
CR Aharonov R, 2003, NEURAL COMPUT, V15, P885, DOI 10.1162/08997660360581949
   Aharonov-Barki R, 2001, NEURAL COMPUT, V13, P691, DOI 10.1162/089976601300014529
   Bugmann G, 1997, BIOSYSTEMS, V40, P11, DOI 10.1016/0303-2647(96)01625-5
   Di Paolo EA, 2002, ADAPT BEHAV, V10, P243, DOI 10.1177/1059712302010003006
   FLOREANO D, 2001, EVOLUTIONARY ROBOTIC, V4
   Hartwell LH, 1999, NATURE, V402, pC47, DOI 10.1038/35011540
   KEINAN A, IN PRESS NEURAL COMP
   Maass W, 1999, INFORM COMPUT, V148, P202, DOI 10.1006/inco.1998.2743
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Mitchell M., 1998, INTRO GENETIC ALGORI
   Ruppin E, 2002, NAT REV NEUROSCI, V3, P132, DOI 10.1038/nrn729
NR 11
TC 2
Z9 2
U1 0
U2 5
PD JUN
PY 2004
VL 58
BP 303
EP 311
DI 10.1016/j.neucom.2004.01.060
UT WOS:000222245900048
DA 2023-11-16
ER

PT C
AU Hunter, R
   Cobb, S
   Graham, BP
AF Hunter, Russell
   Cobb, Stuart
   Graham, Bruce P.
BE Marinaro, M
   Scarpetta, S
   Yamaguchi, Y
TI Improving Recall in an Associative Neural Network of Spiking Neurons
SO DYNAMIC BRAIN - FROM NEURAL SPIKES TO BEHAVIORS
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 12th International Summer School on Neural Networks
CY DEC 05-12, 2007
CL Erice, ITALY
DE Associative memory; mammalian hippocampus; neural networks; pattern
   recall; inhibition
ID MEMORY; MODEL
AB The mammalian hippocampus has often been compared to neural networks of associative memory [6]. Previous investigation of associative memory in the brain using associative neural networks have lacked biological complexity. Using a network of biologically plausible spiking neurons we examine associative memory function against results for a simple artificial neural net [5]. We investigate implementations of methods for improving recall under biologically realistic conditions.
C1 [Hunter, Russell; Graham, Bruce P.] Univ Stirling, Dept Comp Sci & Math, Stirling FK9 4LA, Scotland.
   [Cobb, Stuart] Univ Glasgow, Div Neurosci & Biomed Syst, Glasgow G12 8QQ, Lanark, Scotland.
RP Hunter, R (corresponding author), Univ Stirling, Dept Comp Sci & Math, Stirling FK9 4LA, Scotland.
EM rhu@cs.stir.ac.uk; b.graham@cs.stir.ac.uk; s.cobb@bio.gla.ac.uk
CR [Anonymous], 2005, NEURON BOOK
   COBB SR, 1995, NATURE, V378, P75, DOI 10.1038/378075a0
   Fransen E, 1998, NETWORK-COMP NEURAL, V9, P235, DOI 10.1088/0954-898X/9/2/006
   GRAHAM B, 1995, BIOL CYBERN, V72, P337, DOI 10.1007/BF00202789
   Graham B, 1997, NETWORK-COMP NEURAL, V8, P35, DOI 10.1088/0954-898X/8/1/005
   Graham BP, 2001, NETWORK-COMP NEURAL, V12, P473, DOI 10.1088/0954-898X/12/4/304
   Pinsky P F, 1994, J Comput Neurosci, V1, P39, DOI 10.1007/BF00962717
   Sommer FT, 2001, NEURAL NETWORKS, V14, P825, DOI 10.1016/S0893-6080(01)00064-8
NR 8
TC 2
Z9 2
U1 0
U2 4
PY 2008
VL 5286
BP 137
EP +
UT WOS:000261102900012
DA 2023-11-16
ER

PT C
AU Sadovsky, E
   Jarina, R
   Orjesek, R
AF Sadovsky, Erik
   Jarina, Roman
   Orjesek, Richard
GP IEEE
TI Image Recognition Using Spiking Neural Networks
SO 2021 31ST INTERNATIONAL CONFERENCE RADIOELEKTRONIKA (RADIOELEKTRONIKA)
DT Proceedings Paper
CT 31st International Conference on Radioelektronika (RADIOELEKTRONIKA)
   Part of MAREW Conference
CY APR 19-21, 2021
CL ELECTR NETWORK
DE Spiking neural networks; N-MNIST; slayerPytorch; image recognition
AB Spiking neural networks (SNNs), a successor to today's artificial neural networks (ANNs) represent a more realistic model of biological neuron functionality and is more computationally efficient. This predisposes it for efficient real-time pattern recognition and object detection tasks. While neurons in conventional ANNs communicate using a constant output value, neurons in SNNs communicate using spikes that arc distributed in time. This functionality brings some problems in the process of encoding information into spike-like representation as well as in the SNN training. In this paper, we address some of these issues and introduced our ongoing work on SNN development. The proposed spiking multilayer perceptron and convolutional architectures were evaluated on the N-MNIST dataset for handwritten digit recognition task; the results show that the performance of the proposed solutions is comparable to the state-of-the-art and they even outperform some other related works under the comparison.
C1 [Sadovsky, Erik; Jarina, Roman] Univ Zilina, Dept Multimedia & Informat Commun Technol, FEIT, Zilina, Slovakia.
   [Orjesek, Richard] Brainit Sk, Zilina, Slovakia.
RP Sadovsky, E (corresponding author), Univ Zilina, Dept Multimedia & Informat Commun Technol, FEIT, Zilina, Slovakia.
EM erik.sadovsky@feit.uniza.sk; roman.jarina@feit.uniza.sk;
   richard.orjesek@brainit.sk
CR Anumula J, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00023
   Cohen GK, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00184
   Cramer B., 2020, IEEE T NEUR NET LEAR, P1, DOI [10.1109/INNLS.2020.3044364, DOI 10.1109/INNLS.2020.3044364]
   Gallego G, 2022, IEEE T PATTERN ANAL, V44, P154, DOI 10.1109/TPAMI.2020.3008413
   Gerstner W., 2014, NEURONAL DINAMICS SI
   Gerstner W., 2002, PIKING VERNON AFODEL, V1st
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   He WH, 2020, NEURAL NETWORKS, V132, P108, DOI 10.1016/j.neunet.2020.08.001
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Iranmehr Ensieh, 2020, 2020 8th Iranian Joint Congress on Fuzzy and intelligent Systems (CFIS), P1, DOI 10.1109/CFIS49607.2020.9238722
   Iyer L.R., 2018, ARXIV1807003CS
   Jolivet R, 2003, LECT NOTES COMPUT SC, V2714, P846
   Liu SC, 2014, IEEE T BIOMED CIRC S, V8, P453, DOI 10.1109/TBCAS.2013.2281834
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Morris RGM, 1999, BRAIN RES BULL, V50, P437, DOI 10.1016/S0361-9230(99)00182-3
   Orchard G, 2015, FRONT NEUROSCI-SWITZ, V9, DOI [10.3389/fhins.2015.00437, 10.3389/fnins.2015.00437]
   Ramesh B, 2020, IEEE T PATTERN ANAL, V42, P2767, DOI 10.1109/TPAMI.2019.2919301
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Shrestha S. B., 2018, P 32 INT C NEUR INF, P1419, DOI DOI 10.5555/3326943.3327073
   Taherkhani A, 2020, NEURAL NETWORKS, V122, P253, DOI 10.1016/j.neunet.2019.09.036
   Wu Y., 2009, P AAAI C ARTIF INTEL, DOI [10.1142/S0129065709002002, DOI 10.1142/S0129065709002002]
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
NR 22
TC 0
Z9 0
U1 1
U2 8
PY 2021
DI 10.1109/RADIOELEKTRONIKA52220.2021.9420192
UT WOS:000676146400002
DA 2023-11-16
ER

PT J
AU Chen, X
   Yuan, XP
   Fu, GM
   Luo, YY
   Yue, T
   Yan, F
   Wang, YX
   Pan, HB
AF Chen, Xuan
   Yuan, Xiaopeng
   Fu, Gaoming
   Luo, Yuanyong
   Yue, Tao
   Yan, Feng
   Wang, Yuxuan
   Pan, Hongbing
TI Effective Plug-Ins for Reducing Inference-Latency of Spiking
   Convolutional Neural Networks During Inference Phase
SO FRONTIERS IN COMPUTATIONAL NEUROSCIENCE
DT Article
DE artificial neural network; spiking neural network; deep learning; object
   classification; deep networks; spiking network conversion;
   inference-latency
ID SEQUENTIAL SAMPLING MODELS
AB Convolutional Neural Networks (CNNs) are effective and mature in the field of classification, while Spiking Neural Networks (SNNs) are energy-saving for their sparsity of data flow and event-driven working mechanism. Previous work demonstrated that CNNs can be converted into equivalent Spiking Convolutional Neural Networks (SCNNs) without obvious accuracy loss, including different functional layers such as Convolutional (Conv), Fully Connected (FC), Avg-pooling, Max-pooling, and Batch-Normalization (BN) layers. To reduce inference-latency, existing researches mainly concentrated on the normalization of weights to increase the firing rate of neurons. There are also some approaches during training phase or altering the network architecture. However, little attention has been paid on the end of inference phase. From this new perspective, this paper presents 4 stopping criterions as low-cost plug-ins to reduce the inference-latency of SCNNs. The proposed methods are validated using MATLAB and PyTorch platforms with Spiking-AlexNet for CIFAR-10 dataset and Spiking-LeNet-5 for MNIST dataset. Simulation results reveal that, compared to the state-of-the-art methods, the proposed method can shorten the average inference-latency of Spiking-AlexNet from 892 to 267 time steps (almost 3.34 times faster) with the accuracy decline from 87.95 to 87.72%. With our methods, 4 types of Spiking-LeNet-5 only need 24-70 time steps per image with the accuracy decline not more than 0.1%, while models without our methods require 52-138 time steps, almost 1.92 to 3.21 times slower than us.</p>
C1 [Chen, Xuan; Yuan, Xiaopeng; Fu, Gaoming; Luo, Yuanyong; Yue, Tao; Yan, Feng; Wang, Yuxuan; Pan, Hongbing] Nanjing Univ, Sch Elect Sci & Engn, Nanjing, Peoples R China.
RP Pan, HB (corresponding author), Nanjing Univ, Sch Elect Sci & Engn, Nanjing, Peoples R China.
EM phb@nju.edu.cn
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   [Anonymous], ARXIV160902053
   Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Bogacz R, 2006, PSYCHOL REV, V113, P700, DOI 10.1037/0033-295X.113.4.700
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Diehl PU, 2015, IEEE IJCNN
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Forstmann BU, 2016, ANNU REV PSYCHOL, V67, P641, DOI 10.1146/annurev-psych-122414-033645
   Frenkel C, 2019, IEEE T BIOMED CIRC S, V13, P999, DOI 10.1109/TBCAS.2019.2928793
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   Gong Y., 2014, INT C LEARN REPR ICL, P1
   Guo Y., 2016, ADV NEURAL INFORM PR, P1379
   Han JH, 2020, TSINGHUA SCI TECHNOL, V25, P479, DOI 10.26599/TST.2019.9010019
   Han S., 2016, P INT C LEARN REPR, DOI DOI 10.48550/ARXIV.1510.00149
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He YH, 2017, IEEE I CONF COMP VIS, P1398, DOI 10.1109/ICCV.2017.155
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889
   Jin YYZ, 2018, ADV NEUR IN, V31
   Kim S, 2020, AAAI CONF ARTIF INTE, V34, P11270
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
   Krizhevsky Alex, 2009, LEARNING MULTIPLE LA, V3, P6
   Latty T, 2011, P ROY SOC B-BIOL SCI, V278, P539, DOI 10.1098/rspb.2010.1624
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508
   Mamassian P, 2016, ANNU REV VIS SCI, V2, P459, DOI 10.1146/annurev-vision-111815-114630
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Neil D, 2016, P 31 ANN ACM S APPL
   Panda P, 2016, DES AUT TEST EUROPE, P475
   Paszke A, 2019, ADV NEUR IN, V32
   Ratcliff R, 2004, PSYCHOL REV, V111, P333, DOI 10.1037/0033-295X.111.2.333
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Roy K, 2019, NATURE, V575, P607, DOI 10.1038/s41586-019-1677-2
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tan MX, 2019, PR MACH LEARN RES, V97
   Teodorescu AR, 2013, PSYCHOL REV, V120, P1, DOI 10.1037/a0030776
   Yang X, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-019-1468-0
NR 43
TC 0
Z9 0
U1 1
U2 9
PD OCT 18
PY 2021
VL 15
AR 697469
DI 10.3389/fncom.2021.697469
UT WOS:000715248200001
DA 2023-11-16
ER

PT J
AU Lu, SJ
   Xu, F
AF Lu, Sijia
   Xu, Feng
TI Linear leaky-integrate-and-fire neuron model based spiking neural
   networks and its mapping relationship to deep neural networks
SO FRONTIERS IN NEUROSCIENCE
DT Article
DE leaky integrate-and-fire model; spiking neural networks; rectified
   linear unit; equivalence; deep neural networks
ID TIMING-DEPENDENT PLASTICITY
AB Spiking neural networks (SNNs) are brain-inspired machine learning algorithms with merits such as biological plausibility and unsupervised learning capability. Previous works have shown that converting Artificial Neural Networks (ANNs) into SNNs is a practical and efficient approach for implementing an SNN. However, the basic principle and theoretical groundwork are lacking for training a non-accuracy-loss SNN. This paper establishes a precise mathematical mapping between the biological parameters of the Linear Leaky-Integrate-and-Fire model (LIF)/SNNs and the parameters of ReLU-AN/Deep Neural Networks (DNNs). Such mapping relationship is analytically proven under certain conditions and demonstrated by simulation and real data experiments. It can serve as the theoretical basis for the potential combination of the respective merits of the two categories of neural networks.
C1 [Lu, Sijia; Xu, Feng] Fudan Univ, Key Lab Informat Sci Electromagnet Waves MoE, Shanghai, Peoples R China.
RP Xu, F (corresponding author), Fudan Univ, Key Lab Informat Sci Electromagnet Waves MoE, Shanghai, Peoples R China.
EM fengxu@fudan.edu.cn
CR Abbott LF, 2000, NAT NEUROSCI, V3, P1178, DOI 10.1038/81453
   Abbott LF, 1999, BRAIN RES BULL, V50, P303, DOI 10.1016/S0361-9230(99)00161-6
   AMARI SI, 1977, BIOL CYBERN, V27, P77, DOI 10.1007/BF00337259
   [Anonymous], 1988, NONLINEAR STOCHASTIC
   [Anonymous], 1965, CYBERNETIC PREDICTIN
   Bear M.F., 2007, NEUROSCIENCE EXPLORI, V3rd
   Bing Han, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13555, DOI 10.1109/CVPR42600.2020.01357
   Burkitt AN, 2006, BIOL CYBERN, V95, P97, DOI 10.1007/s00422-006-0082-8
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Caporale N, 2008, ANNU REV NEUROSCI, V31, P25, DOI 10.1146/annurev.neuro.31.060407.125639
   Chen Y., 2020, 16 EUROPEAN C COMPUT, P351
   Choromanska A, 2015, Arxiv, DOI arXiv:1412.0233
   Denham MJ, 2001, LECT NOTES ARTIF INT, V2036, P333
   Diehl PU, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON REBOOTING COMPUTING (ICRC)
   Diehl PU, 2015, IEEE IJCNN
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Falez P, 2019, PATTERN RECOGN, V93, P418, DOI 10.1016/j.patcog.2019.04.016
   FUKUSHIMA K, 1975, BIOL CYBERN, V20, P121, DOI 10.1007/BF00342633
   Gerstner W., 2002, SPIKING NEURON MODEL
   Ghahramani Z, 2004, LECT NOTES ARTIF INT, V3176, P72
   Glorot X., 2011, PMLR, Vvol 15, P315
   Hahnloser RHR, 2003, NEURAL COMPUT, V15, P621, DOI 10.1162/089976603321192103
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hebb D., 2005, ORG BEHAV NEUROPSYCH
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Hunsberger E., 2016, ARXIV, V1, P6566, DOI [10.13140/RG.2.2.10967.06566, DOI 10.13140/RG.2.2.10967.06566]
   Illing B, 2019, NEURAL NETWORKS, V118, P90, DOI 10.1016/j.neunet.2019.06.001
   IVAKHNENKO AG, 1971, IEEE T SYST MAN CYB, VSMC1, P364, DOI 10.1109/TSMC.1971.4308320
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Jarrett K, 2009, IEEE I CONF COMP VIS, P2146, DOI 10.1109/ICCV.2009.5459469
   Jeong DS, 2016, ADV ELECTRON MATER, V2, DOI 10.1002/aelm.201600090
   Jiale Cao, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P1, DOI 10.1007/978-3-030-58568-6_1
   Jiang XH, 2018, IEEE T NEUR NET LEAR, V29, P2684, DOI 10.1109/TNNLS.2017.2689098
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Kim S, 2020, AAAI CONF ARTIF INTE, V34, P11270
   Kostal L, 2007, EUR J NEUROSCI, V26, P2693, DOI 10.1111/j.1460-9568.2007.05880.x
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
   Kulkarni SR, 2018, NEURAL NETWORKS, V103, P118, DOI 10.1016/j.neunet.2018.03.019
   Lapique L., 1907, J PHYSL PATHOL GEN, V9, P620, DOI DOI 10.1007/S00422-007-0189-6
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maday Y., 1990, Journal of Scientific Computing, V5, P263, DOI 10.1007/BF01063118
   Marblestone AH, 2016, FRONT COMPUT NEUROSC, V10, DOI 10.3389/fncom.2016.00094
   McCulloch Warren S., 1943, BULL MATH BIOPHYS, V5, P115, DOI 10.1007/BF02478259
   MENG XL, 1992, PSYCHOL BULL, V111, P172, DOI 10.1037/0033-2909.111.1.172
   Mnih V, 2013, Arxiv, DOI arXiv:1312.5602
   Mozafari M, 2018, IEEE T NEUR NET LEAR, V29, P6178, DOI 10.1109/TNNLS.2018.2826721
   Nair V., 2010, PROC 27 INT C INT C
   Nazari S, 2019, NEUROCOMPUTING, V330, P196, DOI 10.1016/j.neucom.2018.10.066
   Pinto N, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.0040027
   Rathi Nitin, 2020, INT C LEARN REPR
   Richmond B., 2009, ENCY NEUROSCIENCE, P137
   Ruder S, 2017, Arxiv, DOI arXiv:1609.04747
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Simonyan K., 2015, VERY DEEP CONVOLUTIO, V1, P3
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Tan C, 2020, NEURAL PROCESS LETT, V52, P1675, DOI 10.1007/s11063-020-10322-8
   Tavanaei A, 2017, Arxiv, DOI arXiv:1611.03000
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Tavanaei A, 2018, NEURAL NETWORKS, V105, P294, DOI 10.1016/j.neunet.2018.05.018
   Wang XW, 2020, NEURAL NETWORKS, V125, P258, DOI 10.1016/j.neunet.2020.02.011
   Zhao ZQ, 2019, IEEE T NEUR NET LEAR, V30, P3212, DOI 10.1109/TNNLS.2018.2876865
NR 64
TC 4
Z9 4
U1 1
U2 9
PD AUG 24
PY 2022
VL 16
AR 857513
DI 10.3389/fnins.2022.857513
UT WOS:000854965200001
DA 2023-11-16
ER

PT C
AU Ye, T
   Wang, J
   Li, K
   Gao, TS
   Yi, GS
AF Ye, Ting
   Wang, Jiang
   Li, Kai
   Gao, Tianshi
   Yi, Guosheng
BE Peng, C
   Sun, J
TI Effect of local excitation-inhibition ratio on word recognition in
   hierarchical spiking neural network
SO 2021 PROCEEDINGS OF THE 40TH CHINESE CONTROL CONFERENCE (CCC)
SE Chinese Control Conference
DT Proceedings Paper
CT 40th Chinese Control Conference (CCC)
CY JUL 26-28, 2021
CL Shanghai, PEOPLES R CHINA
DE Excitation-inhibition ratio; Hierarchical spiking neural network;
   Recurrent neural network
ID BALANCE; PROPAGATION; PROJECTIONS; SELECTIVITY
AB The auditory neural coding is flexible to acoustic variability and can recognize sounds from competing sound sources. However, it remains unclear how neural circuits encode complex time-varying patterns of sound sources. Here we predict the ascending auditory pathway based on optimized hierarchical spiking neural network (HSNN). Primarily, HSNN model is built and it is found that the local state of HSNN plays an essential role in information processing, which depends on excitation/inhibition (E/I) ratio between the layers. Besides, the effect of E/I ratio on spiking activity propagation and word recognition of recurrent neural network (RNN) is investigated based on HSNN model. It is shown that the performance of sound signal propagation differs dramatically with different local network states. Particularly, an optimal E/I ratio enables HSNN to effectively propagate the spiking activity and filter out information irrelevant to word recognition. Collectively, this work provides a feasible tool to study the neural mechanism of ascending auditory pathway and digital recognition, contributing the field of machine vision and brain-like computing.
C1 [Ye, Ting; Wang, Jiang; Li, Kai; Gao, Tianshi; Yi, Guosheng] Tianjin Univ, Sch Elect Engn & Automat, Tianjin, Peoples R China.
RP Yi, GS (corresponding author), Tianjin Univ, Sch Elect Engn & Automat, Tianjin, Peoples R China.
EM guoshengyi@tju.edu.cn
CR Engel AK, 2001, NAT REV NEUROSCI, V2, P704, DOI 10.1038/35094565
   Escabí MA, 2005, J NEUROSCI, V25, P9524, DOI 10.1523/JNEUROSCI.1804-05.2005
   Felleman DJ, 1991, CEREB CORTEX, V1, P1, DOI 10.1093/cercor/1.1.1
   Guo XM, 2017, PHYSICA A, V482, P308, DOI 10.1016/j.physa.2017.04.040
   Haider B, 2006, J NEUROSCI, V26, P4535, DOI 10.1523/JNEUROSCI.5297-05.2006
   Hromadka T, 2008, PLOS BIOL, V6, P124, DOI 10.1371/journal.pbio.0060016
   Joris PX, 2004, PHYSIOL REV, V84, P541, DOI 10.1152/physrev.00029.2003
   Khatami F, 2020, PLOS COMPUT BIOL, V16, DOI 10.1371/journal.pcbi.1007558
   Klausberger T, 2008, SCIENCE, V321, P53, DOI 10.1126/science.1149381
   Levy RB, 2012, J NEUROSCI, V32, P5609, DOI 10.1523/JNEUROSCI.5158-11.2012
   Liberman Mea, 1991, NIST SPEECH DISC 7 1
   Litvak V, 2003, J NEUROSCI, V23, P3006
   Liu BH, 2011, NEURON, V71, P542, DOI 10.1016/j.neuron.2011.06.017
   Miller LM, 2002, J NEUROPHYSIOL, V87, P516, DOI 10.1152/jn.00395.2001
   Oliver DL, 2000, MICROSC RES TECHNIQ, V51, P355, DOI 10.1002/1097-0029(20001115)51:4<355::AID-JEMT5>3.3.CO;2-A
   Park KB, 2020, ROBOT CIM-INT MANUF, V63, DOI 10.1016/j.rcim.2019.101887
   Qiao M., 2016, MECH PROPAGATION SEL
   Read HL, 2008, NEUROSCIENCE, V152, P151, DOI 10.1016/j.neuroscience.2007.11.026
   Scannell JW, 1999, CEREB CORTEX, V9, P277, DOI 10.1093/cercor/9.3.277
   Tao HZW, 2014, TRENDS NEUROSCI, V37, P528, DOI 10.1016/j.tins.2014.09.001
   Taub AH, 2013, J NEUROSCI, V33, P14359, DOI 10.1523/JNEUROSCI.1748-13.2013
   Trappenberg T., 2010, FUNDAMENTALS COMPUTA
   van Rossum MCW, 2002, J NEUROSCI, V22, P1956, DOI 10.1523/JNEUROSCI.22-05-01956.2002
   Wang ST, 2006, PHYS REV LETT, V96, DOI 10.1103/PhysRevLett.96.097801
   Winer JA, 1996, P NATL ACAD SCI USA, V93, P8005, DOI 10.1073/pnas.93.15.8005
   Xue MS, 2014, NATURE, V511, P596, DOI 10.1038/nature13321
NR 26
TC 0
Z9 0
U1 1
U2 1
PY 2021
BP 8380
EP 8385
UT WOS:000931046708087
DA 2023-11-16
ER

PT C
AU Hu, J
   Tang, HJ
   Tan, KC
   Gee, SB
AF Hu, Jun
   Tang, Huajin
   Tan, Kay Chen
   Gee, Sen Bong
BE Handa, H
   Ishibuchi, H
   Ong, YS
   Tan, KC
TI A Spiking Neural Network Model for Associative Memory Using Temporal
   Codes
SO PROCEEDINGS OF THE 18TH ASIA PACIFIC SYMPOSIUM ON INTELLIGENT AND
   EVOLUTIONARY SYSTEMS, VOL 1
DT Proceedings Paper
CT 18th Asia Pacific Symposium on Intelligent and Evolutionary Systems
CY NOV 10-12, 2014
CL Singapore, SINGAPORE
DE Spiking Neural Networks (SNNs); associative memory;
   Spike-Timing-Dependent Plasticity (STDP); temporal codes
ID SYNAPTIC PLASTICITY; WORKING-MEMORY; NEURONS; ORGANIZATION; INFORMATION;
   LTD
AB Associative memory is defined as the ability to map input patterns to output patterns. Understanding how human brain performs association between unrelated patterns and stores this knowledge is one of the most important goals in computational intelligence. Although this problem has been widely studied using conventional neural networks, increasing biological findings suggest that spiking neural network can be an alternative. The proposed model encodes different memories using different subsets of encoding neurons with temporal codes. A spike-timing based learning algorithm and spike-timing-dependent plasticity (STDP) are used to form associative memory. Simulation results show that hetero-associative memory and auto-associative memory are achievable by the synaptic modification of connections between input layer and hidden layers, and recurrent connections of hidden layers, respectively.
C1 [Hu, Jun; Tan, Kay Chen; Gee, Sen Bong] Natl Univ Singapore, Dept Elect & Comp Engn, 4 Engn Dr 3, Singapore 117576, Singapore.
   [Tang, Huajin] Agcy Sci Technol & Res, Inst Infocomm Res, Singapore 138632, Singapore.
RP Hu, J (corresponding author), Natl Univ Singapore, Dept Elect & Comp Engn, 4 Engn Dr 3, Singapore 117576, Singapore.
EM junhu@nus.edu.sg; htang@i2r.a-star.edu.sg; eletankc@nus.edu.sg;
   a0039834@nus.edu.sg
CR Bear Mark F., 1994, Current Opinion in Neurobiology, V4, P389, DOI 10.1016/0959-4388(94)90101-5
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   CARR CE, 1993, ANNU REV NEUROSCI, V16, P223, DOI 10.1146/annurev.ne.16.030193.001255
   Cutsuridis V, 2010, HIPPOCAMPUS, V20, P423, DOI 10.1002/hipo.20661
   Durstewitz D, 2000, NAT NEUROSCI, V3, P1184, DOI 10.1038/81460
   Eichenbaum H, 2000, NAT REV NEUROSCI, V1, P41, DOI 10.1038/35036213
   Frankland PW, 2005, NAT REV NEUROSCI, V6, P119, DOI 10.1038/nrn1607
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Jensen O, 1996, LEARN MEMORY, V3, P243, DOI 10.1101/lm.3.2-3.243
   Jensen O, 2001, NEURAL COMPUT, V13, P2743, DOI 10.1162/089976601317098510
   Lin LN, 2005, P NATL ACAD SCI USA, V102, P6125, DOI 10.1073/pnas.0408233102
   Litvak V, 2003, J NEUROSCI, V23, P3006
   Malenka RC, 2004, NEURON, V44, P5, DOI 10.1016/j.neuron.2004.09.012
   Mongillo G, 2008, SCIENCE, V319, P1543, DOI 10.1126/science.1150769
   PERRETT DI, 1982, EXP BRAIN RES, V47, P329
   Schreiber S, 2003, NEUROCOMPUTING, V52-4, P925, DOI 10.1016/S0925-2312(02)00838-X
   Shadlen Michael N., 1994, Current Opinion in Neurobiology, V4, P569, DOI 10.1016/0959-4388(94)90059-0
   Sjöström PJ, 2002, CURR OPIN NEUROBIOL, V12, P305, DOI 10.1016/S0959-4388(02)00325-2
   Sommer FT, 2001, NEURAL NETWORKS, V14, P825, DOI 10.1016/S0893-6080(01)00064-8
   THORPE SJ, 1989, CONNECTIONISM IN PERSPECTIVE, P63
   Wiltgen BJ, 2004, NEURON, V44, P101, DOI 10.1016/j.neuron.2004.09.015
NR 21
TC 1
Z9 1
U1 0
U2 3
PY 2015
BP 561
EP 572
DI 10.1007/978-3-319-13359-1_43
UT WOS:000380764500043
DA 2023-11-16
ER

PT J
AU Ma, TM
   Hao, SH
   Wang, X
   Rodríguez-Patón, A
   Wang, SD
   Song, T
AF Ma, Tongmao
   Hao, Shaohua
   Wang, Xun
   Rodriguez-Paton, Alfonso
   Wang, Shudong
   Song, Tao
TI Double Layers Self-Organized Spiking Neural P Systems With Anti-Spikes
   for Fingerprint Recognition
SO IEEE ACCESS
DT Article
DE Fingerprint recognition; membrane computing; self-organization; spiking
   neural P systems
ID SYNAPSES WORKING; NETWORKS; RULES; CODE
AB In this paper, we design a double layers self-organized spiking neural P system with anti-spikes for fingerprint recognition. The system can self-adaptively create and delete synapse between the neurons in different layers and recognize fingerprints by the spike trains emitted out of the output neurons. Data experiments are conducted on FVC2002 and FVC2004 Databases with EER (Equal Error Rate) 9.5% around, and much less parameters are involved in our SN P systems than Capsule Neural Networks. To our best knowledge, it is the first attempt of using SN P systems to do fingerprint recognition, which can also provide theoretical models for spiking neural circuits recognizing fingerprints.
C1 [Ma, Tongmao; Hao, Shaohua; Wang, Xun; Wang, Shudong; Song, Tao] China Univ Petr, Coll Comp & Commun Engn, Qingdao 266580, Peoples R China.
   [Ma, Tongmao; Rodriguez-Paton, Alfonso; Song, Tao] Univ Politecn Madrid, Dept Inteligencia Artificial, E-28660 Madrid, Spain.
RP Wang, X; Song, T (corresponding author), China Univ Petr, Coll Comp & Commun Engn, Qingdao 266580, Peoples R China.; Song, T (corresponding author), Univ Politecn Madrid, Dept Inteligencia Artificial, E-28660 Madrid, Spain.
EM wangsyun@upc.edu.cn; tsong@upc.edu.cn
CR [Anonymous], 1995, PCASYSA PATTERN LEVE
   Binder A., 2007, P 5 BRAINST WEEK MEM, P63
   Buibas M., 2015, U.S. Patent, Patent No. [9 208 432, 9208432]
   Elmir Y., 2009, SPIKE NEURAL NETWORK
   Eurich CW, 2000, NEURAL COMPUT, V12, P1519, DOI 10.1162/089976600300015240
   Ferrara M, 2012, IEEE T INF FOREN SEC, V7, P1727, DOI 10.1109/TIFS.2012.2215326
   Gao RZ, 2016, INFORM SCIENCES, V367, P449, DOI 10.1016/j.ins.2016.05.033
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Hagan MT., 1997, NEURAL NETWORK DESIG
   Han S., 2015, C NEUR INF PROC SYST
   Ionescu M., 2002, INT J UNCONV COMPUT, V3, P974
   Ionescu M, 2006, FUND INFORM, V71, P279
   Ji LP, 2006, LECT NOTES COMPUT SC, V4221, P395
   Jin Z, 2012, EXPERT SYST APPL, V39, P6157, DOI 10.1016/j.eswa.2011.11.091
   Lam T, 2018, THESIS
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Macias-Ramos L., 2012, LECT NOTES COMPUTER, P228
   Metta VP, 2012, NEW MATH NAT COMPUT, V8, P283, DOI 10.1142/S1793005712500032
   Pan L., 2009, INT J COMPUT COMMUN, V3, P273
   Paum G, 2007, J UNIVERS COMPUT SCI, V13, P1707
   Peng H, 2019, KNOWL-BASED SYST, V163, P875, DOI 10.1016/j.knosys.2018.10.016
   Peng H, 2019, IEEE T NEUR NET LEAR, V30, P1672, DOI 10.1109/TNNLS.2018.2872999
   Peng H, 2017, NEURAL NETWORKS, V95, P66, DOI 10.1016/j.neunet.2017.08.003
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Segev A, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0158590
   Song T, 2019, IEEE T NANOBIOSCI, V18, P176, DOI 10.1109/TNB.2019.2896981
   Song T, 2019, NEURAL PROCESS LETT, V50, P1485, DOI 10.1007/s11063-018-9947-9
   Song T, 2015, IEEE T NANOBIOSCI, V14, P465, DOI 10.1109/TNB.2015.2402311
   Song T, 2015, IEEE T NANOBIOSCI, V14, P38, DOI 10.1109/TNB.2014.2367506
   Song T, 2015, NEURAL PROCESS LETT, V42, P199, DOI 10.1007/s11063-014-9352-y
   Song T, 2014, NEURAL COMPUT APPL, V24, P1833, DOI 10.1007/s00521-013-1397-8
   Song T, 2013, IEEE T NANOBIOSCI, V12, P255, DOI 10.1109/TNB.2013.2271278
   Song T, 2013, INFORM SCIENCES, V219, P197, DOI 10.1016/j.ins.2012.07.023
   SPECHT DF, 1990, NEURAL NETWORKS, V3, P109, DOI 10.1016/0893-6080(90)90049-Q
   Wang J, 2010, NEURAL COMPUT, V22, P2615, DOI 10.1162/NECO_a_00022
   Wang X, 2016, SCI REP-UK, V6, DOI 10.1038/srep27624
   Wong WJ, 2016, PATTERN RECOGN, V51, P197, DOI 10.1016/j.patcog.2015.09.032
   Wong WJ, 2013, PATTERN RECOGN LETT, V34, P1221, DOI 10.1016/j.patrec.2013.03.039
   Zeng XX, 2009, FUND INFORM, V97, P275, DOI 10.3233/FI-2009-200
   Zhang XY, 2014, NEURAL COMPUT, V26, P2925, DOI 10.1162/NECO_a_00665
   Zhang XY, 2014, NEURAL COMPUT, V26, P974, DOI 10.1162/NECO_a_00580
   Zhang XY, 2012, BIOSYSTEMS, V108, P52, DOI 10.1016/j.biosystems.2012.01.007
   Zhang Y, 2014, INT J NEURAL SYST, V24, DOI 10.1142/S0129065714500038
NR 44
TC 9
Z9 9
U1 0
U2 13
PY 2019
VL 7
BP 177562
EP 177570
DI 10.1109/ACCESS.2019.2958895
UT WOS:000509483800018
DA 2023-11-16
ER

PT C
AU Stromatias, E
   Neil, D
   Galluppi, F
   Pfeiffer, M
   Liu, SC
   Furber, S
AF Stromatias, Evangelos
   Neil, Daniel
   Galluppi, Francesco
   Pfeiffer, Michael
   Liu, Shih-Chii
   Furber, Steve
GP IEEE
TI Live Demonstration: Handwritten Digit Recognition Using Spiking Deep
   Belief Networks on SpiNNaker
SO 2015 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS (ISCAS)
SE IEEE International Symposium on Circuits and Systems
DT Proceedings Paper
CT IEEE International Symposium on Circuits and Systems (ISCAS)
CY MAY 24-27, 2015
CL Lisbon, PORTUGAL
AB We demonstrate an interactive handwritten digit recognition system with a spike-based deep belief network running in real-time on SpiNNaker, a biologically inspired many-core architecture. Results show that during the simulation a SpiNNaker chip can deliver spikes in under 1 mu s, with a classification latency in the order of tens of milliseconds, while consuming less than 0.3 W. Associated Track 8.1: Neural Networks and Systems: Spiking Neural Network circuits and systems
C1 [Stromatias, Evangelos; Furber, Steve] Univ Manchester, Sch Comp Sci, Adv Processor Technol Grp, Manchester M13 9PL, Lancs, England.
   [Galluppi, Francesco] Univ Paris 06, CNRS, Equipe Vis & Calcul Nat, Vis Inst,UMR S968,Inserm,UMR 7210,CHNO Quinze Vin, Paris, France.
   [Neil, Daniel; Pfeiffer, Michael; Liu, Shih-Chii] Univ Zurich, Inst Neuroinformat, CH-8057 Zurich, Switzerland.
   [Neil, Daniel; Pfeiffer, Michael; Liu, Shih-Chii] ETH, CH-8057 Zurich, Switzerland.
RP Stromatias, E (corresponding author), Univ Manchester, Sch Comp Sci, Adv Processor Technol Grp, Manchester M13 9PL, Lancs, England.
EM stromate@cs.man.ac.uk
CR [Anonymous], IEEE TRANSACTIONS ON
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
NR 3
TC 5
Z9 5
U1 0
U2 2
PY 2015
BP 1901
EP 1901
UT WOS:000371471002051
DA 2023-11-16
ER

PT J
AU Lin, XH
   Pi, XM
   Wang, XW
AF Lin, Xianghong
   Pi, Xiaomei
   Wang, Xiangwen
TI A supervised learning algorithm based on spike train inner products for
   recurrent spiking neural networks
SO INTERNATIONAL JOURNAL OF COMPUTING SCIENCE AND MATHEMATICS
DT Article
DE supervised learning; RSNNs; recurrent spiking neural networks; spike
   train inner product; kernel function
AB For recurrent spiking neural networks (RSNNs), constructing an efficient supervised learning algorithms is difficult because of their complex recurrent structure and an implicit nonlinear spike firing mechanism. This paper presents a supervised learning algorithm based on spike train inner products in RSNNs. The proposed algorithm transforms the discrete spike train into a continuous function using a special kernel function, and we design the corresponding error function for the backpropagation process. The proposed algorithm is successfully applied to spike train learning and pattern classification problems. The experimental results show that our algorithm has higher accuracy than the algorithm for feedback-based online local learning of weights (FOLLOW). Therefore, it is an effective method to solve the spatio-temporal pattern learning problems.
C1 [Lin, Xianghong; Pi, Xiaomei; Wang, Xiangwen] Northwest Normal Univ, Coll Comp Sci & Engn, Lanzhou 730070, Gansu, Peoples R China.
RP Lin, XH (corresponding author), Northwest Normal Univ, Coll Comp Sci & Engn, Lanzhou 730070, Gansu, Peoples R China.
EM linxh@nwnu.edu.cn; 2019211774@nwnu.edu.cn; wangxw2015@nwnu.edu.cn
CR Bellec G, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-17236-y
   Gilra A, 2017, ELIFE, V6, DOI 10.7554/eLife.28295
   Kuroe Y., 2010, 2010 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2010.5596914
   [蔺想红 Lin Xianghong], 2016, [电子学报, Acta Electronica Sinica], V44, P2877
   Lobo JL, 2020, NEURAL NETWORKS, V121, P88, DOI 10.1016/j.neunet.2019.09.004
   Park IM, 2013, IEEE SIGNAL PROC MAG, V30, P149, DOI 10.1109/MSP.2013.2251072
   Pérez J, 2018, NEURAL NETWORKS, V104, P15, DOI 10.1016/j.neunet.2018.04.002
   Tino P, 2006, NEURAL COMPUT, V18, P591, DOI 10.1162/089976606775623360
   Wang XW, 2020, NEURAL NETWORKS, V125, P258, DOI 10.1016/j.neunet.2020.02.011
   Wang XW, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00252
   WERBOS PJ, 1990, P IEEE, V78, P1550, DOI 10.1109/5.58337
   Williams RJ, 1989, NEURAL COMPUT, V1, P270, DOI 10.1162/neco.1989.1.2.270
NR 12
TC 0
Z9 0
U1 0
U2 0
PY 2023
VL 17
IS 4
BP 309
EP 319
DI 10.1504/IJCSM.2023.131629
UT WOS:001018630100001
DA 2023-11-16
ER

PT J
AU Pan, ZH
   Chua, Y
   Wu, JB
   Zhang, ML
   Li, HZ
   Ambikairajah, E
AF Pan, Zihan
   Chua, Yansong
   Wu, Jibin
   Zhang, Malu
   Li, Haizhou
   Ambikairajah, Eliathamby
TI An Efficient and Perceptually Motivated Auditory Neural Encoding and
   Decoding Algorithm for Spiking Neural Networks
SO FRONTIERS IN NEUROSCIENCE
DT Article
DE spiking neural network; neural encoding; auditory perception; spike
   database; auditory masking effects
ID SPEECH QUALITY ASSESSMENT; CORTICAL OSCILLATIONS; ITU STANDARD; PESQ;
   RECOGNITION; MASKING; RESUME
AB The auditory front-end is an integral part of a spiking neural network (SNN) when performing auditory cognitive tasks. It encodes the temporal dynamic stimulus, such as speech and audio, into an efficient, effective and reconstructable spike pattern to facilitate the subsequent processing. However, most of the auditory front-ends in current studies have not made use of recent findings in psychoacoustics and physiology concerning human listening. In this paper, we propose a neural encoding and decoding scheme that is optimized for audio processing. The neural encoding scheme, that we call Biologically plausible Auditory Encoding (BAE), emulates the functions of the perceptual components of the human auditory system, that include the cochlear filter bank, the inner hair cells, auditory masking effects from psychoacoustic models, and the spike neural encoding by the auditory nerve. We evaluate the perceptual quality of the BAE scheme using PESQ; the performance of the BAE based on sound classification and speech recognition experiments. Finally, we also built and published two spike-version of speech datasets: the Spike-TIDIGITS and the Spike-TIMIT, for researchers to use and benchmarking of future SNN research.
C1 [Pan, Zihan; Wu, Jibin; Zhang, Malu; Li, Haizhou] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore, Singapore.
   [Chua, Yansong] Agcy Sci Technol & Res, Inst Infocomm Res, Singapore, Singapore.
   [Ambikairajah, Eliathamby] Univ New South Wales, Sch Elect Engn & Telecommun, Sydney, NSW, Australia.
RP Chua, Y (corresponding author), Agcy Sci Technol & Res, Inst Infocomm Res, Singapore, Singapore.
EM James4424@gmail.com
CR Abdollahi M, 2011, BIOMED CIRC SYST C, P269, DOI 10.1109/BioCAS.2011.6107779
   Ambikairajah E, 1997, ELECTRON COMMUN ENG, V9, P165, DOI 10.1049/ecej:19970403
   Ambikairajah E, 2001, INT CONF ACOUST SPEE, P773, DOI 10.1109/ICASSP.2001.941029
   Amir A, 2017, PROC CVPR IEEE, P7388, DOI 10.1109/CVPR.2017.781
   [Anonymous], 2000, P 2 INT C LANG RES E
   [Anonymous], 2016, 160606160 ARXIV
   [Anonymous], 862 1 MAPP FUNCT TRA
   [Anonymous], P 800 METH SUBJ DET
   [Anonymous], 180701013 ARXIV
   [Anonymous], 160107140 ARXIV
   [Anonymous], 2007, MPEG VIDEO COMPRESSI
   [Anonymous], 2012, FDN MODERN AUDITORY
   [Anonymous], DESIGN IMPLEMENTATIO
   [Anonymous], 2019, PROC IEEE INT JOINT
   [Anonymous], P M IOC SPEECH GROUP
   Anumula J, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00023
   APPELLE S, 1972, PSYCHOL BULL, V78, P266, DOI 10.1037/h0033117
   Arnal LH, 2012, TRENDS COGN SCI, V16, P390, DOI 10.1016/j.tics.2012.05.003
   Barlow H., 1961, POSSIBLE PRINCIPLES, P217, DOI [10.7551/mitpress/9780262518420.003.0013, DOI 10.7551/MITPRESS/9780262518420.003.0013]
   Beerends JG, 2002, J AUDIO ENG SOC, V50, P765
   Besacier L, 2000, INT CONF ACOUST SPEE, P1085
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   BROWN JC, 1991, J ACOUST SOC AM, V89, P425, DOI 10.1121/1.400476
   BROWN JC, 1992, J ACOUST SOC AM, V92, P2698, DOI 10.1121/1.404385
   Cooke M, 2001, SPEECH COMMUN, V34, P267, DOI 10.1016/S0167-6393(00)00034-0
   Darabkh KA, 2018, COMPUT APPL ENG EDUC, V26, P285, DOI 10.1002/cae.21884
   Dean I, 2005, NAT NEUROSCI, V8, P1684, DOI 10.1038/nn1541
   Dennis J, 2013, INT CONF ACOUST SPEE, P803, DOI 10.1109/ICASSP.2013.6637759
   Ehret G, 1997, J COMP PHYSIOL A, V181, P547, DOI 10.1007/s003590050139
   El-Solh A, 2007, IEEE INT SYM MULTIM, P235, DOI 10.1109/ISM.Workshops.2007.47
   Garofolo JS, 1993, TIMIT ACOUSTIC PHONE, DOI DOI 10.35111/17GK-BN40
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Giraud AL, 2012, NAT NEUROSCI, V15, P511, DOI 10.1038/nn.3063
   Gollisch T, 2008, SCIENCE, V319, P1108, DOI 10.1126/science.1149639
   Graves Alex, 2006, P ICML, P369
   Gütig R, 2016, SCIENCE, V351, DOI 10.1126/science.aab4113
   Gütig R, 2009, PLOS BIOL, V7, DOI 10.1371/journal.pbio.1000141
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   HARRIS DM, 1979, J NEUROPHYSIOL, V42, P1083, DOI 10.1152/jn.1979.42.4.1083
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hohmann V, 2002, ACTA ACUST UNITED AC, V88, P433
   Hopfield JJ, 2004, P NATL ACAD SCI USA, V101, P6255, DOI 10.1073/pnas.0401125101
   ITU-T, 2001, P862 ITUT
   Leonard R. G., 1993, TIDIGITS LDC93S10
   Liu SC, 2010, CURR OPIN NEUROBIOL, V20, P288, DOI 10.1016/j.conb.2010.03.007
   Loiselle S, 2005, IEEE IJCNN, P2076
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Mermelstein P., 1976, 1976 Joint Workshop on Pattern Recognition and Artificial Intelligence
   Messaoud ZB, 2011, INT J SPEECH TECHNOL, V14, P393, DOI 10.1007/s10772-011-9119-z
   Mohamed AR, 2012, IEEE T AUDIO SPEECH, V20, P14, DOI 10.1109/TASL.2011.2109382
   Nadasdy Z, 2009, FRONT SYST NEUROSCI, V3, DOI 10.3389/neuro.06.006.2009
   Orchard G, 2015, FRONT NEUROSCI-SWITZ, V9, DOI [10.3389/fhins.2015.00437, 10.3389/fnins.2015.00437]
   Pan Z., 2019, PROCEEDING 2019 INT, P1
   Pan ZF, 2019, INT J ENERG RES, V43, P2583, DOI 10.1002/er.4176
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Rix AW, 2001, INT CONF ACOUST SPEE, P749, DOI 10.1109/ICASSP.2001.941023
   Rix AW, 2002, J AUDIO ENG SOC, V50, P755
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sainath TN, 2013, INT CONF ACOUST SPEE, P8614, DOI 10.1109/ICASSP.2013.6639347
   Shinn-Cunningham BG, 2008, TRENDS COGN SCI, V12, P182, DOI 10.1016/j.tics.2008.02.003
   Shrestha SB, 2018, ADV NEUR IN, V31
   Simonyan K., 2014, VERY DEEP CONVOLUTIO
   Smith JO, 1999, IEEE T SPEECH AUDI P, V7, P697, DOI 10.1109/89.799695
   SRINIVASAN MV, 1982, PROC R SOC SER B-BIO, V216, P427, DOI 10.1098/rspb.1982.0085
   Taherkhani A, 2015, IEEE T NEUR NET LEAR, V26, P3137, DOI 10.1109/TNNLS.2015.2404938
   Tamazin M, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9102166
   Tavanaei A, 2017, LECT NOTES COMPUT SC, V10639, P899, DOI 10.1007/978-3-319-70136-3_95
   Tavanaei A, 2017, NEUROCOMPUTING, V240, P191, DOI 10.1016/j.neucom.2017.01.088
   Wu J, 2019, PEER PEER NETW APPL, V12, P158, DOI 10.1007/s12083-018-0649-x
   Wu JB, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00836
   Yang F, 2016, INT CONF SIGN PROCES, P584, DOI 10.1109/ICSP.2016.7877900
   Yang MH, 2016, IEEE J SOLID-ST CIRC, V51, P2554, DOI 10.1109/JSSC.2016.2604285
   Zhang ML, 2019, AAAI CONF ARTIF INTE, P1327
   Zhang ML, 2018, IEEE T COGN DEV SYST, V10, P151, DOI 10.1109/TCDS.2017.2651943
   Zhang M, 2019, J HOSP MARKET MANAG, V28, P28, DOI 10.1080/19368623.2018.1493711
   Zhang Y, 2015, IEEE T NEUR NET LEAR, V26, P2635, DOI 10.1109/TNNLS.2015.2388544
NR 78
TC 17
Z9 17
U1 3
U2 25
PD JAN 22
PY 2020
VL 13
AR 1420
DI 10.3389/fnins.2019.01420
UT WOS:000511364500001
DA 2023-11-16
ER

PT C
AU Davies, S
   Navaridas, J
   Galluppi, F
   Furber, S
AF Davies, Sergio
   Navaridas, Javier
   Galluppi, Francesco
   Furber, Steve
GP IEEE
TI Population-Based Routing in the SpiNNaker Neuromorphic Architecture
SO 2012 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUN 10-15, 2012
CL Brisbane, AUSTRALIA
ID SPIKING NEURAL-NETWORKS; SIMULATION; NEURONS
AB SpiNNaker is a hardware-based massively-parallel real-time universal neural network simulator designed to simulate large-scale spiking neural networks. Spikes are distributed across the system using a multicast packet router. Each packet represents an event (spike) generated by a neuron. On the basis of the source of the spike (chip, core and neuron), the routers distribute the network packet across the system towards the destination neuron(s). This paper describes a novel approach to the projection routing problem that shows advantages in both the size of the routing tables generated and the computational complexity for the generation of routing tables. To achieve this, spikes are routed on the basis of the source population, leaving to the destination core the duty to propagate the received spike to the appropriate neuron(s).
C1 [Davies, Sergio; Navaridas, Javier; Galluppi, Francesco; Furber, Steve] Univ Manchester, Sch Comp Sci, Manchester, Lancs, England.
RP Davies, S (corresponding author), Univ Manchester, Sch Comp Sci, Manchester, Lancs, England.
EM daviess@cs.man.ac.uk; javier.navaridas@manchester.ac.uk;
   galluppf@cs.man.ac.uk; steve.furber@manchester.ac.uk
CR [Anonymous], 2016, AISB 06 WORKSHOP
   Binzegger T, 2009, NEURAL NETWORKS, V22, P1071, DOI 10.1016/j.neunet.2009.07.011
   Brette R, 2007, J COMPUT NEUROSCI, V23, P349, DOI 10.1007/s10827-007-0038-6
   Dally W.J., 2004, PRINCIPLES PRACTICES
   Draghici S, 2000, Int J Neural Syst, V10, P19, DOI 10.1016/S0129-0657(00)00004-1
   Fingelkurts AA, 2005, NEUROSCI BIOBEHAV R, V28, P827, DOI 10.1016/j.neubiorev.2004.10.009
   Hagmann P, 2008, PLOS BIOL, V6, P1479, DOI 10.1371/journal.pbio.0060159
   Hebb D.O., 1949, ORG BEHAV NEUROPSYCH
   JAMES M, 1992, J PARALLEL DISTR COM, V14, P221, DOI 10.1016/0743-7315(92)90065-U
   JIN X, 2010, COMPUTING SCI ENG
   Jin X., 2010, NEUR NETW 2010 IJCNN
   Jin X, 2008, IEEE IJCNN, P2812, DOI 10.1109/IJCNN.2008.4634194
   Khalili A, 2011, INT J DISTRIB SENS N, DOI 10.1155/2011/756067
   Lefort S, 2009, NEURON, V61, P301, DOI 10.1016/j.neuron.2008.12.020
   LINDSEY CS, 1995, P SOC PHOTO-OPT INS, V2492, P1194, DOI 10.1117/12.205116
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maguire LP, 2007, NEUROCOMPUTING, V71, P13, DOI 10.1016/j.neucom.2006.11.029
   Misra J, 2010, NEUROCOMPUTING, V74, P239, DOI 10.1016/j.neucom.2010.03.021
   Navaridas J, 2009, ICS'09: PROCEEDINGS OF THE 2009 ACM SIGARCH INTERNATIONAL CONFERENCE ON SUPERCOMPUTING, P286, DOI 10.1145/1542275.1542317
   Plana LA, 2007, IEEE DES TEST COMPUT, V24, P454, DOI 10.1109/MDT.2007.149
   Rast A. D., 2010, NEUR NETW 2010 IJCNN
   Rast A, 2011, NEURAL NETWORKS, V24, P961, DOI 10.1016/j.neunet.2011.06.014
   Rast AD, 2010, PROCEEDINGS OF THE 2010 COMPUTING FRONTIERS CONFERENCE (CF 2010), P21, DOI 10.1145/1787275.1787279
   Thomson AM, 2007, FRONT NEUROSCI-SWITZ, V1, P19, DOI 10.3389/neuro.01.1.1.002.2007
NR 24
TC 4
Z9 4
U1 0
U2 1
PY 2012
UT WOS:000309341301135
DA 2023-11-16
ER

PT C
AU Soupizet, T
   Jouni, Z
   Sulzbach, JF
   Benlarbi-Delai, A
   Ferreira, PM
AF Soupizet, Thomas
   Jouni, Zalfa
   Sulzbach, Joao Frischenbruder
   Benlarbi-Delai, Aziz
   Ferreira, Pietro M.
GP IEEE
TI Deep Neural Network Feasibility Using Analog Spiking Neurons
SO 2022 35TH SBC/SBMICRO/IEEE/ACM SYMPOSIUM ON INTEGRATED CIRCUITS AND
   SYSTEMS DESIGN (SBCCI 2022)
DT Proceedings Paper
CT 35th SBC/SBMicro/IEEE/ACM Symposium on Integrated Circuits and Systems
   Design (SBCCI)
CY AUG 22-26, 2022
CL ELECTR NETWORK
DE neuromorphic circuits; spiking neural networks; deep learning; low power
AB Novel non-Von-Neumann solutions based on artificial intelligence (AI) have surfaced such as the neuromorphic spiking processors in either analog or digital domain. This paper proposes to study the feasibility of deep neural networks on ultra-low-power eNeuron technology. The trade-offs in terms of deep learning capabilities and energy efficiency are highlighted. This study reveals that published eNeurons and synapses satisfy linear fittings for an excitation current greater than 200 pA and a spiking frequency higher than 150 kHz, where energy efficiency is optimal. Thus, deep learning and energy efficiency are mutually exclusive for studied analog spiking neurons.
C1 [Soupizet, Thomas] Univ Paris Saclay, CNRS, Lab Genie Elect & Elect Paris, CentraleSupelec, Gif Sur Yvette, France.
   Sorbonne Univ, Lab Genie Elect & Elect Paris, CNRS, F-75252 Paris, France.
RP Soupizet, T (corresponding author), Univ Paris Saclay, CNRS, Lab Genie Elect & Elect Paris, CentraleSupelec, Gif Sur Yvette, France.
EM soupizett@gmail.com; maris@ieee.org
CR Bartolozzi C, 2007, NEURAL COMPUT, V19, P2581, DOI 10.1162/neco.2007.19.10.2581
   Chevalier P, 2014, INT EL DEVICES MEET, DOI 10.1109/IEDM.2014.7046978
   Cybenko G., 1989, Mathematics of Control, Signals, and Systems, V2, P303, DOI 10.1007/BF02551274
   Schuman CD, 2017, Arxiv, DOI arXiv:1705.06963
   Daliri M, 2021, IET CIRC DEVICE SYST, V15, P175, DOI 10.1049/cds2.12017
   Danneville F, 2019, SOLID STATE ELECTRON, V153, P88, DOI 10.1016/j.sse.2019.01.002
   Danneville F, 2021, INT WORK CONTENT MUL, P135, DOI 10.1109/CBMI50038.2021.9461899
   Destexhe A., 1998, METHODS NEURONAL MOD, P1
   Ferreira PM, 2021, ANALOG INTEGR CIRC S, V106, P261, DOI 10.1007/s10470-020-01729-3
   Guo KY, 2017, IEEE MICRO, V37, P18, DOI 10.1109/MM.2017.39
   Indiveri G, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00073
   Krizhevsky A, 2012, NIPS 2012, V1, P1097, DOI 10.1061/(ASCE)GT.1943-5606.0001284
   Orrey D. A., 1991, Proceedings of the IEEE 1991 Custom Integrated Circuits Conference (Cat. No.91CH2994-2), p16.3/1, DOI 10.1109/CICC.1991.164040
   Powell JR, 2008, P IEEE, V96, P1247, DOI 10.1109/JPROC.2008.925411
   Sourikopoulos I, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00123
   Thakur CS, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00891
NR 16
TC 0
Z9 0
U1 0
U2 1
PY 2022
DI 10.1109/SBCCI55532.2022.9893216
UT WOS:000886205500001
DA 2023-11-16
ER

PT S
AU Roth, L
   Jahnke, A
   Klar, H
AF Roth, L
   Jahnke, A
   Klar, H
BE Mira, J
   Sandoval, F
TI Hardware requirements for spike-processing neural networks
SO FROM NATURAL TO ARTIFICIAL NEURAL COMPUTATION
SE LECTURE NOTES IN COMPUTER SCIENCE
DT Article; Proceedings Paper
CT International Workshop on Artificial Neural Networks
CY JUN 07-09, 1995
CL MALAGA-TORREMOLINOS, SPAIN
ID VISUAL-CORTEX; OSCILLATIONS; CAT
AB Experimental results suggest that the rime structure of neuronal spike trains can be relevant in neuronal signal processing. In view of these results, a shift of interest from analog neural networks to spike-processing neural networks has been observed. For tasks like image processing the simulation of these networks has to be performed with the speed of biological neural networks, We investigated the performance of available hardware and showed, that the required performance for large networks could not be achieved. According to these results we formulated concepts for the design of dedicated hardware for spike-processing neurons. For an efficient hardware implementation it is necessary to know the requisite precision for computations. Through simulations with fixed-point numbers we examined the effects of word length limitation on the behaviour of a spike-processing network. The network was able to perform its basic task as long as the word length does not fail below a certain limit. On this basis we derived conditions for the lower bound of the requisite word length.
RP Roth, L (corresponding author), TECH UNIV BERLIN,INST MIKROELEKTR,JEBENSSTR 1,D-10623 BERLIN,GERMANY.
CR ECKHORN R, 1988, BIOL CYBERN, V60, P121, DOI 10.1007/BF00202899
   ECKHORN R, 1989, P ICNN, V1, P723
   GERSTNER W, 1993, BIOL CYBERN, V68, P363, DOI 10.1007/BF00201861
   GRAY CM, 1989, P NATL ACAD SCI USA, V86, P1698, DOI 10.1073/pnas.86.5.1698
   HAMMERSTROM D, 1990, P INT JOINT C NEUR N, V2, P537
   Hartmann G., 1992, Neural Network Dynamics. Proceedings of the Workshop on Complex Dynamics in Neural Networks, P356
   HARTMANN G, 1993, 1 WORKSH FORD EL AUG, P10
   HORN D, 1991, P IJCNN, V2, P243
   KOENIG P, 1991, NEURAL COMPUT, V3, P155
   MAAS W, 1994, NCTR94021 NEUR COLT
   MALSBURG C, 1986, BIOL CYBERN, V54, P29
   PRANGE SJ, 1993, P ISSCC, P243
   RAMACHER U, 1991, P IJCNN, V1, P443
   REITBOECK HJ, 1993, P IEEE ICNN, V2, P638
   SOMPOLINSKY H, 1990, P NATL ACAD SCI USA, V87, P7200, DOI 10.1073/pnas.87.18.7200
   SPORNS O, 1989, P NATL ACAD SCI USA, V86, P7265, DOI 10.1073/pnas.86.18.7265
NR 16
TC 8
Z9 8
U1 0
U2 0
PY 1995
VL 930
BP 720
EP 727
UT WOS:A1995BF02K00095
DA 2023-11-16
ER

PT J
AU Cao, YQ
   Chen, Y
   Khosla, D
AF Cao, Yongqiang
   Chen, Yang
   Khosla, Deepak
TI Spiking Deep Convolutional Neural Networks for Energy-Efficient Object
   Recognition
SO INTERNATIONAL JOURNAL OF COMPUTER VISION
DT Article
DE Deep learning; Machine learning; Convolutional neural networks; Spiking
   neural networks; Neuromorphic circuits; Object recognition
ID RECEPTIVE FIELDS; VIEW-INVARIANT; REPRESENTATIONS; NEOCOGNITRON; MODELS
AB Deep-learning neural networks such as convolutional neural network (CNN) have shown great potential as a solution for difficult vision problems, such as object recognition. Spiking neural networks (SNN)-based architectures have shown great potential as a solution for realizing ultra-low power consumption using spike-based neuromorphic hardware. This work describes a novel approach for converting a deep CNN into a SNN that enables mapping CNN to spike-based hardware architectures. Our approach first tailors the CNN architecture to fit the requirements of SNN, then trains the tailored CNN in the same way as one would with CNN, and finally applies the learned network weights to an SNN architecture derived from the tailored CNN. We evaluate the resulting SNN on publicly available Defense Advanced Research Projects Agency (DARPA) Neovision2 Tower and CIFAR-10 datasets and show similar object recognition accuracy as the original CNN. Our SNN implementation is amenable to direct mapping to spike-based neuromorphic hardware, such as the ones being developed under the DARPA SyNAPSE program. Our hardware mapping analysis suggests that SNN implementation on such spike-based hardware is two orders of magnitude more energy-efficient than the original CNN implementation on off-the-shelf FPGA-based hardware.
C1 [Cao, Yongqiang; Chen, Yang; Khosla, Deepak] HRL Labs LLC, Malibu, CA 90265 USA.
RP Chen, Y (corresponding author), HRL Labs LLC, 3011 Malibu Canyon Rd, Malibu, CA 90265 USA.
EM ychen@hrl.com
CR [Anonymous], 2014, P 2 INT C LEARN REPR
   [Anonymous], 2013, 2013 INT JOINT C NEU
   [Anonymous], 2007, P IEEE C COMP VIS PA, DOI 10.1109/cvpr.2007.383157
   [Anonymous], 2014, 2 INT C LEARN REPR I
   [Anonymous], 2012, 2012 INT JOINT C NEU, DOI [DOI 10.1109/IJCNN.2012.6252637, 10.1109/IJCNN.2012.6252637]
   [Anonymous], 2011, BIGLEARN NIPS WORKSH
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Cao YQ, 2012, NEURAL NETWORKS, V26, P75, DOI 10.1016/j.neunet.2011.10.010
   Cao YQ, 2011, NEURAL NETWORKS, V24, P1050, DOI 10.1016/j.neunet.2011.04.004
   Ciresan D, 2012, PROC CVPR IEEE, P3642, DOI 10.1109/CVPR.2012.6248110
   Cruz-Albrecht JM, 2012, IEEE T BIOMED CIRC S, V6, P246, DOI 10.1109/TBCAS.2011.2174152
   Farabet C, 2010, IEEE INT S CIRC SYST
   Farabet C, 2013, THESIS U PARIS EST
   Fazl A, 2009, COGNITIVE PSYCHOL, V58, P1, DOI 10.1016/j.cogpsych.2008.05.001
   Folowosele F, 2011, IEEE J EM SEL TOP C, V1, P516, DOI 10.1109/JETCAS.2012.2183409
   FUKUSHIMA K, 1988, NEURAL NETWORKS, V1, P119, DOI 10.1016/0893-6080(88)90014-7
   FUKUSHIMA K, 1980, BIOL CYBERN, V36, P193, DOI 10.1007/BF00344251
   Grossberg S, 2011, NEURAL NETWORKS, V24, P1036, DOI 10.1016/j.neunet.2011.04.001
   Grossberg S, 2009, J VISION, V9, DOI 10.1167/9.4.6
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton G, 2006, COGNITIVE SCI, V30, P725, DOI 10.1207/s15516709cog0000_76
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Ho N, 2013, CONVOLUTIONAL NEUR 3
   HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837
   HUBEL DH, 1959, J PHYSIOL-LONDON, V148, P574, DOI 10.1113/jphysiol.1959.sp006308
   Itti L, 2013, NEOVISION2 ANNOTATED
   Jarrett K, 2009, IEEE I CONF COMP VIS, P2146, DOI 10.1109/ICCV.2009.5459469
   Khosla D., 2013, P SPIE, V8713
   Khosla D., 2013, P SPIE, V8745
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
   Krizhevsky A., 2009, REP T 2009
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Masquelier T, 2007, PLOS COMPUT BIOL, V3, P247, DOI 10.1371/journal.pcbi.0030031
   Merolla P., 2011, IEEE CUST INT CIRC C, P1, DOI DOI 10.1109/CICC.2011.6055294
   Perez-Carrasco J. A., 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P3085, DOI 10.1109/ICPR.2010.756
   Razavian A.S., 2014, DEEPVISION CVPR 2014
   Riesenhuber M, 1999, NAT NEUROSCI, V2, P1019, DOI 10.1038/14819
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Serre T, 2007, IEEE T PATTERN ANAL, V29, P411, DOI 10.1109/TPAMI.2007.56
NR 40
TC 428
Z9 450
U1 27
U2 183
PD MAY
PY 2015
VL 113
IS 1
SI SI
BP 54
EP 66
DI 10.1007/s11263-014-0788-3
UT WOS:000354135700005
HC Y
HP N
DA 2023-11-16
ER

PT C
AU Falez, P
   Tirilly, P
   Bilasco, IM
   Devienne, P
   Boulet, P
AF Falez, Pierre
   Tirilly, Pierre
   Bilasco, Ioan Marius
   Devienne, Philippe
   Boulet, Pierre
GP IEEE
TI Mastering the Output Frequency in Spiking Neural Networks
SO 2018 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUL 08-13, 2018
CL Rio de Janeiro, BRAZIL
DE Neurons; Pattern recognition; Biological neural networks; Training;
   Unsupervised learning
ID NEURONS
AB Image recognition tasks require multi-layer networks to achieve good performance on complex data. However, building multi-layer spiking neural networks (SNN) still remains unreachable. One cause is that the learning mechanism of these models decreases the spiking activity throughout the layers. We propose three mechanisms to solve this issue without impacting the performance of the network: target frequency threshold adaptation, which forces neurons to reach a desired frequency, binary coding, which improves the performance of the network at high levels of activity, and mirrored STDP, which improves the convergence of the training. Experiments on single layer networks show that these mechanisms preserve both the recognition rate and the level of spiking activity.
C1 [Falez, Pierre; Tirilly, Pierre; Bilasco, Ioan Marius; Devienne, Philippe; Boulet, Pierre] Univ Lille, CNRS, CRIStAL Ctr Rech Informat Signal & Automat Lille, Cent Lille,UMR 9189, F-59000 Lille, France.
   [Tirilly, Pierre] IMT Lille Douai, F-59000 Lille, France.
RP Falez, P (corresponding author), Univ Lille, CNRS, CRIStAL Ctr Rech Informat Signal & Automat Lille, Cent Lille,UMR 9189, F-59000 Lille, France.
EM pierre.falez@univ-lille.fr; pierre.tirilly@univ-lille.fr;
   ioan.bilasco@univ-lille.fr; philippe.devienne@univ-lille.fr;
   pierre.boulet@univ-lille.fr
CR Bengio Y., 2015, 150204156 ARXIV
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   Boulet P., 2017, TECH REP
   Brette R, 2015, FRONT SYST NEUROSCI, V9, DOI 10.3389/fnsys.2015.00151
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Carlson D, 2013, 2013 9TH IEEE INTERNATIONAL CONFERENCE ON DISTRIBUTED COMPUTING IN SENSOR SYSTEMS (IEEE DCOSS 2013), P1, DOI 10.1109/DCOSS.2013.73
   Cessac B, 2010, J PHYSIOL-PARIS, V104, P5, DOI 10.1016/j.jphysparis.2009.11.002
   Diehl P.U., 2015, 2015 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2015.7280696
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   Hubara I., 2016, ADV NEURAL INFORM PR, P4107
   Iyer LR, 2017, IEEE IJCNN, P1840, DOI 10.1109/IJCNN.2017.7966074
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Mishra RK, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms11552
   Montufar G. F., 2014, ADV NEURAL INFORM PR, P2924, DOI DOI 10.5555/2969033.2969153
   OConnor P., 2017, 170604159 ARXIV
   Paugam-Moisy H., 2012, HDB NATURAL COMPUTIN, V1, P1, DOI [DOI 10.1007/978-3-540-92910-9_10, 10.1007/978-3-540-92910-9_10]
   Querlioz D, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P1775, DOI 10.1109/IJCNN.2011.6033439
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707
   Shrestha A, 2017, IEEE IJCNN, P1999, DOI 10.1109/IJCNN.2017.7966096
   Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270
   Sourikopoulos I, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00123
   Srinivasan G, 2017, IEEE IJCNN, P1847, DOI 10.1109/IJCNN.2017.7966075
   Tavanaei A., 2016, CORR
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   Thorpe S, 1998, COMPUTATIONAL NEUROSCIENCE: TRENDS IN RESEARCH, P113
   Yger P., 2015, J NEUROSCIENCE, V35
   Zhao B, 2015, IEEE T NEUR NET LEAR, V26, P1963, DOI 10.1109/TNNLS.2014.2362542
NR 32
TC 2
Z9 2
U1 0
U2 0
PY 2018
UT WOS:000585967401105
DA 2023-11-16
ER

PT J
AU Claverol-Tinturé, E
   Gross, G
AF Claverol-Tinture, Enric
   Gross, Guenter
TI Commentary: Feedback stabilizes propagation of synchronous spiking in
   cortical neural networks
SO FRONTIERS IN COMPUTATIONAL NEUROSCIENCE
DT Editorial Material
DE spiking neuronal networks; feedback; timing; brain evolution;
   oscillations; synchronization
ID SYNCHRONIZATION; ARCHITECTURE; CONDUCTION; BRAIN
C1 [Claverol-Tinture, Enric] Catalonia Fdn Sci & Innovat, Barcelona, Spain.
   [Gross, Guenter] Univ N Texas, Ctr Network Neurosci, Denton, TX 76203 USA.
RP Claverol-Tinturé, E (corresponding author), Catalonia Fdn Sci & Innovat, Barcelona, Spain.
EM enric.claverol@fundaciorecerca.cat
CR Abeles M., 1982, LOCAL CORTICAL CIRCU, DOI DOI 10.1007/978-3-642-81708-3
   Barardi A, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003723
   Buzsáki G, 2013, NEURON, V80, P751, DOI 10.1016/j.neuron.2013.10.002
   Clark DA, 2001, NATURE, V411, P189, DOI 10.1038/35075564
   Devor A, 2013, NEURON, V80, P270, DOI 10.1016/j.neuron.2013.09.008
   Diesmann M, 1999, NATURE, V402, P529, DOI 10.1038/990101
   Eberle AL, 2015, MICROSCOPY-JPN, V64, P45, DOI 10.1093/jmicro/dfu104
   Gollo LL, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003548
   Gütig R, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0053063
   Hale ME, 2014, NEURON, V83, P1256, DOI 10.1016/j.neuron.2014.08.032
   Harvey MA, 2013, PLOS BIOL, V11, DOI 10.1371/journal.pbio.1001558
   Herculano-Houzel S, 2007, P NATL ACAD SCI USA, V104, P3562, DOI 10.1073/pnas.0611396104
   Herculano-Houzel S, 2014, FRONT NEUROANAT, V8, DOI 10.3389/fnana.2014.00077
   Jahnke S, 2013, FRONT COMPUT NEUROSC, V7, DOI 10.3389/fncom.2013.00153
   Jermakowicz WJ, 2007, BRAIN RES REV, V55, P264, DOI 10.1016/j.brainresrev.2007.06.003
   Liu HS, 2009, NEURON, V62, P281, DOI 10.1016/j.neuron.2009.02.025
   Mitra PP, 2014, NEURON, V83, P1273, DOI 10.1016/j.neuron.2014.08.055
   Moldakarimov S, 2015, P NATL ACAD SCI USA, V112, P2545, DOI 10.1073/pnas.1500643112
   Roelfsema PR, 1997, NATURE, V385, P157, DOI 10.1038/385157a0
   Singer W, 1999, NEURON, V24, P49, DOI 10.1016/S0896-6273(00)80821-1
   SWADLOW HA, 1978, EXP BRAIN RES, V33, P455
   Tang HL, 2014, NEURON, V83, P736, DOI 10.1016/j.neuron.2014.06.017
   Thorpe S, 1996, NATURE, V381, P520, DOI 10.1038/381520a0
   Uhlhaas PJ, 2009, FRONT INTEGR NEUROSC, V3, DOI 10.3389/neuro.07.017.2009
   Van Essen DC, 2013, NEURON, V80, P775, DOI 10.1016/j.neuron.2013.10.027
   Varela F, 2001, NAT REV NEUROSCI, V2, P229, DOI 10.1038/35067550
   Vicente R, 2008, P NATL ACAD SCI USA, V105, P17157, DOI 10.1073/pnas.0809353105
   Vogels TP, 2005, J NEUROSCI, V25, P10786, DOI 10.1523/JNEUROSCI.3508-05.2005
   Zuo YF, 2015, CURR BIOL, V25, P357, DOI 10.1016/j.cub.2014.11.065
NR 29
TC 2
Z9 2
U1 0
U2 17
PD JUN 10
PY 2015
VL 9
AR 71
DI 10.3389/fncom.2015.00071
UT WOS:000357823500001
DA 2023-11-16
ER

PT J
AU Amin, HH
AF Amin, Hesham H.
TI Automated Adaptive Threshold-Based Feature Extraction and Learning for
   Spiking Neural Networks
SO IEEE ACCESS
DT Article
DE Neurons; Computational modeling; Adaptation models; Biological system
   modeling; Biological neural networks; IP networks; Feature extraction;
   Spiking neural networks; adaptive threshold; features extraction; speech
   encoding
ID NEURONS; PLASTICITY; MODEL; STDP; ARCHITECTURES; ADAPTATION; RESPONSES;
   SYSTEM; MEMORY
AB Over the past years Spiking Neural Networks (SNNs) models became attractive as a possible bridge to enable low-power event-driven neuromorphic hardware. SNNs have a high computational power due to the implicit employment of the biologically inspired input times. SNNs employ various parameters such as neuron threshold, synaptic delays, and weights in their structures. However, SNNs applications are still limited and elementary compared with other neural network architectures such as the Convolution Neural Networks (CNNs). In this research, a new SNN-based model named Adaptive Threshold Module (ATM) and its algorithm are proposed. The proposed ATM and algorithm depend on the adaptation of the internal spiking neuron threshold level. Adapting the threshold of the neurons is employed to control the spiking neuron firing rate to uniquely extract the main features of the input pattern that is in the shape of spike trains. It is shown that this technique works as an automated feature extraction method of input patterns in an efficient and faster way than other methods. The proposed method can preserve all information of the input spike trains. Simulations of the proposed model and the algorithm, using the challenging speech TIDIGITS dataset, sound RWCP dataset, and Poisson distribution spike trains, show encouraging results. The ATM can make SNN provide an accuracy surpassing that of the current state-of-the-art SNN algorithms and conventional non-spiking learning models.
C1 [Amin, Hesham H.] Umm Al Qura Univ, Dept Comp Sci Jamoum, Mecca 25371, Saudi Arabia.
   [Amin, Hesham H.] Aswan Univ, Fac Engn, Comp & Syst Sect, Elect Engn Dept, Aswan 81542, Egypt.
RP Amin, HH (corresponding author), Umm Al Qura Univ, Dept Comp Sci Jamoum, Mecca 25371, Saudi Arabia.; Amin, HH (corresponding author), Aswan Univ, Fac Engn, Comp & Syst Sect, Elect Engn Dept, Aswan 81542, Egypt.
EM hhabuelhasan@uqu.edu.sa
CR Abdel-Hamid O, 2014, IEEE-ACM T AUDIO SPE, V22, P1533, DOI 10.1109/TASLP.2014.2339736
   Abdollahi M, 2011, BIOMED CIRC SYST C, P269, DOI 10.1109/BioCAS.2011.6107779
   Amin H. H., 2011, SPIKING NEURAL NETWO
   Amin HH, 2017, IEEE INTL CONF IND I, P206, DOI 10.1109/INDIN.2017.8104772
   Amin HH, 2005, LECT NOTES COMPUT SC, V3644, P621
   Amin HH, 2005, IEICE T INF SYST, VE88D, P1893, DOI 10.1093/ietisy/e88-d.8.1893
   [Anonymous], 2019, PROC IEEE INT JOINT
   Anumula J, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00023
   Nuño-Maganda MA, 2012, 2012 IEEE COMPUTER SOCIETY ANNUAL SYMPOSIUM ON VLSI (ISVLSI), P261, DOI 10.1109/ISVLSI.2012.46
   Baddeley R, 1997, P ROY SOC B-BIOL SCI, V264, P1775, DOI 10.1098/rspb.1997.0246
   Badshah AM, 2017, 2017 INTERNATIONAL CONFERENCE ON PLATFORM TECHNOLOGY AND SERVICE (PLATCON), P125
   Basu A, 2013, NEUROCOMPUTING, V102, P125, DOI 10.1016/j.neucom.2012.01.042
   Benda J, 2003, NEURAL COMPUT, V15, P2523, DOI 10.1162/089976603322385063
   Benda J, 2010, J NEUROPHYSIOL, V104, P2806, DOI 10.1152/jn.00240.2010
   Bollé D, 2007, INT J NEURAL SYST, V17, P241, DOI 10.1142/S012906570700110X
   Brette R, 2007, J COMPUT NEUROSCI, V23, P349, DOI 10.1007/s10827-007-0038-6
   Chacron MJ, 2007, J COMPUT NEUROSCI, V23, P301, DOI 10.1007/s10827-007-0033-y
   Chien JT, 2016, 2016 10TH INTERNATIONAL SYMPOSIUM ON CHINESE SPOKEN LANGUAGE PROCESSING (ISCSLP)
   Chiu CC, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P4774, DOI 10.1109/ICASSP.2018.8462105
   Cooney C, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20164629
   Darabkh KA, 2018, COMPUT APPL ENG EDUC, V26, P285, DOI 10.1002/cae.21884
   Dayan P., 2001, THEORETICAL NEUROSCI
   Dennis J, 2013, INT CONF ACOUST SPEE, P803, DOI 10.1109/ICASSP.2013.6637759
   Desai NS, 1999, NAT NEUROSCI, V2, P515, DOI 10.1038/9165
   Devalle F, 2018, PHYS REV E, V98, DOI 10.1103/PhysRevE.98.042214
   Dong M., 2018, PLOS ONE, V13, P1
   Falez P., 2019, 2019 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2019.8852346
   Fatahi M, 2016, 2016 6TH INTERNATIONAL CONFERENCE ON COMPUTER AND KNOWLEDGE ENGINEERING (ICCKE), P153, DOI 10.1109/ICCKE.2016.7802132
   Fathima K. S., 2015, INT J ADV RES INNOV, V3, P216
   Fayek HM, 2017, NEURAL NETWORKS, V92, P60, DOI 10.1016/j.neunet.2017.02.013
   Fontaine B, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003560
   Ganguly C, 2019, T EMERG TELECOMMUN T, V30, DOI 10.1002/ett.3561
   Gehring J, 2013, INT CONF ACOUST SPEE, P3377, DOI 10.1109/ICASSP.2013.6638284
   Gerstner W., 2002, SPIKING NEURON MODEL
   Greff K, 2017, IEEE T NEUR NET LEAR, V28, P2222, DOI 10.1109/TNNLS.2016.2582924
   Gütig R, 2009, PLOS BIOL, V7, DOI 10.1371/journal.pbio.1000141
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Huang C, 2016, PLOS COMPUT BIOL, V12, DOI 10.1371/journal.pcbi.1004984
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Kasabov N, 2016, NEURAL NETWORKS, V78, P1, DOI 10.1016/j.neunet.2015.09.011
   Kobayashi R, 2009, FRONT COMPUT NEUROSC, V3, DOI 10.3389/neuro.10.009.2009
   Ladenbauer J., 2013, BMC NEUROSCI, V14, pP299
   LeCun Y., 1998, MNIST DATABASE HANDW
   Lee C, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00435
   Lehky SR, 2010, NEURAL COMPUT, V22, P1245, DOI 10.1162/neco.2009.07-08-823
   Lehky SR, 2004, NEURAL COMPUT, V16, P1325, DOI 10.1162/089976604323057407
   Leonard R. G., 1993, LDC93S10 TIDIGITS LDC93S10 TIDIGITS
   Li JL, 2017, LECT NOTES COMPUT SC, V10635, P294, DOI 10.1007/978-3-319-70096-0_31
   Li XM, 2018, PHYSICA A, V491, P716, DOI 10.1016/j.physa.2017.08.053
   Li ZR, 2021, NEUROCOMPUTING, V428, P259, DOI 10.1016/j.neucom.2020.11.025
   Lipton Z. C., 2015, COMPUT SCI, V28, P1
   Liu D, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351591
   Liu SC, 2010, CURR OPIN NEUROBIOL, V20, P288, DOI 10.1016/j.conb.2010.03.007
   Liu YH, 2001, J COMPUT NEUROSCI, V10, P25, DOI 10.1023/A:1008916026143
   Loiselle S, 2005, IEEE IJCNN, P2076
   Luccioli S, 2019, PHYS REV E, V99, DOI 10.1103/PhysRevE.99.052412
   Maass W, 1997, NETWORK-COMP NEURAL, V8, P355, DOI 10.1088/0954-898X/8/4/002
   Marder E, 1996, P NATL ACAD SCI USA, V93, P13481, DOI 10.1073/pnas.93.24.13481
   Masquelier T, 2009, NEURAL COMPUT, V21, P1259, DOI 10.1162/neco.2008.06-08-804
   McLoughlin I, 2015, IEEE-ACM T AUDIO SPE, V23, P540, DOI 10.1109/TASLP.2015.2389618
   Metzen MG, 2016, FRONT COMPUT NEUROSC, V10, DOI 10.3389/fncom.2016.00081
   Mu RH, 2019, KSII T INTERNET INF, V13, P1738, DOI 10.3837/tiis.2019.04.001
   Nakamura S., 2000, LREC
   Neil D, 2016, P 31 ANN ACM S APPL
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   Pan Z., 2020, FRONTIERS NEUROSCI, V13, P1
   Paugam-Moisy H., 2012, HDB NATURAL COMPUTIN, V1, P1, DOI [DOI 10.1007/978-3-540-92910-9_10, 10.1007/978-3-540-92910-9_10]
   Peng H, 2019, KNOWL-BASED SYST, V163, P875, DOI 10.1016/j.knosys.2018.10.016
   Petro B, 2020, IEEE T NEUR NET LEAR, V31, P358, DOI 10.1109/TNNLS.2019.2906158
   Sainath TN, 2013, 2013 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING (ASRU), P315, DOI 10.1109/ASRU.2013.6707749
   Sak H, 2014, INTERSPEECH, P338
   Sboev A, 2018, PROCEDIA COMPUT SCI, V123, P494, DOI 10.1016/j.procs.2018.01.075
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Sheik Sadique, 2013, Biomimetic and Biohybrid Systems. Second International Conference, Living Machines 2013. Proceedings. LNCS 8064, P262, DOI 10.1007/978-3-642-39802-5_23
   Shrestha A, 2019, IEEE ACCESS, V7, P53040, DOI 10.1109/ACCESS.2019.2912200
   Taherkhani A, 2020, NEURAL NETWORKS, V122, P253, DOI 10.1016/j.neunet.2019.09.036
   Taherkhani A, 2015, IEEE T NEUR NET LEAR, V26, P3137, DOI 10.1109/TNNLS.2015.2404938
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Tavanaei A, 2017, LECT NOTES COMPUT SC, V10639, P899, DOI 10.1007/978-3-319-70136-3_95
   Tavanaei A, 2017, NEUROCOMPUTING, V240, P191, DOI 10.1016/j.neucom.2017.01.088
   Vanarse A, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17112591
   Wall JA, 2012, IEEE T NEUR NET LEAR, V23, P574, DOI 10.1109/TNNLS.2011.2178317
   Wang XW, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00252
   Wu JC, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351221
   Wu J, 2018, FRONT CELL NEUROSCI, V12, DOI 10.3389/fncel.2018.00370
   Wu Y., ARXIV180905793
   Xiao R, 2017, COMM COM INF SC, V710, P584, DOI 10.1007/978-981-10-5230-9_57
   Xiong W, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P5934, DOI 10.1109/ICASSP.2018.8461870
   Xu Y, 2017, 2017 IEEE SYMPOSIUM SERIES ON COMPUTATIONAL INTELLIGENCE (SSCI), P1219
   Yang MH, 2016, IEEE J SOLID-ST CIRC, V51, P2554, DOI 10.1109/JSSC.2016.2604285
   Yang YN, 2015, IEEE T NEUR SYS REH, V23, P946, DOI 10.1109/TNSRE.2015.2425736
   Yang YN, 2010, BIOMED CIRC SYST C, P9, DOI 10.1109/BIOCAS.2010.5709558
   Zenke F, 2018, NEURAL COMPUT, V30, P1514, DOI 10.1162/neco_a_01086
   Zhang AG, 2019, NEUROCOMPUTING, V365, P102, DOI 10.1016/j.neucom.2019.07.009
   Zhang SX, 2019, IEEE ACCESS, V7, P73685, DOI 10.1109/ACCESS.2019.2914424
   Zhang W., 2019, FRONTIERS NEUROSCI, V13, P1
   Zhang Y, 2015, IEEE T NEUR NET LEAR, V26, P2635, DOI 10.1109/TNNLS.2015.2388544
   Zhao JF, 2019, BIOMED SIGNAL PROCES, V47, P312, DOI 10.1016/j.bspc.2018.08.035
NR 98
TC 1
Z9 1
U1 4
U2 27
PY 2021
VL 9
BP 97366
EP 97383
DI 10.1109/ACCESS.2021.3094262
UT WOS:000673594800001
DA 2023-11-16
ER

PT J
AU Trousdale, J
   Hu, Y
   Shea-Brown, E
   Josic, K
AF Trousdale, James
   Hu, Yu
   Shea-Brown, Eric
   Josic, Kresimir
TI A generative spike train model with time-structured higher order
   correlations
SO FRONTIERS IN COMPUTATIONAL NEUROSCIENCE
DT Article
DE correlations; spiking neurons; neuronal networks; cumulant; neuronal
   modeling; neuronal network model; point processes
ID TEMPORAL CORRELATIONS; NEURAL VARIABILITY; CORTICAL NETWORKS; FIRING
   PATTERNS; COMPUTATION; INPUT; DYNAMICS; STIMULUS; INTEGRATION;
   STATISTICS
AB Emerging technologies are revealing the spiking activity in ever larger neural ensembles. Frequently, this spiking is far from independent, with correlations in the spike times of different cells. Understanding how such correlations impact the dynamics and function of neural ensembles remains an important open problem. Here we describe a new, generative model for correlated spike trains that can exhibit many of the features observed in data. Extending prior work in mathematical finance, this generalized thinning and shift (GTaS) model creates marginally Poisson spike trains with diverse temporal correlation structures. We give several examples which highlight the model's flexibility and utility. For instance, we use it to examine how a neural network responds to highly structured patterns of inputs. We then show that the GTaS model is analytically tractable, and derive cumulant densities of all orders in terms of model parameters. The GTaS framework can therefore be an important tool in the experimental and theoretical exploration of neural dynamics.
C1 [Trousdale, James; Josic, Kresimir] Univ Houston, Dept Math, Houston, TX 77204 USA.
   [Hu, Yu; Shea-Brown, Eric] Univ Washington, Dept Appl Math, Seattle, WA 98195 USA.
   [Shea-Brown, Eric] Univ Washington, Program Neurobiol Behav, Seattle, WA 98195 USA.
   [Josic, Kresimir] Univ Houston, Dept Biol & Biochem, Houston, TX 77204 USA.
RP Trousdale, J (corresponding author), Univ Houston, Dept Math, 641 PGH Bldg, Houston, TX 77204 USA.
EM jrtrousd@math.uh.edu
CR Abeles M, 1996, J PHYSIOLOGY-PARIS, V90, P249, DOI 10.1016/S0928-4257(97)81433-7
   Abeles M., 1991, CORTICONICS NEURAL C, DOI [10.1017/CBO9780511574566, DOI 10.1017/CBO9780511574566]
   Aertsen A, 1996, J PHYSIOLOGY-PARIS, V90, P243, DOI 10.1016/S0928-4257(97)81432-5
   Agmon-Snir H, 1998, NATURE, V393, P268, DOI 10.1038/30505
   Amari S, 2003, NEURAL COMPUT, V15, P127, DOI 10.1162/089976603321043720
   Amjad AM, 1997, J NEUROSCI METH, V73, P69, DOI 10.1016/S0165-0270(96)02214-5
   [Anonymous], 1990, HDB STOCHASTIC METHO
   Averbeck BB, 2006, NAT REV NEUROSCI, V7, P358, DOI 10.1038/nrn1888
   Aviel Y, 2002, NEUROCOMPUTING, V44, P285, DOI 10.1016/S0925-2312(02)00352-1
   Bäuerle N, 2005, ASTIN BULL, V35, P379, DOI 10.1017/S0515036100014306
   Bair W, 2001, J NEUROSCI, V21, P1676, DOI 10.1523/JNEUROSCI.21-05-01676.2001
   Barreiro AK, 2010, PHYS REV E, V81, DOI 10.1103/PhysRevE.81.011916
   Bathellier B, 2012, NEURON, V76, P435, DOI 10.1016/j.neuron.2012.07.008
   Branco T, 2011, NEURON, V69, P885, DOI 10.1016/j.neuron.2011.02.006
   Branco T, 2010, SCIENCE, V329, P1671, DOI 10.1126/science.1189664
   Brette R, 2009, NEURAL COMPUT, V21, P188, DOI [10.1162/neco.2009.12-07-657, 10.1162/neco.2008.12-07-657]
   BRILLINGER DR, 1965, ANN MATH STAT, V36, P1351, DOI 10.1214/aoms/1177699896
   Buzsáki G, 2010, NEURON, V68, P362, DOI 10.1016/j.neuron.2010.09.023
   Cain N, 2013, NEURAL COMPUT, V25, P289, DOI 10.1162/NECO_a_00398
   Chow BY, 2010, NATURE, V463, P98, DOI 10.1038/nature08652
   Churchland MM, 2010, NAT NEUROSCI, V13, P369, DOI 10.1038/nn.2501
   Cox D R, 1980, POINT PROCESSES, V12
   Daley D.J., 2002, INTRO THEORY POINT P, VI
   Ganmor E, 2011, P NATL ACAD SCI USA, V108, P9679, DOI 10.1073/pnas.1019641108
   Gasparini S, 2006, J NEUROSCI, V26, P2088, DOI 10.1523/JNEUROSCI.4428-05.2006
   Gjorgjieva J, 2011, P NATL ACAD SCI USA, V108, P19383, DOI 10.1073/pnas.1105933108
   Grün S, 2010, SPR SER COMPUT NEURO, V7, P191, DOI 10.1007/978-1-4419-5675-0_10
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Gutnisky DA, 2010, J NEUROPHYSIOL, V103, P2912, DOI 10.1152/jn.00518.2009
   Han X, 2007, PLOS ONE, V2, DOI 10.1371/journal.pone.0000299
   Hansen BJ, 2012, NEURON, V76, P590, DOI 10.1016/j.neuron.2012.08.029
   Harris KD, 2005, NAT REV NEUROSCI, V6, P399, DOI 10.1038/nrn1669
   Harris KD, 2002, NATURE, V417, P738, DOI 10.1038/nature00808
   Hebb D.O., 1949, ORG BEHAV NEUROPSYCH
   Holt GR, 1996, J NEUROPHYSIOL, V75, P1806, DOI 10.1152/jn.1996.75.5.1806
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   Ikegaya Y, 2004, SCIENCE, V304, P559, DOI 10.1126/science.1093173
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   JEFFRESS LA, 1948, J COMP PHYSIOL PSYCH, V41, P35, DOI 10.1037/h0061495
   Johnson DH, 2009, ARXIV09112524
   Joris PX, 1998, NEURON, V21, P1235, DOI 10.1016/S0896-6273(00)80643-1
   Kahn I, 2013, BRAIN RES, V1511, P33, DOI 10.1016/j.brainres.2013.03.011
   Kendall M.G., 1969, ADV THEORY STAT, V1
   Koster U., 2013, ARXIV13010050
   Krumin M, 2009, NEURAL COMPUT, V21, P1642, DOI 10.1162/neco.2009.08-08-847
   Kuhn A, 2003, NEURAL COMPUT, V15, P67, DOI 10.1162/089976603321043702
   Litwin-Kumar A, 2012, NAT NEUROSCI, V15, P1498, DOI 10.1038/nn.3220
   Luczak A, 2007, P NATL ACAD SCI USA, V104, P347, DOI 10.1073/pnas.0605643104
   Luczak A, 2013, J NEUROSCI, V33, P1684, DOI 10.1523/JNEUROSCI.2928-12.2013
   Macke JH, 2011, PHYS REV LETT, V106, DOI 10.1103/PhysRevLett.106.208102
   Macke JH, 2009, NEURAL COMPUT, V21, P397, DOI 10.1162/neco.2008.02-08-713
   Marre O, 2009, PHYS REV LETT, V102, DOI 10.1103/PhysRevLett.102.138101
   Meister M, 1999, NEURON, V22, P435, DOI 10.1016/S0896-6273(00)80700-X
   MENDEL JM, 1991, P IEEE, V79, P278, DOI 10.1109/5.75086
   Montani F, 2013, PHYSICA A, V392, P3066, DOI 10.1016/j.physa.2013.03.012
   Ohiorhenuan IE, 2010, NATURE, V466, P617, DOI 10.1038/nature09178
   Pfister JP, 2006, J NEUROSCI, V26, P9673, DOI 10.1523/JNEUROSCI.1425-06.2006
   Pillow JW, 2008, NATURE, V454, P995, DOI 10.1038/nature07140
   Reddy GD, 2008, NAT NEUROSCI, V11, P713, DOI 10.1038/nn.2116
   Rosenbaum R, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00058
   Rosenbaum RJ, 2010, FRONT COMPUT NEUROSC, V4, DOI 10.3389/fncom.2010.00009
   Ross S. M., 1995, STOCHASTIC PROCESSES, V2nd
   Roudi Y, 2009, PLOS COMPUT BIOL, V5, DOI 10.1371/journal.pcbi.1000380
   Salinas E, 2001, NAT REV NEUROSCI, V2, P539, DOI 10.1038/35086012
   Schneidman E, 2006, NATURE, V440, P1007, DOI 10.1038/nature04701
   Shadlen MN, 1998, J NEUROSCI, V18, P3870
   Shamir M, 2004, NEURAL COMPUT, V16, P1105, DOI 10.1162/089976604773717559
   Shimazaki H, 2012, PLOS COMPUT BIOL, V8, DOI 10.1371/journal.pcbi.1002385
   Shlens J, 2006, J NEUROSCI, V26, P8254, DOI 10.1523/JNEUROSCI.1282-06.2006
   Shlens J, 2009, J NEUROSCI, V29, P5022, DOI 10.1523/JNEUROSCI.5187-08.2009
   Singer W, 1999, NEURON, V24, P49, DOI 10.1016/S0896-6273(00)80821-1
   Staude B, 2010, J COMPUT NEUROSCI, V29, P327, DOI 10.1007/s10827-009-0195-x
   Stratonovich RL., 1967, TOPICS THEORY RANDOM
   Tang A, 2008, J NEUROSCI, V28, P505, DOI 10.1523/JNEUROSCI.3359-07.2008
   Tetzlaff T, 2003, NEUROCOMPUTING, V52-4, P949, DOI 10.1016/S0925-2312(02)00854-8
   Tetzlaff T, 2008, NEURAL COMPUT, V20, P2133, DOI 10.1162/neco.2008.05-07-525
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   Vasquez JC, 2012, J PHYSIOL-PARIS, V106, P120, DOI 10.1016/j.jphysparis.2011.11.001
   White B, 2012, J NEUROPHYSIOL, V108, P2383, DOI 10.1152/jn.00723.2011
   Xu NL, 2012, NATURE, V492, P247, DOI 10.1038/nature11601
   Yu S, 2011, J NEUROSCI, V31, P17514, DOI 10.1523/JNEUROSCI.3127-11.2011
NR 81
TC 11
Z9 11
U1 0
U2 14
PD JUL 17
PY 2013
VL 7
AR 84
DI 10.3389/fncom.2013.00084
UT WOS:000322074300001
DA 2023-11-16
ER

PT J
AU Yi, JH
   Xu, JC
   Chen, EH
   Chamanzar, M
   Chen, V
AF Yi, Jinho
   Xu, Jiachen
   Chen, Ethan
   Chamanzar, Maysamreza
   Chen, Vanessa
TI Multichannel Many-Class Real-Time Neural Spike Sorting With
   Convolutional Neural Networks
SO IEEE OPEN JOURNAL OF CIRCUITS AND SYSTEMS
DT Article
DE Wireless communication; Feature extraction; Real-time systems; Hardware;
   Recording; System-on-chip; Convolutional neural networks; Spike sorting;
   neural network; FPGA; neural recording
ID FUTURE
AB Real-time in-sensor spike sorting is a forefront requirement in the development of brainmachine interfaces (BMIs). This work presents the characterization, design, and efficient implementation on a field-programmable gate array (FPGA) of a novel approach to neural spike sorting intended for implantable devices based on convolutional neural networks (CNNs). While the temporal features, the shape of the spike signals, could be highly mitigated from the ambient noise, the proposed classifier effectively extracts spatial features from the multi-channel neural signal to maintain high accuracy on the noisy data. The proposed classifier mechanism was tested on real data that is recorded from multi-channel electrodes, containing 27 neural units, and the classifier achieves 93.1% accuracy despite high temporal noise in the signal. For hardware synthesis, the CNN weights are quantized to reduce the model storage requirement by 93% compared to its floating point-precision version, and the model achieves an accuracy of 86.1%.
C1 [Yi, Jinho; Xu, Jiachen; Chen, Ethan; Chamanzar, Maysamreza; Chen, Vanessa] Carnegie Mellon Univ, Elect & Comp Engn Dept, Pittsburgh, PA 15213 USA.
RP Xu, JC (corresponding author), Carnegie Mellon Univ, Elect & Comp Engn Dept, Pittsburgh, PA 15213 USA.
EM jxu3@andrew.cmu.edu
CR Adamos DA, 2008, COMPUT METH PROG BIO, V91, P232, DOI 10.1016/j.cmpb.2008.04.011
   Do AT, 2019, IEEE T VLSI SYST, V27, P126, DOI 10.1109/TVLSI.2018.2875934
   Bai SJ, 2018, Arxiv, DOI [arXiv:1803.01271, DOI 10.48550/ARXIV.1803.01271]
   Chung JE, 2017, NEURON, V95, P1381, DOI 10.1016/j.neuron.2017.08.030
   Coelho CN Jr, 2021, Arxiv, DOI arXiv:2006.10159
   Lin DD, 2016, Arxiv, DOI arXiv:1511.06393
   Dragas J, 2015, IEEE T NEUR SYS REH, V23, P149, DOI 10.1109/TNSRE.2014.2370510
   Gibson S, 2013, J NEUROSCI METH, V215, P1, DOI 10.1016/j.jneumeth.2013.01.026
   Gibson S, 2010, IEEE T NEUR SYS REH, V18, P469, DOI 10.1109/TNSRE.2010.2051683
   Gibson S, 2008, IEEE ENG MED BIO, P5015, DOI 10.1109/IEMBS.2008.4650340
   Gooch CL, 2017, ANN NEUROL, V81, P479, DOI 10.1002/ana.24897
   Gu JX, 2018, PATTERN RECOGN, V77, P354, DOI 10.1016/j.patcog.2017.10.013
   Jain AK, 1996, COMPUTER, V29, P31, DOI 10.1109/2.485891
   Jun JJ, 2017, NATURE, V551, P232, DOI 10.1038/nature24636
   Karkare V, 2013, IEEE J SOLID-ST CIRC, V48, P2230, DOI 10.1109/JSSC.2013.2264616
   Kim KH, 2000, IEEE T BIO-MED ENG, V47, P1406, DOI 10.1109/10.871415
   Kim S, 2007, IEEE T NEUR SYS REH, V15, P493, DOI 10.1109/TNSRE.2007.908429
   Krishnamoorthi R, 2018, Arxiv, DOI arXiv:1806.08342
   MALLAT SG, 1989, IEEE T PATTERN ANAL, V11, P674, DOI 10.1109/34.192463
   Nicolelis MAL, 2003, NAT REV NEUROSCI, V4, P417, DOI 10.1038/nrn1105
   Pedreira C, 2012, J NEUROSCI METH, V211, P58, DOI 10.1016/j.jneumeth.2012.07.010
   Reddy JW, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00745
   Rey HG, 2015, BRAIN RES BULL, V119, P106, DOI 10.1016/j.brainresbull.2015.04.007
   Saif-ur-Rehman M, 2019, J NEURAL ENG, V16, DOI 10.1088/1741-2552/ab1e63
   Schäffer L, 2017, IEEE INT SYMP CIRC S
   Schaffer L, 2021, IEEE T BIO-MED ENG, V68, P99, DOI 10.1109/TBME.2020.2996281
   Suzuki K, 2011, ARTIFICIAL NEURAL NETWORKS - METHODOLOGICAL ADVANCES AND BIOMEDICAL APPLICATIONS, P1, DOI 10.5772/644
   Valencia D, 2021, IEEE T NEUR SYS REH, V29, P206, DOI 10.1109/TNSRE.2020.3043403
   Valencia D, 2019, IEEE T BIOMED CIRC S, V13, P1700, DOI 10.1109/TBCAS.2019.2947618
   Valencia D, 2019, IEEE T BIOMED CIRC S, V13, P481, DOI 10.1109/TBCAS.2019.2907882
   Werner CT, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.01174
   Wong KY, 2019, IEEE IJCNN
   Yang YN, 2017, IEEE T BIOMED CIRC S, V11, P743, DOI 10.1109/TBCAS.2017.2679032
   Zhang LI, 2001, NAT NEUROSCI, V4, P1207, DOI 10.1038/nn753
NR 34
TC 1
Z9 1
U1 2
U2 4
PY 2022
VL 3
BP 168
EP 179
DI 10.1109/OJCAS.2022.3184302
UT WOS:000856100000003
DA 2023-11-16
ER

PT C
AU Stanojevic, A
   Eleftheriou, E
   Cherubini, G
   Wozniak, S
   Pantazi, A
   Gerstner, W
AF Stanojevic, Ana
   Eleftheriou, Evangelos
   Cherubini, Giovanni
   Wozniak, Stanislaw
   Pantazi, Angeliki
   Gerstner, Wulfram
GP IEEE
TI APPROXIMATING RELU NETWORKS BY SINGLE-SPIKE COMPUTATION
SO 2022 IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, ICIP
SE IEEE International Conference on Image Processing ICIP
DT Proceedings Paper
CT IEEE International Conference on Image Processing (ICIP)
CY OCT 16-19, 2022
CL Bordeaux, FRANCE
DE spiking neural network; one spike per neuron; image processing; ReLU;
   efficient classification
AB Developing energy-saving neural network models is a topic of rapidly increasing interest in the artificial intelligence community. Spiking neural networks (SNNs) are biologically inspired models that strive to leverage the energy efficiency stemming from a long process of evolution under limited resources. In this paper we propose a SNN model where each neuron integrates piecewise linear postsynaptic potentials caused by input spikes and a positive bias, and spikes maximally once. Transformation of such a network into the ANN domain yields an approximation of a standard ReLU network, leading to a facilitated training based on backpropagation and an adaptation of the batch normalization. With backpropagation-trained weights, SNN inference offers a sparse-signal and low-latency classification, which can be readily adapted for a stream of input patterns, lending itself to an efficient hardware implementation. The supervised classification of MNIST and Fashion-MNIST datasets, using this approach, provides accuracy close to that of an ANN and surpassing other single-spike SNNs.
C1 [Stanojevic, Ana; Eleftheriou, Evangelos; Cherubini, Giovanni; Wozniak, Stanislaw; Pantazi, Angeliki] IBM Res Zurich, Ruschlikon, Switzerland.
   [Stanojevic, Ana; Gerstner, Wulfram] Ecole Polytech Fed Lausanne EPFL, Lausanne, Switzerland.
RP Stanojevic, A (corresponding author), IBM Res Zurich, Ruschlikon, Switzerland.; Stanojevic, A (corresponding author), Ecole Polytech Fed Lausanne EPFL, Lausanne, Switzerland.
CR Bellec G, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-17236-y
   Bianco S, 2018, IEEE ACCESS, V6, P64270, DOI 10.1109/ACCESS.2018.2877890
   Frémaux N, 2016, FRONT NEURAL CIRCUIT, V9, DOI 10.3389/fncir.2015.00085
   Gallego G, 2022, IEEE T PATTERN ANAL, V44, P154, DOI 10.1109/TPAMI.2020.3008413
   Gardner B, 2015, NEURAL COMPUT, V27, P2548, DOI 10.1162/NECO_a_00790
   Göltz J, 2021, NAT MACH INTELL, V3, P823, DOI 10.1038/s42256-021-00388-x
   Gollisch T, 2008, SCIENCE, V319, P1108, DOI 10.1126/science.1149639
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   HUBEL DH, 1959, J PHYSIOL-LONDON, V148, P574, DOI 10.1113/jphysiol.1959.sp006308
   Huh D, 2018, ADV NEUR IN, V31
   Illing B, 2019, NEURAL NETWORKS, V118, P90, DOI 10.1016/j.neunet.2019.06.001
   IuliaMComsa Krzysztof Potempa, 2020, ICASSP 2020 2020 IEE, P8529
   Johansson RS, 2004, NAT NEUROSCI, V7, P170, DOI 10.1038/nn1177
   Kandel Eric R, 2000, PRINCIPLES NEURAL SC, V4
   Kheradpisheh SR, 2020, INT J NEURAL SYST, V30, DOI 10.1142/S0129065720500276
   Kingma DP., 2017, ARXIV
   Kubke MF, 2002, J NEUROSCI, V22, P7671
   Lichtsteiner Patrick, 2008, IEEE Journal of Solid-State Circuits, V43, P566, DOI 10.1109/JSSC.2007.914337
   Markram Henry, 2011, Front Synaptic Neurosci, V3, P4, DOI 10.3389/fnsyn.2011.00004
   McCulloch WS., 1943, B MATH BIOPHYS, V5, P115, DOI DOI 10.1007/BF02478259
   Mostafa H, 2018, IEEE T NEUR NET LEAR, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   OKEEFE J, 1993, HIPPOCAMPUS, V3, P317, DOI 10.1002/hipo.450030307
   Wozniak S, 2020, NAT MACH INTELL, V2, P325, DOI 10.1038/s42256-020-0187-0
   Zenke F, 2018, NEURAL COMPUT, V30, P1514, DOI 10.1162/neco_a_01086
   Zenke F, 2015, NAT COMMUN, V6, DOI 10.1038/ncomms7922
   Zhang M, 2021, IEEE INT CONF COMM, DOI 10.1109/ICCWorkshops50388.2021.9473609
NR 27
TC 2
Z9 2
U1 0
U2 0
PY 2022
BP 1901
EP 1905
DI 10.1109/ICIP46576.2022.9897692
UT WOS:001058109502003
DA 2023-11-16
ER

PT C
AU Tao, TM
   Ma, HZ
   Chen, QK
   Tan, SR
   Liu, EX
   Li, EP
AF Tao, Tuomin
   Ma, Hanzhi
   Chen, Quankun
   Tan, Shurun
   Liu, En-Xiao
   Li, Er-Ping
GP IEEE
TI Electromagnetic Impact of Interconnect Resistance on STDP
   Characteristics in Neuromorphic Crossbar Array
SO 2021 13TH INTERNATIONAL WORKSHOP ON THE ELECTROMAGNETIC COMPATIBILITY OF
   INTEGRATED CIRCUITS (EMC COMPO 2021)
DT Proceedings Paper
CT 13th International Workshop on the Electromagnetic Compatibility of
   Integrated Circuits (EMC Compo)
CY MAR 09-11, 2022
CL ELECTR NETWORK
DE crossbar array; memristor; parasitic effect; spiking neural network;
   neuromorphic chip
ID NETWORK; CIRCUIT
AB This paper presents an unsupervised-learning spiking neural network model based on spike-timing-dependent plasticity (STDP), which is then mapped into a neuromorphic crossbar array. Furthermore, the electromagnetic impact of the interconnect resistance on the STDP performance in the crossbar array is analyzed.
C1 [Tao, Tuomin; Ma, Hanzhi; Chen, Quankun; Li, Er-Ping] Zhejiang Univ, Coll Informat Sci & Elect Engn, Hangzhou, Peoples R China.
   [Tan, Shurun] Zhejiang Univ, Coll Informat Sci & Elect Engn, ZJU UIUC Inst, Hangzhou, Peoples R China.
   [Liu, En-Xiao] ASTAR, IHPC, Singapore, Singapore.
RP Tao, TM (corresponding author), Zhejiang Univ, Coll Informat Sci & Elect Engn, Hangzhou, Peoples R China.
EM tuomin@zju.edu.cn; mahanzhi@zju.edu.cn; chenquankun@zju.edu.cn;
   srtan@intl.zju.edu.cn; liuex@ihpc.a-star.edu.sg; liep@zju.edu.cn
CR Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Lee W, 2019, IEEE T COMP PACK MAN, V9, P2066, DOI 10.1109/TCPMT.2019.2917910
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Serrano-Gotarredona T, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00002
   Strukov DB, 2008, NATURE, V453, P80, DOI 10.1038/nature06932
   Tao TM, 2021, IEEE T CIRCUITS-I, V68, P1906, DOI 10.1109/TCSI.2021.3060798
   Yao P, 2020, NATURE, V577, P641, DOI 10.1038/s41586-020-1942-4
   Yu S., 2016, SYNTH LECT EMERG ENG, V2, P1
NR 9
TC 0
Z9 0
U1 0
U2 4
PY 2022
BP 16
EP 18
DI 10.1109/EMCCOMPO52133.2022.9758593
UT WOS:000833540000004
DA 2023-11-16
ER

PT C
AU Neto, JRD
   Cajueiro, JPC
   Ranhel, J
AF de Oliveira Neto, Jose Rodrigues
   Cerquinho Cajueiro, Joao Paulo
   Ranhel, Joao
GP IEEE
TI Neural Encoding and Spike Generation for Spiking Neural Networks
   implemented in FPGA
SO 25TH INTERNATIONAL CONFERENCE ON ELECTRONICS, COMMUNICATIONS AND
   COMPUTERS (CONIELECOMP 2015)
SE International Conference on Electronics Communications and Computers
   CONIELECOMP
DT Proceedings Paper
CT International conference on electronics, communications and computers
CY FEB 25-27, 2015
CL Puebla, MEXICO
AB In this article a new digital system for generating spike pulses is presented. In real time, the system can convert digital values into artificial neural spikes for Spiking Neural Networks (SNN). The digital system can perform three basic functions: to generate spikes, to convert digital data values into pulse trains and, additionally, to encode spike trains on three major code classes reported in the scientific literature of neuroscience: rate coding, temporal coding or population coding. This system is therefore a digital interface between the physical world, computer systems, or digital data and SNN that are processed in real time. A functional prototype module was developed in a Ciclone IV FPGA using less then 300 logic blocks.
C1 [de Oliveira Neto, Jose Rodrigues; Ranhel, Joao] Univ Fed Pernambuco, Dept Eletron & Sistemas, BR-50740550 Recife, PE, Brazil.
   [Cerquinho Cajueiro, Joao Paulo] Univ Fed Pernambuco, Dept Engn Mecan, BR-50740550 Recife, PE, Brazil.
RP Neto, JRD (corresponding author), Univ Fed Pernambuco, Dept Eletron & Sistemas, BR-50740550 Recife, PE, Brazil.
EM joserodrigues.oliveiraneto@ufpe.br; joaopaulo@ufpe.br; jranhel@ieee.org
CR Adrian ED, 1926, J PHYSIOL-LONDON, V61, P151, DOI 10.1113/jphysiol.1926.sp002281
   [Anonymous], 2014, TRENDS NEUROSCIENCES
   [Anonymous], 2001, THEORETICAL NEUROSCI
   [Anonymous], 2013, BMC NEUROSCIENCE S1
   [Anonymous], 1997, IEEE T NEURAL NETWOR
   Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Cheung Kit, 2012, Artificial Neural Networks and Machine Learning - ICANN 2012. Proceedings of the 22nd International Conference on Artificial Neural Networks, P113, DOI 10.1007/978-3-642-33269-2_15
   DEOLIVEIRANETO JR, 2014, NEUR NETW IJCNN 2014, P3186
   Gerstner W, 1997, P NATL ACAD SCI USA, V94, P12740, DOI 10.1073/pnas.94.24.12740
   Gerstner W., 2002, SPIKING NEURON MODEL
   Ghavami S., 2014, CORR
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   IBM, BRAIN POWER SCI IBM
   Jie Xing, 2012, Proceedings of the 2012 IEEE International Symposium on Information Theory - ISIT, P2964, DOI 10.1109/ISIT.2012.6284081
   Kandel E. R., 2000, PRINCIPLES NEURAL SC, V4
   Kostal L, 2007, EUR J NEUROSCI, V26, P2693, DOI 10.1111/j.1460-9568.2007.05880.x
   Kumar A. A., 2010, NAT REV NEUROSCI
   mery E., 2004, MULTIPLE NEURAL SPIK, V7, P461
   Neftci E, 2013, P NATL ACAD SCI USA, V110, pE3468, DOI 10.1073/pnas.1212083110
   Paugam-Moisy H., 2010, HDB NATURAL COMPUTIN, V1, P1
   Poon CS, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00108
   Qualcomm, INTR QUALC ZER PROC
   RANHEL J, 2013, COMP INT 11 BRAZ C C, P28
   Rice KL, 2009, 2009 INTERNATIONAL CONFERENCE ON RECONFIGURABLE COMPUTING AND FPGAS, P451, DOI 10.1109/ReConFig.2009.77
   Wang RC, 2014, IEEE INT SYMP CIRC S, P457, DOI 10.1109/ISCAS.2014.6865169
   Wang RC, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00014
   Wu N. H., 2002, NEURAL COMPUTATION
NR 27
TC 4
Z9 5
U1 0
U2 2
PY 2015
BP 55
EP 61
UT WOS:000380444100003
DA 2023-11-16
ER

PT C
AU Oniz, Y
   Aras, AC
   Kaynak, O
AF Oniz, Yesim
   Aras, Ayse Cisel
   Kaynak, Okyay
GP IEEE
TI Control of Antilock Braking System Using Spiking Neural Networks
SO 39TH ANNUAL CONFERENCE OF THE IEEE INDUSTRIAL ELECTRONICS SOCIETY (IECON
   2013)
SE IEEE Industrial Electronics Society
DT Proceedings Paper
CT 39th Annual Conference of the IEEE Industrial-Electronics-Society
   (IECON)
CY NOV 10-14, 2013
CL Vienna, AUSTRIA
AB Model-free approaches such as Artificial Neural Networks and Fuzzy Controllers are widely used in the control of Antilock Braking System (ABS) due to its strongly nonlinear structure and uncertainties involved. In this paper the design of a Spiking Neural Network (SNN) controller is considered for the regulation of the wheel slip value at its optimum value. For the training of the network a gradient descent based approach is followed. To formulate the generation of a new spike train from the incoming spikes, the Spike Response Model (SRM) is used. Delay coding is utilized to convert real numbers into spike times. The control algorithm is applied to a quarter vehicle model, and it is verified through simulations indicating fast convergence and good performance of the designed controller.
C1 [Oniz, Yesim; Aras, Ayse Cisel; Kaynak, Okyay] Bogazici Univ, TR-34342 Istanbul, Turkey.
RP Oniz, Y (corresponding author), Bogazici Univ, TR-34342 Istanbul, Turkey.
EM yesim.oniz@boun.edu.tr; cisel.aras@boun.edu.tr; okyay.kaynak@boun.edu.tr
CR Bohte S.M., 2003, THESIS
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Canudas de Wit C., 1999, Proceedings of the 38th IEEE Conference on Decision and Control (Cat. No.99CH36304), P3746, DOI 10.1109/CDC.1999.827937
   Floreano D., 2001, INT S EV ROB
   Ghosh-Dastidar S, 2007, INTEGR COMPUT-AID E, V14, P187
   Hagras H, 2004, IEEE INT CONF ROBOT, P4620, DOI 10.1109/ROBOT.2004.1302446
   *INT LTD, LAB ANT BRAK SYST CO
   Johnson C, 2009, NEURAL NETWORKS, V22, P833, DOI 10.1016/j.neunet.2009.06.033
   Kayacan E, 2009, IEEE T IND ELECTRON, V56, P3244, DOI 10.1109/TIE.2009.2023098
   Li J., 2013, J DYNAMIC SYSTEMS ME, V135
   Li K, 2012, INT J AUTO TECH-KOR, V13, P1077, DOI 10.1007/s12239-012-0110-8
   Schrauwen B, 2003, IEEE IJCNN, P2825
   SINGER W, 1995, ANNU REV NEUROSCI, V18, P555, DOI 10.1146/annurev.ne.18.030195.003011
   Vollebregt EAH, 2012, J SOUND VIB, V331, P2141, DOI 10.1016/j.jsv.2012.01.011
NR 14
TC 1
Z9 1
U1 0
U2 0
PY 2013
BP 3422
EP 3427
UT WOS:000331149503059
DA 2023-11-16
ER

PT C
AU Ratnasingam, S
   Robles-Kelly, A
AF Ratnasingam, Sivalogeswaran
   Robles-Kelly, Antonio
GP IEEE
TI A Spiking Neural Network for Illuminant-invariant Colour Discrimination
SO 2013 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY AUG 04-09, 2013
CL Dallas, TX
ID RETINEX THEORY; CONSTANCY; MODEL
AB In this paper, we propose a biologically inspired spiking neural network approach to obtaining an opponent pair which is invariant to illumination variations and can be employed for colour discrimination. The model is motivated by the neural mechanisms involved in processing the visual stimulus starting from the cone photo receptors to the centresurround receptive fields present in the retinal ganglion cells and the striate cortex. For our spiking neural network, we have employed the excitatory and inhibitory lateral synaptic connections, the Spike-Timing Dependent Plasticity (STDP) and long term potentiation and depression (LTP/LTD). Here, we employ a feed-forward leaky integrate-and-fire spiking neural network trained using a dataset of Munsell spectra. We have performed tests on perceptually similar colours under large illuminant power variations and done experiments on colourbased object recognition. We have also compared our results to those yielded by a number of alternatives.
C1 [Ratnasingam, Sivalogeswaran; Robles-Kelly, Antonio] NICTA, Canberra, ACT 2601, Australia.
   [Ratnasingam, Sivalogeswaran; Robles-Kelly, Antonio] Australian Natl Univ, Res Sch Engn, Canberra, ACT 0200, Australia.
RP Ratnasingam, S (corresponding author), NICTA, Locked Bag 8001, Canberra, ACT 2601, Australia.
EM Siva.Ratnasingam@nicta.com.au; antonio.robles-kelly@nicta.com.au
CR Abrardo A, 1996, FOURTH COLOR IMAGING CONFERENCE: COLOR SCIENCE, SYSTEMS AND APPLICATIONS, P94
   Angelopoulou E, 2000, LECT NOTES COMPUT SC, V1842, P359
   [Anonymous], J PHYSL, DOI DOI 10.1007/BF02459568
   [Anonymous], 2005, COLOR RES APPL, V30, P2130
   [Anonymous], 2003, NEURAL COMPUTING, V15
   [Anonymous], 1949, ORG BEHAV
   Arnold S. E. J., 2008, NATURE P
   Azouz R, 2000, P NATL ACAD SCI USA, V97, P8110, DOI 10.1073/pnas.130200797
   Bi GQ, 2001, ANNU REV NEUROSCI, V24, P139, DOI 10.1146/annurev.neuro.24.1.139
   Brainard DH, 2006, J VISION, V6, P1267, DOI 10.1167/6.11.10
   BRAINARD DH, 1986, J OPT SOC AM A, V3, P1651, DOI 10.1364/JOSAA.3.001651
   BUCHSBAUM G, 1980, J FRANKLIN I, V310, P1, DOI 10.1016/0016-0032(80)90058-7
   Chavez-Noriega L. E., 1990, EXPT BRAIN RES, V79
   COURTNEY SM, 1995, VISION RES, V35, P413, DOI 10.1016/0042-6989(94)00132-6
   Dacey DM, 2000, ANNU REV NEUROSCI, V23, P743, DOI 10.1146/annurev.neuro.23.1.743
   DUFORT PA, 1991, BIOL CYBERN, V65, P293, DOI 10.1007/BF00206226
   DZMURA M, 1986, J OPT SOC AM A, V3, P1662, DOI 10.1364/JOSAA.3.001662
   Ebner M., 2007, IS T SERIES IMAGING
   Ebner M., 2007, 2 INT C ADV BRAIN VI
   Finlayson GD, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P473, DOI 10.1109/ICCV.2001.937663
   Finlayson GD, 2001, J OPT SOC AM A, V18, P253, DOI 10.1364/JOSAA.18.000253
   Foster D. H., VISION RES
   Funt B, 1998, 5 EUR C COMP VIS, VI, P445
   FUNT BV, 1995, IEEE T PATTERN ANAL, V17, P522, DOI 10.1109/34.391390
   GERSTNER W, 1995, PHYS REV E, V51, P738, DOI 10.1103/PhysRevE.51.738
   Gerstner W, 1996, NATURE, V383, P76, DOI 10.1038/383076a0
   Gerstner W., 2002, SPIKING NEURON MODEL
   Herault J, 1996, NEUROCOMPUTING, V12, P113, DOI 10.1016/0925-2312(95)00114-X
   Hernández-Andrés J, 2003, APPL OPTICS, V42, P458, DOI 10.1364/AO.42.000458
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Kries J., 1878, ARCH ANAT PHYSL, V2, P5050
   LAND EH, 1971, J OPT SOC AM, V61, P1, DOI 10.1364/JOSA.61.000001
   LAND EH, 1977, SCI AM, V237, P108, DOI 10.1038/scientificamerican1277-108
   M. C. S. Laborartory, DAYL SPECTR
   MOORE A, 1991, IEEE T NEURAL NETWOR, V2, P237, DOI 10.1109/72.80334
   Ruderman D. L., 1998, J OPT SOC AM A, V15
   Sjöström PJ, 2008, PHYSIOL REV, V88, P769, DOI 10.1152/physrev.00016.2007
   Sjostrom P. J., 2010, SCHOLARPEDIA, V5
   Sjöström PJ, 2001, NEURON, V32, P1149, DOI 10.1016/s0896-6273(01)00542-6
   Stevens S., 2007, PSYCHOPHYSICS INTRO
   Stiles WS, 1955, OPT ACTA, V2, P168, DOI 10.1080/713821039
   SWAIN MJ, 1991, INT J COMPUT VISION, V7, P11, DOI 10.1007/BF00130487
   Trappenberg T., 2010, FUNDAMENTALS COMPUTA
   Van de Weijer J, 2007, IEEE T IMAGE PROCESS, V16, P2207, DOI 10.1109/TIP.2007.901808
   Zickler T, 2008, INT J COMPUT VISION, V79, P13, DOI [10.1007/s11263-007-0087-3, 10.1007/s11I263-007-0087-3]
NR 45
TC 1
Z9 1
U1 0
U2 1
PY 2013
UT WOS:000349557200222
DA 2023-11-16
ER

PT C
AU Khun, J
   Novotny, M
   Skrbek, M
AF Khun, Jiri
   Novotny, Martin
   Skrbek, Miroslav
BE Jozwiak, L
   Stojanovic, R
   Lutovac, B
   Jurisic, D
TI High-Performance Spiking Neural Network Simulator
SO 2019 8TH MEDITERRANEAN CONFERENCE ON EMBEDDED COMPUTING (MECO)
SE Mediterranean Conference on Embedded Computing
DT Proceedings Paper
CT 8th Mediterranean Conference on Embedded Computing (MECO)
CY JUN 10-14, 2019
CL Budva, MONTENEGRO
DE Spiking neural networks simulator; GPU accelerated; parallelization;
   Izhikevich neuron model; OpenCL
ID MODEL
AB Simulation of neural networks is a significant task for contemporary artificial intelligence research. Despite the availability of modern processing hardware, the task is still too demanding to be done in a sequential way. Therefore, a parallel computation approach is almost always necessary. Modern graphical accelerators (GPUs) represent highly parallel machines with a significant computational performance that can be unleashed only under certain conditions including threads scheduling, proper sources occupation, aligned data access, communication management, etc. We have proposed a novel acceleration approach for large neural networks. It is using a GPU and incorporating biologically highly precise spiking neurons that can imitate real biological neurons. The simulator can be, for example, used for research of communication dynamics of large neural networks with tens of thousands of spiking neurons.
C1 [Khun, Jiri; Novotny, Martin; Skrbek, Miroslav] Czech Tech Univ, Fac Informat Technol, Dept Digital Design, Thakurova 9, Prague 16000, Czech Republic.
RP Khun, J (corresponding author), Czech Tech Univ, Fac Informat Technol, Dept Digital Design, Thakurova 9, Prague 16000, Czech Republic.
EM jiri.khun@fit.cvut.cz; martin.novotny@fit.cvut.cz;
   miroslav.skrbek@fit.cvut.cz
CR Abbott LF, 1999, BRAIN RES BULL, V50, P303, DOI 10.1016/S0361-9230(99)00161-6
   [Anonymous], 1988, PARALLEL DISTRIBUTED
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   McCulloch WS., 1943, B MATH BIOPHYS, V5, P115, DOI DOI 10.1007/BF02478259
   Nageswaran Jayram Moorkanikara, 2009, Proceedings 2009 International Joint Conference on Neural Networks (IJCNN 2009 - Atlanta), P2145, DOI 10.1109/IJCNN.2009.5179043
   Rosenblatt F., 1961, PRINCIPLES NEURODYNA
   Rumelhart D.E., 1986, PARALLEL DISTRIBUTED, V1, DOI DOI 10.7551/MITPRESS/5236.001.0001
   Scorcioni R., 2010, BMC NEUROSCI S1, V11, pP133, DOI [10.1186/1471-2202-11-S1-P133, DOI 10.1186/1471-2202-11-S1-P133]
   Yudanov D., 2010, P INT JOINT C NEUR N, P1, DOI [10.1109/IJCNN.2010.5596334, DOI 10.1109/IJCNN.2010.5596334]
NR 10
TC 2
Z9 2
U1 0
U2 1
PY 2019
BP 88
EP 91
UT WOS:000492146100030
DA 2023-11-16
ER

PT C
AU Cui, XP
   Hao, XC
   Liang, Y
   Sun, GY
   Cui, XX
   Wang, Y
   Huang, R
AF Cui, Xiuping
   Hao, Xiaochen
   Liang, Yun
   Sun, Guangyu
   Cui, Xiaoxin
   Wang, Yuan
   Huang, Ru
GP IEEE
TI A Mapping Model of SNNs to Neuromorphic Hardware
SO 2022 IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE CIRCUITS
   AND SYSTEMS (AICAS 2022): INTELLIGENT TECHNOLOGY IN THE POST-PANDEMIC
   ERA
DT Proceedings Paper
CT IEEE International Conference on Artificial Intelligence Circuits and
   Systems (AICAS) - Intelligent Technology in the Post-Pandemic Era
CY JUN 13-15, 2022
CL Incheon, SOUTH KOREA
DE spiking neural network; neuromorphic hardware; mapping model
AB Spiking neural networks (SNNs) can achieve lower power consumption than traditional artificial neural networks. To take full advantage of spiking neural networks, a large number of neuromorphic hardware has emerged. However, it is nontrivial to map SNNs onto neuromorphic hardware due to the hardware constraints and complex networks-on-chip (NoCs). In this paper, we propose a mapping model to bridge the gap between SNNs and neuromorphic hardware. The mapping model is general to neuromorphic hardware with various on-chip networks. The core of the model is a loop-based representation, which can model computation and connection in software and hardware space simultaneously. We further propose transformation primitives to transform networks from software space to hardware space. We evaluate the mapping model using realistic spiking neural networks and three on-chip network topologies. Experiments show that compared to the simulated annealing algorithm without using the model, the energy consumption of computation and communication can be reduced by 19.4% and 27.4% on average, respectively.
C1 [Cui, Xiuping; Hao, Xiaochen; Liang, Yun; Sun, Guangyu; Cui, Xiaoxin; Wang, Yuan; Huang, Ru] Peking Univ, Beijing, Peoples R China.
   [Liang, Yun] Beijing Acad Artificial Intelligence, Beijing, Peoples R China.
RP Cui, XP (corresponding author), Peking Univ, Beijing, Peoples R China.
EM cuixiuping@pku.edu.cn; xiaochen.hao@stu.pku.edu.cn; ericlyun@pku.edu.cn;
   gsun@pku.edu.cn; cuixx@pku.edu.cn; wangyuan@pku.edu.cn;
   ruhuang@pku.edu.cn
CR Abbott LF, 1999, BRAIN RES BULL, V50, P303, DOI 10.1016/S0361-9230(99)00161-6
   Ankit A, 2017, DES AUT CON, DOI 10.1145/3061639.3062311
   Balaji A, 2020, IEEE T VLSI SYST, V28, P76, DOI 10.1109/TVLSI.2019.2951493
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   DeBole MV, 2019, COMPUTER, V52, P20, DOI 10.1109/MC.2019.2903009
   Deng L, 2020, IEEE J SOLID-ST CIRC, V55, P2228, DOI 10.1109/JSSC.2020.2970709
   GERSTNER W, 1992, NETWORK-COMP NEURAL, V3, P139, DOI 10.1088/0954-898X/3/2/004
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Ji Y, 2016, INT SYMP MICROARCH
   Jo SH, 2010, NANO LETT, V10, P1297, DOI 10.1021/nl904092h
   Kajino Hiroshi, 2021, ICML
   Kim Soo Ye, 2020, AAAI
   Lin CK, 2018, ACM SIGPLAN NOTICES, V53, P78, DOI [10.1145/3192366.3192371, 10.1145/3296979.3192371]
   Lu LQ, 2021, CONF PROC INT SYMP C, P720, DOI 10.1109/ISCA52012.2021.00062
   Moradi S, 2018, IEEE T BIOMED CIRC S, V12, P106, DOI 10.1109/TBCAS.2017.2759700
   Shiming Li, 2020, GLSVLSI '20. Proceedings of the 2020 Great Lakes Symposium on VLSI, P9, DOI 10.1145/3386263.3406900
   Singh S, 2020, ANN I S COM, P363, DOI 10.1109/ISCA45697.2020.00039
   Song SH, 2020, 21ST ACM SIGPLAN/SIGBED CONFERENCE ON LANGUAGES, COMPILERS, AND TOOLS FOR EMBEDDED SYSTEMS (LCTES '20), P38, DOI 10.1145/3372799.3394364
   Song Shihao, 2021, ICCAD
   Yang Yi-Rui, 2021, ICML
NR 20
TC 0
Z9 0
U1 1
U2 4
PY 2022
BP 206
EP 209
DI 10.1109/AICAS54282.2022.9869998
UT WOS:000859273200053
DA 2023-11-16
ER

PT J
AU Thangaraj, VK
   Nachimuthu, DS
   Francis, VAR
AF Thangaraj, Vinoth Kumar
   Nachimuthu, Deepa Subramaniam
   Francis, Vijay Amirtha Raj
TI Wind speed forecasting at wind farm locations with an unique hybrid
   PSO-ALO based modified spiking neural network
SO ENERGY SYSTEMS-OPTIMIZATION MODELING SIMULATION AND ECONOMIC ASPECTS
DT Article; Early Access
DE Prediction accuracy; Wind speed; Ant lion optimization; Spiking neural
   network; Particle swarm optimization
ID MULTIOBJECTIVE OPTIMIZATION; TIME-SERIES; ALGORITHM; SYSTEM;
   DECOMPOSITION; PREDICTION; REGRESSION; STRATEGY
AB In this article, a new modified Spiking Neural Network (SNN) model, a form of machine learning (ML) model is employed for forecasting wind speed using real-time wind farm statistics from the site locations. Spiking Neural Network is a third-generation neural network model based on the timing of spikes produced in brain neurons. The development of spiking neural networks is from the fundamental biological understandings of neuron activity. Spiking neural networks that have been built employ transient pulses to execute calculations and need communication between the layers of the network. For calculating the network's output, the mother wavelet function is used replacing the Gaussian function, which is the basic activation function used in SNN models. Additionally, the weights of the modified SNN are tuned and adapted with the developed hybrid optimization technique, particle swarm optimization-ant lion optimization technique. This new work is carried out with the idea of overcoming the lacunae observed in the classic SNN model. The training process being terminated intermittently has to be overcome by employing this new modified SNN tuned for its weight values using the novel hybrid PSO-ALO. The novel hybrid PSO-ALO based modified SNN is used in this research to perform wind speed prediction for the considered wind farm location sites. Numerical simulations executed proved the results to be effective and had better prediction accuracy than the state-of-the-art techniques in previous studies.
C1 [Thangaraj, Vinoth Kumar; Francis, Vijay Amirtha Raj] RVS Coll Engn & Technol, Coimbatore 641402, Tamil Nadu, India.
   [Nachimuthu, Deepa Subramaniam] Natl Inst Technol, Dept Elect Engn, Jote 791113, Arunachal Prade, India.
RP Thangaraj, VK (corresponding author), RVS Coll Engn & Technol, Coimbatore 641402, Tamil Nadu, India.
EM vinothrvs15f@gmail.com; sndeepa@nitap.ac.in; fvijayami@gmail.com
CR Aasim, 2019, RENEW ENERG, V136, P758, DOI 10.1016/j.renene.2019.01.031
   Abusnaina AA, 2019, NEURAL PROCESS LETT, V49, P661, DOI 10.1007/s11063-018-9846-0
   Akinci TC, 2011, ELEKTRON ELEKTROTECH, P41
   Almomani A, 2019, CLUSTER COMPUT, V22, P419, DOI 10.1007/s10586-018-02891-0
   Arsie I., 2006, P ECOS, P12
   Brusca S, 2019, INT J NUMER MODEL EL, V32, DOI 10.1002/jnm.2267
   Chen YY, 2019, NPJ 2D MATER APPL, V3, DOI 10.1038/s41699-019-0114-6
   Chitsazan MA, 2019, RENEW ENERG, V131, P879, DOI 10.1016/j.renene.2018.07.060
   Cho MW, 2019, J KOREAN PHYS SOC, V75, P261, DOI 10.3938/jkps.75.261
   Neto PSGD, 2021, INFORM SCIENCES, V581, P495, DOI 10.1016/j.ins.2021.09.054
   Di ZH, 2019, ATMOS RES, V226, P1, DOI 10.1016/j.atmosres.2019.04.011
   Doborjeh M, 2019, NEURAL NETWORKS, V119, P162, DOI 10.1016/j.neunet.2019.07.021
   Doborjeh Z, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-42863-x
   Dora S, 2019, IEEE T CYBERNETICS, V49, P989, DOI 10.1109/TCYB.2018.2791282
   Espinosa-Ramos JI, 2019, IEEE T COGN DEV SYST, V11, P63, DOI 10.1109/TCDS.2017.2776863
   Fonte P. M., 2005, WSEAS Transactions on Systems, V4, P379
   Haessig G, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-40064-0
   Han XJ, 2010, 2010 8TH WORLD CONGRESS ON INTELLIGENT CONTROL AND AUTOMATION (WCICA), P5891, DOI 10.1109/WCICA.2010.5554531
   He QQ, 2018, APPL ENERG, V226, P756, DOI 10.1016/j.apenergy.2018.06.053
   He YY, 2022, CHAOS SOLITON FRACT, V162, DOI 10.1016/j.chaos.2022.112416
   Heng JN, 2022, APPL ENERG, V306, DOI 10.1016/j.apenergy.2021.118029
   Hong Y.-Y., 2010, P GLOB TEL C GLOBECO, P1, DOI DOI 10.1109/GLOCOM.2010.5683124
   Hu R, 2019, RENEW ENERG, V140, P17, DOI 10.1016/j.renene.2019.03.041
   Hunter D, 2012, IEEE T IND INFORM, V8, P228, DOI 10.1109/TII.2012.2187914
   Iqdour R., 2006, MLP NEURAL NETWORKS
   Jayaraj S., 2004, EUR WIND EN C
   Jia L, 2019, IOP C SER EARTH ENV, V252, DOI 10.1088/1755-1315/252/2/022046
   Jiang P, 2022, ELECTR POW SYST RES, V211, DOI 10.1016/j.epsr.2022.108186
   Jiang P, 2019, APPL SOFT COMPUT, V82, DOI 10.1016/j.asoc.2019.105587
   Jiang P, 2019, ENERGY, V173, P468, DOI 10.1016/j.energy.2019.02.080
   Jiang P, 2019, APPL ENERG, V235, P786, DOI 10.1016/j.apenergy.2018.11.012
   Junli W., 2010, 2010 INT C COMPUTER, V5
   Lanzhen L., 2010, 2010 AS PAC POW EN E
   Li C, 2019, ENERGY, V174, P1219, DOI 10.1016/j.energy.2019.02.194
   Li M, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19050993
   Li Xingpei, 2009, 2009 4th IEEE Conference on Industrial Electronics and Applications, P2448, DOI 10.1109/ICIEA.2009.5138642
   Li YF, 2019, RENEW ENERG, V135, P540, DOI 10.1016/j.renene.2018.12.035
   Liera P.C., 2006, P IEMSS C, P124
   Lin WM, 2011, IEEE T POWER ELECTR, V26, P473, DOI 10.1109/TPEL.2010.2085454
   Liu H, 2020, APPL ENERG, V261, DOI 10.1016/j.apenergy.2019.114367
   Liu JX, 2019, NEUROCOMPUTING, V331, P473, DOI 10.1016/j.neucom.2018.11.078
   Lv MZ, 2022, SUSTAIN ENERGY TECHN, V52, DOI 10.1016/j.seta.2022.102186
   Meng AB, 2016, ENERG CONVERS MANAGE, V114, P75, DOI 10.1016/j.enconman.2016.02.013
   Mi XW, 2019, ENERG CONVERS MANAGE, V180, P196, DOI 10.1016/j.enconman.2018.11.006
   Mohandes MA, 1998, RENEW ENERG, V13, P345, DOI 10.1016/S0960-1481(98)00001-9
   More A, 2003, MAR STRUCT, V16, P35, DOI 10.1016/S0951-8339(02)00053-9
   Mozafari M, 2019, PATTERN RECOGN, V94, P87, DOI 10.1016/j.patcog.2019.05.015
   Niu XS, 2019, APPL ENERG, V241, P519, DOI 10.1016/j.apenergy.2019.03.097
   Nunes RV, 2019, BIOL CYBERN, V113, P309, DOI 10.1007/s00422-019-00796-8
   Oliveira LDR, 2019, NEUROCOMPUTING, V337, P251, DOI 10.1016/j.neucom.2019.01.071
   Qu ZX, 2019, RENEW ENERG, V133, P919, DOI 10.1016/j.renene.2018.10.043
   Shao S, 2019, J ADV COMPUT INTELL, V23, P528, DOI 10.20965/jaciii.2019.p0528
   Sheela KG, 2013, CONTROL ENG APPL INF, V15, P30
   Silva G.X., 2006, P 5 WSEAS INT C ARTI, P286
   Sivanandam SN., 2019, PRINCIPLES SOFT COMP, V3
   Sun SZ, 2022, ENERGY REP, V8, P9899, DOI 10.1016/j.egyr.2022.07.164
   Tavanaei A, 2019, NEUROCOMPUTING, V330, P39, DOI 10.1016/j.neucom.2018.11.014
   Wang C, 2022, RENEW ENERG, V196, P763, DOI 10.1016/j.renene.2022.06.143
   Wang JZ, 2021, RENEW ENERG, V179, P1246, DOI 10.1016/j.renene.2021.07.113
   Wang JZ, 2015, ENERGY, V93, P41, DOI 10.1016/j.energy.2015.08.045
   Wang Y, 2019, RENEW ENERG, V132, P43, DOI 10.1016/j.renene.2018.07.083
   Wu CY, 2020, RENEW ENERG, V146, P149, DOI 10.1016/j.renene.2019.04.157
   Wu J, 2022, ENERGY, V242, DOI 10.1016/j.energy.2021.122960
   Xiang SY, 2019, IEEE J SEL TOP QUANT, V25, DOI 10.1109/JSTQE.2019.2911565
   ZHANG QG, 1992, IEEE T NEURAL NETWOR, V3, P889, DOI 10.1109/72.165591
   Zhao XY, 2020, ENERG CONVERS MANAGE, V203, DOI 10.1016/j.enconman.2019.112239
   Zhou QG, 2019, APPL ENERG, V250, P1559, DOI 10.1016/j.apenergy.2019.05.016
   Zucatelli PJ, 2019, HELIYON, V5, DOI 10.1016/j.heliyon.2019.e01664
NR 68
TC 0
Z9 0
U1 2
U2 2
PD 2023 AUG 7
PY 2023
DI 10.1007/s12667-023-00607-x
EA AUG 2023
UT WOS:001043617300001
DA 2023-11-16
ER

PT C
AU Qian, HX
   Gu, PJ
   Yan, R
   Tang, HJ
AF Qian, Hanxiao
   Gu, Pengjie
   Yan, Rui
   Tang, Huajin
GP IEEE
TI Robust Multipitch Estimation of Piano Sounds Using Deep Spiking Neural
   Networks
SO 2019 IEEE SYMPOSIUM SERIES ON COMPUTATIONAL INTELLIGENCE (IEEE SSCI
   2019)
DT Proceedings Paper
CT IEEE Symposium Series on Computational Intelligence (SSCI)
CY DEC 06-09, 2019
CL Xiamen, PEOPLES R CHINA
DE multipitch estimation; spatiotemporal spiking coding; deep spiking
   neural networks; multi-label classification
ID AUTOMATIC TRANSCRIPTION; ALGORITHM
AB In this paper, we propose a robust multi-label classification system based on deep spiking neural networks to handle multi-pitch estimation tasks. We employ constant-Q transform spectrogram as a time-frequency representation. A keypoint detection technique is used for noise suppression and the extraction of relevant information. We also propose a novel biological spiking coding method that fits the expression of musical signals. This coding method can encode time, frequency, intensity information into spatiotemporal spike trains. And the spatio-temporal credit assignment (STCA) algorithm is used to train deep spiking neural networks. We perform the multi pitch evaluation on the MAPS data set, and our work compares with the state-of-the-art methods by using the F1-score metric. Experimental results show that the proposed scheme has achieved better performance than other state-of-the-art methods and reveal the system's robustness to environmental noise.
C1 [Qian, Hanxiao; Gu, Pengjie; Yan, Rui; Tang, Huajin] Sichuan Univ, Coll Comp Sci, Chengdu, Sichuan, Peoples R China.
   [Tang, Huajin] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou, Peoples R China.
RP Qian, HX (corresponding author), Sichuan Univ, Coll Comp Sci, Chengdu, Sichuan, Peoples R China.
EM HanxiaoQ@gmail.com; gupj1202@gmail.com; ryan@scu.edu.cn;
   huajin.tang@gmail.com
CR Badeau R, 2009, INT CONF ACOUST SPEE, P3073, DOI 10.1109/ICASSP.2009.4960273
   Benetos E, 2019, IEEE SIGNAL PROC MAG, V36, P20, DOI 10.1109/MSP.2018.2869928
   Benetos E, 2011, IEEE J-STSP, V5, P1111, DOI 10.1109/JSTSP.2011.2162394
   Bertin N., 2007, 2007 IEEE INT C AC S
   BROWN JC, 1991, J ACOUST SOC AM, V89, P425, DOI 10.1121/1.400476
   BROWN JC, 1992, J ACOUST SOC AM, V92, P2698, DOI 10.1121/1.404385
   Canadas-Quesada F. J., 2010, 2010 IEEE 12th International Workshop on Multimedia Signal Processing (MMSP), P7, DOI 10.1109/MMSP.2010.5661985
   Christensen M. G., 2009, SYNTHESIS LECT SPEEC, V5, P1, DOI DOI 10.2200/S00178ED1V01Y200903SAP005
   Costantini Giovanni, 2010, Latest Trends on Systems. 14th WSEAS International Conference on Systems. (Part of the 14th WSEAS CSCC Multiconference), P288
   Costantini G, 2009, SIGNAL PROCESS, V89, P1798, DOI 10.1016/j.sigpro.2009.03.024
   Dasgupta S, 2017, SCIENCE, V358, P793, DOI 10.1126/science.aam9868
   Dennis J, 2013, INT CONF ACOUST SPEE, P803, DOI 10.1109/ICASSP.2013.6637759
   Emiya V, 2010, IEEE T AUDIO SPEECH, V18, P1643, DOI 10.1109/TASL.2009.2038819
   Fletcher N.H., 2012, PHYS MUSICAL INSTRUM
   Gu PJ, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1366
   Gütig R, 2016, SCIENCE, V351, DOI 10.1126/science.aab4113
   Klapuri A., 2009, AUTOMATIC MUSIC TRAN
   Li X., 2018, BIOMED RES INT, V2018, P11
   Marolt M, 2004, IEEE T MULTIMEDIA, V6, P439, DOI 10.1109/TMM.2004.827507
   Nakamura E, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P101, DOI 10.1109/ICASSP.2018.8461914
   Raphael C., 2002, INT C INT SOC MUS IN
   Schorkhuber C., 2010, SOUND MUSIC COMPUTIN, P3
   Tavares TF, 2016, J NEW MUSIC RES, V45, P118, DOI 10.1080/09298215.2016.1177552
   Tzanetakis G., 2008, COMPUT MUSIC J, V28, P34
   Virtanen T., 2002, J ACOUST SOC AM, V111, P2417
NR 25
TC 1
Z9 1
U1 0
U2 3
PY 2019
BP 2335
EP 2341
UT WOS:000555467202059
DA 2023-11-16
ER

PT J
AU Mohanty, R
   Mallik, BK
   Solanki, SS
AF Mohanty, Ricky
   Mallik, Bandi Kumar
   Solanki, Sandeep Singh
TI Automatic bird species recognition system using neural network based on
   spike
SO APPLIED ACOUSTICS
DT Article
DE Permutation Pair Frequency Matrix (PPFM); Spiking Neural Network (SNN);
   Automatic Bird species recognition system
ID CLASSIFICATION; SOUNDS
AB Automatic Bird Species Recognition System helps ornithologists and researchers to study particular bird species, effect of climate changes, count of endangered species and their survival. Earlier researchers implemented this automation system using traditional methods such as Gaussian Mixture Model (GMM), Hidden Markov Model (HMM) and Dynamic Time Wrapping (DTW) etc. The efficiency with the systems mentioned above has shown very low accuracy and is time-consuming. The recognition system performance may be improved by using Spiking Neural Network (SNN), a third-generation artificial neural network (ANN). The main focus of work in this paper is to analyze sound waves produced by bird's species. This work is based on the Attribute extrication methods (AEM). The paper briefly explains these methods and evaluates their performance on Spiking Neural Network classification. Spiking Neural Network classification using Permutation Pair Frequency Matrix (PPFM) proves to be a more efficient method in terms of accuracy percentage and lower computation time. (C) 2019 Elsevier Ltd. All rights reserved.
C1 [Mohanty, Ricky; Solanki, Sandeep Singh] Birla Inst Technol, Dept Elect & Commun, Ranchi 835215, Bihar, India.
   [Mallik, Bandi Kumar] Dev Org Eastern Reg, Bhubaneswar, India.
RP Mohanty, R (corresponding author), Birla Inst Technol, Dept Elect & Commun, Ranchi 835215, Bihar, India.
EM mohantyricky@gmail.com; bkmallik@yahoo.co.in; sssolanki@bitmesra.ac.in
CR [Anonymous], 2005, 13 EUR SIGN PROC C E
   Briggs F, 2012, J ACOUST SOC AM, V131, P4640, DOI 10.1121/1.4707424
   Catchpole C. K., 1995, BIRD SONG BIOL THEME
   Chen ZX, 2006, J ACOUST SOC AM, V120, P2974, DOI 10.1121/1.2345831
   Fagerlund S, 2014, APPL ACOUST, V83, P57, DOI 10.1016/j.apacoust.2014.03.006
   Fagerlund S, 2014, INT CONF ACOUST SPEE
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Harma A, 2004, IEEE INT C AC SPEECH
   Jancovic P, 2015, IEEE SIGNAL PROC LET, V22, P1585, DOI 10.1109/LSP.2015.2409173
   Jancovic P, 2011, EURASIP J ADV SIG PR, DOI 10.1155/2011/982936
   Jarcovic P, 2019, IEEE-ACM T AUDIO SPE, V27, P932, DOI 10.1109/TASLP.2019.2904790
   Jian L, 2014, 2014 INT S COMP CONS, DOI [10.1109/1S3C2014.47, DOI 10.1109/IS3C2014.47]
   Juang CF, 2007, NEUROCOMPUTING, V71, P121, DOI 10.1016/j.neucom.2007.08.011
   Kogan JA, 1998, J ACOUST SOC AM, V103, P2185, DOI 10.1121/1.421364
   Kwan C, 2006, EURASIP J APPL SIG P, DOI 10.1155/ASP/2006/96706
   Lee CH, 2008, IEEE T AUDIO SPEECH, V16, P1541, DOI 10.1109/TASL.2008.2005345
   Li W, 2014, IEEE T AUTOM SCI ENG, V11, P348, DOI 10.1109/TASE.2013.2247397
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Mitra S, 2009, IEEE T BIOMED CIRC S, V3, P32, DOI 10.1109/TBCAS.2008.2005781
   Qiao B, 2017, 2017 1 INT C EL INST, DOI [10.1109/EIIS.2017.8298548, DOI 10.1109/EIIS.2017.8298548]
   RABINER LR, 1975, AT&T TECH J, V54, P297, DOI 10.1002/j.1538-7305.1975.tb02840.x
   Selin A, 2007, EURASIP J ADV SIG PR, DOI 10.1155/2007/51806
   Somervuo P, 2006, IEEE T AUDIO SPEECH, V14, P2252, DOI 10.1109/TASL.2006.872624
   Stastny J, 2018, EURASIP J AUDIO SPEE, DOI 10.1186/s13636-018-0143-7
NR 24
TC 16
Z9 18
U1 2
U2 20
PD APR
PY 2020
VL 161
AR 107177
DI 10.1016/j.apacoust.2019.107177
UT WOS:000513986000017
DA 2023-11-16
ER

PT C
AU Prezioso, M
   Zhong, Y
   Gavrilov, D
   Merrikh-Bayat, F
   Hoskins, B
   Adam, G
   Likharev, K
   Strukov, D
AF Prezioso, M.
   Zhong, Y.
   Gavrilov, D.
   Merrikh-Bayat, F.
   Hoskins, B.
   Adam, G.
   Likharev, K.
   Strukov, D.
GP IEEE
TI Spiking Neuromorphic Networks with Metal-Oxide Memristors
SO 2016 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS (ISCAS)
SE IEEE International Symposium on Circuits and Systems
DT Proceedings Paper
CT IEEE International Symposium on Circuits and Systems (ISCAS)
CY MAY 22-25, 2016
CL Montreal, CANADA
DE Spiking neural networks; Memristor; CrossNets; Spike-time-dependent
   plasticity; Spatial-temporal associative memories
ID TIMING-DEPENDENT-PLASTICITY; DEVICES
AB This is a brief review of our recent work on memristor-based spiking neuromorphic networks. We first describe the recent experimental demonstration of several most biology-plausible spike-time-dependent plasticity (STDP) windows in integrated metal-oxide memristors and, for the first time, the observed self-adaptive STDP, which may be crucial for spiking neural network applications. We then discuss recent theoretical work in which an analytical, data-verified STDP model was used to simulate operation of a spiking classifier of spatial-temporal patterns, and the capacity-to-fidelity tradeoff and noise immunity of spiking spatial-temporal associative memories with local and global recording was evaluated.
C1 [Prezioso, M.; Zhong, Y.; Merrikh-Bayat, F.; Hoskins, B.; Adam, G.; Strukov, D.] UC Santa Barbara, Santa Barbara, CA 93106 USA.
   [Zhong, Y.] Huazhong Univ Sci & Technol, Wuhan 430074, Peoples R China.
   [Gavrilov, D.; Likharev, K.] SUNY Stony Brook, Stony Brook, NY 11794 USA.
RP Prezioso, M (corresponding author), UC Santa Barbara, Santa Barbara, CA 93106 USA.
EM mprezioso@ece.ucsb.edu; konstantin.likarev@stonybrook.edu;
   strukov@ece.ucsb.edu
CR Alibart F, 2012, NANOTECHNOLOGY, V23, DOI 10.1088/0957-4484/23/7/075201
   GERSTNER W, 1993, BIOL CYBERN, V69, P503, DOI 10.1007/BF01185422
   Hasler J, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00118
   Indiveri G, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00073
   Likharev K. K., 2014, DARPAS CORT PROC WOR
   Likharev KK, 2011, SCI ADV MATER, V3, P322, DOI 10.1166/sam.2011.1177
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Mandal S, 2014, SCI REP-UK, V4, DOI 10.1038/srep05333
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   Masquelier T, 2008, PLOS ONE, V3, DOI 10.1371/journal.pone.0001377
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Pfeil T, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00011
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Prezioso M, 2016, SCI REP-UK, V6, DOI 10.1038/srep21331
   Prezioso M, 2015, NATURE, V521, P61, DOI 10.1038/nature14441
   Saïghi S, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00051
   Strukov Dmitri, 2015, 2015 IEEE INT MEM WO
   Subramaniam A, 2013, IEEE T NANOTECHNOL, V12, P450, DOI 10.1109/TNANO.2013.2256366
   Wang ZQ, 2012, ADV FUNCT MATER, V22, P2759, DOI 10.1002/adfm.201103148
   Wills Sebastian A., 2004, THESIS
   Wong HSP, 2012, P IEEE, V100, P1951, DOI 10.1109/JPROC.2012.2190369
   Yang JJS, 2013, NAT NANOTECHNOL, V8, P13, DOI [10.1038/nnano.2012.240, 10.1038/NNANO.2012.240]
   Zamarreño-Ramos C, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00026
   Zhong Y., 2016, SIMULATION MEM UNPUB
NR 24
TC 15
Z9 15
U1 0
U2 8
PY 2016
BP 177
EP 180
UT WOS:000390094700045
DA 2023-11-16
ER

PT C
AU Kuang, YS
   Cui, XX
   Zou, CL
   Zhong, Y
   Dai, ZH
   Wang, ZL
   Liu, KF
   Yu, DS
   Wang, Y
AF Kuang, Yisong
   Cui, Xiaoxin
   Zou, Chenglong
   Zhong, Yi
   Dai, Zhenhui
   Wang, Zilin
   Liu, Kefei
   Yu, Dunshan
   Wang, Yuan
GP IEEE
TI An Event-driven Spiking Neural Network Accelerator with On-chip Sparse
   Weight
SO 2022 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS (ISCAS 22)
SE IEEE International Symposium on Circuits and Systems
DT Proceedings Paper
CT IEEE International Symposium on Circuits and Systems (ISCAS)
CY MAY 28-JUN 01, 2022
CL Austin, TX
DE SNN; neuromorphic processor; sparse weight; sparse spike; FPGA
ID MEMORY
AB Spiking neural networks (SNNs) have widely drew attention of recent research. With brain-spired dynamics and spike-based communication, SNN is supposed to be a more energy-efficient neural network than existing artificial neural network (ANN). To make better use of the temporal sparsity of spikes and spatial sparsity of weights in SNN, this paper presents a sparse SNN accelerator. It adopts a novel selfadaptive spike compressing and decompressing (SASCD) mechanism for different input spike sparsity, as well as on-chip compressed weight storage and processing. We implement the octa-core design on field programmable gate array (FPGA). The results demonstrate a peak performance of 35.84 GSOPs/s, which is equivalent to 358.4 GSOPs/s in dense SNN accelerators for 90% weight sparsity. For the single-layer perceptron model in rate coding implemented on the hardware, SASCD reduces the time step intervals from 2.15 mu s to 0.55 mu s.
C1 [Kuang, Yisong; Cui, Xiaoxin; Zou, Chenglong; Zhong, Yi; Dai, Zhenhui; Wang, Zilin; Liu, Kefei; Yu, Dunshan; Wang, Yuan] Peking Univ, Sch Integrated Circuits, Key Lab Microelect Devices & Circuits MoE, Beijing, Peoples R China.
RP Cui, XX; Wang, Y (corresponding author), Peking Univ, Sch Integrated Circuits, Key Lab Microelect Devices & Circuits MoE, Beijing, Peoples R China.
EM cuixx@pku.edu.cn; wangyuan@pku.edu.cn
CR Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   Boahen KA, 2000, IEEE T CIRCUITS-II, V47, P416, DOI 10.1109/82.842110
   Chen GK, 2019, IEEE J SOLID-ST CIRC, V54, P992, DOI 10.1109/JSSC.2018.2884901
   Chen R., 2018 INT JOINT C NEU, P1
   Davies M., 2021 S VLSI CIRC KYO, P1
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Deng L, 2020, IEEE J SOLID-ST CIRC, V55, P2228, DOI 10.1109/JSSC.2020.2970709
   Farsa EZ, 2019, IEEE T CIRCUITS-II, V66, P1582, DOI 10.1109/TCSII.2019.2890846
   Frenkel C, 2019, IEEE T BIOMED CIRC S, V13, P145, DOI 10.1109/TBCAS.2018.2880425
   Gerstner W., 2002, SPIKING NEURON MODEL
   Kuang YS, 2021, IEEE T CIRCUITS-II, V68, P2655, DOI 10.1109/TCSII.2021.3052172
   Liu YC, 2020, IEEE T BIOMED CIRC S, V14, P274, DOI 10.1109/TBCAS.2019.2952714
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Merolla P., 2011 IEEE CUST INT C, P1
   Moradi S, 2018, IEEE T BIOMED CIRC S, V12, P106, DOI 10.1109/TBCAS.2017.2759700
   Rathi N, 2019, IEEE T COMPUT AID D, V38, P668, DOI 10.1109/TCAD.2018.2819366
   Schemmel J, 2010, IEEE INT SYMP CIRC S, P1947, DOI 10.1109/ISCAS.2010.5536970
   Yeh Z. -W., 2021 IEEE INT S CIRC, P1
   Yuan YJ, 2021, ADV APPL MATH MECH, V13, P1, DOI 10.4208/aamm.OA-2019-0121
   Zhong Y., 2021 IEEE INT S CIRC, P1
NR 20
TC 0
Z9 0
U1 9
U2 9
PY 2022
BP 3468
EP 3472
DI 10.1109/ISCAS48785.2022.9937521
UT WOS:000946638603139
DA 2023-11-16
ER

PT J
AU Strohmer, B
   Stagsted, RK
   Manoonpong, P
   Larsen, LB
AF Strohmer, Beck
   Stagsted, Rasmus Karnoe
   Manoonpong, Poramate
   Larsen, Leon Bonde
TI Integrating Non-spiking Interneurons in Spiking Neural Networks
SO FRONTIERS IN NEUROSCIENCE
DT Article
DE spiking neural network; non-spiking interneuron; neuromorphic
   engineering; mixed network; biologically plausible neuron; bio-inspired
   engineering
ID SYNAPTIC-TRANSMISSION; LOCAL INTERNEURONS; MOTOR CONTROL; INSIGHTS;
   SIGNALS; INSECT; OUTPUT
AB Researchers working with neural networks have historically focused on either non-spiking neurons tractable for running on computers or more biologically plausible spiking neurons typically requiring special hardware. However, in nature homogeneous networks of neurons do not exist. Instead, spiking and non-spiking neurons cooperate, each bringing a different set of advantages. A well-researched biological example of such a mixed network is a sensorimotor pathway, responsible for mapping sensory inputs to behavioral changes. This type of pathway is also well-researched in robotics where it is applied to achieve closed-loop operation of legged robots by adapting amplitude, frequency, and phase of the motor output. In this paper we investigate how spiking and non-spiking neurons can be combined to create a sensorimotor neuron pathway capable of shaping network output based on analog input. We propose sub-threshold operation of an existing spiking neuron model to create a non-spiking neuron able to interpret analog information and communicate with spiking neurons. The validity of this methodology is confirmed through a simulation of a closed-loop amplitude regulating network inspired by the internal feedback loops found in insects for posturing. Additionally, we show that non-spiking neurons can effectively manipulate post-synaptic spiking neurons in an event-based architecture. The ability to work with mixed networks provides an opportunity for researchers to investigate new network architectures for adaptive controllers, potentially improving locomotion strategies of legged robots.
C1 [Strohmer, Beck; Stagsted, Rasmus Karnoe; Manoonpong, Poramate; Larsen, Leon Bonde] Univ Southern Denmark, Maersk McKinney Moller Inst, SDU Biorobot, Odense, Denmark.
RP Strohmer, B (corresponding author), Univ Southern Denmark, Maersk McKinney Moller Inst, SDU Biorobot, Odense, Denmark.
EM stroh@mmmi.sdu.dk
CR Adrian ED, 1926, J PHYSIOL-LONDON, V61, P49, DOI 10.1113/jphysiol.1926.sp002273
   Aoi S, 2017, FRONT NEUROROBOTICS, V11, DOI 10.3389/fnbot.2017.00039
   Azarfar A, 2018, NEUROSCI BIOBEHAV R, V94, P238, DOI 10.1016/j.neubiorev.2018.09.007
   Barikhan SS, 2014, LECT NOTES ARTIF INT, V8575, P65, DOI 10.1007/978-3-319-08864-8_7
   Bidaye SS, 2018, J NEUROPHYSIOL, V119, P459, DOI 10.1152/jn.00658.2017
   Bing ZS, 2018, FRONT NEUROROBOTICS, V12, DOI 10.3389/fnbot.2018.00035
   Bohte Sander M., 2004, Natural Computing, V3, P195, DOI 10.1023/B:NACO.0000027755.02868.60
   Brette R, 2005, J NEUROPHYSIOL, V94, P3637, DOI 10.1152/jn.00686.2005
   Burrows, 1996, NEUROBIOLOGY INSECT, DOI DOI 10.1093/ACPROF:OSO/9780198523444.001.0001
   BURROWS M, 1978, J PHYSIOL-LONDON, V285, P231, DOI 10.1113/jphysiol.1978.sp012569
   BUSCHGES A, 1995, J NEUROPHYSIOL, V73, P1843, DOI 10.1152/jn.1995.73.5.1843
   Danner SM, 2017, ELIFE, V6, DOI [10.7554/eLife.31059, 10.7554/eLife.31050]
   Dürr V, 2019, FRONT NEUROROBOTICS, V13, DOI 10.3389/fnbot.2019.00088
   GRAUBARD K, 1978, J NEUROPHYSIOL, V41, P1014, DOI 10.1152/jn.1978.41.4.1014
   Hooper SL, 2007, J NEUROPHYSIOL, V97, P1428, DOI 10.1152/jn.01014.2006
   Jordan J, 2019, **DATA OBJECT**, DOI [10.5281/zenodo.2605422, DOI 10.5281/ZENODO.2605422]
   Larsen L.B, 2021, CLOUDBRAIN REAL TIME, DOI [10.1101/2021.01.21.427662, DOI 10.1101/2021.01.21.427662]
   Liu Q, 2018, CELL, V175, P57, DOI 10.1016/j.cell.2018.08.018
   Mallot H.A, 2013, COMPUTATIONAL NEUROS, P113, DOI [10.1007/978-3-319-00861-5_5, DOI 10.1007/978-3-319-00861-5_5]
   Markin SN, 2010, ANN NY ACAD SCI, V1198, P21, DOI 10.1111/j.1749-6632.2010.05435.x
   McCrea DA, 2008, BRAIN RES REV, V57, P134, DOI 10.1016/j.brainresrev.2007.08.006
   McDonnell MD, 2017, PLOS COMPUT BIOL, V13, DOI 10.1371/journal.pcbi.1005634
   Mileusnic MP, 2006, J NEUROPHYSIOL, V96, P1772, DOI 10.1152/jn.00868.2005
   Nachstedt T, 2013, IEEE INT CONF ROBOT, P3389, DOI 10.1109/ICRA.2013.6631050
   Naud R, 2008, BIOL CYBERN, V99, P335, DOI 10.1007/s00422-008-0264-7
   Niu CXM, 2017, J NEURAL ENG, V14, DOI 10.1088/1741-2552/aa593c
   Panzeri S, 2015, TRENDS COGN SCI, V19, P162, DOI 10.1016/j.tics.2015.01.002
   Petro B, 2020, IEEE T NEUR NET LEAR, V31, P358, DOI 10.1109/TNNLS.2019.2906158
   Pitchai M, 2019, LECT NOTES COMPUT SC, V11727, P698, DOI 10.1007/978-3-030-30487-4_53
   Raphael G, 2010, J NEUROSCI, V30, P9431, DOI 10.1523/JNEUROSCI.5537-09.2010
   Schafer W, 2016, CURR BIOL, V26, pR955, DOI 10.1016/j.cub.2016.07.044
   Schilling M, 2013, BIOL CYBERN, V107, P397, DOI 10.1007/s00422-013-0563-5
   Schrauwen B, 2003, IEEE IJCNN, P2825
   Sengupta N, 2017, INFORM SCIENCES, V406, P133, DOI 10.1016/j.ins.2017.04.017
   Storchi R, 2012, J NEUROPHYSIOL, V108, P1810, DOI 10.1152/jn.00921.2011
   Strohmer B, 2020, FRONT NEUROROBOTICS, V14, DOI 10.3389/fnbot.2020.00041
   Szczecinski NS, 2015, IEEE INT C INT ROBOT, P3875, DOI 10.1109/IROS.2015.7353922
   Thor M, 2019, IEEE ROBOT AUTOM LET, V4, P3324, DOI 10.1109/LRA.2019.2926660
   Tuthill JC, 2016, CURR BIOL, V26, pR1022, DOI 10.1016/j.cub.2016.06.070
   von Uckermann G, 2009, J NEUROPHYSIOL, V102, P1956, DOI 10.1152/jn.00312.2009
   Wozniak S, 2020, NAT MACH INTELL, V2, P325, DOI 10.1038/s42256-020-0187-0
   Yang SM, 2013, J NEUROPHYSIOL, V109, P711, DOI 10.1152/jn.00934.2012
   Zhou K., 2015, 2015 11th Conference on Lasers and Electro-Optics Pacific Rim (CLEO-PR). Proceedings, P1, DOI 10.1109/CLEOPR.2015.7376418
NR 43
TC 6
Z9 6
U1 1
U2 18
PD MAR 5
PY 2021
VL 15
AR 633945
DI 10.3389/fnins.2021.633945
UT WOS:000630353600001
DA 2023-11-16
ER

PT J
AU Guo, SQ
   Yu, ZF
   Deng, F
   Hu, XL
   Chen, F
AF Guo, Shangqi
   Yu, Zhaofei
   Deng, Fei
   Hu, Xiaolin
   Chen, Feng
TI Hierarchical Bayesian Inference and Learning in Spiking Neural Networks
SO IEEE TRANSACTIONS ON CYBERNETICS
DT Article
DE Hierarchical Bayesian model; mean field theory; spike-timing-dependent
   plasticity (STDP); spiking neural network; variational expectation
   maximization; winner-takes-all (WTA) circuits
ID COMPUTATION; INFORMATION; REINFORCEMENT; MODELS
AB Numerous experimental data from neuroscience and psychological science suggest that human brain utilizes Bayesian principles to deal the complex environment. Furthermore, hierarchical Bayesian inference has been proposed as an appropriate theoretical framework for modeling cortical processing. However, it remains unknown how such a computation is organized in the network of biologically plausible spiking neurons. In this paper, we propose a hierarchical network of winner-take-all circuits which can carry out hierarchical Bayesian inference and learning through a spike-based variational expectation maximization (EM) algorithm. Particularly, we show how the firing activities of spiking neurons in response to the input stimuli and the spike-timing-dependent plasticity rule can be understood, respectively, as variational E-step and M-step of variational EM. Finally, we demonstrate the utility of this spiking neural network on the MNIST benchmark for unsupervised classification of handwritten digits.
C1 [Guo, Shangqi; Yu, Zhaofei; Deng, Fei; Chen, Feng] Tsinghua Univ, Dept Automat, Beijing 100086, Peoples R China.
   [Guo, Shangqi; Yu, Zhaofei; Deng, Fei; Chen, Feng] LSBDPA Beijing Key Lab, Beijing 100084, Peoples R China.
   [Hu, Xiaolin] Tsinghua Univ, Ctr Brain Inspired Comp Res, TNLIST, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
RP Chen, F (corresponding author), Tsinghua Univ, Dept Automat, Beijing 100086, Peoples R China.; Hu, XL (corresponding author), Tsinghua Univ, Ctr Brain Inspired Comp Res, TNLIST, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
EM gsq15@mails.tsinghua.edu.cn; yuzf12@mails.tsinghua.edu.cn;
   dengf15@mails.tsinghua.edu.cn; xlhu@mail.tsinghua.edu.cn;
   chenfeng@tsinghua.edu.cn
CR Angelaki DE, 2009, CURR OPIN NEUROBIOL, V19, P452, DOI 10.1016/j.conb.2009.06.008
   Barber D, 2000, ADV NEUR IN, V12, P393
   Beck JM, 2008, NEURON, V60, P1142, DOI 10.1016/j.neuron.2008.09.021
   Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Beyeler M, 2013, NEURAL NETWORKS, V48, P109, DOI 10.1016/j.neunet.2013.07.012
   Bond AH, 2004, J THEOR BIOL, V227, P51, DOI 10.1016/j.jtbi.2003.10.005
   Buesing L., 2011, PLOS COMPUT BIOL, V7, P725
   Chater N, 2006, TRENDS COGN SCI, V10, P287, DOI 10.1016/j.tics.2006.05.007
   Diehl P.U., 2015, 2015 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2015.7280696
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Ernst MO, 2002, NATURE, V415, P429, DOI 10.1038/415429a
   Fontolan L, 2014, NAT COMMUN, V5, DOI 10.1038/ncomms5694
   Friston KJ, 2010, NAT REV NEUROSCI, V11, P127, DOI 10.1038/nrn2787
   Funamizu A, 2016, NAT NEUROSCI, V19, P1682, DOI 10.1038/nn.4390
   Ghahramani Z., 2010, ADV NEURAL INFORM PR, P19
   Harmeling S, 2011, IEEE T PATTERN ANAL, V33, P1087, DOI 10.1109/TPAMI.2010.145
   Heller K.A., 2005, P 22 INT C MACH LEAR, P297, DOI DOI 10.1145/1102351.1102389
   Huang YP, 2016, NEURAL COMPUT, V28, P1503, DOI 10.1162/NECO_a_00851
   HUBEL DH, 1963, J PHYSIOL-LONDON, V165, P559, DOI 10.1113/jphysiol.1963.sp007079
   Huerta R, 2009, NEURAL COMPUT, V21, P2123, DOI 10.1162/neco.2009.03-08-733
   Indiveri G, 2006, IEEE T NEURAL NETWOR, V17, P211, DOI 10.1109/TNN.2005.860850
   Jeon HA, 2014, FRONT SYST NEUROSCI, V8, DOI 10.3389/fnsys.2014.00223
   Kappel D, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003511
   Koller D., 2009, PROBABILISTIC GRAPHI
   Körding KP, 2004, NATURE, V427, P244, DOI 10.1038/nature02169
   Kushner H. J., 1997, J AM STAT ASSOC, V93, P763
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lee TS, 2003, J OPT SOC AM A, V20, P1434, DOI 10.1364/JOSAA.20.001434
   Legenstein R, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.1000180
   Ma WJ, 2006, NAT NEUROSCI, V9, P1432, DOI 10.1038/nn1790
   Maass W, 2014, P IEEE, V102, P860, DOI 10.1109/JPROC.2014.2310593
   Nasrabadi N.M., 2007, PATTERN RECOGN, V16, DOI 10.1117/1.2819119
   Naud R., 2014, NEURONAL DYNAMICS SI
   Neftci EO, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00241
   Nessler B, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003037
   Nouri Ali, 2009, 2009 14th International CSI Computer Conference (CSICC 2009) (Postponed from July 2009), P582, DOI 10.1109/CSICC.2009.5349642
   Oster M, 2009, NEURAL COMPUT, V21, P2437, DOI 10.1162/neco.2009.07-08-829
   Pecevski D, 2016, ENEURO, V3, DOI 10.1523/ENEURO.0048-15.2016
   Pecevski D, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1002294
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Rao RPN, 2004, NEURAL COMPUT, V16, P1, DOI 10.1162/08997660460733976
   Rao RPN, 1999, NAT NEUROSCI, V2, P79, DOI 10.1038/4580
   Seung HS, 2003, NEURON, V40, P1063, DOI 10.1016/S0896-6273(03)00761-X
   Shi L., 2009, ADV NEURAL INFORM PR, V22, P1669
   Sountsov P, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00046
   Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001
   Yu ZF, 2016, NEUROCOMPUTING, V175, P155, DOI 10.1016/j.neucom.2015.10.045
   Zhang NL, 2004, J MACH LEARN RES, V5, P697
NR 48
TC 14
Z9 16
U1 3
U2 27
PD JAN
PY 2019
VL 49
IS 1
BP 133
EP 145
DI 10.1109/TCYB.2017.2768554
UT WOS:000454242300011
DA 2023-11-16
ER

PT C
AU Wu, QX
   McGinnity, TM
   Maguire, L
   Cai, RT
   Chen, MG
AF Wu, QingXiang
   McGinnity, T. Martin
   Maguire, Liam
   Cai, Rongtai
   Chen, Meigui
BE Huang, D
   Gan, Y
   Premaratne, P
   Han, K
TI Simulation of Visual Attention Using Hierarchical Spiking Neural
   Networks
SO BIO-INSPIRED COMPUTING AND APPLICATIONS
SE Lecture Notes in Bioinformatics
DT Proceedings Paper
CT 7th International Conference on Intelligent Computing (ICIC)
CY AUG 11-14, 2011
CL Zhengzhou, PEOPLES R CHINA
DE Visual attention; spiking neural network; receptive field; visual system
AB Based on the information processing functionalities of spiking neurons, a hierarchical spiking neural network model is proposed to simulate visual attention. The network is constructed with a conductance-based integrate-and-fire neuron model and a set of specific receptive fields in different levels. The simulation algorithm and properties of the network are detailed in this paper. Simulation results show that the network is able to perform visual attention to extract objects based on specific image features. Using extraction of horizontal and vertical lines, a demonstration shows how the network can detect a house in a visual image. Using this visual attention principle, many other objects can be extracted by analogy.
C1 [Wu, QingXiang; McGinnity, T. Martin; Maguire, Liam] Univ Ulster, Intelligent Syst Res Ctr, Sch Comp & Intelligent Syst, Magee BT48 7JL, Londonderry, North Ireland.
   [Cai, Rongtai; Chen, Meigui] Fujian Normal Univ, Sch Phys & OptoElectron Technol, Fuzhou 350007, Peoples R China.
RP Wu, QX (corresponding author), Univ Ulster, Intelligent Syst Res Ctr, Sch Comp & Intelligent Syst, Magee BT48 7JL, Londonderry, North Ireland.
EM q.wu@ulster.ac.uk; tm.mcginnity@ulster.ac.uk; lp.maguire@ulster.ac.uk;
   rtcai@fjnu.edu.cn; mgchen@fjnu.edu.cn
CR Anderson J.R., 2004, COGNITIVE PSYCHOL IT
   Gerstner W., 2002, SPIKING NEURON MODEL
   Hosoya T, 2005, NATURE, V436, P71, DOI 10.1038/nature03689
   Jessell T. M, 1981, PRINCIPLES NEURAL SC
   Lauritzen TZ, 2009, J VISION, V9, DOI 10.1167/9.13.18
   Saalmann YB, 2007, SCIENCE, V316, P1612, DOI 10.1126/science.1139140
   Wu QX, 2007, LECT NOTES ARTIF INT, V4682, P26
   Wu QX, 2005, LECT NOTES COMPUT SC, V3610, P420
NR 8
TC 2
Z9 2
U1 0
U2 2
PY 2012
VL 6840
BP 26
EP +
UT WOS:000314259000005
DA 2023-11-16
ER

PT C
AU Stauffer, J
   Zhang, QX
AF Stauffer, Jake
   Zhang, Qingxue
GP IEEE
TI Spiking Neural Network With Backpropagation Learning for Brain Visual
   Dynamics Decoding
SO 2023 11TH INTERNATIONAL IEEE/EMBS CONFERENCE ON NEURAL ENGINEERING, NER
SE International IEEE EMBS Conference on Neural Engineering
DT Proceedings Paper
CT 11th International IEEE EMBS Conference on Neural Engineering (IEEE/EMBS
   NER)
CY APR 24-27, 2023
CL IEEE Engn Med & Biol Soc, Baltimore, MD
HO IEEE Engn Med & Biol Soc
DE Brain Visual Dynamics Decoding; Neuroscience; Spiking Neural Network;
   Backpropagation
AB Spiking neural network, inspired by neuroscience, is of promising potential for effective spatiotemporal mining of dynamics, mimicking the brain functions on sparse and spatiotemporal spike learning. It is expected to provide more efficient learning through abstracting the patterns from the spike trains. Focusing on the challenges on training this special network, in this study, we propose a backpropagation learning algorithm to backward propagate the learning loss to each layer of the spiking network, thereby enabling automatic model updating. The learning loss can be propagated through both neuron somas and the synaptic connections, through specific gradient calculations. The impulse response model has been used for the synaptic response, and the learning loss is sent backward through the synapses to calculate the weight gradients. The probabilistic escape rate model makes gradient on the firing unit convenient to be calculated, and the integral operator accumulates the total gradient over time. Evaluated on a decoding task on brain visual dynamics, eyewriting-based human computer interaction, the algorithm shows promising results and demonstrates the effectiveness of training the special spiking neural network. The highest decoding accuracy is up to 98% and the average over six subjects is 85%. This study will directly contribute to the model learning challenge of the spiking neural network, and broadly advance efficient brain-mimic learning algorithms on brain signal decoding.
C1 [Stauffer, Jake] Purdue Univ, Sch Engn & Technol, Elect & Comp Engn, W Lafayette, IN 47907 USA.
   [Zhang, Qingxue] Purdue Univ, Sch Engn & Technol, Elect & Comp Engn, Biomed Engn, W Lafayette, IN 47907 USA.
RP Zhang, QX (corresponding author), Purdue Univ, Sch Engn & Technol, Elect & Comp Engn, Biomed Engn, W Lafayette, IN 47907 USA.
EM qxzhang@purdue.edu
CR Comsa JM, 2020, INT CONF ACOUST SPEE, P8529, DOI [10.1109/icassp40776.2020.9053856, 10.1109/ICASSP40776.2020.9053856]
   Dhumal Deshmukh Rushali, 2020, 2020 2nd International Conference on Innovative Mechanisms for Industry Applications (ICIMIA). Proceedings, P76, DOI 10.1109/ICIMIA48430.2020.9074941
   Fang FM, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0192684
   Hao YZ, 2020, NEURAL NETWORKS, V121, P387, DOI 10.1016/j.neunet.2019.09.007
   Huang L, 2020, IEEE T MOBILE COMPUT, V19, P2581, DOI 10.1109/TMC.2019.2928811
   Mohammadi M, 2018, IEEE COMMUN SURV TUT, V20, P2923, DOI 10.1109/COMST.2018.2844341
   Pavlidis NG, 2005, IEEE IJCNN, P2190
   Pfister JP, 2006, NEURAL COMPUT, V18, P1318, DOI 10.1162/neco.2006.18.6.1318
   Shrestha A, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0220344
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Wang XW, 2020, NEURAL NETWORKS, V125, P258, DOI 10.1016/j.neunet.2020.02.011
   Xiang SY, 2019, IEEE J SEL TOP QUANT, V25, DOI 10.1109/JSTQE.2019.2911565
   Zhang ZW, 2022, IEEE T KNOWL DATA EN, V34, P249, DOI 10.1109/TKDE.2020.2981333
   Zou J., 2020, 11 IEEE ANN UBIQUITO
   Zou J., IEEE EMBS ANN C 2021
NR 15
TC 0
Z9 0
U1 0
U2 0
PY 2023
DI 10.1109/NER52421.2023.10123753
UT WOS:001009053700040
DA 2023-11-16
ER

PT J
AU Bazhanova, MV
   Gordleeva, SY
   Kazantsev, VB
   Lobov, SA
AF Bazhanova, M., V
   Gordleeva, S. Yu
   Kazantsev, V. B.
   Lobov, S. A.
TI Control of network bursting discharges by local electrical stimulation
   in spiking neuron network
SO IZVESTIYA VYSSHIKH UCHEBNYKH ZAVEDENIY-PRIKLADNAYA NELINEYNAYA DINAMIKA
DT Article
DE mathematical modeling; spiking neuron network; control; stimulation
ID SYNCHRONIZATION; MECHANISMS; PLASTICITY; BEHAVIOR; SLEEP; MODEL
AB Goal. The paper is devoted to controlling the dynamics of spike neural networks by local periodic stimulation of various network sections. Methods. The simulation uses a network of synaptically connected spike neurons distributed in two-dimensional space. The dynamics of the transmembrane potential of neurons is described by the Izhikevich model, short-term synaptic plasticity is represented by the model Tsodyksa-Markram, the effects of changes in the efficiency of connections between neurons are modeled using spike-timing-dependent plasticity (STDP). Results. It is shown that the model reproduces the dynamics of living neural networks grown under in vitro conditions quite well. In its spontaneous dynamics, such a network exhibits a wide range of dynamic modes, including asynchronous spikes and quasi-synchronous spike bundles. It was found that due to STDP, the network can adapt to the stimulating signal, so that the network bundles become synchronized (phase-synchronized) with the stimulation signal. The analysis of the dependence of this effect on the parameters of stimulation, in particular, on the geometric dimensions of the stimulated area, as well as the connectivity of the network. Conclusion. With the help of local periodic stimulation of a part of the neural network,when selecting certain parameters of the stimulating signal, taking into account the characteristics of the network, externalcontrol of the dynamics of the spike neural network is possible.
C1 [Bazhanova, M., V; Gordleeva, S. Yu; Kazantsev, V. B.; Lobov, S. A.] Lobachevsky State Univ Nizhny Novgorod, Nizhnii Novgorod, Russia.
   [Gordleeva, S. Yu; Kazantsev, V. B.; Lobov, S. A.] Innopolis Univ, Innopolis, Russia.
   [Kazantsev, V. B.] Samara State Med Univ, Samara, Russia.
RP Lobov, SA (corresponding author), Lobachevsky State Univ Nizhny Novgorod, Nizhnii Novgorod, Russia.; Lobov, SA (corresponding author), Innopolis Univ, Innopolis, Russia.
EM arksinus-bmw@yandex.ru; gordleeva@neuro.nnov.ru;
   kazantsev@neuron.nnov.ru; losa99@yandex.ru
CR Andreev AV, 2019, PHYS REV E, V100, DOI 10.1103/PhysRevE.100.022224
   Andreev AV, 2020, CHAOS SOLITON FRACT, V139, DOI 10.1016/j.chaos.2020.110061
   Bakkum DJ, 2008, J NEURAL ENG, V5, P310, DOI 10.1088/1741-2560/5/3/004
   Baruchi I, 2007, PHYS REV E, V75, DOI 10.1103/PhysRevE.75.050901
   Benedek M, 2011, NEUROPSYCHOLOGIA, V49, P3505, DOI 10.1016/j.neuropsychologia.2011.09.004
   Botvinick M, 2015, ANNU REV PSYCHOL, V66, P83, DOI 10.1146/annurev-psych-010814-015044
   Braitenberg V, 1991, STUDIES BRAIN FUNCTI, V18, P251
   Cantero JL, 2005, REV NEUROSCIENCE, V16, P69, DOI 10.1515/REVNEURO.2005.16.1.69
   Chao ZC, 2007, J NEURAL ENG, V4, P294, DOI 10.1088/1741-2560/4/3/015
   Esir PM, 2018, PHYS REV E, V98, DOI 10.1103/PhysRevE.98.052401
   Funahashi S, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00431
   Gordleeva S, 2020, SEMIN IMMUNOPATHOL, V42, P647, DOI 10.1007/s00281-020-00816-x
   Hermundstad AM, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003591
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Jiruska P, 2013, J PHYSIOL-LONDON, V591, P787, DOI 10.1113/jphysiol.2012.239590
   Jutrast MJ, 2010, CURR OPIN NEUROBIOL, V20, P150, DOI 10.1016/j.conb.2010.02.006
   Kazantsev VB, 2011, PHYS REV E, V84, DOI 10.1103/PhysRevE.84.031913
   Klimesch W, 1996, INT J PSYCHOPHYSIOL, V24, P61, DOI 10.1016/S0167-8760(96)00057-8
   Levy R, 2000, J NEUROSCI, V20, P7766
   Llinas R. R., 2002, I VORTEX NEURONS SEL
   Lobov S, 2016, EUR PHYS J-SPEC TOP, V225, P29, DOI 10.1140/epjst/e2016-02614-y
   Lobov SA, 2017, MATH MODEL NAT PHENO, V12, P109, DOI 10.1051/mmnp/201712409
   MAEDA E, 1995, J NEUROSCI, V15, P6834
   Makovkin SY, 2020, CHAOS SOLITON FRACT, V138, DOI 10.1016/j.chaos.2020.109951
   Mohns EJ, 2008, J NEUROSCI, V28, P10134, DOI 10.1523/JNEUROSCI.1967-08.2008
   Morrison A, 2008, BIOL CYBERN, V98, P459, DOI 10.1007/s00422-008-0233-1
   Nambu A, 2015, BASAL GANGLIA, V5, P1, DOI 10.1016/j.baga.2014.11.001
   Pankratova EV, 2019, NONLINEAR DYNAM, V97, P647, DOI 10.1007/s11071-019-05004-7
   Pimashkin A, 2013, FRONT NEURAL CIRCUIT, V7, DOI 10.3389/fncir.2013.00087
   Pimashkin A, 2011, FRONT COMPUT NEUROSC, V5, DOI 10.3389/fncom.2011.00046
   Rubchinsky LL, 2012, NONLINEAR DYNAM, V68, P329, DOI 10.1007/s11071-011-0223-z
   Scharfman HE, 2007, CURR NEUROL NEUROSCI, V7, P348, DOI 10.1007/s11910-007-0053-z
   Shahaf G, 2001, J NEUROSCI, V21, P8782, DOI 10.1523/JNEUROSCI.21-22-08782.2001
   Simonov AY, 2014, JETP LETT+, V98, P632, DOI 10.1134/S0021364013230136
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Spencer KM, 2003, J NEUROSCI, V23, P7407
   Tsodyks M, 1998, NEURAL COMPUT, V10, P821, DOI 10.1162/089976698300017502
   Uhlhaas Peter J, 2013, Dialogues Clin Neurosci, V15, P301
   Wagenaar DA, 2006, BMC NEUROSCI, V7, DOI 10.1186/1471-2202-7-11
   Wang XJ, 2012, CURR OPIN NEUROBIOL, V22, P1039, DOI 10.1016/j.conb.2012.08.006
   Whitwell HJ, 2020, FRONT AGING NEUROSCI, V12, DOI 10.3389/fnagi.2020.00136
NR 42
TC 0
Z9 0
U1 0
U2 2
PY 2021
VL 29
IS 3
BP 428
EP 439
DI 10.18500/0869-6632-2021-29-3-428-439
UT WOS:000657799500010
DA 2023-11-16
ER

PT S
AU Cristini, A
   Salerno, M
   Susi, G
AF Cristini, Alessandro
   Salerno, Mario
   Susi, Gianluca
BE Bassis, S
   Esposito, A
   Morabito, FC
TI A Continuous-Time Spiking Neural Network Paradigm
SO ADVANCES IN NEURAL NETWORKS: COMPUTATIONAL AND THEORETICAL ISSUES
SE Smart Innovation Systems and Technologies
DT Article; Book Chapter
DE Neuron Model; Spike Latency; Spiking Neural Network; Synaptic
   Plasticity; Continuous-Time Paradigm; Event-Driven Simulation
ID EVENT-DRIVEN SIMULATION; NEURONS; MODEL; DYNAMICS
AB In this work, a novel continuous-time spiking neural network paradigm is presented. Indeed, because of a neuron can fire at any given time, this kind of approach is necessary. For the purpose of developing a simulation tool having such a property, an ad-hoc event-driven method is implemented. A simplified neuron model is introduced with characteristics similar to the classic Leaky Integrate-and-Fire model, but including the spike latency effect. The latency takes into account that the firing of a given neuron is not instantaneous, but occurs after a continuous-time delay. Both excitatory and inhibitory neurons are considered, and simple synaptic plasticity rules are modeled. Nevetheless the chance to customize the network topology, an example with Cellular Neural Network (CNN)-like connections is presented, and some interesting global effects emerging from the simulations are reported.
C1 [Cristini, Alessandro; Salerno, Mario; Susi, Gianluca] Univ Roma Tor Vergata, Dept Elect Engn, Via Politecn 1, I-00133 Rome, Italy.
RP Cristini, A (corresponding author), Univ Roma Tor Vergata, Dept Elect Engn, Via Politecn 1, I-00133 Rome, Italy.
EM alessandro.cristini84@gmail.com; salerno@uniroma2.it;
   gianluca.susi@uniroma2.it
CR [Anonymous], 2012, 2012 INT JOINT C NEU
   Belatreche A, 2007, SOFT COMPUT, V11, P239, DOI [10.1007/s00500-006-0065-7, 10.1007/S00500-006-0065-7]
   Brette R, 2007, J COMPUT NEUROSCI, V23, P349, DOI 10.1007/s10827-007-0038-6
   Brunel N, 2007, BIOL CYBERN, V97, P337, DOI 10.1007/s00422-007-0190-0
   Burkitt AN, 2006, BIOL CYBERN, V95, P97, DOI 10.1007/s00422-006-0082-8
   Burkitt AN, 2006, BIOL CYBERN, V95, P1, DOI 10.1007/s00422-006-0068-6
   Buzsaki G, 2006, RHYTHMS BRAIN, P198
   CHUA LO, 1988, IEEE T CIRCUITS SYST, V35, P1257, DOI 10.1109/31.7600
   Citri A, 2008, NEUROPSYCHOPHARMACOL, V33, P18, DOI 10.1038/sj.npp.1301559
   D'Haene M, 2009, NEURAL COMPUT, V21, P1068, DOI 10.1162/neco.2008.02-08-707
   Edelman G. M., 1987, NEURAL DARWINISM THE
   FINKEL LH, 1985, P NATL ACAD SCI USA, V82, P1291, DOI 10.1073/pnas.82.4.1291
   FitzHugh R., 1955, B MATH BIOPHYS, V17, P257, DOI [10.1007/BF02477753, DOI 10.1007/BF02477753]
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2004, CEREB CORTEX, V14, P933, DOI 10.1093/cercor/bhh053
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Mattia M, 2000, NEURAL COMPUT, V12, P2305, DOI 10.1162/089976600300014953
   Newman MEJ, 2003, SIAM REV, V45, P167, DOI 10.1137/S003614450342480
   Okun M., 2009, SCHOLARPEDIA, V4, P7467, DOI DOI 10.4249/SCHOLARPEDIA.7467
   Parasuraman K, 2006, WATER RESOUR RES, V42, DOI 10.1029/2005WR004317
   Pernice V, 2012, PHYS REV E, V85, DOI 10.1103/PhysRevE.85.031916
   Ponulak F, 2011, ACTA NEUROBIOL EXP, V71, P409
   Ros E, 2006, NEURAL COMPUT, V18, P2959, DOI 10.1162/neco.2006.18.12.2959
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Sullivan TJ, 2006, NEURAL NETWORKS, V19, P734, DOI 10.1016/j.neunet.2006.05.006
   Wang HT, 2013, J THEOR BIOL, V328, P19, DOI 10.1016/j.jtbi.2013.03.003
   Watts DJ, 1998, NATURE, V393, P440, DOI 10.1038/30918
   Wu QX, 2012, LECT N BIOINFORMAT, V6840, P26
NR 30
TC 7
Z9 7
U1 0
U2 0
PY 2015
VL 37
BP 49
EP 60
DI 10.1007/978-3-319-18164-6_6
D2 10.1007/978-3-319-18164-6
UT WOS:000383980800007
DA 2023-11-16
ER

PT C
AU Vandesompele, A
   Walter, F
   Röhrbein, F
AF Vandesompele, Alexander
   Walter, Florian
   Roehrbein, Florian
GP IEEE
TI Neuro-Evolution of Spiking Neural Networks on SpiNNaker Neuromorphic
   Hardware
SO PROCEEDINGS OF 2016 IEEE SYMPOSIUM SERIES ON COMPUTATIONAL INTELLIGENCE
   (SSCI)
DT Proceedings Paper
CT IEEE Symposium Series on Computational Intelligence (IEEE SSCI)
CY DEC 06-09, 2016
CL Athens, GREECE
AB Neuro-evolutionary algorithms optimize the synaptic connectivity of sets of candidate neural networks based on a task-dependent fitness function. Compared to the commonly used methods from machine learning, many of them not only support the adaptation of connection weights but also of the network topology. However, the evaluation of the current fitness requires running every candidate network in every generation. This becomes a major impediment especially when using biologically inspired spiking neural networks which require considerable amounts of simulation time even on powerful computers. In this paper, we address this issue by offloading the network simulation to SpiNNaker, a state-of-the art neuromorphic hardware architecture which is capable of simulating large spiking neural networks in biological real-time. We were able to apply SpiNNaker's simulation power to the popular NEAT algorithm by running all candidate networks in parallel and successfully evolved spiking neural networks for solving the XOR problem and for playing the Pac-Man arcade game
C1 [Vandesompele, Alexander; Walter, Florian; Roehrbein, Florian] Tech Univ Munich, Dept Informat, Chair Robot & Embedded Syst, Boltzmannstr 3, D-85748 Garching, Germany.
   [Vandesompele, Alexander] Univ Ghent, Elect & Informat Syst Dept, Ghent, Belgium.
RP Vandesompele, A (corresponding author), Tech Univ Munich, Dept Informat, Chair Robot & Embedded Syst, Boltzmannstr 3, D-85748 Garching, Germany.; Vandesompele, A (corresponding author), Univ Ghent, Elect & Informat Syst Dept, Ghent, Belgium.
CR [Anonymous], 2016, NAT METHODS, DOI DOI 10.1038/nmeth.3707
   Clune J, 2011, IEEE T EVOLUT COMPUT, V15, P346, DOI 10.1109/TEVC.2010.2104157
   Clune J, 2009, IEEE C EVOL COMPUTAT, P2764, DOI 10.1109/CEC.2009.4983289
   Davison Andrew P, 2008, Front Neuroinform, V2, P11, DOI 10.3389/neuro.11.011.2008
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   Furber SB, 2013, IEEE T COMPUT, V62, P2454, DOI 10.1109/TC.2012.142
   Gauci J., 2008, P 23 NAT C ART INT A
   Gauci J, 2010, NEURAL COMPUT, V22, P1860, DOI 10.1162/neco.2010.06-09-1042
   Gerstner W., 2002, SPIKING NEURON MODEL
   GOLDBERG DE, 1987, P 2 INT C GEN ALG, P148
   Hausknecht M., 2013, IEEE T COMPUTATIONAL
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   HORNIK K, 1991, NEURAL NETWORKS, V4, P251, DOI 10.1016/0893-6080(91)90009-T
   Lipson H, 2000, NATURE, V406, P974, DOI 10.1038/35023115
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Mahowald M., 1994, ANALOG VLSI SYSTEM S
   McCulloch Warren S., 1943, BULL MATH BIOPHYS, V5, P115, DOI 10.1007/BF02478259
   Painkras E, 2013, IEEE J SOLID-ST CIRC, V48, P1943, DOI 10.1109/JSSC.2013.2259038
   ROSENBLATT F, 1958, PSYCHOL REV, V65, P386, DOI 10.1037/h0042519
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Stanley K, 2005, GECCO 2005: Genetic and Evolutionary Computation Conference, Vols 1 and 2, P1977
   Stanley KO, 2002, EVOL COMPUT, V10, P99, DOI 10.1162/106365602320169811
   Su-Hyung Jang, 2009, 2009 IEEE Symposium on Computational Intelligence and Games (CIG), P75, DOI 10.1109/CIG.2009.5286490
   Valsalam VK, 2012, EVOL INTELL, V5, P45, DOI 10.1007/s12065-011-0067-y
   VERBANCSICS P, J MACHINE LEARNING R, V11, P1737
   Walter F., 2015, NEURAL PROCESS LETT, P1
   Walter F, 2015, NEURAL NETWORKS, V72, P152, DOI 10.1016/j.neunet.2015.07.004
NR 27
TC 0
Z9 0
U1 0
U2 0
PY 2016
UT WOS:000400488303022
DA 2023-11-16
ER

PT C
AU Asai, T
   Hayashi, H
   Amemiya, Y
AF Asai, T
   Hayashi, H
   Amemiya, Y
BE Jamshidi, M
   Hata, Y
   Fathi, M
   Homaifar, A
   Jamshidi, JS
TI Analog integrate-and-fire neurochips: Neural competition in frequency
   and time domains
SO MULTIMEDIA, IMAGE PROCESSING AND SOFT COMPUTING: TRENDS, PRINCIPLES AND
   APPLICATIONS
SE TSI PRESS SERIES
DT Proceedings Paper
CT 5th Biannual World Automation Congress
CY JUN 09-13, 2002
CL ORLANDO, FL
DE analog VLSI; competitive neural network; spike-timing code;
   integrate-and-fire neurons
AB In this report, we present an inhibitory neural network, implemented oil analog CMOS chips, that exhibits competitive behaviors in the frequency and time domains. The circuit for each neuron was designed to produce sequences in time of identically-shaped pulses, called spikes. The result of experiment and simulation revealed that tile network more efficiently achieved the selective activation and inactivation of the neuron circuits oil the basis of spike timing rather than of firing rates. The results indicate that tile spike-timing-based neural processing by spiking neuron circuits providings a possible way of overcoming low tolerance problems of analog devices in noisy environments.
C1 Hokkaido Univ, Dept Elect Engn, Kita Ku, Sapporo, Hokkaido 0608628, Japan.
RP Asai, T (corresponding author), Hokkaido Univ, Dept Elect Engn, Kita Ku, Kita 13,Nishi 8, Sapporo, Hokkaido 0608628, Japan.
CR [Anonymous], 1988, NONLINEAR STOCHASTIC
   [Anonymous], NEUROMORPHIC SYSTEMS
   Fukai T, 1996, BIOL CYBERN, V75, P453, DOI 10.1007/s004220050310
   MAINEN ZF, 1995, SCIENCE, V268, P1503, DOI 10.1126/science.7770778
   RIEKE F, 1989, SPIKES EXPLORING NEU
   SIMMONS JA, 1989, COGNITION, V33, P155, DOI 10.1016/0010-0277(89)90009-7
   VITTOZ EA, 1985, DESIGN MOS VLSI CIRC, P104
NR 7
TC 0
Z9 0
U1 0
U2 0
PY 2002
VL 13
BP 129
EP 134
UT WOS:000180264900021
DA 2023-11-16
ER

PT C
AU Tang, CC
   Han, J
AF Tang, Chengcheng
   Han, Jie
BE IEEE
TI Hardware Efficient Weight-Binarized Spiking Neural Networks
SO 2023 DESIGN, AUTOMATION & TEST IN EUROPE CONFERENCE & EXHIBITION, DATE
SE Design Automation and Test in Europe Conference and Exhibition
DT Proceedings Paper
CT Design, Automation and Test in Europe Conference and Exhibition (DATE)
CY APR 17-19, 2023
CL Antwerp, BELGIUM
DE Spiking neural networks; priority encoder; binarized weights; field
   programmable gate arrays (FPGAs)
AB The advancement in spiking neural networks (SNNs) provides a promising and alternative approach to conventional artificial neural networks (ANNs) with higher energy efficiency. However, the significant requirements on memory usage presents a performance bottleneck on resource constrained devices. Inspired by the notion of binarized neural networks (BNNs), we incorporate the design principles in BNNs into that of SNNs to reduce the stringent resource requirements. Specifically, the weights are binarized to 1 and -1 for implementing the functions of excitatory and inhibitory synapses. Hence, the proposed design is referred to as a weight-binarized spiking neural network (WB-SNN). In the WB-SNN, only one bit is used for the weight or a spike; for the latter, 1 and 0 indicate a spike and no spike, respectively. A priority encoder is used to identify the index of an active neuron as a basic unit to construct the WB-SNN. We further design a fully connected neural network that consists of an input layer, an output layer, and fully connected layers of different sizes. A counter is utilized in each neuron to complete the accumulation of weights. The WB-SNN design is validated by using a multi-layer perceptron on the MNIST dataset. Hardware implementations on FPGAs show that the WB-SNN attains a significant saving of memory with only a limited accuracy loss compared with its SNN and BNN counterparts.
C1 [Tang, Chengcheng; Han, Jie] Univ Alberta, Dept Elect & Comp Engn, Edmonton, AB, Canada.
RP Tang, CC (corresponding author), Univ Alberta, Dept Elect & Comp Engn, Edmonton, AB, Canada.
EM ctang8@ualberta.ca; jhan8@ualberta.ca
CR Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   Aung MTL, 2021, I C FIELD PROG LOGIC, P28, DOI 10.1109/FPL53798.2021.00013
   Bouvier M, 2019, ACM J EMERG TECH COM, V15, DOI 10.1145/3304103
   Chuang PY, 2020, DES AUT CON, DOI 10.1109/dac18072.2020.9218714
   Courbariaux M, 2016, Arxiv, DOI arXiv:1602.02830
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Ghasemzadeh M, 2018, ANN IEEE SYM FIELD P, P57, DOI 10.1109/FCCM.2018.00018
   Jokic P, 2018, INT SYM IND EMBED, P1
   Kumar A, 2010, NAT REV NEUROSCI, V11, P615, DOI 10.1038/nrn2886
   LeCun Y., 1998, MNIST DATABASE HANDW
   Liu ST, 2020, IEEE CIRC SYST MAG, V20, P19, DOI 10.1109/MCAS.2020.3005483
   Liu YJ, 2022, IEEE T CIRCUITS-I, V69, P2553, DOI 10.1109/TCSI.2022.3160693
   Ma D, 2017, J SYST ARCHITECT, V77, P43, DOI 10.1016/j.sysarc.2017.01.003
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Tang C, 2021, IEEE RAD CONF, DOI 10.1109/RadarConf2147009.2021.9455314
   Tang HY, 2019, IEEE T BIOMED CIRC S, V13, P1664, DOI 10.1109/TBCAS.2019.2945406
   Tang W, 2017, AAAI CONF ARTIF INTE, P2625
   Umuroglu Y, 2017, FPGA'17: PROCEEDINGS OF THE 2017 ACM/SIGDA INTERNATIONAL SYMPOSIUM ON FIELD-PROGRAMMABLE GATE ARRAYS, P65, DOI 10.1145/3020078.3021744
   Van Rullen R, 2001, NEURAL COMPUT, V13, P1255, DOI 10.1162/08997660152002852
   Nguyen XT, 2017, IEEE T CIRCUITS-II, V64, P1102, DOI 10.1109/TCSII.2017.2672865
NR 20
TC 0
Z9 0
U1 0
U2 0
PY 2023
UT WOS:001027444200051
DA 2023-11-16
ER

PT J
AU Basu, A
   Shuo, S
   Zhou, HM
   Lim, MH
   Huang, GB
AF Basu, Arindam
   Shuo, Sun
   Zhou, Hongming
   Lim, Meng Hiot
   Huang, Guang-Bin
TI Silicon spiking neurons for hardware implementation of extreme learning
   machines
SO NEUROCOMPUTING
DT Article
DE Spiking neural network; Extreme learning machine; Asynchronous
   communication; Silicon neuron; Neuromorphic
ID MODEL
AB In this paper, we propose a silicon implementation of extreme learning machines (ELM) using spiking neural circuits. The major components of a silicon spiking neural network, neuron, synapse and 'Address Event Representation' (AER) for asynchronous spike based communication, are described. The benefits of using this hardware to implement an ELM as opposed to other single layer feedforward networks (SLFN) are explained. Several possible architectures for efficient implementation of ELM using these circuits are presented and their possible impact on ELM performance is discussed. (C) 2012 Elsevier B.V. All rights reserved.
C1 [Basu, Arindam; Shuo, Sun; Zhou, Hongming; Lim, Meng Hiot; Huang, Guang-Bin] Nanyang Technol Univ, Sch Elect & Elect Engn, Nanyang Ave, Singapore 639798, Singapore.
RP Basu, A (corresponding author), Nanyang Technol Univ, Sch Elect & Elect Engn, Nanyang Ave, Singapore 639798, Singapore.
EM arindam.basu@ntu.edu.sg; ssun2@e.ntu.edu.sg
CR [Anonymous], 2006, ART ANALOG LAYOUT
   Arthur JV, 2007, IEEE T NEURAL NETWOR, V18, P1815, DOI 10.1109/TNN.2007.900238
   Bartolozzi C, 2007, NEURAL COMPUT, V19, P2581, DOI 10.1162/neco.2007.19.10.2581
   Bartolozzi C, 2009, SENSORS-BASEL, V9, P5076, DOI 10.3390/s90705076
   Basu A, 2010, IEEE T CIRCUITS-I, V57, P2938, DOI 10.1109/TCSI.2010.2048772
   Boahen KA, 2000, IEEE T CIRCUITS-II, V47, P416, DOI 10.1109/82.842110
   Chan V, 2007, IEEE T CIRCUITS-I, V54, P48, DOI 10.1109/TCSI.2006.887979
   Choi TYW, 2005, IEEE T CIRCUITS-I, V52, P1049, DOI 10.1109/TCSI.2005.849136
   ENZ CC, 1995, ANALOG INTEGR CIRC S, V8, P83, DOI 10.1007/BF01239381
   Gerstner W., 2002, SPIKING NEURON MODEL
   Haykin S., 1998, NEURAL NETWORKS COMP, Vsecond
   Huang GB, 2006, NEUROCOMPUTING, V70, P489, DOI 10.1016/j.neucom.2005.12.126
   Indiveri G, 2006, IEEE T NEURAL NETWOR, V17, P211, DOI 10.1109/TNN.2005.860850
   Indiveri G, 2001, ANALOG INTEGR CIRC S, V28, P279, DOI 10.1023/A:1011208127849
   Indiveri G, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00073
   Lichtsteiner Patrick, 2008, IEEE Journal of Solid-State Circuits, V43, P566, DOI 10.1109/JSSC.2007.914337
   Mahowald M., 1992, VLSI ANALOGS NEURONA
   MEAD CA, 1989, ADVANCED RESEARCH IN VLSI : PROCEEDINGS OF THE DECENNIAL CALTECH CONFERENCE ON VLSI, P1
   Merolla P, 2004, ADV NEUR IN, V16, P995
   Neftci E, 2010, BIOMED CIRC SYST C, P262, DOI 10.1109/BIOCAS.2010.5709621
   Saïghi S, 2005, I IEEE EMBS C NEUR E, P285
   Saighi S., 2006, P INT S CIRC SYST MA
   Serrano-Gotarredona R, 2006, IEEE T CIRCUITS-I, V53, P2548, DOI 10.1109/TCSI.2006.883843
   Serrano-Gotarredona R, 2009, IEEE T NEURAL NETWOR, V20, P1417, DOI 10.1109/TNN.2009.2023653
   Shuo S., 2011, P IEEE BIOM CIRC SYS
NR 25
TC 58
Z9 58
U1 1
U2 43
PD FEB 15
PY 2013
VL 102
SI SI
BP 125
EP 134
DI 10.1016/j.neucom.2012.01.042
UT WOS:000313761500014
DA 2023-11-16
ER

PT J
AU Liu, GS
   Deng, WJ
   Xie, XR
   Huang, L
   Tang, HJ
AF Liu, Guisong
   Deng, Wenjie
   Xie, Xiurui
   Huang, Li
   Tang, Huajin
TI Human-Level Control Through Directly Trained Deep Spiking
   <i>Q</i>-Networks
SO IEEE TRANSACTIONS ON CYBERNETICS
DT Article; Early Access
DE Atari games; deep reinforcement learning (DRL); directly training;
   spiking neural networks (SNNs)
ID NEURAL-NETWORKS; GO
AB As the third-generation neural networks, spiking neural networks (SNNs) have great potential on neuromorphic hardware because of their high energy efficiency. However, deep spiking reinforcement learning (DSRL), that is, the reinforcement learning (RL) based on SNNs, is still in its preliminary stage due to the binary output and the nondifferentiable property of the spiking function. To address these issues, we propose a deep spiking Q-network (DSQN) in this article. Specifically, we propose a directly trained DSRL architecture based on the leaky integrate-and-fire (LIF) neurons and deep Q-network (DQN). Then, we adapt a direct spiking learning algorithm for the DSQN. We further demonstrate the advantages of using LIF neurons in DSQN theoretically. Comprehensive experiments have been conducted on 17 top-performing Atari games to compare our method with the state-of-the-art conversion method. The experimental results demonstrate the superiority of our method in terms of performance, stability, generalization and energy efficiency. To the best of our knowledge, our work is the first one to achieve state-of-the-art performance on multiple Atari games with the directly trained SNN.
C1 [Liu, Guisong; Huang, Li] Southwestern Univ Finance & Econ, Sch Comp & Artificial Intelligence, Chengdu 611130, Peoples R China.
   [Liu, Guisong] Univ Elect Sci & Technol China, Sch Comp Sci, Zhongshan Inst, Zhongshan 528400, Peoples R China.
   [Deng, Wenjie; Xie, Xiurui] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
   [Tang, Huajin] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310027, Peoples R China.
   [Tang, Huajin] Zhejiang Lab, Inst Artificial Intelligence, Hangzhou 311122, Peoples R China.
RP Xie, XR (corresponding author), Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
EM gliu@swufe.edu.cn; xiexiurui@uestc.edu.cn; htang@zju.edu.cn
CR Pérez-Carrasco JA, 2013, IEEE T PATTERN ANAL, V35, P2706, DOI 10.1109/TPAMI.2013.71
   Aquino G, 2020, IEEE ACCESS, V8, P46324, DOI 10.1109/ACCESS.2020.2979141
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Deng L, 2020, NEURAL NETWORKS, V121, P294, DOI 10.1016/j.neunet.2019.09.005
   Diehl PU, 2015, IEEE IJCNN
   Dominguez-Morales J., 2018, 2018 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2018.8489381
   Fang B, 2020, 2020 12TH INTERNATIONAL CONFERENCE ON ADVANCED COMPUTATIONAL INTELLIGENCE (ICACI), P412, DOI [10.1109/ICACI49185.2020.9177793, 10.1109/icaci49185.2020.9177793]
   Fang W., 2020, SPIKINGJELLY
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   Guo SQ, 2019, IEEE T CYBERNETICS, V49, P133, DOI 10.1109/TCYB.2017.2768554
   Hernández G, 2020, NEUROCOMPUTING, V390, P327, DOI 10.1016/j.neucom.2019.08.095
   Hessel M, 2018, AAAI CONF ARTIF INTE, P3215
   Ju XP, 2020, NEURAL COMPUT, V32, P182, DOI 10.1162/neco_a_01245
   Kim S, 2020, AAAI CONF ARTIF INTE, V34, P11270
   Liu Qianhui, 2021, IJCAI, P1743
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Mnih V, 2013, Arxiv, DOI arXiv:1312.5602
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Patel D, 2019, NEURAL NETWORKS, V120, P108, DOI 10.1016/j.neunet.2019.08.009
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Shen XB, 2022, IEEE T MULTIMEDIA, V24, P1116, DOI 10.1109/TMM.2021.3119868
   Shen XB, 2018, IEEE T NEUR NET LEAR, V29, P4324, DOI 10.1109/TNNLS.2017.2763967
   Shen XB, 2017, IEEE T CYBERNETICS, V47, P4275, DOI 10.1109/TCYB.2016.2606441
   Silver D, 2018, SCIENCE, V362, P1140, DOI 10.1126/science.aar6404
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Tan WH, 2021, AAAI CONF ARTIF INTE, V35, P9816
   Tang G., 2021, P 2020 C ROBOT LEARN, P2016
   Vinyals O, 2019, NATURE, V575, P350, DOI 10.1038/s41586-019-1724-z
   Wang YX, 2021, IEEE T COGN DEV SYST, V13, P514, DOI 10.1109/TCDS.2020.2971655
   Wang ZT, 2022, PARTICUOLOGY, V67, P1, DOI 10.1016/j.partic.2021.09.008
   Weng J, 2021, ARXIV
   Wu JB, 2022, IEEE T PATTERN ANAL, V44, P7824, DOI 10.1109/TPAMI.2021.3114196
   Wu JB, 2023, IEEE T NEUR NET LEAR, V34, P446, DOI 10.1109/TNNLS.2021.3095724
   Wu JB, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00199
   Wu YJ, 2019, AAAI CONF ARTIF INTE, P1311
   Xie XR, 2017, IEEE T NEUR NET LEAR, V28, P1411, DOI 10.1109/TNNLS.2016.2541339
   Xie XR, 2017, NEUROCOMPUTING, V241, P152, DOI 10.1016/j.neucom.2017.01.086
   Xu Y, 2017, 2017 IEEE SYMPOSIUM SERIES ON COMPUTATIONAL INTELLIGENCE (SSCI), P1219
   Yu Q, 2022, IEEE T CYBERNETICS, V52, P1364, DOI 10.1109/TCYB.2020.2984888
   Yuan MW, 2019, NEURAL COMPUT, V31, P2368, DOI 10.1162/neco_a_01238
   Zhang ML, 2022, IEEE T NEUR NET LEAR, V33, P1947, DOI 10.1109/TNNLS.2021.3110991
   Zheng HL, 2021, AAAI CONF ARTIF INTE, V35, P11062
NR 45
TC 3
Z9 3
U1 14
U2 33
PD 2022 SEP 5
PY 2022
DI 10.1109/TCYB.2022.3198259
EA SEP 2022
UT WOS:000852217300001
DA 2023-11-16
ER

PT C
AU Yamashita, D
   Saeki, K
   Sekine, Y
AF Yamashita, Daichi
   Saeki, Katsutoshi
   Sekine, Yoshifumi
GP IEEE
TI IC Implementation of Spike-timing-dependent Synaptic Plasticity Model
   Using Low Capacitance Value
SO 2014 IEEE ASIA PACIFIC CONFERENCE ON CIRCUITS AND SYSTEMS (APCCAS)
DT Proceedings Paper
CT IEEE Asia Pacific Conference on Circuits and Systems (APCCAS)
CY NOV 17-20, 2014
CL JAPAN
DE CMOS; spike timing plasticity; artificial neural network; nonlinear
   electronics circuit; integrated circuit
ID NEURONS; SYNAPSES
AB A number of recent studies on neural networks have been conducted with the purpose of applying engineering to the brain. An artificial neural networks (ANNs) were created that focus on how learning is achieved. In this paper, we focus on spike-timing-dependent synaptic plasticity (STDP) and construct an STDP model using semiconductors. We propose a new STDP model using a low capacitance value with CMOS technology, and show that the proposed integrated circuit has similar characteristics to biological neural networks.
C1 [Yamashita, Daichi] Nihon Univ, Grad Sch Sci & Technol, Funabashi, Chiba, Japan.
   [Saeki, Katsutoshi; Sekine, Yoshifumi] Nihon Univ, Coll Sci & Technol, Funabashi, Chiba, Japan.
RP Yamashita, D (corresponding author), Nihon Univ, Grad Sch Sci & Technol, Funabashi, Chiba, Japan.
EM y.daichi83@gmail.com; saeki.katsutoshi@nihon-u.ac.jp;
   sekine.yoshifumi@nihon-u.ac.jp
CR Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Cassidy AS, 2013, NEURAL NETWORKS, V45, P4, DOI 10.1016/j.neunet.2013.05.011
   Froemke Robert C, 2010, Front Synaptic Neurosci, V2, P19, DOI 10.3389/fnsyn.2010.00019
   Indiveri G, 2006, IEEE T NEURAL NETWOR, V17, P211, DOI 10.1109/TNN.2005.860850
   Mashimo Yuichi, 2012, TECHN M EL CIRC, P99
NR 5
TC 0
Z9 0
U1 0
U2 1
PY 2014
BP 221
EP 224
UT WOS:000361128200053
DA 2023-11-16
ER

PT J
AU Xiong, YZ
   Chen, YT
   Chen, CM
   Wei, XW
   Xue, YY
   Wan, H
   Wang, P
AF Xiong, Yizhou
   Chen, Yuantao
   Chen, Changming
   Wei, Xinwei
   Xue, Yingying
   Wan, Hao
   Wang, Ping
TI An Odor Recognition Algorithm of Electronic Noses Based on Convolutional
   Spiking Neural Network for Spoiled Food Identification
SO JOURNAL OF THE ELECTROCHEMICAL SOCIETY
DT Article
DE Electronic Noses; Odor Recognition Algorithm; Convolutional Neural
   Network; Spiking Neural Network; Spoiled Food Odor Classification
ID CLASSIFICATION; MACHINE
AB The electronic nose is an odor detection instrument utilizing the bionic olfactory theory, and usually consisting of a gas sensor array and an odor recognition algorithm. Traditional odor recognition algorithms for electronic noses suffer from cumbersome feature extraction steps and low recognition accuracy, and some new algorithms based on deep learning have the disadvantage of insufficient computational efficiency. To solve these problems, a novel odor recognition algorithm based on convolutional spiking neural network is proposed. The network model of this algorithm is composed of convolutional spiking layers, fully-connected spiking layers, and skip connections, which combines the feature extraction ability of the residual convolutional neural network and the computational efficiency of the spiking neural network. We also established a spoiled food mixed odor dataset and a rotten fruit odor dataset for evaluation of our method and several other classic algorithms. Experimental results show that our algorithm achieves average test accuracy of 84.5% and 88.6% on these two datasets, respectively, and has better recognition capability and computational efficiency than several other classic algorithms, which indicates that our algorithm can efficiently identify the spoilage odor in food.
C1 [Xiong, Yizhou; Chen, Yuantao; Chen, Changming; Wei, Xinwei; Xue, Yingying; Wan, Hao; Wang, Ping] Zhejiang Univ, Dept Biomed Engn, Key Lab Biomed Engn, Biosensor Natl Special Lab,Educ Minist, Hangzhou 310027, Peoples R China.
   [Xiong, Yizhou; Wan, Hao; Wang, Ping] Chinese Acad Sci, State Key Lab Transducer Technol, Shanghai 200050, Peoples R China.
RP Wan, H; Wang, P (corresponding author), Zhejiang Univ, Dept Biomed Engn, Key Lab Biomed Engn, Biosensor Natl Special Lab,Educ Minist, Hangzhou 310027, Peoples R China.; Wan, H; Wang, P (corresponding author), Chinese Acad Sci, State Key Lab Transducer Technol, Shanghai 200050, Peoples R China.
EM wh1816@zju.edu.cn; cnpwang@zju.edu.cn
CR Acevedo FJ, 2007, SENSOR ACTUAT B-CHEM, V122, P227, DOI 10.1016/j.snb.2006.05.033
   Arya S, 2013, 2013 1ST INTERNATIONAL CONFERENCE ON EMERGING TRENDS AND APPLICATIONS IN COMPUTER SCIENCE (ICETACS), P178, DOI 10.1109/ICETACS.2013.6691418
   Balasubramani V, 2020, J ELECTROCHEM SOC, V167, DOI 10.1149/1945-7111/ab77a0
   Brudzewski K, 2004, SENSOR ACTUAT B-CHEM, V98, P291, DOI 10.1016/j.snb.2003.10.028
   Brudzewski K, 2006, SENSOR ACTUAT B-CHEM, V113, P135, DOI 10.1016/j.snb.2005.02.039
   Delorme A, 1999, NEUROCOMPUTING, V26-7, P989, DOI 10.1016/S0925-2312(99)00095-8
   Fan H, 2018, SENSOR ACTUAT B-CHEM, V259, P183, DOI 10.1016/j.snb.2017.10.063
   Fang W., 2020, SPIKINGJELLY GITHUB
   Gautam A, 2020, APPL INTELL, V50, P830, DOI 10.1007/s10489-019-01552-y
   Gómez AH, 2006, SENSOR ACTUAT B-CHEM, V113, P347, DOI 10.1016/j.snb.2005.03.090
   Güney S, 2012, SENSOR ACTUAT B-CHEM, V166, P721, DOI 10.1016/j.snb.2012.03.047
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hunter GW, 2020, J ELECTROCHEM SOC, V167, DOI 10.1149/1945-7111/ab729c
   Karakaya D, 2020, INT J AUTOM COMPUT, V17, P179, DOI 10.1007/s11633-019-1212-9
   Khan S, 2015, MICROSYST TECHNOL, V21, P2011, DOI 10.1007/s00542-014-2277-6
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Liu YJ, 2019, REV SCI INSTRUM, V90, DOI 10.1063/1.5064540
   Liu YJ, 2018, IEEE SENS J, V18, P692, DOI 10.1109/JSEN.2017.2774438
   Loutfi A, 2015, J FOOD ENG, V144, P103, DOI 10.1016/j.jfoodeng.2014.07.019
   Mumyakmaz B, 2008, SENSOR ACTUAT B-CHEM, V128, P594, DOI 10.1016/j.snb.2007.07.062
   Namuduri S, 2020, J ELECTROCHEM SOC, V167, DOI 10.1149/1945-7111/ab67a8
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Pedregosa F., 2011, J MACH LEARN RES, V12, P2825
   Peng P, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18010157
   Reddy BKS, 2021, J ELECTROCHEM SOC, V168, DOI 10.1149/1945-7111/abf4ea
   Roy K, 2019, NATURE, V575, P607, DOI 10.1038/s41586-019-1677-2
   Sarkar ST, 2015, NEURAL NETWORKS, V71, P142, DOI 10.1016/j.neunet.2015.07.014
   Sun RZ, 2020, MEAS SCI TECHNOL, V31, DOI 10.1088/1361-6501/ab5417
   Vanarse A, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20102756
   Wan H, 2021, J ELECTROCHEM SOC, V168, DOI 10.1149/1945-7111/ac064e
   Wang Y, 2020, SENSOR ACTUAT A-PHYS, V307, DOI 10.1016/j.sna.2020.111874
   Wei GF, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19010217
   Xing YN, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.590164
   Xu S, 2014, SENSORS-BASEL, V14, P5486, DOI 10.3390/s140305486
   Yan K, 2015, SENSOR ACTUAT B-CHEM, V212, P353, DOI 10.1016/j.snb.2015.02.025
   Yang JL, 2016, SENSORS-BASEL, V16, DOI 10.3390/s16122069
   Zhang JY, 2020, J ELECTROCHEM SOC, V167, DOI 10.1149/1945-7111/abc83c
   Zhang JY, 2021, SENSOR ACTUAT B-CHEM, V326, DOI 10.1016/j.snb.2020.128822
   Zhao XJ, 2019, IEEE ACCESS, V7, P12630, DOI 10.1109/ACCESS.2019.2892754
NR 39
TC 14
Z9 14
U1 11
U2 64
PD JUL 1
PY 2021
VL 168
IS 7
AR 077519
DI 10.1149/1945-7111/ac1699
UT WOS:000682903400001
DA 2023-11-16
ER

PT C
AU She, XY
   Long, Y
   Mukhopadhyay, S
AF She, Xueyuan
   Long, Yun
   Mukhopadhyay, Saibal
GP IEEE
TI Improving Robustness of ReRAM-based Spiking Neural Network Accelerator
   with Stochastic Spike-timing-dependent-plasticity
SO 2019 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUL 14-19, 2019
CL Budapest, HUNGARY
DE ReRAM; spiking neural network; spike-timing-dependent-plasticity(STDP);
   process-in-memory(PIM)
AB Spike-timing-dependent-plasticity (STDP) is an unsupervised learning algorithm for spiking neural network (SNN), which promises to achieve deeper understanding of human brain and more powerful artificial intelligence. While conventional computing system fails to simulate SNN efficiently, process-in-memory (PIM) based on devices such as ReRAM can be used in designing fast and efficient STDP based SNN accelerators, as it operates in high resemblance with biological neural network. However, the real-life implementation of such design still suffers from impact of input noise and device variation. In this work, we present a novel stochastic STDP algorithm that uses spiking frequency information to dynamically adjust synaptic behavior. The algorithm is tested in pattern recognition task with noisy input and shows accuracy improvement over deterministic STDP. In addition, we show that the new algorithm can be used for designing a robust ReRAM based SNN accelerator that has strong resilience to device variation.
C1 [She, Xueyuan; Long, Yun; Mukhopadhyay, Saibal] Georgia Inst Technol, Sch Elect & Comp Engn, Atlanta, GA 30332 USA.
RP She, XY (corresponding author), Georgia Inst Technol, Sch Elect & Comp Engn, Atlanta, GA 30332 USA.
EM xshe@gatech.edu; yunlong@gatech.edu; saibal.mukhopadhyay@ece.gatech.edu
CR Bell Curtis C., 1997, NATURE
   Bi Guo-qiang, 2001, ANN REV NEUROSCIENCE
   Bliss T. V P, 1973, J PHYSL
   Carew T J, 1981, J NEUROSCI
   Chua L, 2014, SEMICOND SCI TECH, V29, DOI 10.1088/0268-1242/29/10/104001
   Debanne Dominique, 1998, J PHYSL
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Fieres Johannes, 2008, P INT JOINT C NEUR N
   Gerstner Wulfram, 1993, BIOL CYBERNETICS
   Grimmett G. R., 1993, PROBABILITY THEORY R
   Hawkins R. D., 1983, SCIENCE
   Hebb D O., 1950, AM J PSYCHOL
   Indiveri G., 2006, IEEE T NEURAL NETWOR
   Javed Fahad, 2010, AM J CLIN NUTR
   Jo S. H, 2010, NANO LETT
   Kawahara Akifumi, 2013, IEEE J SOLID STATE C
   Krizhevsky A, 2012, NEURIPS
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee Tsung-Wen, 2012, IEEE ELECT DEVICE LE
   LEVY WB, 1983, NEUROSCIENCE, V8, P791, DOI 10.1016/0306-4522(83)90010-6
   Levy William B., 1979, BRAIN RES
   Lin MY, 2018, ICCAD-IEEE ACM INT, DOI 10.1145/3240765.3240800
   Liu CC, 2015, DES AUT CON, DOI 10.1145/2744769.2744783
   Long Y, 2018, DES AUT TEST EUROPE, P159, DOI 10.23919/DATE.2018.8341996
   Long Yun, 2019, 2019 DES AUT TEST EU
   Magee Jeffrey C., 1997, SCIENCE
   Markram Henry, 1997, SCIENCE
   Mott N. F., 1979, ELECT PROCESSES NONC
   Napolean Francis, 2018, OVERVIEW CURRENT COM
   Querlioz D, 2013, IEEE T NANOTECHNOL, V12, P288, DOI 10.1109/TNANO.2013.2250995
   Saighi Sylvain, 2015, PLASTICITY MEMRISTIV
   Sainath Tara N., 2013, LCASSP IEEE INT C AC
   Seo K, 2011, NANOTECHNOLOGY, V22, DOI 10.1088/0957-4484/22/25/254023
   She Xueyuan, 2019, 2019 DES AUT TEST EU
   Srinivasan G, 2016, SCI REP-UK, V6, DOI 10.1038/srep29545
   Wang Y, 2015, INT GEOSCI REMOTE SE, P5015, DOI 10.1109/IGARSS.2015.7326959
   Wei Zhiqiang, 2017, IEEE T ELECT DEVICES
   Yu Shimeng, 2011, TECHNICAL DIGEST INT
NR 38
TC 10
Z9 12
U1 0
U2 3
PY 2019
UT WOS:000530893801017
DA 2023-11-16
ER

PT J
AU Thalmeier, D
   Uhlmann, M
   Kappen, HJ
   Memmesheimer, RM
AF Thalmeier, Dominik
   Uhlmann, Marvin
   Kappen, Hilbert J.
   Memmesheimer, Raoul-Martin
TI Learning Universal Computations with Spikes
SO PLOS COMPUTATIONAL BIOLOGY
DT Article
ID NEURAL-NETWORKS; SYNAPTIC PLASTICITY; NEURONS; MEMORY; GENERATION;
   DYNAMICS; REPRESENTATION; INHIBITION; PREDICTION; FREQUENCY
AB Providing the neurobiological basis of information processing in higher animals, spiking neural networks must be able to learn a variety of complicated computations, including the generation of appropriate, possibly delayed reactions to inputs and the self-sustained generation of complex activity patterns, e.g. for locomotion. Many such computations require previous building of intrinsic world models. Here we show how spiking neural networks may solve these different tasks. Firstly, we derive constraints under which classes of spiking neural networks lend themselves to substrates of powerful general purpose computing. The networks contain dendritic or synaptic nonlinearities and have a constrained connectivity. We then combine such networks with learning rules for outputs or recurrent connections. We show that this allows to learn even difficult benchmark tasks such as the self-sustained generation of desired low-dimensional chaotic dynamics or memory-dependent computations. Furthermore, we show how spiking networks can build models of external world systems and use the acquired knowledge to control them.
C1 [Thalmeier, Dominik; Kappen, Hilbert J.] Radboud Univ Nijmegen, Donders Inst, Dept Biophys, Nijmegen, Netherlands.
   [Uhlmann, Marvin] Max Planck Inst Psycholinguist, Dept Neurobiol Language, Nijmegen, Netherlands.
   [Uhlmann, Marvin; Memmesheimer, Raoul-Martin] Radboud Univ Nijmegen, Donders Inst, Dept Neuroinformat, Nijmegen, Netherlands.
   [Memmesheimer, Raoul-Martin] Columbia Univ, Ctr Theoret Neurosci, New York, NY USA.
RP Memmesheimer, RM (corresponding author), Radboud Univ Nijmegen, Donders Inst, Dept Neuroinformat, Nijmegen, Netherlands.; Memmesheimer, RM (corresponding author), Columbia Univ, Ctr Theoret Neurosci, New York, NY USA.
EM rm3354@cumc.columbia.edu
CR Abbott LF, 2016, NAT NEUROSCI, V19, P350, DOI 10.1038/nn.4241
   Abbott LF, 2004, NATURE, V431, P796, DOI 10.1038/nature03010
   Barak O, 2014, CURR OPIN NEUROBIOL, V25, P20, DOI 10.1016/j.conb.2013.10.008
   Bean BP, 2007, NAT REV NEUROSCI, V8, P451, DOI 10.1038/nrn2148
   Bekkers JM, 2009, CURR BIOL, V19, pR296, DOI 10.1016/j.cub.2009.02.010
   Blitz DM, 2004, NAT REV NEUROSCI, V5, P630, DOI 10.1038/nrn1475
   Boerlin M, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003258
   Boerlin M, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1001080
   Bourdoukan R, 2015, ADV NEURAL INFORM PR, V28, P982
   BOURDOUKAN R, 2012, ADV NEURAL INFORM PR, V25, P2294
   Branco T, 2011, NEURON, V69, P885, DOI 10.1016/j.neuron.2011.02.006
   Brown JT, 2009, J PHYSIOL-LONDON, V587, P1265, DOI 10.1113/jphysiol.2008.167007
   Buchli J, 2011, ROBOTICS: SCIENCE AND SYSTEMS VI, P153
   Cazé RD, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1002867
   Chen S, 2008, J PHYSIOL-LONDON, V586, P1351, DOI 10.1113/jphysiol.2007.148171
   Dayan P., 2001, THEORETICAL NEUROSCI
   Deadwyler SA, 2006, BEHAV BRAIN RES, V174, P272, DOI 10.1016/j.bbr.2006.05.038
   DePasquale B, 2016, ARXIV 1601 07620
   Eliasmith C, 2005, NEURAL COMPUT, V17, P1276, DOI 10.1162/0899766053630332
   Eliasmith C, 2003, NEURAL ENGINEERING C
   Eliasmith C, 2012, SCIENCE, V338, P1202, DOI 10.1126/science.1225266
   Fiete IR, 2007, J NEUROPHYSIOL, V98, P2038, DOI 10.1152/jn.01311.2006
   Friston K, 2011, NEURON, V72, P488, DOI 10.1016/j.neuron.2011.10.018
   Goldman MS, 2009, NEURON, V61, P621, DOI 10.1016/j.neuron.2008.12.012
   Grillner S, 2006, NEURON, V52, P751, DOI 10.1016/j.neuron.2006.11.008
   Harish O, 2015, PLOS COMPUT BIOL, V11, DOI 10.1371/journal.pcbi.1004266
   Haykin S, 2002, ADAPTIVE FILTER THEO
   Hennequin G, 2014, NEURON, V82, P1394, DOI 10.1016/j.neuron.2014.04.045
   HIRSCH MW, 1989, NEURAL NETWORKS, V2, P331, DOI 10.1016/0893-6080(89)90018-X
   HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554
   Huang GB, 2006, NEUROCOMPUTING, V70, P489, DOI 10.1016/j.neucom.2005.12.126
   Ivry RB, 2004, CURR OPIN NEUROBIOL, V14, P225, DOI 10.1016/j.conb.2004.03.013
   Jaeger H, 2004, SCIENCE, V304, P78, DOI 10.1126/science.1091277
   Jaeger H, 2001, 14834 GMD GERM NAT R, V148, P34
   Jahnke S, 2009, FRONT COMPUT NEUROSC, V3, DOI 10.3389/neuro.10.013.2009
   Joshi P, 2005, NEURAL COMPUT, V17, P1715, DOI 10.1162/0899766054026684
   Kadmon J, 2015, PHYS REV X, V5, DOI 10.1103/PhysRevX.5.041030
   Kappen HJ, 2005, PHYS REV LETT, V95, DOI 10.1103/PhysRevLett.200201
   Klampfl S, 2013, J NEUROSCI, V33, P11515, DOI 10.1523/JNEUROSCI.5044-12.2013
   Lansink CS, 2008, J NEUROSCI, V28, P6372, DOI 10.1523/JNEUROSCI.1054-08.2008
   Lansink CS, 2009, PLOS BIOL, V7, DOI 10.1371/journal.pbio.1000173
   Lazar A, 2009, FRONT COMPUT NEUROSC, V3, DOI 10.3389/neuro.10.023.2009
   Legenstein R, 2007, NEURAL NETWORKS, V20, P323, DOI 10.1016/j.neunet.2007.04.017
   Li LX, 2014, P NATL ACAD SCI USA, V111, P8392, DOI 10.1073/pnas.1407083111
   Litwin-Kumar A, 2014, NAT COMMUN, V5, DOI 10.1038/ncomms6319
   London M, 2005, ANNU REV NEUROSCI, V28, P503, DOI 10.1146/annurev.neuro.28.061604.135703
   Losonczy A, 2008, NATURE, V452, P436, DOI 10.1038/nature06725
   Lubke J, 1996, J NEUROSCI, V16, P3209
   Lukosevicius M, 2012, KUNSTL INTELL, V26, P365, DOI 10.1007/s13218-012-0204-5
   Lukosevicius M, 2009, COMPUT SCI REV, V3, P127, DOI 10.1016/j.cosrev.2009.03.005
   Luscher C, 1997, NEURON, V19, P687, DOI 10.1016/S0896-6273(00)80381-5
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Maass W, 2010, LIQUID STATE MACHINE, V189
   Maass W, 2007, PLOS COMPUT BIOL, V3, P15, DOI 10.1371/journal.pcbi.0020165
   Matell MS, 2003, BEHAV NEUROSCI, V117, P760, DOI 10.1037/0735-7044.117.4.760
   Mayor J, 2005, PHYS REV E, V72, DOI 10.1103/PhysRevE.72.051906
   Memmesheimer RM, 2010, P NATL ACAD SCI USA, V107, P11092, DOI 10.1073/pnas.0909615107
   Ostojic S, 2014, NAT NEUROSCI, V17, P594, DOI 10.1038/nn.3658
   Pastor P., 2011, 2011 IEEE International Conference on Robotics and Automation (ICRA 2011), P3828, DOI 10.1109/ICRA.2011.5980200
   Petrides T, 2007, EXP BRAIN RES, V177, P370, DOI 10.1007/s00221-006-0681-6
   Pfeiffer BE, 2013, NATURE, V497, P74, DOI 10.1038/nature12112
   Quiroga RQ, 2005, NATURE, V435, P1102, DOI 10.1038/nature03687
   Raman IM, 1997, J NEUROSCI, V17, P4517, DOI 10.1523/jneurosci.17-12-04517.1997
   Sakai K, 2003, NAT NEUROSCI, V6, P75, DOI 10.1038/nn987
   Sceniak MP, 2008, BMC NEUROSCI, V9, DOI 10.1186/1471-2202-9-8
   Schwemmer MA, 2015, J NEUROSCI, V35, P10112, DOI 10.1523/JNEUROSCI.4951-14.2015
   Seung HS, 2000, J COMPUT NEUROSCI, V9, P171, DOI 10.1023/A:1008971908649
   SOMPOLINSKY H, 1988, PHYS REV LETT, V61, P259, DOI 10.1103/PhysRevLett.61.259
   STORM JF, 1987, J PHYSIOL-LONDON, V385, P733, DOI 10.1113/jphysiol.1987.sp016517
   Sussillo D, 2013, NEURAL COMPUT, V25, P626, DOI 10.1162/NECO_a_00409
   Sussillo D, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0037372
   Sussillo D, 2009, NEURON, V63, P544, DOI 10.1016/j.neuron.2009.07.018
   Theodorou E, 2010, IEEE INT CONF ROBOT, P2397, DOI 10.1109/ROBOT.2010.5509336
   Timme M, 2004, PHYS REV LETT, V92, DOI 10.1103/PhysRevLett.92.074101
   Todorov E, 2009, P NATL ACAD SCI USA, V106, P11478, DOI 10.1073/pnas.0710743106
   van der Meer MAA, 2010, FRONT NEUROSCI-SWITZ, V4, DOI 10.3389/neuro.01.006.2010
   van der Meer MAA, 2009, FRONT INTEGR NEUROSC, V3, DOI 10.3389/neuro.07.001.2009
   VONHOFSTEN C, 1982, DEV PSYCHOL, V18, P450, DOI 10.1037/0012-1649.18.3.450
   Wallace E, 2013, NEURAL COMPUT, V25, P1408, DOI 10.1162/NECO_a_00449
   Wang HX, 2008, P NATL ACAD SCI USA, V105, P16791, DOI 10.1073/pnas.0804318105
   White OL, 2004, PHYS REV LETT, V92, DOI 10.1103/PhysRevLett.92.148102
   Yildiz IB, 2012, NEURAL NETWORKS, V35, P1, DOI 10.1016/j.neunet.2012.07.005
   Zenke F, 2015, NAT COMMUN, V6, DOI 10.1038/ncomms7922
NR 83
TC 44
Z9 44
U1 0
U2 20
PD JUN
PY 2016
VL 12
IS 6
AR e1004895
DI 10.1371/journal.pcbi.1004895
UT WOS:000379349700014
DA 2023-11-16
ER

PT J
AU Sommer, J
   Ozkan, MA
   Keszocze, O
   Teich, J
AF Sommer, Jan
   Ozkan, M. Akif
   Keszocze, Oliver
   Teich, Juergen
TI Efficient Hardware Acceleration of Sparsely Active Convolutional Spiking
   Neural Networks
SO IEEE TRANSACTIONS ON COMPUTER-AIDED DESIGN OF INTEGRATED CIRCUITS AND
   SYSTEMS
DT Article; Proceedings Paper
CT ACM/IEEE Int Conf on Hardware/Software Codesign and Syst Synthesis / Int
   Conf on Compilers, Architectures, and Synthesis for Embedded Syst / Int
   Conf on Embedded Software part of the Embedded Syst Week
CY OCT 08-15, 2021
CL ELECTR NETWORK
DE Event-based processing; field-programmable gate array (FPGA); hardware
   acceleration; spiking convolutional neural networks (SNNs)
AB Spiking neural networks (SNNs) compute in an event-based manner to achieve a more efficient computation than standard neural networks. In SNNs, neuronal outputs are not encoded as real-valued activations but as sequences of binary spikes. The motivation of using SNNs over conventional neural networks is rooted in the special computational aspects of spike-based processing, especially the high degree of sparsity of spikes. Well-established implementations of convolutional neural networks (CNNs) feature large spatial arrays of processing elements (PEs) that remain highly underutilized in the face of activation sparsity. We propose a novel architecture optimized for the processing of convolutional SNNs (CSNNs) featuring a high degree of sparsity. The proposed architecture consists of an array of PEs of the size of the kernel of a convolution and an intelligent spike queue that provides a high PE utilization. A constant flow of spikes is ensured by compressing the feature maps into queues that can then be processed spike-by-spike. This compression is performed at run-time, leading to a self-timed schedule. This allows the processing time to scale with the number of spikes. Also, a novel memory organization scheme is introduced to efficiently store and retrieve the membrane potentials of the individual neurons using multiple small parallel on-chip RAMs. Each RAM is hardwired to its PE, reducing switching circuitry. We implemented the proposed architecture on an FPGA and achieved a significant speedup compared to previously proposed SNN implementations (similar to 10 times) while needing less hardware resources and maintaining a higher energy efficiency (similar to 15 times).
C1 [Sommer, Jan; Ozkan, M. Akif] Friedrich Alexander Univ Erlangen Nurnberg, Chair Hardware Software Codesign, D-91058 Erlangen, Germany.
   [Keszocze, Oliver; Teich, Juergen] Max Planck Inst Sci Light, D-91058 Erlangen, Germany.
RP Keszocze, O (corresponding author), Max Planck Inst Sci Light, D-91058 Erlangen, Germany.
EM jan.sommer@fau.de; akif.oezkan@fau.de; oliver.keszoecze@fau.de;
   juergen.teich@fau.de
CR Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   Bing Han, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P388, DOI 10.1007/978-3-030-58607-2_23
   Bouvier M, 2019, ACM J EMERG TECH COM, V15, DOI 10.1145/3304103
   Chen YH, 2019, IEEE J EM SEL TOP C, V9, P292, DOI 10.1109/JETCAS.2019.2910232
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Fang HW, 2020, ICCAD-IEEE ACM INT, DOI 10.1145/3400302.3415608
   Howard AG, 2017, Arxiv, DOI arXiv:1704.04861
   Guo SS, 2019, PR GR LAK SYMP VLSI, P63, DOI 10.1145/3299874.3317966
   Guo WZ, 2021, FRONT NEUROSCI-SWITZ, V15, DOI [10.3389/fnins.2021.638474, 10.1007/s11704-020-9230-x]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Horowitz M, 2014, ISSCC DIG TECH PAP I, V57, P10, DOI 10.1109/ISSCC.2014.6757323
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Jacob B, 2017, Arxiv, DOI arXiv:1712.05877
   Kang Z., 2020, ACM J EMERG TECH COM, V16, P1
   Khodamoradi Alireza, 2021, FPGA '21: The 2021 ACM/SIGDA International Symposium on Field-Programmable, P194, DOI 10.1145/3431920.3439283
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lee C, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00119
   Mostafa H., 2017, 2017 IEEE INT S CIRC, P1, DOI [10.1109/ISCAS.2017.8050527, DOI 10.1109/ISCAS.2017.8050527]
   Narayanan S, 2020, ANN I S COM, P349, DOI 10.1109/ISCA45697.2020.00038
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Panzeri S, 2010, TRENDS NEUROSCI, V33, P111, DOI 10.1016/j.tins.2009.12.001
   Parashar A, 2017, 44TH ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA 2017), P27, DOI 10.1145/3079856.3080254
   Rueckauer B, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351295
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Shin JH, 2022, INT S HIGH PERF COMP, P861, DOI 10.1109/HPCA53966.2022.00068
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sze V, 2017, IEEE CUST INTEGR CIR
   Sze V, 2017, P IEEE, V105, P2295, DOI 10.1109/JPROC.2017.2761740
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tapiador-Morales R, 2019, IEEE T BIOMED CIRC S, V13, P159, DOI 10.1109/TBCAS.2018.2880012
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   Wang SQ, 2020, J COMPUT SCI TECH-CH, V35, P475, DOI 10.1007/s11390-020-9686-z
   Wu JB, 2023, IEEE T NEUR NET LEAR, V34, P446, DOI 10.1109/TNNLS.2021.3095724
   Yousefzadeh A, 2019, IEEE J EM SEL TOP C, V9, P668, DOI 10.1109/JETCAS.2019.2951121
   Zhou XD, 2018, 2018 51ST ANNUAL IEEE/ACM INTERNATIONAL SYMPOSIUM ON MICROARCHITECTURE (MICRO), P15, DOI 10.1109/MICRO.2018.00011
NR 38
TC 2
Z9 2
U1 2
U2 7
PD NOV
PY 2022
VL 41
IS 11
BP 3767
EP 3778
DI 10.1109/TCAD.2022.3197512
UT WOS:000877295000021
DA 2023-11-16
ER

PT C
AU Benmeziane, H
   Ounnoughene, AZ
   Hamzaoui, I
   Bouhadjar, Y
AF Benmeziane, Hadjer
   Ounnoughene, Amine Ziad
   Hamzaoui, Imane
   Bouhadjar, Younes
GP IEEE
TI Skip Connections in Spiking Neural Networks: An Analysis of Their Effect
   on Network Training
SO 2023 IEEE INTERNATIONAL PARALLEL AND DISTRIBUTED PROCESSING SYMPOSIUM
   WORKSHOPS, IPDPSW
SE IEEE International Symposium on Parallel and Distributed Processing
   Workshops
DT Proceedings Paper
CT 37th IEEE International Parallel and Distributed Processing Symposium
   (IPDPS)
CY MAY 15-19, 2023
CL St Petersburg, FL
DE Spiking Neural Network; efficient deep learning; neural architecture
   search
AB Spiking neural networks (SNNs) have gained attention as a promising alternative to traditional artificial neural networks (ANNs) due to their potential for energy efficiency and their ability to model spiking behavior in biological systems. However, the training of SNNs is still a challenging problem, and new techniques are needed to improve their performance. In this paper, we study the impact of skip connections on SNNs and propose a hyperparameter optimization technique that adapts models from ANN to SNN. We demonstrate that optimizing the position, type, and number of skip connections can significantly improve the accuracy and efficiency of SNNs by enabling faster convergence and increasing information flow through the network. Our results show an average +8% accuracy increase on CIFAR-10-DVS and DVS128 Gesture datasets adaptation of multiple state-of-the-art models.
C1 [Benmeziane, Hadjer] Univ Polytech Hauts de France, Valenciennes, France.
   [Ounnoughene, Amine Ziad] Belmihoub Abd El Rahmane High Sch, Bordj Bou Arreidj, Algeria.
   [Hamzaoui, Imane] Ecole Natl Super Informat, Sch AI Algiers, Algiers, Algeria.
   [Bouhadjar, Younes] Forschungszentrum Julich, Peter Grunberg Inst PGI 7 15, Julich, Germany.
RP Benmeziane, H (corresponding author), Univ Polytech Hauts de France, Valenciennes, France.
EM hadjer.benmeziane@uphf.fr; amine.ziad.ounnoughene@gmail.com;
   ji_hamzaoui@csi.dz; y.bouhadjar@fz-juelich.de
CR Amir A, 2017, PROC CVPR IEEE, P7388, DOI 10.1109/CVPR.2017.781
   Auer P., 2003, Journal of Machine Learning Research, V3, P397, DOI 10.1162/153244303321897663
   Eshraghian J. K., 2021, TRAINING SPIKING NEU
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Kaiser J, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00424
   Kim Y, 2022, LECT NOTES COMPUT SC, V13684, P36, DOI 10.1007/978-3-031-20053-3_3
   Kim Y, 2021, FRONT NEUROSCI-SWITZ, V15, DOI 10.3389/fnins.2021.773954
   Krizhevsky A., 2009, REP T 2009
   Li HM, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00309
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Sandler M., ARXIV
   Song JL, 2019, PR MACH LEARN RES, V89
   Wunderlich TC, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-91786-z
   Zenke F, 2021, NEURAL COMPUT, V33, P899, DOI 10.1162/neco_a_01367
   Zenke F, 2018, NEURAL COMPUT, V30, P1514, DOI 10.1162/neco_a_01086
NR 16
TC 0
Z9 0
U1 0
U2 0
PY 2023
BP 790
EP 794
DI 10.1109/IPDPSW59300.2023.00132
UT WOS:001055030700095
DA 2023-11-16
ER

PT C
AU Wright, PW
   Wiles, J
AF Wright, Paul W.
   Wiles, Janet
GP IEEE
TI Learning Transmission Delays in Spiking Neural Networks: A Novel
   Approach to Sequence Learning Based on Spike Delay Variance
SO 2012 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT IEEE International Conference on Fuzzy Systems (FUZZ-IEEE) /
   International Joint Conference on Neural Networks (IJCNN) / IEEE
   Congress on Evolutionary Computation (IEEE-CEC) / IEEE World Congress on
   Computational Intelligence (IEEE-WCCI)
CY JUN 10-15, 2012
CL Brisbane, AUSTRALIA
DE spiking neural networks; transmission delays; delay learning; sequence
   learning; spike-delay-variance learning; STDP
ID SYNAPTIC-TRANSMISSION; BRAIN-STEM; NEURONS; MODEL; COMPUTATION; TIME;
   POLYCHRONIZATION; RECOGNITION; CONDUCTION; DYNAMICS
AB Transmission delays are an inherent component of spiking neural networks (SNNs) but relatively little is known about how delays are adapted in biological systems and studies on computational learning mechanisms have focused on spike-timing-dependent plasticity (STDP) which adjusts synaptic weights rather than synaptic delays. We propose a novel algorithm for learning temporal delays in SNNs with Gaussian synapses, which we call spike-delay-variance learning (SDVL). A key feature of the algorithm is adaptation of the shape (mean and variance) of the postsynaptic release profiles only, rather than the conventional STDP approach of adapting the network's synaptic weights. The algorithm's ability to learn temporal input sequences was tested in three studies using supervised and unsupervised learning within feed-forward networks. SDVL was able to successfully classify forty spatiotemporal patterns without supervision by providing robust, effective adaption of the postsynaptic release profiles. The results demonstrate how delay learning can contribute to the stability of spiking sequences, and that there is a potential role for adaption of variance as well as mean values in learning algorithms for spiking neural networks.
C1 [Wright, Paul W.; Wiles, Janet] Univ Queensland, Sch Informat Technol & Elect Engn, Brisbane, Qld 4072, Australia.
RP Wright, PW (corresponding author), Univ Queensland, Sch Informat Technol & Elect Engn, Brisbane, Qld 4072, Australia.
EM paul.wright1@uqconnect.edu.au; wiles@itee.uq.edu.au
CR Abbott LF, 2004, NATURE, V431, P796, DOI 10.1038/nature03010
   Abbott LF, 1999, BRAIN RES BULL, V50, P303, DOI 10.1016/S0361-9230(99)00161-6
   Arieli A, 1996, SCIENCE, V273, P1868, DOI 10.1126/science.273.5283.1868
   Barak O, 2006, NEURAL COMPUT, V18, P2343, DOI 10.1162/neco.2006.18.10.2343
   Bennett MVL, 2004, NEURON, V41, P495, DOI 10.1016/S0896-6273(04)00043-1
   Brette R, 2007, J COMPUT NEUROSCI, V23, P349, DOI 10.1007/s10827-007-0038-6
   Caporale N, 2008, ANNU REV NEUROSCI, V31, P25, DOI 10.1146/annurev.neuro.31.060407.125639
   CARNEY T, 1989, VISION RES, V29, P155, DOI 10.1016/0042-6989(89)90121-1
   Carr Catherine E., 1996, Advances in Psychology, V115, P27
   CARR CE, 1990, J NEUROSCI, V10, P3227
   Clopath C, 2010, NAT NEUROSCI, V13, P344, DOI 10.1038/nn.2479
   Haag J, 2004, P NATL ACAD SCI USA, V101, P16333, DOI 10.1073/pnas.0407368101
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Huning H, 1998, NEURAL COMPUT, V10, P555, DOI 10.1162/089976698300017665
   ILES JF, 1977, PROC R SOC SER B-BIO, V197, P225, DOI 10.1098/rspb.1977.0066
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Izhikevich EM, 2010, PHILOS T R SOC A, V368, P5061, DOI 10.1098/rsta.2010.0130
   KANDLER K, 1995, J NEUROSCI, V15, P6890
   Kerr JND, 2008, NAT REV NEUROSCI, V9, P195, DOI 10.1038/nrn2338
   KONISHI M, 1973, AM SCI, V61, P414
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Maass W, 2001, THEOR COMPUT SCI, V261, P157, DOI 10.1016/S0304-3975(00)00137-7
   Paugam-Moisy H, 2008, NEUROCOMPUTING, V71, P1143, DOI 10.1016/j.neucom.2007.12.027
   Pressley J, 2011, NEURAL COMPUT, V23, P1234, DOI 10.1162/NECO_a_00114
   REICHARDT W, 1961, AUTOCORRELATION PRIN
   Senn W, 2002, NEURAL COMPUT, V14, P583, DOI 10.1162/089976602317250915
   Silberberg G, 2004, J NEUROPHYSIOL, V91, P704, DOI 10.1152/jn.00415.2003
   Steuber V, 2004, J COMPUT NEUROSCI, V17, P149, DOI 10.1023/B:JCNS.0000037678.26155.b5
   SWADLOW HA, 1994, J NEUROPHYSIOL, V71, P437, DOI 10.1152/jn.1994.71.2.437
   Tolnai S, 2009, J NEUROPHYSIOL, V102, P1206, DOI 10.1152/jn.00275.2009
   WAXMAN SG, 1980, MUSCLE NERVE, V3, P141, DOI 10.1002/mus.880030207
   Wright P., 2011, THESIS U QUEENSLAND
NR 34
TC 4
Z9 4
U1 0
U2 1
PY 2012
UT WOS:000309341300009
DA 2023-11-16
ER

PT C
AU de Ladurantaye, V
   Lavoie, J
   Bergeron, J
   Parenteau, M
   Lu, HZ
   Pichevar, R
   Rouat, J
AF de ladurantaye, Vincent
   Lavoie, Jean
   Bergeron, Jocelyn
   Parenteau, Maxime
   Lu, Huizhong
   Pichevar, Ramin
   Rouat, Jean
GP IOP
TI A Parallel Supercomputer Implementation of a Biological Inspired Neural
   Network and its use for Pattern Recognition
SO HIGH PERFORMANCE COMPUTING SYMPOSIUM 2011
SE Journal of Physics Conference Series
DT Proceedings Paper
CT High Performance Computing Symposium (HPCS)
CY JUN 15-17, 2011
CL Univ Quebec, Montreal, CANADA
HO Univ Quebec
ID MODEL
AB A parallel implementation of a large spiking neural network is proposed and evaluated. The neural network implements the binding by synchrony process using the Oscillatory Dynamic Link Matcher (ODLM). Scalability, speed and performance are compared for 2 implementations: Message Passing Interface (MPI) and Compute Unified Device Architecture (CUDA) running on clusters of multicore supercomputers and NVIDIA graphical processing units respectively. A global spiking list that represents at each instant the state of the neural network is described. This list indexes each neuron that fires during the current simulation time so that the influence of their spikes are simultaneously processed on all computing units. Our implementation shows a good scalability for very large networks. A complex and large spiking neural network has been implemented in parallel with success, thus paving the road towards real-life applications based on networks of spiking neurons. MPI offers a better scalability than CUDA, while the CUDA implementation on a GeForce GTX 285 gives the best cost to performance ratio. When running the neural network on the GTX 285, the processing speed is comparable to the MPI implementation on RQCHP's Mammouth parallel with 64 notes (128 cores).
C1 [de ladurantaye, Vincent; Lavoie, Jean; Bergeron, Jocelyn; Parenteau, Maxime; Lu, Huizhong; Pichevar, Ramin; Rouat, Jean] Univ Sherbrooke Que, NECOTIS, GEGI, Sherbrooke, PQ, Canada.
RP de Ladurantaye, V (corresponding author), Univ Sherbrooke Que, NECOTIS, GEGI, Sherbrooke, PQ, Canada.
CR Brette R, 2007, J COMPUT NEUROSCI, V23, P349, DOI 10.1007/s10827-007-0038-6
   Fidjeland A, 2010, IJCNN 2010
   GERSTNER W, 1998, PULSED NEURAL NETWOR
   Han B, 2010, IJCNN 2010
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   MILNER PM, 1974, PSYCHOL REV, V81, P521, DOI 10.1037/h0037149
   Molotchnikoff S, 2011, FRONTIERS BIOSCIENCE
   Nageswaran Jayram Moorkanikara, 2009, 2009 IEEE International Symposium on Circuits and Systems - ISCAS 2009, P1917, DOI 10.1109/ISCAS.2009.5118157
   Nageswaran JM, 2009, IEEE IJCNN, P3201
   Pallipuran V K, 2011, J SUPERCOMPUT, P1
   Pichevar R, 2006, NEUROCOMPUTING, V69, P1837, DOI 10.1016/j.neucom.2005.11.011
   Plesser HE, 2007, LECT NOTES COMPUT SC, V4641, P672
   Yudanov D, 2010, IJCNN 2010
NR 14
TC 3
Z9 3
U1 0
U2 1
PY 2012
VL 341
AR 012024
DI 10.1088/1742-6596/341/1/012024
UT WOS:000305907300024
DA 2023-11-16
ER

PT J
AU Afifi, A
   Ayatollahi, A
   Raissi, F
   Hajghassem, H
AF Afifi, Ahmad
   Ayatollahi, Ahmad
   Raissi, Farshid
   Hajghassem, Hasan
TI Efficient Hybrid CMOS-Nano Circuit Design for Spiking Neurons and
   Memristive Synapses with STDP
SO IEICE TRANSACTIONS ON FUNDAMENTALS OF ELECTRONICS COMMUNICATIONS AND
   COMPUTER SCIENCES
DT Article
DE spiking neural network; STDP; memristor; CMOS-Nano hybrid
ID NEURAL-NETWORK
AB This paper introduces a new hybrid CMOS-Nano circuit for efficient implementation of spiking neurons and spike-timing dependent plasticity (STDP) rule. In our spiking neural architecture, the STDP rule has been implemented by using neuron circuits which generate two-part spikes and send them in both forward and backward directions along their axons and dendrites, simultaneously. The two-part spikes form STDP windows and also they carry temporal information relating to neuronal activities. However, to reduce power consumption, we take the circuitry of two-part spike generation out of the neuron circuit and use the regular shaped pulses, after the training has been performed. Furthermore, the performance of the rule as spike-timing correlation learning and character recognition in a two layer winner-take-all (WTA) network of integrate-and-fire neurons and memristive synapses is demonstrated as a case example.
C1 [Afifi, Ahmad; Ayatollahi, Ahmad] Iran Univ Sci & Technol, EE Dept, Tehran, Iran.
   [Raissi, Farshid] KN Toosi Univ Technol, ECE Dept, Tehran, Iran.
   [Hajghassem, Hasan] S Beheshti Univ, EE Dept, Tehran, Iran.
RP Afifi, A (corresponding author), Iran Univ Sci & Technol, EE Dept, Tehran, Iran.
EM ayatollahi@iust.ac.ir
CR AFIFI A, 2009, 19 EUR C CIRC THEOR
   Afifi A, 2011, INT J CIRC THEOR APP, V39, P357, DOI 10.1002/cta.638
   Afifi A, 2009, IEICE ELECTRON EXPR, V6, P148, DOI 10.1587/elex.6.148
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Delorme A, 2001, NEURAL NETWORKS, V14, P795, DOI 10.1016/S0893-6080(01)00049-1
   Gao CJ, 2007, IEEE T CIRCUITS-I, V54, P2502, DOI 10.1109/TCSI.2007.907830
   GUPTA A, 2007, P INT JOINT C NEUR N
   Indiveri G, 2006, IEEE T NEURAL NETWOR, V17, P211, DOI 10.1109/TNN.2005.860850
   Likharev KK, 2008, J NANOELECTRON OPTOE, V3, P203, DOI 10.1166/jno.2008.301
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Masoumi M, 2006, NANOTECHNOLOGY, V17, P89, DOI 10.1088/0957-4484/17/1/015
   Oya T, 2005, IEICE ELECTRON EXPR, V2, P76, DOI 10.1587/elex.2.76
   Sasaki K, 2006, IEICE T ELECTRON, VE89C, P1637, DOI 10.1093/ietele/e89-c.11.1637
   Saudargiene A, 2004, NEURAL COMPUT, V16, P595, DOI 10.1162/089976604772744929
   Snider GS, 2007, NANOTECHNOLOGY, V18, DOI 10.1088/0957-4484/18/36/365202
   Snider GS, 2008, 2008 IEEE INTERNATIONAL SYMPOSIUM ON NANOSCALE ARCHITECTURES, P85, DOI 10.1109/NANOARCH.2008.4585796
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Strukov DB, 2005, NANOTECHNOLOGY, V16, P888, DOI 10.1088/0957-4484/16/6/045
   Strukov DB, 2008, NATURE, V453, P80, DOI 10.1038/nature06932
   STUART GJ, 1994, NATURE, V367, P69, DOI 10.1038/367069a0
   Tanaka H, 2009, IEICE T FUND ELECTR, VE92A, P1690, DOI 10.1587/transfun.E92.A.1690
   TUREL O, 2004, INT J CIRCUIT THEORY
   Van Rullen R, 1998, BIOSYSTEMS, V48, P229, DOI 10.1016/S0303-2647(98)00070-7
   VOGELSTEIN RJ, 2003, P NIPS 03, V15
   Wijekoon JHB, 2008, NEURAL NETWORKS, V21, P524, DOI 10.1016/j.neunet.2007.12.037
NR 25
TC 12
Z9 13
U1 2
U2 33
PD SEP
PY 2010
VL E93A
IS 9
BP 1670
EP 1677
DI 10.1587/transfun.E93.A.1670
UT WOS:000282245600010
DA 2023-11-16
ER

PT J
AU Szczesny, S
   Huderek, D
   Przyborowski, L
AF Szczesny, Szymon
   Huderek, Damian
   Przyborowski, Lukasz
TI Explainable spiking neural network for real time feature classification
SO JOURNAL OF EXPERIMENTAL & THEORETICAL ARTIFICIAL INTELLIGENCE
DT Article
DE XAI; spiking neural network; cusp catastrophe; amperometry; feature
   classification
AB The work presents a concept of an implementation of an explainable artificial intelligence (XAI) using effective models of third-generation neurons. The article discusses a concept of building a neural network based on spiking neurons modelled on ladder nervous systems. A distinction is made between voltage signals encoding information in a network and current signals which contain the correlation between information in the network and pattern features. Analyzes feature a neuron model based on the cusp catastrophe theory eliminating network sensitivity to problems of synapse plasticity, weight mismatch and coupling of neurons based on electric models. The paper presents applications of a spiking neural network for reporting the state of water quality while generating justifications. The article contains results of an analysis of confusion of justifications with ACC = 1 for a set of 10,000 patterns. It also discusses the speed of pattern analysis in the simulated network.
C1 [Szczesny, Szymon; Huderek, Damian; Przyborowski, Lukasz] Poznan Univ Tech, Fac Comp & Telecommun, Piotrowo 2 St, PL-60965 Poznan, Poland.
RP Szczesny, S (corresponding author), Poznan Univ Tech, Fac Comp & Telecommun, Piotrowo 2 St, PL-60965 Poznan, Poland.
EM Szymon.Szczesny@put.poznan.pl
CR Abusnaina Ahmed A., 2014, International Journal of Digital Content Technology and its Applications, V8, P14
   Adadi A, 2018, IEEE ACCESS, V6, P52138, DOI 10.1109/ACCESS.2018.2870052
   Aha D.W., 2017, P WORKSH EXPL AI XAI, P1
   Alonso J.M., 2018, ADV EXPLAINABLE ARTI
   Banks C., 2015, ELECTROCHEMISTRY
   Biundo S., 2018, P ICAPS WORKSH EXPL
   Caruana R, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1721, DOI 10.1145/2783258.2788613
   Chander A, 2018, P MAKE EXPL AI SPRIN
   Chen DG, 2016, PROCEEDINGS OF 3RD IEEE/ACM INTERNATIONAL CONFERENCE ON DATA SCIENCE AND ADVANCED ANALYTICS, (DSAA 2016), P100, DOI 10.1109/DSAA.2016.17
   Cheng H.-P., 2017, DESIGN AUTOMATION TE
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   de Graaf MMA, 2018, ACMIEEE INT CONF HUM, P387, DOI 10.1145/3173386.3173568
   Deweerdt S, 2019, NATURE, V571, pS6, DOI 10.1038/d41586-019-02208-0
   European Space Agency, 2020, ROB CAR SOLV MAZ US
   Guyon I, 2017, P IJCNN EXPL LEARN M
   Howard A, 2017, IEEE WORK ADV ROBOT
   Huderek D, 2019, FOUND COMPUT DECIS S, V44, P273, DOI 10.2478/fcds-2019-0014
   Islam MA, 2020, IEEE T FUZZY SYST, V28, P1291, DOI 10.1109/TFUZZ.2019.2917124
   Karnick M, 2008, IEEE IJCNN, P3455, DOI 10.1109/IJCNN.2008.4634290
   Koizumi O., 2016, HIKAKU SEIRI SEIKAGA, V33, P116, DOI [10.3330/hikakuseiriseika.33.116, DOI 10.3330/HIKAKUSEIRISEIKA.33.116]
   Komatsu T., 2018, ACM INT INT IUI WORK
   Lee N, 2020, ARXIV PREPRINT ARXIV
   Li O, 2018, AAAI CONF ARTIF INTE, P3530
   Liu PW, 2014, J NEUROSCI, V34, P4991, DOI 10.1523/JNEUROSCI.1925-13.2014
   Nowak LG, 1997, CEREB CORTEX, V7, P487, DOI 10.1093/cercor/7.6.487
   Richter S, 2010, FRONT ZOOL, V7, DOI 10.1186/1742-9994-7-29
   Rowcliffe P, 2006, IEEE T NEURAL NETWOR, V17, P803, DOI 10.1109/TNN.2006.873274
   Samek W., 2017, ARXIV PREPRINT ARXIV
   Sharma S., 2019, PHYSL RESTING POTENT
   Sourikopoulos I, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00123
   Szczesny S, 2020, IEEE SENS J, V20, P5733, DOI 10.1109/JSEN.2020.2974701
   Szczesny S, 2020, J COMPUT ELECTRON, V19, P242, DOI 10.1007/s10825-019-01431-2
   Szczesny S, 2017, IEEE SENS J, V17, P5399, DOI 10.1109/JSEN.2017.2726459
   Szczesny S, 2017, CIRC SYST SIGNAL PR, V36, P2672, DOI 10.1007/s00034-016-0449-6
   Szczesny S, 2016, MICROELECTRON ENG, V165, P41, DOI [10.1016/j.mec.2016.08.010, 10.1016/j.mee.2016.08.010]
   WHITE JG, 1986, PHILOS T R SOC B, V314, P1, DOI 10.1098/rstb.1986.0056
NR 36
TC 1
Z9 1
U1 2
U2 11
PD JAN 2
PY 2023
VL 35
IS 1
BP 77
EP 92
DI 10.1080/0952813X.2021.1957024
EA AUG 2021
UT WOS:000680820000001
DA 2023-11-16
ER

PT C
AU Huang, LP
   Wu, QX
   Wang, XW
   Zhuo, ZQ
   Zhang, ZM
AF Huang, Liuping
   Wu, Qingxiang
   Wang, Xiaowei
   Zhuo, Zhiqiang
   Zhang, Zhenmin
BE Yuan, Z
   Wang, L
   Xu, W
   Yu, K
TI Circle Detection Using a Spiking Neural Network
SO 2013 6TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING (CISP),
   VOLS 1-3
DT Proceedings Paper
CT 6th International Congress on Image and Signal Processing (CISP)
CY DEC 16-18, 2013
CL Hangzhou, PEOPLES R CHINA
DE spiking neural networks; circle detection; hough transform; receptive
   field
AB The receptive field of neurons plays various roles in biological neural networks. In this paper a spiking neural network model is proposed using a mechanism inspired by the biological receptive field. The network is composed of multiple layers, and the neurons are connected by excitatory and inhibitory synapses. When a visual image presents to the network, location and radius of a circle on the visual image can be obtained from firing rates of the neurons from the corresponding layers. The simulation results show that the network can perform circle detection similar to Hough circle detection and calculations are conducted by a parallel mechanism in a biological manner. This model can be used to explain how a spiking neuron-based network to detect circle, and the high speed parallel mechanism in the model can be used in artificial intelligent systems.
C1 [Huang, Liuping; Wu, Qingxiang; Wang, Xiaowei; Zhuo, Zhiqiang; Zhang, Zhenmin] Fujian Normal Univ, Coll Optoelect & Informat Engn, Fuzhou, Peoples R China.
RP Wu, QX (corresponding author), Fujian Normal Univ, Coll Optoelect & Informat Engn, Fuzhou, Peoples R China.
CR Engert F, 2002, NATURE, V419, P470, DOI 10.1038/nature00988
   Fritz J, 2003, NAT NEUROSCI, V6, P1216, DOI 10.1038/nn1141
   Froemke RC, 2007, NATURE, V450, P425, DOI 10.1038/nature06289
   Gerstner W., 2002, SPIKING NEURON MODEL
   Hough P.V., 1962, Methods and means for recognizing complex patterns, Patent No. 2069654
   Jessell T. M, 1981, PRINCIPLES NEURAL SC
   Kay KN, 2008, NATURE, V452, P352, DOI 10.1038/nature06713
   Kilgard MP, 1998, SCIENCE, V279, P1714, DOI 10.1126/science.279.5357.1714
   Martinez LM, 2005, NAT NEUROSCI, V8, P372, DOI 10.1038/nn1404
   Petrou M, 2008, ARTECH HSE BIOINF BI, P1
   Wörgötter F, 1998, NATURE, V396, P165, DOI 10.1038/24157
   Wu Q. X., 2007, 7 INT WORKSH INF PRO
   Wu QX, 2007, PROCEEDINGS OF 2007 INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND CYBERNETICS, VOLS 1-7, P1974
   Wu QX, 2008, LECT NOTES COMPUT SC, V5227, P76
   Wu QX, 2007, LECT NOTES ARTIF INT, V4682, P26
   Wu QX, 2007, STUD COMPUT INTELL, V35, P171
   Wu QX, 2013, NEUROCOMPUTING, V116, P3, DOI 10.1016/j.neucom.2012.01.046
   Wu QX, 2008, NEURAL NETWORKS, V21, P1318, DOI 10.1016/j.neunet.2008.05.014
NR 18
TC 1
Z9 1
U1 0
U2 3
PY 2013
BP 1442
EP 1446
UT WOS:000341115000270
DA 2023-11-16
ER

PT J
AU Pabian, M
   Rzepka, D
   Bibrzycki, L
   Pawlak, M
AF Pabian, Mateusz
   Rzepka, Dominik
   Bibrzycki, Lukasz
   Pawlak, Miroslaw
TI Differentiating signal from artefacts in cosmic ray detection: Applying
   Siamese spiking neural networks to CREDO experimental data
SO MEASUREMENT
DT Article
DE Spiking neural networks; Siamese neural networks; Event-based computing;
   Sparse coding; Cosmic rays; CREDO
ID CLASSIFICATION; DISTANCE
AB The Cosmic Ray Extremely Distributed Observatory (CREDO) is an international research consortium aimed at observing high energy cosmic ray particles. The associated Android/iOS application enables the registration of muons with smartphone devices. The ubiquity of the CREDO infrastructure entails virtually no control over the detectors' working conditions. In order to tag artefacts appearing in the CREDO database, we propose a Siamese spiking neural network (SNN) model, trained by optimizing Earth Mover's Distance (EMD) between spike train outputs of the SNN. We first test the feasibility of our approach by training models on MNIST images converted into the spiking domain with novel conversion schemes. Then, on a binary classification problem of signal/artefact discrimination on CREDO images our model has achieved a class-balanced accuracy of 96.35%, close to the existing non-spiking solutions. Notably, the model has shown adaptability to input data properties in terms of the spiking network activity sparsity and prediction latency.
C1 [Pabian, Mateusz; Rzepka, Dominik; Pawlak, Miroslaw] AGH Univ Sci & Technol, Dept Measurement & Elect, Al Mickiewicza 30, PL-30059 Krakow, Poland.
   [Bibrzycki, Lukasz] AGH Univ Sci & Technol, Fac Phys & Appl Comp Sci, Al Mickiewicza 30, PL-30059 Krakow, Poland.
RP Pabian, M (corresponding author), AGH Univ Sci & Technol, Dept Measurement & Elect, Al Mickiewicza 30, PL-30059 Krakow, Poland.
EM pabian@agh.edu.pl
CR Adrian ED, 1926, J PHYSIOL-LONDON, V61, P151, DOI 10.1113/jphysiol.1926.sp002281
   Aggarwal CC, 2001, LECT NOTES COMPUT SC, V1973, P420
   Bar O, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21227718
   Bibrzycki L, 2020, SYMMETRY-BASEL, V12, DOI 10.3390/sym12111802
   Bredin H, 2017, INT CONF ACOUST SPEE, P5430, DOI 10.1109/ICASSP.2017.7953194
   Bromley J., 1993, International Journal of Pattern Recognition and Artificial Intelligence, V7, P669, DOI 10.1142/S0218001493000339
   Cohen S., 1999, THESIS STANFORD U, P71
   Daudt RC, 2018, IEEE IMAGE PROC, P4063, DOI 10.1109/ICIP.2018.8451652
   Diehl PU, 2015, IEEE IJCNN
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Dong Y., 2023, MEASUREMENT
   Doughty H, 2018, PROC CVPR IEEE, P6057, DOI 10.1109/CVPR.2018.00634
   Dunnhofer M, 2020, MED IMAGE ANAL, V60, DOI 10.1016/j.media.2019.101631
   Gollisch T, 2008, SCIENCE, V319, P1108, DOI 10.1126/science.1149639
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Harper NS, 2004, NATURE, V430, P682, DOI 10.1038/nature02768
   Hermans A, 2017, Arxiv, DOI arXiv:1703.07737
   Jeyapoovan T, 2013, MEASUREMENT, V46, P2065, DOI 10.1016/j.measurement.2013.03.014
   Koch Gregory, 2015, P 32 INT C MACH LEAR, P8
   Kreuz T, 2015, J NEUROPHYSIOL, V113, P3432, DOI 10.1152/jn.00848.2014
   Kreuz T, 2013, J NEUROPHYSIOL, V109, P1457, DOI 10.1152/jn.00873.2012
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lin XH, 2017, NEUROCOMPUTING, V237, P59, DOI 10.1016/j.neucom.2016.08.087
   Liu XL, 2017, IEEE I CONF COMP VIS, P1040, DOI 10.1109/ICCV.2017.118
   Luo YH, 2021, LECT NOTES COMPUT SC, V12895, P182, DOI 10.1007/978-3-030-86383-8_15
   Mostafa H, 2018, IEEE T NEUR NET LEAR, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Mueller J, 2016, AAAI CONF ARTIF INTE, P2786
   Neftci E, 2014, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00272
   Olshausen BA, 2003, J COGNITIVE NEUROSCI, V15, P154, DOI 10.1162/089892903321107891
   Pabian M, 2022, INT CONF ACOUST SPEE, P4233, DOI 10.1109/ICASSP43922.2022.9746630
   Peyré G, 2019, FOUND TRENDS MACH LE, V11, P355, DOI 10.1561/2200000073
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Piekarczyk M, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21144804
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Rzepka D, 2018, IEEE ACCESS, V6, P35001, DOI 10.1109/ACCESS.2018.2839186
   Satuvuori E, 2018, J NEUROSCI METH, V299, P22, DOI 10.1016/j.jneumeth.2018.02.009
   Schreiber S, 2003, NEUROCOMPUTING, V52-4, P925, DOI 10.1016/S0925-2312(02)00838-X
   Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682
   Sihn D, 2019, FRONT COMPUT NEUROSC, V13, DOI 10.3389/fncom.2019.00082
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26, DOI DOI 10.1007/S12654-012-0173-1
   Truccolo W, 2005, J NEUROPHYSIOL, V93, P1074, DOI 10.1152/jn.00697.2004
   van Rossum MCW, 2001, NEURAL COMPUT, V13, P751, DOI 10.1162/089976601300014321
   Victor JD, 1997, NETWORK-COMP NEURAL, V8, P127, DOI 10.1088/0954-898X/8/2/003
   Xie LY, 2022, MEASUREMENT, V197, DOI 10.1016/j.measurement.2022.111312
   Xing YN, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.590164
   Xu Y, 2021, MEASUREMENT, V179, DOI 10.1016/j.measurement.2021.109506
   Zareef F, 2022, J INSTRUM, V17, DOI 10.1088/1748-0221/17/11/C11004
   Zenke F, 2018, NEURAL COMPUT, V30, P1514, DOI 10.1162/neco_a_01086
NR 49
TC 0
Z9 0
U1 6
U2 6
PD OCT
PY 2023
VL 220
AR 113273
DI 10.1016/j.measurement.2023.113273
UT WOS:001042837600001
DA 2023-11-16
ER

PT J
AU DeWolf, T
AF DeWolf, Travis
TI Spiking neural networks take control
SO SCIENCE ROBOTICS
DT Editorial Material
AB Brain-inspired neural network architecture overcomes unsolved classical control theory problem for telerobotics.
C1 [DeWolf, Travis] Appl Brain Res Inc, Toronto, ON, Canada.
RP DeWolf, T (corresponding author), Appl Brain Res Inc, Toronto, ON, Canada.
EM travis.dewolf@appliedbrainresearch.com
CR Abadía I, 2021, SCI ROBOT, V6, DOI 10.1126/scirobotics.abf2756
   DeWolf T, 2020, FRONT NEUROROBOTICS, V14, DOI 10.3389/fnbot.2020.568359
   Eliasmith C., 2003, NEURAL ENG COMPUTATI
   FUKUSHIMA K, 1988, NEURAL NETWORKS, V1, P119, DOI 10.1016/0893-6080(88)90014-7
   Fukushima K., 1980, P 5 INT C PATT REC, V1, P459
   HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837
NR 6
TC 8
Z9 9
U1 3
U2 31
PD SEP 8
PY 2021
VL 6
IS 58
AR eabk3268
DI 10.1126/scirobotics.abk3268
UT WOS:000697021700003
DA 2023-11-16
ER

PT J
AU Pagkalos, M
   Chavlis, S
   Poirazi, P
AF Pagkalos, Michalis
   Chavlis, Spyridon
   Poirazi, Panayiota
TI Introducing the Dendrify framework for incorporating dendrites to
   spiking neural networks
SO NATURE COMMUNICATIONS
DT Article
ID BASAL DENDRITES; INTEGRATIVE PROPERTIES; SYNAPTIC PLASTICITY; PYRAMIDAL
   NEURONS; ACTION-POTENTIALS; MODEL; MECHANISMS; SPIKES; DYNAMICS; AMPA
AB Computational modeling has been indispensable for understanding how subcellular neuronal features influence circuit processing. However, the role of dendritic computations in network-level operations remains largely unexplored. This is partly because existing tools do not allow the development of realistic and efficient network models that account for dendrites. Current spiking neural networks, although efficient, are usually quite simplistic, overlooking essential dendritic properties. Conversely, circuit models with morphologically detailed neuron models are computationally costly, thus impractical for large-network simulations. To bridge the gap between these two extremes and facilitate the adoption of dendritic features in spiking neural networks, we introduce Dendrify, an open-source Python package based on Brian 2. Dendrify, through simple commands, automatically generates reduced compartmental neuron models with simplified yet biologically relevant dendritic and synaptic integrative properties. Such models strike a good balance between flexibility, performance, and biological accuracy, allowing us to explore dendritic contributions to network-level functions while paving the way for developing more powerful neuromorphic systems. Biologically inspired spiking neural networks are highly promising, but remain simplified omitting relevant biological details. The authors introduce here theoretical and numerical frameworks for incorporating dendritic features in spiking neural networks to improve their flexibility and performance.
C1 [Pagkalos, Michalis; Chavlis, Spyridon; Poirazi, Panayiota] Fdn Res & Technol Hellas Forth, Inst Mol Biol & Biotechnol IMBB, Iraklion 70013, Greece.
   [Pagkalos, Michalis] Univ Crete, Dept Biol, Iraklion 70013, Greece.
RP Poirazi, P (corresponding author), Fdn Res & Technol Hellas Forth, Inst Mol Biol & Biotechnol IMBB, Iraklion 70013, Greece.
EM poirazi@imbb.forth.gr
CR Acharya J, 2022, NEUROSCIENCE, V489, P275, DOI 10.1016/j.neuroscience.2021.10.001
   Akar Nora Abi, 2022, Zenodo, DOI 10.5281/ZENODO.7323982
   Alevi D, 2022, FRONT NEUROINFORM, V16, DOI 10.3389/fninf.2022.883700
   Amsalem O, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-019-13932-6
   Andersen P, 2009, HIPPOCAMPUS BOOK, DOI [10.1093/acprof:oso/9780195100273.001.0001, DOI 10.1093/ACPROF:OSO/9780195100273.001.0001]
   Andrásfalvy BK, 2001, J NEUROSCI, V21, P9151, DOI 10.1523/JNEUROSCI.21-23-09151.2001
   Ariav G, 2003, J NEUROSCI, V23, P7750
   Ascoli GA, 2007, J NEUROSCI, V27, P9247, DOI 10.1523/JNEUROSCI.2055-07.2007
   Bellec G, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-17236-y
   Benavides-Piccione R, 2020, CEREB CORTEX, V30, P730, DOI 10.1093/cercor/bhz122
   Bianchi D, 2012, J COMPUT NEUROSCI, V33, P207, DOI 10.1007/s10827-012-0383-y
   Bilash OM, 2022, bioRxiv, DOI [10.1101/2022.01.13.476247, 10.1101/2022.01.13.476247, DOI 10.1101/2022.01.13.476247]
   Bittner KC, 2015, NAT NEUROSCI, V18, P1133, DOI 10.1038/nn.4062
   Bittner KC, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0046652
   Bono J, 2017, NAT COMMUN, V8, DOI 10.1038/s41467-017-00740-z
   Bower J. M., 2003, GENESIS SIMULATION S
   Branco T, 2011, NEURON, V69, P885, DOI 10.1016/j.neuron.2011.02.006
   Brette R, 2005, J NEUROPHYSIOL, V94, P3637, DOI 10.1152/jn.00686.2005
   BUSH PC, 1993, J NEUROSCI METH, V46, P159, DOI 10.1016/0165-0270(93)90151-G
   Chavlis S, 2021, CURR OPIN NEUROBIOL, V70, P1, DOI 10.1016/j.conb.2021.04.007
   Chavlis S, 2017, HIPPOCAMPUS, V27, P89, DOI 10.1002/hipo.22675
   Chiovini B, 2014, NEURON, V82, P908, DOI 10.1016/j.neuron.2014.04.004
   Christensen D. V., 2022, NEUROMORPH COMPUT EN, DOI [10.1088/2634-4386/ac4a83, DOI 10.1088/2634-4386/AC4A83]
   Colbert C. M., 2002, RESTOR NEUROL NEUROS
   Destexhe A, 2001, NEUROCOMPUTING, V38, P167, DOI 10.1016/S0925-2312(01)00428-3
   Enoki R, 2004, NEUROSCI RES, V48, P325, DOI 10.1016/j.neures.2003.11.011
   Ermentrout GB, 2010, INTERD APPL MATH, V35, P29, DOI 10.1007/978-0-387-87708-2_2
   Gerstner W., 2002, SPIKING NEURON MODEL
   Gidon A, 2012, NEURON, V75, P330, DOI 10.1016/j.neuron.2012.05.015
   Golding NL, 2005, J PHYSIOL-LONDON, V568, P69, DOI 10.1113/jphysiol.2005.086793
   Golding NL, 2001, J NEUROPHYSIOL, V86, P2998, DOI 10.1152/jn.2001.86.6.2998
   Golding NL, 1998, NEURON, V21, P1189, DOI 10.1016/S0896-6273(00)80635-2
   Górski T, 2021, NEURAL COMPUT, V33, P41, DOI 10.1162/neco_a_01342
   Grewal K., 2021, BIORXIV, DOI 10.25.465651
   Häusser M, 2000, SCIENCE, V290, P739, DOI 10.1126/science.290.5492.739
   Hendrickson EB, 2011, J COMPUT NEUROSCI, V30, P301, DOI 10.1007/s10827-010-0258-z
   Hines ML, 2000, NEURAL COMPUT, V12, P995, DOI 10.1162/089976600300015475
   Hines ML, 2001, NEUROSCIENTIST, V7, P123, DOI 10.1177/107385840100700207
   Indiveri G, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00073
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Jarsky T, 2005, NAT NEUROSCI, V8, P1667, DOI 10.1038/nn1599
   Kaifosh P, 2016, NEURON, V90, P622, DOI 10.1016/j.neuron.2016.03.019
   Kaiser J, 2022, NEUROSCIENCE, V489, P290, DOI 10.1016/j.neuroscience.2021.08.013
   Kastellakis G, 2016, CELL REP, V17, P1491, DOI 10.1016/j.celrep.2016.10.015
   Kastellakis G, 2015, PROG NEUROBIOL, V126, P19, DOI 10.1016/j.pneurobio.2014.12.002
   Kim YJ, 2015, ELIFE, V4, DOI 10.7554/eLife.06414
   Kleindienst T, 2011, NEURON, V72, P1012, DOI 10.1016/j.neuron.2011.10.015
   Lee KJ, 2013, NEURON, V77, P99, DOI 10.1016/j.neuron.2012.10.033
   Legenstein R, 2011, J NEUROSCI, V31, P10787, DOI 10.1523/JNEUROSCI.5684-10.2011
   Li XY, 2020, NAT NANOTECHNOL, V15, P776, DOI 10.1038/s41565-020-0722-5
   London M, 2005, ANNU REV NEUROSCI, V28, P503, DOI 10.1146/annurev.neuro.28.061604.135703
   Losonczy A, 2006, NEURON, V50, P291, DOI 10.1016/j.neuron.2006.03.016
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Magee JC, 2000, NAT NEUROSCI, V3, P895, DOI 10.1038/78800
   Marasco A, 2012, SCI REP-UK, V2, DOI 10.1038/srep00928
   Mascagni MV., 1989, METHODS, V131, P215101
   Masurkar AV, 2020, J NEUROPHYSIOL, V123, P980, DOI 10.1152/jn.00397.2019
   Mel Bartlett W, 2004, Sci STKE, V2004, pPE44
   Michaelis C., 2021, BRIAN2LOIHI EMULATOR
   Migliore R, 2018, PLOS COMPUT BIOL, V14, DOI 10.1371/journal.pcbi.1006423
   Nevian T, 2007, NAT NEUROSCI, V10, P206, DOI 10.1038/nn1826
   Otmakhova NA, 2002, J NEUROSCI, V22, P1199, DOI 10.1523/JNEUROSCI.22-04-01199.2002
   Pagkalos M., 2022, DENDRIFY 1 0 6, DOI [10.5281/zenodo.7442615, DOI 10.5281/ZENODO.7442615]
   Payeur A, 2021, NAT NEUROSCI, V24, P1010, DOI 10.1038/s41593-021-00857-x
   Perez-Nieves N, 2021, NAT COMMUN, V12, DOI 10.1038/s41467-021-26022-3
   Pinitas K, 2021, Arxiv, DOI [arXiv:2110.13611, 10.48550/arxiv.2110.13611, DOI 10.48550/ARXIV.2110.13611]
   Plotnikov D, 2016, Arxiv, DOI [arXiv:1606.02882, 10.48550/arXiv.1606.02882, 10.48550/arxiv.1606.02882, DOI 10.48550/ARXIV.1606.02882]
   Poirazi P, 2001, NEURON, V29, P779, DOI 10.1016/S0896-6273(01)00252-5
   Poirazi P, 2003, NEURON, V37, P977, DOI 10.1016/S0896-6273(03)00148-X
   Poirazi P, 2020, NAT REV NEUROSCI, V21, P303, DOI 10.1038/s41583-020-0301-7
   Poleg-Polsky A, 2019, J NEUROSCI, V39, P9173, DOI 10.1523/JNEUROSCI.0638-19.2019
   Polsky A, 2004, NAT NEUROSCI, V7, P621, DOI 10.1038/nn1253
   Pulvermüller F, 2021, NAT REV NEUROSCI, V22, P488, DOI 10.1038/s41583-021-00473-5
   Richards BA, 2019, NAT NEUROSCI, V22, P1761, DOI 10.1038/s41593-019-0520-2
   Roth A, 2009, COMPUT NEUROSCI-MIT, P139
   Shine JM, 2021, NAT NEUROSCI, V24, P765, DOI 10.1038/s41593-021-00824-6
   Shipman SL, 2013, J NEUROSCI, V33, P13312, DOI 10.1523/JNEUROSCI.0678-13.2013
   Sjöström PJ, 2006, NEURON, V51, P227, DOI 10.1016/j.neuron.2006.06.017
   SOFTKY W, 1994, NEUROSCIENCE, V58, P13, DOI 10.1016/0306-4522(94)90154-6
   Spreizer Sebastian, 2022, Zenodo
   Spruston N, 2008, NAT REV NEUROSCI, V9, P206, DOI 10.1038/nrn2286
   Stimberg M, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-019-54957-7
   Stimberg M, 2019, ELIFE, V8, DOI 10.7554/eLife.47314
   Stuart G, 2007, DENDRITES, DOI DOI 10.1093/ACPROF:OSO/9780198566564.001.0001
   Stuart GJ, 2015, NAT NEUROSCI, V18, P1713, DOI 10.1038/nn.4157
   Taherkhani A, 2020, NEURAL NETWORKS, V122, P253, DOI 10.1016/j.neunet.2019.09.036
   Takahashi H, 2009, NEURON, V62, P102, DOI 10.1016/j.neuron.2009.03.007
   Tikidji-Hamburyan RA, 2017, FRONT NEUROINFORM, V11, DOI 10.3389/fninf.2017.00046
   Tomko M, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-87002-7
   Tran-Van-Minh A, 2015, FRONT CELL NEUROSCI, V9, DOI 10.3389/fncel.2015.00067
   Tzilivaki A, 2019, NAT COMMUN, V10, DOI 10.1038/s41467-019-11537-7
   Ujfalussy B, 2009, PLOS COMPUT BIOL, V5, DOI 10.1371/journal.pcbi.1000500
   Ujfalussy BB, 2018, NEURON, V100, P579, DOI 10.1016/j.neuron.2018.08.032
   Van Geit W, 2016, FRONT NEUROINFORM, V10, DOI 10.3389/fninf.2016.00017
   Wang XW, 2020, NEURAL NETWORKS, V125, P258, DOI 10.1016/j.neunet.2020.02.011
   Waters J, 2005, PROG BIOPHYS MOL BIO, V87, P145, DOI 10.1016/j.pbiomolbio.2004.06.009
   Wu X., 2018, ADV NEURAL INFORM PR
   Wybo WAM, 2021, ELIFE, V10, DOI 10.7554/eLife.60936
   Yamazaki K, 2022, BRAIN SCI, V12, DOI 10.3390/brainsci12070863
   Yin B, 2020, IEEE INTERNET THINGS, V7, P8748, DOI [10.1109/JIOT.2020.2996562, 10.1145/3407197.3407225]
   Zenke F, 2021, NEURON, V109, P571, DOI 10.1016/j.neuron.2021.01.009
NR 101
TC 1
Z9 1
U1 8
U2 15
PD JAN 10
PY 2023
VL 14
IS 1
AR 131
DI 10.1038/s41467-022-35747-8
UT WOS:000928461700014
DA 2023-11-16
ER

PT C
AU Sasaki, K
   Morie, T
   Iwata, A
AF Sasaki, K
   Morie, T
   Iwata, A
GP ieee
TI A spiking neural network with negative thresholding and its application
   to associative memory
SO 2004 47TH MIDWEST SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL III, CONFERENCE
   PROCEEDINGS
DT Proceedings Paper
CT 47th Midwest Symposium on Circuits and Systems (MWSCAS 2004)
CY JUL 25-28, 2004
CL Hiroshima Univ, Hiroshima, JAPAN
HO Hiroshima Univ
AB This paper proposes a feedback-type spiking neural network using integrate-and-fire neurons with negative thresholding. Spiking neuron models express analog information by the timing of neuronall spike firing. Since these models operate asynchronously, it is expected that the spiking network operates faster than the conventional synchronous models. In order to apply such models to feedback networks with continuous states, spike firing expressing the zero value is required. However, the integrate-and-fire neurons generate no spikes when their internal potentials corresponding to the zero value do not exceed the threshold. To solve this problem, we propose negative thresholding that operates immediately after the spike input. In this paper, this thresholding operation is achieved by introducing a global excitatory unit. We have designed a spiking feedback network circuit with the global excitatory unit. The simulation results of associative memory using the network have revealed that the output spike-timing difference is independent of the input spike-timing range.
C1 Hiroshima Univ, Grad Sch Adv Sci Matter, Higashihiroshima 7398530, Japan.
RP Sasaki, K (corresponding author), Hiroshima Univ, Grad Sch Adv Sci Matter, Higashihiroshima 7398530, Japan.
CR Hirai Y., 1996, Progress in Neural Information Processing. Proceedings of the International Conference on Neural Information Processing, P1251
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maass W., 1999, PULSED NEURAL NETWOR
   MORIE T, 1994, IEEE J SOLID-ST CIRC, V29, P1086, DOI 10.1109/4.309904
   MURRAY A, 1994, ANALOGUE NEURAL VLSI
   Rumelhart DE, 1986, PARALLEL DISTRIBUTED, V1, DOI DOI 10.7551/MITPRESS/5236.001.0001
   Thorpe SJ, 1997, ADV NEUR IN, V9, P901
NR 7
TC 0
Z9 0
U1 0
U2 0
PY 2004
BP 89
EP 92
UT WOS:000225098800023
DA 2023-11-16
ER

PT J
AU Liao, XJ
   Wu, YL
   Wang, Z
   Wang, DH
   Zhang, HM
AF Liao, Xiaojian
   Wu, Yuli
   Wang, Zi
   Wang, Deheng
   Zhang, Hongmiao
TI A convolutional spiking neural network with adaptive coding for motor
   imagery classification
SO NEUROCOMPUTING
DT Article
DE Spiking neural network; Motor imagery; Adaptive coding
ID BRAIN-COMPUTER INTERFACE; OPTIMIZATION; ALGORITHM; SELECTION
AB Motor imagery (MI) signal classification is crucial for brain-computer interfaces (BCI). The third-generation neural network, spiking neural network (SNN), has rich neurodynamic properties in the spa-tiotemporal domain, and therefore it is more suitable for processing EEG signals. However, the feature extraction capability of the SNN previously applied to MI signal classification is limited by its structure, and the model's classification accuracy is not comparable to the state-of-the-art algorithms. In this paper, we propose a spiking neural network model called SCNet, which combines the feature extraction capa-bility of CNN with the biological interpretability of SNN, making the model structurally closer to the bio-logical neuronal dynamical system and improving the classification accuracy. SCNet reduces information loss by adaptive coding with learnability and solves the training difficulties of spiking neural networks by surrogate gradient learning. We evaluated the performance of the proposed SCNet on three typically rep-resentative motor imagery datasets. The validation shows that the model outperforms state-of-the-art SNN-based MI classification methods and various ANN and machine learning methods. The experimental results demonstrate the generality and effectiveness of the proposed motor imagery EEG signal classifi-cation model. Better classification results can be obtained by designing a well-structured spiking neural network.& COPY; 2023 Elsevier B.V. All rights reserved.
C1 [Liao, Xiaojian; Wu, Yuli; Wang, Zi; Zhang, Hongmiao] Soochow Univ, Sch Mech & Elect Engn, Jiangsu Prov Key Lab Adv Robot, Suzhou 215000, Peoples R China.
   [Wang, Deheng] Shanghai Univ Tradit Chinese Med, Sch Basic Med Sci, Shanghai 201203, Peoples R China.
RP Zhang, HM (corresponding author), Soochow Univ, Sch Mech & Elect Engn, Jiangsu Prov Key Lab Adv Robot, Suzhou 215000, Peoples R China.
EM zhanghongmiao@suda.edu.cn
CR Alimardani F, 2017, NEURAL NETWORKS, V92, P69, DOI 10.1016/j.neunet.2017.02.014
   Altaheri H, 2023, IEEE T IND INFORM, V19, P2249, DOI 10.1109/TII.2022.3197419
   Altaheri H, 2023, NEURAL COMPUT APPL, V35, P14681, DOI 10.1007/s00521-021-06352-5
   Altuwaijri GA, 2022, DIAGNOSTICS, V12, DOI 10.3390/diagnostics12040995
   Amin SU, 2022, IEEE T IND INFORM, V18, P5412, DOI 10.1109/TII.2021.3132340
   [Anonymous], 2015, 2015 INT JOINT C NEU, DOI DOI 10.1109/IJCNN.2015.7280754
   [Anonymous], 2003, OUTCOME BCI COMPETIT
   Bing Han, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13555, DOI 10.1109/CVPR42600.2020.01357
   Blankertz B, 2004, IEEE T BIO-MED ENG, V51, P1044, DOI 10.1109/TBME.2004.826692
   Cheng X, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1519
   Deng L, 2020, NEURAL NETWORKS, V121, P294, DOI 10.1016/j.neunet.2019.09.005
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Dolzhikova I, 2021, IEEE ENG MED BIO, P319, DOI 10.1109/EMBC46164.2021.9630419
   Fang W, 2021, ADV NEUR IN, V34
   Feng HF, 2001, NEURAL NETWORKS, V14, P955, DOI 10.1016/S0893-6080(01)00074-0
   Gaur P, 2018, EXPERT SYST APPL, V95, P201, DOI 10.1016/j.eswa.2017.11.007
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   Gong PL, 2023, IEEE T NEUR SYS REH, V31, P1440, DOI 10.1109/TNSRE.2023.3246989
   Hou YM, 2022, NEUROSCI RES, V176, P40, DOI 10.1016/j.neures.2021.09.002
   Huang E, 2022, IRBM, V43, P107, DOI 10.1016/j.irbm.2021.04.004
   Lee C, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00119
   Luo J, 2016, COMPUT BIOL MED, V75, P45, DOI 10.1016/j.compbiomed.2016.03.004
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Meng JJ, 2018, IEEE T BIO-MED ENG, V65, P2417, DOI 10.1109/TBME.2018.2872855
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Rasteh A, 2022, NEUROCOMPUTING, V503, P272, DOI 10.1016/j.neucom.2022.06.055
   Raza H, 2016, SOFT COMPUT, V20, P3085, DOI 10.1007/s00500-015-1937-5
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Schirrmeister RT, 2017, HUM BRAIN MAPP, V38, P5391, DOI 10.1002/hbm.23730
   Shrestha SB, 2018, ADV NEUR IN, V31
   Sun L, 2018, BIOMED SIGNAL PROCES, V41, P1, DOI 10.1016/j.bspc.2017.10.012
   Tabar YR, 2017, J NEURAL ENG, V14, DOI 10.1088/1741-2560/14/1/016003
   Tangermann M, 2012, FRONT NEUROSCI-SWITZ, V6, DOI 10.3389/fnins.2012.00055
   Virgilio CD, 2020, NEURAL NETWORKS, V122, P130, DOI 10.1016/j.neunet.2019.09.037
   Wang J, 2018, COMPUT BIOL MED, V97, P161, DOI 10.1016/j.compbiomed.2018.04.022
   Wei ZH, 2016, J INTEGR NEUROSCI, V15, P347, DOI 10.1142/S0219635216500229
   Wu XY, 2023, NEUROCOMPUTING, V529, P222, DOI 10.1016/j.neucom.2023.01.087
   Xu Y, 2019, ROBOT AUTON SYST, V115, P121, DOI 10.1016/j.robot.2019.02.014
   Yan Zhanglu, 2022, SMART HLTH, V24
   Yang HJ, 2015, IEEE ENG MED BIO, P2620, DOI 10.1109/EMBC.2015.7318929
   Yuanfang Ren, 2014, 2014 International Joint Conference on Neural Networks (IJCNN), P2850, DOI 10.1109/IJCNN.2014.6889383
   Zhang WR, 2019, ADV NEUR IN, V32
   Zheng QQ, 2018, IEEE T NEUR SYS REH, V26, P551, DOI 10.1109/TNSRE.2018.2794534
NR 44
TC 0
Z9 0
U1 14
U2 14
PD SEP 7
PY 2023
VL 549
AR 126470
DI 10.1016/j.neucom.2023.126470
EA JUN 2023
UT WOS:001035213300001
DA 2023-11-16
ER

PT C
AU Zhou, Q
   Xu, GZ
   Chen, YZ
   Guo, MM
AF Zhou, Qian
   Xu, Guizhi
   Chen, Yunzhi
   Guo, Miaomiao
BE Wen, Z
   Li, T
TI Stable Frequency Transmission Emerging from Spiking-Timing-Dependent
   Plasticity in Spiking Neural Network
SO FOUNDATIONS OF INTELLIGENT SYSTEMS (ISKE 2013)
SE Advances in Intelligent Systems and Computing
DT Proceedings Paper
CT 8th International Conference on Intelligent Systems and Knowledge
   Engineering (ISKE)
CY NOV 20-23, 2013
CL Shenzhen, PEOPLES R CHINA
DE Spike-timing-dependent plasticity; Spiking neural network; Frequency
   transmission
ID NEURONS; MODEL
AB Cerebral information processing is fast, intelligent, and robust. Research of electrical information transmission in nervous system and its robust mechanism has great significance in improving the immunity and protection technology of electronic systems. Spike-timing-dependent plasticity (STDP) has been recognized as a key mechanism of how information is processed in the brain. In this paper, we focus on the information transmission process of spiking neurons and frequency adaptation at network level. It is found that the regulation of STDP has an effect of keeping the total synaptic input to the postsynaptic neuron roughly constant when synapse number alters. In a three-layer forward neural network, STDP organizes the synaptic connections to achieve stable frequency transmission in a three-layer spiking network when the network has some interior damage. These results indicate that STDP plays a crucial role in the adaptive information processing in nervous systems.
C1 [Zhou, Qian; Xu, Guizhi; Chen, Yunzhi; Guo, Miaomiao] Hebei Univ Technol, Prov Minist, Joint Key Lab Elect Field & Elect Apparat Reliabi, Tianjin, Peoples R China.
RP Zhou, Q (corresponding author), Hebei Univ Technol, Prov Minist, Joint Key Lab Elect Field & Elect Apparat Reliabi, Tianjin, Peoples R China.
EM qzhou@hebut.edu.cn
CR Bartolozzi C, 2009, SENSORS-BASEL, V9, P5076, DOI 10.3390/s90705076
   Bi GQ, 2001, ANNU REV NEUROSCI, V24, P139, DOI 10.1146/annurev.neuro.24.1.139
   Chang X, 2012, INT C BIOM ENG INF, P777
   Chen H, 2011, COMPUT SYST APPL, V4, P182
   Fernando S, 2012, COMPUT INTEL NEUROSC, V2012
   FERSTER D, 1995, SCIENCE, V270, P756, DOI 10.1126/science.270.5237.756
   Goodhill G, 1993, NEURAL COMPUT, V6, P256
   Hennequin G, 2010, FRONT COMPUT NEUROSC, V4, DOI 10.3389/fncom.2010.00143
   Indiveri G, 2006, IEEE T NEURAL NETWOR, V17, P211, DOI 10.1109/TNN.2005.860850
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Liu JK, 2009, PLOS ONE, V4, DOI 10.1371/journal.pone.0006247
   Liu S. H., 2009, CHIN J NAT MED-CHINA, V31, P1
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Vreeken J, 2002, UUCS2003008 U UTR I
   Wang S, 2004, EUR PHYS J B, V39, P351, DOI 10.1140/epjb/e2004-00200-4
   Wijekoon JHB, 2012, J NEUROSCI METH, V210, P93, DOI 10.1016/j.jneumeth.2012.01.019
NR 17
TC 0
Z9 0
U1 1
U2 2
PY 2014
VL 277
BP 361
EP 370
DI 10.1007/978-3-642-54924-3_34
UT WOS:000349457000034
DA 2023-11-16
ER

PT C
AU Hiraoka, R
   Matsumoto, K
   Nguyen, K
   Torikai, H
   Sekiya, H
AF Hiraoka, Ryuya
   Matsumoto, Kazuki
   Nguyen, Kien
   Torikai, Hiroyuki
   Sekiya, Hiroo
BE Gedeon, T
   Wong, KW
   Lee, M
TI Implementation of Spiking Neural Network with Wireless Communications
SO NEURAL INFORMATION PROCESSING, ICONIP 2019, PT V
SE Communications in Computer and Information Science
DT Proceedings Paper
CT 26th International Conference on Neural Information Processing (ICONIP)
   of the Asia-Pacific-Neural-Network-Society (APNNS)
CY DEC 12-15, 2019
CL Sydney, AUSTRALIA
DE Spiking Neural Network; Wireless communication; Sensor networks
AB This paper proposes and implements the Spiking Neural Network (SNN) with radio-frequency wireless communications. The implemented network could obtain the XOR function through reinforcement learning. By applying the wireless communication for Internet of Things to the SNN, the SNN works with sufficient communication distance and low power consumptions for not only the line of sight environment but also the non-line of sight one. Additionally, it is unnecessary to consider communication directivity and obstacles for constructing the networks. The experimental results showed the extensibility and the scalability of the implemented system in this paper.
C1 [Hiraoka, Ryuya; Matsumoto, Kazuki; Nguyen, Kien; Sekiya, Hiroo] Chiba Univ, Inage Ward, 1-33 Yayoicho, Chiba, Chiba 2638522, Japan.
   [Torikai, Hiroyuki] Hosei Univ, 3-7-2 Kazinocho, Koganei, Tokyo 1848584, Japan.
RP Hiraoka, R (corresponding author), Chiba Univ, Inage Ward, 1-33 Yayoicho, Chiba, Chiba 2638522, Japan.
EM r.hiraoka@chiba-u.jp; torikai@hosei.ac.jp; sekiya@faculty.chiba-u.jp
CR Dominguez-Morales J., 2018, 2018 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2018.8489381
   Florian RV, 2007, NEURAL COMPUT, V19, P1468, DOI 10.1162/neco.2007.19.6.1468
   Karl H, 2005, PROTOCOLS AND ARCHITECTURES FOR WIRELESS SENSOR NETWORKS, P1, DOI 10.1002/0470095121
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Matsumoto K, 2018, ASIAPAC SIGN INFO PR, P1289, DOI 10.23919/APSIPA.2018.8659484
   Mostafa Hesham, 2018, IEEE Trans Neural Netw Learn Syst, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Roy S, 2017, IEEE T NEUR NET LEAR, V28, P900, DOI 10.1109/TNNLS.2016.2582517
   Wang ZZ, 2014, IEEE ENG MED BIO, P6810, DOI 10.1109/EMBC.2014.6945192
NR 8
TC 1
Z9 1
U1 0
U2 1
PY 2019
VL 1143
BP 619
EP 626
DI 10.1007/978-3-030-36802-9_66
UT WOS:000632759700066
DA 2023-11-16
ER

PT J
AU Zhang, AG
   Gao, YM
   Niu, YZ
   Li, XM
   Chen, Q
AF Zhang, Anguo
   Gao, Yueming
   Niu, Yuzhen
   Li, Xiumin
   Chen, Qing
TI Intrinsic Plasticity for Online Unsupervised Learning Based on
   Soft-Reset Spiking Neuron Model
SO IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS
DT Article
DE Neurons; IP networks; Membrane potentials; Noise robustness;
   Computational modeling; Biological neural networks; Adaptation models;
   Intrinsic plasticity (IP); online unsupervised learning; soft-reset
   spiking neuron; spiking neural network (SNN)
ID MEMORY
AB Intrinsic plasticity (IP) is an unsupervised, self-adaptive, local learning rule that was first found in biological nerve cells, and has been shown to be able to maximize neuronal information transmission entropy. In this article, we propose a soft-reset leaky integrate-and-fire (LIF) model, a spiking neuron model based on widely used LIF neurons, with a new IP learning rule that optimizes the neuronal membrane potential state to be exponentially distributed. Previous studies have generally used such as spiking neuron expected firing rate as the target variable to maximize output spike distribution. In contrast, the proposed soft-reset model can avoid the problem that conventional LIF neuronal membrane potential is not fully differentiable, hence the proposed IP rule can directly regulate the membrane potential as an auxiliary "output signal" to desired distribution to maximize its information entropy. We experimentally evaluated the proposed IP rule for pattern recognition on the spiking feed-forward and spiking convolutional neural network models. Experimental results verified that the proposed IP rule can effectively improve spiking neural network computational performance in terms of classification accuracy, spiking inference speed, and noise robustness.
C1 [Zhang, Anguo; Gao, Yueming] Fuzhou Univ, Coll Phys & Informat Engn, Fuzhou 350108, Peoples R China.
   [Zhang, Anguo] Fuzhou Univ, Key Lab Med Instrumentat & Pharmaceut Technol Fuji, Fuzhou 350116, Peoples R China.
   [Zhang, Anguo] Ruijie Networks Co Ltd, Res Inst Ruijie, Fuzhou 350002, Peoples R China.
   [Gao, Yueming] Fuzhou Univ, Key Lab Med Instrumentat & Pharmaceut Technol Fuji, Fuzhou 350116, Peoples R China.
   [Niu, Yuzhen] Fuzhou Univ, Coll Math & Comp Sci, Fuzhou 350108, Peoples R China.
   [Li, Xiumin] Chongqing Univ, Coll Automat, Chongqing 400030, Peoples R China.
RP Niu, YZ (corresponding author), Fuzhou Univ, Coll Math & Comp Sci, Fuzhou 350108, Peoples R China.
EM yuzhenniu@gmail.com
CR Baddeley R, 1997, P ROY SOC B-BIOL SCI, V264, P1775, DOI 10.1098/rspb.1997.0246
   Canto CB, 2018, P NATL ACAD SCI USA, V115, P9824, DOI 10.1073/pnas.1813866115
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350
   Desai NS, 1999, NAT NEUROSCI, V2, P515, DOI 10.1038/9165
   Diehl P.U., 2015, 2015 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2015.7280696
   Disterhoft JF, 2006, TRENDS NEUROSCI, V29, P587, DOI 10.1016/j.tins.2006.08.005
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Krizhevsky A., 2009, RT2009
   Li CG, 2013, IEEE T AUTON MENT DE, V5, P62, DOI 10.1109/TAMD.2012.2211101
   Li CG, 2011, IEEE T AUTON MENT DE, V3, P277, DOI 10.1109/TAMD.2011.2159379
   Li XM, 2018, PHYSICA A, V491, P716, DOI 10.1016/j.physa.2017.08.053
   Liberman M., 1993, LDC CATALOG LDC93S9
   Liu Q, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00496
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Marder E, 1996, P NATL ACAD SCI USA, V93, P13481, DOI 10.1073/pnas.93.24.13481
   Netzer Y., 2011, ADV NEURAL INFORM PR, P1
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   Sehgal M, 2013, NEUROBIOL LEARN MEM, V105, P186, DOI 10.1016/j.nlm.2013.07.008
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Stepp N, 2015, PLOS COMPUT BIOL, V11, DOI 10.1371/journal.pcbi.1004043
   Triesch J, 2007, NEURAL COMPUT, V19, P885, DOI 10.1162/neco.2007.19.4.885
   Watt Alanna J, 2010, Front Synaptic Neurosci, V2, P5, DOI 10.3389/fnsyn.2010.00005
   Xiao H, 2017, Arxiv, DOI arXiv:1708.07747
   Xue F., 2017, ADV NEURAL NETWORKS, V10261, P466
   Zambrano D, 2016, Arxiv, DOI arXiv:1609.02053
   Zhang AG, 2019, NEUROCOMPUTING, V365, P102, DOI 10.1016/j.neucom.2019.07.009
   Zhang W, 2003, NAT REV NEUROSCI, V4, P885, DOI 10.1038/nrn1248
   Zhang WR, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00031
NR 29
TC 2
Z9 2
U1 5
U2 5
PD JUN
PY 2023
VL 15
IS 2
BP 337
EP 347
DI 10.1109/TCDS.2020.3041610
UT WOS:001005746000003
DA 2023-11-16
ER

PT J
AU Hajiabadi, Z
   Shalchian, M
AF Hajiabadi, Zohreh
   Shalchian, Majid
TI Memristor-based synaptic plasticity and unsupervised learning of spiking
   neural networks
SO JOURNAL OF COMPUTATIONAL ELECTRONICS
DT Article
DE Memristor; Synapse; Spiking neural network (SNN); Unsupervised learning;
   Spike-timing-dependent plasticity (STDP)
ID HODGKIN-HUXLEY; CIRCUIT; NEURONS; DESIGNS; MODEL
AB Synaptic plasticity is studied herein using a voltage-driven memristor model. The bidirectional weight update technique is demonstrated, and significant synaptic features, including nonlinear and threshold-based learning and long-term potentiation and long-term depression, are emulated. The spike-timing-dependent plasticity (STDP) learning characteristic curve is obtained from exhaustive simulations. Then, using leaky integrate and fire neurons and memristive synapses, fully connected spiking neural networks with 2 x 2 and 4 x 2 architectures are constructed, and unsupervised learning using the STDP rule and winner-takes-all strategy is evaluated in those networks for pattern classification.
C1 [Hajiabadi, Zohreh; Shalchian, Majid] Amirkabir Univ Technol, Dept Elect Engn, Tehran, Iran.
RP Shalchian, M (corresponding author), Amirkabir Univ Technol, Dept Elect Engn, Tehran, Iran.
EM z-hajiabadi@aut.ac.ir; shalchian@aut.ac.ir
CR Amirsoleimani A, 2017, IEEE IJCNN, P3409, DOI 10.1109/IJCNN.2017.7966284
   Amirsoleimani A, 2016, IEEE I C ELECT CIRC, P81, DOI 10.1109/ICECS.2016.7841137
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Biolek Z, 2009, RADIOENGINEERING, V18, P210
   Chua L, 2013, NANOTECHNOLOGY, V24, DOI 10.1088/0957-4484/24/38/383001
   CHUA LO, 1971, IEEE T CIRCUITS SYST, VCT18, P507, DOI 10.1109/TCT.1971.1083337
   Covi E, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00482
   Gerstner W., 2002, SPIKING NEURON MODEL
   Hajiabadi Z, 2020, IRAN CONF ELECTR ENG, P1380
   Hong QH, 2019, NEUROCOMPUTING, V330, P11, DOI 10.1016/j.neucom.2018.11.043
   Indiveri G, 2006, IEEE T NEURAL NETWOR, V17, P211, DOI 10.1109/TNN.2005.860850
   Jo SH, 2010, NANO LETT, V10, P1297, DOI 10.1021/nl904092h
   Kim Y, 2019, AIP ADV, V9, DOI 10.1063/1.5092177
   Kvatinsky S, 2015, IEEE T CIRCUITS-II, V62, P786, DOI 10.1109/TCSII.2015.2433536
   Kvatinsky S, 2013, IEEE T CIRCUITS-I, V60, P211, DOI 10.1109/TCSI.2012.2215714
   Lehtonen E, 2010, P 12 INT WORKSH CELL, P1
   Long KL, 2020, J COMPUT ELECTRON, V19, P435, DOI 10.1007/s10825-019-01437-w
   Mirsadeghi M, 2021, NEUROCOMPUTING, V427, P131, DOI 10.1016/j.neucom.2020.11.052
   Pickett MD, 2009, J APPL PHYS, V106, DOI 10.1063/1.3236506
   Plank J.S., 2017, ARXIV PREPRINT ARXIV
   Rziga FO, 2019, J COMPUT ELECTRON, V18, P1055, DOI 10.1007/s10825-019-01357-9
   Serrano-Gotarredona T, 2012, IEEE I C ELECT CIRC, P949, DOI 10.1109/ICECS.2012.6463504
   Strukov DB, 2008, NATURE, V453, P80, DOI 10.1038/nature06932
   Williams RS, 2013, CHAOS, CNN, MEMRISTORS AND BEYOND: A FESTSCHRIFT FOR LEON CHUA, P483
   Zamarreño-Ramos C, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00026
   Zhao L, 2018, NEUROCOMPUTING, V314, P207, DOI 10.1016/j.neucom.2018.06.062
NR 26
TC 4
Z9 3
U1 8
U2 67
PD AUG
PY 2021
VL 20
IS 4
BP 1625
EP 1636
DI 10.1007/s10825-021-01719-2
EA MAY 2021
UT WOS:000650194400002
DA 2023-11-16
ER

PT C
AU Sheng, D
   Xu, RX
   Wang, Q
   Zhao, C
AF Sheng, Dian
   Xu, Rongxuan
   Wang, Qinan
   Zhao, Chun
GP IEEE
TI Spiking Neural Networks for digital hand-written number recognition
SO 2022 19TH INTERNATIONAL SOC DESIGN CONFERENCE (ISOCC)
SE International SoC Design Conference
DT Proceedings Paper
CT 19th International SoC Design Conference (ISOCC) - SoC Technology
   Towards a New Era of Innovation
CY OCT 19-22, 2022
CL Gangwon Do, SOUTH KOREA
DE Leaky integrate-and-fire (LIF); MINIST; Neural computing
AB Nowadays, the advancement of deep learning has been staggering during the past decades. The state-of-art spiking neural networks (SNNs) demonstrate outstanding characteristics in accuracy, power-efficiency, and spiking timing-dependent plasticity (STDP). In view of these advantages, SNNs are a promising candidate for neural morphic application. This paper presents the performances and applications of SNNs, which are used to recognize digital hand-written numbers.
C1 [Sheng, Dian; Xu, Rongxuan; Wang, Qinan; Zhao, Chun] Xian Jiaotong Liverpool Univ, Dept Sch Adv Technol, Suzhou, Peoples R China.
   [Wang, Qinan; Zhao, Chun] Univ Liverpool, Dept Elect Engn & Elect, Liverpool, Merseyside, England.
RP Zhao, C (corresponding author), Xian Jiaotong Liverpool Univ, Dept Sch Adv Technol, Suzhou, Peoples R China.; Zhao, C (corresponding author), Univ Liverpool, Dept Elect Engn & Elect, Liverpool, Merseyside, England.
EM Chun.Zhao@xjtlu.edu.cn
CR Bofill-i-Petit A, 2004, IEEE T NEURAL NETWOR, V15, P1296, DOI 10.1109/TNN.2004.832842
   Eshraghian JK, 2023, Arxiv, DOI [arXiv:2109.12894, DOI 10.48550/ARXIV.2109.12894]
   Li Y, 2021, ADV FUNCT MATER, V31, DOI 10.1002/adfm.202100042
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Sun J, 2018, ADV FUNCT MATER, V28, DOI 10.1002/adfm.201804397
   Yu SM, 2018, P IEEE, V106, P260, DOI 10.1109/JPROC.2018.2790840
NR 6
TC 0
Z9 0
U1 2
U2 2
PY 2022
BP 185
EP 186
DI 10.1109/ISOCC56007.2022.10031396
UT WOS:000971297000090
DA 2023-11-16
ER

PT C
AU Parque, V
   Kobayashi, M
   Higashi, M
AF Parque, Victor
   Kobayashi, Masakazu
   Higashi, Masatake
BE Loo, CK
   Yap, KS
   Wong, KW
   Teoh, A
   Huang, K
TI Neural Computing with Concurrent Synchrony
SO NEURAL INFORMATION PROCESSING (ICONIP 2014), PT I
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 21st International Conference on Neural Information Processing (ICONIP)
CY NOV 03-06, 2014
CL Kuching, MALAYSIA
DE synchrony; concurrency; spiking networks; neural representation
ID NETWORKS; SYSTEMS
AB Neural networks are important modeling tools to implement intelligent behaviour in a wide variety of phenomena. We introduce the concept of concurrent synchrony in spikes to enable the efficient representation of neural networks to process sensory stimuli. Using different sensory modalities, we show that information processing from stimuli can be represented compactly. This approach aims at introducing homeostasis into the behavior of neural populations in order to construct diverse and sophisticated control rules without increasing network complexity.
C1 [Parque, Victor; Kobayashi, Masakazu; Higashi, Masatake] Toyota Technol Inst, Tempaku Ku, 2-12-1 Hisakata, Nagoya, Aichi 4688511, Japan.
RP Parque, V (corresponding author), Toyota Technol Inst, Tempaku Ku, 2-12-1 Hisakata, Nagoya, Aichi 4688511, Japan.
CR Brette R, 2012, PLOS COMPUT BIOL, V8, DOI 10.1371/journal.pcbi.1002561
   Cui GH, 2013, NATURE, V494, P238, DOI 10.1038/nature11846
   Gerfen CR, 2011, ANNU REV NEUROSCI, V34, P441, DOI 10.1146/annurev-neuro-061010-113641
   Goodman Dan, 2008, Front Neuroinform, V2, P5, DOI 10.3389/neuro.11.005.2008
   Hirasawa K., 1995, IEEE T MAN CYBERNE B, V30, P419
   HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554
   Izhikevich EM, 2010, PHILOS T R SOC A, V368, P5061, DOI 10.1098/rsta.2010.0130
   LIN DT, 1995, NEURAL NETWORKS, V8, P447, DOI 10.1016/0893-6080(94)00104-T
   MAASS W, 1996, AUSTR C NEUR NETW
   Malvezzi M, 2013, IEEE INT CONF ROBOT, P1088, DOI 10.1109/ICRA.2013.6630708
   McCulloch Warren S., 1943, BULL MATH BIOPHYS, V5, P115, DOI 10.1007/BF02478259
   Mink JW, 2003, ARCH NEUROL-CHICAGO, V60, P1365, DOI 10.1001/archneur.60.10.1365
   Moody J, 1989, NEURAL COMPUT, V1, P281, DOI 10.1162/neco.1989.1.2.281
   Nabavi S, 2014, NATURE, V511, P348, DOI 10.1038/nature13294
   Pearl J, 2011, ANN MATH ARTIF INTEL, V61, P29, DOI 10.1007/s10472-011-9247-9
   ROSENBLATT F, 1958, PSYCHOL REV, V65, P386, DOI 10.1037/h0042519
   Rumelhart DE, 1986, PARALLEL DISTRIBUTED, V1, DOI DOI 10.7551/MITPRESS/5236.001.0001
   Williams R. J., 1990, NUCCS909
   Williams RJ, 1989, NEURAL COMPUT, V1, P270, DOI 10.1162/neco.1989.1.2.270
NR 19
TC 6
Z9 6
U1 1
U2 1
PY 2014
VL 8834
BP 304
EP 311
UT WOS:000432659500038
DA 2023-11-16
ER

PT J
AU Gilani, SQ
   Syed, T
   Umair, M
   Marques, O
AF Gilani, Syed Qasim
   Syed, Tehreem
   Umair, Muhammad
   Marques, Oge
TI Skin Cancer Classification Using Deep Spiking Neural Network
SO JOURNAL OF DIGITAL IMAGING
DT Article
DE Deep learning; Image analysis; Spiking neural networks; Skin lesion
   classification
ID PREVALENCE; DIAGNOSIS; LESIONS; US
AB Skin cancer is one of the primary causes of death globally, and experts diagnose it by visual inspection, which can be inaccurate. The need for developing a computer-aided method to aid dermatologists in diagnosing skin cancer is highlighted by the fact that early identification can lower the number of deaths caused by skin malignancies. Among computer-aided techniques, deep learning is the most popular for identifying cancer from skin lesion images. Due to their power-efficient behavior, spiking neural networks are attractive deep neural networks for hardware implementation. We employed deep spiking neural networks using the surrogate gradient descent method to classify 3670 melanoma and 3323 non-melanoma images from the ISIC 2019 dataset. We achieved an accuracy of 89.57% and an F1 score of 90.07% using the proposed spiking VGG-13 model, which is higher than the VGG-13 and AlexNet using less trainable parameters.
C1 [Gilani, Syed Qasim; Marques, Oge] Florida Atlantic Univ, Dept Elect Engn & Comp Sci, Boca Raton, FL 33431 USA.
   [Syed, Tehreem] Tech Univ Dresden, Dept Elect Engn & Comp Engn, D-01069 Dresden, Saxony, Germany.
   [Umair, Muhammad] George Mason Univ, Dept Elect & Comp Engn, Fairfax, VA 22030 USA.
RP Gilani, SQ (corresponding author), Florida Atlantic Univ, Dept Elect Engn & Comp Sci, Boca Raton, FL 33431 USA.
EM sgilani2020@fau.edu; tehreem.syed@tu-dresden.de; mumair4@gmu.edu;
   omarques@fau.edu
CR Albahar MA, 2019, IEEE ACCESS, V7, P38306, DOI 10.1109/ACCESS.2019.2906241
   Ali AA, 2017, PROC INT C ELECT COM, P1, DOI 10.1109/ICECTA.2017.8252041
   [Anonymous], US
   [Anonymous], 2013, 2013 INT JOINT C NEU
   Bing Han, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13555, DOI 10.1109/CVPR42600.2020.01357
   Chen ZP, 1998, 1998 INTERNATIONAL SYMPOSIUM ON LOW POWER ELECTRONICS AND DESIGN - PROCEEDINGS, P239
   Codella NCF, 2018, I S BIOMED IMAGING, P168, DOI 10.1109/ISBI.2018.8363547
   Diehl P.U., 2015, 2015 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2015.7280696
   Diehl PU, 2016, IEEE IJCNN, P4278, DOI 10.1109/IJCNN.2016.7727758
   Dorj UO, 2018, MULTIMED TOOLS APPL, V77, P9909, DOI 10.1007/s11042-018-5714-1
   Escobar MJ, 2009, INT J COMPUT VISION, V82, P284, DOI 10.1007/s11263-008-0201-1
   Esteva A, 2017, NATURE, V542, P115, DOI 10.1038/nature21056
   Fan HD, 2017, COMPUT BIOL MED, V85, P75, DOI 10.1016/j.compbiomed.2017.03.025
   Fang W., 2020, SPIKINGJELLY
   Fornaciali Michel, 2016, ARXIV
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   Ghosh-Dastidar S, 2007, INTEGR COMPUT-AID E, V14, P187
   Guy GP, 2015, MMWR-MORBID MORTAL W, V64, P591
   Guy GP, 2014, AM J PUBLIC HEALTH, V104, pE69, DOI [10.2105/AJPH.2013.301850, 10.1016/j.amepre.2014.08.036]
   Harangi B, 2018, IEEE ENG MED BIO, P2575, DOI 10.1109/EMBC.2018.8512800
   Hasan MK, 2020, COMPUT BIOL MED, V120, DOI 10.1016/j.compbiomed.2020.103738
   Huang G., 2017, 2017 IEEE C COMPUTER, P4700
   Jalalian A, 2017, EXCLI J, V16, P113, DOI [10.17179/excli2016-701, 10.17179/excli201-701]
   Jemal A, 2009, CA-CANCER J CLIN, V59, P225, DOI [10.3322/caac.20006, 10.3322/caac.21332, 10.3322/caac.21387, 10.3322/caac.21601, 10.3322/caac.20073, 10.3322/caac.21254]
   Acosta MFJ, 2021, BMC MED IMAGING, V21, DOI 10.1186/s12880-020-00534-8
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90
   Kalouche S., 2016, VISION BASED CLASSIF
   Kasabov N, 2014, NEUROCOMPUTING, V134, P269, DOI 10.1016/j.neucom.2013.09.049
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Kittler H, 2002, LANCET ONCOL, V3, P159, DOI 10.1016/S1470-2045(02)00679-4
   Korotkov K, 2012, ARTIF INTELL MED, V56, P69, DOI 10.1016/j.artmed.2012.08.002
   Kourou K, 2015, COMPUT STRUCT BIOTEC, V13, P8, DOI 10.1016/j.csbj.2014.11.005
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
   Ledinauskas Eimantas, 2020, ARXIV
   Liu DQ, 2017, NEUROCOMPUTING, V249, P212, DOI 10.1016/j.neucom.2017.04.003
   Lobov S, 2015, SENSORS-BASEL, V15, P27894, DOI 10.3390/s151127894
   Mahbod A, 2019, INT CONF ACOUST SPEE, P1229, DOI [10.1109/icassp.2019.8683352, 10.1109/ICASSP.2019.8683352]
   Majtner T, 2019, MULTIMED TOOLS APPL, V78, P11883, DOI 10.1007/s11042-018-6734-6
   Marghoob AA, 2003, J AM ACAD DERMATOL, V49, P777, DOI 10.1016/S0190-9622(03)02470-8
   Nasr-Esfahani E, 2016, IEEE ENG MED BIO, P1373, DOI 10.1109/EMBC.2016.7590963
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Ponulak F, 2011, ACTA NEUROBIOL EXP, V71, P409
   Rezvantalab A., 2018, ARXIV
   Rogers HW, 2015, JAMA DERMATOL, V151, P1081, DOI 10.1001/jamadermatol.2015.1187
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707
   Sagar A., 2020, CONVOLUTIONAL NEURAL, DOI [10.1101/2020.05.22.110973, DOI 10.1101/2020.05.22.110973]
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Stern RS, 2010, ARCH DERMATOL, V146, P279, DOI 10.1001/archdermatol.2010.4
   Syed T, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21093240
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tan C, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20185328
   Vanarse A, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20102756
   Ye C, 2016, P 24 ACM INT C MULT, P1156, DOI 10.1145/2964284
   Yu Z, 2019, IEEE T BIO-MED ENG, V66, P1006, DOI 10.1109/TBME.2018.2866166
   Zenke F, 2018, NEURAL COMPUT, V30, P1514, DOI 10.1162/neco_a_01086
   Zhou Q, 2020, IEEE ACCESS, V8, P101309, DOI 10.1109/ACCESS.2020.2998098
NR 60
TC 1
Z9 1
U1 9
U2 17
PD JUN
PY 2023
VL 36
IS 3
BP 1137
EP 1147
DI 10.1007/s10278-023-00776-2
EA JAN 2023
UT WOS:000919711400001
DA 2023-11-16
ER

PT C
AU Zhang, SL
AF Zhang Silin
GP IEEE
TI DEEP LEARNING ON POINT CLOUD FOR 3D CLASSIFICATION BASED ON SPIKING
   NEURAL NETWORK
SO 2022 19TH INTERNATIONAL COMPUTER CONFERENCE ON WAVELET ACTIVE MEDIA
   TECHNOLOGY AND INFORMATION PROCESSING (ICCWAMTIP)
SE International Computer Conference on Wavelet Active Media Technology and
   Information Processing
DT Proceedings Paper
CT 19th IEEE International Computer Conference on Wavelet Active Media
   Technology and Information Processing (ICCWAMTIP)
CY DEC 16-18, 2022
CL Univ Elect Sci & Technol China, Chengdu, PEOPLES R CHINA
HO Univ Elect Sci & Technol China
DE Spiking neural network; Point cloud; STDP; Classification
AB Point cloud is a real representation of the physical world in a digital way. As an important geometric data structure, it is widely used in visualization, animation, rendering and modeling. Because of its irregular format, we have to convert the data to a normal way thus it can be used, which also leads to the data becoming extremely large. In this paper, we build a spiking neural network (SNN) based on spike-timing-dependent plastic rules (STDP) [1-3], which can process point cloud data more efficiently by using the sparse characteristics of the spiking neural network. At the same time, the SNN naturally takes into account the displacement invariance and rotation invariance of point clouds. The accuracy of the neural network model we designed in the classification task is more or less equal to that of the existing point cloud processing technology, but it is deployed on edge devices with better performance as lower power consumption, higher efficiency and so on.
C1 [Zhang Silin] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 610054, Peoples R China.
RP Zhang, SL (corresponding author), Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 610054, Peoples R China.
EM zh_silin@foxmail.com
CR Zhang ML, 2022, IEEE T NEUR NET LEAR, V33, P1947, DOI 10.1109/TNNLS.2021.3110991
   Zhang ML, 2020, NEUROCOMPUTING, V409, P103, DOI 10.1016/j.neucom.2020.03.079
   Zhang ML, 2019, IEEE T NEUR NET LEAR, V30, P123, DOI 10.1109/TNNLS.2018.2833077
NR 3
TC 0
Z9 0
U1 1
U2 2
PY 2022
DI 10.1109/ICCWAMTIP56608.2022.10016491
UT WOS:000932922500013
DA 2023-11-16
ER

PT C
AU Anwar, A
AF Anwar, Abrar
GP Assoc Advancement Artificial Intelligence
TI Evolving Spiking Circuit Motifs Using Weight Agnostic Neural Networks
SO THIRTY-FIFTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, THIRTY-THIRD
   CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE AND THE
   ELEVENTH SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE
SE AAAI Conference on Artificial Intelligence
DT Proceedings Paper
CT 35th AAAI Conference on Artificial Intelligence / 33rd Conference on
   Innovative Applications of Artificial Intelligence / 11th Symposium on
   Educational Advances in Artificial Intelligence
CY FEB 02-09, 2021
CL ELECTR NETWORK
AB Neural architecture search (NAS) has emerged as an algorithmic method of developing neural network architectures. Weight Agnostic Neural Networks (WANNs) are an evolutionary-based NAS approach. Fundamentally, WANNs find network structures that are relatively insensitive to shifts in weight values and are typically much smaller than an equivalent performance dense network. Here, we extend the WANN framework to search for spiking circuits and in doing so investigate whether these circuit motifs can also yield task performance that is weight agnostic. We analyze properties such as the complexity of the solution, as well as performance. Our results successfully show the performance of spiking WANNs on several exemplar tasks.
C1 [Anwar, Abrar] Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA.
   [Anwar, Abrar] Sandia Natl Labs, Livermore, CA 94550 USA.
RP Anwar, A (corresponding author), Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA.; Anwar, A (corresponding author), Sandia Natl Labs, Livermore, CA 94550 USA.
EM abraranwar@utexas.edu
CR Gaier A., 2019, ADV NEURAL INFORM PR, VVolume 32, P5364
   Severa W, 2019, NAT MACH INTELL, V1, P86, DOI 10.1038/s42256-018-0015-y
   Stanley KO, 2002, EVOL COMPUT, V10, P99, DOI 10.1162/106365602320169811
NR 3
TC 1
Z9 1
U1 0
U2 4
PY 2021
VL 35
BP 15956
EP 15957
UT WOS:000681269807175
DA 2023-11-16
ER

PT C
AU Guo, Y
   Zhang, L
   Chen, Y
   Tong, X
   Liu, X
   Wang, Y
   Huang, X
   Ma, Z
AF Guo, Yufei
   Zhang, Liwen
   Chen, Yuanpei
   Tong, Xinyi
   Liu, Xiaode
   Wang, YingLei
   Huang, Xuhui
   Ma, Zhe
BE Avidan, S
   Brostow, G
   Cisse, M
   Farinella, GM
   Hassner, T
TI Real Spike: Learning Real-Valued Spikes for Spiking Neural Networks
SO COMPUTER VISION, ECCV 2022, PT XII
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 17th European Conference on Computer Vision (ECCV)
CY OCT 23-27, 2022
CL Tel Aviv, ISRAEL
DE Spiking neural network; Real spike; Binary spike;
   Training-inference-decoupled; Re-parameterization
ID PROCESSOR
AB Brain-inspired spiking neural networks (SNNs) have recently drawn more and more attention due to their event-driven and energy-efficient characteristics. The integration of storage and computation paradigm on neuromorphic hardwares makes SNNs much different from Deep Neural Networks (DNNs). In this paper, we argue that SNNs may not benefit from the weight-sharing mechanism, which can effectively reduce parameters and improve inference efficiency in DNNs, in some hardwares, and assume that an SNN with unshared convolution kernels could perform better. Motivated by this assumption, a training-inference decoupling method for SNNs named as Real Spike is proposed, which not only enjoys both unshared convolution kernels and binary spikes in inference-time but also maintains both shared convolution kernels and Real-valued Spikes during training. This decoupling mechanism of SNN is realized by a re-parameterization technique. Furthermore, based on the training-inference-decoupled idea, a series of different forms for implementing Real Spike on different levels are presented, which also enjoy shared convolutions in the inference and are friendly to both neuromorphic and non-neuromorphic hardware platforms. A theoretical proof is given to clarify that the Real Spike-based SNN network is superior to its vanilla counterpart. Experimental results show that all different Real Spike versions can consistently improve the SNN performance. Moreover, the proposed method outperforms the state-of-the-art models on both non-spiking static and neuromorphic datasets.
C1 [Guo, Yufei; Zhang, Liwen; Chen, Yuanpei; Tong, Xinyi; Liu, Xiaode; Wang, YingLei; Huang, Xuhui; Ma, Zhe] Intelligent Sci & Technol Acad CASIC, Beijing 100854, Peoples R China.
RP Huang, X; Ma, Z (corresponding author), Intelligent Sci & Technol Acad CASIC, Beijing 100854, Peoples R China.
EM yfguo@pku.edu.cn; starhxh@126.com; mazhethu@163.com
CR Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   Bing Han, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P388, DOI 10.1007/978-3-030-58607-2_23
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Carnevale NT., 2006, NEURON BOOK, DOI DOI 10.1017/CBO9780511541612
   Cheng X, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1519
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Fang W., 2021, ADV NEURAL INFORM PR, V34, P21056
   Fang W, 2021, INCORPORATING LEARNA, P2661
   Fang W., 2020, SPIKINGJELLY
   Goodman DFM, 2009, FRONT NEUROSCI-SWITZ, V3, P192, DOI 10.3389/neuro.01.026.2009
   Guo YF, 2022, PROC CVPR IEEE, P326, DOI 10.1109/CVPR52688.2022.00042
   Hao YZ, 2020, NEURAL NETWORKS, V121, P387, DOI 10.1016/j.neunet.2019.09.007
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huh D, 2018, ADV NEUR IN, V31
   Khan MM, 2008, IEEE IJCNN, P2849, DOI 10.1109/IJCNN.2008.4634199
   Kim J, 2018, NEUROCOMPUTING, V311, P373, DOI 10.1016/j.neucom.2018.05.087
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
   Krizhevsky A., 2010, CIFAR 10 CANADIAN I, V5, P1
   Kugele A, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00439
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee C, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00119
   Li HM, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00309
   Li Yan, 2021, arXiv
   Li Y., 2021, INT C MACHINE LEARNI, V139, P6316
   Lu S, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00535
   Ma D, 2017, J SYST ARCHITECT, V77, P43, DOI 10.1016/j.sysarc.2017.01.003
   Marc-Oliver G., 2007, SCHOLARPEDIA, DOI [DOI 10.4249/SCHOLARPEDIA.1430, 10.4249/scholarpedia.1430]
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Park S, 2020, DES AUT CON, DOI [10.1109/dac18072.2020.9218689, 10.1007/s00779-020-01476-2]
   Park S, 2019, PROCEEDINGS OF THE 2019 56TH ACM/EDAC/IEEE DESIGN AUTOMATION CONFERENCE (DAC), DOI 10.1145/3316781.3317822
   Pei J, 2019, NATURE, V572, P106, DOI 10.1038/s41586-019-1424-8
   Rathi N, 2020, Arxiv, DOI arXiv:2008.03658
   Rathi N, 2020, Arxiv, DOI arXiv:2005.01807
   Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Shrestha SB, 2018, ADV NEUR IN, V31
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Szegedy C., 2015, 2015 IEEE C COMPUTER, P1, DOI [10.1109/CVPR.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Wu YJ, 2019, AAAI CONF ARTIF INTE, P1311
   Yao P, 2020, NATURE, V577, P641, DOI 10.1038/s41586-020-1942-4
   Zhang W, 2020, ADV NEURAL INFORM PR, V33, P12022, DOI DOI 10.48550/ARXIV.2002.10085
   Zheng HL, 2021, AAAI CONF ARTIF INTE, V35, P11062
NR 43
TC 5
Z9 5
U1 2
U2 3
PY 2022
VL 13672
BP 52
EP 68
DI 10.1007/978-3-031-19775-8_4
UT WOS:000897093900004
DA 2023-11-16
ER

PT J
AU Zhou, Y
   Li, XY
   Wu, XY
   Zhao, YF
   Song, Y
AF Zhou, Ya
   Li, Xinyi
   Wu, Xiyan
   Zhao, Yufei
   Song, Yong
TI Object Detection Method with Spiking Neural Network Based on DT-LIF
   Neuron and SSD
SO JOURNAL OF ELECTRONICS & INFORMATION TECHNOLOGY
DT Article
DE Computer vision; Object detection; Spiking Neural Network (SNN); Neuron
AB Compared with traditional Artificial Neural Network (ANN), the Spiking Neural Network (SNN) has advantages of bioligical reliability and high computational efficiency. However, for object detection task, SNN has problems such as high training difficulty and low accuracy. In response to the above problems, an object detection method with SNN based on Dynamic Threshold Leaky Integrate-and-Fire (DT-LIF) neuron and Single Shot multibox Detector (SSD) is proposed. First, a DT-LIF neuron is designed, which can dynamically adjust the threshold of neuron according to the cumulative membrane potential to drive spike activity of the deep network and imporve the inferance speed. Meanwhile, using DT-LIF neuron as primitive, a hybrid SNN based on SSD is constructed. The network uses Spiking Visual Geometry Group (Spiking VGG) and Spiking Densely Connected Convolutional Network (Spiking DenseNet) as the backbone, and combines with SSD prediction head and three additional layers composed of Batch Normalization (BN) layer , Spiking Convolution (SC) layer, and DT-LIF neuron. Experimental results show that compared with LIF neuron network, the object detection accuracy of DT-LIF neuron network on the Prophesee GEN1 dataset is improved by 25.2%. Compared with the AsyNet algorithm, the object detection accuracy of the proposed method is improved by 17.9%.
C1 [Zhou, Ya; Li, Xinyi; Wu, Xiyan; Zhao, Yufei; Song, Yong] Beijing Inst Technol, Sch Opt & Photon, Beijing 100081, Peoples R China.
RP Song, Y (corresponding author), Beijing Inst Technol, Sch Opt & Photon, Beijing 100081, Peoples R China.
EM yongsong@bit.edu.cn
CR Azouz R, 2000, P NATL ACAD SCI USA, V97, P8110, DOI 10.1073/pnas.130200797
   Chakraborty B, 2021, IEEE T IMAGE PROCESS, V30, P9014, DOI 10.1109/TIP.2021.3122092
   de Tournemire P, 2020, Arxiv, DOI arXiv:2001.08499
   Diehl PU, 2015, IEEE IJCNN
   Dong XW, 2021, J ELECTRON INF TECHN, V43, P2113, DOI 10.11999/JEIT200450
   Fang W, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2641, DOI 10.1109/ICCV48922.2021.00266
   FANG Wei, 2021, 34 INT C NEUR INF PR, P21056
   Fontaine B, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003560
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   Gerstner W, 2002, SPIKING NEURON MODEL, DOI [10.1017/CBO9780511815706, DOI 10.1017/CBO9780511815706]
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   HE BB D O, 2002, ORG BEHAV NEUROPSYCH, DOI [10.4324/9781410612403, DOI 10.4324/9781410612403]
   He FS, 2020, J ELECTRON INF TECHN, V42, P119, DOI 10.11999/JEIT180899
   He KM, 2015, IEEE T PATTERN ANAL, V37, P1904, DOI 10.1109/TPAMI.2015.2389824
   [胡一凡 Hu Yifan], 2021, [控制与决策, Control and Decision], V36, P1
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Kim S, 2020, AAAI CONF ARTIF INTE, V34, P11270
   KUGELE A, 2022, 43 DAGM GERM C PATT, P297, DOI [10.1007/978-3-030-92659-5_19, DOI 10.1007/978-3-030-92659-5_19]
   Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Messikommer Nico, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P415, DOI 10.1007/978-3-030-58598-3_25
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tan MX, 2019, PR MACH LEARN RES, V97
   TOYOIZUMI T, 2004, 17 INT C NEUR INF PR, P1409
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Xiao R, 2020, IEEE T NEUR NET LEAR, V31, P3649, DOI 10.1109/TNNLS.2019.2945630
   Zhang DX, 2022, J ELECTRON INF TECHN, V44, P3249, DOI 10.11999/JEIT210664
   Zheng HL, 2021, AAAI CONF ARTIF INTE, V35, P11062
NR 31
TC 0
Z9 0
U1 1
U2 1
PD AUG
PY 2023
VL 45
IS 8
BP 2722
EP 2730
DI 10.11999/JEIT221367
UT WOS:001066225400005
DA 2023-11-16
ER

PT J
AU Slazynski, L
   Bohte, S
AF Slazynski, Leszek
   Bohte, Sander
TI Streaming parallel GPU acceleration of large-scale filter-based spiking
   neural networks
SO NETWORK-COMPUTATION IN NEURAL SYSTEMS
DT Article
DE GPU; spiking neural networks; spike response models; parallel computing
ID FIRE MODEL; NEURONS
AB The arrival of graphics processing (GPU) cards suitable for massively parallel computing promises affordable large-scale neural network simulation previously only available at supercomputing facilities. While the raw numbers suggest that GPUs may outperform CPUs by at least an order of magnitude, the challenge is to develop fine-grained parallel algorithms to fully exploit the particulars of GPUs. Computation in a neural network is inherently parallel and thus a natural match for GPU architectures: given inputs, the internal state for each neuron can be updated in parallel. We show that for filter-based spiking neurons, like the Spike Response Model, the additive nature of membrane potential dynamics enables additional update parallelism. This also reduces the accumulation of numerical errors when using single precision computation, the native precision of GPUs. We further show that optimizing simulation algorithms and data structures to the GPU's architecture has a large pay-off: for example, matching iterative neural updating to the memory architecture of the GPU speeds up this simulation step by a factor of three to five. With such optimizations, we can simulate in better-than-realtime plausible spiking neural networks of up to 50 000 neurons, processing over 35 million spiking events per second.
C1 [Slazynski, Leszek; Bohte, Sander] CWI, Dept Life Sci, NL-1098 XG Amsterdam, Netherlands.
RP Bohte, S (corresponding author), CWI, Dept Life Sci, Sci Pk 123, NL-1098 XG Amsterdam, Netherlands.
EM sander@bohte.com
CR AMD, 2012, AMD ACC PAR PROC OPE
   [Anonymous], 2006, ADV NEURAL INFORM PR
   [Anonymous], NVR2008003 NVIDIA
   Brette R, 2005, J NEUROPHYSIOL, V94, P3637, DOI 10.1152/jn.00686.2005
   Brette R, 2007, J COMPUT NEUROSCI, V23, P349, DOI 10.1007/s10827-007-0038-6
   Brette R, 2011, NEURAL COMPUT, V23, P1503, DOI 10.1162/NECO_a_00123
   Clopath C, 2007, NEUROCOMPUTING, V70, P1668, DOI 10.1016/j.neucom.2006.10.047
   Du P, 2012, PARALLEL COMPUT, V38, P391, DOI 10.1016/j.parco.2011.10.002
   Fidjeland AK, 2009, IEEE INT CONF ASAP, P137, DOI 10.1109/ASAP.2009.24
   Fidjeland Andreas K, 2010, 2010 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2010.5596678
   Gerstner W., 2002, SPIKING NEURON MODEL
   Goldman MS, 2009, NEURON, V61, P621, DOI 10.1016/j.neuron.2008.12.012
   Han B, 2010, THESIS U DAYTON
   Han B, 2010, APPL OPTICS, V49, pB83, DOI 10.1364/AO.49.000B83
   Harris M, 2008, OPTIMIZING PARALLEL
   HODGKIN AL, 1952, PROC R SOC SER B-BIO, V140, P177, DOI 10.1098/rspb.1952.0054
   Jianbin Fang, 2011, 2011 International Conference on Parallel Processing, P216, DOI 10.1109/ICPP.2011.45
   Krishnamani P, 2010, THESIS CLEMSON U
   Lazar AA, 2012, NEURAL NETWORKS, V32, P303, DOI 10.1016/j.neunet.2012.02.007
   Markram H, 2006, NAT REV NEUROSCI, V7, P153, DOI 10.1038/nrn1848
   Mutch J., 2010, CNS GPU BASED FRAMEW
   Nageswaran JM, 2008, ARCHITECTURE
   Naud R, 2011, THESIS EPFL LAUSANNE
   NVidia, 2012, OPENCL PROGR GUID CU
   Oberlaender M., 2011, CEREBRAL CORTEX
   Owens JD, 2008, P IEEE, V96, P879, DOI 10.1109/JPROC.2008.917757
   Sengupta S., 2008, EFFICIENT PARALLEL S
   Stone JE, 2010, COMPUT SCI ENG, V12, P66, DOI 10.1109/MCSE.2010.69
   Vekterli T, 2009, THESIS NORWEGIAN U S
   Yudanov D, 2009, THESIS ROCHESTER I T
   Yudanov D, 2010, WCCI 2010 IEEE WORLD
NR 31
TC 6
Z9 6
U1 1
U2 9
PY 2012
VL 23
IS 4
BP 183
EP 211
DI 10.3109/0954898X.2012.733842
UT WOS:000311837300006
DA 2023-11-16
ER

PT J
AU Liang, ZZ
   Schwartz, D
   Ditzler, G
   Koyluoglu, OO
AF Liang, Zhengzhong
   Schwartz, David
   Ditzler, Gregory
   Koyluoglu, O. Ozan
TI The impact of encoding-decoding schemes and weight normalization in
   spiking neural networks
SO NEURAL NETWORKS
DT Article
DE Spiking neural network; Spike-timing dependent plasticity; Learning
   window; Encoding; Decoding; Normalization
ID MODEL; NEURONS
AB Spike-timing Dependent Plasticity (STDP) is a learning mechanism that can capture causal relationships between events. STDP is considered a foundational element of memory and learning in biological neural networks. Previous research efforts endeavored to understand the functionality of STDP's learning window in spiking neural networks (SNNs). In this study, we investigate the interaction among different encoding/ decoding schemes, STDP learning windows and normalization rules for the SNN classifier, trained and tested on MNIST, NIST and ETH80-Contour datasets. The results show that when no normalization rules are applied, classical STDP typically achieves the best performance. Additionally, first-spike decoding classifiers require much less decoding time than a spike count decoding classifier. Thirdly, when no normalization rule is applied, the classifier accuracy decreases as the encoding duration increases from 10 ms to 34 ms using count decoding scheme. Finally, normalization of output weights is shown to improve the performance of a first-spike decoding classifier, which reveals the importance of weight normalization to SNN. (C) 2018 Elsevier Ltd. All rights reserved.
C1 [Liang, Zhengzhong; Schwartz, David; Ditzler, Gregory] Univ Arizona, Dept Elect & Comp Engn, Tucson, AZ 85721 USA.
   [Koyluoglu, O. Ozan] Univ Calif Berkeley, EECS, Berkeley, CA 94720 USA.
RP Liang, ZZ (corresponding author), Univ Arizona, Dept Elect & Comp Engn, Tucson, AZ 85721 USA.
EM zhengzhongliang@email.arizona.edu; dmschwar@email.arizona.edu;
   ditzler@email.arizona.edu; ozan.koyluoglu@berkeley.edu
CR Abarbanel HDI, 2002, P NATL ACAD SCI USA, V99, P10132, DOI 10.1073/pnas.132651299
   [Anonymous], ARXIV170509132
   BARLOW HB, 1992, OPTICAL SOC AM TECHN, V23, P172
   Bengio Yoshua, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P437, DOI 10.1007/978-3-642-35289-8_26
   Bi GQ, 2001, ANNU REV NEUROSCI, V24, P139, DOI 10.1146/annurev.neuro.24.1.139
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Buchanan K. A., 2010, FRONTIERS SYNAPTIC N, V2
   Caporale N, 2008, ANNU REV NEUROSCI, V31, P25, DOI 10.1146/annurev.neuro.31.060407.125639
   CURCIO CA, 1990, J COMP NEUROL, V292, P497, DOI 10.1002/cne.902920402
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Esser S. K., 2015, ADV NEURAL INFORM PR, P1117
   Fregnac Y., 2010, FRONTIERS SYNAPTIC N, V2
   Gerstner W, 2002, BIOL CYBERN, V87, P404, DOI 10.1007/s00422-002-0353-y
   Gerstner W., 2002, SPIKING NEURON MODEL
   Hearst MA, 1998, IEEE INTELL SYST APP, V13, P18, DOI 10.1109/5254.708428
   HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2007, CEREB CORTEX, V17, P2443, DOI 10.1093/cercor/bhl152
   Jaderberg M., 2017, ARXIV171109846
   Kasabov N, 2013, NEURAL NETWORKS, V41, P188, DOI 10.1016/j.neunet.2012.11.014
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lorenzo PR, 2017, PROCEEDINGS OF THE 2017 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE (GECCO'17), P481, DOI 10.1145/3071178.3071208
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Masquelier T, 2007, PLOS COMPUT BIOL, V3, P247, DOI 10.1371/journal.pcbi.0030031
   Mishra RK, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms11552
   Olshausen BA, 2003, J COGNITIVE NEUROSCI, V15, P154, DOI 10.1162/089892903321107891
   Ponulak F., 2011, ACTA NEUROBIOLOGIAE, V4
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   RASMUSSEN CE, 1993, BIOL CYBERN, V68, P409, DOI 10.1007/BF00198773
   Scarpetta Silvia, 2010, Front Synaptic Neurosci, V2, P32, DOI 10.3389/fnsyn.2010.00032
   Shouval HZ, 2010, FRONT COMPUT NEUROSC, V4, DOI 10.3389/fncom.2010.00019
   Sjostrom P. J., 2011, FRONTIERS SYNAPTIC N, V3
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Stimberg M, 2014, FRONT NEUROINFORM, V8, DOI 10.3389/fninf.2014.00006
   Watson AB, 2014, J VISION, V14, DOI 10.1167/14.7.15
   WILLSHAW DJ, 1979, PHILOS T ROY SOC B, V287, P203, DOI 10.1098/rstb.1979.0056
   Wysoski SG, 2010, NEURAL NETWORKS, V23, P819, DOI 10.1016/j.neunet.2010.04.009
NR 38
TC 9
Z9 9
U1 1
U2 11
PD DEC
PY 2018
VL 108
BP 365
EP 378
DI 10.1016/j.neunet.2018.08.024
UT WOS:000450298900026
DA 2023-11-16
ER

PT J
AU Tonnelier, A
AF Tonnelier, Arnaud
TI Propagation of Spike Sequences in Neural Networks
SO SIAM JOURNAL ON APPLIED DYNAMICAL SYSTEMS
DT Article
DE spiking neuron; propagation; traveling wave; spike sequence
ID TRAVELING-WAVES; SYNCHRONOUS SPIKING; STABLE PROPAGATION; NEURONAL
   NETWORKS; PULSES; PATTERNS; CORTEX
AB Precise spatiotemporal sequences of action potentials are observed in many brain areas and are thought to be involved in the neural processing of sensory stimuli. Here, we examine the ability of spiking neural networks to propagate stably a spatiotemporal sequence of spikes in the limit where each neuron fires only one spike. In contrast to previous studies on propagation in neural networks, we assume only homogeneous connectivity and do not use the continuum approximation. When the propagation is associated with a simple traveling wave, or a one-spike sequence, we derive some analytical results for the wave speed and show that its stability is determined by the Schur criterion. The propagation of a sequence of several spikes corresponds to the existence of stable composite waves, i.e., stable spatiotemporal periodic traveling waves. The stability of composite waves is related to the roots of a system of multivariate polynomials. Using the simplest synaptic architecture that supports composite waves, a three nearest-neighbor coupling feedforward network, we analytically and numerically investigate the propagation of 2-composite waves, i.e., two-spike sequence propagation. The influence of the synaptic coupling, stochastic perturbations, and neuron parameters on the propagation of larger sequences is also investigated.
C1 INRIA Grenoble Rhone Alpes, Lab Jean Kuntzmann, F-38334 Montbonnot St Martin, Saint Ismier, France.
RP Tonnelier, A (corresponding author), INRIA Grenoble Rhone Alpes, Lab Jean Kuntzmann, Inovallee 655 Ave Europe, F-38334 Montbonnot St Martin, Saint Ismier, France.
EM arnaud.tonnelier@inria.fr
CR ABELES M, 1993, J NEUROPHYSIOL, V70, P1629, DOI 10.1152/jn.1993.70.4.1629
   Aertsen A, 1996, J PHYSIOLOGY-PARIS, V90, P243, DOI 10.1016/S0928-4257(97)81432-5
   Bressloff PC, 2000, J MATH BIOL, V40, P169, DOI 10.1007/s002850050008
   Bressloff PC, 1999, PHYS REV LETT, V82, P2979, DOI 10.1103/PhysRevLett.82.2979
   Coombes S, 2004, SIAM J APPL DYN SYST, V3, P574, DOI 10.1137/040605953
   Corless RM, 1996, ADV COMPUT MATH, V5, P329, DOI 10.1007/BF02124750
   Destexhe A, 1996, J NEUROPHYSIOL, V76, P2049, DOI 10.1152/jn.1996.76.3.2049
   Diesmann M, 1999, NATURE, V402, P529, DOI 10.1038/990101
   Ermentrout B, 1998, J COMPUT NEUROSCI, V5, P191, DOI 10.1023/A:1008822117809
   Feinerman O, 2005, J NEUROPHYSIOL, V94, P3406, DOI 10.1152/jn.00264.2005
   Galarreta M, 2001, SCIENCE, V292, P2295, DOI 10.1126/science.1061395
   Gerstner W, 1996, NEURAL COMPUT, V8, P1653, DOI 10.1162/neco.1996.8.8.1653
   Gerstner W., 2002, SPIKING NEURON MODEL
   Golomb D, 1999, P NATL ACAD SCI USA, V96, P13480, DOI 10.1073/pnas.96.23.13480
   Golomb D, 1996, J NEUROPHYSIOL, V75, P750, DOI 10.1152/jn.1996.75.2.750
   Hahnloser RHR, 2006, J NEUROPHYSIOL, V96, P794, DOI 10.1152/jn.01064.2005
   HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554
   Ikegaya Y, 2004, SCIENCE, V304, P559, DOI 10.1126/science.1093173
   Izhikevich EM, 2009, INT J BIFURCAT CHAOS, V19, P1733, DOI 10.1142/S0218127409023809
   JEFFRESS LA, 1948, J COMP PHYSIOL PSYCH, V41, P35, DOI 10.1037/h0061495
   Jin DZ, 2004, PHYS REV E, V69, DOI 10.1103/PhysRevE.69.021905
   Jin DZ, 2002, PHYS REV LETT, V89, DOI 10.1103/PhysRevLett.89.208102
   Johansson RS, 2004, NAT NEUROSCI, V7, P170, DOI 10.1038/nn1177
   Kistler WM, 2002, NEURAL COMPUT, V14, P987, DOI 10.1162/089976602753633358
   LAURENT G, 1994, SCIENCE, V265, P1872, DOI 10.1126/science.265.5180.1872
   Masuda N, 2002, NEURAL COMPUT, V14, P1599, DOI 10.1162/08997660260028638
   Nádasdy Z, 1999, J NEUROSCI, V19, P9497
   Osan R, 2004, J MATH BIOL, V48, P243, DOI 10.1007/s00285-003-0228-4
   Osan R, 2002, PHYSICA D, V163, P217, DOI 10.1016/S0167-2789(02)00347-0
   Pinto DJ, 2005, SIAM J APPL DYN SYST, V4, P954, DOI 10.1137/040613020
   Pinto DJ, 2001, SIAM J APPL MATH, V62, P206, DOI 10.1137/S0036139900346453
   Rinzel J, 1998, SCIENCE, V279, P1351, DOI 10.1126/science.279.5355.1351
   Rolston JD, 2007, NEUROSCIENCE, V148, P294, DOI 10.1016/j.neuroscience.2007.05.025
   Stegun Irene A, 1964, NBS APPL MATH SER, V55
   Thorpe S, 1996, NATURE, V381, P520, DOI 10.1038/381520a0
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   Troy WC, 2008, SIAM J APPL DYN SYST, V7, P1247, DOI 10.1137/070709888
   VanRullen R, 2005, TRENDS NEUROSCI, V28, P1, DOI 10.1016/j.tins.2004.10.010
   Watt AJ, 2009, NAT NEUROSCI, V12, P463, DOI 10.1038/nn.2285
NR 39
TC 5
Z9 5
U1 0
U2 2
PY 2010
VL 9
IS 3
BP 1090
EP 1118
DI 10.1137/100782504
UT WOS:000285549800013
DA 2023-11-16
ER

PT J
AU Guo, L
   Zhang, W
   Zhang, JL
AF Guo, Lei
   Zhang, Wei
   Zhang, Jialei
TI The effect of an exogenous alternating magnetic field on neural coding
   in deep spiking neural networks
SO JOURNAL OF INTEGRATIVE NEUROSCIENCE
DT Article
DE Spiking neural network; neural coding; reduced neuron model
ID BRAIN; MODEL
AB A ten-layer feed forward network was constructed in the presence of an exogenous alternating magnetic field. Results indicate that for rate coding, the firing rate is increased in the presence of an exogenous alternating magnetic field and particularly with increasing enhancement of the alternating magnetic field amplitude. For temporal coding, in the presence of alternating magnetic field, the interspike intervals of the spiking sequence are decreased and the distribution of interspike intervals tends to be uniform.
C1 [Guo, Lei; Zhang, Wei; Zhang, Jialei] Hebei Univ Technol, State Key Lab Reliabil & Intelligence Elect Equip, Tianjin 300130, Peoples R China.
   [Guo, Lei; Zhang, Wei; Zhang, Jialei] Hebei Univ Technol, Key Lab Electromagnet Field & Elect Apparat Relia, Tianjin 300130, Peoples R China.
RP Guo, L (corresponding author), Hebei Univ Technol, State Key Lab Reliabil & Intelligence Elect Equip, Tianjin 300130, Peoples R China.; Guo, L (corresponding author), Hebei Univ Technol, Key Lab Electromagnet Field & Elect Apparat Relia, Tianjin 300130, Peoples R China.
EM 2004008@hebut.edu.cn
CR Alves-Pinto A, 2014, FRONT NEUROSCI-SWITZ, V8, DOI 10.3389/fnins.2014.00112
   Bédard C, 2006, PHYS REV E, V73, DOI 10.1103/PhysRevE.73.051911
   Camera F, 2013, I IEEE EMBS C NEUR E, P223, DOI 10.1109/NER.2013.6695912
   Davey K, 2003, CLIN NEUROPHYSIOL, V114, P2204, DOI 10.1016/S1388-2457(03)00240-2
   Djurfeldt M, 2008, IBM J RES DEV, V52, P31, DOI 10.1147/rd.521.0031
   Ebrahimian H, 2013, J ARDABIL U MED SCI, V13, P119
   Giannì M, 2006, BIOL CYBERN, V94, P118, DOI 10.1007/s00422-005-0029-5
   Graben PB, 2008, J COMPUTATIONAL STRU, V27, P297
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Jiang XY, 2012, CHIN CONTR CONF, P7355
   Jones HG, 2015, J NEUROPHYSIOL, V114, P531, DOI 10.1152/jn.00062.2015
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Modolo J., 2010, 2010 IEEE Fifth International Conference on Bio-Inspired Computing: Theories and Applications (BIC-TA), P1356, DOI 10.1109/BICTA.2010.5645067
   Paoli M, 2016, EUR J NEUROSCI, V44, P2387, DOI 10.1111/ejn.13344
   Radman T, 2009, BRAIN STIMUL, V2, P215, DOI 10.1016/j.brs.2009.03.007
   Rosenbaum R, 2014, NEUROBIOL DIS, V62, P86, DOI 10.1016/j.nbd.2013.09.006
   Ruohonen J, 1997, J Peripher Nerv Syst, V2, P17
   Tsubo Y, 2012, PLOS COMPUT BIOL, V8, DOI 10.1371/journal.pcbi.1002461
   Wang HT, 2016, NONLINEAR DYNAM, V85, P881, DOI 10.1007/s11071-016-2730-4
   Won JH, 2016, J ACOUST SOC AM, V139, P1, DOI 10.1121/1.4931909
   Yang Y, 2016, J NEUROSCI, V36, P11999, DOI 10.1523/JNEUROSCI.1475-16.2016
   Yi G, 2014, INT J NEURAL SYST, V24, P527
   [于凯 Yu Kai], 2013, [天津大学学报. 自然科学与工程技术版, Journal of Tianjin University], V46, P726
NR 24
TC 1
Z9 1
U1 0
U2 11
PY 2018
VL 17
IS 2
BP 97
EP 104
DI 10.31083/JIN-170046
UT WOS:000450607600002
DA 2023-11-16
ER

PT C
AU Tavanaei, A
   Kirby, Z
   Maida, AS
AF Tavanaei, Amirhossein
   Kirby, Zachary
   Maida, Anthony S.
GP IEEE
TI Training Spiking ConvNets by STDP and Gradient Descent
SO 2018 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUL 08-13, 2018
CL Rio de Janeiro, BRAZIL
ID VISUAL FEATURES; NEURAL-NETWORKS; MODEL
AB This paper proposes a new method for training multi-layer spiking convolutional neural networks (CNNs). Training a multi-layer spiking network poses difficulties because the output spikes do not have derivatives and the commonly used backpropagation method for non-spiking networks is not easily applied. Our method uses a novel version of layered spike-timing-dependent plasticity (STDP) that incorporates supervised and unsupervised components. Our method starts with conventional learning methods and converts them to spatio-temporally local rules suited for spiking neural networks (SNNs).
   The training process uses two components for unsupervised feature extraction and supervised classification. The first component is a new STDP rule for spike-based representation learning which trains convolutional filters. The second introduces a new STDP-based supervised learning rule for spike pattern classification via an approximation to gradient descent. Stacking these components implements a novel spiking CNN of integrate-and-fire (IF) neurons with performances comparable with the state-of-the-art deep SNNs. The experimental results show the success of the proposed model for the MNIST handwritten digit classification. Our network architecture is the only high performance, spiking CNN which provides bio-inspired STDP rules in a hierarchy of feature extraction and classification in an entirely spike-based framework.
C1 [Tavanaei, Amirhossein; Kirby, Zachary; Maida, Anthony S.] Univ Louisiana Lafayette, Sch Comp & Informat, Lafayette, LA 70504 USA.
RP Tavanaei, A (corresponding author), Univ Louisiana Lafayette, Sch Comp & Informat, Lafayette, LA 70504 USA.
EM tavanaei@louisiana.edu; zjk2775@louisiana.edu; maida@louisiana.edu
CR [Anonymous], 2017, ARXIV170602609
   [Anonymous], 2016, PROC AUSTRALAS TRANS
   [Anonymous], 2016, ARXIV161101421
   [Anonymous], 2017, ARXIV170604698
   Beyeler M, 2013, NEURAL NETWORKS, V48, P109, DOI 10.1016/j.neunet.2013.07.012
   Burbank KS, 2015, PLOS COMPUT BIOL, V11, DOI 10.1371/journal.pcbi.1004566
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Coates Adam, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P561, DOI 10.1007/978-3-642-35289-8_30
   DeFelipe J, 2012, FRONT NEUROANAT, V6, DOI [10.3389/fnana.2012.00022, 10.3389/fnsyn.2012.00002, 10.3389/fnana.2012.00005]
   Diehl P.U., 2015, 2015 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2015.7280696
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Eliasmith C, 2012, SCIENCE, V338, P1202, DOI 10.1126/science.1225266
   Esser S. K., 2015, ADV NEURAL INFORM PR, P1117
   Foldiak P., 1989, IJCNN: International Joint Conference on Neural Networks (Cat. No.89CH2765-6), P401, DOI 10.1109/IJCNN.1989.118615
   FOLDIAK P, 1990, BIOL CYBERN, V64, P165, DOI 10.1007/BF02331346
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Hunsberger E., 2016, TRAINING SPIKING DEE
   Kasabov N, 2013, NEURAL NETWORKS, V41, P188, DOI 10.1016/j.neunet.2012.11.014
   Kasabov NK, 2014, NEURAL NETWORKS, V52, P62, DOI 10.1016/j.neunet.2014.01.006
   Kheradpisheh SR, 2016, NEUROCOMPUTING, V205, P382, DOI 10.1016/j.neucom.2016.04.029
   King PD, 2013, J NEUROSCI, V33, P5475, DOI 10.1523/JNEUROSCI.4188-12.2013
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y, 1998, THE MNIST DATABASE
   Lee JM, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00191
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maass W, 2015, P IEEE, V103, P2219, DOI 10.1109/JPROC.2015.2496679
   Masquelier T, 2007, PLOS COMPUT BIOL, V3, P247, DOI 10.1371/journal.pcbi.0030031
   Nasrabadi N.M., 2007, PATTERN RECOGN, V16, DOI 10.1117/1.2819119
   Neftci E, 2014, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00272
   Neil D, 2016, P 31 ANN ACM S APPL
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   OConnor P., 2016, ARXIV160208323
   Panda P, 2016, IEEE IJCNN, P299, DOI 10.1109/IJCNN.2016.7727212
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Tavanaei A., 2017, ARXIV171104214
   Tavanaei A., 2017, ARXIV170606699
   Tavanaei A, 2017, IEEE IJCNN, P2023, DOI 10.1109/IJCNN.2017.7966099
   Zylberberg J, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1002250
NR 42
TC 0
Z9 0
U1 1
U2 8
PY 2018
UT WOS:000585967401101
DA 2023-11-16
ER

PT C
AU Pharn, DT
   Packianather, MS
   Charles, EYA
AF Pharn, D. T.
   Packianather, M. S.
   Charles, E. Y. A.
GP IEEE
TI A self-organising spiking neural network trained using delay adaptation
SO 2007 IEEE INTERNATIONAL SYMPOSIUM ON INDUSTRIAL ELECTRONICS,
   PROCEEDINGS, VOLS 1-8
DT Proceedings Paper
CT IEEE International Symposium on Industrial Electronics
CY JUN 04-07, 2007
CL Vigo, SPAIN
ID PATTERN-RECOGNITION; COMPUTATIONAL POWER; BACKPROPAGATION; DYNAMICS;
   NEURONS
AB This paper proposes a self-organising delay adaptation spiking neural network model for clustering control chart patterns. This temporal coding spiking neural network model employs a Hebbian-based rule to shift the connection delays instead of the previous approaches of delay selection. Here the tuned delays compensate the differences in the input firing times of temporal patterns and enables them to coincide. The coincidence detection capability of the spiking neuron has been utilised for pattern detection. The structure of the network is similar to that of a Kohonen's Self-Organising Map (SOM) except that the output layer neurons are coincidence detecting spiking neurons. An input pattern is represented by the neuron that is the first to fire among all the competing spiking neurons. Clusters within the input data are identified with the location of the winning neurons and their firing times.
   The proposed spiking neural network has been utilised to cluster SPC control chart patterns. The trained network obtained an average clustering accuracy of 96.1% on previously unseen test data. This was achieved with a network of W spiking neurons trained for 20 epochs containing 1000 training examples. The clustering accuracy of the proposed model was found to be better than that of Kohonen's SOM.
C1 [Pharn, D. T.; Packianather, M. S.; Charles, E. Y. A.] Cardiff Univ, MEC, Cardiff, Wales.
RP Pharn, DT (corresponding author), Cardiff Univ, MEC, Cardiff, Wales.
EM phamdt@cf.ac.uk; packianathenns@cf.ac.uk; charlesey@cf.ac.uk
CR BALDI P, 1994, IEEE T NEURAL NETWOR, V5, P612, DOI 10.1109/72.298231
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   DAY SP, 1993, IEEE T NEURAL NETWOR, V4, P348, DOI 10.1109/72.207622
   Eurich CW, 1999, PHYS REV LETT, V82, P1594, DOI 10.1103/PhysRevLett.82.1594
   Gerstner W, 1996, NATURE, V383, P76, DOI 10.1038/383076a0
   Haykin S., 1999, NEURAL NETWORKS COMP
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   Jahnke A, 1998, PULSED NEURAL NETWORKS, P237
   Konig P, 1996, TRENDS NEUROSCI, V19, P130, DOI 10.1016/S0166-2236(96)80019-1
   Maass W, 1998, PULSED NEURAL NETWORKS, P55
   Maass W, 1996, NEURAL COMPUT, V8, P1, DOI 10.1162/neco.1996.8.1.1
   Maass W, 1997, NEURAL COMPUT, V9, P279, DOI 10.1162/neco.1997.9.2.279
   Maass W, 1997, ADV NEUR IN, V9, P211
   Natschlager T, 1998, NETWORK-COMP NEURAL, V9, P319, DOI 10.1088/0954-898X/9/3/003
   PANCHEV C, 2001, P WORLD C NEUR
   PHAM DT, 1994, INT J PROD RES, V32, P721, DOI 10.1080/00207549408956963
   Pham DT, 2001, INT J MACH TOOL MANU, V41, P419, DOI 10.1016/S0890-6955(00)00073-0
   Pham DT, 1998, SOFT COMPUTING IN ENGINEERING DESIGN AND MANUFACTURING, P381
   Pham DT, 1998, P I MECH ENG I-J SYS, V212, P115, DOI 10.1243/0959651981539343
   PHAM DT, 1994, J PROCESS MECH ENG, V207, P113
   PHAN DT, 2006, INTELLIGENT PRODUCTI, P307
   Ruf B, 1997, NEURAL PROCESS LETT, V5, P9, DOI 10.1023/A:1009697008681
   Ruf B, 1998, IEEE T NEURAL NETWOR, V9, P575, DOI 10.1109/72.668899
   Tao XL, 2004, IC-AI '04 & MLMTA'04 , VOL 1 AND 2, PROCEEDINGS, P168
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   Tversky T, 2002, NEUROCOMPUTING, V44, P679, DOI 10.1016/S0925-2312(02)00457-5
   Ultsch A., 2005, ESOM MAPS TOOLS CLUS
   Zador AM, 2000, NAT NEUROSCI, V3, P1167, DOI 10.1038/81432
NR 29
TC 12
Z9 16
U1 0
U2 2
PY 2007
BP 3441
EP 3446
DI 10.1109/ISIE.2007.4375170
UT WOS:000252265107075
DA 2023-11-16
ER

PT J
AU Olin-Ammentorp, W
   Beckmann, K
   Schuman, CD
   Plank, JS
   Cady, NC
AF Olin-Ammentorp, Wilkie
   Beckmann, Karsten
   Schuman, Catherine D.
   Plank, James S.
   Cady, Nathaniel C.
TI Stochasticity and robustness in spiking neural networks
SO NEUROCOMPUTING
DT Article
DE Spiking neural networks; Synaptic devices; Memristors; Robustness;
   Non-deterministic networks; Stochastic networks; ReRAM
ID NOISE
AB Despite drawing inspiration from biological systems which are inherently noisy and variable, artificial neural networks have been shown to require precise weights to carry out the task which they are trained to accomplish. This creates a challenge when adapting these artificial networks to specialized execution platforms which may encode weights in a manner which restricts their accuracy and/or precision.
   Reflecting back on the non-idealities which are observed in biological systems, we investigated the effect these properties have on the robustness of spiking neural networks under perturbations to weights. First, we examined techniques extant in conventional neural networks which resemble noisy processes, and postulated they may produce similar beneficial effects in spiking neural networks. Second, we evolved a set of spiking neural networks utilizing biological non-idealities to solve a pole-balancing task, and estimated their robustness. We showed it is higher in networks using noisy neurons, and demon-strated that one of these networks can perform well under the variance expected when a hafnium oxide based resistive memory is used to encode synaptic weights. Lastly, we trained a series of networks using a surrogate gradient method on the MNIST classification task. We confirmed that these networks demonstrate similar trends in robustness to the evolved networks. We discuss these results and argue that they display empirical evidence supporting the role of noise as a regularizer which can increase network robustness. (C) 2020 Elsevier B.V. All rights reserved.
C1 [Olin-Ammentorp, Wilkie; Beckmann, Karsten; Cady, Nathaniel C.] SUNY Albany, Polytech Inst, 257 Fuller Rd, Albany, NY 12203 USA.
   [Schuman, Catherine D.] Oak Ridge Natl Lab, 1 Bethel Valley Rd, Oak Ridge, TN 37830 USA.
   [Plank, James S.] Univ Tennessee, Knoxville, TN 37996 USA.
RP Cady, NC (corresponding author), SUNY Albany, Polytech Inst, 257 Fuller Rd, Albany, NY 12203 USA.
EM ncady@sunypoly.edu
CR Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   [Anonymous], 2015, RESISTIVE SWITCHING
   Baldi P, 2013, ADV NEURAL INFORM PR, P2814
   Beckmann K, 2016, MRS ADV, V1, P3355, DOI 10.1557/adv.2016.377
   Branco T, 2009, NAT REV NEUROSCI, V10, P373, DOI 10.1038/nrn2634
   Buesing L, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1002211
   Burkitt AN, 2006, BIOL CYBERN, V95, P1, DOI 10.1007/s00422-006-0068-6
   CALVIN WH, 1968, J NEUROPHYSIOL, V31, P574, DOI 10.1152/jn.1968.31.4.574
   Chen LR, 2017, DES AUT TEST EUROPE, P19, DOI 10.23919/DATE.2017.7926952
   Coleman C., 2017, 31 ANN C NEUR INF PR
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Diehl PU, 2016, IEEE IJCNN, P4278, DOI 10.1109/IJCNN.2016.7727758
   Faisal AA, 2008, NAT REV NEUROSCI, V9, P292, DOI 10.1038/nrn2258
   Gaier A., 2019, WEIGHT AGNOSTIC NEUR
   Gal Y, 2016, PR MACH LEARN RES, V48
   Gerstner W., 2002, SPIKING NEURON MODEL
   Goodfellow I., 2015, CORR ABS14126572
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   He X, 2018, I SYMPOS LOW POWER E, P110, DOI 10.1145/3218603.3218643
   Hochreiter S., 1995, Advances in Neural Information Processing Systems 7, P529
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Ioffe S., 2015, INT C MACH LEARN ICM, P448
   Kannan S, 2015, IEEE T COMPUT AID D, V34, P822, DOI 10.1109/TCAD.2015.2394434
   Kurakin A., 2019, 5 INT C LEARNING REP, P1
   Kuzum D, 2013, NANOTECHNOLOGY, V24, DOI 10.1088/0957-4484/24/38/382001
   Kuzum D, 2012, NANO LETT, V12, P2179, DOI 10.1021/nl201040y
   LeCun Y., 1998, MNIST DATABASE HANDW
   Li BX, 2015, DES AUT CON, DOI 10.1145/2744769.2744870
   Li H, 2018, ADV NEUR IN, V31
   Liu BY, 2015, DES AUT CON, DOI 10.1145/2744769.2744930
   LiWan Matthew Zeiler, 2013, P 30 INT C MACH LEAR, P1058
   Mack S, 2013, PRINCIPLES NEURAL SC
   Neftci E.O., 2019, SURROGATE GRADIENT L, P1
   Olin-Ammentorp W., 2017, GOMACTECH P
   Olin-Ammentorp W., 2018, GOMACTECH P, P41
   Pakkenberg B, 2003, EXP GERONTOL, V38, P95, DOI 10.1016/S0531-5565(02)00151-1
   Pei J, 2019, NATURE, V572, P106, DOI 10.1038/s41586-019-1424-8
   Plank J. S., 2018, IEEE LETT COMPUTER S, V1, P17, DOI DOI 10.1109/LOCS.2018.2885976
   Plesser HE, 2000, NEURAL COMPUT, V12, P367, DOI 10.1162/089976600300015835
   Poole B., 2014, ANAL NOISE AUTOENCOD, P1
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Sangwan VK, 2020, NAT NANOTECHNOL, V15, P517, DOI 10.1038/s41565-020-0647-z
   Schuman C.D., 2015, NEUROSCI INSPIRED DY, P13
   Schuman C.D., 2019, 2018 31 IEEE INT SYS, P37, DOI [10.1109/socc.2018.8618553., DOI 10.1109/SOCC.2018.8618553]
   Schumann CL, 2019, AIDS BEHAV, V23, P5, DOI 10.1007/s10461-017-1727-4
   SIETSMA J, 1991, NEURAL NETWORKS, V4, P67, DOI 10.1016/0893-6080(91)90033-2
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Stanley KO, 2002, EVOL COMPUT, V10, P99, DOI 10.1162/106365602320169811
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Tramonti F, 2019, PSYCHOL HEALTH MED, V24, P27, DOI 10.1080/13548506.2018.1510131
   Yuste R, 2015, NAT REV NEUROSCI, V16, P487, DOI 10.1038/nrn3962
   ZHANG ML, 2017, NEUROCOMPUTING, V219, P333, DOI DOI 10.1016/J.NEUC0M.2016.09.044
NR 52
TC 5
Z9 5
U1 6
U2 25
PD JAN 2
PY 2021
VL 419
BP 23
EP 36
DI 10.1016/j.neucom.2020.07.105
UT WOS:000590175500003
DA 2023-11-16
ER

PT J
AU Taherkhani, A
   Belatreche, A
   Li, YH
   Cosma, G
   Maguire, LP
   McGinnity, TM
AF Taherkhani, Aboozar
   Belatreche, Ammar
   Li, Yuhua
   Cosma, Georgina
   Maguire, Liam P.
   McGinnity, T. M.
TI A review of learning in biologically plausible spiking neural networks
SO NEURAL NETWORKS
DT Review
DE Spiking neural network (SNN); Learning; Synaptic plasticity
ID TIMING-DEPENDENT PLASTICITY; LIQUID-STATE-MACHINE; NEUROTRANSMITTER
   RELEASE; GRADIENT DESCENT; VISUAL FEATURES; TIME-COURSE; NEURONS; MODEL;
   DELAY; ALGORITHM
AB Artificial neural networks have been used as a powerful processing tool in various areas such as pattern recognition, control, robotics, and bioinformatics. Their wide applicability has encouraged researchers to improve artificial neural networks by investigating the biological brain. Neurological research has significantly progressed in recent years and continues to reveal new characteristics of biological neurons. New technologies can now capture temporal changes in the internal activity of the brain in more detail and help clarify the relationship between brain activity and the perception of a given stimulus. This new knowledge has led to a new type of artificial neural network, the Spiking Neural Network (SNN), that draws more faithfully on biological properties to provide higher processing abilities. A review of recent developments in learning of spiking neurons is presented in this paper. First the biological background of SNN learning algorithms is reviewed. The important elements of a learning algorithm such as the neuron model, synaptic plasticity, information encoding and SNN topologies are then presented. Then, a critical review of the state-of-the-art learning algorithms for SNNs using single and multiple spikes is presented. Additionally, deep spiking neural networks are reviewed, and challenges and opportunities in the SNN field are discussed. (c) 2019 Elsevier Ltd. All rights reserved.
C1 [Taherkhani, Aboozar] De Montfort Univ, Fac Comp Engn & Media, Sch Comp Sci & Informat, Leicester, Leics, England.
   [Belatreche, Ammar] Northumbria Univ, Dept Comp & Informat Sci, Newcastle Upon Tyne, Tyne & Wear, England.
   [Li, Yuhua] Cardiff Univ, Sch Comp Sci & Informat, Cardiff, S Glam, Wales.
   [Cosma, Georgina] Univ Loughborough, Dept Comp Sci, Loughborough, Leics, England.
   [Maguire, Liam P.; McGinnity, T. M.] Ulster Univ, Intelligent Syst Res Ctr, Derry, North Ireland.
   [McGinnity, T. M.] Nottingham Trent Univ, Sch Sci & Technol, Nottingham, England.
RP Taherkhani, A (corresponding author), De Montfort Univ, Fac Comp Engn & Media, Sch Comp Sci & Informat, Leicester, Leics, England.
EM aboozar.taherkhani@dmu.ac.uk
CR ADELI H, 1994, APPL MATH COMPUT, V62, P81, DOI 10.1016/0096-3003(94)90134-1
   Adibi P, 2005, NEUROCOMPUTING, V64, P335, DOI 10.1016/j.neucom.2004.10.111
   [Anonymous], 2019, SURROGATE GRADIENT L
   [Anonymous], 2016, ARXIV161203214
   [Anonymous], 2019, BIOL INSPIRED ALTERN
   ARTOLA A, 1990, NATURE, V347, P69, DOI 10.1038/347069a0
   Azevedo FAC, 2009, J COMP NEUROL, V513, P532, DOI 10.1002/cne.21974
   Bassett D. S, 2012, ARXIV12103555
   Bélanger-Gravel A, 2013, PSYCHOL HEALTH, V28, P217, DOI 10.1080/08870446.2012.723711
   Belatreche A, 2003, P IEEE CYB INT CHALL, P39
   Belatreche A, 2007, SOFT COMPUT, V11, P239, DOI [10.1007/s00500-006-0065-7, 10.1007/S00500-006-0065-7]
   Bellec G., 2018, ADV NEURAL INFORM PR
   Bi GQ, 2002, PHYSIOL BEHAV, V77, P551, DOI 10.1016/S0031-9384(02)00933-2
   Bichler O, 2012, NEURAL NETWORKS, V32, P339, DOI 10.1016/j.neunet.2012.02.022
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Booij O, 2005, INFORM PROCESS LETT, V95, P552, DOI 10.1016/j.ipl.2005.05.023
   Borst A, 1999, NAT NEUROSCI, V2, P947, DOI 10.1038/14731
   Boudkkazi S, 2011, J PHYSIOL-LONDON, V589, P1117, DOI 10.1113/jphysiol.2010.199653
   Brette R, 2007, J COMPUT NEUROSCI, V23, P349, DOI 10.1007/s10827-007-0038-6
   Brette R, 2015, FRONT SYST NEUROSCI, V9, DOI 10.3389/fnsys.2015.00151
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Carey MR, 2005, NAT NEUROSCI, V8, P813, DOI 10.1038/nn1470
   Cariani PA, 2004, IEEE T NEURAL NETWOR, V15, P1100, DOI 10.1109/TNN.2004.833305
   Clopath C, 2010, NAT NEUROSCI, V13, P344, DOI 10.1038/nn.2479
   Delorme A, 2001, NEUROCOMPUTING, V38, P539, DOI 10.1016/S0925-2312(01)00403-9
   Diehl PU, 2015, IEEE IJCNN
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Douglas RJ, 2004, ANNU REV NEUROSCI, V27, P419, DOI 10.1146/annurev.neuro.27.070203.144152
   Doya K, 1999, NEURAL NETWORKS, V12, P961, DOI 10.1016/S0893-6080(99)00046-5
   Eliasmith C, 2012, SCIENCE, V338, P1202, DOI 10.1126/science.1225266
   Feldman DE, 2012, NEURON, V75, P556, DOI 10.1016/j.neuron.2012.08.001
   Florian RV, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0040233
   Frégnac Y, 1999, J NEUROBIOL, V41, P69
   Friedman J., 2001, ELEMENTS STAT LEARNI, Vtenth, DOI DOI 10.1007/978-0-387-84858-7_2
   Froemke RC, 2006, J NEUROPHYSIOL, V95, P1620, DOI 10.1152/jn.00910.2005
   Froemke RC, 2002, NATURE, V416, P433, DOI 10.1038/416433a
   Gardner B, 2015, NEURAL COMPUT, V27, P2548, DOI 10.1162/NECO_a_00790
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   Gerstner W., 2002, SPIKING NEURON MODEL
   Gerstner W, 2012, SCIENCE, V338, P60, DOI 10.1126/science.1227356
   Ghosh-Dastidar S, 2007, INTEGR COMPUT-AID E, V14, P187
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   Gilson M, 2012, NEURAL COMPUT, V24, P2251, DOI 10.1162/NECO_a_00331
   Glackin B, 2010, FRONT COMPUT NEUROSC, V4, DOI 10.3389/fncom.2010.00018
   Glackin C, 2011, NEURAL NETWORKS, V24, P247, DOI 10.1016/j.neunet.2010.11.008
   Gonzalez-Nalda P, 2009, STDP LEARNING TIME W
   Guerguiev J, 2017, ELIFE, V6, DOI 10.7554/eLife.22901
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Haykin S., 2009, NEURAL NETWORKS LEAR, V3rd ed.
   Hazan H, 2012, EXPERT SYST APPL, V39, P1597, DOI 10.1016/j.eswa.2011.06.052
   Heiligenberg W., 1991, NEURAL NETS ELECT FI
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   Huh D, 2018, GRADIENT DESCENT SPI, P1433
   HUNG SL, 1994, NEUROCOMPUTING, V6, P45, DOI 10.1016/0925-2312(94)90033-7
   HUNG SL, 1993, NEUROCOMPUTING, V5, P287, DOI 10.1016/0925-2312(93)90042-2
   Illing B, 2019, NEURAL NETWORKS, V118, P90, DOI 10.1016/j.neunet.2019.06.001
   Ito M, 2000, BRAIN RES, V886, P237, DOI 10.1016/S0006-8993(00)03142-5
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Jedlicka P, 2002, Bratisl Lek Listy, V103, P137
   Jörntell H, 2006, NEURON, V52, P227, DOI 10.1016/j.neuron.2006.09.032
   Joshi P, 2004, LECT NOTES COMPUT SC, V3141, P258
   Ju H, 2013, NEURAL NETWORKS, V38, P39, DOI 10.1016/j.neunet.2012.11.003
   Kampa BM, 2007, TRENDS NEUROSCI, V30, P456, DOI 10.1016/j.tins.2007.06.010
   Kasabov N, 2013, NEURAL NETWORKS, V41, P188, DOI 10.1016/j.neunet.2012.11.014
   KATZ B, 1965, PROC R SOC SER B-BIO, V161, P483, DOI 10.1098/rspb.1965.0016
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Kheradpisheh SR, 2016, NEUROCOMPUTING, V205, P382, DOI 10.1016/j.neucom.2016.04.029
   Knudsen EI, 2002, NATURE, V417, P322, DOI 10.1038/417322a
   Koch C., 1998, METHODS NEURONAL MOD
   Kohonen T, 2013, NEURAL NETWORKS, V37, P52, DOI 10.1016/j.neunet.2012.09.018
   Konig P, 1996, TRENDS NEUROSCI, V19, P130, DOI 10.1016/S0166-2236(96)80019-1
   KUWABARA N, 1993, J NEUROPHYSIOL, V69, P1713, DOI 10.1152/jn.1993.69.5.1713
   Lameu EL, 2012, CHAOS, V22, DOI 10.1063/1.4772998
   Lee C, 2019, ENABLING SPIKE BASED, V113, P54
   Lee C, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00435
   Legenstein R, 2005, NEURAL COMPUT, V17, P2337, DOI 10.1162/0899766054796888
   Letzkus JJ, 2006, J NEUROSCI, V26, P10420, DOI 10.1523/JNEUROSCI.2650-06.2006
   Lin JW, 2002, TRENDS NEUROSCI, V25, P449, DOI 10.1016/S0166-2236(02)02212-9
   Liu SC, 2015, EVENT-BASED NEUROMORPHIC SYSTEMS, P1, DOI 10.1002/9781118927601
   Maass W, 1996, NEURAL COMPUT, V8, P1, DOI 10.1162/neco.1996.8.1.1
   Maass W, 2004, MATH COMP BIOL SER, P575
   Maass W, 1998, PULSED NEURAL NETWORKS, P321
   Maggi S, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-04638-2
   Masquelier T, 2007, PLOS COMPUT BIOL, V3, P247, DOI 10.1371/journal.pcbi.0030031
   Masquelier T, 2013, PRINCIPLES OF NEURAL CODING, P513
   McKennoch S, 2006, IEEE IJCNN, P3970
   Memmesheimer RM, 2014, NEURON, V82, P925, DOI 10.1016/j.neuron.2014.03.026
   Minneci F, 2012, J NEUROSCI METH, V205, P49, DOI 10.1016/j.jneumeth.2011.12.015
   Mohemmed A, 2013, NEUROCOMPUTING, V107, P3, DOI 10.1016/j.neucom.2012.08.034
   Mohemmed A, 2012, INT J NEURAL SYST, V22, DOI 10.1142/S0129065712500128
   Morrison A, 2008, BIOL CYBERN, V98, P459, DOI 10.1007/s00422-008-0233-1
   Mostafa H, 2018, IEEE T NEUR NET LEAR, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Neftci E, 2014, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00272
   Neftci EO, 2018, ISCIENCE, V5, P52, DOI 10.1016/j.isci.2018.06.010
   Neftci EO, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00324
   Orchard G, 2015, IEEE T PATTERN ANAL, V37, P2028, DOI 10.1109/TPAMI.2015.2392947
   Panda P, 2016, IEEE IJCNN, P299, DOI 10.1109/IJCNN.2016.7727212
   Parnas I, 2010, PFLUG ARCH EUR J PHY, V460, P975, DOI 10.1007/s00424-010-0872-7
   Paugam-Moisy H., 2012, HDB NATURAL COMPUTIN, V1, P1, DOI [DOI 10.1007/978-3-540-92910-9_10, 10.1007/978-3-540-92910-9_10]
   Paugam-Moisy H, 2008, NEUROCOMPUTING, V71, P1143, DOI 10.1016/j.neucom.2007.12.027
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Pfister JP, 2006, J NEUROSCI, V26, P9673, DOI 10.1523/JNEUROSCI.1425-06.2006
   Pfister JP, 2006, NEURAL COMPUT, V18, P1318, DOI 10.1162/neco.2006.18.6.1318
   Pham DT, 2008, P I MECH ENG B-J ENG, V222, P1201, DOI 10.1243/09544054JEM1054
   Ponulak F., 2005, RESUME NEW SUPERVISE
   Ponulak F, 2011, ACTA NEUROBIOL EXP, V71, P409
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Ramesh B, 2020, IEEE T PATTERN ANAL, V42, P2767, DOI 10.1109/TPAMI.2019.2919301
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Ruf B, 1997, NEURAL PROCESS LETT, V5, P9, DOI 10.1023/A:1009697008681
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707
   Schrauwen B, 2008, NEURAL NETWORKS, V21, P511, DOI 10.1016/j.neunet.2007.12.009
   Schultz W, 1997, SCIENCE, V275, P1593, DOI 10.1126/science.275.5306.1593
   Seth AK, 2015, CURR BIOL, V25, pR110, DOI 10.1016/j.cub.2014.12.043
   Shrestha SB., 2018, ADV NEURAL INFORM PR, V31, P1412
   Shrestha SB, 2015, NEURAL NETWORKS, V63, P185, DOI 10.1016/j.neunet.2014.12.001
   Shrestha SB, 2013, ANN ALLERTON CONF, P506, DOI 10.1109/Allerton.2013.6736567
   Silva SM, 2005, PROCEEDINGS OF THE 2005 INTERNATIONAL CONFERENCE ON NEURAL NETWORKS AND BRAIN, VOLS 1-3, P1354
   Sporea I, 2013, NEURAL COMPUT, V25, P473, DOI 10.1162/NECO_a_00396
   Srinivasa N, 2012, IEEE T NEUR NET LEAR, V23, P1526, DOI 10.1109/TNNLS.2012.2207738
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   SWADLOW HA, 1992, J NEUROPHYSIOL, V68, P605, DOI 10.1152/jn.1992.68.2.605
   Taherkhani A, 2014, P ESANN, P11
   Taherkhani A, 2020, IEEE T COGN DEV SYST, V12, P427, DOI 10.1109/TCDS.2019.2909355
   Taherkhani A, 2018, NEUROCOMPUTING, V322, P22, DOI 10.1016/j.neucom.2018.09.040
   Taherkhani A, 2018, IEEE T NEUR NET LEAR, V29, P5394, DOI 10.1109/TNNLS.2018.2797801
   Taherkhani A, 2015, LECT NOTES COMPUT SC, V9490, P190, DOI 10.1007/978-3-319-26535-3_22
   Taherkhani A, 2015, IEEE T NEUR NET LEAR, V26, P3137, DOI 10.1109/TNNLS.2015.2404938
   Takase Haruhiko, 2009, Proceedings 2009 International Joint Conference on Neural Networks (IJCNN 2009 - Atlanta), P3062, DOI 10.1109/IJCNN.2009.5178756
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Tavanaei A, 2018, NEURAL NETWORKS, V105, P294, DOI 10.1016/j.neunet.2018.05.018
   Tavanaei A, 2016, IEEE IJCNN, P307, DOI 10.1109/IJCNN.2016.7727213
   Tetzlaff C, 2012, BIOL CYBERN, V106, P715, DOI 10.1007/s00422-012-0529-z
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   Turrigiano GG, 2004, NAT REV NEUROSCI, V5, P97, DOI 10.1038/nrn1327
   Van Rullen R, 2001, NEURAL COMPUT, V13, P1255, DOI 10.1162/08997660152002852
   Vasilaki E, 2013, ARXIV13017187
   Verstraeten D, 2005, INFORM PROCESS LETT, V95, P521, DOI 10.1016/j.ipl.2005.05.019
   Vreeken J, 2003, 2003008 UUCS, P1
   Wade JJ, 2010, IEEE T NEURAL NETWOR, V21, P1817, DOI 10.1109/TNN.2010.2074212
   Wang HX, 2005, NAT NEUROSCI, V8, P187, DOI 10.1038/nn1387
   Wang JL, 2014, NEUROCOMPUTING, V144, P526, DOI 10.1016/j.neucom.2014.04.017
   Whittington JCR, 2019, TRENDS COGN SCI, V23, P235, DOI 10.1016/j.tics.2018.12.005
   Wu J, 2019, IEEE
   Wysoski SG, 2008, NEUROCOMPUTING, V71, P2563, DOI 10.1016/j.neucom.2007.12.038
   Xin JG, 2001, IEEE IJCNN, P1772, DOI 10.1109/IJCNN.2001.938430
   Xu B, 2013, SCI CHINA CHEM, V56, P222, DOI 10.1007/s11426-012-4710-y
   Xu Y, 2019, NEURAL NETWORKS, V116, P11, DOI 10.1016/j.neunet.2019.03.017
   Xu Y, 2013, NEURAL NETWORKS, V43, P99, DOI 10.1016/j.neunet.2013.02.003
   Yu Q, 2014, NEUROCOMPUTING, V138, P3, DOI 10.1016/j.neucom.2013.06.052
   Yu Q, 2013, IEEE T NEUR NET LEAR, V24, P1539, DOI 10.1109/TNNLS.2013.2245677
   Yu QF, 2013, PLOS ONE, V8, DOI [10.1371/journal.pone.0059627, 10.1371/journal.pone.0078318]
   Yu Q, 2012, IEEE ICC
   Zenke F, 2018, NEURAL COMPUT, V30, P1514, DOI 10.1162/neco_a_01086
   Zhao B, 2015, IEEE T NEUR NET LEAR, V26, P1963, DOI 10.1109/TNNLS.2014.2362542
   Zuo YF, 2015, CURR BIOL, V25, P357, DOI 10.1016/j.cub.2014.11.065
NR 162
TC 150
Z9 153
U1 20
U2 208
PD FEB
PY 2020
VL 122
BP 253
EP 272
DI 10.1016/j.neunet.2019.09.036
UT WOS:000505021700019
HC Y
HP N
DA 2023-11-16
ER

PT C
AU Huang, FQ
   Riehl, J
   Ching, SN
AF Huang, Fuqiang
   Riehl, James
   Ching, ShiNung
GP IEEE
TI Optimizing the Dynamics of Spiking Networks for Decoding and Control
SO 2017 AMERICAN CONTROL CONFERENCE (ACC)
SE Proceedings of the American Control Conference
DT Proceedings Paper
CT American Control Conference (ACC)
CY MAY 24-26, 2017
CL Seattle, WA
ID NEURAL CODE; NEURONS
AB In this paper, an optimization-based approach to construct spiking networks for the purposes of decoding and control is presented. Specifically, we postulate a simple objective function wherein a network of interacting, primitive spiking units is decoded in order to drive a linear system along a prescribed trajectory. The units are assumed to spike only if doing so will decrease a specified objective function. The optimization gives rise to an emergent network of neurons with diffusive dynamics and a threshold-based spiking rule that bears resemblance to the Integrate and Fire neural model.
C1 [Huang, Fuqiang; Riehl, James; Ching, ShiNung] Washington Univ, Dept Elect & Syst Engn, St Louis, MO 63130 USA.
RP Huang, FQ (corresponding author), Washington Univ, Dept Elect & Syst Engn, St Louis, MO 63130 USA.
EM fuqiang@wustl.edu
CR Abbott LF, 2016, NAT NEUROSCI, V19, P350, DOI 10.1038/nn.4241
   Abbott LF, 1999, BRAIN RES BULL, V50, P303, DOI 10.1016/S0361-9230(99)00161-6
   BIALEK W, 1991, SCIENCE, V252, P1854, DOI 10.1126/science.2063199
   Boerlin M, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003258
   Boerlin M, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1001080
   Dayan P., 2001, THEORETICAL NEUROSCI, V10
   HUNT KJ, 1992, AUTOMATICA, V28, P1083, DOI 10.1016/0005-1098(92)90053-I
   Johnson EC, 2016, J COMPUT NEUROSCI, V40, P193, DOI 10.1007/s10827-016-0592-x
   Lewis F. L., 1998, NEURAL NETWORK CONTR
   Schwemmer MA, 2015, J NEUROSCI, V35, P10112, DOI 10.1523/JNEUROSCI.4951-14.2015
NR 10
TC 2
Z9 2
U1 0
U2 0
PY 2017
BP 2792
EP 2798
UT WOS:000427033302138
DA 2023-11-16
ER

PT C
AU Huayaney, FLM
   Tanaka, H
   Matsuo, T
   Morie, T
   Aihara, K
AF Huayaney, Frank L. Maldonado
   Tanaka, Hideki
   Matsuo, Takayuki
   Morie, Takashi
   Aihara, Kazuyuki
BE Lu, BL
   Zhang, LQ
   Kwok, J
TI A VLSI Spiking Neural Network with Symmetric STDP and Associative Memory
   Operation
SO NEURAL INFORMATION PROCESSING, PT III
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 18th International Conference on Neural Information Processing (ICONIP)
CY NOV 13-17, 2011
CL Shanghai, PEOPLES R CHINA
DE VLSI; spiking neural network; STDP; associative memory
ID NEURONS; SYNAPSES; CIRCUIT
AB This paper proposes an analog CMOS VLSI circuit which implements integrate-and-fire spiking neural networks with spike-timing dependent synaptic plasticity (STDP). The designed VLSI chip includes 25 neurons and 600 synapse circuits with symmetric all-to-all connection STDP. Using the fabricated VLSI chip, we implement a Hopfield-type feedback network, and demonstrate its associative memory operation. In our chip, analog information is represented by the relative timing of spike firing events. Symmetric STDP provides an auto-correlation learning function depending on relative timing between spikes consisting of a learning pattern. Each learning and test pattern consists of 20 spike pulses each of which has a relative delay corresponding to a gray-scale pixel intensity. The chip has successfully associated from an input pattern the most similar learning pattern.
C1 [Huayaney, Frank L. Maldonado; Tanaka, Hideki; Matsuo, Takayuki; Morie, Takashi] Kyushu Inst Technol, Grad Sch Life Sci & Syst Engn, Kitakyushu, Fukuoka 8080196, Japan.
   [Aihara, Kazuyuki] Univ Tokyo, Inst Ind Sci, Tokyo 1538505, Japan.
RP Huayaney, FLM (corresponding author), Kyushu Inst Technol, Grad Sch Life Sci & Syst Engn, Kitakyushu, Fukuoka 8080196, Japan.
CR Bofill-i-Petit A, 2004, IEEE T NEURAL NETWOR, V15, P1296, DOI 10.1109/TNN.2004.832842
   Indiveri G, 2006, IEEE T NEURAL NETWOR, V17, P211, DOI 10.1109/TNN.2005.860850
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maass W., 1999, PULSED NEURAL NETWOR
   Morie T, 2004, IEICE T ELECTRON, VE87C, P1856
   Sasaki K, 2006, IEICE T ELECTRON, VE89C, P1637, DOI 10.1093/ietele/e89-c.11.1637
   Tanaka H, 2009, IEICE T FUND ELECTR, VE92A, P1690, DOI 10.1587/transfun.E92.A.1690
NR 7
TC 0
Z9 0
U1 0
U2 4
PY 2011
VL 7064
BP 381
EP +
PN III
UT WOS:000307328500043
DA 2023-11-16
ER

PT J
AU Li, XM
   Chen, Q
   Xue, FZ
AF Li, Xiumin
   Chen, Qing
   Xue, Fangzheng
TI Biological modelling of a computational spiking neural network with
   neuronal avalanches
SO PHILOSOPHICAL TRANSACTIONS OF THE ROYAL SOCIETY A-MATHEMATICAL PHYSICAL
   AND ENGINEERING SCIENCES
DT Article
DE spiking neural network; critical dynamic; neuronal avalanches;
   computational performance; spike-timing-dependent plasticity
ID SELF-ORGANIZED CRITICALITY
AB In recent years, an increasing number of studies have demonstrated that networks in the brain can self-organize into a critical state where dynamics exhibit a mixture of ordered and disordered patterns. This critical branching phenomenon is termed neuronal avalanches. It has been hypothesized that the homeostatic level balanced between stability and plasticity of this critical state may be the optimal state for performing diverse neural computational tasks. However, the critical region for high performance is narrow and sensitive for spiking neural networks (SNNs). In this paper, we investigated the role of the critical state in neural computations based on liquid-state machines, a biologically plausible computational neural network model for real-time computing. The computational performance of an SNN when operating at the critical state and, in particular, with spike-timing-dependent plasticity for updating synaptic weights is investigated. The network is found to show the best computational performance when it is subjected to critical dynamic states. Moreover, the active-neuron-dominant structure refined from synaptic learning can remarkably enhance the robustness of the critical state and further improve computational accuracy. These results may have important implications in the modelling of spiking neural networks with optimal computational performance.
   This article is part of the themed issue 'Mathematical methods in medicine: neuroscience, cardiology and pathology'.
C1 [Li, Xiumin; Chen, Qing; Xue, Fangzheng] Chongqing Univ, Minist Educ, Key Lab Dependable Serv Comp Cyber Phys Soc, Chongqing 400044, Peoples R China.
   [Li, Xiumin; Chen, Qing; Xue, Fangzheng] Chongqing Univ, Coll Automat, Chongqing 400044, Peoples R China.
RP Li, XM (corresponding author), Chongqing Univ, Minist Educ, Key Lab Dependable Serv Comp Cyber Phys Soc, Chongqing 400044, Peoples R China.; Li, XM (corresponding author), Chongqing Univ, Coll Automat, Chongqing 400044, Peoples R China.
EM xmli@cqu.edu.cn
CR [Anonymous], 1998, SELF ORG CRITICALITY
   [Anonymous], 2002, THEORY BRANCHING PRO
   [Anonymous], 2008, ADV NEURAL INF PROCE
   [Anonymous], 2013, NATURE WORKS SCI SEL
   BAK P, 1987, PHYS REV LETT, V59, P381, DOI 10.1103/PhysRevLett.59.381
   Beggs JM, 2004, J NEUROSCI, V24, P5216, DOI 10.1523/JNEUROSCI.0540-04.2004
   Beggs JM, 2003, J NEUROSCI, V23, P11167
   Bengio Y., 2006, ADV NEURAL INFORM PR, P153
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Bornholdt S, 2000, PHYS REV LETT, V84, P6114, DOI 10.1103/PhysRevLett.84.6114
   de Arcangelis L, 2006, PHYS REV LETT, V96, DOI 10.1103/PhysRevLett.96.028107
   Diehl PU, 2015, P INT JOINT C NEUR N
   Droste F, 2013, J R SOC INTERFACE, V10, DOI 10.1098/rsif.2012.0558
   Goh KI, 2003, PHYS REV LETT, V91, DOI 10.1103/PhysRevLett.91.148701
   Haldeman C, 2005, PHYS REV LETT, V94, DOI 10.1103/PhysRevLett.94.058101
   Hesse J, 2014, FRONT SYST NEUROSCI, V8, DOI 10.3389/fnsys.2014.00166
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Jaeger H, 2005, IEEE IJCNN, P1460
   Kinouchi O, 2006, NAT PHYS, V2, P348, DOI 10.1038/nphys289
   Legenstein R., 2007, NEW DIRECTIONS STAT, P127, DOI DOI 10.1016/J.JNEUMETH.2008.04.015
   Legenstein R, 2007, NEURAL NETWORKS, V20, P323, DOI 10.1016/j.neunet.2007.04.017
   Levina A, 2007, NAT PHYS, V3, P857, DOI 10.1038/nphys758
   Levina A, 2009, PHYS REV LETT, V102, DOI 10.1103/PhysRevLett.102.118110
   Li XM, 2012, CHAOS, V22, DOI 10.1063/1.3701946
   Li XM, 2009, CHAOS, V19, DOI 10.1063/1.3076394
   Lin M, 2005, PHYS REV E, V71, DOI 10.1103/PhysRevE.71.016133
   Maass W, 2007, PLOS COMPUT BIOL, V3, P15, DOI 10.1371/journal.pcbi.0020165
   Meisel C, 2009, PHYS REV E, V80, DOI 10.1103/PhysRevE.80.061917
   Natschlager T., 2002, SPECIAL ISSUE FDN IN, P39, DOI [DOI 10.1017/CBO9781107415324.004, 10.1017/CBO9781107415324.004]
   Norton D, 2006, IEEE IJCNN, P4243
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   Pajevic S, 2009, PLOS COMPUT BIOL, V5, DOI 10.1371/journal.pcbi.1000271
   Pasquale V, 2008, NEUROSCIENCE, V153, P1354, DOI 10.1016/j.neuroscience.2008.03.050
   Shew WL, 2011, J NEUROSCI, V31, P55, DOI 10.1523/JNEUROSCI.4637-10.2011
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Wainrib G, 2013, CHAOS, V23, DOI 10.1063/1.4841396
   Wang SJ, 2012, NEW J PHYS, V14, DOI 10.1088/1367-2630/14/2/023005
   Wang Sheng-Jun, 2011, Front Comput Neurosci, V5, P30, DOI 10.3389/fncom.2011.00030
   Xue FZ, 2013, NEUROCOMPUTING, V122, P324, DOI 10.1016/j.neucom.2013.06.019
NR 41
TC 10
Z9 10
U1 0
U2 17
PD MAY 17
PY 2017
VL 375
IS 2096
AR 20160286
DI 10.1098/rsta.2016.0286
UT WOS:000401341900006
DA 2023-11-16
ER

PT C
AU Kim, H
   Yu, J
   Choi, K
AF Kim, Heesu
   Yu, Joonsang
   Choi, Kiyoung
GP IEEE
TI Hybrid Spiking-Stochastic Deep Neural Network
SO 2017 INTERNATIONAL SYMPOSIUM ON VLSI DESIGN, AUTOMATION AND TEST
   (VLSI-DAT)
SE International Symposium on VLSI Design Automation and Test
DT Proceedings Paper
CT International Symposium on VLSI Design, Automation and Test (VLSI-DAT)
CY APR 24-27, 2017
CL Hsinchu, TAIWAN
AB Stochastic computing has been adopted in various fields to improve the power efficiency of systems. Recent work showed that DNN based on stochastic computing can greatly reduce the power consumption. However, stochastic computing has a limitation of high latency overhead as it computes values only one bit per cycle. This paper proposes a new scheme to improve the latency of DNN implementation based on stochastic computing by combining it with the concept of spiking neural networks. It uses a spiking neural network as the front end and pipeline the results to the stochastic-computing neural network at the back end. Such a hybrid spiking-stochastic DNN has the benefits of both approaches including low latency and low power consumption, while maintaining the same level of accuracy.
C1 [Kim, Heesu; Yu, Joonsang; Choi, Kiyoung] Seoul Natl Univ, Dept Elect & Comp Engn, Seoul, South Korea.
RP Kim, H (corresponding author), Seoul Natl Univ, Dept Elect & Comp Engn, Seoul, South Korea.
EM hkim@dal.snu.ac.kr; joonsang.yu@dal.snu.ac.kr; kchoi@dal.snu.ac.kr
CR Alaghi A, 2013, ACM T EMBED COMPUT S, V12, DOI 10.1145/2465787.2465794
   [Anonymous], 2015, CISC VIS NETW IND GL
   [Anonymous], 2016, P DES AUT C DAC
   [Anonymous], ARXIV160104183
   Chen TS, 2014, ACM SIGPLAN NOTICES, V49, P269, DOI 10.1145/2541940.2541967
   David J.-P., 2015, WORKSH CONTR INT C L
   Diehl P.U., 2015, 2015 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2015.7280696
   Orchard G, 2015, FRONT NEUROSCI-SWITZ, V9, DOI [10.3389/fhins.2015.00437, 10.3389/fnins.2015.00437]
   Posch C, 2011, IEEE J SOLID-ST CIRC, V46, P259, DOI 10.1109/JSSC.2010.2085952
   Suda N, 2016, PROCEEDINGS OF THE 2016 ACM/SIGDA INTERNATIONAL SYMPOSIUM ON FIELD-PROGRAMMABLE GATE ARRAYS (FPGA'16), P16, DOI 10.1145/2847263.2847276
   Venkatesan R, 2015, DES AUT TEST EUROPE, P1575
NR 11
TC 0
Z9 0
U1 0
U2 0
PY 2017
UT WOS:000411184600002
DA 2023-11-16
ER

PT J
AU Santos, BA
   Gomes, RM
   Husbands, P
AF Santos, Bruno Andre
   Gomes, Rogerio Martins
   Husbands, Phil
TI The role of rebound spikes in the maintenance of self-sustained neural
   spiking activity
SO NONLINEAR DYNAMICS
DT Article
DE Self-sustained neural activity; Rebound spike; Neural dynamics; Spiking
   neurons
ID TIMING-DEPENDENT PLASTICITY; WORKING-MEMORY; WAVE DISCHARGES; MODEL;
   DYNAMICS; IMPLEMENTATION; BISTABILITY; NETWORK
AB In general, the mechanisms that maintain the activity of neural systems after a triggering stimulus has been removed are not well understood. Different mechanisms involving at the cellular and network levels have been proposed. In this work, based on analysis of a computational model of a spiking neural network, it is proposed that the spike that occurs after a neuron is inhibited (the rebound spike) can be used to sustain the activity in a recurrent inhibitory neural circuit after the stimulation has been removed. It is shown that, in order to sustain the activity, the neurons participating in the recurrent circuit should fire at low frequencies. It is also shown that the occurrence of a rebound spike depends on a combination of factors including synaptic weights, synaptic conductances and the neuron state. We point out that the model developed here is minimalist and does not aim at empirical accuracy. Its purpose is to raise and discuss theoretical issues that could contribute to the understanding of neural mechanisms underlying self-sustained neural activity.
C1 [Santos, Bruno Andre; Gomes, Rogerio Martins] Fed Ctr Technol Educ Minas Gerais, Belo Horizonte, MG, Brazil.
   [Husbands, Phil] Univ Sussex, Ctr Computat Neurosci & Robot, Brighton, E Sussex, England.
RP Santos, BA (corresponding author), Fed Ctr Technol Educ Minas Gerais, Belo Horizonte, MG, Brazil.; Husbands, P (corresponding author), Univ Sussex, Ctr Computat Neurosci & Robot, Brighton, E Sussex, England.
EM bsantos@cefetmg.br; p.husbands@sussex.ac.uk
CR Abbasova KR, 2010, BRAIN RES, V1366, P257, DOI 10.1016/j.brainres.2010.10.007
   Afraimovich VS, 2004, INT J BIFURCAT CHAOS, V14, P1195, DOI 10.1142/S0218127404009806
   Alves L.F., 2017, COMMUNICATIONS COMPU, V720
   Ambroise M, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00215
   Appleby PA, 2012, BIOL CYBERN, V106, P373, DOI 10.1007/s00422-012-0504-8
   Barak O, 2010, J NEUROSCI, V30, P9424, DOI 10.1523/JNEUROSCI.1875-10.2010
   Bizzarri F, 2013, J COMPUT NEUROSCI, V35, P201, DOI 10.1007/s10827-013-0448-6
   Bojanek K, 2020, PLOS COMPUT BIOL, V16, DOI 10.1371/journal.pcbi.1007409
   Borges FS, 2020, PHYSICA A, V537, DOI 10.1016/j.physa.2019.122671
   Bunge SA, 2000, P NATL ACAD SCI USA, V97, P3573, DOI 10.1073/pnas.050583797
   Conway ARA, 2005, PSYCHON B REV, V12, P769, DOI 10.3758/BF03196772
   Dickinson P.S., 1998, HDB BRAIN THEORY NEU, P631
   Drew PJ, 2006, P NATL ACAD SCI USA, V103, P8876, DOI 10.1073/pnas.0600676103
   Erickson MA, 2010, J COGNITIVE NEUROSCI, V22, P2530, DOI 10.1162/jocn.2009.21375
   Fiebig F, 2017, J NEUROSCI, V37, P83, DOI 10.1523/JNEUROSCI.1989-16.2016
   Fujisawa S, 2008, NAT NEUROSCI, V11, P823, DOI 10.1038/nn.2134
   Gottwald GA, 2004, P ROY SOC A-MATH PHY, V460, P603, DOI 10.1098/rspa.2003.1183
   Gottwald GA, 2009, SIAM J APPL DYN SYST, V8, P129, DOI 10.1137/080718851
   HARRISWARRICK RM, 1991, ANNU REV NEUROSCI, V14, P39, DOI 10.1146/annurev.ne.14.030191.000351
   Harvey I., 2009, EUROPEAN C ARTIFICIA, P126
   Hashemi M, 2012, PHYS REV E, V85, DOI 10.1103/PhysRevE.85.021917
   Hebb D., 2005, ORG BEHAV NEUROPSYCH
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Humpstone H J, 1919, Psychol Clin, V12, P196
   Husbands P, 2014, INTEL ROBOT AUTON AG, P17
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2008, P NATL ACAD SCI USA, V105, P3593, DOI 10.1073/pnas.0712231105
   Izquierdo EJ, 2010, J NEUROSCI, V30, P12908, DOI 10.1523/JNEUROSCI.2606-10.2010
   Kim Y, 2010, J KOREAN PHYS SOC, V57, P1363, DOI 10.3938/jkps.57.1363
   Korkmaz N, 2018, COMPUT APPL ENG EDUC, V26, P782, DOI 10.1002/cae.21972
   Kriener B, 2014, FRONT COMPUT NEUROSC, V8, DOI 10.3389/fncom.2014.00136
   Kusters JMAM, 2007, PHYS REV LETT, V98, DOI 10.1103/PhysRevLett.98.098107
   Leroy F, 2017, NEURON, V95, P1089, DOI 10.1016/j.neuron.2017.07.036
   Loewenstein Y, 2005, NAT NEUROSCI, V8, P202, DOI 10.1038/nn1393
   Majhi S, 2019, PHYS LIFE REV, V28, P100, DOI 10.1016/j.plrev.2018.09.003
   Marder E, 1996, P NATL ACAD SCI USA, V93, P13481, DOI 10.1073/pnas.93.24.13481
   Medvedeva TM, 2020, PLOS ONE, V15, DOI 10.1371/journal.pone.0239125
   Medvedeva TM, 2018, NEURAL NETWORKS, V98, P271, DOI 10.1016/j.neunet.2017.12.002
   Mi YY, 2017, NEURON, V93, P323, DOI 10.1016/j.neuron.2016.12.004
   Moioli RC, 2012, BIOL CYBERN, V106, P407, DOI 10.1007/s00422-012-0507-5
   Mongillo G, 2008, SCIENCE, V319, P1543, DOI 10.1126/science.1150769
   Nobukawa S, 2015, J ARTIF INTELL SOFT, V5, P109, DOI 10.1515/jaiscr-2015-0023
   Nobukawa S, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0138919
   Osaka M, 2003, NEUROIMAGE, V18, P789, DOI 10.1016/S1053-8119(02)00032-0
   Osaka N, 2004, NEUROIMAGE, V21, P623, DOI 10.1016/j.neuroimage.2003.09.069
   Rabinovich MI, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.1000072
   Rabinovich MI, 2020, PHYS REP, V883, DOI 10.1016/j.physrep.2020.08.003
   Ranhel J, 2012, IEEE T NEUR NET LEAR, V23, P916, DOI 10.1109/TNNLS.2012.2190421
   Roth A, 2009, COMPUT NEUROSCI-MIT, P139
   Sanchez-Vives MV, 2000, NAT NEUROSCI, V3, P1027, DOI 10.1038/79848
   Santos B, 2012, CONNECT SCI, V24, P143, DOI 10.1080/09540091.2013.770821
   Shilnikov A, 2005, PHYS REV E, V71, DOI 10.1103/PhysRevE.71.056214
   Shim Y, 2016, PLOS COMPUT BIOL, V12, DOI 10.1371/journal.pcbi.1005137
   Shu YS, 2003, NATURE, V423, P288, DOI 10.1038/nature01616
   Skokos C, 2016, LECT NOTES PHYS, V915, P221, DOI 10.1007/978-3-662-48410-4_7
   Soares G. E., 2010, Proceedings of the 2010 Eleventh Brazilian Symposium on Neural Networks (SBRN 2010), P43, DOI 10.1109/SBRN.2010.16
   Stokes MG, 2015, TRENDS COGN SCI, V19, P394, DOI 10.1016/j.tics.2015.05.004
   Suvrathan A, 2019, CURR OPIN NEUROBIOL, V54, P12, DOI 10.1016/j.conb.2018.06.011
   Szatmáry B, 2010, PLOS COMPUT BIOL, V6, DOI 10.1371/journal.pcbi.1000879
   Tamura A., 2009, DYNAM CONT DIS SER A, V16, P759
   TEL T, 2015, CHAOS INTERDISCIP J, V25
   Toker D, 2020, COMMUN BIOL, V3, DOI 10.1038/s42003-019-0715-9
   Tolmachev P, 2018, LECT NOTES COMPUT SC, V11301, P603, DOI 10.1007/978-3-030-04167-0_55
   Tomov P, 2014, FRONT COMPUT NEUROSC, V8, DOI 10.3389/fncom.2014.00103
   Tomov P, 2016, FRONT COMPUT NEUROSC, V10, DOI 10.3389/fncom.2016.00023
   Trübutschek D, 2017, ELIFE, V6, DOI 10.7554/eLife.23871
   Vasu MC, 2017, PROCEEDINGS OF THE 2017 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE (GECCO'17), P75, DOI 10.1145/3071178.3071336
   Wang XJ, 2001, TRENDS NEUROSCI, V24, P455, DOI 10.1016/S0166-2236(00)01868-3
   Watanabe K, 2007, CEREB CORTEX, V17, pI88, DOI 10.1093/cercor/bhm102
   Williams SR, 2002, J PHYSIOL-LONDON, V539, P469, DOI 10.1113/jphysiol.2001.013136
   Womack M, 2002, J NEUROSCI, V22, P10603
   Zheng TW, 2012, EPILEPSIA, V53, P1948, DOI 10.1111/j.1528-1167.2012.03720.x
NR 73
TC 2
Z9 2
U1 1
U2 14
PD JUL
PY 2021
VL 105
IS 1
BP 767
EP 784
DI 10.1007/s11071-021-06581-2
EA JUN 2021
UT WOS:000668418400002
DA 2023-11-16
ER

PT J
AU Wu, QX
   McGinnity, TM
   Maguire, L
   Cai, RT
   Chen, MG
AF Wu, QingXiang
   McGinnity, T. M.
   Maguire, Liam
   Cai, Rongtai
   Chen, Meigui
TI A visual attention model based on hierarchical spiking neural networks
SO NEUROCOMPUTING
DT Article; Proceedings Paper
CT 7th International Conference on Intelligent Computing (ICIC)
CY AUG 11-14, 2011
CL Zhengzhou, PEOPLES R CHINA
DE Visual attention; Spiking neural network; Receptive field; Visual system
ID MECHANISMS
AB Based on the information processing functionalities of spiking neurons, hierarchical spiking neural networks are proposed to simulate visual attention. Using spiking neural networks inspired by the visual system, an image can be decomposed into multiple visual image components. Based on specific visual image components and image features, a visual attention system is proposed to extract attention areas according to top-down volition-controlled signals. The hierarchical spiking neural networks are constructed with a conductance-based integrate-and-fire neuron model and a set of specific receptive fields in different levels. The simulation algorithm and properties of the networks are detailed in this paper. Simulation results show that the attention system is able to perform visual attention of objects based on specific image components or features, and a demonstration shows how the attention system can detect a house in a visual image. Using the proposed saliency index, attention areas of interest can be extracted from spike rate maps of multiple visual pathways, such as ON/OFF colour pathways. According to this visual attention principle, the visual image processing system can quickly focus on specific areas while ignoring other areas. (C) 2012 Elsevier B.V. All rights reserved.
C1 [Wu, QingXiang; Cai, Rongtai; Chen, Meigui] Fujian Normal Univ, Sch Phys & OptoElect Technol, Fuzhou 350007, Peoples R China.
   [Wu, QingXiang; McGinnity, T. M.; Maguire, Liam] Univ Ulster, Sch Comp & Intelligent Syst, Intelligent Syst Res Ctr, Magee BT48 7JL, Londonderry, North Ireland.
RP Wu, QX (corresponding author), Fujian Normal Univ, Sch Phys & OptoElect Technol, Fuzhou 350007, Peoples R China.
EM q.wu@ulster.ac.uk; tm.mcginnity@ulster.ac.uk; lp.maguire@ulster.ac.uk;
   rtcai@fjnu.edu.cn; mgchen@fjnu.edu.cn
CR Anderson J.R., 2004, COGNITIVE PSYCHOL IT
   Gerstner W., 2002, SPIKING NEURON MODEL
   Hosoya T, 2005, NATURE, V436, P71, DOI 10.1038/nature03689
   Huang DS, 1998, IEEE T SYST MAN CY B, V28, P477, DOI 10.1109/3477.678658
   Huang DS, 1997, INT J PATTERN RECOGN, V11, P873, DOI 10.1142/S0218001497000391
   Huang DS, 1999, INT J PATTERN RECOGN, V13, P1083, DOI 10.1142/S0218001499000604
   Huang KQ, 2011, IEEE T SYST MAN CY B, V41, P307, DOI 10.1109/TSMCB.2009.2037923
   Huang Y., 2008, P IEEE INT C COMP VI
   Hush DR, 1993, IEEE SIGNAL PROC MAG, V10, P8, DOI 10.1109/79.180705
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Jessell T. M, 1981, PRINCIPLES NEURAL SC
   Kim IJ, 2008, NATURE, V452, P478, DOI 10.1038/nature06739
   Lauritzen TZ, 2009, J VISION, V9, DOI 10.1167/9.13.18
   Luo YJ, 2001, COGNITIVE BRAIN RES, V12, P371, DOI 10.1016/S0926-6410(01)00065-9
   Masland RH, 2001, NAT NEUROSCI, V4, P877, DOI 10.1038/nn0901-877
   Moody J, 1989, NEURAL COMPUT, V1, P281, DOI 10.1162/neco.1989.1.2.281
   Mu Y, 2009, COGN COMPUT, V1, P327, DOI 10.1007/s12559-009-9028-5
   Niebur E, 1998, ATTENTIVE BRAIN, P163
   Peelen MV, 2009, NATURE, V460, P94, DOI 10.1038/nature08103
   Qiaorong Zhang, 2010, 2010 Second International Conference on Communication Systems, Networks and Applications (ICCSNA 2010), P267, DOI 10.1109/ICCSNA.2010.5588713
   Reppas JB, 1997, NATURE, V388, P175, DOI 10.1038/40633
   Saalmann YB, 2007, SCIENCE, V316, P1612, DOI 10.1126/science.1139140
   Simon-Thomas ER, 2003, COGNITIVE BRAIN RES, V16, P457, DOI 10.1016/S0926-6410(03)00060-0
   Wu QX, 2008, NEUROCOMPUTING, V71, P2055, DOI 10.1016/j.neucom.2007.10.020
   Wu QX, 2007, LECT NOTES ARTIF INT, V4682, P26
   Wu QX, 2010, LECT NOTES COMPUT SC, V6215, P49
   Wu QX, 2008, NEURAL NETWORKS, V21, P1318, DOI 10.1016/j.neunet.2008.05.014
   Wu QX, 2005, LECT NOTES COMPUT SC, V3610, P420
   Ying Yang, 2010, Proceedings 2010 Sixth International Conference on Natural Computation (ICNC 2010), P2005, DOI 10.1109/ICNC.2010.5582402
NR 29
TC 28
Z9 30
U1 1
U2 28
PD SEP 20
PY 2013
VL 116
SI SI
BP 3
EP 12
DI 10.1016/j.neucom.2012.01.046
UT WOS:000320971900002
DA 2023-11-16
ER

PT J
AU Shen, GB
   Zhao, DC
   Zeng, Y
AF Shen, Guobin
   Zhao, Dongcheng
   Zeng, Yi
TI Backpropagation with biologically plausible spatiotemporal adjustment
   for training deep spiking neural networks
SO PATTERNS
DT Article
ID NEURONS
AB The spiking neural network (SNN) mimics the information-processing operation in the human brain. Directly applying backpropagation to the training of the SNN still has a performance gap compared with traditional deep neural networks. To address the problem, we propose a biologically plausible spatial adjustment that rethinks the relationship between membrane potential and spikes and realizes a reasonable adjustment of gradients to different time steps. It precisely controls the backpropagation of the error along the spatial dimension. Secondly, we propose a biologically plausible temporal adjustment to make the error propagate across the spikes in the temporal dimension, which overcomes the problem of the temporal dependency within a single spike period of traditional spiking neurons. We have verified our algorithm on several datasets, and the experimental results have shown that our algorithm greatly reduces network latency and energy consumption while also improving network performance.
C1 [Shen, Guobin; Zhao, Dongcheng; Zeng, Yi] Chinese Acad Sci, Res Ctr Brain Inspired Intelligence, Inst Automat, Beijing 100190, Peoples R China.
   [Zeng, Yi] Chinese Acad Sci, Ctr Excellence Brain Sci & Intelligence Technol, Shanghai 200031, Peoples R China.
   [Zeng, Yi] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Shen, Guobin; Zeng, Yi] Univ Chinese Acad Sci, Sch Future Technol, Beijing 100190, Peoples R China.
   [Zeng, Yi] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100190, Peoples R China.
RP Zeng, Y (corresponding author), Chinese Acad Sci, Res Ctr Brain Inspired Intelligence, Inst Automat, Beijing 100190, Peoples R China.; Zeng, Y (corresponding author), Chinese Acad Sci, Ctr Excellence Brain Sci & Intelligence Technol, Shanghai 200031, Peoples R China.; Zeng, Y (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.; Zeng, Y (corresponding author), Univ Chinese Acad Sci, Sch Future Technol, Beijing 100190, Peoples R China.; Zeng, Y (corresponding author), Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100190, Peoples R China.
EM yi.zeng@ia.ac.cn
CR Amir A, 2017, PROC CVPR IEEE, P7388, DOI 10.1109/CVPR.2017.781
   Bastos AM, 2012, NEURON, V76, P695, DOI 10.1016/j.neuron.2012.10.038
   Bereshpolova Y, 2007, J NEUROSCI, V27, P9392, DOI 10.1523/JNEUROSCI.2218-07.2007
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Chakraborty B., 2021, ARXIV, DOI 10.48550/2104.10719
   Cheng X, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1519
   de Andrade DC, 2018, Arxiv, DOI [arXiv:1808.08929, DOI 10.48550/ARXIV.1808.08929]
   Diehl PU, 2015, IEEE IJCNN
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Ding JH, 2021, Arxiv, DOI arXiv:2105.11654
   Dosovitskiy A., 2020, ARXIV, DOI DOI 10.48550/ARXIV
   Fang HW, 2020, Arxiv, DOI [arXiv:2003.02944, 10.48550/arXiv.2003.02944]
   Fang W, 2021, INCORPORATING LEARNA, P2661
   Fang W., 2020, SPIKINGJELLY
   Fitzsimonds RM, 1997, NATURE, V388, P439, DOI 10.1038/41267
   HEBB D. O., 1949
   Hu YF, 2020, Arxiv, DOI [arXiv:1805.01352, 10.48550/arXiv.1805.01352]
   Hunsberger E, 2015, Arxiv, DOI arXiv:1510.08829
   Jin Y., 2018, ADV NEURAL INFORM PR, P7005, DOI DOI 10.48550/ARXIV.1805.07866
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Kheradpisheh SR, 2016, NEUROCOMPUTING, V205, P382, DOI 10.1016/j.neucom.2016.04.029
   Kim S., 2020, AAAI CONF ARTIF INTE, V34, P11270
   Kim T, 2019, IEEE J-STSP, V13, P285, DOI 10.1109/JSTSP.2019.2909479
   Kim Y, 2022, Arxiv, DOI arXiv:2201.10355
   Kim Y, 2022, Arxiv, DOI arXiv:2104.03414
   Kim Y, 2021, FRONT NEUROSCI-SWITZ, V15, DOI 10.3389/fnins.2021.773954
   Kim Y, 2021, NEURAL NETWORKS, V144, P686, DOI 10.1016/j.neunet.2021.09.022
   Kim Y, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-98448-0
   Kok P., 2015, INTRO MODEL BASED CO, P221, DOI [10.1007/978-1-4939-2236-9_11, DOI 10.1007/978-1-4939-2236-9_11]
   Lee C, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00119
   Lee J., 2017, ARXIV, DOI DOI 10.48550/ARXIV.1703.01789
   Li HM, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00309
   Li PX, 2018, PATTERN RECOGN, V76, P323, DOI 10.1016/j.patcog.2017.11.007
   Li Y, 2021, Arxiv, DOI arXiv:2105.12917
   Lillicrap TP, 2020, NAT REV NEUROSCI, V21, P335, DOI 10.1038/s41583-020-0277-3
   Loshchilov I., 2016, ARXIV
   Loshchilov Ilya, 2019, Arxiv, DOI arXiv:1711.05101
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Masi I, 2018, SIBGRAPI, P471, DOI 10.1109/SIBGRAPI.2018.00067
   Orchard G, 2015, FRONT NEUROSCI-SWITZ, V9, DOI [10.3389/fhins.2015.00437, 10.3389/fnins.2015.00437]
   Panda P, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00653
   Paszke A, 2019, ADV NEUR IN, V32
   Rao RPN, 1999, NAT NEUROSCI, V2, P79, DOI 10.1038/4580
   Richards BA, 2019, CURR OPIN NEUROBIOL, V54, P28, DOI 10.1016/j.conb.2018.08.003
   Roelfsema PR, 2018, NAT REV NEUROSCI, V19, P166, DOI 10.1038/nrn.2018.6
   Roy K, 2019, NATURE, V575, P607, DOI 10.1038/s41586-019-1677-2
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Schiess M, 2016, PLOS COMPUT BIOL, V12, DOI 10.1371/journal.pcbi.1004638
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Shrestha SB, 2018, ADV NEUR IN, V31
   Venkatesha Y, 2021, IEEE T SIGNAL PROCES, V69, P6183, DOI 10.1109/TSP.2021.3121632
   Warden P, 2018, Arxiv, DOI arXiv:1804.03209
   Won M, 2020, INT CONF ACOUST SPEE, P536, DOI [10.1109/icassp40776.2020.9053669, 10.1109/ICASSP40776.2020.9053669]
   Wu YJ, 2019, AAAI CONF ARTIF INTE, P1311
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Xu Q, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1646
   Zhang W, 2020, ADV NEURAL INFORM PR, V33, P12022, DOI DOI 10.48550/ARXIV.2002.10085
   Zhang WR, 2019, ADV NEUR IN, V32
   Zhao D., ARXIV
   Zhao DC, 2020, FRONT COMPUT NEUROSC, V14, DOI 10.3389/fncom.2020.576841
   Zheng HL, 2021, AAAI CONF ARTIF INTE, V35, P11062
   Zou Z., 2019, ARXIV, DOI DOI 10.1109/JPROC.2023.3238524
NR 62
TC 6
Z9 6
U1 2
U2 5
PD JUN 10
PY 2022
VL 3
IS 6
AR 100522
DI 10.1016/j.patter.2022.100522
EA JUN 2022
UT WOS:000836532900006
DA 2023-11-16
ER

PT J
AU Lin, P
   Chang, S
   Wang, H
   Huang, QJ
   He, J
AF Lin, Peng
   Chang, Sheng
   Wang, Hao
   Huang, Qijun
   He, Jin
TI SpikeCD: a parameter-insensitive spiking neural network with clustering
   degeneracy strategy
SO NEURAL COMPUTING & APPLICATIONS
DT Article
DE Spiking neural network; Clustering degeneracy; RBF neuron; Strong
   robustness
ID NEURONS; CLASSIFICATION; INTEGRATE; CODE
AB A clustering degeneracy algorithm, called SpikeCD, with spiking RBF neurons for classification is proposed in this paper. Unlike traditional spiking RBF networks where their performance severely relies on the time-costing process of parameter optimization, SpikeCD uses a clustering degeneracy strategy to adjust the number and centers of spiking RBF neurons, which is insensitive to parameters. A supervised learning is followed to improve network's classification ability. Its performance is demonstrated on several benchmark datasets from the UCI Machine Learning Repository and image datasets. The results show SpikeCD can achieve good classification accuracy with simple structure. Moreover, the variation of parameters has a little effect on it. We hope this algorithm can be a new inspiration for improving the robustness of evolving spiking neural networks and other machine learning methods.
C1 [Lin, Peng; Chang, Sheng; Wang, Hao; Huang, Qijun; He, Jin] Wuhan Univ, Wuhan 430072, Hubei, Peoples R China.
   [Chang, Sheng] Wuhan Univ, Suzhou Inst, Suzhou 215123, Peoples R China.
RP Chang, S (corresponding author), Wuhan Univ, Wuhan 430072, Hubei, Peoples R China.; Chang, S (corresponding author), Wuhan Univ, Suzhou Inst, Suzhou 215123, Peoples R China.
EM changsheng@whu.edu.cn
CR [Anonymous], 2012 INT JOINT C NEU, DOI DOI 10.1109/IJCNN.2012.6252439
   [Anonymous], 2015, 2015 INT JOINT C NEU, DOI DOI 10.1109/IJCNN.2015.7280592
   Belatreche A., 2015, 2015 INT JOINT C NEU, P1
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   Butts DA, 2007, NATURE, V449, P92, DOI [10.1038/nature06105, 10.1038/natureO6105]
   Delorme A, 1999, NEUROCOMPUTING, V26-7, P989, DOI 10.1016/S0925-2312(99)00095-8
   Delorme A, 2001, NEUROCOMPUTING, V38, P539, DOI 10.1016/S0925-2312(01)00403-9
   Delorme A, 2001, NEURAL NETWORKS, V14, P795, DOI 10.1016/S0893-6080(01)00049-1
   Doborjeh MG, 2016, IEEE T BIO-MED ENG, V63, P1830, DOI 10.1109/TBME.2015.2503400
   Florian RV, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0040233
   Gerstner W., 2002, SPIKING NEURON MODEL
   HABERLY LB, 1985, CHEM SENSES, V10, P219, DOI 10.1093/chemse/10.2.219
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   Kasabov N, 2014, NEUROCOMPUTING, V134, P269, DOI 10.1016/j.neucom.2013.09.049
   Kasabov NK, 2014, NEURAL NETWORKS, V52, P62, DOI 10.1016/j.neunet.2014.01.006
   Kasinski A, 2005, LECT NOTES COMPUT SC, V3696, P145, DOI 10.1007/11550822_24
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Mehta MR, 2002, NATURE, V417, P741, DOI 10.1038/nature00807
   Natschlager T, 1998, NETWORK-COMP NEURAL, V9, P319, DOI 10.1088/0954-898X/9/3/003
   OKEEFE J, 1993, HIPPOCAMPUS, V3, P317, DOI 10.1002/hipo.450030307
   Oyedotun OK, 2017, NEURAL COMPUT APPL, V28, P3941, DOI 10.1007/s00521-016-2294-8
   Ponulak F, 2008, INT J APPL MATH COMP, V18, P117, DOI 10.2478/v10006-008-0011-1
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Schrauwen B, 2004, P PROR WORKSH
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Wang JL, 2017, IEEE T NEUR NET LEAR, V28, P30, DOI 10.1109/TNNLS.2015.2501322
   Wang JL, 2014, NEUROCOMPUTING, V144, P526, DOI 10.1016/j.neucom.2014.04.017
   Wang WW, 2012, IEEE T NEUR NET LEAR, V23, P1574, DOI 10.1109/TNNLS.2012.2208477
   Wysoski SG, 2006, LECT NOTES COMPUT SC, V4131, P61
   Yu Q, 2016, IEEE T NEUR NET LEAR, V27, P621, DOI 10.1109/TNNLS.2015.2416771
   Yu Q, 2013, IEEE T NEUR NET LEAR, V24, P1539, DOI 10.1109/TNNLS.2013.2245677
   Zhang HJ, 2017, IEEE T IND INFORM, V13, P520, DOI 10.1109/TII.2016.2605629
   Zhang H, 2016, IEEE T NEUR NET LEAR, V27, P2537, DOI 10.1109/TNNLS.2015.2496281
NR 37
TC 13
Z9 13
U1 0
U2 13
PD AUG
PY 2019
VL 31
IS 8
BP 3933
EP 3945
DI 10.1007/s00521-017-3336-6
UT WOS:000485922300047
DA 2023-11-16
ER

PT C
AU Vigneron, A
   Martinet, J
AF Vigneron, Alex
   Martinet, Jean
GP IEEE
TI A critical survey of STDP in Spiking Neural Networks for Pattern
   Recognition
SO 2020 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN) held as part
   of the IEEE World Congress on Computational Intelligence (IEEE WCCI)
CY JUL 19-24, 2020
CL ELECTR NETWORK
DE Spiking Neural Networks; Machine Learning; Artificial Neural Networks;
   Pattern Recognition; Unsupervised Learning; STDP; Bio-inspiration
ID TIMING-DEPENDENT PLASTICITY; INTELLIGENCE; NEURONS; MODEL
AB The bio-inspired concept of Spike-Timing-Dependent Plasticity (STDP) derived from neurobiology is increasingly used in Spiking Neural Networks (SNNs) nowadays. Mostly found in unsupervised learning, though recent work has shown its usefulness in supervised or reinforced paradigms too, STDP is a key element to understanding SNN architectures' learning process. This review introduces a categorisation of its several variants and discusses their specificities and applications, from a pattern recognition perspective. It gathers a variety of definitions used in machine learning for pattern recognition. It provides relevant information for research communities of various backgrounds looking for an overview of this field.
C1 [Vigneron, Alex] Univ Lille, CNRS, Cent Lille, UMR 9189,CRIStAL, F-59000 Lille, France.
   [Martinet, Jean] Univ Cote Azur, CNRS, I3S, Nice, France.
RP Vigneron, A (corresponding author), Univ Lille, CNRS, Cent Lille, UMR 9189,CRIStAL, F-59000 Lille, France.
EM a.vigneron@protonmail.com; jean.martinet@univ-cotedazur.fr
CR Attneave F., 1950, AJP, V63, P633, DOI [DOI 10.1002/SCE.37303405110.-633, DOI 10.2307/1418888, 10.2307/1418888]
   Bichler O, 2012, NEURAL NETWORKS, V32, P339, DOI 10.1016/j.neunet.2012.02.022
   Bing ZS, 2018, IEEE INT CONF ROBOT, P4725
   Bing ZS, 2018, FRONT NEUROROBOTICS, V12, DOI 10.3389/fnbot.2018.00035
   Brette R, 2015, FRONT SYST NEUROSCI, V9, DOI 10.3389/fnsys.2015.00151
   Burbank K. S., 2012, PLOS COMPUTATIONAL B, V8, P1
   Burbank KS, 2015, PLOS COMPUT BIOL, V11, DOI 10.1371/journal.pcbi.1004566
   Caporale N, 2008, ANNU REV NEUROSCI, V31, P25, DOI 10.1146/annurev.neuro.31.060407.125639
   Danneville F, 2019, SOLID STATE ELECTRON, V153, P88, DOI 10.1016/j.sse.2019.01.002
   Falez P, 2019, PATTERN RECOGN, V93, P418, DOI 10.1016/j.patcog.2019.04.016
   HOLLAND PW, 1986, J AM STAT ASSOC, V81, P945, DOI 10.2307/2289064
   Jin YYZ, 2016, IEEE IJCNN, P1158, DOI 10.1109/IJCNN.2016.7727328
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Khosla A, 2012, LECT NOTES COMPUT SC, V7572, P158, DOI 10.1007/978-3-642-33718-5_12
   KLOPF AH, 1988, PSYCHOBIOLOGY, V16, P85
   Krunglevicius D., 2016, ADV ARTIFICIAL NEURA, V2016, P1, DOI [10.1155/2016/1746514, DOI 10.1155/2016/1746514]
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Masquelier T. E., 2007, PLOS COMPUTATIONAL B
   Mozafari M, 2018, IEEE T NEUR NET LEAR, V29, P6178, DOI 10.1109/TNNLS.2018.2826721
   Nessler B., ADV NEURAL INFORM PR, V22
   Paredes-Valles F., 2018, PATTERN ANAL MACHINE
   Pei J, 2019, NATURE, V572, P106, DOI 10.1038/s41586-019-1424-8
   Rolls ET, 2000, NEURAL COMPUT, V12, P2547, DOI 10.1162/089976600300014845
   Roy K, 2019, NATURE, V575, P607, DOI 10.1038/s41586-019-1677-2
   Sboev A, 2016, J PHYS CONF SER, V681, DOI 10.1088/1742-6596/681/1/012013
   Shim MS, 2017, IEEE IJCNN, P3098, DOI 10.1109/IJCNN.2017.7966242
   Sjostrom J., 2010, SCHOLARPEDIA 52 REVI, V5, P1362, DOI [10.4249%2Fscholarpedia.1362, DOI 10.4249/SCHOLARPEDIA.1362]
   Sutton RS, 2018, ADAPT COMPUT MACH LE, P1
   Taherkhani A., 2020, NEURAL NETWORKS, V12
   Tanner B., 2005, P 22 INT C MACH LEAR, P888, DOI DOI 10.1145/1102351.1102463
   Tavanaei A, 2016, IEEE IJCNN, P307, DOI 10.1109/IJCNN.2016.7727213
   TORRALBA A, 2011, PROC CVPR IEEE, P1521, DOI DOI 10.1109/CVPR.2011.5995347
   Verzi S. J., 2018, NEURAL COMPUTATION, V30
   Wang TL, 2019, IEEE I CONF COMP VIS, P5309, DOI 10.1109/ICCV.2019.00541
   Xie XH, 2001, ADV NEUR IN, V13, P350
NR 36
TC 10
Z9 10
U1 1
U2 5
PY 2020
DI 10.1109/ijcnn48605.2020.9207239
UT WOS:000626021405015
DA 2023-11-16
ER

PT C
AU Fricker, P
   Chauhan, T
   Hurter, C
   Cottereau, B
AF Fricker, Paul
   Chauhan, Tushar
   Hurter, Christophe
   Cottereau, Benoit
BE Farinella, GM
   Radeva, P
   Bouatouch, K
TI Event-based Extraction of Navigation Features from Unsupervised Learning
   of Optic Flow Patterns
SO PROCEEDINGS OF THE 17TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER
   VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS (VISAPP),
   VOL 5
SE VISIGRAPP
DT Proceedings Paper
CT 17th International Joint Conference on Computer Vision, Imaging and
   Computer Graphics Theory and Applications (VISIGRAPP) / 17th
   International Conference on Computer Vision Theory and Applications
   (VISAPP)
CY FEB 06-08, 2022
CL ELECTR NETWORK
DE Optic Flow; Spiking Neural Network; Unsupervised Learning; STDP
ID VISION SENSORS; SPIKE; NEURONS; POWER
AB We developed a Spiking Neural Network composed of two layers that processes event-based data captured by a dynamic vision sensor during navigation conditions. The training of the network was performed using a biologically plausible and unsupervised learning rule, Spike-Timing-Dependent Plasticity. With such an approach, neurons in the network naturally become selective to different components of optic flow, and a simple classifier is able to predict self-motion properties from the neural population output spiking activity. Our network has a simple architecture and a restricted number of neurons. Therefore, it is easy to implement on a neuromorphic chip and could be used for embedded applications necessitating low energy consumption.
C1 [Fricker, Paul; Chauhan, Tushar; Cottereau, Benoit] CNRS, Ctr Rech Cerveau & Cognit, UMR5549, Toulouse, France.
   [Fricker, Paul; Hurter, Christophe] Ecole Natl Aviat Civile, Toulouse, France.
RP Fricker, P (corresponding author), CNRS, Ctr Rech Cerveau & Cognit, UMR5549, Toulouse, France.; Fricker, P (corresponding author), Ecole Natl Aviat Civile, Toulouse, France.
CR Aamir SA, 2018, IEEE T CIRCUITS-I, V65, P4299, DOI 10.1109/TCSI.2018.2840718
   Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   Asghar MS, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21134462
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Bichler O, 2012, NEURAL NETWORKS, V32, P339, DOI 10.1016/j.neunet.2012.02.022
   Brandli C, 2014, IEEE J SOLID-ST CIRC, V49, P2333, DOI 10.1109/JSSC.2014.2342715
   Chankyu Lee, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12374), P366, DOI 10.1007/978-3-030-58526-6_22
   Chauhan T, 2018, J NEUROSCI, V38, P9563, DOI 10.1523/JNEUROSCI.1259-18.2018
   Dan Y, 2004, NEURON, V44, P23, DOI 10.1016/j.neuron.2004.09.007
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Debat G, 2021, FRONT COMPUT NEUROSC, V15, DOI 10.3389/fncom.2021.658764
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Gallego G, 2017, IEEE ROBOT AUTOM LET, V2, P632, DOI 10.1109/LRA.2016.2647639
   Gerstner W., 2002, SPIKING NEURON MODEL
   Indiveri G, 2006, IEEE T NEURAL NETWOR, V17, P211, DOI 10.1109/TNN.2005.860850
   Lakshmi A, 2019, WIRES DATA MIN KNOWL, V9, DOI 10.1002/widm.1310
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   MINK JW, 1981, AM J PHYSIOL, V241, pR203, DOI 10.1152/ajpregu.1981.241.3.R203
   Mueggler E, 2017, INT J ROBOT RES, V36, P142, DOI 10.1177/0278364917691115
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Neil D, 2016, IEEE INT SYMP CIRC S, P2282, DOI 10.1109/ISCAS.2016.7539039
   Nguyen A., 2019, P IEEE CVF C COMP VI
   Oudjail V, 2020, VISAPP: PROCEEDINGS OF THE 15TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS, VOL 4: VISAPP, P853, DOI 10.5220/0009324908530860
   Oudjail V, 2019, VISAPP: PROCEEDINGS OF THE 14TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS, VOL 4, P389, DOI 10.5220/0007397303890394
   Paredes-Vallés F, 2020, IEEE T PATTERN ANAL, V42, P2051, DOI 10.1109/TPAMI.2019.2903179
   Posch C, 2014, P IEEE, V102, P1470, DOI 10.1109/JPROC.2014.2346153
   Stromatias E, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00350
   Zenke F, 2021, NEURON, V109, P571, DOI 10.1016/j.neuron.2021.01.009
   Zhu A., 2018, ROBOTICS SCI SYSTEMS, P1, DOI 10.15607/RSS.2018.XIV.062
   Zhu AZ, 2019, PROC CVPR IEEE, P989, DOI 10.1109/CVPR.2019.00108
NR 30
TC 0
Z9 0
U1 0
U2 0
PY 2022
BP 702
EP 710
DI 10.5220/0010836200003124
UT WOS:000777505000075
DA 2023-11-16
ER

PT C
AU Hamed, HNA
   Saleh, AY
   Shamsuddin, SM
AF Hamed, Haza Nuzly Abdull
   Saleh, Abdulrazak Yahya
   Shamsuddin, Siti Mariyam
BE Hu, X
   Xia, Y
   Zhang, Y
   Zhao, D
TI A Novel K-Means Evolving Spiking Neural Network Model for Clustering
   Problems
SO ADVANCES IN NEURAL NETWORKS - ISNN 2015
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 12th International Symposium on Neural Networks (ISNN)
CY OCT 15-18, 2015
CL Jeju, SOUTH KOREA
DE Clustering; Evolving Spiking Neural Networks; K-ESNN; K-means; Spiking
   Neural Network
AB In this paper, a novel K-means evolving spiking neural network (K-ESNN) model for clustering problems has been presented. K-means has been utilised to improve the original ESNN model. This model enhances the flexibility of the ESNN algorithm in producing better solutions to overcoming the disadvantages of K-means. Several standard data sets from UCI machine learning are used for evaluating the performance of this model. It has been found that the K-ESNN provides competitive results in clustering accuracy and speed performance measures compared to the standard K-means. More discussion is provided to prove the effectiveness of the new model in clustering problems.
C1 [Hamed, Haza Nuzly Abdull] Univ Teknol Malaysia, Fac Comp, Soft Comp Res Grp, Skudai 81310, Johor, Malaysia.
   [Saleh, Abdulrazak Yahya; Shamsuddin, Siti Mariyam] Univ Teknol Malaysia, UTM Big Data Ctr, Skudai 81310, Johor, Malaysia.
RP Hamed, HNA (corresponding author), Univ Teknol Malaysia, Fac Comp, Soft Comp Res Grp, Skudai 81310, Johor, Malaysia.
EM haza@utm.my; abdulrazakalhababi@gmail.com; mariyam@utm.my
CR [Anonymous], INT J ARTIF INTELL
   Berkhin P, 2006, GROUPING MULTIDIMENSIONAL DATA: RECENT ADVANCES IN CLUSTERING, P25
   Bock H.-H., 2007, SELECTED CONTRIBUTIO, P161, DOI [10.1007/978-3-540-73560-1_15, DOI 10.1007/978-3-540-73560-1_15, DOI 10.1093/humrep/dew218]
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Fayyad U, 1996, AI MAG, V17, P37
   Firouzi B., 2008, WORLD ACAD SCI ENG T, V36, P605
   Hamed HNA, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P2653, DOI 10.1109/IJCNN.2011.6033565
   Hamed HNA, 2009, LECT NOTES COMPUT SC, V5864, P611, DOI 10.1007/978-3-642-10684-2_68
   Jain AK, 2010, PATTERN RECOGN LETT, V31, P651, DOI 10.1016/j.patrec.2009.09.011
   Kasabov NK, 2014, NEURAL NETWORKS, V52, P62, DOI 10.1016/j.neunet.2014.01.006
   Kotsiantis S., 2004, WSEAS T INF SCI APPL, V1, P73
   MacQueen J, 1967, 5 BERK S MATH STAT P, DOI DOI 10.1007/S11665-016-2173-6
   Mandloi M., 2014, SURVEY CLUSTERING AL
   Patel VR, 2011, COMM COM INF SC, V250, P307
   Saleh A.Y., 2014, INT J ADV SOFT COMPU, V6
   Saleh A.Y., 2014, INT C REC TRENDS INF, V13
   Schliebs S, 2013, EVOL SYST-GER, V4, P87, DOI 10.1007/s12530-013-9074-9
   Schliebs S, 2009, NEURAL NETWORKS, V22, P623, DOI 10.1016/j.neunet.2009.06.038
   Thorpe S., 1997, CAN HUMAN VISUAL SYS
   Wu J., 2012, ADV K MEANS CLUSTERI, DOI 10.1007/978-3-642-29807-3
   Wu XD, 2008, KNOWL INF SYST, V14, P1, DOI 10.1007/s10115-007-0114-2
NR 21
TC 1
Z9 1
U1 0
U2 1
PY 2015
VL 9377
BP 382
EP 389
DI 10.1007/978-3-319-25393-0_42
UT WOS:000374293300042
DA 2023-11-16
ER

PT C
AU Mueller, E
   Auge, D
   Knoll, A
AF Mueller, Etienne
   Auge, Daniel
   Knoll, Alois
BE Xiong, N
   Li, M
   Li, K
   Xiao, Z
   Liao, L
   Wang, L
TI Exploiting Inhomogeneities of Subthreshold Transistors as Populations of
   Spiking Neurons
SO ADVANCES IN NATURAL COMPUTATION, FUZZY SYSTEMS AND KNOWLEDGE DISCOVERY,
   ICNC-FSKD 2022
SE Lecture Notes on Data Engineering and Communications Technologies
DT Proceedings Paper
CT 18th International Conference on Natural Computation, Fuzzy Systems and
   Knowledge Discovery (ICNC-FSKD)
CY JUL 30-AUG 01, 2022
CL Fuzhou, PEOPLES R CHINA
DE Spiking neural networks; Conversion; Long short-term memory;
   Subthreshold analog neuromorphic hardware
ID NETWORK; COINCIDENCE
AB As machine learning applications are becoming increasingly more powerful and are deployed to an increasing number of different appliances, the need for energy-efficient implementations is rising. To meet this demand, a promising field of research is the adoption of spiking neural networks jointly used with neuromorphic hardware, as energy is solely consumed when information is processed. The approach that maximizes energy efficiency, an analog layout with transistors operating in subthreshold mode, suffers from inhomogeneities such as device mismatch which makes it challenging to create a uniform threshold necessary for spiking neurons. Furthermore, previous work mainly focused on spiking feedforward or convolutional networks, as neural networks based on rectified linear units translate well to rate coded spiking neurons. Consequently, the processing of continuous sequential data remains challenging, as neural networks, based on long short-term memory or gated recurrent units as recurrent cells, utilize sigmoid and tanh as activation functions. We show how these two disadvantages can compensate for each other, as a population of spiking neurons with a normally distributed threshold can reliably represent the sigmoid and tanh activation functions. With this finding we present a novel method how to convert a long short-term memory recurrent network to a spiking neural network. Although computationally expensive in a simulation environment, this approach offers a significant opportunity for energy reduction and hardware feasibility as it leverages the often unwanted process variance as a design feature.
C1 [Mueller, Etienne; Auge, Daniel; Knoll, Alois] Tech Univ Munich, Dept Informat, Munich, Germany.
   [Auge, Daniel] Infineon Technol AG, Munich, Germany.
RP Mueller, E (corresponding author), Tech Univ Munich, Dept Informat, Munich, Germany.
EM etienne.mueller@tum.de; daniel.auge@tum.de; alois.knoll@tum.de
CR Pérez-Carrasco JA, 2013, IEEE T PATTERN ANAL, V35, P2706, DOI 10.1109/TPAMI.2013.71
   Bellec G, 2018, ADV NEUR IN, V31
   Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Chicca E, 2003, IEEE T NEURAL NETWOR, V14, P1297, DOI 10.1109/TNN.2003.816367
   Cho KYHY, 2014, Arxiv, DOI arXiv:1406.1078
   Diehl PU, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON REBOOTING COMPUTING (ICRC)
   Diehl PU, 2015, IEEE IJCNN
   ELMAN JL, 1990, COGNITIVE SCI, V14, P179, DOI 10.1207/s15516709cog1402_1
   Furber SB, 2013, IEEE T COMPUT, V62, P2454, DOI 10.1109/TC.2012.142
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Kingma DP., 2017, ARXIV
   Liu S.C., 2002, ANALOG VLSI CIRCUITS
   Liu SC, 2015, EVENT-BASED NEUROMORPHIC SYSTEMS, P1, DOI 10.1002/9781118927601
   Maas Andrew L., 2011, P 49 ANN M ASS COMP
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   Meier K, 2015, 2015 IEEE INTERNATIONAL ELECTRON DEVICES MEETING (IEDM)
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Mueller Etienne, 2021, 2021 International Joint Conference on Neural Networks (IJCNN), DOI 10.1109/IJCNN52387.2021.9533874
   Nair V., 2010, PROC 27 INT C INT C
   Pfister JP, 2006, NEURAL COMPUT, V18, P1318, DOI 10.1162/neco.2006.18.6.1318
   Poon CS, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00108
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Shrestha A, 2017, ICCAD-IEEE ACM INT, P631, DOI 10.1109/ICCAD.2017.8203836
   Zambrano D, 2016, Arxiv, DOI arXiv:1609.02053
NR 26
TC 0
Z9 0
U1 1
U2 1
PY 2023
VL 153
BP 483
EP 492
DI 10.1007/978-3-031-20738-9_55
UT WOS:000964184200055
DA 2023-11-16
ER

PT C
AU Loyez, C
   Carpentier, K
   Sourikopoulos, I
   Danneville, F
AF Loyez, Christophe
   Carpentier, Kevin
   Sourikopoulos, Ilias
   Danneville, Francois
GP IEEE
TI Subthreshold neuromorphic devices for Spiking Neural Networks applied to
   embedded AI
SO 2021 19TH IEEE INTERNATIONAL NEW CIRCUITS AND SYSTEMS CONFERENCE
   (NEWCAS)
SE IEEE International New Circuits and Systems Conference
DT Proceedings Paper
CT 19th IEEE International New Circuits and Systems Conference (NEWCAS)
CY JUN 13-16, 2021
CL ELECTR NETWORK
DE Spiking Neural Network; subthreshold; energy efficiency
AB Energy autonomy is one of the major challenges of embedded Artificial Intelligence. Among the candidate technologies likely to take up such a challenge, spiking neural networks are the most promising because of both their spatio-temporal and sparse representation of the information. In this context, this paper presents a neuromorphic approach based on an industrial CMOS technology and adopting an entirely subthreshold mode of operation (supply voltage VDD lower than the MOSFET threshold voltage). The detailed topologies of fabricated artificial neurons and synapses are presented as well as experimental results, validating an energy consumption of the order of a few femto-Joules per spike. Also, an arrangement of neurons and synapses is proposed to qualify experimentally this subthreshold approach in the perspective of highly energy efficient spiking neural networks.
C1 [Loyez, Christophe; Danneville, Francois] Univ Lille, Inst Elect Microelect & Nanotechnol IEMN, CNRS, Univ Polytech Hauts France,UMR 8520,IEMN, F-59000 Lille, France.
   [Carpentier, Kevin; Sourikopoulos, Ilias] SATT NORD, 25 Ave Charles St Venand, Lille, France.
RP Loyez, C (corresponding author), Univ Lille, Inst Elect Microelect & Nanotechnol IEMN, CNRS, Univ Polytech Hauts France,UMR 8520,IEMN, F-59000 Lille, France.
CR [Anonymous], 1989, ANALOG VLSI NEURAL S
   Bartolozzi C, 2007, NEURAL COMPUT, V19, P2581, DOI 10.1162/neco.2007.19.10.2581
   Danneville F, 2019, SOLID STATE ELECTRON, V153, P88, DOI 10.1016/j.sse.2019.01.002
   Danneville F., Patent, Patent No. 2019219618
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   MORRIS C, 1981, BIOPHYS J, V35, P193, DOI 10.1016/S0006-3495(81)84782-0
   Sourikopoulos I, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00123
   Vittoz EA, 2009, IEEE ASIAN SOLID STA, P129, DOI 10.1109/ASSCC.2009.5357240
NR 9
TC 0
Z9 0
U1 0
U2 0
PY 2021
DI 10.1109/NEWCAS50681.2021.9462779
UT WOS:000713009500050
DA 2023-11-16
ER

PT C
AU Skontranis, M
   Sarantoglou, G
   Deligiannidis, S
   Bogris, A
   Mesaritakis, C
AF Skontranis, M.
   Sarantoglou, G.
   Deligiannidis, S.
   Bogris, A.
   Mesaritakis, C.
GP IEEE
TI Unsupervised Image Classification Through Time-Multiplexed Photonic
   Multi-Layer Spiking Convolutional Neural Network
SO 2020 EUROPEAN CONFERENCE ON OPTICAL COMMUNICATIONS (ECOC)
DT Proceedings Paper
CT European Conference on Optical Communications (ECOC)
CY DEC 06-10, 2020
CL ELECTR NETWORK
AB We present results of a deep photonic spiking convolutional neural network, based on two section VCSELs, targeting image classification. Training is based on unsupervised spike-timing dependent plasticity, whereas neuron time-multiplexing and ultra-fast response are exploited towards a a reduction of the physical neuron count by 90%.
C1 [Skontranis, M.; Sarantoglou, G.; Mesaritakis, C.] Univ Aegean, Dept Informat & Commun Syst Engn, Palama 2, Samos 83200, Greece.
   [Deligiannidis, S.; Bogris, A.] Univ West Attica, Dept Informat & Comp Engn, Ag Spiridonos 12243, Egaleo, Greece.
RP Skontranis, M (corresponding author), Univ Aegean, Dept Informat & Commun Syst Engn, Palama 2, Samos 83200, Greece.
EM mskontranis@icsd.aegean.gr
CR Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Liqun L, 2016, PRINCIPLES NEUROBIOL
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Miller DAB, 2017, J LIGHTWAVE TECHNOL, V35, P346, DOI 10.1109/JLT.2017.2647779
   Pruncal P. R., 2016, ADV OPT PHOTONICS, V8, P230
   Roberts JA, 2020, INT J HUM-COMPUT INT, V36, P386, DOI 10.1080/10447318.2019.1646517
   Robertson J, 2007, ARXIV PREPRINT ARXIV
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   Xiang S, 2018, J LIGHTWAVE TECHNOL, P1
   Xiang SY, 2021, IEEE J SEL TOP QUANT, V27, DOI 10.1109/JSTQE.2020.3005589
   Xiang SY, 2019, IEEE J SEL TOP QUANT, V25, DOI 10.1109/JSTQE.2019.2911565
NR 12
TC 0
Z9 0
U1 0
U2 1
PY 2020
DI 10.1109/ECOC48923.2020.9333320
UT WOS:000662178500195
DA 2023-11-16
ER

PT J
AU Al-Jamali, NAS
   Al-Raweshidy, HS
AF Al-Jamali, Nadia Adnan Shiltagh
   Al-Raweshidy, Hamed S.
TI Modified Elman Spike Neural Network for Identification and Control of
   Dynamic System
SO IEEE ACCESS
DT Article
DE Identification; dynamic system; modified Elman spike neural network;
   spike neural network
AB The utilization of conventional modeling strategies in the identification and control of a nonlinear dynamical system suffers from some weaknesses. These include absence of precise, conventional knowledge about the system, a high degree of uncertainty, strongly nonlinear and time-varying behavior. In this paper, a modified training algorithm for the identification and control of a nonlinear system using a soft-computing approach is proposed. Specifically, a modified structure of the Elman neural network with spike neural networks is proposed. This modified structure includes self-feedback, which provides a dynamic trace of the training algorithm. This self-feedback has weights, which can be trained during the training process. The simulation results show that the modified structure with the modified training algorithm is capable of the identification and control of a dynamic system in a more robust manor than when solely applying the other types of neural networks by 70% in terms of minimization of the percentage of error.
C1 [Al-Jamali, Nadia Adnan Shiltagh] Univ Baghdad, Dept Comp Engn, Baghdad 10071, Iraq.
   [Al-Jamali, Nadia Adnan Shiltagh; Al-Raweshidy, Hamed S.] Brunel Univ London, Coll Engn Design & Phys Sci, Elect & Comp Engn Dept, Uxbridge UB8 3PH, Middx, England.
RP Al-Jamali, NAS (corresponding author), Univ Baghdad, Dept Comp Engn, Baghdad 10071, Iraq.; Al-Jamali, NAS (corresponding author), Brunel Univ London, Coll Engn Design & Phys Sci, Elect & Comp Engn Dept, Uxbridge UB8 3PH, Middx, England.
EM nadiaadnanshiltagh.aljamli@brunel.ac.uk
CR Abiyev RH, 2012, IEEE ASME INT C ADV, P1030, DOI 10.1109/AIM.2012.6265983
   [Anonymous], 2018, 2018 INT JOINT C NEU
   Dahmani K, 2019, I C SCI TECH AUTO CO, P59, DOI [10.1109/STA.2019.8717273, 10.1109/sta.2019.8717273]
   Deo R, 2016, IEEE IJCNN, P489, DOI 10.1109/IJCNN.2016.7727239
   Dong L, 2019, IEEE T CYBERNETICS, V49, P4206, DOI 10.1109/TCYB.2018.2859801
   El-Sousy FFM, 2019, IEEE T IND APPL, V55, P1998, DOI 10.1109/TIA.2018.2876642
   ELMAN JL, 1990, COGNITIVE SCI, V14, P179, DOI 10.1207/s15516709cog1402_1
   Guan SP, 2018, CHIN CONT DECIS CONF, P332, DOI 10.1109/CCDC.2018.8407154
   Han FY, 2018, INT C ADV MECH SYST, P359, DOI 10.1109/ICAMechS.2018.8507102
   Khun J, 2019, MEDD C EMBED COMPUT, P88
   Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508
   Lin CM, 2015, IEEE T SYST MAN CY-S, V45, P1281, DOI 10.1109/TSMC.2015.2389752
   Mahmmod BM, 2019, IEEE ACCESS, V7, P103485, DOI 10.1109/ACCESS.2019.2929864
   Mostafa Hesham, 2018, IEEE Trans Neural Netw Learn Syst, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Oniz Y., 2013, 2013 IEEE International Conference on Mechatronics (ICM), P94, DOI 10.1109/ICMECH.2013.6518517
   Qiu HN, 2018, 2018 IEEE SYMPOSIUM SERIES ON COMPUTATIONAL INTELLIGENCE (IEEE SSCI), P1367, DOI 10.1109/SSCI.2018.8628848
   Sadek AM, 2018, PROCEEDINGS OF 2018 13TH INTERNATIONAL CONFERENCE ON COMPUTER ENGINEERING AND SYSTEMS (ICCES), P481, DOI 10.1109/ICCES.2018.8639194
   Suprapto BY, 2018, 2018 INTERNATIONAL CONFERENCE ON ELECTRICAL ENGINEERING AND COMPUTER SCIENCE (ICECOS), P79, DOI 10.1109/ICECOS.2018.8605240
   Taghavifar H, 2019, IEEE T VEH TECHNOL, V68, P6293, DOI 10.1109/TVT.2019.2914027
   Taherkhani A, 2018, IEEE T NEUR NET LEAR, V29, P5394, DOI 10.1109/TNNLS.2018.2797801
   Thupae R, 2018, IEEE IND ELEC, P4645, DOI 10.1109/IECON.2018.8591178
   Tian YF, 2022, IEEE T CYBERNETICS, V52, P4574, DOI [10.1109/TCYB.2020.3030503, 10.1109/TNNLS.2020.3008691]
   Xu DZ, 2018, IEEE T IND ELECTRON, V65, P6625, DOI 10.1109/TIE.2017.2767544
   Zhang L, 2019, IEEE ACCESS, V7, P75132, DOI 10.1109/ACCESS.2019.2920867
NR 24
TC 12
Z9 12
U1 0
U2 10
PY 2020
VL 8
BP 61246
EP 61254
DI 10.1109/ACCESS.2020.2984311
UT WOS:000527413300009
DA 2023-11-16
ER

PT C
AU Yusoff, N
   Ahmad, FK
AF Yusoff, Nooraini
   Ahmad, Farzana Kabir
GP IEEE
TI Stimulus-stimulus Association via Reinforcement Learning in Spiking
   Neural Network
SO 2013 13TH INTERNATIONAL CONFERENCE ON INTELLIGENT SYSTEMS DESIGN AND
   APPLICATIONS (ISDA)
SE International Conference on Intelligent Systems Design and Applications
DT Proceedings Paper
CT 13th International Conference on Intelligent Systems Design and
   Applications (ISDA)
CY DEC 08-10, 2013
CL Bangi, MALAYSIA
DE component; associative learning; spiking neural network; reinforcement
   learning; spike-time dependent plasticity; priming effect
ID INTERFERENCE; MODEL
AB In this paper, we propose an algorithm that performs stimulus-stimulus association via reinforcement learning. In particular, we develop a recurrent network with dynamic properties of Izhikevich spiking neuron model and train the network to associate a stimulus pair using reward modulated spike-time dependent plasticity. The learning algorithm associates a prime stimulus, known as the predictor, with a second stimulus, known as the choice, comes after an inter-stimulus interval. The influence of the prime stimulus on the neural response after the onset of the later stimulus is then observed. A series of probe trials resemble the retrospective and prospective activities in human response processing.
C1 [Yusoff, Nooraini; Ahmad, Farzana Kabir] Univ Utara Malaysia, Coll Arts & Sci, Sch Comp, Sintok 06010, Kedah, Malaysia.
RP Yusoff, N (corresponding author), Univ Utara Malaysia, Coll Arts & Sci, Sch Comp, Sintok 06010, Kedah, Malaysia.
EM nooraini@uum.edu.my; farzana58@uum.edu.my
CR Barto A, 1998, INTRO REINFORCEMENT, V1st
   COHEN JD, 1990, PSYCHOL REV, V97, P332, DOI 10.1037/0033-295X.97.3.332
   Cutsuridis V, 2009, NEURAL NETWORKS, V22, P1120, DOI 10.1016/j.neunet.2009.07.009
   DUNBAR K, 1984, J EXP PSYCHOL HUMAN, V10, P622, DOI 10.1037/0096-1523.10.5.622
   Erickson CA, 1999, J NEUROSCI, V19, P10404
   Filippova MG, 2011, SPAN J PSYCHOL, V14, P20, DOI 10.5209/rev_SJOP.2011.v14.n1.2
   Gruning A., 2012, LNCS, V7663
   Gu Q, 2002, NEUROSCIENCE, V111, P815, DOI 10.1016/S0306-4522(02)00026-X
   Herd SA, 2006, J COGNITIVE NEUROSCI, V18, P22, DOI 10.1162/089892906775250012
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2007, CEREB CORTEX, V17, P2443, DOI 10.1093/cercor/bhl152
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Kaplan GB, 2007, NEUROCOMPUTING, V70, P1414, DOI 10.1016/j.neucom.2006.05.009
   Legenstein R, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.1000180
   MACLEOD CM, 1988, J EXP PSYCHOL LEARN, V14, P126, DOI 10.1037/0278-7393.14.1.126
   SHIFFRIN RM, 1977, PSYCHOL REV, V84, P127, DOI 10.1037/0033-295X.84.2.127
   Smith WB, 2005, NEURON, V45, P765, DOI 10.1016/j.neuron.2005.01.015
   Stroop JR, 1935, J EXP PSYCHOL, V18, P643, DOI 10.1037/h0054651
   Yusoff N, 2012, PROCEDIA ENGINEER, V41, P319, DOI 10.1016/j.proeng.2012.07.179
NR 19
TC 0
Z9 0
U1 0
U2 2
PY 2013
BP 131
EP 136
UT WOS:000364966500023
DA 2023-11-16
ER

PT J
AU Vitay, J
   Dinkelbach, HÜ
   Hamker, FH
AF Vitay, Julien
   Dinkelbach, Helges Ue
   Hamker, Fred H.
TI ANNarchy: a code generation approach to neural simulations on parallel
   hardware
SO FRONTIERS IN NEUROINFORMATICS
DT Article
DE neural simulator; Python; rate-coded networks; spiking networks;
   parallel computing; code generation
ID SYNAPTIC PLASTICITY; SPIKING NEURONS; NETWORKS; MODEL; CORTEX;
   RECOGNITION; FRAMEWORK; BRAIN; GPU
AB Many modern neural simulators focus on the simulation of networks of spiking neurons on parallel hardware. Another important framework in computational neuroscience, rate -coded neural networks, is mostly difficult or impossible to implement using these simulators. We present here the ANNarchy (Artificial Neural Networks architect) neural simulator, which allows to easily define and simulate rate -coded and spiking networks, as well as combinations of both. The interface in Python has been designed to be close to the PyNN interface, while the definition of neuron and synapse models can be specified using an equation -oriented mathematical description similar to the Brian neural simulator. This information is used to generate C++ code that will efficiently perform the simulation on the chosen parallel hardware (multi -core system or graphical processing unit). Several numerical methods are available to transform ordinary differential equations into an efficient C++code. We compare the parallel performance of the simulator to existing solutions.
C1 [Vitay, Julien; Dinkelbach, Helges Ue; Hamker, Fred H.] Tech Univ Chemnitz, Dept Comp Sci, Chemnitz, Germany.
   [Hamker, Fred H.] Charite, Bernstein Ctr Computat Neurosci, D-13353 Berlin, Germany.
RP Vitay, J (corresponding author), Tech Univ Chemnitz, Fak Informat, Kunstliche Intelligenz, Str Nationen 62, D-09107 Chemnitz, Germany.
EM julien.vitay@informatik.tu-chemnitz.de
CR Aisa B, 2008, NEURAL NETWORKS, V21, P1146, DOI 10.1016/j.neunet.2008.06.016
   Bednar James A, 2009, Front Neuroinform, V3, P8, DOI 10.3389/neuro.11.008.2009
   Behnel S, 2009, P 8 PYTHON SCI C, P4
   Bekolay T, 2014, FRONT NEUROINFORM, V7, DOI 10.3389/fninf.2013.00048
   Beuth F, 2015, VISION RES, V116, P241, DOI 10.1016/j.visres.2015.04.004
   BIENENSTOCK EL, 1982, J NEUROSCI, V2, P32, DOI 10.1523/jneurosci.02-01-00032.1982
   Bower James M, 2007, Methods Mol Biol, V401, P103, DOI 10.1007/978-1-59745-520-6_7
   Brette R, 2007, J COMPUT NEUROSCI, V23, P349, DOI 10.1007/s10827-007-0038-6
   Brette R, 2012, NETWORK-COMP NEURAL, V23, P167, DOI 10.3109/0954898X.2012.730170
   Brette R, 2011, NEURAL COMPUT, V23, P1503, DOI 10.1162/NECO_a_00123
   Butz M, 2009, BRAIN RES REV, V60, P287, DOI 10.1016/j.brainresrev.2008.12.023
   Carandini M, 2012, NAT REV NEUROSCI, V13, P51, DOI 10.1038/nrn3136
   Carlson KD, 2014, FRONT NEUROSCI-SWITZ, V8, DOI 10.3389/fnins.2014.00010
   Cunningham JP, 2009, NEURAL NETWORKS, V22, P1235, DOI 10.1016/j.neunet.2009.02.004
   Davison Andrew P, 2008, Front Neuroinform, V2, P11, DOI 10.3389/neuro.11.011.2008
   Dayan P., 2001, THEORETICAL NEUROSCI
   Dinkelbach HÜ, 2012, NETWORK-COMP NEURAL, V23, P212, DOI 10.3109/0954898X.2012.739292
   Djurfeldt M, 2012, NEUROINFORMATICS, V10, P287, DOI 10.1007/s12021-012-9146-1
   Dranias MR, 2008, BRAIN RES, V1238, P239, DOI 10.1016/j.brainres.2008.07.013
   Eliasmith C, 2012, SCIENCE, V338, P1202, DOI 10.1126/science.1225266
   Eppler Jochen Martin, 2008, Front Neuroinform, V2, P12, DOI 10.3389/neuro.11.012.2008
   Fidjeland AK, 2009, IEEE INT CONF ASAP, P137, DOI 10.1109/ASAP.2009.24
   GERSTEIN GL, 1960, BIOPHYS J, V1, P15, DOI 10.1016/S0006-3495(60)86872-5
   Gerstner W, 1996, NATURE, V383, P76, DOI 10.1038/383076a0
   Goodman Dan, 2008, Front Neuroinform, V2, P5, DOI 10.3389/neuro.11.005.2008
   Goodman DFM, 2010, NEUROINFORMATICS, V8, P183, DOI 10.1007/s12021-010-9082-x
   Hamker FH, 2004, NEUROCOMPUTING, V56, P329, DOI 10.1016/j.neucom.2003.09.006
   Hines ML, 1997, NEURAL COMPUT, V9, P1179, DOI 10.1162/neco.1997.9.6.1179
   INTRATOR N, 1992, NEURAL NETWORKS, V5, P3, DOI 10.1016/S0893-6080(05)80003-6
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Joyner David, 2012, ACM COMMUN COMPUT AL, V45, P225, DOI DOI 10.1145/2110170.2110185
   Kelefouras V, 2015, J SUPERCOMPUT, V71, P2644, DOI 10.1007/s11227-015-1409-9
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Marc-Oliver G., 2007, SCHOLARPEDIA, DOI [DOI 10.4249/SCHOLARPEDIA.1430, 10.4249/scholarpedia.1430]
   Markram H, 1998, P NATL ACAD SCI USA, V95, P5323, DOI 10.1073/pnas.95.9.5323
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   Morrison A, 2007, NEURAL COMPUT, V19, P47, DOI 10.1162/neco.2007.19.1.47
   Mutch J., 2010, CNS GPU BASED FRAMEW
   Nawrot M, 1999, J NEUROSCI METH, V94, P81, DOI 10.1016/S0165-0270(99)00127-2
   Nordlie E, 2009, PLOS COMPUT BIOL, V5, DOI 10.1371/journal.pcbi.1000456
   O'Reilly RC, 2006, NEURAL COMPUT, V18, P283, DOI 10.1162/089976606775093909
   OJA E, 1982, J MATH BIOL, V15, P267, DOI 10.1007/BF00275687
   Riesenhuber M, 1999, NAT NEUROSCI, V2, P1019, DOI 10.1038/14819
   Rougier NP, 2012, NETWORK-COMP NEURAL, V23, P237, DOI 10.3109/0954898X.2012.721573
   SamuelWilliams Leonid Oliker, 2007, SC 07, P1, DOI DOI 10.1145/1362622.1362674
   Schroll H, 2014, EUR J NEUROSCI, V39, P688, DOI 10.1111/ejn.12434
   Shimokawa T, 2009, NEURAL COMPUT, V21, P1931, DOI 10.1162/neco.2009.08-08-841
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Stimberg M, 2014, FRONT NEUROINFORM, V8, DOI 10.3389/fninf.2014.00006
   Thibeault C. M., 2011, Proceedings of the ISCA 3rd International Conference on Bioinformatics and Computational Biology, P146
   van der Walt S, 2011, COMPUT SCI ENG, V13, P22, DOI 10.1109/MCSE.2011.37
   Vogels TP, 2005, J NEUROSCI, V25, P10786, DOI 10.1523/JNEUROSCI.3508-05.2005
   Wang XJ, 2002, NEURON, V36, P955, DOI 10.1016/S0896-6273(02)01092-9
   Wu W, 2004, IEEE T BIO-MED ENG, V51, P933, DOI 10.1109/TBME.2004.826666
   Zaytsev YV, 2014, FRONT NEUROINFORM, V8, DOI 10.3389/fninf.2014.00023
   Zenke F, 2014, FRONT NEUROINFORM, V8, DOI 10.3389/fninf.2014.00076
   Zimsak M, 2011, EUR J NEUROSCI, V33, P2035, DOI 10.1111/j.1460-9568.2011.07718.x
NR 57
TC 54
Z9 54
U1 0
U2 10
PD JUL 31
PY 2015
VL 9
AR 9
DI 10.3389/fninf.2015.00019
UT WOS:000370606600001
DA 2023-11-16
ER

PT J
AU Kasabov, N
   Zhou, L
   Doborjeh, MG
   Doborjeh, ZG
   Yang, J
AF Kasabov, Nikola
   Zhou, Lei
   Doborjeh, Maryam Gholami
   Doborjeh, Zohreh Gholami
   Yang, Jie
TI New Algorithms for Encoding, Learning and Classification of fMRI Data in
   a Spiking Neural Network Architecture: A Case on Modeling and
   Understanding of Dynamic Cognitive Processes
SO IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS
DT Article
DE Brain functional connectivity; classification; deep learning in spiking
   neural networks; functional magnetic resonance imaging (fMRI) data;
   NeuCube; neuromorphic cognitive systems; perceptual dynamics; spiking
   neural networks (SNNs)
ID EEG DATA; BRAIN; CONNECTIVITY; SCHIZOPHRENIA; COMMUNICATION;
   METHODOLOGY; COMPUTATION; SENTENCES; NEURONS; NEUCUBE
AB This paper argues that, the third generation of neural networks-the spiking neural networks (SNNs), can be used to model dynamic, spatio-temporal, cognitive brain processes measured as functional magnetic resonance imaging (fMRI) data. This paper proposes a novel method based on the NeuCube SNN architecture for which the following new algorithms are introduced: fMRI data encoding into spike sequences; deep unsupervised learning of fMRI data in a 3-D SNN reservoir; classification of cognitive states; and connectivity visualization and analysis for the purpose of understanding cognitive dynamics. The method is illustrated on two case studies of cognitive data modeling from a benchmark fMRI data set of seeing a picture versus reading a sentence.
C1 [Kasabov, Nikola; Doborjeh, Maryam Gholami; Doborjeh, Zohreh Gholami] Auckland Univ Technol, Knowledge Engn & Discovery Res Inst, Auckland 1010, New Zealand.
   [Zhou, Lei; Yang, Jie] Shanghai Jiao Tong Univ, Shanghai 200240, Peoples R China.
RP Doborjeh, MG (corresponding author), Auckland Univ Technol, Knowledge Engn & Discovery Res Inst, Auckland 1010, New Zealand.; Zhou, L (corresponding author), Shanghai Jiao Tong Univ, Shanghai 200240, Peoples R China.
EM nkasabov@aut.ac.nz; minizon@sina.com; mgholami@aut.ac.nz
CR [Anonymous], 2010, SPIKE TIMING DEPENDE
   [Anonymous], HDB BRAIN THEORY NEU
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Bohte SM, 2005, INFORM PROCESS LETT, V95, P519, DOI 10.1016/j.ipl.2005.05.018
   Brette R, 2007, J COMPUT NEUROSCI, V23, P349, DOI 10.1007/s10827-007-0038-6
   Capecci E, 2015, NEURAL NETWORKS, V68, P62, DOI 10.1016/j.neunet.2015.03.009
   Carroll MK, 2009, NEUROIMAGE, V44, P112, DOI 10.1016/j.neuroimage.2008.08.020
   Christensen KR, 2009, J NEUROLINGUIST, V22, P1, DOI 10.1016/j.jneuroling.2008.05.001
   Doborjeh MG, 2016, IEEE T BIO-MED ENG, V63, P1830, DOI 10.1109/TBME.2015.2503400
   Doborjeh MG, 2014, 2014 IEEE SYMPOSIUM ON EVOLVING AND AUTONOMOUS LEARNING SYSTEMS (EALS), P73, DOI 10.1109/EALS.2014.7009506
   Fornito A, 2012, NEUROIMAGE, V62, P2296, DOI 10.1016/j.neuroimage.2011.12.090
   Franchin T, 2013, IEEE ENG MED BIO, P6011, DOI 10.1109/EMBC.2013.6610922
   FRISTON KJ, 1993, J CEREBR BLOOD F MET, V13, P5, DOI 10.1038/jcbfm.1993.4
   Friston KJ., 1994, HUM BRAIN MAP, V2, P189, DOI [10.1002/hbm.460020402, DOI 10.1002/HBM.460020402]
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   GERSTNER W, 1995, PHYS REV E, V51, P738, DOI 10.1103/PhysRevE.51.738
   Gerstner W., 2002, SPIKING NEURON MODEL
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2007, TRENDS COGN SCI, V11, P428, DOI 10.1016/j.tics.2007.09.004
   Indiveri G, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00073
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Just M., STARPLUS FMRI DATA
   Kasabov N. K., IEEE T NEUR IN PRESS
   Kasabov N, 2016, NEURAL NETWORKS, V78, P1, DOI 10.1016/j.neunet.2015.09.011
   Kasabov N, 2015, INFORM SCIENCES, V294, P565, DOI 10.1016/j.ins.2014.06.028
   Kasabov N, 2013, NEURAL NETWORKS, V41, P188, DOI 10.1016/j.neunet.2012.11.014
   Kasabov NK, 2014, NEURAL NETWORKS, V52, P62, DOI 10.1016/j.neunet.2014.01.006
   Liu AP, 2014, IEEE T BIO-MED ENG, V61, P946, DOI 10.1109/TBME.2013.2294151
   Liu YJ, 2000, NATURE, V405, P1058, DOI 10.1038/35016590
   Lynall ME, 2010, J NEUROSCI, V30, P9477, DOI 10.1523/JNEUROSCI.0333-10.2010
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Maggioni E, 2014, J NEUROSCI METH, V228, P86, DOI 10.1016/j.jneumeth.2014.03.004
   Masquelier T, 2008, PLOS ONE, V3, DOI 10.1371/journal.pone.0001377
   Masquelier T, 2009, NEURAL COMPUT, V21, P1259, DOI 10.1162/neco.2008.06-08-804
   McKeown MJ, 1998, HUM BRAIN MAPP, V6, P160, DOI 10.1002/(SICI)1097-0193(1998)6:3<160::AID-HBM5>3.0.CO;2-1
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Mouao-Miranda J, 2007, NEUROIMAGE, V36, P88, DOI 10.1016/j.neuroimage.2007.02.020
   Neumann J, 2005, HUM BRAIN MAPP, V25, P165, DOI 10.1002/hbm.20133
   Ng B., 2011, Proceedings of the 2011 International Workshop on Pattern Recognition in Neuroimaging (PRNI), P65, DOI 10.1109/PRNI.2011.10
   Rodriguez PA, 2015, IEEE T BIO-MED ENG, V62, P922, DOI 10.1109/TBME.2014.2371791
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Siegelmann H., 2015, BRAIN ARCHITECTURE L
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Tana MG, 2012, BRAIN TOPOGR, V25, P345, DOI 10.1007/s10548-012-0225-2
   Thorpe S, 1998, COMPUTATIONAL NEUROSCIENCE: TRENDS IN RESEARCH, P113
   Verstraeten D, 2007, NEURAL NETWORKS, V20, P391, DOI 10.1016/j.neunet.2007.04.003
   Yuasa M, 2011, ELECTR COMMUN JPN, V94, P17, DOI 10.1002/ecj.10311
NR 48
TC 28
Z9 29
U1 3
U2 56
PD DEC
PY 2017
VL 9
IS 4
BP 293
EP 303
DI 10.1109/TCDS.2016.2636291
UT WOS:000418069600001
DA 2023-11-16
ER

PT C
AU Wouters, J
   Kloosterman, F
   Bertrand, A
AF Wouters, Jasper
   Kloosterman, Fabian
   Bertrand, Alexander
GP IEEE
TI A NEURAL NETWORK-BASED SPIKE SORTING FEATURE MAP THAT RESOLVES SPIKE
   OVERLAP IN THE FEATURE SPACE
SO 2020 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL
   PROCESSING
SE International Conference on Acoustics Speech and Signal Processing
   ICASSP
DT Proceedings Paper
CT IEEE International Conference on Acoustics, Speech, and Signal
   Processing (ICASSP)
CY MAY 04-08, 2020
CL Barcelona, SPAIN
DE spike sorting; spike overlap; neural network; feature extraction
AB When inserting an electrode array in the brain, its electrodes will record so-called 'spikes' which are generated by the neurons in the neighbourhood of the array. Spike sorting is the process of detecting and assigning these recorded spikes to their putative neurons. Many spike sorting pipelines rely on a clustering algorithm that groups the spikes coming from the same neuron in a pre-defined feature space. However, classical spike sorting algorithms fail when spike overlap, i.e., the near-simultaneous occurrence of two or more spikes from different neurons, is present in the recording. In such cases, the overlapping spikes segment ends up in a seemingly random position in the feature space and is not assigned to the correct cluster. This problem has been addressed before by extending the sorting algorithm with a template matching postprocessor. In this work, a novel approach is presented to resolve spike overlap directly in the feature space. To this end, a neural network feature map is presented, that generates spike embeddings (feature vectors) that behave as a linear superposition in the feature space in the case of spike overlap. Its performance is quantified on semi-synthetic data obtained through a data augmentation procedure applied to real neural recordings.
C1 [Wouters, Jasper; Bertrand, Alexander] Katholieke Univ Leuven, Elect Engn Dept, ESAT, Stadius Ctr Dynam Syst Signal Proc & Data Analyt, Leuven, Belgium.
   [Kloosterman, Fabian] Neuroelect Res Flanders NERF, Leuven, Belgium.
   [Kloosterman, Fabian] Katholieke Univ Leuven, Brain & Cognit Res Unit, Leuven, Belgium.
   [Kloosterman, Fabian] VIB, Leuven, Belgium.
   [Wouters, Jasper; Kloosterman, Fabian; Bertrand, Alexander] Katholieke Univ Leuven, ESAT Lab, Leuven, Belgium.
RP Wouters, J (corresponding author), Katholieke Univ Leuven, Elect Engn Dept, ESAT, Stadius Ctr Dynam Syst Signal Proc & Data Analyt, Leuven, Belgium.
CR Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027
   Azzalini A., 2013, SKEW NORMAL RELATED, V3
   Carlson D, 2019, CURR OPIN NEUROBIOL, V55, P90, DOI 10.1016/j.conb.2019.02.007
   Caruana R, 2001, ADV NEUR IN, V13, P402
   Ekanadham C, 2014, J NEUROSCI METH, V222, P47, DOI 10.1016/j.jneumeth.2013.10.001
   Franke F, 2015, J COMPUT NEUROSCI, V38, P439, DOI 10.1007/s10827-015-0547-7
   Franke F, 2010, J COMPUT NEUROSCI, V29, P127, DOI 10.1007/s10827-009-0163-5
   Gibson S, 2012, IEEE SIGNAL PROC MAG, V29, P124, DOI 10.1109/MSP.2011.941880
   Jun JJ, 2017, NATURE, V551, P232, DOI 10.1038/nature24636
   Kingma DP., 2017, ARXIV
   Lee J, 2017, ADV NEUR IN, V30
   Lewicki MS, 1998, NETWORK-COMP NEURAL, V9, pR53, DOI 10.1088/0954-898X/9/4/001
   Pachitariu M., 2016, BIORXIV, DOI DOI 10.1101/061481
   Pillow JW, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0062123
   RAND WM, 1971, J AM STAT ASSOC, V66, P846, DOI 10.2307/2284239
   Rodriguez A, 2014, SCIENCE, V344, P1492, DOI 10.1126/science.1242072
   Steinmetz Nick, 2019, SINGLE PHASE3 DUAL P
   Wouters J, 2019, I IEEE EMBS C NEUR E, P247, DOI [10.1109/ner.2019.8716953, 10.1109/NER.2019.8716953]
   Wouters J, 2018, J NEURAL ENG, V15, DOI 10.1088/1741-2552/aace8a
   Yger P, 2018, ELIFE, V7, DOI 10.7554/eLife.34518
NR 20
TC 4
Z9 4
U1 2
U2 7
PY 2020
BP 1175
EP 1179
DI 10.1109/icassp40776.2020.9053530
UT WOS:000615970401083
DA 2023-11-16
ER

PT C
AU Pehlevan, C
AF Pehlevan, Cengiz
GP IEEE
TI A SPIKING NEURAL NETWORK WITH LOCAL LEARNING RULES DERIVED FROM
   NONNEGATIVE SIMILARITY MATCHING
SO 2019 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)
SE International Conference on Acoustics Speech and Signal Processing
   ICASSP
DT Proceedings Paper
CT 44th IEEE International Conference on Acoustics, Speech and Signal
   Processing (ICASSP)
CY MAY 12-17, 2019
CL Brighton, ENGLAND
DE nonnegative similarity matching; spiking neural networks; online
   optimization
ID HEBBIAN/ANTI-HEBBIAN NETWORK; SPARSE; SELECTION
AB The design and analysis of spiking neural network algorithms will be accelerated by the advent of new theoretical approaches. In an attempt at such approach, we provide a principled derivation of a spiking algorithm for unsupervised learning, starting from the nonnegative similarity matching cost function. The resulting network consists of integrate-and-fire units and exhibits local learning rules, making it biologically plausible and also suitable for neuromorphic hardware. We show in simulations that the algorithm can perform sparse feature extraction and manifold learning, two tasks which can be formulated as nonnegative similarity matching problems.
C1 [Pehlevan, Cengiz] Harvard Univ, John A Paulson Sch Engn & Appl Sci, Cambridge, MA 02138 USA.
RP Pehlevan, C (corresponding author), Harvard Univ, John A Paulson Sch Engn & Appl Sci, Cambridge, MA 02138 USA.
CR [Anonymous], AAAI
   Arora S., 2015, P MACH LEARN RES
   Bahroun Y, 2017, LECT NOTES COMPUT SC, V10613, P354, DOI 10.1007/978-3-319-68600-4_41
   Bahroun Y, 2017, LECT NOTES COMPUT SC, V10634, P316, DOI 10.1007/978-3-319-70087-8_34
   Boerlin M, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1001080
   Davies M., 2017, ARXIV170505475
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Geusebroek JM, 2005, INT J COMPUT VISION, V61, P103, DOI 10.1023/B:VISI.0000042993.50813.60
   Gilra A, 2017, ELIFE, V6, DOI 10.7554/eLife.28295
   Hu T, 2014, CONF REC ASILOMAR C, P613, DOI 10.1109/ACSSC.2014.7094519
   Hu T, 2012, NEURAL COMPUT, V24, P2852, DOI 10.1162/NECO_a_00353
   Kuang D., 2012, SDM
   LeCun Y., 1998, MNIST DATABASE HANDW, V10, P34
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0
   Pehlevan C, 2018, NEURAL COMPUT, V30, P84, DOI [10.1162/NECO_a_01018, 10.1162/neco_a_01018]
   Pehlevan C, 2017, NEURAL COMPUT, V29, P2925, DOI [10.1162/neco_a_01007, 10.1162/NECO_a_01007]
   Pehlevan C, 2014, CONF REC ASILOMAR C, P769, DOI 10.1109/ACSSC.2014.7094553
   Pehlevan C, 2015, NEURAL COMPUT, V27, P1461, DOI 10.1162/NECO_a_00745
   Schumann CL, 2019, AIDS BEHAV, V23, P5, DOI 10.1007/s10461-017-1727-4
   Sengupta A., 2018, NEURIPS
   Seung H. S., 2017, ARXIV170400646
   Shapero S, 2014, INT J NEURAL SYST, V24, DOI 10.1142/S0129065714400012
   Tang P. T. P., 2016, ARXIV160301644
   Tang P.T.P., 2018, ARXIV180508952
   Tibshirani R, 1996, J ROY STAT SOC B, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x
   Vertechi P, 2014, ADV NEURAL INFORM PR, P3653
   Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x
NR 28
TC 6
Z9 6
U1 1
U2 1
PY 2019
BP 7958
EP 7962
UT WOS:000482554008039
DA 2023-11-16
ER

PT J
AU Kuroda, K
   Ashizawa, T
   Ikeguchi, T
AF Kuroda, Kaori
   Ashizawa, Tohru
   Ikeguchi, Tohru
TI Estimation of network structures only from spike sequences
SO PHYSICA A-STATISTICAL MECHANICS AND ITS APPLICATIONS
DT Article
DE Spike sequence; Neural network; Partialization analysis
ID IDENTIFICATION; MODEL
AB A neuron, the fundamental element of neural systems, interacts with other neurons, often producing very complicated behavior. To analyze, model, or predict such complicated behavior, it is important to understand how neurons are connected as well as how they behave. In this paper, we propose two measures, the spike time metric coefficient and the partial spike time metric coefficient, to estimate the network structure, that is, the topological connectivity between neurons. The proposed measures are based on the spike time metric and partialization analysis. To check the validity, we applied the proposed measures to asynchronous spike sequences that are produced by a mathematical neural network model. It was found that the proposed measure has high performance for estimating the network structures even though the structures have a complex topology such as a small-world structure or a scale-free structure. (C) 2011 Elsevier B.V. All rights reserved.
C1 [Kuroda, Kaori; Ashizawa, Tohru; Ikeguchi, Tohru] Saitama Univ, Grad Sch Sci & Engn, Sakura Ku, Saitama 3388570, Japan.
   [Ikeguchi, Tohru] Saitama Univ, Brain Sci Inst, Saitama 3388570, Japan.
RP Kuroda, K (corresponding author), Saitama Univ, Grad Sch Sci & Engn, Sakura Ku, 225 Shimo Ohkubo, Saitama 3388570, Japan.
EM kuroda@nls.ics.saitama-u.ac.jp; ashizawa@nls.ics.saitama-u.ac.jp;
   tohru@mail.saitama-u.ac.jp
CR Barabási AL, 1999, SCIENCE, V286, P509, DOI 10.1126/science.286.5439.509
   Eichler M, 2003, BIOL CYBERN, V89, P289, DOI 10.1007/s00422-003-0400-3
   Frenzel S, 2007, PHYS REV LETT, V99, DOI 10.1103/PhysRevLett.99.204101
   Fujii H, 1996, NEURAL NETWORKS, V9, P1303, DOI 10.1016/S0893-6080(96)00054-8
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Kistler WM, 1997, NEURAL COMPUT, V9, P1015, DOI 10.1162/neco.1997.9.5.1015
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Schelter B, 2006, PHYS REV LETT, V96, DOI 10.1103/PhysRevLett.96.208103
   Smirnov D, 2007, CHAOS, V17, DOI 10.1063/1.2430639
   Takekawa T, 2010, EUR J NEUROSCI, V31, P263, DOI 10.1111/j.1460-9568.2009.07068.x
   Victor JD, 1997, NETWORK-COMP NEURAL, V8, P127, DOI 10.1088/0954-898X/8/2/003
   Watts DJ, 1998, NATURE, V393, P440, DOI 10.1038/30918
   Yu DC, 2006, PHYS REV LETT, V97, DOI 10.1103/PhysRevLett.97.188701
   Zhou J, 2007, PHYSICA A, V386, P481, DOI 10.1016/j.physa.2007.07.050
NR 14
TC 4
Z9 4
U1 2
U2 12
PD OCT 15
PY 2011
VL 390
IS 21-22
BP 4002
EP 4011
DI 10.1016/j.physa.2011.06.026
UT WOS:000295297000041
DA 2023-11-16
ER

PT J
AU Tang, ZR
   Chen, YH
   Wang, ZH
   Hu, RH
   Wu, EQ
AF Tang, Zhiri
   Chen, Yanhua
   Wang, Zhihua
   Hu, Ruihan
   Wu, Edmond Q.
TI Non-spike timing-dependent plasticity learning mechanism for memristive
   neural networks
SO APPLIED INTELLIGENCE
DT Article
DE Spike timing-dependent plasticity; Memristive neural networks;
   Feedforward neural networks; Crossbar; Hardware performance
ID PATTERN-RECOGNITION; FRAMEWORK; MODEL
AB Memristive neural networks (MNNs) attract the attention of many researchers because memristor can mimic the learning mechanism of biologic neuron, spike timing-dependent plasticity (STDP). While STDP brings huge potentials on many applications for memristive neural networks, it also gives complex calculation process for hardware implement. In this work, a non-STDP learning mechanism is proposed, which is implemented in two common frameworks including feedforward neural network and crossbar. The non-STDP learning mechanism relies on the linear relationship between the value of memristor and area of input spikes, which gives the proposed method a simple calculation process and better hardware compatibility. Experimental results show that the non-STDP learning mechanism can help to achieve good hardware performance in both feedforward neural network and crossbar frameworks. Compared with STDP based memristive neural networks, the proposed method can save 2.19%-24.4% hardware resource (ALMs) and improve 1.56-12.25 MHz processing speed under a set of different network scales. In future, some other complex memristor models with non-STDP learning mechanism should be taken into consideration, which will give more room for practical applications of memristive neural networks.
C1 [Tang, Zhiri] Wuhan Univ, Sch Phys & Technol, Wuhan, Peoples R China.
   [Tang, Zhiri; Wang, Zhihua] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.
   [Chen, Yanhua] Univ Hong Kong, Dept Geog, Hong Kong, Peoples R China.
   [Hu, Ruihan] Guangdong Inst Intelligent Mfg, Guangdong Key Lab Modern Control Technol, Guangzhou, Peoples R China.
   [Wu, Edmond Q.] Shanghai Jiao Tong Univ, Dept Automat, Shanghai, Peoples R China.
RP Hu, RH (corresponding author), Guangdong Inst Intelligent Mfg, Guangdong Key Lab Modern Control Technol, Guangzhou, Peoples R China.
EM GerinTang@163.com; yhchen19@connect.hku.hk; zhihua.wang@my.cityu.edu.hk;
   rh.hu@giim.ac.cn; edmondqwu@gmail.com
CR AMARA SG, 1993, ANNU REV NEUROSCI, V16, P73, DOI 10.1146/annurev.ne.16.030193.000445
   Ambrogio S, 2013, NANOTECHNOLOGY, V24, DOI 10.1088/0957-4484/24/38/384012
   Ankit A, 2017, DES AUT CON, DOI 10.1145/3061639.3062311
   [Anonymous], 2001, HDB BIOL PHYS
   Basu A, 2018, IEEE J EM SEL TOP C, V8, P6, DOI 10.1109/JETCAS.2018.2816339
   CHUA LO, 1971, IEEE T CIRCUITS SYST, VCT18, P507, DOI 10.1109/TCT.1971.1083337
   Covi E, 2016, IEEE INT SYMP CIRC S, P393, DOI 10.1109/ISCAS.2016.7527253
   Dai YT, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-017-02527-8
   Duan SK, 2015, IEEE T NEUR NET LEAR, V26, P1202, DOI 10.1109/TNNLS.2014.2334701
   Falez P, 2019, THESIS
   Fan DL, 2014, IEEE T NANOTECHNOL, V13, P574, DOI 10.1109/TNANO.2014.2312177
   Hao YZ, 2020, NEURAL NETWORKS, V121, P387, DOI 10.1016/j.neunet.2019.09.007
   Hu M, 2018, ADV MATER, V30, DOI 10.1002/adma.201705914
   Hu R, 2019, COMPUTATIONAL INTELL
   Hu R., 2020, NEURAL COMPUT APPL, P1
   Jo SH, 2015, 2015 IEEE 11 INT C A, P1
   Kvatinsky S, 2014, IEEE T CIRCUITS-II, V61, P895, DOI 10.1109/TCSII.2014.2357292
   Li C, 2019, NAT MACH INTELL, V1, P49, DOI 10.1038/s42256-018-0001-4
   Lin QJ, 2017, PHYSICA A, V484, P199, DOI 10.1016/j.physa.2017.04.165
   Moon J, 2019, NAT ELECTRON, V2, P480, DOI 10.1038/s41928-019-0313-3
   Pershin YV, 2010, NEURAL NETWORKS, V23, P881, DOI 10.1016/j.neunet.2010.05.001
   Pham VT, 2016, SCI CHINA TECHNOL SC, V59, P358, DOI 10.1007/s11431-015-5981-2
   Phinyomark A, 2012, EXPERT SYST APPL, V39, P7420, DOI 10.1016/j.eswa.2012.01.102
   Schumann CL, 2019, AIDS BEHAV, V23, P5, DOI 10.1007/s10461-017-1727-4
   Shi YZ, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-03156-5
   Strukov DB, 2008, NATURE, V453, P80, DOI 10.1038/nature06932
   Tang ZR, 2021, IEEE T COGN DEV SYST, V13, P645, DOI 10.1109/TCDS.2020.3003377
   Tang ZR, 2020, NEUROCOMPUTING, V403, P80, DOI 10.1016/j.neucom.2020.04.012
   Tang ZR, 2019, NEUROCOMPUTING, V332, P193, DOI 10.1016/j.neucom.2018.12.049
   Pham VT, 2014, OPTOELECTRON ADV MAT, V8, P1157
   WANG J, 2018, SCI REP UK, V8
   Wang Z., 2020, NAT COMMUN, V11, P1
   Wang Z, 2020, IEEE T BIOMEDICAL CI
   Wang Zhan-shan, 2018, Huanjing Kexue, V39, P1, DOI 10.13227/j.hjkx.201705276
   Wang ZR, 2020, NAT REV MATER, V5, P173, DOI 10.1038/s41578-019-0159-3
   Wang ZR, 2019, NAT MACH INTELL, V1, P434, DOI 10.1038/s42256-019-0089-1
   Wang ZR, 2019, NAT ELECTRON, V2, P115, DOI 10.1038/s41928-019-0221-6
   Wang ZR, 2017, NAT MATER, V16, P101, DOI [10.1038/nmat4756, 10.1038/NMAT4756]
   Wu A., 2015, IEEE T NEUR NET LEAR, V28, P206
   Wu E.Q., 2020, IEEE T CYBERNETICS
   Wu EQ, 2020, IEEE T COGNITIVE DEV
   Xia QF, 2009, NANO LETT, V9, P3640, DOI 10.1021/nl901874j
   Xie XD, 2019, CIRC SYST SIGNAL PR, V38, P1452, DOI 10.1007/s00034-018-0926-1
   Yang JJS, 2013, NAT NANOTECHNOL, V8, P13, DOI [10.1038/nnano.2012.240, 10.1038/NNANO.2012.240]
   Zhang XM, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-019-13827-6
   Zhu RH, 2019, IEEE ACCESS, V7, P47472, DOI 10.1109/ACCESS.2019.2909295
   Zhu RH, 2017, IEEE ELECTR DEVICE L, V38, P1367, DOI 10.1109/LED.2017.2736006
   Zhu XJ, 2019, NAT MATER, V18, P141, DOI 10.1038/s41563-018-0248-5
NR 48
TC 6
Z9 6
U1 1
U2 27
PD JUN
PY 2021
VL 51
IS 6
BP 3684
EP 3695
DI 10.1007/s10489-020-01985-w
EA NOV 2020
UT WOS:000590258800002
DA 2023-11-16
ER

PT C
AU Pan, SY
   Wang, XQ
   Zhang, P
AF Pan, Shiying
   Wang, Xiuqing
   Zhang, Peng
BE Xiao, Z
   Tong, Z
   Li, K
   Wang, X
   Li, K
TI Fault Diagnosis For Manipulators Based on NeuCube
SO 2015 11TH INTERNATIONAL CONFERENCE ON NATURAL COMPUTATION (ICNC)
DT Proceedings Paper
CT 11th International Conference on Natural Computation (ICNC) / 12th
   International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)
CY AUG 15-17, 2015
CL Zhangjiajie, PEOPLES R CHINA
DE fault diagnosis; manipulators; spiking neural network; Liquid State
   Machine; NeuCube
ID MODEL
AB In this paper, a new fault diagnosis approach for manipulators based on spiking neural network is investigated. The newly proposed evolving spiking model is named NeuCube, it can be employed for classification, pattern recognition, and other kinds of problems. As NeuCube is good at processing spatio-temporal data, we apply it to fault diagnosis for manipulators. And receive better results than other traditional methods. This paper analyses the basic concepts of the spiking neural network and several spiking neuron models existed, then introduced the spiking neuron models, such as NeuCube, in detail about the structure and feature of the model. Finally, after experiment concluded the characteristics and effectiveness of NeuCube.
C1 [Pan, Shiying; Wang, Xiuqing] Hebei Normal Univ, Coll Vocat Technol, Shijia Zhuang, Peoples R China.
   [Zhang, Peng] Army 66010, Shijia Zhuang, Peoples R China.
RP Pan, SY (corresponding author), Hebei Normal Univ, Coll Vocat Technol, Shijia Zhuang, Peoples R China.
CR ABBOTT LF, 1990, LECT NOTES PHYS, V368, P5
   Bohte Sander M., 2004, Natural Computing, V3, P195, DOI 10.1023/B:NACO.0000027755.02868.60
   Brette R, 2005, AM PHYSL SOC    0713, V13, P3637
   Caccavale F, 2009, CONTROL ENG PRACT, V17, P146, DOI 10.1016/j.conengprac.2008.05.012
   Caccavale F, 2010 IEEE INT S IND, P2121
   Hourdakis E, 2013, NEUROCOMPUTING, V107, P40, DOI 10.1016/j.neucom.2012.07.032
   Kasabov Nikola, 2012, Artificial Neural Networks in Pattern Recognition. Proceedings of the 5th INNS IAPR TC 3 GIRPR Workshop, ANNPR 2012, P225, DOI 10.1007/978-3-642-33212-8_21
   Kasabov N, 2013, NEURAL NETWORKS, V41, P188, DOI 10.1016/j.neunet.2012.11.014
   Kasabov Nikola, 2013, NEURAL INFORM PROCES
   Kasabov Nikola, EVOLVING SPATI UNPUB
   Kasabov NK, 2014, NEURAL NETWORKS, V52, P62, DOI 10.1016/j.neunet.2014.01.006
   Kempter R, 1999, PHYS REV E, V59, P4498, DOI 10.1103/PhysRevE.59.4498
   Kistler WM, 1997, NEURAL COMPUT, V9, P1015, DOI 10.1162/neco.1997.9.5.1015
   Liu YH, 2001, J COMPUT NEUROSCI, V10, P25, DOI 10.1023/A:1008916026143
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maass Wolfgang, 2011, COMPUTABILITY SCI, P275
   Maass Wolfgang, 1999, PULSED NEURAL NETWOR, pp4
   Maass Wolfgang, NOISY SPIKING UNPUB
   Mohemmed A, 2013, NEUROCOMPUTING, V107, P3, DOI 10.1016/j.neucom.2012.08.034
   Mohemmed A, 2012, INT J NEURAL SYST, V22, DOI 10.1142/S0129065712500128
   Nuntalid N., 2011, INT C NEUR INF PROC, P451
   Scott Nathan, 2013, Neural Information Processing. 20th International Conference, ICONIP 2013. Proceedings: LNCS 8228, P78, DOI 10.1007/978-3-642-42051-1_11
   Wang Xiuqing, RES ENV PERCEP UNPUB
   Xue FZ, 2013, NEUROCOMPUTING, V122, P324, DOI 10.1016/j.neucom.2013.06.019
NR 25
TC 0
Z9 0
U1 0
U2 0
PY 2015
BP 709
EP 713
UT WOS:000380617000122
DA 2023-11-16
ER

PT C
AU Allen, JN
   Hasan, SB
   Abdel-Aty-Zohdy, HS
   Ewing, RL
AF Allen, Jacob N.
   Hasan, Safa B.
   Abdel-Aty-Zohdy, Hoda S.
   Ewing, Robert L.
GP IEEE
TI An e-nose Haar wavelet preprocessing circuit for spiking neural network
   classification
SO PROCEEDINGS OF 2008 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND
   SYSTEMS, VOLS 1-10
SE IEEE International Symposium on Circuits and Systems
DT Proceedings Paper
CT IEEE International Symposium on Circuits and Systems
CY MAY 18-21, 2008
CL Seattle, WA
AB A simulation model for polymer film chemical sensors is developed based on a 1 dimensional diffusion equation. Using this model, electronic nose smell prints produced by the 32 sensor array of a Cyranose 320 are simulated to test pattern classification. A Haar wavelet filter reduces noise and captures information about the diffusion rate of the analyte in each sensor. Inputs are encoded into a binary Hamming pattern and fed into a binary spiking neural network for pattern classification. The preprocessing circuit for the spiking neural network, including the wavelet filter, is designed using standard cells for an 180nm process. Real and simulated results from the spiking neural network classification algorithm are favorably compared to Bayes, Canonical, and PCA-PNN classifiers.
C1 [Allen, Jacob N.; Hasan, Safa B.; Abdel-Aty-Zohdy, Hoda S.] Oakland Univ, Dept Elect & Comp Engn, Microelect Syst Design Lab, Rochester, MI 48309 USA.
   [Abdel-Aty-Zohdy, Hoda S.; Ewing, Robert L.] Air Force Res Labs, Informat Directorate, Wright Patterson AFB, OH 45433 USA.
RP Allen, JN (corresponding author), Oakland Univ, Dept Elect & Comp Engn, Microelect Syst Design Lab, Rochester, MI 48309 USA.
CR ALLEN J, 2005, THESIS OAKLAND U
   Allen JN, 2007, IDT 2007: SECOND INTERNATIONAL DESIGN AND TEST WORKSHOP, PROCEEDINGS, P222
   Bartlett PN, 1996, PHILOS T R SOC A, V354, P35, DOI 10.1098/rsta.1996.0002
   Duda R., 2001, PATTERN CLASSIFICATI
   DUTTA R, 2002, BIOMEDICAL ENG O OCT, P4
   Homer ML, 2003, IEEE SENSOR, P877, DOI 10.1109/ICSENS.2003.1279068
   JENSEN AA, RIPPLES MATH DISCRET
   Kreyszig E., 2006, ADV ENG MATH
   RYAN MA, 2004, IEEE SENSORS J, V4
   *SMITHS DET INC, CYR 320 ENOSE US MAN
   Watta P, 2001, NEURAL PROCESS LETT, V13, P183, DOI 10.1023/A:1011384407294
   Zhou HY, 2006, SENSORS-BASEL, V6, P1
NR 12
TC 4
Z9 4
U1 0
U2 3
PY 2008
BP 2178
EP +
DI 10.1109/ISCAS.2008.4541883
UT WOS:000258532102006
DA 2023-11-16
ER

PT J
AU Pham, DT
   Packianather, MS
   Charles, EYA
AF Pham, D. T.
   Packianather, M. S.
   Charles, E. Y. A.
TI Control chart pattern clustering using a new self-organizing spiking
   neural network
SO PROCEEDINGS OF THE INSTITUTION OF MECHANICAL ENGINEERS PART B-JOURNAL OF
   ENGINEERING MANUFACTURE
DT Article
DE spiking neural networks; temporal coding; Hebbian learning;
   self-organizing map
ID COMPUTATIONAL POWER; CORTICAL NEURON; RECOGNITION; BACKPROPAGATION;
   INTEGRATOR
AB This paper focuses on the architecture and learning algorithm associated with using a new self-organizing delay adaptation spiking neural network model for clustering control chart patterns. This temporal coding spiking neural network model employs a Hebbian-based rule to shift the connection delays instead of the previous approaches of delay selection. Here the tuned delays compensate the differences in the input firing times of temporal patterns and enables them to coincide. The coincidence detection capability of the spiking neuron has been utilized for pattern clustering. The structure of the network is similar to that of a Kohonen self-organizing map (SOM) except that the output layer neurons are coincidence detecting spiking neurons. An input pattern is represented by the neuron that is the first to fire among all the competing spiking neurons. Clusters within the input data are identified with the location of the winning neurons and their firing times.
   The proposed self-organized delay adaptation spiking neural network (SODA SNN) has been utilized to cluster control chart patterns. The trained network obtained an average clustering accuracy of 96.1 per cent on previously unseen test data. This was achieved with a network of 8x8 spiking neurons trained for 20 epochs containing 1000 training examples. The improvement in clustering accuracy achieved by the proposed SODA-SNN on the unseen test data was twice as much as that on the training data when compared to the SOM.
C1 [Pham, D. T.; Packianather, M. S.] Cardiff Univ, Mfg Engn Ctr, Cardiff CF24 3AA, S Glam, Wales.
   [Charles, E. Y. A.] Univ Jaffna, Dept Comp Sci, Jaffna, Sri Lanka.
RP Packianather, MS (corresponding author), Cardiff Univ, Mfg Engn Ctr, Queens Bldg,Newport Rd, Cardiff CF24 3AA, S Glam, Wales.
EM PackianatizerMS@cf.ac.uk
CR ABELES M, 1982, ISRAEL J MED SCI, V18, P83
   BALDI P, 1994, IEEE T NEURAL NETWOR, V5, P612, DOI 10.1109/72.298231
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   CARR CE, 1993, ANNU REV NEUROSCI, V16, P223, DOI 10.1146/annurev.ne.16.030193.001255
   DAY SP, 1993, IEEE T NEURAL NETWOR, V4, P348, DOI 10.1109/72.207622
   ENRICH CW, 1999, PHYS REV LETT, V82, P1594
   Gerstner W, 1996, NATURE, V383, P76, DOI 10.1038/383076a0
   Gerstner W., 2002, SPIKING NEURON MODEL
   Haykin S., 1999, NEURAL NETWORKS COMP
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   Jahnke A, 1998, PULSED NEURAL NETWORKS, P237
   Konig P, 1996, TRENDS NEUROSCI, V19, P130, DOI 10.1016/S0166-2236(96)80019-1
   Maass W, 1998, PULSED NEURAL NETWORKS, P55
   Maass W, 1996, NEURAL COMPUT, V8, P1, DOI 10.1162/neco.1996.8.1.1
   Maass W, 1997, NEURAL COMPUT, V9, P279, DOI 10.1162/neco.1997.9.2.279
   Maass W, 1997, ADV NEUR IN, V9, P211
   Natschlager T, 1998, NETWORK-COMP NEURAL, V9, P319, DOI 10.1088/0954-898X/9/3/003
   Panchev C., 2001, P WORLD C NEUR AUSTR, P378
   Pham D.T., 2006, INTELLIGENT PRODUCTI, P307
   PHAM DT, 1994, INT J PROD RES, V32, P721, DOI 10.1080/00207549408956963
   Pham DT, 2001, INT J MACH TOOL MANU, V41, P419, DOI 10.1016/S0890-6955(00)00073-0
   Pham DT, 1998, SOFT COMPUTING IN ENGINEERING DESIGN AND MANUFACTURING, P381
   Pham DT, 1998, P I MECH ENG I-J SYS, V212, P115, DOI 10.1243/0959651981539343
   PHAM DT, 1994, J PROCESS MECH ENG, V207, P113
   Ruf B, 1997, NEURAL PROCESS LETT, V5, P9, DOI 10.1023/A:1009697008681
   Ruf B, 1998, IEEE T NEURAL NETWOR, V9, P575, DOI 10.1109/72.668899
   Tao XL, 2004, IC-AI '04 & MLMTA'04 , VOL 1 AND 2, PROCEEDINGS, P168
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   Tversky T, 2002, NEUROCOMPUTING, V44, P679, DOI 10.1016/S0925-2312(02)00457-5
   ULTSCH A, 2005, 46 U MARB DEP MATH C, P1
   Zador AM, 2000, NAT NEUROSCI, V3, P1167, DOI 10.1038/81432
NR 32
TC 8
Z9 8
U1 1
U2 8
PD OCT
PY 2008
VL 222
IS 10
BP 1201
EP 1211
DI 10.1243/09544054JEM1054
UT WOS:000261270100002
DA 2023-11-16
ER

PT J
AU Rueckauer, B
   Lungu, IA
   Hu, YH
   Pfeiffer, M
   Liu, SC
AF Rueckauer, Bodo
   Lungu, Iulia-Alexandra
   Hu, Yuhuang
   Pfeiffer, Michael
   Liu, Shih-Chii
TI Conversion of Continuous-Valued Deep Networks to Efficient Event-Driven
   Networks for Image Classification
SO FRONTIERS IN NEUROSCIENCE
DT Article
DE artificial neural network; spiking neural network; deep learning; object
   classification; deep networks; spiking network conversion
AB Spiking neural networks (SNNs) can potentially offer an efficient way of doing inference because the neurons in the networks are sparsely activated and computations are event-driven. Previous work showed that simple continuous-valued deep Convolutional Neural Networks (CNNs) can be converted into accurate spiking equivalents. These networks did not include certain common operations such as max-pooling, softmax, batch-normalization and Inception-modules. This paper presents spiking equivalents of these operations therefore allowing conversion of nearly arbitrary CNN architectures. We show conversion of popular CNN architectures, including VGG-16 and lnception-v3, into SNNs that produce the best results reported to date on MNIST, CIFAR-10 and the challenging ImageNet dataset. SNNs can trade off classification error rate against the number of available operations whereas deep continuous-valued neural networks require a fixed number of operations to achieve their classification error rate. From the examples of LeNet for MNIST and BinaryNet for CIFAR-10, we show that with an increase in error rate of a few percentage points, the SNNs can achieve more than 2x reductions in operations compared to the original CNNs. This highlights the potential of SNNs in particular when deployed on power-efficient neuromorphic spiking neuron chips, for use in embedded applications.
C1 [Rueckauer, Bodo; Lungu, Iulia-Alexandra; Hu, Yuhuang; Pfeiffer, Michael; Liu, Shih-Chii] Univ Zurich, Inst Neuroinformat, Zurich, Switzerland.
   [Rueckauer, Bodo; Lungu, Iulia-Alexandra; Hu, Yuhuang; Pfeiffer, Michael; Liu, Shih-Chii] Swiss Fed Inst Technol, Zurich, Switzerland.
   [Pfeiffer, Michael] Bosch Ctr Artificial Intelligence, Renningen, Germany.
RP Rueckauer, B (corresponding author), Univ Zurich, Inst Neuroinformat, Zurich, Switzerland.; Rueckauer, B (corresponding author), Swiss Fed Inst Technol, Zurich, Switzerland.
EM rbodo@ini.uzh.ch
CR [Anonymous], ARXIV160902053
   [Anonymous], 2015, 2015 INT JOINT C NEU, DOI DOI 10.1109/IJCNN.2015.7280625
   [Anonymous], 2016, ARXIV161105141
   [Anonymous], 2009, ADV NEURAL INFORM PR
   Pérez-Carrasco JA, 2013, IEEE T PATTERN ANAL, V35, P2706, DOI 10.1109/TPAMI.2013.71
   Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Cassidy AS, 2013, IEEE IJCNN
   Chen YH, 2017, IEEE J SOLID-ST CIRC, V52, P127, DOI 10.1109/JSSC.2016.2616357
   Chollet F., 2015, KERAS VERSION 2 0 CO
   Courbariaux M., 2015, ADV NEURAL INFORM PR, V28, P1
   Courbariaux M., 2016, C NEUR INF PROC SYST
   Diehl PU, 2015, IEEE IJCNN
   Diehl PU, 2016, IEEE IJCNN, P4278, DOI 10.1109/IJCNN.2016.7727758
   Esser Steve K., 2015, ADV NEURAL INFORM PR, P1
   Esser SK, 2016, P NATL ACAD SCI USA, V113, P11441, DOI 10.1073/pnas.1604850113
   Farabet C, 2012, FRONT NEUROSCI-SWITZ, V6, DOI 10.3389/fnins.2012.00032
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   Gokhale V, 2014, IEEE COMPUT SOC CONF, P696, DOI 10.1109/CVPRW.2014.106
   Horowitz M, 2014, ISSCC DIG TECH PAP I, V57, P10, DOI 10.1109/ISSCC.2014.6757323
   Hu YH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00405
   Ioffe S., 2015, INT C MACH LEARN PML, V1, P448
   Kiselev I, 2016, IEEE INT SYMP CIRC S, P2495, DOI 10.1109/ISCAS.2016.7539099
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee JM, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00191
   Liu SC, 2015, EVENT-BASED NEUROMORPHIC SYSTEMS, P1, DOI 10.1002/9781118927601
   Masquelier T, 2007, PLOS COMPUT BIOL, V3, P247, DOI 10.1371/journal.pcbi.0030031
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Mostafa H., 2017, ISCAS
   Neftci E, 2014, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00272
   Neil D, 2016, P 31 ANN ACM S APPL
   Neil D, 2014, IEEE T VLSI SYST, V22, P2621, DOI 10.1109/TVLSI.2013.2294916
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   Orchard G, 2015, IEEE T PATTERN ANAL, V37, P2028, DOI 10.1109/TPAMI.2015.2392947
   Orchard G, 2015, IEEE INT SYMP CIRC S, P2413, DOI 10.1109/ISCAS.2015.7169171
   Pedroni BU, 2016, IEEE T BIOMED CIRC S, V10, P837, DOI 10.1109/TBCAS.2016.2539352
   Posch C, 2014, P IEEE, V102, P1470, DOI 10.1109/JPROC.2014.2346153
   Rueckauer B., 2016, ARXIV161204052
   Rueckauer B, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00176
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Serrano-Gotarredona T, 2015, IEEE INT SYMP CIRC S, P2405, DOI 10.1109/ISCAS.2015.7169169
   Simonyan K., 2015, ARXIV
   Stromatias E, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00350
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
NR 47
TC 458
Z9 471
U1 9
U2 68
PD DEC 7
PY 2017
VL 11
AR 682
DI 10.3389/fnins.2017.00682
UT WOS:000417266300001
HC Y
HP N
DA 2023-11-16
ER

PT J
AU Schmalz, J
   Kumar, G
AF Schmalz, Joseph
   Kumar, Gautam
TI Controlling Synchronization of Spiking Neuronal Networks by Harnessing
   Synaptic Plasticity
SO FRONTIERS IN COMPUTATIONAL NEUROSCIENCE
DT Article
DE synchronization; desynchronization; spiking neural network; spike-timing
   dependent plasticity; harnessing plasticity
ID DEEP-BRAIN-STIMULATION; COORDINATED RESET; RECURRENT NETWORKS;
   DESYNCHRONIZATION; MODEL
AB Disrupting the pathological synchronous firing patterns of neurons with high frequency stimulation is a common treatment for Parkinsonian symptoms and epileptic seizures when pharmaceutical drugs fail. In this paper, our goal is to design a desynchronization strategy for large networks of spiking neurons such that the neuronal activity of the network remains in the desynchronized regime for a long period of time after the removal of the stimulation. We develop a novel "Forced Temporal-Spike Time Stimulation (FTSTS)" strategy that harnesses the spike-timing dependent plasticity to control the synchronization of neural activity in the network by forcing the neurons in the network to artificially fire in a specific temporal pattern. Our strategy modulates the synaptic strengths of selective synapses to achieve a desired synchrony of neural activity in the network. Our simulation results show that the FTSTS strategy can effectively synchronize or desynchronize neural activity in large spiking neuron networks and keep them in the desired state for a long period of time after the removal of the external stimulation. Using simulations, we demonstrate the robustness of our strategy in desynchronizing neural activity of networks against uncertainties in the designed stimulation pulses and network parameters. Additionally, we show in simulation, how our strategy could be incorporated within the existing desynchronization strategies to improve their overall efficacy in desynchronizing large networks. Our proposed strategy provides complete control over the synchronization of neurons in large networks and can be used to either synchronize or desynchronize neural activity based on specific applications. Moreover, it can be incorporated within other desynchronization strategies to improve the efficacy of existing therapies for numerous neurological and psychiatric disorders associated with pathological synchronization.
C1 [Schmalz, Joseph; Kumar, Gautam] Univ Idaho, Dept Chem & Mat Engn, Moscow, ID 83843 USA.
RP Kumar, G (corresponding author), Univ Idaho, Dept Chem & Mat Engn, Moscow, ID 83843 USA.
EM gkumar@uidaho.edu
CR Abbott LF, 2000, NAT NEUROSCI, V3, P1178, DOI 10.1038/81453
   [Anonymous], 2007, PHASE RESETTING MED
   Bell CC, 1997, NATURE, V387, P278, DOI 10.1038/387278a0
   Benabid AL, 2009, LANCET NEUROL, V8, P67, DOI 10.1016/S1474-4422(08)70291-6
   Brunel N, 2006, NEURAL COMPUT, V18, P1066, DOI 10.1162/neco.2006.18.5.1066
   DAIDO H, 1992, PROG THEOR PHYS, V88, P1213, DOI 10.1143/PTP.88.1213
   Deuschl G, 2006, NEW ENGL J MED, V355, P896, DOI 10.1056/NEJMoa060281
   Duncan JS, 2006, LANCET, V367, P1087, DOI 10.1016/S0140-6736(06)68477-8
   Ebert M, 2014, FRONT COMPUT NEUROSC, V8, DOI 10.3389/fncom.2014.00154
   Forli A, 2018, CELL REP, V22, P3087, DOI 10.1016/j.celrep.2018.02.063
   Hammond C, 2007, TRENDS NEUROSCI, V30, P357, DOI 10.1016/j.tins.2007.05.004
   Hauptmann C, 2005, NEUROCOMPUTING, V65, P759, DOI 10.1016/j.neucom.2004.10.072
   Hauptmann C, 2009, J NEURAL ENG, V6, DOI 10.1088/1741-2560/6/1/016004
   Hebb D., 2005, ORG BEHAV NEUROPSYCH
   Hegeman DJ, 2016, EUR J NEUROSCI, V43, P1239, DOI 10.1111/ejn.13196
   Johnston MV, 2004, BRAIN DEV-JPN, V26, P73, DOI 10.1016/S0387-7604(03)00102-5
   Kiss IZ, 2007, SCIENCE, V316, P1886, DOI 10.1126/science.1140858
   Klinger NV, 2016, CLIN NEUROL NEUROSUR, V140, P11, DOI 10.1016/j.clineuro.2015.11.009
   Kuramoto Y., 1984, CHEM OSCILLATIONS WA
   Lanciego JL, 2012, CSH PERSPECT MED, V2, DOI 10.1101/cshperspect.a009621
   Luz Y, 2012, PLOS COMPUT BIOL, V8, DOI 10.1371/journal.pcbi.1002334
   Mathern GW, 1998, EPILEPSY RES, V32, P154, DOI 10.1016/S0920-1211(98)00048-5
   Mauroy A, 2014, SIAM J APPL DYN SYST, V13, P306, DOI 10.1137/130931151
   Monga B, 2018, P AMER CONTR CONF, P2808, DOI 10.23919/ACC.2018.8431114
   Nabi A, 2013, J NEURAL ENG, V10, DOI 10.1088/1741-2560/10/3/036005
   Nabi A, 2013, J COMPUT NEUROSCI, V34, P259, DOI 10.1007/s10827-012-0419-3
   Pfister JP, 2010, FRONT COMPUT NEUROSC, V4, DOI 10.3389/fncom.2010.00022
   Popovych O, 2014, FRONT NEUROL, V5, DOI 10.3389/fneur.2014.00268
   Popovych OV, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0173363
   Popovych OV, 2005, PHYS REV LETT, V94, DOI 10.1103/PhysRevLett.94.164102
   Postuma RB, 2015, MOVEMENT DISORD, V30, P1591, DOI 10.1002/mds.26424
   Shen WX, 2008, SCIENCE, V321, P848, DOI 10.1126/science.1160575
   Singh A, 2018, EUR J NEUROSCI, V48, P2869, DOI 10.1111/ejn.13853
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Tass PA, 2009, RESTOR NEUROL NEUROS, V27, P589, DOI 10.3233/RNN-2009-0484
   Tass PA, 2006, BIOL CYBERN, V94, P58, DOI 10.1007/s00422-005-0028-6
   Tass PA, 2003, PROG THEOR PHYS SUPP, P281, DOI 10.1143/PTPS.150.281
   Tass PA, 2003, BIOL CYBERN, V89, P81, DOI [10.1007/s00422-003-0425-7, 10.1007/S00422-003-0425-7]
   Tass PA, 2007, INT J PSYCHOPHYSIOL, V64, P53, DOI 10.1016/j.ijpsycho.2006.07.013
   Temperli P, 2003, NEUROLOGY, V60, P78, DOI 10.1212/WNL.60.1.78
   Vlachos I, 2016, PLOS COMPUT BIOL, V12, DOI 10.1371/journal.pcbi.1004720
   Wilson D, 2014, SIAM J APPL DYN SYST, V13, P276, DOI 10.1137/120901702
   Zeitler M, 2015, FRONT SYST NEUROSCI, V9, DOI 10.3389/fnsys.2015.00049
NR 43
TC 6
Z9 6
U1 0
U2 6
PD SEP 4
PY 2019
VL 13
AR 61
DI 10.3389/fncom.2019.00061
UT WOS:000486042000001
DA 2023-11-16
ER

PT J
AU Reddy, VK
   Melingi, SB
   Kumar, CVMSNP
   Kumar, KA
   Mojjada, RK
AF Reddy, Vanga Karunakar
   Melingi, Sunil Babu
   Kumar, Ch. V. M. S. N. Pavan
   Kumar, K. Ashok
   Mojjada, Ramesh Kumar
TI Optimization Driven Spike Deep Belief Neural Network classifier: a
   deep-learning based Multichannel Spike Sorting Neural Signal Processor
   (NSP) module for high-channel-count Brain Machine Interfaces (BMIs)
SO ARTIFICIAL INTELLIGENCE REVIEW
DT Article; Early Access
DE Adam based Cuckoo search (Adam-CS) algorithm; Brain Machine Interfaces
   (BMIs); Deep Belief Network (DBN)
AB An Optimization Driven Spike Deep Belief Neural Networks is a type of neural network that is inspired by the functioning of the human brain. It is a variant of the more general class of Deep Belief Networks (DBNs), which are artificial neural networks composed of multiple layers of hidden units. Spike sorting is a critical process in neural signal processing that involves separating and identifying individual action potentials, spikes, from extracellular recordings of neuronal activity. This process is essential for understanding the behaviour of individual neurons and for decoding neural signals in various applications, such as Brain Machine Interfaces (BMIs) and neuro science research. Spike sorting is challenging due to the complexity of the recorded signals, including overlapping spikes and noise from other sources. This manuscript proposes A deep-learning based Multichannel Spike Sorting Neural Signal Processor (SSNSP) Module for High-Channel-Count Brain Machine Interfaces to record spike activity (SA) of brain neuron signals with less noise. Here first data acquisition is first step and the data's are took form Neural Signal Processor (NSP). Then the collected features are stored in BMIs. After this process feature is extracted using Haar DWT. Haar DWT is a frequency based feature extractor used to extract the spike or noisy signals from the neuron signals. Then the extracted features are given to driven spike DBN, this is a combination of multi-layer perceptron (MLP) layer and DBN. To increase the accuracy, Adam-Cuckoo Search optimization is used, which optimize the driven spike DBN weight parameter. An FPGA was used to construct and test a prototype 32-channel SSNSP component based on this analysis. Synthesised signals are used at various signals to noise ratios. Then, human neurons are classified based on the channels containing neural spike data. The impact of busy as well as idle state prediction errors on the spectrum efficacy is examined. The proposed technique is implemented in MATLAB platform. Finally, the proposed technique attains better detection accuracy 22.86%, 28.94%, 31.11% and 27.34% compared to the existing models, like Deep Learning Laser Speckle Contrast ESNN (DL-LSC-ESNN), Highly Stretchable Hydro gels as Wearable with Implantable Sensors for Recording Physiological with Brain Neural Signals (HSN-WIS-RPBNS), Lower-power band of neuronal spiking action dominated through local single units enhances the Presentation of BMI (LPB-NSA-LSU-BMI) and Emotion Categorization Utilizing Feature Fusion of Multimodal Data along Deep Learning in Brain-Inspired Spiking Neural Network (EC-FFMD-BISNN) respectively.
C1 [Reddy, Vanga Karunakar; Kumar, K. Ashok] Matrusri Engn Coll, Dept Elect & Commun Engn, Hyderabad, Telangana, India.
   [Melingi, Sunil Babu] Deemed Univ, Vignans Fdn Sci Technol & Res, Dept Comp Sci & Engn, Vadlamudi, AP, India.
   [Kumar, Ch. V. M. S. N. Pavan] Bapatla Engn Coll, Dept Elect & Commun Engn, Bapatla, AP, India.
   [Mojjada, Ramesh Kumar] Renault Nissan Technol & Business Ctr India, Ascendas IT Pk,Mahindra World City SEZ Plot NoTP2,, Chengalpattu 603002, Tamil Nadu, India.
RP Reddy, VK (corresponding author), Matrusri Engn Coll, Dept Elect & Commun Engn, Hyderabad, Telangana, India.
EM karunakarece@matrusri.edu.in
CR Ahmadi N, 2022, IEEE ACCESS, V10, P29341, DOI 10.1109/ACCESS.2022.3159225
   Ahmadi N, 2021, J NEURAL ENG, V18, DOI 10.1088/1741-2552/abce3c
   Capizzi G, 2020, NEURAL NETWORKS, V129, P271, DOI 10.1016/j.neunet.2020.06.001
   Chahid A, 2020, IEEE J BIOMED HEALTH, V24, P2814, DOI 10.1109/JBHI.2020.2972286
   Donati ARC, 2016, SCI REP-UK, V6, DOI 10.1038/srep30383
   Formento E, 2021, bioRxiv, DOI [10.1101/2021.03.22.436518, 10.1101/2021.03.22.436518, DOI 10.1101/2021.03.22.436518]
   Francis SH, 2022, CIRC SYST SIGNAL PR, V41, P1751, DOI 10.1007/s00034-021-01850-2
   Girges C, 2022, NEUROMODULATION, V25, P1187, DOI 10.1016/j.neurom.2022.01.007
   Goncalves Cristhiane, 2021, Vibration Engineering and Technology of Machinery. Proceedings of VETOMAC XV 2019. Mechanisms and Machine Science (MMS 95), P343, DOI 10.1007/978-3-030-60694-7_22
   Holderrieth P., 2021, NEW COLLECT, V15, P113
   Kapgate D, 2022, INT J HUM-COMPUT INT, V38, P42, DOI 10.1080/10447318.2021.1921482
   Kim HH, 2022, APPL SOFT COMPUT, V117, DOI 10.1016/j.asoc.2021.108393
   Kim S, 2021, IEEE ACCESS, V9, P2633, DOI 10.1109/ACCESS.2020.3047071
   Liang QD, 2022, ADV SCI, V9, DOI 10.1002/advs.202201059
   Liu JZ, 2021, J NEURAL ENG, V18, DOI 10.1088/1741-2552/ac1ed0
   Mohsin M, 2020, IEEE ACCESS, V8, P105542, DOI 10.1109/ACCESS.2020.2999865
   Nakatani S, 2021, J NEURAL ENG, V18, DOI 10.1088/1741-2552/abd1bf
   Nason SR, 2020, NAT BIOMED ENG, V4, P973, DOI 10.1038/s41551-020-0591-0
   Pan HG, 2022, IEEE T HUM-MACH SYST, V52, P877, DOI 10.1109/THMS.2021.3138677
   Pan HG, 2021, NEURAL COMPUT APPL, V33, P5471, DOI 10.1007/s00521-020-05323-6
   Park YS, 2020, IEEE T BIO-MED ENG, V67, P817, DOI 10.1109/TBME.2019.2921448
   Rajesh P, 2022, APPL SOFT COMPUT, V128, DOI 10.1016/j.asoc.2022.109442
   Rajesh P., 2022, APPL SCI ENG PROG, V15, P5514, DOI DOI 10.14416/J.ASEP.2021.10.004
   Rapeaux AB, 2021, CURR OPIN BIOTECH, V72, P102, DOI 10.1016/j.copbio.2021.10.001
   Roy AM, 2022, ENG APPL ARTIF INTEL, V116, DOI 10.1016/j.engappai.2022.105347
   Saif-ur-Rehman M, 2021, J NEURAL ENG, V18, DOI 10.1088/1741-2552/abc8d4
   Salleh ASM, 2020, IEEE ACCESS, V8, P216419, DOI 10.1109/ACCESS.2020.3040895
   Serino A, 2022, NAT HUM BEHAV, V6, P565, DOI 10.1038/s41562-021-01233-2
   Shaikh S, 2022, HDB BIOCHIPS, P869
   Shajin FH, 2023, COMP M BIO BIO E-IV, V11, P750, DOI 10.1080/21681163.2022.2111719
   Shokur S, 2021, BRAIN COMPUTER INTER, P133
   Tan C, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20185328
   Tankus A, 2021, J NEURAL ENG, V18, DOI 10.1088/1741-2552/ac3315
   Wang ZM, 2020, IEEE T CIRCUITS-II, V67, P3592, DOI 10.1109/TCSII.2020.2992285
   Wen S., 2020, RES SQUARE, DOI [10.21203/rs.3.rs-128879/v1, DOI 10.21203/RS.3.RS-128879/V1]
   Wozniak M, 2023, NEURAL COMPUT APPL, V35, P14611, DOI 10.1007/s00521-021-05841-x
   Wu DP, 2019, IEEE ACCESS, V7, P41551, DOI 10.1109/ACCESS.2019.2904949
   Yoshimine T., 2021, FRONT NEUROSCI, V12, P478
   Zheng L, 2020, IEEE T MED IMAGING, V39, P1833, DOI 10.1109/TMI.2019.2958699
NR 39
TC 0
Z9 0
U1 2
U2 2
PD 2023 SEP 7
PY 2023
DI 10.1007/s10462-023-10575-4
EA SEP 2023
UT WOS:001060737800001
DA 2023-11-16
ER

PT C
AU Dehpanah, M
   Del Prete, Z
   Grigg, P
AF Dehpanah, M
   Del Prete, Z
   Grigg, P
BE Wolf, LJ
   Strock, JL
TI Using neural networks to determine the strength of association between
   mechanical variables and spike responses in rapidly adapting cutaneous
   mechanoreceptor neurons in mouse skin
SO 1ST INTERNATIONAL IEEE EMBS CONFERENCE ON NEURAL ENGINEERING 2003,
   CONFERENCE PROCEEDINGS
SE International IEEE EMBS Conference on Neural Engineering
DT Proceedings Paper
CT 1st International IEEE/EMBS Conference on Neural Engineering
CY MAR 20-22, 2003
CL CAPRI, ITALY
DE mechanoreceptors; logistic regression; neural network
AB Rapidly adapting mechanoreceptor neurons were recorded in a preparation of isolated skin and nerve from rat hairy skin. The skin was stretched dynamically, and tensile stress, strain, and the spike responses of single neurons were recorded. We determined the strength of association between spikes and stress, strain, and their rates of change using multiple logistic regression and a neural network. Both methods revealed that spikes were more strongly associated with the rate of change of stress than with other variables.
C1 Univ Massachusetts, Sch Med, Dept Physiol, Worcester, MA 01605 USA.
RP Dehpanah, M (corresponding author), Univ Massachusetts, Sch Med, Dept Physiol, Worcester, MA 01605 USA.
CR DELPRETE Z, 2002, J NEUROPHYSIOLOGY
   HANLEY JA, 1989, CRIT REV DIAGN IMAG, V29, P307
   SEARLE WS, P 19 ANN SAS US GROU, P1
NR 3
TC 0
Z9 0
U1 0
U2 1
PY 2003
BP 403
EP 405
DI 10.1109/CNE.2003.1196846
UT WOS:000182404500109
DA 2023-11-16
ER

PT C
AU Chen, Y
   Qu, H
   Zhang, ML
   Wang, YC
AF Chen, Yi
   Qu, Hong
   Zhang, Malu
   Wang, Yuchen
GP Assoc Advancement Artificial Intelligence
TI Deep Spiking Neural Network with Neural Oscillation and Spike-Phase
   Information
SO THIRTY-FIFTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, THIRTY-THIRD
   CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE AND THE
   ELEVENTH SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE
SE AAAI Conference on Artificial Intelligence
DT Proceedings Paper
CT 35th AAAI Conference on Artificial Intelligence / 33rd Conference on
   Innovative Applications of Artificial Intelligence / 11th Symposium on
   Educational Advances in Artificial Intelligence
CY FEB 02-09, 2021
CL ELECTR NETWORK
ID LEARNING ALGORITHM
AB Deep spiking neural network (DSNN) is a promising computational model towards artificial intelligence. It benefits from both the DNNs and SNNs through a hierarchy structure to extract multiple levels of abstraction and the event-driven computational manner to provide ultra-low-power neuromorphic implementation, respectively. However, how to efficiently train the DSNNs remains an open question because of the non-differentiable spike function that prevents the traditional back-propagation (BP) learning algorithm directly applied to DSNNs. Here, inspired by the findings from the biological neural networks, we address the above-mentioned problem by introducing neural oscillation and spike-phase information to DSNNs. Specifically, we propose an Oscillation Postsynaptic Potential (Os-PSP) and phase-locking active function, and further put forward a new spiking neuron model, namely Resonate Spiking Neuron (RSN). Based on the RSN, we propose a Spike-Level-Dependent Back-Propagation (SLDBP) learning algorithm for DSNNs. Experimental results show that the proposed learning algorithm resolves the problems caused by the incompatibility between the BP learning algorithm and SNNs, and achieves state-of-the-art performance in single spike-based learning algorithms. This work investigates the contribution of introducing biologically inspired mechanisms, such as neural oscillation and spike-phase information to DSNNs and providing a new perspective to design future DSNNs.
C1 [Chen, Yi; Qu, Hong; Zhang, Malu; Wang, Yuchen] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu, Sichuan, Peoples R China.
   [Zhang, Malu] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore, Singapore.
RP Chen, Y (corresponding author), Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu, Sichuan, Peoples R China.
EM chenyi@std.uestc.edu.cn; hongqu@uestc.edu.cn; maluzhang@u.nus.edu;
   wangyuchen@std.uestc.edu.cn
CR Abdel-Hamid O, 2014, IEEE-ACM T AUDIO SPE, V22, P1533, DOI 10.1109/TASLP.2014.2339736
   Bagur S, 2018, NEURON, V100, P768, DOI 10.1016/j.neuron.2018.11.008
   Battaglia FP, 2011, TRENDS COGN SCI, V15, P310, DOI 10.1016/j.tics.2011.05.008
   Bing Han, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13555, DOI 10.1109/CVPR42600.2020.01357
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Deng L, 2020, NEURAL NETWORKS, V121, P294, DOI 10.1016/j.neunet.2019.09.005
   Diehl PU, 2016, IEEE IJCNN, P4278, DOI 10.1109/IJCNN.2016.7727758
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Esser S. K., 2015, ADV NEURAL INFORM PR, P1117
   Esser SK, 2016, P NATL ACAD SCI USA, V113, P11441, DOI 10.1073/pnas.1604850113
   Ghaeini R., 2018, P C N AM ASS CHAPT A, VI, P1460, DOI DOI 10.18653/V1/N18-1132
   Gütig R, 2016, SCIENCE, V351, DOI 10.1126/science.aab4113
   Hanslmayr S, 2016, TRENDS NEUROSCI, V39, P16, DOI 10.1016/j.tins.2015.11.004
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hunsberger E., 2016, TRAINING SPIKING DEE
   Hunsberger E., 2015, ARXIV PREPRINT ARXIV
   Kheradpisheh SR, 2020, INT J NEURAL SYST, V30, DOI 10.1142/S0129065720500276
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Lam MWY, 2019, INT CONF ACOUST SPEE, P7235, DOI 10.1109/ICASSP.2019.8683660
   Lee G, 2020, 58TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2020), P860
   Liu CJ, 2018, IEEE T NEUR NET LEAR, V29, P4857, DOI 10.1109/TNNLS.2017.2782266
   Liu Q., 2017, ARXIV170603609
   Luo XL, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00559
   Mostafa Hesham, 2018, IEEE Trans Neural Netw Learn Syst, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Mozafari M, 2018, IEEE T NEUR NET LEAR, V29, P6178, DOI 10.1109/TNNLS.2018.2826721
   Nair V., 2010, ICML, P807
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   Rueckauer B, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351295
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Shao YL, 2020, IEEE T NEUR NET LEAR, V31, P3319, DOI 10.1109/TNNLS.2019.2942951
   Shrestha S. B., 2018, ADV NEURAL INFORM PR
   Shrestha S. B., 2017, IEEE T NEURAL NETWOR, V29, P3126
   Shrestha SB, 2017, NEURAL NETWORKS, V96, P33, DOI 10.1016/j.neunet.2017.08.010
   Voelker A. R., 2020, ABS200203553 ARXIV
   Wu YJ, 2019, AAAI CONF ARTIF INTE, P1311
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Xu CQ, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00104
   Xu Y, 2013, NEURAL NETWORKS, V43, P99, DOI 10.1016/j.neunet.2013.02.003
   Young T, 2018, IEEE COMPUT INTELL M, V13, P55, DOI 10.1109/MCI.2018.2840738
   Zhang M., 2020, ARXIVABS200311837
   Zhang ML, 2020, IEEE J-STSP, V14, P592, DOI 10.1109/JSTSP.2020.2983547
   Zhang W, 2020, ARXIV PREPRINT ARXIV
NR 44
TC 3
Z9 3
U1 0
U2 7
PY 2021
VL 35
BP 7073
EP 7080
UT WOS:000680423507021
DA 2023-11-16
ER

PT C
AU Maass, W
AF Maass, W
BE Mozer, MC
   Jordan, MI
   Petsche, T
TI Noisy spiking neurons with temporal coding have more computational power
   than sigmoidal neurons
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE
   1996 CONFERENCE
SE ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS
DT Proceedings Paper
CT 10th Annual Conference on Neural Information Processing Systems (NIPS)
CY DEC 02-05, 1996
CL DENVER, CO
AB We exhibit a novel way of simulating sigmoidal neural nets by networks of noisy spiking neurons in temporal coding. Furthermore it is shown that networks of noisy spiking neurons with temporal coding have a strictly larger computational power than sigmoidal neural nets with the same number of units.
RP Maass, W (corresponding author), GRAZ TECH UNIV,INST THEORET COMP SCI,KLOSTERWIESGASSE 32-2,A-8010 GRAZ,AUSTRIA.
NR 0
TC 73
Z9 79
U1 0
U2 2
PY 1997
VL 9
BP 211
EP 217
UT WOS:A1997BH93C00030
DA 2023-11-16
ER

PT C
AU Auge, D
   Hille, J
   Mueller, E
   Knoll, A
AF Auge, Daniel
   Hille, Julian
   Mueller, Etienne
   Knoll, Alois
BE Struc, V
   Ivanovska, M
TI Hand Gesture Recognition in Range-Doppler Images Using Binary Activated
   Spiking Neural Networks
SO 2021 16TH IEEE INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE
   RECOGNITION (FG 2021)
SE IEEE International Conference on Automatic Face and Gesture Recognition
   and Workshops
DT Proceedings Paper
CT 16th IEEE International Conference on Automatic Face and Gesture
   Recognition (FG)
CY DEC 15-18, 2021
CL TIH iHub Drishti, ELECTR NETWORK
HO TIH iHub Drishti
AB Many hand gesture recognition systems use radar to sense the motion of the hand due to its independence of lighting and its inherent privacy. As in the case of cameras, complex signal processing chains consisting of classical algorithms and neural network-base approaches are necessary to evaluate the incoming data stream. Especially on mobile devices, the reduction of the total energy consumption of the recognition system is crucial as it would lead to an increased battery life. Spiking neural networks have been shown to consume much less energy than current networks by operating event-driven and using time as the main information carrier. However, practical applications in which they are on par with classical approaches are rare. In this paper we utilize spiking neural networks to perform hand gesture recognition in radar data. We show that the temporal affinity of spiking networks and the possibility to binarize the radar-generated range-Doppler images without large loss of information introduces a promising synergy. Using simple networks consisting of 75 recurrently connected spiking neurons, we are able to reach current state-of-the-art performance on two public datasets. With this approach, gesture recognition systems can operate much more energy-efficient, making spiking neural networks viable alternatives to current solutions.
C1 [Auge, Daniel; Hille, Julian; Mueller, Etienne; Knoll, Alois] Tech Univ Munich, Dept Informat, Munich, Germany.
   [Hille, Julian] Infineon Technol AG, Munich, Germany.
RP Auge, D (corresponding author), Tech Univ Munich, Dept Informat, Munich, Germany.
CR Amin MG, 2019, IEEE RAD CONF, DOI [10.1109/radar.2019.8835661, 10.1109/pst47121.2019.8949029]
   Banerjee D, 2020, IEEE IJCNN, DOI 10.1109/ijcnn48605.2020.9206853
   Bellec G., 2018, ADV NEURAL INFORM PR
   Benalcazar M.E., 2017, IEEE 2 ECUADOR TECHN, P1, DOI 10.1109/ETCM.2017.8247458
   Blouw P, 2020, PROCEEDINGS OF THE 2019 7TH ANNUAL NEURO-INSPIRED COMPUTATIONAL ELEMENTS WORKSHOP (NICE 2019), DOI 10.1145/3320288.3320304
   Craley J, 2017, 2017 51ST ANNUAL CONFERENCE ON INFORMATION SCIENCES AND SYSTEMS (CISS)
   Davidson S, 2021, FRONT NEUROSCI-SWITZ, V15, DOI 10.3389/fnins.2021.651141
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Dekker B, 2017, EUROP RADAR CONF, P163, DOI 10.23919/EURAD.2017.8249172
   Gerstner W., 2002, SPIKING NEURON MODEL
   Gramann C., 2003, SIMULATION PULSVERAR
   Khan MM, 2008, IEEE IJCNN, P2849, DOI 10.1109/IJCNN.2008.4634199
   Lien JM, 2016, ACM T GRAPHIC, V35, DOI [10.1145/2897824.2925953, 10.1145/9999997.9999999]
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Mayr C., 2019, ARXIV191102385
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Poon CS, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00108
   Rogez G, 2015, IEEE I CONF COMP VIS, P3889, DOI 10.1109/ICCV.2015.443
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Scherer M., 2020, ARXIV PREPRINT ARXIV
   Skaria S, 2019, IEEE SENS J, V19, P3041, DOI 10.1109/JSEN.2019.2892073
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Wang SW, 2016, UIST 2016: PROCEEDINGS OF THE 29TH ANNUAL SYMPOSIUM ON USER INTERFACE SOFTWARE AND TECHNOLOGY, P851, DOI 10.1145/2984511.2984565
   Winner H., 2014, HDB DRIVER ASSISTANC
   Zhang ZY, 2018, IEEE SENS J, V18, P3278, DOI 10.1109/JSEN.2018.2808688
   Zimmermann C, 2017, IEEE I CONF COMP VIS, P4913, DOI 10.1109/ICCV.2017.525
NR 28
TC 2
Z9 2
U1 0
U2 1
PY 2021
UT WOS:000784811600055
DA 2023-11-16
ER

PT C
AU Lee, YJ
   On, MB
   Xiao, X
   Ben Yoo, SJ
AF Lee, Yun-Jhu
   On, Mehmet Berkay
   Xiao, Xian
   Ben Yoo, S. J.
GP IEEE
TI Energy-Efficient Photonic Spiking Neural Network on a monolithic silicon
   CMOS photonic platform
SO 2021 OPTICAL FIBER COMMUNICATIONS CONFERENCE AND EXPOSITION (OFC)
DT Proceedings Paper
CT Optical Fiber Communications Conference and Exhibition (OFC)
CY JUN 06-11, 2021
CL ELECTR NETWORK
ID COMMUNICATION; PROCESSOR; SYSTEM
AB We designed, simulated, and taped-out a photonic spiking neural network on a monolithic silicon CMOS photonic platform. Benchmarking shows proposed PSNN outperforms other neuromorphic hardware with 21.09fJ/spike and 61.4 mu W average power at MNIST experiment. (C) 2021 The Author(s)
C1 [Lee, Yun-Jhu; On, Mehmet Berkay; Xiao, Xian; Ben Yoo, S. J.] Univ Calif Davis, Dept Elect & Comp Engn, Davis, CA 95616 USA.
RP Ben Yoo, SJ (corresponding author), Univ Calif Davis, Dept Elect & Comp Engn, Davis, CA 95616 USA.
EM fexlee@ucdavis.edu; sbyoo@ucdavis.edu
CR Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Buhler FN, 2017, SYMP VLSI CIRCUITS, pC30, DOI 10.23919/VLSIC.2017.8008536
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Esser S. K., 2015, ADV NEURAL INFORM PR, P1117
   Hsu J, 2014, IEEE SPECTRUM, V51, P17, DOI 10.1109/MSPEC.2014.6905473
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Kim D, 2020, IEEE SOLID-ST CIRC L, V3, P278, DOI 10.1109/LSSC.2020.3013448
   Kirchhoff-Institut fur Physik, HICANN
   Knowles S., 2017, SCALABLE SILICON COM
   Lee YJ, 2020, CONF LASER ELECTR
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   MILLER DAB, 1989, OPT LETT, V14, P146, DOI 10.1364/OL.14.000146
   Miller DAB, 2017, J LIGHTWAVE TECHNOL, V35, P346, DOI 10.1109/JLT.2017.2647779
   Osada A, 2018, APPL PHYS EXPRESS, V11, DOI 10.7567/APEX.11.072002
   Park J, 2019, ISSCC DIG TECH PAP I, V62, P140, DOI 10.1109/ISSCC.2019.8662398
   Rubino A, 2019, IEEE I C ELECT CIRC, P458, DOI [10.1109/ICECS46596.2019.8964713, 10.1109/icecs46596.2019.8964713]
   Schemmel J, 2008, IEEE IJCNN, P431, DOI 10.1109/IJCNN.2008.4633828
   Smith, NVIDIA VOLTA UNVEILE
   Stromatias E., 2016, SCALABILITY ROBUSTNE
   Tan P.-Y., 2020, POWER EFFICIENT BINA
   Teich, TEARING APART GOOGLE
   Totovic AR, 2020, IEEE J SEL TOP QUANT, V26, DOI 10.1109/JSTQE.2020.2975579
   Whatmough PN, 2018, IEEE J SOLID-ST CIRC, V53, P2722, DOI 10.1109/JSSC.2018.2841824
   Wijesinghe P, 2018, IEEE TETCI, V2, P345, DOI 10.1109/TETCI.2018.2829924
   Xiao X., 2020, 2020 IEEE PHOT C IPC, P1
   Yin SY, 2017, DES AUT CON, DOI 10.1145/3061639.3062232
NR 27
TC 2
Z9 2
U1 0
U2 0
PY 2021
AR Tu5H.5
UT WOS:000698978300425
DA 2023-11-16
ER

PT J
AU Chen, YH
   Chen, Y
   Zhang, GX
   Paul, P
   Wu, TB
   Zhang, XH
   Rong, HN
   Ma, XM
AF Chen, Yunhui
   Chen, Ying
   Zhang, Gexiang
   Paul, Prithwineel
   Wu, Tianbao
   Zhang, Xihai
   Rong, Haina
   Ma, Xiaomin
TI A Survey of Learning Spiking Neural P Systems and A Novel Instance
SO INTERNATIONAL JOURNAL OF UNCONVENTIONAL COMPUTING
DT Article
DE Membrane computing; spiking neural networks; spiking neural P systems;
   neural plasticity; structural plasticity; machine learning
ID NETWORKS
AB In the last few decades membrane computing has established itself as an important branch of natural computing. Investigating computational power, complexity aspects and real-world applications of different variants of membrane computing models have been a successful direction of research. In recent years with the invention of efficient learning algorithms, many researchers have concentrated their research into construction of intelligent biological computing systems inspired by the working of neurons in human brains to emulate human thinking. Spiking neural P systems are such types of computing systems. In this paper we survey spiking neural P systems (i.e., neural-like membrane computing models) with learning ability, their architecture, learning mechanism and compare these models, discuss their advantages and disadvantages and application of these models in solving real-world problems. We further discuss the learning mechanism of associative memory network based on spiking neural P systems with white holes and weights. At the end, we discuss some new ideas to further extend the study of membrane computing models having learning ability.
C1 [Chen, Yunhui; Chen, Ying; Wu, Tianbao; Zhang, Xihai; Ma, Xiaomin] State Grid Sichuan Elect Power Co, Chengdu 610094, Peoples R China.
   [Zhang, Gexiang; Paul, Prithwineel] Chengdu Univ Technol, Res Ctr Artificial Intelligence, Chengdu 610059, Peoples R China.
   [Zhang, Gexiang; Paul, Prithwineel; Zhang, Xihai; Rong, Haina] Southwest Jiaotong Univ, Sch Elect Engn, Chengdu 611756, Peoples R China.
RP Zhang, GX (corresponding author), Chengdu Univ Technol, Res Ctr Artificial Intelligence, Chengdu 610059, Peoples R China.; Zhang, GX (corresponding author), Southwest Jiaotong Univ, Sch Elect Engn, Chengdu 611756, Peoples R China.
EM zhgxdylan@126.com
CR [Anonymous], P 5 BRAINST WEEK MEM
   [Anonymous], 1991, RETINA NEOCORTEX, DOI DOI 10.1007/978-1-4684-6775-8_5
   [Anonymous], COLLECTIVE PROPERTIE
   [Anonymous], 2017, REAL LIFE APPL MEMBR
   [Anonymous], 2015, INT J SWARM INTELL
   [Anonymous], 1997, MACH LEARN
   Buiu C, 2019, J MEMBRANE COMPUT, V1, P262, DOI 10.1007/s41965-019-00029-8
   Cabarle Francis George C., 2015, Unconventional Computation and Natural Computation. 14th International Conference, UCNC 2015. Proceedings, P132, DOI 10.1007/978-3-319-21819-9_9
   Cabarle F. G. C., 2017, IEEE T NANOBIOSCIENC, V16
   Cabarle F.G.C., 2018, LNCS, V11270
   Cabarle FGC, 2018, IEEE T NANOBIOSCI, V17, P560, DOI 10.1109/TNB.2018.2879345
   Cabarle FGC, 2016, NEURAL COMPUT APPL, V27, P1337, DOI 10.1007/s00521-015-1937-5
   Cabarle FGC, 2015, NEURAL COMPUT APPL, V26, P1905, DOI 10.1007/s00521-015-1857-4
   Caroni P, 2012, NAT REV NEUROSCI, V13, P478, DOI 10.1038/nrn3258
   Cavaliere M, 2008, LECT NOTES COMPUT SC, V4848, P246
   Chen ZH, 2018, NEURAL COMPUT APPL, V29, P695, DOI 10.1007/s00521-016-2489-z
   COOPER LN, 1979, BIOL CYBERN, V33, P9, DOI 10.1007/BF00337414
   de la Cruz RTA, 2019, J MEMBRANE COMPUT, V1, P161, DOI 10.1007/s41965-019-00021-2
   Díaz-Pernil D, 2019, J MEMBRANE COMPUT, V1, P58, DOI 10.1007/s41965-018-00002-x
   DOUGLAS RJ, 1967, PSYCHOL BULL, V67, P416, DOI 10.1037/h0024599
   Freund R, 2008, INT J FOUND COMPUT S, V19, P1223, DOI 10.1142/S0129054108006248
   Geirhos R., 2018, ARXIV170606969V2CSCV
   Gerstner W, 2002, BIOL CYBERN, V87, P404, DOI 10.1007/s00422-002-0353-y
   Gutierrez-Naranjo M.A., 2008, LNCS, V5391, P217
   Hebb D., 2005, ORG BEHAV NEUROPSYCH
   Hong Peng, 2010, Proceedings 2010 Sixth International Conference on Natural Computation (ICNC 2010), P3008, DOI 10.1109/ICNC.2010.5584269
   HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554
   Ionescu M, 2006, FUND INFORM, V71, P279
   Ishdorj TO, 2010, THEOR COMPUT SCI, V411, P2345, DOI 10.1016/j.tcs.2010.01.019
   Jiang Y, 2019, J MEMBRANE COMPUT, V1, P270, DOI 10.1007/s41965-019-00025-y
   Jimenez ZB, 2019, J MEMBRANE COMPUT, V1, P145, DOI 10.1007/s41965-019-00020-3
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
   Lee C, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00435
   Leporati Alberto, 2009, Natural Computing, V8, P681, DOI 10.1007/s11047-008-9091-y
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Mozafari M, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00625
   Orellana-Martín D, 2019, J MEMBRANE COMPUT, V1, P29, DOI 10.1007/s41965-018-00003-w
   Pan LQ, 2019, J MEMBRANE COMPUT, V1, P1, DOI 10.1007/s41965-019-00010-5
   Pan LQ, 2012, NEURAL COMPUT, V24, P805, DOI 10.1162/NECO_a_00238
   Pan LQ, 2011, SCI CHINA INFORM SCI, V54, P1596, DOI 10.1007/s11432-011-4303-y
   Pan LQ, 2009, INT J COMPUT COMMUN, V4, P273, DOI 10.15837/ijccc.2009.3.2435
   Päun G, 2000, J COMPUT SYST SCI, V61, P108, DOI 10.1006/jcss.1999.1693
   Paun G, 2007, ROM J INF SCI TECH, V10, P303
   Paun Gh, 2010, OXFORD HDB MEMBRANE
   Peng H, 2013, INFORM SCIENCES, V235, P106, DOI 10.1016/j.ins.2012.07.015
   Pérez-Hurtado I, 2019, J MEMBRANE COMPUT, V1, P93, DOI 10.1007/s41965-019-00014-1
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Rong H., 2018, LNCS, V11270
   Rong HN, 2019, COMPLEXITY, V2019, DOI 10.1155/2019/2635714
   Sánchez-Karhunen E, 2019, J MEMBRANE COMPUT, V1, P40, DOI 10.1007/s41965-019-00008-z
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Simon H., 2003, LEAST MEAN SQUARE AD, V31
   Song BS, 2019, MATH PROBL ENG, V2019, DOI 10.1155/2019/5793234
   Song BS, 2017, INFORM SCIENCES, V378, P177, DOI 10.1016/j.ins.2016.10.046
   Song T, 2016, IEEE T NANOBIOSCI, V15, P1
   Song T, 2019, IEEE T NANOBIOSCI, V18, P176, DOI 10.1109/TNB.2019.2896981
   Song T, 2018, IEEE T NANOBIOSCI, V17, P474, DOI 10.1109/TNB.2018.2873221
   Song T, 2018, IEEE T COGN DEV SYST, V10, P1106, DOI 10.1109/TCDS.2017.2785332
   Song T, 2016, NEUROCOMPUTING, V193, P193, DOI 10.1016/j.neucom.2016.02.023
   Song T, 2014, THEOR COMPUT SCI, V529, P82, DOI 10.1016/j.tcs.2014.01.001
   Spiess R, 2016, FRONT COMPUT NEUROSC, V10, DOI 10.3389/fncom.2016.00093
   Sun MM, 2017, AIP CONF PROC, V1890, DOI 10.1063/1.5005251
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Tu M, 2014, CHINESE J ELECTRON, V23, P87
   Wang J, 2013, INT J COMPUT MATH, V90, P857, DOI 10.1080/00207160.2012.743653
   Wang T, 2014, INT J COMPUT COMMUN, V9, P786, DOI 10.15837/ijccc.2014.6.1485
   Wang T, 2015, IEEE T POWER SYST, V30, P1182, DOI 10.1109/TPWRS.2014.2347699
   Zeng XX, 2014, SCI CHINA INFORM SCI, V57, DOI 10.1007/s11432-013-4848-z
   Zhang GX, 2014, INFORM SCIENCES, V279, P528, DOI 10.1016/j.ins.2014.04.007
   Zhang YQ, 2006, PROG NAT SCI, V16, P338, DOI 10.1080/10020070612330002
   Zhang Y, 2014, INT J NEURAL SYST, V24, DOI 10.1142/S0129065714500038
NR 71
TC 7
Z9 8
U1 0
U2 11
PY 2021
VL 16
IS 2-3
SI SI
BP 173
EP 200
UT WOS:000613543300005
DA 2023-11-16
ER

PT J
AU Zhang, JL
   Zhu, Y
   Li, Y
   Chen, F
   Liu, Y
   Qu, H
AF Zhang, Jilun
   Zhu, Yi
   Li, Ying
   Chen, Fang
   Liu, Ying
   Qu, Hong
TI Non-contact Liquid Level Detection Method Based on Multilayer Spiking
   Neural Network
SO JOURNAL OF ELECTRONICS & INFORMATION TECHNOLOGY
DT Article
DE Spiking Neural Networks (SNNs); Pattern recognition; LIF neuron; Spiking
   encoding; Non-contact liquid level measurement
ID IMAGE CLASSIFICATION; SENSOR; GAME; GO
AB Although the non-contact liquid level detection method based on deep learning can perform well, its high demand on computational resources makes it not suitable for embedded devices with limited resource. To solve this problem, a non-contact liquid level detection method is first proposed based on multilayer spiking neural network; Furthermore, spiking encoding methods based on single frame and frame difference are proposed to encode the temporal dynamics of video stream into reconfigurable spike patterns; Finally, the model is tested in the real scene. The experimental results show that the proposed method has high application value.
C1 [Zhang, Jilun; Liu, Ying; Qu, Hong] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
   [Zhu, Yi; Li, Ying; Chen, Fang] Beijing Xiaomi Mobile Software Co Ltd, Beijing 100085, Peoples R China.
RP Liu, Y (corresponding author), Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
EM liuying770315@std.uestc.edu.cn
CR Ali A, 2021, COMMUN ACM, V64, P124, DOI 10.1145/3451150
   [Anonymous], 2022, M20G51, V12, P233, DOI [10.12677/app.2022.125026, DOI 10.12677/APP.2022.125026]
   Areekath L, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22155508
   Bing Han, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P388, DOI 10.1007/978-3-030-58607-2_23
   Bowen LI, 2019, THESIS S CHINA U TEC, DOI [10.27151/d.cnki.ghnlu.2019.003946, DOI 10.27151/D.CNKI.GHNLU.2019.003946]
   Caporale N, 2008, ANNU REV NEUROSCI, V31, P25, DOI 10.1146/annurev.neuro.31.060407.125639
   Chan TH, 2015, IEEE T IMAGE PROCESS, V24, P5017, DOI 10.1109/TIP.2015.2475625
   Esmaeilpour M, 2022, IEEE T INF FOREN SEC, V17, P2044, DOI 10.1109/TIFS.2022.3175603
   He FS, 2020, J ELECTRON INF TECHN, V42, P119, DOI 10.11999/JEIT180899
   He RJ, 2022, IEEE SENS J, V22, P1081, DOI 10.1109/JSEN.2021.3132098
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   [胡一凡 Hu Yifan], 2021, [控制与决策, Control and Decision], V36, P1
   Huang Ling, 2012, Automation & Instrumentation, V27, P57
   Huang ZY, 2021, MATH BIOSCI ENG, V18, P3491, DOI 10.3934/mbe.2021175
   Islam T, 2021, IEEE SENS J, V21, P24812, DOI 10.1109/JSEN.2021.3112848
   ISMAEL M A, 2018, AL QADISIYAH J ENG S, V10, P550, DOI [10.30772/qjes.v10i4.504, DOI 10.30772/QJES.V10I4.504]
   JIA Jing, 2022, APPL PHYS, V12, P233, DOI [10.12677/app.2022.125026, DOI 10.12677/APP.2022.125026]
   Jiang YJ, 2019, LECT NOTES COMPUT SC, V11295, P202, DOI 10.1007/978-3-030-05710-7_17
   Jiao LC, 2022, IEEE T NEUR NET LEAR, V33, P3195, DOI 10.1109/TNNLS.2021.3053249
   Kim Y, 2021, NEURAL NETWORKS, V144, P686, DOI 10.1016/j.neunet.2021.09.022
   LAN Yanling, 2020, P 4 INT C COMP SCI A, P127, DOI [10.1145/3424978.3425112, DOI 10.1145/3424978.3425112]
   Lapique L., 1907, J PHYSL PATHOL GEN, V9, P620, DOI DOI 10.1007/S00422-007-0189-6
   Li ST, 2019, IEEE T GEOSCI REMOTE, V57, P6690, DOI 10.1109/TGRS.2019.2907932
   Liang X, 2021, ACTA OPT SIN, V41, DOI 10.3788/A0S202141.2110001
   Liao KY, 2022, OPT FIBER TECHNOL, V70, DOI 10.1016/j.yofte.2022.102874
   LIAO Yun, 2020, J COMPUTER APPL, V40, P274, DOI [10.11772/j.issn.1001-9081.2019081360, DOI 10.11772/J.ISSN.1001-9081.2019081360]
   Liao ZY, 2022, J ELECTRON INF TECHN, V44, P1539, DOI 10.11999/JEIT211381
   Luo XL, 2022, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2022.3164930
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maass W, 2001, PULSED NEURAL NETWOR
   Qiao GC, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22103714
   Ren Ming-wu, 2007, Computer Engineering and Applications, V43, P204
   SHEN J, 1992, CVGIP-GRAPH MODEL IM, V54, P112, DOI 10.1016/1049-9652(92)90060-B
   Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   WANG Luping, 2021, ELECT MEASUREMENT TE, V44, P87, DOI [10.19651/j.cnki.emt.2107326, DOI 10.19651/J.CNKI.EMT.2107326]
   Wang R, 2020, IEEE ACCESS, V8, P132761, DOI 10.1109/ACCESS.2020.3010272
   Wei MH, 2022, IEEE T APPL SUPERCON, V32, DOI 10.1109/TASC.2021.3131398
   Xu Q, 2022, IEEE T NEUR NET LEAR, V33, P1935, DOI 10.1109/TNNLS.2021.3107449
   Xu Q, 2020, NEURAL NETWORKS, V121, P512, DOI 10.1016/j.neunet.2019.08.034
   Zhang JP, 2019, MED IMAGE ANAL, V54, P10, DOI 10.1016/j.media.2019.02.010
   Zhang L, 2019, AAAI CONF ARTIF INTE, P1319
   [张铁林 Zhang Tielin], 2021, [计算机学报, Chinese Journal of Computers], V44, P1767
NR 43
TC 0
Z9 0
U1 0
U2 0
PD AUG
PY 2023
VL 45
IS 8
BP 2759
EP 2769
DI 10.11999/JEIT221388
UT WOS:001066225400009
DA 2023-11-16
ER

PT C
AU Agebure, MA
   Oyetunji, EO
   Baagyere, EY
AF Agebure, Moses Apambila
   Oyetunji, Elkanah Olaosebikan
   Baagyere, Edward Yellakuor
GP IEEE
TI A Supervised Method for Learning Precise Timing of Multiple Spikes in
   Spiking Neural Networks
SO 2019 IEEE AFRICON
SE Africon
DT Proceedings Paper
CT IEEE AFRICON Conference - Powering Africa s Sustainable Energy for AD
   Agenda - The Role ofICT and Engineering.
CY SEP 25-27, 2019
CL Accra, GHANA
DE Supervised learning; spike sequence learning; least squares; temporal
   coding
ID GRADIENT DESCENT; MACHINE; NEURONS; RESUME; MODEL
AB Information in spiking neural network is encoded using the precise timing of spikes as done in biological neural systems. Neurons learn from these spikes based on their timing. It is evident that supervised learning do occur in biological neural systems, but exactly how it occurs and the development of definite mathematical formulation for training these neurons to fire multiple spikes at precise times remains an open research area. Significant strives have been made to formulate supervised spiking neural network learning rules for multi-spiking neurons, however, convergence is not guaranteed in most of these methods when the output spike train contains more than one spike. To this end, a new learning scheme is proposed in this paper which ensures convergence with an increase in the number of spikes in an output spike train. The proposed method elicits a locality concept of spikes and the approximation capabilities of the least squares method to derive a weight update scheme for training multi-spiking neurons using the precise timing of spikes. The performance of the proposed method is evaluated on spike sequence learning and compared with a well-known supervised learning method for multi-spiking neurons, the ReSuMe. The performance is measured using the correlation-based metric and the proposed scheme achieved better accuracy and convergence rates than the well-known learning method for varying learning periods.
C1 [Agebure, Moses Apambila; Baagyere, Edward Yellakuor] Univ Dev Studies, Comp Sci Dept, Navrongo Campus, Navrongo, Ghana.
   [Oyetunji, Elkanah Olaosebikan] Lagos State Univ, Mech Engn Dept, Lagos, Nigeria.
RP Agebure, MA (corresponding author), Univ Dev Studies, Comp Sci Dept, Navrongo Campus, Navrongo, Ghana.
EM magebure@uds.edu.gh; eoyetunji@yahoo.com; ybaagyere@uds.edu.gh
CR Belatreche A, 2006, NEW MATH NAT COMPUT, V2, P237, DOI 10.1142/S179300570600049X
   Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   Booij O, 2005, INFORM PROCESS LETT, V95, P552, DOI 10.1016/j.ipl.2005.05.023
   FITZHUGH R, 1961, BIOPHYS J, V1, P445, DOI 10.1016/S0006-3495(61)86902-6
   Florian RV, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0040233
   Gerstner W, 2002, SPIKING NEURON MODEL, DOI [10.1017/CBO9780511815706, DOI 10.1017/CBO9780511815706]
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   Hamed H. N. A., 2010, AUSTR J INTELLIGENT, V11
   Hebb D. O., 1949, J COGNITIVE NEUROSCI, V99, P70
   Huang GB, 2004, IEEE IJCNN, P985
   Huang GB, 2006, NEUROCOMPUTING, V70, P489, DOI 10.1016/j.neucom.2005.12.126
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Kasinski A., 2006, International Journal of Applied Mathematics and Computer Science, V16, P101
   Lobo JL, 2018, NEURAL NETWORKS, V108, P1, DOI 10.1016/j.neunet.2018.07.014
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   McKennoch S, 2006, IEEE IJCNN, P3970
   Mohemmed A, 2013, NEUROCOMPUTING, V107, P3, DOI 10.1016/j.neucom.2012.08.034
   Mostafa H, 2018, IEEE T NEUR NET LEAR, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Nicola W, 2017, NAT COMMUN, V8, DOI 10.1038/s41467-017-01827-3
   Pavlidis NG, 2005, IEEE IJCNN, P2190
   Ponulak F., 2014, ALLEN 2011 INTRO SPI, P2011
   Ponulak F., 2006, THESIS
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   ROSENBLATT F, 1958, PSYCHOL REV, V65, P386, DOI 10.1037/h0042519
   Schliebs S, 2013, EVOL SYST-GER, V4, P87, DOI 10.1007/s12530-013-9074-9
   Schreiber S, 2003, NEUROCOMPUTING, V52-4, P925, DOI 10.1016/S0925-2312(02)00838-X
   Silva M, 2017, J VOICE, V31, P24, DOI 10.1016/j.jvoice.2016.02.019
   Sporea I, 2013, NEURAL COMPUT, V25, P473, DOI 10.1162/NECO_a_00396
   Taherkhani A., 2015, EDL EXTENDED DELAY L, V9490, P11
   Taherkhani A, 2018, IEEE T NEUR NET LEAR, V29, P5394, DOI 10.1109/TNNLS.2018.2797801
   Taherkhani A, 2015, IEEE T NEUR NET LEAR, V26, P3137, DOI 10.1109/TNNLS.2015.2404938
   Thorpe S, 1998, COMPUTATIONAL NEUROSCIENCE: TRENDS IN RESEARCH, P113
   Widrow B., 1960, ADAPTIVE SWITCHING C
   Xin JG, 2001, IEEE IJCNN, P1772, DOI 10.1109/IJCNN.2001.938430
   Xu Y, 2013, NEURAL NETWORKS, V43, P99, DOI 10.1016/j.neunet.2013.02.003
NR 35
TC 0
Z9 0
U1 0
U2 1
PY 2019
DI 10.1109/africon46755.2019.9134053
UT WOS:000614822800192
DA 2023-11-16
ER

PT C
AU Ru, D
   Zhang, X
   Xu, ZY
   Ferrari, S
   Mazumder, P
AF Ru, Di
   Zhang, Xu
   Xu, Ziye
   Ferrari, Silvia
   Mazumder, Pinaki
GP IEEE
TI Digital Implementation of a Spiking Neural Network (SNN) Capable of
   Spike-Timing-Dependent Plasticity (STDP) Learning
SO 2014 IEEE 14TH INTERNATIONAL CONFERENCE ON NANOTECHNOLOGY (IEEE-NANO)
DT Proceedings Paper
CT 14th IEEE International Conference on Nanotechnology (IEEE-NANO)
CY AUG 14-21, 2014
CL Toronto, CANADA
AB The neural network model of computation has been proven to be faster and more energy-efficient than Boolean CMOS computations in numerous real-world applications. As a result, neuromorphic circuits have been garnering growing interest as the integration complexity within chips has reached several billion transistors. This article presents a digital implementation of a re-scalable spiking neural network (SNN) to demonstrate how spike timing-dependent plasticity (STDP) learning can be employed to train a virtual insect to navigate through a terrain with obstacles by processing information from the environment.
C1 [Ru, Di; Mazumder, Pinaki] Univ Michigan, Ann Arbor, MI 48109 USA.
   [Zhang, Xu; Xu, Ziye; Ferrari, Silvia] Duke Univ, Durham, NC 27708 USA.
RP Ru, D (corresponding author), Univ Michigan, Ann Arbor, MI 48109 USA.
EM hudi@umich.edu; xz70@duke.edu; dec.ziye@gmail.com; sferrari@duke.edu;
   mazum@umich.edu
CR Arena P, 2009, IEEE T NEURAL NETWOR, V20, P202, DOI 10.1109/TNN.2008.2005134
   Chicca E, 2003, IEEE T NEURAL NETWOR, V14, P1297, DOI 10.1109/TNN.2003.816367
   Ebong IE, 2012, P IEEE, V100, P2050, DOI 10.1109/JPROC.2011.2173089
   Le Dung, 2007, IEEE International Symposium on Computational Intelligence in Robotics and Automation, 2007, P285
   Shinzato Patrick Y., 2010, 2010 IEEE International Conference on Industrial Technology (ICIT 2010), P1457, DOI 10.1109/ICIT.2010.5472489
   Snider GS, 2008, 2008 IEEE INTERNATIONAL SYMPOSIUM ON NANOSCALE ARCHITECTURES, P85, DOI 10.1109/NANOARCH.2008.4585796
   Vogelstein RJ, 2004, 2004 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL 5, PROCEEDINGS, P385
   Weigel T, 2002, IEEE T ROBOTIC AUTOM, V18, P685, DOI 10.1109/TRA.2002.804041
NR 8
TC 11
Z9 11
U1 0
U2 4
PY 2014
BP 873
EP 876
UT WOS:000365620600201
DA 2023-11-16
ER

PT J
AU Pérez, J
   Cabrera, JA
   Castillo, JJ
   Velasco, JM
AF Perez, Javier
   Cabrera, Juan A.
   Castillo, Juan J.
   Velasco, Juan M.
TI Bio-inspired spiking neural network for nonlinear systems control
SO NEURAL NETWORKS
DT Article
DE Spiking neural network; Supervised learning; Genetic algorithm;
   Nonlinear systems; Control
ID MODEL; PATTERNS
AB Spiking neural networks (SNN) are the third generation of artificial neural networks. SNN are the closest approximation to biological neural networks. SNNs make use of temporal spike trains to command inputs and outputs, allowing a faster and more complex computation. As demonstrated by biological organisms, they are a potentially good approach to designing controllers for highly nonlinear dynamic systems in which the performance of controllers developed by conventional techniques is not satisfactory or difficult to implement. SNN-based controllers exploit their ability for online learning and self-adaptation to evolve when transferred from simulations to the real world. SNN's inherent binary and temporary way of information codification facilitates their hardware implementation compared to analog neurons. Biological neural networks often require a lower number of neurons compared to other controllers based on artificial neural networks. In this work, these neuronal systems are imitated to perform the control of non-linear dynamic systems. For this purpose, a control structure based on spiking neural networks has been designed. Particular attention has been paid to optimizing the structure and size of the neural network. The proposed structure is able to control dynamic systems with a reduced number of neurons and connections. A supervised learning process using evolutionary algorithms has been carried out to perform controller training. The efficiency of the proposed network has been verified in two examples of dynamic systems control. Simulations show that the proposed control based on SNN exhibits superior performance compared to other approaches based on Neural Networks and SNNs. (C) 2018 Elsevier Ltd. All rights reserved.
C1 [Perez, Javier; Cabrera, Juan A.; Castillo, Juan J.; Velasco, Juan M.] Univ Malaga, C Ortiz Ramos S-N, E-29071 Malaga, Spain.
RP Pérez, J (corresponding author), Univ Malaga, C Ortiz Ramos S-N, E-29071 Malaga, Spain.
EM javierperez@uma.es
CR Arena P, 2009, IEEE T NEURAL NETWOR, V20, P202, DOI 10.1109/TNN.2008.2005134
   Balderas David, 2016, HUMAN MOVEMENT CONTR, DOI [10.5772/63720, DOI 10.5772/63720]
   Belatreche A, 2003, P IEEE CYB INT CHALL, P39
   BIENENSTOCK EL, 1982, J NEUROSCI, V2, P32, DOI 10.1523/jneurosci.02-01-00032.1982
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   Bouganis A., 2010, P 2010 INT JOINT C N, P1, DOI DOI 10.1109/IJCNN.2010.5596525
   Braitenberg V., 1984, VEHICLES EXPT SYNTHE
   Cabrera J. A., 2015, IEEE T FUZZY SYSTEMS, V23
   Carrillo RR, 2008, BIOSYSTEMS, V94, P18, DOI 10.1016/j.biosystems.2008.05.008
   Chadderdon GL, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0047251
   Clawson TS, 2016, IEEE DECIS CONTR P, P3381, DOI 10.1109/CDC.2016.7798778
   Froemke RC, 2002, NATURE, V416, P433, DOI 10.1038/416433a
   Gerstner W., 2002, SPIKING NEURON MODEL
   Hill AV, 1938, PROC R SOC SER B-BIO, V126, P136, DOI 10.1098/rspb.1938.0050
   Hulea M, 2014, INT CONF SYST THEO, P163, DOI 10.1109/ICSTCC.2014.6982409
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Jin X, 2010, COMPUT SCI ENG, V12, P91, DOI 10.1109/MCSE.2010.112
   Kaiser J, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON SIMULATION, MODELING, AND PROGRAMMING FOR AUTONOMOUS ROBOTS (SIMPAR), P127, DOI 10.1109/SIMPAR.2016.7862386
   Kandel ER, 2001, SCIENCE, V294, P1030, DOI 10.1126/science.1067020
   Meng M., 2017, COMMUNICATIONS COMPU, V761
   Oniz Y, 2014, J FRANKLIN I, V351, P3269, DOI 10.1016/j.jfranklin.2014.03.002
   Oniz Y, 2013, IEEE IND ELEC, P3422, DOI 10.1109/IECON.2013.6699678
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Ruf B, 1997, NEURAL PROCESS LETT, V5, P9, DOI 10.1023/A:1009697008681
   Spuler M., 2015, P INT JOINT C NEURAL, V2015, DOI [10.1109/IJCNN.2015.7280521, DOI 10.1109/IJCNN.2015.7280521]
   Storn R, 1997, J GLOBAL OPTIM, V11, P341, DOI 10.1023/A:1008202821328
   Valluru S. K., 2016, 2016 IEEE 1 INT C PO, P1
   Webb A., 2011, LECT NOTES COMPUTER, V7064
   WINTERS JM, 1985, IEEE T BIO-MED ENG, V32, P826, DOI 10.1109/TBME.1985.325498
NR 31
TC 14
Z9 14
U1 0
U2 32
PD AUG
PY 2018
VL 104
BP 15
EP 25
DI 10.1016/j.neunet.2018.04.002
UT WOS:000432820300002
DA 2023-11-16
ER

PT C
AU Xing, F
   Yuan, Y
   Huo, H
   Fang, T
AF Xing, Fu
   Yuan, Ye
   Huo, Hong
   Fang, Tao
BE Gedeon, T
   Wong, KW
   Lee, M
TI Homeostasis-Based CNN-to-SNN Conversion of Inception and Residual
   Architectures
SO NEURAL INFORMATION PROCESSING (ICONIP 2019), PT III
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 26th International Conference on Neural Information Processing (ICONIP)
   of the Asia-Pacific-Neural-Network-Society (APNNS)
CY DEC 12-15, 2019
CL Sydney, AUSTRALIA
DE Spiking neural network; Homeostatic plasticity; Brain-inspired
   computing; Object classification
ID PLASTICITY; NETWORK; DRIVEN
AB Event-driven mode of computation provides SNNs with potential to bridge the gap between excellent performance and computational load of deep neural networks. However, SNNs are difficult to train because of the discontinuity of spike signals. This paper proposes an efficient framework for CNN-to-SNN conversion, which converts pretrained convolution neural networks (CNNs) into corresponding spiking equivalents. Different from previous work, this paper focuses on the conversion of deep CNN architectures, such as Inception and ResNet. As networks in conversion are rate-encoding, a novel weight normalization method is employed to approximate the spiking rates of SNNs to the activations of CNNs. And, inspired from homeostatic plasticity in neural system, a compensation approach is introduced to reduce the deterioration of spiking rates at deep layers and accelerate the inference of SNNs. Experimental results on CIFAR dataset show that the SNNs built by the conversion framework achieve better performance than those trained with spike-based algorithms. In particular, the accuracy gap between converted SNNs and original CNNs is further reduced, which is helpful for large-scale employment of spiking networks.
C1 [Xing, Fu; Yuan, Ye; Huo, Hong; Fang, Tao] Shanghai Jiao Tong Univ, Dept Automat, Shanghai, Peoples R China.
RP Fang, T (corresponding author), Shanghai Jiao Tong Univ, Dept Automat, Shanghai, Peoples R China.
EM xingfu@sjtu.edu.cn; tfang@sjtu.edu.cn
CR Pérez-Carrasco JA, 2013, IEEE T PATTERN ANAL, V35, P2706, DOI 10.1109/TPAMI.2013.71
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Diehl P.U., 2015, 2015 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2015.7280696
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Esser SK, 2016, P NATL ACAD SCI USA, V113, P11441, DOI 10.1073/pnas.1604850113
   Fernandes D, 2016, J NEUROCHEM, V139, P973, DOI 10.1111/jnc.13687
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Liu SC, 2015, EVENT-BASED NEUROMORPHIC SYSTEMS, P1, DOI 10.1002/9781118927601
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Miner D, 2016, PLOS COMPUT BIOL, V12, DOI 10.1371/journal.pcbi.1004759
   Neil D, 2014, IEEE T VLSI SYST, V22, P2621, DOI 10.1109/TVLSI.2013.2294916
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Salimans T, 2016, ADV NEUR IN, V29
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Szegedy C., 2017, P 31 AAAI C ARTIFICI, V31, DOI 10.1609/aaai.v31i1.11231
   Szegedy C., 2016, PROC CVPR IEEE, P2818, DOI DOI 10.1109/CVPR.2016.308
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Turrigiano GG, 2004, NAT REV NEUROSCI, V5, P97, DOI 10.1038/nrn1327
   Wang Y., 2018, IEEE T NEUR NET LEAR
NR 22
TC 5
Z9 5
U1 0
U2 3
PY 2019
VL 11955
BP 173
EP 184
DI 10.1007/978-3-030-36718-3_15
UT WOS:000612961500015
DA 2023-11-16
ER

PT J
AU Hu, J
   Tang, HJ
   Tan, KC
   Li, HZ
   Shi, LP
AF Hu, Jun
   Tang, Huajin
   Tan, K. C.
   Li, Haizhou
   Shi, Luping
TI A Spike-Timing-Based Integrated Model for Pattern Recognition
SO NEURAL COMPUTATION
DT Article
ID RETINAL GANGLION-CELLS; ATTRACTOR NETWORKS; NEURAL-NETWORKS;
   INFORMATION; PHASE; COMPUTATION; DYNAMICS; CODES; TRANSMISSION;
   DEPENDENCE
AB During the past few decades, remarkable progress has been made in solving pattern recognition problems using networks of spiking neurons. However, the issue of pattern recognition involving computational process from sensory encoding to synaptic learning remains underexplored, as most existing models or algorithms target only part of the computational process. Furthermore, many learning algorithms proposed in the literature neglect or pay little attention to sensory information encoding, which makes them incompatible with neural-realistic sensory signals encoded from real-world stimuli. By treating sensory coding and learning as a systematic process, we attempt to build an integrated model based on spiking neural networks (SNNs), which performs sensory neural encoding and supervised learning with precisely timed sequences of spikes. With emerging evidence of precise spike-timing neural activities, the view that information is represented by explicit firing times of action potentials rather than mean firing rates has been receiving increasing attention. The external sensory stimulation is first converted into spatiotemporal patterns using a latency-phase encoding method and subsequently transmitted to the consecutive network for learning. Spiking neurons are trained to reproduce target signals encoded with precisely timed spikes. We show that when a supervised spike-timing-based learning is used, different spatiotemporal patterns are recognized by different spike patterns with a high time precision in milliseconds.
C1 [Hu, Jun; Tan, K. C.] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 117576, Singapore.
   [Tang, Huajin; Li, Haizhou] Agcy Sci Technol & Res, Inst Infocomm Res, Singapore 138632, Singapore.
   [Li, Haizhou] Univ New S Wales, Sch Elect Engn & Telecommun, Sydney, NSW 2052, Australia.
   [Shi, Luping] Agcy Sci Technol & Res, Data Storage Inst, Singapore 117608, Singapore.
RP Tang, HJ (corresponding author), Agcy Sci Technol & Res, Inst Infocomm Res, Singapore 138632, Singapore.
EM junhu@nus.edu.sg; htang@i2r.a-star-edu.sg; eletankc@nus.edu.sg;
   hli@i2r.a-star.edu.sg; SHI_Luping@dsi.a-star.edu.sg
CR ABELES M, 1994, PROG BRAIN RES, V102, P395
   Adrian E. D., 1928, BASIS SENSATION
   [Anonymous], 1960, IRE WESCON CONVENTIO, DOI DOI 10.21236/AD0241531
   ARNETT DW, 1978, EXP BRAIN RES, V32, P49
   Barak O, 2006, NEURAL COMPUT, V18, P2343, DOI 10.1162/neco.2006.18.10.2343
   Bi GQ, 2001, ANNU REV NEUROSCI, V24, P139, DOI 10.1146/annurev.neuro.24.1.139
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   BIALEK W, 1991, SCIENCE, V252, P1854, DOI 10.1126/science.2063199
   Blumenfeld B, 2006, NEURON, V52, P383, DOI 10.1016/j.neuron.2006.08.016
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Brader JM, 2007, NEURAL COMPUT, V19, P2881, DOI 10.1162/neco.2007.19.11.2881
   Brody CD, 2003, NEURON, V37, P843, DOI 10.1016/S0896-6273(03)00120-X
   CARR CE, 1993, ANNU REV NEUROSCI, V16, P223, DOI 10.1146/annurev.ne.16.030193.001255
   Chrobak JJ, 1998, J NEUROSCI, V18, P388
   DeVries SH, 1999, J NEUROPHYSIOL, V81, P908, DOI 10.1152/jn.1999.81.2.908
   du Bois-Reymond Emil, 1848, UNTERSUCHUNGEN THIER
   Gawne TJ, 1996, J NEUROPHYSIOL, V76, P1356, DOI 10.1152/jn.1996.76.2.1356
   Gerstner W., 2002, SPIKING NEURON MODEL
   Gollisch T, 2008, SCIENCE, V319, P1108, DOI 10.1126/science.1149639
   Greschner M, 2006, J NEUROPHYSIOL, V96, P2845, DOI 10.1152/jn.01131.2005
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Heiligenberg W., 1991, NEURAL NETS ELECT FI
   Hopfield IJ, 2000, P NATL ACAD SCI USA, V97, P13919, DOI 10.1073/pnas.250483697
   Hopfield JJ, 2001, P NATL ACAD SCI USA, V98, P1282, DOI 10.1073/pnas.031567098
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   Ito M, 2000, BRAIN RES, V886, P237, DOI 10.1016/S0006-8993(00)03142-5
   Ito M, 2008, NAT REV NEUROSCI, V9, P304, DOI 10.1038/nrn2332
   Jensen O, 2001, NEURAL COMPUT, V13, P2743, DOI 10.1162/089976601317098510
   Kayser C, 2009, NEURON, V61, P597, DOI 10.1016/j.neuron.2009.01.008
   Keat J, 2001, NEURON, V30, P803, DOI 10.1016/S0896-6273(01)00322-1
   KNUDSEN EI, 1994, J NEUROSCI, V14, P3985
   Koepsell K, 2009, FRONT SYST NEUROSCI, V3, DOI 10.3389/neuro.06.004.2009
   Litvak V, 2003, J NEUROSCI, V23, P3006
   LLINAS RR, 1991, P NATL ACAD SCI USA, V88, P897, DOI 10.1073/pnas.88.3.897
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   MAINEN ZF, 1995, SCIENCE, V268, P1503, DOI 10.1126/science.7770778
   Manette OFL, 2004, IEEE T NEURAL NETWOR, V15, P1260, DOI 10.1109/TNN.2004.833127
   Meister M, 1999, NEURON, V22, P435, DOI 10.1016/S0896-6273(00)80700-X
   MEISTER M, 1995, SCIENCE, V270, P1207, DOI 10.1126/science.270.5239.1207
   Meng Y, 2011, IEEE COMPUT INTELL M, V6, P43, DOI 10.1109/MCI.2010.939579
   Montgomery J, 2002, BIOL BULL, V203, P238, DOI 10.2307/1543417
   Müller-Putz GR, 2010, FRONT NEUROSCI-SWITZ, V4, DOI 10.3389/fnins.2010.00034
   Nadasdy Z, 2009, FRONT SYST NEUROSCI, V3, DOI 10.3389/neuro.06.006.2009
   O'Keefe J, 2005, HIPPOCAMPUS, V15, P853, DOI 10.1002/hipo.20115
   Panzeri S, 2010, TRENDS NEUROSCI, V33, P111, DOI 10.1016/j.tins.2009.12.001
   Perlovsky L, 2011, IEEE COMPUT INTELL M, V6, P20, DOI 10.1109/MCI.2010.939581
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Reich DS, 2001, J NEUROPHYSIOL, V85, P1039, DOI 10.1152/jn.2001.85.3.1039
   Riesenhuber M, 1999, NAT NEUROSCI, V2, P1019, DOI 10.1038/14819
   Russell BC, 2008, INT J COMPUT VISION, V77, P157, DOI 10.1007/s11263-007-0090-8
   Schreiber S, 2003, NEUROCOMPUTING, V52-4, P925, DOI 10.1016/S0925-2312(02)00838-X
   Seung HS, 2003, NEURON, V40, P1063, DOI 10.1016/S0896-6273(03)00761-X
   Shadlen Michael N., 1994, Current Opinion in Neurobiology, V4, P569, DOI 10.1016/0959-4388(94)90059-0
   SINGER W, 1995, ANNU REV NEUROSCI, V18, P555, DOI 10.1146/annurev.ne.18.030195.003011
   Softky W R, 1995, Curr Opin Neurobiol, V5, P239, DOI 10.1016/0959-4388(95)80032-8
   Takase H, 2009, IEEE IJCNN, P1225
   Tang HJ, 2010, NEURAL COMPUT, V22, P1899, DOI 10.1162/neco.2010.07-09-1050
   Tsodyks MV, 1996, HIPPOCAMPUS, V6, P271, DOI 10.1002/(SICI)1098-1063(1996)6:3<271::AID-HIPO5>3.3.CO;2-Q
   Van Rullen R, 2001, NEURAL COMPUT, V13, P1255, DOI 10.1162/08997660152002852
   van Wyk M, 2006, J NEUROSCI, V26, P13250, DOI 10.1523/JNEUROSCI.1991-06.2006
   Victor JD, 2000, BRAIN RES, V886, P33, DOI 10.1016/S0006-8993(00)02751-7
   Yan R, 2012, IEEE COMPUT INTELL M, V7, P64, DOI 10.1109/MCI.2011.2176767
NR 62
TC 64
Z9 68
U1 2
U2 39
PD FEB
PY 2013
VL 25
IS 2
BP 450
EP 472
DI 10.1162/NECO_a_00395
UT WOS:000313403600005
DA 2023-11-16
ER

PT C
AU Alexandru, R
   Malhotra, P
   Reynolds, S
   Dragotti, PL
AF Alexandru, Roxana
   Malhotra, Pranav
   Reynolds, Stephanie
   Dragotti, Pier Luigi
GP IEEE
TI Estimating the Topology of Neural Networks from Distributed Observations
SO 2018 26TH EUROPEAN SIGNAL PROCESSING CONFERENCE (EUSIPCO)
SE European Signal Processing Conference
DT Proceedings Paper
CT European Signal Processing Conference (EUSIPCO)
CY SEP 03-07, 2018
CL Rome, ITALY
DE Neural networks; network topology inference; stability analysis of spike
   propagation; Izhikevich neuron model; Brian simulator; NetRate algorithm
ID DYNAMICS
AB We address the problem of estimating the effective connectivity of the brain network, using the input stimulus model proposed by Izhikevich in [1], which accurately reproduces the behaviour of spiking and bursting biological neurons, whilst ensuring computational simplicity. We first analyse the temporal dynamics of neural networks, showing that the spike propagation within the brain can be modelled as a diffusion process. This helps prove the suitability of NetRate algorithm proposed by Rodriguez in [2] to infer the structure of biological neural networks. Finally, we present simulation results using synthetic data to verify the performance of the topology estimation algorithm.
C1 [Alexandru, Roxana; Malhotra, Pranav; Reynolds, Stephanie; Dragotti, Pier Luigi] Imperial Coll London, Elect & Elect Engn Dept, London, England.
RP Alexandru, R (corresponding author), Imperial Coll London, Elect & Elect Engn Dept, London, England.
EM roxana.alexandru12@imperial.ac.uk; pranav.malhotra13@imperial.ac.uk;
   stephanie.reynolds09@imperial.ac.uk; p.dragotti@imperial.ac.uk
CR [Anonymous], CVX MATLAB SOFTWARE
   Bressler SL, 2011, NEUROIMAGE, V58, P323, DOI 10.1016/j.neuroimage.2010.02.059
   Brovelli A, 2004, P NATL ACAD SCI USA, V101, P9849, DOI 10.1073/pnas.0308538101
   Brown EN, 2004, NAT NEUROSCI, V7, P456, DOI 10.1038/nn1228
   Bullmore ET, 2009, NAT REV NEUROSCI, V10, P186, DOI 10.1038/nrn2575
   Fries P, 2005, TRENDS COGN SCI, V9, P474, DOI 10.1016/j.tics.2005.08.011
   Friston KJ, 2003, NEUROIMAGE, V19, P1273, DOI 10.1016/S1053-8119(03)00202-7
   Gerhard F, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003138
   Goodman DFM, 2009, FRONT NEUROSCI-SWITZ, V3, P192, DOI 10.3389/neuro.01.026.2009
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Karbasi A, 2016, INT CONF ACOUST SPEE, P699, DOI 10.1109/ICASSP.2016.7471765
   Liu Y, 2008, BRAIN, V131, P945, DOI 10.1093/brain/awn018
   Packer AM, 2015, NAT METHODS, V12, P140, DOI 10.1038/nmeth.3217
   Peron SP, 2015, NEURON, V86, P783, DOI 10.1016/j.neuron.2015.03.027
   Rodriguez M.G., 2011, P 28 INT C MACH LEAR
   Schneidman E, 2003, PHYS REV LETT, V91, DOI 10.1103/PhysRevLett.91.238701
   Supekar K, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.1000100
   Vardi Y, 1996, J AM STAT ASSOC, V91, P365, DOI 10.2307/2291416
   Watts DJ, 1998, NATURE, V393, P440, DOI 10.1038/30918
NR 19
TC 0
Z9 0
U1 1
U2 2
PY 2018
BP 420
EP 424
UT WOS:000455614900085
DA 2023-11-16
ER

PT J
AU Rathi, N
   Roy, K
AF Rathi, Nitin
   Roy, Kaushik
TI STDP Based Unsupervised Multimodal Learning With Cross-Modal Processing
   in Spiking Neural Networks
SO IEEE TRANSACTIONS ON EMERGING TOPICS IN COMPUTATIONAL INTELLIGENCE
DT Article
DE Synapses; Biological neural networks; Timing; Hidden Markov models;
   Unsupervised learning; Multimodal Learning; Spiking Neural Network;
   Spike Timing Dependent Plasticity; Unsupervised Learning; Cross-modal
   Connections; Synergistic Learning
AB Spiking neural networks perform reasonably well in recognition applications for single modality (e.g., images, audio, or text). In this paper, we propose a multimodal spiking neural network that combines two modalities (image and audio). The two unimodal ensembles are connected with cross-modal connections and the entire network is trained with unsupervised learning. The network receives inputs in both modalities for the same class and predicts the class label. The excitatory connections in the unimodal ensemble and the cross-modal connections are trained with power-law weight-dependent spike timing dependent plasticity learning rule. The cross-modal connections capture the correlation between neurons of different modalities. The multimodal network learns features of both modalities and improves the classification accuracy compared to unimodal topology, even when one of the modality is distorted by noise. The cross-modal connections suppress the effect of noise on classification accuracy. The well-learned cross-modal connections invoke additional spiking activity in neurons of the correct label. The cross-modal connections are only excitatory and do not inhibit the normal activity of the unimodal ensembles. We evaluated our multimodal network on images from MNIST dataset and utterances of digits from TI46 speech corpus. The multimodal network achieved a classification accuracy of 98% on the combined MNIST and TI46 dataset.
C1 [Rathi, Nitin; Roy, Kaushik] Purdue Univ, Sch Elect & Comp Engn, W Lafayette, IN 47907 USA.
RP Rathi, N (corresponding author), Purdue Univ, Sch Elect & Comp Engn, W Lafayette, IN 47907 USA.
EM rathi2@purdue.edu; kaushik@purdue.edu
CR [Anonymous], 2012, P INT C MACH LEARN W
   [Anonymous], 2018, IEEE J EM SEL TOP C, DOI DOI 10.1109/JETCAS.2017.2769684
   Asano M, 2015, CORTEX, V63, P196, DOI 10.1016/j.cortex.2014.08.025
   Basu S, 2017, NEURAL PROCESS LETT, V45, P855, DOI 10.1007/s11063-016-9556-4
   Bohte S. M., 2000, 8th European Symposium on Artificial Neural Networks. ESANN"2000. Proceedings, P419
   Calvert GA, 2001, CEREB CORTEX, V11, P1110, DOI 10.1093/cercor/11.12.1110
   Diehl PU, 2015, IEEE IJCNN
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Eitel A, 2015, IEEE INT C INT ROBOT, P681, DOI 10.1109/IROS.2015.7353446
   Goodman DFM, 2009, FRONT NEUROSCI-SWITZ, V3, P192, DOI 10.3389/neuro.01.026.2009
   Haller M, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P1817, DOI 10.1109/ICME.2006.262906
   Hong CQ, 2015, IEEE T IMAGE PROCESS, V24, P5659, DOI 10.1109/TIP.2015.2487860
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Kulkarni SR, 2017, IEEE I C ELECT CIRC, P128, DOI 10.1109/ICECS.2017.8292015
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Liberman M, 1993, WEB DOWNLOAD
   Liu DQ, 2017, NEUROCOMPUTING, V249, P212, DOI 10.1016/j.neucom.2017.04.003
   Lyon R. F., 1982, Proceedings of ICASSP 82. IEEE International Conference on Acoustics, Speech and Signal Processing, P1282
   MCGURK H, 1976, NATURE, V264, P746, DOI 10.1038/264746a0
   Panda P, 2017, IEEE IJCNN, P2629, DOI 10.1109/IJCNN.2017.7966177
   Park C., 2006, IEEE INT C COMP VIS, P54
   Sanderson C, 2004, DIGIT SIGNAL PROCESS, V14, P449, DOI 10.1016/j.dsp.2004.05.001
   Slaney M, 1998, 1998010 NT RES CORP 1998010 NT RES CORP
   Srivastava N., 2012, P INT C NEUR INF PRO, P2222
   Stein B E, 1989, J Cogn Neurosci, V1, P12, DOI 10.1162/jocn.1989.1.1.12
   von Kriegstein K, 2006, PLOS BIOL, V4, P1809, DOI 10.1371/journal.pbio.0040326
   Wade JJ, 2010, IEEE T NEURAL NETWOR, V21, P1817, DOI 10.1109/TNN.2010.2074212
   Wang AR, 2015, IEEE T MULTIMEDIA, V17, P1887, DOI 10.1109/TMM.2015.2476655
   Zhang Y, 2015, IEEE T NEUR NET LEAR, V26, P2635, DOI 10.1109/TNNLS.2015.2388544
NR 29
TC 7
Z9 7
U1 6
U2 62
PD FEB
PY 2021
VL 5
IS 1
BP 143
EP 153
DI 10.1109/TETCI.2018.2872014
UT WOS:000677872800013
DA 2023-11-16
ER

PT C
AU Yoshioka, M
   Scarpetta, S
   Marinaro, M
AF Yoshioka, Masahiko
   Scarpetta, Silvia
   Marinaro, Maria
BE MarquesDeSa, J
   Alexandre, LA
   Duch, W
   Mandic, DP
TI Spike-timing-dependent synaptic plasticity to learn spatiotemporal
   patterns in recurrent neural networks
SO ARTIFICIAL NEURAL NETWORKS - ICANN 2007, PT 1, PROCEEDINGS
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 17th International Conference on Artificial Neural Networks (ICANN 2007)
CY SEP 09-13, 2007
CL Oporto, PORTUGAL
ID ASSOCIATIVE MEMORY; SEQUENCES; MODEL; RETRIEVAL; NEURONS; REPLAY; STATE
AB Assuming asymmetric time window of the spike-timing-dependent synaptic plasticity (STDP), we study spatiotemporal learning in recurrent neural networks. We first show numerical simulations of spiking neural networks in which spatiotemporal Poisson patterns (i.e., random spatiotemporal patterns generated by independent Poisson process) are successfully memorized by the STDP-based learning rule. Then, we discuss the underlying mechanism of the STDP-based learning, mentioning our recent analysis on associative memory analog neural networks for periodic spatiotemporal patterns. Order parameter dynamics in the analog neural networks explains time scale change in retrieval process and the shape of the STDP time window optimal to encode a large number of spatiotemporal patterns. The analysis further elucidates phase transition due to destabilization of retrieval state. These findings on analog neural networks are found to be consistent with the previous results on spiking neural networks. These STDP-based spatiotemporal associative memory possibly gives some insights into the recent experimental results in which spatiotemporal patterns are found to be retrieved at the various time scale.
C1 [Yoshioka, Masahiko; Scarpetta, Silvia; Marinaro, Maria] Univ Salerno, Dept Phys, I-84081 Baronissi, SA, Italy.
   [Scarpetta, Silvia; Marinaro, Maria] INFN, I-84100 Salerno, Italy.
   [Scarpetta, Silvia; Marinaro, Maria] IIASS, I-84019 Vietri Sul Mare, Italy.
RP Yoshioka, M (corresponding author), Univ Salerno, Dept Phys, I-84081 Baronissi, SA, Italy.
CR AMARI S, 1988, NEURAL NETWORKS, V1, P63, DOI 10.1016/0893-6080(88)90022-6
   AMARI SI, 1972, IEEE T COMPUT, VC 21, P1197, DOI 10.1109/T-C.1972.223477
   AMIT DJ, 1985, PHYS REV LETT, V55, P1530, DOI 10.1103/PhysRevLett.55.1530
   ARENAS A, 1994, EUROPHYS LETT, V26, P79, DOI 10.1209/0295-5075/26/2/001
   Bi GQ, 2001, ANNU REV NEUROSCI, V24, P139, DOI 10.1146/annurev.neuro.24.1.139
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   COOK J, 1989, J PHYS A-MATH GEN, V22, P2057, DOI 10.1088/0305-4470/22/12/011
   COOLEN ACC, 1988, PHYS REV A, V38, P4253, DOI 10.1103/PhysRevA.38.4253
   Foster DJ, 2006, NATURE, V440, P680, DOI 10.1038/nature04587
   HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554
   KUHN R, 1991, PHYS REV A, V43, P2084, DOI 10.1103/PhysRevA.43.2084
   LORENZO PMD, 2003, BEHAV NEUROSCI, V117, P1423
   MacLeod K, 1996, SCIENCE, V274, P976, DOI 10.1126/science.274.5289.976
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   Matsumoto N, 2002, NEURAL COMPUT, V14, P2883, DOI 10.1162/089976602760805322
   Nádasdy Z, 1999, J NEUROSCI, V19, P9497
   NISHIMORI H, 1990, PHYS REV A, V41, P3346, DOI 10.1103/PhysRevA.41.3346
   OKADA M, 1995, NEURAL NETWORKS, V8, P833, DOI 10.1016/0893-6080(95)00001-G
   Scarpetta S, 2005, HIPPOCAMPUS, V15, P979, DOI 10.1002/hipo.20124
   Scarpetta S, 2002, NEURAL COMPUT, V14, P2371, DOI 10.1162/08997660260293265
   SHIINO M, 1992, J PHYS A-MATH GEN, V25, pL375, DOI 10.1088/0305-4470/25/7/017
   SOMPOLINSKY H, 1986, PHYS REV LETT, V57, P2861, DOI 10.1103/PhysRevLett.57.2861
   Yoshioka M, 2002, PHYS REV E, V66, DOI 10.1103/PhysRevE.66.061913
   Yoshioka M, 2002, PHYS REV E, V65, DOI 10.1103/PhysRevE.65.011903
   Yoshioka M, 2007, PHYS REV E, V75, DOI 10.1103/PhysRevE.75.051917
   Zhang LI, 1998, NATURE, V395, P37, DOI 10.1038/25665
NR 26
TC 2
Z9 2
U1 0
U2 2
PY 2007
VL 4668
BP 757
EP +
PN I
UT WOS:000250338200077
DA 2023-11-16
ER

PT C
AU Paudel, BR
   Itani, A
   Tragoudas, S
AF Paudel, Bijay Raj
   Itani, Aashish
   Tragoudas, Spyros
BE Wani, MA
   Sethi, I
   Shi, W
   Qu, G
   Raicu, DS
   Jin, R
TI Resiliency of SNN on Black-Box Adversarial Attacks
SO 20TH IEEE INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS
   (ICMLA 2021)
DT Proceedings Paper
CT 20th IEEE International Conference on Machine Learning and Applications
   (ICMLA)
CY DEC 13-16, 2021
CL ELECTR NETWORK
DE Spiking Neural Network; Deep Neural Network; Adversarial Attacks;
   Black-box Attacks; SpiNNaker
ID SPIKING; NETWORKS; MODEL
AB Existing works indicate that Spiking Neural Networks (SNNs) are resilient to adversarial attacks by testing against few attack models. This paper studies adversarial attacks on SNNs using additional attack models and shows that SNNs are not inherently robust against many few-pixel L-0 black-box attacks. Additionally, a method to defend against such attacks in SNNs is presented. The SNNs and the effects of adversarial attacks are tested on both software simulators as well as on SpiNNaker neuromorphic hardware.
C1 [Paudel, Bijay Raj; Itani, Aashish; Tragoudas, Spyros] Southern Illinois Univ, Sch Elect Comp & Biomed Engn, Carbondale, IL 62901 USA.
RP Paudel, BR (corresponding author), Southern Illinois Univ, Sch Elect Comp & Biomed Engn, Carbondale, IL 62901 USA.
EM bijayraj.paudel@siu.edu; aashish.itani@siu.edu; spyros@siu.edu
CR [Anonymous], QUANTITATIVE DESCRIP, V117
   [Anonymous], NEURAL CODING SPIKIN, V15
   Bagheri A, 2018, IEEE INT WORK SIGN P, P261
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Brown Tom B, 2017, ARXIV171209665
   Burkitt AN, 2006, BIOL CYBERN, V95, P1, DOI 10.1007/s00422-006-0068-6
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49
   Diehl PU, 2015, IEEE IJCNN
   Faghihi F., 2019, ONE SHOT LEARNING NE
   Furber SB, 2013, IEEE T COMPUT, V62, P2454, DOI 10.1109/TC.2012.142
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   Gonzalez R. C., 2008, DIGITAL IMAGE PROCES
   Goodfellow Ian J., 2015, EXPLAINING HARNESSIN
   Guo Chuan, 2017, ARXIV171100117
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hebb D.O., 1949, WILEY BOOK CLIN PSYC, V62, P78
   Heeger D, 1997, J NEUROSCI, P1
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Jeyasothy A., 2019, ABS190411367
   Jha S., 2016, ABS160202697
   Khalid F., 2018, ABS181101444
   Kingma D. P., 2015, INT C LEARNING REPRE
   Kurakin A., 2016, INT C LEARN REPR
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y., 1998, MNIST DATABASE HANDW
   Liang L., 2020, ARXIV200101587CSEESS
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Madry Aleksander, 2017, ARXIV170606083
   Marchisio A, 2020, IEEE IJCNN, DOI 10.1109/ijcnn48605.2020.9207297
   Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282
   Mozafari M, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00625
   Mozafari M, 2019, PATTERN RECOGN, V94, P87, DOI 10.1016/j.patcog.2019.05.015
   Nicolae Maria-Irina, 2018, ADVERSARIAL ROBUSTNE
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   Panda P, 2019, IEEE ACCESS, V7, P70157, DOI 10.1109/ACCESS.2019.2919463
   Papernot N., 2016, ABS160507277
   Papernot N, 2016, 1ST IEEE EUROPEAN SYMPOSIUM ON SECURITY AND PRIVACY, P372, DOI 10.1109/EuroSP.2016.36
   Paszke Adam, 2017, AUTOMATIC DIFFERENTI
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Pitas I., 1990, MEDIAN FILTERS, P63
   Ponulak F, 2011, ACTA NEUROBIOL EXP, V71, P409
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Sharmin Saima, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12374), P399, DOI 10.1007/978-3-030-58526-6_24
   Sharmin S., 2019, IEEE IJCNN, P1, DOI DOI 10.1109/ijcnn.2019.8851732
   Stimberg M, 2019, ELIFE, V8, DOI 10.7554/eLife.47314
   Su JW, 2019, IEEE T EVOLUT COMPUT, V23, P828, DOI 10.1109/TEVC.2019.2890858
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Tan Y. X. M., 2020, ARXIV191203609CSSTAT
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Vargas D. V., 2019, CORR
   Xiao H., 2017, ARXIV170807747
NR 52
TC 2
Z9 2
U1 0
U2 0
PY 2021
BP 799
EP 806
DI 10.1109/ICMLA52953.2021.00132
UT WOS:000779208200124
DA 2023-11-16
ER

PT J
AU Gerlinghoff, D
   Luo, T
   Goh, RSM
   Wong, WF
AF Gerlinghoff, Daniel
   Luo, Tao
   Goh, Rick Siow Mong
   Wong, Weng-Fai
TI Desire backpropagation: A lightweight training algorithm for multi-layer
   spiking neural networks based on spike-timing-dependent plasticity
SO NEUROCOMPUTING
DT Article
DE Spiking neural network; Spike-timing-dependent plasticity; Supervised
   learning
ID ARCHITECTURE; RESUME
AB Spiking neural networks (SNNs) are a viable alternative to conventional artificial neural networks when resource efficiency and computational complexity are of importance. A major advantage of SNNs is their binary information transfer through spike trains which eliminates multiplication operations. The training of SNNs has, however, been a challenge, since neuron models are non-differentiable and traditional gradient -based backpropagation algorithms cannot be applied directly. Furthermore, spike-timing-dependent plasticity (STDP), albeit being a spike-based learning rule, updates weights locally and does not optimize for the output error of the network. We present desire backpropagation, a method to derive the desired spike activity of all neurons, including the hidden ones, from the output error. By incorporating this desire value into the local STDP weight update, we can efficiently capture the neuron dynamics while minimizing the global error and attaining a high classification accuracy. That makes desire backpropagation a spike-based supervised learning rule. We trained three-layer networks to classify MNIST and Fashion-MNIST images and reached an accuracy of 98.41% and 87.56%, respectively. In addition, by eliminating a multiplication during the backward pass, we reduce computational complexity and balance arithmetic resources between forward and backward pass, making desire backpropagation a candidate for training on low-resource devices.
C1 [Gerlinghoff, Daniel; Luo, Tao; Goh, Rick Siow Mong] ASTAR, Inst High Performance Comp IHPC, 1 Fusionopolis Way,16-16 Connexis, Singapore 138632, Singapore.
   [Wong, Weng-Fai] Natl Univ Singapore, Dept Comp Sci, Comp 1, 13 Comp Dr, Singapore 117417, Singapore.
RP Luo, T (corresponding author), ASTAR, Inst High Performance Comp IHPC, 1 Fusionopolis Way,16-16 Connexis, Singapore 138632, Singapore.
EM tluo001@e.ntu.edu.sg
CR Aung M.T.L., 2023, IEEE T COMPUT, P1
   Aung MTL, 2021, I C FIELD PROG LOGIC, P28, DOI 10.1109/FPL53798.2021.00013
   Bellec G, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-17236-y
   Blouw P, 2020, PROCEEDINGS OF THE 2019 7TH ANNUAL NEURO-INSPIRED COMPUTATIONAL ELEMENTS WORKSHOP (NICE 2019), DOI 10.1145/3320288.3320304
   Bohte S. M., 2000, 8th European Symposium on Artificial Neural Networks. ESANN"2000. Proceedings, P419
   Borst A, 1999, NAT NEUROSCI, V2, P947, DOI 10.1038/14731
   Caporale N, 2008, ANNU REV NEUROSCI, V31, P25, DOI 10.1146/annurev.neuro.31.060407.125639
   Cassidy AS, 2013, IEEE IJCNN
   Thiele JC, 2019, Arxiv, DOI arXiv:1906.00851
   Comsa JM, 2020, INT CONF ACOUST SPEE, P8529, DOI [10.1109/icassp40776.2020.9053856, 10.1109/ICASSP40776.2020.9053856]
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Falez P., 2019, 2019 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2019.8852346
   Fang HW, 2020, ICCAD-IEEE ACM INT, DOI 10.1145/3400302.3415608
   Farabet C, 2012, FRONT NEUROSCI-SWITZ, V6, DOI 10.3389/fnins.2012.00032
   Feldman DE, 2012, NEURON, V75, P556, DOI 10.1016/j.neuron.2012.08.001
   Fu Q, 2021, NEUROCOMPUTING, V419, P47, DOI 10.1016/j.neucom.2020.07.109
   Gardner B, 2015, NEURAL COMPUT, V27, P2548, DOI 10.1162/NECO_a_00790
   Gerlinghoff D, 2022, DES AUT TEST EUROPE, P92, DOI 10.23919/DATE54114.2022.9774596
   Gerlinghoff D, 2022, IEEE T PARALL DISTR, V33, P3207, DOI 10.1109/TPDS.2021.3128945
   GERSTNER W, 1993, BIOL CYBERN, V69, P503, DOI 10.1007/BF01185422
   Gerstner W, 1996, NATURE, V383, P76, DOI 10.1038/383076a0
   Gütig R, 2014, CURR OPIN NEUROBIOL, V25, P134, DOI 10.1016/j.conb.2014.01.004
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Hao YZ, 2020, NEURAL NETWORKS, V121, P387, DOI 10.1016/j.neunet.2019.09.007
   HEBB D. O., 1949
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Kaiser J, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00424
   Kempter R, 1999, ADV NEUR IN, V11, P125
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Koch C., 1998, METHODS NEURONAL MOD
   LeCun Y., 1998, MNIST DATABASE HANDW
   Lee C, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00119
   Lee H, 2021, NEUROCOMPUTING, V455, P125, DOI 10.1016/j.neucom.2021.05.020
   Lin J, 2020, NEUROCOMPUTING, V375, P102, DOI 10.1016/j.neucom.2019.09.082
   Lin XH, 2016, LECT NOTES ARTIF INT, V9773, P44, DOI 10.1007/978-3-319-42297-8_5
   Lu H, 2021, NEUROCOMPUTING, V458, P308, DOI 10.1016/j.neucom.2021.06.027
   Luo T., 2021, IEEE T COMPUT AIDED
   Luo XL, 2022, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2022.3164930
   Luo YL, 2017, LECT NOTES COMPUT SC, V10305, P569, DOI 10.1007/978-3-319-59153-7_49
   Masquelier T, 2010, IEEE IJCNN, DOI 10.1109/IJCNN.2010.5596934
   Matsuda S, 2016, IEEE IJCNN, P293, DOI 10.1109/IJCNN.2016.7727211
   Mirsadeghi M, 2021, NEUROCOMPUTING, V427, P131, DOI 10.1016/j.neucom.2020.11.052
   Mohemmed A, 2013, NEUROCOMPUTING, V107, P3, DOI 10.1016/j.neucom.2012.08.034
   Mohemmed A, 2012, INT J NEURAL SYST, V22, DOI 10.1142/S0129065712500128
   Neftci EO, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00324
   Neil D, 2016, P 31 ANN ACM S APPL
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Qiao GC, 2021, NEUROCOMPUTING, V457, P203, DOI 10.1016/j.neucom.2021.06.070
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Sboev A, 2020, MATH METHOD APPL SCI, V43, P7802, DOI 10.1002/mma.6241
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   SHATZ CJ, 1992, SCI AM, V267, P61, DOI 10.1038/scientificamerican0992-60
   Shrestha A., 2019, P INT C NEUR SYST, P1
   Shrestha A, 2017, IEEE IJCNN, P1999, DOI 10.1109/IJCNN.2017.7966096
   Shrestha SB., 2018, ADV NEURAL INFORM PR, V31, P1412
   Sporea I, 2013, NEURAL COMPUT, V25, P473, DOI 10.1162/NECO_a_00396
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Stromatias E, 2015, IEEE IJCNN
   Taherkhani A, 2015, IEEE T NEUR NET LEAR, V26, P3137, DOI 10.1109/TNNLS.2015.2404938
   Tang GZ, 2021, Arxiv, DOI arXiv:2110.14092
   Tang H, 2020, NEUROCOMPUTING, V407, P300, DOI 10.1016/j.neucom.2020.05.031
   Tavanaei A, 2019, NEUROCOMPUTING, V330, P39, DOI 10.1016/j.neucom.2018.11.014
   Thiele JC, 2018, IEEE IJCNN
   Wang JL, 2014, NEUROCOMPUTING, V144, P526, DOI 10.1016/j.neucom.2014.04.017
   Wang Z., 2022, IEEE T NEUR NET LEAR
   Wu J., 2019, 2019 INT JOINT C NEU
   Xiao H., 2017, ARXIV170807747
   Xu Y, 2013, NEURAL COMPUT, V25, P1472, DOI 10.1162/NECO_a_00450
   Yang LW, 2022, NEUROCOMPUTING, V474, P128, DOI 10.1016/j.neucom.2021.12.021
   Zhang GH, 2020, NEUROCOMPUTING, V382, P106, DOI 10.1016/j.neucom.2019.11.045
   Zhang TL, 2018, AAAI CONF ARTIF INTE, P620
   Zhang W, 2003, NAT REV NEUROSCI, V4, P885, DOI 10.1038/nrn1248
   Zhao DC, 2020, FRONT COMPUT NEUROSC, V14, DOI 10.3389/fncom.2020.576841
   Zhao W., 2021, FRONT NEUROSCI-SWITZ, V15
NR 76
TC 0
Z9 0
U1 1
U2 1
PD DEC 1
PY 2023
VL 560
AR 126773
DI 10.1016/j.neucom.2023.126773
EA OCT 2023
UT WOS:001088380200001
DA 2023-11-16
ER

PT J
AU Fang, HJ
   Wang, YJ
   He, JP
AF Fang, Huijuan
   Wang, Yongji
   He, Jiping
TI Spiking Neural Networks for Cortical Neuronal Spike Train Decoding
SO NEURAL COMPUTATION
DT Article
ID VISUAL-CORTEX; GENERATION; INTERFACES; POSITION; MODEL; ARM
AB Recent investigation of cortical coding and computation indicates that temporal coding is probably a more biologically plausible scheme used by neurons than the rate coding used commonly in most published work. We propose and demonstrate in this letter that spiking neural networks (SNN), consisting of spiking neurons that propagate information by the timing of spikes, are a better alternative to the coding scheme based on spike frequency (histogram) alone. The SNN model analyzes cortical neural spike trains directly without losing temporal information for generating more reliable motor command for cortically controlled prosthetics. In this letter, we compared the temporal pattern classification result from the SNN approach with results generated from firing-rate-based approaches: conventional artificial neural networks, support vector machines, and linear regression. The results show that the SNN algorithm can achieve higher classification accuracy and identify the spiking activity related to movement control earlier than the other methods. Both are desirable characteristics for fast neural information processing and reliable control command pattern recognition for neuroprosthetic applications.
C1 [Fang, Huijuan; Wang, Yongji] Huazhong Univ Sci & Technol, Key Lab Image Proc & Intelligent Control, Educ Minist China, Dept Control Sci & Engn, Wuhan 430074, Peoples R China.
   [Fang, Huijuan] Huaqiao Univ, Coll Informat Sci & Engn, Xiamen 361021, Peoples R China.
   [He, Jiping] Arizona State Univ, Harrington Dept Bioengn, Tempe, AZ 85287 USA.
   [He, Jiping] Arizona State Univ, Ctr Neural Interface Design, Tempe, AZ 85287 USA.
RP Fang, HJ (corresponding author), Huazhong Univ Sci & Technol, Key Lab Image Proc & Intelligent Control, Educ Minist China, Dept Control Sci & Engn, Wuhan 430074, Peoples R China.
EM huijuan.fang@gmail.com; wangyjch@mail.hust.edu.cn; jiping.he@asu.edu
CR [Anonymous], MODELS NEURAL NETWOR
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Booij O, 2005, INFORM PROCESS LETT, V95, P552, DOI 10.1016/j.ipl.2005.05.023
   Buonomano DV, 1999, NEURAL COMPUT, V11, P103, DOI 10.1162/089976699300016836
   Donoghue JP, 2002, NAT NEUROSCI, V5, P1085, DOI 10.1038/nn947
   ECKHORN R, 1988, BIOL CYBERN, V60, P121, DOI 10.1007/BF00202899
   FAN J, 2006, THESIS ARIZONA STATE
   FAN J, 2006, P IEEE ENG MED BIOL, P5472
   FANG H, 2007, 2 INT C BIOINSP COMP
   FANG H, 2006, P WORLD C INT CONTR, P9940
   Gautrais J, 1998, BIOSYSTEMS, V48, P57, DOI 10.1016/S0303-2647(98)00050-1
   GEORGOPOULOS AP, 1986, SCIENCE, V233, P1416, DOI 10.1126/science.3749885
   Gerstner W., 2002, SPIKING NEURON MODEL
   Hochberg LR, 2006, NATURE, V442, P164, DOI 10.1038/nature04970
   Hung CP, 2005, SCIENCE, V310, P863, DOI 10.1126/science.1117593
   JACOBS RA, 1988, NEURAL NETWORKS, V1, P295, DOI 10.1016/0893-6080(88)90003-2
   Joshi P, 2005, NEURAL COMPUT, V17, P1715, DOI 10.1162/0899766054026684
   Kempter R, 1996, ADV NEUR IN, V8, P124
   Lestienne R, 1996, BIOL CYBERN, V74, P55, DOI 10.1007/BF00199137
   Lin SM, 1997, NEURAL COMPUT, V9, P607, DOI 10.1162/neco.1997.9.3.607
   Maass W, 1997, NEURAL COMPUT, V9, P279, DOI 10.1162/neco.1997.9.2.279
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maass W, 1997, ADV NEUR IN, V9, P211
   MOORE S, 2002, THESIS U BATH
   Natschlager T, 1998, NETWORK-COMP NEURAL, V9, P319, DOI 10.1088/0954-898X/9/3/003
   Nicolelis MAL, 2003, NAT REV NEUROSCI, V4, P417, DOI 10.1038/nrn1105
   Paninski L, 2004, J NEUROPHYSIOL, V91, P515, DOI 10.1152/jn.00587.2002
   PERRETT DI, 1982, EXP BRAIN RES, V47, P329
   Schrauwen B, 2004, IEEE IJCNN, P471, DOI 10.1109/IJCNN.2004.1379954
   Schwartz AB, 2001, CURR OPIN NEUROBIOL, V11, P701, DOI 10.1016/S0959-4388(01)00272-0
   Taylor DM, 2002, SCIENCE, V296, P1829, DOI 10.1126/science.1070291
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   THORPE SJ, 1989, CONNECTIONISM IN PERSPECTIVE, P63
   Velliste M, 2008, NATURE, V453, P1098, DOI 10.1038/nature06996
   Wahnoun R, 2006, J NEURAL ENG, V3, P162, DOI 10.1088/1741-2560/3/2/010
   Wang XQ, 2008, NEUROCOMPUTING, V71, P655, DOI 10.1016/j.neucom.2007.08.025
   Wessberg J, 2000, NATURE, V408, P361, DOI 10.1038/35042582
   Wolpaw JR, 2002, CLIN NEUROPHYSIOL, V113, P767, DOI 10.1016/S1388-2457(02)00057-3
   Wu W, 2006, NEURAL COMPUT, V18, P80, DOI 10.1162/089976606774841585
   [No title captured]
NR 40
TC 14
Z9 14
U1 4
U2 14
PD APR
PY 2010
VL 22
IS 4
BP 1060
EP 1085
DI 10.1162/neco.2009.10-08-885
UT WOS:000275367000008
DA 2023-11-16
ER

PT J
AU Xing, YN
   Di Caterina, G
   Soraghan, J
AF Xing, Yannan
   Di Caterina, Gaetano
   Soraghan, John
TI A New Spiking Convolutional Recurrent Neural Network (SCRNN) With
   Applications to Event-Based Hand Gesture Recognition
SO FRONTIERS IN NEUROSCIENCE
DT Article
DE spiking neural network; DVS; gesture recognition; event-based
   processing; video processing
AB The combination of neuromorphic visual sensors and spiking neural network offers a high efficient bio-inspired solution to real-world applications. However, processing event- based sequences remains challenging because of the nature of their asynchronism and sparsity behavior. In this paper, a novel spiking convolutional recurrent neural network (SCRNN) architecture that takes advantage of both convolution operation and recurrent connectivity to maintain the spatial and temporal relations from event-based sequence data are presented. The use of recurrent architecture enables the network to have a sampling window with an arbitrary length, allowing the network to exploit temporal correlations between event collections. Rather than standard ANN to SNN conversion techniques, the network utilizes a supervised Spike Layer Error Reassignment (SLAYER) training mechanism that allows the network to adapt to neuromorphic (event-based) data directly. The network structure is validated on the DVS gesture dataset and achieves a 10 class gesture recognition accuracy of 96.59% and an 11 class gesture recognition accuracy of 90.28%.
C1 [Xing, Yannan; Di Caterina, Gaetano; Soraghan, John] Univ Strathclyde, Dept Elect & Elect Engn, Ctr Signal & Image Proc CeSIP, Neuromorph Sensor Signal Proc Lab, Glasgow, Lanark, Scotland.
RP Xing, YN (corresponding author), Univ Strathclyde, Dept Elect & Elect Engn, Ctr Signal & Image Proc CeSIP, Neuromorph Sensor Signal Proc Lab, Glasgow, Lanark, Scotland.
EM yannan.xing@strath.ac.uk
CR Abbott LF, 1999, BRAIN RES BULL, V50, P303, DOI 10.1016/S0361-9230(99)00161-6
   Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   Amir A, 2017, PROC CVPR IEEE, P7388, DOI 10.1109/CVPR.2017.781
   [Anonymous], 2012, IEEE IJCNN
   [Anonymous], 1997, NEURAL COMPUT, DOI 10.1162/neco.1997.9.8.1735
   Bae Soo Hyun, 2016, ACOUSTIC SCENE CLASS
   Bower J. M., 1995, THE BOOK OF GENESIS, DOI [10.1007/978-1-4684-0189-9, DOI 10.1007/978-1-4684-0189-9]
   Brandli C, 2014, IEEE J SOLID-ST CIRC, V49, P2333, DOI 10.1109/JSSC.2014.2342715
   Çakir E, 2017, IEEE-ACM T AUDIO SPE, V25, P1291, DOI 10.1109/TASLP.2017.2690575
   Choi K, 2017, INT CONF ACOUST SPEE, P2392, DOI 10.1109/ICASSP.2017.7952585
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Davison Andrew P, 2008, Front Neuroinform, V2, P11, DOI 10.3389/neuro.11.011.2008
   Demin V, 2018, FRONT NEUROINFORM, V12, DOI 10.3389/fninf.2018.00079
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Donahue J, 2017, IEEE T PATTERN ANAL, V39, P677, DOI 10.1109/TPAMI.2016.2599174
   Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316
   Droeschel D, 2011, ACMIEEE INT CONF HUM, P481, DOI 10.1145/1957656.1957822
   Fan Y, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P445, DOI 10.1145/2993148.2997632
   Feng HF, 2001, NEURAL NETWORKS, V14, P955, DOI 10.1016/S0893-6080(01)00074-0
   Feng JF, 2000, B MATH BIOL, V62, P467, DOI 10.1006/bulm.1999.0162
   Frati V., 2011, 2011 IEEE World Haptics Conference (WHC 2011), P317, DOI 10.1109/WHC.2011.5945505
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   Furber SB, 2013, IEEE T COMPUT, V62, P2454, DOI 10.1109/TC.2012.142
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   Gerstner W., 2009, ENCY NEUROSCIENCE, DOI [10.1016/B978-008045046-9.01405-4, DOI 10.1016/B978-008045046-9.01405-4]
   Gerstner W, 2018, SCHOLARPEDIA, V3, P1343
   Han B, 2010, APPL OPTICS, V49, pB83, DOI 10.1364/AO.49.000B83
   Haria A, 2017, PROCEDIA COMPUT SCI, V115, P367, DOI 10.1016/j.procs.2017.09.092
   Hinton G., 2012, NEURAL NETWORKS MACH
   Hinz G, 2017, LECT NOTES ARTIF INT, V10505, P142, DOI 10.1007/978-3-319-67190-1_11
   HODGKIN AL, 1990, B MATH BIOL, V52, P25, DOI 10.1016/S0092-8240(05)80004-7
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Jiang ZY, 2019, IEEE INT CONF ROBOT, P8332, DOI [10.1109/icra.2019.8793924, 10.1109/ICRA.2019.8793924]
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Kasabov N, 2013, NEURAL NETWORKS, V41, P188, DOI 10.1016/j.neunet.2012.11.014
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Kingma DP., 2015, P INT C LEARN REPR I, P1
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
   Liang R. H., 1998, P 3 IEEE INT C AUT F
   Lichtsteiner P, 2008, IEEE J SOLID-ST CIRC, V43, P566, DOI 10.1109/JSSC.2007.914337
   Liu HY, 2018, INT J IND ERGONOM, V68, P355, DOI 10.1016/j.ergon.2017.02.004
   Liu YH, 2001, J COMPUT NEUROSCI, V10, P25, DOI 10.1023/A:1008916026143
   Loiselle S, 2005, IEEE IJCNN, P2076
   Majd M, 2019, APPL INTELL, V49, P2515, DOI 10.1007/s10489-018-1395-8
   Mitra S, 2007, IEEE T SYST MAN CY C, V37, P311, DOI 10.1109/TSMCC.2007.893280
   Mohemmed A, 2012, INT J NEURAL SYST, V22, DOI 10.1142/S0129065712500128
   Nair V., 2010, PROC 27 INT C INT C
   Nekhaev D, 2020, STUD COMPUT INTELL, V856, P255, DOI 10.1007/978-3-030-30425-6_30
   Perez-Carrasco J. A., 2010, P INT C PATT REC IST, DOI [10.1109/ICPR.2010.756, DOI 10.1109/ICPR.2010.756]
   Pigou L, 2015, LECT NOTES COMPUT SC, V8925, P572, DOI 10.1007/978-3-319-16178-5_40
   Posch C, 2011, IEEE J SOLID-ST CIRC, V46, P259, DOI 10.1109/JSSC.2010.2085952
   Rautaray SS, 2015, ARTIF INTELL REV, V43, P1, DOI 10.1007/s10462-012-9356-9
   Shi XJ, 2015, ADV NEUR IN, V28
   Shrestha S. B., 2018, ADV NEURAL INFORM PR
   Song HM, 2018, LECT NOTES COMPUT SC, V11215, P744, DOI 10.1007/978-3-030-01252-6_44
   Srivastava N, 2015, PR MACH LEARN RES, V37, P843
   Tan K, 2018, INTERSPEECH, P3229
   Tavanaei A, 2017, LECT NOTES COMPUT SC, V10639, P899, DOI 10.1007/978-3-319-70136-3_95
   Teka W, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003526
   Vreeken J., 2002, SPIKING NEURAL NETWO
   Wang LB, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00434
   Wang QY, 2019, IEEE WINT CONF APPL, P1826, DOI 10.1109/WACV.2019.00199
   Wang W, 2019, IEEE ACCESS, V7, P117165, DOI 10.1109/ACCESS.2019.2936604
   Wang XH, 2017, IEEE SIGNAL PROC LET, V24, P510, DOI 10.1109/LSP.2016.2611485
   WERBOS PJ, 1990, P IEEE, V78, P1550, DOI 10.1109/5.58337
   Wickeroth D, 2009, 2009 SECOND INTERNATIONAL CONFERENCE ON THE APPLICATIONS OF DIGITAL INFORMATION AND WEB TECHNOLOGIES (ICADIWT 2009), P682, DOI 10.1109/ICADIWT.2009.5273873
   Wysoski SG, 2010, NEURAL NETWORKS, V23, P819, DOI 10.1016/j.neunet.2010.04.009
   Yang HD, 2018, APPL SCI-BASEL, V8, DOI 10.3390/app8030383
   Yang RD, 2010, IEEE T PATTERN ANAL, V32, P462, DOI 10.1109/TPAMI.2009.26
   Zhou K., 2017, 2017 IEEE VISUAL COM, P1, DOI [10.1109/VCIP.2017.8305063, DOI 10.1109/VCIP.2017.8305063]
NR 70
TC 31
Z9 33
U1 2
U2 19
PD NOV 17
PY 2020
VL 14
AR 590164
DI 10.3389/fnins.2020.590164
UT WOS:000596378800001
DA 2023-11-16
ER

PT C
AU Kotariya, V
   Ganguly, U
AF Kotariya, Vineet
   Ganguly, Udayan
GP IEEE
TI Spiking-GAN: A Spiking Generative Adversarial Network Using
   Time-To-First-Spike Coding
SO 2022 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT IEEE International Conference on Fuzzy Systems (FUZZ-IEEE) / IEEE World
   Congress on Computational Intelligence (IEEE WCCI) / International Joint
   Conference on Neural Networks (IJCNN) / IEEE Congress on Evolutionary
   Computation (IEEE CEC)
CY JUL 18-23, 2022
CL Padua, ITALY
DE Spiking Neural Networks; Generative Adversarial Networks; Temporal
   Backpropagation
ID ON-CHIP
AB Spiking Neural Networks (SNNs) have shown great potential in solving deep learning problems in an energy-efficient manner. However, they are still limited to simple classification tasks. In this paper, we propose Spiking-GAN, the first spike-based Generative Adversarial Network (GAN). It employs a kind of temporal coding scheme called time-to-first-spike coding. We train it using approximate backpropagation in the temporal domain. We use simple integrate-and-fire (IF) neurons with very high refractory period for our network which ensures a maximum of one spike per neuron. This makes the model much sparser than a spike rate-based system. Our modified temporal loss function called 'Aggressive TTFS' improves the inference time of the network by over 33% and reduces the number of spikes in the network by more than 11% compared to previous works. Our experiments show that on training the network on the MNIST dataset using this approach, we can generate high quality samples with 57x lower energy consumption compared to ANN-based GANs. Thereby demonstrating the potential of this framework for solving such problems in the spiking domain.
C1 [Kotariya, Vineet; Ganguly, Udayan] Indian Inst Technol, Dept Elect Engn, Mumbai, Maharashtra, India.
RP Kotariya, V (corresponding author), Indian Inst Technol, Dept Elect Engn, Mumbai, Maharashtra, India.
EM vineetkotariya@iitb.ac.in; udayan@ee.iitb.ac.in
CR Alqahtani H, 2021, ARCH COMPUT METHOD E, V28, P525, DOI 10.1007/s11831-019-09388-y
   Arjovsky M, 2017, PR MACH LEARN RES, V70
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   DeBole MV, 2019, COMPUTER, V52, P20, DOI 10.1109/MC.2019.2903009
   Fabbri M, 2017, 2017 14TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE (AVSS)
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Illing B, 2019, NEURAL NETWORKS, V118, P90, DOI 10.1016/j.neunet.2019.06.001
   Isola P, 2017, PROC CVPR IEEE, P1125, DOI DOI 10.1109/CVPR.2017.632
   Joshi K, 2020, 2020 7TH IEEE INTERNATIONAL CONFERENCE ON SMART STRUCTURES AND SYSTEMS (ICSSS 2020), P12, DOI 10.1109/icsss49621.2020.9202024
   Kheradpisheh SR, 2020, INT J NEURAL SYST, V30, DOI 10.1142/S0129065720500276
   Kim S, 2020, AAAI CONF ARTIF INTE, V34, P11270
   Kundu S, 2021, IEEE WINT CONF APPL, P3952, DOI 10.1109/WACV48630.2021.00400
   Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19
   Lin H., 2020, ADV NEURAL INFORM PR, V33, p19 534
   Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304
   Oh S., 2020, HARDWARE IMPLEMENTAT
   Painkras E, 2013, IEEE J SOLID-ST CIRC, V48, P1943, DOI 10.1109/JSSC.2013.2259038
   Park S, 2020, DES AUT CON, DOI [10.1109/dac18072.2020.9218689, 10.1007/s00779-020-01476-2]
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Rajendran B, 2019, IEEE SIGNAL PROC MAG, V36, P97, DOI 10.1109/MSP.2019.2933719
   Reed S, 2016, PR MACH LEARN RES, V48
   Rueckauer B, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351295
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Tavanaei A, 2019, NEUROCOMPUTING, V330, P39, DOI 10.1016/j.neucom.2018.11.014
   Tuckwell HC, 2005, PHYSICA A, V351, P427, DOI 10.1016/j.physa.2004.11.059
   Van Rullen R, 2001, NEURAL COMPUT, V13, P1255, DOI 10.1162/08997660152002852
NR 26
TC 2
Z9 2
U1 4
U2 6
PY 2022
DI 10.1109/IJCNN55064.2022.9892262
UT WOS:000867070903022
DA 2023-11-16
ER

PT C
AU Ritter, GX
   Urcid, G
AF Ritter, Gerhard X.
   Urcid, Gonzalo
BE Corchado, E
   Romay, MG
   Savio, AM
TI Lattice Neural Networks with Spike Trains
SO HYBRID ARTIFICIAL INTELLIGENCE SYSTEMS, PT 2
SE Lecture Notes in Artificial Intelligence
DT Proceedings Paper
CT 5th International Conference on Hybrid Artificial Intelligence Systems
CY JUN 23-25, 2010
CL Univ Pais Vasco, San Sebastian, SPAIN
HO Univ Pais Vasco
AB Lattice based neural networks have proven their capability of resolving difficult non-linear problems and have been successfully employed to resolve real-world problems. In this paper we introduce a novel lattice neural net that generalizes previous dendritic models. The new model employs the biological notion of dendritic spines and spike trains. We show by example that it can accomplish tasks previous lattice neural networks were incapable of achieving.
C1 [Ritter, Gerhard X.] Univ Florida, CISE Dept, Gainesville, FL 32611 USA.
RP Urcid, G (corresponding author), INAOE, Dept Opt, Tonantzintla 72000, Pue, Mexico.
EM ritter@cise.ufl.edu; gurcid@inaoep.mx
CR Barmpoutis A, 2007, STUD COMPUT INTELL, V67, P45
   Holmes W. T., 1992, SINGLE NEURON COMPUT, P7
   Koch C., 1992, SINGLE NEURON COMPUT, P315, DOI DOI 10.1016/B978-0-12-484815-3.50019-0
   Koch Christof, 1999, P1
   Lampl I, 2004, J NEUROPHYSIOL, V92, P2704, DOI 10.1152/jn.00060.2004
   Ritter GX, 2003, IEEE T NEURAL NETWOR, V14, P282, DOI 10.1109/TNN.2003.809427
   Yu AJ, 2002, NEURAL COMPUT, V14, P2857, DOI 10.1162/089976602760805313
NR 7
TC 2
Z9 2
U1 0
U2 0
PY 2010
VL 6077
BP 367
EP +
UT WOS:000286905700046
DA 2023-11-16
ER

PT C
AU Dominguez-Morales, JP
   Jimenez-Fernandez, A
   Rios-Navarro, A
   Cerezuela-Escudero, E
   Gutierrez-Galan, D
   Dominguez-Morales, MJ
   Jimenez-Moreno, G
AF Pedro Dominguez-Morales, Juan
   Jimenez-Fernandez, Angel
   Rios-Navarro, Antonio
   Cerezuela-Escudero, Elena
   Gutierrez-Galan, Daniel
   Dominguez-Morales, Manuel J.
   Jimenez-Moreno, Gabriel
BE Villa, AEP
   Masulli, P
   Rivero, AJP
TI Multilayer Spiking Neural Network for Audio Samples Classification Using
   SpiNNaker
SO ARTIFICIAL NEURAL NETWORKS AND MACHINE LEARNING - ICANN 2016, PT I
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 25th International Conference on Artificial Neural Networks (ICANN)
CY SEP 06-09, 2016
CL Barcelona, SPAIN
DE SpiNNaker; Spiking neural network; Audio samples classification; Spikes;
   Neuromorphic auditory sensor; Address-Event Representation
AB Audio classification has always been an interesting subject of research inside the neuromorphic engineering field. Tools like Nengo or Brian, and hardware platforms like the SpiNNaker board are rapidly increasing in popularity in the neuromorphic community due to the ease of modelling spiking neural networks with them. In this manuscript a multilayer spiking neural network for audio samples classification using SpiNNaker is presented. The network consists of different leaky integrate-and-fire neuron layers. The connections between them are trained using novel firing rate based algorithms and tested using sets of pure tones with frequencies that range from 130.813 to 1396.91 Hz. The hit rate percentage values are obtained after adding a random noise signal to the original pure tone signal. The results show very good classification results (above 85 % hit rate) for each class when the Signal-to-noise ratio is above 3 decibels, validating the robustness of the network configuration and the training step.
C1 [Pedro Dominguez-Morales, Juan; Jimenez-Fernandez, Angel; Rios-Navarro, Antonio; Cerezuela-Escudero, Elena; Gutierrez-Galan, Daniel; Dominguez-Morales, Manuel J.; Jimenez-Moreno, Gabriel] Univ Seville, Robot & Technol Comp Lab, Dept Architecture & Technol Comp, Seville, Spain.
RP Dominguez-Morales, JP (corresponding author), Univ Seville, Robot & Technol Comp Lab, Dept Architecture & Technol Comp, Seville, Spain.
EM jpdominguez@atc.us.es; ajimenez@atc.us.es; arios@atc.us.es;
   ecerezuela@atc.us.es; dgutierrez@atc.us.es; mdominguez@atc.us.es;
   gaji@atc.us.es
CR Bekolay T, 2014, FRONT NEUROINFORM, V7, DOI 10.3389/fninf.2013.00048
   Berner R, 2007, IEEE INT SYMP CIRC S, P2451, DOI 10.1109/ISCAS.2007.378616
   Boahen KA, 2000, IEEE T CIRCUITS-II, V47, P416, DOI 10.1109/82.842110
   Chan V, 2007, IEEE T CIRCUITS-I, V54, P48, DOI 10.1109/TCSI.2006.887979
   Davison Andrew P, 2008, Front Neuroinform, V2, P11, DOI 10.3389/neuro.11.011.2008
   Dominguez-Morales J.P., MULTILAYER SPIKING N
   Goodman Dan, 2008, Front Neuroinform, V2, P5, DOI 10.3389/neuro.11.005.2008
   Häfliger P, 2007, IEEE T NEURAL NETWOR, V18, P551, DOI 10.1109/TNN.2006.884676
   Hamilton TJ, 2008, IEEE T BIOMED CIRC S, V2, P30, DOI 10.1109/TBCAS.2008.921602
   Indiveri G, 2006, IEEE T NEURAL NETWOR, V17, P211, DOI 10.1109/TNN.2005.860850
   Jimenez-Fernandez A., 2010, INT JOINT C NEUR NET
   Jimenez-Fernandez A., 2016, IEEE T NEURAL NETWOR, V1
   Jimenez-Fernandez A, 2012, SENSORS-BASEL, V12, P3831, DOI 10.3390/s120403831
   Lichtsteiner P, 2008, IEEE J SOLID-ST CIRC, V43, P566, DOI 10.1109/JSSC.2007.914337
   Linares-Barranco A, 2007, IEEE INT SYMP CIRC S, P1192, DOI 10.1109/ISCAS.2007.378265
   Linares-Barranco A, 2015, IEEE INT SYMP CIRC S, P2417, DOI 10.1109/ISCAS.2015.7169172
   Painkras E, 2013, IEEE J SOLID-ST CIRC, V48, P1943, DOI 10.1109/JSSC.2013.2259038
NR 17
TC 10
Z9 10
U1 0
U2 7
PY 2016
VL 9886
BP 45
EP 53
DI 10.1007/978-3-319-44778-0_6
UT WOS:000389086300006
DA 2023-11-16
ER

PT C
AU Saranirad, V
   Dora, S
   McGinnity, TM
   Coyle, D
AF Saranirad, Vahid
   Dora, Shirin
   McGinnity, T. M.
   Coyle, Damien
GP IEEE
TI Assembly-based STDP: A New Learning Rule for Spiking Neural Networks
   Inspired by Biological Assemblies
SO 2022 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT IEEE International Conference on Fuzzy Systems (FUZZ-IEEE) / IEEE World
   Congress on Computational Intelligence (IEEE WCCI) / International Joint
   Conference on Neural Networks (IJCNN) / IEEE Congress on Evolutionary
   Computation (IEEE CEC)
CY JUL 18-23, 2022
CL Padua, ITALY
DE STDP; assembly of neurons; DoB; degree of belonging; spiking neural
   network; SNN
ID CLASSIFICATION
AB Spiking Neural Networks (SNNs), An alternative to sigmoidal neural networks, include time into their operations using discrete signals called spikes. Employing spikes enables SNNs to mimic any feedforward sigmoidal neural network with lower power consumption. Recently a new type of SNN has been introduced for classification problems, known as Degree of Belonging SNN (DoB-SNN). DoB-SNN is a two-layer spiking neural network that shows significant potential as an alternative SNN architecture and learning algorithm. This paper introduces a new variant of Spike-Timing Dependent Plasticity (STDP), which is based on the assembly of neurons and expands the DoB-SNN's training algorithm for multilayer architectures. The new learning rule, known as assembly-based STDP, employs trained DoBs in each layer to train the next layer and build strong connections between neurons from the same assembly while creating inhibitory connections between neurons from different assemblies in two consecutive layers. The performance of the multilayer DoB-SNN is evaluated on five datasets from the UCI machine learning repository. Detailed comparisons on these datasets with other supervised learning algorithms show that the multilayer DoB-SNN can achieve better performance on 4/5 datasets and comparable performance on 5th when compared to multilayer algorithms that employ considerably more trainable parameters.
C1 [Saranirad, Vahid; McGinnity, T. M.; Coyle, Damien] Ulster Univ, Intelligent Syst Res Ctr, Derry, North Ireland.
   [Dora, Shirin] Loughborough Univ, Dept Comp Sci, Loughborough, Leics, England.
RP Saranirad, V (corresponding author), Ulster Univ, Intelligent Syst Res Ctr, Derry, North Ireland.
EM saranirad-v@ulster.ac.uk; s.dora@lboro.ac.uk; tm.mcgirmity@ulster.ac.uk;
   dh.coyle@ulster.ac.uk
CR Bavandpour M, 2014, MICROELECTRON J, V45, P1450, DOI 10.1016/j.mejo.2014.09.001
   Bing ZS, 2019, IEEE INT CONF ROBOT, P9645, DOI [10.1109/icra.2019.8793774, 10.1109/ICRA.2019.8793774]
   Bing ZS, 2019, FRONT NEUROROBOTICS, V13, DOI 10.3389/fnbot.2019.00018
   Bodyanskiy Y, 2008, PRO BIENN BALT EL C, P213, DOI 10.1109/BEC.2008.4657517
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Dan Y, 2006, PHYSIOL REV, V86, P1033, DOI 10.1152/physrev.00030.2005
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Dora S, 2016, NEUROCOMPUTING, V171, P1216, DOI 10.1016/j.neucom.2015.07.086
   Dua D., 2017, UCI MACHINE LEARNING
   Fisher R.A., 1949, DESIGN EXPT, V5
   Fisher RA, 1921, METRON, V1, P3, DOI DOI 10.1093/BIOMET/9.1-2.22
   Frémaux N, 2016, FRONT NEURAL CIRCUIT, V9, DOI 10.3389/fncir.2015.00085
   GERSTEIN GL, 1989, IEEE T BIO-MED ENG, V36, P4, DOI 10.1109/10.16444
   Gerstner W, 2018, FRONT NEURAL CIRCUIT, V12, DOI 10.3389/fncir.2018.00053
   Glackin C, 2008, LECT NOTES COMPUT SC, V5164, P258, DOI 10.1007/978-3-540-87559-8_27
   Glackin C, 2011, NEURAL NETWORKS, V24, P247, DOI 10.1016/j.neunet.2010.11.008
   Jeyasothy A, 2019, IEEE T NEUR NET LEAR, V30, P1231, DOI 10.1109/TNNLS.2018.2868874
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Kohavi R., 1995, APP INT JOINT C ART
   Lee C, 2019, IEEE T COGN DEV SYST, V11, P384, DOI 10.1109/TCDS.2018.2833071
   Maass W, 1997, NEURAL COMPUT, V9, P279, DOI 10.1162/neco.1997.9.2.279
   Markram Henry, 2011, Front Synaptic Neurosci, V3, P4, DOI 10.3389/fnsyn.2011.00004
   Mozafari M, 2018, IEEE T NEUR NET LEAR, V29, P6178, DOI 10.1109/TNNLS.2018.2826721
   Papadimitriou CH, 2020, P NATL ACAD SCI USA, V117, P14464, DOI 10.1073/pnas.2001893117
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Querlioz D, 2013, IEEE T NANOTECHNOL, V12, P288, DOI 10.1109/TNANO.2013.2250995
   Saranirad V., 2021, PROC INT JT C NEURAL
   Srinivasan G, 2017, IEEE IJCNN, P1847, DOI 10.1109/IJCNN.2017.7966075
   Tang D, 2015, 2015 IEEE SYMPOSIUM SERIES ON COMPUTATIONAL INTELLIGENCE (IEEE SSCI), P130, DOI 10.1109/SSCI.2015.29
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Thiele J., 2018, 2018 INT JOINT C NEU, P1
   Thiele JC, 2018, FRONT COMPUT NEUROSC, V12, DOI 10.3389/fncom.2018.00046
   Wade JJ, 2010, IEEE T NEURAL NETWOR, V21, P1817, DOI 10.1109/TNN.2010.2074212
NR 33
TC 0
Z9 0
U1 2
U2 5
PY 2022
DI 10.1109/IJCNN55064.2022.9891925
UT WOS:000867070900057
DA 2023-11-16
ER

PT C
AU Cordone, L
   Miramond, B
   Ferrante, S
AF Cordone, Loic
   Miramond, Benoit
   Ferrante, Sonia
GP IEEE
TI Learning from Event Cameras with Sparse Spiking Convolutional Neural
   Networks
SO 2021 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUL 18-22, 2021
CL ELECTR NETWORK
DE spiking neural networks; event cameras; sparse operations
AB Convolutional neural networks (CNNs) are now the de facto solution for computer vision problems thanks to their impressive results and ease of learning. These networks are composed of layers of connected units called artificial neurons, loosely modeling the neurons in a biological brain. However, their implementation on conventional hardware (CPU/GPU) results in high power consumption, making their integration on embedded systems difficult. In a car for example, embedded algorithms have very high constraints in term of energy, latency and accuracy. To design more efficient computer vision algorithms, we propose to follow an end-to-end biologically inspired approach using event cameras and spiking neural networks (SNNs). Event cameras output asynchronous and sparse events, providing an incredibly efficient data source, but processing these events with synchronous and dense algorithms such as CNNs does not yield any significant benefits. To address this limitation, we use spiking neural networks (SNNs), which are more biologically realistic neural networks where units communicate using discrete spikes. Due to the nature of their operations, they are hardware friendly and energy-efficient, but training them still remains a challenge. Our method enables the training of sparse spiking convolutional neural networks directly on event data, using the popular deep learning framework PyTorch. The performances in terms of accuracy, sparsity and training time on the popular DVS128 Gesture Dataset make it possible to use this bio-inspired approach for the future embedding of real-time applications on low-power neuromorphic hardware.
C1 [Cordone, Loic; Ferrante, Sonia] Renault, Sophia Antipolis, France.
   [Cordone, Loic; Miramond, Benoit] LEAT, CNRS UMR 7248, Sophia Antipolis, France.
   [Miramond, Benoit] Univ Cote Azur, Sophia Antipolis, France.
RP Cordone, L (corresponding author), Renault, Sophia Antipolis, France.
EM loic.cordone@renault.com; benoit.miramond@univ-cotedazur.fr;
   sonia.ferrante@renault.com
CR Abderrahmane N., 2020, HARDWARE DESIGN SPIK
   Abderrahmane N, 2020, IEEE IJCNN, DOI 10.1109/ijcnn48605.2020.9207702
   Abderrahmane N, 2020, NEURAL NETWORKS, V121, P366, DOI 10.1016/j.neunet.2019.09.024
   Amir A., 2017, P IEEE C COMP VIS PA, P7243, DOI DOI 10.1109/CVPR.2017.781
   Bardow P, 2016, PROC CVPR IEEE, P884, DOI 10.1109/CVPR.2016.102
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Chen G, 2020, IEEE SIGNAL PROC MAG, V37, P34, DOI 10.1109/MSP.2020.2985815
   Choy C, 2019, PROC CVPR IEEE, P3070, DOI 10.1109/CVPR.2019.00319
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Delorme A, 1999, NEUROCOMPUTING, V26-7, P989, DOI 10.1016/S0925-2312(99)00095-8
   Fang W., 2020, ARXIV200705785
   Gallego Guillermo, 2018, COMPUTER VISION PATT, V1
   Graham B, 2018, PROC CVPR IEEE, P9224, DOI 10.1109/CVPR.2018.00961
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Kaiser J., 2020, FRONTIERS NEUROSCIEN
   Khacef L, 2018, IEEE IJCNN
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Lagorce X, 2017, IEEE T PATTERN ANAL, V39, P1346, DOI 10.1109/TPAMI.2016.2574707
   Lichtsteiner P., 2006, IEEE INT SOL STAT CI, P2060, DOI DOI 10.1109/ISSCC.2006.1696265
   Liu LM, 2020, INT J MACH LEARN CYB, V11, P2371, DOI 10.1007/s13042-020-01124-4
   Lu HY, 2015, PROC CVPR IEEE, P806, DOI 10.1109/CVPR.2015.7298681
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maqueda AI, 2018, PROC CVPR IEEE, P5419, DOI 10.1109/CVPR.2018.00568
   Messikommer Nico, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P415, DOI 10.1007/978-3-030-58598-3_25
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Pellegrini T, 2021, IEEE W SP LANG TECH, P97, DOI 10.1109/SLT48900.2021.9383587
   Perot E., 2020, ADV NEURAL INFORM PR
   Rebecq Henri, 2019, T PATTERN ANAL MACHI
   Shrestha S. B., 2018, ADV NEURAL INFORM PR
   Xiao R, 2020, IEEE T NEUR NET LEAR, V31, P3649, DOI 10.1109/TNNLS.2019.2945630
   Xing YN, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.590164
   Zhao B, 2015, IEEE T NEUR NET LEAR, V26, P1963, DOI 10.1109/TNNLS.2014.2362542
NR 32
TC 7
Z9 7
U1 0
U2 0
PY 2021
DI 10.1109/IJCNN52387.2021.9533514
UT WOS:000722581701104
DA 2023-11-16
ER

PT J
AU Memmesheimer, RM
   Timme, M
AF Memmesheimer, Raoul-Martin
   Timme, Marc
TI Designing the dynamics of spiking neural networks
SO PHYSICAL REVIEW LETTERS
DT Article
ID SYNFIRE CHAINS; SYNCHRONIZATION; OSCILLATORS; CORTEX
AB Precise timing of spikes and temporal locking are key elements of neural computation. Here we demonstrate how even strongly heterogeneous, deterministic neural networks with delayed interactions and complex topology can exhibit periodic patterns of spikes that are precisely timed. We develop an analytical method to find the set of all networks exhibiting a predefined pattern dynamics. Such patterns may be arbitrarily long and of complicated temporal structure. We point out that the same pattern can exist in very different networks and have different stability properties.
C1 MPIDS, D-37073 Gottingen, Germany.
   BCCN, Gottingen, Germany.
   Univ Gottingen, Fak Phys, D-3400 Gottingen, Germany.
   Cornell Univ, Ctr Appl Math, Ithaca, NY 14853 USA.
RP Memmesheimer, RM (corresponding author), MPIDS, D-37073 Gottingen, Germany.
CR Abeles M, 2004, SCIENCE, V304, P523, DOI 10.1126/science.1097725
   ABELES M, 1993, J NEUROPHYSIOL, V70, P1629, DOI 10.1152/jn.1993.70.4.1629
   Abeles M., 1982, LOCAL CORTICAL CIRCU, DOI DOI 10.1007/978-3-642-81708-3
   Bloch IJM, 2002, PHYS REV E, V66, DOI 10.1103/PhysRevE.66.036127
   Denker M, 2004, PHYS REV LETT, V92, DOI 10.1103/PhysRevLett.92.074103
   Diesmann M, 1999, NATURE, V402, P529, DOI 10.1038/990101
   ENGEL AK, 1991, SCIENCE, V252, P1177, DOI 10.1126/science.252.5009.1177
   ERNST U, 1995, PHYS REV LETT, V74, P1570, DOI 10.1103/PhysRevLett.74.1570
   Gansel K, 2005, SOC NEUR ABSTR
   Hansel D, 2001, PHYS REV LETT, V86, P4175, DOI 10.1103/PhysRevLett.86.4175
   HERRMANN M, 1995, NETWORK-COMP NEURAL, V6, P403, DOI 10.1088/0954-898X/6/3/006
   Ikegaya Y, 2004, SCIENCE, V304, P559, DOI 10.1126/science.1093173
   Jin DZ, 2002, PHYS REV LETT, V89, DOI 10.1103/PhysRevLett.89.208102
   LAPICQUE L, 2007, J PHYSL PATHOL GEN, V9, P357
   Makarov VA, 2005, J NEUROSCI METH, V144, P265, DOI 10.1016/j.jneumeth.2004.11.013
   MEMMESHEIMER RM, IN PRESS PHYSICA D A
   MEMMESHEIMER RM, QBIONC0606041
   MIROLLO RE, 1990, SIAM J APPL MATH, V50, P1645, DOI 10.1137/0150098
   Roxin A, 2005, PHYS REV LETT, V94, DOI 10.1103/PhysRevLett.94.238103
   Singer W, 1999, NEURON, V24, P49, DOI 10.1016/S0896-6273(00)80821-1
   Timme M, 2002, PHYS REV LETT, V89, DOI 10.1103/PhysRevLett.89.258701
   Timme M, 2003, CHAOS, V13, P377, DOI 10.1063/1.1501274
   Timme M, 2002, PHYS REV LETT, V89, DOI 10.1103/PhysRevLett.89.154105
   *WOLFR RES, 2003, MATH 5
   Zumdieck A, 2004, PHYS REV LETT, V93, DOI 10.1103/PhysRevLett.93.244103
NR 25
TC 45
Z9 48
U1 0
U2 7
PD NOV 3
PY 2006
VL 97
IS 18
AR 188101
DI 10.1103/PhysRevLett.97.188101
UT WOS:000241757600063
DA 2023-11-16
ER

PT J
AU Xu, Y
   Zeng, XQ
   Han, LX
   Yang, J
AF Xu, Yan
   Zeng, Xiaoqin
   Han, Lixin
   Yang, Jing
TI A supervised multi-spike learning algorithm based on gradient descent
   for spiking neural networks
SO NEURAL NETWORKS
DT Article
DE Spiking neural networks; Multi-spike learning; Single-spike learning;
   Spike sequence learning; Classification
ID DEPENDENT SYNAPTIC PLASTICITY; NEURONS; BACKPROPAGATION; CLASSIFICATION
AB We use a supervised multi-spike learning algorithm for spiking neural networks (SNNs) with temporal encoding to simulate the learning mechanism of biological neurons in which the SNN output spike trains are encoded by firing times. We first analyze why existing gradient-descent-based learning methods for SNNs have difficulty in achieving multi-spike learning. We then propose a new multi-spike learning method for SNNs based on gradient descent that solves the problems of error function construction and interference among multiple output spikes during learning. The method could be widely applied to single spiking neurons to learn desired output spike trains and to multilayer SNNs to solve classification problems. By overcoming learning interference among multiple spikes, our method has high learning accuracy when there are a relatively large number of output spikes in need of learning. We also develop an output encoding strategy with respect to multiple spikes for classification problems. This effectively improves the classification accuracy of multi-spike learning compared to that of single-spike learning. (c) 2013 Elsevier Ltd. All rights reserved.
C1 [Xu, Yan; Zeng, Xiaoqin; Han, Lixin] Hohai Univ, Inst Intelligence Sci & Technol, Nanjing 210098, Jiangsu, Peoples R China.
   [Yang, Jing] Beijing Normal Univ, Sch Management, Zhuhai 519087, Peoples R China.
RP Xu, Y (corresponding author), Hohai Univ, Inst Intelligence Sci & Technol, Nanjing 210098, Jiangsu, Peoples R China.
EM xuyanhehai@163.com; xzeng@hhu.edu.cn
CR Adrian ED, 1926, J PHYSIOL-LONDON, V61, P151, DOI 10.1113/jphysiol.1926.sp002281
   [Anonymous], 1996, NEURAL NETWORK FUNDA
   [Anonymous], 2005, RESUME NEW SUPERVISE
   Belatreche A, 2003, P IEEE CYB INT CHALL, P39
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Booij O, 2005, INFORM PROCESS LETT, V95, P552, DOI 10.1016/j.ipl.2005.05.023
   C. Blake, 1998, UCI REPOSITORY MACHI
   Carnell A., 2005, P ESANN, P363
   Florian RV, 2007, NEURAL COMPUT, V19, P1468, DOI 10.1162/neco.2007.19.6.1468
   Gerstner W., 2002, SPIKING NEURON MODEL
   Ghosh-Dastidar S, 2007, INTEGR COMPUT-AID E, V14, P187
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Hebb D., 2005, ORG BEHAV NEUROPSYCH
   Kandel E. R., 1991, PRINCIPLES NEURAL SC
   Kasinski A., 2006, International Journal of Applied Mathematics and Computer Science, V16, P101
   Kistler WM, 2002, BIOL CYBERN, V87, P416, DOI 10.1007/s00422-002-0359-5
   Legenstein R, 2005, NEURAL COMPUT, V17, P2337, DOI 10.1162/0899766054796888
   Legenstein R, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.1000180
   Maass W, 1997, NEURAL COMPUT, V9, P279, DOI 10.1162/neco.1997.9.2.279
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maass W, 1997, ADV NEUR IN, V9, P211
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   McKennoch S, 2006, IEEE IJCNN, P3970
   Minsky M.L, 1969, PERCEPTRONS
   Pfister JP, 2006, NEURAL COMPUT, V18, P1318, DOI 10.1162/neco.2006.18.6.1318
   Ponulak F, 2008, INT J APPL MATH COMP, V18, P117, DOI 10.2478/v10006-008-0011-1
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Roberts PD, 2002, BIOL CYBERN, V87, P392, DOI 10.1007/s00422-002-0361-y
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Schrauwen B, 2004, IEEE IJCNN, P471, DOI 10.1109/IJCNN.2004.1379954
   Schrauwen B, 2006, IEEE IJCNN, P1797
   Schreiber S, 2003, NEUROCOMPUTING, V52-4, P925, DOI 10.1016/S0925-2312(02)00838-X
   Silva SM, 2005, PROCEEDINGS OF THE 2005 INTERNATIONAL CONFERENCE ON NEURAL NETWORKS AND BRAIN, VOLS 1-3, P1354
   THEUNISSEN F, 1995, J COMPUT NEUROSCI, V2, P149, DOI 10.1007/BF00961885
   Tino P, 2006, NEURAL COMPUT, V18, P591, DOI 10.1162/089976606775623360
   Van Rullen R, 2001, NEURAL COMPUT, V13, P1255, DOI 10.1162/08997660152002852
   VanRullen R, 2005, TRENDS NEUROSCI, V28, P1, DOI 10.1016/j.tins.2004.10.010
   WOLBERG WH, 1990, P NATL ACAD SCI USA, V87, P9193, DOI 10.1073/pnas.87.23.9193
   Wu QX, 2006, NEUROCOMPUTING, V69, P1912, DOI 10.1016/j.neucom.2005.11.023
   Xin JG, 2001, IEEE IJCNN, P1772, DOI 10.1109/IJCNN.2001.938430
NR 42
TC 106
Z9 120
U1 1
U2 62
PD JUL
PY 2013
VL 43
BP 99
EP 113
DI 10.1016/j.neunet.2013.02.003
UT WOS:000319237700010
DA 2023-11-16
ER

PT S
AU Yu, Q
   Tang, HJ
   Hu, J
   Tan, KC
AF Yu, Qiang
   Tang, Huajin
   Hu, Jun
   Tan, Kay Chen
BA Yu, Q
   Tang, H
   Hu, J
   Tan, KC
BF Yu, Q
   Tang, H
   Hu, J
   Tan, KC
TI A Spike-Timing Based Integrated Model for Pattern Recognition
SO NEUROMORPHIC COGNITIVE SYSTEMS: A LEARNING AND MEMORY CENTERED APPROACH
SE Intelligent Systems Reference Library
DT Article; Book Chapter
ID RETINAL GANGLION-CELLS; ATTRACTOR NETWORKS; NEURAL-NETWORKS; NEURONS;
   CODE; INFORMATION; DYNAMICS; CORTEX; PHASE; COMPUTATION
AB During the last few decades, remarkable progress has been made in solving pattern recognition problems using network of spiking neurons. However, the issue of pattern recognition involving computational process from sensory encoding to synaptic learning remains underexplored, as most existing models or algorithms only target part of the computational process. Furthermore, many learning algorithms proposed in literature neglect or pay little attention to sensory information encoding, which makes them incompatible with neural-realistic sensory signals encoded from real-world stimuli. By treating sensory coding and learning as a systematic process, we attempt to build an integrated model based on spiking neural networks (SNNs), which performs sensory neural encoding and supervised learning with precisely timed sequences of spikes. With emerging evidence of precise spike-timing neural activities, the view that information is represented by explicit firing times of action potentials rather than mean firing rates has received increasing attention recently. The external sensory stimulation is first converted into spatiotemporal patterns using latency-phase encoding method and subsequently transmitted to the consecutive network for learning. Spiking neurons are trained to reproduce target signals encoded with precisely timed spikes. It is shown that using a supervised spike-timing based learning, different spatiotemporal patterns are recognized by different spike patterns with a high time precision in milliseconds.
C1 [Yu, Qiang] Inst Infocomm Res, Singapore, Singapore.
   [Tang, Huajin] Sichuan Univ, Coll Comp Sci, Chengdu, Sichuan, Peoples R China.
   [Hu, Jun] AGI Technol, Singapore, Singapore.
   [Tan, Kay Chen] City Univ Hong Kong, Dept Comp Sci, Kowloon Tong, Hong Kong, Peoples R China.
RP Yu, Q (corresponding author), Inst Infocomm Res, Singapore, Singapore.
CR Adrian E. D., 1928, BASIS SENSATION ACTI
   [Anonymous], 1960 IRE WESCON CONV
   Arnett D., 1978, EXP BRAIN RES, V32
   Barak O, 2006, NEURAL COMPUT, V18, P2343, DOI 10.1162/neco.2006.18.10.2343
   Bi GQ, 2001, ANNU REV NEUROSCI, V24, P139, DOI 10.1146/annurev.neuro.24.1.139
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   BIALEK W, 1991, SCIENCE, V252, P1854, DOI 10.1126/science.2063199
   Blumenfeld B, 2006, NEURON, V52, P383, DOI 10.1016/j.neuron.2006.08.016
   Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   Brader JM, 2007, NEURAL COMPUT, V19, P2881, DOI 10.1162/neco.2007.19.11.2881
   Brody CD, 2003, NEURON, V37, P843, DOI 10.1016/S0896-6273(03)00120-X
   CARR CE, 1993, ANNU REV NEUROSCI, V16, P223, DOI 10.1146/annurev.ne.16.030193.001255
   Chrobak JJ, 1998, J NEUROSCI, V18, P388
   DeVries SH, 1999, J NEUROPHYSIOL, V81, P908, DOI 10.1152/jn.1999.81.2.908
   du Bois-Reymond Emil, 1848, UNTERSUCHUNGEN THIER
   Gawne TJ, 1996, J NEUROPHYSIOL, V76, P1356, DOI 10.1152/jn.1996.76.2.1356
   Gerstner W., 2002, SPIKING NEURON MODEL
   Gollisch T, 2008, SCIENCE, V319, P1108, DOI 10.1126/science.1149639
   Greschner M, 2006, J NEUROPHYSIOL, V96, P2845, DOI 10.1152/jn.01131.2005
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Heiligenberg W., 1991, NEURAL NETS ELECT FI
   Hopfield IJ, 2000, P NATL ACAD SCI USA, V97, P13919, DOI 10.1073/pnas.250483697
   Hopfield JJ, 2001, P NATL ACAD SCI USA, V98, P1282, DOI 10.1073/pnas.031567098
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   Ito M, 2000, BRAIN RES, V886, P237, DOI 10.1016/S0006-8993(00)03142-5
   Ito M, 2008, NAT REV NEUROSCI, V9, P304, DOI 10.1038/nrn2332
   Jensen O, 2001, NEURAL COMPUT, V13, P2743, DOI 10.1162/089976601317098510
   Kayser C, 2009, NEURON, V61, P597, DOI 10.1016/j.neuron.2009.01.008
   Keat J, 2001, NEURON, V30, P803, DOI 10.1016/S0896-6273(01)00322-1
   KNUDSEN EI, 1994, J NEUROSCI, V14, P3985
   Koepsell K, 2009, FRONT SYST NEUROSCI, V3, DOI 10.3389/neuro.06.004.2009
   Litvak V, 2003, J NEUROSCI, V23, P3006
   LLINAS RR, 1991, P NATL ACAD SCI USA, V88, P897, DOI 10.1073/pnas.88.3.897
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   MAINEN ZF, 1995, SCIENCE, V268, P1503, DOI 10.1126/science.7770778
   Manette OFL, 2004, IEEE T NEURAL NETWOR, V15, P1260, DOI 10.1109/TNN.2004.833127
   Meister M, 1999, NEURON, V22, P435, DOI 10.1016/S0896-6273(00)80700-X
   MEISTER M, 1995, SCIENCE, V270, P1207, DOI 10.1126/science.270.5239.1207
   MENG Y, 2011, COMPUTATIONAL INTELL, V6, P43
   Montgomery J, 2002, BIOL BULL, V203, P238, DOI 10.2307/1543417
   MullerPutz G. R., 2010, FRONT NEUROSCI, V4
   Nadasdy Z, 2009, FRONT SYST NEUROSCI, V3, DOI 10.3389/neuro.06.006.2009
   O'Keefe J, 2005, HIPPOCAMPUS, V15, P853, DOI 10.1002/hipo.20115
   Panzeri S, 2010, TRENDS NEUROSCI, V33, P111, DOI 10.1016/j.tins.2009.12.001
   PERLOVSKY L, 2011, COMPUT INTELL MAG IE, V6, P20, DOI DOI 10.1109/MCI.2010.939581
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Reich DS, 2001, J NEUROPHYSIOL, V85, P1039, DOI 10.1152/jn.2001.85.3.1039
   Riesenhuber M, 1999, NAT NEUROSCI, V2, P1019, DOI 10.1038/14819
   Russell BC, 2008, INT J COMPUT VISION, V77, P157, DOI 10.1007/s11263-007-0090-8
   Schreiber S, 2003, NEUROCOMPUTING, V52-4, P925, DOI 10.1016/S0925-2312(02)00838-X
   Seung HS, 2003, NEURON, V40, P1063, DOI 10.1016/S0896-6273(03)00761-X
   Shadlen Michael N., 1994, Current Opinion in Neurobiology, V4, P569, DOI 10.1016/0959-4388(94)90059-0
   SINGER W, 1995, ANNU REV NEUROSCI, V18, P555, DOI 10.1146/annurev.ne.18.030195.003011
   Softky W R, 1995, Curr Opin Neurobiol, V5, P239, DOI 10.1016/0959-4388(95)80032-8
   Takase H, 2009, IEEE IJCNN, P1225
   Tang HJ, 2010, NEURAL COMPUT, V22, P1899, DOI 10.1162/neco.2010.07-09-1050
   Tsodyks MV, 1996, HIPPOCAMPUS, V6, P271, DOI 10.1002/(SICI)1098-1063(1996)6:3<271::AID-HIPO5>3.3.CO;2-Q
   Van Rullen R, 2001, NEURAL COMPUT, V13, P1255, DOI 10.1162/08997660152002852
   van Wyk M, 2006, J NEUROSCI, V26, P13250, DOI 10.1523/JNEUROSCI.1991-06.2006
   Victor JD, 2000, BRAIN RES, V886, P33, DOI 10.1016/S0006-8993(00)02751-7
   Yan R, 2012, IEEE COMPUT INTELL M, V7, P64, DOI 10.1109/MCI.2011.2176767
NR 61
TC 0
Z9 0
U1 1
U2 2
PY 2017
VL 126
BP 43
EP 63
DI 10.1007/978-3-319-55310-8_3
D2 10.1007/978-3-319-55310-8
UT WOS:000433900100004
DA 2023-11-16
ER

PT J
AU Muramatsu, N
   Yu, HT
   Satoh, T
AF Muramatsu, Naoya
   Yu, Hai-Tao
   Satoh, Tetsuji
TI Combining Spiking Neural Networks with Artificial Neural Networks for
   Enhanced Image Classification
SO IEICE TRANSACTIONS ON INFORMATION AND SYSTEMS
DT Article
DE key spiking neural network; artificial neural network; machine learning
ID ERROR-BACKPROPAGATION; ON-CHIP; POWER
AB With the continued innovation of deep neural networks, spiking neural networks (SNNs) that more closely resemble biological brain synapses have attracted attention because of their low power con-sumption. Unlike artificial neural networks (ANNs), for continuous data values, they must employ an encoding process to convert the values to spike trains, suppressing the SNN's performance. To avoid this degrada-tion, the incoming analog signal must be regulated prior to the encoding process, which is also realized in living things e.g., the basement mem-branes of humans mechanically perform the Fourier transform. To this end, we combine an ANN and an SNN to build ANN-to-SNN hybrid neu-ral networks (HNNs) that improve the concerned performance. To qualify this performance and robustness, MNIST and CIFAR-10 image datasets are used for various classification tasks in which the training and encod-ing methods changes. In addition, we present simultaneous and separate training methods for the artificial and spiking layers, considering the en-coding methods of each. We find that increasing the number of artificial layers at the expense of spiking layers improves the HNN performance. For straightforward datasets such as MNIST, similar performances as ANN's are achieved by using duplicate coding and separate learning. However, for more complex tasks, the use of Gaussian coding and simultaneous learning is found to improve the accuracy of the HNN while lower power consump-tion.
C1 [Muramatsu, Naoya] Univ Tsukuba, Grad Sch Lib, Informat & Media Studies, Tsukuba, Japan.
   [Yu, Hai-Tao] Univ Tokushima, Fac Engn, Tokushima, Japan.
   [Satoh, Tetsuji] Univ Yamanashi, Fac Engn, Dept Elect Engn, Yamanashi, Japan.
RP Muramatsu, N (corresponding author), Univ Tsukuba, Grad Sch Lib, Informat & Media Studies, Tsukuba, Japan.
EM mrmnao001@myuct.ac.za
CR Abiodun OI, 2018, HELIYON, V4, DOI 10.1016/j.heliyon.2018.e00938
   Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   Auge D, 2021, NEURAL PROCESS LETT, V53, P4693, DOI 10.1007/s11063-021-10562-2
   Bing Z., 2019, THESIS TU MUNCHEN MU
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Brette R, 2015, FRONT SYST NEUROSCI, V9, DOI 10.3389/fnsys.2015.00151
   Chankyu Lee, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12374), P366, DOI 10.1007/978-3-030-58526-6_22
   Ciaparrone G, 2020, NEUROCOMPUTING, V381, P61, DOI 10.1016/j.neucom.2019.11.023
   Cruz-Albrecht JM, 2012, IEEE T BIOMED CIRC S, V6, P246, DOI 10.1109/TBCAS.2011.2174152
   Schuman CD, 2017, Arxiv, DOI arXiv:1705.06963
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Deng L, 2020, NEURAL NETWORKS, V121, P294, DOI 10.1016/j.neunet.2019.09.005
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Fang HW, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2799
   Feldmann J, 2019, NATURE, V569, P208, DOI 10.1038/s41586-019-1157-8
   Gallego G, 2022, IEEE T PATTERN ANAL, V44, P154, DOI 10.1109/TPAMI.2020.3008413
   Hazan H, 2018, FRONT NEUROINFORM, V12, DOI 10.3389/fninf.2018.00089
   He K., 2016, P IEEE C COMP VIS PA, DOI DOI 10.1007/978-3-319-46493-0_38
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   Höppner S, 2019, IEEE T CIRCUITS-I, V66, P2973, DOI 10.1109/TCSI.2019.2911898
   Hu BT, 2015, Arxiv, DOI [arXiv:1503.03244, 10.48550/arXiv.1503.03244, DOI 10.48550/ARXIV.1503.03244]
   Krizhevsky Alex, 2009, LEARNING MULTIPLE LA
   Kucik AS, 2021, IEEE COMPUT SOC CONF, P2020, DOI 10.1109/CVPRW53098.2021.00230
   Kugele A., 2021, DAGM GERM C PATT REC, P297, DOI [10.1007/978-3-030-92659-5_19, DOI 10.1007/978-3-030-92659-5_19]
   Kumar A, 2010, NAT REV NEUROSCI, V11, P615, DOI 10.1038/nrn2886
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee Chankyu, 2022, 2022 International Conference on Robotics and Automation (ICRA), P6504, DOI 10.1109/ICRA46639.2022.9811821
   Liu G, 2019, NEUROCOMPUTING, V337, P325, DOI 10.1016/j.neucom.2019.01.078
   Liu WB, 2017, NEUROCOMPUTING, V234, P11, DOI 10.1016/j.neucom.2016.12.038
   Mirsadeghi M, 2021, NEUROCOMPUTING, V427, P131, DOI 10.1016/j.neucom.2020.11.052
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Orchard G, 2015, FRONT NEUROSCI-SWITZ, V9, DOI [10.3389/fhins.2015.00437, 10.3389/fnins.2015.00437]
   Painkras E, 2013, IEEE J SOLID-ST CIRC, V48, P1943, DOI 10.1109/JSSC.2013.2259038
   Querlioz D, 2013, IEEE T NANOTECHNOL, V12, P288, DOI 10.1109/TNANO.2013.2250995
   Rosenfeld B, 2022, IEEE T COMPUT, V71, P2778, DOI 10.1109/TC.2022.3191738
   Shrestha SB, 2018, ADV NEUR IN, V31
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Singh S, 2020, ANN I S COM, P363, DOI 10.1109/ISCA45697.2020.00039
   STEIN RB, 1965, BIOPHYS J, V5, P173, DOI 10.1016/S0006-3495(65)86709-1
   Stewart K, 2022, PROCEEDINGS OF THE 2022 ANNUAL NEURO-INSPIRED COMPUTATIONAL ELEMENTS CONFERENCE (NICE 2022), P88, DOI 10.1145/3517343.3517372
   Togaçar M, 2021, CHAOS SOLITON FRACT, V144, DOI 10.1016/j.chaos.2021.110714
   Togaçar M, 2021, NEURAL COMPUT APPL, V33, P6147, DOI 10.1007/s00521-020-05388-3
   Van Rullen R, 2001, NEURAL COMPUT, V13, P1255, DOI 10.1162/08997660152002852
   Welling M, 2013, P INT C LEARN REPR B, P1
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Young T, 2018, IEEE COMPUT INTELL M, V13, P55, DOI 10.1109/MCI.2018.2840738
   Yu X, 2021, NEUROCOMPUTING, V452, P592, DOI 10.1016/j.neucom.2020.07.144
   Zhang S.Q., 2022, INT J MACH LEARN CYB, V22
NR 48
TC 0
Z9 0
U1 4
U2 7
PD FEB
PY 2023
VL E106D
IS 2
BP 252
EP 261
DI 10.1587/transinf.2021EDP7237
UT WOS:000931333500019
DA 2023-11-16
ER

PT C
AU Garreau, G
   Proxenou, E
   Andreou, AG
   Georgiou, J
AF Garreau, Guillaume
   Proxenou, Eleni
   Andreou, Andreas G.
   Georgiou, Julius
GP IEEE
TI Person Localization Through Ground Vibrations using a Sand-Scorpion
   Inspired Spiking Neural Network
SO 2013 47TH ANNUAL CONFERENCE ON INFORMATION SCIENCES AND SYSTEMS (CISS)
DT Proceedings Paper
CT 47th Annual Conference on Information Sciences and Systems (CISS)
CY MAR 20-22, 2013
CL Baltimore, MD
DE Spiking neuron; seismic sensor; ground vibration; sensor network;
   localization; scorpion
ID PREY-LOCALIZING BEHAVIOR; PARUROCTONUS-MESAENSIS; NOCTURNAL SCORPION;
   ORIENTATION; MECHANISM
AB Sand-scorpions can locate a prey using the vibration it creates in the ground when moving. We introduce a spiking neural model of the sand-scorpion and a successful implementation of this model with data collected with a network of seismic sensors.
C1 [Garreau, Guillaume; Proxenou, Eleni; Andreou, Andreas G.; Georgiou, Julius] Univ Cyprus, Holist Elect Res Lab, CY-1678 Nicosia, Cyprus.
RP Garreau, G (corresponding author), Univ Cyprus, Holist Elect Res Lab, Kallipoleos 75, CY-1678 Nicosia, Cyprus.
EM ggarreau@ucy.ac.cy; andreou@jhu.edu; julio@ucy.ac.cy
CR Adams S., 2011, NEUROCOMPUTING
   [Anonymous], P 45 ANN C INF SCI S
   Brette R., 2009, NEUROMORPHIC ENG JUL
   BROWNELL P, 1979, J COMP PHYSIOL, V131, P23, DOI 10.1007/BF00613080
   BROWNELL P, 1979, ANIM BEHAV, V27, P185, DOI 10.1016/0003-3472(79)90138-6
   BROWNELL P, 1979, J COMP PHYSIOL, V131, P31, DOI 10.1007/BF00613081
   Brownell PH, 2001, AM ZOOL, V41, P1229, DOI 10.1668/0003-1569(2001)041[1229:VSAACT]2.0.CO;2
   Goodman Dan, 2008, Front Neuroinform, V2, P5, DOI 10.3389/neuro.11.005.2008
   Julián P, 2004, IEEE T CIRCUITS-I, V51, P640, DOI 10.1109/TCSI.2004.826205
   Kim D., 2008, SENSORS FOCUS TACTIL, P431
   Kim D, 2006, IEEE T NEURAL NETWOR, V17, P1070, DOI 10.1109/TNN.2006.875971
   O'Connell-Rodwell CE, 2007, PHYSIOLOGY, V22, P287, DOI 10.1152/physiol.00008.2007
   Stürzl W, 2000, PHYS REV LETT, V84, P5668, DOI 10.1103/PhysRevLett.84.5668
   Wallander A., 2000, P AUSTR C ROB AUT ME, V2, P75
NR 14
TC 0
Z9 0
U1 0
U2 10
PY 2013
UT WOS:000325182600035
DA 2023-11-16
ER

PT C
AU Alnajjar, F
   Murase, K
AF Alnajjar, Fady
   Murase, Kazuyaki
BE Mohammadian, M
TI Self-organization of spiking neural network generating autonomous
   Behavior in a real mobile robot
SO INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE FOR MODELLING,
   CONTROL & AUTOMATION JOINTLY WITH INTERNATIONAL CONFERENCE ON
   INTELLIGENT AGENTS, WEB TECHNOLOGIES & INTERNET COMMERCE, VOL 1,
   PROCEEDINGS
DT Proceedings Paper
CT International Conference on Computational Intelligence for Modelling,
   Control and Automation/International Conference on Intelligent Agents
   Web Technologies and International Commerce
CY NOV 28-30, 2005
CL Vienna, AUSTRIA
AB In this paper, we study the relation between neural dynamics and robot behavior to develop self-organization algorithm of spiking neural network applicable to autonomous robot. We first formulated a spiking neural network model whose inputs and outputs were analog. We then implemented it into a miniature mobile robot Khepera. In order to see whether or not a solution(s) for the given task exists with the spiking neural network, the robot was evolved with the genetic algorithm (GA) in an environment. The robot acquired the obstacle avoidance and navigation task successfully, exhibiting the presence of the solution. Then, a self-organization algorithm based on the use-dependent synaptic potentiation and depotentiation was formulated and implemented into the robot. In the environment, the robot gradually organized the network and the obstacle avoidance behavior was formed. The time needed for the training was much less than with genetic evolution, approximately one fifth (115)..
C1 [Alnajjar, Fady; Murase, Kazuyaki] Univ Fukui, Dept Human & Artificial Intelligence Syst, Fukui 9108507, Japan.
RP Alnajjar, F (corresponding author), Univ Fukui, Dept Human & Artificial Intelligence Syst, Fukui 9108507, Japan.
EM fady@synapse.his.fukui-u.ac.jp; murase@synapse.his.fukui-u.ac.jp
CR Floreano D., 2001, LNCS, P38
   Gerstner W., 2002, SPIKING NEURON MODEL
   Islam M, 2002, IEICE T INF SYST, VE85D, P1118
   MAASS W, 1998, NETWORKS SPIKING NEU
   MAASS W, 1996, AUSTR C NEUR NETW
   MAASS W, 1998, P INT C NEUR INF PRO
   Ruf B, 1997, NEURAL PROCESS LETT, V5, P9, DOI 10.1023/A:1009697008681
   Ruf B, 1998, IEEE T NEURAL NETWOR, V9, P575, DOI 10.1109/72.668899
   Sala D. M., 1998, Australian Journal of Intelligent Information Processing Systems, V5, P161
NR 9
TC 0
Z9 0
U1 0
U2 0
PY 2006
BP 1134
EP +
UT WOS:000239912700183
DA 2023-11-16
ER

PT C
AU Huang, XY
   Jones, E
   Zhang, SR
   Xie, SY
   Furber, S
   Goulermas, Y
   Marsden, E
   Baistow, I
   Mitra, S
   Hamilton, A
AF Huang, Xiaoyu
   Jones, Edward
   Zhang, Siru
   Xie, Shouyu
   Furber, Steve
   Goulermas, Yannis
   Marsden, Edward
   Baistow, Ian
   Mitra, Srinjoy
   Hamilton, Alister
GP IEEE
TI An FPGA Implementation of Convolutional Spiking Neural Networks for
   Radioisotope Identification
SO 2021 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS (ISCAS)
SE IEEE International Symposium on Circuits and Systems
DT Proceedings Paper
CT IEEE International Symposium on Circuits and Systems (IEEE ISCAS)
CY MAY 22-28, 2021
CL Daegu, SOUTH KOREA
DE event-based signal processing; low power; radioisotope identification;
   convolutional spiking neural networks; FPGA; SpiNNaker
AB This paper details FPGA implementation methodology for Convolutional Spiking Neural Networks (CSNN) and applies this methodology to low-power radioisotope identification using high resolution data. A power consumption of 75 mW has been achieved on an FPGA implementation of a CSNN, with the inference accuracy of 90.62% on a synthetic dataset. The chip validation method is presented. Prototyping was accelerated by evaluating SNN parameters using SpiNNaker neuromorphic platform.
C1 [Huang, Xiaoyu; Xie, Shouyu; Mitra, Srinjoy; Hamilton, Alister] Univ Edinburgh, Edinburgh, Midlothian, Scotland.
   [Jones, Edward; Furber, Steve] Univ Manchester, Manchester, Lancs, England.
   [Zhang, Siru; Goulermas, Yannis] Univ Liverpool, Liverpool, Merseyside, England.
   [Marsden, Edward; Baistow, Ian] Kromek Grp Plc, Durham, England.
RP Huang, XY (corresponding author), Univ Edinburgh, Edinburgh, Midlothian, Scotland.
EM xiaoyu.huang@ed.ac.uk; alister.hamilton@ed.ac.uk
CR Abadi M, 2015, PRELIMINARY WHITE PA
   Altmann Y, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-62947-3
   Bilton KJ, 2019, IEEE T NUCL SCI, V66, P827, DOI 10.1109/TNS.2019.2907267
   Furber S., 2020, SPINNAKER SPIKING NE
   Huang X., 2020, IEEE EBCCSP
   Huang X., 2020, IEEE I C ELECT CIRC
   Ju XP, 2020, NEURAL COMPUT, V32, P182, DOI 10.1162/neco_a_01245
   Kamuda M, 2017, IEEE T NUCL SCI, V64, P1858, DOI 10.1109/TNS.2017.2693152
   LAZZARO J, 1993, IEEE T NEURAL NETWOR, V4, P523, DOI 10.1109/72.217193
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   T. M. Team, 2020, QUANTIZATIONAWARE TR
NR 11
TC 5
Z9 5
U1 2
U2 45
PY 2021
DI 10.1109/ISCAS51556.2021.9401412
UT WOS:000696765400355
DA 2023-11-16
ER

PT C
AU Vasu, MC
   Izquierdo, EJ
AF Vasu, Madhavun Candadai
   Izquierdo, Eduardo J.
GP ACM
TI Evolution and Analysis of Embodied Spiking Neural Networks Reveals
   Task-Specific Clusters of Effective Networks
SO PROCEEDINGS OF THE 2017 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE
   (GECCO'17)
DT Proceedings Paper
CT Genetic and Evolutionary Computation Conference (GECCO)
CY JUL 15-19, 2017
CL Berlin, GERMANY
DE Spiking Neural networks; Evolutionary algorithms; Transfer Entropy;
   Information Theory; Evolutionary Robotics
ID EFFECTIVE CONNECTIVITY; INFORMATION-TRANSFER; BRAIN; MODEL; DYNAMICS;
   NEURONS; BODY; ORGANIZATION
AB Elucidating principles that underlie computation in neural networks is currently a major research topic of interest in neuroscience. Transfer Entropy (TE) is increasingly used as a tool to bridge the gap between network structure, function, and behavior in fMRI studies. Computational models allow us to bridge the gap even further by directly associating individual neuron activity with behavior. However, most computational models that have analyzed embodied behaviors have employed non-spiking neurons. On the other hand, computational models that employ spiking neural networks tend to be restricted to disembodied tasks. We show for the first time the artificial evolution and TE-analysis of embodied spiking neural networks to perform a cognitively-interesting behavior. Specifically, we evolved an agent controlled by an Izhikevich neural network to perform a visual categorization task. The smallest networks capable of performing the task were found by repeating evolutionary runs with different network sizes. Informational analysis of the best solution revealed task-specific TE-network clusters, suggesting that within-task homogeneity and across-task heterogeneity were key to behavioral success. Moreover, analysis of the ensemble of solutions revealed that task-specificity of TE-network clusters correlated with fitness. This provides an empirically testable hypothesis that links network structure to behavior.
C1 [Vasu, Madhavun Candadai; Izquierdo, Eduardo J.] Indiana Univ, Bloomington, IN 47405 USA.
RP Vasu, MC (corresponding author), Indiana Univ, Bloomington, IN 47405 USA.
EM madcanda@indiana.edu; edizquie@indiana.edu
CR Anderson ML, 2010, BEHAV BRAIN SCI, V33, P245, DOI 10.1017/S0140525X10000853
   Beer R. D., 1996, From Animals to Animats 4. Proceedings of the Fourth International Conference on Simulation of Adaptive Behavior, P421
   Beer RD, 2003, ADAPT BEHAV, V11, P209, DOI 10.1177/1059712303114001
   Blitz DM, 1999, J NEUROSCI, V19, P5449
   Briggman KL, 2006, J NEUROSCI, V26, P10925, DOI 10.1523/JNEUROSCI.3265-06.2006
   Brunel N, 2000, J PHYSIOLOGY-PARIS, V94, P445, DOI 10.1016/S0928-4257(00)01084-6
   Chiel HJ, 1997, TRENDS NEUROSCI, V20, P553, DOI 10.1016/S0166-2236(97)01149-1
   DePasquale B., 2016, NATURE, V201, P6
   deRuyter van Steveninck R., 1996, SPIKES EXPLORING NEU
   Floreano D., 2001, LNCS, P38
   Fox MD, 2005, P NATL ACAD SCI USA, V102, P9673, DOI 10.1073/pnas.0504136102
   Garofalo M, 2009, PLOS ONE, V4, DOI 10.1371/journal.pone.0006482
   Gourévitch B, 2007, J NEUROPHYSIOL, V97, P2533, DOI 10.1152/jn.01106.2006
   Harnad S. R., 1987, CATEGORICAL PERCEPTI
   Harvey I, 2005, ARTIF LIFE, V11, P79, DOI 10.1162/1064546053278991
   Ito SY, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0027431
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Kaiser A, 2002, PHYSICA D, V166, P43, DOI 10.1016/S0167-2789(02)00432-3
   Kriegeskorte N, 2006, P NATL ACAD SCI USA, V103, P3863, DOI 10.1073/pnas.0600244103
   Lzquierdo EJ, 2016, CURR OPIN NEUROBIOL, V40, P23, DOI 10.1016/j.conb.2016.06.005
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Moioli RC, 2013, NEURAL COMPUT, V25, P2934, DOI 10.1162/NECO_a_00502
   Moioli Renan C., 2012, BIOL CYBERN, P1
   Nigam S, 2016, J NEUROSCI, V36, P670, DOI 10.1523/JNEUROSCI.2177-15.2016
   Pfeifer R, 2007, SCIENCE, V318, P1088, DOI 10.1126/science.1145803
   Pfeifer R, 2009, LECT NOTES ARTIF INT, V5436, P66, DOI 10.1007/978-3-642-00616-6_5
   Rubinov M, 2010, NEUROIMAGE, V52, P1059, DOI 10.1016/j.neuroimage.2009.10.003
   Schaffer ES, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003301
   Schreiber T, 2000, PHYS REV LETT, V85, P461, DOI 10.1103/PhysRevLett.85.461
   Shimono M, 2015, CEREB CORTEX, V25, P3743, DOI 10.1093/cercor/bhu252
   Thalmeier D, 2016, PLOS COMPUT BIOL, V12, DOI 10.1371/journal.pcbi.1004895
   THompson E., 1991, EMBODIED MIND
   Timme N, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0115764
   vanVreeswijk C, 1996, SCIENCE, V274, P1724, DOI 10.1126/science.274.5293.1724
   Vicente R, 2011, J COMPUT NEUROSCI, V30, P45, DOI 10.1007/s10827-010-0262-3
   Wang X, 2016, COMPUT INTEL NEUROSC, V2016, DOI 10.1155/2016/5403105
   WhiSington M. A., 2000, P NATL ACAD SCI USA, V97, P1867
   Wibral M, 2014, UNDERST COMPLEX SYST, P3, DOI 10.1007/978-3-642-54474-3_1
   WILSON FAW, 1994, P NATL ACAD SCI USA, V91, P4009, DOI 10.1073/pnas.91.9.4009
NR 40
TC 3
Z9 3
U1 1
U2 2
PY 2017
BP 75
EP 82
DI 10.1145/3071178.3071336
UT WOS:000530095200010
DA 2023-11-16
ER

PT C
AU Yu, Q
   Wang, LB
   Dang, JW
AF Yu, Qiang
   Wang, Longbiao
   Dang, Jianwu
BE Liu, D
   Xie, S
   Li, Y
   Zhao, D
   ElAlfy, ESM
TI Neuronal Classifier for both Rate and Timing-Based Spike Patterns
SO NEURAL INFORMATION PROCESSING (ICONIP 2017), PT VI
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 24th International Conference on Neural Information Processing (ICONIP)
CY NOV 14-18, 2017
CL Guangzhou, PEOPLES R CHINA
DE Spiking neural network; Learning; Rate code; Timing code; Classification
ID NEURAL-NETWORK; INFORMATION
AB Spikes play an essential role in information transmission and neural computation, but how neurons learn them remains unclear. Most learning rules depend on either the rate- or timing-based code, but rare one is suitable for both. In this paper, we present an efficient multi-spike learning rule which is suitable to train neurons to classify both rate- and timing-based spike patterns. With our learning rule, neurons can be trained to fire different numbers of output spikes in response to their input patterns, and therefore single neurons are capable for multi-category classification.
C1 [Yu, Qiang; Wang, Longbiao; Dang, Jianwu] Tianjin Univ, Sch Comp Sci & Technol, Tianjin Key Lab Cognit Comp & Applicat, Tianjin, Peoples R China.
RP Yu, Q (corresponding author), Tianjin Univ, Sch Comp Sci & Technol, Tianjin Key Lab Cognit Comp & Applicat, Tianjin, Peoples R China.
EM yuqnus@gmail.com; longbiao_wang@tju.edu.cn; jdang@jaist.ac.jp
CR Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Borst A, 1999, NAT NEUROSCI, V2, P947, DOI 10.1038/14731
   Brader JM, 2007, NEURAL COMPUT, V19, P2881, DOI 10.1162/neco.2007.19.11.2881
   Brette R, 2015, FRONT SYST NEUROSCI, V9, DOI 10.3389/fnsys.2015.00151
   deCharms RC, 1996, NATURE, V381, P610, DOI 10.1038/381610a0
   Florian RV, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0040233
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   Gütig R, 2016, SCIENCE, V351, DOI 10.1126/science.aab4113
   Gütig R, 2014, CURR OPIN NEUROBIOL, V25, P134, DOI 10.1016/j.conb.2014.01.004
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Panzeri S, 2010, TRENDS NEUROSCI, V33, P111, DOI 10.1016/j.tins.2009.12.001
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Reinagel P, 2000, J NEUROSCI, V20, P5392, DOI 10.1523/JNEUROSCI.20-14-05392.2000
   Wehr M, 1996, NATURE, V384, P162, DOI 10.1038/384162a0
   Yu Q, 2017, INTEL SYST REF LIBR, V126, P1, DOI 10.1007/978-3-319-55310-8
   Yu Q, 2016, IEEE T NEUR NET LEAR, V27, P621, DOI 10.1109/TNNLS.2015.2416771
   Yu Q, 2014, NEUROCOMPUTING, V138, P3, DOI 10.1016/j.neucom.2013.06.052
   Yu Q, 2013, IEEE T NEUR NET LEAR, V24, P1539, DOI 10.1109/TNNLS.2013.2245677
   Yu QF, 2013, PLOS ONE, V8, DOI [10.1371/journal.pone.0059627, 10.1371/journal.pone.0078318]
NR 19
TC 4
Z9 4
U1 0
U2 1
PY 2017
VL 10639
BP 759
EP 766
DI 10.1007/978-3-319-70136-3_80
PN VI
UT WOS:000576768500080
DA 2023-11-16
ER

PT J
AU Ceterchi, R
   Tomescu, AI
AF Ceterchi, Rodica
   Tomescu, Alexandru Ioan
TI Implementing Sorting Networks with Spiking Neural P Systems
SO FUNDAMENTA INFORMATICAE
DT Article; Proceedings Paper
CT 5th Brainstorming Week on Membrane Computing
CY FEB 04-08, 2008
CL Seville, SPAIN
DE membrane computing; spiking neural P systems; sorting networks; bitonic
   sort; optimal data layouts
AB Spiking neural P systems simulate the behavior of neurons sending signals through axons. Recently, some applications concerning Boolean circuits and sorting algorithms have been proposed. In this paper, we study the ability of such systems to simulate a well known parallel sorting model, sorting networks. First, we construct spiking neural P systems which act as comparators of two values, and then show how to assemble these building blocks according to the topology of a sorting network of N values. In the second part of the paper, we formalize a framework to transform any sorting network into a network composed of comparators which sort n values, 2 < n < N, having the same behaviour as the original sorting network, but using fewer neurons and synapses than the direct simulation. A comparison between the two models proposed here and the sorting model of Ionescu and Sburlan is also given.
C1 [Ceterchi, Rodica; Tomescu, Alexandru Ioan] Univ Bucharest, Fac Math & Comp Sci, RO-010014 Bucharest, Romania.
RP Ceterchi, R (corresponding author), Univ Bucharest, Fac Math & Comp Sci, Acad 14, RO-010014 Bucharest, Romania.
EM rceterchi@gmail.com; alexandru.tomescu@gmail.com
CR [Anonymous], 1998, SORTING SEARCHING
   BILARDI G, 1989, IEEE T COMPUT, V38, P1396, DOI 10.1109/12.35835
   DOWD M, 1989, J ACM, V36, P738, DOI 10.1145/76359.76362
   Ionescu M, 2006, FUND INFORM, V71, P279
   RUDOLPH L, 1985, IEEE T COMPUT, V34, P326, DOI 10.1109/TC.1985.5009383
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
   [No title captured]
NR 20
TC 6
Z9 6
U1 0
U2 5
PY 2008
VL 87
IS 1
BP 35
EP 48
UT WOS:000262368400003
DA 2023-11-16
ER

PT C
AU Yao, YL
   Yu, Q
   Wang, LB
   Dang, JW
AF Yao, Yanli
   Yu, Qiang
   Wang, Longbiao
   Dang, Jianwu
GP IEEE
TI A Spiking Neural Network with Distributed Keypoint Encoding for Robust
   Sound Recognition
SO 2019 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUL 14-19, 2019
CL Budapest, HUNGARY
DE Key-point encoding; spiking neural network; robust sound recognition
ID EVENT CLASSIFICATION; MACHINE; MODEL
AB Compared to traditional artificial neural networks, spiking neural networks (SNNs) operate on an additional dimension of time which makes them more suitable for processing sound signals. However, two of the major challenges in sound recognition with SNNs are neural encoding and learning which demand more research efforts. In this paper, we propose a novel method by combining an improved local time-frequency encoding using key-points detection and biologically plausible tempotron spike learning for robust sound recognition. In the neural encoding part, local energy peaks, called key-points, are firstly extracted from local temporal and spectral regions in the spectrogram. The extracted key-points in each frequency channel are then distributed to multiple sub-channels according to their energy amplitudes with their temporal positions being retained. The resulted spatio-temporal spike patterns are then used as the inputs for spiking neural networks to learn and classify patterns of different categories. We use the RWCP database to evaluate the performance of our proposed system in mismatched environments. Our experimental results highlight that our proposed system, namely DKP-SNN, is effective and reliable for robust sound recognition, resulting in an improved recognition performance as compared to baseline methods.
C1 [Yao, Yanli; Yu, Qiang; Wang, Longbiao; Dang, Jianwu] Tianjin Univ, Coll Intelligence & Comp, Tianjin Key Lab Cognit Comp & Applicat, Tianjin, Peoples R China.
RP Yao, YL (corresponding author), Tianjin Univ, Coll Intelligence & Comp, Tianjin Key Lab Cognit Comp & Applicat, Tianjin, Peoples R China.
EM yaoyanli@tju.edu.cn; yuqiang@tju.edu.cn; longbiao_wang@tju.edu.cn;
   jdang@jaistac.jp
CR Allen JB, 1994, IEEE T SPEECH AUDI P, V2, P567, DOI 10.1109/89.326615
   [Anonymous], 2001, THEORETICAL NEUROSCI
   [Anonymous], 2018, IEEE T CYBERNETICS
   Asano F., 2000, ACOUSTICAL SOUND DAT
   Butts DA, 2007, NATURE, V449, P92, DOI [10.1038/nature06105, 10.1038/natureO6105]
   Clavel Chloe, 2005, 2005 IEEE INT C MULT
   Dennis J., 2013, AC SPEECH SIGN PROC
   Dennis J, 2011, IEEE SIGNAL PROC LET, V18, P130, DOI 10.1109/LSP.2010.2100380
   Gerosa L., 2007, SIGN PROC C 2007 15
   Gerstner W., 2002, SPIKING NEURON MODEL
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Phan H, 2016, IEEE-ACM T AUDIO SPE, V24, P807, DOI 10.1109/TASLP.2016.2530401
   Indiveri Giacomo, 2003, CIRC SYST 2003 ISCAS, V4
   Kasabov NK, 2014, NEURAL NETWORKS, V52, P62, DOI 10.1016/j.neunet.2014.01.006
   Kwak C, 2012, IET SIGNAL PROCESS, V6, P326, DOI 10.1049/iet-spr.2011.0170
   Lee JM, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00191
   LEE YH, 1985, IEEE T ACOUST SPEECH, V33, P672
   Lestienne R, 2001, PROG NEUROBIOL, V65, P545, DOI 10.1016/S0301-0082(01)00019-3
   Lyon RF, 2010, IEEE SIGNAL PROC MAG, V27, P131, DOI 10.1109/MSP.2010.937498
   McLoughlin I, 2015, IEEE-ACM T AUDIO SPE, V23, P540, DOI 10.1109/TASLP.2015.2389618
   Ozer I, 2018, NEUROCOMPUTING, V272, P505, DOI 10.1016/j.neucom.2017.07.021
   Rospars JP, 2000, BIOSYSTEMS, V58, P133, DOI 10.1016/S0303-2647(00)00116-7
   Sharan RV, 2016, NEUROCOMPUTING, V200, P22, DOI 10.1016/j.neucom.2016.03.020
   Tu EM, 2017, IEEE T NEUR NET LEAR, V28, P1305, DOI 10.1109/TNNLS.2016.2536742
   VanRullen R, 2005, TRENDS NEUROSCI, V28, P1, DOI 10.1016/j.tins.2004.10.010
   VARGA A, 1993, SPEECH COMMUN, V12, P247, DOI 10.1016/0167-6393(93)90095-3
   Verstraeten D, 2005, INFORM PROCESS LETT, V95, P521, DOI 10.1016/j.ipl.2005.05.019
   Walters T.C., 2011, THESIS
   Yang F, 2016, INT CONF SIGN PROCES, P584, DOI 10.1109/ICSP.2016.7877900
   Yu Q., 2019, ARXIV190201094
   Yu Q, 2016, IEEE T NEUR NET LEAR, V27, P621, DOI 10.1109/TNNLS.2015.2416771
   Yu Q, 2014, NEUROCOMPUTING, V138, P3, DOI 10.1016/j.neucom.2013.06.052
   Yu Q, 2013, IEEE T NEUR NET LEAR, V24, P1539, DOI 10.1109/TNNLS.2013.2245677
   Yu Qiang, 2017, INT C NEUR INF PROC
   Yu Qiang, 2018, INT C NEUR INF PROC
   Zhang Haomin, 2015, AC SPEECH SIGN PROC
NR 36
TC 0
Z9 0
U1 0
U2 1
PY 2019
UT WOS:000530893803098
DA 2023-11-16
ER

PT J
AU Kunev, M
   Kuznetsov, P
   Sheynikhovich, D
AF Kunev, Martin
   Kuznetsov, Petr
   Sheynikhovich, Denis
TI Agreement in Spiking Neural Networks
SO JOURNAL OF COMPUTATIONAL BIOLOGY
DT Article
DE binary agreement; complexity; consensus; spiking neural network;
   winner-take-all
AB We study the problem of binary agreement in a spiking neural network (SNN). We show that binary agreement on n inputs can be achieved with O(n) of auxiliary neurons. Our simulation results suggest that agreement can be achieved in our network in O(logn) time. We then describe a subclass of SNNs with a biologically plausible property, which we call size-independence. We prove that solving a class of problems, including agreement and Winner-Take-All, in this model requires omega(n) auxiliary neurons, which makes our agreement network size-optimal.
C1 [Kunev, Martin; Kuznetsov, Petr] Inst Polytech Paris, LTCI, Telecom Paris, Palaiseau, France.
   [Sheynikhovich, Denis] Sorbonne Univ, Inst Vis, INSERM, CNRS, Paris, France.
   [Kuznetsov, Petr] Inst Polytech Paris, LTCI, Telecom Paris, 19 Pl Marguerite Perey, Palaiseau F-91123, France.
RP Kuznetsov, P (corresponding author), Inst Polytech Paris, LTCI, Telecom Paris, 19 Pl Marguerite Perey, Palaiseau F-91123, France.
EM petr.kuznetsov@telecom-paris.fr
CR Barlow H., 1961, POSSIBLE PRINCIPLES, P217, DOI [10.7551/mitpress/9780262518420.003.0013, DOI 10.7551/MITPRESS/9780262518420.003.0013]
   Gerstner W., 2002, SPIKING NEURON MODEL
   GOLDMANRAKIC PS, 1995, NEURON, V14, P477, DOI 10.1016/0896-6273(95)90304-6
   HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837
   Kandel E. R., 1991, PRINCIPLES NEURAL SC
   Lynch N., 2018, ARXIV PREPRINT ARXIV
   Lynch N., 2017, SPIKING NEURAL NETWO
   Lynch N.A., 2017, LIPICS, V67
   MOUNTCASTLE VB, 1957, J NEUROPHYSIOL, V20, P408, DOI 10.1152/jn.1957.20.4.408
   Shriki O, 2003, NEURAL COMPUT, V15, P1809, DOI 10.1162/08997660360675053
   Su LL, 2019, NEURAL COMPUT, V31, P2523, DOI 10.1162/neco_a_01242
NR 11
TC 0
Z9 0
U1 0
U2 6
PD APR 1
PY 2022
VL 29
IS 4
BP 358
EP 369
DI 10.1089/cmb.2021.0365
UT WOS:000791271600005
DA 2023-11-16
ER

PT J
AU Carrillo, RR
   Ros, E
   Barbour, B
   Boucheny, C
   Coenen, O
AF Carrillo, Richard R.
   Ros, Eduardo
   Barbour, Boris
   Boucheny, Christian
   Coenen, Olivier
TI Event-driven simulation of neural population synchronization facilitated
   by electrical coupling
SO BIOSYSTEMS
DT Article; Proceedings Paper
CT 6th International Workshop on Information Processing in Cells and
   Tissues
CY AUG 30-SEP 01, 2005
CL St Williams Coll, York, ENGLAND
HO St Williams Coll
DE event-driven; spiking neuron; neural synchronization; electrical
   coupling
ID SPIKING NEURONS; LARGE NETWORKS; FREQUENCY; SYNAPSES; MODEL; DYNAMICS
AB Most neural communication and processing tasks are driven by spikes. This has enabled the application of the event-driven simulation schemes. However the simulation of spiking neural networks based on complex models that cannot be simplified to analytical expressions (requiring numerical calculation) is very time consuming. Here we describe briefly an event-driven simulation scheme that uses pre-calculated table-based neuron characterizations to avoid numerical calculations during a network simulation, allowing the simulation of large-scale neural systems. More concretely we explain how electrical coupling can be simulated efficiently within this computation scheme, reproducing synchronization processes observed in detailed simulations of neural populations. (c) 2006 Elsevier Ireland Ltd. All rights reserved.
C1 Univ Granada, ETSI Informat, Dept Comp Architecture & Technol, E-18071 Granada, Spain.
   Ecole Normale Super, CNRS, UMR 8544, Neurobiol Lab, F-75230 Paris 05, France.
   INRIA, Lab GRAVIR, Rhone Alpes, France.
   Sony Comp Sci Lab Paris, F-75005 Paris, France.
RP Carrillo, RR (corresponding author), Univ Granada, ETSI Informat, Dept Comp Architecture & Technol, E-18071 Granada, Spain.
EM rcarrillo@atc.ugr.es; eros@atc.ugr.es; barbour@ens.fr;
   Christian.Boucheny@college-de-france.fr; coenen@csl.sony.fr
CR Aho Alfred V., 1974, DESIGN ANAL COMPUTER, V1st
   [Anonymous], BIOPHYSICS COMPUTATI
   Boucheny C, 2005, LECT NOTES COMPUT SC, V3512, P136
   BOWER JM, 1998, BOOK GENESIS
   CHEZ C, 1991, PRINCIPLES NEURAL SC, P626
   D'Angelo E, 2001, J NEUROSCI, V21, P759, DOI 10.1523/JNEUROSCI.21-03-00759.2001
   Delorme A, 2003, NETWORK-COMP NEURAL, V14, P613, DOI 10.1088/0954-898X/14/4/301
   Draguhn A, 1998, NATURE, V394, P189, DOI 10.1038/28184
   FURSHPAN EJ, 1959, J PHYSIOL-LONDON, V145, P289, DOI 10.1113/jphysiol.1959.sp006143
   Gerstner W., 2002, SPIKING NEURON MODEL
   Gibson JR, 1999, NATURE, V402, P75, DOI 10.1038/47035
   Hines ML, 1997, NEURAL COMPUT, V9, P1179, DOI 10.1162/neco.1997.9.6.1179
   KEPLER TB, 1990, SCIENCE, V248, P83, DOI 10.1126/science.2321028
   Kopell N, 2004, P NATL ACAD SCI USA, V101, P15482, DOI 10.1073/pnas.0406343101
   Latham PE, 2000, J NEUROPHYSIOL, V83, P808, DOI 10.1152/jn.2000.83.2.808
   Long MA, 2004, J NEUROSCI, V24, P341, DOI 10.1523/JNEUROSCI.3358-03.2004
   Makino T, 2003, NEURAL COMPUT APPL, V11, P210, DOI 10.1007/s00521-003-0358-z
   Mann-Metzer P, 1999, J NEUROSCI, V19, P3298
   Mattia M, 2000, NEURAL COMPUT, V12, P2305, DOI 10.1162/089976600300014953
   NEYTON J, 1985, NATURE, V317, P331, DOI 10.1038/317331a0
   Reutimann J, 2003, NEURAL COMPUT, V15, P811, DOI 10.1162/08997660360581912
   ROS E, 2006, IN PRESS NEURAL COMP, V18
   Traub RD, 2000, J NEUROSCI, V20, P2086
   WILLIAMS JWJ, 1964, COMMUN ACM, V7, P347
NR 24
TC 10
Z9 10
U1 0
U2 3
PD FEB
PY 2007
VL 87
IS 2-3
BP 275
EP 280
DI 10.1016/j.biosystems.2006.09.023
UT WOS:000244128500023
DA 2023-11-16
ER

PT C
AU Wu, JB
   Chua, YS
   Zhang, ML
   Yang, Q
   Li, G
   Li, HZ
AF Wu, Jibin
   Chua, Yansong
   Zhang, Malu
   Yang, Qu
   Li, Guoqi
   Li, Haizhou
GP IEEE
TI Deep Spiking Neural Network with Spike Count based Learning Rule
SO 2019 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUL 14-19, 2019
CL Budapest, HUNGARY
AB Deep spiking neural networks (SNNs) support asynchronous event-driven computation, massive parallelism and demonstrate great potential to improve the energy efficiency of its synchronous analog counterpart. However, insufficient attention has been paid to neural encoding when designing SNN learning rules. Remarkably, the temporal credit assignment has been performed on rate-coded spiking inputs, leading to poor learning efficiency. In this paper, we introduce a novel spike-based learning rule for rate-coded deep SNNs, whereby the spike count of each neuron is used as a surrogate for gradient backpropagation. We evaluate the proposed learning rule by training deep spiking multi-layer perceptron (MLP) and spiking convolutional neural network (CNN) on the UCI machine learning and MNIST handwritten digit datasets. We show that the proposed learning rule achieves state-of-the-art accuracies on all benchmark datasets. The proposed learning rule allows introducing latency, spike rate and hardware constraints into the SNN learning, which is superior to the indirect approach in which conventional artificial neural networks are first trained and then converted to SNNs. Hence, it allows direct deployment to the neuromorphic hardware and supports efficient inference. Notably, a test accuracy of 98.40% was achieved on the MNIST dataset in our experiments with only 10 simulation time steps, when the same latency constraint is imposed during training.
C1 [Wu, Jibin; Zhang, Malu; Yang, Qu; Li, Haizhou] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore, Singapore.
   [Chua, Yansong] ASTAR, Inst Infocomm Res, Singapore, Singapore.
   [Li, Guoqi] Tsinghua Univ, Beijing Innovat Ctr Future Chip, Dept Precis Instrument, Beijing, Peoples R China.
RP Chua, YS (corresponding author), ASTAR, Inst Infocomm Res, Singapore, Singapore.
EM chuays@i2r.a-star.edu.sg
CR [Anonymous], 2018, ARXIV180701013
   [Anonymous], 2018, ARXIV180905793
   [Anonymous], 2016, NAT METHODS, DOI DOI 10.1038/nmeth.3707
   [Anonymous], 2018, ARXIV180202627
   [Anonymous], 2016, ARXIV160208323
   [Anonymous], 2001, THEORETICAL NEUROSCI
   Asuncion A, 2007, UCI MACHINE LEARNING
   Brown EN, 2004, NAT NEUROSCI, V7, P456, DOI 10.1038/nn1228
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Diehl P.U., 2015, 2015 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2015.7280696
   Gerstner W., 2002, SPIKING NEURON MODEL
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Kingma DP., 2017, ARXIV
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
   Lee JM, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00191
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Mostafa Hesham, 2018, IEEE Trans Neural Netw Learn Syst, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Neftci EO, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00324
   Neil D, 2016, P 31 ANN ACM S APPL
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Shrestha S. B., 2018, ARXIV181008646
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Stromatias E, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00350
   Tavanaei A., 2018, ARXIV180408150
   Wang JL, 2017, IEEE T NEUR NET LEAR, V28, P30, DOI 10.1109/TNNLS.2015.2501322
   Wang Y., 2018, IEEE T NEUR NET LEAR
   Wu JB, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00836
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
NR 31
TC 0
Z9 0
U1 2
U2 4
PY 2019
UT WOS:000530893805055
DA 2023-11-16
ER

PT C
AU Zeng, Y
   Devincentis, K
   Xiao, Y
   Ferdous, ZI
   Guo, XC
   Yan, ZY
   Berdichevsky, Y
AF Zeng, Yuan
   Devincentis, Kevin
   Xiao, Yao
   Ferdous, Zubayer Ibne
   Guo, Xiaochen
   Yan, Zhiyuan
   Berdichevsky, Yevgeny
GP IEEE
TI A SUPERVISED STDP-BASED TRAINING ALGORITHM FOR LIVING NEURAL NETWORKS
SO 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)
DT Proceedings Paper
CT IEEE International Conference on Acoustics, Speech and Signal Processing
   (ICASSP)
CY APR 15-20, 2018
CL Calgary, CANADA
DE Spiking neural network; Spike timing dependent plasticity; Supervised
   learning; Biological neural network
AB Neural networks have shown great potential in many applications like speech recognition, drug discovery, image classification, and object detection. Neural network models are inspired by biological neural networks, but they are optimized to perform machine learning tasks on digital computers. The proposed work explores the possibility of using living neural networks in vitro as the basic computational elements for machine learning applications. A new supervised STDP-based learning algorithm is proposed in this work, which considers neuron engineering constraints. A 74.7% accuracy is achieved on the MNIST benchmark for handwritten digit recognition.
C1 [Zeng, Yuan; Ferdous, Zubayer Ibne; Guo, Xiaochen; Yan, Zhiyuan; Berdichevsky, Yevgeny] Lehigh Univ, Elect & Comp Engn Dept, Bethlehem, PA 18015 USA.
   [Berdichevsky, Yevgeny] Lehigh Univ, Bioengn Dept, Bethlehem, PA 18015 USA.
   [Devincentis, Kevin] Carnegie Mellon Univ, Elect & Comp Engn Dept, Pittsburgh, PA 15213 USA.
   [Xiao, Yao] Univ Sci & Technol China, Sch Gifted Young, Hefei, Anhui, Peoples R China.
RP Zeng, Y (corresponding author), Lehigh Univ, Elect & Comp Engn Dept, Bethlehem, PA 18015 USA.
CR Allred JM, 2016, IEEE IJCNN, P2492, DOI 10.1109/IJCNN.2016.7727509
   [Anonymous], 2010, SPIKE TIMING DEPENDE
   [Anonymous], 2009, ADV NEURAL INFORM PR
   [Anonymous], 2015, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2015.7299170
   Berdichevsky Y, 2010, LAB CHIP, V10, P999, DOI 10.1039/b922365g
   Berdichevsky Y, 2009, J NEUROSCI METH, V178, P59, DOI 10.1016/j.jneumeth.2008.11.016
   Beyeler M, 2013, NEURAL NETWORKS, V48, P109, DOI 10.1016/j.neunet.2013.07.012
   Brader JM, 2007, NEURAL COMPUT, V19, P2881, DOI 10.1162/neco.2007.19.11.2881
   Caporale N, 2008, ANNU REV NEUROSCI, V31, P25, DOI 10.1146/annurev.neuro.31.060407.125639
   Davies Melissa, 2002, NEUROSCIENCE JOURNEY
   Davison Andrew P., 2007, MODELLING STDP NEURO
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Du ZD, 2015, PROCEEDINGS OF THE 48TH ANNUAL IEEE/ACM INTERNATIONAL SYMPOSIUM ON MICROARCHITECTURE (MICRO-48), P494, DOI 10.1145/2830772.2830789
   Hasan MF, 2016, MICROMACHINES-BASEL, V7, DOI 10.3390/mi7090157
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Iakymchuk T, 2015, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-015-0059-4
   KEENER JP, 1981, SIAM J APPL MATH, V41, P503, DOI 10.1137/0141042
   King P., 2012, IS HUMAN BRAIN SO EN
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lewis R, 2011, J CELL PHYSIOL, V226, P2979, DOI 10.1002/jcp.22646
   Meyer DE, 1997, PSYCHOL REV, V104, P749, DOI 10.1037/0033-295X.104.4.749
   Paugam-Moisy H., 2012, HDB NATURAL COMPUTIN, V1, P1, DOI [DOI 10.1007/978-3-540-92910-9_10, 10.1007/978-3-540-92910-9_10]
   Querlioz D, 2013, IEEE T NANOTECHNOL, V12, P288, DOI 10.1109/TNANO.2013.2250995
   Seker E, 2010, NANOTECHNOLOGY, V21, DOI 10.1088/0957-4484/21/12/125504
   Shepherd GM, 1998, TRENDS NEUROSCI, V21, P460, DOI 10.1016/S0166-2236(98)01300-9
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Simonyan Karen, 2017, NATURE
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Tavanaei Amirhossein, 2015, International Journal of Advanced Research in Artificial Intelligence, V4, P1
   van Rossum MCW, 2000, J NEUROSCI, V20, P8812
   Wenyu Yang, 2012, Advances in Neural Networks - ISNN 2012. Proceedings 9th International Symposium on Neural Networks, P149, DOI 10.1007/978-3-642-31346-2_18
NR 31
TC 5
Z9 5
U1 0
U2 2
PY 2018
BP 1154
EP 1158
UT WOS:000446384601069
DA 2023-11-16
ER

PT J
AU Chakraborty, B
   Mukhopadhyay, S
AF Chakraborty, Biswadeep
   Mukhopadhyay, Saibal
TI Characterization of Generalizability of Spike Timing Dependent
   Plasticity Trained Spiking Neural Networks
SO FRONTIERS IN NEUROSCIENCE
DT Article
DE spiking neural networks; leaky integrate and fire; generalization;
   Hausdorff dimension; logSTDP; addSTDP; multSTDP; Bayesian optimization
ID SAMPLE PATHS; DIMENSION; NEURONS
AB A Spiking Neural Network (SNN) is trained with Spike Timing Dependent Plasticity (STDP), which is a neuro-inspired unsupervised learning method for various machine learning applications. This paper studies the generalizability properties of the STDP learning processes using the Hausdorff dimension of the trajectories of the learning algorithm. The paper analyzes the effects of STDP learning models and associated hyper-parameters on the generalizability properties of an SNN. The analysis is used to develop a Bayesian optimization approach to optimize the hyper-parameters for an STDP model for improving the generalizability properties of an SNN.</p>
C1 [Chakraborty, Biswadeep; Mukhopadhyay, Saibal] Georgia Inst Technol, Dept Elect & Comp Engn, Atlanta, GA 30332 USA.
RP Chakraborty, B (corresponding author), Georgia Inst Technol, Dept Elect & Comp Engn, Atlanta, GA 30332 USA.
EM biswadeep@gatech.edu
CR Allen-Zhu Z., 2018, ARXIV PREPRINT ARXIV
   Allen-Zhu Z, 2019, ARXIV PREPRINT ARXIV
   [Anonymous], 2017, ARXIV 170807747
   Baity-Jesi M, 2018, PR MACH LEARN RES, V80
   Bell CC, 1997, NATURE, V387, P278, DOI 10.1038/387278a0
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Blumenthal RM., 1960, T AM MATH SOC, V95, P263, DOI [10.1090/S0002-9947-1960-0119247-6, DOI 10.1090/S0002-9947-1960-0119247-6]
   Burkitt AN, 2004, NEURAL COMPUT, V16, P885, DOI 10.1162/089976604773135041
   CAPOCELLI RM, 1976, MATH BIOSCI, V29, P219, DOI 10.1016/0025-5564(76)90104-8
   Câteau H, 2003, NEURAL COMPUT, V15, P597, DOI 10.1162/089976603321192095
   Chichilnisky EJ, 2001, NETWORK-COMP NEURAL, V12, P199, DOI 10.1088/0954-898X/12/2/306
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Erdogdu, 2020, ADV NEURAL INFORM PR, V33
   Feldman DE, 2012, NEURON, V75, P556, DOI 10.1016/j.neuron.2012.08.001
   FEURER M, 2015, P AAAI C ARTIFICIAL
   Gerstner W, 2002, BIOL CYBERN, V87, P404, DOI 10.1007/s00422-002-0353-y
   Gerstner W, 2002, SPIKING NEURON MODEL, DOI [10.1017/CBO9780511815706, DOI 10.1017/CBO9780511815706]
   Gilson M, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0025339
   Gong P., 2020, ARXIV PREPRINT ARXIV
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Gurbuzbalaban M., 2020, ARXIV200604740
   Gütig R, 2003, J NEUROSCI, V23, P3697
   Han VZ, 2000, NEURON, V27, P611, DOI 10.1016/S0896-6273(00)00070-2
   Helson P., 2017, ARXIV PREPRINT ARXIV
   Hodgkinson L., 2021, INT C MACH LEARN, P4262
   Jones M. C., 1992, PROGR DATA BASED BAN
   Kawaguchi K., 2017, GEN DEEP LEARNING
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Khoshnevisan D., 2017, STOCHASTIC ANAL RELA, P179, DOI [10.1007/978-3-319-59671-6_9, DOI 10.1007/978-3-319-59671-6_9]
   Khoshnevisan D, 2009, PROG PROBAB, V61, P111, DOI 10.1007/978-3-0346-0030-9_4
   Kubota S, 2009, NEURAL NETWORKS, V22, P527, DOI 10.1016/j.neunet.2009.06.012
   Le Guével R, 2019, J THEOR PROBAB, V32, P765, DOI 10.1007/s10959-018-0847-8
   Lee C, 2019, IEEE T COGN DEV SYST, V11, P384, DOI 10.1109/TCDS.2018.2833071
   Legenstein R, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.1000180
   Lorinczi J, 2019, CHAOS SOLITON FRACT, V120, P83, DOI 10.1016/j.chaos.2019.01.008
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Magee JC, 1997, SCIENCE, V275, P209, DOI 10.1126/science.275.5297.209
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   Masquelier T, 2009, NEURAL COMPUT, V21, P1259, DOI 10.1162/neco.2008.06-08-804
   Meerschaert MM, 2005, STOCH PROC APPL, V115, P55, DOI 10.1016/j.spa.2004.08.004
   Mohammadi M, 2015, METRIKA, V78, P549, DOI 10.1007/s00184-014-0515-7
   Morrison A, 2007, NEURAL COMPUT, V19, P1437, DOI 10.1162/neco.2007.19.6.1437
   Mozafari M, 2019, PATTERN RECOGN, V94, P87, DOI 10.1016/j.patcog.2019.05.015
   Neyshabur B., 2017, ADV NEURAL INFORM PR, V31, P5949
   Panda P, 2018, IEEE J EM SEL TOP C, V8, P51, DOI 10.1109/JETCAS.2017.2769684
   Peres Y., 2017, FRACTALS PROBABILITY, DOI DOI 10.1017/9781316460238
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Poggio T., 2019, ARXIV190809375
   Querlioz D, 2013, IEEE T NANOTECHNOL, V12, P288, DOI 10.1109/TNANO.2013.2250995
   Richardson MJE, 2010, PHYS REV LETT, V105, DOI 10.1103/PhysRevLett.105.178102
   Roberts PD, 2000, J COMPUT NEUROSCI, V9, P67, DOI 10.1023/A:1008938428112
   Rubin J, 2001, PHYS REV LETT, V86, P364, DOI 10.1103/PhysRevLett.86.364
   She XY, 2021, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.615756
   SHEATHER SJ, 1991, J ROY STAT SOC B, V53, P683
   Simsekli U., 2020, INT C MACHINE LEARNI, P8970
   SIMSEKLI U, 2019, ARXIV PREPRINT ARXIV
   Sinz FH, 2019, NEURON, V103, P967, DOI 10.1016/j.neuron.2019.08.034
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   STEIN RB, 1965, BIOPHYS J, V5, P173, DOI 10.1016/S0006-3495(65)86709-1
   van Rossum MCW, 2000, J NEUROSCI, V20, P8812
   Vignoud G., 2020, ARXIV PREPRINT ARXIV
   Aceituno PV, 2020, BIOL CYBERN, V114, P43, DOI 10.1007/s00422-019-00813-w
   Xiao Y., 2003, MATH PREPRINT ARCH, V2003, P830
   Yang XC, 2018, ANN I H POINCARE-PR, V54, P2042, DOI 10.1214/17-AIHP864
   Zador AM, 2019, NAT COMMUN, V10, DOI 10.1038/s41467-019-11786-6
   Zhang W, 2003, NAT REV NEUROSCI, V4, P885, DOI 10.1038/nrn1248
   Zhu L., 2021, ARXIV PREPRINT ARXIV
   Zilinskas A., 1978, GLOBAL OPTIMISATION, DOI [DOI 10.1007/978-94-009-0909-0_80394.90090, 10.1007/978-94-009-0909-0_8 0394.90090]
NR 68
TC 5
Z9 5
U1 0
U2 10
PD OCT 29
PY 2021
VL 15
AR 695357
DI 10.3389/fnins.2021.695357
UT WOS:000717812600001
DA 2023-11-16
ER

PT C
AU Babic, R
AF Babic, R
BE Reljin, B
   Stankovic, S
TI Some architectures of neural networks with temporal effects
SO 2002 6TH SEMINAR ON NEURAL NETWORK APPLICATIONS IN ELECTRICAL
   ENGINEERING, PROCEEDINGS
DT Proceedings Paper
CT 6th Seminar on Neural Network Applications in Electrical Engineering
CY SEP 26-28, 2002
CL UNIV BELGRADE, FAC ELECT ENGN, BELGRADE, YUGOSLAVIA
HO UNIV BELGRADE, FAC ELECT ENGN
DE Coincidence neural detectors; Spike timing; Neural differentiator;
   Neural delay element
AB Following a new paradigm of information encoding by spike timings and its processing by neurons as coincidence detectors we firstly discuss some aspects of temporal neural phenomena, and give evolutionary interpretation of relationship between axon diameter; propagation speed and density of neural tissue. Then we propose a recurrent architecture of neural network capable to convert periodic spike train into desired pattern of spike timings. Another configuration that we propose represent neural fiber as delay element where. changeable delay effect is attained over lateral loops with creeping synapses which shortcut spanned portions of basic fiber. As the beginning and the termination might represent important indicators of a spike burst we also propose structure of a neural differentiator with cross inhibition. Finally, we give internal structure of neural delay element with incremental change of delay value, including explanation of changing i.e. learning. process.
C1 Fac Tech Sci, Kosovska Mitrovica, Yugoslavia.
RP Babic, R (corresponding author), Fac Tech Sci, Kosovska Mitrovica, Yugoslavia.
EM babic57@eunet.yu
CR Curtis H., 1983, BIOLOGY
   Fujii H, 1996, NEURAL NETWORKS, V9, P1303, DOI 10.1016/S0893-6080(96)00054-8
   NOBACK CR, 1975, HUMAN NERVOUS SYSTEM, P57
   RUBIN AB, 1987, BIOFIZIKA, P139
   SOFTKY WR, 1993, J NEUROSCI, V13, P334
NR 5
TC 0
Z9 0
U1 0
U2 0
PY 2002
BP 139
EP 141
UT WOS:000180818900028
DA 2023-11-16
ER

PT J
AU Tang, H
   Cho, D
   Lew, D
   Kim, T
   Park, J
AF Tang, Hoyoung
   Cho, Donghyeon
   Lew, Dongwoo
   Kim, Taehwan
   Park, Jongsun
TI Rank order coding based spiking convolutional neural network
   architecture with energy-efficient membrane voltage updates
SO NEUROCOMPUTING
DT Article
DE Spiking neural network; Rank order coding; Unsupervised learning; MNIST;
   Neuromorphic; Spike-timing dependent plasticity
ID TIMING-DEPENDENT PLASTICITY
AB Spiking neural network (SNN) system that uses rank order coding (ROC) as input spike encoding, gener-ally suffers from low recognition accuracy and unnecessary computations that increase complexities. In this paper, we present a Spiking convolutional neural network (Spiking CNN) architecture that signifi-cantly improves recognition accuracy as well as computation efficiencies based on a novel ROC and mod-ified kernel sizes. The proposed ROC generates spike trains based on maximum input value without sorting operations. In addition, as the recognition accuracy is affected by the reduced number of spikes as layers become deeper, the proposed ROC is inserted just before the final layer to increase the number of input spikes. The 2 x 2 pooling kernels are also replaced with 4 x 4 to reduce the network size. The hardware architecture of the proposed Spiking CNN has been implemented using 65 nm CMOS process. Neuron-centric membrane voltage update approach is also efficiently exploited in convolutional and fully connected layers to improve the hardware energy efficiencies. The Spiking CNN processor is seamlessly processing 2.85 K classifications per second with 6.79 uJ/classification. It also achieves 90.2% of recogni-tion accuracy for MNIST dataset using unsupervised learning with STDP. (C) 2020 Elsevier B.V. All rights reserved.
C1 [Tang, Hoyoung; Cho, Donghyeon; Lew, Dongwoo; Kim, Taehwan; Park, Jongsun] Korea Univ, Seoul, South Korea.
RP Park, J (corresponding author), Korea Univ, Sch Elect Engn, 145 Anam Ro, Seoul 02841, South Korea.
EM jongsun@korea.ac.kr
CR Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   Chen GK, 2019, IEEE J SOLID-ST CIRC, V54, P992, DOI 10.1109/JSSC.2018.2884901
   Cichy RM, 2016, SCI REP-UK, V6, DOI 10.1038/srep27755
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Delorme A, 2001, NEUROCOMPUTING, V38, P539, DOI 10.1016/S0925-2312(01)00403-9
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Frenkel C, 2019, IEEE T BIOMED CIRC S, V13, P145, DOI 10.1109/TBCAS.2018.2880425
   HUBEL DH, 1959, J PHYSIOL-LONDON, V148, P574, DOI 10.1113/jphysiol.1959.sp006308
   Johnson AP, 2018, IEEE T CIRCUITS-I, V65, P687, DOI 10.1109/TCSI.2017.2726763
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Kim LW, 2018, IEEE T NEUR NET LEAR, V29, P1441, DOI 10.1109/TNNLS.2017.2665555
   Lee C., 2018, IEEE T COGN DEV SYST, V8920
   Leibe B., 2003, 2003 IEEE COMP VIS P, DOI 10.1109/CVPR.2003.1211497
   Lin CK, 2018, COMPUTER, V51, P52, DOI 10.1109/MC.2018.157113521
   Lin XH, 2017, NEUROCOMPUTING, V237, P59, DOI 10.1016/j.neucom.2016.08.087
   Linares-Barranco A, 2006, IEEE T NEURAL NETWOR, V17, P771, DOI 10.1109/TNN.2006.872253
   Liu D., 2018, IEEE T CYBERN
   Liu DQ, 2017, NEUROCOMPUTING, V249, P212, DOI 10.1016/j.neucom.2017.04.003
   Luo WJ, 2016, ADV NEUR IN, V29
   Masquelier T, 2007, PLOS COMPUT BIOL, V3, P247, DOI 10.1371/journal.pcbi.0030031
   Merolla P., 2011, IEEE CUST INT CIRC C, P1, DOI DOI 10.1109/CICC.2011.6055294
   Mozafari M, 2019, PATTERN RECOGN, V94, P87, DOI 10.1016/j.patcog.2019.05.015
   Mozafari M, 2018, IEEE T NEUR NET LEAR, V29, P6178, DOI 10.1109/TNNLS.2018.2826721
   Niebur E, 2007, NEURAL COMPUT, V19, P1720, DOI 10.1162/neco.2007.19.7.1720
   Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7
   Pelayo FJ, 2004, NEUROCOMPUTING, V58, P885, DOI 10.1016/j.neucom.2004.01.142
   Rabaey J., 2003, DIGITAL INTEGRATED C, P122
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Sun QY, 2017, NEUROCOMPUTING, V228, P119, DOI 10.1016/j.neucom.2016.09.093
   Tapiador-Morales R, 2019, IEEE T BIOMED CIRC S, V13, P159, DOI 10.1109/TBCAS.2018.2880012
   Thiele J.C., 2018, INT J C NEUR NETW, P1666
   Thiele JC, 2018, FRONT COMPUT NEUROSC, V12, DOI 10.3389/fncom.2018.00046
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   Van Rullen R, 2001, NEURAL COMPUT, V13, P1255, DOI 10.1162/08997660152002852
   Wang Q, 2017, NEUROCOMPUTING, V221, P146, DOI 10.1016/j.neucom.2016.09.071
   Wiesel T.N., 1968, J PHYSL, V195, P215
   Yousefzadeh A., 2017, 2017 IEEE INT S CIRC, P1
   Zuo F, 2017, NAT COMMUN, V8, DOI 10.1038/s41467-017-00248-6
   Zuppicich A., 2008, ADV NEUR PROC 15 I 1, P1129
NR 39
TC 11
Z9 12
U1 2
U2 27
PD SEP 24
PY 2020
VL 407
BP 300
EP 312
DI 10.1016/j.neucom.2020.05.031
UT WOS:000555461000012
DA 2023-11-16
ER

PT J
AU Su, J
   Li, J
AF Su, Jing
   Li, Jing
TI HF-SNN: High-Frequency Spiking Neural Network
SO IEEE ACCESS
DT Article
DE Neurons; Biological system modeling; Training; Frequency-domain
   analysis; Feature extraction; Biological neural networks;
   Backpropagation; Spiking neural network; high-frequency; deep learning
AB As the third generation of neural networks, spiking neural network (SNN) motivated by neurophysiology enjoys considerable advances due to integrating different information, such as time and space. The frequency-domain provides a powerful capability of modeling and training convolutional neural networks (CNNs). However, SNN with binary input and output will lose much information and slightly inferior to deep neural networks (DNN). We consider how to make the most of information to protect input. Binary input and output are different from DNN, the essence of difference at frequency distribution. In this work, from the insight of frequency distribution, we rethink the SNN training process and give a novel method to transfer SNN to high-frequency spiking neural network (HF-SNN). This approach preserves considerably more information than other optimizing strategies and enables flexibility in the training process. Besides, we evaluate the HF-SNN with extensive experiments on three large datasets: CIFAR-10, CIFAR-100, and ImageNet. Finally, our model supports training a deeper SNN model from scratch and achieves better performance on these datasets than the existing SNN model.
C1 [Su, Jing; Li, Jing] Fudan Univ, Shanghai Ultraprecis Opt Mfg Engn Ctr, Dept Opt Sci & Engn, Shanghai 200433, Peoples R China.
RP Su, J (corresponding author), Fudan Univ, Shanghai Ultraprecis Opt Mfg Engn Ctr, Dept Opt Sci & Engn, Shanghai 200433, Peoples R China.
EM lijing@fudan.edu.cn
CR ABBOTT LF, 1990, LECT NOTES PHYS, V368, P5
   Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   [Anonymous], 2018, ARXIV180701013
   [Anonymous], 2018, ARXIV180905793
   Caporale N, 2008, ANNU REV NEUROSCI, V31, P25, DOI 10.1146/annurev.neuro.31.060407.125639
   Clevert D-A, 2015, ARXIV151107289, DOI DOI 10.48550/ARXIV.1511.07289
   Diehl PU, 2015, IEEE IJCNN
   Guerguiev J, 2017, ELIFE, V6, DOI 10.7554/eLife.22901
   Hao YZ, 2020, NEURAL NETWORKS, V121, P387, DOI 10.1016/j.neunet.2019.09.007
   Hazan H, 2018, FRONT NEUROINFORM, V12, DOI 10.3389/fninf.2018.00089
   Heeger D., 2000, HANDOUT U STANDFORD, V5, P76
   Hershey JR, 2007, INT CONF ACOUST SPEE, P317, DOI 10.1109/icassp.2007.366913
   Hu Q., 2018, COMPUTER VISION ECCV
   Hunsberger Eric, 2015, COMPUT SCI
   Jin Y., 2018, ADV NEURAL INFORM PR, P7005, DOI DOI 10.48550/ARXIV.1805.07866
   Kempter R, 1999, PHYS REV E, V59, P4498, DOI 10.1103/PhysRevE.59.4498
   Kim D, 2020, IEEE ACCESS, V8, P110523, DOI 10.1109/ACCESS.2020.3001296
   Kim S., 2019, SPIKING YOLO SPIKING
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
   Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508
   LEROUX BG, 1992, ANN STAT, V20, P1350, DOI 10.1214/aos/1176348772
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Nagatsuka H, 2014, COMMUN STAT-THEOR M, V43, P3905, DOI 10.1080/03610926.2012.714035
   Neil D, 2016, P 31 ANN ACM S APPL
   Neves FS, 2020, IEEE ACCESS, V8, P179648, DOI 10.1109/ACCESS.2020.3027966
   Panda P, 2016, IEEE IJCNN, P299, DOI 10.1109/IJCNN.2016.7727212
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Sun JK, 2020, J CLOUD COMPUT-ADV S, V9, DOI 10.1186/s13677-020-00200-y
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Yuan MM, 2021, IEEE T NEUR NET LEAR, V32, P151, DOI 10.1109/TNNLS.2020.2977614
   Zhang M, 2020, IEEE ACCESS, V8, P98156, DOI 10.1109/ACCESS.2020.2994360
NR 33
TC 1
Z9 1
U1 3
U2 39
PY 2021
VL 9
BP 51950
EP 51957
DI 10.1109/ACCESS.2021.3068159
UT WOS:000639856800001
DA 2023-11-16
ER

PT J
AU Lansdell, BJ
   Kording, KP
AF Lansdell, Benjamin James
   Kording, Konrad Paul
TI Neural spiking for causal inference and learning
SO PLOS COMPUTATIONAL BIOLOGY
DT Article
ID NETWORKS; REWARD; REINFORCEMENT; EMERGENCE; DOPAMINE; NEURONS
AB Author summaryDespite significant research, models of spiking neural networks still lag behind artificial neural networks in terms of performance in machine learning and modeling cognitive tasks. Given this, we may wonder, why do neurons spike? A key problem that must be solved in any neural network is the credit assignment problem. That is, how does a neuron know its effect on downstream computation and rewards, and thus how it should change its synaptic weights to improve? Artificial neural networks solve this problem with the back-propagation algorithm. We are still seeking to understand how biological neural networks effectively solve this problem. In this work we show that the discontinuous, all-or-none spiking response of a neuron can in fact be used to estimate a neuron's causal effect on downstream processes. Inspired by methods from econometrics, we show that the thresholded response of a neuron can be used to get at that neuron's unique contribution to a reward signal, separating it from other neurons whose activity it may be correlated with. This proposal provides insights into a novel function of spiking that we explore in simple networks and learning tasks.
   When a neuron is driven beyond its threshold, it spikes. The fact that it does not communicate its continuous membrane potential is usually seen as a computational liability. Here we show that this spiking mechanism allows neurons to produce an unbiased estimate of their causal influence, and a way of approximating gradient descent-based learning. Importantly, neither activity of upstream neurons, which act as confounders, nor downstream non-linearities bias the results. We show how spiking enables neurons to solve causal estimation problems and that local plasticity can approximate gradient descent using spike discontinuity learning.
C1 [Lansdell, Benjamin James; Kording, Konrad Paul] Univ Penn, Dept Bioengn, Philadelphia, PA 19104 USA.
   [Kording, Konrad Paul] Univ Penn, Dept Neurosci, Philadelphia, PA USA.
   [Lansdell, Benjamin James] St Jude Childrens Res Hosp, Dept Dev Neurobiol, Nashville, TN USA.
RP Lansdell, BJ (corresponding author), Univ Penn, Dept Bioengn, Philadelphia, PA 19104 USA.
EM ben.lansdell@stjude.org
CR Alawad M, 2017, IEEE INT CONF BIG DA, P311, DOI 10.1109/BigData.2017.8257939
   Angrist JD, 2009, MOSTLY HARMLESS ECONOMETRICS: AN EMPIRICISTS COMPANION, P1
   Angrist JD, 2010, J ECON PERSPECT, V24, P3, DOI 10.1257/jep.24.2.3
   [Anonymous], 2019, SURROGATE GRADIENT L
   ARTOLA A, 1990, NATURE, V347, P69, DOI 10.1038/347069a0
   Azouz R, 2000, P NATL ACAD SCI USA, V97, P8110, DOI 10.1073/pnas.130200797
   Babu PN, 2021, IEEE SYST J, V15, P2859, DOI 10.1109/JSYST.2020.2985164
   Baker C., 2018, BIORXIV
   Bejjanki VR, 2017, PLOS COMPUT BIOL, V13, DOI 10.1371/journal.pcbi.1005674
   Bellay T, 2015, ELIFE, V4, DOI 10.7554/eLife.07224
   Bellec G., 2018, ARXIV
   Bellec G, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-17236-y
   Bellesi G, 2019, BRAIN INJURY, V33, P1272, DOI 10.1080/02699052.2019.1641621
   Bengio Yoshua, 2013, ABS13083432 CORR
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Brunel N, 2000, J COMPUT NEUROSCI, V8, P183, DOI 10.1023/A:1008925309027
   Clopath C., 2016, BIORXIV, P053785
   Clopath Claudia, 2010, Front Synaptic Neurosci, V2, P25, DOI 10.3389/fnsyn.2010.00025
   Cohen MR, 2011, NAT NEUROSCI, V14, P811, DOI 10.1038/nn.2842
   Fiete IR, 2007, J NEUROPHYSIOL, V98, P2038, DOI 10.1152/jn.01311.2006
   Fiete IR, 2006, PHYS REV LETT, V97, DOI 10.1103/PhysRevLett.97.048104
   Fino Elodie, 2010, Front Synaptic Neurosci, V2, P6, DOI 10.3389/fnsyn.2010.00006
   Fino E, 2009, PLOS ONE, V4, DOI 10.1371/journal.pone.0006557
   Frémaux N, 2016, FRONT NEURAL CIRCUIT, V9, DOI 10.3389/fncir.2015.00085
   Gershman S. J., 2017, OXFORD HDB CAUSAL RE
   Gershman SJ, 2023, BIOSYSTEMS, V224, DOI 10.1016/j.biosystems.2022.104825
   Guergiuev J, 2016, ARXIV PREPRINT, V1610, P1
   Hagmayer Y, 2017, CAUSALITY DECISION M, V1, DOI [10.1093/oxfordhb/9780199399550.001.0001/oxfordhb-9780199399550-e-27, DOI 10.1093/OXFORDHB/9780199399550.001.0001/OXFORDHB-9780199399550-E-27]
   Hoel EP, 2013, P NATL ACAD SCI USA, V110, P19790, DOI 10.1073/pnas.1314922110
   Hoerzer GM, 2014, CEREB CORTEX, V24, P677, DOI 10.1093/cercor/bhs348
   Imbens G, 2012, REV ECON STUD, V79, P933, DOI 10.1093/restud/rdr043
   Imbens GW, 2008, J ECONOMETRICS, V142, P615, DOI 10.1016/j.jeconom.2007.05.001
   Jacob R., 2012, PRACTICAL GUIDE REGR
   Kanitscheider I, 2015, P NATL ACAD SCI USA, V112, pE6973, DOI 10.1073/pnas.1508738112
   Lansdell B., 2020, ICLR 2020, P1
   Le Pelley ME., 2017, CLIN INFECT DIS, V1, P1, DOI [10.1093/oxfordhb/9780199399550.001.0001/oxfordhb-9780199399550-e-2, DOI 10.1093/OXFORDHB/9780199399550.001.0001/OXFORDHB-9780199399550-E-2]
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Legenstein R, 2010, J NEUROSCI, V30, P8400, DOI 10.1523/JNEUROSCI.4284-09.2010
   Liakoni V, 2021, NEURAL COMPUT, V33, P269, DOI 10.1162/neco_a_01352
   Loewenstein Y, 2006, P NATL ACAD SCI USA, V103, P15224, DOI 10.1073/pnas.0505220103
   Miconi T, 2017, ELIFE, V6, DOI 10.7554/eLife.20899
   Neftci EO, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00324
   Pawlak Verena, 2010, Front Synaptic Neurosci, V2, P146, DOI 10.3389/fnsyn.2010.00146
   Pearl J., 2000, CAUSALITY MODELS REA
   Peters J, 2017, ADAPT COMPUT MACH LE
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Purves D., 2014, BOECK SINAUER SUNDER
   Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278
   Rosas FE, 2020, PLOS COMPUT BIOL, V16, DOI 10.1371/journal.pcbi.1008289
   Schultz W, 2002, NEURON, V36, P241, DOI 10.1016/S0896-6273(02)00967-4
   Seol GH, 2007, NEURON, V55, P919, DOI 10.1016/j.neuron.2007.08.013
   Seung HS, 2003, NEURON, V40, P1063, DOI 10.1016/S0896-6273(03)00761-X
   Shafi M, 2007, NEUROSCIENCE, V146, P1082, DOI 10.1016/j.neuroscience.2006.12.072
   Shea-Brown E, 2008, PHYS REV LETT, V100, DOI 10.1103/PhysRevLett.100.108102
   Shrestha SB, 2018, ADV NEUR IN, V31
   Sjöström PJ, 2004, J NEUROPHYSIOL, V92, P3338, DOI 10.1152/jn.00376.2004
   Tang G, 2019, ARXIV
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   THEUNISSEN F, 1995, J COMPUT NEUROSCI, V2, P149, DOI 10.1007/BF00961885
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696
   Wise RA, 2004, NAT REV NEUROSCI, V5, P483, DOI 10.1038/nrn1406
   Woodward J., 2007, CAUSAL LEARNING PSYC, P19, DOI 10.1093/acprof:oso/9780195176803.003.0002
   Xie XH, 2004, PHYS REV E, V69, DOI 10.1103/PhysRevE.69.041909
   Xu HA, 2021, PLOS COMPUT BIOL, V17, DOI 10.1371/journal.pcbi.1009070
   Yim MY, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1002254
   Zenke F, 2018, NEURAL COMPUT, V30, P1514, DOI 10.1162/neco_a_01086
NR 66
TC 1
Z9 1
U1 3
U2 4
PD APR
PY 2023
VL 19
IS 4
AR e1011005
DI 10.1371/journal.pcbi.1011005
UT WOS:000964485300002
DA 2023-11-16
ER

PT J
AU Wang, ZR
   Liu, JW
   Ma, YQ
   Chen, BD
   Zheng, NN
   Ren, PJ
AF Wang, Ziru
   Liu, Jiawen
   Ma, Yongqiang
   Chen, Badong
   Zheng, Nanning
   Ren, Pengju
TI Perturbation of Spike Timing Benefits Neural Network Performance on
   Similarity Search
SO IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
DT Article
DE Neurons; Perturbation methods; Encoding; Oscillators; Biological neural
   networks; Biological information theory; Olfactory; Locality sensitive
   hashing (LSH); spiking neural network (SNN); temporal perturbation
ID PATTERN; MODEL; CLASSIFICATION; RECEPTORS; PRECISION; ALGORITHM;
   GABA(A); NEURONS; NMDA; AMPA
AB Perturbation has a positive effect, as it contributes to the stability of neural systems through adaptation and robustness. For example, deep reinforcement learning generally engages in exploratory behavior by injecting noise into the action space and network parameters. It can consistently increase the agent's exploration ability and lead to richer sets of behaviors. Evolutionary strategies also apply parameter perturbations, which makes network architecture robust and diverse. Our main concern is whether the notion of synaptic perturbation introduced in a spiking neural network (SNN) is biologically relevant or if novel frameworks and components are desired to account for the perturbation properties of artificial neural systems. In this work, we first review part of the locality-sensitive hashing (LSH) of similarity search, the FLY algorithm, as recently published in Science, and propose an improved architecture, time-shifted spiking LSH (TS-SLSH), with the consideration of temporal perturbations of the firing moments of spike pulses. Experiment results show promising performance of the proposed method and demonstrate its generality to various spiking neuron models. Therefore, we expect temporal perturbation to play an active role in SNN performance.
C1 [Wang, Ziru; Liu, Jiawen; Ma, Yongqiang; Chen, Badong; Zheng, Nanning; Ren, Pengju] Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian 710049, Peoples R China.
RP Ren, PJ (corresponding author), Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian 710049, Peoples R China.
EM pengjuren@mail.xjtu.edu.cn
CR Andoni A, 2006, ANN IEEE SYMP FOUND, P459
   Andoni A, 2008, PROCEEDINGS OF THE NINETEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P343
   Andrew A. M, 2003, KYBERNETES, V32, P277
   [Anonymous], 2018, 2018 IEEE INTELLIGEN, DOI gp574f
   Bair W, 1996, NEURAL COMPUT, V8, P1185, DOI 10.1162/neco.1996.8.6.1185
   BenAri Y, 1997, TRENDS NEUROSCI, V20, P523, DOI 10.1016/S0166-2236(97)01147-8
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Borst A, 1999, NAT NEUROSCI, V2, P947, DOI 10.1038/14731
   Brader JM, 2007, NEURAL COMPUT, V19, P2881, DOI 10.1162/neco.2007.19.11.2881
   Butts DA, 2007, NATURE, V449, P92, DOI [10.1038/nature06105, 10.1038/natureO6105]
   Calude CS, 2016, NAT COMPUT, V15, P263, DOI 10.1007/s11047-015-9533-2
   Caron SJC, 2013, NATURE, V497, P113, DOI 10.1038/nature12063
   Dasgupta S, 2017, SCIENCE, V358, P793, DOI 10.1126/science.aam9868
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   Hu J, 2013, NEURAL COMPUT, V25, P450, DOI 10.1162/NECO_a_00395
   Ivkovic M, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0035029
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Jin X, 2010, COMPUT SCI ENG, V12, P91, DOI 10.1109/MCSE.2010.112
   Kempter R, 1999, ADV NEUR IN, V11, P125
   Khosruf M. A, 2004, SPIKING NEURAL NETWO
   Koepsell K, 2009, FRONT SYST NEUROSCI, V3, DOI 10.3389/neuro.06.004.2009
   KUEBLER ES, 2013, PROC INT JOINT C NEU, P1
   Lin Y, 2013, PROC CVPR IEEE, P446, DOI 10.1109/CVPR.2013.64
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   MAINEN ZF, 1995, SCIENCE, V268, P1503, DOI 10.1126/science.7770778
   Masquelier T, 2009, NEURAL COMPUT, V21, P1259, DOI 10.1162/neco.2008.06-08-804
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Nadasdy Z, 2009, FRONT SYST NEUROSCI, V3, DOI 10.3389/neuro.06.006.2009
   Panzeri S, 2010, TRENDS NEUROSCI, V33, P111, DOI 10.1016/j.tins.2009.12.001
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Salimans T., 2017, ARXIV170303864
   Thivierge JP, 2008, J NEUROSCI, V28, P7968, DOI 10.1523/JNEUROSCI.0870-08.2008
   Uzzell VJ, 2004, J NEUROPHYSIOL, V92, P780, DOI 10.1152/jn.01171.2003
   van Rossum MCW, 2001, NEURAL COMPUT, V13, P751, DOI 10.1162/089976601300014321
   Wade JJ, 2010, IEEE T NEURAL NETWOR, V21, P1817, DOI 10.1109/TNN.2010.2074212
   Wu SH, 2004, J NEUROSCI, V24, P4625, DOI 10.1523/JNEUROSCI.0318-04.2004
   Wysoski SG, 2008, NEUROCOMPUTING, V71, P2563, DOI 10.1016/j.neucom.2007.12.038
   Yu Q, 2014, NEUROCOMPUTING, V138, P3, DOI 10.1016/j.neucom.2013.06.052
   Yu QF, 2013, PLOS ONE, V8, DOI [10.1371/journal.pone.0059627, 10.1371/journal.pone.0078318]
NR 43
TC 3
Z9 3
U1 2
U2 13
PD SEP
PY 2022
VL 33
IS 9
BP 4361
EP 4372
DI 10.1109/TNNLS.2021.3056694
EA FEB 2021
UT WOS:000732394100001
DA 2023-11-16
ER

PT J
AU Marks, S
AF Marks, Stefan
TI Immersive visualisation of 3-dimensional spiking neural networks
SO EVOLVING SYSTEMS
DT Article
DE Spiking neural network; 3-Dimensional; Visualisation; Virtual reality;
   Immersive
ID BRAIN
AB Recent development in artificial neural networks has led to an increase in performance, but also in complexity and size. This poses a significant challenge for the exploration and analysis of the spatial structure and temporal behaviour of such networks. Several projects for the 3D visualisation of neural networks exist, but they focus largely on the exploration of the spatial structure alone, and are using standard 2D screens as output and mouse and keyboard as input devices. In this article, we present NeuVis, a framework for an intuitive and immersive 3D visualisation of spiking neural networks in virtual reality, allowing for a larger variety of input and output devices. We apply NeuVis to NeuCube, a 3-dimensional spiking neural network learning framework, significantly improving the user's abilities to explore, analyse, and also debug the network. Finally, we discuss further venues of development and alternative render methods that are currently under development and will increase the visual accuracy and realism of the visualisation, as well as further extending its analysis and exploration capabilities.
C1 [Marks, Stefan] Auckland Univ Technol, 55 Wellesley St East, Auckland 1010, New Zealand.
RP Marks, S (corresponding author), Auckland Univ Technol, 55 Wellesley St East, Auckland 1010, New Zealand.
EM smarks@aut.ac.nz
CR Armstrong JD, 2009, PHILOS T R SOC A, V367, P2387, DOI 10.1098/rsta.2008.0308
   Bruckner S, 2009, IEEE T VIS COMPUT GR, V15, P1497, DOI 10.1109/TVCG.2009.121
   Foottit J, 2014, 10 AUSTR C INT ENT I, DOI 10.1145/2677758.2677774
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   Kasabov Nikola, 2012, Artificial Neural Networks in Pattern Recognition. Proceedings of the 5th INNS IAPR TC 3 GIRPR Workshop, ANNPR 2012, P225, DOI 10.1007/978-3-642-33212-8_21
   Kasabov N, 2016, NEURAL NETWORKS, V78, P1, DOI 10.1016/j.neunet.2015.09.011
   Kolasinski EM, 1995, TECH REP
   Lancaster JL, 1997, HUM BRAIN MAPP, V5, P238, DOI 10.1002/(SICI)1097-0193(1997)5:4<238::AID-HBM6>3.0.CO;2-4
   Lancaster JL, 2000, HUM BRAIN MAPP, V10, P120, DOI 10.1002/1097-0193(200007)10:3<120::AID-HBM30>3.0.CO;2-8
   Lin CY, 2011, IEEE PAC VIS SYMP, P35, DOI 10.1109/PACIFICVIS.2011.5742370
   Maciel P. W. C., 1995, Proceedings 1995 Symposium on Interactive 3D Graphics, P95, DOI 10.1145/199404.199420
   Oculus VR, 2015, TECH REP
   Oculus VR, 2016, OCULUS RIFT
   Ridder Md, 2015, BIG DATA VISUAL ANAL, P1, DOI [10.1109/BDVA.2015.7314293, DOI 10.1109/BDVA.2015.7314293]
   Scott Nathan, 2013, Neural Information Processing. 20th International Conference, ICONIP 2013. Proceedings: LNCS 8228, P78, DOI 10.1007/978-3-642-42051-1_11
   Sherif T, 2015, FRONT NEUROINFORM, V8, DOI 10.3389/fninf.2014.00089
   Von Kapri A, 2011, STUD HEALTH TECHNOL, V163, P685, DOI 10.3233/978-1-60750-706-2-685
   Wejchert J, 1990, ADV NEURAL INFORMATI, V2, P465
   Xia MR, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0068910
NR 19
TC 6
Z9 6
U1 0
U2 2
PD SEP
PY 2017
VL 8
IS 3
BP 193
EP 201
DI 10.1007/s12530-016-9170-8
UT WOS:000408572700003
DA 2023-11-16
ER

PT J
AU Niu, LY
   Wei, Y
AF Niu, Li-Ye
   Wei, Ying
TI CIRM-SNN: Certainty Interval Reset Mechanism Spiking Neuron for Enabling
   High Accuracy Spiking Neural Network
SO NEURAL PROCESSING LETTERS
DT Article
DE Spiking neural network; Continuous normalization; Certainty interval
   reset; Firing rate; Modulation factor
ID PLASTICITY
AB Spiking neural network (SNN) based on sparse trigger and event-driven information processing has the advantages of ultra-low power consumption and hardware friendliness. As a new generation of neural networks, SNN is widely concerned. At present, the most effective way to realize deep SNN is through artificial neural network (ANN) conversion. Compared with the original ANN, the converted SNN suffers from performance loss. This paper adjusts the spike firing rate of spiking neurons to minimize the performance loss of SNN in the conversion process. We map the ANN weights to the corresponding SNN after continuous normalization, which ensures that the spike firing rate of the neuron is in the normal range. We propose a certainty interval reset mechanism (CIRM), which effectively reduces the loss of membrane potential and avoids the problem of neuronal over-activation. In the experiment, we added a modulation factor to the CIRM to further adjust the spike firing rate of neurons. The accuracy of the converted SNN on CIFAR-10 is 1.026% higher than that of the original ANN. The algorithm not only achieves the lossless conversion of ANN, but also reduces the network energy consumption. Our algorithm also effectively improves the accuracy of SNN (VGG-15) on CIFAR-100 and decreases the network delay. The work of this paper is of great significance for developing high-precision depth SNN.
C1 [Niu, Li-Ye; Wei, Ying] Northeastern Univ, Coll Informat Sci & Engn, Shenyang 110819, Peoples R China.
   [Wei, Ying] Peking Univ, Informat Technol R&D Innovat Ctr, Shaoxing, Peoples R China.
RP Wei, Y (corresponding author), Northeastern Univ, Coll Informat Sci & Engn, Shenyang 110819, Peoples R China.; Wei, Y (corresponding author), Peking Univ, Informat Technol R&D Innovat Ctr, Shaoxing, Peoples R China.
EM weiying@ise.neu.edu.cn
CR Beauchemin SS, 2012, IEEE T INSTRUM MEAS, V61, P391, DOI 10.1109/TIM.2011.2164854
   Bing Han, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13555, DOI 10.1109/CVPR42600.2020.01357
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Chen YH, 2022, NEUROCOMPUTING, V469, P189, DOI 10.1016/j.neucom.2021.10.080
   Cohen GK, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00184
   Diehl PU, 2015, IEEE IJCNN
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Esser SK, 2016, P NATL ACAD SCI USA, V113, P11441, DOI 10.1073/pnas.1604850113
   FUKUSHIMA K, 1980, BIOL CYBERN, V36, P193, DOI 10.1007/BF00344251
   Hu ZH, 2017, LECT NOTES COMPUT SC, V10635, P92, DOI 10.1007/978-3-319-70096-0_10
   Hunsberger E, 2015, ARXIV, DOI DOI 10.48550/ARXIV.1510.08829
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Lee JG, 2017, KOREAN J RADIOL, V18, P570, DOI 10.3348/kjr.2017.18.4.570
   Lee JM, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00191
   Legenstein R, 2010, J NEUROSCI, V30, P8400, DOI 10.1523/JNEUROSCI.4284-09.2010
   Legenstein R, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.1000180
   Li JL, 2017, LECT NOTES COMPUT SC, V10635, P294, DOI 10.1007/978-3-319-70096-0_31
   Liu J., 2018, 2018 IEEE INT C COMP, P1, DOI [10.1109/ICSPCC.2018.8567796, DOI 10.1109/WCSP.2018.8555665, DOI 10.1109/COMPEM.2018.8496497]
   Lu S, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00535
   Masquelier T, 2007, PLOS COMPUT BIOL, V3, P247, DOI 10.1371/journal.pcbi.0030031
   Mostafa H, 2018, IEEE T NEUR NET LEAR, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Mozafari M, 2019, PATTERN RECOGN, V94, P87, DOI 10.1016/j.patcog.2019.05.015
   Neftci E, 2014, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00272
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Neftci EO, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00324
   Neil D, 2016, P 31 ANN ACM S APPL
   Neil D, 2016, IEEE INT SYMP CIRC S, P2282, DOI 10.1109/ISCAS.2016.7539039
   Neil D, 2014, IEEE T VLSI SYST, V22, P2621, DOI 10.1109/TVLSI.2013.2294916
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   Panchev C, 2004, NEUROCOMPUTING, V58, P365, DOI 10.1016/j.neucom.2004.01.068
   Panda P, 2016, IEEE IJCNN, P299, DOI 10.1109/IJCNN.2016.7727212
   Pouladzadeh P, 2014, IEEE T INSTRUM MEAS, V63, P1947, DOI 10.1109/TIM.2014.2303533
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Seiffert U, 2002, STUD FUZZ SOFT COMP, V78, P45
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Shirmohammadi S, 2014, IEEE INSTRU MEAS MAG, V17, P41, DOI 10.1109/MIM.2014.6825388
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Tan WH, 2021, AAAI CONF ARTIF INTE, V35, P9816
   Tavanaei A, 2017, IEEE IJCNN, P2023, DOI 10.1109/IJCNN.2017.7966099
   Thiele JC, 2018, IEEE IJCNN
   Wang JJ, 2022, NANOSCALE, V14, P1318, DOI 10.1039/d1nr06144e
   Wu J, 2019, IEEE
   Wu YJ, 2019, AAAI CONF ARTIF INTE, P1311
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Xiao R, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1445
   Yu M, 2018, 2018 INT JOINT C NEU
   Zenke F, 2018, NEURAL COMPUT, V30, P1514, DOI 10.1162/neco_a_01086
   Zhang D, 2020, P NATL ACAD SCI USA, V117, P20254, DOI 10.1073/pnas.1914869117
   Zhang L, 2019, AAAI CONF ARTIF INTE, P1319
   Zhang ML, 2022, IEEE T NEUR NET LEAR, V33, P1947, DOI 10.1109/TNNLS.2021.3110991
   Zhang ML, 2020, IEEE J-STSP, V14, P592, DOI 10.1109/JSTSP.2020.2983547
   Zheng H, 2021, P AAAI C ART INT, V35
NR 52
TC 0
Z9 0
U1 6
U2 6
PD DEC
PY 2023
VL 55
IS 6
BP 7561
EP 7582
DI 10.1007/s11063-023-11274-5
EA APR 2023
UT WOS:000968701400001
DA 2023-11-16
ER

PT C
AU Valko, M
   Marques, NC
   Castellani, M
AF Valko, Michal
   Marques, Nuno C.
   Castellani, Marco
BE Bento, C
   Cardoso, A
   Dias, G
TI Evolutionary feature selection for spiking neural network pattern
   classifiers
SO 2005 Portuguese Conference on Artificial Intelligence, Proceedings
DT Proceedings Paper
CT Portuguese Conference on Artificial Intelligence
CY 2005
CL Univ Beira Interior, Covilha, PORTUGAL
HO Univ Beira Interior
DE artificial intelligence; neural networks; genetic algorithms; spiking
   neuron models; JASTAP
ID TIME
AB This paper presents an application of the biologically realistic JASTAP neural network model to classification tasks. The JASTAP neural network model is presented as an alternative to the basic multi-layer perceptron model. An evolutionary procedure previously applied to the simultaneous solution of feature selection and neural network training on standard multi-layer perceptrons is extended with JASTAP model. Preliminary results on IRIS standard data set give evidence that this extension allows the use of smaller neural networks that can handle noisier data without any degradation in classification accuracy.
C1 Comenius Univ, Fac Math Phys & Informat, Inst Appl Informat, Dept Artificial Intelligence, Bratislava 84248, Slovakia.
RP Valko, M (corresponding author), Comenius Univ, Fac Math Phys & Informat, Inst Appl Informat, Dept Artificial Intelligence, Bratislava 84248, Slovakia.
CR Abbott L.F., 1999, NEURAL CODES DISTRIB
   [Anonymous], BIOPHYSICS COMPUTATI
   Arbib M., 2003, HDB BRAIN THEORY NEU
   ASTELLANI M, 2004, TECHNICAL REPORT EVO
   ASTELLANI M, 2004, TECHNICAL NEURAL NET
   Blum AL, 1997, ARTIF INTELL, V97, P245, DOI 10.1016/S0004-3702(97)00063-5
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   CASTELLANI M, 2005, EPIA 05 12 PORT C AR
   Fishman GS., 1996, MONTE CARLO CONCEPTS, V1
   GARCEZ A, 2004, J APPL LOGIC EDITORI, V2, P241
   GERSTNER W, 1995, PHYS REV E, V51, P738, DOI 10.1103/PhysRevE.51.738
   Hettich C.B.S, 1998, UCI REPOSITORY MACHI
   Hitzler P., 2004, J APPL LOGIC, V3, P245, DOI DOI 10.1016/J.JAL.2004.03.002
   JANCO J, 1994, COMPUT ARTIF INTELL, V13, P603
   MAASS W, 1999, PULSED NEURAL NETWOR, V1
   MARQUES NC, 2001, SPRINGER LECT NOTES, V2189, P63
   MITCHELL TM, 1997, ACHINE LEARNING
   Pavlásek J, 2003, ACTA NEUROBIOL EXP, V63, P83
   Pavlásek J, 2001, BIOLOGIA, V56, P591
   REDMAN S, 1983, J PHYSIOL-LONDON, V343, P117, DOI 10.1113/jphysiol.1983.sp014884
   RMELHART DE, 1986, PARALLEL DISTRIBUTED, V1, P318
   Singer W, 1999, CURR OPIN NEUROBIOL, V9, P189, DOI 10.1016/S0959-4388(99)80026-9
   VALKO M, 2005, THESIS COMENIUS U BR
NR 23
TC 1
Z9 1
U1 0
U2 0
PY 2005
BP 181
EP 187
DI 10.1109/EPIA.2005.341291
UT WOS:000245387100031
DA 2023-11-16
ER

PT J
AU Neves, FS
   Timme, M
AF Neves, Fabio Schittler
   Timme, Marc
TI Reconfigurable Computation in Spiking Neural Networks
SO IEEE ACCESS
DT Article
DE Analog computing; coupled oscillators; network dynamics; nonlinear
   dynamics; spiking neural networks; winner-takes-all; heteroclinic
   dynamics
ID SYNCHRONIZATION; POPULATIONS; DYNAMICS
AB The computation of rank ordering plays a fundamental role in cognitive tasks and offers a basic building block for computing arbitrary digital functions. Spiking neural networks have been demonstrated to be capable of identifying the largest k out of N analog input signals through their collective nonlinear dynamics. By finding partial rank orderings, they perform k-winners-take-all computations. Yet, for any given study so far, the value of k is fixed, often to k equal one. Here we present a concept for spiking neural networks that are capable of (re)configurable computation by choosing k via one global system parameter. The spiking network acts via pulse-suppression induced by inhibitory pulse-couplings. Couplings are proportional to each units' state variable (neuron voltage), constituting an uncommon but straightforward type of leaky integrate-and-fire neural network. The result of a computation is encoded as a stable periodic orbit with k units spiking at some frequency and others at lower frequency or not at all. Orbit stability makes the resulting analog-to-digital computation robust to sufficiently small variations of both, parameters and signals. Moreover, the computation is completed quickly within a few spike emissions per neuron. These results indicate how reconfigurable k -winners-take-all computations may be implemented and effectively exploited in simple hardware relying only on basic dynamical units and spike interactions resembling simple current leakages to a common ground.
C1 [Neves, Fabio Schittler; Timme, Marc] Ctr Adv Elect Dresden Cfaed, Chair Network Dynam, D-01062 Dresden, Germany.
   [Neves, Fabio Schittler; Timme, Marc] Tech Univ Dresden, Inst Theoret Phys, D-01062 Dresden, Germany.
   [Timme, Marc] Tech Univ Dresden, Cluster Excellence Phys Life, D-01062 Dresden, Germany.
   [Timme, Marc] Lakeside Labs, A-9020 Klagenfurt Am Worthersee, Austria.
RP Timme, M (corresponding author), Ctr Adv Elect Dresden Cfaed, Chair Network Dynam, D-01062 Dresden, Germany.; Timme, M (corresponding author), Tech Univ Dresden, Inst Theoret Phys, D-01062 Dresden, Germany.; Timme, M (corresponding author), Tech Univ Dresden, Cluster Excellence Phys Life, D-01062 Dresden, Germany.; Timme, M (corresponding author), Lakeside Labs, A-9020 Klagenfurt Am Worthersee, Austria.
EM marc.timme@tu-dresden.de
CR Abbott LF, 2016, NAT NEUROSCI, V19, P350, DOI 10.1038/nn.4241
   Ashwin P, 2004, PHYS REV E, V70, DOI 10.1103/PhysRevE.70.026203
   Ashwin P, 2005, PHYS LETT A, V347, P208, DOI 10.1016/j.physleta.2005.08.013
   Ashwin P, 2005, NATURE, V436, P36, DOI 10.1038/436036b
   Bridewell W., 2016, P 4 ANN C ADV COGN S, P1
   Chen YQ, 2017, FRONT COMPUT NEUROSC, V11, DOI 10.3389/fncom.2017.00020
   Hansel D, 1998, NEURAL COMPUT, V10, P467, DOI 10.1162/089976698300017845
   Hopfield JJ, 2015, NEURAL COMPUT, V27, P2011, DOI 10.1162/NECO_a_00768
   Kielblock H, 2011, CHAOS, V21, DOI 10.1063/1.3589960
   Klinglmayr J, 2012, NEW J PHYS, V14, DOI 10.1088/1367-2630/14/7/073031
   Krupa M, 1997, J NONLINEAR SCI, V7, P129, DOI 10.1007/BF02677976
   Laing CR, 2001, NEURAL COMPUT, V13, P1473, DOI 10.1162/089976601750264974
   Lukosevicius M, 2009, COMPUT SCI REV, V3, P127, DOI 10.1016/j.cosrev.2009.03.005
   Maass W, 2000, NEURAL COMPUT, V12, P2519, DOI 10.1162/089976600300014827
   Maass W., 1999, PULSED NEURAL NETWOR
   McKinstry JL, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0162155
   Memmesheimer RM, 2006, PHYS REV LETT, V97, DOI 10.1103/PhysRevLett.97.188101
   Memmesheimer RM, 2006, PHYSICA D, V224, P182, DOI 10.1016/j.physd.2006.09.037
   MIROLLO RE, 1990, SIAM J APPL MATH, V50, P1645, DOI 10.1137/0150098
   Neves FS, 2017, CHAOS, V27, DOI 10.1063/1.4977552
   Neves FS, 2012, PHYS REV LETT, V109, DOI 10.1103/PhysRevLett.109.018701
   Neves FS, 2009, J PHYS A-MATH THEOR, V42, DOI 10.1088/1751-8113/42/34/345103
   Rabinovich M, 2001, PHYS REV LETT, V87, DOI 10.1103/PhysRevLett.87.068102
   Rabinovich M, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0064406
   Rabinovich MI, 2006, REV MOD PHYS, V78, P1213, DOI 10.1103/RevModPhys.78.1213
   Sandamirskaya Y, 2014, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00276
   Schöner G, 2008, CAMB HANDB PSYCHOL, P101
   Steingrube S, 2010, NAT PHYS, V6, P224, DOI 10.1038/NPHYS1508
   Strogatz SH, 2000, PHYSICA D, V143, P1, DOI 10.1016/S0167-2789(00)00094-4
   Strub C, 2017, FRONT COMPUT NEUROSC, V11, DOI 10.3389/fncom.2017.00074
   Timme M, 2003, CHAOS, V13, P377, DOI 10.1063/1.1501274
   Timme M, 2002, PHYS REV LETT, V89, DOI 10.1103/PhysRevLett.89.154105
   Wang JJ, 2019, APPL PHYS LETT, V115, DOI 10.1063/1.5120973
   WINFREE AT, 1967, J THEOR BIOL, V16, P15, DOI 10.1016/0022-5193(67)90051-3
   Wordsworth J, 2008, PHYS REV E, V78, DOI 10.1103/PhysRevE.78.066203
NR 35
TC 9
Z9 9
U1 1
U2 15
PY 2020
VL 8
BP 179648
EP 179655
DI 10.1109/ACCESS.2020.3027966
UT WOS:000577883600001
DA 2023-11-16
ER

PT C
AU Saleh, AY
   Shamsuddin, SM
   Hamed, HNA
AF Saleh, Abdulrazak Yahya
   Shamsuddin, Siti Mariyam
   Hamed, Haza Nuzly Abdull
BE Chbeir, R
   Manolopoulos, Y
   Maglogiannis, I
   Alhajj, R
TI Multi-Objective Differential Evolution of Evolving Spiking Neural
   Networks for Classification Problems
SO Artificial Intelligence Applications and Innovations
SE IFIP Advances in Information and Communication Technology
DT Proceedings Paper
CT 11th IFIP WG 12.5 International Conference on Artificial Intelligence
   Applications and Innovations (AIAI)
CY SEP 14-17, 2015
CL Bayonne, FRANCE
DE Differential Evolution; Evolutionary algorithms; Evolving spiking neural
   networks; Multi objective Optimization
ID OF-THE-ART; PATTERN-RECOGNITION; OPTIMIZATION; ALGORITHMS;
   BACKPROPAGATION; NEURONS
AB Spiking neural network (SNN) plays an essential role in classification problems. Although there are many models of SNN, Evolving Spiking Neural Network (ESNN) is widely used in many recent research works. Evolutionary algorithms, mainly differential evolution (DE) have been used for enhancing ESNN algorithm. However, many real-world optimization problems include several contradictory objectives. Rather than single optimization, Multi-Objective Optimization (MOO) can be utilized as a set of optimal solutions to solve these problems. In this paper, MOO is used in a hybrid learning of ESNN to determine the optimal pre-synaptic neurons (network structure) and accuracy performance for classification problems simultaneously. Standard data sets from the UCI machine learning are used for evaluating the performance of this multi objective hybrid model. The experimental results have proved that the multi-objective hybrid of Differential Evolution with Evolving Spiking Neural Network (MODE-ESNN) gives better results in terms of accuracy and network structure.
C1 [Saleh, Abdulrazak Yahya; Shamsuddin, Siti Mariyam] Univ Teknol Malaysia, UTM Big Data Ctr, Skudai 81310, Johor, Malaysia.
   [Hamed, Haza Nuzly Abdull] Univ Teknol Malaysia, Fac Comp, Soft Comp Res Grp, Skudai 81310, Johor, Malaysia.
RP Saleh, AY (corresponding author), Univ Teknol Malaysia, UTM Big Data Ctr, Skudai 81310, Johor, Malaysia.
EM Abdulrazakalhababi@gmail.com
CR Abbass H.A., 2003, 2003 C EV COMP CEC 2
   Abbass H.A., 2001, P 2001 C EV COMP
   Abbass HA, 2003, NEURAL COMPUT, V15, P2705, DOI 10.1162/089976603322385126
   Abdull Hamed H.N., 2009, LNCS, V5864, P611
   Ahmed FYH, 2013, MATH PROBL ENG, V2013, DOI 10.1155/2013/257085
   Alcalá-Fdez J, 2011, J MULT-VALUED LOG S, V17, P255
   Ambroise C., 2004, ANAL MICROARRAY GENE, V14, P1080
   [Anonymous], 2005, SCALABLE TEST PROBLE
   [Anonymous], 2006, MULTIOBJECTIVE MACHI
   [Anonymous], 2004, METAHEURISTICS MULTI
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Chandra A, 2004, LECT NOTES COMPUT SC, V3177, P619
   CHARNES A, 1957, MANAGE SCI, V4, P38, DOI 10.1287/mnsc.4.1.38
   Cohon J.L, 1978, MULTIOBJECTIVE PROGR
   Das S, 2011, IEEE T EVOLUT COMPUT, V15, P4, DOI 10.1109/TEVC.2010.2059031
   Derrac J., 2011, 2011 7 INT C NEXT GE
   Fieldsend JE, 2005, IEEE T NEURAL NETWOR, V16, P338, DOI 10.1109/TNN.2004.841794
   García S, 2015, INTEL SYST REF LIBR, V72, P285, DOI 10.1007/978-3-319-10247-4_10
   Gerstner W., 2002, SPIKING NEURON MODEL
   Hamed, 2012, NOVEL INTEGRATED MET
   Igel C, 2005, LECT NOTES COMPUT SC, V3410, P534
   Ijiri Y., 1965, MANAGEMENT GOALS ACC, V3
   Jin Y., 2004, C EV COMP CEC 2004
   Jin YC, 2007, LECT NOTES COMPUT SC, V4668, P370
   Kasabov N., 2012, LNCS, V7311, P234
   Kasabov N, 2014, NEUROCOMPUTING, V134, P269, DOI 10.1016/j.neucom.2013.09.049
   Kasabov N, 2013, NEURAL NETWORKS, V41, P188, DOI 10.1016/j.neunet.2012.11.014
   Kasabov NK, 2014, NEURAL NETWORKS, V52, P62, DOI 10.1016/j.neunet.2014.01.006
   King RTFA, 2005, LECT NOTES COMPUT SC, V3410, P677
   Marler RT, 2004, STRUCT MULTIDISCIP O, V26, P369, DOI 10.1007/s00158-003-0368-6
   Mezura-Montes E, 2008, STUD COMPUT INTELL, V143, P173
   PHILIPSON RH, 1978, J MECH DES-T ASME, V100, P286, DOI 10.1115/1.3453913
   Qasem SN, 2011, APPL SOFT COMPUT, V11, P5565, DOI 10.1016/j.asoc.2011.05.002
   Saleh A.Y., 2014, INT J ADV SOFT COMPU, V6
   Saleh A.Y., 2014, INT C REC TRENDS INF, P13
   Schaffer J. D., 1985, P 1 INT C GEN ALG PI, P93
   Schliebs S, 2013, EVOL SYST-GER, V4, P87, DOI 10.1007/s12530-013-9074-9
   Schliebs S, 2009, NEURAL NETWORKS, V22, P623, DOI 10.1016/j.neunet.2009.06.038
   Steuer R.E., 1986, MULTIPLE CRITERIA OP
   Tan KC, 2001, IEEE T EVOLUT COMPUT, V5, P565, DOI 10.1109/4235.974840
   Thorpe S., 1997, CAN HUMAN VISUAL SYS
   Wysoski SG, 2006, LECT NOTES COMPUT SC, V4131, P61
   Yen GG, 2003, IEEE T EVOLUT COMPUT, V7, P253, DOI 10.1109/TEVC.2003.810068
NR 43
TC 5
Z9 5
U1 0
U2 4
PY 2015
VL 458
BP 351
EP 368
DI 10.1007/978-3-319-23868-5_25
UT WOS:000380534000025
DA 2023-11-16
ER

PT C
AU Hamilton, KE
   Imam, N
   Humble, TS
AF Hamilton, Kathleen E.
   Imam, Neena
   Humble, Travis S.
GP ACM
TI Community detection with spiking neural networks for neuromorphic
   hardware
SO PROCEEDINGS OF NEUROMORPHIC COMPUTING SYMPOSIUM (NCS 2017)
DT Proceedings Paper
CT Neuromorphic Computing Symposium
CY JUL 17-19, 2017
CL Knoxville, TN
DE neuromorphic; community detection; spiking neural networks
ID NEURONS
AB We present results related to the performance of an algorithm for community detection which incorporates event-driven computation. We define a mapping which takes a graph G to a system of symmetrically connected, spiking neurons and use spike train similarities to identify vertex communities. On a random graph with 128 vertices and known community structure we show how our approach can be used to identify individual communities from spiking neuron responses.
C1 [Hamilton, Kathleen E.; Imam, Neena; Humble, Travis S.] Oak Ridge Natl Lab, Comp & Computat Sci Dir, Oak Ridge, TN 37830 USA.
RP Hamilton, KE (corresponding author), Oak Ridge Natl Lab, Comp & Computat Sci Dir, Oak Ridge, TN 37830 USA.
EM hamiltonke@ornl.gov; imamn@ornl.gov; humblets@ornl.gov
CR [Anonymous], 2013, 2013 INT JOINT C NEU
   [Anonymous], 1991, SANTA FE I STUDIES S
   [Anonymous], 2008, P ACM SIGKDD INT C K, DOI DOI 10.1145/1401890.1401925
   Blatt M, 1996, PHYS REV LETT, V76, P3251, DOI 10.1103/PhysRevLett.76.3251
   Boccaletti S, 2006, PHYS REP, V424, P175, DOI 10.1016/j.physrep.2005.10.009
   Clauset A, 2004, PHYS REV E, V70, DOI 10.1103/PhysRevE.70.066111
   Fortunato S, 2010, PHYS REP, V486, P75, DOI 10.1016/j.physrep.2009.11.002
   Gerstner W., 2002, SPIKING NEURON MODEL
   Girvan M, 2002, P NATL ACAD SCI USA, V99, P7821, DOI 10.1073/pnas.122653799
   Goodman Dan, 2008, Front Neuroinform, V2, P5, DOI 10.3389/neuro.11.005.2008
   Hamilton Kathleen E., 2017, SIAMS NETW SCI WORKS
   HOPFIELD JJ, 1985, BIOL CYBERN, V52, P141
   HOPFIELD JJ, 1984, P NATL ACAD SCI-BIOL, V81, P3088, DOI 10.1073/pnas.81.10.3088
   HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554
   Humphries MD, 2011, J NEUROSCI, V31, P2321, DOI 10.1523/JNEUROSCI.2853-10.2011
   Kanamaru T, 2000, PHYS REV E, V62, P2629, DOI 10.1103/PhysRevE.62.2629
   Kanamaru T, 2001, PHYS REV E, V64, DOI 10.1103/PhysRevE.64.031904
   Lancichinetti A, 2009, PHYS REV E, V80, DOI 10.1103/PhysRevE.80.016118
   Lancichinetti A, 2008, PHYS REV E, V78, DOI 10.1103/PhysRevE.78.046110
   LOWEL S, 1992, SCIENCE, V255, P209, DOI 10.1126/science.1372754
   Maass W, 1997, NETWORK-COMP NEURAL, V8, P355, DOI 10.1088/0954-898X/8/4/002
   Maass W, 2001, PULSED NEURAL NETWOR
   Malliaros FD, 2013, PHYS REP, V533, P95, DOI 10.1016/j.physrep.2013.08.002
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Newman ME., 2010, NETWORKS INTRO OXFOR
   Newman MEJ, 2004, PHYS REV E, V69, DOI 10.1103/PhysRevE.69.026113
   Peel L., 2016, ARXIV161205001
   Peel L, 2017, SCI ADV, V3, DOI 10.1126/sciadv.1602548
   Raghavan UN, 2007, PHYS REV E, V76, DOI 10.1103/PhysRevE.76.036106
   Reichardt J, 2004, PHYS REV LETT, V93, DOI 10.1103/PhysRevLett.93.218701
   Reichardt J, 2006, PHYS REV E, V74, DOI 10.1103/PhysRevE.74.016110
   Santo Fortunato, 2017, S FORT WEBS SOFTW
   Schaub Michael T, 2017, Appl Netw Sci, V2, P4, DOI 10.1007/s41109-017-0023-6
   Tanaka Hideki., 2005, INT S NONL THEOR ITS
   Tibély G, 2008, PHYSICA A, V387, P4982, DOI 10.1016/j.physa.2008.04.024
   Van den Bout D E, 1990, IEEE Trans Neural Netw, V1, P192, DOI 10.1109/72.80231
NR 36
TC 3
Z9 3
U1 0
U2 0
PY 2017
DI 10.1145/3183584.3183621
UT WOS:000788641400009
DA 2023-11-16
ER

PT J
AU Cessac, B
   Paugam-Moisy, H
   Viéville, T
AF Cessac, Bruno
   Paugam-Moisy, Helene
   Vieville, Thierry
TI Overview of facts and issues about neural coding by spikes
SO JOURNAL OF PHYSIOLOGY-PARIS
DT Article; Proceedings Paper
CT 2nd French Conference in Computational Neuroscience
CY OCT 08-11, 2008
CL Marseille, FRANCE
DE Spiking neuron networks; Neural code; Time constraints; Spike train
   metrics
ID TIMING-DEPENDENT PLASTICITY; SPIKING NEURONS; FIRE NEURONS; NETWORKS;
   MODEL; TIME; SIMULATION; COMPUTATION; CODES; RULE
AB In the present overview, our wish is to demystify some aspects of coding with spike-timing, through a simple review of well-understood technical facts regarding spike coding. Our goal is a better understanding of the extent to which computing and modeling with spiking neuron networks might be biologically plausible and computationally efficient.
   We intentionally restrict ourselves to a deterministic implementation of spiking neuron networks and we consider that the dynamics of a network is defined by a non-stochastic mapping. By staying in this rather simple framework, we are able to propose results, formula and concrete numerical values, on several topics: (i) general time constraints, (ii) links between continuous signals and spike trains, (iii) spiking neuron networks parameter adjustment. Beside an argued review of several facts and issues about neural coding by spikes, we propose new results, such as a numerical evaluation of the most critical temporal variables that schedule the progress of realistic spike trains.
   When implementing spiking neuron networks, for biological simulation or computational purpose, it is important to take into account the indisputable facts here unfolded. This precaution could prevent one from implementing mechanisms that would be meaningless relative to obvious time constraints, or from artificially introducing spikes when continuous calculations would be sufficient and more simple. It is also pointed out that implementing a large-scale spiking neuron network is finally a simple task. (C) 2009 Elsevier Ltd. All rights reserved.
C1 [Cessac, Bruno] LJAD, F-06108 Nice, France.
   [Paugam-Moisy, Helene] INRIA TAO, LRI, F-91405 Orsay, France.
   [Cessac, Bruno] INRIA NeuroMathComp, F-06902 Sophia Antipolis, France.
   [Vieville, Thierry] INRIA Cortex, F-54600 Villers Les Nancy, France.
RP Cessac, B (corresponding author), LJAD, Parc Valrose, F-06108 Nice, France.
EM bruno.cessac@inln.cnrs.fr
CR Amitai Y, 2002, J NEUROSCI, V22, P4142, DOI 10.1523/JNEUROSCI.22-10-04142.2002
   ARONOV D, 2003, J NEUROSCIENCE METHO, V124
   BAUDOT P, 2007, THESIS
   Bohte SM, 2007, NEURAL COMPUT, V19, P371, DOI 10.1162/neco.2007.19.2.371
   Brette R, 2005, J NEUROPHYSIOL, V94, P3637, DOI 10.1152/jn.00686.2005
   Brette R, 2007, J COMPUT NEUROSCI, V23, P349, DOI 10.1007/s10827-007-0038-6
   BURNOD Y, 1993, ADAPTIVE NEURAL NETW
   CAMERA GL, 2008, BIOL CYBERN, V99, P279
   CAMERA GL, 2008, BIOL CYBERN, V99, P303
   Carandini M, 2005, J NEUROSCI, V25, P10577, DOI 10.1523/JNEUROSCI.3726-05.2005
   Carandini M, 2000, J NEUROSCI, V20, P470, DOI 10.1523/JNEUROSCI.20-01-00470.2000
   Cessac B, 2008, J MATH BIOL, V56, P311, DOI 10.1007/s00285-007-0117-3
   Cessac B, 2009, J STAT PHYS, V136, P565, DOI 10.1007/s10955-009-9786-1
   CESSAC B, 2009, RR6924 INRIA HAL
   Cessac B, 2008, FRONT COMPUT NEUROSC, V2, DOI 10.3389/neuro.10.002.2008
   Chechik G, 2003, NEURAL COMPUT, V15, P1481, DOI 10.1162/089976603321891774
   Cooper L., 2004, THEORY CORTICAL PLAS
   Cronin J., 1987, MATH ASPECTS HODGKIN, DOI DOI 10.1017/CBO9780511983955
   CROOK S, 1998, NEURAL COMPUTATION, V10
   Dayan P., 2001, THEORETICAL NEUROSCI
   Delorme A, 2001, NEUROCOMPUTING, V38, P539, DOI 10.1016/S0925-2312(01)00403-9
   Destexhe A, 2003, NAT REV NEUROSCI, V4, P739, DOI 10.1038/nrn1198
   Destexhe A, 1997, NEURAL COMPUT, V9, P503, DOI 10.1162/neco.1997.9.3.503
   FITZGIBBON W, 1996, J DIFFERENTIAL EQUAT
   FREGNAC Y, 2003, EUR C VIS PERC PAR
   FREGNAC Y, 2004, PROG BIOCHEM BIOPHYS, V31, P6
   Galarreta M, 2001, NAT REV NEUROSCI, V2, P425, DOI 10.1038/35077566
   Gautrais J, 1998, BIOSYSTEMS, V48, P57, DOI 10.1016/S0303-2647(98)00050-1
   GERSTEIN GL, 1964, BIOPHYS J, V4, P41, DOI 10.1016/S0006-3495(64)86768-0
   Gerstner W, 2002, BIOL CYBERN, V87, P404, DOI 10.1007/s00422-002-0353-y
   Gerstner W., 2002, SPIKING NEURON MODEL
   GRAY CM, 1989, P NATL ACAD SCI USA, V86, P1698, DOI 10.1073/pnas.86.5.1698
   Guyonneau R, 2004, J PHYSIOL-PARIS, V98, P487, DOI 10.1016/j.jphysparis.2005.09.004
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2007, DYNAMICAL SYSTEMS NE
   Jaeger H., 2003, ADV NEURAL INFORM PR, V15, P609
   Katok A., 1998, INTRO MODERN THEORY
   Kirst C, 2009, PHYS REV LETT, V102, DOI 10.1103/PhysRevLett.102.068101
   Koch C., 1998, METHODS NEURONAL MOD
   Koch Christof, 1999, P1
   KREUZ T, 2007, COMP NEUR M CNS
   Lazar AA, 2005, NEUROCOMPUTING, V65, P401, DOI 10.1016/j.neucom.2004.10.034
   Lewis TJ, 2003, J COMPUT NEUROSCI, V14, P283, DOI 10.1023/A:1023265027714
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Maass W, 1997, NETWORK-COMP NEURAL, V8, P355, DOI 10.1088/0954-898X/8/4/002
   Maass W, 1997, NEURAL COMPUT, V9, P279, DOI 10.1162/neco.1997.9.2.279
   Maass W, 1999, INFORM COMPUT, V153, P26, DOI 10.1006/inco.1999.2806
   Maass W, 2001, THEOR COMPUT SCI, V261, P157, DOI 10.1016/S0304-3975(00)00137-7
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maass W., 2009, COMPUTABILITY CONTEX
   Maass W, 2003, PULSED NEURAL NETWOR
   MAINEN ZF, 1995, SCIENCE, V268, P1503, DOI 10.1126/science.7770778
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   MARTINEZ R, 2008, P 2 FRENCH C COMP NE, P265
   McCormick DA, 1997, ANNU REV NEUROSCI, V20, P185, DOI 10.1146/annurev.neuro.20.1.185
   MEUNIER D, 2008, ADV COMPUTATIONAL IN, P367
   Morrison A, 2005, NEURAL COMPUT, V17, P1776, DOI 10.1162/0899766054026648
   MOURAUD A, 2006, INT C PAR DISTR COMP, P393
   PAR D, 1990, EXPT BRAIN RES, V80
   Paugam-Moisy H, 2010, HDB NATURAL COMPUTIN
   Paugam-Moisy H, 2008, NEUROCOMPUTING, V71, P1143, DOI 10.1016/j.neucom.2007.12.027
   Pfister JP, 2006, J NEUROSCI, V26, P9673, DOI 10.1523/JNEUROSCI.1425-06.2006
   POLITI A, 2009, STABLE CHAOS
   Rauch A, 2003, J NEUROPHYSIOL, V90, P1598, DOI 10.1152/jn.00293.2003
   Rieke F., 1996, SPIKES EXPLORING NEU
   ROSTROGONZALEZ H, 2009, COMP NEUR M CNS
   Rudolph M, 2007, NEUROCOMPUTING, V70, P1966, DOI 10.1016/j.neucom.2006.10.138
   Schäfer AM, 2006, LECT NOTES COMPUT SC, V4131, P632
   Schrauwen B, 2007, THESIS U GENT BELGIU
   Shadlen Michael N., 1994, Current Opinion in Neurobiology, V4, P569, DOI 10.1016/0959-4388(94)90059-0
   Síma J, 2005, NEURAL COMPUT, V17, P2635, DOI 10.1162/089976605774320601
   Simoncelli EP, 2001, ANNU REV NEUROSCI, V24, P1193, DOI 10.1146/annurev.neuro.24.1.1193
   STEIN RB, 1965, BIOPHYS J, V5, P173, DOI 10.1016/S0006-3495(65)86709-1
   SWADLOW HA, 1992, J NEUROPHYSIOL, V68, P605, DOI 10.1152/jn.1992.68.2.605
   SWADLOW HA, 1985, J NEUROPHYSIOL, V54, P1346, DOI 10.1152/jn.1985.54.5.1346
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   Thorpe SJ, 2001, SCIENCE, V291, P260, DOI 10.1126/science.1058249
   Touboul J, 2008, BIOL CYBERN, V99, P319, DOI 10.1007/s00422-008-0267-4
   Toyoizumi T, 2005, P NATL ACAD SCI USA, V102, P5239, DOI 10.1073/pnas.0500495102
   Toyoizumi T, 2007, NEURAL COMPUT, V19, P639, DOI 10.1162/neco.2007.19.3.639
   Verstraeten D, 2007, NEURAL NETWORKS, V20, P391, DOI 10.1016/j.neunet.2007.04.003
   Victor JD, 2005, CURR OPIN NEUROBIOL, V15, P585, DOI 10.1016/j.conb.2005.08.002
   Victor JD, 1996, J NEUROPHYSIOL, V76, P1310, DOI 10.1152/jn.1996.76.2.1310
   Viéville T, 2004, J COMPUT NEUROSCI, V17, P271, DOI 10.1023/B:JCNS.0000044873.20850.9c
NR 87
TC 45
Z9 80
U1 1
U2 20
PD JAN-MAR
PY 2010
VL 104
IS 1-2
BP 5
EP 18
DI 10.1016/j.jphysparis.2009.11.002
UT WOS:000276123600002
DA 2023-11-16
ER

PT J
AU Song, ZW
   Xiang, SY
   Ren, ZX
   Han, GQ
   Hao, Y
AF Song, Ziwei
   Xiang, Shuiying
   Ren, Zhenxing
   Han, Genquan
   Hao, Yue
TI Spike Sequence Learning in a Photonic Spiking Neural Network Consisting
   of VCSELs-SA With Supervised Training
SO IEEE JOURNAL OF SELECTED TOPICS IN QUANTUM ELECTRONICS
DT Article
DE Photonics; Neurons; Vertical cavity surface emitting lasers;
   Neuromorphics; Biological neural networks; Supervised learning;
   Encoding; Photonic spiking neural network; vertical-cavity
   surface-emitting lasers; photonic spike-timing-dependent plasticity;
   supervised spike sequence learning
ID TIMING-DEPENDENT PLASTICITY; INHIBITORY DYNAMICS; IMPLEMENTATION;
   ALGORITHMS; SUBJECT; LASERS
AB We propose a fully-connected photonic spiking neural network (SNN) consisting of excitable vertical-cavity surface-emitting lasers with an embedded saturable absorber (VCSELs-SA) to implement spike sequence learning by a supervised training. The photonic spike-timing-dependent plasticity (STDP) is incorporated into a classical remote supervised method (ReSuMe) algorithm to implement supervised training of a photonic SNN for the first time. The computation model of the photonic SNN is derived based on the Yamada model. To optimize the learning process, we further propose a novel measure, the so-called spike sequence distance, to quantitatively evaluate the effects of controllable parameters. The numerical results show that, the photonic SNN successfully reproduces a desirable output spike sequence in response to a spatiotemporal input spike pattern by means of the iteration algorithm to update synaptic weights continuously. These results contribute one step forward toward the device-algorithm co-design and optimization of the all-VCSELs-based energy-efficient photonic SNN.
C1 [Song, Ziwei; Xiang, Shuiying; Ren, Zhenxing] Xidian Univ, State Key Lab Integrated Serv Networks, Xian 710071, Peoples R China.
   [Xiang, Shuiying; Han, Genquan; Hao, Yue] Xidian Univ, Sch Microelect, State Key Discipline Lab Wide Bandgap Semicond Te, Xian 710071, Peoples R China.
RP Xiang, SY (corresponding author), Xidian Univ, State Key Lab Integrated Serv Networks, Xian 710071, Peoples R China.
EM 1064971297@qq.com; syxiang@xidian.edu.cn; 584401206@qq.com;
   gqhan@xidian.edu.cn; yhao@xidian.edu.cn
CR Basheer IA, 2000, J MICROBIOL METH, V43, P3, DOI 10.1016/S0167-7012(00)00201-3
   Benner AF, 2005, IBM J RES DEV, V49, P755, DOI 10.1147/rd.494.0755
   Bohte Sander M., 2004, Natural Computing, V3, P195, DOI 10.1023/B:NACO.0000027755.02868.60
   Boybat I, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-04933-y
   Cheng ZG, 2017, SCI ADV, V3, DOI 10.1126/sciadv.1700160
   Coomans W, 2011, PHYS REV E, V84, DOI 10.1103/PhysRevE.84.036209
   Deng T, 2017, IEEE J SEL TOP QUANT, V23, DOI 10.1109/JSTQE.2017.2685140
   Fok MP, 2013, OPT LETT, V38, P419, DOI 10.1364/OL.38.000419
   Gholipour B, 2015, ADV OPT MATER, V3, P635, DOI 10.1002/adom.201400472
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Hu SG, 2019, J PHYS D APPL PHYS, V52, DOI 10.1088/1361-6463/ab1a10
   Hurtado A, 2012, APPL PHYS LETT, V100, DOI 10.1063/1.3692726
   Hurtado A, 2010, OPT EXPRESS, V18, P25170, DOI 10.1364/OE.18.025170
   Javed F, 2010, AM J CLIN NUTR, V91, P907, DOI 10.3945/ajcn.2009.28512
   Jiang N, 2020, OPT EXPRESS, V28, P1999, DOI 10.1364/OE.385889
   Jörntell H, 2006, NEURON, V52, P227, DOI 10.1016/j.neuron.2006.09.032
   Li Q, 2016, PROC SPIE, V10019, DOI 10.1117/12.2245976
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   MEAD C, 1990, P IEEE, V78, P1629, DOI 10.1109/5.58356
   Nahmias MA, 2013, IEEE J SEL TOP QUANT, V19, DOI 10.1109/JSTQE.2013.2257700
   Nguyen THO, 2017, CLIN TRANSL IMMUNOL, V6, DOI 10.1038/cti.2017.4
   Park S, 2015, SCI REP-UK, V5, DOI 10.1038/srep10123
   Ponulak F., 2006, THESIS
   Ponulak F, 2011, ACTA NEUROBIOL EXP, V71, P409
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Prezioso M, 2015, NATURE, V521, P61, DOI 10.1038/nature14441
   Prucnal PR, 2016, ADV OPT PHOTONICS, V8, P228, DOI 10.1364/AOP.8.000228
   Ren QS, 2015, OPT EXPRESS, V23, P25247, DOI 10.1364/OE.23.025247
   Robertson J, 2020, IEEE J SEL TOP QUANT, V26, DOI 10.1109/JSTQE.2019.2931215
   Robertson J, 2017, OPT LETT, V42, P1560, DOI 10.1364/OL.42.001560
   Romeira B, 2016, SCI REP-UK, V6, DOI 10.1038/srep19510
   Roy K, 2019, NATURE, V575, P607, DOI 10.1038/s41586-019-1677-2
   Runyan CA, 2017, NATURE, V548, P92, DOI 10.1038/nature23020
   Selmi F, 2014, PHYS REV LETT, V112, DOI 10.1103/PhysRevLett.112.183902
   Seurin JF, 2016, PROC SPIE, V9766, DOI 10.1117/12.2213295
   Shastri BJ, 2016, SCI REP-UK, V6, DOI 10.1038/srep19126
   Shi J, 2013, NAT COMMUN, V4, DOI 10.1038/ncomms3676
   Song ZW, 2020, OPT EXPRESS, V28, P1561, DOI 10.1364/OE.381229
   Sourikopoulos I, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00123
   TANK DW, 1986, IEEE T CIRCUITS SYST, V33, P533, DOI 10.1109/TCS.1986.1085953
   Toole R, 2016, J LIGHTWAVE TECHNOL, V34, P470, DOI 10.1109/JLT.2015.2475275
   Toole R, 2015, OPT EXPRESS, V23, P16133, DOI 10.1364/OE.23.016133
   Turing AM., 1950, MIND, VLIX, P433
   Van Vaerenbergh T, 2012, OPT EXPRESS, V20, P20292, DOI 10.1364/OE.20.020292
   Victor JD, 1997, NETWORK-COMP NEURAL, V8, P127, DOI 10.1088/0954-898X/8/2/003
   Xiang SY, 2020, OPT LETT, V45, P1104, DOI 10.1364/OL.383942
   Xiang SY, 2019, IEEE J SEL TOP QUANT, V25, DOI 10.1109/JSTQE.2019.2911565
   Xiang SY, 2018, IEEE J QUANTUM ELECT, V54, DOI 10.1109/JQE.2018.2879484
   Xiang SY, 2018, J LIGHTWAVE TECHNOL, V36, P4227, DOI 10.1109/JLT.2018.2818195
   Xu W, 2016, SCI ADV, V2, DOI 10.1126/sciadv.1501326
   Zhang Y, 2019, OPT LETT, V44, P1548, DOI 10.1364/OL.44.001548
   Zhang YH, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-34537-x
   Zhang YH, 2018, APPL OPTICS, V57, P1731, DOI 10.1364/AO.57.001731
   Zhu JD, 2018, ADV MATER, V30, DOI 10.1002/adma.201800195
NR 54
TC 24
Z9 24
U1 4
U2 123
PD SEP-OCT
PY 2020
VL 26
IS 5
AR 1700209
DI 10.1109/JSTQE.2020.2975564
UT WOS:000520468600001
DA 2023-11-16
ER

PT J
AU Yellakuor, BE
   Moses, AA
   Zhen, Q
   Olaosebikan, OE
   Qin, ZG
AF Yellakuor, Baagyere Edward
   Moses, Agebure Apambila
   Zhen, Qin
   Olaosebikan, Oyetunji Elkanah
   Qin, Zhiguang
TI A Multi-Spiking Neural Network Learning Model for Data Classification
SO IEEE ACCESS
DT Article
DE Neurons; Biological neural networks; Biological system modeling; Data
   models; Encoding; Supervised learning; Task analysis; Multi-spiking
   neural network; supervised learning; temporal coding
ID GRADIENT DESCENT; NEURONS; RESUME
AB Classical Artificial Neural Networks (ANNs) though well exploited in solving classification problems, do not model perfectly the information encoding process in the human brain because ANNs encode information using rate-based coding. However, biological neurons in the brain are known to encode information using temporal coding. In order to mimic the biological method of encoding information, various Spiking Neural Network (SNN) models have been developed. However, some of these models are limited in the number of spikes and do not leverage well on some classification problems. In order to address some of the inherent challenges associated with SNN, a multi-layer learning model for a multi-spiking network is proposed in this paper. The model exploits the temporal coding of spikes and the least-squares method to derive a weight update scheme. It also employs a spike locality concept in order to determine how the synaptic weights are to be adjusted at a particular spike time so as to minimize the learning interference, and thereby, increasing the number of spikes for learning. The performance of the model is evaluated on benchmarked classification datasets. A correlation-based metric is combined with a threshold concept to measure the classification accuracy of the model. The experimental results showed that the proposed model achieved better classification accuracy than some state-of-the-art multi-layer SNN learning models.
C1 [Yellakuor, Baagyere Edward; Zhen, Qin; Qin, Zhiguang] Univ Elect Sci & Technol China, Sch Informat & Software Engn, Chengdu 610054, Peoples R China.
   [Yellakuor, Baagyere Edward; Moses, Agebure Apambila] Univ Dev Studies, Dept Comp Sci, Tamale Tl 1350, Ghana.
   [Olaosebikan, Oyetunji Elkanah] Lagos State Univ, Dept Mech Engn, Lagos 102101, Nigeria.
RP Qin, ZG (corresponding author), Univ Elect Sci & Technol China, Sch Informat & Software Engn, Chengdu 610054, Peoples R China.
EM qinzg@uestc.edu.cn
CR Apambila A. M., 2019, P IEEE AFRICON NOV
   Belatreche A, 2006, NEW MATH NAT COMPUT, V2, P237, DOI 10.1142/S179300570600049X
   Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   Booij O, 2005, INFORM PROCESS LETT, V95, P552, DOI 10.1016/j.ipl.2005.05.023
   Butts DA, 2007, NATURE, V449, P92, DOI [10.1038/nature06105, 10.1038/natureO6105]
   Dora S, 2016, NEUROCOMPUTING, V171, P1216, DOI 10.1016/j.neucom.2015.07.086
   Dua D., 2017, UCI MACHINE LEARNING
   FITZHUGH R, 1961, BIOPHYS J, V1, P445, DOI 10.1016/S0006-3495(61)86902-6
   Gerstner W., 2002, SPIKING NEURON MODEL
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   Hamed H, 2010, AUST J INTEL INF PRO, V11, P23
   Hebb DO, 1950, J CLIN PSYCHOL, V6, P307
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lobo JL, 2018, NEURAL NETWORKS, V108, P1, DOI 10.1016/j.neunet.2018.07.014
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   McKennoch S, 2006, IEEE IJCNN, P3970
   Mostafa Hesham, 2018, IEEE Trans Neural Netw Learn Syst, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Pavlidis NG, 2005, IEEE IJCNN, P2190
   Pérez-Sánchez B, 2018, ARTIF INTELL REV, V49, P281, DOI 10.1007/s10462-016-9526-2
   Ponulak F., 2006, THESIS
   Ponulak F, 2011, ACTA NEUROBIOL EXP, V71, P409
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Rossi R. A., 2015, P AAAI MAR
   Sboev A, 2018, PROCEDIA COMPUT SCI, V123, P494, DOI 10.1016/j.procs.2018.01.075
   Schliebs S, 2013, EVOL SYST-GER, V4, P87, DOI 10.1007/s12530-013-9074-9
   Silva M, 2017, J VOICE, V31, P24, DOI 10.1016/j.jvoice.2016.02.019
   Sporea I, 2013, NEURAL COMPUT, V25, P473, DOI 10.1162/NECO_a_00396
   Taherkhani A, 2018, IEEE T NEUR NET LEAR, V29, P5394, DOI 10.1109/TNNLS.2018.2797801
   Taherkhani A, 2015, LECT NOTES COMPUT SC, V9490, P190, DOI 10.1007/978-3-319-26535-3_22
   Taherkhani A, 2015, IEEE T NEUR NET LEAR, V26, P3137, DOI 10.1109/TNNLS.2015.2404938
   Thorpe S, 1998, COMPUTATIONAL NEUROSCIENCE: TRENDS IN RESEARCH, P113
   Van Rullen R, 2001, NEURAL COMPUT, V13, P1255, DOI 10.1162/08997660152002852
   VanRullen R, 2005, TRENDS NEUROSCI, V28, P1, DOI 10.1016/j.tins.2004.10.010
   WOLBERG WH, 1990, P NATL ACAD SCI USA, V87, P9193, DOI 10.1073/pnas.87.23.9193
   Wysoski SG, 2006, LECT NOTES COMPUT SC, V4179, P1133
   Xin JG, 2001, IEEE IJCNN, P1772, DOI 10.1109/IJCNN.2001.938430
   Xu Y, 2013, NEURAL NETWORKS, V43, P99, DOI 10.1016/j.neunet.2013.02.003
   Zemouri R.A., 2019, NEURAL COMPUT APPL, P1
NR 40
TC 5
Z9 5
U1 0
U2 5
PY 2020
VL 8
BP 72360
EP 72371
DI 10.1109/ACCESS.2020.2985257
UT WOS:000530827300010
DA 2023-11-16
ER

PT C
AU Han, IS
AF Han, IS
GP IEEE
TI Biologically plausible VLSI neural network implementation with
   asynchronous neuron and spike-based synapse
SO Proceedings of the International Joint Conference on Neural Networks
   (IJCNN), Vols 1-5
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT IEEE International Joint Conference on Neural Networks (IJCNN 2005)
CY JUL 31-AUG 04, 2005
CL Montreal, CANADA
AB This paper describes a new asynchronous spike based neural networks VLSI implementation, inspired by the biological plausibility and low power requirement. The voltage-controlled linear conductance produces the synaptic function of multiplication, weight programming, and summation of synaptic spike currents for the neuron. The operation speed of synaptic computation is up to 300 Mega operations with a small power consumption of 33 microwatts. The overall power consumption can be less in real applications, as individual synapse only consumes the power when there is an active neural input.
   The neuron is based on multiple combinations of synapses and the HSPICE simulation demonstrates the asynchronous spike behavior of integration-and-firing with a refractory period. The advantages of asynchronous operation, removal of reference clocks, and low voltage operation are exhibited compared to previous pulse-based analogue-mixed neural networks VLSI.
   The asynchronous spike based neural networks in 0.18 mu m CMOS VLSI technology is proposed to provide the advantage of analogue-mixed neural network VLSI with small power consumption and no need for a synchronous operation.
C1 Univ Sheffield, Dept Elect & Elect Engn, Sheffield S1 3JD, S Yorkshire, England.
RP Han, IS (corresponding author), Univ Sheffield, Dept Elect & Elect Engn, Sheffield S1 3JD, S Yorkshire, England.
CR BARTOLOZZI C, 2004, P BICS 2004
   Bugmann G, 1997, BIOSYSTEMS, V40, P11, DOI 10.1016/0303-2647(96)01625-5
   Christodoulou C, 2002, NEURAL NETWORKS, V15, P891, DOI 10.1016/S0893-6080(02)00034-5
   Eckmiller R, 2004, LECT NOTES COMPUT SC, V3316, P10
   Floreano D., 2001, LNCS, P38
   Häusser M, 2000, NAT NEUROSCI, V3, P1165, DOI 10.1038/81426
   HAN I, 1997, P EANN 97, P299
   HAN I, 1997, KOREA TELECOM J, V2, P12
   HAN I, 2004, P BICS 2004
   HAN I, 2004, Patent No. 105608
   LINARESBARRANCO B, 1991, IEEE JSSC, V26
   MATINOIA S, 2004, IEEE T BIOMEDICAL EN, V51, P859
   PEARCE T, 2004, P BICS 2004
   Simoni ME, 2004, IEEE T BIO-MED ENG, V51, P342, DOI 10.1109/TBME.2003.820390
   TAYLOR J, 2004, COMMUNICATION
   Taylor JG, 2003, PROG NEUROBIOL, V71, P305, DOI 10.1016/j.pneurobio.2003.10.002
   TAYLOR N, 2004, P BICS 2004
NR 17
TC 2
Z9 2
U1 0
U2 0
PY 2005
BP 3244
EP 3248
UT WOS:000235178004103
DA 2023-11-16
ER

PT C
AU Ji, X
   Zhang, YZ
   Li, CX
   Wu, TH
   Hu, XF
AF Ji, Xun
   Zhang, Yaozhong
   Li, Chuxi
   Wu, Tanghong
   Hu, Xiaofang
BE Liu, L
   Yang, C
   Ke, J
TI Reinforcement Learning in Memristive Spiking Neural Networks through
   Modulation of ReSuMe
SO ADVANCES IN MATERIALS, MACHINERY, ELECTRONICS III
SE AIP Conference Proceedings
DT Proceedings Paper
CT 3rd International Conference on Advances in Materials, Machinery,
   Electronics (AMME)
CY JAN 19-20, 2019
CL Wuhan, PEOPLES R CHINA
DE Reinforcement Learning; Spiking Neural Network; Remote Supervised
   Method; Memristor
AB In this paper, a novel hardware-friendly reinforcement learning algorithm based on memristive spiking neural networks (MSNN-RL) is proposed. Neurons for spike coding are designed specifically to complete transformation between analog data and discrete spikes. Then, remote supervised method (ReSuMe) is used to combine SNN with basic reforcement learing (Sarsa). Besides, bionic memristive snynapses are designed to speed up ReSuMe. Furthermore, the circuit scheme of MSNN-RL is designed with modulation of memristor synapses. Finally, the application of MSNN-RL in acrobot system is discussed. Simulation results and analysis verify the effectiveness of the proposed algorithm (MSNN-RL) and show it is superior to traditional apporach.
C1 [Ji, Xun; Zhang, Yaozhong; Li, Chuxi; Wu, Tanghong; Hu, Xiaofang] Southwest Univ, Coll Comp & Informat Sci, Chongqing 400715, Peoples R China.
RP Hu, XF (corresponding author), Southwest Univ, Coll Comp & Informat Sci, Chongqing 400715, Peoples R China.
EM Jixun_97@163.com; zhangyaozhong9@126.com; chuxi_li@163.com;
   tangdowney@foxmail.com; huxf@swu.edu.cn
CR Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Ferre P., 2018, FRONTIERS COMPUTATIO, V12, P1
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Goodman DFM, 2009, FRONT NEUROSCI-SWITZ, V3, P192, DOI 10.3389/neuro.01.026.2009
   Kvatinsky S, 2013, IEEE T CIRCUITS-I, V60, P211, DOI 10.1109/TCSI.2012.2215714
   Lilith N, 2005, REDUCED STATE SARSA
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Pershin YV, 2010, IEEE T CIRCUITS-I, V57, P1857, DOI 10.1109/TCSI.2009.2038539
   Richard S. S, 2005, REINFORCEMENT LEARNI, P182
   Serrano-Gotarredona T, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00002
   Strukov DB, 2008, NATURE, V453, P80, DOI 10.1038/nature06932
   Tang Hao, 2010, Acta Automatica Sinica, V36, P289, DOI 10.3724/SP.J.1004.2010.00289
   Yang JJS, 2013, NAT NANOTECHNOL, V8, P13, DOI [10.1038/nnano.2012.240, 10.1038/NNANO.2012.240]
   Zamarreño-Ramos C, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00026
NR 15
TC 1
Z9 1
U1 2
U2 26
PY 2019
VL 2073
AR 020094
DI 10.1063/1.5090748
UT WOS:000471324500094
DA 2023-11-16
ER

PT C
AU Beaulieu, PO
   Nabki, F
   Boukadoum, M
AF Beaulieu, Philippe-Olivier
   Nabki, Frederic
   Boukadoum, Mounir
GP IEEE
TI 8-bit Partial Magnitude Comparator for Spike Generation in the Leaky
   Integrate-and-Fire Neuron Model using Gate-Diffusion-Input Logic Gates
SO 2022 29TH IEEE INTERNATIONAL CONFERENCE ON ELECTRONICS, CIRCUITS AND
   SYSTEMS (IEEE ICECS 2022)
SE IEEE International Conference on Electronics Circuits and Systems
DT Proceedings Paper
CT 29th IEEE International Conference on Electronics, Circuits and Systems
   (IEEE ICECS)
CY OCT 24-26, 2022
CL Glasgow, SCOTLAND
DE Spiking Neural Networks; Magnitude; Comparator; Artificial Intelligence;
   Neuromorphic Processors
AB In an effort to reduce the power consumption required by artificial intelligence algorithms, many works take inspiration from biological neurons to develop spiking neural networks (SNN). The idea is to replace the currently used continuous neural output functions with low-energy spikes that are sparse in time. However, although possible in simulation, SNNs do not run efficiently on current hardware such as FPGAs, CPUs, or GPUs. This work presents a foundation for the development of a neuromorphic spiking neural network processor based on the Leaky Integrate-and-Fire (LIF) neuron model. A low-power digital spiking circuit is designed from an 8-bit partial magnitude comparator (PMC). The 8-bit PMC is made from modified Gate Diffusion Input (m-GDI) logic gates. When implemented in 65nm CMOS, it achieves an average power of 4.51 mu W at a 1.2 V supply, and an average delay of 80.32ps. The spiking circuit has an average power of 6.861 mu W and can operate at up to 14.28GHz. This spiking circuit is the first step towards the realization of a large-scale synchronous spiking neural network processor.
C1 [Beaulieu, Philippe-Olivier; Nabki, Frederic] Ecole Technol Super, Dept Elect Engn, Montreal, PQ, Canada.
   [Boukadoum, Mounir] Univ Quebec Montreal, Dept Comp Sci, Montreal, PQ, Canada.
RP Beaulieu, PO (corresponding author), Ecole Technol Super, Dept Elect Engn, Montreal, PQ, Canada.
EM philippe-olivier.beaulieu.1@ens.etsmtl.ca; frederic.nabki@etsmtl.ca;
   boukadoum.mounir@uqam.ca
CR Abbott LF, 1999, BRAIN RES BULL, V50, P303, DOI 10.1016/S0361-9230(99)00161-6
   Agrawal A, 2018, IEEE T CIRCUITS-I, V65, P4219, DOI 10.1109/TCSI.2018.2848999
   Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   Balasubramanian P, 2006, IEEE DTIS: 2006 INTERNATIONAL CONFERENCE ON DESIGN & TEST OF INTEGRATED SYSTEMS IN NANOSCALE TECHNOLOGY, PROCEEDINGS, P190, DOI 10.1109/DTIS.2006.1708713
   Bellec G, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-17236-y
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Fisher S, 2009, IEEE INT SYMP CIRC S, P1573, DOI 10.1109/ISCAS.2009.5118070
   Freund K., 2022, GRAI MATTER LABS BRA, P6
   Kasabov N., 2018, TIME SPACE SPIKING N, DOI 10.1007/978-3-662-57715-8
   Khanfir L, 2021, ANALOG INTEGR CIRC S, V106, P409, DOI 10.1007/s10470-020-01656-3
   Machupalli Madhusudhan Reddy, 2022, Intelligent Systems and Sustainable Computing: Proceedings of ICISSC 2021. Smart Innovation, Systems and Technologies (289), P335, DOI 10.1007/978-981-19-0011-2_31
   Morgenshtein A, 2002, IEEE T VLSI SYST, V10, P566, DOI 10.1109/TVLSI.2002.801578
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Pu JR, 2021, IEEE T CIRCUITS-I, V68, P5081, DOI 10.1109/TCSI.2021.3112979
   Sourikopoulos I, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00123
   Uenohara S, 2022, IEEE ACCESS, V10, P48338, DOI 10.1109/ACCESS.2022.3170579
   Wang N., TRAINING DEEP NEURAL, P10
NR 17
TC 0
Z9 0
U1 1
U2 1
PY 2022
DI 10.1109/ICECS202256217.2022.9970788
UT WOS:000913346300010
DA 2023-11-16
ER

PT J
AU Vu, HT
   Okuyama, Y
   Ben Abdallah, A
AF Huy-The Vu
   Okuyama, Yuichi
   Ben Abdallah, Abderazek
TI Analytical performance assessment and high-throughput low-latency spike
   routing algorithm for spiking neural network systems
SO JOURNAL OF SUPERCOMPUTING
DT Article
DE Analytical assessment; 3DNoC-SNN; Low-latency multicast algorithm;
   Spiking neural network
ID ON-CHIP; COMMUNICATION; CONNECTIVITY; ARCHITECTURE; CIRCUIT
AB Large-scale artificial neural networks (ANNs) have been used to mimic the information processing function of the brain. Spiking neural networks (SNNs) are a kind of ANN, which mimic real biological neural networks, conveying information through the communication of short pulses between neurons. Since each neuron in these networks is connected to thousands of others, high bandwidth is required. Moreover, since the spike times are used to encode information in SNN, very low communication latency is also required. The 2D-NoC was used as a solution to provide a scalable interconnection fabric in large-scale parallel SNN systems. The 3D-ICs have also attracted a lot of attention as a potential solution to resolve the interconnect bottleneck. The combination of these two emerging technologies provides a new horizon for IC designs to satisfy the high requirements of low-power and small footprint in emerging AI applications. This paper first presents an analytical model to analyze the performance of different neural network topologies and compare it with a system-level simulation. Second, we present an architecture and a low-latency routing algorithm for spike traffic routing in 3D-NoC of spiking neurons (3DNoC-SNN). The 3DNoC-SNN is validated based on an RTL-level implementation, while area/power analysis is performed using 45-nm CMOS technology.
C1 [Huy-The Vu; Okuyama, Yuichi; Ben Abdallah, Abderazek] Univ Aizu, Grad Sch Comp Sci & Engn, Adapt Syst Lab, Aizu Wakamatsu, Fukushima 9658580, Japan.
   [Huy-The Vu] Hung Yen Univ Technol & Educ, Hai Duong, Vietnam.
RP Vu, HT (corresponding author), Univ Aizu, Grad Sch Comp Sci & Engn, Adapt Syst Lab, Aizu Wakamatsu, Fukushima 9658580, Japan.; Vu, HT (corresponding author), Hung Yen Univ Technol & Educ, Hai Duong, Vietnam.
EM d8182106@u-aizu.ac.jp; okuyama@u-aizu.ac.jp; benab@u-aizu.ac.jp
CR Akbari N, 2017, IEEE INT C COMPUT, P9, DOI 10.1109/CSE-EUC.2017.188
   Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   Belhadj B, 2014, INT CONF COMPIL ARCH, DOI 10.1145/2656106.2656130
   Ben Ahmed A, 2016, J PARALLEL DISTR COM, V93-94, P30, DOI 10.1016/j.jpdc.2016.03.014
   BenAbdallah A., 2017, ADV MULTICORE SYSTEM
   Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Boahen KA, 2000, IEEE T CIRCUITS-II, V47, P416, DOI 10.1109/82.842110
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Carrillo S., 2012, 2012 Sixth IEEE/ACM International Symposium on Networks-on-Chip (NoCS), P83, DOI 10.1109/NOCS.2012.17
   Carrillo S, 2013, IEEE T PARALL DISTR, V24, P2451, DOI 10.1109/TPDS.2012.289
   Carrillo S, 2012, NEURAL NETWORKS, V33, P42, DOI 10.1016/j.neunet.2012.04.004
   Cawley S, 2011, GENET PROGRAM EVOL M, V12, P257, DOI 10.1007/s10710-011-9130-9
   Dang KN, 2020, IEEE T EMERG TOP COM, V8, P577, DOI 10.1109/TETC.2017.2762407
   Dang KN, 2016, ASIAN TEST SYMPOSIUM, P161, DOI 10.1109/ATS.2016.37
   Dong YP, 2010, J SEMICOND TECH SCI, V10, P28, DOI 10.5573/JSTS.2010.10.1.028
   Ebrahimi M, 2014, IEEE T COMPUT, V63, P718, DOI 10.1109/TC.2012.255
   Ehsan MA, 2017, 2017 IEEE INTERNATIONAL SYMPOSIUM ON ELECTROMAGNETIC COMPATIBILITY & SIGNAL/POWER INTEGRITY (EMCSI), P745, DOI 10.1109/ISEMC.2017.8077966
   Furber S, 2008, STUD COMPUT INTELL, V115, P763, DOI 10.1098/rsif.2006.0177
   Furber S, 2016, J NEURAL ENG, V13, DOI 10.1088/1741-2560/13/5/051001
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   Goldwyn JH, 2011, PHYS REV E, V83, DOI 10.1103/PhysRevE.83.041908
   Hojabr R, 2017, IEEE T COMPUT, V66, P1865, DOI 10.1109/TC.2017.2715158
   Lazzaro J., 1995, Proceedings. Sixteenth Conference on Advanced Research in VLSI, P158, DOI 10.1109/ARVLSI.1995.515618
   Legenstein R, 2007, NEURAL NETWORKS, V20, P323, DOI 10.1016/j.neunet.2007.04.017
   Levin J.A., 2014, Patent No. US, Patent No. [2014/0351190 A1, 20140351190]
   LIN XL, 1993, IEEE T PARALL DISTR, V4, P1105, DOI 10.1109/71.246072
   Liu JX, 2016, IEEE T CIRCUITS-I, V63, P2290, DOI 10.1109/TCSI.2016.2615051
   Liu SC, 2001, NEURAL NETWORKS, V14, P629, DOI 10.1016/S0893-6080(01)00054-5
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Markram H, 2006, NAT REV NEUROSCI, V7, P153, DOI 10.1038/nrn1848
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Monroe TB, 2017, J ALZHEIMERS DIS, V57, P71, DOI 10.3233/JAD-161187
   MORTARA A, 1995, IEEE J SOLID-ST CIRC, V30, P660, DOI 10.1109/4.387069
   Naveros F, 2015, IEEE T NEUR NET LEAR, V26, P1567, DOI 10.1109/TNNLS.2014.2345844
   Pande Sandeep, 2010, Proceedings 2010 International Symposium on System-on-Chip - SOC, P139, DOI 10.1109/ISSOC.2010.5625566
   Parhami B., 2013, COMPUTER SCI INFORM, V1, P165
   Pasero E, 2004, IEEE IJCNN, P3161
   Rojas R, 1996, NEURAL NETWORKS, P149, DOI 10.1007/978-3-642-61068-4{\_}7
   Samman FA, 2010, IEEE T VLSI SYST, V18, P1067, DOI 10.1109/TVLSI.2009.2019758
   Schemmel J, 2008, IEEE IJCNN, P431, DOI 10.1109/IJCNN.2008.4633828
   Strogatz SH, 2001, NATURE, V410, P268, DOI 10.1038/35065725
   Suzuki K, 2018, INF PROC SOC TOH BRA
   Vainbrand Dmitri, 2010, 2010 ACM/IEEE International Symposium on Networks-on-Chip (NOCS), P135, DOI 10.1109/NOCS.2010.23
   Vainbrand D, 2011, MICROPROCESS MICROSY, V35, P152, DOI 10.1016/j.micpro.2010.08.005
   Vu TH, 2018, INT CONF BIG DATA, P326, DOI 10.1109/BigComp.2018.00055
   Xiang D, 2016, ACM T DES AUTOMAT EL, V21, DOI 10.1145/2821506
   Yang SM, 2019, IEEE T CYBERNETICS, V49, P2490, DOI 10.1109/TCYB.2018.2823730
NR 47
TC 3
Z9 3
U1 1
U2 8
PD AUG
PY 2019
VL 75
IS 8
SI SI
BP 5367
EP 5397
DI 10.1007/s11227-019-02792-y
UT WOS:000485886700064
DA 2023-11-16
ER

PT J
AU Chen, YQ
   McKinstry, JL
   Edelman, GM
AF Chen, Yanqing
   McKinstry, Jeffrey L.
   Edelman, Gerald M.
TI Versatile networks of simulated spiking neurons displaying
   winner-take-all behavior
SO FRONTIERS IN COMPUTATIONAL NEUROSCIENCE
DT Article
DE brain-based computational model; spiking neuronal networks;
   winner-take-all; motor control and learning/plasticity;
   spike-timing-dependent plasticity; sensorimotor control; large-scale
   spiking neural networks; neurorobotics
ID LATERAL INHIBITION; VISUAL-CORTEX; MODEL; NEOCORTEX; PRINCIPLE;
   DIRECTION; CIRCUITS; DYNAMICS
AB We describe simulations of large-scale networks of excitatory and inhibitory spiking neurons that can generate dynamically stable winner-take-all (WTA) behavior. The network connectivity is a variant of center-surround architecture that we call center-annular-surround (CAS). In this architecture each neuron is excited by nearby neighbors and inhibited by more distant neighbors in an annular-surround region. The neural units of these networks simulate conductance-based spiking neurons that interact via mechanisms susceptible to both short-term synaptic plasticity and STDP. We show that such CAS networks display robust WTA behavior unlike the center-surround networks and other control architectures that we have studied. We find that a large-scale network of spiking neurons with separate populations of excitatory and inhibitory neurons can give rise to smooth maps of sensory input. In addition, we show that a humanoid brain-based-device (BBD) under the control of a spiking WTA neural network can learn to reach to target positions in its visual field, thus demonstrating the acquisition of sensorimotor coordination.
C1 [Chen, Yanqing; McKinstry, Jeffrey L.; Edelman, Gerald M.] Inst Neurosci, San Diego, CA USA.
   [McKinstry, Jeffrey L.] Point Loma Nazarene Univ, San Diego, CA USA.
RP McKinstry, JL (corresponding author), Inst Neurosci, 800 Silverado St,Suite 302, La Jolla, CA 92037 USA.
EM mckinstry@nsi.edu
CR [Anonymous], 1988, C PROGRAMMING LANGUA
   [Anonymous], 1984, SELF ORG ASS MEMORY
   Ben-Ari Y, 2012, FRONT CELL NEUROSCI, V6, DOI 10.3389/fncel.2012.00035
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Binshtok AM, 2006, J NEUROSCI, V26, P708, DOI 10.1523/JNEUROSCI.4409-05.2006
   Choe Y, 2004, BIOL CYBERN, V90, P75, DOI 10.1007/s00422-003-0435-5
   Crook JM, 1998, EUR J NEUROSCI, V10, P2056, DOI 10.1046/j.1460-9568.1998.00218.x
   Davison AP, 2006, J NEUROSCI, V26, P5604, DOI 10.1523/JNEUROSCI.5263-05.2006
   Dayan P., 2001, THEORETICAL NEUROSCI, P255
   Derdikman D, 2003, J NEUROSCI, V23, P3100
   Douglas RJ, 2004, ANNU REV NEUROSCI, V27, P419, DOI 10.1146/annurev.neuro.27.070203.144152
   Fino E, 2011, NEURON, V69, P1188, DOI 10.1016/j.neuron.2011.02.025
   GEORGOPOULOS AP, 1986, SCIENCE, V233, P1416, DOI 10.1126/science.3749885
   GOLDMANRAKIC PS, 1995, NEURON, V14, P477, DOI 10.1016/0896-6273(95)90304-6
   Haider B, 2010, NEURON, V65, P107, DOI 10.1016/j.neuron.2009.12.005
   Holmgren C, 2003, J PHYSIOL-LONDON, V551, P139, DOI 10.1113/jphysiol.2003.044784
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2008, P NATL ACAD SCI USA, V105, P3593, DOI 10.1073/pnas.0712231105
   Izhikevich EM, 2010, PHILOS T R SOC A, V368, P5061, DOI 10.1098/rsta.2010.0130
   Kaschube M, 2010, SCIENCE, V330, P1113, DOI 10.1126/science.1194869
   Kisvárday ZF, 2000, J NEUROSCI METH, V103, P91, DOI 10.1016/S0165-0270(00)00299-5
   Laing CR, 2001, NEURAL COMPUT, V13, P1473, DOI 10.1162/089976601750264974
   Lumer ED, 2000, NEURAL COMPUT, V12, P181, DOI 10.1162/089976600300015943
   Myme CIO, 2003, J NEUROPHYSIOL, V90, P771, DOI 10.1152/jn.00070.2003
   OBERMAYER K, 1990, P NATL ACAD SCI USA, V87, P8345, DOI 10.1073/pnas.87.21.8345
   Oster M, 2009, NEURAL COMPUT, V21, P2437, DOI 10.1162/neco.2009.07-08-829
   Perin R, 2011, P NATL ACAD SCI USA, V108, P5419, DOI 10.1073/pnas.1016051108
   Rutishauser U, 2011, NEURAL COMPUT, V23, P735, DOI 10.1162/NECO_a_00091
   Shriki O, 2003, NEURAL COMPUT, V15, P1809, DOI 10.1162/08997660360675053
   von der Malsburg C, 1973, Kybernetik, V14, P85
   Willmore B, 2001, NETWORK-COMP NEURAL, V12, P255, DOI 10.1088/0954-898X/12/3/302
   WILSON HR, 1973, KYBERNETIK, V13, P55, DOI 10.1007/BF00288786
   Wohrer A, 2009, J COMPUT NEUROSCI, V26, P219, DOI 10.1007/s10827-008-0108-4
NR 34
TC 11
Z9 11
U1 0
U2 12
PD MAR 19
PY 2013
VL 7
AR 16
DI 10.3389/fncom.2013.00016
UT WOS:000317574800001
DA 2023-11-16
ER

PT C
AU Cho, SG
   Beigné, E
   Zhang, ZY
AF Cho, Sung-Gun
   Beigne, Edith
   Zhang, Zhengya
GP IEEE
TI A 2048-Neuron Spiking Neural Network Accelerator with Neuro-Inspired
   Pruning and Asynchronous Network on Chip in 40nm CMOS
SO 2019 IEEE CUSTOM INTEGRATED CIRCUITS CONFERENCE (CICC)
SE IEEE Custom Integrated Circuits Conference
DT Proceedings Paper
CT 40th Annual IEEE Custom Integrated Circuits Conference (CICC)
CY APR 14-17, 2019
CL Austin, TX
DE spiking neural network; asynchronous network-on-chip; distance-based
   pruning; deadlock handling
ID SPARSE CODE
AB A 40nm, 2.56mm(2), 2048-neuron globally asynchronous locally synchronous (GALS) spiking neural network (SNN) chip is presented. For scalability, we allow neurons to specialize to excitatory or inhibitory, and apply distance-based pruning to cut communication and memory. An asynchronous router limits the latency to 1.32ns per hop. The reduced traffic and lower latency allow the input channel to be parallelized to achieve 7.85GSOP/s at 0.7V, consuming 5.9pJ/SOP.
C1 [Cho, Sung-Gun; Zhang, Zhengya] Univ Michigan, Ann Arbor, MI 48109 USA.
   [Beigne, Edith] Univ Grenoble Alpes, CEA LETI MINATEC, Grenoble, France.
RP Cho, SG (corresponding author), Univ Michigan, Ann Arbor, MI 48109 USA.
CR Beigne Edith, 2016, IEEE Solid-State Circuits Magazine, V8, P39, DOI 10.1109/MSSC.2016.2573864
   Buhler FN, 2017, SYMP VLSI CIRCUITS, pC30, DOI 10.23919/VLSIC.2017.8008536
   Chen G. K., 2018, 2018 S VLSI CIRC VLS
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   King PD, 2013, J NEUROSCI, V33, P5475, DOI 10.1523/JNEUROSCI.4188-12.2013
   Knag P, 2015, IEEE J SOLID-ST CIRC, V50, P1070, DOI 10.1109/JSSC.2014.2386892
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0
   Ooyen A., 2014, PLOS ONE, V9
   Vivet P, 2017, IEEE J SOLID-ST CIRC, V52, P33, DOI 10.1109/JSSC.2016.2611497
NR 10
TC 14
Z9 14
U1 2
U2 2
PY 2019
DI 10.1109/CICC.2019.8780116
UT WOS:000501010700001
DA 2023-11-16
ER

PT C
AU Fouda, ME
   Neftci, E
   Eltawil, A
   Kurdahi, E
AF Fouda, Mohammed E.
   Neftci, E.
   Eltawil, A.
   Kurdahi, E.
BE Matthews, MB
TI Effect of Asymmetric Nonlinearity Dynamics in RRAMs on Spiking Neural
   Network Performance
SO CONFERENCE RECORD OF THE 2019 FIFTY-THIRD ASILOMAR CONFERENCE ON
   SIGNALS, SYSTEMS & COMPUTERS
SE Conference Record of the Asilomar Conference on Signals Systems and
   Computers
DT Proceedings Paper
CT 53rd Asilomar Conference on Signals, Systems, and Computers (ACSSC)
CY NOV 03-06, 2019
CL Pacific Grove, CA
DE Spiking Neural Networks; Online Learning; RRAMs; Memristor;
   Nonidealities; Asymmetric Nonlinearity
AB Crossbar-based Resistive Random Access Memory (RRAM) array is a promising candidate for fast and efficient implementation of the vector-matrix multiplication, an essential step in a wide variety of workloads. However, several RRAM devices, demonstrating promising synaptic behaviors, are characterized by nonlinear and asymmetric update dynamics, which is a major obstacle for large-scale deployment in neural networks, especially for online learning tasks. In this work, we first introduce a memristive Spiking Neural Network (SNN) with local learning. Then, we study the effect of this asymmetric and nonlinear behavior on the spiking neural network performance and propose a method to overcome the performance degradation without extra nonlinearity cancellation hardware and read cycles. The performance of the proposed method approaches the baseline performance with 1 similar to 2% drop in recognition accuracy.
C1 [Fouda, Mohammed E.; Eltawil, A.; Kurdahi, E.] Univ Calif Irvine, Elect Engn & Comp Sci Dept, Irvine, CA 92697 USA.
   [Neftci, E.] Univ Calif Irvine, Cognit Sci Dept, Irvine, CA 92697 USA.
   [Eltawil, A.] King Abdullah Univ Sci & Technol, Thuwal, Saudi Arabia.
RP Fouda, ME (corresponding author), Univ Calif Irvine, Elect Engn & Comp Sci Dept, Irvine, CA 92697 USA.
EM foudam@uci.edu
CR Ambrogio S, 2018, NATURE, V558, P60, DOI 10.1038/s41586-018-0180-5
   [Anonymous], 2017, ARXIV171106756
   Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS)
   Cauwenberghs G, 2013, P NATL ACAD SCI USA, V110, P15512, DOI 10.1073/pnas.1313114110
   Dai YT, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-017-02527-8
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Fouda M., 2019, SPIKING NEURAL NETWO
   Fouda ME, 2019, IEEE T NANOTECHNOL, V18, P611, DOI 10.1109/TNANO.2018.2880734
   Friedmann S, 2017, IEEE T BIOMED CIRC S, V11, P128, DOI 10.1109/TBCAS.2016.2579164
   Gupta S, 2015, PR MACH LEARN RES, V37, P1737
   Li C, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351877
   Lillicrap TP, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms13276
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Neftci E. O., 2019, SIGNAL PROCESSING MA
   Neftci EO, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00324
   Nokland A., 2019, ARXIV190106656
   Park J, 2016, IEEE ELECTR DEVICE L, V37, P1559, DOI 10.1109/LED.2016.2622716
   Prezioso M, 2015, NATURE, V521, P61, DOI 10.1038/nature14441
   Puglisi FM, 2015, IEEE ELECTR DEVICE L, V36, P1030, DOI 10.1109/LED.2015.2464256
   Qiao N, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00141
   Woo J, 2018, IEEE NANOTECHNOL MAG, V12, P36, DOI 10.1109/MNANO.2018.2844902
   Xia QF, 2019, NAT MATER, V18, P309, DOI 10.1038/s41563-019-0291-x
   Yu SM, 2018, P IEEE, V106, P260, DOI 10.1109/JPROC.2018.2790840
NR 23
TC 5
Z9 5
U1 0
U2 2
PY 2019
BP 495
EP 499
DI 10.1109/ieeeconf44664.2019.9049043
UT WOS:000544249200097
DA 2023-11-16
ER

PT C
AU Ltaief, M
   Bezine, H
   Alimi, AM
AF Ltaief, Mahmoud
   Bezine, Hala
   Alimi, Adel M.
BE Abraham, A
   Haqiq, A
   Alimi, AM
   Mezzour, G
   Rokbani, N
   Muda, AK
TI A Spiking Neural Network Model with Fuzzy Learning Rate Application for
   Complex Handwriting Movements Generation
SO PROCEEDINGS OF THE 16TH INTERNATIONAL CONFERENCE ON HYBRID INTELLIGENT
   SYSTEMS (HIS 2016)
SE Advances in Intelligent Systems and Computing
DT Proceedings Paper
CT 16th International Conference on Hybrid Intelligent Systems (HIS) / 8th
   World Congress on Nature and Biologically Inspired Computing (NaBIC)
CY NOV 21-23, 2016
CL Marrakech, MOROCCO
DE Online handwriting; Beta-elliptic model; Spiking neural network; Fuzzy
   Learning Rate
ID NEURONS
AB In this paper a spiking neural network model with fuzzy learning rate for online complex handwriting movement generation is proposed. The network is composed of an input layer which uses a set of Beta-elliptic parameters as input, a hidden layer and an output layer dealing with the estimation of the script coordinates X(t) and Y (t). An additional input is used as a timing network to prepare the input parameters. We also propose a Fuzzy Learning Rate (FLR) for our spiking neural network. This rate is obtained by combining an Adaptive Learning Rate (ALR) with a fuzzy logic based supervisor. The obtained results showed the efficiency of the proposed fuzzy strategy for the online adjustment of the learning rate. Indeed, we have improved, indifferently from the initialization, the Neural Network training quality in terms of rapidity and precision. Similarity degree is measured between original and generated scripts to evaluate our model.
C1 [Ltaief, Mahmoud; Bezine, Hala; Alimi, Adel M.] Univ Sfax, ENIS, REsearch Grp Intelligent Machines, REGIM Lab, BP 1173, Sfax 3038, Tunisia.
RP Ltaief, M (corresponding author), Univ Sfax, ENIS, REsearch Grp Intelligent Machines, REGIM Lab, BP 1173, Sfax 3038, Tunisia.
EM mahmoud.ltaief@ieee.org
CR Alimi A. M., 2003, TASK Quarterly, V7, P23
   [Anonymous], INT J COMPUT SCI INF
   [Anonymous], 2001, HDB BIOL PHYS
   Bezine H, 2004, NINTH INTERNATIONAL WORKSHOP ON FRONTIERS IN HANDWRITING RECOGNITION, PROCEEDINGS, P515, DOI 10.1109/IWFHR.2004.45
   Bezine H, 2007, INT J PATTERN RECOGN, V21, P5, DOI 10.1142/S0218001407005272
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Ltaief M, 2012, INT C FRONT HANDWR R, P799
   Ltaief M., 2012, J INTELLIGENT LEARNI, V4, P256
   Natschläger T, 2002, THEOR COMPUT SCI, V287, P251, DOI 10.1016/S0304-3975(02)00099-3
   Schomaker L, 1998, ELECTRON COMMUN ENG, V10, P93, DOI 10.1049/ecej:19980302
   Schomaker L. R. B., 1991, THESIS
   Teulings H. L., 1986, 312 ESPRIT
   VanRullen R, 2005, TRENDS NEUROSCI, V28, P1, DOI 10.1016/j.tins.2004.10.010
NR 13
TC 1
Z9 1
U1 0
U2 0
PY 2017
VL 552
BP 403
EP 412
DI 10.1007/978-3-319-52941-7_40
UT WOS:000418854600040
DA 2023-11-16
ER

PT J
AU Yang, YK
   Ren, J
   Duan, F
AF Yang, Yikang
   Ren, Jia
   Duan, Feng
TI The Spiking Rates Inspired Encoder and Decoder for Spiking Neural
   Networks: An Illustration of Hand Gesture Recognition
SO COGNITIVE COMPUTATION
DT Article
DE Spiking neural network; Surface electromyography; Surface
   electromyography encoder; Spiking neural network decoder
ID CLASSIFICATION; INTELLIGENCE; INFORMATION; MODEL
AB The spiking neural network (SNN) is the third generation of artificial neural networks. The transmission and expression of information in SNN are performed by spike trains, making the SNN have the advantages of high calculation speed and low power consumption. Recently, researchers have employed the SNN to recognize surface electromyography (sEMG) signals, but problems are still left. The sEMG encoders may cause information loss, and the network decoders may cause poor training performance. The strength of the neuron stimulated can be expressed by the frequency of the input or output spikes (namely firing rate). Inspired by the firing rate principle, we proposed the smoothed frequency-domain decomposition encoder, which converts the sEMG to spike trains. Furthermore, we also proposed the network efferent energy decoder, which converts the network output to recognizing results. The employed SNN is a three-layer fully-connected network trained by the grey wolf optimizer. The proposed methods are verified by a hand gestures recognition task. A total of 11 subjects participated in the experiment, and sEMG signals were acquired from five commonly used hand gestures by three sEMG sensors. The results indicate that the loss function can be reduced to below 0.4, and the average gesture recognizing accuracy is 91.21%. These results show the potential of using the proposed methods for the actual prosthesis. In the future, we will optimize the SNN training method to improve the training speed and stability.
C1 [Yang, Yikang; Ren, Jia; Duan, Feng] Nankai Univ, Coll Artificial Intelligence, 38 Tongyan Rd, Tianjin 300350, Peoples R China.
RP Duan, F (corresponding author), Nankai Univ, Coll Artificial Intelligence, 38 Tongyan Rd, Tianjin 300350, Peoples R China.
EM yikangyang@mail.nankai.edu.cn; zhaozhao_151027@foxmail.com;
   duanf@nankai.edu.cn
CR Adrian ED, 1926, J PHYSIOL-LONDON, V61, P151, DOI 10.1113/jphysiol.1926.sp002281
   Albu F, 1997, INT C MICR COMP SCI, P131
   Behrenbeck J, 2019, J NEURAL ENG, V16, DOI 10.1088/1741-2552/aafabc
   Ceolini E, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00637
   Chen C, 2020, IEEE T BIO-MED ENG, V67, P3501, DOI 10.1109/TBME.2020.2989311
   Cheng L, 2021, IEEE T COGN DEV SYST, V13, P151, DOI 10.1109/TCDS.2019.2918228
   Christianini N., 2000, INTRO SUPPORT VECTOR, DOI 10.1017/CBO9780511801389
   Delorme A, 1999, NEUROCOMPUTING, V26-7, P989, DOI 10.1016/S0925-2312(99)00095-8
   Donati E, 2019, IEEE T BIOMED CIRC S, V13, P795, DOI 10.1109/TBCAS.2019.2925454
   Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504
   Hinton GE., 2012, IMPROVING NEURAL NET
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   HORNIK K, 1991, NEURAL NETWORKS, V4, P251, DOI 10.1016/0893-6080(91)90009-T
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Karnam NK, 2021, BIOMED SIGNAL PROCES, V70, DOI 10.1016/j.bspc.2021.102948
   Lee C, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00119
   Lemaire E, 2022, ACM T EMBED COMPUT S, V21, DOI 10.1145/3520133
   Lin ZT, 2018, NEUROCOMPUTING, V275, P94, DOI 10.1016/j.neucom.2017.05.009
   Liu Y, 2018, 2018 INT JOINT C NEU, P16
   Lobov SA, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20020500
   Lu ZY, 2019, J NEURAL ENG, V16, DOI 10.1088/1741-2552/ab0cf0
   Ma YQ, 2020, IEEE J EM SEL TOP C, V10, P578, DOI 10.1109/JETCAS.2020.3037951
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Menon R, 2017, IEEE T NEUR SYS REH, V25, P1832, DOI 10.1109/TNSRE.2017.2687761
   Mirjalili S, 2014, ADV ENG SOFTW, V69, P46, DOI 10.1016/j.advengsoft.2013.12.007
   Orchard G, 2015, IEEE T PATTERN ANAL, V37, P2028, DOI 10.1109/TPAMI.2015.2392947
   Pei J, 2019, NATURE, V572, P106, DOI 10.1038/s41586-019-1424-8
   ROSENBLATT F, 1958, PSYCHOL REV, V65, P386, DOI 10.1037/h0042519
   Roy K, 2019, NATURE, V575, P607, DOI 10.1038/s41586-019-1677-2
   Rumelhart D.E., 1986, PARALLEL DISTRIBUTED, V1, DOI [10.21236/ada164453, 10.1016/b978-1-4832-1446-7.50035-2]
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Tam S, 2020, IEEE T BIOMED CIRC S, V14, P232, DOI 10.1109/TBCAS.2019.2955641
   Xu H, 2021, IEEE SENS J, V21, P13019, DOI 10.1109/JSEN.2021.3068521
   Yahya U, 2020, NEURAL COMPUT APPL, V32, P1481, DOI 10.1007/s00521-018-3653-4
NR 34
TC 0
Z9 0
U1 2
U2 13
PD JUL
PY 2023
VL 15
IS 4
BP 1257
EP 1272
DI 10.1007/s12559-022-10027-1
EA MAY 2022
UT WOS:000803780800001
DA 2023-11-16
ER

PT J
AU Hamilton, KE
   Imam, N
   Humble, TS
AF Hamilton, Kathleen E.
   Imam, Neena
   Humble, Travis S.
TI Sparse Hardware Embedding of Spiking Neuron Systems for Community
   Detection
SO ACM JOURNAL ON EMERGING TECHNOLOGIES IN COMPUTING SYSTEMS
DT Article
DE Optimization; community detection; neural network; graph algorithm
ID NETWORKS
AB We study the applicability of spiking neural networks and neuromorphic hardware for solving general optimization problems without the use of adaptive training or learning algorithms. We leverage the dynamics of Hopfield networks and spin-glass systems to construct a fully connected spiking neural system to generate synchronous spike responses indicative of the underlying community structure in an undirected, unweighted graph. Mapping this fully connected system to current generation neuromorphic hardware is done by embedding sparse tree graphs to generate only the leading-order spiking dynamics. We demonstrate that for a chosen set of benchmark graphs, the spike responses generated on a current generation neuromorphic processor can improve the stability of graph partitions and non-overlapping communities can be identified even with the loss of higher-order spiking behavior if the graphs are sufficiently dense. For sparse graphs, the loss of higher-order spiking behavior improves the stability of certain graph partitions but does not retrieve the known community memberships.
C1 [Hamilton, Kathleen E.; Imam, Neena; Humble, Travis S.] Oak Ridge Natl Lab, One Bethel Valley Rd, Oak Ridge, TN 37831 USA.
RP Hamilton, KE (corresponding author), Oak Ridge Natl Lab, One Bethel Valley Rd, Oak Ridge, TN 37831 USA.
EM hamiltonke@ornl.gov; imamn@ornl.gov; humblets@ornl.gov
CR [Anonymous], 2013, 2013 INT JOINT C NEU
   [Anonymous], 1991, SANTA FE I STUDIES S
   Das S, 2015, IEEE INT SYMP CIRC S, P2704, DOI 10.1109/ISCAS.2015.7169244
   Esser S. K., 2015, ADV NEURAL INFORM PR, P1117
   Esser SK, 2016, P NATL ACAD SCI USA, V113, P11441, DOI 10.1073/pnas.1604850113
   Goodman Dan, 2008, Front Neuroinform, V2, P5, DOI 10.3389/neuro.11.005.2008
   Hamilton K. E., 2018, ARXIV180103571
   Hamilton Kathleen E., 2017, P NEUR COMP S
   HOPFIELD JJ, 1985, BIOL CYBERN, V52, P141
   HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554
   Humphries MD, 2011, J NEUROSCI, V31, P2321, DOI 10.1523/JNEUROSCI.2853-10.2011
   LOWEL S, 1992, SCIENCE, V255, P209, DOI 10.1126/science.1372754
   Meila M, 2007, J MULTIVARIATE ANAL, V98, P873, DOI 10.1016/j.jmva.2006.11.013
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Pedroni BU, 2016, IEEE T BIOMED CIRC S, V10, P837, DOI 10.1109/TBCAS.2016.2539352
   Raghavan UN, 2007, PHYS REV E, V76, DOI 10.1103/PhysRevE.76.036106
   Ronhovde P, 2010, PHYS REV E, V81, DOI 10.1103/PhysRevE.81.046114
   Ronhovde P, 2009, PHYS REV E, V80, DOI 10.1103/PhysRevE.80.016109
NR 18
TC 2
Z9 2
U1 2
U2 7
PD DEC
PY 2018
VL 14
IS 4
SI SI
AR 40
DI 10.1145/3223048
UT WOS:000457138300002
DA 2023-11-16
ER

PT J
AU Yang, J
   Zhao, JH
AF Yang, Jie
   Zhao, Junhong
TI A novel parallel merge neural network with streams of spiking neural
   network and artificial neural network
SO INFORMATION SCIENCES
DT Article
DE Spiking neural network; Artificial neural network; Parallel networks;
   Temporal and spatiotemporal information
ID TIME; ALGORITHM
AB Some neuroscientific studies have demonstrated that the human brain is a complex integrated spatiotemporal system. The human brain supports a wide range of models, algorithms and coding schemes. Inspired by this point, we propose a novel neural network model, the parallel merge neural network (PMNN), which processes spatial and temporal information in parallel with two streams: a spiking neural network (SNN) and an artificial neural network (ANN). The information learned through these two streams is finally merged and flows to the final output layer. Unlike existing wide networks that process a single type of information, the PMNN can learn from the rich spatial, temporal and spatiotemporal information underlying training samples in a manner that is similar to that of the human brain. The experimental results with five UCI datasets and the MNIST dataset demonstrate that the PMNN is very efficient in terms of convergence and generalizable in terms of accuracy, generation, macro-recall, macro-precision and macro-F1.
C1 [Yang, Jie] Dalian Univ Technol, Sch Math Sci, Dalian 116024, Peoples R China.
   [Zhao, Junhong] Chongqing Univ Posts & Telecommun, Key Lab Computat Intelligence, Chongqing 400065, Peoples R China.
RP Zhao, JH (corresponding author), Chongqing Univ Posts & Telecommun, Key Lab Computat Intelligence, Chongqing 400065, Peoples R China.
EM zhaojh@cqupt.edu.cn
CR Asuncion A, 2007, UCI MACHINE LEARNING
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Chamanbaz M, 2023, INFORM SCIENCES, V634, P73, DOI 10.1016/j.ins.2023.03.061
   Chen BL, 2021, INT J MACH LEARN CYB, V12, P651, DOI 10.1007/s13042-020-01194-4
   Chen YP, 2017, Arxiv, DOI [arXiv:1707.01629, 10.48550/arXiv.1707.01629]
   Cheng X, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1519
   Comsa JM, 2020, INT CONF ACOUST SPEE, P8529, DOI [10.1109/icassp40776.2020.9053856, 10.1109/ICASSP40776.2020.9053856]
   Dora S, 2016, NEUROCOMPUTING, V171, P1216, DOI 10.1016/j.neucom.2015.07.086
   Eichenbaum H, 2017, NEURON, V95, P1007, DOI 10.1016/j.neuron.2017.06.036
   Feng QY, 2022, INFORM SCIENCES, V599, P127, DOI 10.1016/j.ins.2022.03.058
   GERSTNER W, 1995, PHYS REV E, V51, P738, DOI 10.1103/PhysRevE.51.738
   GOODALE MA, 1992, TRENDS NEUROSCI, V15, P20, DOI 10.1016/0166-2236(92)90344-8
   Hoffer E, 2017, ADV NEUR IN, V30
   HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837
   Jain AK, 1996, COMPUTER, V29, P31, DOI 10.1109/2.485891
   Kadam SS., 2020, J SCI RES, V64, P374, DOI DOI 10.37398/JSR.2020.640251
   Khan SI, 2022, J KING SAUD UNIV-COM, V34, P6217, DOI 10.1016/j.jksuci.2021.08.004
   Kheradpisheh SR, 2020, Arxiv, DOI arXiv:1910.09495
   Kraus BJ, 2015, NEURON, V88, P578, DOI 10.1016/j.neuron.2015.09.031
   Lee C, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00119
   Li H, 2018, Arxiv, DOI arXiv:1712.09913
   Lin SB, 2023, Arxiv, DOI arXiv:2111.14039
   Liu F., 2021, IEEE T FUZZY SYST
   López-Vázquez G, 2019, COMPUT INTEL NEUROSC, V2019, DOI 10.1155/2019/4182639
   Machingal P, 2020, IEEE IJCNN, DOI 10.1109/ijcnn48605.2020.9207620
   Mostafa H, 2018, IEEE T NEUR NET LEAR, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Pascanu R., 2013, INT C MACHINE LEARNI, V28, P1310
   Putra RVW, 2023, Arxiv, DOI arXiv:2303.01826
   Ran HY, 2022, INFORM SCIENCES, V592, P402, DOI 10.1016/j.ins.2022.01.036
   Rudin C, 2022, STAT SURV, V16, P1, DOI 10.1214/21-SS133
   Song PY, 2022, NEUROCOMPUTING, V488, P359, DOI 10.1016/j.neucom.2022.03.012
   Srinivasan G, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00189
   Tang JX, 2022, NEUROCOMPUTING, V501, P499, DOI 10.1016/j.neucom.2022.06.036
   Taylor C, 1994, MACHINE LEARNING NEU, V13, P1, DOI DOI 10.1080/00401706.1995.10484383
   Tong GX, 2023, INFORM SCIENCES, V636, DOI 10.1016/j.ins.2023.03.137
   Tsao A, 2018, NATURE, V561, P57, DOI 10.1038/s41586-018-0459-6
   Wade JJ, 2010, IEEE T NEURAL NETWOR, V21, P1817, DOI 10.1109/TNN.2010.2074212
   Wang J, 2019, IEEE T CYBERNETICS, V49, P4346, DOI 10.1109/TCYB.2018.2864142
   Wang W, 2018, SCI ADV, V4, DOI 10.1126/sciadv.aat4752
   Wang X, 2022, IEEE T MULTIMEDIA, V24, P2804, DOI 10.1109/TMM.2021.3088639
   Yang AC, 2022, NATURE, V603, P885, DOI 10.1038/s41586-021-04369-3
   Yang S., 2023, INFORM SCIENCES
   Zhang TL, 2022, IEEE T NEUR NET LEAR, V33, P7621, DOI 10.1109/TNNLS.2021.3085966
   Zhao DC, 2022, INFORM SCIENCES, V610, P1, DOI 10.1016/j.ins.2022.07.152
   Zhao DC, 2020, FRONT COMPUT NEUROSC, V14, DOI 10.3389/fncom.2020.576841
NR 45
TC 2
Z9 2
U1 17
U2 17
PD SEP
PY 2023
VL 642
AR 119034
DI 10.1016/j.ins.2023.119034
EA MAY 2023
UT WOS:001006784400001
DA 2023-11-16
ER

PT C
AU Huang, LP
   Wu, QX
   Zhang, GR
   Wang, X
AF Huang, L. P.
   Wu, Q. X.
   Zhang, G. R.
   Wang, X.
BE Lee, CJ
TI Gesture Segmentation Using Spiking Neural Networks Inspired by the
   Visual System
SO PROCEEDINGS OF THE 2015 INTERNATIONAL CONFERENCE ON ARTIFICIAL
   INTELLIGENCE AND INDUSTRIAL ENGINEERING (AIIE 2015)
SE Advances in Intelligent Systems Research
DT Proceedings Paper
CT International Conference on Artificial Intelligence and Industrial
   Engineering (AIIE)
CY JUL 26-27, 2015
CL Phuket, THAILAND
DE gesture segmentation; spiking neural network; visual system; motion
   detection; skin detection
ID DELAYS; OWLS
AB The human visual system demonstrates powerful image processing functionalities. Inspired by the behaviour of the human visual system, a method of gesture segmentation with the complex background is investigated based on fusion of outputs from two kinds of spiking neural networks. The structures and the properties of the two networks are detailed in this paper. One of two networks performs the movement region of gesture. Meanwhile, another spiking neural network performs identification of skin area according to the biologically colour processing mechanism. Finally, two outputs from two neural networks are integrated to segment the meaningful region of gestures from the video image sequence. Simulation results show that the proposed algorithm works efficiently and can perform gesture segmentation with the satisfying accuracy for dynamic visual image sequence under complex background. It is promising to apply this approach to video processing domain and robotic visual systems.
C1 [Huang, L. P.; Wu, Q. X.; Zhang, G. R.; Wang, X.] Fujian Normal Univ, Coll Photon & Elect Engn, Minist Educ, Key Lab OptoElect Sci & Technol, Fuzhou, Peoples R China.
RP Huang, LP (corresponding author), Fujian Normal Univ, Coll Photon & Elect Engn, Minist Educ, Key Lab OptoElect Sci & Technol, Fuzhou, Peoples R China.
CR CARR CE, 1988, P NATL ACAD SCI USA, V85, P8311, DOI 10.1073/pnas.85.21.8311
   Chaki A., 2011, 2 INT C INT SYST MOD, P172
   CHOUDHURY A, 2014, SIGNAL PROCESSING IN, P136
   Crook S. M., 1997, J COMPUTATIONAL NEUR, V4, P1573
   Dhruva N., 2013, COMMUN COMPUT PHYS, V361, P537
   JEFFRESS LA, 1948, J COMP PHYSIOL PSYCH, V41, P35, DOI 10.1037/h0061495
   JEFFRESS LA, 1948, AM J PSYCHOL, V61, P468, DOI 10.2307/1418312
   Jessell T. M, 1981, PRINCIPLES NEURAL SC
   Lin JW, 2002, TRENDS NEUROSCI, V25, P449, DOI 10.1016/S0166-2236(02)02212-9
   Mo Shu, INT C
   Peña JL, 2001, J NEUROSCI, V21, P9455, DOI 10.1523/JNEUROSCI.21-23-09455.2001
   Petrou M, 2008, ARTECH HSE BIOINF BI, P1
   Senn W, 2002, NEURAL COMPUT, V14, P583, DOI 10.1162/089976602317250915
   Wu Q., 2009, BROADB COMM NETW SYS, P1
   Wu Q. X., 2007, 7 INT WORKSH INF PRO
   WU Q X, 2008, MOTION DETECTION USI
   Wu QX, 2007, LECT NOTES ARTIF INT, V4682, P26
   Wu QX, 2009, INT CONF BIOMED, P368
   Zhu C. M., 2006, J FUZHOU U, V34
NR 19
TC 0
Z9 1
U1 0
U2 0
PY 2015
VL 123
BP 184
EP 187
UT WOS:000360009400052
DA 2023-11-16
ER

PT J
AU Valencia, D
   Alimohammad, A
AF Valencia, Daniel
   Alimohammad, Amir
TI Neural Spike Sorting Using Binarized Neural Networks
SO IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING
DT Article
DE Biomedical signal processing; artificial neural networks (ANNs);
   application specific integrated circuits (ASICs)
ID ARCHITECTURE; PROCESSOR; NEURONS; DESIGN
AB This article presents the design and efficient hardware implementation of binarized neural networks (BNNs) for brain-implantable neural spike sorting. In contrast to the conventional artificial neural networks (ANNs), in which the weights and activation functions of neurons are represented using real values, the BNNs utilize binarized weights and activation functions to dramatically reduce the memory requirement and computational complexity of the ANNs. The designed BNN is trained using several realistic neural datasets to verify its accuracy for neural spike sorting. The application-specific integrated circuit (ASIC) implementation of the designed BNN in a standard 0.18-mu m CMOS process occupies 0.33 mm(2) of silicon area. Power consumption estimation of the ASIC layout shows that the BNN dissipates 2.02 mu W of power from a 1.8 V supply while operating at 24 kHz. The designed BNN- based spike sorting system is also implemented on a field-programmable gate array and is shown to reduce the required on-chip memory by 89% compared to those of the alternative state-of-the-art spike sorting systems. To the best of our knowledge, this is the first work employing BNNs for real-time in vivo neural spike sorting.
C1 [Valencia, Daniel; Alimohammad, Amir] San Diego State Univ, Dept Elect & Comp Engn, San Diego, CA 92182 USA.
RP Valencia, D (corresponding author), San Diego State Univ, Dept Elect & Comp Engn, San Diego, CA 92182 USA.
EM dlvalencia@sdsu.edu
CR Ando H, 2016, IEEE T BIOMED CIRC S, V10, P1068, DOI 10.1109/TBCAS.2016.2514522
   Do AT, 2019, IEEE T VLSI SYST, V27, P126, DOI 10.1109/TVLSI.2018.2875934
   [Anonymous], 2016, P IEEE HIGH PERF EXT
   [Anonymous], 2011, ECEVCL20114 VLSI COM
   [Anonymous], 2016, P 35 CHIN CONTR C CH
   Bilbao Imanol, 2017, 2017 Eighth International Conference on Intelligent Computing and Information Systems (ICICIS). Proceedings, P173, DOI 10.1109/INTELCIS.2017.8260032
   Chen F, 2012, IEEE J SOLID-ST CIRC, V47, P744, DOI 10.1109/JSSC.2011.2179451
   Dragas J, 2015, IEEE T NEUR SYS REH, V23, P149, DOI 10.1109/TNSRE.2014.2370510
   Geiger Lukas, 2020, J OPEN SOURCE SOFTW, V5, P1746, DOI DOI 10.21105/JOSS.01746
   Gibson, 2012, THESIS U CALIFORNIA
   Gibson S, 2013, J NEUROSCI METH, V215, P1, DOI 10.1016/j.jneumeth.2013.01.026
   Gibson S, 2010, IEEE T NEUR SYS REH, V18, P469, DOI 10.1109/TNSRE.2010.2051683
   Harrison RR, 2007, IEEE J SOLID-ST CIRC, V42, P123, DOI 10.1109/JSSC.2006.886567
   Hubara I., 2016, ADV NEURAL INFORM PR, P4107
   Jacob B, 2018, PROC CVPR IEEE, P2704, DOI 10.1109/CVPR.2018.00286
   KAISER JF, 1990, INT CONF ACOUST SPEE, P381, DOI 10.1109/ICASSP.1990.115702
   Kamboh AM, 2010, BIOMED CIRC SYST C, P13, DOI 10.1109/BIOCAS.2010.5709559
   Karkare V, 2013, IEEE J SOLID-ST CIRC, V48, P2230, DOI 10.1109/JSSC.2013.2264616
   Karkare V, 2011, IEEE J SOLID-ST CIRC, V46, P1214, DOI 10.1109/JSSC.2011.2116410
   Kim KH, 2003, IEEE T BIO-MED ENG, V50, P999, DOI 10.1109/TBME.2003.814523
   Kim S, 2007, IEEE T NEUR SYS REH, V15, P493, DOI 10.1109/TNSRE.2007.908429
   Lewicki MS, 1998, NETWORK-COMP NEURAL, V9, pR53, DOI 10.1088/0954-898X/9/4/001
   MacQueen J., 1967, P 5 BERKELEY S MATH
   Martinez J, 2009, J NEUROSCI METH, V184, P285, DOI 10.1016/j.jneumeth.2009.08.017
   Noh H, 2017, ADV NEUR IN, V30
   Obeid I, 2004, IEEE T BIO-MED ENG, V51, P905, DOI 10.1109/TBME.2004.826683
   Olukanmi PO, 2018, INT CONF SOFT COMP, P54, DOI 10.1109/ISCMI.2018.8703210
   Pedreira C, 2012, J NEUROSCI METH, V211, P58, DOI 10.1016/j.jneumeth.2012.07.010
   Quiroga RQ, 2004, NEURAL COMPUT, V16, P1661, DOI 10.1162/089976604774201631
   Quiroga RQ, 2005, NATURE, V435, P1102, DOI 10.1038/nature03687
   Rijsbergen CJV, 1979, INFORM RETRIEVAL
   Rutishauser U, 2006, J NEUROSCI METH, V154, P204, DOI 10.1016/j.jneumeth.2005.12.033
   Schalk LM, 2017, IEEEAAIA DIGIT AVION
   Simons T, 2019, ELECTRONICS-SWITZ, V8, DOI 10.3390/electronics8060661
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Valencia D, 2020, IEEE T CIRCUITS-I, V67, P5200, DOI 10.1109/TCSI.2020.3003769
   Valencia D, 2019, IEEE T BIOMED CIRC S, V13, P1700, DOI 10.1109/TBCAS.2019.2947618
   Valencia D, 2019, IEEE T BIOMED CIRC S, V13, P1714, DOI 10.1109/TBCAS.2019.2947130
   Valencia D, 2019, IEEE T BIOMED CIRC S, V13, P481, DOI 10.1109/TBCAS.2019.2907882
   Wu SH, 2018, 2018 52ND ANNUAL CONFERENCE ON INFORMATION SCIENCES AND SYSTEMS (CISS), DOI 10.1109/CISS.2018.8362280
   Xu H, 2019, J NEUROSCI METH, V311, P111, DOI 10.1016/j.jneumeth.2018.10.019
   Yang YN, 2017, IEEE T BIOMED CIRC S, V11, P743, DOI 10.1109/TBCAS.2017.2679032
   Yu B, 2011, IEEE J EM SEL TOP C, V1, P502, DOI 10.1109/JETCAS.2012.2183430
   Zamani M, 2020, IEEE T BIOMED CIRC S, V14, P221, DOI 10.1109/TBCAS.2020.2969910
   Zamani M, 2018, IEEE T BIOMED CIRC S, V12, P665, DOI 10.1109/TBCAS.2018.2825421
NR 45
TC 10
Z9 10
U1 1
U2 12
PY 2021
VL 29
BP 206
EP 214
DI 10.1109/TNSRE.2020.3043403
UT WOS:000626331500001
DA 2023-11-16
ER

PT C
AU Ni, YW
   Cui, XX
   Fan, YN
   Han, QK
   Liu, KF
   Cui, XL
AF Ni, Yewen
   Cui, Xiaoxin
   Fan, Yuanning
   Han, Qiankun
   Liu, Kefei
   Cui, Xiaole
BE Qin, YJ
   Hong, ZL
   Tang, TA
TI Design of Router for Spiking Neural Networks
SO 2017 IEEE 12TH INTERNATIONAL CONFERENCE ON ASIC (ASICON)
SE International Conference on ASIC
DT Proceedings Paper
CT 12th IEEE International Conference on ASIC (ASICON)
CY OCT 25-28, 2017
CL Guiyang, PEOPLES R CHINA
AB The development of large-scale networks with artificial neurons and adaptive synapses has suggested new avenues of exploration for brain-like cognitive computing. Spiking neural networks (SNNs), highly inspired from natural computing in the brain and recent advances in neurosciences, are often referred to as the 3th generation of neural network, and become a new research hotspot in the era of artificial intelligence. SNN architecture supports simpler category of biologically-inspired neuron models and more complex large-scale interconnection in on-chip network of neurosynaptic cores. This paper introduces the design of router used in a spiking neural network, which is able to send and receive spiking information in network properly, as well as perfectly dealing with network anomaly such as data race or traffic congestion. This router is designed for the network at a scale of 64 * 64 neurosynaptic cores at the most, with 256 neurons in each cores. Both the area and estimated power consumption is acceptable. This router could also be applied to larger scale of SNN architecture networks effectively.
C1 [Ni, Yewen; Cui, Xiaoxin; Fan, Yuanning; Han, Qiankun; Liu, Kefei] Peking Univ, Inst Microelect, Beijing 100871, Peoples R China.
   [Cui, Xiaole] Peking Univ, Shenzhen Grad Sch, Key Lab Integrated Microsyst, Shenzhen 518055, Peoples R China.
RP Cui, XX (corresponding author), Peking Univ, Inst Microelect, Beijing 100871, Peoples R China.
EM cuixx@pku.edu.cn; cuixl@pkusz.edu.cn
CR Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   [Anonymous], 2013, 2013 INT JOINT C NEU
   [Anonymous], CUST INT CIRC C CICC
   Rumelhart DE, 1986, PARALLEL DISTRIBUTED, V1, DOI DOI 10.7551/MITPRESS/5236.001.0001
   VONNEUMANN J, 1993, IEEE ANN HIST COMPUT, V15, P28
NR 5
TC 2
Z9 2
U1 0
U2 6
PY 2017
BP 965
EP 968
UT WOS:000426983400242
DA 2023-11-16
ER

PT J
AU Boerlin, M
   Machens, CK
   Denève, S
AF Boerlin, Martin
   Machens, Christian K.
   Deneve, Sophie
TI Predictive Coding of Dynamical Variables in Balanced Spiking Networks
SO PLOS COMPUTATIONAL BIOLOGY
DT Article
ID PERSISTENT NEURAL ACTIVITY; SPATIAL WORKING-MEMORY; DECISION-MAKING;
   MODEL; INHIBITION; NEURONS; INFORMATION; RELIABILITY; COMPUTATION;
   EXCITATION
AB Two observations about the cortex have puzzled neuroscientists for a long time. First, neural responses are highly variable. Second, the level of excitation and inhibition received by each neuron is tightly balanced at all times. Here, we demonstrate that both properties are necessary consequences of neural networks that represent information efficiently in their spikes. We illustrate this insight with spiking networks that represent dynamical variables. Our approach is based on two assumptions: We assume that information about dynamical variables can be read out linearly from neural spike trains, and we assume that neurons only fire a spike if that improves the representation of the dynamical variables. Based on these assumptions, we derive a network of leaky integrate-and-fire neurons that is able to implement arbitrary linear dynamical systems. We show that the membrane voltage of the neurons is equivalent to a prediction error about a common population-level signal. Among other things, our approach allows us to construct an integrator network of spiking neurons that is robust against many perturbations. Most importantly, neural variability in our networks cannot be equated to noise. Despite exhibiting the same single unit properties as widely used population code models (e. g. tuning curves, Poisson distributed spike trains), balanced networks are orders of magnitudes more reliable. Our approach suggests that spikes do matter when considering how the brain computes, and that the reliability of cortical representations could have been strongly underestimated.
C1 [Boerlin, Martin; Deneve, Sophie] Ecole Normale Super, Dept Etud Cognit, Grp Neural Theory, 24 Rue Lhomond, F-75231 Paris, France.
   [Machens, Christian K.] Champalimaud Ctr Unknown, Champalimaud Neurosci Programme, Lisbon, Portugal.
RP Boerlin, M (corresponding author), Ecole Normale Super, Dept Etud Cognit, Grp Neural Theory, 24 Rue Lhomond, F-75231 Paris, France.
EM sophie.deneve@ens.fr
CR Abbott LF, 1999, NEURAL COMPUT, V11, P91, DOI 10.1162/089976699300016827
   [Anonymous], 1991, INTRO THEORY NEURAL
   [Anonymous], 2010, ADV NEURAL INFORM PR
   Batista AP, 1999, SCIENCE, V285, P257, DOI 10.1126/science.285.5425.257
   Boerlin M, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1001080
   Bourdoukan R, 2012, ADV NEURAL INFORM PR, V25, P2294
   Brunel N, 1998, NEURAL COMPUT, V10, P1731, DOI 10.1162/089976698300017115
   Brunel N, 2000, J COMPUT NEUROSCI, V8, P183, DOI 10.1023/A:1008925309027
   Cain N, 2013, J NEUROPHYSIOL, V109, P2542, DOI 10.1152/jn.00976.2012
   Compte A, 2000, CEREB CORTEX, V10, P910, DOI 10.1093/cercor/10.9.910
   Deneve S, 2008, NEURAL COMPUT, V20, P91, DOI 10.1162/neco.2008.20.1.91
   Ecker AS, 2010, SCIENCE, V327, P584, DOI 10.1126/science.1179867
   Eliasmith C, 2005, NEURAL COMPUT, V17, P1276, DOI 10.1162/0899766053630332
   Faisal AA, 2008, NAT REV NEUROSCI, V9, P292, DOI 10.1038/nrn2258
   Gentet LJ, 2010, NEURON, V65, P422, DOI 10.1016/j.neuron.2010.01.006
   Gold JI, 2007, ANNU REV NEUROSCI, V30, P535, DOI 10.1146/annurev.neuro.29.051605.113038
   Goldman MS, 2009, NEURON, V61, P621, DOI 10.1016/j.neuron.2008.12.012
   Goldman MS, 2003, CEREB CORTEX, V13, P1185, DOI 10.1093/cercor/bhg095
   Haider B, 2006, J NEUROSCI, V26, P4535, DOI 10.1523/JNEUROSCI.5297-05.2006
   Koulakov AA, 2002, NAT NEUROSCI, V5, P775, DOI 10.1038/nn893
   Leigh JR, 2004, CONTROL THEORY GUIDE
   Lim S, 2013, NAT NEUROSCI, V16, P1306, DOI 10.1038/nn.3492
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Machens CK, 2008, NEURAL COMPUT, V20, P452, DOI 10.1162/neco.2007.07-06-297
   Machens CK, 2005, SCIENCE, V307, P1121, DOI 10.1126/science.1104171
   MAINEN ZF, 1995, SCIENCE, V268, P1503, DOI 10.1126/science.7770778
   Major G, 2004, CURR OPIN NEUROBIOL, V14, P675, DOI 10.1016/j.conb.2004.10.017
   Moreau L, 2003, PHYS REV E, V68, DOI 10.1103/PhysRevE.68.020901
   Okun M, 2008, NAT NEUROSCI, V11, P535, DOI 10.1038/nn.2105
   Pillow JW, 2008, NATURE, V454, P995, DOI 10.1038/nature07140
   Renart A, 2003, NEURON, V38, P473, DOI 10.1016/S0896-6273(03)00255-1
   Renart A, 2010, SCIENCE, V327, P587, DOI 10.1126/science.1179850
   Roudi Y, 2007, PLOS COMPUT BIOL, V3, P1679, DOI 10.1371/journal.pcbi.0030141
   Schneidman E, 1998, NEURAL COMPUT, V10, P1679, DOI 10.1162/089976698300017089
   Seung HS, 1996, P NATL ACAD SCI USA, V93, P13339, DOI 10.1073/pnas.93.23.13339
   Shadlen MN, 1998, J NEUROSCI, V18, P3870
   Shu YS, 2003, NATURE, V423, P288, DOI 10.1038/nature01616
   Snyder LH, 1998, NATURE, V394, P887, DOI 10.1038/29777
   SOFTKY WR, 1993, J NEUROSCI, V13, P334
   Tchumatchenko T, 2010, PHYS REV LETT, V104, DOI 10.1103/PhysRevLett.104.058102
   TOLHURST DJ, 1983, VISION RES, V23, P775, DOI 10.1016/0042-6989(83)90200-6
   van Vreeswijk C, 1998, NEURAL COMPUT, V10, P1321, DOI 10.1162/089976698300017214
   Vogels TP, 2005, ANNU REV NEUROSCI, V28, P357, DOI 10.1146/annurev.neuro.28.061604.135637
   Wang XJ, 2008, NEURON, V60, P215, DOI 10.1016/j.neuron.2008.09.034
   Wehr M, 2003, NATURE, V426, P442, DOI 10.1038/nature02116
   Wolpert DM, 2000, NAT NEUROSCI, V3, P1212, DOI 10.1038/81497
   ZOHARY E, 1994, NATURE, V370, P140, DOI 10.1038/370140a0
NR 47
TC 126
Z9 126
U1 0
U2 11
PD NOV
PY 2013
VL 9
IS 11
AR e1003258
DI 10.1371/journal.pcbi.1003258
UT WOS:000330357200001
DA 2023-11-16
ER

PT C
AU Goodman, E
   Ventura, D
AF Goodman, E
   Ventura, D
GP IEEE
TI Effectively using recurrently-connected spiking neural networks
SO Proceedings of the International Joint Conference on Neural Networks
   (IJCNN), Vols 1-5
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT IEEE International Joint Conference on Neural Networks (IJCNN 2005)
CY JUL 31-AUG 04, 2005
CL Montreal, CANADA
AB Recurrently-connected spiking neural networks are difficult to use and understand because of the complex non linear dynamics of the system. Through empirical studies of spiking networks, we deduce several principles which are critical to success. Network parameters such as synaptic time delays and time constants and the connection probabilities can be adjusted to have a significant impact on accuracy. We show how to adjust these parameters to fit the type of problem.
C1 Brigham Young Univ, Dept Comp Sci, Provo, UT 84602 USA.
RP Goodman, E (corresponding author), Brigham Young Univ, Dept Comp Sci, Provo, UT 84602 USA.
CR Gerstner W., 2002, SPIKING NEURON MODEL
   Gupta A, 2000, SCIENCE, V287, P273, DOI 10.1126/science.287.5451.273
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   MAASS W, 1995, ADV NEURAL INFORMATI, V7, P183
   Markram H, 1998, P NATL ACAD SCI USA, V95, P5323, DOI 10.1073/pnas.95.9.5323
   NATSCHLAGER T, 2002, SPECIAL ISSUE FDN IN, V8, P39
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
NR 7
TC 7
Z9 7
U1 0
U2 0
PY 2005
BP 1542
EP 1547
UT WOS:000235178002040
DA 2023-11-16
ER

PT C
AU Sboev, A
   Vlasov, D
   Rybka, R
   Serenko, A
AF Sboev, Alexander
   Vlasov, Danila
   Rybka, Roman
   Serenko, Alexey
BE Samsonovich, AV
   Lebiere, CJ
TI Spiking neural network reinforcement learning method based on temporal
   coding and STDP
SO POSTPROCEEDINGS OF THE 9TH ANNUAL INTERNATIONAL CONFERENCE ON
   BIOLOGICALLY INSPIRED COGNITIVE ARCHITECTURES (BICA 2018)
SE Procedia Computer Science
DT Proceedings Paper
CT 9th Annual International Conference of the
   Biologically-Inspired-Cognitive-Architectures-Society (BICA) held as
   part of the Joint Multi-Conference on Human-Level Artificial
   Intelligence (HLAI)
CY AUG 22-24, 2018
CL Czech Tech Univ, Prague, CZECH REPUBLIC
HO Czech Tech Univ
DE spike-timing-dependent plasticity; spiking neural network; reinforcement
   learning; temporal coding; classification
ID NEURONS
AB A method to train a spiking network to solve a classification task using Spike-Timing-Dependent Plasticity is proposed. Learning is based on the ability of STDP to memorize repeating spike patterns: earliest spikes of pattern contribute to output spike and those weights stay high, while others weights falls down to zero. The output neurons are provided with information on the classes by stimulating the neuron corresponding to the desired class to fire an early spike. The network is single-layer, with competition introduced by inhibitory interconnections. The network consists of leaky integrate-and-fire neurons tuned to provide one spike per pattern. Input data is encoded by Gaussian receptive fields, where earliest spikes contains the most information. The learning method is tested on Fisher's Iris and Wisconsin Breast Cancer datasets, and results compared to Support Vector Machines, Random Forest and formal neural networks with Adam optimizer (C) 2018 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0/) Peer-review under responsibility of the scientific committee of the 9th Annual International Conference on Biologically Inspired Cognitive Architectures.
C1 [Sboev, Alexander; Vlasov, Danila; Rybka, Roman; Serenko, Alexey] Kurchatov Inst, Natl Res Ctr, Moscow, Russia.
   [Sboev, Alexander; Vlasov, Danila] MEPhI Natl Res Nucl Univ, Moscow, Russia.
RP Sboev, A (corresponding author), Kurchatov Inst, Natl Res Ctr, Moscow, Russia.; Sboev, A (corresponding author), MEPhI Natl Res Nucl Univ, Moscow, Russia.
EM sag111@mail.ru
CR Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   Diehl P., 2015, FRONTIERS COMPUTATIO
   Gütig R, 2014, CURR OPIN NEUROBIOL, V25, P134, DOI 10.1016/j.conb.2014.01.004
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Masquelier T, 2008, PLOS ONE, V3, DOI 10.1371/journal.pone.0001377
   Morrison A, 2008, BIOL CYBERN, V98, P459, DOI 10.1007/s00422-008-0233-1
   Saïghi S, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00051
   Sboev A, 2018, PROCEDIA COMPUT SCI, V123, P494, DOI 10.1016/j.procs.2018.01.075
   Wang JL, 2014, NEUROCOMPUTING, V144, P526, DOI 10.1016/j.neucom.2014.04.017
NR 11
TC 3
Z9 3
U1 1
U2 4
PY 2018
VL 145
BP 458
EP 463
DI 10.1016/j.procs.2018.11.107
UT WOS:000551069000070
DA 2023-11-16
ER

PT J
AU Dora, S
   Subramanian, K
   Suresh, S
   Sundararajan, N
AF Dora, S.
   Subramanian, K.
   Suresh, S.
   Sundararajan, N.
TI Development of a Self-Regulating Evolving Spiking Neural Network for
   classification problem
SO NEUROCOMPUTING
DT Article
DE Spiking neural network; Self-regulation; Pattern classification
ID LEARNING ALGORITHM; PRECISION; MODEL
AB This paper presents a new spiking neural network for pattern classification problems, referred to as the Self-Regulating Evolving Spiking Neural (SRESN) classifier, that regulates the learning process of the network. It uses a two layered spiking neural network and the input layer consists of receptive field neurons, which convert a real valued input to spikes using the population coding scheme without any delays. The output layer consists of leaky integrate-and-fire neurons. Since SRESN does not use any delays, the number of network parameters for SRESN is significantly lower than that used by other spiking neural networks, used in this study. During training, the learning algorithm for SRESN, automatically evolves neurons in the output layer based on the training data stream and the current knowledge stored in the network. Depending on the knowledge in the sample and the class specific knowledge stored in the network, it can choose to either add a neuron or update the network parameters or skip learning the sample resulting in self-regulation of the learning process. In case of neuron addition, the weights for the newly added neuron are initialized using a modified rank order scheme which facilitates SRESN for use in online/sequential as well as batch learning modes. The parameter update strategy in SRESN ensures that connections with non-zero postsynaptic potential at the time of the spike are alone updated which helps prevent over training. While evaluating the performance of SRESN, first a study is conducted to assess the impact of various parameters on its performance and establish guidelines to choose suitable values for these parameters. Next, the performance of SRESN, operating in batch mode, is compared with other spiking neural classifiers, including SpikeProp and MuSpiNN, for the UCI benchmark problems of Iris flower classification and Wisconsin breast cancer. Subsequently, the performance of SRFSN in online and batch learning mode is compared with an evolving spiking neural classifier for five benchmark data sets from the UCI machine learning repository. Finally, SRESN is applied to solve the practical problem of Epilepsy detection. The performance comparison clearly indicates that SRESN provides a higher generalization accuracy using fewer network parameters. (C) 2015 Elsevier B.V. All rights reserved.
C1 [Dora, S.; Subramanian, K.; Suresh, S.] Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
   [Sundararajan, N.] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
RP Suresh, S (corresponding author), Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
EM ssundaram@ntu.edu.sg
CR Abbott LF, 1999, BRAIN RES BULL, V50, P303, DOI 10.1016/S0361-9230(99)00161-6
   Adeli H., 2007, IEEE T BIOMED ENG, V54, P1545
   Andrzejak RG, 2001, PHYS REV E, V64, DOI 10.1103/PhysRevE.64.061907
   [Anonymous], 2013, INT JOINT C NEUR NET, DOI DOI 10.1109/AGILE.2013.7
   [Anonymous], 2007, EVOLVING CONNECTIONI
   Babu GS, 2012, NEUROCOMPUTING, V81, P86, DOI 10.1016/j.neucom.2011.12.001
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Butts DA, 2007, NATURE, V449, P92, DOI [10.1038/nature06105, 10.1038/natureO6105]
   Dora S, 2014, IEEE IJCNN, P2415, DOI 10.1109/IJCNN.2014.6889775
   Florian RV, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0040233
   Ghosh-Dastidar S, 2007, INTEGR COMPUT-AID E, V14, P187
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Kasabov N, 2013, NEURAL NETWORKS, V41, P188, DOI 10.1016/j.neunet.2012.11.014
   Lichman M., 2013, UCI MACHINE LEARNING
   Maass W, 1997, NEURAL COMPUT, V9, P279, DOI 10.1162/neco.1997.9.2.279
   Maass W., TR1999037 TU GRAZ I
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   McKennoch S, 2006, IEEE IJCNN, P3970
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Savitha R, 2012, NEURAL NETWORKS, V32, P209, DOI 10.1016/j.neunet.2012.02.015
   Shrestha SB, 2015, NEURAL NETWORKS, V63, P185, DOI 10.1016/j.neunet.2014.12.001
   Subramanian Kartick, 2014, IEEE Transactions on Neural Networks and Learning Systems, V25, P1659, DOI 10.1109/TNNLS.2014.2321420
   Subramanian Kartick, 2013, IEEE Transactions on Fuzzy Systems, V21, P1080, DOI 10.1109/TFUZZ.2013.2242894
   Suresh S, 2008, NEUROCOMPUTING, V71, P1345, DOI 10.1016/j.neucom.2007.06.003
   Suresh S, 2010, NEUROCOMPUTING, V73, P3012, DOI 10.1016/j.neucom.2010.07.003
   Thorpe S, 1998, COMPUTATIONAL NEUROSCIENCE: TRENDS IN RESEARCH, P113
   van Rossum MCW, 2001, NEURAL COMPUT, V13, P751, DOI 10.1162/089976601300014321
   Van Rullen R, 2001, NEURAL COMPUT, V13, P1255, DOI 10.1162/08997660152002852
   VanRullen R, 2005, TRENDS NEUROSCI, V28, P1, DOI 10.1016/j.tins.2004.10.010
   Victor JD, 1996, J NEUROPHYSIOL, V76, P1310, DOI 10.1152/jn.1996.76.2.1310
   Wade JJ, 2010, IEEE T NEURAL NETWOR, V21, P1817, DOI 10.1109/TNN.2010.2074212
   Wang JL, 2014, NEUROCOMPUTING, V144, P526, DOI 10.1016/j.neucom.2014.04.017
   Wysoski SG, 2008, NEUROCOMPUTING, V71, P2563, DOI 10.1016/j.neucom.2007.12.038
   Xin JG, 2001, IEEE IJCNN, P1772, DOI 10.1109/IJCNN.2001.938430
   Xu Y, 2013, NEURAL NETWORKS, V43, P99, DOI 10.1016/j.neunet.2013.02.003
   Yu Q, 2014, NEUROCOMPUTING, V138, P3, DOI 10.1016/j.neucom.2013.06.052
   Yu Q, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0078318
NR 38
TC 46
Z9 50
U1 1
U2 34
PD JAN 1
PY 2016
VL 171
BP 1216
EP 1229
DI 10.1016/j.neucom.2015.07.086
UT WOS:000364883900120
DA 2023-11-16
ER

PT C
AU Mohemmed, A
   Matsuda, S
   Schliebs, S
   Dhoble, K
   Kasabov, N
AF Mohemmed, Ammar
   Matsuda, Satoshi
   Schliebs, Stefan
   Dhoble, Kshitij
   Kasabov, Nikola
GP IEEE
TI Optimization of Spiking Neural Networks with Dynamic Synapses for Spike
   Sequence Generation using PSO
SO 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUL 31-AUG 05, 2011
CL San Jose, CA
AB We present a method that is based on Particle Swarm Optimization (PSO) for training a Spiking Neural Network (SNN) with dynamic synapses to generate precise time spike sequences. The similarity between the desired spike sequence and the actual output sequence is measured by a simple leaky integrate and fire spiking neuron. This measurement is used as a fitness function for PSO algorithm to tune the dynamic synapses until a desired spike output sequence is obtained when certain input spike sequence is presented. Simulations are made to illustrate the performance of the proposed method.
C1 [Mohemmed, Ammar; Schliebs, Stefan; Dhoble, Kshitij; Kasabov, Nikola] Auckland Univ Technol, Knowledge Engn & Discovery Res Inst, Auckland, New Zealand.
   [Matsuda, Satoshi] Nihon Univ, Coll Ind Technol, Dept Math Informat Engn, Tokyo 102, Japan.
RP Mohemmed, A (corresponding author), Auckland Univ Technol, Knowledge Engn & Discovery Res Inst, Auckland, New Zealand.
EM amohemme@aut.ac.nz; matsuda.satoshi@nihon-u.ac.jp; nkasabov@aut.ac.nz
CR Bapi D. R. S., 2005, INVESTIGATION SEQUEN
   Belatreche A, 2007, SOFT COMPUT, V11, P239, DOI [10.1007/s00500-006-0065-7, 10.1007/S00500-006-0065-7]
   Bohte S. M., 2004, NATURAL COMPUTING, V3, P2004
   Bohte SM, 2005, INFORM PROCESS LETT, V95, P519, DOI 10.1016/j.ipl.2005.05.018
   Dauwels J, 2009, LECT NOTES COMPUT SC, V5506, P177, DOI 10.1007/978-3-642-02490-0_22
   ELMAN JL, 1990, COGNITIVE SCI, V14, P179, DOI 10.1207/s15516709cog1402_1
   GERSTNER W, 1993, BIOL CYBERN, V69, P503, DOI 10.1007/BF01185422
   Giles CL, 1995, NEURAL NETWORKS, V8, P1359, DOI 10.1016/0893-6080(95)00041-0
   Kennedy J., 1995, 1995 IEEE International Conference on Neural Networks Proceedings (Cat. No.95CH35828), P1942, DOI 10.1109/ICNN.1995.488968
   Legenstein R, 2005, NEURAL COMPUT, V17, P2337, DOI 10.1162/0899766054796888
   Liaw JS, 1997, 1997 IEEE INTERNATIONAL CONFERENCE ON NEURAL NETWORKS, VOLS 1-4, P350, DOI 10.1109/ICNN.1997.611692
   Liaw JS, 1999, NEUROCOMPUTING, V26-7, P199, DOI 10.1016/S0925-2312(99)00063-6
   Maass W, 2002, NEURAL NETWORKS, V15, P155, DOI 10.1016/S0893-6080(01)00144-7
   Maass W, 2000, NEURAL COMPUT, V12, P1743, DOI 10.1162/089976600300015123
   Maass W, 1998, PULSED NEURAL NETWORKS, P321
   Marc-Oliver G., 2007, SCHOLARPEDIA, DOI [DOI 10.4249/SCHOLARPEDIA.1430, 10.4249/scholarpedia.1430]
   Mehrtash N, 2003, NEURAL COMPUT APPL, V12, P33, DOI 10.1007/s00521-030-0371-2
   NAMARVAR HH, 2001, NEUR NETW 2001 P IJC
   Natschläger T, 2002, THEOR COMPUT SCI, V287, P251, DOI 10.1016/S0304-3975(02)00099-3
   Paiva ARC, 2010, NEURAL COMPUT APPL, V19, P405, DOI 10.1007/s00521-009-0307-6
   Pavlidis N., 2005, NEUR NETW 2005 IJCNN, P31
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Poulsen TM, 2009, LECT NOTES COMPUT SC, V5769, P784, DOI 10.1007/978-3-642-04277-5_79
   Sichtig H., 2007, COMP INT BIOINF COMP, P346
   Sun R, 2001, IEEE INTELL SYST, V16, P67, DOI 10.1109/MIS.2001.1463065
   Tsodyks M, 1998, NEURAL COMPUT, V10, P821, DOI 10.1162/089976698300017502
   van Rossum MCW, 2001, NEURAL COMPUT, V13, P751, DOI 10.1162/089976601300014321
NR 27
TC 6
Z9 6
U1 1
U2 8
PY 2011
BP 2969
EP 2974
UT WOS:000297541203015
DA 2023-11-16
ER

PT J
AU Leleu, T
   Levi, T
   Kohno, T
   Aihara, K
AF Leleu, Timothee
   Levi, Timothee
   Kohno, Takashi
   Aihara, Kazuyuki
TI Network structure reconstruction using packets of spikes in cultured
   neuronal networks coupled to microelectrode arrays
SO IEICE NONLINEAR THEORY AND ITS APPLICATIONS
DT Article
DE network reconstruction; spike analysis; avalanches; cultured neural
   networks; microelectrode arrays
AB Reconstructing accurately the structure of neural networks from biological data is essential for the analysis of simultaneous recordings from many neurons, and, in turn, for the understanding of neural codes and the design of neural prostheses. Classical techniques are generally based on cross-correlations and cannot reconstruct unambiguously the network structure. Recently, we have proposed a method for which there is one-to-one correspondence between statistical properties of packets of spikes (or avalanches) and the network structure, but this mapping was only proven for simpler neuronal model. In the following, we show using numerical simulation of the Izhikevich model that the proposed method is general, and is particularly well-fitted for the analysis of neural activity recorded from cultured neuronal networks coupled to microelectrode arrays.
C1 [Leleu, Timothee; Levi, Timothee; Kohno, Takashi; Aihara, Kazuyuki] Univ Tokyo, Inst Ind Sci, Meguro Ku, 4-6-1 Komaba, Tokyo 1538505, Japan.
RP Leleu, T (corresponding author), Univ Tokyo, Inst Ind Sci, Meguro Ku, 4-6-1 Komaba, Tokyo 1538505, Japan.
EM timothee@sat.t.u-tokyo.ac.jp
CR Ambroise M, 2017, ARTIF LIFE ROBOT, V22, P398, DOI 10.1007/s10015-017-0366-1
   Bêche JF, 2009, I IEEE EMBS C NEUR E, P590
   Beggs JM, 2003, J NEUROSCI, V23, P11167
   Bonifazi P, 2013, FRONT NEURAL CIRCUIT, V7, DOI 10.3389/fncir.2013.00040
   Chialvo DR, 2004, PHYSICA A, V340, P756, DOI 10.1016/j.physa.2004.05.064
   Dodel S, 2002, NEUROCOMPUTING, V44, P1065, DOI 10.1016/S0925-2312(02)00416-2
   Eguíluz VM, 2005, PHYS REV LETT, V94, DOI 10.1103/PhysRevLett.94.018102
   Eurich CW, 2002, PHYS REV E, V66, DOI 10.1103/PhysRevE.66.066137
   Friedman N, 2012, PHYS REV LETT, V108, DOI 10.1103/PhysRevLett.108.208102
   Gireesh ED, 2008, P NATL ACAD SCI USA, V105, P7576, DOI 10.1073/pnas.0800537105
   Hahn G, 2010, J NEUROPHYSIOL, V104, P3312, DOI 10.1152/jn.00953.2009
   Hochberg LR, 2012, NATURE, V485, P372, DOI 10.1038/nature11076
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Itoh M, 2017, NEURAL COMPUT, V29, P1263, DOI 10.1162/NECO_a_00952
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Kohno T, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00273
   Leleu T, 2015, PHYS REV E, V91, DOI 10.1103/PhysRevE.91.022804
   Levnajic Z, 2011, PHYS REV LETT, V107, DOI 10.1103/PhysRevLett.107.034101
   Luczak A, 2015, NAT REV NEUROSCI, V16, P745, DOI 10.1038/nrn4026
   Muthmann JO, 2015, FRONT NEUROINFORM, V9, DOI 10.3389/fninf.2015.00028
   Nenadic Z, 2005, IEEE T BIO-MED ENG, V52, P74, DOI 10.1109/TBME.2004.839800
   Pajevic S, 2009, PLOS COMPUT BIOL, V5, DOI 10.1371/journal.pcbi.1000271
   PERKEL DH, 1967, BIOPHYS J, V7, P419, DOI 10.1016/S0006-3495(67)86597-4
   Schreiber T, 2000, PHYS REV LETT, V85, P461, DOI 10.1103/PhysRevLett.85.461
   Timme M, 2007, PHYS REV LETT, V98, DOI 10.1103/PhysRevLett.98.224101
   Van Bussel F, 2011, FRONT COMPUT NEUROSC, V5, DOI 10.3389/fncom.2011.00003
NR 26
TC 0
Z9 0
U1 0
U2 1
PY 2018
VL 9
IS 2
SI SI
BP 281
EP 294
DI 10.1587/nolta.9.281
UT WOS:000551504200013
DA 2023-11-16
ER

PT J
AU Lu, JQ
   Wu, XN
   Cao, S
   Wang, XK
   Yu, HC
AF Lu, Junqi
   Wu, Xinning
   Cao, Su
   Wang, Xiangke
   Yu, Huangchao
TI An Implementation of Actor-Critic Algorithm on Spiking Neural Network
   Using Temporal Coding Method
SO APPLIED SCIENCES-BASEL
DT Article
DE spiking neural network; actor-critic algorithm; temporal coding; UAV
ID MODEL; REINFORCEMENT
AB Featured Application Rapid decision-making on micro drones. Taking advantage of faster speed, less resource consumption and better biological interpretability of spiking neural networks, this paper developed a novel spiking neural network reinforcement learning method using actor-critic architecture and temporal coding. The simple improved leaky integrate-and-fire (LIF) model was used to describe the behavior of a spike neuron. Then the actor-critic network structure and the update formulas using temporally encoded information were provided. The current model was finally examined in the decision-making task, the gridworld task, the UAV flying through a window task and the avoiding a flying basketball task. In the 5 x 5 grid map, the value function learned was close to the ideal situation and the quickest way from one state to another was found. A UAV trained by this method was able to fly through the window quickly in simulation. An actual flight test of a UAV avoiding a flying basketball was conducted. With this model, the success rate of the test was 96% and the average decision time was 41.3 ms. The results show the effectiveness and accuracy of the temporal coded spiking neural network RL method. In conclusion, an attempt was made to provide insights into developing spiking neural network reinforcement learning methods for decision-making and autonomous control of unmanned systems.
C1 [Lu, Junqi; Wu, Xinning; Cao, Su; Wang, Xiangke; Yu, Huangchao] Natl Univ Def Technol, Coll Intelligence Sci & Technol, Changsha 410073, Peoples R China.
RP Yu, HC (corresponding author), Natl Univ Def Technol, Coll Intelligence Sci & Technol, Changsha 410073, Peoples R China.
EM yuhc1221@nudt.edu.cn
CR Baras D, 2007, BMC NEUROSCI, V8, P197, DOI [10.1186/1471-2202-8-S2-P197, DOI 10.1186/1471-2202-8-S2-P197]
   Bellec G, 2018, ADV NEUR IN, V31
   Bing ZS, 2018, IEEE INT CONF ROBOT, P4725
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Cao S, 2022, IEEE ROBOT AUTOM LET, V7, P5771, DOI 10.1109/LRA.2022.3153987
   Comsa IM, 2022, IEEE T NEUR NET LEAR, V33, P5939, DOI 10.1109/TNNLS.2021.3071976
   Doya K, 2007, HFSP J, V1, P30, DOI [10.2976/1.2732246, 10.2976/1.2732246/10.2976/1]
   Florian R, 2003, AUTONOMOUS ARTIFICIA
   Florian RV, 2007, NEURAL COMPUT, V19, P1468, DOI 10.1162/neco.2007.19.6.1468
   Foster DJ, 2000, HIPPOCAMPUS, V10, P1, DOI 10.1002/(SICI)1098-1063(2000)10:1<1::AID-HIPO1>3.0.CO;2-1
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   [胡一凡 Hu Yifan], 2021, [控制与决策, Control and Decision], V36, P1
   Kun Xiao, 2020, 2020 4th International Conference on Robotics and Automation Sciences (ICRAS), P55, DOI 10.1109/ICRAS49812.2020.9134922
   Lee C, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00119
   Lee D, 2012, ANNU REV NEUROSCI, V35, P287, DOI 10.1146/annurev-neuro-062111-150512
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maass W, 1997, ADV NEUR IN, V9, P211
   Mostafa H, 2018, IEEE T NEUR NET LEAR, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Ozawa S, 2004, STUD FUZZ SOFT COMP, V152, P238
   Potjans W, 2009, NEURAL COMPUT, V21, P301, DOI 10.1162/neco.2008.08-07-593
   Qi Y, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1597
   Rao RPN, 2001, NEURAL COMPUT, V13, P2221, DOI 10.1162/089976601750541787
   Seung HS, 2003, NEURON, V40, P1063, DOI 10.1016/S0896-6273(03)00761-X
   Suri RE, 2001, NEURAL COMPUT, V13, P841, DOI 10.1162/089976601300014376
   Takita K., 2005, Systems and Computers in Japan, V36, P42, DOI 10.1002/scj.10645
   Virtanen K, 1999, J AIRCRAFT, V36, P632, DOI 10.2514/2.2505
   Wei H, 2017, COGN NEURODYNAMICS, V11, P415, DOI 10.1007/s11571-017-9436-2
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Xie XH, 2004, PHYS REV E, V69, DOI 10.1103/PhysRevE.69.041909
   [张铁林 Zhang Tielin], 2021, [计算机学报, Chinese Journal of Computers], V44, P1767
   Zhao FF, 2018, FRONT NEUROROBOTICS, V12, DOI 10.3389/fnbot.2018.00056
   Zhao FF, 2018, COGN COMPUT, V10, P296, DOI 10.1007/s12559-017-9511-3
NR 32
TC 1
Z9 1
U1 8
U2 18
PD OCT
PY 2022
VL 12
IS 20
AR 10430
DI 10.3390/app122010430
UT WOS:000872181100001
DA 2023-11-16
ER

PT C
AU Yedjour, H
   Meftah, B
   Yedj, D
   Benyettou, A
AF Yedjour, Hayat
   Meftah, Boudjelal
   Yedj, Dounia
   Benyettou, Abdelkader
GP ACM
TI The leaky integrate-and-fire neuron model for a rigid and a non-rigid
   object tracking
SO PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON SOFTWARE ENGINEERING
   AND NEW TECHNOLOGIES (ICSENT '18)
DT Proceedings Paper
CT 7th ACM International Conference on Software Engineering and New
   Technologies (ICSENT)
CY DEC 26-28, 2018
CL Hammamet, TUNISIA
DE Spiking neural network; leaky integrate and fire neuron model; edge
   detection; object tracking; Hausdorff distance
AB Spiking neural networks (SNNs) fall into the third generation of artificial neural network models, increasing the level of realism in a neural simulation. In this paper, a spiking neural network is presented for detecting and tracking of a moving object in video sequences with a static camera. The motion estimation of the object is carried out by minimizing a Hausdorff distance measure. The system has been successfully tested with various real video sequences. The results showed that our system can track the identified target over subsequent video frames even in occlusion case.
C1 [Yedjour, Hayat; Yedj, Dounia; Benyettou, Abdelkader] Univ Mohamed Boudiaf, Lab Signal & Parole, Oran, Algeria.
   [Meftah, Boudjelal] Univ Mustapha Stambouli, Lab LRSBG, Mascara, Algeria.
RP Yedjour, H (corresponding author), Univ Mohamed Boudiaf, Lab Signal & Parole, Oran, Algeria.
EM hyedjour@yahoo.fr; meftahb@yahoo.fr; dyedjour@yahoo.fr;
   a_benyettou@yahoo.fr
CR Abbott LF, 1999, BRAIN RES BULL, V50, P303, DOI 10.1016/S0361-9230(99)00161-6
   Belogay E, 1997, INFORM PROCESS LETT, V64, P17, DOI 10.1016/S0020-0190(97)00140-3
   Dayan P., 2001, THEORETICAL NEUROSCI
   Kim C, 2002, IEEE T CIRC SYST VID, V12, P122, DOI 10.1109/76.988659
   LEYMARIE F, 1993, IEEE T PATTERN ANAL, V15, P617, DOI 10.1109/34.216733
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maguire L.P., 2006, STUDIES COMPUTATIONA, V35
   Paugam-Moisy H., 2006, SPIKING NEURON NETWO, P2
   PHUTTNLOCHER D, 1993, IEEE T PAMI, V15, P850
   Serra J, 2006, J MATH IMAGING VIS, V24, P83, DOI 10.1007/s10851-005-3616-0
NR 10
TC 0
Z9 0
U1 0
U2 2
PY 2018
DI 10.1145/3330089.3330096
UT WOS:000508203600001
DA 2023-11-16
ER

PT J
AU Iakymchuk, T
   Rosado-Muñoz, A
   Guerrero-Martínez, JF
   Bataller-Mompeán, M
   Francés-Víllora, JV
AF Iakymchuk, Taras
   Rosado-Munoz, Alfredo
   Guerrero-Martinez, Juan F.
   Bataller-Mompean, Manuel
   Frances-Villora, Jose V.
TI Simplified spiking neural network architecture and STDP learning
   algorithm applied to image classification
SO EURASIP JOURNAL ON IMAGE AND VIDEO PROCESSING
DT Article
DE Spiking neural networks - SNN; STDP; Visual receptive fields; Spike
   coding; Embedded system; Artificial neuron; Image classification
ID NEURONS; CHIP
AB Spiking neural networks (SNN) have gained popularity in embedded applications such as robotics and computer vision. The main advantages of SNN are the temporal plasticity, ease of use in neural interface circuits and reduced computation complexity. SNN have been successfully used for image classification. They provide a model for the mammalian visual cortex, image segmentation and pattern recognition. Different spiking neuron mathematical models exist, but their computational complexity makes them ill-suited for hardware implementation. In this paper, a novel, simplified and computationally efficient model of spike response model (SRM) neuron with spike-time dependent plasticity (STDP) learning is presented. Frequency spike coding based on receptive fields is used for data representation; images are encoded by the network and processed in a similar manner as the primary layers in visual cortex. The network output can be used as a primary feature extractor for further refined recognition or as a simple object classifier. Results show that the model can successfully learn and classify black and white images with added noise or partially obscured samples with up to x20 computing speed-up at an equivalent classification ratio when compared to classic SRM neuron membrane models. The proposed solution combines spike encoding, network topology, neuron membrane model and STDP learning.
C1 [Iakymchuk, Taras; Rosado-Munoz, Alfredo; Guerrero-Martinez, Juan F.; Bataller-Mompean, Manuel; Frances-Villora, Jose V.] Univ Valencia, ETSE, GPDS, E-46100 Valencia, Spain.
RP Iakymchuk, T (corresponding author), Univ Valencia, ETSE, GPDS, Ave Univ, E-46100 Valencia, Spain.
EM taras.yakymchuk@uv.es; Alfredo.Rosado@uv.es; Juan.Guerrero@uv.es;
   Manuel.Bataller@uv.es; Jose.V.Frances@uv.es
CR Alnajjar F, 2008, IEEE IJCNN, P2200, DOI 10.1109/IJCNN.2008.4634102
   Ambard M, 2008, DELTA 2008: FOURTH IEEE INTERNATIONAL SYMPOSIUM ON ELECTRONIC DESIGN, TEST AND APPLICATIONS, PROCEEDINGS, P394, DOI 10.1109/DELTA.2008.116
   [Anonymous], 2011, DS160 XIL INC
   [Anonymous], 2006, P 14 EUROPEAN S ARTI
   [Anonymous], 2000, J NEUROSCI
   Pérez-Carrasco JA, 2013, IEEE T PATTERN ANAL, V35, P2706, DOI 10.1109/TPAMI.2013.71
   Pérez-Carrasco JA, 2010, IEEE T NEURAL NETWOR, V21, P609, DOI 10.1109/TNN.2009.2039943
   Argüello E, 2012, IEEE ENG MED BIO, P1234, DOI 10.1109/EMBC.2012.6346160
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Booij O., 2004, TEMPORAL PATTERN CLA
   Botzheim J, 2012, JOINT INT CONF SOFT, P1954, DOI 10.1109/SCIS-ISIS.2012.6505305
   Bouganis A., 2010, P 2010 INT JOINT C N, P1, DOI DOI 10.1109/IJCNN.2010.5596525
   Fang HJ, 2010, NEURAL COMPUT, V22, P1060, DOI 10.1162/neco.2009.10-08-885
   Foldiak P, 1998, HDB BRAIN THEORY NEU, P895
   Gerstner W., 2002, SPIKING NEURON MODEL
   Hylton T., 2008, SYATEMS NEUROMORPHIC
   Ishikawa Y, 2004, PROCEEDINGS OF 2004 IEEE ASIA-PACIFIC CONFERENCE ON ADVANCED SYSTEM INTEGRATED CIRCUITS, P436, DOI 10.1109/APASIC.2004.1349526
   Lorenzo R, 2006, IEEE IJCNN, P3903
   Lovejoy JC, 2010, NUTR HEALTH SER, P1, DOI 10.1007/978-1-60327-431-9_1
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Martinez LM, 2003, NEUROSCIENTIST, V9, P317, DOI 10.1177/1073858403252732
   Painkras E, 2013, IEEE J SOLID-ST CIRC, V48, P1943, DOI 10.1109/JSSC.2013.2259038
   PAUGAMMOISY H, 2009, HDB NATURAL COMPUTIN
   Pharn DT, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON INDUSTRIAL ELECTRONICS, PROCEEDINGS, VOLS 1-8, P3441, DOI 10.1109/ISIE.2007.4375170
   Ratnasingam S, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P880, DOI 10.1109/IJCNN.2011.6033314
   Repository UML, 2014, SEM HANDW DIG DAT
   Rice KL, 2009, 2009 INTERNATIONAL CONFERENCE ON RECONFIGURABLE COMPUTING AND FPGAS, P451, DOI 10.1109/ReConFig.2009.77
   Schemmel Johannes, 2012, 2012 IEEE International Symposium on Circuits and Systems - ISCAS 2012, DOI 10.1109/ISCAS.2012.6272131
   Schoenauer T, 2002, IEEE T NEURAL NETWOR, V13, P205, DOI 10.1109/72.977304
NR 29
TC 60
Z9 63
U1 3
U2 43
PD FEB 19
PY 2015
AR 4
DI 10.1186/s13640-015-0059-4
UT WOS:000350250200001
DA 2023-11-16
ER

PT C
AU Banerjee, S
   Ghosh, S
   Banerjee, A
   Mohalik, SK
AF Banerjee, Soham
   Ghosh, Sumana
   Banerjee, Ansuman
   Mohalik, Swarup K.
BE Dragoi, C
   Emmi, M
   Wang, J
TI SMT-Based Modeling and Verification of Spiking Neural Networks: A Case
   Study
SO VERIFICATION, MODEL CHECKING, AND ABSTRACT INTERPRETATION, VMCAI 2023
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 24th International Conference on Verification, Model Checking, and
   Abstract Interpretation (VMCAI) part of the 50th ACM SIGPLAN Symposium
   on Principles of Programming Languages (POPL)
CY JAN 16-17, 2023
CL Boston, MA
DE Spiking neural networks; Satisfiability modulo theory; Verification;
   Adversarial robustness
AB In this paper, we present a case study on modeling and verification of Spiking Neural Networks (SNN) using Satisfiability Modulo Theory (SMT) solvers. SNN are special neural networks that have great similarity in their architecture and operation with the human brain. These networks have shown similar performance when compared to traditional networks with comparatively lesser energy requirement. We discuss different properties of SNNs and their functioning. We then use Z3, a popular SMT solver to encode the network and its properties. Specifically, we use the theory of Linear Real Arithmetic (LRA). Finally, we present a framework for verification and adversarial robustness analysis and demonstrate it on the Iris and MNIST benchmarks.
C1 [Banerjee, Soham; Ghosh, Sumana; Banerjee, Ansuman] Indian Stat Inst, Kolkata, India.
   [Mohalik, Swarup K.] Ericsson Res, Bangalore, Karnataka, India.
RP Banerjee, A (corresponding author), Indian Stat Inst, Kolkata, India.
EM ansuman@isical.ac.in
CR Alur R., 1999, Computer Aided Verification. 11th International Conference, CAV'99. Proceedings (Lecture Notes in Computer Science Vol.1633), P8
   Aman B, 2016, THEOR COMPUT SCI, V623, P92, DOI 10.1016/j.tcs.2015.11.005
   [Anonymous], CODE BENCHMARKS
   Barrett C., 2018, TECHNICAL REPORT
   De Maria E., 2018, SPIKING NEURAL NETWO
   de Maria E., 2018, BIOINFORMATICS 2018, P1
   De Maria E, 2016, LECT N BIOINFORMAT, V9957, P97, DOI 10.1007/978-3-319-47151-8_7
   de Moura L, 2008, LECT NOTES COMPUT SC, V4963, P337, DOI 10.1007/978-3-540-78800-3_24
   Demin V, 2018, FRONT NEUROINFORM, V12, DOI 10.3389/fninf.2018.00079
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Ding J., 2021, OPTIMAL ANN SNN CONV
   Elboher YY, 2020, LECT NOTES COMPUT SC, V12224, P43, DOI 10.1007/978-3-030-53288-8_3
   Eshraghian J. K., 2021, TRAINING SPIKING NEU
   Fisher RA, 1988, UCI Machine Learning Repository
   Furui S, 2012, IEEE SIGNAL PROC MAG, V29, P16, DOI 10.1109/MSP.2012.2209906
   Gokulanathan S, 2020, LECT NOTES COMPUT SC, V12229, P85, DOI 10.1007/978-3-030-55754-6_5
   Goldberger B., 2020, P 23 INT C LOG PROGR, VVolume 73, P260
   Guo WZ, 2021, FRONT NEUROSCI-SWITZ, V15, DOI [10.3389/fnins.2021.638474, 10.1007/s11704-020-9230-x]
   Katz G, 2022, FORM METHOD SYST DES, V60, P87, DOI 10.1007/s10703-021-00363-7
   Kim T, 2021, FRONT COMPUT NEUROSC, V15, DOI 10.3389/fncom.2021.646125
   Lahav O., 2021, PRUNING SLICING NEUR
   Li SX, 2021, IEEE T CIRCUITS-I, V68, P1543, DOI 10.1109/TCSI.2021.3052885
   Liu TY, 2022, PLOS ONE, V17, DOI 10.1371/journal.pone.0264364
   Malik N., 2005, ARTIFICIAL NEURAL NE
   Stimberg M, 2019, ELIFE, V8, DOI 10.7554/eLife.47314
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Tjeng V., 2017, EVALUATING ROBUSTNES
   Yu ZQ, 2020, IEEE ACCESS, V8, P67085, DOI 10.1109/ACCESS.2020.2985839
NR 28
TC 0
Z9 0
U1 0
U2 1
PY 2023
VL 13881
BP 25
EP 43
DI 10.1007/978-3-031-24950-1_2
UT WOS:000968142400002
DA 2023-11-16
ER

PT C
AU Miao, Y
   Tang, HJ
   Pan, G
AF Miao, Yu
   Tang, Huajin
   Pan, Gang
GP IEEE
TI A Supervised Multi-Spike Learning Algorithm for Spiking Neural Networks
SO 2018 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUL 08-13, 2018
CL Rio de Janeiro, BRAZIL
DE spiking neural network (SNN); multi-spike learning; supervised learning;
   sound recognition
AB The formulation of efficient supervised learning algorithms for Spiking Neural Network (SNN) is difficult and remains challenging. This paper presents a supervised multi spike learning algorithm, which is used to train neurons to output spike train with a target firing rate. The proposed algorithm simplifies the expression of the membrane potential by assuming a special condition of the threshold, thus allows the application of a gradient descent to optimize the synaptic weights. Additionally, in the presented experimental results, the proposed algorithm is evaluated regarding its initial setups, its classification performance for rate-based and timing-based patterns and its capability to sound recognition. The results also demonstrate that the proposed algorithm can achieve a competitive accuracy in temporal pattern classification and sound recognition.
C1 [Miao, Yu; Tang, Huajin] Sichuan Univ, Coll Comp Sci, Chengdu, Peoples R China.
   [Pan, Gang] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou, Peoples R China.
RP Miao, Y (corresponding author), Sichuan Univ, Coll Comp Sci, Chengdu, Peoples R China.
EM miaoyu@stu.scu.edu.cn; htang@scu.edu.cn; gpan@zju.edu.cn
CR Bohte Sander M., 2004, Natural Computing, V3, P195, DOI 10.1023/B:NACO.0000027755.02868.60
   Brader JM, 2007, NEURAL COMPUT, V19, P2881, DOI 10.1162/neco.2007.19.11.2881
   Butts DA, 2007, NATURE, V449, P92, DOI [10.1038/nature06105, 10.1038/natureO6105]
   Dennis J, 2013, INT CONF ACOUST SPEE, P803, DOI 10.1109/ICASSP.2013.6637759
   Florian RV, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0040233
   Gerstner W., 2002, SPIKING NEURON MODEL
   Gütig R, 2016, SCIENCE, V351, DOI 10.1126/science.aab4113
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Mohemmed A, 2012, INT J NEURAL SYST, V22, DOI 10.1142/S0129065712500128
   Nakamura S., 2000, LREC
   Ponulak F., 2005, RESUME NEW SUPERVISE
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Tiesinga P, 2008, NAT REV NEUROSCI, V9, P97, DOI 10.1038/nrn2315
   VanRullen R, 2005, TRENDS NEUROSCI, V28, P1, DOI 10.1016/j.tins.2004.10.010
   Yang F, 2016, INT CONF SIGN PROCES, P584, DOI 10.1109/ICSP.2016.7877900
   Yu Q, 2017, LECT NOTES COMPUT SC, V10639, P759, DOI 10.1007/978-3-319-70136-3_80
   Yu Q, 2016, IEEE T NEUR NET LEAR, V27, P621, DOI 10.1109/TNNLS.2015.2416771
   Yu Q, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0078318
NR 18
TC 0
Z9 0
U1 0
U2 1
PY 2018
BP 420
EP 426
UT WOS:000585967400057
DA 2023-11-16
ER

PT C
AU Shi, WY
AF Shi, Weiya
BE Zhang, H
   Shen, G
   Jin, D
TI Soft-Reward Based Reinforcement Learning by Spiking Neural Networks
SO ADVANCED RESEARCH ON INFORMATION SCIENCE, AUTOMATION AND MATERIAL
   SYSTEM, PTS 1-6
SE Advanced Materials Research
DT Proceedings Paper
CT International Conference on Information Science, Automation and Material
   System
CY MAY 21-22, 2011
CL Zhengzhou, PEOPLES R CHINA
DE Spike; Reinforcement; Adaptability; Soft-rewarded
ID NEURONS
AB In this paper, we propose algorithm based reinforcement learning for spiking neural networks. The algorithm simulates biological adaptability and uses the soft-reward from environment to modulate the synaptic weight, which combines spike-timing-dependent plasticity (STDP), winner-take-all mechanism. The algorithm is tested to classify a number of standard benchmark dataset. The obtained results show the effectiveness of the proposed algorithm.
C1 Henan Univ Technol, Sch Informat Sci & Engn, Zhengzhou 450001, Henan Province, Peoples R China.
RP Shi, WY (corresponding author), Henan Univ Technol, Sch Informat Sci & Engn, Zhengzhou 450001, Henan Province, Peoples R China.
EM wyshi@fudan.edu.cn
CR Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   Dayan P., 2001, THEORETICAL NEUROSCI
   Gerstner W., 2002, SPIKING NEURON MODEL
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maass W., 1999, PULSED NEURAL NETWOR
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
NR 7
TC 0
Z9 0
U1 0
U2 4
PY 2011
VL 219-220
BP 770
EP 773
DI 10.4028/www.scientific.net/AMR.219-220.770
PN 1-6
UT WOS:000292631200164
DA 2023-11-16
ER

PT J
AU Macdonald, FLA
   Lepora, NF
   Conradt, J
   Ward-Cherrier, B
AF Macdonald, Fraser L. A.
   Lepora, Nathan F.
   Conradt, Jorg
   Ward-Cherrier, Benjamin
TI Neuromorphic Tactile Edge Orientation Classification in an Unsupervised
   Spiking Neural Network
SO SENSORS
DT Article
DE tactile robotics; neuromorphic; spiking neural network
ID PERCEPTION; SIGNALS; SURFACE; SYSTEM
AB Dexterous manipulation in robotic hands relies on an accurate sense of artificial touch. Here we investigate neuromorphic tactile sensation with an event-based optical tactile sensor combined with spiking neural networks for edge orientation detection. The sensor incorporates an event-based vision system (mini-eDVS) into a low-form factor artificial fingertip (the NeuroTac). The processing of tactile information is performed through a Spiking Neural Network with unsupervised Spike-Timing-Dependent Plasticity (STDP) learning, and the resultant output is classified with a 3-nearest neighbours classifier. Edge orientations were classified in 10-degree increments while tapping vertically downward and sliding horizontally across the edge. In both cases, we demonstrate that the sensor is able to reliably detect edge orientation, and could lead to accurate, bio-inspired, tactile processing in robotics and prosthetics applications.
C1 [Macdonald, Fraser L. A.; Lepora, Nathan F.; Ward-Cherrier, Benjamin] Univ Bristol, Dept Engn Math, Bristol BS8 1TW, Avon, England.
   [Macdonald, Fraser L. A.; Lepora, Nathan F.; Ward-Cherrier, Benjamin] Univ West England, Bristol Robot Lab, Bristol BS34 8QZ, Avon, England.
   [Conradt, Jorg] KTH Royal Inst Technol, Sch Elect Engn & Comp Sci, S-11428 Stockholm, Sweden.
RP Macdonald, FLA (corresponding author), Univ Bristol, Dept Engn Math, Bristol BS8 1TW, Avon, England.; Macdonald, FLA (corresponding author), Univ West England, Bristol Robot Lab, Bristol BS34 8QZ, Avon, England.
EM fraser.macdonald@bristol.ac.uk
CR Aquilina K, 2019, IEEE INT CONF ROBOT, P4283, DOI [10.1109/icra.2019.8794307, 10.1109/ICRA.2019.8794307]
   Bing ZS, 2018, FRONT NEUROROBOTICS, V12, DOI 10.3389/fnbot.2018.00035
   Birkoben T, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-74219-1
   Bohte S. M., 2000, 8th European Symposium on Artificial Neural Networks. ESANN"2000. Proceedings, P419
   Brette R, 2005, J NEUROPHYSIOL, V94, P3637, DOI 10.1152/jn.00686.2005
   Davies M, 2021, P IEEE, V109, P911, DOI 10.1109/JPROC.2021.3067593
   Davison Andrew P, 2008, Front Neuroinform, V2, P11, DOI 10.3389/neuro.11.011.2008
   DeFelipe J, 2012, FRONT NEUROANAT, V6, DOI [10.3389/fnana.2012.00022, 10.3389/fnsyn.2012.00002, 10.3389/fnana.2012.00005]
   Delbruck Tobi, 2008, P INT S SECURE LIFE, P21
   Fardet Tanguy, 2020, NEST 2 20 0, DOI [10.5281/ zenodo.3605514, DOI 10.5281/ZENODO.3605514]
   Friedl KE, 2016, IEEE ROBOT AUTOM LET, V1, P516, DOI 10.1109/LRA.2016.2517213
   Furber S., 2020, SPINNAKER SPIKING NE
   Göger D, 2009, IEEE INT CONF ROBOT, P2972
   Gütig R, 2003, J NEUROSCI, V23, P3697
   James JW, 2018, IEEE ROBOT AUTOM LET, V3, P3340, DOI 10.1109/LRA.2018.2852797
   Johansson RS, 2009, NAT REV NEUROSCI, V10, P345, DOI 10.1038/nrn2621
   JOHANSSON RS, 1979, J PHYSIOL-LONDON, V286, P283, DOI 10.1113/jphysiol.1979.sp012619
   Johnson KO, 2001, CURR OPIN NEUROBIOL, V11, P455, DOI 10.1016/S0959-4388(00)00234-8
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Kumar D, 2020, NEUROCOMPUTING, V407, P246, DOI 10.1016/j.neucom.2020.04.131
   Lederman SJ, 1997, J EXP PSYCHOL HUMAN, V23, P1680, DOI 10.1037/0096-1523.23.6.1680
   Lee WW, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00005
   Legenstein R, 2005, NEURAL COMPUT, V17, P2337, DOI 10.1162/0899766054796888
   Lepora NE, 2019, IEEE ROBOT AUTOM LET, V4, P2101, DOI 10.1109/LRA.2019.2899192
   Li J, 2018, PROCEEDINGS OF 2018 IEEE 3RD ADVANCED INFORMATION TECHNOLOGY, ELECTRONIC AND AUTOMATION CONTROL CONFERENCE (IAEAC 2018), P1728, DOI 10.1109/IAEAC.2018.8577629
   Muhammad HB, 2011, MICROELECTRON ENG, V88, P1811, DOI 10.1016/j.mee.2011.01.045
   Muller G. R., 2011, 2011 IEEE International Conference on Robotics and Biomimetics (ROBIO), P2429, DOI 10.1109/ROBIO.2011.6181669
   Nichols E, 2010, INT J NEURAL SYST, V20, P501, DOI 10.1142/S0129065710002577
   Oddo CM, 2011, IEEE T ROBOT, V27, P522, DOI 10.1109/TRO.2011.2116930
   Parvizi-Fard A, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-020-80132-4
   Ponulak F., 2006, THESIS POZNAN U TECH
   Pruszynski JA, 2014, NAT NEUROSCI, V17, P1404, DOI 10.1038/nn.3804
   Rongala UB, 2019, FRONT NEUROROBOTICS, V13, DOI 10.3389/fnbot.2019.00044
   Shan Luo, 2014, IEEE Sensors 2014. Proceedings, P1030, DOI 10.1109/ICSENS.2014.6985179
   Shrestha SB, 2018, ADV NEUR IN, V31
   Stassi S, 2014, SENSORS-BASEL, V14, P5296, DOI 10.3390/s140305296
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Ward-Cherrier B, 2020, IEEE INT CONF ROBOT, P2654, DOI [10.1109/ICRA40945.2020.9197046, 10.1109/icra40945.2020.9197046]
   Ward-Cherrier B, 2018, SOFT ROBOT, V5, P216, DOI 10.1089/soro.2017.0052
   Wong RDP, 2014, IEEE T HAPTICS, V7, P191, DOI 10.1109/TOH.2013.56
   Xue FZ, 2013, NEUROCOMPUTING, V122, P324, DOI 10.1016/j.neucom.2013.06.019
   Yuan WZ, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17122762
   Yuan WZ, 2015, IEEE INT CONF ROBOT, P304, DOI 10.1109/ICRA.2015.7139016
   Zaghloul KA, 2006, J NEURAL ENG, V3, P257, DOI 10.1088/1741-2560/3/4/002
   Zhang T, 2013, IEEE SENS J, V13, P510, DOI 10.1109/JSEN.2012.2220345
NR 46
TC 1
Z9 1
U1 1
U2 12
PD SEP
PY 2022
VL 22
IS 18
AR 6998
DI 10.3390/s22186998
UT WOS:000856855600001
DA 2023-11-16
ER

PT J
AU Berlin, SJ
   John, M
AF Berlin, S. Jeba
   John, Mala
TI R-STDP Based Spiking Neural Network for Human Action Recognition
SO APPLIED ARTIFICIAL INTELLIGENCE
DT Article
ID HISTOGRAMS; FEATURES; PATTERN
AB Video surveillance systems are omnipresent and automatic monitoring of human activities is gaining importance in highly secured environments. The proposed work explores the use of the bio-inspired third generation neural network called spiking neural network (SNN) in order to recognize the action sequences present in a video. The SNN used in this work carries the neural information in terms of timing of spikes rather than the shape of the spikes. The learning technique used herein is reward-modulated spike time-dependent plasticity (R-STDP). It is based on reinforcement learning that modulates or demodulates the synaptic weights depending on the reward or the punishment signal that it receives from the decision layer. The absence of gradient descent techniques and external classifiers makes the system computationally efficient and simple. Finally, the performance of the network is evaluated on the two benchmark datasets, viz., Weizmann and KTH datasets.
C1 [Berlin, S. Jeba; John, Mala] Anna Univ, Dept Elect Engn, Madras Inst Technol, Chennai, Tamil Nadu, India.
RP Berlin, SJ (corresponding author), Madras Inst Technol, Dept Elect Engn, Campus Anna Univ, Chennai 600044, Tamil Nadu, India.
EM jebaberlin@gmail.com
CR Aggarwal JK, 2011, ACM COMPUT SURV, V43, DOI 10.1145/1922649.1922653
   Al-Berry MN, 2016, IET COMPUT VIS, V10, P153, DOI 10.1049/iet-cvi.2015.0087
   [Anonymous], P 2010 IEEE ANT PROP
   [Anonymous], 2012 INT JOINT C NEU, DOI DOI 10.1109/IJCNN.2012.6252439
   Berlin SJ, 2016, INT CARN CONF SECU, P143
   Blank M, 2005, IEEE I CONF COMP VIS, P1395
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Cao XQ, 2015, IEEE T FUZZY SYST, V23, P1581, DOI 10.1109/TFUZZ.2014.2370678
   CHENG J, 2015, IEEE T IMAGE PROCESS, V24
   Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177
   Dalal N, 2006, LECT NOTES COMPUT SC, V3952, P428, DOI 10.1007/11744047_33
   Deng ZW, 2016, PROC CVPR IEEE, P4772, DOI 10.1109/CVPR.2016.516
   Du WB, 2018, IEEE T IMAGE PROCESS, V27, P1347, DOI 10.1109/TIP.2017.2778563
   Escobar MJ, 2012, COMPUT VIS IMAGE UND, V116, P593, DOI 10.1016/j.cviu.2012.01.002
   Gao YB, 2018, IEEE ACCESS, V6, P52277, DOI 10.1109/ACCESS.2018.2869790
   Colque RVHM, 2017, IEEE T CIRC SYST VID, V27, P673, DOI 10.1109/TCSVT.2016.2637778
   Jhuang H, 2007, IEEE I CONF COMP VIS, P1253
   Kamel A, 2019, INT J HUM-COMPUT INT, V35, P427, DOI 10.1080/10447318.2018.1543081
   Liu AA, 2015, IEEE T CYBERNETICS, V45, P1194, DOI 10.1109/TCYB.2014.2347057
   Liu HH, 2018, IEEE T NEUR NET LEAR, V29, P1427, DOI 10.1109/TNNLS.2017.2669522
   Liu L, 2016, IEEE T CYBERNETICS, V46, P158, DOI 10.1109/TCYB.2015.2399172
   Maity S, 2017, IETE J RES, V63, P160, DOI 10.1080/03772063.2016.1242383
   Meng Y, 2011, IEEE T NEURAL NETWOR, V22, P1952, DOI 10.1109/TNN.2011.2171044
   Moayedi F, 2016, APPL ARTIF INTELL, V30, P297, DOI 10.1080/08839514.2016.1169094
   Mozafari M, 2018, IEEE T NEUR NET LEAR, V29, P6178, DOI 10.1109/TNNLS.2018.2826721
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Schüldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462
   Shan YH, 2015, IEEE T CIRC SYST VID, V25, P1624, DOI 10.1109/TCSVT.2014.2376136
   Vishwakarma S, 2013, VISUAL COMPUT, V29, P983, DOI 10.1007/s00371-012-0752-6
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Xiang YD, 2018, INT J PARALLEL EMERG, V33, P526, DOI 10.1080/17445760.2017.1399206
   Xu K, 2017, IEEE T CIRC SYST VID, V27, P567, DOI 10.1109/TCSVT.2017.2665359
   Xu XL, 2018, IEEE T COGN DEV SYST, V10, P205, DOI 10.1109/TCDS.2017.2769166
   Yao L, 2016, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-016-0145-2
   Yu Q, 2013, IEEE T NEUR NET LEAR, V24, P1539, DOI 10.1109/TNNLS.2013.2245677
   Zhang ML, 2019, IEEE T NEUR NET LEAR, V30, P123, DOI 10.1109/TNNLS.2018.2833077
NR 36
TC 6
Z9 6
U1 0
U2 12
PD JUL 28
PY 2020
VL 34
IS 9
BP 656
EP 673
DI 10.1080/08839514.2020.1765110
EA MAY 2020
UT WOS:000536316400001
DA 2023-11-16
ER

PT J
AU Petschenig, H
   Legenstein, R
AF Petschenig, Horst
   Legenstein, Robert
TI Quantized rewiring: hardware-aware training of sparse deep neural
   networks
SO NEUROMORPHIC COMPUTING AND ENGINEERING
DT Article
DE network rewiring; hardware-aware training; sparse networks; efficient
   networks; weight quantization; spiking neural networks
ID ON-CHIP; ARCHITECTURE; PLASTICITY; SYSTEM; LOIHI
AB Mixed-signal and fully digital neuromorphic systems have been of significant interest for deploying spiking neural networks in an energy-efficient manner. However, many of these systems impose constraints in terms of fan-in, memory, or synaptic weight precision that have to be considered during network design and training. In this paper, we present quantized rewiring (Q-rewiring), an algorithm that can train both spiking and non-spiking neural networks while meeting hardware constraints during the entire training process. To demonstrate our approach, we train both feedforward and recurrent neural networks with a combined fan-in/weight precision limit, a constraint that is, for example, present in the DYNAP-SE mixed-signal analog/digital neuromorphic processor. Q-rewiring simultaneously performs quantization and rewiring of synapses and synaptic weights through gradient descent updates and projecting the trainable parameters to a constraint-compliant region. Using our algorithm, we find trade-offs between the number of incoming connections to neurons and network performance for a number of common benchmark datasets.
C1 [Petschenig, Horst; Legenstein, Robert] Graz Univ Technol, Inst Theoret Comp Sci, A-8010 Graz, Austria.
RP Petschenig, H (corresponding author), Graz Univ Technol, Inst Theoret Comp Sci, A-8010 Graz, Austria.
EM petschenig@igi.tugraz.at
CR BACKUS J, 1978, COMMUN ACM, V21, P613, DOI 10.1145/359576.359579
   Bellec G, 2018, Arxiv, DOI arXiv:1711.05136
   Bellec G, 2018, ADV NEUR IN, V31
   Bellec G, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-17236-y
   Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Billaudelle S, 2021, NEURAL NETWORKS, V133, P11, DOI 10.1016/j.neunet.2020.09.024
   Bishop CM., 2006, PATTERN RECOGN
   Bohnstingl T, 2022, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2022.3153985
   Cramer B, 2022, P NATL ACAD SCI USA, V119, DOI 10.1073/pnas.2109194119
   Cubuk ED, 2020, IEEE COMPUT SOC CONF, P3008, DOI 10.1109/CVPRW50498.2020.00359
   Davies M, 2021, P IEEE, V109, P911, DOI 10.1109/JPROC.2021.3067593
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Esser SK, 2016, P NATL ACAD SCI USA, V113, P11441, DOI 10.1073/pnas.1604850113
   Anthony LFW, 2020, Arxiv, DOI arXiv:2007.03051
   Franco J, 2021, PHYSIOTHER THEOR PR, V37, P1419, DOI 10.1080/09593985.2019.1709234
   Frenkel C., 2022, 2022 IEEE INT SOL ST, DOI [10.1109/ISSCC42614.2022.9731734, DOI 10.1109/ISSCC42614.2022.9731734]
   Gao Chang, 2022, IEEE Trans Neural Netw Learn Syst, VPP, DOI 10.1109/TNNLS.2022.3180209
   Göltz J, 2021, NAT MACH INTELL, V3, P823, DOI 10.1038/s42256-021-00388-x
   Han S, 2015, ADV NEUR IN, V28
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hendrycks D, 2022, PROC CVPR IEEE, P16762, DOI 10.1109/CVPR52688.2022.01628
   Holtmaat A, 2009, NAT REV NEUROSCI, V10, P647, DOI 10.1038/nrn2699
   Hubara I, 2018, J MACH LEARN RES, V18
   Jin Q, 2020, PROC CVPR IEEE, P2143, DOI 10.1109/CVPR42600.2020.00222
   Kaiming H., 2015, PROC IEEE INT C COMP
   Kappel D, 2015, ADV NEUR IN, V28
   Kappel D, 2015, PLOS COMPUT BIOL, V11, DOI 10.1371/journal.pcbi.1004485
   Krizhevsky A., 2012, NEURIPS, VVolume 25
   Krizhevsky Alex, 2009, LEARNING MULTIPLE LA
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Liang TL, 2021, NEUROCOMPUTING, V461, P370, DOI 10.1016/j.neucom.2021.07.045
   Liu C, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00840
   Liu Zhuang, 2019, INT C LEARNING REPRE
   Loshchilov I., 2016, ARXIV
   Maass W, 2001, PULSED NEURAL NETWOR
   Majumdar S, 2020, INTERSPEECH, P3356, DOI 10.21437/Interspeech.2020-1058
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Moradi S, 2018, IEEE T BIOMED CIRC S, V12, P106, DOI 10.1109/TBCAS.2017.2759700
   Morcos AS, 2016, NAT NEUROSCI, V19, P1672, DOI 10.1038/nn.4403
   Neckar A, 2019, P IEEE, V107, P144, DOI 10.1109/JPROC.2018.2881432
   Nocedal J., 1999, NUMERICAL OPTIMIZATI, DOI [10.1007/b98874, DOI 10.1007/B98874]
   Nokland A, 2016, ADV NEUR IN, V29
   Painkras E, 2013, IEEE J SOLID-ST CIRC, V48, P1943, DOI 10.1109/JSSC.2013.2259038
   Pei J, 2019, NATURE, V572, P106, DOI 10.1038/s41586-019-1424-8
   Pellegrini T, 2021, IEEE W SP LANG TECH, P97, DOI 10.1109/SLT48900.2021.9383587
   Qiao N, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00141
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Salaj D, 2021, ELIFE, V10, DOI 10.7554/eLife.65459
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Schmitt S, 2017, IEEE IJCNN, P2227, DOI 10.1109/IJCNN.2017.7966125
   Schneider S., 2019, ARXIV
   Srinivas S., 2015, BRIT MACH VIS C, V31, P1, DOI [DOI 10.5244/C.29.31, 10.5244/C.29.31]
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Tan MX, 2019, PR MACH LEARN RES, V97
   Le QV, 2015, Arxiv, DOI arXiv:1504.00941
   van Baalen Mart, 2020, ARXIV200507093
   Wan WE, 2020, ISSCC DIG TECH PAP I, P498, DOI 10.1109/ISSCC19947.2020.9062979
   Warden P, 2018, Arxiv, DOI arXiv:1804.03209
   Welling Max, 2011, P 28 INT C MACH LEAR, P681
   Werbos Paul John, 1994, ROOTS BACKPROPAGATIO
   WERBOS PJ, 1990, P IEEE, V78, P1550, DOI 10.1109/5.58337
   Yan BN, 2019, S VLSI TECH, pT86, DOI [10.23919/vlsit.2019.8776485, 10.23919/VLSIT.2019.8776485]
NR 62
TC 1
Z9 1
U1 2
U2 2
PD JUN 1
PY 2023
VL 3
IS 2
AR 024006
DI 10.1088/2634-4386/accd8f
UT WOS:001064851400001
DA 2023-11-16
ER

PT C
AU Yilmaz, E
   Gevrek, ÖB
   Wu, JB
   Chen, YX
   Meng, XB
   Li, HZ
AF Yilmaz, Emre
   Gevrek, Ozgur Bora
   Wu, Jibin
   Chen, Yuxiang
   Meng, Xuanbo
   Li, Haizhou
GP Int Speech Commun Assoc
TI Deep Convolutional Spiking Neural Networks for Keyword Spotting
SO INTERSPEECH 2020
SE Interspeech
DT Proceedings Paper
CT Interspeech Conference
CY OCT 25-29, 2020
CL Shanghai, PEOPLES R CHINA
DE spiking neural networks; keyword spotting; wakeword detection; tandem
   learning; deep learning
AB This paper investigates the use of deep convolutional spiking neural networks (SNN) for keyword spotting (KWS) and wakeword detection tasks. The brain-inspired SNN mimic the spike-based information processing of biological neural networks and they can operate on the emerging ultra-low power neuromorphic chips. Unlike conventional artificial neural networks (ANN), SNN process input information asynchronously in an event-driven manner. With temporally sparse input information, this event-driven processing substantially reduces the computational requirements compared to the synchronous computation performed in ANN-based KWS approaches. To explore the effectiveness and computational complexity of SNN on KWS and wakeword detection, we compare the performance and computational costs of spiking fully-connected and convolutional neural networks with ANN counterparts under clean and noisy testing conditions. The results obtained on the Speech Commands and Hey Snips corpora have shown the effectiveness of the convolutional SNN model compared to a conventional CNN with comparable performance on KWS and better performance on the wakeword detection task. With its competitive performance and reduced computational complexity, convolutional SNN models running on energy-efficient neuromorphic hardware offer a low-power and effective solution for mobile KWS applications.
C1 [Yilmaz, Emre; Gevrek, Ozgur Bora; Wu, Jibin; Chen, Yuxiang; Meng, Xuanbo; Li, Haizhou] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore, Singapore.
RP Yilmaz, E (corresponding author), Natl Univ Singapore, Dept Elect & Comp Engn, Singapore, Singapore.
EM emrey@kth.se
CR [Anonymous], 2019, ARXIV190109948
   Bellec G., 2018, ADV NEURAL INFORM PR
   Blouw P., 2019, NICE 19
   Blouw P, 2020, INT CONF ACOUST SPEE, P8534, DOI [10.1109/ICASSP40776.2020.9053043, 10.1109/icassp40776.2020.9053043]
   Chen G, 2014, 2014 IEEE INT C AC S, P4087, DOI [DOI 10.1109/ICASSP.2014.6854370, 10.1109/ICASSP.2014.6854370]
   Coucke A, 2019, INT CONF ACOUST SPEE, P6351, DOI 10.1109/ICASSP.2019.8683474
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Diehl P.U., 2015, 2015 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2015.7280696
   Fernández-Marqués J, 2018, PROCEEDINGS OF THE 2018 INTERNATIONAL WORKSHOP ON EMBEDDED AND MOBILE DEEP LEARNING (EMDL '18), P13, DOI 10.1145/3212725.3212731
   Hunsberger E., 2016, TRAINING SPIKING DEE
   Lengerich C., 2016, END TO END ARCHITECT
   Leroy D, 2019, INT CONF ACOUST SPEE, P6341, DOI 10.1109/ICASSP.2019.8683546
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Monroe D, 2014, COMMUN ACM, V57, P13, DOI 10.1145/2601069
   Myer S, 2018, INTERSPEECH, P1264
   Panchapagesan S, 2016, INTERSPEECH, P760, DOI 10.21437/Interspeech.2016-1485
   Paszke A., 2019, ADV NEURAL INFORM PR
   Pedroni BU, 2018, BIOMED CIRC SYST C, P591
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Sainath TN, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P1478
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Shrestha SB, 2018, ADV NEUR IN, V31
   Sigtia S, 2018, INTERSPEECH, P2092
   Snyder D., 2015, ARXIV151008484
   Wang Y., 2018, IEEE T NEUR NET LEAR
   Warden Pete, 2018, ARXIV180403209
   Wu JB, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00199
   Wu Jindou, 2019, ARXIV E PRINTS
   Wu J, 2019, J IMMUNOL RES, V2019, DOI 10.1155/2019/1749803
   Wu YJ, 2019, AAAI CONF ARTIF INTE, P1311
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Zenke F, 2018, NEURAL COMPUT, V30, P1514, DOI 10.1162/neco_a_01086
   Zhang Y., 2017, ARXIV171107128
NR 34
TC 10
Z9 10
U1 1
U2 6
PY 2020
BP 2557
EP 2561
DI 10.21437/Interspeech.2020-1230
UT WOS:000833594102138
DA 2023-11-16
ER

PT J
AU Li, XM
   Wang, W
   Xue, FZ
   Song, YD
AF Li, Xiumin
   Wang, Wei
   Xue, Fangzheng
   Song, Yongduan
TI Computational modeling of spiking neural network with learning rules
   from STDP and intrinsic plasticity
SO PHYSICA A-STATISTICAL MECHANICS AND ITS APPLICATIONS
DT Article
DE STDP; Intrinsic plasticity; Spiking neural network; Reservoir computing
ID MEMORY; EXCITABILITY; NEURONS; SIDE
AB Recently there has been continuously increasing interest in building up computational models of spiking neural networks (SNN), such as the Liquid State Machine (LSM). The biologically inspired self-organized neural networks with neural plasticity can enhance the capability of computational performance, with the characteristic features of dynamical memory and recurrent connection cycles which distinguish them from the more widely used feedforward neural networks. Despite a variety of computational models for brain-like learning and information processing have been proposed, the modeling of self organized neural networks with multi-neural plasticity is still an important open challenge. The main difficulties lie in the interplay among different forms of neural plasticity rules and understanding how structures and dynamics of neural networks shape the computational performance. In this paper, we propose a novel approach to develop the models of LSM with a biologically inspired self-organizing network based on two neural plasticity learning rules. The connectivity among excitatory neurons is adapted by spike-timing-dependent plasticity (STOP) learning; meanwhile, the degrees of neuronal excitability are regulated to maintain a moderate average activity level by another learning rule: intrinsic plasticity (IP). Our study shows that LSM with STDP+IP performs better than LSM with a random SNN or SNN obtained by STDP alone. The noticeable improvement with the proposed method is due to the better reflected competition among different neurons in the developed SNN model, as well as the more effectively encoded and processed relevant dynamic information with its learning and self-organizing mechanism. This result gives insights to the optimization of computational models of spiking neural networks with neural plasticity. (C) 2017 Elsevier B.V. All rights reserved.
C1 Chongqing Univ, Key Lab Dependable Serv Comp Cyber Phys Soc, Minist Educ, Chongqing 400044, Peoples R China.
   [Li, Xiumin] Chongqing Univ, Coll Automat, Chongqing 400044, Peoples R China.
RP Li, XM (corresponding author), Chongqing Univ, Coll Automat, Chongqing 400044, Peoples R China.
EM xmli@cqu.edu.cn
CR Abbott LF, 2016, NAT NEUROSCI, V19, P350, DOI 10.1038/nn.4241
   [Anonymous], P ADV NEUR INF PROC
   Antonelo EA, 2015, IEEE T NEUR NET LEAR, V26, P763, DOI 10.1109/TNNLS.2014.2323247
   Antoniou IE, 2008, DISCRETE DYN NAT SOC, V2008, DOI 10.1155/2008/375452
   Baddeley R, 1997, P ROY SOC B-BIOL SCI, V264, P1775, DOI 10.1098/rspb.1997.0246
   Boerlin M, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003258
   Boerlin M, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1001080
   Burgsteiner H, 2007, APPL INTELL, V26, P99, DOI 10.1007/s10489-006-0007-1
   Cudmore RH, 2004, J NEUROPHYSIOL, V92, P341, DOI 10.1152/jn.01059.2003
   Daoudal G, 2003, LEARN MEMORY, V10, P456, DOI 10.1101/lm.64103
   Desai NS, 1999, NAT NEUROSCI, V2, P515, DOI 10.1038/9165
   Destexhe A, 2004, NATURE, V431, P789, DOI 10.1038/nature03011
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Jaeger H, 2001, 14834 GMD GERM NAT R, V148, P34
   Klampfl S, 2013, J NEUROSCI, V33, P11515, DOI 10.1523/JNEUROSCI.5044-12.2013
   Lazar A, 2007, NEURAL NETWORKS, V20, P312, DOI 10.1016/j.neunet.2007.04.020
   Lazar A, 2009, FRONT COMPUT NEUROSC, V3, DOI 10.3389/neuro.10.023.2009
   Li CG, 2011, IEEE T AUTON MENT DE, V3, P277, DOI 10.1109/TAMD.2011.2159379
   Li XM, 2010, NEW J PHYS, V12, DOI 10.1088/1367-2630/12/8/083045
   Li XM, 2009, CHAOS, V19, DOI 10.1063/1.3076394
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Maass W, 2007, PLOS COMPUT BIOL, V3, P15, DOI 10.1371/journal.pcbi.0020165
   Marder E, 1996, P NATL ACAD SCI USA, V93, P13481, DOI 10.1073/pnas.93.24.13481
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   Matthews PBC, 1996, J PHYSIOL-LONDON, V492, P597, DOI 10.1113/jphysiol.1996.sp021332
   Naudé J, 2012, PLOS COMPUT BIOL, V8, DOI 10.1371/journal.pcbi.1002349
   Norton D, 2006, IEEE IJCNN, P4243
   Renart A, 2003, NEURON, V38, P473, DOI 10.1016/S0896-6273(03)00255-1
   Schrauwen B, 2008, NEUROCOMPUTING, V71, P1159, DOI 10.1016/j.neucom.2007.12.020
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Steil JJ, 2004, IEEE IJCNN, P843
   Steil JJ, 2007, NEURAL NETWORKS, V20, P353, DOI 10.1016/j.neunet.2007.04.011
   Stemmler M, 1999, NAT NEUROSCI, V2, P521, DOI 10.1038/9173
   Triefenbach F, 2014, IEEE SIGNAL PROC LET, V21, P311, DOI 10.1109/LSP.2014.2302080
   Verstraeten D, 2007, NEURAL NETWORKS, V20, P391, DOI 10.1016/j.neunet.2007.04.003
   Verstraeten D., 2005, P 16 ANN PRORISC WOR, P454
   Wang XJ, 2002, NEURON, V36, P955, DOI 10.1016/S0896-6273(02)01092-9
   Wang Z, 2013, NEURAL NETWORKS, V43, P55, DOI 10.1016/j.neunet.2013.01.024
   Xue FZ, 2013, NEUROCOMPUTING, V122, P324, DOI 10.1016/j.neucom.2013.06.019
   Yassin L, 2010, NEURON, V68, P1043, DOI 10.1016/j.neuron.2010.11.029
   Zhang J, 2016, BRAIN, V139, P2307, DOI 10.1093/brain/aww143
   Zhang J, 2015, CEREB CORTEX, V25, P3475, DOI 10.1093/cercor/bhu173
   Zhang W, 2003, NAT REV NEUROSCI, V4, P885, DOI 10.1038/nrn1248
   Zheng PS, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1002848
NR 44
TC 22
Z9 25
U1 2
U2 48
PD FEB 1
PY 2018
VL 491
BP 716
EP 728
DI 10.1016/j.physa.2017.08.053
UT WOS:000417661500063
DA 2023-11-16
ER

PT J
AU Tao, LY
   Li, P
   Meng, MH
   Yang, ZL
   Liu, XZ
   Hu, JH
   Dong, J
   Qiao, SS
   Ye, TC
   Shang, DL
AF Tao, Liying
   Li, Pan
   Meng, Meihua
   Yang, Zonglin
   Liu, Xiaozhuang
   Hu, Jinhua
   Dong, Ji
   Qiao, Shushan
   Ye, Tianchun
   Shang, Delong
TI Blended Glial Cell's Spiking Neural Network
SO IEEE ACCESS
DT Article
DE Glial cells; Neural networks; Synapses; Spatiotemporal phenomena; Brain
   modeling; Biological neural networks; Statistics; Glial cell; spiking
   neural networks; spatiotemporal information integration; sudoku solver
ID ORGANIZATION; INTELLIGENCE; TIME; CNS
AB Spiking Neural Networks (SNNs), the third generation of artificial neural networks, have been widely employed. However, the realization of advanced artificial intelligence is challenging due to the dearth of efficient spatiotemporal information integration models. Inspired by brain neuroscientists, this paper proposes a novel spiking neural network - Blended Glial Cell's Spiking Neural Network (BGSNN). BGSNN introduces glial cells as spatiotemporal information processing units based on neurons and synapses, and also provides four new network dynamics connection models which extend the information processing dimension, enhance the network global information integration in the spatiotemporal domain, as well as the plasticity of neurons and synapses. In this paper, a BGSNN application - Sudoku solver is designed and implemented on the "WenTian" neuromorphic prototype. On the Easybrain dataset, the BGSNN solver achieves 100% accuracy, outperforming the same structure SNN solver by 97% at the Evil difficulty level, and has faster converges speed compared with the SOTA Sudoku solver LSGA. On the kaggle dataset, the BGSNN solver achieves over 99.99% accuracy, outperforming the publicly available optimal DNN solver under this dataset by 3.82%. In addition, BGSNN exhibits good parallelism and sparsity, decreasing computation by at least 92.9% compared to serial solvers and reducing sparsity by 88% compared to the equal fully dense DNN. BGSNN improves the expression, feedback, and regulation capabilities of neural networks while maintaining the advantages of SNN parallel sparsity, making it simpler to implement advanced artificial intelligence.
C1 [Tao, Liying; Qiao, Shushan; Ye, Tianchun; Shang, Delong] Chinese Acad Sci, Inst Microelect, Beijing 100000, Peoples R China.
   [Tao, Liying; Dong, Ji; Qiao, Shushan] Univ Chinese Acad Sci, Sch Elect Elect & Commun Engn, Beijing 100000, Peoples R China.
   [Li, Pan; Meng, Meihua; Yang, Zonglin; Liu, Xiaozhuang; Hu, Jinhua; Dong, Ji; Shang, Delong] Nanjing Inst Intelligent Technol, Nanjing 210000, Peoples R China.
RP Qiao, SS; Ye, TC; Shang, DL (corresponding author), Chinese Acad Sci, Inst Microelect, Beijing 100000, Peoples R China.; Qiao, SS (corresponding author), Univ Chinese Acad Sci, Sch Elect Elect & Commun Engn, Beijing 100000, Peoples R China.; Shang, DL (corresponding author), Nanjing Inst Intelligent Technol, Nanjing 210000, Peoples R China.
EM qiaoshushan@ime.ac.cn; yetianchun@ime.ac.cn; shangdelong@ime.ac.cn
CR Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   Araque A, 2014, NEURON, V81, P728, DOI 10.1016/j.neuron.2014.02.007
   Beattie EC, 2002, SCIENCE, V295, P2282, DOI 10.1126/science.1067859
   Butt AM, 2011, SEMIN CELL DEV BIOL, V22, P205, DOI 10.1016/j.semcdb.2011.02.023
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Deco G, 2019, NAT COMMUN, V10, DOI 10.1038/s41467-018-08186-7
   Eshraghian JK, 2022, IEEE NANOTECHNOL MAG, V16, P14, DOI 10.1109/MNANO.2022.3141443
   Fang W, 2021, INCORPORATING LEARNA, P2661
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   Hajos F., 1984, SURFACE CONTACT GLIA
   HAMA K, 1994, MICROSC RES TECHNIQ, V29, P357, DOI 10.1002/jemt.1070290506
   Han J., CELL
   Hasselmo ME, 2006, TRENDS COGN SCI, V10, P487, DOI 10.1016/j.tics.2006.09.005
   Ivanov V., 2021, ADV NEURAL INF PROCE, V34, P25703
   Jin-Yi Z., 2020, Q MECH, V41, P16
   kaggle, 1 MILLION SUDOKU GAM
   kaggle, SUDOKU SOLVER 9745 A
   kaggle, SUDOKU SOLVER BACKTR
   kaggle, SOLVE SUDOKU CNN ACC
   Koch C, 1996, CEREB CORTEX, V6, P93, DOI 10.1093/cercor/6.2.93
   Kwisthout J., 2020, PROC NEURO INSPIRED, P1
   Letellier M, 2016, P NATL ACAD SCI USA, V113, pE2685, DOI 10.1073/pnas.1523717113
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Mattia M, 2002, PHYS REV E, V66, DOI 10.1103/PhysRevE.66.051917
   Mauch DH, 2001, SCIENCE, V294, P1354, DOI 10.1126/science.294.5545.1354
   Midya R, 2019, ADV ELECTRON MATER, V5, DOI 10.1002/aelm.201900060
   Mitterauer B, 2009, MED HYPOTHESES, V73, P393, DOI 10.1016/j.mehy.2009.04.003
   Pei J, 2019, NATURE, V572, P106, DOI 10.1038/s41586-019-1424-8
   Perea G, 2014, FRONT CELL NEUROSCI, V8, DOI [10.3389/1ncel.2014.00378, 10.3389/fncel.2014.00378]
   Perea G, 2014, NAT COMMUN, V5, DOI 10.1038/ncomms4262
   Roy K, 2019, NATURE, V575, P607, DOI 10.1038/s41586-019-1677-2
   Rui-Bin S., 2004, FOREIGN MED PHARM, V31, P216
   Shankar KH, 2012, NEURAL COMPUT, V24, P134, DOI 10.1162/NECO_a_00212
   Stevens B, 2002, NEURON, V36, P855, DOI 10.1016/S0896-6273(02)01067-X
   Strauss AL, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0129957
   Sudoku.com, US
   Tang G., 2019, P 7 ANN NEUR COMP EL, P1
   Wang Chuan, 2023, IEEE T GAMES, DOI [10.1109/TG.2023.3236490, DOI 10.1109/TG.2023.3236490]
   Watanabe M, 2002, TRENDS NEUROSCI, V25, P5, DOI 10.1016/S0166-2236(00)01993-7
   Web Sudoku, US
   Xiang-Hong L., 2011, COMPUT ENG APPL, V47, P41
   Xie M., P 9 NAT C 5 GEN M CH
   Yamane Y, 2002, NEUROSCIENCE, V112, P593, DOI 10.1016/S0306-4522(02)00095-7
   Yato T, 2003, IEICE T FUND ELECTR, VE86A, P1052
   Zhao ZY, 2020, IEEE T CIRCUITS-II, V67, P931, DOI 10.1109/TCSII.2020.2980054
NR 46
TC 0
Z9 0
U1 2
U2 2
PY 2023
VL 11
BP 43566
EP 43582
DI 10.1109/ACCESS.2023.3267856
UT WOS:000986562000001
DA 2023-11-16
ER

PT C
AU Simoes, AD
   Costa, AHR
AF Simoes, Alexandre da Silva
   Reali Costa, Anna Helena
BE Zaverucha, G
   LoureiroDaCosta, A
TI A Learning Function for Parameter Reduction in Spiking Neural Networks
   with Radial Basis Function
SO ADVANCES IN ARTIFICIAL INTELLIGENCE - SBIA 2008, PROCEEDINGS
SE Lecture Notes in Artificial Intelligence
DT Proceedings Paper
CT 19th Brazilian Symposium on Artificial Intelligence
CY OCT 26-30, 2008
CL Federal Univ Bahia, Salvador, BRAZIL
HO Federal Univ Bahia
AB Spiking neural networks - networks that encode information in the timing of spikes - are arising as a new approach in the artificial neural networks paradigm, emergent from cognitive science. One of these new models is the pulsed neural network with radial basis function, a network able to store information in the axonal propagation delay of neurons. Learning algorithms have been proposed to this model looking for mapping input pulses into output pulses. Recently, a new method was proposed to encode constant data into a temporal sequence of spikes, stimulating deeper studies in order to establish abilities and frontiers of this new approach. However, a well known problem of this kind of network is the high number of free parameters - more that 15 - to be properly configured or tuned in order to allow network convergence. This work presents for the first time a new learning function for this network training that allow the automatic configuration of one of the key network parameters: the synaptic weight decreasing factor.
C1 [Simoes, Alexandre da Silva] Sao Paulo State Univ UNESP, Automat & Integrated Syst Grp, Av Tres de Marco,511 Alto da Boa Vista, BR-18087180 Sorocaba, SP, Brazil.
   [Reali Costa, Anna Helena] Univ Sao Paulo, Intelligent Tech Lab, BR-05508900 Sao Paulo, Brazil.
RP Simoes, AD (corresponding author), Sao Paulo State Univ UNESP, Automat & Integrated Syst Grp, Av Tres de Marco,511 Alto da Boa Vista, BR-18087180 Sorocaba, SP, Brazil.
EM assimoes@sorocaba.unesp.br; anna.reali@poli.usp.br
CR Bohte S.M., 2003, THESIS CTR MATH COMP
   DASIMOES AS, 2006, THESIS ESCOLA POLITE
   Gerstner W, 1996, NATURE, V383, P76, DOI 10.1038/383076a0
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   Maass W., 1999, PULSED NEURAL NETWOR
   Natschlager T, 1998, NETWORK-COMP NEURAL, V9, P319, DOI 10.1088/0954-898X/9/3/003
   Prechelt L., 1994, PROBEN1 SET NEURAL N, V21, P94
NR 7
TC 9
Z9 10
U1 0
U2 0
PY 2008
VL 5249
BP 227
EP +
UT WOS:000261373200028
DA 2023-11-16
ER

PT C
AU Hulea, M
AF Hulea, Mircea
TI USING SPIKING NEURAL NETWORKS FOR LIGHT SPOT TRACKING
SO 2012 PROCEEDINGS OF THE 20TH EUROPEAN SIGNAL PROCESSING CONFERENCE
   (EUSIPCO)
SE European Signal Processing Conference
DT Proceedings Paper
CT 20th European Signal Processing Conference (EUSIPCO)
CY AUG 27-31, 2012
CL Bucharest, ROMANIA
DE spiking neural networks; tracking device; associative learning
ID MODEL; PLASTICITY
AB This paper introduces a new method for automatically compensating the light spot displacement from the normal position in laser spot trackers. The method is based on hardware implementation of the spiking neural networks which provides fast response due to real time operation and ability to learn unsupervised when they are stimulated by concurrent events. To validate this method we implemented in hardware a spiking neural network structure able to process the input from a photodiode array and to control a positioning system. The performance of the neural network that is based on an electronic neuron of biological inspiration was tested using the output of the photodiode array placed in strait line. The results show that the rapport between the energy consumed by the spiking neural network and the accuracy in compensating the spot moving on horizontal or vertical directions is significantly better than the rapport which is obtainable when programmable computing devices solve the same task. These results are encouraging to develop low power spot tracking system for enhancing the receiving accuracy in free space optics or for enhancing the efficacy of the photovoltaic systems.
C1 Gheorghe Asachi Tech Univ Iasi, Fac Elect Telecommun & Informat Technol, Iasi, Romania.
RP Hulea, M (corresponding author), Gheorghe Asachi Tech Univ Iasi, Fac Elect Telecommun & Informat Technol, Bd Carol 1, Iasi, Romania.
EM mhulea@tuiasi.ro
CR [Anonymous], 2009, AFRICON 2009 AFRICON
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Dan Y, 2004, NEURON, V44, P23, DOI 10.1016/j.neuron.2004.09.007
   Duh FB, 2004, IEEE T SYST MAN CY B, V34, P16, DOI 10.1109/TSMCB.2003.810953
   Gerstner W., 2002, SPIKING NEURON MODEL
   Haibin S., 2010, P 2010 INT C COMP AP, DOI [10.1109/ICCASM.2010.5619423, DOI 10.1109/ICCASM.2010.5619423]
   Hines ML, 2008, J COMPUT NEUROSCI, V25, P439, DOI 10.1007/s10827-008-0087-5
   Hines ML, 2008, J COMPUT NEUROSCI, V25, P203, DOI 10.1007/s10827-007-0073-3
   Hopfield JJ, 2001, P NATL ACAD SCI USA, V98, P1282, DOI 10.1073/pnas.031567098
   Hulea M., 2010, MEMOIRS SCI, VXXXIII, P129
   Hulea M., 2008, CEAI J, V10, P32
   Hulea M., 2011, P 15 C SYST THEOR CO, P282
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Jacob V, 2007, J NEUROSCI, V27, P1271, DOI 10.1523/JNEUROSCI.4264-06.2007
   Jimenez-Fernandez A, 2012, SENSORS-BASEL, V12, P3831, DOI 10.3390/s120403831
   Jolivet R, 2004, J NEUROPHYSIOL, V92, P959, DOI 10.1152/jn.00190.2004
   Kobayashi R, 2009, FRONT COMPUT NEUROSC, V3, DOI 10.3389/neuro.10.009.2009
   Lovelace JJ, 2008, NEURAL COMPUT, V20, P65, DOI 10.1162/neco.2008.20.1.65
   Maass W, 1998, PULSED NEURAL NETWORKS, P55
   Maass W, 2004, MATH COMP BIOL SER, P575
   Masquelier T, 2009, NEURAL COMPUT, V21, P1259, DOI 10.1162/neco.2008.06-08-804
   O'Reylli R. C., 2000, COMPUTATIONAL EXPLOR, P32
   Rast A. D., 2009, Proceedings 2009 International Joint Conference on Neural Networks (IJCNN 2009 - Atlanta), P2611, DOI 10.1109/IJCNN.2009.5179067
   Swiercz W, 2006, IEEE T NEURAL NETWOR, V17, P94, DOI 10.1109/TNN.2005.860834
   Wong YC, 1998, IEEE WORLD CONGRESS ON COMPUTATIONAL INTELLIGENCE, P1024, DOI 10.1109/IJCNN.1998.685912
NR 26
TC 2
Z9 2
U1 0
U2 2
PY 2012
BP 1708
EP 1712
UT WOS:000310623800343
DA 2023-11-16
ER

PT C
AU Wang, S
   Hu, YH
   Liu, SC
AF Wang, Shu
   Hu, Yuhuang
   Liu, Shih-Chii
GP IEEE
TI T-NGA: TEMPORAL NETWORK GRAFTING ALGORITHM FOR LEARNING TO PROCESS
   SPIKING AUDIO SENSOR EVENTS
SO 2022 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)
SE International Conference on Acoustics Speech and Signal Processing
   ICASSP
DT Proceedings Paper
CT 47th IEEE International Conference on Acoustics, Speech and Signal
   Processing (ICASSP)
CY MAY 22-27, 2022
CL Singapore, SINGAPORE
DE spiking silicon cochlea sensor; event-driven audio processing; deep
   neural network; self-supervised learning; speech recognition
ID SILICON COCHLEA
AB Spiking silicon cochlea sensors encode sound as an asynchronous stream of spikes from different frequency channels. The lack of labeled training datasets for spiking cochleas makes it difficult to train deep neural networks on the outputs of these sensors. This work proposes a self-supervised method called Temporal Network Grafting Algorithm (T-NGA), which grafts a recurrent network pretrained on spectrogram features so that the network works with the cochlea event features. T-NGA training requires only temporally aligned audio spectrograms and event features. Our experiments show that the accuracy of the grafted network was similar to the accuracy of a supervised network trained from scratch on a speech recognition task using events from a software spiking cochlea model. Despite the circuit non-idealities of the spiking silicon cochlea, the grafted network accuracy on the silicon cochlea spike recordings was only about 5% lower than the supervised network accuracy using the N-TIDIGITS18 dataset. T-NGA can train networks to process spiking audio sensor events in the absence of large labeled spike datasets.
C1 [Wang, Shu] Univ Zurich, Inst Neuroinformat, Zurich, Switzerland.
   Swiss Fed Inst Technol, Zurich, Switzerland.
RP Wang, S (corresponding author), Univ Zurich, Inst Neuroinformat, Zurich, Switzerland.
CR Anumula J, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351856
   Anumula J, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00023
   Ceolini E, 2019, INT CONF ACOUST SPEE, P7953, DOI [10.1109/ICASSP.2019.8683669, 10.1109/icassp.2019.8683669]
   Chan V, 2007, IEEE T CIRCUITS-I, V54, P48, DOI 10.1109/TCSI.2006.887979
   Cho K., 2014, ARXIV14061078, V1406, P1078
   Gao C., 2019, 2019 IEEE INT S CIRC, P1
   Giorgino T, 2009, J STAT SOFTW, V31, P1, DOI 10.18637/jss.v031.i07
   Graves Alex, 2006, ICML, DOI [10.1145/1143844.1143891, DOI 10.1145/1143844.1143891]
   HU Y, 2020, COMPUTER VISION ECCV, P80, DOI DOI 10.14733/CADAPS.2021.S2.80-91
   Kingma DP., 2017, ARXIV
   Leonard R. G., 1993, TIDIGITS LDC93S10
   Liu SC, 2014, IEEE T BIOMED CIRC S, V8, P453, DOI 10.1109/TBCAS.2013.2281834
   Maas AL, 2013, P 30 INT C MACH LEAR, V30, P3, DOI DOI 10.1016/0010-0277(84)90022-2
   Martinelli F, 2020, INT CONF ACOUST SPEE, P8544, DOI [10.1109/icassp40776.2020.9053412, 10.1109/ICASSP40776.2020.9053412]
   Panayotov V, 2015, INT CONF ACOUST SPEE, P5206, DOI 10.1109/ICASSP.2015.7178964
   Prabhavalkar R, 2015, INT CONF ACOUST SPEE, P4704, DOI 10.1109/ICASSP.2015.7178863
   Wu JB, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00199
   Yang MH, 2018, ISSCC DIG TECH PAP I, P346, DOI 10.1109/ISSCC.2018.8310326
   Yang MH, 2016, IEEE J SOLID-ST CIRC, V51, P2554, DOI 10.1109/JSSC.2016.2604285
   Yilmaz E, 2020, INTERSPEECH, P2557, DOI 10.21437/Interspeech.2020-1230
   Zhang B, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P2830, DOI 10.1109/ICASSP39728.2021.9414848
NR 21
TC 1
Z9 1
U1 1
U2 3
PY 2022
BP 3273
EP 3277
DI 10.1109/ICASSP43922.2022.9747093
UT WOS:000864187903112
DA 2023-11-16
ER

PT J
AU Taherkhani, A
   Belatreche, A
   Li, YH
   Maguire, LP
AF Taherkhani, Aboozar
   Belatreche, Ammar
   Li, Yuhua
   Maguire, Liam P.
TI A Supervised Learning Algorithm for Learning Precise Timing of Multiple
   Spikes in Multilayer Spiking Neural Networks
SO IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
DT Article
DE Multilayer neural network; spiking neural network (SNN); supervised
   learning; synaptic delay
ID TRAINING ALGORITHM; GRADIENT DESCENT; CLASSIFICATION; NEURONS;
   COMPUTATION; MODEL; BACKPROPAGATION; PLASTICITY; RESUME
AB There is a biological evidence to prove information is coded through precise timing of spikes in the brain. However, training a population of spiking neurons in a multilayer network to fire at multiple precise times remains a challenging task. Delay learning and the effect of a delay on weight learning in a spiking neural network (SNN) have not been investigated thoroughly. This paper proposes a novel biologically plausible supervised learning algorithm for learning precisely timed multiple spikes in a multilayer SNNs. Based on the spike-timing-dependent plasticity learning rule, the proposed learning method trains an SNN through the synergy between weight and delay learning. The weights of the hidden and output neurons are adjusted in parallel. The proposed learning method captures the contribution of synaptic delays to the learning of synaptic weights. Interaction between different layers of the network is realized through biofeedback signals sent by the output neurons. The trained SNN is used for the classification of spatiotemporal input patterns. The proposed learning method also trains the spiking network not to fire spikes at undesired times which contribute to misclassification. Experimental evaluation on benchmark data sets from the UCI machine learning repository shows that the proposed method has comparable results with classical rate-based methods such as deep belief network and the autoencoder models. Moreover, the proposed method can achieve higher classification accuracies than single layer and a similar multilayer SNN.
C1 [Taherkhani, Aboozar] Nottingham Trent Univ, Computat Neurosci & Cognit Robot Lab, Nottingham NG1 4FQ, England.
   [Belatreche, Ammar] Northumbria Univ, Dept Comp & Informat Sci, Newcastle Upon Tyne NE1 8ST, Tyne & Wear, England.
   [Li, Yuhua] Cardiff Univ, Sch Comp Sci & Informat, Cardiff CF10 3AT, S Glam, Wales.
   [Maguire, Liam P.] Ulster Univ, Fac Comp & Engn, Derry BT48 7JL, Londonderry, North Ireland.
RP Taherkhani, A (corresponding author), Nottingham Trent Univ, Computat Neurosci & Cognit Robot Lab, Nottingham NG1 4FQ, England.
EM aboozar.taherkhani@ntu.ac.uk; ammar.belatreche@northumbria.ac.uk;
   liy180@cardiff.ac.uk; lp.maguire@ulster.ac.uk
CR ADELI H, 1994, APPL MATH COMPUT, V62, P81, DOI 10.1016/0096-3003(94)90134-1
   [Anonymous], 2015, 2015 INT JOINT C NEU, DOI DOI 10.1109/IJCNN.2015.7280592
   Bako L, 2010, BRIEF BIOINFORM, V11, P348, DOI 10.1093/bib/bbp066
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   Booij O, 2005, INFORM PROCESS LETT, V95, P552, DOI 10.1016/j.ipl.2005.05.023
   Borst A, 1999, NAT NEUROSCI, V2, P947, DOI 10.1038/14731
   Brette R, 2007, J COMPUT NEUROSCI, V23, P349, DOI 10.1007/s10827-007-0038-6
   Cariani PA, 2004, IEEE T NEURAL NETWOR, V15, P1100, DOI 10.1109/TNN.2004.833305
   Dora S, 2016, NEUROCOMPUTING, V171, P1216, DOI 10.1016/j.neucom.2015.07.086
   Florian RV, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0040233
   Gerstner W., 2002, SPIKING NEURON MODEL
   Ghosh-Dastidar S, 2007, INTEGR COMPUT-AID E, V14, P187
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   Glackin B, 2010, FRONT COMPUT NEUROSC, V4, DOI 10.3389/fncom.2010.00018
   Gueorguieva N, 2006, J EXP THEOR ARTIF IN, V18, P73, DOI 10.1080/09528130600552888
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Hong SH, 2012, J NEUROSCI, V32, P1413, DOI 10.1523/JNEUROSCI.3735-11.2012
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   Hu J, 2013, NEURAL COMPUT, V25, P450, DOI 10.1162/NECO_a_00395
   HUNG SL, 1994, NEUROCOMPUTING, V6, P45, DOI 10.1016/0925-2312(94)90033-7
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   KANDEL ER, 2004, PRINCIPLES NEURAL SC
   Kasabov N, 2013, NEURAL NETWORKS, V41, P188, DOI 10.1016/j.neunet.2012.11.014
   Kasinski A., 2006, International Journal of Applied Mathematics and Computer Science, V16, P101
   Laughlin SB, 2001, CURR OPIN NEUROBIOL, V11, P475, DOI 10.1016/S0959-4388(00)00237-3
   Laughlin SB, 1998, NAT NEUROSCI, V1, P36, DOI 10.1038/236
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maass W., 1999, PULSED NEURAL NETWOR, V6, P321
   McKennoch S, 2006, IEEE IJCNN, P3970
   Memmesheimer RM, 2014, NEURON, V82, P925, DOI 10.1016/j.neuron.2014.03.026
   Mohemmed A, 2013, NEUROCOMPUTING, V107, P3, DOI 10.1016/j.neucom.2012.08.034
   Mohemmed A, 2012, INT J NEURAL SYST, V22, DOI 10.1142/S0129065712500128
   Morrison A, 2008, BIOL CYBERN, V98, P459, DOI 10.1007/s00422-008-0233-1
   Paugam-Moisy H., 2012, HDB NATURAL COMPUTIN, V1, P1, DOI [DOI 10.1007/978-3-540-92910-9_10, 10.1007/978-3-540-92910-9_10]
   Pfister JP, 2006, NEURAL COMPUT, V18, P1318, DOI 10.1162/neco.2006.18.6.1318
   Pham DT, 2008, P I MECH ENG B-J ENG, V222, P1201, DOI 10.1243/09544054JEM1054
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Ramaswamy V, 2014, J COMPUT NEUROSCI, V37, P209, DOI 10.1007/s10827-014-0497-5
   Rosselló JL, 2014, INT J NEURAL SYST, V24, DOI 10.1142/S0129065714300034
   Shrestha SB, 2015, NEURAL NETWORKS, V63, P185, DOI 10.1016/j.neunet.2014.12.001
   Sporea I, 2013, NEURAL COMPUT, V25, P473, DOI 10.1162/NECO_a_00396
   Taherkhani A., 2014, P ESANN BRUG BELG, P1
   Taherkhani A., 2015, NEURAL NETWORKS IJCN, P1
   Taherkhani A, 2015, LECT NOTES COMPUT SC, V9490, P190, DOI 10.1007/978-3-319-26535-3_22
   Taherkhani A, 2015, IEEE T NEUR NET LEAR, V26, P3137, DOI 10.1109/TNNLS.2015.2404938
   Tanaka M, 2014, INT C PATT RECOG, P1526, DOI 10.1109/ICPR.2014.271
   Wade JJ, 2010, IEEE T NEURAL NETWOR, V21, P1817, DOI 10.1109/TNN.2010.2074212
   Xu Y, 2013, NEURAL COMPUT, V25, P1472, DOI 10.1162/NECO_a_00450
   Xu Y, 2013, NEURAL NETWORKS, V43, P99, DOI 10.1016/j.neunet.2013.02.003
   Yoon SY, 1999, NEURAL PROCESS LETT, V10, P171, DOI 10.1023/A:1018772122605
   Zhang M., IEEE T COGN DE UNPUB
   Zhang ML, 2017, NEUROCOMPUTING, V219, P333, DOI 10.1016/j.neucom.2016.09.044
NR 55
TC 48
Z9 51
U1 1
U2 28
PD NOV
PY 2018
VL 29
IS 11
BP 5394
EP 5407
DI 10.1109/TNNLS.2018.2797801
UT WOS:000447832200018
DA 2023-11-16
ER

PT C
AU Mohapatra, S
   Gotzig, H
   Yogamani, S
   Milz, S
   Zöllner, R
AF Mohapatra, Sambit
   Gotzig, Heinrich
   Yogamani, Senthil
   Milz, Stefan
   Zoellner, Raoul
BE Tremeau, A
   Farinella, GM
   Braz, J
TI Exploring Deep Spiking Neural Networks for Automated Driving
   Applications
SO PROCEEDINGS OF THE 14TH INTERNATIONAL JOINT CONFERENCE ON COMPUTER
   VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS (VISAPP),
   VOL 5
DT Proceedings Paper
CT 14th International Joint Conference on Computer Vision, Imaging and
   Computer Graphics Theory and Applications (VISAPP)
CY FEB 25-27, 2019
CL Prague, CZECH REPUBLIC
DE Visual Perception; Efficient Networks; Automated Driving
ID VISION; IMPLEMENTATION; DESIGN
AB Neural networks have become the standard model for various computer vision tasks in automated driving including semantic segmentation, moving object detection, depth estimation, visual odometry, etc. The main flavors of neural networks which are used commonly are convolutional (CNN) and recurrent (RNN). In spite of rapid progress in embedded processors, power consumption and cost is still a bottleneck. Spiking Neural Networks (SNNs) are gradually progressing to achieve low-power event-driven hardware architecture which has a potential for high efficiency. In this paper, we explore the role of deep spiking neural networks (SNN) for automated driving applications. We provide an overview of progress on SNN and argue how it can be a good fit for automated driving applications.
C1 [Mohapatra, Sambit; Gotzig, Heinrich] Valeo Bietigheim, Bietigheim Bissingen, Germany.
   [Yogamani, Senthil] Valeo Vis Syst, Dublin, Ireland.
   [Milz, Stefan] Valeo Kronach, Kronach, Germany.
   [Zoellner, Raoul] Heilbronn Univ, Heilbronn, Germany.
RP Mohapatra, S (corresponding author), Valeo Bietigheim, Bietigheim Bissingen, Germany.
CR [Anonymous], 1991, INTRO THEORY NEURAL
   [Anonymous], 2018, ARXIV180202627
   [Anonymous], 2016, INT C HIGH PERFORMAN
   Azghadi MR, 2014, P IEEE, V102, P717, DOI 10.1109/JPROC.2014.2314454
   Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Bois-Reymond Y., 1848, ANN PHYS, V151, P463
   Borst A, 1999, NAT NEUROSCI, V2, P947, DOI 10.1038/14731
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Cassidy Andrew, 2006, 2006 IEEE Biomedical Circuits and Systems Conference - Healthcare Technology (BioCas), P45, DOI 10.1109/BIOCAS.2006.4600304
   Chen HT, 2011, IEEE T BIOMED CIRC S, V5, P160, DOI 10.1109/TBCAS.2010.2075928
   Chou C.-N., 2019, P 10 INNOVATIONS THE
   Diehl P.U., 2015, 2015 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2015.7280696
   Escobar MJ, 2009, INT J COMPUT VISION, V82, P284, DOI 10.1007/s11263-008-0201-1
   Farabet C, 2012, FRONT NEUROSCI-SWITZ, V6, DOI 10.3389/fnins.2012.00032
   Fu J, 2007, SENSOR ACTUAT B-CHEM, V125, P489, DOI 10.1016/j.snb.2007.02.058
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   Gerstner W., 2002, SPIKING NEURON MODEL
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Heimberger M, 2017, IMAGE VISION COMPUT, V68, P88, DOI 10.1016/j.imavis.2017.07.002
   Hinton G. E., 1999, UNSUPERVISED LEARNIN
   Hu YH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00405
   Hunsberger Eric, 2015, COMPUT SCI
   Indiveri G, 2006, IEEE T NEURAL NETWOR, V17, P211, DOI 10.1109/TNN.2005.860850
   Indiveri G, 2007, IEEE INT SYMP CIRC S, P3371, DOI 10.1109/ISCAS.2007.378290
   Leñero-Bardallo JA, 2010, IEEE INT SYMP CIRC S, P2438, DOI 10.1109/ISCAS.2010.5537152
   Lichtsteiner P, 2008, IEEE J SOLID-ST CIRC, V43, P566, DOI 10.1109/JSSC.2007.914337
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maqueda AI, 2018, PROC CVPR IEEE, P5419, DOI 10.1109/CVPR.2018.00568
   Ponulak F, 2011, ACTA NEUROBIOL EXP, V71, P409
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Singh A, 2017, IEEE WINT CONF APPL, P100, DOI 10.1109/WACV.2017.19
   Tavanaei A., 2018, ARXIV180408150
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   Wunderlich T., 2018, ARXIV181103618
   Zhao B, 2015, IEEE T NEUR NET LEAR, V26, P1963, DOI 10.1109/TNNLS.2014.2362542
   Zhou S., 2018, ARXIV181012436
NR 38
TC 0
Z9 0
U1 0
U2 2
PY 2019
BP 548
EP 555
DI 10.5220/0007469405480555
UT WOS:000570349800062
DA 2023-11-16
ER

PT J
AU Zou, CL
   Cui, XX
   Kuang, YS
   Liu, KF
   Wang, Y
   Wang, XA
   Huang, R
AF Zou, Chenglong
   Cui, Xiaoxin
   Kuang, Yisong
   Liu, Kefei
   Wang, Yuan
   Wang, Xinan
   Huang, Ru
TI A Scatter-and-Gather Spiking Convolutional Neural Network on a
   Reconfigurable Neuromorphic Hardware
SO FRONTIERS IN NEUROSCIENCE
DT Article
DE convolutional neural network; spiking neural network; network
   quantization; network conversion; neuromorphic hardware; network mapping
AB Artificial neural networks (ANNs), like convolutional neural networks (CNNs), have achieved the state-of-the-art results for many machine learning tasks. However, inference with large-scale full-precision CNNs must cause substantial energy consumption and memory occupation, which seriously hinders their deployment on mobile and embedded systems. Highly inspired from biological brain, spiking neural networks (SNNs) are emerging as new solutions because of natural superiority in brain-like learning and great energy efficiency with event-driven communication and computation. Nevertheless, training a deep SNN remains a main challenge and there is usually a big accuracy gap between ANNs and SNNs. In this paper, we introduce a hardware-friendly conversion algorithm called "scatter-and-gather" to convert quantized ANNs to lossless SNNs, where neurons are connected with ternary {-1,0,1} synaptic weights. Each spiking neuron is stateless and more like original McCulloch and Pitts model, because it fires at most one spike and need be reset at each time step. Furthermore, we develop an incremental mapping framework to demonstrate efficient network deployments on a reconfigurable neuromorphic chip. Experimental results show our spiking LeNet on MNIST and VGG-Net on CIFAR-10 datasetobtain 99.37% and 91.91% classification accuracy, respectively. Besides, the presented mapping algorithm manages network deployment on our neuromorphic chip with maximum resource efficiency and excellent flexibility. Our four-spike LeNet and VGG-Net on chip can achieve respective real-time inference speed of 0.38 ms/image, 3.24 ms/image, and an average power consumption of 0.28 mJ/image and 2.3 mJ/image at 0.9 V, 252 MHz, which is nearly two orders of magnitude more efficient than traditional GPUs.
C1 [Zou, Chenglong; Cui, Xiaoxin; Kuang, Yisong; Liu, Kefei; Wang, Yuan; Huang, Ru] Peking Univ, Inst Microelect, Beijing, Peoples R China.
   [Zou, Chenglong; Wang, Xinan] Peking Univ, Shenzhen Grad Sch, Sch ECE, Shenzhen, Peoples R China.
RP Cui, XX (corresponding author), Peking Univ, Inst Microelect, Beijing, Peoples R China.
EM cuixx@pku.edu.cn
CR Abbott LF, 1999, BRAIN RES BULL, V50, P303, DOI 10.1016/S0361-9230(99)00161-6
   Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   [Anonymous], 2013, 2013 INT JOINT C NEU
   Bengio Yoshua, 2013, ESTIMATING PROPAGATI
   Boahen KA, 2000, IEEE T CIRCUITS-II, V47, P416, DOI 10.1109/82.842110
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Bouvier M, 2019, ACM J EMERG TECH COM, V15, DOI 10.1145/3304103
   Chen YR, 2020, ENGINEERING-PRC, V6, P264, DOI 10.1016/j.eng.2020.01.007
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Deng L, 2020, IEEE J SOLID-ST CIRC, V55, P2228, DOI 10.1109/JSSC.2020.2970709
   Deng L, 2020, P IEEE, V108, P485, DOI 10.1109/JPROC.2020.2976475
   Dong H, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1201, DOI 10.1145/3123266.3129391
   Esser SK, 2015, ADV NEUR IN, V28
   Esser SK, 2016, P NATL ACAD SCI USA, V113, P11441, DOI 10.1073/pnas.1604850113
   Falez P, 2019, PATTERN RECOGN, V93, P418, DOI 10.1016/j.patcog.2019.04.016
   Glorot X., 2011, PMLR, Vvol 15, P315
   Grning A., 2014, 2014 EUR S ART NEUR
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Hayman S., 1999, IJCNN'99. International Joint Conference on Neural Networks. Proceedings (Cat. No.99CH36339), P4438, DOI 10.1109/IJCNN.1999.830886
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hubara I, 2018, J MACH LEARN RES, V18
   Kingma D. P., 2015, INT C LEARNING REPRE
   Krizhevsky Alex, 2009, LEARNING MULTIPLE LA
   Kuang YS, 2021, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS51556.2021.9401195
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lecun Y, 2019, ISSCC DIG TECH PAP I, V62, P12, DOI 10.1109/ISSCC.2019.8662396
   Lee C, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00119
   Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508
   [李凡杰 Li Fanjie], 2016, [低温工程, Cryogenics], P1
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lobov SA, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00088
   Mostafa H., 2017, 2017 IEEE INT S CIRC, P1, DOI [10.1109/ISCAS.2017.8050527, DOI 10.1109/ISCAS.2017.8050527]
   Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32
   Rueckauer B, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351295
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Schuman CD., 2017, ARXIV
   Sheik Sadique, 2013, Biomimetic and Biohybrid Systems. Second International Conference, Living Machines 2013. Proceedings. LNCS 8064, P262, DOI 10.1007/978-3-642-39802-5_23
   Simonyan K., 2015, 3 INT C LEARNING REP, P1
   Springenberg Jost Tobias, 2014, ARXIV14126806
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Stckl C., 2019, ARXIV PREPRINT ARXIV
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Wei F., 2020, ARXIV PREPRINT ARXIV
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Xu Y, 2017, 2017 IEEE SYMPOSIUM SERIES ON COMPUTATIONAL INTELLIGENCE (SSCI), P1219
   Yang SM, 2022, IEEE T NEUR NET LEAR, V33, P2801, DOI 10.1109/TNNLS.2020.3045492
   Yang SM, 2021, FRONT NEUROSCI-SWITZ, V15, DOI 10.3389/fnins.2021.601109
   Yousefzadeh A, 2019, 2019 IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE CIRCUITS AND SYSTEMS (AICAS 2019), P81, DOI [10.1109/AICAS.2019.8771624, 10.1109/aicas.2019.8771624]
   Zhou Shuchang, 2016, ARXIV160606160
   Zou C., 2020, IEEE INT SYMP CIRC S, P1, DOI DOI 10.1109/iscas45731.2020.9180918
NR 52
TC 2
Z9 2
U1 3
U2 24
PD NOV 16
PY 2021
VL 15
AR 694170
DI 10.3389/fnins.2021.694170
UT WOS:000725623200001
DA 2023-11-16
ER

PT C
AU Yamaguchi, A
   Arakane, S
   Kubo, M
AF Yamaguchi, Akihiro
   Arakane, Satoshi
   Kubo, Masao
BE Jia, Y
   Ito, T
   Lee, JJ
   Sugisaka, M
TI Feature Linking by Synchronized Response in Chaotic Cellar Neural
   Network for Visual Stimulus of Moving Objects
SO PROCEEDINGS OF THE 2016 INTERNATIONAL CONFERENCE ON ARTIFICIAL LIFE AND
   ROBOTICS (ICAROB 2016)
DT Proceedings Paper
CT International Conference on Artificial Life and Robotics (ICAROB)
CY JAN 29-31, 2016
CL Okinawa, JAPAN
DE chaotic synchronization; neural coding; spike response model; feature
   linking
AB A feature linking mechanism by the synchronized response of neural assemblies was studied for the chaotic cellar neural network (Chaotic-CNN). The Chaotic-CNN consists of chaotic spike response neurons that show the chaotic inter-spike-interval dynamics. In our scheme of feature linking, the features of the target objects are linked by the synchronized spike responses that are characterized by the temporal chaotic pattern of spike sequence. In this paper, we analyzed the synchronized spike responses that invoked by the visual stimulus of moving bars. As a result, neural assemblies have higher correlation for the visual stimulus of moving two bars in the same direction than the opposite direction. Then we discussed a possibility of feature linking by the chaotic synchronized response in the view point of neural coding.
C1 [Yamaguchi, Akihiro; Arakane, Satoshi] Fukuoka Inst Technol, Dept Informat & Syst Engn, Higashi Ku, 3-30-1 Wajiro Higashi, Fukuoka 8110116, Japan.
   [Kubo, Masao] Natl Def Acad Japan, Dept Comp Sci, 1-10-20 Hashirimizu, Yokosuka, Kanagawa 2398686, Japan.
RP Yamaguchi, A (corresponding author), Fukuoka Inst Technol, Dept Informat & Syst Engn, Higashi Ku, 3-30-1 Wajiro Higashi, Fukuoka 8110116, Japan.
EM aki@fit.ac.jp
CR Eckhorn R., 1989, IJCNN: International Joint Conference on Neural Networks (Cat. No.89CH2765-6), P723, DOI 10.1109/IJCNN.1989.118659
   Fujiwara M., 2016, J ROBOTICS NETWORKIN, V2, P26
   Gerstner W., 2002, SPIKING NEURON MODEL
   GRAY CM, 1989, NATURE, V338, P334, DOI 10.1038/338334a0
   Lee G, 2001, NEURAL NETWORKS, V14, P115, DOI 10.1016/S0893-6080(00)00083-6
   Yamaguchi A, 2014, FUKUOKA I TECHNOLOGY, V25, P1
   Yamaguchi A, 2013, FUKUOKA I TECHNOLOGY, V24, P1
   Yamaguchi A., 2000, IEICE TECH REP, V99, p[NC99, 15]
   Yamaguti Y., 2002, IEICE TECH REP, V101, P127
NR 9
TC 0
Z9 0
U1 0
U2 0
PY 2016
BP 130
EP 133
UT WOS:000400192600032
DA 2023-11-16
ER

PT J
AU Pimashkin, A
   Gladkov, A
   Agrba, E
   Mukhina, I
   Kazantsev, V
AF Pimashkin, Alexey
   Gladkov, Arseniy
   Agrba, Ekaterina
   Mukhina, Irina
   Kazantsev, Victor
TI Selectivity of stimulus induced responses in cultured hippocampal
   networks on microelectrode arrays
SO COGNITIVE NEURODYNAMICS
DT Article
DE Neural networks; Microelectrode array; Electrical stimulation in vitro;
   Hippocampal cultures; Brain information decoding
ID CORTICAL-NEURONS; DIRECTION SELECTIVITY; VISUAL AREA; STIMULATION;
   PLASTICITY; PROPAGATION; DYNAMICS; SPIKES; POTENTIALS; MECHANISMS
AB Sensory information can be encoded using the average firing rate and spike occurrence times in neuronal network responses to external stimuli. Decoding or retrieving stimulus characteristics from the response pattern generally implies that the corresponding neural network has a selective response to various input signals. The role of various spiking activity characteristics (e.g., spike rate and precise spike timing) for basic information processing was widely investigated on the level of neural populations but gave inconsistent evidence for particular mechanisms. Multisite electrophysiology of cultured neural networks grown on microelectrode arrays is a recently developed tool and currently an active research area. In this study, we analyzed the stimulus responses represented by network-wide bursts evoked from various spatial locations (electrodes). We found that the response characteristics, such as the burst initiation time and the spike rate, can be used to retrieve information about the stimulus location. The best selectivity in the response spiking pattern could be found for a small subpopulation of neurones (electrodes) at relatively short post-stimulus intervals. Such intervals were unique for each culture due to the non-uniform organization of the functional connectivity in the network during spontaneous development.
C1 [Pimashkin, Alexey; Gladkov, Arseniy; Agrba, Ekaterina; Mukhina, Irina; Kazantsev, Victor] Lobachevsky State Univ Nizhny Novgorod, Translat Technol Ctr, Neuroengn Lab, Gagarin Ave 23, Nizhnii Novgorod 603950, Russia.
   [Gladkov, Arseniy; Mukhina, Irina] Nizhny Novgorod State Med Acad, Cell Technol Dept, Cent Res Lab, 10-1 Minin & Pozharsky Sq, Nizhnii Novgorod 603005, Russia.
RP Pimashkin, A (corresponding author), Lobachevsky State Univ Nizhny Novgorod, Translat Technol Ctr, Neuroengn Lab, Gagarin Ave 23, Nizhnii Novgorod 603950, Russia.
EM pimashkin@neuro.nnov.ru
CR Adrian ED, 1926, J PHYSIOL-LONDON, V61, P151, DOI 10.1113/jphysiol.1926.sp002281
   Bakkum DJ, 2008, PLOS ONE, V3, DOI 10.1371/journal.pone.0002088
   Birznieks I, 2001, J NEUROSCI, V21, P8222, DOI 10.1523/JNEUROSCI.21-20-08222.2001
   Bologna LL, 2010, NEUROSCIENCE, V165, P692, DOI 10.1016/j.neuroscience.2009.11.018
   BOVE M, 1995, BIOELECTROCH BIOENER, V38, P255, DOI 10.1016/0302-4598(95)01848-9
   Cariani P. A., 2001, Acoustical Science and Technology, V22, P77, DOI 10.1250/ast.22.77
   Carmena JM, 2003, PLOS BIOL, V1, P193, DOI 10.1371/journal.pbio.0000042
   CELEBRINI S, 1993, VISUAL NEUROSCI, V10, P811, DOI 10.1017/S0952523800006052
   Chiappalone M, 2008, EUR J NEUROSCI, V28, P221, DOI 10.1111/j.1460-9568.2008.06259.x
   Cozzi L, 2006, BIOL CYBERN, V94, P335, DOI 10.1007/s00422-006-0051-2
   Cozzi L, 2005, NEUROCOMPUTING, V65, P783, DOI 10.1016/j.neucom.2004.10.075
   Crook JM, 1998, EUR J NEUROSCI, V10, P2056, DOI 10.1046/j.1460-9568.1998.00218.x
   DAVIES DL, 1979, IEEE T PATTERN ANAL, V1, P224, DOI 10.1109/TPAMI.1979.4766909
   DeMarse TB, 2001, AUTON ROBOT, V11, P305, DOI 10.1023/A:1012407611130
   Doud AJ, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0026322
   Engel AK, 2005, NAT REV NEUROSCI, V6, P35, DOI 10.1038/nrn1585
   Eytan D, 2003, J NEUROSCI, V23, P9349
   Fanini A, 2009, J NEUROPHYSIOL, V101, P289, DOI 10.1152/jn.00400.2007
   Fukushima Y, 2007, COGN NEURODYNAMICS, V1, P305, DOI 10.1007/s11571-007-9026-9
   Gong HY, 2010, COGN NEURODYNAMICS, V4, P337, DOI 10.1007/s11571-010-9121-1
   Gu YQ, 2007, COGN NEURODYNAMICS, V1, P275, DOI 10.1007/s11571-007-9028-7
   Heil P, 1997, J NEUROPHYSIOL, V77, P2616, DOI 10.1152/jn.1997.77.5.2616
   Ikegaya Y, 2004, SCIENCE, V304, P559, DOI 10.1126/science.1093173
   Jenmalm P, 1997, J NEUROSCI, V17, P4486
   Jimbo Y, 2000, BIOELECTROCHEMISTRY, V51, P107, DOI 10.1016/S0302-4598(99)00083-5
   JIMBO Y, 1992, BIOELECTROCH BIOENER, V29, P193, DOI 10.1016/0302-4598(92)80067-Q
   Jimbo Y, 2000, BIOL CYBERN, V83, P1, DOI 10.1007/PL00007970
   Johansson RS, 2004, NAT NEUROSCI, V7, P170, DOI 10.1038/nn1177
   Kraskov A, 2007, J COGNITIVE NEUROSCI, V19, P479, DOI 10.1162/jocn.2007.19.3.479
   le Feber J, 2015, LEARN MEMORY, V22, P594, DOI 10.1101/lm.039362.115
   le Feber J, 2010, PLOS ONE, V5, DOI 10.1371/journal.pone.0008871
   Lebedev MA, 2005, J NEUROSCI, V25, P4681, DOI 10.1523/JNEUROSCI.4088-04.2005
   Li YL, 2007, BIOPHYS J, V93, P4151, DOI 10.1529/biophysj.107.111153
   Lo James Ting-Ho, 2010, Cogn Neurodyn, V4, P401, DOI 10.1007/s11571-010-9127-8
   Luo A, 2010, J NEURAL ENG, V7, DOI 10.1088/1741-2560/7/2/026010
   Maccione A, 2012, J NEUROSCI METH, V207, P161, DOI 10.1016/j.jneumeth.2012.04.002
   Maccione A, 2009, J NEUROSCI METH, V177, P241, DOI 10.1016/j.jneumeth.2008.09.026
   Madhavan R, 2007, PHYS BIOL, V4, P181, DOI 10.1088/1478-3975/4/3/005
   MAEDA E, 1995, J NEUROSCI, V15, P6834
   Marom S, 2002, Q REV BIOPHYS, V35, P63, DOI 10.1017/S0033583501003742
   Martinoia S, 2005, BIOSENS BIOELECTRON, V20, P2071, DOI 10.1016/j.bios.2004.09.012
   MAUNSELL JHR, 1983, J NEUROPHYSIOL, V49, P1127, DOI 10.1152/jn.1983.49.5.1127
   Mokeichev A, 2007, NEURON, V53, P413, DOI 10.1016/j.neuron.2007.01.017
   Novellino A, 2007, Comput Intell Neurosci, P12725, DOI 10.1155/2007/12725
   Pimashkin A, 2013, FRONT NEURAL CIRCUIT, V7, DOI 10.3389/fncir.2013.00087
   Pimashkin A, 2011, FRONT COMPUT NEUROSC, V5, DOI 10.3389/fncom.2011.00046
   Potter SM, 2001, J NEUROSCI METH, V110, P17, DOI 10.1016/S0165-0270(01)00412-5
   Potter SM, 2005, ADV NETWORK ELECTROP, P215
   Quiroga RQ, 2004, NEURAL COMPUT, V16, P1661, DOI 10.1162/089976604774201631
   Raichman N, 2008, J NEUROSCI METH, V170, P96, DOI 10.1016/j.jneumeth.2007.12.020
   Rasch M, 2009, J NEUROSCI, V29, P13785, DOI 10.1523/JNEUROSCI.2390-09.2009
   Rolston JD, 2007, NEUROSCIENCE, V148, P294, DOI 10.1016/j.neuroscience.2007.05.025
   Segev R, 2002, PHYS REV LETT, V88, DOI 10.1103/PhysRevLett.88.118102
   Seif Z, 2015, COGN NEURODYNAMICS, V9, P509, DOI 10.1007/s11571-015-9336-2
   Shahaf G, 2001, J NEUROSCI, V21, P8782, DOI 10.1523/JNEUROSCI.21-22-08782.2001
   Shahaf G, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.1000228
   Sigala N, 2002, NATURE, V415, P318, DOI 10.1038/415318a
   Sompolinsky H, 1997, CURR OPIN NEUROBIOL, V7, P514, DOI 10.1016/S0959-4388(97)80031-1
   Stegenga J, 2010, BIOPHYS J, V98, P2452, DOI 10.1016/j.bpj.2010.02.026
   Sundberg KA, 2009, NEURON, V61, P952, DOI 10.1016/j.neuron.2009.02.023
   Tateno T, 1999, BIOL CYBERN, V80, P45, DOI 10.1007/s004220050503
   Tessadori J, 2012, FRONT NEURAL CIRCUIT, V6, DOI 10.3389/fncir.2012.00099
   Vajda I, 2008, BIOPHYS J, V94, P5028, DOI 10.1529/biophysj.107.112730
   Wagenaar DA, 2005, J NEUROSCI, V25, P680, DOI 10.1523/JNEUROSCI.4209-04.2005
   Wagenaar DA, 2004, J NEUROSCI METH, V138, P27, DOI 10.1016/j.jneumeth.2004.03.005
   Wagenaar DA, 2006, J NEGAT RESULTS BIOM, V5, DOI 10.1186/1477-5751-5-16
   Wang ZS, 2008, J NEURAL ENG, V5, P433, DOI 10.1088/1741-2560/5/4/008
   Warwick K, 2010, DEFENCE SCI J, V60, P5, DOI 10.14429/dsj.60.11
   Wesson DW, 2008, PLOS BIOL, V6, P717, DOI 10.1371/journal.pbio.0060082
   Wilent WB, 2005, J NEUROSCI, V25, P2983, DOI 10.1523/JNEUROSCI.4906-04.2005
   Yoneyama M, 2011, COGN NEURODYNAMICS, V5, P333, DOI 10.1007/s11571-011-9158-9
NR 71
TC 14
Z9 16
U1 1
U2 24
PD AUG
PY 2016
VL 10
IS 4
BP 287
EP 299
DI 10.1007/s11571-016-9380-6
UT WOS:000379978800002
DA 2023-11-16
ER

PT J
AU Godin, C
   Gordon, MB
   Muller, JD
AF Godin, C
   Gordon, MB
   Muller, JD
TI SpikeCell: a deterministic spiking neuron
SO NEURAL NETWORKS
DT Article
DE neural networks; spiking neurons; pulsed neural networks; formal
   neurons; learning; hardware implementation; harsh environment
ID NETWORKS
AB We present a model of spiking neuron that emulates the output of the usual static neurons with sigmoidal activation functions. It allows for hardware implementations of standard feedforward networks, trained off-line with any classical learning algorithm (i.e. back-propagation and its variants). The model is validated on hand-written digits recognition, and image classification tasks. A digital architecture is proposed and evaluated. The area needed for implementing the spiking neuron on a chip is 10 times smaller than that for the corresponding static neuron. The accuracy of the network's output increases with time, and reaches that of the emulated static neural network after an adequate integration period. Single errors in the spike trains, or interruption of the relaxation process, due for example to irradiation in harsh environments, are harmless. (C) 2002 Published by Elsevier Science Ltd.
C1 CEA Grenoble, SPSMS, DRFMC, F-38054 Grenoble, France.
   CEA, DAM, DASE, F-91680 Bruyeres Le Chatel, France.
RP Gordon, MB (corresponding author), Imag Lab Grenoble, Lab Leibniz, 46 Av Felix Viallet, F-38031 Grenoble, France.
EM mirta.gordon@imag.fr
CR ALSPECTOR J, 1988, NEURAL INFORMATION P
   [Anonymous], 1998, IMAGE PROCESSING USI
   BENNANI Y, 1998, SN28 RELEASE F 1 2
   Bishop C., 1995, NEURAL NETWORKS PATT
   CHENTOUF R, 1995, LECT NOTES COMPUTER, V930
   Conti M, 2000, NEURAL NETWORKS, V13, P125, DOI 10.1016/S0893-6080(99)00101-X
   CUN YL, 1989, P NIPS 89
   Delorme A, 1999, NEUROCOMPUTING, V26-7, P989, DOI 10.1016/S0925-2312(99)00095-8
   Edwards PJ, 2000, IEE P-COMPUT DIG T, V147, P27, DOI 10.1049/ip-cdt:20000357
   GERSTNER W, 2001, HDB BIOL PHYSICS, P447
   Gorse D, 1997, NEUROCOMPUTING, V14, P319, DOI 10.1016/S0925-2312(96)00034-3
   HASSIBI B, 1993, ADV NEURAL INFORMATI, V5
   Johnson JL, 1999, IEEE T NEURAL NETWOR, V10, P461, DOI 10.1109/TNN.1999.761704
   LeCun Y., 1995, INT C ART NEUR NETW
   LERAY J, 1988, T NUCL SCI, V35, pR20
   Maass W, 1997, NEURAL COMPUT, V9, P279, DOI 10.1162/neco.1997.9.2.279
   Maass W., 1999, PULSED NEURAL NETWOR
   PELLOIE J, 1996, P INT SOI C
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   SAMUELIDES M, 1997, LECT NOTES COMPUTER, V1327, P145
   Shawe-Taylor J., 1991, Connection Science, V3, P317, DOI 10.1080/09540099108946589
   TOMLINSON M, 1990, INT JOINT C NEUR NET, V2
   Van Rullen R, 2001, NEURAL COMPUT, V13, P1255, DOI 10.1162/08997660152002852
NR 23
TC 1
Z9 1
U1 0
U2 2
PD SEP
PY 2002
VL 15
IS 7
BP 873
EP 879
AR PII S0893-6080(02)00033-3
DI 10.1016/S0893-6080(02)00033-3
UT WOS:000178497800006
DA 2023-11-16
ER

PT J
AU Mazumder, P
   Hu, D
   Ebong, I
   Zhang, X
   Xu, Z
   Ferrari, S
AF Mazumder, P.
   Hu, D.
   Ebong, I.
   Zhang, X.
   Xu, Z.
   Ferrari, S.
TI Digital implementation of a virtual insect trained by spike-timing
   dependent plasticity
SO INTEGRATION-THE VLSI JOURNAL
DT Article
DE Spike timing dependent plasticity; Neural network
ID NETWORKS; NEURONS
AB Neural network approach to processing have been shown successful and efficient in numerous real world applications. The most successful of this approach are implemented in software but in order to achieve real-time processing similar to that of biological neural networks, hardware implementations of these networks need to be continually improved. This work presents a spiking neural network (SNN) implemented in digital CMOS. The SNN is constructed based on an indirect training algorithm that utilizes spike-timing dependent plasticity (STDP). The SNN is validated by using its outputs to control the motion of a virtual insect. The indirect training algorithm is used to train the SNN to navigate through a terrain with obstacles. The indirect approach is more appropriate for nanoscale CMOS implementation synaptic training since it is getting more difficult to perfectly control matching in CMOS circuits. (C) 2016 Elsevier B.V. All rights reserved.
C1 [Mazumder, P.; Hu, D.; Ebong, I.] Univ Michigan, Ann Arbor, MI 48109 USA.
   [Zhang, X.; Xu, Z.; Ferrari, S.] Duke Univ, Durham, NC 27708 USA.
RP Mazumder, P (corresponding author), Univ Michigan, Ann Arbor, MI 48109 USA.
EM pinakimazum@gmail.com; hudi@umich.edu; idong@eecs.umich.edu;
   xz70@duke.edu; dec.ziyer@gmail.com; sferrari@duke.edu
CR Arena P, 2009, IEEE T NEURAL NETWOR, V20, P202, DOI 10.1109/TNN.2008.2005134
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Burgsteiner H, 2006, ENG APPL ARTIF INTEL, V19, P741, DOI 10.1016/j.engappai.2006.05.007
   Ferrari S, 2008, IEEE IJCNN, P1780, DOI 10.1109/IJCNN.2008.4634039
   Gerstner W., 2001, SPIKING NEURON MODEL
   GLESNER M, 1994, NEUROCOMPUTERS OVERV
   HAMMERSTROM D, 1991, VLSI ARTIFICIAL INTELLIGENCE AND NEURAL NETWORKS, P357
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   LaValle M. Steven, 2006, PLANNING ALGORITHMS
   Liu Y, 2007, IFIP INT C NETW PARA, P748, DOI 10.1109/NPC.2007.26
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maass W, 2001, PULSED NEURAL NETWOR
   Mead C., 1989, ANALOG VLSI IMPLEMEN
   RAMACHER U, 1992, J PARALLEL DISTR COM, V14, P306, DOI 10.1016/0743-7315(92)90070-4
   SCHOENAUER T, 1998, NEURONAL NETWORKS AP, P101
   Snider GS, 2008, 2008 IEEE INTERNATIONAL SYMPOSIUM ON NANOSCALE ARCHITECTURES, P85, DOI 10.1109/NANOARCH.2008.4585796
   TRELEAVEN P, 1989, IEEE MICRO, V9, P8, DOI 10.1109/40.42984
   Zamarreño-Ramos C, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00026
   Zhang X, 2013, IEEE DECIS CONTR P, P6798, DOI 10.1109/CDC.2013.6760966
NR 20
TC 13
Z9 13
U1 0
U2 11
PD JUN
PY 2016
VL 54
BP 109
EP 117
DI 10.1016/j.vlsi.2016.01.002
UT WOS:000374362900009
DA 2023-11-16
ER

PT S
AU Espinal, A
   Carpio, M
   Ornelas, M
   Puga, H
   Melín, P
   Sotelo-Figueroa, M
AF Espinal, Andres
   Carpio, Martin
   Ornelas, Manuel
   Puga, Hector
   Melin, Patricia
   Sotelo-Figueroa, Marco
BE Melin, P
   Castillo, O
   Kacprzyk, J
TI Evolutionary Indirect Design of Feed-Forward Spiking Neural Networks
SO DESIGN OF INTELLIGENT SYSTEMS BASED ON FUZZY LOGIC, NEURAL NETWORKS AND
   NATURE-INSPIRED OPTIMIZATION
SE Studies in Computational Intelligence
DT Article; Book Chapter
ID NEURONS
AB The present paper proposes the automatic design of Feed-Forward Spiking Neural Networks by representing several inherent aspects of the neural architecture in a proposed Context-Free Grammar; which is evolved through an Evolutionary Strategy. In the indirect design, the power of the design and the capabilities of the designed neural network are strongly related with the complexity of the grammars. The neural networks designed with the proposed grammar are tested with two well-known benchmark datasets of pattern recognition. Finally, neural networks derived from the proposed grammar are compared with other generated by similar grammars which were designed for the same purposed, the neural network design.
C1 [Espinal, Andres; Carpio, Martin; Ornelas, Manuel; Puga, Hector; Sotelo-Figueroa, Marco] Tecnol Nacl Mexico Inst Tecnol Leon, Leon, Gto, Mexico.
   [Melin, Patricia] Tecnol Nacl Mexico Inst Tecnol Tijuana, Calz Tecnol S-N, Tijuana, Bc, Mexico.
RP Melín, P (corresponding author), Tecnol Nacl Mexico Inst Tecnol Tijuana, Calz Tecnol S-N, Tijuana, Bc, Mexico.
EM andres.espinal@itleon.edu.mx; pmelin@tectijuana.mx
CR [Anonymous], NEURAL NETWORK MODEL
   [Anonymous], P 14 INT C COMP SUPP
   [Anonymous], 2009, FDN GRAMMATICAL EVOL
   [Anonymous], 2010, BIOL INSPIRED NEURAL
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   López LFD, 2012, J COMPUT SCI-NETH, V3, P46, DOI 10.1016/j.jocs.2011.12.005
   Ding SF, 2013, ARTIF INTELL REV, V39, P251, DOI 10.1007/s10462-011-9270-6
   Espinal A, 2014, LECT NOTES COMPUT SC, V8495, P71, DOI 10.1007/978-3-319-07491-7_8
   FANG HL, 1993, PROCEEDINGS OF THE FIFTH INTERNATIONAL CONFERENCE ON GENETIC ALGORITHMS, P375
   GERSTNER W, 1995, PHYS REV E, V51, P738, DOI 10.1103/PhysRevE.51.738
   Gerstner W., 2002, SPIKING NEURON MODEL
   Holland JH., 1975, ADAPTATION NATURAL A
   Johnson C, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P1802, DOI 10.1109/IJCNN.2011.6033443
   Kohavi R., 1995, INT JOINT C ART INT, P1137, DOI DOI 10.1067/MOD.2000.109031
   Koza JR., 2005, SEARCH METHODOLOGIES, P127, DOI DOI 10.1007/0-387-28356-0_5
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maass W., 1996, NEURAL INFORM PROCES, P211
   O'Neill M., 2006, INT C ART INT ICAI 0
   Rechenberg I., 1973, EVOLUTIONS STRATEGIE
   Ryan C., 1998, Genetic Programming. First European Workshop, EuroGP'98. Proceedings, P83, DOI 10.1007/BFb0055930
   Yao X, 1999, P IEEE, V87, P1423, DOI 10.1109/5.784219
NR 22
TC 0
Z9 0
U1 0
U2 0
PY 2015
VL 601
BP 89
EP 101
DI 10.1007/978-3-319-17747-2_7
D2 10.1007/978-3-319-17747-2
UT WOS:000383955000008
DA 2023-11-16
ER

PT C
AU Batllori, R
   Laramee, CB
   Land, W
   Schaffer, JD
AF Batllori, R.
   Laramee, C. B.
   Land, W.
   Schaffer, J. D.
BE Dagli, CH
TI Evolving spiking neural networks for robot control
SO COMPLEX ADAPTIVE SYSTEMS
SE Procedia Computer Science
DT Proceedings Paper
CT Conference of the Complex Adaptive Systems on Responding to Continuous
   Global Change in Systems Needs
CY OCT 30-NOV 02, 2011
CL Chicago, IL
DE robotics; spiking neural networks; genetic algorithms
AB We describe a sequence of experiments in which a robot "brain" was evolved to mimic the behaviours captured under control of a heuristic rule program (imitation learning). The task was light-seeking while avoiding obstacles using binocular light sensors and a trio of IR proximity sensors. The "brain" was a spiking neural network simulator whose parameters were tuned by a genetic algorithm, where fitness was assessed by the closeness to target output spike trains. Spike trains were frequency encoded. The network topology was manually designed, and then modified in response to observed difficulties during evolution. We noted that good performance seems best approached by judicious mixing of excitation and inhibition. Besides robotic applications, the domain of "smart" prosthetics also appears promising. (C) 2011 Published by Elsevier B.V.
C1 [Batllori, R.; Laramee, C. B.; Land, W.; Schaffer, J. D.] SUNY Binghamton, Dept Bioengn, New York, NY 13901 USA.
RP Batllori, R (corresponding author), SUNY Binghamton, Dept Bioengn, New York, NY 13901 USA.
EM robbatllori@gmail.com; dschaffe@binghamton.edu
CR Braitenberg V., 1984, VEHICLES EXPT SYNTHE
   ESHELMAN LJ, 1990, P 1 WORKSH FDN GEN A, P265
   Floreano D., 2008, BIOINSPIRED ARTIFICI
   Floreano D., 2001, LNCS, P38
   Floreano D, 2008, EVOL INTELL, V1, P47, DOI 10.1007/s12065-007-0002-4
   Gerstner W., 2002, SPIKING NEURON MODEL
   Hagras H, 2004, IEEE INT CONF ROBOT, P4620, DOI 10.1109/ROBOT.2004.1302446
   Maass W, 1997, ADV NEUR IN, V9, P211
   Rosen AM, 2010, J NEUROPHYSIOL, V104, P4, DOI 10.1152/jn.01098.2009
   Schaffer J.D., 2009, GEN EV COMP C GECCO2, P2661
   Sichtig H., 2009, THESIS BINGHAMTON U
   Wang XQ, 2008, NEUROCOMPUTING, V71, P655, DOI 10.1016/j.neucom.2007.08.025
   Zufferey JC, 2002, LECT NOTES COMPUT SC, V2525, P592
NR 13
TC 29
Z9 29
U1 0
U2 2
PY 2011
VL 6
DI 10.1016/j.procs.2011.08.060
UT WOS:000299124600051
DA 2023-11-16
ER

PT S
AU Maida, AS
AF Maida, A. S.
BE Gudivada, VN
   Raghavan, VV
   Govindaraju, V
   Rao, CR
TI Cognitive Computing and Neural Networks: Reverse Engineering the Brain
SO COGNITIVE COMPUTING: THEORY AND APPLICATIONS
SE Handbook of Statistics
DT Article; Book Chapter
DE Brain simulation; Deep belief networks; Convolutional networks; Liquid
   computing; Biological neural networks; Neocortex
ID COMPUTATIONAL POWER; VISUAL FEATURES; SPIKE; RECOGNITION; MODEL;
   RECONSTRUCTION; CIRCUIT; CORTEX; CELLS
AB Cognitive computing seeks to build applications which model and mimic human thinking. One approach toward achieving this goal is to develop brain-inspired computational models. A prime example of such a model is the class of deep convolutional networks which is currently used in pattern recognition, machine vision, and machine learning.
   We offer a brief review of the mammalian neocortex, the minicolumn, and the ventral pathway. We provide descriptions of abstract neural circuits that have been used to model these areas of the brain. This include Poisson spiking networks, liquid computing networks, spiking models of feature discovery in the ventral pathway, spike-timing-dependent plasticity learning, restricted Boltzmann machines, deep belief networks, and deep convolutional networks.
   In summary, this chapter explores abstractions of neural networks found within the mammalian neocortex that support cognition and the beginnings of cognitive computation.
C1 [Maida, A. S.] Univ Louisiana Lafayette, Ctr Adv Comp Studies, Lafayette, LA 70504 USA.
RP Maida, AS (corresponding author), Univ Louisiana Lafayette, Ctr Adv Comp Studies, Lafayette, LA 70504 USA.
CR ADELSON EH, 1985, J OPT SOC AM A, V2, P284, DOI 10.1364/JOSAA.2.000284
   Allman J., 1991, CEREBRAL CORTEX A, V8a, P269
   [Anonymous], 2005, NEURON BOOK
   [Anonymous], 2009, ADV NEURAL INFORM PR
   Azevedo FAC, 2009, J COMP NEUROL, V513, P532, DOI 10.1002/cne.21974
   BELL AJ, 1995, NEURAL COMPUT, V7, P1129, DOI 10.1162/neco.1995.7.6.1129
   Bell AJ, 1997, VISION RES, V37, P3327, DOI 10.1016/S0042-6989(97)00121-1
   Bengio Y., 2007, LARGE SCALE KERNEL M, V34, P1, DOI DOI 10.1038/NATURE14539
   Bengio Y., 2015, DEEP LEARN WORKSH IN
   Bengio Yoshua, 2015, ARXIV150204156
   Bishop Christopher M., 2007, PATTERN RECOGNITION, V3, DOI [10.1117/1.2819119, DOI 10.1198/TECH.2007.S518]
   Buxhoeveden DP, 2002, BRAIN, V125, P935, DOI 10.1093/brain/awf110
   Cadieu CF, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003963
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Caporale N, 2008, ANNU REV NEUROSCI, V31, P25, DOI 10.1146/annurev.neuro.31.060407.125639
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Domingos P., 2015, MASTER ALGORITHM QUE
   Ermentrout G.B., 2010, MATHEMATICAL FDN NEU
   FUKUSHIMA K, 1980, BIOL CYBERN, V36, P193, DOI 10.1007/BF00344251
   Gerstner W, 1997, P NATL ACAD SCI USA, V94, P12740, DOI 10.1073/pnas.94.24.12740
   Gerstner W, 2012, SCIENCE, V338, P60, DOI 10.1126/science.1227356
   HARNAD S, 1990, PHYSICA D, V42, P335, DOI 10.1016/0167-2789(90)90087-6
   Harris KD, 2015, NAT NEUROSCI, V18, P170, DOI 10.1038/nn.3917
   Haykin S., 2008, NEURAL NETWORKS LEAR
   Hegdé J, 2000, J NEUROSCI, V20, part. no.
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hinton GE, 2007, PROG BRAIN RES, V165, P535, DOI 10.1016/S0079-6123(06)65034-6
   HUBEL DH, 1968, J PHYSIOL-LONDON, V195, P215, DOI 10.1113/jphysiol.1968.sp008455
   HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837
   Hung CP, 2005, SCIENCE, V310, P863, DOI 10.1126/science.1117593
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Jarrett K, 2009, IEEE I CONF COMP VIS, P2146, DOI 10.1109/ICCV.2009.5459469
   Jolivet R, 2006, J COMPUT NEUROSCI, V21, P35, DOI 10.1007/s10827-006-7074-5
   Kappel D, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003511
   Kasthuri N, 2015, CELL, V162, P648, DOI 10.1016/j.cell.2015.06.054
   Kheradpisheh SR, 2016, NEUROCOMPUTING, V205, P382, DOI 10.1016/j.neucom.2016.04.029
   Klampfl S, 2013, J NEUROSCI, V33, P11515, DOI 10.1523/JNEUROSCI.5044-12.2013
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
   Krüger N, 2013, IEEE T PATTERN ANAL, V35, P1847, DOI 10.1109/TPAMI.2012.272
   Kurzweil Ray, 2013, HOW TO CREATE A MIND
   LeCun Yann, 1989, ADV NEURAL INFORM PR, V2, P1, DOI DOI 10.1111/DSU.12130
   Lee H., 2008, ADV NEURAL INFORM PR, P873
   Lee H, 2011, COMMUN ACM, V54, P95, DOI 10.1145/2001269.2001295
   Lukosevicius M, 2009, COMPUT SCI REV, V3, P127, DOI 10.1016/j.cosrev.2009.03.005
   Maass W, 2000, NEURAL COMPUT, V12, P2519, DOI 10.1162/089976600300014827
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Maass W, 1996, NEURAL COMPUT, V8, P1, DOI 10.1162/neco.1996.8.1.1
   Maass W, 2015, P IEEE, V103, P2219, DOI 10.1109/JPROC.2015.2496679
   Markram H, 2015, CELL, V163, P456, DOI 10.1016/j.cell.2015.09.029
   Marr D., 1982, Vision. A computational investigation into the human representation and processing of visual information
   Masquelier T, 2007, PLOS COMPUT BIOL, V3, P247, DOI 10.1371/journal.pcbi.0030031
   Masquelier T, 2010, IEEE IJCNN, DOI 10.1109/IJCNN.2010.5596934
   McManus JNJ, 2008, J NEUROPHYSIOL, V99, P2086, DOI 10.1152/jn.00871.2007
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Modha DS, 2011, COMMUN ACM, V54, P62, DOI 10.1145/1978542.1978559
   Modha DS, 2010, P NATL ACAD SCI USA, V107, P13485, DOI 10.1073/pnas.1008054107
   Mountcastle V. B., 1998, PERCEPTUAL NEUROSCIE
   Mountcastle VB, 1997, BRAIN, V120, P701, DOI 10.1093/brain/120.4.701
   Murphy KP, 2012, MACHINE LEARNING: A PROBABILISTIC PERSPECTIVE, P1
   Nair V., 2010, ICML, P807
   Nessler B, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003037
   Norton D., 2006, P IEEE INT JOINT C N, P8359
   Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0
   Ramaswamy S, 2015, FRONT NEURAL CIRCUIT, V9, DOI 10.3389/fncir.2015.00044
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707
   Savarese S, 2007, IEEE I CONF COMP VIS, P1245
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Schurmann Felix, 2014, Supercomputing. 29th International Conference, ISC 2014. Proceedings: LNCS 8488, P331, DOI 10.1007/978-3-319-07518-1_21
   Serre T, 2007, IEEE T PATTERN ANAL, V29, P411, DOI 10.1109/TPAMI.2007.56
   Shadlen M., 2006, RATE VERSUS TEMPORAL
   SHARMA J, 2000, INDUCTION VISUAL ORI, V404, P841
   Sherman S.M., 2004, SYNAPTIC ORG BRAIN, P311
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Tanaka K, 1996, ANNU REV NEUROSCI, V19, P109, DOI 10.1146/annurev.ne.19.030196.000545
   Tavanaei A, 2016, IEEE IJCNN, P307, DOI 10.1109/IJCNN.2016.7727213
   TREISMAN A, 1977, PERCEPT PSYCHOPHYS, V22, P1, DOI 10.3758/BF03206074
   van Hateren JH, 1998, P ROY SOC B-BIOL SCI, V265, P359, DOI 10.1098/rspb.1998.0303
   Verstraeten D, 2005, INFORM PROCESS LETT, V95, P521, DOI 10.1016/j.ipl.2005.05.019
   Wolpert DH, 1996, NEURAL COMPUT, V8, P1341, DOI 10.1162/neco.1996.8.7.1341
NR 79
TC 5
Z9 5
U1 3
U2 7
PY 2016
VL 35
BP 39
EP 78
DI 10.1016/bs.host.2016.07.011
UT WOS:000401902500003
DA 2023-11-16
ER

PT J
AU Guo, YF
   Peng, WH
   Chen, YP
   Zhang, LW
   Liu, XD
   Huang, XH
   Ma, Z
AF Guo, Yufei
   Peng, Weihang
   Chen, Yuanpei
   Zhang, Liwen
   Liu, Xiaode
   Huang, Xuhui
   Ma, Zhe
TI Joint A-SNN: Joint training of artificial and spiking neural networks
   via self-Distillation and weight factorization
SO PATTERN RECOGNITION
DT Article
DE Spiking neural networks; Artificial neural networks; Knowledge
   distillation; Weight factorization
ID NEURONS
AB Emerged as a biology-inspired method, Spiking Neural Networks (SNNs) mimic the spiking nature of brain neurons and have received lots of research attention. SNNs deal with binary spikes as their activation and therefore derive extreme energy efficiency on hardware. However, it also leads to an intrinsic obstacle that training SNNs from scratch requires a re-definition of the firing function for computing gradient. Artificial Neural Networks (ANNs), however, are fully differentiable to be trained with gradient descent. In this paper, we propose a joint training framework of ANN and SNN, in which the ANN can guide the SNN's optimization. This joint framework contains two parts: First, the knowledge inside ANN is distilled to SNN by using multiple branches from the networks. Second, we restrict the parameters of ANN and SNN, where they share partial parameters and learn different singular weights. Extensive experiments over several widely used network structures show that our method consistently outperforms many other state-of-the-art training methods. For example, on the CIFAR100 classification task, the spiking ResNet-18 model trained by our method can reach to 77.39 % top-1 accuracy with only 4 time steps.(c) 2023 Published by Elsevier Ltd.
C1 [Guo, Yufei; Peng, Weihang; Chen, Yuanpei; Zhang, Liwen; Liu, Xiaode; Huang, Xuhui; Ma, Zhe] CASIC, Intelligent Sci & Technol Acad, Beijing 100144, Peoples R China.
   [Guo, Yufei; Peng, Weihang; Chen, Yuanpei; Zhang, Liwen; Liu, Xiaode; Huang, Xuhui; Ma, Zhe] Sci Res Lab Aerosp Intelligent Syst & Technol, Beijing 100144, Peoples R China.
RP Ma, Z (corresponding author), CASIC, Intelligent Sci & Technol Acad, Beijing 100144, Peoples R China.; Ma, Z (corresponding author), Sci Res Lab Aerosp Intelligent Syst & Technol, Beijing 100144, Peoples R China.
EM mazhe_thu@163.com
CR Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   Bengio Y, 2013, Arxiv, DOI arXiv:1308.3432
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Bing Han, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13555, DOI 10.1109/CVPR42600.2020.01357
   Bu T., 2022, INT C LEARNING REPRE
   Chowdhury SS, 2022, LECT NOTES COMPUT SC, V13671, P709, DOI 10.1007/978-3-031-20083-0_42
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Dean J., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1063/1.4931082
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng S., 2022, INT C LEARN REPR
   Deng S., 2021, INT C LEARN REPR
   Diehl PU, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON REBOOTING COMPUTING (ICRC)
   Falez P., 2019, PATTERN RECOGNIT, V93
   Fang W, 2021, INCORPORATING LEARNA, P2661
   Garg I, 2021, P IEEE CVF INT C COM, P4671
   Guo Y., 2022, ADV NEURAL INFORM PR
   Guo YF, 2022, LECT NOTES COMPUT SC, V13671, P36, DOI 10.1007/978-3-031-20083-0_3
   Guo Y, 2022, LECT NOTES COMPUT SC, V13672, P52, DOI 10.1007/978-3-031-19775-8_4
   Guo YF, 2022, PROC CVPR IEEE, P326, DOI 10.1109/CVPR52688.2022.00042
   HEBB D. O., 1949
   Horowitz M, 2014, ISSCC DIG TECH PAP I, V57, P10, DOI 10.1109/ISSCC.2014.6757323
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90
   Kim Y., 2020, FRONT NEUROSCI-SWITZ, P1638
   Kingma DP., 2017, ARXIV
   Krizhevsky Alex, CIFAR 10 CANADIAN I, P2
   Kundu S, 2021, IEEE WINT CONF APPL, P3952, DOI 10.1109/WACV48630.2021.00400
   Kushawaha R., 2020, DISTILLING SPIKES KN
   Li Yan, 2021, arXiv
   Li Y., 2021, INT C MACHINE LEARNI, V139, P6316
   Loshchilov Ilya, 2016, ARXIV160803983
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Meng QY, 2022, NEURAL NETWORKS, V153, P254, DOI 10.1016/j.neunet.2022.06.001
   Mozafari M, 2019, PATTERN RECOGN, V94, P87, DOI 10.1016/j.patcog.2019.05.015
   Nahmias MA, 2013, IEEE J SEL TOP QUANT, V19, DOI 10.1109/JSTQE.2013.2257700
   Panda P, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00653
   Rathi N, 2020, Arxiv, DOI arXiv:2008.03658
   Rathi Nitin, 2020, INT C LEARN REPR
   Romero A, 2015, Arxiv, DOI [arXiv:1412.6550, DOI 10.48550/ARXIV.1412.6550]
   Roy K, 2019, NATURE, V575, P607, DOI 10.1038/s41586-019-1677-2
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tang R., 2023, PATTERN RECOGN
   Teerapittayanon S, 2016, INT C PATT RECOG, P2464, DOI 10.1109/ICPR.2016.7900006
   Wang L., 2018, ADV NEURAL INF PROCE, V31
   Wang S., 2022, PAYING MORE ATTENTIO
   Wang ZR, 2021, PATTERN RECOGN, V111, DOI 10.1016/j.patcog.2020.107722
   Wu YJ, 2019, AAAI CONF ARTIF INTE, P1311
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Yim J, 2017, PROC CVPR IEEE, P7130, DOI 10.1109/CVPR.2017.754
   Zhang S., 2022, PATTERN RECOGN, P130
   Zhang W, 2020, ADV NEURAL INFORM PR, V33, P12022, DOI DOI 10.48550/ARXIV.2002.10085
   Zhao PS, 2022, PATTERN RECOGN, V129, DOI 10.1016/j.patcog.2022.108741
   Zheng HL, 2021, AAAI CONF ARTIF INTE, V35, P11062
NR 53
TC 2
Z9 2
U1 4
U2 4
PD OCT
PY 2023
VL 142
AR 109639
DI 10.1016/j.patcog.2023.109639
EA MAY 2023
UT WOS:001001249800001
DA 2023-11-16
ER

PT J
AU Feng, LC
   Zhang, YQ
   Zhu, ZM
AF Feng, Lichen
   Zhang, Yueqi
   Zhu, Zhangming
TI An Efficient Multilayer Spiking Convolutional Neural Network Processor
   for Object Recognition With Low Bitwidth and Channel-Level Parallelism
SO IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS II-EXPRESS BRIEFS
DT Article
DE Channel-level parallel; rank-order coding; sparse event; spike-centric;
   spiking convolutional neural network
AB Previous studies have shown that the event-driven multilayer spiking convolutional neural network (SCNN) can reduce computational complexity largely while keeping accurate. To fully utilize the advantages of SCNN, this brief proposed an efficient multilayer SCNN processor for object recognition. The interconnection between spiking layers is implemented for the first time. The rank-order coding with mutual and lateral inhibitions enables sparse event transmission. By further combining the spike-centric membrane potential update, channel-level parallel operation, and the low bitwidths of synapse weights and potentials, the proposed design achieves 500 classifications/s, and 68 uJ/classification for recognizing images with 160x250 resolution, which is superior to the recent works.
C1 [Feng, Lichen; Zhang, Yueqi; Zhu, Zhangming] Xidian Univ, Sch Microelect, Shaanxi Key Lab Integrated Circuits & Syst, Xian 710071, Peoples R China.
RP Zhu, ZM (corresponding author), Xidian Univ, Sch Microelect, Shaanxi Key Lab Integrated Circuits & Syst, Xian 710071, Peoples R China.
EM zmyh@263.net
CR Abraham WC, 1996, TRENDS NEUROSCI, V19, P126, DOI 10.1016/S0166-2236(96)80018-X
   Chen GK, 2019, IEEE J SOLID-ST CIRC, V54, P992, DOI 10.1109/JSSC.2018.2884901
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Horowitz M, 2014, ISSCC DIG TECH PAP I, V57, P10, DOI 10.1109/ISSCC.2014.6757323
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Kirkland P, 2020, IEEE IJCNN, DOI 10.1109/ijcnn48605.2020.9207075
   LeCun Y, 2010, IEEE INT SYMP CIRC S, P253, DOI 10.1109/ISCAS.2010.5537907
   Lichtsteiner P, 2008, IEEE J SOLID-ST CIRC, V43, P566, DOI 10.1109/JSSC.2007.914337
   Liu DQ, 2017, NEUROCOMPUTING, V249, P212, DOI 10.1016/j.neucom.2017.04.003
   Maass W., 2002, INF PROCESS TELEMATI, V8, P32
   Ponulak F, 2011, ACTA NEUROBIOL EXP, V71, P409
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Sourikopoulos I, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00123
   Tang H, 2020, NEUROCOMPUTING, V407, P300, DOI 10.1016/j.neucom.2020.05.031
   Tapiador-Morales R, 2019, IEEE T BIOMED CIRC S, V13, P159, DOI 10.1109/TBCAS.2018.2880012
   Tavanaei A, 2019, NEUROCOMPUTING, V330, P39, DOI 10.1016/j.neucom.2018.11.014
   Wang Q, 2017, NEUROCOMPUTING, V221, P146, DOI 10.1016/j.neucom.2016.09.071
   Yousefzadeh A., 2015, IEEE, P1, DOI 10.1109/EBCCSP.2015.7300698
   Zhou ZL, 2020, IEEE TETCI, V4, P593, DOI 10.1109/TETCI.2019.2909936
NR 19
TC 1
Z9 1
U1 2
U2 3
PD DEC
PY 2022
VL 69
IS 12
BP 5129
EP 5133
DI 10.1109/TCSII.2022.3207989
UT WOS:000922028300101
DA 2023-11-16
ER

PT C
AU Asai, Y
   Villa, AEP
AF Asai, Yoshiyuki
   Villa, Alessandro E. P.
BE Diamantaras, K
   Duch, W
   Iliadis, LS
TI Transmission of Distributed Deterministic Temporal Information through a
   Diverging/Converging Three-Layers Neural Network
SO ARTIFICIAL NEURAL NETWORKS-ICANN 2010, PT I
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 20th International Conference on Artificial Neural Networks
CY SEP 15-18, 2010
CL Thessaloniki, GREECE
DE Spiking neural networks; synfire chains; adaptive threshold neurons;
   computational neuroscience; preferred firing sequences
ID FIRE MODEL; INTEGRATE; ATTRACTOR; PATTERNS
AB This study investigates the ability of a diverging/converging neural network to transmit and integrate a complex temporally organized activity embedded in afferent spike trains. The temporal information is originally generated by a deterministic nonlinear dynamical system whose parameters determine a chaotic attractor. We present the simulations obtained with a network formed by simple spiking neurons (SSN) and a network formed by a multiple-timescale adaptive threshold neurons (MAT). The assessment of the temporal structure embedded in the spike trains is carried out by sorting the preferred firing sequences detected by the pattern grouping algorithm (PGA). The results suggest that adaptive threshold neurons are much more efficient in maintaining a specific temporal structure distributed across multiple spike trains throughout the layers of a feed-forward network.
C1 [Asai, Yoshiyuki] Osaka Univ, Ctr Adv Med Engn & Informat, Osaka, Japan.
   [Asai, Yoshiyuki; Villa, Alessandro E. P.] Univ Joseph Fourier, Grenoble Inst Neurosci, Neuro Heurist Res Grp, INSERM U836, Grenoble, France.
   [Asai, Yoshiyuki; Villa, Alessandro E. P.] Univ Lausanne, Neuro Heurist Res Grp, Inst Informat Sci, Lausanne, Switzerland.
RP Asai, Y (corresponding author), Osaka Univ, Ctr Adv Med Engn & Informat, Osaka, Japan.
EM asai@bpe.es.osaka-u.ac.jp; alessandro.villa@ujf-grenoble.fr
CR Abeles M, 2001, J NEUROSCI METH, V107, P141, DOI 10.1016/S0165-0270(01)00364-8
   Abeles M., 1982, LOCAL CORTICAL CIRCU, DOI DOI 10.1007/978-3-642-81708-3
   ASAI Y, 2009, 8 INT NEUR COD WORKS, P81
   Asai Y, 2008, NEURAL NETWORKS, V21, P799, DOI 10.1016/j.neunet.2008.06.014
   Asai Y, 2006, LECT NOTES COMPUT SC, V4131, P623
   Brette R, 2005, J NEUROPHYSIOL, V94, P3637, DOI 10.1152/jn.00686.2005
   Celletti A, 1996, J STAT PHYS, V84, P1379, DOI 10.1007/BF02174137
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Kobayashi R, 2009, FRONT COMPUT NEUROSC, V3, DOI 10.3389/neuro.10.009.2009
   MPITSOS GJ, 1989, DYNAMICS SENSORY COG, P521
   Sacerdote L, 2006, B MATH BIOL, V68, P1257, DOI 10.1007/s11538-006-9107-7
   Tetko IV, 2001, J NEUROSCI METH, V105, P1, DOI 10.1016/S0165-0270(00)00336-8
   TETKO IV, 1997, LECT NOTES COMPUTER, V1327, P37
   Villa AEP, 1999, P SOC PHOTO-OPT INS, V3728, P20, DOI 10.1117/12.343039
   ZASLAVSKY GM, 1978, PHYS LETT A, V69, P145, DOI 10.1016/0375-9601(78)90195-0
NR 15
TC 1
Z9 1
U1 0
U2 0
PY 2010
VL 6352
BP 145
EP +
PN I
UT WOS:000287889800019
DA 2023-11-16
ER

PT J
AU Yuan, Y
   Zhu, YT
   Wang, JQ
   Li, RS
   Xu, X
   Fang, T
   Huo, H
   Wan, LH
   Li, QD
   Liu, N
   Yang, SY
AF Yuan, Ye
   Zhu, Yongtong
   Wang, Jiaqi
   Li, Ruoshi
   Xu, Xin
   Fang, Tao
   Huo, Hong
   Wan, Lihong
   Li, Qingdu
   Liu, Na
   Yang, Shiyan
TI Incorporating structural plasticity into self-organization recurrent
   networks for sequence learning
SO FRONTIERS IN NEUROSCIENCE
DT Article
DE spiking neural network; self-organization; reward-modulated spike
   timing-dependent plasticity; homeostatic plasticity; structural
   plasticity
ID TIMING-DEPENDENT PLASTICITY; MODEL
AB IntroductionSpiking neural networks (SNNs), inspired by biological neural networks, have received a surge of interest due to its temporal encoding. Biological neural networks are driven by multiple plasticities, including spike timing-dependent plasticity (STDP), structural plasticity, and homeostatic plasticity, making network connection patterns and weights to change continuously during the lifecycle. However, it is unclear how these plasticities interact to shape neural networks and affect neural signal processing. MethodHere, we propose a reward-modulated self-organization recurrent network with structural plasticity (RSRN-SP) to investigate this issue. Specifically, RSRN-SP uses spikes to encode information, and incorporate multiple plasticities including reward-modulated spike timing-dependent plasticity (R-STDP), homeostatic plasticity, and structural plasticity. On the one hand, combined with homeostatic plasticity, R-STDP is presented to guide the updating of synaptic weights. On the other hand, structural plasticity is utilized to simulate the growth and pruning of synaptic connections. Results and discussionExtensive experiments for sequential learning tasks are conducted to demonstrate the representational ability of the RSRN-SP, including counting task, motion prediction, and motion generation. Furthermore, the simulations also indicate that the characteristics arose from the RSRN-SP are consistent with biological observations.
C1 [Yuan, Ye; Zhu, Yongtong; Wang, Jiaqi; Li, Ruoshi; Xu, Xin; Li, Qingdu; Liu, Na] Univ Shanghai Sci & Technol, Inst Machine Intelligence, Sch Hlth Sci & Engn, Shanghai, Peoples R China.
   [Fang, Tao; Huo, Hong] Shanghai Jiao Tong Univ, Automat Dept, Shanghai, Peoples R China.
   [Wan, Lihong] Origin Dynam Intelligent Robot Co Ltd, Zhengzhou, Peoples R China.
   [Yang, Shiyan] Shanghai Acad Agr Sci, Ecoenvironm Protect Inst, Shanghai, Peoples R China.
RP Yang, SY (corresponding author), Shanghai Acad Agr Sci, Ecoenvironm Protect Inst, Shanghai, Peoples R China.
EM yangsy@zju.edu.cn
CR Anwar H, 2022, PLOS ONE, V17, DOI 10.1371/journal.pone.0265808
   Aswolinskiy W, 2015, FRONT COMPUT NEUROSC, V9, DOI [10.3389/fncom.7015.00036, 10.3389/fncom.2015.00036]
   Bassett DS, 2017, NAT NEUROSCI, V20, P353, DOI 10.1038/nn.4502
   Bellec G, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-17236-y
   Brzosko Z, 2019, NEURON, V103, P563, DOI 10.1016/j.neuron.2019.05.041
   Caporale N, 2008, ANNU REV NEUROSCI, V31, P25, DOI 10.1146/annurev.neuro.31.060407.125639
   Caroni P, 2012, NAT REV NEUROSCI, V13, P478, DOI 10.1038/nrn3258
   Dayan P., 2001, THEORETICAL NEUROSCI
   Delvendah I, 2019, CURR OPIN NEUROBIOL, V54, P155, DOI 10.1016/j.conb.2018.10.003
   Dora S, 2016, NEUROCOMPUTING, V171, P1216, DOI 10.1016/j.neucom.2015.07.086
   Frémaux N, 2016, FRONT NEURAL CIRCUIT, V9, DOI 10.3389/fncir.2015.00085
   Gao HR, 2023, FRONT NEUROSCI-SWITZ, V17, DOI 10.3389/fnins.2023.1141701
   Hasegan D, 2022, FRONT COMPUT NEUROSC, V16, DOI 10.3389/fncom.2022.1017284
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2007, CEREB CORTEX, V17, P2443, DOI 10.1093/cercor/bhl152
   Ju XP, 2020, NEURAL COMPUT, V32, P182, DOI 10.1162/neco_a_01245
   Lamprecht R, 2004, NAT REV NEUROSCI, V5, P45, DOI 10.1038/nrn1301
   Lazar A, 2009, FRONT COMPUT NEUROSC, V3, DOI 10.3389/neuro.10.023.2009
   Loewenstein Y, 2011, J NEUROSCI, V31, P9481, DOI 10.1523/JNEUROSCI.6130-10.2011
   Milano G, 2020, ADV INTELL SYST-GER, V2, DOI 10.1002/aisy.202000096
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Song S, 2005, PLOS BIOL, V3, P507, DOI 10.1371/journal.pbio.0030068
   Taherkhani A, 2020, NEURAL NETWORKS, V122, P253, DOI 10.1016/j.neunet.2019.09.036
   Wang XW, 2020, NEURAL NETWORKS, V125, P258, DOI 10.1016/j.neunet.2020.02.011
   Xing F, 2019, LECT NOTES COMPUT SC, V11955, P173, DOI 10.1007/978-3-030-36718-3_15
   Xu Q, 2022, IEEE T NEUR NET LEAR, V33, P1935, DOI 10.1109/TNNLS.2021.3107449
   Yu Q, 2014, NEUROCOMPUTING, V138, P3, DOI 10.1016/j.neucom.2013.06.052
   Yuan Y, 2018, FRONT COMPUT NEUROSC, V12, DOI 10.3389/fncom.2018.00091
   Zhang TY, 2018, AAAI CONF ARTIF INTE, P6053
   Zhang TL, 2022, IEEE T NEUR NET LEAR, V33, P7621, DOI 10.1109/TNNLS.2021.3085966
   Zhang TL, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1653
   Zheng PS, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1002848
NR 32
TC 0
Z9 0
U1 1
U2 1
PD AUG 1
PY 2023
VL 17
AR 1224752
DI 10.3389/fnins.2023.1224752
UT WOS:001047899000001
DA 2023-11-16
ER

PT C
AU Sharmin, S
   Panda, P
   Sarwar, SS
   Lee, C
   Ponghiran, W
   Roy, K
AF Sharmin, Saima
   Panda, Priyadarshini
   Sarwar, Syed Shakib
   Lee, Chankyu
   Ponghiran, Wachirawit
   Roy, Kaushik
GP IEEE
TI A Comprehensive Analysis on Adversarial Robustness of Spiking Neural
   Networks
SO 2019 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUL 14-19, 2019
CL Budapest, HUNGARY
DE Adversarial attack; Spiking Neural Network; Artificial Neural Network;
   Blackbox attack; Whitebox attack
AB In this era of machine learning models, their functionality is being threatened by adversarial attacks. In the face of this struggle for making artificial neural networks robust, finding a model, resilient to these attacks, is very important. In this work, we present, for the first time, a comprehensive analysis of the behavior of more bio-plausible networks, namely Spiking Neural Network (SNN) under state-of-the-art adversarial tests. We perform a comparative study of the accuracy degradation between conventional VGG-9 Artificial Neural Network (ANN) and equivalent spiking network with CIFAR-10 dataset in both whitebox and blackbox setting for different types of single-step and multi-step FGSM (Fast Gradient Sign Method) attacks. We demonstrate that SNNs tend to show more resiliency compared to ANN under blackbox attack scenario. Additionally, we find that SNN robustness is largely dependent on the corresponding training mechanism. We observe that SNNs trained by spike-based backpropagation are more adversarially robust than the ones obtained by ANN-to-SNN conversion rules in several whitebox and blackbox scenarios. Finally, we also propose a simple, yet, effective framework for crafting adversarial attacks from SNNs. Our results suggest that attacks crafted from SNNs following our proposed method are much stronger than those crafted from ANNs.
C1 [Sharmin, Saima; Panda, Priyadarshini; Sarwar, Syed Shakib; Lee, Chankyu; Ponghiran, Wachirawit; Roy, Kaushik] Purdue Univ, Sch Elect & Comp Engn, W Lafayette, IN 47907 USA.
RP Sharmin, S (corresponding author), Purdue Univ, Sch Elect & Comp Engn, W Lafayette, IN 47907 USA.
EM sshannin@purduc.edu; pandap@purduc.edu; sarwar@purduc.edu;
   chankyu@purduc.edu; wponghir@purduc.edu; kaushik@purduc.edu
CR [Anonymous], 2018, ICLR
   [Anonymous], 2014, CORR
   [Anonymous], 2019, CORR
   Diehl P. U., 2015, NEUR NETW IJCNN 2015, P18
   Hinton G., 2012, SIGNAL PROCESSING MA
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
   Kurakin A., 2017, WORKSH TRACK ICLR 20, DOI 10.1201/9781351251389-8
   Kurakin A., 2017, ICLR
   Lee C, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00435
   Maass W., 1995, P 8 INT C NEUR INF P
   Panda P., 2018, IMPLICIT GENERATIVE
   Papernot Nicolas, 2016, CORR
   Pazske A., NIPS 2017 WORKSH AUT
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Sharif Mahmood, 2016, P 2016 ACM SIGSAC C
   Szegedy C., 2014, 2 INT C LEARN REPR I
NR 16
TC 1
Z9 1
U1 0
U2 3
PY 2019
UT WOS:000530893800051
DA 2023-11-16
ER

PT C
AU Araki, O
   Aihara, K
AF Araki, O
   Aihara, K
BE Wang, L
   Rajapakse, JC
   Fukushima, K
   Lee, SY
   Yao, X
TI Relationship between spike irregularity and neural network dynamics
SO ICONIP'02: PROCEEDINGS OF THE 9TH INTERNATIONAL CONFERENCE ON NEURAL
   INFORMATION PROCESSING: COMPUTATIONAL INTELLIGENCE FOR THE E-AGE
DT Proceedings Paper
CT 9th International Conference on Neural Information Processing
CY NOV 18-22, 2002
CL SINGAPORE, SINGAPORE
ID VARIABILITY; PATTERNS; NEURONS; MODEL
AB It is well known that interspike intervals (ISI) in the cortex have high values of the coefficient of variation (Cv). The purpose of this study is to clarify the relation between spike irregularity and network dynamics. We observe the irregularity of interspike intervals when we change the values of two parameters T and T-ref which strongly affect the network dynamics in a spiking neural network model. The results show that spike irregularity depends on the neural network dynamics as follows: When T (T-ref) is larger (smaller), the irregularity increases and the stability of firing rates decreases. On the other hand, these changes are unrelated to the largest lyapunov exponent, an indicator of instability of an orbit in the state space.
C1 Sci Univ Tokyo, Dept Appl Phys, Tokyo 1628601, Japan.
RP Araki, O (corresponding author), Sci Univ Tokyo, Dept Appl Phys, Tokyo 1628601, Japan.
CR ABELES M, 1991, CORTICOMICS NEURAL C
   Araki O, 2001, NEURAL COMPUT, V13, P2799, DOI 10.1162/089976601317098538
   Feng JF, 1998, J PHYS A-MATH GEN, V31, P1239, DOI 10.1088/0305-4470/31/4/013
   Holt GR, 1996, J NEUROPHYSIOL, V75, P1806, DOI 10.1152/jn.1996.75.5.1806
   NODA H, 1970, BRAIN RES, V18, P513, DOI 10.1016/0006-8993(70)90134-4
   Shadlen Michael N., 1994, Current Opinion in Neurobiology, V4, P569, DOI 10.1016/0959-4388(94)90059-0
   SOFTKY WR, 1993, J NEUROSCI, V13, P334
   Stevens CF, 1998, NAT NEUROSCI, V1, P210, DOI 10.1038/659
   Troyer TW, 1997, NEURAL COMPUT, V9, P971, DOI 10.1162/neco.1997.9.5.971
   USHER M, 1994, NEURAL COMPUT, V6, P795, DOI 10.1162/neco.1994.6.5.795
NR 10
TC 0
Z9 0
U1 0
U2 0
PY 2002
BP 566
EP 570
UT WOS:000182832400117
DA 2023-11-16
ER

PT C
AU Schmitt, FJ
   Nawrot, MP
AF Schmitt, Felix Johannes
   Nawrot, Martin Paul
GP ACM
TI Evaluating parameter tuning and real-time closed-loop simulation of
   large scale spiking networks before mapping to neuromorphic hardware:
   Comparing GeNN and NEST
SO PROCEEDINGS OF THE 2022 ANNUAL NEURO-INSPIRED COMPUTATIONAL ELEMENTS
   CONFERENCE (NICE 2022)
DT Proceedings Paper
CT Annual Neuro-Inspired Computational Elements Conference (NICE)
CY MAR 28-APR 01, 2022
CL ELECTR NETWORK
DE spiking neural network; cortical attractor network; real-time
   simulation; GPU based simulation
C1 [Schmitt, Felix Johannes; Nawrot, Martin Paul] Univ Cologne, Inst Zool, Cologne, Germany.
RP Schmitt, FJ (corresponding author), Univ Cologne, Inst Zool, Cologne, Germany.
EM felix.schmitt@uni-koeln.de; martin.nawrot@uni-koeln.de
CR Gewaltig M-O., 2007, SCHOLARPEDIA, V2, P1430, DOI [10.4249/scholarpedia.1430, DOI 10.4249/SCHOLARPEDIA.1430]
   Knight JC, 2021, NAT COMPUT SCI, V1, P136, DOI 10.1038/s43588-020-00022-7
   Knight JC, 2021, FRONT NEUROINFORM, V15, DOI 10.3389/fninf.2021.659005
   Rost T, 2018, BIOL CYBERN, V112, P81, DOI 10.1007/s00422-017-0737-7
   Rostami V, 2022, bioRxiv, DOI [10.1101/2020.02.27.968339, 10.1101/2020.02.27.968339, DOI 10.1101/2020.02.27.968339]
   Yavuz E, 2016, SCI REP-UK, V6, DOI 10.1038/srep18854
NR 6
TC 0
Z9 0
U1 0
U2 0
PY 2022
BP 29
EP 31
DI 10.1145/3517343.3517350
UT WOS:000934089300006
DA 2023-11-16
ER

PT C
AU Azimirad, V
   Ramezanlou, MT
   Shahabi, P
AF Azimirad, Vahid
   Ramezanlou, Mohammad Tayefe
   Shahabi, Parviz
BA Ardekany, AN
BF Ardekany, AN
TI Learning of 2 DOF robotic arm using integrated architecture of neural
   network and Spike Timing Dependent Plasticity
SO 2018 6TH RSI INTERNATIONAL CONFERENCE ON ROBOTICS AND MECHATRONICS
   (ICROM 2018)
SE RSI International Conference on Robotics and Mechatronics ICRoM
DT Proceedings Paper
CT 6th RSI International Conference on Robotics and Mechatronics (IcRoM)
CY OCT 23-25, 2018
CL Tehran, IRAN
DE unsupervised learning; Spike timing-dependent plasticity; spiking neural
   network; STDP; robotic arm
ID MODEL
AB In this paper, an integrated architecture of spiking neural network is used for learning of 2 DOF robotic arm. Brain architecture is consisting of 6 different areas that have different tasks. Two sensors are used to detect the target position and send the signals to sensory neurons. As an integrated architecture, all of the sensory neurons are connected to all motor neurons at the beginning of the process. The neural network is trained to learn a specific task using spike timing-dependent plasticity. Simultaneous sensing of target and movement of robot toward target result in unsupervised learning which is useful for learning of robots in unknown environments. To remove the effects of random inputs, the experiment is repeated six times. Through the learning process, the synaptic weights are changed. The connection between left sensory neurons and Flexor motor neurons also the connection between right sensory neurons and Extensor motor neurons are increased while other connections are weakened. The results show that the spiking neural network is effective in controlling the robot's motion.
C1 [Azimirad, Vahid; Ramezanlou, Mohammad Tayefe] Univ Tabriz, Sch Engn Emerging Technol, Dept Mechatron, Tabriz, Iran.
   [Shahabi, Parviz] Tabriz Univ Med Sci, Neurosci Res Ctr, Tabriz 5166616471, Iran.
RP Azimirad, V (corresponding author), Univ Tabriz, Sch Engn Emerging Technol, Dept Mechatron, Tabriz, Iran.
EM azimirad@tabrizu.ac.ir; mohammad_tayeferamezanloo95@ms.tabrizu;
   Parvizshahabi@gmail.com
CR Azimirad V, 2017, 2017 IEEE 4TH INTERNATIONAL CONFERENCE ON KNOWLEDGE-BASED ENGINEERING AND INNOVATION (KBEI), P428, DOI 10.1109/KBEI.2017.8325015
   Bouganis A., 2010, P 2010 INT JOINT C N, P1, DOI DOI 10.1109/IJCNN.2010.5596525
   Carrillo RR, 2008, BIOSYSTEMS, V94, P18, DOI 10.1016/j.biosystems.2008.05.008
   Chadderdon GL, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0047251
   Dura-Bernal S, 2015, FRONT NEUROROBOTICS, V9, DOI 10.3389/fnbot.2015.00013
   Dura-Bernal S, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00028
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2007, CEREB CORTEX, V17, P2443, DOI 10.1093/cercor/bhl152
   Korayem MH, 2012, ROBOTICA, V30, P53, DOI 10.1017/S0263574711000336
   Korayem MH, 2009, INT J ADV MANUF TECH, V44, P725, DOI 10.1007/s00170-008-1862-1
   Luque NR, 2011, INT J NEURAL SYST, V21, P385, DOI 10.1142/S0129065711002900
   Neymotin SA, 2013, NEURAL COMPUT, V25, P3263, DOI 10.1162/NECO_a_00521
   Singh N., 2017, NEURON, V1, P65624
   Spuler M., 2015, P INT JOINT C NEURAL, V2015, DOI [10.1109/IJCNN.2015.7280521, DOI 10.1109/IJCNN.2015.7280521]
   Wang XQ, 2008, NEUROCOMPUTING, V71, P655, DOI 10.1016/j.neucom.2007.08.025
   Wang XQ, 2008, ICNC 2008: FOURTH INTERNATIONAL CONFERENCE ON NATURAL COMPUTATION, VOL 4, PROCEEDINGS, P125, DOI 10.1109/ICNC.2008.718
NR 16
TC 2
Z9 2
U1 0
U2 3
PY 2018
BP 126
EP 129
UT WOS:000465373900022
DA 2023-11-16
ER

PT C
AU Stitt, JP
   Gaumond, RP
   Frazier, JL
   Hanson, FE
AF Stitt, JP
   Gaumond, RP
   Frazier, JL
   Hanson, FE
GP IEEE
   IEEE
TI A comparison of neural spike classification techniques.
SO PROCEEDINGS OF THE 19TH ANNUAL INTERNATIONAL CONFERENCE OF THE IEEE
   ENGINEERING IN MEDICINE AND BIOLOGY SOCIETY, VOL 19, PTS 1-6:
   MAGNIFICENT MILESTONES AND EMERGING OPPORTUNITIES IN MEDICAL ENGINEERING
SE PROCEEDINGS OF ANNUAL INTERNATIONAL CONFERENCE OF THE IEEE ENGINEERING
   IN MEDICINE AND BIOLOGY SOCIETY
DT Proceedings Paper
CT International Conference of the IEEE
   Engineering-in-Medicine-and-Biology-Society
CY OCT 30-NOV 02, 1997
CL CHICAGO, IL
AB This paper presents an Artificial Neural Network (ANN) capable of sorting neural spikes contained in a single-channel multiunit recording. The ANN performs very well when compared with Template Matching and Principal Components, two of the conventional optimal spike classification methods that have been widely used for sorting action potentials.
C1 Penn State Univ, University Pk, PA 16802 USA.
RP Stitt, JP (corresponding author), Penn State Univ, University Pk, PA 16802 USA.
CR FRAZIER JLL, 1986, ELECTROPHYSIOLOGICAL
   WHEELER BC, 1996, MULTIPLE UNIT NEURAL
   Zurada J., 1992, INTRO ARTIFICIAL NEU
NR 3
TC 0
Z9 0
U1 0
U2 0
PY 1997
VL 19
BP 1092
EP 1094
PN 1-6
UT WOS:000080103400313
DA 2023-11-16
ER

PT C
AU Rueckauer, B
   Liu, SC
AF Rueckauer, Bodo
   Liu, Shih-Chii
GP IEEE
TI Conversion of analog to spiking neural networks using sparse temporal
   coding
SO 2018 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS (ISCAS)
SE IEEE International Symposium on Circuits and Systems
DT Proceedings Paper
CT IEEE International Symposium on Circuits and Systems (ISCAS)
CY MAY 27-30, 2018
CL Florence, ITALY
AB The activations of an analog neural network (ANN) are usually treated as representing an analog firing rate. When mapping the ANN onto an equivalent spiking neural network (SNN), this rate-based conversion can lead to undesired increases in computation cost and memory access, if firing rates are high. This work presents an efficient temporal encoding scheme, where the analog activation of a neuron in the ANN is treated as the instantaneous firing rate given by the time-to-first-spike (TTFS) in the converted SNN. By making use of temporal information carried by a single spike, we show a new spiking network model that uses 7-10X fewer operations than the original rate-based analog model on the MNIST handwritten dataset, with an accuracy loss of <1%.
C1 [Rueckauer, Bodo; Liu, Shih-Chii] Univ Zurich, Inst Neuroinformat, Zurich, Switzerland.
   [Rueckauer, Bodo; Liu, Shih-Chii] Swiss Fed Inst Technol, Zurich, Switzerland.
RP Rueckauer, B (corresponding author), Univ Zurich, Inst Neuroinformat, Zurich, Switzerland.; Rueckauer, B (corresponding author), Swiss Fed Inst Technol, Zurich, Switzerland.
EM rbodo@ini.uzh.ch; shih@ini.uzh.ch
CR Aimar A., 2017, ARXIV170601406
   [Anonymous], ARXIV160902053
   Chen YH, 2016, CONF PROC INT SYMP C, P367, DOI 10.1109/ISCA.2016.40
   Diehl P. U., 2015, P INT JOINT C NEUR N, V2015
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   Horowitz M, 2014, ISSCC DIG TECH PAP I, V57, P10, DOI 10.1109/ISSCC.2014.6757323
   Hubara I., 2016, ADV NEURAL INFORM PR, P4107
   Hunsberger E., 2016, ARXIV, V1, P6566, DOI [10.13140/RG.2.2.10967.06566, DOI 10.13140/RG.2.2.10967.06566]
   Kheradpisheh S. R., 2016, STDP BASED SPIKING D
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lee J. H., 2016, TRAINING DEEP SPIKIN, V10, P1, DOI DOI 10.HTTP://DX.D0I.0RG/10.3389/FNINS.2016.00508
   Lichtsteiner Patrick, 2008, IEEE Journal of Solid-State Circuits, V43, P566, DOI 10.1109/JSSC.2007.914337
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Mostafa H., 2016, ARXIV160608165
   Mostafa H., 2017, ISCAS
   Orchard G, 2015, IEEE T PATTERN ANAL, V37, P2028, DOI 10.1109/TPAMI.2015.2392947
   Rueckauer B., 2016, ARXIV161204052
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   Van Rullen R, 2001, NEURAL COMPUT, V13, P1255, DOI 10.1162/08997660152002852
   Zhao B, 2015, IEEE T NEUR NET LEAR, V26, P1963, DOI 10.1109/TNNLS.2014.2362542
NR 24
TC 99
Z9 99
U1 2
U2 5
PY 2018
DI 10.1109/ISCAS.2018.8351295
UT WOS:000451218701197
DA 2023-11-16
ER

PT J
AU Luo, XL
   Qu, H
   Zhang, Y
   Chen, Y
AF Luo, Xiaoling
   Qu, Hong
   Zhang, Yun
   Chen, Yi
TI First Error-Based Supervised Learning Algorithm for Spiking Neural
   Networks
SO FRONTIERS IN NEUROSCIENCE
DT Article
DE spike neural networks; supervised learning; synaptic plasticity; first
   error learning; speech recognition
ID PERCEPTRON; PRECISION; CORTEX; RESUME; TRAINS
AB Neural circuits respond to multiple sensory stimuli by firing precisely timed spikes. Inspired by this phenomenon, the spike timing-based spiking neural networks (SNNs) are proposed to process and memorize the spatiotemporal spike patterns. However, the response speed and accuracy of the existing learning algorithms of SNNs are still lacking compared to the human brain. To further improve the performance of learning precisely timed spikes, we propose a new weight updating mechanism which always adjusts the synaptic weights at the first wrong output spike time. The proposed learning algorithm can accurately adjust the synaptic weights that contribute to the membrane potential of desired and non-desired firing time. Experimental results demonstrate that the proposed algorithm shows higher accuracy, better robustness, and less computational resources compared with the remote supervised method (ReSuMe) and the spike pattern association neuron (SPAN), which are classic sequence learning algorithms. In addition, the SNN-based computational model equipped with the proposed learning method achieves better recognition results in speech recognition task compared with other bio-inspired baseline systems.
C1 [Luo, Xiaoling; Qu, Hong; Zhang, Yun; Chen, Yi] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu, Sichuan, Peoples R China.
RP Qu, H (corresponding author), Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu, Sichuan, Peoples R China.
EM hongqu@uestc.edu.cn
CR Abdollahi M, 2011, BIOMED CIRC SYST C, P269, DOI 10.1109/BioCAS.2011.6107779
   [Anonymous], 2018, P IEEE INT JOINT C N
   Bair W, 1996, NEURAL COMPUT, V8, P1185, DOI 10.1162/neco.1996.8.6.1185
   Bastos AM, 2012, NEURON, V76, P695, DOI 10.1016/j.neuron.2012.10.038
   Berry MJ, 1998, ADV NEUR IN, V10, P110
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Cariani PA, 2004, IEEE T NEURAL NETWOR, V15, P1100, DOI 10.1109/TNN.2004.833305
   Dominguez-Morales JP, 2017, NEUROCOMPUTING, V237, P418, DOI 10.1016/j.neucom.2016.12.046
   Florian RV, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0040233
   Gautrais J, 1998, BIOSYSTEMS, V48, P57, DOI 10.1016/S0303-2647(98)00050-1
   Gerstner W., 2002, SPIKING NEURON MODEL
   Gerstner W, 2002, SPIKING NEURON MODEL, DOI [10.1017/CBO9780511815706, DOI 10.1017/CBO9780511815706]
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Gollisch T, 2008, SCIENCE, V319, P1108, DOI 10.1126/science.1149639
   Gütig R, 2016, SCIENCE, V351, DOI 10.1126/science.aab4113
   Gütig R, 2009, PLOS BIOL, V7, DOI 10.1371/journal.pbio.1000141
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   Ito M, 2000, BRAIN RES, V886, P237, DOI 10.1016/S0006-8993(00)03142-5
   Keller GB, 2012, NEURON, V74, P809, DOI 10.1016/j.neuron.2012.03.040
   Keller GB, 2009, NATURE, V457, P187, DOI 10.1038/nature07467
   Leonard R. G., 1993, TIDIGITS SPEECH CORP
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Masquelier T, 2009, NEURAL COMPUT, V21, P1259, DOI 10.1162/neco.2008.06-08-804
   Memmesheimer RM, 2014, NEURON, V82, P925, DOI 10.1016/j.neuron.2014.03.026
   Mohemmed A, 2013, NEUROCOMPUTING, V107, P3, DOI 10.1016/j.neucom.2012.08.034
   Mohemmed A, 2012, INT J NEURAL SYST, V22, DOI 10.1142/S0129065712500128
   Neil D, 2016, IEEE INT SYMP CIRC S, P2282, DOI 10.1109/ISCAS.2016.7539039
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Qu H, 2015, NEUROCOMPUTING, V151, P310, DOI 10.1016/j.neucom.2014.09.034
   Reinagel P, 2000, J NEUROSCI, V20, P5392, DOI 10.1523/JNEUROSCI.20-14-05392.2000
   Schreiber S, 2003, NEUROCOMPUTING, V52-4, P925, DOI 10.1016/S0925-2312(02)00838-X
   Taherkhani A., 2015, NEURAL NETWORKS IJCN, P1
   Taherkhani A, 2018, IEEE T NEUR NET LEAR, V29, P5394, DOI 10.1109/TNNLS.2018.2797801
   Taherkhani A, 2015, IEEE T NEUR NET LEAR, V26, P3137, DOI 10.1109/TNNLS.2015.2404938
   Tavanaei A, 2017, LECT NOTES COMPUT SC, V10639, P899, DOI 10.1007/978-3-319-70136-3_95
   Tavanaei A, 2017, NEUROCOMPUTING, V240, P191, DOI 10.1016/j.neucom.2017.01.088
   Thach WT, 1996, BEHAV BRAIN SCI, V19, P501, DOI 10.1017/S0140525X00082017
   Uzzell VJ, 2004, J NEUROPHYSIOL, V92, P780, DOI 10.1152/jn.01171.2003
   van Rossum MCW, 2001, NEURAL COMPUT, V13, P751, DOI 10.1162/089976601300014321
   Victor JD, 1997, NETWORK-COMP NEURAL, V8, P127, DOI 10.1088/0954-898X/8/2/003
   Nguyen VA, 2012, IEEE T NEUR NET LEAR, V23, P971, DOI 10.1109/TNNLS.2012.2191419
   Wang WW, 2012, IEEE T NEUR NET LEAR, V23, P1574, DOI 10.1109/TNNLS.2012.2208477
   WIDROW B, 1990, P IEEE, V78, P1415, DOI 10.1109/5.58323
   Wu JC, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351221
   Wu JB, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00836
   Wu JY, 2019, 2019 5TH INTERNATIONAL CONFERENCE ON EVENT-BASED CONTROL, COMMUNICATION, AND SIGNAL PROCESSING (EBCCSP), DOI 10.1109/ebccsp.2019.8836892
   Xie XR, 2017, NEUROCOMPUTING, V241, P152, DOI 10.1016/j.neucom.2017.01.086
   Xie XR, 2014, LECT NOTES COMPUT SC, V8834, P366, DOI 10.1007/978-3-319-12637-1_46
   Xu Y, 2013, NEURAL COMPUT, V25, P1472, DOI 10.1162/NECO_a_00450
   Xu Y, 2013, NEURAL NETWORKS, V43, P99, DOI 10.1016/j.neunet.2013.02.003
   Yu Q, 2019, IEEE T CYBERNETICS, V49, P2178, DOI 10.1109/TCYB.2018.2821692
   Yu QF, 2013, PLOS ONE, V8, DOI [10.1371/journal.pone.0059627, 10.1371/journal.pone.0078318]
   Zhang ML, 2019, AAAI CONF ARTIF INTE, P1327
   Zhang ML, 2019, IEEE T NEUR NET LEAR, V30, P123, DOI 10.1109/TNNLS.2018.2833077
   Zhang ML, 2018, IEEE T COGN DEV SYST, V10, P151, DOI 10.1109/TCDS.2017.2651943
   Zhang Y, 2015, IEEE T NEUR NET LEAR, V26, P2635, DOI 10.1109/TNNLS.2015.2388544
NR 58
TC 10
Z9 10
U1 0
U2 6
PD JUN 6
PY 2019
VL 13
AR 559
DI 10.3389/fnins.2019.00559
UT WOS:000471265700001
DA 2023-11-16
ER

PT C
AU Obo, T
   Kubota, N
AF Obo, Takenori
   Kubota, Naoyuki
GP IEEE
TI Structured Learning in Fuzzy Spiking Neural Networks for Human State
   Estimation
SO 2014 WORLD AUTOMATION CONGRESS (WAC): EMERGING TECHNOLOGIES FOR A NEW
   PARADIGM IN SYSTEM OF SYSTEMS ENGINEERING
SE World Automation Congress
DT Proceedings Paper
CT World Automation Congress (WAC) on Emerging Technologies for a New
   Paradigm in System of Systems Engineering
CY AUG 03-07, 2014
CL Waikoloa Hilton, HI
DE structured learning; spiking neural networks; human state estimation;
   fuzzy theory; health care system
AB In this paper, we focus on the monitoring of sleep using an optical oscillosensor and a pneumatic sensor for the health care of the elderly people. The system composed of these sensors applies thresholds for the estimation of human behaviors. We can use membership functions to extract the feature of sensor data, and spiking neural networks to estimate the human state in the bed. However, it is difficult to design the membership function in advance because of environmental condition and personal difference. Therefore, we propose a structured learning in fuzzy spiking neural networks to enable optimization of the membership functions in the learning process. We discuss the effectiveness of the proposed method through comparative experiments.
C1 [Obo, Takenori; Kubota, Naoyuki] Tokyo Metropolitan Univ, Dept Syst Design, Tokyo 158, Japan.
RP Obo, T (corresponding author), Tokyo Metropolitan Univ, Dept Syst Design, Tokyo 158, Japan.
EM takebo@tmu.ac.jp; kubota@tmu.ac.jp
CR Anderson J.A., 1988, NEUROCOMPUTING
   [Anonymous], 1987, COMMUNITY REENTRY HE
   [Anonymous], 1991, FDN GENETIC ALGORITH, DOI DOI 10.1016/B978-0-08-050684-5.50009-4
   Aztiria A, 2010, ARTIF INTELL REV, V34, P35, DOI 10.1007/s10462-010-9160-3
   Frias-Martinez E., 2005, EXPERT SYSTEMS APPL, V29, P104
   Gerstner W, 1998, PULSED NEURAL NETWORKS, P3
   Jinwoo Kim, 2004, International Journal of Mobile Communications, V2, P1, DOI 10.1504/IJMC.2004.004484
   Kamozaki Y., 2005, P 2005 IEEE INT ULTR
   Kubota N., 2008, P WORLD AUT C 2008
   Kubota N, 2010, J ADV COMPUT INTELL, V14, P309, DOI 10.20965/jaciii.2010.p0309
   Kubota N, 2012, MECATRONICS REM 2012, P464, DOI 10.1109/MECATRONICS.2012.6451049
   Maass W., 1999, PULSED NEURAL NETWOR
   Obo T., 2012, P WORLD AUT C 2012
   Obo T., 2010, P WORLD AUT C 2010
   Obo T., 2011, P IEEE S SER COMP IN
   Schwefel HP., 1981, NUMERICAL OPTIMIZATI
NR 16
TC 0
Z9 0
U1 0
U2 0
PY 2014
UT WOS:000349117400132
DA 2023-11-16
ER

PT J
AU Thibeault, CM
   Minkovich, K
   O'Brien, MJ
   Harris, FC
   Srinivasa, N
AF Thibeault, Corey M.
   Minkovich, Kirill
   O'Brien, Michael J.
   Harris, Frederick C., Jr.
   Srinivasa, Narayan
TI Efficiently passing messages in distributed spiking neural network
   simulation
SO FRONTIERS IN COMPUTATIONAL NEUROSCIENCE
DT Article
DE parallel spiking neuron simulation; neural networks; parallel
   simulation; distributed computing; distributed message passing
ID CONNECTIVITY
AB Efficiently passing spiking messages in a neural model is an important aspect of high-performance simulation. As the scale of networks has increased so has the size of the computing systems required to simulate them. In addition, the information exchange of these resources has become more of an impediment to performance. In this paper we explore spike message passing using different mechanisms provided by the Message Passing Interface (MPI). A specific implementation, MVAPICH, designed for high-performance clusters with Infiniband hardware is employed. The focus is on providing information about these mechanisms for users of commodity high-performance spiking simulators. In addition, a novel hybrid method for spike exchange was implemented and benchmarked.
C1 [Thibeault, Corey M.; Minkovich, Kirill; O'Brien, Michael J.; Srinivasa, Narayan] HRL Labs LLC, Ctr Neural & Emergent Syst, Informat & Syst Sci Lab, Malibu, CA 90265 USA.
   [Thibeault, Corey M.] Univ Nevada, Dept Elect & Biomed Engn, Reno, NV 89557 USA.
   [Thibeault, Corey M.; Harris, Frederick C., Jr.] Univ Nevada, Dept Comp Sci & Engn, Reno, NV 89557 USA.
   [O'Brien, Michael J.] Univ Calif Los Angeles, Dept Math, Los Angeles, CA 90024 USA.
RP Thibeault, CM (corresponding author), HRL Labs LLC, Ctr Neural & Emergent Syst, Informat & Syst Sci Lab, 3011 Mailbu Canyon Dr, Malibu, CA 90265 USA.
EM cmthibeault@hrl.com
CR Boahen KA, 2000, IEEE T CIRCUITS-II, V47, P416, DOI 10.1109/82.842110
   Eppler JM, 2007, LECT NOTES COMPUT SC, V4757, P391
   Furber SB, 2013, IEEE T COMPUT, V62, P2454, DOI 10.1109/TC.2012.142
   Harris Jr F. C., 2011, ISCAS 3 INT C BIOINF
   Hines M, 2011, FRONT COMPUT NEUROSC, V5, DOI 10.3389/fncom.2011.00049
   Humphries MD, 2007, PHILOS T R SOC B, V362, P1627, DOI 10.1098/rstb.2007.2057
   Khan MM, 2008, IEEE IJCNN, P2849, DOI 10.1109/IJCNN.2008.4634199
   Merolla P., 2011, IEEE CUST INT CIRC C, P1, DOI DOI 10.1109/CICC.2011.6055294
   Migliore M, 2006, J COMPUT NEUROSCI, V21, P119, DOI 10.1007/s10827-006-7949-5
   Morrison A, 2005, NEURAL COMPUT, V17, P1776, DOI 10.1162/0899766054026648
   Navaridas J, 2009, ICS'09: PROCEEDINGS OF THE 2009 ACM SIGARCH INTERNATIONAL CONFERENCE ON SUPERCOMPUTING, P286, DOI 10.1145/1542275.1542317
   Pecevski Dejan, 2009, Front Neuroinform, V3, P11
   Srinivasa N, 2012, IEEE PULSE, V3, P51, DOI 10.1109/MPUL.2011.2175639
   Sur S., 2005, P INT C HIGH PERF CO, DOI [10.1007/11602569_19, DOI 10.1007/11602569_]
   Thibeault C. M., 2012, UMI PUBLICATION, VAAT3550403
   Wilson E.C., 2001, P 10 SIAM C PAR PROC, P1
NR 16
TC 4
Z9 6
U1 2
U2 6
PD JUN 10
PY 2013
VL 7
AR 77
DI 10.3389/fncom.2013.00077
UT WOS:000320850900001
DA 2023-11-16
ER

PT C
AU Liang, MX
   Zhang, JL
   Chen, H
AF Liang, Mingxuan
   Zhang, Jilin
   Chen, Hong
GP IEEE
TI A 1.13μJ/classification Spiking Neural Network Accelerator with a
   Single-spike Neuron Model and Sparse Weights
SO 2021 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS (ISCAS)
SE IEEE International Symposium on Circuits and Systems
DT Proceedings Paper
CT IEEE International Symposium on Circuits and Systems (IEEE ISCAS)
CY MAY 22-28, 2021
CL Daegu, SOUTH KOREA
DE SNN; FPGA; neuromorphic computation; single-spike; weight sparsity
AB In this paper, we implement a single-spike spiking neural network (SNN) accelerator on field programmable gate array (FPGA) with 512 hidden neurons. A single-spike spiking integrate-and-fire neural model is adopted, which emits only one spike during classification, and adder instead of multiplier is used to integrate input spikes in its implementation, consuming fewer power and area compared with other models. Different level sparse connection is adopted to reduce up to 75% weight memory with 0.016% overhead for storing connections. The SNN accelerator is verified by MNIST handwriting dataset with Xilinx VC707 FPGA. Results show that the single-spike SNN accelerator reached 96% accuracy, 2.8us classification latency and 1.13 mu J/classification energy efficiency with MNIST dataset.
C1 [Liang, Mingxuan; Zhang, Jilin; Chen, Hong] Tsinghua Univ, Inst Microelect, Tsinghua Natl Lab Informat Sci & Technol, Beijing, Peoples R China.
RP Chen, H (corresponding author), Tsinghua Univ, Inst Microelect, Tsinghua Natl Lab Informat Sci & Technol, Beijing, Peoples R China.
EM hongchen@tsinghua.edu.cn
CR Asgari H., 2019, DIGITAL MULTIPLIER L
   Chen GK, 2019, IEEE J SOLID-ST CIRC, V54, P992, DOI 10.1109/JSSC.2018.2884901
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Jolivet R, 2003, LECT NOTES COMPUT SC, V2714, P846
   Kheradpisheh SR, 2020, INT J NEURAL SYST, V30, DOI 10.1142/S0129065720500276
   Kiselev I, 2016, IEEE INT SYMP CIRC S, P2495, DOI 10.1109/ISCAS.2016.7539099
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Maguire LP, 2007, NEUROCOMPUTING, V71, P13, DOI 10.1016/j.neucom.2006.11.029
   Nambiar V. P, 2020, SCALABLE BLOCK BASED, P1, DOI [10.1109/iscas45731.2020.9180629, DOI 10.1109/ISCAS45731.2020.9180629]
   Stromatias E, 2015, IEEE IJCNN
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   Zhang JL, 2019, IEEE ASIAN SOLID STA, P213, DOI [10.1109/A-SSCC47793.2019.9056903, 10.1109/a-sscc47793.2019.9056903]
NR 12
TC 7
Z9 7
U1 3
U2 16
PY 2021
DI 10.1109/ISCAS51556.2021.9401607
UT WOS:000706507900130
DA 2023-11-16
ER

PT C
AU Jia, SC
   Zuo, RC
   Zhang, TL
   Liu, HX
   Xu, B
AF Jia, Shuncheng
   Zuo, Ruichen
   Zhang, Tielin
   Liu, Hongxing
   Xu, Bo
GP IEEE
TI MOTIF-TOPOLOGY AND REWARD-LEARNING IMPROVED SPIKING NEURAL NETWORK FOR
   EFFICIENT MULTI-SENSORY INTEGRATION
SO 2022 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)
SE International Conference on Acoustics Speech and Signal Processing
   ICASSP
DT Proceedings Paper
CT 47th IEEE International Conference on Acoustics, Speech and Signal
   Processing (ICASSP)
CY MAY 22-27, 2022
CL Singapore, SINGAPORE
DE Spiking Neural Network; Multi-sensory Integration; Motif Topology;
   Reward Learning
ID BACKPROPAGATION
AB Network architectures and learning principles are key in forming complex functions in artificial neural networks (ANNs) and spiking neural networks (SNNs). SNNs are considered the new-generation artificial networks by incorporating more biological features than ANNs, including dynamic spiking neurons, functionally specified architectures, and efficient learning paradigms. In this paper, we propose a Motiftopology and Reward-learning improved SNN (MR-SNN) for efficient multi-sensory integration. MR-SNN contains 13 types of 3-node Motif topologies which are first extracted from independent single-sensory learning paradigms and then integrated for multi-sensory classification. The experimental results showed higher accuracy and stronger robustness of the proposed MR-SNN than other conventional SNNs without using Motifs. Furthermore, the proposed reward learning paradigm was biologically plausible and can better explain the cognitive McGurk effect caused by incongruent visual and auditory sensory signals.
C1 [Jia, Shuncheng; Zuo, Ruichen; Zhang, Tielin; Liu, Hongxing; Xu, Bo] Chinese Acad Sci, Inst Automat, Beijing, Peoples R China.
   [Jia, Shuncheng; Zhang, Tielin; Xu, Bo] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing, Peoples R China.
   [Xu, Bo] Chinese Acad Sci, Ctr Excellence Brain Sci & Intelligence Technol, Beijing, Peoples R China.
   [Zuo, Ruichen] Beijing Inst Technol, Sch Informat & Elect, Beijing, Peoples R China.
RP Zhang, TL; Xu, B (corresponding author), Chinese Acad Sci, Inst Automat, Beijing, Peoples R China.; Zhang, TL; Xu, B (corresponding author), Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing, Peoples R China.; Xu, B (corresponding author), Chinese Acad Sci, Ctr Excellence Brain Sci & Intelligence Technol, Beijing, Peoples R China.
EM tielin.zhang@ia.ac.cn; xubo@ia.ac.cn
CR Abraham WC, 1996, TRENDS NEUROSCI, V19, P126, DOI 10.1016/S0166-2236(96)80018-X
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Cheng ZX, 2016, CELL REP, V15, P1013, DOI 10.1016/j.celrep.2016.03.089
   Diehl P.U., 2015, 2015 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2015.7280696
   Ernst MO, 2002, NATURE, V415, P429, DOI 10.1038/415429a
   Gary Leonard R., 1993, TIDIGITS LDC93S10
   Hassabis D, 2017, NEURON, V95, P245, DOI 10.1016/j.neuron.2017.06.011
   Hromadka T, 2008, PLOS BIOL, V6, P124, DOI 10.1371/journal.pbio.0060016
   LeCun Y., 1998, MNIST DATABASE HANDW
   Lee Jun Haeng, 2016, FRONTIERS NEUROSCIEN, V10
   Logan Beth, 2000, ISMIR, V270, P1
   Luo LQ, 2021, SCIENCE, V373, P1103, DOI 10.1126/science.abg7285
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Platt JC, 1999, ADVANCES IN KERNEL METHODS, P185
   Rueckert E, 2016, SCI REP-UK, V6, DOI 10.1038/srep21142
   Shan F, 2018, IOP CONF SER-MAT SCI, V322, DOI 10.1088/1757-899X/322/2/022018
   Shen K, 2012, J NEUROSCI, V32, P17465, DOI 10.1523/JNEUROSCI.2709-12.2012
   Soltani A, 2010, NAT NEUROSCI, V13, P112, DOI 10.1038/nn.2450
   Stein B E, 1989, J Cogn Neurosci, V1, P12, DOI 10.1162/jocn.1989.1.1.12
   Tiippana K, 2014, FRONT PSYCHOL, V5, DOI 10.3389/fpsyg.2014.00725
   Vinje W. E., 2000, Science, V287, P1273, DOI 10.1126/science.287.5456.1273
   WERBOS PJ, 1990, P IEEE, V78, P1550, DOI 10.1109/5.58337
   Zhang TY, 2018, AAAI CONF ARTIF INTE, P6053
   Zhang TL, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1653
   Zhang T, 2021, INT J GEOGR INF SCI, V35, P2216, DOI 10.1080/13658816.2020.1869746
NR 25
TC 3
Z9 3
U1 1
U2 3
PY 2022
BP 8917
EP 8921
DI 10.1109/ICASSP43922.2022.9746157
UT WOS:000864187909046
DA 2023-11-16
ER

PT J
AU Chien, YC
   Xiang, H
   Shi, YF
   Duong, NT
   Li, SF
   Ang, KW
AF Chien, Yu-Chieh
   Xiang, Heng
   Shi, Yufei
   Duong, Ngoc Thanh
   Li, Sifan
   Ang, Kah-Wee
TI A MoS<sub>2</sub> Hafnium Oxide Based Ferroelectric Encoder for
   Temporal-Efficient Spiking Neural Network
SO ADVANCED MATERIALS
DT Article
DE 2D materials; ferroelectric encoder; hafnium zirconium oxide; spiking
   neural networks; time-to-first-spike encoding scheme
ID NONVOLATILE MEMORY; POLARIZATION; FIELD; MODEL
AB Spiking neural network (SNN), where the information is evaluated recurrently through spikes, has manifested significant promises to minimize the energy expenditure in data-intensive machine learning and artificial intelligence. Among these applications, the artificial neural encoders are essential to convert the external stimuli to a spiking format that can be subsequently fed to the neural network. Here, a molybdenum disulfide (MoS2) hafnium oxide-based ferroelectric encoder is demonstrated for temporal-efficient information processing in SNN. The fast domain switching attribute associated with the polycrystalline nature of hafnium oxide-based ferroelectric material is exploited for spike encoding, rendering it suitable for realizing biomimetic encoders. Accordingly, a high-performance ferroelectric encoder is achieved, featuring a superior switching efficiency, negligible charge trapping effect, and robust ferroelectric response, which successfully enable a broad dynamic range. Furthermore, an SNN is simulated to verify the precision of the encoded information, in which an average inference accuracy of 95.14% can be achieved, using the Modified National Insitute of Standards and Technology (MNIST) dataset for digit classification. Moreover, this ferroelectric encoder manifests prominent resilience against noise injection with an overall prediction accuracy of 94.73% under various Gaussian noise levels, showing practical promises to reduce the computational load for the neural network.
C1 [Chien, Yu-Chieh; Xiang, Heng; Shi, Yufei; Duong, Ngoc Thanh; Li, Sifan; Ang, Kah-Wee] Natl Univ Singapore, Dept Elect & Comp Engn, 4 Engn Dr 3, Singapore 117583, Singapore.
   [Ang, Kah-Wee] ASTAR, Inst Mat Res & Engn, 2 Fusionopolis Way, Singapore 138634, Singapore.
RP Ang, KW (corresponding author), Natl Univ Singapore, Dept Elect & Comp Engn, 4 Engn Dr 3, Singapore 117583, Singapore.; Ang, KW (corresponding author), ASTAR, Inst Mat Res & Engn, 2 Fusionopolis Way, Singapore 138634, Singapore.
EM eleakw@nus.edu.sg
CR [Anonymous], SNNTORCH 0 5 3
   Baumgarten L, 2021, APPL PHYS LETT, V118, DOI 10.1063/5.0035686
   BENEDETTO JM, 1994, J APPL PHYS, V75, P460, DOI 10.1063/1.355875
   Black CT, 1997, APPL PHYS LETT, V71, P2041, DOI 10.1063/1.119781
   Boescke TS, 2011, APPL PHYS LETT, V99, DOI 10.1063/1.3634052
   Bouvier M, 2019, ACM J EMERG TECH COM, V15, DOI 10.1145/3304103
   Chen CS, 2022, ADV MATER, V34, DOI 10.1002/adma.202201895
   Cho HW, 2021, NPJ 2D MATER APPL, V5, DOI 10.1038/s41699-021-00229-w
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Demasius KU, 2021, NAT ELECTRON, V4, P748, DOI 10.1038/s41928-021-00649-y
   Dutta S, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00634
   Eshraghian JK, 2021, ARXIV
   Feng XW, 2021, ACS NANO, V15, P1764, DOI 10.1021/acsnano.0c09441
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Gong N, 2018, APPL PHYS LETT, V112, DOI 10.1063/1.5010207
   Guo WZ, 2021, FRONT NEUROSCI-SWITZ, V15, DOI [10.3389/fnins.2021.638474, 10.1007/s11704-020-9230-x]
   Gupta I, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms12805
   Hsain HA, 2020, APPL PHYS LETT, V116, DOI 10.1063/5.0002835
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Jo JY, 2007, PHYS REV LETT, V99, DOI 10.1103/PhysRevLett.99.267602
   Kang BS, 2003, APPL PHYS LETT, V82, P248, DOI 10.1063/1.1534411
   Khan AI, 2020, NAT ELECTRON, V3, P588, DOI 10.1038/s41928-020-00492-7
   Lee TY, 2019, ACS APPL MATER INTER, V11, P3142, DOI 10.1021/acsami.8b11681
   Leroux C., 2004, TECH DIG INT EL DEV
   Li C, 2019, NAT MACH INTELL, V1, P49, DOI 10.1038/s42256-018-0001-4
   Li SF, 2022, ADV MATER, V34, DOI 10.1002/adma.202103376
   Luo ZD, 2022, ACS NANO, V16, P3362, DOI 10.1021/acsnano.2c00079
   Lyu X, 2019, INT EL DEVICES MEET, DOI 10.1109/iedm19573.2019.8993509
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Müller J, 2012, NANO LETT, V12, P4318, DOI 10.1021/nl302049k
   Mulaosmanovic H, 2018, IEEE ELECTR DEVICE L, V39, P135, DOI 10.1109/LED.2017.2771818
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Ni K, 2018, IEEE T ELECTRON DEV, V65, P2461, DOI 10.1109/TED.2018.2829122
   Oh S, 2022, IEEE ACCESS, V10, P24444, DOI 10.1109/ACCESS.2022.3149577
   Park MH, 2018, NANOSCALE, V10, P716, DOI 10.1039/c7nr06342c
   Park MH, 2013, APPL PHYS LETT, V102, DOI 10.1063/1.4811483
   Radhakrishnan SS, 2021, NAT COMMUN, V12, DOI 10.1038/s41467-021-22332-8
   Ribes G, 2005, IEEE T DEVICE MAT RE, V5, P5, DOI 10.1109/TDMR.2005.845236
   Rueckauer B., 2020, FRONT NEUROSCI, V11, P682
   Rueckauer B, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351295
   Sang XH, 2015, APPL PHYS LETT, V106, DOI 10.1063/1.4919135
   Sangwan VK, 2020, NAT NANOTECHNOL, V15, P517, DOI 10.1038/s41565-020-0647-z
   Schroeder U, 2022, NAT REV MATER, V7, P653, DOI 10.1038/s41578-022-00431-2
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Si MW, 2019, APPL PHYS LETT, V115, DOI 10.1063/1.5098786
   Sourikopoulos I, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00123
   Sun LF, 2021, SCI ADV, V7, DOI 10.1126/sciadv.abg1455
   Taherkhani A, 2020, NEURAL NETWORKS, V122, P253, DOI 10.1016/j.neunet.2019.09.036
   Tse K, 2007, MICROELECTRON ENG, V84, P2028, DOI 10.1016/j.mee.2007.04.020
   Wang Z., 2021, TECH DIG INT EL DEV
   Xiong K, 2005, APPL PHYS LETT, V87, DOI 10.1063/1.2119425
   You WX, 2019, IEEE ELECTR DEVICE L, V40, P1415, DOI 10.1109/LED.2019.2929277
   Yu Q, 2014, NEUROCOMPUTING, V138, P3, DOI 10.1016/j.neucom.2013.06.052
   Yurchuk E, 2016, IEEE T ELECTRON DEV, V63, P3501, DOI 10.1109/TED.2016.2588439
   Zhao CY, 2015, ACM J EMERG TECH COM, V12, DOI 10.1145/2738040
   Zhou FC, 2020, NAT ELECTRON, V3, P664, DOI 10.1038/s41928-020-00501-9
   Zhou FC, 2019, NAT NANOTECHNOL, V14, P776, DOI 10.1038/s41565-019-0501-3
NR 59
TC 3
Z9 3
U1 15
U2 43
PD JAN 12
PY 2023
VL 35
IS 2
DI 10.1002/adma.202204949
EA NOV 2022
UT WOS:000891955800001
DA 2023-11-16
ER

PT C
AU Wu, H
   Zhang, YY
   Weng, WM
   Zhang, YT
   Xiong, ZW
   Zha, ZJ
   Sun, XY
   Wu, F
AF Wu, Hao
   Zhang, Yueyi
   Weng, Wenming
   Zhang, Yongting
   Xiong, Zhiwei
   Zha, Zheng-Jun
   Sun, Xiaoyan
   Wu, Feng
GP Assoc Advancement Artificial Intelligence
TI Training Spiking Neural Networks with Accumulated Spiking Flow
SO THIRTY-FIFTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, THIRTY-THIRD
   CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE AND THE
   ELEVENTH SYMPOSIUM ON EDUCATIONAL ADVANCES IN ARTIFICIAL INTELLIGENCE
SE AAAI Conference on Artificial Intelligence
DT Proceedings Paper
CT 35th AAAI Conference on Artificial Intelligence / 33rd Conference on
   Innovative Applications of Artificial Intelligence / 11th Symposium on
   Educational Advances in Artificial Intelligence
CY FEB 02-09, 2021
CL ELECTR NETWORK
AB The fast development of neuromorphic hardwares promotes Spiking Neural Networks (SNNs) to a thrilling research avenue. Current SNNs, though much efficient, are less effective compared with leading Artificial Neural Networks (ANNs) especially in supervised learning tasks. Recent efforts further demonstrate the potential of SNNs in supervised learning by introducing approximated backpropagation (BP) methods. To deal with the non-differentiable spike function in SNNs, these BP methods utilize information from the spatio-temporal domain to adjust the model parameters. With the increasing of time window and network size, the computational complexity of spatio-temporal backpropagation augments dramatically. In this paper, we propose a new backpropagation method for SNNs based on the accumulated spiking flow (ASF), i.e. ASF-BP. In the proposed ASF-BP method, updating parameters does not rely on the spike train of spiking neurons but leverage accumulated inputs and outputs of spiking neurons over the time window, which reduces the BP complexity significantly. We further present an adaptive linear estimation model to approach the dynamic characteristics of spiking neurons statistically. Experimental results demonstrate that with our proposed ASF-BP method, light-weight convolutional SNNs achieve superior performances compared with other spike-based BP methods on both non-neuromorphic (MNIST, CIFAR10) and neuromorphic (CIFAR10-DVS) datasets. The code is available at https://github.com/neural-lab/ASF-BP.
C1 [Wu, Hao; Zhang, Yueyi; Weng, Wenming; Zhang, Yongting; Xiong, Zhiwei; Zha, Zheng-Jun; Sun, Xiaoyan; Wu, Feng] Univ Sci & Technol China, Natl Engn Lab Brain Inspired Intelligence Technol, Hefei, Peoples R China.
RP Zhang, YY (corresponding author), Univ Sci & Technol China, Natl Engn Lab Brain Inspired Intelligence Technol, Hefei, Peoples R China.
EM wuhao@mail.ustc.edu.cn; zhyuey@ustc.edu.cn; wmweng@mail.ustc.edu.cn;
   zytabcd@mail.ustc.edu.cn; zwxiong@ustc.edu.cn; zhazj@ustc.edu.cn;
   sunxiaoyan@ustc.edu.cn; fengwu@ustc.edu.cn
CR Agatonovic-Kustrin S, 2000, J PHARMACEUT BIOMED, V22, P717, DOI 10.1016/S0731-7085(99)00272-1
   Alex Krizhevsky G. H., 2009, 1 U TOR COMP SCI DEP
   BOUREAU YL, 2010, PROC CVPR IEEE, P2559, DOI DOI 10.1109/CVPR.2010.5539963
   Burkitt AN, 2006, BIOL CYBERN, V95, P1, DOI 10.1007/s00422-006-0068-6
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Caporale N, 2008, ANNU REV NEUROSCI, V31, P25, DOI 10.1146/annurev.neuro.31.060407.125639
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   Jin Y., 2018, ADV NEURAL INFORM PR, P7005, DOI DOI 10.48550/ARXIV.1805.07866
   Kingma DP., 2017, ARXIV
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
   Lagorce X, 2017, IEEE T PATTERN ANAL, V39, P1346, DOI 10.1109/TPAMI.2016.2574707
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee C, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00119
   Li HM, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00309
   Luo X., 2019, FRONTIERS NEUROSCIEN, V13
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Orchard G, 2015, IEEE T PATTERN ANAL, V37, P2028, DOI 10.1109/TPAMI.2015.2392947
   Panda P, 2016, IEEE IJCNN, P299, DOI 10.1109/IJCNN.2016.7727212
   Paszke Adam, 2017, NIPS W
   Ruder Sebastian, 2016, OVERVIEW GRADIENT DE
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Shrestha SB., 2018, ADV NEURAL INFORM PR, V31, P1412
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Sironi A, 2018, PROC CVPR IEEE, P1731, DOI 10.1109/CVPR.2018.00186
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Tavanaei A, 2019, NEUROCOMPUTING, V330, P39, DOI 10.1016/j.neucom.2018.11.014
   Wu J, 2004, PHYS REV LETT, V93, DOI 10.1103/PhysRevLett.93.017404
   Wu YJ, 2019, AAAI CONF ARTIF INTE, P1311
   Zhang WR, 2019, ADV NEUR IN, V32
NR 35
TC 16
Z9 17
U1 1
U2 6
PY 2021
VL 35
BP 10320
EP 10328
UT WOS:000681269802001
DA 2023-11-16
ER

PT J
AU Chevtchenko, SF
   Ludermir, TB
AF Chevtchenko, Sergio F.
   Ludermir, Teresa B.
TI Combining STDP and binary networks for reinforcement learning from
   images and sparse rewards
SO NEURAL NETWORKS
DT Article
DE Spiking neural networks; Binary neural networks; STDP; Reinforcement
   learning
AB Spiking neural networks (SNNs) aim to replicate energy efficiency, learning speed and temporal processing of biological brains. However, accuracy and learning speed of such networks is still behind reinforcement learning (RL) models based on traditional neural models. This work combines a pre trained binary convolutional neural network with an SNN trained online through reward-modulated STDP in order to leverage advantages of both models. The spiking network is an extension of its previous version, with improvements in architecture and dynamics to address a more challenging task. We focus on extensive experimental evaluation of the proposed model with optimized state-ofthe-art baselines, namely proximal policy optimization (PPO) and deep Q network (DQN). The models are compared on a grid-world environment with high dimensional observations, consisting of RGB images with up to 256 x 256 pixels. The experimental results show that the proposed architecture can be a competitive alternative to deep reinforcement learning (DRL) in the evaluated environment and provide a foundation for more complex future applications of spiking networks. (C) 2021 Elsevier Ltd. All rights reserved.
C1 [Chevtchenko, Sergio F.; Ludermir, Teresa B.] Univ Fed Pernambuco, Ctr Informat CIn, Av Jornalista Anibal Fernandes S-N,Cidade Univ, BR-50740560 Recife, PE, Brazil.
RP Chevtchenko, SF (corresponding author), Univ Fed Pernambuco, Ctr Informat CIn, Av Jornalista Anibal Fernandes S-N,Cidade Univ, BR-50740560 Recife, PE, Brazil.
EM sfc@cin.ufpe.br; tbl@cin.ufpe.br
CR Akiba T, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P2623, DOI 10.1145/3292500.3330701
   Andrychowicz M, 2020, INT J ROBOT RES, V39, P3, DOI 10.1177/0278364919887447
   Andrychowicz Marcin, 2017, NIPS
   Bellec G, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-17236-y
   Bergstra J., 2011, P 24 INT C NEURAL IN, P2546
   Bing ZS, 2020, NEURAL NETWORKS, V121, P21, DOI 10.1016/j.neunet.2019.05.019
   Bing ZS, 2018, FRONT NEUROROBOTICS, V12, DOI 10.3389/fnbot.2018.00035
   Chaladze G., 2017, LINNAEUS 5 DATASET M
   Chevtchenko SF, 2020, IEEE IJCNN, DOI 10.1109/ijcnn48605.2020.9206846
   Chung S, 2020, ARXIV PREPRINT ARXIV
   Cully A, 2015, NATURE, V521, P503, DOI 10.1038/nature14422
   Engstrom Logan, 2019, INT C LEARN REPR
   Feldmann J, 2019, NATURE, V569, P208, DOI 10.1038/s41586-019-1157-8
   Florian RV, 2007, NEURAL COMPUT, V19, P1468, DOI 10.1162/neco.2007.19.6.1468
   Frady E.P., 2020, P NEUR COMP EL WORKS, P1
   Frémaux N, 2016, FRONT NEURAL CIRCUIT, V9, DOI 10.3389/fncir.2015.00085
   Frémaux N, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003024
   Gallego G, 2022, IEEE T PATTERN ANAL, V44, P154, DOI 10.1109/TPAMI.2020.3008413
   Gerstner W, 2018, FRONT NEURAL CIRCUIT, V12, DOI 10.3389/fncir.2018.00053
   Hubara I, 2018, J MACH LEARN RES, V18
   Hwangbo J, 2019, SCI ROBOT, V4, DOI 10.1126/scirobotics.aau5872
   Izhikevich EM, 2007, CEREB CORTEX, V17, P2443, DOI 10.1093/cercor/bhl152
   Kaiser J, 2019, FRONT NEUROROBOTICS, V13, DOI 10.3389/fnbot.2019.00081
   Kappel D, 2018, ENEURO, V5, DOI 10.1523/ENEURO.0301-17.2018
   Kreis B., 2020, KERAS TRAINING
   Legenstein R, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.1000180
   Lillicrap T.P., 2016, CONTINUOUS CONTROL D
   Mamad O, 2017, PLOS BIOL, V15, DOI 10.1371/journal.pbio.2002365
   Michmizos K. P., 2020, ARXIV PREPRINT ARXIV
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Mozafari M, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00625
   Nakano T, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0115620
   OKEEFE J, 1979, BEHAV BRAIN SCI, V2, P487, DOI 10.1017/S0140525X00063949
   Otsuka M, 2010, ESANN
   Potjans W, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1001133
   Qiu HN, 2018, 2018 IEEE SYMPOSIUM SERIES ON COMPUTATIONAL INTELLIGENCE (IEEE SSCI), P1367, DOI 10.1109/SSCI.2018.8628848
   Raffin A., 2019, STABLE BASELINES3
   Rosenfeld B., 2019, P 2019 IEEE 20 INT W, P1
   Schulman John, 2017, ARXIV170706347, DOI DOI 10.48550/ARXIV.1707.06347
   Shixiang Gu, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P3389, DOI 10.1109/ICRA.2017.7989385
   Sutton R. S., 1988, Machine Learning, V3, P9, DOI 10.1023/A:1022633531479
   Sutton RS, 2018, ADAPT COMPUT MACH LE, P1
   Thakur CS, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00891
   van Hasselt H, 2016, AAAI CONF ARTIF INTE, P2094
   Watkins C.J.C.H., 1989, LEARNING DELAYED REW
   Wunderlich T, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00260
   Zhang DK, 2013, FRONT COMPUT NEUROSC, V7, DOI 10.3389/fncom.2013.00141
NR 47
TC 4
Z9 5
U1 4
U2 25
PD DEC
PY 2021
VL 144
BP 496
EP 506
DI 10.1016/j.neunet.2021.09.010
EA SEP 2021
UT WOS:000707983400003
DA 2023-11-16
ER

PT C
AU Jin, C
   Wang, J
   Deng, B
   Qin, YM
   Han, CX
AF Jin, Chen
   Wang, Jiang
   Deng, Bin
   Qin, Yingmei
   Han, Chunxiao
GP IEEE
TI The Effect of Inhibitory Feedback on Temporal Regularity in Neural
   Networks
SO PROCEEDINGS OF THE 2016 12TH WORLD CONGRESS ON INTELLIGENT CONTROL AND
   AUTOMATION (WCICA)
DT Proceedings Paper
CT 12th World Congress on Intelligent Control and Automation (WCICA)
CY JUN 12-15, 2016
CL Guilin, PEOPLES R CHINA
ID MODEL; SYNCHRONIZATION; CONNECTIVITY
AB The regularity of the spiking activity is closely related to the neural encoding and transmission of neural information. However, it is largely unknown how the inhibitory feedback, as a common interaction existed in cortex, contributes to the precise transmission of neural information. In order to explore the answer to this question, we construct a feed-forward network (FFN) model with lateral inhibitory circuits and study the effect of inhibitory feedback on the temporal regularity of spiking activities. Besides, we discuss the spike-time regularity of pure FFN model without feedback for selecting typical dynamic states of neural network. Results show that the inhibitory feedback with adaptive strength enhance the regularity of spike-time in typical states. It indicates that the spike-time regularity observed in the experiment is resulted by the interaction between feed-forward and inhibitory feedback.
C1 [Jin, Chen; Wang, Jiang; Deng, Bin] Tianjin Univ, Sch Elect & Automat Engn, Tianjin 300072, Peoples R China.
   [Qin, Yingmei; Han, Chunxiao] Tianjin Univ Technol & Educ, Sch Automat & Elect Engn, Tianjin 300222, Peoples R China.
RP Jin, C (corresponding author), Tianjin Univ, Sch Elect & Automat Engn, Tianjin 300072, Peoples R China.
EM jinc@tju.edu.cn; jiangwang@tju.edu.cn; dengbin@tju.edu.cn;
   eeymqin@tju.edu.cn; cxhan@tju.edu.cn
CR Arevian AC, 2008, NAT NEUROSCI, V11, P80, DOI 10.1038/nn2030
   Bartsch AP, 2001, BIOL CYBERN, V84, P41, DOI 10.1007/s004220170003
   Callaway EM, 2004, NEURAL NETWORKS, V17, P625, DOI 10.1016/j.neunet.2004.04.004
   Couey JJ, 2013, NAT NEUROSCI, V16, P318, DOI 10.1038/nn.3310
   Dorval AD, 2008, J NEUROSCI METH, V173, P129, DOI 10.1016/j.jneumeth.2008.05.013
   Fetz E., 1991, SYNAPTIC INTERACTION, V9, P1
   Fukai T, 1997, NEURAL COMPUT, V9, P77, DOI 10.1162/neco.1997.9.1.77
   GRAY CM, 1989, NATURE, V338, P334, DOI 10.1038/338334a0
   Han R., 2005, IEEE T NEURAL NETWOR
   Hiratani N, 2015, PLOS COMPUT BIOL, V11, DOI 10.1371/journal.pcbi.1004227
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Litvak V, 2003, J NEUROSCI, V23, P3006
   Maimon G, 2009, NEURON, V62, P426, DOI 10.1016/j.neuron.2009.03.021
   Moldakarimov S, 2015, P NATL ACAD SCI USA, V112, P2545, DOI 10.1073/pnas.1500643112
   Rabinovich MI, 2006, REV MOD PHYS, V78, P1213, DOI 10.1103/RevModPhys.78.1213
   Shadlen Michael N., 1994, Current Opinion in Neurobiology, V4, P569, DOI 10.1016/0959-4388(94)90059-0
   Shadlen MN, 1998, J NEUROSCI, V18, P3870
   SHANNON CE, 1948, BELL SYST TECH J, V27, P379, DOI 10.1002/j.1538-7305.1948.tb01338.x
   van Rossum MCW, 2002, J NEUROSCI, V22, P1956, DOI 10.1523/JNEUROSCI.22-05-01956.2002
   Womelsdorf T, 2006, NATURE, V439, P733, DOI 10.1038/nature04258
NR 21
TC 0
Z9 0
U1 0
U2 0
PY 2016
BP 945
EP 949
UT WOS:000388373802053
DA 2023-11-16
ER

PT J
AU Zhang, YH
   Xiang, SY
   Guo, XX
   Wen, AJ
   Hao, Y
AF Zhang, Yahui
   Xiang, Shuiying
   Guo, Xingxing
   Wen, Aijun
   Hao, Yue
TI A modified supervised learning rule for training a photonic spiking
   neural network to recognize digital patterns
SO SCIENCE CHINA-INFORMATION SCIENCES
DT Article
DE vertical-cavity surface-emitting laser; modified supervised learning
   rule; optical spiking neural networks; learning system; pattern
   recognition
ID TIMING-DEPENDENT PLASTICITY; IMPLEMENTATION
AB A modified supervised learning rule which is suitable for training photonic spiking neural networks (SNN) is proposed for the first time. The proposed learning rule is independent of the time intervals between actual spike and desired spike or between presynaptic spike and postsynaptic spike. Based on the proposed supervised learning rule, 10 digital images are learned in photonic neural network which consists of 30 presynaptic neurons and 10 postsynaptic neurons. Presynaptic and postsynaptic neurons are photonic neurons based on vertical-cavity surface-emitting lasers with an embedded saturable absorber (VCSEL-SA). The results show that 10 digital images are recognized correctly in photonic SNN after enough training. Additionally, the effects of learning rate, the jitters of learning rate, initial weights distribution of SNN and bias current of postsynaptic neurons (VCSELs-SA) on the recognized error are examined carefully based on the proposed learning rule. To the best of our knowledge, such modified supervised learning rule has not yet been reported, which would contribute to training photonic neural networks, and hence is interesting for neuromorphic photonic systems and pattern recognition.
C1 [Zhang, Yahui; Xiang, Shuiying; Guo, Xingxing; Wen, Aijun] Xidian Univ, State Key Lab Integrated Serv Networks, Xian 710071, Peoples R China.
   [Xiang, Shuiying; Hao, Yue] Xidian Univ, Sch Microelect, State Key Discipline Lab Wide Bandgap Semicond Te, Xian 710071, Peoples R China.
RP Xiang, SY (corresponding author), Xidian Univ, State Key Lab Integrated Serv Networks, Xian 710071, Peoples R China.; Xiang, SY (corresponding author), Xidian Univ, Sch Microelect, State Key Discipline Lab Wide Bandgap Semicond Te, Xian 710071, Peoples R China.
EM jxxsy@126.com
CR Alibart F, 2013, NAT COMMUN, V4, DOI 10.1038/ncomms3072
   Barlow HB, 1989, NEURAL COMPUT, V1, P295, DOI 10.1162/neco.1989.1.3.295
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Coomans W, 2011, PHYS REV E, V84, DOI 10.1103/PhysRevE.84.036209
   Deng T, 2017, IEEE J SEL TOP QUANT, V23, DOI 10.1109/JSTQE.2017.2685140
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Feldmann J, 2019, NATURE, V569, P208, DOI 10.1038/s41586-019-1157-8
   Fok MP, 2013, OPT LETT, V38, P419, DOI 10.1364/OL.38.000419
   Giard MH, 1999, J COGNITIVE NEUROSCI, V11, P473, DOI 10.1162/089892999563544
   Li Q, 2018, OPT COMMUN, V407, P327, DOI 10.1016/j.optcom.2017.09.066
   Nahmias MA, 2013, IEEE J SEL TOP QUANT, V19, DOI 10.1109/JSTQE.2013.2257700
   Park S, 2015, SCI REP-UK, V5, DOI 10.1038/srep10123
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Prezioso M, 2015, NATURE, V521, P61, DOI 10.1038/nature14441
   Ren QS, 2015, OPT EXPRESS, V23, P25247, DOI 10.1364/OE.23.025247
   Robertson J, 2020, IEEE J SEL TOP QUANT, V26, DOI 10.1109/JSTQE.2019.2931215
   Shen YC, 2017, NAT PHOTONICS, V11, P441, DOI [10.1038/NPHOTON.2017.93, 10.1038/nphoton.2017.93]
   Song ZW, 2020, IEEE J SEL TOP QUANT, V26, DOI 10.1109/JSTQE.2020.2975564
   Toole R, 2016, J LIGHTWAVE TECHNOL, V34, P470, DOI 10.1109/JLT.2015.2475275
   Toole R, 2015, OPT EXPRESS, V23, P16133, DOI 10.1364/OE.23.016133
   Widrow B, 1988, NEUROCOMPUTING FDN R
   Xiang SY, 2017, IEEE J SEL TOP QUANT, V23, DOI 10.1109/JSTQE.2017.2678170
   Xiang SY, 2021, IEEE T NEUR NET LEAR, V32, P2494, DOI 10.1109/TNNLS.2020.3006263
   Xiang SY, 2020, OPT LETT, V45, P1104, DOI 10.1364/OL.383942
   Xiang SY, 2019, IEEE J SEL TOP QUANT, V25, DOI 10.1109/JSTQE.2019.2911565
   Xiang SY, 2018, IEEE J QUANTUM ELECT, V54, DOI 10.1109/JQE.2018.2879484
   Xu SF, 2020, OPT EXPRESS, V28, P21854, DOI 10.1364/OE.398063
   Zhang YH, 2018, APPL OPTICS, V57, P1731, DOI 10.1364/AO.57.001731
   Zhang Y, 2017, IEEE T ELECTRON DEV, V64, P1806, DOI 10.1109/TED.2017.2671433
NR 29
TC 1
Z9 2
U1 3
U2 19
PD FEB
PY 2021
VL 64
IS 2
AR 122403
DI 10.1007/s11432-020-3040-1
UT WOS:000613005900003
DA 2023-11-16
ER

PT J
AU Soula, H
   Chow, CC
AF Soula, Hedi
   Chow, Carson C.
TI Stochastic dynamics of a finite-size spiking neural network
SO NEURAL COMPUTATION
DT Article
ID FIRE NEURONS; ASYNCHRONOUS STATES; WORKING-MEMORY; SYNAPTIC MECHANISMS;
   PERSISTENT ACTIVITY; INHIBITORY NEURONS; CORTEX; SYNCHRONY; INTEGRATION;
   AVALANCHES
AB We present a simple Markov model of spiking neural dynamics that can be analytically solved to characterize the stochastic dynamics of a finite-size spiking neural network. We give closed-form estimates for the equilibrium distribution, mean rate, variance, and autocorrelation function of the network activity. The model is applicable to any network where the probability of firing of a neuron in the network depends on only the number of neurons that fired in a previous temporal epoch. Networks with statistically homogeneous connectivity and membrane and synaptic time constants that are not excessively long could satisfy these conditions. Our model completely accounts for the size of the network and correlations in the firing activity. It also allows us to examine how the network dynamics can deviate from mean field theory. We show that the model and solutions are applicable to spiking neural networks in biophysically plausible parameter regimes.
C1 NIDDK, Lab Biol Modelling, NIH, Bethesda, MD 20892 USA.
RP Soula, H (corresponding author), NIDDK, Lab Biol Modelling, NIH, Bethesda, MD 20892 USA.
EM hedi.soula@insa-lyon.fr; carsonc@mail.nih.gov
CR ABBOTT LF, 1993, PHYS REV E, V48, P1483, DOI 10.1103/PhysRevE.48.1483
   Amit DJ, 1997, CEREB CORTEX, V7, P237, DOI 10.1093/cercor/7.3.237
   Amit DJ, 1997, NETWORK-COMP NEURAL, V8, P373, DOI 10.1088/0954-898X/8/4/003
   [Anonymous], 1989, METHODS SOLUTION APP
   Beggs JM, 2004, J NEUROSCI, V24, P5216, DOI 10.1523/JNEUROSCI.0540-04.2004
   Beggs JM, 2003, J NEUROSCI, V23, P11167
   Brunel N, 2003, CEREB CORTEX, V13, P1151, DOI 10.1093/cercor/bhg096
   Brunel N, 2000, J COMPUT NEUROSCI, V8, P183, DOI 10.1023/A:1008925309027
   Brunel N, 2006, NEURAL COMPUT, V18, P1066, DOI 10.1162/neco.2006.18.5.1066
   Cai D, 2004, P NATL ACAD SCI USA, V101, P7757, DOI 10.1073/pnas.0401906101
   Compte A, 2000, CEREB CORTEX, V10, P910, DOI 10.1093/cercor/10.9.910
   Compte A, 2006, NEUROSCIENCE, V139, P135, DOI 10.1016/j.neuroscience.2005.06.011
   DELGIUDICE P, 2003, ADV CONDENSED MATTER, P125
   Fourcaud N, 2002, NEURAL COMPUT, V14, P2057, DOI 10.1162/089976602320264015
   Fries P, 2001, SCIENCE, V291, P1560, DOI 10.1126/science.1055465
   Fusi S, 1999, NEURAL COMPUT, V11, P633, DOI 10.1162/089976699300016601
   GERSTNER W, 1995, PHYS REV E, V51, P738, DOI 10.1103/PhysRevE.51.738
   Gerstner W, 2000, NEURAL COMPUT, V12, P43, DOI 10.1162/089976600300015899
   GERSTNER W, 1992, NETWORK-COMP NEURAL, V3, P139, DOI 10.1088/0954-898X/3/2/004
   Gerstner W., 2002, SPIKING NEURON MODEL
   GINZBURG I, 1994, PHYS REV E, V50, P3171, DOI 10.1103/PhysRevE.50.3171
   Golomb D, 2000, NEURAL COMPUT, V12, P1095, DOI 10.1162/089976600300015529
   Gutkin BS, 2001, J COMPUT NEUROSCI, V11, P121, DOI 10.1023/A:1012837415096
   Hansel D, 2003, NEURAL COMPUT, V15, P1, DOI 10.1162/089976603321043685
   HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554
   Laing CR, 2001, NEURAL COMPUT, V13, P1473, DOI 10.1162/089976601750264974
   Mattia M, 2002, PHYS REV E, V66, DOI 10.1103/PhysRevE.66.051917
   Meyer C, 2002, NEURAL COMPUT, V14, P369, DOI 10.1162/08997660252741167
   Nicholls J. G., 1992, NEURON BRAIN, V3
   Nykamp DQ, 2000, J COMPUT NEUROSCI, V8, P19, DOI 10.1023/A:1008912914816
   Pesaran B, 2002, NAT NEUROSCI, V5, P805, DOI 10.1038/nn890
   Plesser HE, 2000, NEURAL COMPUT, V12, P367, DOI 10.1162/089976600300015835
   Salinas E, 2002, NEURAL COMPUT, V14, P2111, DOI 10.1162/089976602320264024
   Seung HS, 1996, P NATL ACAD SCI USA, V93, P13339, DOI 10.1073/pnas.93.23.13339
   SINGER W, 1995, ANNU REV NEUROSCI, V18, P555, DOI 10.1146/annurev.ne.18.030195.003011
   SOFTKY WR, 1993, J NEUROSCI, V13, P334
   Soula H, 2006, NEURAL COMPUT, V18, P60, DOI 10.1162/089976606774841567
   Steinmetz PN, 2000, NATURE, V404, P187, DOI 10.1038/35004588
   TREVES A, 1993, NETWORK-COMP NEURAL, V4, P259, DOI 10.1088/0954-898X/4/3/002
   vanVreeswijk C, 1996, SCIENCE, V274, P1724, DOI 10.1126/science.274.5293.1724
NR 40
TC 33
Z9 33
U1 0
U2 1
PD DEC
PY 2007
VL 19
IS 12
BP 3262
EP 3292
DI 10.1162/neco.2007.19.12.3262
UT WOS:000250751900006
DA 2023-11-16
ER

PT J
AU Sboev, A
   Serenko, A
   Rybka, R
   Vlasov, D
AF Sboev, Alexander
   Serenko, Alexey
   Rybka, Roman
   Vlasov, Danila
TI Solving a classification task by spiking neural network with STDP based
   on rate and temporal input encoding
SO MATHEMATICAL METHODS IN THE APPLIED SCIENCES
DT Article
DE classification task; genetic algorithm; spike-timing-dependent
   plasticity; spiking neural network learning; synaptic plasticity
ID MODEL
AB This paper develops local learning algorithms to solve a classification task with the help of biologically inspired mathematical models of spiking neural networks involving the mechanism of spike-timing-dependent plasticity (STDP). The advantages of the models are their simplicity and, hence, the potential ability to be hardware-implemented in low-energy-consuming biomorphic computing devices. The methods developed are based on two key effects observed in neurons with STDP: mean firing rate stabilization and memorizing repeating spike patterns. As the result, two algorithms to solve a classification task with a spiking neural network are proposed: the first based on rate encoding of the input data and the second based on temporal encoding. The accuracy of the algorithms is tested on the benchmark classification tasks of Fisher's Iris and Wisconsin breast cancer, with several combinations of input data normalization and preprocessing. The respective accuracies are 99% and 94% by F1-score.
C1 [Sboev, Alexander; Serenko, Alexey; Rybka, Roman; Vlasov, Danila] Natl Res Ctr, Kurchatov Inst, Moscow, Russia.
   [Sboev, Alexander; Vlasov, Danila] MEPhI Natl Res Nucl Univ, Moscow, Russia.
RP Sboev, A (corresponding author), Natl Res Ctr, Kurchatov Inst, Moscow, Russia.; Sboev, A (corresponding author), MEPhI Natl Res Nucl Univ, Moscow, Russia.
EM Sboev_AG@nrcki.ru
CR Arena P, 2019, NONLINEAR DYNAM, V95, P1999, DOI 10.1007/s11071-018-4673-4
   Bi GQ, 2001, ANNU REV NEUROSCI, V24, P139, DOI 10.1146/annurev.neuro.24.1.139
   Bohte SM, 2002, ERROR BACKPROPAGATIO, P17
   Demin V, 2018, FRONT NEUROINFORM, V12, DOI 10.3389/fninf.2018.00079
   Diehl P.U., 2015, 2015 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2015.7280696
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Dua D., 2017, UCI MACHINE LEARNING
   Esser SK, 2016, P NATL ACAD SCI USA, V113, P11441, DOI 10.1073/pnas.1604850113
   Fisher RA, 1936, ANN EUGENIC, V7, P179, DOI 10.1111/j.1469-1809.1936.tb02137.x
   Gütig R, 2014, CURR OPIN NEUROBIOL, V25, P134, DOI 10.1016/j.conb.2014.01.004
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Izhikevich EM, 2003, NEURAL COMPUT, V15, P1511, DOI 10.1162/089976603321891783
   Karabatak M, 2009, EXPERT SYST APPL, V36, P3465, DOI 10.1016/j.eswa.2008.02.064
   Kasinski A., 2006, International Journal of Applied Mathematics and Computer Science, V16, P101
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Klikauer T, 2016, TRIPLEC-COMMUN CAPIT, V14, P260
   Kunkel S, 2017, NEST 2 12 0, DOI DOI 10.5281/ZENODO.259534;2017
   Lee JunHaengandDelbruck, 2016, TRAINING DEEP SPIKIN, V10
   Maass W, 1997, NEURAL COMPUT, V9, P279, DOI 10.1162/neco.1997.9.2.279
   Maass W, 2002, MODELS NEURAL NETWOR
   Masquelier T, 2008, PLOS ONE, V3, DOI 10.1371/journal.pone.0001377
   Masquelier T, 2007, PLOS COMPUT BIOL, V3, P247, DOI 10.1371/journal.pcbi.0030031
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Mitra S, 2009, IEEE T BIOMED CIRC S, V3, P32, DOI 10.1109/TBCAS.2008.2005781
   Morrison A, 2008, BIOL CYBERN, V98, P459, DOI 10.1007/s00422-008-0233-1
   Mozafari M., 2018, ARXIV180400227
   Mozafari M, 2019, PATTERN RECOGN, V94, P87, DOI 10.1016/j.patcog.2019.05.015
   Orhan E., 2012, LEAKY INTEGRATE AND
   Ourdighi A, 2016, NEUROCOMPUTING, V13
   Patil PM, 2007, PATTERN RECOGN, V40, P2110, DOI 10.1016/j.patcog.2006.12.018
   Pfeil T, 2012, FRONT NEUROSCI-SWITZ, V6, DOI 10.3389/fnins.2012.00090
   Roy S, 2016, NEURAL COMPUT, V28, P2557, DOI 10.1162/NECO_a_00886
   Saïghi S, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00051
   Sboev A, 2019, P 16 INT C NUM AN AP, V2116
   Sboev A, 2018, PROCEDIA COMPUT SCI, V145, P488, DOI 10.1016/j.procs.2018.11.111
   Sboev A, 2018, PROCEDIA COMPUT SCI, V123, P432, DOI 10.1016/j.procs.2018.01.066
   Schliebs S, 2013, EVOL SYST-GER, V4, P87, DOI 10.1007/s12530-013-9074-9
   Serrano-Gotarredona T, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00002
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Stanley K, MULTINEAT PORTABLE S
   Stanley KO, 2002, EVOL COMPUT, V10, P99, DOI 10.1162/106365602320169811
   STREET WN, 1993, P SOC PHOTO-OPT INS, V1905, P861, DOI 10.1117/12.148698
   Tavanaei A, 2019, NEUROCOMPUTING, V330, P39, DOI 10.1016/j.neucom.2018.11.014
   Wang JL, 2014, NEUROCOMPUTING, V144, P526, DOI 10.1016/j.neucom.2014.04.017
   Yu Q, 2014, NEUROCOMPUTING, V138, P3, DOI 10.1016/j.neucom.2013.06.052
NR 45
TC 11
Z9 11
U1 2
U2 9
PD SEP 15
PY 2020
VL 43
IS 13
SI SI
BP 7802
EP 7814
DI 10.1002/mma.6241
EA JAN 2020
UT WOS:000509860300001
DA 2023-11-16
ER

PT J
AU Baladhandapani, A
   Nachimuthu, DS
AF Baladhandapani, Arunadevi
   Nachimuthu, Deepa Subramaniam
TI Evolutionary learning of spiking neural networks towards quantification
   of 3D MRI brain tumor tissues
SO SOFT COMPUTING
DT Article
DE 3D Magnetic resonance imaging; Multi-dimensional co-occurrence matrices;
   Spiking neural networks; Izhikevich neurons; Genetic algorithm
ID TEXTURE ANALYSIS; NEURONS; RECOGNITION; CLASSIFICATION; OPTIMIZATION
AB This paper presents a new classification technique for 3D MR images, based on a third-generation network of spiking neurons. Implementation of multi-dimensional co-occurrence matrices for the identification of pathological tumor tissue and normal brain tissue features are assessed. The results show the ability of spiking classifier with iterative training using genetic algorithm to automatically and simultaneously recover tissue-specific structural patterns and achieve segmentation of tumor part. The spiking network classifier has been validated and tested for various real-time and Harvard benchmark datasets, where appreciable performance in terms of mean square error, accuracy and computational time is obtained. The spiking network employed Izhikevich neurons as nodes in a multi-layered structure. The classifier has been compared with computational power of multi-layer neural networks with sigmoidal neurons. The results on misclassified tumors are analyzed and suggestions for future work are discussed.
C1 [Baladhandapani, Arunadevi; Nachimuthu, Deepa Subramaniam] Anna Univ, Dept EEE, Reg Ctr, Coimbatore 641047, Tamil Nadu, India.
RP Baladhandapani, A (corresponding author), Anna Univ, Dept EEE, Reg Ctr, Coimbatore 641047, Tamil Nadu, India.
EM arunaamurthy@gmail.com; deepapsg@gmail.com
CR [Anonymous], 2012, CANC FACTS FIG 2012
   Belatreche A, 2006, NEW MATH NAT COMPUT, V2, P237, DOI 10.1142/S179300570600049X
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Chang CC, 2013, INT J INNOV COMPUT I, V9, P1201
   Deepa SN, 2014, J DIGIT IMAGING, V2, P377
   Gerstner W., 2002, SPIKING NEURON MODEL
   Ghodrati M, 2012, PLOS ONE, V17, DOI [10.1371/journal.pone.003235, DOI 10.1371/JOURNAL.PONE.003235]
   Ghosh-Dastidar S, 2007, INTEGR COMPUT-AID E, V14, P187
   Goel P, 2008, INT J KNOWL-BASED IN, V12, P295, DOI 10.3233/KES-2008-12404
   Guo XF, 2010, J TISSUE ENG REGEN M, V4, P181, DOI 10.1002/term.223
   Gupta A, 2007, IEEE IJCNN, P53, DOI 10.1109/IJCNN.2007.4370930
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Jin YC, 2007, LECT NOTES COMPUT SC, V4668, P370
   Jun Chen, 2011, 2011 IEEE 1st International Conference on Computational Advances in Bio and Medical Sciences (ICCABS), P57, DOI 10.1109/ICCABS.2011.5729941
   Kamoi S, 2003, 2003 IEEE INTERNATIONAL SYMPOSIUM ON COMPUTATIONAL INTELLIGENCE IN ROBOTICS AND AUTOMATION, VOLS I-III, PROCEEDINGS, P977
   Kampakis S, 2012, SOFT COMPUT, V16, P943, DOI 10.1007/s00500-011-0793-1
   Kasinski A., 2006, International Journal of Applied Mathematics and Computer Science, V16, P101
   Kaus MR, 2001, RADIOLOGY, V218, P586, DOI 10.1148/radiology.218.2.r01fe44586
   Kovalev V, 1996, GRAPH MODEL IM PROC, V58, P187, DOI 10.1006/gmip.1996.0016
   Kovalev VA, 2001, IEEE T MED IMAGING, V20, P424, DOI 10.1109/42.925295
   Long L.N., 2010, AIAA INFOTECH AEROSP
   MAAS W, 2002, HDB BRAIN THEORY NEU, P1080
   Maass W, 1997, NEURAL COMPUT, V9, P279, DOI 10.1162/neco.1997.9.2.279
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maass W., 1999, PULSED NEURAL NETWOR
   Meftah B, 2010, NEURAL PROCESS LETT, V32, P131, DOI 10.1007/s11063-010-9149-6
   Mizoguchi N, 2011, ARTIF LIFE ROBOT, V16, P383, DOI 10.1007/s10015-011-0956-2
   O'Halloran M, 2011, PROG ELECTROMAGN RES, V113, P413, DOI 10.2528/PIER10122203
   Panchev C, 2006, CONNECT SCI, V18, P1, DOI 10.1080/09540090500132385
   Russell A, 2010, IEEE T NEURAL NETWOR, V21, P1950, DOI 10.1109/TNN.2010.2083685
   Sivanandam SN, 2005, INTRO SOFT COMPUTING
   Stromatias E, 2011, THESIS U LIVERPOOL
   Vazquez R.A., 2010, AUSTR J INTELLIGENT, V11, P35
   Vázquez RA, 2010, LECT NOTES ARTIF INT, V6433, P423
   Vázquez RA, 2011, LECT NOTES COMPUT SC, V6728, P242, DOI 10.1007/978-3-642-21515-5_29
   Wu QX, 2008, NEUROCOMPUTING, V71, P2055, DOI 10.1016/j.neucom.2007.10.020
   Wysoski SG, 2007, LECT NOTES COMPUT SC, V4669, P758
   Zhang J, 2012, BRAIN IMAGING BEHAV, V6, P61, DOI 10.1007/s11682-011-9142-3
   Zhi LF, 2012, IEEE T NEUR NET LEAR, V23, P834, DOI 10.1109/TNNLS.2012.2187539
   ZUCKER SW, 1981, IEEE T PATTERN ANAL, V3, P324, DOI 10.1109/TPAMI.1981.4767105
NR 40
TC 1
Z9 1
U1 1
U2 15
PD JUL
PY 2015
VL 19
IS 7
BP 1803
EP 1816
DI 10.1007/s00500-014-1364-z
UT WOS:000355934500004
DA 2023-11-16
ER

PT J
AU Pavaloiu, B
   Cristea, P
AF Pavaloiu, B.
   Cristea, P.
TI TRAINING SPIKING NEURONS WITH ISOLATED SPIKES CODING
SO UNIVERSITY POLITEHNICA OF BUCHAREST SCIENTIFIC BULLETIN SERIES
   C-ELECTRICAL ENGINEERING AND COMPUTER SCIENCE
DT Article
DE spiking neural network; integrate-and-fire; spiking response model;
   coding; timing coding; supervised training; perceptron training rule
AB The paper presents the structure and function of spiking neural networks. There are described the main models, as well as the modalities of data representation and processing. For an "Integrate-and-Fire" neuron, it is used a timing coding of input data. It is described a method of supervised training for this type of neuron and it is proved that an equivalent perceptron/ perceptron training rule can be found.
C1 [Pavaloiu, B.] Univ Politehn Bucuresti, Fac Engn Foreign Languages, Bucharest, Romania.
   [Cristea, P.] Univ Politehn Bucuresti, Fac Elect Engn, Bucharest, Romania.
RP Pavaloiu, B (corresponding author), Univ Politehn Bucuresti, Fac Engn Foreign Languages, Bucharest, Romania.
CR Adrian E. D., 1928, BASIS SENSATION
   Cristea P. D., 2000, Revue Roumaine des Sciences Techniques, Serie Electrotechnique et Energetique, V45, P75
   Cybenko G., 1989, Mathematics of Control, Signals, and Systems, V2, P303, DOI 10.1007/BF02551274
   Gerstner W, 2001, MATH MODELL, V13, P23
   GERSTNER W, 1995, PHYS REV E, V51, P738, DOI 10.1103/PhysRevE.51.738
   Gerstner W., 2002, SPIKING NEURON MODEL
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   McCulloch Warren S., 1943, BULL MATH BIOPHYS, V5, P115, DOI 10.1007/BF02478259
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   Thorpe S, 1998, COMPUTATIONAL NEUROSCIENCE: TRENDS IN RESEARCH, P113
NR 10
TC 1
Z9 1
U1 0
U2 1
PY 2007
VL 69
IS 3
BP 93
EP 104
UT WOS:000421589200008
DA 2023-11-16
ER

PT C
AU Borsos, T
   Condoluci, M
   Daoutis, M
   Hága, P
   Veres, A
AF Borsos, Tamas
   Condoluci, Massimo
   Daoutis, Marios
   Haga, Peter
   Veres, Andras
GP IEEE
TI Resilience Analysis of Distributed Wireless Spiking Neural Networks
SO 2022 IEEE WIRELESS COMMUNICATIONS AND NETWORKING CONFERENCE (WCNC)
SE IEEE Wireless Communications and Networking Conference
DT Proceedings Paper
CT IEEE Wireless Communications and Networking Conference (IEEE WCNC)
CY APR 10-13, 2022
CL Austin, TX
DE Spiking Neural Network Architecture; Distributed Wireless AI; Traffic
   Prioritization
AB Spiking neural networks (SNN) are expected to enable several use-cases in future communication networks (beyond 5G and 6G), as edge AI and battery-constrained systems can leverage the fast computation and high-power efficiency offered by SNNs. In this work we consider a Distributed Wireless SNN (DW-SNN) system and we analyze its performance in terms of inference accuracy and total neural activity when radio losses are applied to spikes transferred during the inference phase. Our aim is to understand how radio losses impact performance when considering different SNN spike communication types, i.e., input, excitatory, and inhibitory spikes. Then we evaluate the impact of different traffic prioritization approaches among SNN spikes when considering a shared channel capacity being available for SNN activity. From these analyses, we derive some key insights and features that can be considered when applying a DW-SNN and handling its traffic over wireless communication systems. Finally, we report a prototype implementation of DW-SNN using custom-built IoT components, which we use to further investigate different coverage scenarios.
C1 [Borsos, Tamas; Condoluci, Massimo; Daoutis, Marios; Haga, Peter; Veres, Andras] Ericsson Res, Budapest, Hungary.
RP Borsos, T (corresponding author), Ericsson Res, Budapest, Hungary.
EM tamas.borsos@ericsson.com; massimo.condoluci@ericsson.com;
   marios.daoutis@ericsson.com; peter.haga@ericsson.com;
   andras.veres@ericsson.com
CR [Anonymous], PROPHESEE METAVISION
   [Anonymous], 2021, HEXA X PROJECT
   Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Brochini L, 2016, SCI REP-UK, V6, DOI 10.1038/srep35831
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Fernandez-Musoles C, 2019, FRONT NEUROINFORM, V13, DOI 10.3389/fninf.2019.00019
   Furber S, 2016, J NEURAL ENG, V13, DOI 10.1088/1741-2560/13/5/051001
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   Gerstner W., 2002, SPIKING NEURON MODEL
   Ghosh-Dastidar S, 2009, ADV INTEL SOFT COMPU, V61, P167
   Hananel H., 2018, FRONT NEUROINFORM
   Hazan H., 2018, 2018 INT JOINT C NEU, P1
   Hsu J, 2014, IEEE SPECTRUM, V51, P17, DOI 10.1109/MSPEC.2014.6905473
   iniVation AG, INIVATION
   Intel Labs, NEUR COMP
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Yann, 2010, MNIST HANDWRITTEN DI
   Lee WW, 2019, SCI ROBOT, V4, DOI 10.1126/scirobotics.aax2198
   Neuralink, NEURALINK
   Rebecq H, 2019, PROC CVPR IEEE, P3852, DOI 10.1109/CVPR.2019.00398
   Skatchkovsky N, 2021, IEEE COMMUN LETT, V25, P1746, DOI 10.1109/LCOMM.2021.3050212
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
NR 23
TC 0
Z9 0
U1 0
U2 1
PY 2022
BP 2375
EP 2380
DI 10.1109/WCNC51071.2022.9771543
UT WOS:000819473100400
DA 2023-11-16
ER

PT J
AU Yang, X
   Zhang, ZX
   Zhu, WP
   Yu, SM
   Liu, LY
   Wu, NJ
AF Yang, Xu
   Zhang, Zhongxing
   Zhu, Wenping
   Yu, Shuangming
   Liu, Liyuan
   Wu, Nanjian
TI Deterministic conversion rule for CNNs to efficient spiking
   convolutional neural networks
SO SCIENCE CHINA-INFORMATION SCIENCES
DT Article
DE convolutional neural networks (CNN); spiking neural networks (SNN);
   image classification; conversion rule; noise robustness; neuromorphic
   hardware
ID CATEGORIZATION; ARCHITECTURE; VISION
AB This paper proposes a general conversion theory to reveal the relations between convolutional neural network (CNN) and spiking convolutional neural network (spiking CNN) from structure to information processing. Based on the conversion theory and the statistical features of the activations distribution in CNN, we establish a deterministic conversion rule to convert CNNs into spiking CNNs with definite conversion procedure and the optimal setting of all parameters. Included in conversion rule, we propose a novel "n-scaling" weight mapping method to realize high-accuracy, low-latency and power efficient object classification on hardware. For the first time, the minimum dynamic range of spiking neuron's membrane potential is studied to help to balance the trade-off between representation range and precise of the data type adopted by dedicated hardware when spiking CNNs run on it. The simulation results demonstrate that the converted spiking CNNs perform well on MNIST, SVHN and CIFAR-10 datasets. The accuracy loss over three datasets is no more than 0.4%. 39% of processing time is shortened at best, and less power consumption is benefited from lower latency achieved by our conversion rule. Furthermore, the results of noise robustness experiments indicate that spiking CNN inherits the robustness from its corresponding CNN.
C1 [Yang, Xu; Zhang, Zhongxing; Zhu, Wenping; Yu, Shuangming; Liu, Liyuan; Wu, Nanjian] Chinese Acad Sci, Inst Semicond, State Key Lab Superlattices & Microstruct, Beijing 100083, Peoples R China.
   [Yang, Xu; Zhang, Zhongxing; Zhu, Wenping; Yu, Shuangming; Liu, Liyuan; Wu, Nanjian] Univ Chinese Acad Sci, Ctr Mat Sci & Optoelect Engn, Beijing 100049, Peoples R China.
   [Wu, Nanjian] Chinese Acad Sci, Ctr Excellence Brain Sci & Intelligence Technol, Beijing 100083, Peoples R China.
RP Wu, NJ (corresponding author), Chinese Acad Sci, Inst Semicond, State Key Lab Superlattices & Microstruct, Beijing 100083, Peoples R China.; Wu, NJ (corresponding author), Univ Chinese Acad Sci, Ctr Mat Sci & Optoelect Engn, Beijing 100049, Peoples R China.; Wu, NJ (corresponding author), Chinese Acad Sci, Ctr Excellence Brain Sci & Intelligence Technol, Beijing 100083, Peoples R China.
EM nanjian@red.semi.ac.cn
CR Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   Ando K, 2018, IEEE J SOLID-ST CIRC, V53, P983, DOI 10.1109/JSSC.2017.2778702
   Andri R, 2018, IEEE T COMPUT AID D, V37, P48, DOI 10.1109/TCAD.2017.2682138
   [Anonymous], 2016, PROC AUSTRALAS TRANS
   [Anonymous], 2012, 2012 INT JOINT C NEU, DOI [DOI 10.1109/IJCNN.2012.6252637, 10.1109/IJCNN.2012.6252637]
   Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Beyeler M, 2013, NEURAL NETWORKS, V48, P109, DOI 10.1016/j.neunet.2013.07.012
   Brader JM, 2007, NEURAL COMPUT, V19, P2881, DOI 10.1162/neco.2007.19.11.2881
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Chen YH, 2016, ISSCC DIG TECH PAP I, V59, P262, DOI 10.1109/ISSCC.2016.7418007
   Culurciello E, 2003, IEEE J SOLID-ST CIRC, V38, P281, DOI 10.1109/JSSC.2002.807412
   Diehl P.U., 2015, 2015 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2015.7280696
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Du ZD, 2015, 2015 ACM/IEEE 42ND ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA), P92, DOI 10.1145/2749469.2750389
   Esser SK, 2016, P NATL ACAD SCI USA, V113, P11441, DOI 10.1073/pnas.1604850113
   Farabet C, 2012, FRONT NEUROSCI-SWITZ, V6, DOI 10.3389/fnins.2012.00032
   Genc H, 2017, IEEE MICRO, V37, P40, DOI 10.1109/MM.2017.4241339
   Goodfellow I. J., 2013, ARXIV13126082
   Huang J, 2018, IEEE INT C BIOINFORM, P1653, DOI 10.1109/BIBM.2018.8621427
   Hunsberger E., 2016, TRAINING SPIKING DEE
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
   Kulkarni SR, 2018, NEURAL NETWORKS, V103, P118, DOI 10.1016/j.neunet.2018.03.019
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Morrison A, 2007, NEURAL COMPUT, V19, P1437, DOI 10.1162/neco.2007.19.6.1437
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   Querlioz D, 2013, IEEE T NANOTECHNOL, V12, P288, DOI 10.1109/TNANO.2013.2250995
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Shen JC, 2016, SCI CHINA INFORM SCI, V59, DOI 10.1007/s11432-015-5511-7
   Shi C, 2014, IEEE J SOLID-ST CIRC, V49, P2067, DOI 10.1109/JSSC.2014.2332134
   Shin D, 2017, ISSCC DIG TECH PAP I, P240, DOI 10.1109/ISSCC.2017.7870350
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412
   Wang JQ, 2019, SCI CHINA INFORM SCI, V62, DOI 10.1007/s11432-017-9406-0
   Wu NJ, 2018, SCI CHINA INFORM SCI, V61, DOI 10.1007/s11432-017-9303-0
   Xu Y, 2017, 2017 IEEE SYMPOSIUM SERIES ON COMPUTATIONAL INTELLIGENCE (SSCI), P1219
   Yang J, 2018, IEEE T CIRC SYST VID, V28, P746, DOI 10.1109/TCSVT.2016.2618753
   Zhao B, 2015, IEEE T NEUR NET LEAR, V26, P1963, DOI 10.1109/TNNLS.2014.2362542
   Zheng N, 2018, IEEE T NEUR NET LEAR, V29, P4287, DOI 10.1109/TNNLS.2017.2761335
NR 40
TC 15
Z9 15
U1 2
U2 25
PD JAN 15
PY 2020
VL 63
IS 2
AR 122402
DI 10.1007/s11432-019-1468-0
UT WOS:000514585200001
DA 2023-11-16
ER

PT J
AU Naveros, F
   Luque, NR
   Garrido, JA
   Carrillo, RR
   Anguita, M
   Ros, E
AF Naveros, Francisco
   Luque, Niceto R.
   Garrido, Jesus A.
   Carrillo, Richard R.
   Anguita, Mancia
   Ros, Eduardo
TI A Spiking Neural Simulator Integrating Event-Driven and Time-Driven
   Computation Schemes Using Parallel CPU-GPU Co-Processing: A Case Study
SO IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
DT Article
DE Co-processing CPU-graphic processor units (GPUs); event-driven
   execution; event-driven neural simulator based on lookup table (EDLUT);
   real time; simulation; spiking neural network; time-driven execution
ID NETWORKS; NEURONS; MODEL
AB Time-driven simulation methods in traditional CPU architectures perform well and precisely when simulating small-scale spiking neural networks. Nevertheless, they still have drawbacks when simulating large-scale systems. Conversely, event-driven simulation methods in CPUs and time-driven simulation methods in graphic processing units (GPUs) can outperform CPU time-driven methods under certain conditions. With this performance improvement in mind, we have developed an event-and-time-driven spiking neural network simulator suitable for a hybrid CPU-GPU platform. Our neural simulator is able to efficiently simulate bio-inspired spiking neural networks consisting of different neural models, which can be distributed heterogeneously in both small layers and large layers or subsystems. For the sake of efficiency, the low-activity parts of the neural network can be simulated in CPU using event-driven methods while the high-activity subsystems can be simulated in either CPU (a few neurons) or GPU (thousands or millions of neurons) using time-driven methods. In this brief, we have undertaken a comparative study of these different simulation methods. For benchmarking the different simulation methods and platforms, we have used a cerebellar-inspired neural-network model consisting of a very dense granular layer and a Purkinje layer with a smaller number of cells (according to biological ratios). Thus, this cerebellar-like network includes a dense diverging neural layer (increasing the dimensionality of its internal representation and sparse coding) and a converging neural layer (integration) similar to many other biologically inspired and also artificial neural networks.
C1 [Naveros, Francisco; Luque, Niceto R.; Carrillo, Richard R.; Anguita, Mancia; Ros, Eduardo] Univ Granada, Granada 18009, Spain.
   [Garrido, Jesus A.] Univ Pavia, Dept Brain & Behav Sci, Neurophysiol Unit, I-27100 Pavia, Italy.
   [Garrido, Jesus A.] Consorzio Interuniv Sci Fis Mat, I-27100 Pavia, Italy.
RP Naveros, F (corresponding author), Univ Granada, Granada 18009, Spain.
EM fnaveros@ugr.es; nluque@ugr.es; jesus.garrido@unipv.it;
   rcarrillo@ugr.es; manguita@ugr.es; eros@ugr.es
CR Ahmadi A., 2011, P 19 ICEE TEHR IR MA
   Amdahl G. M., 1967, P AFIPS C
   [Anonymous], 2011, FRONT NEUROINFORM
   [Anonymous], P IJCNN
   [Anonymous], 1998, BOOK GENESIS EXPLORI, DOI DOI 10.1007/978-1-4612-1634-63
   [Anonymous], P IJCNN
   Brette R, 2007, J COMPUT NEUROSCI, V23, P349, DOI 10.1007/s10827-007-0038-6
   Brette R, 2012, NETWORK-COMP NEURAL, V23, P167, DOI 10.3109/0954898X.2012.730170
   Carrillo RR, 2008, BIOSYSTEMS, V94, P18, DOI 10.1016/j.biosystems.2008.05.008
   Chadderton P, 2004, NATURE, V428, P856, DOI 10.1038/nature02442
   Chen C, 2011, PROC SPIE, V8134, DOI 10.1117/12.897269
   Chen H, 2010, IEEE T NEURAL NETWOR, V21, P1511, DOI 10.1109/TNN.2010.2049028
   Delorme A, 2003, NETWORK-COMP NEURAL, V14, P613, DOI 10.1088/0954-898X/14/4/301
   Eccles JC., 1967, CEREBELLUM NEURONAL, DOI [10.1007/978-3-662-13147-3, DOI 10.1007/978-3-662-13147-3]
   Fidjeland A. K., 2009, P 20 IEEE INT C ASAP
   Garrido JA, 2011, LECT NOTES COMPUT SC, V6691, P554, DOI 10.1007/978-3-642-21501-8_69
   Gerstner W., 2002, SPIKING NEURON MODEL
   Goodman DFM, 2009, FRONT NEUROSCI-SWITZ, V3, P192, DOI 10.3389/neuro.01.026.2009
   Hines ML, 1997, NEURAL COMPUT, V9, P1179, DOI 10.1162/neco.1997.9.6.1179
   Houghton C, 2012, NETWORK-COMP NEURAL, V23, P48, DOI 10.3109/0954898X.2012.673048
   Kandel E. R., 2000, PRINCIPLES NEURAL SC, V4
   Luque NR, 2011, IEEE T NEURAL NETWOR, V22, P1321, DOI 10.1109/TNN.2011.2156809
   Marc-Oliver G., 2007, SCHOLARPEDIA, DOI [DOI 10.4249/SCHOLARPEDIA.1430, 10.4249/scholarpedia.1430]
   Nowotny T., GENN
   O'reilly R.C., 2000, COMPUTATIONAL EXPLOR
   Oscarsson O., 1976, AFFERENT INTRINSIC O, P34
   Pearson MJ, 2007, IEEE T NEURAL NETWOR, V18, P1472, DOI 10.1109/TNN.2007.891203
   Luque NR, 2011, IEEE T SYST MAN CY B, V41, P1299, DOI 10.1109/TSMCB.2011.2138693
   Ros E, 2006, NEURAL COMPUT, V18, P2959, DOI 10.1162/neco.2006.18.12.2959
   Ros E, 2006, IEEE T NEURAL NETWOR, V17, P1050, DOI 10.1109/TNN.2006.875980
   Rudolph-Lilith M, 2012, NEURAL COMPUT, V24, P1426, DOI 10.1162/NECO_a_00278
   Solinas S, 2010, FRONT CELL NEUROSCI, V4, DOI 10.3389/fncel.2010.00012
   Voogd J, 1998, TRENDS NEUROSCI, V21, P370, DOI 10.1016/S0166-2236(98)01318-6
NR 33
TC 37
Z9 37
U1 1
U2 18
PD JUL
PY 2015
VL 26
IS 7
BP 1567
EP 1574
DI 10.1109/TNNLS.2014.2345844
UT WOS:000356506700021
DA 2023-11-16
ER

PT C
AU Chen, RZ
   Ma, H
   Guo, P
   Xie, SL
   Li, P
   Wang, DL
AF Chen, Ruizhi
   Ma, Hong
   Guo, Peng
   Xie, Shaolin
   Li, Pin
   Wang, Donglin
GP IEEE
TI Low Latency Spiking ConvNets with Restricted Output Training and False
   Spike Inhibition
SO 2018 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUL 08-13, 2018
CL Rio de Janeiro, BRAZIL
DE Spiking Neural Networks; Convolutional Neural Networks; CNN-SNN
   Conversion
ID NEURAL-NETWORKS; MODEL
AB Deep convolutional neural networks (ConvNets) have achieved the state-of-the-art performance on many real world applications. However, significant computation and storage demands are required by ConvNets. Spiking neural networks (SNNs), with sparsely activated neurons and event-driven computations, show great potential to take advantage of the ultra low power spike-based hardware architectures. Yet, training SNN with similar accuracy as ConvNets is difficult. Recent researchers have demonstrated the work of converting ConvNets to SNNs (CNN-SNN conversion) with similar accuracy. However, the energy-efficiency of the converted SNNs is impaired by the increased classification latency. In this paper, we focus on optimizing the classification latency of the converted SNNs. First, we propose a restricted output training method to normalize the converted weights dynamically in the CNN-SNN training phase. Second, false spikes are identified and the false spike inhibition theory is derived to speedup the convergence of the classification process. Third, we propose a temporal max pooling method to approximate the max pooling operation in ConvNets without accuracy loss. The evaluation shows that the converted SNNs converge in about 30 time-steps and achieve the best classification accuracy of 94% on CIFAR-10 dataset.
C1 [Chen, Ruizhi; Ma, Hong; Guo, Peng; Xie, Shaolin; Li, Pin; Wang, Donglin] Chinese Acad Sci, Dept Inst Automat, Univ Chinese Acad Sci UCAS, Beijing, Peoples R China.
RP Chen, RZ (corresponding author), Chinese Acad Sci, Dept Inst Automat, Univ Chinese Acad Sci UCAS, Beijing, Peoples R China.
EM chenruizhi2014@ia.ac.cn; hong.ma@ia.ac.cn
CR [Anonymous], 2017, ABS171004838 CORR
   Boahen K., 2012, IEEE INT S AS CIRC S, pXIV
   Burkitt AN, 2006, BIOL CYBERN, V95, P1, DOI 10.1007/s00422-006-0068-6
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Caporale N, 2008, ANNU REV NEUROSCI, V31, P25, DOI 10.1146/annurev.neuro.31.060407.125639
   Courbariaux M., 2015, ADV NEURAL INF PROCE, V2, P3123, DOI [DOI 10.1109/TWC.2016.2633262, DOI 10.5555/2969442.2969588]
   Diehl P.U., 2015, 2015 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2015.7280696
   Diehl PU, 2016, IEEE IJCNN, P4278, DOI 10.1109/IJCNN.2016.7727758
   Eliasmith C, 2012, SCIENCE, V338, P1202, DOI 10.1126/science.1225266
   Esser SK, 2016, P NATL ACAD SCI USA, V113, P11441, DOI 10.1073/pnas.1604850113
   Furber S, 2016, J NEURAL ENG, V13, DOI 10.1088/1741-2560/13/5/051001
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   Han B, 2017, IEEE T MULTISCALE CO, P1
   Han S, 2015, ARXIV151000149
   Hubara I., 2016, ADV NEURAL INFORM PR, P4107
   HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837
   Hunsberger E., 2016, TRAINING SPIKING DEE
   Ioffe S., 2015, PR MACH LEARN RES, P448
   Jouppi NP, 2017, 44TH ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA 2017), P1, DOI 10.1145/3079856.3080246
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee Jun Haeng, 2016, FRONTIERS NEUROSCIEN
   [李凡杰 Li Fanjie], 2016, [低温工程, Cryogenics], P1
   Li JL, 2017, LECT NOTES COMPUT SC, V10635, P294, DOI 10.1007/978-3-319-70096-0_31
   Lin ZT, 2017, ELECTRON LETT, V53, P1347, DOI 10.1049/el.2017.2219
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Orchard G, 2015, IEEE T PATTERN ANAL, V37, P2028, DOI 10.1109/TPAMI.2015.2392947
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Schemmel J, 2010, IEEE INT SYMP CIRC S, P1947, DOI 10.1109/ISCAS.2010.5536970
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
NR 34
TC 2
Z9 2
U1 0
U2 0
PY 2018
BP 404
EP 411
UT WOS:000585967400055
DA 2023-11-16
ER

PT J
AU Xie, ZJ
   Xie, CG
AF Xie, Zhi-Jiang
   Xie, Chang-Gui
TI FAULT DIAGNOSIS OF GAS BLOWER SETS BASED ON FUZZY SPIKING NEURAL NETWORK
SO METALURGIA INTERNATIONAL
DT Article
DE gas blower; fuzzy spiking neural network; clustering; fault Diagnosis
AB This paper is designed to handle the gas blower failure with regard to the fuzzy classification boundaries and the traditional neural network algorithms that are difficult to solve application problems on the contradictions between an instance of scale and the network scale. Fuzzy spiking neural network was used in the Fault Diagnosis of Gas blower sets, leading to diagnostic algorithms proposed for fuzzy spiking neural network. The first step is to use the receptive fields of neurons, the algorithm converts the input mode into the output pulse train of the neuron, and then pulse sequence with a population coding and unsupervised learning for clustering analysis, thus being able to better overcome the gas blower failure with regard to the classification boundary, and the related clustering analysis of invalidity. The applications showed that the algorithm effectively solves the fuzzy boundaries of thegas blower failure, greatly improving the accuracy of fault diagnosis.
C1 [Xie, Zhi-Jiang; Xie, Chang-Gui] Chongqing Univ, Natl Key Lab Mech Transmiss, Chongqing 630044, Peoples R China.
RP Xie, CG (corresponding author), Chongqing Univ, Natl Key Lab Mech Transmiss, Chongqing 630044, Peoples R China.
EM xiechanggui_125@sina.com
CR Allen Jacob N, 2007, P IDT 07 2 INT DES T, P222
   HAN Manlin, 2011, MACHINE TOOL HYDRAUL, V24, P231
   Herbert J., 2006, NEW APPROACH COMPETI
   Kubota N., 2005, COMPUTATIONAL INTELL, P2410
   Lgwe Philip, 1994, FERROELECTRICS, P287
   Mostafa Mohamed M., 2011, EXPERT SYSTEMS APPL, V38, P6906
   Pakhira MK, 2004, PATTERN RECOGN, V37, P487, DOI 10.1016/j.patcog.2003.06.005
   [仇国庆 Qiu Guoqing], 2008, [中国机械工程, China Mechanical Engineering], V19, P2642
   Rasti J, 2011, EXPERT SYST APPL, V38, P13188, DOI 10.1016/j.eswa.2011.04.132
   Tang YN, 2011, PEER PEER NETW APPL, V4, P439, DOI 10.1007/s12083-010-0100-4
   Yang Xiaofan, 1994, COMPUTER SCI, V21, P23
   [姚剑飞 Yao Jianfei], 2009, [振动、测试与诊断, Journal of Vibration, Measurement and Diagnosis], V29, P74
   YE YU-JIE, 2008, EQUIPMENT MANUFACTUR, V11, P2
   ZI VAN-YANG, 2006, CHINESE J MECH ENG, V42, P117
NR 14
TC 0
Z9 0
U1 0
U2 4
PY 2012
VL 17
IS 7
BP 22
EP 26
UT WOS:000304382600004
DA 2023-11-16
ER

PT C
AU Florian, RV
AF Florian, RV
BE Zaharie, D
   Petcu, D
   Negru, V
   Jebelean, T
   Ciobanu, G
   Cicortas, A
   Abraham, A
   Paprzycki, M
TI A reinforcement learning algorithm for spiking neural networks
SO Seventh International Symposium on Symbolic and Numeric Algorithms for
   Scientific Computing, Proceedings
DT Proceedings Paper
CT 7th International Symposium on Symbolic and Numeric Algorithms for
   Scientific Computing
CY SEP 25-29, 2005
CL Timisoara, ROMANIA
ID POLICY-GRADIENT ESTIMATION/; DEPENDENT PLASTICITY; SYNAPTIC PLASTICITY;
   PREFRONTAL CORTEX; INFINITE-HORIZON; NEURONS; MODULATION; POTENTIATION;
   RECEPTORS; DOPAMINE
AB The paper presents a new reinforcement learning mechanism for spiking neural networks. The algorithm is derived for networks of stochastic integrate-and-fire neurons, but it can be also applied to generic spiking neural networks. Learning is achieved by synaptic changes that depend on the firing of pre- and postsynaptic neurons, and that are modulated with a global reinforcement signal. The efficacy of the algorithm is verified in a biologically-inspired experiment, featuring a simulated worm that searches for food. Our model recovers a form of neural plasticity, experimentally observed in animals, combining spike-timing-dependent synaptic changes of one sign with non-associative synaptic changes of the opposite sign determined by presynaptic spikes. The model also predicts that the time constant of spike-timing-dependent synaptic changes is equal to the membrane time constant of the neuron, in agreement with experimental observations in the brain. This study also led to the discovery of a biologically-plausible reinforcement learning mechanism that works by modulating spike-timing-dependent plasticity (STDP) with a global reward signal.
C1 Ctr Cognit & Neural Studies Coneural, Cluj Napoca, Romania.
RP Florian, RV (corresponding author), Ctr Cognit & Neural Studies Coneural, Cluj Napoca, Romania.
CR Abbott LF, 2000, NAT NEUROSCI, V3, P1178, DOI 10.1038/81453
   BARTLETT P, 2000, P 39 IEEE C DEC CONT
   Bartlett P. L., 2000, BIOL PLAUSIBLE LOCAL
   BARTLETT PL, 1999, HEBBIAN SYNAPTIC MOD
   BARTLETT PL, 2000, P 13 ANN C COMP LEAR, P133
   BARTLETT PL, 1999, DIRECT GRADIENT BASE, V1
   Baxter J, 2001, J ARTIF INTELL RES, V15, P319, DOI 10.1613/jair.806
   Baxter J, 2001, J ARTIF INTELL RES, V15, P351, DOI 10.1613/jair.807
   BAXTER J, 1999, DIRECT GRADIENT BASE, V2
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   BOHTE SM, 2004, ADV NEURAL INFORMATI, V17
   Dan Y, 2004, NEURON, V44, P23, DOI 10.1016/j.neuron.2004.09.007
   FARRIES MA, 2005, P COMP SYST NEUR C C
   FLORIAN RV, 2003, CONEURAL0301 CTR COG
   Gerstner W., 2002, SPIKING NEURON MODEL
   Han VZ, 2000, NEURON, V27, P611, DOI 10.1016/S0896-6273(00)00070-2
   Huang YY, 2004, P NATL ACAD SCI USA, V101, P3236, DOI 10.1073/pnas.0308280101
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Lin YW, 2003, J NEUROSCI, V23, P4173
   MAAS W, 1999, PULSED NEURAL NETWOR
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maass W, 1997, ADV NEUR IN, V9, P211
   MARBACH P, 1999, P 38 C DEC CONTR
   MARBACH P, 2000, DISCRETE EVENT DYN S, V13, P111
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   Pfeifer R., 1999, UNDERSTANDING INTELL
   Ruppin E, 2002, NAT REV NEUROSCI, V3, P132, DOI 10.1038/nrn729
   Schultz W, 2002, NEURON, V36, P241, DOI 10.1016/S0896-6273(02)00967-4
   Seamans JK, 2004, PROG NEUROBIOL, V74, P1, DOI 10.1016/j.pneurobio.2004.05.006
   Seung HS, 2003, NEURON, V40, P1063, DOI 10.1016/S0896-6273(03)00761-X
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Thiel CM, 2002, NEURON, V35, P567, DOI 10.1016/S0896-6273(02)00801-2
   Xie XH, 2004, PHYS REV E, V69, DOI 10.1103/PhysRevE.69.041909
NR 33
TC 14
Z9 14
U1 0
U2 3
PY 2005
BP 299
EP 306
UT WOS:000235867000043
DA 2023-11-16
ER

PT J
AU Valencia, D
   Alimohammad, A
AF Valencia, Daniel
   Alimohammad, Amir
TI Partially binarized neural networks for efficient spike sorting
SO BIOMEDICAL ENGINEERING LETTERS
DT Article
DE Neural networks; Brain-computer interfaces; Spike sorting;
   Application-specific integrated circuits; Neural signal processing
ID FEATURE-EXTRACTION; REAL-TIME; ARCHITECTURE; PROCESSOR; ALGORITHM;
   DESIGN
AB While brain-implantable neural spike sorting can be realized using efficient algorithms, the presence of noise may make it difficult to maintain high-peformance sorting using conventional techniques. In this article, we explore the use of partially binarized neural networks (PBNNs), to the best of our knowledge for the first time, for sorting of neural spike feature vectors. It is shown that compared to the waveform template-based methods, PBNNs offer robust spike sorting over various datasets and noise levels. The ASIC implementation of the PBNN-based spike sorting system in a standard 180-nm CMOS process is presented. The post place and route simulations results show that the synthesized PBNN consumes only 0.59 mu W of power from a 1.8 V supply while operating at 24 kHz and occupies 0.15 mm2 of silicon area. It is shown that the designed PBNN-based spike sorting system not only offers comparable accuracy to the state-of-the-art spike sorting systems over various noise levels and datasets, it also occupies a smaller silicon area and consumes less power and energy. This makes PBNNs a viable alternative towards the implementation of brain-implantable spike sorting systems.
C1 [Valencia, Daniel; Alimohammad, Amir] San Diego State Univ, Dept Elect & Comp Engn, San Diego, CA 92182 USA.
   [Valencia, Daniel] Univ Calif La Jolla, Dept Elect & Comp Engn, La Jolla, CA 92093 USA.
RP Valencia, D (corresponding author), San Diego State Univ, Dept Elect & Comp Engn, San Diego, CA 92182 USA.; Valencia, D (corresponding author), Univ Calif La Jolla, Dept Elect & Comp Engn, La Jolla, CA 92093 USA.
EM dlvalencia@sdsu.edu
CR Do AT, 2019, IEEE T VLSI SYST, V27, P126, DOI 10.1109/TVLSI.2018.2875934
   Chang EWF, 2020, JAMA-J AM MED ASSOC, V323, P413, DOI 10.1001/jama.2019.19813
   Chen F, 2012, IEEE J SOLID-ST CIRC, V47, P744, DOI 10.1109/JSSC.2011.2179451
   Geiger Lukas, 2020, J OPEN SOURCE SOFTW, V5, P1746, DOI DOI 10.21105/JOSS.01746
   Gibson, 2012, THESIS U CALIFORNIA
   Gibson S, 2010, IEEE T NEUR SYS REH, V18, P469, DOI 10.1109/TNSRE.2010.2051683
   Glaser Joshua I, 2020, eNeuro, V7, DOI 10.1523/ENEURO.0506-19.2020
   Guillory KS, 1999, J NEUROSCI METH, V91, P21, DOI 10.1016/S0165-0270(99)00076-X
   Hubara I, 2016, ADV NEUR IN, V29
   Hyvärinen A, 2000, NEURAL NETWORKS, V13, P411, DOI 10.1016/S0893-6080(00)00026-5
   KAISER JF, 1990, INT CONF ACOUST SPEE, P381, DOI 10.1109/ICASSP.1990.115702
   Kamboh AM, 2010, BIOMED CIRC SYST C, P13, DOI 10.1109/BIOCAS.2010.5709559
   Karkare V, 2013, IEEE J SOLID-ST CIRC, V48, P2230, DOI 10.1109/JSSC.2013.2264616
   Karkare V, 2011, IEEE J SOLID-ST CIRC, V46, P1214, DOI 10.1109/JSSC.2011.2116410
   Kim MS, 2019, IEEE T COMPUT, V68, P660, DOI 10.1109/TC.2018.2880742
   Kim S, 2007, IEEE T NEUR SYS REH, V15, P493, DOI 10.1109/TNSRE.2007.908429
   Kocaturk M, 2015, FRONT NEUROROBOTICS, V9, DOI 10.3389/fnbot.2015.00008
   Korat UA, 2019, CIRC SYST SIGNAL PR, V38, P2097, DOI 10.1007/s00034-018-0953-y
   Lewicki MS, 1998, NETWORK-COMP NEURAL, V9, pR53, DOI 10.1088/0954-898X/9/4/001
   Li P, 2014, BIOMED CIRC SYST C, P1, DOI 10.1109/BioCAS.2014.6981630
   LILLIEFORS HW, 1967, J AM STAT ASSOC, V62, P399, DOI 10.2307/2283970
   Liu YN, 2016, IEEE HIGH PERF EXTR
   Lopes MV, 2013, ISSNIP BIOSIG BIOROB, P111
   Mora-Mora H, 2006, REAL-TIME SYST, V34, P53, DOI 10.1007/s11241-006-8753-z
   Nadasdy Z, 2002, P ANN M SOC NEUR
   Paraskevopoulou SE, 2013, J NEUROSCI METH, V215, P29, DOI 10.1016/j.jneumeth.2013.01.012
   Pedreira C, 2012, J NEUROSCI METH, V211, P58, DOI 10.1016/j.jneumeth.2012.07.010
   Quiroga RQ, 2004, NEURAL COMPUT, V16, P1661, DOI 10.1162/089976604774201631
   Simons T, 2019, ELECTRONICS-SWITZ, V8, DOI 10.3390/electronics8060661
   Stillmaker A, 2011, VCL20114 ECE, V4, pm8
   Tang W, 2017, AAAI CONF ARTIF INTE, P2625
   Valencia D, 2022, J NEURAL ENG, V19, DOI 10.1088/1741-2552/ac8077
   Valencia D, 2021, IEEE T NEUR SYS REH, V29, P206, DOI 10.1109/TNSRE.2020.3043403
   Valencia D, 2020, IEEE T CIRCUITS-I, V67, P5200, DOI 10.1109/TCSI.2020.3003769
   Valencia D, 2019, IEEE T BIOMED CIRC S, V13, P1700, DOI 10.1109/TBCAS.2019.2947618
   Valencia D, 2019, IEEE T BIOMED CIRC S, V13, P1714, DOI 10.1109/TBCAS.2019.2947130
   Valencia D, 2019, IEEE T BIOMED CIRC S, V13, P481, DOI 10.1109/TBCAS.2019.2907882
   Willett FR, 2021, NATURE, V593, P249, DOI 10.1038/s41586-021-03506-2
   Xu H, 2019, J NEUROSCI METH, V311, P111, DOI 10.1016/j.jneumeth.2018.10.019
   Yang YN, 2017, IEEE T BIOMED CIRC S, V11, P743, DOI 10.1109/TBCAS.2017.2679032
   Zamani M, 2020, IEEE T BIOMED CIRC S, V14, P221, DOI 10.1109/TBCAS.2020.2969910
   Zamani M, 2018, IEEE T BIOMED CIRC S, V12, P665, DOI 10.1109/TBCAS.2018.2825421
   Zamani M, 2014, IEEE T NEUR SYS REH, V22, P716, DOI 10.1109/TNSRE.2014.2309678
   Zviagintsev A, 2005, I IEEE EMBS C NEUR E, P162
NR 44
TC 0
Z9 0
U1 0
U2 1
PD FEB
PY 2023
VL 13
IS 1
BP 73
EP 83
DI 10.1007/s13534-022-00255-7
EA DEC 2022
UT WOS:000896032600001
DA 2023-11-16
ER

PT J
AU Wu, QX
   McGinnity, TM
   Maguire, LP
   Belatreche, A
   Glackin, B
AF Wu, Q. X.
   McGinnity, T. M.
   Maguire, L. P.
   Belatreche, A.
   Glackin, B.
TI Processing visual stimuli using hierarchical spiking neural networks
SO NEUROCOMPUTING
DT Article
DE hierarchical spiking neural network; receptive field; lateral
   connection; segmentation
ID SCENE SEGMENTATION; MODEL; SYNCHRONIZATION; PLASTICITY; NEURONS; AREAS
AB Based on spiking neuron models and different receptive field models, hierarchical networks are proposed to process visual stimuli, in which multiple overlapped objects are represented by different orientation bars. The main purpose of this paper is to show that hierarchical spiking neural networks are able to segment the objects and bind their pixels to form shapes of objects using local excitatory lateral connections. The presented architecture is based on biologically inspired hierarchical structures. Segmentation is achieved through temporal correlation of neuron activities. The properties of these networks are demonstrated using a series of visual scenes representing different stimuli settings. (C) 2008 Elsevier B.V. All rights reserved.
C1 [Wu, Q. X.; McGinnity, T. M.; Maguire, L. P.; Belatreche, A.; Glackin, B.] Univ Ulster Magee, Sch Comp & Intelligent Syst, Derry BT48 7JL, North Ireland.
RP Wu, QX (corresponding author), Univ Ulster Magee, Sch Comp & Intelligent Syst, Derry BT48 7JL, North Ireland.
EM q.wu@ulster.ac.uk; tm.mcginnity@ulster.ac.uk; lp.maguire@ulster.ac.uk;
   a.belatreche@ulster.ac.uk; b.glackin@ulster.ac.uk
CR Borisyuk RA, 2004, NEURAL NETWORKS, V17, P899, DOI 10.1016/j.neunet.2004.03.005
   Buhmann JM, 2005, NEURAL COMPUT, V17, P1010, DOI 10.1162/0899766053491913
   CHAN JS, 2005, P 2005 INT SENSEMAKE, P10
   CHEN Y, 2002, ACTA METALL SIN, V15, P439
   Choe Y, 2004, BIOL CYBERN, V90, P75, DOI 10.1007/s00422-003-0435-5
   Choe Y, 1998, NEUROCOMPUTING, V21, P139, DOI 10.1016/S0925-2312(98)00040-X
   Tao DC, 2007, KNOWL INF SYST, V13, P1, DOI 10.1007/s10115-006-0050-6
   Dayan P., 2001, THEORETICAL NEUROSCI
   Destexhe A, 2004, NATURE, V431, P789, DOI 10.1038/nature03011
   Gawne TJ, 2002, J NEUROPHYSIOL, V88, P1128, DOI 10.1152/jn.2002.88.3.1128
   Gerstner W, 1996, NATURE, V383, P76, DOI 10.1038/383076a0
   Gerstner W., 2002, SPIKING NEURON MODEL
   Glackin B, 2005, Proceedings of the 8th Joint Conference on Information Sciences, Vols 1-3, P1412
   GLACKIN B, 2005, LNCS, V540, P552
   Hosoya T, 2005, NATURE, V436, P71, DOI 10.1038/nature03689
   Jessell T. M, 1981, PRINCIPLES NEURAL SC
   Knoblauch A, 2002, BIOL CYBERN, V87, P168, DOI 10.1007/s00422-002-0332-3
   Knoblauch A, 2002, BIOL CYBERN, V87, P151, DOI 10.1007/s00422-002-0331-4
   Koch Christof, 1999, P1
   MULLER E, 2003, HDKIP0322 U HEID, P22
   Purushothaman G, 1998, NATURE, V396, P424, DOI 10.1038/24766
   Quiroga RQ, 2005, NATURE, V435, P1102, DOI 10.1038/nature03687
   Riesenhuber M, 2002, CURR OPIN NEUROBIOL, V12, P162, DOI 10.1016/S0959-4388(02)00304-5
   *SENSEMAKER PROJ, 2002, IST200134712
   Sirosh J, 1997, NEURAL COMPUT, V9, P577, DOI 10.1162/neco.1997.9.3.577
   SOMPOLINSKY H, 1990, P NATL ACAD SCI USA, V87, P7200, DOI 10.1073/pnas.87.18.7200
   Song S, 2001, NEURON, V32, P339, DOI 10.1016/S0896-6273(01)00451-2
   Swindale NV, 1996, NETWORK-COMP NEURAL, V7, P161, DOI 10.1088/0954-898X/7/2/002
   Tao DC, 2007, IEEE T PATTERN ANAL, V29, P1700, DOI 10.1109/TPAMI.2007.1096
   Thorpe SJ, 2000, ISCAS 2000: IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS - PROCEEDINGS, VOL IV, P405, DOI 10.1109/ISCAS.2000.858774
   VanRullen R, 2002, VISION RES, V42, P2593, DOI 10.1016/S0042-6989(02)00298-5
   Wu QX, 2007, PROCEEDINGS OF 2007 INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND CYBERNETICS, VOLS 1-7, P1974
   Wu QX, 2007, STUD COMPUT INTELL, V35, P171
   Wu QX, 2005, LECT NOTES COMPUT SC, V3610, P420
   XING J, 1993, BIOL CYBERN, V69, P97, DOI 10.1007/BF00226193
   [No title captured]
NR 36
TC 19
Z9 22
U1 2
U2 7
PD JUN
PY 2008
VL 71
IS 10-12
BP 2055
EP 2068
DI 10.1016/j.neucom.2007.10.020
UT WOS:000257413300030
DA 2023-11-16
ER

PT S
AU Fischer, AD
   Dagli, CH
AF Fischer, AD
   Dagli, CH
BE Kaynak, O
   Alpaydin, E
   Oja, E
   Xu, L
TI Indirect differentiation of function for a network of biologically
   plausible neurons
SO ARTIFICIAL NEURAL NETWORKS AND NEURAL INFORMATION PROCESSING -
   ICAN/ICONIP 2003
SE Lecture Notes in Computer Science
DT Article; Proceedings Paper
CT Joint International Conference on Artificial Neural Networks
   (ICANN)/International Conference on Neural Information Processing
   (ICONIP)
CY JUN 26-29, 2002
CL ISTANBUL, TURKEY
DE artificial neural networks; spiking neural networks; integrate-and-fire
   neuron models; computational intelligence; neuro-modeling; evolutionary
   algorithms; evolutionary neural networks
ID SPIKING NEURONS; MOMENT
AB This paper introduces a new method to model differentiation of biologically plausible neurons, introducing the capability for indirectly defining the characteristics for a network of spiking neurons. Due to its biological plausibility and greater potential for computational power, a spiking neuron model is employed as the basic functional unit in our system. The method for designing the architecture (network design, communication structure, and neuron functionality) for networks of spiking neurons has been purely a manual process. In this paper, we propose a new design for the differentiation of a network of spiking neurons, such that these networks can be indirectly specified, thus enabling a method for the automatic creation of a network for a predetermined function. In this manner, the difficulties associated with the manual creation of these networks are overcome, and opportunity is provided for the utilization of these networks more readily for applications. Thus, this paper provides a new method for indirectly constructing these powerful networks, such as could be easily linked to an evolutionary system or other optimization algorithm.
C1 Univ Missouri, Smart Engn Syst Lab, Dept Engn Management, Rolla, MO 65401 USA.
RP Fischer, AD (corresponding author), Univ Missouri, Smart Engn Syst Lab, Dept Engn Management, Rolla, MO 65401 USA.
CR Berthouze L, 2000, IEEE SYS MAN CYBERN, P86, DOI 10.1109/ICSMC.2000.884969
   BILLARD A, 2001, 230 SAL U SO CAL ROB
   Campbell S, 1996, IEEE T NEURAL NETWOR, V7, P541, DOI 10.1109/72.501714
   Choe Y, 2000, NEUROCOMPUTING, V32, P77, DOI 10.1016/S0925-2312(00)00146-6
   CHOE Y, 2001, PERCEPTUAL GROUPING
   GABBINI F, 1999, METHODS NEURAL MODEL, P313
   GIROSI F, 1990, BIOL CYBERN, V63, P169, DOI 10.1007/BF00195855
   HENKEL RD, 2000, SYNCHRONIZATION COHE
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Hopfield IJ, 2000, P NATL ACAD SCI USA, V97, P13919, DOI 10.1073/pnas.250483697
   Hopfield JJ, 2001, P NATL ACAD SCI USA, V98, P1282, DOI 10.1073/pnas.031567098
   HOPPENSTEADT FC, 1997, INTRO MATH NEURONS
   Horn D, 1998, PULSED NEURAL NETWORKS, P297
   IJSPEERT AJ, 2000, IEEE
   Labbi A, 1999, INT J BIFURCAT CHAOS, V9, P2279, DOI 10.1142/S0218127499001759
   LEVITAN I, 1997, NEURON CELL MOL BIOL
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   MALAKA R, 2000, INT JOINT C NEUR NET
   MILANESE R, 1994, TR94044
   Opher I, 1999, IEE CONF PUBL, P485, DOI 10.1049/cp:19991156
   Rinzel J., 1998, METHODS NEURONAL MOD, P251
   Ruf B, 1998, IEEE T NEURAL NETWOR, V9, P575, DOI 10.1109/72.668899
   VONDERMALSBURG C, 1986, BIOL CYBERN, V54, P29, DOI 10.1007/BF00337113
NR 23
TC 0
Z9 0
U1 0
U2 0
PY 2003
VL 2714
BP 1089
EP 1099
UT WOS:000185378100130
DA 2023-11-16
ER

PT C
AU Li, Y
   Harris, JG
AF Li, Y
   Harris, JG
BE Smailagic, A
   Bayoumi, M
TI A spiking recurrent neural network
SO VLSI 2004: IEEE COMPUTER SOCIETY ANNUAL SYMPOSIUM ON VLSI, PROCEEDINGS
SE IEEE Computer Society Annual Symposium on VLSI
DT Proceedings Paper
CT IEEE-Computer-Society Annual Symposium on VLSI
CY FEB 19-20, 2004
CL Lafayette, LA
ID ASSOCIATIVE MEMORY
AB A spiking recurrent neural network implementing an associative memory is proposed The circuit including four integrate-and-fire (IF) and Willshaw-type binary synapses is designed with the AMI 0.5 um CMOS process. A large-scale network is simulated with Matlab and its storage capacity is calculated and analyzed.
C1 Univ Florida, Dept Elect & Comp Engn, Gainesville, FL 32611 USA.
RP Li, Y (corresponding author), Univ Florida, Dept Elect & Comp Engn, Gainesville, FL 32611 USA.
EM liyuanyl@cnel.ufl.edu; harris@cnel.ufl.edu
CR Hasegawa H, 2001, J PHYS SOC JPN, V70, P2210, DOI 10.1143/JPSJ.70.2210
   Lytton WW, 1998, J COMPUT NEUROSCI, V5, P353, DOI 10.1023/A:1026456411040
   Maass W., 1999, PULSED NEURAL NETWOR
   WILLSHAW DJ, 1969, NATURE, V222, P960, DOI 10.1038/222960a0
NR 4
TC 2
Z9 2
U1 0
U2 1
PY 2004
BP 321
EP 322
DI 10.1109/ISVLSI.2004.1339571
UT WOS:000189433700070
DA 2023-11-16
ER

PT J
AU Fushiki, T
   Aihara, K
AF Fushiki, T
   Aihara, K
TI A phenomenon like stochastic resonance in the process of spike-timing
   dependent synaptic plasticity
SO IEICE TRANSACTIONS ON FUNDAMENTALS OF ELECTRONICS COMMUNICATIONS AND
   COMPUTER SCIENCES
DT Letter
DE temporal spike coding; Hebbian learning; STDP; stochastic resonance
ID NEURAL NETWORKS; SYNCHRONOUS SPIKING; NEURONS; PROPAGATION; INFORMATION;
   OUTPUT; CORTEX; INPUT; EPSPS
AB Recent physiological studies on synaptic plasticity have shown that synaptic weights change depending on fine timing of presynaptic and postsynaptic spikes. Here, we show that a phenomenon similar to stochastic resonance with respect to background noise is observed on spike-timing dependent synaptic plasticity (STDP) that can contribute to stable propagation of precisely timed spikes in a multi-layered feed-forward neural network.
C1 Univ Tokyo, Grad Sch Engn, Dept Math Engn & Informat Phys, Tokyo 1138656, Japan.
   Japan Sci & Technol Corp, JST, CREST, Tokyo 1138656, Japan.
   Univ Tokyo, Dept Complex Sci & Engn, Grad Sch Frontier Sci, Tokyo 1138656, Japan.
RP Fushiki, T (corresponding author), Univ Tokyo, Grad Sch Engn, Dept Math Engn & Informat Phys, Tokyo 1138656, Japan.
EM tada@sat.t.u-tokyo.ac.jp
CR Abbott LF, 2000, NAT NEUROSCI, V3, P1178, DOI 10.1038/81453
   Aertsen A, 1996, J PHYSIOLOGY-PARIS, V90, P243, DOI 10.1016/S0928-4257(97)81432-5
   [Anonymous], 1991, CORTICONICS
   Benzi R., 1981, J PHYS A, V11, P453, DOI DOI 10.1088/0305-4470/14/11/006
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   BULSARA A, 1991, J THEOR BIOL, V152, P531, DOI 10.1016/S0022-5193(05)80396-0
   Burkitt AN, 1999, NEURAL COMPUT, V11, P871, DOI 10.1162/089976699300016485
   COLLINS JJ, 1995, NATURE, V376, P236, DOI 10.1038/376236a0
   Deco G, 1999, NEURAL COMPUT, V11, P919, DOI 10.1162/089976699300016502
   Diesmann M, 1999, NATURE, V402, P529, DOI 10.1038/990101
   Fujii H, 1996, NEURAL NETWORKS, V9, P1303, DOI 10.1016/S0893-6080(96)00054-8
   Gerstner W, 1996, NATURE, V383, P76, DOI 10.1038/383076a0
   Hebb D., 2005, ORG BEHAV NEUROPSYCH
   Kistler WM, 2000, NEURAL COMPUT, V12, P385, DOI 10.1162/089976600300015844
   LONGTIN A, 1991, PHYS REV LETT, V67, P656, DOI 10.1103/PhysRevLett.67.656
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   Marsalek PR, 1997, P NATL ACAD SCI USA, V94, P735, DOI 10.1073/pnas.94.2.735
   MOSS F, 1994, INT J BIFURCAT CHAOS, V4, P1383, DOI 10.1142/S0218127494001118
   SOFTKY WR, 1993, J NEUROSCI, V13, P334
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   WIESENFELD K, 1995, NATURE, V373, P33, DOI 10.1038/373033a0
   Zhang LI, 1998, NATURE, V395, P37, DOI 10.1038/25665
NR 22
TC 5
Z9 5
U1 0
U2 3
PD OCT
PY 2002
VL E85A
IS 10
BP 2377
EP 2380
UT WOS:000178422100027
DA 2023-11-16
ER

PT C
AU Schliebs, S
   Hamed, HNA
   Kasabov, N
AF Schliebs, Stefan
   Hamed, Haza Nuzly Abdull
   Kasabov, Nikola
BE Lu, BL
   Zhang, L
   Kwok, J
TI Reservoir-Based Evolving Spiking Neural Network for Spatio-temporal
   Pattern Recognition
SO NEURAL INFORMATION PROCESSING, PT II
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 18th International Conference on Neural Information Processing (ICONIP)
CY NOV 13-17, 2011
CL Shanghai, PEOPLES R CHINA
DE Spiking Neural Networks; Evolving Systems; Spatio-Temporal Patterns
ID LIQUID-STATE MACHINES
AB Evolving spiking neural networks (eSNN) are computational models that are trained in an one-pass mode from streams of data. They evolve their structure and functionality from incoming data. The paper presents an extension of eSNN called reservoir-based eSNN (reSNN) that allows efficient processing of spatio-temporal data. By classifying the response of a recurrent spiking neural network that is stimulated by a spatio-temporal input signal. the eSNN acts as a readout function for a Liquid State Machine. The classification characteristics of the extended eSNN are illustrated and investigated using the LIBRAS sign language dataset. The paper provides some practical guidelines for configuring the proposed model and shows a competitive classification performance in the obtained experimental results.
C1 [Schliebs, Stefan; Hamed, Haza Nuzly Abdull; Kasabov, Nikola] Auckland Univ Technol, KEDRI, Auckland, New Zealand.
RP Schliebs, S (corresponding author), Auckland Univ Technol, KEDRI, Auckland, New Zealand.
EM sschlieb@aut.ac.nz; hnuzly@aut.ac.nz; nkasabov@aut.ac.nz
CR Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Dias Daniel B., 2009, Proceedings 2009 International Joint Conference on Neural Networks (IJCNN 2009 - Atlanta), P697, DOI 10.1109/IJCNN.2009.5178917
   Gerstner W., 2002, SPIKING NEURON MODEL
   Goodman D., 2008, BMC NEUROSCIENCE S1, V9, P92
   Hamed H, 2010, AUST J INTEL INF PRO, V11, P23
   Hamed HNA, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P2653, DOI 10.1109/IJCNN.2011.6033565
   Indiveri G, 2009, COGN COMPUT, V1, P119, DOI 10.1007/s12559-008-9003-6
   Indiveri G, 2010, IEEE INT SYMP CIRC S, P1951, DOI 10.1109/ISCAS.2010.5536980
   Kasabov N., 1998, J ADV COMPUTATIONAL, V2, P195
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Norton D, 2010, NEUROCOMPUTING, V73, P2893, DOI 10.1016/j.neucom.2010.08.005
   Norton D, 2006, IEEE IJCNN, P4243
   Schliebs S, 2010, LECT NOTES COMPUT SC, V6443, P163, DOI 10.1007/978-3-642-17537-4_21
   Schliebs S, 2009, NEURAL NETWORKS, V22, P623, DOI 10.1016/j.neunet.2009.06.038
   Schrauwen B, 2008, NEURAL NETWORKS, V21, P511, DOI 10.1016/j.neunet.2007.12.009
   Thorpe S.J., 1997, ESANN D FACTO PUBLIC
   Watts MJ, 2009, IEEE T SYST MAN CY C, V39, P253, DOI 10.1109/TSMCC.2008.2012254
   Wysoski SG, 2006, LECT NOTES COMPUT SC, V4179, P1133
NR 18
TC 7
Z9 7
U1 0
U2 1
PY 2011
VL 7063
BP 160
EP 168
PN II
UT WOS:000306990600019
DA 2023-11-16
ER

PT J
AU Nobukawa, S
   Wagatsuma, N
   Nishimura, H
AF Nobukawa, Sou
   Wagatsuma, Nobuhiko
   Nishimura, Haruhiko
TI Deterministic characteristics of spontaneous activity detected by
   multi-fractal analysis in a spiking neural network with long-tailed
   distributions of synaptic weights
SO COGNITIVE NEURODYNAMICS
DT Article
DE Complexity; Fluctuation; Log-normal distribution; Spiking neural
   network; Spontaneous activity
ID BRAIN SIGNAL VARIABILITY; ALZHEIMERS-DISEASE; SPATIOTEMPORAL PATTERNS;
   EEG; CLASSIFICATION; MODELS; MULTIFRACTALITY; NONLINEARITY; COMPUTATION;
   MEG
AB Cortical neural networks maintain autonomous electrical activity called spontaneous activity that represents the brain's dynamic internal state even in the absence of sensory stimuli. The spatio-temporal complexity of spontaneous activity is strongly related to perceptual, learning, and cognitive brain functions; multi-fractal analysis can be utilized to evaluate the complexity of spontaneous activity. Recent studies have shown that the deterministic dynamic behavior of spontaneous activity especially reflects the topological neural network characteristics and changes of neural network structures. However, it remains unclear whether multi-fractal analysis, recently widely utilized for neural activity, is effective for detecting the complexity of the deterministic dynamic process. To verify this point, we focused on the log-normal distribution of excitatory postsynaptic potentials (EPSPs) to evaluate the multi-fractality of spontaneous activity in a spiking neural network with a log-normal distribution of EPSPs. We found that the spiking activities exhibited multi-fractal characteristics. Moreover, to investigate the presence of a deterministic process in the spiking activity, we conducted a surrogate data analysis against the time-series of spiking activity. The results showed that the spontaneous spiking activity included the deterministic dynamic behavior. Overall, the combination of multi-fractal analysis and surrogate data analysis can detect deterministic complex neural activity. The multi-fractal analysis of neural activity used in this study could be widely utilized for brain modeling and evaluation methods for signals obtained by neuroimaging modalities.
C1 [Nobukawa, Sou] Chiba Inst Technol, Dept Comp Sci, 2-17-1 Tsudanuma, Narashino, Chiba 2750016, Japan.
   [Wagatsuma, Nobuhiko] Toho Univ, Fac Sci, Dept Informat Sci, 2-2-1 Miyama, Funabashi, Chiba 2748510, Japan.
   [Nishimura, Haruhiko] Univ Hyogo, Grad Sch Appl Informat, Chuo Ku, Kobe, Hyogo 6500047, Japan.
RP Nobukawa, S (corresponding author), Chiba Inst Technol, Dept Comp Sci, 2-17-1 Tsudanuma, Narashino, Chiba 2750016, Japan.
EM nobukawa@cs.it-chiba.ac.jp
CR Adeli H, 2005, J ALZHEIMERS DIS, V7, P187
   Adeli H, 2005, CLIN EEG NEUROSCI, V36, P131, DOI 10.1177/155005940503600303
   Adeli H, 2008, NEUROSCI LETT, V444, P190, DOI 10.1016/j.neulet.2008.08.008
   Bellec G., 2018, ADV NEURAL INFORM PR
   Bonzon P, 2017, COGN NEURODYNAMICS, V11, P327, DOI 10.1007/s11571-017-9435-3
   Brookes MJ, 2011, P NATL ACAD SCI USA, V108, P16783, DOI 10.1073/pnas.1112685108
   Buzsáki G, 2014, NAT REV NEUROSCI, V15, P264, DOI 10.1038/nrn3687
   da Silva FL, 2013, NEURON, V80, P1112, DOI 10.1016/j.neuron.2013.10.017
   Destexhe A, 2009, J COMPUT NEUROSCI, V27, P493, DOI 10.1007/s10827-009-0164-4
   Easwaramoorthy D., 2010, 2010 International Conference on Communication Control and Computing Technologies, P544, DOI 10.1109/ICCCCT.2010.5670780
   Fox MD, 2007, NAT REV NEUROSCI, V8, P700, DOI 10.1038/nrn2201
   Garrett DD, 2011, J NEUROSCI, V31, P4496, DOI 10.1523/JNEUROSCI.5641-10.2011
   Gautama T, 2003, PHYS REV E, V67, DOI 10.1103/PhysRevE.67.046204
   Goodman Dan FM, 2014, BMC NEUROSCI, V15, P1, DOI DOI 10.1186/1471-2202-15-S1-P199
   Guo DQ, 2010, IEEE T NEURAL NETWOR, V21, P895, DOI 10.1109/TNN.2010.2044419
   Hahn G, 2017, PLOS COMPUT BIOL, V13, DOI 10.1371/journal.pcbi.1005543
   Hasegawa C, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00566
   Jaffard S, 2007, APPL NUMER HARMON AN, P201, DOI 10.1007/978-3-7643-7778-6_17
   Kanamaru T, 2017, NEURAL COMPUT, V29, P1696, DOI 10.1162/NECO_a_00965
   Kantz H., 2004, NONLINEAR TIME SERIE, V7, DOI DOI 10.1017/CBO9780511755798
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Kim SY, 2018, COGN NEURODYNAMICS, V12, P315, DOI 10.1007/s11571-017-9470-0
   Kim SY, 2017, COGN NEURODYNAMICS, V11, P395, DOI 10.1007/s11571-017-9441-5
   Klimesch W, 2007, NEUROSCI BIOBEHAV R, V31, P1003, DOI 10.1016/j.neubiorev.2007.03.005
   Kulkarni SR, 2018, NEURAL NETWORKS, V103, P118, DOI 10.1016/j.neunet.2018.03.019
   La Rocca D, 2018, J NEUROSCI METH, V309, P175, DOI 10.1016/j.jneumeth.2018.09.010
   Lee JM, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00191
   Lin XH, 2017, NEUROCOMPUTING, V237, P59, DOI 10.1016/j.neucom.2016.08.087
   Lin ZT, 2018, NEUROCOMPUTING, V275, P94, DOI 10.1016/j.neucom.2017.05.009
   Maksimenko VA, 2018, NONLINEAR DYNAM, V91, P2803, DOI 10.1007/s11071-018-4047-y
   McCormick DA, 1999, SCIENCE, V285, P541, DOI 10.1126/science.285.5427.541
   Mizuno T, 2010, CLIN NEUROPHYSIOL, V121, P1438, DOI 10.1016/j.clinph.2010.03.025
   Mozafari M, 2018, IEEE T NEUR NET LEAR, V29, P6178, DOI 10.1109/TNNLS.2018.2826721
   Nobukawa S, 2018, P 2018 INT S NONL TH, P375
   Nobukawa S, 2020, FRONT PSYCHIATRY, V11, DOI 10.3389/fpsyt.2020.00255
   Nobukawa S, 2019, J ARTIF INTELL SOFT, V9, P283, DOI 10.2478/jaiscr-2019-0009
   Nobukawa S, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-49286-8
   Nobukawa S, 2019, COGN NEURODYNAMICS, V13, P1, DOI 10.1007/s11571-018-9509-x
   Nobukawa S, 2018, LECT NOTES COMPUT SC, V11301, P535, DOI 10.1007/978-3-030-04167-0_48
   Nobukawa S, 2019, NEUROIMAGE, V188, P357, DOI 10.1016/j.neuroimage.2018.12.008
   Nurujjaman Md, 2009, Nonlinear Biomed Phys, V3, P6, DOI 10.1186/1753-4631-3-6
   Okazaki R, 2015, FRONT HUM NEUROSCI, V9, DOI 10.3389/fnhum.2015.00106
   Oprea L, 2020, COGN NEURODYNAMICS, V14, P267, DOI 10.1007/s11571-020-09568-8
   Rabinovich MI, 2006, REV MOD PHYS, V78, P1213, DOI 10.1103/RevModPhys.78.1213
   Sakata S, 2009, NEURON, V64, P404, DOI 10.1016/j.neuron.2009.09.020
   Samura T, 2015, COGN NEURODYNAMICS, V9, P265, DOI 10.1007/s11571-015-9329-1
   Schreiber T, 1996, PHYS REV LETT, V77, P635, DOI 10.1103/PhysRevLett.77.635
   Stam CJ, 2005, CLIN NEUROPHYSIOL, V116, P2266, DOI 10.1016/j.clinph.2005.06.011
   Takahashi T, 2016, HUM BRAIN MAPP, V37, P1038, DOI 10.1002/hbm.23089
   Takahashi T, 2013, PROG NEURO-PSYCHOPH, V45, P258, DOI 10.1016/j.pnpbp.2012.05.001
   Takahashi T, 2010, NEUROIMAGE, V51, P173, DOI 10.1016/j.neuroimage.2010.02.009
   Tavanaei A, 2018, 2018 INT JOINT C NEU, DOI DOI 10.1109/IJCNN.2018.8489104
   Tavanaei A, 2018, NEURAL NETWORKS, V105, P294, DOI 10.1016/j.neunet.2018.05.018
   Teplan M., 2002, MEAS SCI REV, V2, P1, DOI DOI 10.1021/PR070350L
   Teramae J.-N., 2012, SCI REP-UK, V2, P1
   Tetko IV, 2001, J NEUROSCI METH, V105, P1, DOI 10.1016/S0165-0270(00)00336-8
   THEILER J, 1992, PHYSICA D, V58, P77, DOI 10.1016/0167-2789(92)90102-S
   Uthayakumar R, 2013, FRACTALS, V21, DOI 10.1142/S0218348X13500114
   Van de Ville D, 2010, P NATL ACAD SCI USA, V107, P18179, DOI 10.1073/pnas.1007841107
   Vogels TP, 2005, J NEUROSCI, V25, P10786, DOI 10.1523/JNEUROSCI.3508-05.2005
   Wendt H, 2007, IEEE T SIGNAL PROCES, V55, P4811, DOI 10.1109/TSP.2007.896269
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Yang AC, 2013, PROG NEURO-PSYCHOPH, V45, P253, DOI 10.1016/j.pnpbp.2012.09.015
   Zhang J, 2016, BRAIN, V139, P2307, DOI 10.1093/brain/aww143
NR 64
TC 5
Z9 5
U1 0
U2 6
PD DEC
PY 2020
VL 14
IS 6
BP 829
EP 836
DI 10.1007/s11571-020-09605-6
EA JUN 2020
UT WOS:000543011400002
DA 2023-11-16
ER

PT C
AU Tabarez-Paz, I
   Rudomin, I
   Pérez, H
AF Tabarez-Paz, Israel
   Rudomin, Isaac
   Perez, Hugo
BE Gershenson, C
   Froese, T
   Siqueiros, JM
   Aguilar, W
   Izquierdo, E
   Sayama, H
TI Support Vector Machine and Spiking Neural Networks for Data Driven
   prediction of crowd character movement
SO ALIFE 2016, THE FIFTEENTH INTERNATIONAL CONFERENCE ON THE SYNTHESIS AND
   SIMULATION OF LIVING SYSTEMS
DT Proceedings Paper
CT 15th International Conference on the Synthesis and Simulation of Living
   Systems (ALife)
CY JUL 04-08, 2016
CL Cancun, MEXICO
AB Microscopic crowd simulation usually uses ad-hoc models. While these have been proven to be useful, they are difficult to calibrate and do not always reflect real behaviour. For this reason we propose a machine learning approach using neural networks. The main contribution of the project is a first exploration of prediction of agent trajectories using two specific types of neural networks, Support Vector Machine (SVM) and Spiking Neural Networks (SNN).
C1 [Tabarez-Paz, Israel; Rudomin, Isaac; Perez, Hugo] Barcelona Super Comp Ctr, Barcelona, Spain.
RP Tabarez-Paz, I (corresponding author), Barcelona Super Comp Ctr, Barcelona, Spain.
EM israeltabarez@gmail.com; rudomin.isaac@gmail.com; vhpvmx@gmail.com
CR Abbeel P, 2008, 2008 IEEE/RSJ INTERNATIONAL CONFERENCE ON ROBOTS AND INTELLIGENT SYSTEMS, VOLS 1-3, CONFERENCE PROCEEDINGS, P1083, DOI 10.1109/IROS.2008.4651222
   [Anonymous], 2002, LEAST SQUARES SUPPOR
   [Anonymous], 1987, P 14 ANN C COMPUTER, DOI DOI 10.1145/37402.37406
   Bifet A, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P59, DOI 10.1145/2783258.2783372
   Bonabeau E, 2002, P NATL ACAD SCI USA, V99, P7280, DOI 10.1073/pnas.082080899
   Burges CJC, 1998, DATA MIN KNOWL DISC, V2, P121, DOI 10.1023/A:1009715923555
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   HELBING D, 1995, PHYS REV E, V51, P4282, DOI 10.1103/PhysRevE.51.4282
   HERNANDEZ B., 2011, GPU PRO, P369
   Horváth G, 2003, IEEE IMTC P, P1108
   Huang S., 2016, ARXIV160107265
   Joachims, 1999, SVM SUPPORT VECTOR M, V19, P4
   Kim Sujeong, 2016, P VR
   Lee KH, 2007, SYMPOSIUM ON COMPUTER ANIMATION 2007: ACM SIGGRAPH/ EUROGRAPHICS SYMPOSIUM PROCEEDINGS, P109
   Lerner G, 2009, LIVING WITH HISTORY/MAKING SOCIAL CHANGE, P199
   Maass W., 1995, Advances in Neural Information Processing Systems 7, P183
   Moussaïd M, 2011, P NATL ACAD SCI USA, V108, P6884, DOI 10.1073/pnas.1016507108
   Ondrej J, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778860
   Paris S, 2007, COMPUT GRAPH FORUM, V26, P665, DOI 10.1111/j.1467-8659.2007.01090.x
   Rivas J. I. R., 2014, COMP GRAPH THEOR APP, P1
   Rodriguez M, 2011, IEEE I CONF COMP VIS, P1235, DOI 10.1109/ICCV.2011.6126374
   Rudomin I, 2013, COMPUT SIST, V17, P365
   Ruiz S., 2013, REDUCING MEMORY REQU, P77
   Sujeong Kim A. B., 2015, P IEEE INT S MULT
   Tabarez Paz Israel, 2013, Advances in Soft Computing and Its Applications. 12th Mexican International Conference on Artificial Intelligence, MICAI 2013. Proceedings, LNCS 8266, P279, DOI 10.1007/978-3-642-45111-9_25
   Paz IT, 2014, COMM COM INF SC, V459, P25
   Tabarez-Paz I., 2013, INT J SOFT COMPUTING, V4, P1
   van den Berg J, 2008, I3D 2008: SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, PROCEEDINGS, P139
   van der Pols JC, 2011, SKIN CANCER - A WORLD-WIDE PERSPECTIVE, P3, DOI 10.1007/978-3-642-05072-5_1.1
   Wang H, 2016, PROCEEDINGS I3D 2016: 20TH ACM SIGGRAPH SYMPOSIUM ON INTERACTIVE 3D GRAPHICS AND GAMES, P49, DOI 10.1145/2856400.2856410
   Yang JC, 2009, PROC CVPR IEEE, P1794, DOI 10.1109/CVPRW.2009.5206757
NR 32
TC 0
Z9 0
U1 0
U2 2
PY 2016
BP 638
EP 645
UT WOS:000502979200103
DA 2023-11-16
ER

PT C
AU Duwek, HC
   Shalumov, A
   Tsur, EE
AF Duwek, Hadar Cohen
   Shalumov, Albert
   Tsur, Elishai Ezra
GP IEEE Comp Soc
TI Image Reconstruction from Neuromorphic Event Cameras using
   Laplacian-Prediction and Poisson Integration with Spiking and Artificial
   Neural Networks
SO 2021 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION
   WORKSHOPS, CVPRW 2021
SE IEEE Computer Society Conference on Computer Vision and Pattern
   Recognition Workshops
DT Proceedings Paper
CT IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CY JUN 19-25, 2021
CL ELECTR NETWORK
ID VISION
AB Event cameras are robust neuromorphic visual sensors, which communicate transients in luminance as events. Current paradigm for image reconstruction from event data relies on direct optimization of artificial Convolutional Neural Networks (CNNs). Here we proposed a two-phase neural network, which comprises a CNN, optimized for Laplacian prediction followed by a Spiking Neural Network (SNN) optimized for Poisson integration. By introducing Laplacian prediction into the pipeline, we provide image reconstruction with a network comprising only 200 parameters. We converted the CNN to SNN, providing a full neuromorphic implementation. We further optimized the network with Mish activation and a novel convoluted CNN design, proposing a hybrid of spiking and artificial neural network with < 100 parameters. Models were evaluated on both N-MNIST and N-Caltech101 datasets.
C1 [Duwek, Hadar Cohen; Shalumov, Albert; Tsur, Elishai Ezra] Open Univ Israel, Neurobiomorph Engn Lab NBEL, Dept Math & Comp Sci, Raanana, Israel.
RP Tsur, EE (corresponding author), Open Univ Israel, Neurobiomorph Engn Lab NBEL, Dept Math & Comp Sci, Raanana, Israel.
EM elishai@nbel-lab.com
CR Agarap Abien Fred, 2018, ARXIV PREPRINT ARXIV, P7
   Bekolay T, 2014, FRONT NEUROINFORM, V7, DOI 10.3389/fninf.2013.00048
   Boahen K, 2017, COMPUT SCI ENG, V19, P14, DOI 10.1109/MCSE.2017.33
   Eliasmith C., 2003, NEURAL ENG COMPUTATI
   Eliasmith C, 2012, SCIENCE, V338, P1202, DOI 10.1126/science.1225266
   Fischl KD, 2018, BIOMED CIRC SYST C, P587
   Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699
   Hazan A, 2021, FRONT NEUROSCI-SWITZ, V15, DOI 10.3389/fnins.2021.627221
   Indiveri G, 2000, SCIENCE, V288, P1189, DOI 10.1126/science.288.5469.1189
   Jiang ZY, 2019, FRONT NEUROROBOTICS, V13, DOI 10.3389/fnbot.2019.00029
   Kim H, 2016, LECT NOTES COMPUT SC, V9910, P349, DOI 10.1007/978-3-319-46466-4_21
   Kim Hanme, 2014, P BRIT MACH VIS C, DOI [10.5244/C.28.26, DOI 10.5244/C.28.26]
   Kingma DP., 2017, ARXIV
   Lin CK, 2018, COMPUTER, V51, P52, DOI 10.1109/MC.2018.157113521
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Massa R., 2020, ARXIV200609985, DOI DOI 10.1109/IJCNN48605.2020.9207109
   Mescheder L, 2018, PR MACH LEARN RES, V80
   Misra Diganta, 2019, ARXIV190808681
   Miyatani Y, 2016, IEEE WINT CONF APPL
   Mostafavi M., 2020, IEEE CVF C COMP VIS
   Mostafavi M, 2021, INT J COMPUT VISION, V129, P900, DOI 10.1007/s11263-020-01410-2
   Mundy A, 2015, IEEE IJCNN
   Orchard G, 2015, FRONT NEUROSCI-SWITZ, V9, DOI [10.3389/fhins.2015.00437, 10.3389/fnins.2015.00437]
   Osswald M., 2017, SCI REP-UK, V7, P1, DOI [10.1038/srep40703, DOI 10.1038/S41598-016-0028-X, 10.1038/s41598-016-0028-x]
   Pérez P, 2003, ACM T GRAPHIC, V22, P313, DOI 10.1145/882262.882269
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Posch C, 2014, P IEEE, V102, P1470, DOI 10.1109/JPROC.2014.2346153
   Rasmussen D, 2019, NEUROINFORMATICS, V17, P611, DOI 10.1007/s12021-019-09424-z
   Rebecq H, 2019, PROC CVPR IEEE, P3852, DOI 10.1109/CVPR.2019.00398
   Rebecq H, 2017, IEEE ROBOT AUTOM LET, V2, P593, DOI 10.1109/LRA.2016.2645143
   Rebecq Henri, 2019, IEEE T PATTERN ANAL
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Scheerlinck C, 2020, IEEE WINT CONF APPL, P156, DOI 10.1109/WACV45572.2020.9093366
   Seifozzakerini S., 2016, BMVC
   SIMCHONY T, 1990, IEEE T PATTERN ANAL, V12, P435, DOI 10.1109/34.55103
   Su BY, 2020, IEEE IMAGE PROC, P86, DOI 10.1109/ICIP40778.2020.9191114
   Tsur EE, 2020, NEUROCOMPUTING, V374, P54, DOI 10.1016/j.neucom.2019.09.072
   Volpert V, 2011, MG MATH, V101, P1, DOI 10.1007/978-3-0346-0537-3
   Wang Lijun, 2020, P IEEE CVF C COMP VI, P3
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Zaidel Y, 2021, FRONT NEUROROBOTICS, V15, DOI 10.3389/fnbot.2021.631159
NR 41
TC 6
Z9 6
U1 3
U2 6
PY 2021
BP 1333
EP 1341
DI 10.1109/CVPRW53098.2021.00147
UT WOS:000705890201046
DA 2023-11-16
ER

PT C
AU Gaurav, R
   Tripp, B
   Narayan, A
AF Gaurav, Ramashish
   Tripp, Bryan
   Narayan, Apurva
GP IEEE
TI Spiking Approximations of the MaxPooling Operation in Deep SNNs
SO 2022 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT IEEE International Conference on Fuzzy Systems (FUZZ-IEEE) / IEEE World
   Congress on Computational Intelligence (IEEE WCCI) / International Joint
   Conference on Neural Networks (IJCNN) / IEEE Congress on Evolutionary
   Computation (IEEE CEC)
CY JUL 18-23, 2022
CL Padua, ITALY
AB Spiking Neural Networks (SNNs) are an emerging domain of biologically inspired neural networks that have shown promise for low-power AI. A number of methods exist for building deep SNNs, with Artificial Neural Network (ANN)-to-SNN conversion being highly successful. MaxPooling layers in Convolutional Neural Networks (CNNs) are an integral component to downsample the intermediate feature maps and introduce translational invariance, but the absence of their hardwarefriendly spiking equivalents limits such CNNs' conversion to deep SNNs. In this paper, we present two hardware-friendly methods to implement Max-Pooling in deep SNNs, thus facilitating easy conversion of CNNs with MaxPooling layers to SNNs. In a first, we also execute SNNs with spiking-MaxPooling layers on Intel's Loihi neuromorphic hardware (with MNIST, FMNIST, & CIFAR10 dataset); thus, showing the feasibility of our approach.
C1 [Gaurav, Ramashish; Tripp, Bryan] Univ Waterloo, Syst Design Engn, Waterloo, ON, Canada.
   [Narayan, Apurva] Univ British Columbia, Dept Comp Sci, Vancouver, BC, Canada.
RP Gaurav, R (corresponding author), Univ Waterloo, Syst Design Engn, Waterloo, ON, Canada.
EM rgaurav@uwaterloo.ca; bptripp@uwaterloo.ca; apurva.narayan@ubc.ca
CR [Anonymous], NEURAL ENG COMPUTATI
   [Anonymous], 2016, MAX POOLING OPERATIO
   Boureau Y.-L., 2010, ICML 2010 P 27 INT C, P111, DOI DOI 10.5555/3104322.3104338
   Cheng X, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1519
   Chollet Francois, 2017, PROC CVPR IEEE, P1251, DOI DOI 10.1109/CVPR.2017.195
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Nguyen DA, 2020, APCCAS 2020: PROCEEDINGS OF THE 2020 IEEE ASIA PACIFIC CONFERENCE ON CIRCUITS AND SYSTEMS (APCCAS 2020), P209, DOI [10.1109/APCCAS50809.2020.9301703, 10.1109/apccas50809.2020.9301703]
   Esser SK, 2016, P NATL ACAD SCI USA, V113, P11441, DOI 10.1073/pnas.1604850113
   Garg I., 2020, ARXIV201001795
   Han B., 2020, COMPUTER VISION ECCV
   He K., 2016, P IEEE C COMP VIS PA, DOI DOI 10.1007/978-3-319-46493-0_38
   Huang X., 2021, 2021 IEEE INT S CIRC
   Kim S. Y., 2020, P AAAI C ART INT, V34
   Kucik AS, 2021, IEEE COMPUT SOC CONF, P2020, DOI 10.1109/CVPRW53098.2021.00230
   Kundu S., 2021, P IEEE CVF WINT C CO
   Li C, 2020, IEEE IJCNN, DOI 10.1109/ijcnn48605.2020.9207019
   Li JL, 2017, LECT NOTES COMPUT SC, V10635, P294, DOI 10.1007/978-3-319-70096-0_31
   Lin ZT, 2017, ELECTRON LETT, V53, P1347, DOI 10.1049/el.2017.2219
   Masquelier T, 2007, PLOS COMPUT BIOL, V3, P247, DOI 10.1371/journal.pcbi.0030031
   Mozafari M, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00625
   Orchard G., 2015, IEEE T PATTERN ANAL
   Patel K., 2021, ARXIV210608921
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Rasmussen D, 2019, NEUROINFORMATICS, V17, P611, DOI 10.1007/s12021-019-09424-z
   Rueckauer Bodo, 2017, FRONTIERS NEUROSCIEN
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Sharmin Saima, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12374), P399, DOI 10.1007/978-3-030-58526-6_24
   Sharmin S., 2019 INT JOINT C NEU
   Stromatias E, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00222
   Tan MX, 2019, PR MACH LEARN RES, V97
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Yan ZL, 2021, AAAI CONF ARTIF INTE, V35, P10577
   Zhang ML, 2019, AAAI CONF ARTIF INTE, P1327
   Zhang Malu, 2021, IEEE T NEURAL NETWOR
   Zhao B., 2014, 2014 INT JOINT C NEU
NR 35
TC 1
Z9 1
U1 3
U2 5
PY 2022
DI 10.1109/IJCNN55064.2022.9892504
UT WOS:000867070905016
DA 2023-11-16
ER

PT J
AU Wysoski, SG
   Benuskova, L
   Kasabov, N
AF Wysoski, Simei Gomes
   Benuskova, Lubica
   Kasabov, Nikola
TI Evolving spiking neural networks for audiovisual information processing
SO NEURAL NETWORKS
DT Article
DE Spiking neural network; Audio and visual pattern recognition; Face
   recognition; Speaker authentication; Online classification
ID PATTERN-RECOGNITION; OBJECT RECOGNITION; NEURONS; FACE; COMPUTATION;
   INTEGRATION; MODEL
AB This paper presents a new modular and integrative sensory information system inspired by the way the brain performs information processing, in particular, pattern recognition. Spiking neural networks are used to model human-like visual and auditory pathways. This bimodal system is trained to perform the specific task of person authentication. The two unimodal systems are individually tuned and trained to recognize faces and speech signals from spoken utterances, respectively. New learning procedures are designed to operate in an online evolvable and adaptive way. Several ways of modelling sensory integration using spiking neural network architectures are suggested and evaluated in computer experiments. (C) 2010 Elsevier Ltd. All rights reserved.
C1 [Wysoski, Simei Gomes; Benuskova, Lubica; Kasabov, Nikola] Auckland Univ Technol, Knowledge Engn & Discovery Res Inst, Auckland 1051, New Zealand.
   [Benuskova, Lubica] Univ Otago, Dept Comp Sci, Dunedin, New Zealand.
RP Wysoski, SG (corresponding author), Auckland Univ Technol, Knowledge Engn & Discovery Res Inst, Auckland 1051, New Zealand.
EM wysoski@hotmail.com
CR Abeles M, 2001, J NEUROSCI METH, V107, P141, DOI 10.1016/S0165-0270(01)00364-8
   Abeles M., 1982, LOCAL CORTICAL CIRCU, DOI DOI 10.1007/978-3-642-81708-3
   [Anonymous], 1993, MERGING SENSES MERGI
   [Anonymous], THESIS U PATRAS GREE
   [Anonymous], 2007, EVOLVING CONNECTIONI
   BIENENSTOCK E, 1995, NETWORK-COMP NEURAL, V6, P179, DOI 10.1088/0954-898X/6/2/004
   Bimbot F, 2004, EURASIP J APPL SIG P, V2004, P430, DOI 10.1155/S1110865704310024
   BRUNELLI R, 1995, IEEE T PATTERN ANAL, V17, P955, DOI 10.1109/34.464560
   Burileanu C., 2002, International Journal of Speech Technology, V5, P247, DOI 10.1023/A:1020244924468
   Calvert GA, 2001, CEREB CORTEX, V11, P1110, DOI 10.1093/cercor/11.12.1110
   CHEVALLIER S, 2005, INT MULT PAR DISTR C, P393
   Chibelushi CC, 2002, IEEE T MULTIMEDIA, V4, P23, DOI 10.1109/6046.985551
   Chibelushi CC, 1999, IEEE T SYST MAN CY B, V29, P902, DOI 10.1109/3477.809043
   Crépet A, 2000, IC-AI'2000: PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE, VOL 1-III, P921
   Delorme A, 1999, NEUROCOMPUTING, V26-7, P989, DOI 10.1016/S0925-2312(99)00095-8
   Delorme A, 2001, NEURAL NETWORKS, V14, P795, DOI 10.1016/S0893-6080(01)00049-1
   Ellis HD, 1997, BRIT J PSYCHOL, V88, P143, DOI 10.1111/j.2044-8295.1997.tb02625.x
   Eriksson JL, 2006, BEHAV PROCESS, V73, P348, DOI 10.1016/j.beproc.2006.08.005
   ERIKSSON JL, 2006, INT JOINT C NEUR NET, P1253
   FUKUSHIMA K, 1982, LECT NOTES BIOMATH, V45, P267
   Gallant. S.I., 1995, NEURAL NETWORK LEARN, V3rd
   Gerstner W., 2002, SPIKING NEURON MODEL
   Ghazanfar AA, 2005, J NEUROSCI, V25, P5004, DOI 10.1523/JNEUROSCI.0799-05.2005
   GHITZA O, 1988, J PHONETICS, V16, P109, DOI 10.1016/S0095-4470(19)30469-3
   Gray R. M., 1984, IEEE ASSP Magazine, V1, P4, DOI 10.1109/MASSP.1984.1162229
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Holmberg M., 2005, INTERSPEECH 2005, P1253, DOI [10.21437/Interspeech.2005-480, DOI 10.21437/INTERSPEECH.2005-480]
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837
   Iwasa K, 2007, LECT NOTES COMPUT SC, V4669, P748
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Kiang NY-S., 1966, DISCHARGE PATTERNS S
   Kittler J, 1998, IEEE T PATTERN ANAL, V20, P226, DOI 10.1109/34.667881
   KRUGER N, 2004, INTERDISCIPLINARY J, V1, P417
   KUROYANAGI S, 1994, IEICE T INF SYST, VE77D, P466
   Loiselle S, 2005, IEEE IJCNN, P2076
   MACIOKAS J, 2002, TECHNICAL REPORT BRA
   Matsugu M, 2002, ICONIP'02: PROCEEDINGS OF THE 9TH INTERNATIONAL CONFERENCE ON NEURAL INFORMATION PROCESSING, P660
   Matsugu M, 2003, NEURAL NETWORKS, V16, P555, DOI 10.1016/S0893-6080(03)00115-1
   Mazurek ME, 2002, NAT NEUROSCI, V5, P463, DOI 10.1038/nn836
   MCLENNAN S, 2001, IULC WORKING PAPERS, V201
   Mel BW, 1997, NEURAL COMPUT, V9, P777, DOI 10.1162/neco.1997.9.4.777
   MERCIER D, 2002, 3 WSES INT C NEUR NE
   Movellan J. R., 1995, Advances in Neural Information Processing Systems 7, P851
   Mozayyani N, 1998, IEEE WORLD CONGRESS ON COMPUTATIONAL INTELLIGENCE, P160, DOI 10.1109/IJCNN.1998.682255
   Natschläger T, 1999, NEUROCOMPUTING, V26-7, P463, DOI 10.1016/S0925-2312(99)00052-1
   Natschlager T, 1998, NETWORK-COMP NEURAL, V9, P319, DOI 10.1088/0954-898X/9/3/003
   PERRINET L, 2002, EUR S ART NEUR NETW, P313
   POGGIO T, 1990, SCIENCE, V247, P978, DOI 10.1126/science.247.4945.978
   Rabiner L.R., 1993, FUNDAMENTALS SPEECH
   REECE M, 2001, PULSED NEURAL NETWOR
   Reynolds DA, 2000, DIGIT SIGNAL PROCESS, V10, P19, DOI 10.1006/dspr.1999.0361
   Riesenhuber M, 1999, NAT NEUROSCI, V2, P1019, DOI 10.1038/14819
   Robert A, 1999, J ACOUST SOC AM, V106, P1852, DOI 10.1121/1.427935
   Rosenberg A. E., 1987, Computer Speech and Language, V2, P143, DOI 10.1016/0885-2308(87)90005-2
   Rouat J, 2005, LECT NOTES ARTIF INT, V3445, P317
   SANDERSON C, 2002, DIGIT SIGNAL PROCESS, V14, P449
   SEGUIER R, 2001, 5 INT C ART NEUR NET
   Serre T, 2007, IEEE T PATTERN ANAL, V29, P411, DOI 10.1109/TPAMI.2007.56
   SHAMMA SA, 1985, J ACOUST SOC AM, V78, P1612, DOI 10.1121/1.392799
   Thorpe S, 1996, NATURE, V381, P520, DOI 10.1038/381520a0
   Tikovic P., 2001, Journal of Electrical Engineering, V52, P68
   VanRullen R, 2001, NEUROCOMPUTING, V38, P1003, DOI 10.1016/S0925-2312(01)00445-3
   Vaucher G, 1998, BIOSYSTEMS, V48, P241, DOI 10.1016/S0303-2647(98)00077-X
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   von Kriegstein K, 2005, J COGNITIVE NEUROSCI, V17, P367, DOI 10.1162/0898929053279577
   von Kriegstein K, 2006, PLOS BIOL, V4, P1809, DOI 10.1371/journal.pbio.0040326
   Wysoski SG, 2008, NEUROCOMPUTING, V71, P2563, DOI 10.1016/j.neucom.2007.12.038
   Wysoski SG, 2008, LECT NOTES COMPUT SC, V4985, P406
   Wysoski SG, 2007, LECT NOTES COMPUT SC, V4669, P758
   Wysoski SG, 2006, LECT NOTES COMPUT SC, V4131, P61
   Yamauchi K, 1999, NEURAL NETWORKS, V12, P1347, DOI 10.1016/S0893-6080(99)00064-7
   Yamauchi K., 2001, International Journal of Knowledge-Based Intelligent Engineering Systems, V5, P142
NR 73
TC 83
Z9 86
U1 4
U2 24
PD SEP
PY 2010
VL 23
IS 7
BP 819
EP 835
DI 10.1016/j.neunet.2010.04.009
UT WOS:000281005300005
DA 2023-11-16
ER

PT J
AU Tavanaei, A
   Masquelier, T
   Maida, A
AF Tavanaei, Amirhossein
   Masquelier, Timothee
   Maida, Anthony
TI Representation learning using event-based STDP
SO NEURAL NETWORKS
DT Article
DE Representation learning; Spiking neural networks; Quantization; STDP;
   Bio-inspired model
ID SPIKING NEURAL-NETWORKS; VISUAL FEATURES; SPARSE CODE; NEURONS; MODEL
AB Although representation learning methods developed within the framework of traditional neural networks are relatively mature, developing a spiking representation model remains a challenging problem. This paper proposes an event-based method to train a feedforward spiking neural network (SNN) layer for extracting visual features. The method introduces a novel spike-timing-dependent plasticity (STDP) learning rule and a threshold adjustment rule both derived from a vector quantization-like objective function subject to a sparsity constraint. The STDP rule is obtained by the gradient of a vector quantization criterion that is converted to spike-based, spatio-temporally local update rules in a spiking network of leaky, integrate-and-fire (LIF) neurons. Independence and sparsity of the model are achieved by the threshold adjustment rule and by a softmax function implementing inhibition in the representation layer consisting of WTA-thresholded spiking neurons. Together, these mechanisms implement a form of spike-based, competitive learning. Two sets of experiments are performed on the MNIST and natural image datasets. The results demonstrate a sparse spiking visual representation model with low reconstruction loss comparable with state-of-the-art visual coding approaches, yet our rule is local in both time and space, thus biologically plausible and hardware friendly. (C) 2018 Elsevier Ltd. All rights reserved.
C1 [Tavanaei, Amirhossein; Maida, Anthony] Univ Louisiana Lafayette, Sch Comp & Informat, Lafayette, LA 70504 USA.
   [Masquelier, Timothee] Univ Toulouse 3, CNRS, UMR 5549, CERCO, F-31300 Toulouse, France.
RP Tavanaei, A (corresponding author), Univ Louisiana Lafayette, Sch Comp & Informat, Lafayette, LA 70504 USA.
EM tavanaei@louisiana.edu; timothee.masquelier@cnrs.fr; maida@louisiana.edu
CR [Anonymous], 2016, PROC AUSTRALAS TRANS
   [Anonymous], 2011, P ADV NEUR INF PROC
   Barlow HB, 1989, NEURAL COMPUT, V1, P295, DOI 10.1162/neco.1989.1.3.295
   Bell AJ, 1997, VISION RES, V37, P3327, DOI 10.1016/S0042-6989(97)00121-1
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bienenstock E.L, 1892, J NEUROSCI, V2, P32
   Bishop C., 1995, NEURAL NETWORKS PATT
   Brito CSN, 2016, PLOS COMPUT BIOL, V12, DOI 10.1371/journal.pcbi.1005070
   Burbank KS, 2015, PLOS COMPUT BIOL, V11, DOI 10.1371/journal.pcbi.1004566
   Caporale N, 2008, ANNU REV NEUROSCI, V31, P25, DOI 10.1146/annurev.neuro.31.060407.125639
   Coates Adam, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P561, DOI 10.1007/978-3-642-35289-8_30
   Coates Adam, 2011, P 14 INT C ART INT S, P215
   DeFelipe J, 2012, FRONT NEUROANAT, V6, DOI [10.3389/fnana.2012.00022, 10.3389/fnsyn.2012.00002, 10.3389/fnana.2012.00005]
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Foldiak P., 1989, IJCNN: International Joint Conference on Neural Networks (Cat. No.89CH2765-6), P401, DOI 10.1109/IJCNN.1989.118615
   FOLDIAK P, 1990, BIOL CYBERN, V64, P165, DOI 10.1007/BF02331346
   Foldiak P., 2008, SCHOLARPEDIA, V3, P2984, DOI DOI 10.4249/SCHOLARPEDIA.2984
   Frégnac Y, 2016, RES PER NEUROSCI, P43, DOI 10.1007/978-3-319-28802-4_4
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Grossberg S, 2012, NEURAL NETWORKS, V27, P1, DOI [10.1016/j.neunet.2011.10.011, 10.1016/j.neunet.2012.09.017]
   Hammer B, 2002, NEURAL NETWORKS, V15, P1059, DOI 10.1016/S0893-6080(02)00079-5
   Hinton G. E., 2012, NEURAL NETWORKS TRIC, P599, DOI 10.1007/978-3-642-35289-8_32
   HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Kasabov N, 2013, NEURAL NETWORKS, V41, P188, DOI 10.1016/j.neunet.2012.11.014
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Kheradpisheh SR, 2016, NEUROCOMPUTING, V205, P382, DOI 10.1016/j.neucom.2016.04.029
   King PD, 2013, J NEUROSCI, V33, P5475, DOI 10.1523/JNEUROSCI.4188-12.2013
   Landi SM, 2017, SCIENCE, V357, P591, DOI 10.1126/science.aan1139
   Le Roux N, 2008, NEURAL COMPUT, V20, P1631, DOI 10.1162/neco.2008.04-07-510
   LeCun Y., THE MNIST DATABASE
   Lee H., 2008, ADV NEURAL INFORM PR, P873
   Logothetis NK, 1996, ANNU REV NEUROSCI, V19, P577, DOI 10.1146/annurev.ne.19.030196.003045
   Maass W, 2002, NEURAL NETWORKS, V15, P155, DOI 10.1016/S0893-6080(01)00144-7
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maass W, 1996, ADV NEUR IN, V8, P211
   Maass W, 2015, P IEEE, V103, P2219, DOI 10.1109/JPROC.2015.2496679
   MALSBURG CV, 1973, KYBERNETIK, V14, P85, DOI 10.1007/BF00288907
   Masquelier T, 2007, PLOS COMPUT BIOL, V3, P247, DOI 10.1371/journal.pcbi.0030031
   Masquelier T, 2012, J COMPUT NEUROSCI, V32, P425, DOI 10.1007/s10827-011-0361-9
   Nasrabadi N.M., 2007, PATTERN RECOGN, V16, DOI 10.1117/1.2819119
   Neil D, 2016, P 31 ANN ACM S APPL
   OJA E, 1982, J MATH BIOL, V15, P267, DOI 10.1007/BF00275687
   Olshausen B.A, 2009, P SOC PHOTO-OPT INS, V7446
   Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0
   Quiroga RQ, 2005, NATURE, V435, P1102, DOI 10.1038/nature03687
   Rehn M, 2007, J COMPUT NEUROSCI, V22, P135, DOI 10.1007/s10827-006-0003-9
   Riesenhuber M, 2002, CURR OPIN NEUROBIOL, V12, P162, DOI 10.1016/S0959-4388(02)00304-5
   ROLLS ET, 1995, J NEUROPHYSIOL, V73, P713, DOI 10.1152/jn.1995.73.2.713
   Savin C, 2010, PLOS COMPUT BIOL, V6, DOI 10.1371/journal.pcbi.1000757
   Schliebs S, 2013, EVOL SYST-GER, V4, P87, DOI 10.1007/s12530-013-9074-9
   Schneider P, 2009, NEURAL COMPUT, V21, P2942, DOI 10.1162/neco.2009.10-08-892
   Self MW, 2016, PLOS BIOL, V14, DOI 10.1371/journal.pbio.1002420
   Shrestha A, 2017, IEEE IJCNN, P1999, DOI 10.1109/IJCNN.2017.7966096
   Sjostrom J., 2010, SPIKE TIMING DEPENDE, P35, DOI DOI 10.4249/SCHOLARPEDIA.1362
   Soltic S, 2010, INT J NEURAL SYST, V20, P437, DOI 10.1142/S012906571000253X
   Tavanaei A, 2017, IEEE IJCNN, P2023, DOI 10.1109/IJCNN.2017.7966099
   Tavanaei A, 2016, IEEE IJCNN, P307, DOI 10.1109/IJCNN.2016.7727213
   Thorpe S, 1998, COMPUTATIONAL NEUROSCIENCE: TRENDS IN RESEARCH, P113
   Wandell B. A., 1995, FDN VISION
   Wysoski SG, 2008, NEUROCOMPUTING, V71, P2563, DOI 10.1016/j.neucom.2007.12.038
   Wysoski SG, 2010, NEURAL NETWORKS, V23, P819, DOI 10.1016/j.neunet.2010.04.009
   YOUNG MP, 1992, SCIENCE, V256, P1327, DOI 10.1126/science.1598577
   Zylberberg J, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1002250
NR 65
TC 17
Z9 17
U1 1
U2 16
PD SEP
PY 2018
VL 105
BP 294
EP 303
DI 10.1016/j.neunet.2018.05.018
UT WOS:000441874700024
DA 2023-11-16
ER

PT J
AU Liang, Q
   Zeng, Y
AF Liang, Qian
   Zeng, Yi
TI Stylistic Composition of Melodies Based on a Brain-Inspired Spiking
   Neural Network
SO FRONTIERS IN SYSTEMS NEUROSCIENCE
DT Article
DE spiking neural network; spike-timing dependent plasticity; sequential
   memory; musical learning; melody composition
ID PREFRONTAL CORTEX; EPISODIC RETRIEVAL; THETA; MEMORY; PITCH; TIME;
   PERCEPTION; DEPENDS; MODEL
AB Current neural network based algorithmic composition methods are very different compared to human brain's composition process, while the biological plausibility of composition and generative models are essential for the future of Artificial Intelligence. To explore this problem, this paper presents a spiking neural network based on the inspiration from brain structures and musical information processing mechanisms at multiple scales. Unlike previous methods, our model has three novel characteristics: (1) Inspired by brain structures, multiple brain regions with different cognitive functions, including musical memory and knowledge learning, are simulated and cooperated to generate stylistic melodies. A hierarchical neural network is constructed to formulate musical knowledge. (2) Biologically plausible neural model is employed to construct the network and synaptic connections are modulated using spike-timing-dependent plasticity (STDP) learning rule. Besides, brain oscillation activities with different frequencies perform importantly during the learning and generating process. (3) Based on significant musical memory and knowledge learning, genre-based and composer-based melody composition can be achieved by different neural circuits, the experiments show that the model can compose melodies with different styles of composers or genres.
C1 [Liang, Qian; Zeng, Yi] Chinese Acad Sci, Inst Automat, Res Ctr Brain Inspired Intelligence, Beijing, Peoples R China.
   [Liang, Qian; Zeng, Yi] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing, Peoples R China.
   [Zeng, Yi] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing, Peoples R China.
   [Zeng, Yi] Chinese Acad Sci, Ctr Excellence Brain Sci & Intelligence Technol, Shanghai, Peoples R China.
RP Zeng, Y (corresponding author), Chinese Acad Sci, Inst Automat, Res Ctr Brain Inspired Intelligence, Beijing, Peoples R China.; Zeng, Y (corresponding author), Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing, Peoples R China.; Zeng, Y (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing, Peoples R China.; Zeng, Y (corresponding author), Chinese Acad Sci, Ctr Excellence Brain Sci & Intelligence Technol, Shanghai, Peoples R China.
EM yi.zeng@ia.ac.cn
CR [Anonymous], 2015, BIOL PSYCHOL
   [Anonymous], 2002, I DALLE MOLLE STUDI
   Bashwiner DM, 2016, SCI REP-UK, V6, DOI 10.1038/srep20482
   Bell CC, 1997, NATURE, V387, P278, DOI 10.1038/387278a0
   Bengtsson SL, 2007, J COGNITIVE NEUROSCI, V19, P830, DOI 10.1162/jocn.2007.19.5.830
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Bowman CR, 2018, J NEUROSCI, V38, P2605, DOI 10.1523/JNEUROSCI.2811-17.2018
   Bretan M, 2016, ARXIV PREPRINT ARXIV
   Briot J., 2017, ABS170901620 CORR
   Fernández JD, 2013, J ARTIF INTELL RES, V48, P513, DOI 10.1613/jair.3908
   Dietrich A, 2004, PSYCHON B REV, V11, P1011, DOI 10.3758/BF03196731
   Dong HW, 2018, AAAI CONF ARTIF INTE, P34
   Finke C, 2012, CURR BIOL, V22, pR591, DOI 10.1016/j.cub.2012.05.041
   Fujii RH, 2006, LECT NOTES COMPUT SC, V4131, P780
   Fuster JM, 2000, PSYCHOBIOLOGY, V28, P125
   Fuster JM, 2002, J NEUROCYTOL, V31, P373, DOI 10.1023/A:1024190429920
   Fuster JM, 2000, EXP BRAIN RES, V133, P66, DOI 10.1007/s002210000401
   Fuster JM, 2001, NEURON, V30, P319, DOI 10.1016/S0896-6273(01)00285-9
   Gale E, 2014, INT J UNCONV COMPUT, V10, P181
   Gerstner W, 1996, NATURE, V383, P76, DOI 10.1038/383076a0
   Grachten M., 2016, ARXIV PREPRINT ARXIV
   Hadjeres G., 2017, P MACHINE LEARNING R, P1362
   HUBEL DH, 1968, J PHYSIOL-LONDON, V195, P215, DOI 10.1113/jphysiol.1968.sp008455
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Jensen O, 2002, EUR J NEUROSCI, V15, P1395, DOI 10.1046/j.1460-9568.2002.01975.x
   Johnson DD, 2017, LECT NOTES COMPUT SC, V10198, P128, DOI 10.1007/978-3-319-55750-2_9
   JONES K, 1981, COMPUT MUSIC J, V5, P45, DOI 10.2307/3679879
   Jung RE, 2013, FRONT HUM NEUROSCI, V7, DOI 10.3389/fnhum.2013.00330
   Kahana MJ, 2001, CURR OPIN NEUROBIOL, V11, P739, DOI 10.1016/S0959-4388(01)00278-1
   Kerlleñevich H, 2011, LECT NOTES COMPUT SC, V6625, P344, DOI 10.1007/978-3-642-20520-0_35
   Klimesch W, 2001, COGNITIVE BRAIN RES, V12, P33, DOI 10.1016/S0926-6410(01)00024-6
   Klimesch W, 2001, NEUROSCI LETT, V302, P49, DOI 10.1016/S0304-3940(01)01656-1
   Koelsch S., 2012, BRAIN MUSIC
   Korshunova, 2016, ARXIV PREPRINT ARXIV
   Krueger, 2016, CLASSICAL PIANO MIDI
   Liang Q, 2020, FRONT COMPUT NEUROSC, V14, DOI 10.3389/fncom.2020.00051
   Limb CJ, 2008, PLOS ONE, V3, DOI 10.1371/journal.pone.0001679
   LLINAS RR, 1991, P NATL ACAD SCI USA, V88, P897, DOI 10.1073/pnas.88.3.897
   Lo MY, 2006, IEEE C EVOL COMPUTAT, P601
   Lu J, 2015, SCI REP-UK, V5, DOI 10.1038/srep12277
   Makris D, 2017, COMM COM INF SC, V744, P570, DOI 10.1007/978-3-319-65172-9_48
   McDermott JH, 2008, CURR OPIN NEUROBIOL, V18, P452, DOI 10.1016/j.conb.2008.09.005
   Merchant H, 2013, ANNU REV NEUROSCI, V36, P313, DOI 10.1146/annurev-neuro-062012-170349
   Mozer M. C., 1994, Connection Science, V6, P247, DOI 10.1080/09540099408915726
   Nelson GL, 1996, COMPUT MATH APPL, V32, P109, DOI 10.1016/0898-1221(96)00094-6
   Oxenham AJ, 2012, J NEUROSCI, V32, P13335, DOI 10.1523/JNEUROSCI.3815-12.2012
   Poo MM, 2008, INT J DEV NEUROSCI, V26, P827, DOI 10.1016/j.ijdevneu.2008.09.008
   Sauseng P, 2008, NEUROSCI BIOBEHAV R, V32, P1001, DOI 10.1016/j.neubiorev.2008.03.014
   Schaal NK, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00677
   Sederberg PB, 2003, J NEUROSCI, V23, P10809
   Simon Ian, 2017, PERFORMANCE RNN GENE
   Wu J, 2020, IEEE T CYBERNETICS, V50, P2749, DOI 10.1109/TCYB.2019.2953194
NR 52
TC 4
Z9 4
U1 2
U2 9
PD MAR 11
PY 2021
VL 15
AR 639484
DI 10.3389/fnsys.2021.639484
UT WOS:000632426100001
DA 2023-11-16
ER

PT J
AU Yamaguchi, A
   Arakane, S
   Kubo, M
AF Yamaguchi, Akihiro
   Arakane, Satoshi
   Kubo, Masao
TI Feature Linking using Synchronized Responses in Chaotic Cellar Neural
   Networks for Visual Stimulus of Moving Objects
SO JOURNAL OF ROBOTICS NETWORKING AND ARTIFICIAL LIFE
DT Article
DE chaotic synchronization; neural coding; spike response model; feature
   linking
AB A feature-linking mechanism using the synchronized responses of neural assemblies was studied for chaotic cellar neural networks (Chaotic-CNN). Chaotic-CNNs consist of chaotic spike response neurons that show chaotic inter-spike-interval dynamics. In our scheme of feature linking, the features of the target objects are linked by the synchronized spike responses characterized by the temporal chaotic pattern of the spike sequence. In this paper, we analyze the synchronized spike responses invoked by the visual stimulus of moving bars. Consequently, the resulting neural assemblies have higher correlation for the visual stimulus of two bars moving in the same direction than in the opposite direction. We also discuss the possibility of feature linking using the chaotic synchronized responses.
C1 [Yamaguchi, Akihiro; Arakane, Satoshi] Fukuoka Inst Technol, Dept Informat & Syst Engn, Higashi Ku, 3-30-1 Wajiro Higashi, Fukuoka 8110116, Japan.
   [Kubo, Masao] Natl Def Acad Japan, Dept Comp Sci, 1-10-20 Hashirimizu, Yokosuka, Kanagawa 2398686, Japan.
RP Yamaguchi, A (corresponding author), Fukuoka Inst Technol, Dept Informat & Syst Engn, Higashi Ku, 3-30-1 Wajiro Higashi, Fukuoka 8110116, Japan.
EM aki@fit.ac.jp
CR Eckhorn R., 1989, IJCNN: International Joint Conference on Neural Networks (Cat. No.89CH2765-6), P723, DOI 10.1109/IJCNN.1989.118659
   Fujiwara M., 2016, J ROBOTICS NETWORKIN, V2, P26
   Gerstner W., 2002, SPIKING NEURON MODEL
   GRAY CM, 1989, NATURE, V338, P334, DOI 10.1038/338334a0
   Lee G, 2001, NEURAL NETWORKS, V14, P115, DOI 10.1016/S0893-6080(00)00083-6
   Yamaguchi A, 2014, FUKUOKA I TECHNOLOGY, V25, P1
   Yamaguchi A, 2013, FUKUOKA I TECHNOLOGY, V24, P1
   Yamaguchi A., 2000, IEICE TECH REP, V99, p[NC99, 15]
   Yamaguti Y., 2002, IEICE TECH REP, V101, P127
NR 9
TC 3
Z9 3
U1 0
U2 1
PD MAR
PY 2016
VL 2
IS 4
BP 230
EP 233
DI 10.2991/jrnal.2016.2.4.6
UT WOS:000379116500006
DA 2023-11-16
ER

PT C
AU Meng, MY
   Yang, XY
   Xiao, SL
   Yu, ZY
AF Meng, Mingyuan
   Yang, Xingyu
   Xiao, Shanlin
   Yu, Zhiyi
GP IEEE
TI Spiking Inception Module for Multi-layer Unsupervised Spiking Neural
   Networks
SO 2020 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN) held as part
   of the IEEE World Congress on Computational Intelligence (IEEE WCCI)
CY JUL 19-24, 2020
CL ELECTR NETWORK
DE Spiking neural networks; Unsupervised learning; Inception module
ID CLASSIFICATION; MODEL
AB Spiking Neural Network (SNN), as a brain-inspired approach, is attracting attention due to its potential to produce ultra-high-energy-efficient hardware. Competitive learning based on Spike-Timing-Dependent Plasticity (STDP) is a popular method to train an unsupervised SNN. However, previous unsupervised SNNs trained through this method are limited to a shallow network with only one learnable layer and cannot achieve satisfactory results when compared with multi-layer SNNs. In this paper, we eased this limitation by: 1)We proposed a Spiking Inception (Sp-Inception) module, inspired by the Inception module in the Artificial Neural Network (ANN) literature. This module is trained through STDP-based competitive learning and outperforms the baseline modules on learning capability, learning efficiency, and robustness. 2)We proposed a Pooling-Reshape-Activate (PRA) layer to make the Sp-Inception module stackable. 3)We stacked multiple Sp-Inception modules to construct multilayer SNNs. Our algorithm outperforms the baseline algorithms on the hand-written digit classification task, and reaches state-of-the-art results on the MNIST dataset among the existing unsupervised SNNs.
C1 [Meng, Mingyuan; Yang, Xingyu; Xiao, Shanlin; Yu, Zhiyi] Sun Yat Sen Univ, Sch Elect & Informat Technol, Guangzhou 510006, Peoples R China.
RP Xiao, SL; Yu, ZY (corresponding author), Sun Yat Sen Univ, Sch Elect & Informat Technol, Guangzhou 510006, Peoples R China.
EM mengmy3@mail.sysu.edu.cn; yangxy266@mail2.sysu.edu.cn;
   xiaoshlin@mail.sysu.edu.cn; yuzhiyi@mail.sysu.edu.cn
CR [Anonymous], 2018, IEEE J EM SEL TOP C, DOI DOI 10.1109/JETCAS.2017.2769684
   Bichler O, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P859, DOI 10.1109/IJCNN.2011.6033311
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Falez P, 2019, IEEE IJCNN
   Frackowiak Richard SJ, 2004, HUMAN BRAIN FUNCTION
   Goodman DFM, 2009, FRONT NEUROSCI-SWITZ, V3, P192, DOI 10.3389/neuro.01.026.2009
   Hazan H., 2018, 2018 INT JOINT C NEU, P1
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V116, P449, DOI 10.1113/jphysiol.1952.sp004717
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Lammie C, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351532
   LeCun Y., 1998, MNIST DATABASE HANDW, V10, P34
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Meng Mingyuan, 2019, ARXIV200101680
   Neil Daniel, 2016, P 31 ANN ACM S APPL
   Pfister JP, 2006, J NEUROSCI, V26, P9673, DOI 10.1523/JNEUROSCI.1425-06.2006
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Saunders DJ, 2019, NEURAL NETWORKS, V119, P332, DOI 10.1016/j.neunet.2019.08.016
   She XY, 2019, DES AUT TEST EUROPE, P450, DOI [10.23919/DATE.2019.8714846, 10.23919/date.2019.8714846]
   Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Wade JJ, 2010, IEEE T NEURAL NETWOR, V21, P1817, DOI 10.1109/TNN.2010.2074212
   Xing Fu, 2019, INT C NEUR INF PROC
NR 25
TC 4
Z9 5
U1 0
U2 0
PY 2020
DI 10.1109/ijcnn48605.2020.9207161
UT WOS:000626021404067
DA 2023-11-16
ER

PT C
AU Oltra-Oltra, JA
   Madrenas, J
   Zapata, M
   Vallejo, B
   Mata-Hernandez, D
   Sato, S
AF Angel Oltra-Oltra, Josep
   Madrenas, Jordi
   Zapata, Mireya
   Vallejo, Bernardo
   Mata-Hernandez, Diana
   Sato, Shigeo
GP IEEE
TI Hardware-Software Co-Design for Efficient and Scalable Real-Time
   Emulation of SNNs on the Edge
SO 2021 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS (ISCAS)
SE IEEE International Symposium on Circuits and Systems
DT Proceedings Paper
CT IEEE International Symposium on Circuits and Systems (IEEE ISCAS)
CY MAY 22-28, 2021
CL Daegu, SOUTH KOREA
DE Spiking Neural Networks; SNN; Edge Computing; Neural Computing;
   Hardware-Software Integration; SNAVA; HEENS
AB This paper introduces a novel workflow for Distributed Spiking Neural Network Architecture (DSNA). As such, the hardware implementation of Single Instruction Multiple Data (SIMD)-based Spiking Neural Network (SNN) requires the development of user-friendly and efficient toolchain in order to maximise the potential that the architecture brings. By using a novel SNN architecture, a custom designed hardware/software toolchain has been developed. The toolchain performance has been experimentally checked on a Band-Pass Filter (BPF), obtaining optimized code and data.
C1 [Angel Oltra-Oltra, Josep; Madrenas, Jordi; Vallejo, Bernardo; Mata-Hernandez, Diana] Univ Politecn Cataluna, Dept Elect Engn, Barcelona, Spain.
   [Zapata, Mireya] Univ Tecnol Indoamer, Res Ctr Mechatron & Interact Syst, Quito, Ecuador.
   [Sato, Shigeo] Tohoku Univ, Res Inst Elect Commun, Sendai, Miyagi, Japan.
RP Oltra-Oltra, JA (corresponding author), Univ Politecn Cataluna, Dept Elect Engn, Barcelona, Spain.
CR Balaji A, 2020, IEEE IJCNN, DOI 10.1109/ijcnn48605.2020.9207142
   Cassidy A. S., 2016, TRUENORTH HIGH PERFO
   Dang K. N., 2020, 2019 INT C INT THING, P155
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Dorta T, 2016, NEUROCOMPUTING, V171, P1684, DOI 10.1016/j.neucom.2015.07.080
   Furber SB, 2013, IEEE T COMPUT, V62, P2454, DOI 10.1109/TC.2012.142
   Hazan H, 2018, FRONT NEUROINFORM, V12, DOI 10.3389/fninf.2018.00089
   IEEE, 2020, PACK INT WHIT PAP
   Madrenas J., 2020, IEEE I C ELECT CIRC
   Oster M, 2005, LECT NOTES COMPUT SC, V3696, P161, DOI 10.1007/11550822_26
   Sripad A, 2018, NEURAL NETWORKS, V97, P28, DOI 10.1016/j.neunet.2017.09.011
NR 11
TC 1
Z9 1
U1 0
U2 1
PY 2021
DI 10.1109/ISCAS51556.2021.9401615
UT WOS:000706507900138
DA 2023-11-16
ER

PT C
AU Wróbel, B
AF Wrobel, Borys
BE Handl, J
   Hart, E
   Lewis, PR
   LopezIbanez, M
   Ochoa, G
   Paechter, B
TI Evolution of Spiking Neural Networks Robust to Noise and Damage for
   Control of Simple Animats
SO PARALLEL PROBLEM SOLVING FROM NATURE - PPSN XIV
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 14th International Conference on Parallel Problem Solving from Nature
   (PPSN)
CY SEP 17-21, 2016
CL Edinburgh, ENGLAND
DE Spiking neural networks; Adaptive exponential integrate-and-fire model;
   Genetic algorithm; Robustness to noise; Robustness to damage
AB One of the central questions of biology is how complex biological systems can continue functioning in the presence of perturbations, damage, and mutational insults. This paper investigates evolution of spiking neural networks, consisting of adaptive exponential neurons. The networks are encoded in linear genomes in a manner inspired by genetic networks. The networks control a simple animat, with two sensors and two actuators, searching for targets in a simple environment. The results show that the presence of noise on the membrane voltage during evolution allows for evolution of efficient control and robustness to perturbations to the value of the neural parameters of neurons.
C1 [Wrobel, Borys] IO PAN, Syst Modeling Grp, Sopot, Poland.
   [Wrobel, Borys] Adam Mickiewicz Univ, Evolutionary Syst Grp, Poznan, Poland.
RP Wróbel, B (corresponding author), IO PAN, Syst Modeling Grp, Sopot, Poland.; Wróbel, B (corresponding author), Adam Mickiewicz Univ, Evolutionary Syst Grp, Poznan, Poland.
EM wrobel@evosys.org
CR Braitenberg V., 1986, VEHICLES EXPT SYNTHE
   Brette R, 2005, J NEUROPHYSIOL, V94, P3637, DOI 10.1152/jn.00686.2005
   Joachimczak M, 2010, ARTIF LIFE, P348
   Joachimczak M, 2012, BIOSYSTEMS, V109, P498, DOI 10.1016/j.biosystems.2012.05.014
   Naud R, 2008, BIOL CYBERN, V99, P335, DOI 10.1007/s00422-008-0264-7
   Stromatias E, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00222
   Wagner A., 2013, ROBUSTNESS EVOLVABIL
   Wrobel B., 2014, SCI, V557, P187, DOI DOI 10.1007/978-3-642-55337-0_6
   Wrobel B., 2014, LNICST, V134, P135, DOI DOI 10.1007/978-3-319-06944-9_10
NR 9
TC 1
Z9 1
U1 0
U2 2
PY 2016
VL 9921
BP 686
EP 696
DI 10.1007/978-3-319-45823-6_64
UT WOS:000387962100064
DA 2023-11-16
ER

PT C
AU Dora, S
   Suresh, S
   Sundararajan, N
AF Dora, Shirin
   Suresh, S.
   Sundararajan, N.
GP IEEE
TI A Sequential Learning Algorithm for a Minimal Spiking Neural Network
   (MSNN) Classifier
SO PROCEEDINGS OF THE 2014 INTERNATIONAL JOINT CONFERENCE ON NEURAL
   NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUL 06-11, 2014
CL Beijing, PEOPLES R CHINA
ID GRADIENT DESCENT; NEURONS
AB In this paper, we develop a new sequential learning algorithm for a spiking neural network classifier. The algorithm handles the input features that are not in the form of a spike train but in a real-valued (analog) form. The sequential learning algorithm evolves the number of spiking neuron automatically based on the information present in the current sample and results in a compact architecture. Hence, it is referred to as a Minimal Spiking Neural Network (MSNN). The learning algorithm can either add a new neuron to the network or update the parameters of the existing neurons based on the information contained in the arriving samples. The update rule uses excitatory/inhibitatory rule to capture the knowledge contained in the current sample. Performance evaluation of the proposed MSNN is presented using two benchmark problems from the UCI machine learning repository, namely, the Iris flower classification and Wisconsin breast cancer problem and the results are compared with other existing spiking neural algorithms like SpikeProp, MuSpiNN and Multi-spike learning algorithms. The results clearly indicate the better performance of MSNN with a compact architecture.
C1 [Dora, Shirin; Suresh, S.] Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
   [Sundararajan, N.] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
RP Dora, S (corresponding author), Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
CR Abbott LF, 1999, BRAIN RES BULL, V50, P303, DOI 10.1016/S0361-9230(99)00161-6
   [Anonymous], 2013, INT JOINT C NEUR NET, DOI DOI 10.1109/AGILE.2013.7
   Belatreche A, 2007, SOFT COMPUT, V11, P239, DOI [10.1007/s00500-006-0065-7, 10.1007/S00500-006-0065-7]
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   Booij O, 2005, INFORM PROCESS LETT, V95, P552, DOI 10.1016/j.ipl.2005.05.023
   Florian RV, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0040233
   Ghosh-Dastidar S, 2007, INTEGR COMPUT-AID E, V14, P187
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Hough M., 1999, INT C ROB ART LIF
   Johansson RS, 2004, NAT NEUROSCI, V7, P170, DOI 10.1038/nn1177
   Kasabov N, 2013, NEURAL NETWORKS, V41, P188, DOI 10.1016/j.neunet.2012.11.014
   Lapique L., 1907, J PHYSL PATHOL GEN, V9, P620, DOI DOI 10.1007/S00422-007-0189-6
   Maass W, 1997, NEURAL COMPUT, V9, P279, DOI 10.1162/neco.1997.9.2.279
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maass W, 1996, ADV NEUR IN, V8, P211
   McKennoch S, 2006, IEEE IJCNN, P3970
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Schrauwen B, 2003, IEEE IJCNN, P2825
   Silva SM, 2005, PROCEEDINGS OF THE 2005 INTERNATIONAL CONFERENCE ON NEURAL NETWORKS AND BRAIN, VOLS 1-3, P1354
   THORPE SJ, 1989, CONNECTIONISM IN PERSPECTIVE, P63
   Victor JD, 1996, J NEUROPHYSIOL, V76, P1310, DOI 10.1152/jn.1996.76.2.1310
   Wade JJ, 2010, IEEE T NEURAL NETWOR, V21, P1817, DOI 10.1109/TNN.2010.2074212
   Xu Y, 2013, NEURAL NETWORKS, V43, P99, DOI 10.1016/j.neunet.2013.02.003
NR 25
TC 9
Z9 9
U1 0
U2 2
PY 2014
BP 2415
EP 2421
UT WOS:000371465702072
DA 2023-11-16
ER

PT J
AU Ocker, GK
   Josic, K
   Shea-Brown, E
   Buice, MA
AF Ocker, Gabriel Koch
   Josic, Kresimir
   Shea-Brown, Eric
   Buice, Michael A.
TI Linking structure and activity in nonlinear spiking networks
SO PLOS COMPUTATIONAL BIOLOGY
DT Article
ID STABILIZED SUPRALINEAR NETWORK; PRIMARY VISUAL-CORTEX; CORTICAL
   CIRCUITS; NEURAL-NETWORKS; FIRING RATE; INHIBITORY STABILIZATION;
   NEURONAL NETWORKS; FIELD-THEORY; DYNAMICS; PROPAGATION
AB Recent experimental advances are producing an avalanche of data on both neural connectivity and neural activity. To take full advantage of these two emerging datasets we need a framework that links them, revealing how collective neural activity arises from the structure of neural connectivity and intrinsic neural dynamics. This problem of structure-driven activity has drawn major interest in computational neuroscience. Existing methods for relating activity and architecture in spiking networks rely on linearizing activity around a central operating point and thus fail to capture the nonlinear responses of individual neurons that are the hallmark of neural information processing. Here, we overcome this limitation and present a new relationship between connectivity and activity in networks of nonlinear spiking neurons by developing a diagrammatic fluctuation expansion based on statistical field theory. We explicitly show how recurrent network structure produces pairwise and higher-order correlated activity, and how nonlinearities impact the networks' spiking activity. Our findings open new avenues to investigating how single-neuron nonlinearities-including those of different cell types-combine with connectivity to shape population activity and function.
C1 [Ocker, Gabriel Koch; Shea-Brown, Eric; Buice, Michael A.] Allen Inst Brain Sci, Seattle, WA 98109 USA.
   [Josic, Kresimir] Univ Houston, Dept Math, Houston, TX 77204 USA.
   [Josic, Kresimir] Univ Houston, Dept Biol & Biochem, Houston, TX USA.
   [Josic, Kresimir] Rice Univ, Dept Biosci, Houston, TX USA.
   [Shea-Brown, Eric; Buice, Michael A.] Univ Washington, Dept Appl Math, Seattle, WA 98195 USA.
   [Shea-Brown, Eric] Univ Washington, Dept Physiol & Biophys, Seattle, WA 98195 USA.
   [Shea-Brown, Eric] Univ Washington, UW Inst Neuroengn, Seattle, WA 98195 USA.
RP Buice, MA (corresponding author), Allen Inst Brain Sci, Seattle, WA 98109 USA.; Buice, MA (corresponding author), Univ Washington, Dept Appl Math, Seattle, WA 98195 USA.
EM michaelbu@alleninstitute.org
CR ABELES M, 1982, ISRAEL J MED SCI, V18, P83
   Ahmadian Y, 2013, NEURAL COMPUT, V25, P1994, DOI 10.1162/NECO_a_00472
   Aljadeff J, 2016, PHYS REV E, V93, DOI 10.1103/PhysRevE.93.022302
   Aljadeff J, 2015, PHYS REV LETT, V114, DOI 10.1103/PhysRevLett.114.088101
   [Anonymous], 1996, FOKKER PLANCK EQUATI
   [Anonymous], 1991, CORTICONICS
   Averbeck BB, 2006, NAT REV NEUROSCI, V7, P358, DOI 10.1038/nrn1888
   BIENENSTOCK EL, 1982, J NEUROSCI, V2, P32, DOI 10.1523/jneurosci.02-01-00032.1982
   Bremaud P, 1996, ANN PROBAB, V24, P1563
   Bressloff PC, 2015, J MATH NEUROSCI, V5, DOI 10.1186/s13408-014-0016-z
   Bressloff PC, 2009, SIAM J APPL MATH, V70, P1488, DOI 10.1137/090756971
   BRILLINGER DR, 1976, J ROY STAT SOC B MET, V38, P60
   Brunel N, 2000, J COMPUT NEUROSCI, V8, P183, DOI 10.1023/A:1008925309027
   Bruno RM, 2006, SCIENCE, V312, P1622, DOI 10.1126/science.1124593
   Buice MA, 2007, PHYS REV E, V75, DOI 10.1103/PhysRevE.75.051919
   Buice MA, 2010, NEURAL COMPUT, V22, P377, DOI 10.1162/neco.2009.02-09-960
   Buicel MA, 2013, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2013/03/P03003
   Cardy J., 2008, NONEQUILIBRIUM STAT
   Chow CC, 2015, J MATH NEUROSCI, V5, DOI 10.1186/s13408-015-0018-5
   Cossell L, 2015, NATURE, V518, P399, DOI 10.1038/nature14182
   de la Rocha J, 2007, NATURE, V448, P802, DOI 10.1038/nature06028
   Diesmann M, 1999, NATURE, V402, P529, DOI 10.1038/990101
   DOI M, 1976, J PHYS A-MATH GEN, V9, P1479, DOI 10.1088/0305-4470/9/9/009
   DOI M, 1976, J PHYS A-MATH GEN, V9, P1465, DOI 10.1088/0305-4470/9/9/008
   Doiron B, 2004, PHYS REV LETT, V93, DOI 10.1103/PhysRevLett.93.048101
   Doiron B, 2016, NAT NEUROSCI, V19, P383, DOI 10.1038/nn.4242
   Fourcaud N, 2002, NEURAL COMPUT, V14, P2057, DOI 10.1162/089976602320264015
   Franke F, 2016, NEURON, V89, P409, DOI 10.1016/j.neuron.2015.12.037
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   Gerstner W, 1996, NATURE, V383, P76, DOI 10.1038/383076a0
   GINZBURG I, 1994, PHYS REV E, V50, P3171, DOI 10.1103/PhysRevE.50.3171
   Goedeke S., 2016, ARXIV160301880
   Harish O, 2015, PLOS COMPUT BIOL, V11, DOI 10.1371/journal.pcbi.1004266
   HAWKES AG, 1971, BIOMETRIKA, V58, P83, DOI 10.1093/biomet/58.1.83
   HAWKES AG, 1974, J APPL PROBAB, V11, P493, DOI 10.2307/3212693
   HEEGER DJ, 1992, VISUAL NEUROSCI, V9, P181, DOI 10.1017/S0952523800009640
   Histed MH, 2014, P NATL ACAD SCI USA, V111, pE178, DOI 10.1073/pnas.1318750111
   Hu Y, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003469
   Hu Y, 2014, PHYS REV E, V89, DOI 10.1103/PhysRevE.89.032802
   Hu Y, 2013, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2013/03/P03012
   Iyer R, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003248
   Josic K, 2009, NEURAL COMPUT, V21, P2774, DOI 10.1162/neco.2009.10-08-879
   Jovanovic S, 2016, PLOS COMPUT BIOL, V12, DOI 10.1371/journal.pcbi.1004963
   Jovanovic S, 2015, PHYS REV E, V91, DOI 10.1103/PhysRevE.91.042802
   Kadmon J, 2015, PHYS REV X, V5, DOI 10.1103/PhysRevX.5.041030
   Kass R. E., 2014, ANAL NEURAL DATA
   Kumar A, 2010, NAT REV NEUROSCI, V11, P615, DOI 10.1038/nrn2886
   Ledoux E, 2011, FRONT COMPUT NEUROSC, V5, DOI 10.3389/fncom.2011.00025
   Lee WCA, 2016, NATURE, V532, P370, DOI 10.1038/nature17192
   Lefèvre A, 2007, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2007/07/P07024
   Litwin-Kumar A, 2016, J NEUROPHYSIOL, V115, P1399, DOI 10.1152/jn.00732.2015
   MARTIN PC, 1973, PHYS REV A, V8, P423, DOI 10.1103/PhysRevA.8.423
   Mastrogiuseppe F, 2016, ARXIV160504221
   Mattia M, 2002, PHYS REV E, V66, DOI 10.1103/PhysRevE.66.051917
   Miller KD, 2002, J NEUROPHYSIOL, V87, P653, DOI 10.1152/jn.00425.2001
   Moreno-Bote R, 2014, NAT NEUROSCI, V17, P1410, DOI 10.1038/nn.3807
   Morgenstern NA, 2016, NAT NEUROSCI, V19, P1034, DOI 10.1038/nn.4339
   Ocker GK, 2015, PLOS COMPUT BIOL, V11, DOI 10.1371/journal.pcbi.1004458
   OHIRA T, 1993, PHYS REV E, V48, P2259, DOI 10.1103/PhysRevE.48.2259
   Ostojic S, 2014, NAT NEUROSCI, V17, P594, DOI 10.1038/nn.3658
   Ozeki H, 2009, NEURON, V62, P578, DOI 10.1016/j.neuron.2009.03.028
   Panzeri S, 2015, TRENDS COGN SCI, V19, P162, DOI 10.1016/j.tics.2015.01.002
   PELITI L, 1985, J PHYS-PARIS, V46, P1469, DOI 10.1051/jphys:019850046090146900
   Perin R, 2011, P NATL ACAD SCI USA, V108, P5419, DOI 10.1073/pnas.1016051108
   Pernice V, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1002059
   Pfister JP, 2006, J NEUROSCI, V26, P9673, DOI 10.1523/JNEUROSCI.1425-06.2006
   Priebe NJ, 2005, NEURON, V45, P133, DOI 10.1016/j.neuron.2004.12.024
   Priebe NJ, 2006, NAT NEUROSCI, V9, P552, DOI 10.1038/nn1660
   Priebe NJ, 2004, NAT NEUROSCI, V7, P1113, DOI 10.1038/nn1310
   Reid R C, 2001, Prog Brain Res, V130, P141
   Renart A, 2010, SCIENCE, V327, P587, DOI 10.1126/science.1179850
   Reyes AD, 2003, NAT NEUROSCI, V6, P593, DOI 10.1038/nn1056
   Roxin A, 2011, FRONT COMPUT NEUROSC, V5, P1, DOI 10.3389/fncom.2011.00008
   Rubin DB, 2015, NEURON, V85, P402, DOI 10.1016/j.neuron.2014.12.026
   Saichev AI, 2011, EUR PHYS J B, V83, P271, DOI 10.1140/epjb/e2011-20298-3
   Salinas E, 2000, J NEUROSCI, V20, P6193, DOI 10.1523/JNEUROSCI.20-16-06193.2000
   SEJNOWSKI TJ, 1977, J MATH BIOL, V4, P303, DOI 10.1007/BF00275079
   Seriès P, 2004, NAT NEUROSCI, V7, P1129, DOI 10.1038/nn1321
   SOMPOLINSKY H, 1988, PHYS REV LETT, V61, P259, DOI 10.1103/PhysRevLett.61.259
   Song S, 2005, PLOS BIOL, V3, P507, DOI 10.1371/journal.pbio.0030068
   Stern M, 2014, PHYS REV E, V90, DOI 10.1103/PhysRevE.90.062710
   Tannenbaum NR, 2016, PLOS COMPUT BIOL, V12, DOI 10.1371/journal.pcbi.1005056
   Täuber UC, 2007, LECT NOTES PHYS, V716, P295, DOI [10.1007/3-540-69684-9̲7, 10.1007/3-540-69684-9]
   Trousdale J, 2012, PLOS COMPUT BIOL, V8, DOI 10.1371/journal.pcbi.1002408
   Tsodyks MV, 1997, J NEUROSCI, V17, P4382
   Usrey WM, 1998, NATURE, V395, P384, DOI 10.1038/26487
   van Vreeswijk C, 1998, NEURAL COMPUT, V10, P1321, DOI 10.1162/089976698300017214
   vanVreeswijk C, 1996, SCIENCE, V274, P1724, DOI 10.1126/science.274.5293.1724
   Yoshimura Y, 2005, NATURE, V433, P868, DOI 10.1038/nature03252
   Yoshimura Y, 2005, NAT NEUROSCI, V8, P1552, DOI 10.1038/nn1565
   Zhao Liqiong, 2011, Front Comput Neurosci, V5, P28, DOI 10.3389/fncom.2011.00028
   Zinn-Justin J., 2002, INT SERIES MONOGRAPH, V113
   Zylberberg J, 2016, NEURON, V89, P369, DOI 10.1016/j.neuron.2015.11.019
NR 93
TC 30
Z9 30
U1 0
U2 7
PD JUN
PY 2017
VL 13
IS 6
AR e1005583
DI 10.1371/journal.pcbi.1005583
UT WOS:000404565400035
DA 2023-11-16
ER

PT J
AU Hu, RH
   Huang, QJ
   Wang, H
   He, J
   Chang, S
AF Hu, Ruihan
   Huang, Qijun
   Wang, Hao
   He, Jin
   Chang, Sheng
TI Monitor-Based Spiking Recurrent Network for the Representation of
   Complex Dynamic Patterns
SO INTERNATIONAL JOURNAL OF NEURAL SYSTEMS
DT Article
DE Spiking recurrent network; monitor; online learning; robustness; storage
   capacity
ID ECHO STATE NETWORKS; NEURAL-NETWORK; STRUCTURAL OPTIMIZATION; MODEL;
   DESIGN; PREDICTION; NOISE; CHAOS
AB Neural networks are powerful computation tools for mimicking the human brain to solve realistic problems. Since spiking neural networks are a type of brain-inspired network, called the novel spiking system, Monitor-based Spiking Recurrent network (MbSRN), is derived to learn and represent patterns in this paper. This network provides a computational framework for memorizing the targets using a simple dynamic model that maintains biological plasticity. Based on a recurrent reservoir, the MbSRN presents a mechanism called a 'monitor' to track the components of the state space in the training stage online and to self-sustain the complex dynamics in the testing stage. The network firing spikes are optimized to represent the target dynamics according to the accumulation of the membrane potentials of the units. Stability analysis of the monitor conducted by limiting the coefficient penalty in the loss function verifies that our network has good anti-interference performance under neuron loss and noise. The results of solving some realistic tasks show that the MbSRN not only achieves a high goodness-of-fit of the target patterns but also maintains good spiking efficiency and storage capacity.
C1 [Hu, Ruihan; Huang, Qijun; Wang, Hao; He, Jin; Chang, Sheng] Wuhan Univ, Sch Phys & Technol, Wuhan 430072, Hubei, Peoples R China.
RP Chang, S (corresponding author), Wuhan Univ, Sch Phys & Technol, Wuhan 430072, Hubei, Peoples R China.
EM changsheng@whu.edu.cn
CR Adeli H, 1996, AI MAG, V17, P87
   ADELI H, 1995, COMPUT STRUCT, V57, P383, DOI 10.1016/0045-7949(95)00048-L
   Aldwaik M, 2014, STRUCT MULTIDISCIP O, V50, P899, DOI 10.1007/s00158-014-1148-1
   Alemi A, 2015, PLOS COMPUT BIOL, V11, DOI 10.1371/journal.pcbi.1004439
   Alomar ML, 2015, IEEE T CIRCUITS-II, V62, P977, DOI 10.1109/TCSII.2015.2458071
   Antonietti A, 2016, IEEE T BIO-MED ENG, V63, P210, DOI 10.1109/TBME.2015.2485301
   Bacciu D, 2017, IEEE IJCNN, P2080, DOI 10.1109/IJCNN.2017.7966106
   Bacciu D, 2014, NEURAL COMPUT APPL, V24, P1451, DOI 10.1007/s00521-013-1364-4
   Barrett DGT, 2016, ELIFE, V5, DOI 10.7554/eLife.12454
   Blankenburg S, 2016, MATH BIOSCI ENG, V13, P461, DOI 10.3934/mbe.2016001
   Bourdoukan R., 2012, ADV NEURAL INFORM PR, P2285
   BRITTEN KH, 1993, VISUAL NEUROSCI, V10, P1157, DOI 10.1017/S0952523800010269
   Brunner D, 2013, NAT COMMUN, V4, DOI 10.1038/ncomms2368
   Burger B., 2013, P 10 SOUND MUSIC COM, P172
   Cabessa J, 2014, INT J NEURAL SYST, V24, DOI 10.1142/S0129065714500294
   Casellato C, 2014, P IEEE RAS-EMBS INT, P813, DOI 10.1109/BIOROB.2014.6913879
   Denève S, 2016, NAT NEUROSCI, V19, P375, DOI 10.1038/nn.4243
   DePasquale B., ARXIV160107620
   Engel TA, 2015, NAT COMMUN, V6, DOI 10.1038/ncomms7454
   Fernández A, 2017, INT J NEURAL SYST, V27, DOI 10.1142/S0129065717500289
   Garrido JA, 2016, INT J NEURAL SYST, V26, DOI 10.1142/S0129065716500209
   Geminiani A., INT J NEURAL SYST
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Guo LL, 2017, INT J NEURAL SYST, V27, DOI 10.1142/S0129065717500022
   Hao X., 2015, IEEE T NEUR NET LEAR, V26, P1
   Jaeger H, 2007, NEURAL NETWORKS, V20, P287, DOI 10.1016/j.neunet.2007.04.001
   Jaeger H, 2017, J MACH LEARN RES, V18
   Jaegera H, 2007, NEURAL NETWORKS, V20, P335, DOI 10.1016/j.neunet.2007.04.016
   Palomo EJ, 2016, INT J NEURAL SYST, V26, DOI 10.1142/S0129065716500192
   Kempter R, 1999, PHYS REV E, V59, P4498, DOI 10.1103/PhysRevE.59.4498
   Knieling S, 2016, INT J NEURAL SYST, V26, DOI 10.1142/S0129065715500422
   Laje R, 2013, NAT NEUROSCI, V16, P925, DOI 10.1038/nn.3405
   Lee J, 2016, NEURON, V90, P165, DOI 10.1016/j.neuron.2016.02.012
   Legenstein R, 2007, NEURAL NETWORKS, V20, P323, DOI 10.1016/j.neunet.2007.04.017
   Lin P, 2019, NEURAL COMPUT APPL, V31, P3933, DOI 10.1007/s00521-017-3336-6
   Ling H, 2013, BIOSYSTEMS, V114, P191, DOI 10.1016/j.biosystems.2013.08.004
   Loewenstein Y, 2006, P NATL ACAD SCI USA, V103, P15224, DOI 10.1073/pnas.0505220103
   London M, 2010, NATURE, V466, P123, DOI 10.1038/nature09086
   López-Rubio E, 2018, INT J NEURAL SYST, V28, DOI 10.1142/S0129065717500563
   Lukosevicius M, 2012, KUNSTL INTELL, V26, P365, DOI 10.1007/s13218-012-0204-5
   Lukosevicius M, 2009, COMPUT SCI REV, V3, P127, DOI 10.1016/j.cosrev.2009.03.005
   Luque NR, 2016, FRONT COMPUT NEUROSC, V10, DOI 10.3389/fncom.2016.00017
   Martens J, 2011, P 28 INT C MACH LEAR, P1033, DOI DOI 10.1145/346152.346166
   Morro A, 2018, IEEE T NEUR NET LEAR, V29, P1371, DOI 10.1109/TNNLS.2017.2657601
   Nobukawa S, 2016, INT J NEURAL SYST, V26, DOI 10.1142/S0129065715500409
   Panakkat A, 2009, COMPUT-AIDED CIV INF, V24, P280, DOI 10.1111/j.1467-8667.2009.00595.x
   PARK HS, 1995, COMPUT STRUCT, V57, P391, DOI 10.1016/0045-7949(95)00047-K
   Park HS, 1997, J STRUCT ENG, V123, P880, DOI 10.1061/(ASCE)0733-9445(1997)123:7(880)
   Pierre E., 2016, PLOS COMPUT BIOL, V12
   Quintián H, 2017, INT J NEURAL SYST, V27, DOI 10.1142/S0129065717500241
   Rosselló JL, 2016, INT J NEURAL SYST, V26, DOI 10.1142/S0129065715500367
   Rosselló JL, 2014, INT J NEURAL SYST, V24, DOI 10.1142/S0129065714300034
   Roveri M, 2017, INT J NEURAL SYST, V27, DOI 10.1142/S0129065716500477
   Rumelhart D.E., 1986, PARALLEL DISTRIBUTED, V1, DOI [10.21236/ada164453, 10.1016/b978-1-4832-1446-7.50035-2]
   Senouci AB, 2001, J CONSTR ENG M, V127, P28, DOI 10.1061/(ASCE)0733-9364(2001)127:1(28)
   Sussillo D, 2013, NEURAL COMPUT, V25, P626, DOI 10.1162/NECO_a_00409
   Sussillo D, 2009, NEURON, V63, P544, DOI 10.1016/j.neuron.2009.07.018
   Tashakori A, 2002, J CONSTR STEEL RES, V58, P1545, DOI 10.1016/S0143-974X(01)00105-5
   Thalmeier D, 2016, PLOS COMPUT BIOL, V12, DOI 10.1371/journal.pcbi.1004895
   VanRullen R, 2005, TRENDS NEUROSCI, V28, P1, DOI 10.1016/j.tins.2004.10.010
   Wang J, 2013, IEEE T FUZZY SYST, V21, P209, DOI 10.1109/TFUZZ.2012.2208974
   Wang Z., 2014, INT J NEURAL SYST, V24, P1659
   Wei L., 2018, INT J NEURAL SYST, V28
   Weissenberger F, 2017, INT J NEURAL SYST, V27, DOI 10.1142/S0129065717500447
   Williams RJ, 1989, NEURAL COMPUT, V1, P270, DOI 10.1162/neco.1989.1.2.270
   Zhang ML, 2017, NEUROCOMPUTING, V219, P333, DOI 10.1016/j.neucom.2016.09.044
NR 66
TC 24
Z9 24
U1 3
U2 25
PD OCT
PY 2019
VL 29
IS 8
AR 1950006
DI 10.1142/S0129065719500060
UT WOS:000488036200004
DA 2023-11-16
ER

PT J
AU Hahne, J
   Dahmen, D
   Schuecker, J
   Frommer, A
   Bolten, M
   Helias, M
   Diesmann, M
AF Hahne, Jan
   Dahmen, David
   Schuecker, Jannis
   Frommer, Andreas
   Bolten, Matthias
   Helias, Moritz
   Diesmann, Markus
TI Integration of Continuous-Time Dynamics in a Spiking Neural Network
   Simulator
SO FRONTIERS IN NEUROINFORMATICS
DT Article
DE rate models; spiking neural network simulator; stochastic (delay)
   differential equations; waveform relaxation; parallelization;
   supercomputing
ID SPATIOTEMPORAL DYNAMICS; POPULATION-DYNAMICS; NEURONAL NETWORKS; MODEL;
   CONNECTIVITY; CORTEX; STATE; PROBABILITY; PERCEPTION; INHIBITION
AB Contemporary modeling approaches to the dynamics of neural networks include two important classes of models: biologically grounded spiking neuron models and functionally inspired rate-based units. We present a unified simulation framework that supports the combination of the two for multi-scale modeling, enables the quantitative validation of mean-field approaches by spiking network simulations, and provides an increase in reliability by usage of the same simulation code and the same network model specifications for both model classes. While most spiking simulations rely on the communication of discrete events, rate models require time-continuous interactions between neurons. Exploiting the conceptual similarity to the inclusion of gap junctions in spiking network simulations, we arrive at a reference implementation of instantaneous and delayed interactions between rate-based models in a spiking network simulator. The separation of rate dynamics from the general connection and communication infrastructure ensures flexibility of the framework. In addition to the standard implementation we present an iterative approach based on waveform-relaxation techniques to reduce communication and increase performance for large-scale simulations of rate-based models with instantaneous interactions. Finally we demonstrate the broad applicability of the framework by considering various examples from the literature, ranging from random networks to neural-field models. The study provides the prerequisite for interactions between rate-based and spiking models in a joint simulation.
C1 [Hahne, Jan; Frommer, Andreas; Bolten, Matthias] Berg Univ Wuppertal, Sch Math & Nat Sci, Wuppertal, Germany.
   [Dahmen, David; Schuecker, Jannis; Helias, Moritz; Diesmann, Markus] Julich Res Ctr, JARA BRAIN Inst 1, Inst Adv Simulat IAS 6, Inst Neurosci & Med INM 6, Julich, Germany.
   [Helias, Moritz; Diesmann, Markus] Rhein Westfal TH Aachen, Fac 1, Dept Phys, Aachen, Germany.
   [Diesmann, Markus] Rhein Westfal TH Aachen, Med Fac, Dept Psychiat Psychotherapy & Psychosomat, Aachen, Germany.
RP Hahne, J (corresponding author), Berg Univ Wuppertal, Sch Math & Nat Sci, Wuppertal, Germany.
EM hahne@math.uni-wuppertal.de
CR Abbott LF, 2016, NAT NEUROSCI, V19, P350, DOI 10.1038/nn.4241
   Adamu I. A., 2011, THESIS
   Al-Mohy AH, 2009, SIAM J MATRIX ANAL A, V31, P970, DOI 10.1137/09074721X
   AMARI SI, 1977, BIOL CYBERN, V27, P77, DOI 10.1007/BF00337259
   Amiri M, 2012, J THEOR BIOL, V292, P60, DOI 10.1016/j.jtbi.2011.09.013
   Amit DJ, 1997, CEREB CORTEX, V7, P237, DOI 10.1093/cercor/7.3.237
   [Anonymous], 1991, SOLVING ORDINARY DIF, DOI DOI 10.1007/978-3-662-09947-6
   [Anonymous], 1991, INTRO THEORY NEURAL
   [Anonymous], 2002, NEURAL SIMULATION LA
   [Anonymous], 1996, FOKKER PLANCK EQUATI
   [Anonymous], 1974, HDB MATH FUNCTIONS F, DOI DOI 10.5555/1098650
   [Anonymous], 2004, QUANTUM NOISE A HDB
   [Anonymous], 1986, LEARNING INTERNAL RE
   [Anonymous], METHODS NEURONAL MOD
   Bednar James A, 2009, Front Neuroinform, V3, P8, DOI 10.3389/neuro.11.008.2009
   Bos H., 2015, ZENODO, DOI [10.5281/zenodo.44222, DOI 10.5281/ZENODO.44222]
   Bos H, 2016, PLOS COMPUT BIOL, V12, DOI 10.1371/journal.pcbi.1005132
   Bower J. M., 2007, SCHOLARPEDIA, V2, P1383, DOI [10.4249/scholarpedia.1383, DOI 10.4249/SCHOLARPEDIA.1383]
   Bressloff PC, 2015, J MATH NEUROSCI, V5, DOI 10.1186/s13408-014-0016-z
   Bressloff PC, 2012, J PHYS A-MATH THEOR, V45, DOI 10.1088/1751-8113/45/3/033001
   Brette R, 2007, J COMPUT NEUROSCI, V23, P349, DOI 10.1007/s10827-007-0038-6
   Brette R, 2015, FRONT SYST NEUROSCI, V9, DOI 10.3389/fnsys.2015.00151
   Brunel N, 1999, NEURAL COMPUT, V11, P1621, DOI 10.1162/089976699300016179
   Brunel N, 2000, J COMPUT NEUROSCI, V8, P183, DOI 10.1023/A:1008925309027
   Buice MA, 2007, PHYS REV E, V75, DOI 10.1103/PhysRevE.75.051919
   Buice MA, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1002872
   Buice MA, 2010, NEURAL COMPUT, V22, P377, DOI 10.1162/neco.2009.02-09-960
   Buice MA, 2007, PHYS REV E, V76, DOI 10.1103/PhysRevE.76.031118
   Cain N, 2016, PLOS COMPUT BIOL, V12, DOI 10.1371/journal.pcbi.1005045
   Carnevale T.N., 2006, NEURON BOOK
   Chaudhuri R, 2015, NEURON, V88, P419, DOI 10.1016/j.neuron.2015.09.008
   Cichocki A., 2009, NONNEGATIVE MATRIX T, DOI DOI 10.1002/9780470747278
   Coombes S, 2005, BIOL CYBERN, V93, P91, DOI 10.1007/s00422-005-0574-y
   Dahmen D, 2016, PHYS REV X, V6, DOI 10.1103/PhysRevX.6.031024
   Davison Andrew P, 2008, Front Neuroinform, V2, P11, DOI 10.3389/neuro.11.011.2008
   de Kamps M., 2013, 13091654V2QBIONC ARX
   de Kamps M, 2008, NEURAL NETWORKS, V21, P1164, DOI 10.1016/j.neunet.2008.07.006
   Deadman E., 2012, 11 INT C PARA 2012, P171, DOI DOI 10.1007/978-3-642-36803-5_12
   Deco G, 2012, J NEUROSCI, V32, P3366, DOI 10.1523/JNEUROSCI.2523-11.2012
   Deco G, 2011, NAT REV NEUROSCI, V12, P43, DOI 10.1038/nrn2961
   Deco G, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.1000092
   Destexhe A, 1999, J NEUROPHYSIOL, V81, P1531, DOI 10.1152/jn.1999.81.4.1531
   Djurfeldt M, 2008, IBM J RES DEV, V52, P31, DOI 10.1147/rd.521.0031
   Djurfeldt M, 2014, FRONT NEUROINFORM, V8, DOI 10.3389/fninf.2014.00043
   Djurfeldt M, 2010, NEUROINFORMATICS, V8, P43, DOI 10.1007/s12021-010-9064-z
   Ecker AS, 2010, SCIENCE, V327, P584, DOI 10.1126/science.1179867
   Eppler Jochen Martin, 2008, Front Neuroinform, V2, P12, DOI 10.3389/neuro.11.012.2008
   Fan ZC, 2013, APPL MATH COMPUT, V219, P4992, DOI 10.1016/j.amc.2012.11.055
   FELDMAN JA, 1982, COGNITIVE SCI, V6, P205, DOI 10.1207/s15516709cog0603_1
   Fourcaud N, 2002, NEURAL COMPUT, V14, P2057, DOI 10.1162/089976602320264015
   GALASSI M, 2006, GNU SCI LIB REFERENC
   Gancarz G, 1998, NEURAL NETWORKS, V11, P1159, DOI 10.1016/S0893-6080(98)00096-3
   Gentet LJ, 2010, NEURON, V65, P422, DOI 10.1016/j.neuron.2010.01.006
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   GERSTNER W, 1995, PHYS REV E, V51, P738, DOI 10.1103/PhysRevE.51.738
   Gerstner W, 2000, NEURAL COMPUT, V12, P43, DOI 10.1162/089976600300015899
   GINZBURG I, 1994, PHYS REV E, V50, P3171, DOI 10.1103/PhysRevE.50.3171
   Goedeke S., 2016, ARXIV160301880V1QBIO
   Goodman D., 2013, SCHOLARPEDIA, V8, P10883, DOI [10.4249/scholarpedia.10883, DOI 10.4249/SCHOLARPEDIA.10883]
   GRIFFITH JS, 1966, J PHYSIOL-LONDON, V186, P516, DOI 10.1113/jphysiol.1966.sp008053
   GROSSBERG S, 1973, STUD APPL MATH, V52, P213
   Grytskyy D, 2013, FRONT COMPUT NEUROSC, V7, DOI 10.3389/fncom.2013.00131
   Hahne J, 2015, FRONT NEUROINFORM, V9, DOI 10.3389/fninf.2015.00022
   Hanuschkin Alexander, 2010, Front Neuroinform, V4, P113, DOI 10.3389/fninf.2010.00113
   Haykin S., 2009, NEURAL NETWORKS LEAR
   Helias M, 2013, NEW J PHYS, V15, DOI 10.1088/1367-2630/15/2/023002
   Helias M, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003428
   Helias M, 2012, FRONT NEUROINFORM, V6, DOI 10.3389/fninf.2012.00026
   Helias M, 2008, FRONT COMPUT NEUROSC, V2, DOI 10.3389/neuro.10.007.2008
   Hines M, 2011, FRONT COMPUT NEUROSC, V5, DOI 10.3389/fncom.2011.00049
   Hines ML, 2008, J COMPUT NEUROSCI, V25, P203, DOI 10.1007/s10827-007-0073-3
   Julich Supercomputing Centre, 2015, JLSRF, V1, DOI [10.17815/jlsrf-1-18, DOI 10.17815/JLSRF-1-18]
   Jung, 2015, ENCY COMPUTATIONAL N, P1849, DOI [DOI 10.1007/SPRINGERREFERENCE_348323, 10.1007/978-1-4614-6675-8_258, DOI 10.1007/978-1-4614-6675-8_258]
   Kelley C.T., 1995, ITERATIVE METHODS LI
   Kilpatrick ZP, 2014, ENCY COMPUTATIONAL N, P3159, DOI 10.1007/978-1-4614-7320-6_80-1
   Kloeden PE, 2011, NUMERICAL SOLUTION S
   KNIGHT BW, 1972, J GEN PHYSIOL, V59, P734, DOI 10.1085/jgp.59.6.734
   KOCH KW, 1989, EXP BRAIN RES, V76, P292
   Komori Y, 2014, BIT, V54, P1067, DOI 10.1007/s10543-014-0485-1
   Kriener B, 2014, FRONT COMPUT NEUROSC, V8, DOI 10.3389/fncom.2014.00136
   Kumar Sameer, 2010, Proc IPDPS (Conf), V2010, P1, DOI 10.1155/2010/819029
   Kunkel S, 2014, FRONT NEUROINFORM, V8, DOI 10.3389/fninf.2014.00078
   Kunkel S, 2011, FRONT COMPUT NEUROSC, V4, DOI 10.3389/fncom.2010.00160
   Lelarasmee E., 1982, M8240 UCBERL, DOI [10.1109/tcad.1982.1270004, DOI 10.1109/TCAD.1982.1270004]
   Leon PS, 2013, FRONT NEUROINFORM, V7, DOI 10.3389/fninf.2013.00010
   Lindner B, 2005, PHYS REV E, V72, DOI 10.1103/PhysRevE.72.061919
   MALACH R, 1993, P NATL ACAD SCI USA, V90, P10469, DOI 10.1073/pnas.90.22.10469
   Marc-Oliver G., 2007, SCHOLARPEDIA, DOI [DOI 10.4249/SCHOLARPEDIA.1430, 10.4249/scholarpedia.1430]
   Markram H, 2015, CELL, V163, P456, DOI 10.1016/j.cell.2015.09.029
   Mattia M, 2004, PHYS REV E, V70, DOI 10.1103/PhysRevE.70.052903
   Mattia M, 2002, PHYS REV E, V66, DOI 10.1103/PhysRevE.66.051917
   MCCLELLAND JL, 1981, PSYCHOL REV, V88, P375, DOI 10.1037/0033-295X.88.5.375
   McClelland JL., 1989, EXPLORATIONS PARALLE
   Meyer C, 2002, NEURAL COMPUT, V14, P369, DOI 10.1162/08997660252741167
   MIEKKALA U, 1987, SIAM J SCI STAT COMP, V8, P459, DOI 10.1137/0908046
   Miyazaki H, 2012, FUJITSU SCI TECH J, V48, P255
   Montbrió E, 2015, PHYS REV X, V5, DOI 10.1103/PhysRevX.5.021028
   Morrison A, 2005, NEURAL COMPUT, V17, P1776, DOI 10.1162/0899766054026648
   Morrison Abigail, 2008, P267
   Nichols EJ, 2015, FRONT NEUROINFORM, V9, DOI 10.3389/fninf.2015.00025
   O'reilly R.C., 2000, COMPUTATIONAL EXPLOR
   O'Reilly R. C., 2014, COMP NEURAL NETWORK
   Ohbayashi M, 2003, SCIENCE, V301, P233, DOI 10.1126/science.1084884
   Okun M, 2008, NAT NEUROSCI, V11, P535, DOI 10.1038/nn.2105
   OReilly R. C., 2012, COMPUTATIONAL COGNIT
   Ostojic S, 2014, NAT NEUROSCI, V17, P594, DOI 10.1038/nn.3658
   Ostojic S, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1001056
   Pernice V, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1002059
   Plotnikov D., 2016, LNI, V254, P93
   Potjans TC, 2014, CEREB CORTEX, V24, P785, DOI 10.1093/cercor/bhs358
   Rajan K, 2006, PHYS REV LETT, V97, DOI 10.1103/PhysRevLett.97.188104
   Renart A, 2010, SCIENCE, V327, P587, DOI 10.1126/science.1179850
   Rougier NP, 2012, NETWORK-COMP NEURAL, V23, P237, DOI 10.3109/0954898X.2012.721573
   Roxin A, 2006, PROG THEOR PHYS SUPP, P68, DOI 10.1143/PTPS.161.68
   Roxin A, 2005, PHYS REV LETT, V94, DOI 10.1103/PhysRevLett.94.238103
   Roxin A, 2011, J NEUROSCI, V31, P16217, DOI 10.1523/JNEUROSCI.1677-11.2011
   Sadeh S, 2015, PLOS COMPUT BIOL, V11, DOI 10.1371/journal.pcbi.1004045
   Schmidt M., 2016, ARXIV151109364V3
   Schoner G., 2015, OXFORD SERIES DEV CO
   Schuecker J., 2016, ARXIV160506758CONDMA
   Schuecker J, 2017, PLOS COMPUT BIOL, V13, DOI 10.1371/journal.pcbi.1005179
   Schuecker J, 2015, PHYS REV E, V92, DOI 10.1103/PhysRevE.92.052119
   Schurz H, 2006, INT J NUMER ANAL MOD, V3, P232
   Schwalger T., 2016, 161100294 ARXIV
   Shadlen MN, 1998, J NEUROSCI, V18, P3870
   Shoji I, 2011, COMMUN NONLINEAR SCI, V16, P2667, DOI 10.1016/j.cnsns.2010.09.008
   SIEGERT AJF, 1951, PHYS REV, V81, P617, DOI 10.1103/PhysRev.81.617
   SOFTKY WR, 1993, J NEUROSCI, V13, P334
   SOMPOLINSKY H, 1988, PHYS REV LETT, V61, P259, DOI 10.1103/PhysRevLett.61.259
   Stern M, 2014, PHYS REV E, V90, DOI 10.1103/PhysRevE.90.062710
   Tetzlaff T, 2012, PLOS COMPUT BIOL, V8, DOI 10.1371/journal.pcbi.1002596
   Trousdale J, 2012, PLOS COMPUT BIOL, V8, DOI 10.1371/journal.pcbi.1002408
   van Albada SJ, 2015, PLOS COMPUT BIOL, V11, DOI 10.1371/journal.pcbi.1004490
   vanVreeswijk C, 1996, SCIENCE, V274, P1724, DOI 10.1126/science.274.5293.1724
   Voges N, 2012, FRONT COMPUT NEUROSC, V6, DOI 10.3389/fncom.2012.00041
   Voges N, 2010, PROG NEUROBIOL, V92, P277, DOI 10.1016/j.pneurobio.2010.05.001
   WILSON HR, 1972, BIOPHYS J, V12, P1, DOI 10.1016/S0006-3495(72)86068-5
   WILSON HR, 1973, KYBERNETIK, V13, P55, DOI 10.1007/BF00288786
   Wong KF, 2006, J NEUROSCI, V26, P1314, DOI 10.1523/JNEUROSCI.3733-05.2006
   Yger P, 2011, J COMPUT NEUROSCI, V31, P229, DOI 10.1007/s10827-010-0310-z
NR 140
TC 14
Z9 14
U1 0
U2 12
PD MAY 24
PY 2017
VL 11
AR 34
DI 10.3389/fninf.2017.00034
UT WOS:000402096400001
DA 2023-11-16
ER

PT J
AU Vidybida, A
AF Vidybida, Alexander
TI TESTING OF INFORMATION CONDENSATION IN A MODEL REVERBERATING SPIKING
   NEURAL NETWORK
SO INTERNATIONAL JOURNAL OF NEURAL SYSTEMS
DT Article
DE Neural network; information condensation; periodic dynamics; attractor;
   conceptual domain; binding neuron
ID OPTIMIZATION; EXTRACTION; SEQUENCES; MULTIPLE; ENTITIES; SPEED
AB Information about external world is delivered to the brain in the form of structured in time spike trains. During further processing in higher areas, information is subjected to a certain condensation process, which results in formation of abstract conceptual images of external world, apparently, represented as certain uniform spiking activity partially independent on the input spike trains details. Possible physical mechanism of condensation at the level of individual neuron was discussed recently. In a reverberating spiking neural network, due to this mechanism the dynamics should settle down to the same uniform/periodic activity in response to a set of various inputs. Since the same periodic activity may correspond to different input spike trains, we interpret this as possible candidate for information condensation mechanism in a network. Our purpose is to test this possibility in a network model consisting of five fully connected neurons, particularly, the influence of geometric size of the network, on its ability to condense information. Dynamics of 20 spiking neural networks of different geometric sizes are modelled by means of computer simulation. Each network was propelled into reverberating dynamics by applying various initial input spike trains. We run the dynamics until it becomes periodic. The Shannon's formula is used to calculate the amount of information in any input spike train and in any periodic state found. As a result, we obtain explicit estimate of the degree of information condensation in the networks, and conclude that it depends strongly on the net's geometric size.
C1 Bogolyubov Inst Theoret Phys, Dept Synerget, UA-03680 Kiev, Ukraine.
RP Vidybida, A (corresponding author), Bogolyubov Inst Theoret Phys, Dept Synerget, Metrologichna Str 14-B, UA-03680 Kiev, Ukraine.
EM vidybida@bitp.kiev.ua
CR Acharya R, 2010, INT J NEURAL SYST, V20, P509, DOI 10.1142/S0129065710002589
   Bullier J, 2001, BRAIN RES REV, V36, P96, DOI 10.1016/S0165-0173(01)00085-6
   Buonomano DV, 1998, ANNU REV NEUROSCI, V21, P149, DOI 10.1146/annurev.neuro.21.1.149
   Cariani P, 2001, J NEW MUSIC RES, V30, P107, DOI 10.1076/jnmr.30.2.107.7115
   Cessac B, 2008, J MATH BIOL, V56, P311, DOI 10.1007/s00285-007-0117-3
   Cromer JA, 2010, NEURON, V66, P796, DOI 10.1016/j.neuron.2010.05.005
   Damasio AR, 1989, NEURAL COMPUT, V1, P123, DOI 10.1162/neco.1989.1.1.123
   DuchampViret P, 1997, PROG NEUROBIOL, V53, P561, DOI 10.1016/S0301-0082(97)00049-X
   DURBIN R, 1990, NATURE, V343, P644, DOI 10.1038/343644a0
   ECKHORN R, 1988, BIOL CYBERN, V60, P121, DOI 10.1007/BF00202899
   ENGEL AK, 1991, NONLIN SYST, V2, P3
   Feldman J, 2010, COGN NEURODYNAMICS, V4, P25, DOI 10.1007/s11571-009-9090-4
   Ghosh-Dastidar S, 2007, INTEGR COMPUT-AID E, V14, P187
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Gorban AN, 2010, INT J NEURAL SYST, V20, P219, DOI 10.1142/S0129065710002383
   Haken H., 2007, BRAIN DYNAMICS SYNCH
   Iglesias J, 2008, INT J NEURAL SYST, V18, P267, DOI 10.1142/S0129065708001580
   Johnston SP, 2010, INT J NEURAL SYST, V20, P447, DOI 10.1142/S0129065710002541
   Kandel E. R., 2000, PRINCIPLES NEURAL SC, V4
   Kirchner H, 2006, VISION RES, V46, P1762, DOI 10.1016/j.visres.2005.10.002
   König P, 2006, BIOL CYBERN, V94, P325, DOI [10.1007/s00422-006-0050-3, 10.1007/S00422-006-0050-3]
   Konig P, 1996, TRENDS NEUROSCI, V19, P130, DOI 10.1016/S0166-2236(96)80019-1
   MacKay D.M., 1962, Self-Organizing Systems 1962, P37, DOI 10.1016/j.biosystems.2017.07.008
   MacKAY DONALD M., 1952, BULL MATH BIOPHYS, V14, P127, DOI 10.1007/BF02477711
   Nichols E, 2010, INT J NEURAL SYST, V20, P501, DOI 10.1142/S0129065710002577
   Passaglia CL, 2004, J NEUROPHYSIOL, V91, P1217, DOI 10.1152/jn.00796.2003
   Rudolph M, 2003, J COMPUT NEUROSCI, V14, P239, DOI 10.1023/A:1023245625896
   Scharf MT, 2002, J NEUROPHYSIOL, V87, P2770, DOI 10.1152/jn.2002.87.6.2770
   Schliebs S, 2010, INT J NEURAL SYST, V20, P481, DOI 10.1142/S0129065710002565
   SHANNON CE, 1948, BELL SYST TECH J, V27, P379, DOI 10.1002/j.1538-7305.1948.tb01338.x
   SHANNON CE, 1948, BELL SYST TECH J, V27, P623, DOI 10.1002/j.1538-7305.1948.tb00917.x
   Siegel M, 2009, P NATL ACAD SCI USA, V106, P21341, DOI 10.1073/pnas.0908193106
   Soltic S, 2010, INT J NEURAL SYST, V20, P437, DOI 10.1142/S012906571000253X
   Strain TJ, 2010, INT J NEURAL SYST, V20, P463, DOI 10.1142/S0129065710002553
   Tang SM, 2004, SCIENCE, V305, P1020, DOI 10.1126/science.1099839
   Thorpe S, 1996, NATURE, V381, P520, DOI 10.1038/381520a0
   Vidybida AK, 2008, EUR PHYS J B, V65, P577, DOI 10.1140/epjb/e2008-00360-1
   Vidybida AK, 2009, EUR PHYS J B, V69, P313, DOI 10.1140/epjb/e2009-00139-x
   Vidybida AK, 1998, BIOSYSTEMS, V48, P263
   Vidybida AK, 1996, BIOL CYBERN, V74, P537, DOI 10.1007/BF00209424
   VIDYBIDA AK, 1996, P BIONET 96 BIOL INF, P96
   Vogel EK, 2009, P NATL ACAD SCI USA, V106, P21017, DOI 10.1073/pnas.0912084107
   Wehr M, 1996, NATURE, V384, P162, DOI 10.1038/384162a0
   Widrow B., 1962, SELF ORG SYSTEMS, P435
   Wu W, 2009, INT J NEURAL SYST, V19, P425, DOI 10.1142/S0129065709002129
NR 46
TC 19
Z9 19
U1 0
U2 5
PD JUN
PY 2011
VL 21
IS 3
BP 187
EP 198
DI 10.1142/S0129065711002742
UT WOS:000293293900002
DA 2023-11-16
ER

PT J
AU Lee, KH
   Kwon, D
   Woo, SY
   Ko, JH
   Choi, WY
   Park, BG
   Lee, JH
AF Lee, Kyu-Ho
   Kwon, Dongseok
   Woo, Sung Yun
   Ko, Jong Hyun
   Choi, Woo Young
   Park, Byung-Gook
   Lee, Jong-Ho
TI Highly Linear Analog Spike Processing Block Integrated With an AND-Type
   Flash Array and CMOS Neuron Circuits
SO IEEE TRANSACTIONS ON ELECTRON DEVICES
DT Article
DE AND-type array; charge-trap flash (CTF); integrate-and-fire(IF) neuron;
   neuromorphic; spike processing block (SPB); spiking neural networks
   (SNNs)
ID LOW-POWER; NETWORKS; DEVICE
AB In this article, a highly linear spike processing block (SPB) integrating AND-type charge-trap flash (CTF) synapse array (25 x 4 synapses) and CMOS integrate-and-fire (IF) neurons is fabricated for hardware-based spiking neural networks (SNNs). We investigate the synaptic behavior of the CTF cells and the operating principle of the neuron circuits. Under the given operating conditions, the fabricated SPB consistently exhibits a highly linear relationship (R-2 > 0.999) between the current sum and the output spike frequency, enabling the SNNs to precisely mimic the layer of artificial neural networks (ANNs) with rectified linear unit (ReLU) activation function. Based on the fabricated SPB, a single-layer SNN is experimentally demonstrated for classifying the 5 x 5 digit patterns.
C1 [Lee, Kyu-Ho; Kwon, Dongseok; Woo, Sung Yun; Ko, Jong Hyun; Choi, Woo Young; Park, Byung-Gook; Lee, Jong-Ho] Seoul Natl Univ, Sch Elect & Comp Engn, Seoul 151742, South Korea.
   Seoul Natl Univ, Interuniv Semicond Res Ctr ISRC, Seoul 151742, South Korea.
RP Lee, JH (corresponding author), Seoul Natl Univ, Sch Elect & Comp Engn, Seoul 151742, South Korea.
EM jhl@snu.ac.kr
CR Ambrogio S, 2018, NATURE, V558, P60, DOI 10.1038/s41586-018-0180-5
   Pérez-Carrasco JA, 2013, IEEE T PATTERN ANAL, V35, P2706, DOI 10.1109/TPAMI.2013.71
   Chai Z, 2018, IEEE ELECTR DEVICE L, V39, P1652, DOI 10.1109/LED.2018.2869072
   Chakraborty I, 2020, P IEEE, V108, P2276, DOI 10.1109/JPROC.2020.3003007
   Devlin J., 2018, ARXIV, DOI 10.18653/v1/N19-1423
   Diehl P.U., 2015, 2015 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2015.7280696
   Diehl PU, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON REBOOTING COMPUTING (ICRC)
   Diorio C, 1996, IEEE T ELECTRON DEV, V43, P1972, DOI 10.1109/16.543035
   Dutta S, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-07418-y
   Iandola Forrest N., 2016, P IEEE C COMPUTER VI
   Indiveri G, 2006, IEEE T NEURAL NETWOR, V17, P211, DOI 10.1109/TNN.2005.860850
   Kang WM, 2019, IEEE IJCNN
   Kim CH, 2018, IEEE T ELECTRON DEV, V65, P1774, DOI 10.1109/TED.2018.2817266
   Kwon D, 2021, IEEE T ELECTRON DEV, V68, P4766, DOI 10.1109/TED.2021.3098503
   Kwon D, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00423
   Liang J, 2010, IEEE T ELECTRON DEV, V57, P2531, DOI 10.1109/TED.2010.2062187
   Lin J, 2018, IEEE T BIOMED CIRC S, V12, P1004, DOI 10.1109/TBCAS.2018.2843286
   Lu AN, 2021, 2021 IEEE 3RD INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE CIRCUITS AND SYSTEMS (AICAS), DOI 10.1109/AICAS51828.2021.9458501
   Luo J, 2019, INT EL DEVICES MEET, DOI 10.1109/iedm19573.2019.8993535
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Malavena G, 2018, INT EL DEVICES MEET
   Merrikh-Bayat F, 2018, IEEE T NEUR NET LEAR, V29, P4782, DOI 10.1109/TNNLS.2017.2778940
   Noh Y, 2019, IEEE INT MEM WORKSH, P165, DOI 10.1109/imw.2019.8739698
   Oh S, 2021, NAT NANOTECHNOL, V16, P680, DOI 10.1038/s41565-021-00874-8
   Prezioso M, 2016, IEEE INT SYMP CIRC S, P177, DOI 10.1109/ISCAS.2016.7527199
   Riggert C, 2014, SEMICOND SCI TECH, V29, DOI 10.1088/0268-1242/29/10/104011
   Rueckauer B, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351295
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Sidler S, 2017, LECT NOTES COMPUT SC, V10613, P281, DOI 10.1007/978-3-319-68600-4_33
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Tavanaei A, 2019, NEUROCOMPUTING, V330, P39, DOI 10.1016/j.neucom.2018.11.014
   Wang Z, 2019, IEEE J EXPLOR SOLID-, V5, P151, DOI 10.1109/JXCDC.2019.2928769
   Woo SY, 2020, IEEE ACCESS, V8, P202639, DOI 10.1109/ACCESS.2020.3036088
   Woo SY, 2020, SOLID STATE ELECTRON, V165, DOI 10.1016/j.sse.2019.107741
   Yu SM, 2018, P IEEE, V106, P260, DOI 10.1109/JPROC.2018.2790840
   Zhang QT, 2018, NEURAL NETWORKS, V108, P217, DOI 10.1016/j.neunet.2018.08.012
NR 36
TC 3
Z9 3
U1 1
U2 6
PD NOV
PY 2022
VL 69
IS 11
BP 6065
EP 6071
DI 10.1109/TED.2022.3207707
EA SEP 2022
UT WOS:000865062300001
DA 2023-11-16
ER

PT C
AU Auge, D
   Hille, J
   Kreutz, F
   Mueller, E
   Knoll, A
AF Auge, Daniel
   Hille, Julian
   Kreutz, Felix
   Mueller, Etienne
   Knoll, Alois
BE Farkas, I
   Masulli, P
   Otte, S
   Wermter, S
TI End-to-End Spiking Neural Network for Speech Recognition Using
   Resonating Input Neurons
SO ARTIFICIAL NEURAL NETWORKS AND MACHINE LEARNING, ICANN 2021, PT V
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 30th International Conference on Artificial Neural Networks (ICANN)
CY SEP 14-17, 2021
CL ELECTR NETWORK
DE Spiking neural networks; Speech processing; Keyword detection
AB The growing demand for complex computations in edge devices requires the development of algorithms and hardware accelerators that are powerful while remaining energy-efficient. A possible solution are spiking neural networks, as they have been demonstrated to be energy-efficient in several data processing and classification tasks when executed on specialized neuromorphic hardware. In the field of speech processing, they are especially suited for the online classification of audio streams due to their strong temporal affinity. However, so far, there has been a lack of emphasis on small-scale networks that will ultimately fit into restricted neuromorphic implementations. We propose the use of resonating neurons as an input layer to spiking neural networks for online audio classification to enable an end-to-end solution. We compare different architectures to the established method of using mel-frequency-based spectral features. With our approach, spiking neural networks can be directly used without additional preprocessing, thereby making them suitable for simple continuous low-power analysis of audio streams. We compare the classification accuracy of different network architectures with ours in a keyword spotting benchmark to demonstrate the performance of our approach.
C1 [Auge, Daniel; Hille, Julian; Mueller, Etienne; Knoll, Alois] Tech Univ Munich, Dept Informat, Munich, Germany.
   [Hille, Julian] Infineon Technol AG, Munich, Germany.
   [Kreutz, Felix] Infineon Technol Dresden GmbH & Co KG, Dresden, Germany.
RP Auge, D (corresponding author), Tech Univ Munich, Dept Informat, Munich, Germany.
EM daniel.auge@tum.de; knoll@in.tum.de
CR Abdel-Hamid O, 2012, INT CONF ACOUST SPEE, P4277, DOI 10.1109/ICASSP.2012.6288864
   Auge D., 2020, RESONATE FIRE NEURON
   Banbury C., 2020, ARXIV PREPRINT ARXIV
   Bellec G., 2018, ADV NEURAL INFORM PR
   Blouw P, 2020, INT CONF ACOUST SPEE, P8534, DOI [10.1109/ICASSP40776.2020.9053043, 10.1109/icassp40776.2020.9053043]
   Blouw P, 2020, PROCEEDINGS OF THE 2019 7TH ANNUAL NEURO-INSPIRED COMPUTATIONAL ELEMENTS WORKSHOP (NICE 2019), DOI 10.1145/3320288.3320304
   Chan V, 2007, IEEE T CIRCUITS-I, V54, P48, DOI 10.1109/TCSI.2006.887979
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Eldan R., 2016, C LEARN THEOR, P907
   Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947
   Izhikevich EM, 2001, NEURAL NETWORKS, V14, P883, DOI 10.1016/S0893-6080(01)00078-8
   Kim T, 2019, IEEE J-STSP, V13, P285, DOI 10.1109/JSTSP.2019.2909479
   Kumatani K, 2017, 2017 IEEE AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING WORKSHOP (ASRU), P252, DOI 10.1109/ASRU.2017.8268943
   Lee Jongpil, 2017, SAMPLE LEVEL DEEP CO
   Mayr C., 2019, ARXIV191102385
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Ostrau C, 2020, LECT NOTES COMPUT SC, V12397, P610, DOI 10.1007/978-3-030-61616-8_49
   Pellegrini T., 2020, ARXIV PREPRINT ARXIV
   Rybakov O, 2020, INTERSPEECH, P2277, DOI 10.21437/Interspeech.2020-1003
   Sainath TN, 2017, IEEE-ACM T AUDIO SPE, V25, P965, DOI 10.1109/TASLP.2017.2672401
   Sheik S, 2012, FRONT NEUROSCI-SWITZ, V6, DOI 10.3389/fnins.2012.00017
   Warden Pete, 2018, ARXIV180403209
   Wu JB, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00199
   Yilmaz E, 2020, INTERSPEECH, P2557, DOI 10.21437/Interspeech.2020-1230
   Yin B, 2020, IEEE INTERNET THINGS, V7, P8748, DOI [10.1109/JIOT.2020.2996562, 10.1145/3407197.3407225]
   Zhang Y., 2017, ARXIV171107128
NR 26
TC 4
Z9 4
U1 1
U2 2
PY 2021
VL 12895
BP 245
EP 256
DI 10.1007/978-3-030-86383-8_20
UT WOS:000711936300020
DA 2023-11-16
ER

PT J
AU Zeng, Y
   Zhang, TL
   Xu, B
AF Zeng, Yi
   Zhang, Tielin
   Xu, Bo
TI Improving multi-layer spiking neural networks by incorporating
   brain-inspired rules
SO SCIENCE CHINA-INFORMATION SCIENCES
DT Article
DE brain-inspired rules; spiking neural network; plasticity; classification
   task
ID MODEL; PLASTICITY; ALGORITHM; STDP
AB This paper introduces seven brain-inspired rules that are deeply rooted in the understanding of the brain to improve multi-layer spiking neural networks (SNNs). The dynamics of neurons, synapses, and plasticity models are considered to be major characteristics of information processing in brain neural networks. Hence, incorporating these models and rules to traditional SNNs is expected to improve their efficiency. The proposed SNN model can mainly be divided into three parts: the spike generation layer, the hidden layers, and the output layer. In the spike generation layer, non-temporary signals such as static images are converted into spikes by both local and global feature-converting methods. In the hidden layers, the rules of dynamic neurons, synapses, the proportion of different kinds of neurons, and various spike timing dependent plasticity (STDP) models are incorporated. In the output layer, the function of classification for excitatory neurons and winner take all (WTA) for inhibitory neurons are realized. MNIST dataset is used to validate the classification accuracy of the proposed neural network model. Experimental results show that higher accuracy will be achieved when more brain-inspired rules (with careful selection) are integrated into the learning procedure.
C1 [Zeng, Yi; Zhang, Tielin; Xu, Bo] Chinese Acad Sci, Inst Automat, Beijing 100190, Peoples R China.
   [Zeng, Yi; Xu, Bo] Chinese Acad Sci, Ctr Excellence Brain Sci & Intelligence Technol, Shanghai 200031, Peoples R China.
RP Zeng, Y; Zhang, TL (corresponding author), Chinese Acad Sci, Inst Automat, Beijing 100190, Peoples R China.; Zeng, Y (corresponding author), Chinese Acad Sci, Ctr Excellence Brain Sci & Intelligence Technol, Shanghai 200031, Peoples R China.
EM yi.zeng@ia.ac.cn; tielin.zhang@ia.ac.cn
CR Beyeler M, 2015, NEURAL NETWORKS, V72, P75, DOI 10.1016/j.neunet.2015.09.005
   Beyeler M, 2013, NEURAL NETWORKS, V48, P109, DOI 10.1016/j.neunet.2013.07.012
   Brette R, 2005, J NEUROPHYSIOL, V94, P3637, DOI 10.1152/jn.00686.2005
   Chrol-Cannon J, 2014, BIOSYSTEMS, V125, P43, DOI 10.1016/j.biosystems.2014.04.003
   Clopath C, 2010, NAT NEUROSCI, V13, P344, DOI 10.1038/nn.2479
   Destexhe A, 1997, NEURAL COMPUT, V9, P503, DOI 10.1162/neco.1997.9.3.503
   Eliasmith C, 2012, SCIENCE, V338, P1202, DOI 10.1126/science.1225266
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Häusser M, 2000, NAT NEUROSCI, V3, P1165, DOI 10.1038/81426
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   Heeger DJ, 2002, NAT REV NEUROSCI, V3, P142, DOI 10.1038/nrn730
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Iakymchuk T, 2015, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-015-0059-4
   Ionescu M, 2006, FUND INFORM, V71, P279
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Jia YQ, 2012, PROC CVPR IEEE, P3370, DOI 10.1109/CVPR.2012.6248076
   Kravitz DJ, 2013, TRENDS COGN SCI, V17, P26, DOI 10.1016/j.tics.2012.10.011
   Lytton W W, 1992, INDUCED RHYTHMS BRAI, P357
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maffei G, 2015, NEURAL NETWORKS, V72, P88, DOI 10.1016/j.neunet.2015.10.004
   Rolls E.T., 2007, COMPUTATIONAL NEUROS
   Schaul T, 2010, J MACH LEARN RES, V11, P743
   SERESS L, 1984, J NEUROCYTOL, V13, P215, DOI 10.1007/BF01148116
   SHARKEY NE, 1995, KLUWER INT SER ENG C, V292, P223
   SOKAL RR, 1969, BIOMETRY, P776
   Song HF, 2016, PLOS COMPUT BIOL, V12, DOI 10.1371/journal.pcbi.1004792
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Waddington A, 2012, FRONT COMPUT NEUROSC, V6, P1, DOI 10.3389/fncom.2012.00088
   Wade JJ, 2010, IEEE T NEURAL NETWOR, V21, P1817, DOI 10.1109/TNN.2010.2074212
   Zenke F, 2015, NAT COMMUN, V6, DOI 10.1038/ncomms7922
   Zhao YZ, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0162882
NR 31
TC 14
Z9 16
U1 0
U2 29
PD MAY
PY 2017
VL 60
IS 5
AR 052201
DI 10.1007/s11432-016-0439-4
UT WOS:000405775100001
DA 2023-11-16
ER

PT J
AU Xie, ZJ
   Xie, CG
AF Xie, Zhi-Jiang
   Xie, Chang-Gui
TI APPLICATION OF FUZZY SPIKING NEURAL NETWORK ON FAULT DIAGNOSIS OF GAS
   BLOWER SETS
SO METALURGIA INTERNATIONAL
DT Article
DE gas blower; fuzzy spiking neural network; clustering; fault Diagnosis
AB This paper is designed to handle thegas blower failure with regard to the fuzzy classification boundaries and the traditional neural network algorithms that are difficult to solve application problems on the contradictions between an instance of scale and the network scale. Fuzzy spiking neural network was used in the Fault Diagnosis of Gas blower sets, leading to diagnostic algorithms proposed for fuzzy spiking neural network. The first step is to use the receptive fields of neurons, the algorithm converts the input mode into the output pulse train of the neuron, and then pulse sequence with a population coding and unsupervised learning for clustering analysis, thus being able to better overcome thegas blower failure with regard to the classification bout:duly, and the related clustering analysis of invalidity. The applications showed that the algorithm effectively solves the fuzzy boundaries of thegas blower failure, greatly improving the accuracy of fault diagnosis.
C1 [Xie, Zhi-Jiang; Xie, Chang-Gui] Chongqing Univ, Natl Key Lab Mech Transmiss, Chongqing, Peoples R China.
RP Xie, ZJ (corresponding author), Chongqing Univ, Natl Key Lab Mech Transmiss, Chongqing, Peoples R China.
CR Allen JN, 2007, IDT 2007: SECOND INTERNATIONAL DESIGN AND TEST WORKSHOP, PROCEEDINGS, P222
   HAN Manlin, 2011, MACHINE TOOL HYDRAUL, V24, P231
   Herbert J., 2006, NEW APPROACH COMPETI
   Kubota N., 2005, COMPUTATIONAL INTELL, P2410
   Mandieh Adeeb Samer, 1994, FERROELECTRICS, P287
   Mostafa MM, 2011, EXPERT SYST APPL, V38, P6906, DOI 10.1016/j.eswa.2010.12.033
   Pakhira MK, 2004, PATTERN RECOGN, V37, P487, DOI 10.1016/j.patcog.2003.06.005
   [仇国庆 Qiu Guoqing], 2008, [中国机械工程, China Mechanical Engineering], V19, P2642
   Rasti J, 2011, EXPERT SYST APPL, V38, P13188, DOI 10.1016/j.eswa.2011.04.132
   Tang YN, 2011, PEER PEER NETW APPL, V4, P439, DOI 10.1007/s12083-010-0100-4
   Yang Xiaofan, 1994, COMPUTER SCI, V21, P23
   [姚剑飞 Yao Jianfei], 2009, [振动、测试与诊断, Journal of Vibration, Measurement and Diagnosis], V29, P74
   YE YU-JIE, 2008, EQUIPMENT MANUFACTUR, V11, P2
   ZI VAN-YANG, 2006, CHINESE J MECH ENG, V42, P117
NR 14
TC 1
Z9 1
U1 0
U2 4
PY 2012
VL 17
IS 8
BP 34
EP 38
UT WOS:000306160200007
DA 2023-11-16
ER

PT C
AU Silva, RG
   de Sousa, LT
AF Silva, RG
   de Sousa, LT
BE DelPobil, AP
TI A robust methodology for tool condition monitoring using spiking neuron
   networks
SO Proceedings of the Ninth IASTED International Conference on Artificial
   Intelligence and Soft Computing
DT Proceedings Paper
CT 9th IASTED International Conference on Artificial Intelligence and Soft
   Computing
CY SEP 12-14, 2005
CL Benidorm, SPAIN
DE spiking neuron networks; machining; condition monitoring; tool wear
ID COMPUTATION; OPERATIONS; SYSTEM
AB Artificial neural networks of sigmoidal and McCulloch-Pitts neurons have found increasing favour in industry research because of their most attractive features, abstraction of hardly accessible knowledge and generalisation from distorted sensor signals. In recent years experimental evidence has been accumulating to suggest that biological neural networks, which communicate through spikes, use the timing of these spikes to encode and compute information in a more efficient way. In this paper it is presented a simplified version of a Self Organizing neural architecture based on Spiking Neurons and it is shown that this computational architectures have a greater potential to unveil embedded information in tool wear monitoring data sets and that smaller structures, compared to sigmoidal neural networks, are needed to capture and model the inherent complexity embedded in tool wear monitoring data. Additional, it is proposed a robust methodology based on tool wear estimation historical evolution that should improve estimation and predictive capabilities of Tool Condition Monitoring systems.
C1 Univ Lusiada VN Famalicao, P-4760108 Vila Nova De Famalicao, Portugal.
RP Silva, RG (corresponding author), Univ Lusiada VN Famalicao, Largo Tinoco Sousa, P-4760108 Vila Nova De Famalicao, Portugal.
CR [Anonymous], PULSED NEURAL NETWOR
   Balazinski M, 2002, ENG APPL ARTIF INTEL, V15, P73, DOI 10.1016/S0952-1976(02)00004-0
   Bugmann G, 1997, BIOSYSTEMS, V40, P11, DOI 10.1016/0303-2647(96)01625-5
   Byrne G., 1995, ANN CIRP, V44, P541
   CHOUDHURY S, 1990, INT J MACH TOOL MANU, V39, P489
   Cybenko G., 1989, Mathematics of Control, Signals, and Systems, V2, P303, DOI 10.1007/BF02551274
   DAN L, 1990, INT J MACH TOOL MANU, V30, P579, DOI DOI 10.1016/0890-6955(90)90009-8
   Dimla DE, 1997, INT J MACH TOOL MANU, V37, P1219, DOI 10.1016/S0890-6955(97)00020-5
   Dimla DE, 2000, INT J MACH TOOL MANU, V40, P1073, DOI 10.1016/S0890-6955(99)00122-4
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8
   Jantunen E, 2002, INT J MACH TOOL MANU, V42, P997, DOI 10.1016/S0890-6955(02)00040-8
   Lennox B, 2001, J PROCESS CONTR, V11, P497, DOI 10.1016/S0959-1524(00)00027-5
   Maass W, 1999, INFORM COMPUT, V148, P202, DOI 10.1006/inco.1998.2743
   Maass W., 1999, PULSED NEURAL NETWOR
   MAASS W, 1997, NEURAL NETWORKS, V10, P1656
   Natschläger T, 2002, THEOR COMPUT SCI, V287, P251, DOI 10.1016/S0304-3975(02)00099-3
   Parlos AG, 2000, NEURAL NETWORKS, V13, P765, DOI 10.1016/S0893-6080(00)00048-4
   Ruf B, 1997, NEURAL PROCESS LETT, V5, P9, DOI 10.1023/A:1009697008681
   Silva RG, 1998, MECH SYST SIGNAL PR, V12, P319, DOI 10.1006/mssp.1997.0123
   Silva RG, 2000, MECH SYST SIGNAL PR, V14, P287, DOI 10.1006/mssp.1999.1286
   WARNECKE A, 1990, WINT ANN M ASME DALL, P43
NR 22
TC 0
Z9 0
U1 0
U2 1
PY 2005
BP 153
EP 158
UT WOS:000233165700028
DA 2023-11-16
ER

PT C
AU Anderson, SE
AF Anderson, SE
BE Hamza, MH
TI Learning precise spike times in a two-variable spiking neural model
SO PROCEEDINGS OF THE IASTED INTERNATIONAL CONFERENCE ON COMPUTATIONAL
   INTELLIGENCE
DT Proceedings Paper
CT IASTED International Conference on Computational Intelligence
CY JUL 04-06, 2005
CL Calgary, CANADA
DE pulse-coupled neural network; spiking; reinforcement
ID BRAIN
AB We evaluate the ability of reinforcement comparison learning to induce multispike patterns with sub-millisecond precision in a two-variable spiking neural model. We assume that a single reinforcement signal derived from the fit of the produced spike pattern with a target pattern is communicated with the neural model following production of all spikes of the pattern. We find that arbitrary multispike patterns can be learned with a precision of 0.2 msec. Patterns of one to five spikes can be teamed with a probability of success ranging from 20% to 70%.
C1 Bard Coll, Comp Sci Program, Annandale On Hudson, NY 12504 USA.
RP Anderson, SE (corresponding author), Bard Coll, Comp Sci Program, Annandale On Hudson, NY 12504 USA.
EM sanderso@bard.edu
CR Barto A, 1998, INTRO REINFORCEMENT, V1st
   CARR CE, 1990, J NEUROSCI, V10, P3227
   Chi ZY, 2001, NEURON, V32, P899, DOI 10.1016/S0896-6273(01)00524-4
   DeWeese M. R., 2003, ADV NEURAL INFORM PR, V15, P101
   Di Paolo EA, 2002, ADAPT BEHAV, V10, P243, DOI 10.1177/1059712302010003006
   Doya K, 1998, CENTRAL AUDITORY PROCESSING AND NEURAL MODELING, P77
   Hahnloser RHR, 2002, NATURE, V419, P65, DOI 10.1038/nature00974
   ITAKURA F, 1975, IEEE T ACOUST SPEECH, VAS23, P67, DOI 10.1109/TASSP.1975.1162641
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Seung HS, 2003, NEURON, V40, P1063, DOI 10.1016/S0896-6273(03)00761-X
   Troyer TW, 2001, CURR OPIN NEUROBIOL, V11, P721, DOI 10.1016/S0959-4388(01)00275-6
NR 11
TC 0
Z9 0
U1 0
U2 0
PY 2005
BP 209
EP 213
UT WOS:000233166400037
DA 2023-11-16
ER

PT C
AU Koohestan-Mahalian, F
   Cotter, NE
AF Koohestan-Mahalian, Fatemeh
   Cotter, Neil E.
BE Matthews, MB
TI Exact Characterization of Phase Locking in a Linear Recurrent Spiking
   Neural Network
SO 2020 54TH ASILOMAR CONFERENCE ON SIGNALS, SYSTEMS, AND COMPUTERS
SE Conference Record of the Asilomar Conference on Signals Systems and
   Computers
DT Proceedings Paper
CT 54th Asilomar Conference on Signals, Systems, and Computers
CY NOV 01-05, 2020
CL ELECTR NETWORK
DE spiking neuron; response surface; phase locking; recurrent; spike time
ID MODEL
AB This paper presents a graphical method for determining the phase-tracking behavior of a linear spiking neuron with a recurrent connection. By employing what we refer to as the response surface method, we show that the response of the neuron to a steady input spike train is of three types: convergence to a stable fixed point, divergence from an unstable fixed point and finite firing, or chaotic firing. We also present a matrix formula for spiking times in a recurrent linear spiking neural network.
C1 [Koohestan-Mahalian, Fatemeh; Cotter, Neil E.] Univ Utah, Elect & Comp Engn Dept, Salt Lake City, UT 84112 USA.
RP Koohestan-Mahalian, F (corresponding author), Univ Utah, Elect & Comp Engn Dept, Salt Lake City, UT 84112 USA.
EM Fatima.mahalian@gmail.com; necotter@ece.utah.edu
CR Abbott LF, 1999, BRAIN RES BULL, V50, P303, DOI 10.1016/S0361-9230(99)00161-6
   [Anonymous], 2001, HDB BIOL PHYS
   Bressloff PC, 2000, NEURAL COMPUT, V12, P91, DOI 10.1162/089976600300015907
   Esser SK, 2016, P NATL ACAD SCI USA, V113, P11441, DOI 10.1073/pnas.1604850113
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   KEENER JP, 1981, SIAM J APPL MATH, V41, P503, DOI 10.1137/0141042
   Paugam-Moisy H, 2012, HDB NATURAL COMPUTIN, V1, P1
NR 9
TC 0
Z9 0
U1 0
U2 0
PY 2020
BP 802
EP 807
DI 10.1109/IEEECONF51394.2020.9443314
UT WOS:000681731800155
DA 2023-11-16
ER

PT J
AU Jiang, CM
   Yang, L
   Zhang, YL
AF Jiang, Chunming
   Yang, Le
   Zhang, Yilei
TI A Spiking Neural Network With Spike-Timing-Dependent Plasticity for
   Surface Roughness Analysis
SO IEEE SENSORS JOURNAL
DT Article
DE Neurons; Surface roughness; Rough surfaces; Sensors; Films; Membrane
   potentials; Biological system modeling; Surface roughness
   discrimination; tactile sensor; spiking neural network;
   spike-timing-dependent plasticity
ID DISCRIMINATION; TEXTURE; STDP
AB Spiking neural network (SNN) utilizes spike trains for information processing among neurons, which is more biologically plausible and widely regarded as the third-generation artificial neural network (ANN). It has the potential for effectively processing spatial-temporal information and has the characteristics of lower power consumption and smaller calculation load compared with conventional ANNs. In this work, we demonstrate the feasibility of applying SNN to classify tactile signals collected by a bionic artificial fingertip that touches a group of real-world metal surfaces with different roughness levels. A two-layer SNN is adopted and trained using an unsupervised learning method with spike-timing-dependent plasticity (STDP). Experiments show that the trained SNN can categorize the input tactile signals into different surface roughness of metal textures with more than 80% accuracy. This work lays the foundation of applying SNNs to more complex tactile signal processing in robotics, manufacturing, and other engineering fields.
C1 [Jiang, Chunming; Zhang, Yilei] Univ Canterbury, Dept Mech Engn, Christchurch 8041, New Zealand.
   [Yang, Le] Univ Canterbury, Dept Comp Engn, Christchurch 8041, New Zealand.
RP Zhang, YL (corresponding author), Univ Canterbury, Dept Mech Engn, Christchurch 8041, New Zealand.
EM chunming.jiang@pg.canbterbury.ac.nz; le.yang@canterbury.ac.nz;
   yilei.zhang@canterbury.ac.nz
CR [Anonymous], 2009, ROBOTICS SCI SYSTEMS
   [Anonymous], 2012, PIEZOELECTRIC CERAMI
   Brzosko Z, 2015, ELIFE, V4, DOI 10.7554/eLife.09685
   Cooper LN, 2012, NAT REV NEUROSCI, V13, P798, DOI 10.1038/nrn3353
   Deng L, 2020, NEURAL NETWORKS, V121, P294, DOI 10.1016/j.neunet.2019.09.005
   Diehl PU, 2015, IEEE IJCNN
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Hao YZ, 2020, NEURAL NETWORKS, V121, P387, DOI 10.1016/j.neunet.2019.09.007
   Hebb DO, 1950, J CLIN PSYCHOL, V6, P307
   Heeger D., 2000, HANDOUT U STANDFORD, V5, P76
   Isett BR, 2018, NEURON, V97, P418, DOI 10.1016/j.neuron.2017.12.021
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Kroemer O, 2011, IEEE T ROBOT, V27, P545, DOI 10.1109/TRO.2011.2121130
   Liang QZ, 2017, IEEE SENS J, V17, P7954, DOI 10.1109/JSEN.2017.2763245
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Mayol-Cuevas WW, 1998, IEEE SYS MAN CYBERN, P4246, DOI 10.1109/ICSMC.1998.727512
   Mozafari M, 2018, IEEE T NEUR NET LEAR, V29, P6178, DOI 10.1109/TNNLS.2018.2826721
   Muhammad HB, 2011, MICROELECTRON ENG, V88, P1811, DOI 10.1016/j.mee.2011.01.045
   Oddo CM, 2011, IEEE T ROBOT, V27, P522, DOI 10.1109/TRO.2011.2116930
   Pozo K, 2010, NEURON, V66, P337, DOI 10.1016/j.neuron.2010.04.028
   Qin LH, 2017, SENSOR ACTUAT A-PHYS, V264, P133, DOI 10.1016/j.sna.2017.07.054
   Rolls ET, 2011, PROG NEUROBIOL, V95, P448, DOI 10.1016/j.pneurobio.2011.08.002
   Sun QQ, 2009, J NEUROPHYSIOL, V102, P2955, DOI 10.1152/jn.00562.2009
   Taherkhani A, 2020, NEURAL NETWORKS, V122, P253, DOI 10.1016/j.neunet.2019.09.036
   Tang W, 2013, APPL SURF SCI, V273, P199, DOI 10.1016/j.apsusc.2013.02.013
   Virgilio CD, 2020, NEURAL NETWORKS, V122, P130, DOI 10.1016/j.neunet.2019.09.037
   Wang Y., 2018, IEEE T NEUR NET LEAR
   Yeung LC, 2004, P NATL ACAD SCI USA, V101, P14943, DOI 10.1073/pnas.0405555101
   Yi ZK, 2017, NEUROCOMPUTING, V244, P102, DOI 10.1016/j.neucom.2017.03.025
   Yi ZK, 2017, SENSOR ACTUAT A-PHYS, V255, P46, DOI 10.1016/j.sna.2016.12.021
   Zhang JC, 2009, P NATL ACAD SCI USA, V106, P13028, DOI 10.1073/pnas.0900546106
   Zhang W, 2003, NAT REV NEUROSCI, V4, P885, DOI 10.1038/nrn1248
NR 32
TC 2
Z9 2
U1 7
U2 28
PD JAN 1
PY 2022
VL 22
IS 1
BP 438
EP 445
DI 10.1109/JSEN.2021.3120845
UT WOS:000735528200055
DA 2023-11-16
ER

PT C
AU Liu, D
   Yue, S
AF Liu, Daqi
   Yue, Shigang
GP IEEE
TI Visual Pattern Recognition using Unsupervised Spike Timing Dependent
   Plasticity Learning
SO 2016 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUL 24-29, 2016
CL Vancouver, CANADA
ID NEURONS; INTEGRATE; NETWORKS; NOISE
AB Neuroscience study shows mammalian brain only use millisecond scale time window to process complicated real-life recognition scenarios. However, such speed cannot be achieved by traditional rate-based spiking neural network (SNN). Compared with spiking rate, the specific spiking timing (also called spiking pattern) may convey much more information. In this paper, by using modified rank order coding scheme, the generated absolute analog features have been encoded into the first spike wave with specific spatiotemporal structural information. An intuitive yet powerful feed-forward spiking neural network framework has been proposed, along with its own unsupervised spike-timing- dependent plasticity (STDP) learning rule with dynamic post-synaptic potential threshold. Compared with other state-ofart spiking algorithms, the proposed method uses biologically plausible STDP learning method to learn the selectivity while the dynamic post-synaptic potential threshold guarantees no training sample will be ignored during the learning procedure. Furthermore, unlike the complicated frameworks used in those state-of-art spiking algorithms, the proposed intuitive spiking neural network is not time-consuming and quite capable of on-line learning. A satisfactory experimental result has been achieved on classic MNIST handwritten character database.
C1 [Liu, Daqi; Yue, Shigang] Univ Lincoln, Sch Comp Sci, Brayford Pool LN6 7TS, Lincoln, England.
RP Liu, D (corresponding author), Univ Lincoln, Sch Comp Sci, Brayford Pool LN6 7TS, Lincoln, England.
EM dliu@lincoln.ac.uk; syue@lincoln.ac.uk
CR [Anonymous], SPIKING NEURAL MODEL
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Brunel N, 2001, PHYS REV LETT, V86, P2186, DOI 10.1103/PhysRevLett.86.2186
   Delorme A, 1999, NEUROCOMPUTING, V26-7, P989, DOI 10.1016/S0925-2312(99)00095-8
   Delorme A, 2001, NEUROCOMPUTING, V38, P539, DOI 10.1016/S0925-2312(01)00403-9
   Delorme A, 2001, NEURAL NETWORKS, V14, P795, DOI 10.1016/S0893-6080(01)00049-1
   Gerstner W, 2000, NEURAL COMPUT, V12, P43, DOI 10.1162/089976600300015899
   Gerstner W, 1996, NATURE, V383, P76, DOI 10.1038/383076a0
   Hebb D.O., 1949, ORG BEHAV NEUROPSYCH
   Kempter R, 1999, PHYS REV E, V59, P4498, DOI 10.1103/PhysRevE.59.4498
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   Masquelier T, 2008, PLOS ONE, V3, DOI 10.1371/journal.pone.0001377
   Masquelier T, 2007, PLOS COMPUT BIOL, V3, P247, DOI 10.1371/journal.pcbi.0030031
   Rieke F., 1996, SPIKES EXPLORING NEU
   Rubin JE, 2005, J NEUROPHYSIOL, V93, P2600, DOI 10.1152/jn.00803.2004
   Serre T., 2005, THEORY OBJECT RECOGN
   Serre T, 2007, IEEE T PATTERN ANAL, V29, P411, DOI 10.1109/TPAMI.2007.56
   Shadlen Michael N., 1994, Current Opinion in Neurobiology, V4, P569, DOI 10.1016/0959-4388(94)90059-0
   Sjöström PJ, 2001, NEURON, V32, P1149, DOI 10.1016/s0896-6273(01)00542-6
   Thériault C, 2013, IEEE T IMAGE PROCESS, V22, P764, DOI 10.1109/TIP.2012.2222900
   Thorpe S, 1996, NATURE, V381, P520, DOI 10.1038/381520a0
   van Rossum MCW, 2000, J NEUROSCI, V20, P8812
   Yu Q, 2013, IEEE T NEUR NET LEAR, V24, P1539, DOI 10.1109/TNNLS.2013.2245677
NR 25
TC 8
Z9 8
U1 0
U2 1
PY 2016
BP 285
EP 292
UT WOS:000399925500038
DA 2023-11-16
ER

PT C
AU Yaqoob, M
   Wróbel, B
AF Yaqoob, Muhammad
   Wrobel, Borys
GP IEEE
TI Very Small Spiking Neural Networks Evolved to Recognize a Pattern in a
   Continuous Input Stream
SO 2017 IEEE SYMPOSIUM SERIES ON COMPUTATIONAL INTELLIGENCE (SSCI)
DT Proceedings Paper
CT IEEE Symposium Series on Computational Intelligence (IEEE SSCI)
CY NOV 27-DEC 01, 2017
CL Honolulu, HI
DE artificial evolution; complex networks; evolutionary algorithm; minimal
   cognition; spiking neural networks; temporal pattern recognition
ID REPRESENTATION; MODELS; DELAYS; TIME
AB We obtained, with artificial evolution, very small (one or two interneurons, one output neuron) spiking neural networks (SNNs) recognizing a simple temporal pattern in a continuous input stream. The patterns the network evolved to recognize consisted of three different signals. In other words, the task was equivalent to searching in a stream (sequence) of three symbols (say, ABBCACBC..) for a specific subsequence (ABC). The fitness function we used rewarded spiking after the occurrence of the correct pattern (subsequence), and penalized spikes elsewhere. We found out that the networks did not go below two interneurons when they evolved to solve this task with a brief interval of silence between signals. However-surprisingly-for a longer interval of silences between signals the task could be accomplished with just one interneuron. We then analyzed how the spiking networks work by mapping the states of the network onto states of Finite State Machines-a general model of computation on time series. Our long term goal is to understand the mechanisms governing the neural networks that accomplish computational tasks in a way that is robust to noise and damage.
C1 [Yaqoob, Muhammad; Wrobel, Borys] Adam Mickiewicz Univ, Evolving Syst Lab, Poznan, Poland.
   [Wrobel, Borys] IOPAN, Sopot, Poland.
RP Yaqoob, M (corresponding author), Adam Mickiewicz Univ, Evolving Syst Lab, Poznan, Poland.
EM yaqoob@evosys.org; wrobel@evosys.org
CR Ahissar E, 2001, NEURON, V32, P185, DOI 10.1016/S0896-6273(01)00466-4
   [Anonymous], 2016, 2016 IEE INT C REB C, DOI [10.1109/ICRC.2016.7738691, DOI 10.1109/ICRC.2016.7738691]
   BIALEK W, 1991, SCIENCE, V252, P1854, DOI 10.1126/science.2063199
   deCharms RC, 2000, ANNU REV NEUROSCI, V23, P613, DOI 10.1146/annurev.neuro.23.1.613
   Gerstner W, 1996, NATURE, V383, P76, DOI 10.1038/383076a0
   Huxter J, 2003, NATURE, V425, P828, DOI 10.1038/nature02058
   Isaacson JS, 2010, CURR OPIN NEUROBIOL, V20, P328, DOI 10.1016/j.conb.2010.02.004
   Joachimczak M, 2012, BIOSYSTEMS, V109, P498, DOI 10.1016/j.biosystems.2012.05.014
   Joris P, 2007, TRENDS NEUROSCI, V30, P70, DOI 10.1016/j.tins.2006.12.004
   Laurent G, 1996, TRENDS NEUROSCI, V19, P489, DOI 10.1016/S0166-2236(96)10054-0
   Maex R, 2009, NEURAL NETWORKS, V22, P1105, DOI 10.1016/j.neunet.2009.07.022
   Natschläger T, 2002, THEOR COMPUT SCI, V287, P251, DOI 10.1016/S0304-3975(02)00099-3
   Naud R, 2008, BIOL CYBERN, V99, P335, DOI 10.1007/s00422-008-0264-7
   Rieke F., 1999, SPIKES EXPLORING NEU
   Rutishauser U, 2009, NEURAL COMPUT, V21, P478, DOI 10.1162/neco.2008.03-08-734
   Savage J E., 1997, MODELS COMPUTATION E
   Steuber V, 2004, J COMPUT NEUROSCI, V17, P149, DOI 10.1023/B:JCNS.0000037678.26155.b5
   Steuber V, 2002, NEUROCOMPUTING, V44, P183, DOI 10.1016/S0925-2312(02)00388-0
   Steuber V, 1999, NEUROCOMPUTING, V26-7, P271, DOI 10.1016/S0925-2312(99)00021-1
   Steuber V, 2006, NETWORK-COMP NEURAL, V17, P173, DOI 10.1080/09548980500520328
   Thorpe S, 1996, NATURE, V381, P520, DOI 10.1038/381520a0
   Tino P, 2005, LECT NOTES COMPUT SC, V3611, P666
   Wrobel B., 2014, SCI, V557, P187, DOI DOI 10.1007/978-3-642-55337-0_6
   Wrobel B., 2014, LNICST, V134, P135, DOI DOI 10.1007/978-3-319-06944-9_10
NR 24
TC 4
Z9 4
U1 0
U2 0
PY 2017
BP 3496
EP 3503
UT WOS:000428251403079
DA 2023-11-16
ER

PT J
AU Nasser, H
   Cessac, B
AF Nasser, Hassan
   Cessac, Bruno
TI Parameter Estimation for Spatio-Temporal Maximum Entropy Distributions:
   Application to Neural Spike Trains
SO ENTROPY
DT Article
DE neural coding; Gibbs distribution; maximum entropy; convex duality;
   spatio-temporal constraints; large-scale analysis; spike train; MEA
   recordings
ID TEMPORAL CORRELATIONS; INFORMATION; POPULATION; NETWORKS
AB We propose a numerical method to learn maximum entropy (MaxEnt) distributions with spatio-temporal constraints from experimental spike trains. This is an extension of two papers, [10] and [4], which proposed the estimation of parameters where only spatial constraints were taken into account. The extension we propose allows one to properly handle memory effects in spike statistics, for large-sized neural networks.
C1 [Nasser, Hassan; Cessac, Bruno] INRIA, F-06560 Sophia Antipolis, France.
RP Nasser, H (corresponding author), INRIA, 2004 Route Lucioles, F-06560 Sophia Antipolis, France.
EM Hassan.Nasser@inria.fr; Bruno.Cessac@inria.fr
CR Amari S, 2001, IEEE T INFORM THEORY, V47, P1701, DOI 10.1109/18.930911
   [Anonymous], 2007, ARXIV07122437
   [Anonymous], 2000, P C COMPUTATIONAL NA
   [Anonymous], 1975, LECT NOTES MATH
   Berger AL, 1996, COMPUT LINGUIST, V22, P39
   Cessac B., 2013, RR8329 INRIA
   Chazottes J., 2008, ISR J MATH, V131
   Chen S.F., 1999, ENT LANG MOD
   Chichilnisky EJ, 2001, NETWORK-COMP NEURAL, V12, P199, DOI 10.1088/0954-898X/12/2/306
   Collins M, 2002, MACH LEARN, V48, P253, DOI 10.1023/A:1013912006537
   CSISZAR I, 1974, IEEE T INFORM THEORY, V20, P122, DOI 10.1109/TIT.1974.1055146
   Dudik M., 2004, P 17 ANN C COMP LEAR
   Fernández R, 2005, J STAT PHYS, V118, P555, DOI 10.1007/s10955-004-8821-5
   Ferrea E, 2012, FRONT NEURAL CIRCUIT, V6, DOI 10.3389/fncir.2012.00080
   Ganmor E, 2011, P NATL ACAD SCI USA, V108, P9679, DOI 10.1073/pnas.1019641108
   Ganmor E, 2011, J NEUROSCI, V31, P3044, DOI 10.1523/JNEUROSCI.3682-10.2011
   Garibaldi U., 1985, P 5 NAT C HIST PHYS, V9, P341
   Georgii H. O., 1988, GIBBS MEASURES PHASE
   Gikhman II., 1979, THEORY STOCHASTIC PR, DOI [10.1007/978-1-4615-8065-2, DOI 10.1007/978-1-4615-8065-2]
   Hill DN, 2011, J NEUROSCI, V31, P8699, DOI 10.1523/JNEUROSCI.0971-11.2011
   Jaynes E.T., 1985, MACROSCOPIC PREDICTI, P254
   JAYNES ET, 1957, PHYS REV, V106, P620, DOI 10.1103/PhysRev.106.620
   JAYNES ET, 1980, ANNU REV PHYS CHEM, V31, P579, DOI 10.1146/annurev.pc.31.100180.003051
   Jaynes ET., 1979, MAXIMUM ENTROPY FORM, P15
   Kappen HJ, 1998, ADV NEUR IN, V10, P280
   Keller G., 1998, EQUILIBRIUM STATES E
   Li ZH, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0070894
   Litke AM, 2004, IEEE T NUCL SCI, V51, P1434, DOI 10.1109/TNS.2004.832706
   Marre O, 2009, PHYS REV LETT, V102, DOI 10.1103/PhysRevLett.102.138101
   Marre O, 2012, J NEUROSCI, V32, P14859, DOI 10.1523/JNEUROSCI.0723-12.2012
   Mayer V., 2010, MEMOIR AM MATH SOC, V203
   Nakahara H., 2001, ADV NEURAL INFORM PR, P253
   Nasser H, 2013, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2013/03/P03006
   Otten M, 2010, J CHEM PHYS, V133, DOI 10.1063/1.3455333
   Pillow JW, 2008, NATURE, V454, P995, DOI 10.1038/nature07140
   Quiroga RQ, 2004, NEURAL COMPUT, V16, P1661, DOI 10.1162/089976604774201631
   Rosenfeld R., 1994, TECHNICAL REPORT
   Ruelle D., 1978, THERMODYNAMIC FORMAL
   Ruelle D., 1969, STAT MECH RIGOROUS R
   Schaub M.T., 2010, ARXIV10091828
   Schneidman E, 2006, NATURE, V440, P1007, DOI 10.1038/nature04701
   Stevenson IH, 2011, NAT NEUROSCI, V14, P139, DOI 10.1038/nn.2731
   Strong SP, 1998, PHYS REV LETT, V80, P197, DOI 10.1103/PhysRevLett.80.197
   Tang A, 2008, J NEUROSCI, V28, P505, DOI 10.1523/JNEUROSCI.3359-07.2008
   Tkacik G., 2009, PHYS REV LETT
   Tkacik G, 2013, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2013/03/P03011
   Truccolo W, 2010, NAT NEUROSCI, V13, P105, DOI 10.1038/nn.2455
   Vasquez JC, 2012, J PHYSIOL-PARIS, V106, P120, DOI 10.1016/j.jphysparis.2011.11.001
   Zhou YQ, 2003, PROCEEDINGS OF THE 2003 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING, P153
NR 49
TC 16
Z9 16
U1 0
U2 10
PD APR
PY 2014
VL 16
IS 4
BP 2244
EP 2277
DI 10.3390/e16042244
UT WOS:000335001500024
DA 2023-11-16
ER

PT J
AU Lobov, SA
   Mikhaylov, AN
   Shamshin, M
   Makarov, VA
   Kazantsev, VB
AF Lobov, Sergey A.
   Mikhaylov, Alexey N.
   Shamshin, Maxim
   Makarov, Valeri A.
   Kazantsev, Victor B.
TI Spatial Properties of STDP in a Self-Learning Spiking Neural Network
   Enable Controlling a Mobile Robot
SO FRONTIERS IN NEUROSCIENCE
DT Article
DE spiking neural networks; spike-timing-dependent plasticity; learning;
   neurorobotics; neuroanimat; synaptic competition; neural competition;
   memristive devices
ID MODEL; NEURONS; BRAINS
AB Development of spiking neural networks (SNNs) controlling mobile robots is one of the modern challenges in computational neuroscience and artificial intelligence. Such networks, being replicas of biological ones, are expected to have a higher computational potential than traditional artificial neural networks (ANNs). The critical problem is in the design of robust learning algorithms aimed at building a "living computer" based on SNNs. Here, we propose a simple SNN equipped with a Hebbian rule in the form of spike-timing-dependent plasticity (STDP). The SNN implements associative learning by exploiting the spatial properties of STDP. We show that a LEGO robot controlled by the SNN can exhibit classical and operant conditioning. Competition of spike-conducting pathways in the SNN plays a fundamental role in establishing associations of neural connections. It replaces the irrelevant associations by new ones in response to a change in stimuli. Thus, the robot gets the ability to relearn when the environment changes. The proposed SNN and the stimulation protocol can be further enhanced and tested in developing neuronal cultures, and also admit the use of memristive devices for hardware implementation.
C1 [Lobov, Sergey A.; Mikhaylov, Alexey N.; Shamshin, Maxim; Makarov, Valeri A.; Kazantsev, Victor B.] Lobachevsky State Univ Nizhny Novgorod, Neurotechnol Dept, Nizhnii Novgorod, Russia.
   [Lobov, Sergey A.; Kazantsev, Victor B.] Innopolis Univ, Ctr Technol Robot & Mechatron Components, Neurosci & Cognit Technol Lab, Innopolis, Russia.
   [Makarov, Valeri A.] Univ Complutense Madrid, Fac Ciencias Matemat, Inst Matemat Interdisciplinar, Madrid, Spain.
RP Lobov, SA (corresponding author), Lobachevsky State Univ Nizhny Novgorod, Neurotechnol Dept, Nizhnii Novgorod, Russia.; Lobov, SA (corresponding author), Innopolis Univ, Ctr Technol Robot & Mechatron Components, Neurosci & Cognit Technol Lab, Innopolis, Russia.
EM lobov@neuro.nnov.ru
CR Bakkum DJ, 2008, J NEURAL ENG, V5, P310, DOI 10.1088/1741-2560/5/3/004
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Calvo Tapia C, 2020, COMMUN NONLINEAR SCI, V82, DOI 10.1016/j.cnsns.2019.105065
   Tapia CC, 2018, PHYS REV E, V97, DOI 10.1103/PhysRevE.97.052308
   Chou TS, 2015, FRONT NEUROROBOTICS, V9, DOI 10.3389/fnbot.2015.00006
   Clopath C, 2010, NAT NEUROSCI, V13, P344, DOI 10.1038/nn.2479
   Dauth S, 2017, J NEUROPHYSIOL, V117, P1320, DOI 10.1152/jn.00575.2016
   Dayan P., 2001, THEORETICAL NEUROSCI
   DeMarse TB, 2001, AUTON ROBOT, V11, P305, DOI 10.1023/A:1012407611130
   Du C, 2015, ADV FUNCT MATER, V25, P4290, DOI 10.1002/adfm.201501427
   Emelyanov AV, 2019, MICROELECTRON ENG, V215, DOI 10.1016/j.mee.2019.110988
   Esser SK, 2016, P NATL ACAD SCI USA, V113, P11441, DOI 10.1073/pnas.1604850113
   Gladkov A, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-15506-2
   Gong PL, 2009, PLOS COMPUT BIOL, V5, DOI 10.1371/journal.pcbi.1000611
   Gorban AN, 2019, PHYS LIFE REV, V29, P55, DOI 10.1016/j.plrev.2018.09.005
   HEBB D. O., 1949
   Hong S., 2010, P 14 INT C COMP SUPP
   Houk J., 1995, MODEL INF PROCESS BA, V13
   Hull C. L., 1943, PRINCIPLES BEHAV
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2002, BIOSYSTEMS, V67, P95, DOI 10.1016/S0303-2647(02)00067-9
   Izhikevich EM, 2007, CEREB CORTEX, V17, P2443, DOI 10.1093/cercor/bhl152
   Kandel E.R., 2000, PRINCIPLES NEURAL SC
   Kim S, 2015, NANO LETT, V15, P2203, DOI 10.1021/acs.nanolett.5b00697
   KOHONEN T, 1982, BIOL CYBERN, V43, P59, DOI 10.1007/BF00337288
   Kuzum D, 2013, NANOTECHNOLOGY, V24, DOI 10.1088/0957-4484/24/38/382001
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li C, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351877
   Liu JX, 2019, IEEE T NEUR NET LEAR, V30, P865, DOI 10.1109/TNNLS.2018.2854291
   Lobov S., 2017, P 5 INT C NEUR EL IN
   Lobov SA, 2017, MATH MODEL NAT PHENO, V12, P109, DOI 10.1051/mmnp/201712409
   Lobov SA, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20020500
   Malishev E, 2015, J PHYS CONF SER, V643, DOI 10.1088/1742-6596/643/1/012025
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   Meyer J.-A., 1991, P 1 INT C SIM AD BEH
   MILO V, 2017, INT EL DEVICES MEET
   Minnekhanov AA, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-47263-9
   Morrison A, 2008, BIOL CYBERN, V98, P459, DOI 10.1007/s00422-008-0233-1
   Palmer JHC, 2014, FRONT COMPUT NEUROSC, V8, DOI 10.3389/fncom.2014.00079
   Pamies D, 2014, EXP BIOL MED, V239, P1096, DOI 10.1177/1535370214537738
   Pavlov IP, 2010, ANN NEUROSCI, V17, P136, DOI 10.5214/ans.0972-7531.1017309
   Pfister JP, 2006, J NEUROSCI, V26, P9673, DOI 10.1523/JNEUROSCI.1425-06.2006
   Pimashkin A, 2016, COGN NEURODYNAMICS, V10, P287, DOI 10.1007/s11571-016-9380-6
   Pimashkin A, 2013, FRONT NEURAL CIRCUIT, V7, DOI 10.3389/fncir.2013.00087
   Potter S, 1997, SIGHT SOUND, P4
   Reger BD, 2000, ARTIF LIFE, V6, P307, DOI 10.1162/106454600300103656
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Shahaf G, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.1000228
   Sjöström PJ, 2001, NEURON, V32, P1149, DOI 10.1016/s0896-6273(01)00542-6
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Tan ZH, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-00849-7
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Tsodyks M, 1998, NEURAL COMPUT, V10, P821, DOI 10.1162/089976698300017502
   Tyukin I, 2019, B MATH BIOL, V81, P4856, DOI 10.1007/s11538-018-0415-5
   Wade J. J., 2008, PROCEEDINGS OF THE 2
   Wang FZ, 2019, J APPL PHYS, V125, DOI 10.1063/1.5042281
   Wang W, 2018, SCI ADV, V4, DOI 10.1126/sciadv.aat4752
   Xu Y, 2013, NEURAL NETWORKS, V43, P99, DOI 10.1016/j.neunet.2013.02.003
   Yu SM, 2011, IEEE T ELECTRON DEV, V58, P2729, DOI 10.1109/TED.2011.2147791
   Zamarreño-Ramos C, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00026
   Ziegler M, 2012, ADV FUNCT MATER, V22, P2744, DOI 10.1002/adfm.201200244
NR 62
TC 61
Z9 61
U1 2
U2 32
PD FEB 26
PY 2020
VL 14
AR 88
DI 10.3389/fnins.2020.00088
UT WOS:000525043800001
DA 2023-11-16
ER

PT J
AU Gardner, B
   Grüning, A
AF Gardner, Brian
   Gruening, Andre
TI Supervised Learning With First-to-Spike Decoding in Multilayer Spiking
   Neural Networks
SO FRONTIERS IN COMPUTATIONAL NEUROSCIENCE
DT Article
DE spiking neural networks; multilayer SNN; supervised learning;
   backpropagation; temporal coding; classification; MNIST
ID BACKPROPAGATION; CLASSIFICATION; PLASTICITY; NEURONS; MODELS; RULE
AB Experimental studies support the notion of spike-based neuronal information processing in the brain, with neural circuits exhibiting a wide range of temporally-based coding strategies to rapidly and efficiently represent sensory stimuli. Accordingly, it would be desirable to apply spike-based computation to tackling real-world challenges, and in particular transferring such theory to neuromorphic systems for low-power embedded applications. Motivated by this, we propose a new supervised learning method that can train multilayer spiking neural networks to solve classification problems based on a rapid, first-to-spike decoding strategy. The proposed learning rule supports multiple spikes fired by stochastic hidden neurons, and yet is stable by relying on first-spike responses generated by a deterministic output layer. In addition to this, we also explore several distinct, spike-based encoding strategies in order to form compact representations of presented input data. We demonstrate the classification performance of the learning rule as applied to several benchmark datasets, including MNIST. The learning rule is capable of generalizing from the data, and is successful even when used with constrained network architectures containing few input and hidden layer neurons. Furthermore, we highlight a novel encoding strategy, termed "scanline encoding," that can transform image data into compact spatiotemporal patterns for subsequent network processing. Designing constrained, but optimized, network structures and performing input dimensionality reduction has strong implications for neuromorphic applications.
C1 [Gardner, Brian] Univ Surrey, Dept Comp Sci, Guildford, Surrey, England.
   [Gruening, Andre] Univ Appl Sci, Fac Elect Engn & Comp Sci, Stralsund, Germany.
RP Gardner, B (corresponding author), Univ Surrey, Dept Comp Sci, Guildford, Surrey, England.
EM b.gardner@surrey.ac.uk
CR Albers C, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0148948
   [Anonymous], 2017, ARXIV 170807747
   Bagheri A, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P2986, DOI 10.1109/ICASSP.2018.8462410
   Bellec G, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-17236-y
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Booij O, 2005, INFORM PROCESS LETT, V95, P552, DOI 10.1016/j.ipl.2005.05.023
   Brea J, 2013, J NEUROSCI, V33, P9565, DOI 10.1523/JNEUROSCI.4098-12.2013
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Diehl PU, 2015, IEEE IJCNN
   Fisher RA, 1936, ANN EUGENIC, V7, P179, DOI 10.1111/j.1469-1809.1936.tb02137.x
   Florian RV, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0040233
   Frémaux N, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003024
   Friedmann S, 2017, IEEE T BIOMED CIRC S, V11, P128, DOI 10.1109/TBCAS.2016.2579164
   Gardner B, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0161335
   Gardner B, 2015, NEURAL COMPUT, V27, P2548, DOI 10.1162/NECO_a_00790
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   Gerstner W., 2002, SPIKING NEURON MODEL, DOI [DOI 10.1017/CBO9780511815706, 10.1017/cbo9780511815706]
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   Gollisch T, 2008, SCIENCE, V319, P1108, DOI 10.1126/science.1149639
   Grüning A, 2012, NEURAL PROCESS LETT, V36, P117, DOI 10.1007/s11063-012-9225-1
   Gruning A., 2020, ARXIV PREPRINT ARXIV
   Gruning A., 2014, COMPUT INTELL-US
   Gütig R, 2014, CURR OPIN NEUROBIOL, V25, P134, DOI 10.1016/j.conb.2014.01.004
   Gütig R, 2009, PLOS BIOL, V7, DOI 10.1371/journal.pbio.1000141
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Hinton G., 2012, RMSPROP DIVIDE GRADI
   Hung CP, 2005, SCIENCE, V310, P863, DOI 10.1126/science.1117593
   Jang H., 2020, ARXIV PREPRINT ARXIV
   Jang H, 2019, IEEE SIGNAL PROC MAG, V36, P64, DOI 10.1109/MSP.2019.2935234
   Kheradpisheh SR, 2020, INT J NEURAL SYST, V30, DOI 10.1142/S0129065720500276
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Kiani R, 2005, J NEUROPHYSIOL, V94, P1587, DOI 10.1152/jn.00540.2004
   Klikauer T, 2016, TRIPLEC-COMMUN CAPIT, V14, P260
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508
   Lin CK, 2018, COMPUTER, V51, P52, DOI 10.1109/MC.2018.157113521
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Memmesheimer RM, 2014, NEURON, V82, P925, DOI 10.1016/j.neuron.2014.03.026
   Mohemmed A, 2012, INT J NEURAL SYST, V22, DOI 10.1142/S0129065712500128
   Morrison A, 2008, BIOL CYBERN, V98, P459, DOI 10.1007/s00422-008-0233-1
   Mostafa Hesham, 2018, IEEE Trans Neural Netw Learn Syst, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Neftci E, 2014, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00272
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   Pfister JP, 2006, NEURAL COMPUT, V18, P1318, DOI 10.1162/neco.2006.18.6.1318
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Rezende DJ, 2014, FRONT COMPUT NEUROSC, V8, DOI 10.3389/fncom.2014.00038
   Simard PY, 2003, PROC INT CONF DOC, P958
   Sporea I, 2013, NEURAL COMPUT, V25, P473, DOI 10.1162/NECO_a_00396
   Tavanaei A, 2019, NEUROCOMPUTING, V330, P39, DOI 10.1016/j.neucom.2018.11.014
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   Urbanczik R, 2009, NEURAL COMPUT, V21, P340, DOI 10.1162/neco.2008.09-07-605
   van Rossum MCW, 2000, J NEUROSCI, V20, P8812
   VanRullen R, 2005, TRENDS NEUROSCI, V28, P1, DOI 10.1016/j.tins.2004.10.010
   WOLBERG WH, 1990, P NATL ACAD SCI USA, V87, P9193, DOI 10.1073/pnas.87.23.9193
   Yu Q, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0078318
   Zenke F., 2020, REMARKABLE ROBUSTNES, DOI [10.1101/2020.06.29.176925, DOI 10.1101/2020.06.29.176925]
   Zenke F, 2018, NEURAL COMPUT, V30, P1514, DOI 10.1162/neco_a_01086
NR 59
TC 1
Z9 1
U1 1
U2 10
PD APR 12
PY 2021
VL 15
AR 617862
DI 10.3389/fncom.2021.617862
UT WOS:000643702000001
DA 2023-11-16
ER

PT J
AU Luo, YL
   Wan, L
   Liu, JX
   Harkin, J
   McDaid, L
   Cao, Y
   Ding, XM
AF Luo, Yuling
   Wan, Lei
   Liu, Junxiu
   Harkin, Jim
   McDaid, Liam
   Cao, Yi
   Ding, Xuemei
TI Low Cost Interconnected Architecture for the Hardware Spiking Neural
   Networks
SO FRONTIERS IN NEUROSCIENCE
DT Article
DE interconnected architecture; spiking neural networks; Networks-on-Chip;
   system scalability; arbitration scheme
ID ON-CHIP; ROUTING ALGORITHM; ADAPTIVE NETWORK; SYSTEM
AB A novel low cost interconnected architecture (LCIA) is proposed in this paper, which is an efficient solution for the neuron interconnections for the hardware spiking neural networks (SNNs). It is based on an all-to-all connection that takes each paired input and output nodes of multi-layer SNNs as the source and destination of connections. The aim is to maintain an efficient routing performance under low hardware overhead. A Networks-on-Chip (NoC) router is proposed as the fundamental component of the LCIA, where an effective scheduler is designed to address the traffic challenge due to irregular spikes. The router can find requests rapidly, make the arbitration decision promptly, and provide equal services to different network traffic requests. Experimental results show that the LCIA can manage the intercommunication of the multi-layer neural networks efficiently and have a low hardware overhead which can maintain the scalability of hardware SNNs.
C1 [Luo, Yuling; Wan, Lei; Liu, Junxiu] Guangxi Normal Univ, Fac Elect Engn, Guilin, Peoples R China.
   [Harkin, Jim; McDaid, Liam; Ding, Xuemei] Univ Ulster, Sch Comp Engn & Intelligent Syst, Coleraine, Londonderry, North Ireland.
   [Cao, Yi] Univ Edinburgh, Business Sch, Management Sci & Business Econ Grp, Edinburgh, Midlothian, Scotland.
   [Ding, Xuemei] Fujian Normal Univ, Coll Math & Informat, Fuzhou, Fujian, Peoples R China.
RP Liu, JX (corresponding author), Guangxi Normal Univ, Fac Elect Engn, Guilin, Peoples R China.
EM j.liu@ieee.org
CR Abdali E. M, 2017, P 12 INT S REC COMM, P1, DOI [10.1109/ReCoSoC.2017.8016160, DOI 10.1109/RECOSOC.2017.8016160]
   Agarwal A., 2009, J ENG COMPUT ARCHITE, V3, P21
   Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   Ang CH, 2012, ELECTRON LETT, V48, P145, DOI 10.1049/el.2011.3651
   Basu A, 2010, IEEE T BIOMED CIRC S, V4, P311, DOI 10.1109/TBCAS.2010.2055157
   Benini L, 2002, COMPUTER, V35, P70, DOI 10.1109/2.976921
   Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Billaudelle S, 2016, ARXIV150502142
   Carrillo S, 2013, IEEE T PARALL DISTR, V24, P2451, DOI 10.1109/TPDS.2012.289
   Carrillo S, 2012, NEURAL NETWORKS, V33, P42, DOI 10.1016/j.neunet.2012.04.004
   Carrillo S, 2010, LECT NOTES COMPUT SC, V6274, P133, DOI 10.1007/978-3-642-15323-5_12
   Cawley S, 2011, GENET PROGRAM EVOL M, V12, P257, DOI 10.1007/s10710-011-9130-9
   Charleston-Villalobos S, 2011, MED BIOL ENG COMPUT, V49, P15, DOI 10.1007/s11517-010-0663-5
   Cios K. J., 1997, NEUROCOMPUTING, V16, P259, DOI [10.1007/978-1-59745-520-6_8, DOI 10.1007/978-1-59745-520-6_8]
   Cui YH, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-26436-y
   Dally W.J., 2004, PRINCIPLES PRACTICES
   Dally WJ, 2001, DES AUT CON, P684, DOI 10.1109/DAC.2001.935594
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Emery R, 2009, 2009 3RD ACM/IEEE INTERNATIONAL SYMPOSIUM ON NETWORKS-ON-CHIP, P144, DOI 10.1109/NOCS.2009.5071462
   Fidjeland Andreas K, 2010, 2010 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2010.5596678
   Furber SB, 2013, IEEE T COMPUT, V62, P2454, DOI 10.1109/TC.2012.142
   Gerstner W., 2002, SPIKING NEURON MODEL, DOI [DOI 10.1017/CBO9780511815706, 10.1017/cbo9780511815706]
   Graas EL, 2004, NEUROINFORMATICS, V2, P417, DOI 10.1385/NI:2:4:417
   Harkin J, 2009, INT J RECONFIGURABLE, V2009, DOI 10.1155/2009/908740
   Hu JC, 2004, ICCAD-2004: INTERNATIONAL CONFERENCE ON COMPUTER AIDED DESIGN, IEEE/ACM DIGEST OF TECHNICAL PAPERS, P354, DOI 10.1109/ICCAD.2004.1382601
   Jin X, 2010, COMPUT SCI ENG, V12, P91, DOI 10.1109/MCSE.2010.112
   Jin-xiang Wang, 2010, 2010 10th IEEE International Conference on Solid-State and Integrated Circuit Technology (ICSICT), P382, DOI 10.1109/ICSICT.2010.5667710
   Jordan J, 2018, FRONT NEUROINFORM, V12, DOI 10.3389/fninf.2018.00002
   Jovanovic S, 2009, MICROPROCESS MICROSY, V33, P24, DOI 10.1016/j.micpro.2008.08.004
   Kepa K, 2009, I C FIELD PROG LOGIC, P403, DOI 10.1109/FPL.2009.5272250
   KLEFENZ F, 1992, CERN REPORT, V92, P799
   Kulkarni S., 2012, 2012 4th International Conference on Computational Intelligence and Communication Networks (CICN 2012), P804, DOI 10.1109/CICN.2012.26
   Kwon H, 2017, 11 IEEE ACM INT S NE, V2017, P1, DOI [10.1145/3130218.3130230, DOI 10.1145/3130218.3130230]
   Kwon H., 2018, COMPUTING RES REPOSI, P1
   Lagorce X, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00206
   Liu JX, 2016, IEEE T COMPUT AID D, V35, P260, DOI 10.1109/TCAD.2015.2459050
   Liu JX, 2015, MICROPROCESS MICROSY, V39, P358, DOI 10.1016/j.micpro.2015.06.002
   Luo YL, 2018, NEURAL PROCESS LETT, V48, P1777, DOI 10.1007/s11063-018-9797-5
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Moctezuma JC, 2015, MICROPROCESS MICROSY, V39, P693, DOI 10.1016/j.micpro.2015.09.003
   Mohammadi R, 2015, LECT NOTES COMP SCI, V9491, P356
   Moscibroda T, 2009, CONF PROC INT SYMP C, P196, DOI 10.1145/1555815.1555781
   Nageswaran Jayram Moorkanikara, 2009, Proceedings 2009 International Joint Conference on Neural Networks (IJCNN 2009 - Atlanta), P2145, DOI 10.1109/IJCNN.2009.5179043
   Painkras E, 2013, THESIS, P19
   Painkras E, 2013, IEEE J SOLID-ST CIRC, V48, P1943, DOI 10.1109/JSSC.2013.2259038
   Pande S, 2013, NEURAL PROCESS LETT, V38, P131, DOI 10.1007/s11063-012-9274-5
   Park D, 1999, J TRANSP ENG, V125, P515, DOI 10.1061/(ASCE)0733-947X(1999)125:6(515)
   Perrinet LU, 2008, PROC SPIE, V7000, DOI 10.1117/12.787076
   Rast AD, 2008, IEEE IJCNN, P2727, DOI 10.1109/IJCNN.2008.4634181
   Sabogal S., 2017, P 31 ANN AIAAUSU C S
   Schemmel J, 2008, IEEE IJCNN, P431, DOI 10.1109/IJCNN.2008.4633828
   Schmitt S, 2017, IEEE IJCNN, P2227, DOI 10.1109/IJCNN.2017.7966125
   Schuman CD., 2017, ARXIV
   Upegui A, 2005, MICROPROCESS MICROSY, V29, P211, DOI 10.1016/j.micpro.2004.08.012
   van Albada SJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00291
   Walter F, 2017, IEEE INT SYMP CIRC S, P2715
   Wan L, 2016, IR SIGN SYST C, P1, DOI DOI 10.1109/ISSC.2016.7528472
   Wang MC, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P3184, DOI 10.1109/IJCNN.2011.6033643
   Wang R, 2015, COMPUT SCI, V7, P1
   Wu QX, 2008, NEUROCOMPUTING, V71, P2055, DOI 10.1016/j.neucom.2007.10.020
   Wysoski SG, 2008, NEUROCOMPUTING, V71, P2563, DOI 10.1016/j.neucom.2007.12.038
   Zhang Z, 2008, DES AUT CON, P441
   Zheng SQ, 2007, IEEE T PARALL DISTR, V18, P84, DOI 10.1109/TPDS.2007.253283
NR 63
TC 11
Z9 11
U1 0
U2 4
PD NOV 21
PY 2018
VL 12
AR 857
DI 10.3389/fnins.2018.00857
UT WOS:000450942500001
DA 2023-11-16
ER

PT J
AU Pang, CCC
   Upton, ARM
   Shine, G
   Kamath, MV
AF Pang, CCC
   Upton, ARM
   Shine, G
   Kamath, MV
TI A comparison of algorithms for detection of spikes in the
   electroencephalogram
SO IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING
DT Article
DE Classification; EEG; neural networks; spike detection
ID NEURAL-NETWORKS; AUTOMATIC RECOGNITION; SEIZURE DETECTION
AB Identification of the short transient waveform, called a spike, in the cortical electroencephalogram (EEG) plays an important role during diagnosis of neurological disorders such as epilepsy. It has been suggested that artificial neural networks (ANN) can be employed for spike detection in the EEG, if suitable features are provided as input to an ANN. In.this paper, we explore the performance of neural network-based classifiers using features selected by four previous investigations. Of these, three algorithms model the spike by mathematical parameters and use them as features for classification while the fourth algorithm uses raw EEG to train the classifier. The objective of this paper is to examine if there is any inherent advantage to any particular set of features, subject to the condition that the same data are used for all feature selection algorithms. Our results suggest that artificial neural networks trained with features selected using any one of the above three algorithms as well as raw EEG directly fed to the ANN will yield similar results.
C1 McMaster Univ, Dept Med, Hamilton, ON L8N 3Z5, Canada.
   McMaster Univ, Dept Elect & Comp Engn, Hamilton, ON L8N 3Z5, Canada.
RP Kamath, MV (corresponding author), McMaster Univ, Dept Med, Room 3E25,Hlth Sci Bldg, Hamilton, ON L8N 3Z5, Canada.
EM kamathm@mcmail.cis.mcmaster.ca
CR DAUBECHIES I, 1990, IEEE T INFORM THEORY, V36, P961, DOI 10.1109/18.57199
   Demuth H., 1998, NEURAL NETWORK TOOLB
   EBERHART RC, 1990, NEURAL NETWORK PC TO
   GOTMAN J, 1979, ELECTROEN CLIN NEURO, V46, P510, DOI 10.1016/0013-4694(79)90004-X
   GOTMAN J, 1982, ELECTROEN CLIN NEURO, V54, P530, DOI 10.1016/0013-4694(82)90038-4
   Gotman J, 1997, ELECTROEN CLIN NEURO, V103, P356, DOI 10.1016/S0013-4694(97)00003-9
   HAUKIN S, 1994, NEURAL NETWORKS COMP
   HJORTH B, 1973, ELECTROENCEPHALOGR, V34, P306
   KALAYCI T, 1995, IEEE ENG MED BIOL, V14, P160, DOI 10.1109/51.376754
   KILOH LG, 1981, CLIN ELECTROENCEPHAL
   KLOPPEL B, 1994, NEUROPSYCHOBIOLOGY, V29, P33, DOI 10.1159/000119060
   Kurth C, 2000, ANN BIOMED ENG, V28, P1362, DOI 10.1114/1.1331312
   NIEDERMEYER Ernst., 1982, ELECTROEN CLIN NEURO
   Osorio I, 1998, EPILEPSIA, V39, P615, DOI 10.1111/j.1528-1157.1998.tb01430.x
   Özdamar Ö, 1998, COMPUT BIOMED RES, V31, P122, DOI 10.1006/cbmr.1998.1475
   PANG CC, 2001, THESIS MCMASTER U HA
   Qu H, 1997, IEEE T BIO-MED ENG, V44, P115, DOI 10.1109/10.552241
   Tarassenko L, 1998, IEE P-SCI MEAS TECH, V145, P270, DOI 10.1049/ip-smt:19982328
   WALMSLEY M, 1984, IEEE T BIO-MED ENG, V31, P720, DOI 10.1109/TBME.1984.325397
   Webber WRS, 1996, ELECTROEN CLIN NEURO, V98, P250, DOI 10.1016/0013-4694(95)00277-4
   Weng W, 1996, NEURAL NETWORKS, V9, P1223, DOI 10.1016/0893-6080(96)00032-9
NR 21
TC 80
Z9 84
U1 0
U2 5
PD APR
PY 2003
VL 50
IS 4
BP 521
EP 526
DI 10.1109/TBME.2003.809479
UT WOS:000182426100015
DA 2023-11-16
ER

PT J
AU Ben Abdallah, A
   Dang, KN
AF Ben Abdallah, Abderazek
   Dang, Khanh N.
TI Toward Robust Cognitive 3D Brain-Inspired Cross-Paradigm System
SO FRONTIERS IN NEUROSCIENCE
DT Article
DE spiking neural network; neuromorphic; 3D-ICs; fault-tolerance; mapping
   algorithm
ID 3-D ICS; ALGORITHM; DESIGN
AB Spiking Neuromorphic systems have been introduced as promising platforms for energy-efficient spiking neural network (SNNs) execution. SNNs incorporate neuronal and synaptic states in addition to the variant time scale into their computational model. Since each neuron in these networks is connected to many others, high bandwidth is required. Moreover, since the spike times are used to encode information in SNN, a precise communication latency is also needed, although SNN is tolerant to the spike delay variation in some limits when it is seen as a whole. The two-dimensional packet-switched network-on-chip was proposed as a solution to provide a scalable interconnect fabric in large-scale spike-based neural networks. The 3D-ICs have also attracted a lot of attention as a potential solution to resolve the interconnect bottleneck. Combining these two emerging technologies provides a new horizon for IC design to satisfy the high requirements of low power and small footprint in emerging AI applications. Moreover, although fault-tolerance is a natural feature of biological systems, integrating many computation and memory units into neuromorphic chips confronts the reliability issue, where a defective part can affect the overall system's performance. This paper presents the design and simulation of R-NASH-a reliable three-dimensional digital neuromorphic system geared explicitly toward the 3D-ICs biological brain's three-dimensional structure, where information in the network is represented by sparse patterns of spike timing and learning is based on the local spike-timing-dependent-plasticity rule. Our platform enables high integration density and small spike delay of spiking networks and features a scalable design. R-NASH is a design based on the Through-Silicon-Via technology, facilitating spiking neural network implementation on clustered neurons based on Network-on-Chip. We provide a memory interface with the host CPU, allowing for online training and inference of spiking neural networks. Moreover, R-NASH supports fault recovery with graceful performance degradation.
C1 [Ben Abdallah, Abderazek; Dang, Khanh N.] Univ Aizu, Grad Sch Comp Sci & Engn, Adapt Syst Lab, Aizu Wakamatsu, Fukushima, Japan.
   [Dang, Khanh N.] Vietnam Natl Univ, VNU Univ Engn & Technol, VNU Key Lab Smart Integrated Syst SISLAB, Hanoi, Vietnam.
RP Ben Abdallah, A (corresponding author), Univ Aizu, Grad Sch Comp Sci & Engn, Adapt Syst Lab, Aizu Wakamatsu, Fukushima, Japan.
EM benab@u-aizu.ac.jp
CR Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   Arka AI, 2021, ACM T DES AUTOMAT EL, V26, DOI 10.1145/3424239
   Bamford SA, 2010, IEEE T NEURAL NETWOR, V21, P286, DOI 10.1109/TNN.2009.2036912
   Banerjee K, 2001, P IEEE, V89, P602, DOI 10.1109/5.929647
   Ben Ahmed A, 2014, J PARALLEL DISTR COM, V74, P2229, DOI 10.1016/j.jpdc.2014.01.002
   Ben Ahmed A, 2013, J SUPERCOMPUT, V66, P1507, DOI 10.1007/s11227-013-0940-9
   Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Chen GK, 2019, IEEE J SOLID-ST CIRC, V54, P992, DOI 10.1109/JSSC.2018.2884901
   Dang Khanh N., 2019, 2019 International Conference on Internet of Things, Embedded Systems and Communications (IINTEC). Proceedings, P155, DOI 10.1109/IINTEC48298.2019.9112123
   Dang KN, 2020, IEEE T VLSI SYST, V28, P672, DOI 10.1109/TVLSI.2019.2948878
   Dang KN, 2020, IEEE T EMERG TOP COM, V8, P577, DOI 10.1109/TETC.2017.2762407
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Deb K, 2002, IEEE T EVOLUT COMPUT, V6, P182, DOI 10.1109/4235.996017
   Diehl PU, 2015, IEEE IJCNN
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Eliasmith C, 2012, SCIENCE, V338, P1202, DOI 10.1126/science.1225266
   Frenkel C, 2019, IEEE T BIOMED CIRC S, V13, P999, DOI 10.1109/TBCAS.2019.2928793
   Frenkel C, 2019, IEEE T BIOMED CIRC S, V13, P145, DOI 10.1109/TBCAS.2018.2880425
   Furber S, 2016, J NEURAL ENG, V13, DOI 10.1088/1741-2560/13/5/051001
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   Goldwyn JH, 2011, PHYS REV E, V83, DOI 10.1103/PhysRevE.83.041908
   Hazan H, 2018, FRONT NEUROINFORM, V12, DOI 10.3389/fninf.2018.00089
   HSIAO MY, 1970, IBM J RES DEV, V14, P395, DOI 10.1147/rd.144.0395
   Ikechukwu OM, 2021, IEEE ACCESS, V9, P64331, DOI 10.1109/ACCESS.2021.3071089
   Ikechukwu OM, 2020, INT CONF BIG DATA, P133, DOI 10.1109/BigComp48618.2020.00-86
   Jin X., 2010, PARALLEL SIMULATION
   Joseph J.M., 2021, INORG NANO-MET CHEM, P1
   Jung Kuk Kim, 2015, 2015 Symposium on VLSI Circuits (VLSI Circuits), pC50, DOI 10.1109/VLSIC.2015.7231323
   Dang KN, 2022, IEEE T COMPUT AID D, V41, P799, DOI 10.1109/TCAD.2021.3069370
   Kim S, 2011, 2011 11TH INTERNATIONAL CONFERENCE ON CONTROL, AUTOMATION AND SYSTEMS (ICCAS), P1
   Lee C, 2019, IEEE T COGN DEV SYST, V11, P384, DOI 10.1109/TCDS.2018.2833071
   Lee HG, 2007, ACM T DES AUTOMAT EL, V12, DOI 10.1145/1255456.1255460
   Levin J.A., 2014, Patent No. US, Patent No. [2014/0351190 A1, 20140351190]
   Mahmoodi H, 2009, IEEE T VLSI SYST, V17, P33, DOI 10.1109/TVLSI.2008.2008453
   Merolla P, 2014, IEEE T CIRCUITS-I, V61, P820, DOI 10.1109/TCSI.2013.2284184
   Panth S, 2014, I SYMPOS LOW POWER E, P171, DOI 10.1145/2627369.2627642
   Purves D., 2018, NEUROSCIENCE
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Schemmel J, 2010, IEEE INT SYMP CIRC S, P1947, DOI 10.1109/ISCAS.2010.5536970
   Scholze S, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00117
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Shi C, 2021, IEEE T CIRCUITS-II, V68, P1581, DOI 10.1109/TCSII.2021.3063784
   Stimberg M, 2019, ELIFE, V8, DOI 10.7554/eLife.47314
   Vu TH, 2019, ACM J EMERG TECH COM, V15, DOI 10.1145/3340963
   Waldrop MM, 2016, NATURE, V530, P144, DOI 10.1038/530144a
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Yin SY, 2017, DES AUT CON, DOI 10.1145/3061639.3062232
   Zamarreño-Ramos C, 2013, IEEE T BIOMED CIRC S, V7, P82, DOI 10.1109/TBCAS.2012.2195725
   Zhao MR, 2020, APPL PHYS REV, V7, DOI 10.1063/1.5124915
NR 49
TC 4
Z9 4
U1 1
U2 6
PD JUN 25
PY 2021
VL 15
AR 690208
DI 10.3389/fnins.2021.690208
UT WOS:000670922100001
DA 2023-11-16
ER

PT C
AU Jaoudi, Y
   Yakopcic, C
   Taha, T
AF Jaoudi, Yassine
   Yakopcic, Chris
   Taha, Tarek
GP IEEE
TI Conversion of an Unsupervised Anomaly Detection System to Spiking Neural
   Network for Car Hacking Identification
SO 2020 11TH INTERNATIONAL GREEN AND SUSTAINABLE COMPUTING WORKSHOPS (IGSC)
DT Proceedings Paper
CT 11th International Green and Sustainable Computing Workshop (IGSC)
CY OCT 19-22, 2020
CL ELECTR NETWORK
DE Autoencoder; Spiking Neural Network; Intrusion detection; Controller
   area network; Conversion; Loihi; Neuromorphic processor
AB Across industry, there is an increasing availability of streaming, time-varying data, where it is important to detect anomalous behavior. These data are found in an enormous number of sensor-based applications, in cybersecurity (where anomalous behavior could indicate an attack), and in finance. Spiking Neural Networks (SNNs) have come under the spotlight for machine learning applications due to the extreme energy efficiency of their implementation on neuromorphic processors like the Intel Loihi research chip. In this paper we explore the applicability of spiking neural networks for in vehicle cyberattack detection. We show exemplary results by converting an autoencoder model to spiking form. We present a learning model comparison that shows the proposed SNN autoencoder outperforms a One Class Support Vector Machine and an Isolation Forest. Furthermore, only a slight reduction in accuracy is observed when compared to a traditional autoencoder.
C1 [Jaoudi, Yassine; Yakopcic, Chris; Taha, Tarek] Univ Dayton, Dept Elect & Comp Engn, Dayton, OH 45469 USA.
RP Jaoudi, Y (corresponding author), Univ Dayton, Dept Elect & Comp Engn, Dayton, OH 45469 USA.
EM jaoudiy1@udayton.edu; cyakopcic1@udayton.edu; tarek.taha@udayton.edu
CR Ahmad S, 2017, NEUROCOMPUTING, V262, P134, DOI 10.1016/j.neucom.2017.04.070
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Fu TC, 2011, ENG APPL ARTIF INTEL, V24, P164, DOI 10.1016/j.engappai.2010.09.007
   Habeeb RAA, 2019, INT J INFORM MANAGE, V45, P289, DOI 10.1016/j.ijinfomgt.2018.08.006
   Kang MJ, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0155781
   Lee H, 2017, ANN CONF PRIV SECUR, P57, DOI 10.1109/PST.2017.00017
   Miller C., 2013, DEF CON, V21, P15
   Müter M, 2011, IEEE INT VEH SYM, P1110, DOI 10.1109/IVS.2011.5940552
   Prasad NR, 2009, CMC-COMPUT MATER CON, V14, P1, DOI 10.1145/1541880.1541882
   Rasmussen D, 2019, NEUROINFORMATICS, V17, P611, DOI 10.1007/s12021-019-09424-z
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Seo E., 2018 16 ANN C 1 SECU, P1, DOI DOI 10.1109/PST.2018.8514157
   Song HM, 2016, 2016 INTERNATIONAL CONFERENCE ON INFORMATION NETWORKING (ICOIN), P63, DOI 10.1109/ICOIN.2016.7427089
   Taylor A, 2016, PROCEEDINGS OF 3RD IEEE/ACM INTERNATIONAL CONFERENCE ON DATA SCIENCE AND ADVANCED ANALYTICS, (DSAA 2016), P130, DOI 10.1109/DSAA.2016.20
   Wang CD, 2018, IEEE ACCESS, V6, P9091, DOI 10.1109/ACCESS.2018.2799210
   Zang D, 2018, CHIN CONT DECIS CONF, P1059, DOI 10.1109/CCDC.2018.8407286
NR 16
TC 0
Z9 0
U1 0
U2 1
PY 2020
UT WOS:000803074600021
DA 2023-11-16
ER

PT C
AU Romanov, EL
   Novitskaya, YV
AF Romanov, Evgeniy L.
   Novitskaya, Yuliya V.
GP IEEE
TI Program Toolkit for Auditory Scenes' Analysis
SO 2016 13TH INTERNATIONAL SCIENTIFIC-TECHNICAL CONFERENCE ON ACTUAL
   PROBLEMS OF ELECTRONIC INSTRUMENT ENGINEERING (APEIE), VOL 2
SE International Conference on Actual Problems of Electronic Instrument
   Engineering
DT Proceedings Paper
CT 13th International Scientific-Technical Conference on Actual Problems of
   Electronics Instrument Engineering (APEIE)
CY OCT 03-06, 2016
CL Novosibirsk, RUSSIA
DE Auditory scene; spiking neural network; Java; spectrum; cochleagram
AB In this paper the architecture of program toolkit for auditory scenes' analysis and the features of spiking neural networks and neuromorphic structures using are proposed. The programming examples and the test results of main components are given.
C1 [Romanov, Evgeniy L.; Novitskaya, Yuliya V.] Novosibirsk State Tech Univ, Novosibirsk, Russia.
RP Romanov, EL (corresponding author), Novosibirsk State Tech Univ, Novosibirsk, Russia.
CR Aleksandrov Yu.I., 2008, NEURON SIGNAL PROCES
   [Anonymous], SWARM ALGORITHMS
   [Anonymous], API DAT PAR JAV EL R
   Gavrilov Andrey V., 2016, P 11 INT FOR STRAT T
   Karpov Y.G., 2010, MODEL CHECKING VERIF
   Kashchenko S.A., 2013, MODELS WAVE MEMORY
   Ma N., EFFICIENT IMPLEMENTA
   Nicholls JG, 2003, NEURON BRAIN
   Romanov E. L., 2016, ROB ART INT P 7 ALL, P155
   Wang D., COMPUTATIONAL AUDITO
NR 10
TC 0
Z9 0
U1 0
U2 0
PY 2016
BP 464
EP 470
UT WOS:000392625500107
DA 2023-11-16
ER

PT J
AU Li, XM
   Chen, Q
   Xue, FZ
AF Li, Xiumin
   Chen, Qing
   Xue, Fangzheng
TI Bursting dynamics remarkably improve the performance of neural networks
   on liquid computing
SO COGNITIVE NEURODYNAMICS
DT Article
DE Spiking; Bursting; Liquid computing
ID PANCREATIC BETA-CELLS; STOCHASTIC RESONANCE; SELECTIVE COMMUNICATION;
   SPIKING NEURONS; SYNCHRONIZATION; INFORMATION; PLASTICITY; STATES;
   ORDER; MODEL
AB Burst firings are functionally important behaviors displayed by neural circuits, which plays a primary role in reliable transmission of electrical signals for neuronal communication. However, with respect to the computational capability of neural networks, most of relevant studies are based on the spiking dynamics of individual neurons, while burst firing is seldom considered. In this paper, we carry out a comprehensive study to compare the performance of spiking and bursting dynamics on the capability of liquid computing, which is an effective approach for intelligent computation of neural networks. The results show that neural networks with bursting dynamic have much better computational performance than those with spiking dynamics, especially for complex computational tasks. Further analysis demonstrate that the fast firing pattern of bursting dynamics can obviously enhance the efficiency of synaptic integration from pre-neurons both temporally and spatially. This indicates that bursting dynamic can significantly enhance the complexity of network activity, implying its high efficiency in information processing.
C1 [Li, Xiumin; Chen, Qing; Xue, Fangzheng] Chongqing Univ, Key Lab Dependable Serv Comp Cyber Phys Soc, Minist Educ, Chongqing 400044, Peoples R China.
   [Li, Xiumin; Chen, Qing; Xue, Fangzheng] Chongqing Univ, Coll Automat, Chongqing 400044, Peoples R China.
RP Li, XM (corresponding author), Chongqing Univ, Key Lab Dependable Serv Comp Cyber Phys Soc, Minist Educ, Chongqing 400044, Peoples R China.; Li, XM (corresponding author), Chongqing Univ, Coll Automat, Chongqing 400044, Peoples R China.
EM xmli@cqu.edu.cn
CR BENZI R, 1981, J PHYS A-MATH GEN, V14, pL453, DOI 10.1088/0305-4470/14/11/006
   Bertram R, 2000, J BIOSCIENCE, V25, P197, DOI 10.1007/BF03404915
   Burgsteiner H., 2005, THESIS
   Burgsteiner Harald, 2005, P 9 INT C ENG APPL N, P129
   Dambre J, 1995, AM J HYPERTENS, V8
   Gammaitoni L, 1998, REV MOD PHYS, V70, P223, DOI 10.1103/RevModPhys.70.223
   Gerstner W., 2002, SPIKING NEURON MODEL
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2002, BIOSYSTEMS, V67, P95, DOI 10.1016/S0303-2647(02)00067-9
   Izhikevich EM, 2003, TRENDS NEUROSCI, V26, P161, DOI 10.1016/S0166-2236(03)00034-1
   Jaeger H, 2001, 14834 GMD GERM NAT R, V148, P34
   Joshi P, 2004, LECT NOTES COMPUT SC, V3141, P258
   Kim SY, 2015, COGN NEURODYNAMICS, V9, P411, DOI 10.1007/s11571-015-9334-4
   Kim SJ, 2015, INT J SECUR APPL, V9, P9
   Kosko B, 2004, PHYS REV E, V70, DOI 10.1103/PhysRevE.70.031911
   Li XM, 2007, PHYS REV E, V76, DOI 10.1103/PhysRevE.76.041902
   Lisman JE, 1997, TRENDS NEUROSCI, V20, P38, DOI 10.1016/S0166-2236(96)10070-9
   Llinás RR, 2006, J NEUROPHYSIOL, V95, P3297, DOI 10.1152/jn.00166.2006
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   MAASS W, 2002, NEURAL INFORM PROCES, P213
   Maass W, 2007, PLOS COMPUT BIOL, V3, P15, DOI 10.1371/journal.pcbi.0020165
   Maass W, 2007, LECT NOTES COMPUT SC, V4497, P507
   Meng P, 2013, COGN NEURODYNAMICS, V7, P197, DOI 10.1007/s11571-012-9226-9
   Moulins M, 1987, CRUSTACEAN STOMATOGA, P330
   Norton D, 2010, NEUROCOMPUTING, V73, P2893, DOI 10.1016/j.neucom.2010.08.005
   Saha AA, 2003, SIGNAL PROCESS, V83, P1193, DOI 10.1016/S0165-1684(03)00039-2
   Schrauwen B, 2008, NEUROCOMPUTING, V71, P1159, DOI 10.1016/j.neucom.2007.12.020
   Sherman SM, 2001, TRENDS NEUROSCI, V24, P122, DOI 10.1016/S0166-2236(00)01714-8
   Shew WL, 2011, J NEUROSCI, V31, P55, DOI 10.1523/JNEUROSCI.4637-10.2011
   Shi X, 2008, COGN NEURODYNAMICS, V2, P195, DOI 10.1007/s11571-008-9055-z
   Sohal VS, 2001, NEURON, V31, P3, DOI 10.1016/S0896-6273(01)00349-X
   WIESENFELD K, 1995, NATURE, V373, P33, DOI 10.1038/373033a0
   Xue FZ, 2013, NEUROCOMPUTING, V122, P324, DOI 10.1016/j.neucom.2013.06.019
NR 34
TC 6
Z9 6
U1 0
U2 13
PD OCT
PY 2016
VL 10
IS 5
BP 415
EP 421
DI 10.1007/s11571-016-9387-z
UT WOS:000383319300005
DA 2023-11-16
ER

PT C
AU Ciurletti, M
   Traub, M
   Karlbauer, M
   Butz, MV
   Otte, S
AF Ciurletti, Melvin
   Traub, Manuel
   Karlbauer, Matthias
   Butz, Martin, V
   Otte, Sebastian
BE Farkas, I
   Masulli, P
   Otte, S
   Wermter, S
TI Signal Denoising with Recurrent Spiking Neural Networks and Active
   Tuning
SO ARTIFICIAL NEURAL NETWORKS AND MACHINE LEARNING, ICANN 2021, PT V
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 30th International Conference on Artificial Neural Networks (ICANN)
CY SEP 14-17, 2021
CL ELECTR NETWORK
DE Recurrent spiking neural networks; Signal denoising; Active Tuning;
   Temporal gradients
AB Active Tuning is an optimization paradigm specifically designed to increase the robustness and generalization ability of temporal forward models like recurrent neural networks (RNNs). This work explores how the Active Tuning method can be used to optimize the internal dynamics of recurrent spiking neural networks (RSNNs). Active Tuning decouples the network from direct influence of the data stream and instead tunes its internal dynamics. This is based on the temporal gradient signals from propagating the error between outputs and observations backwards through time. Meanwhile, the network is running in a closed-loop prediction cycle, where the own output is used as the next input. As modern ANNs often demand excessive amounts of computational resources, spiking neural networks (SNNs) aim for the energy efficiency demonstrated by the human brain. This is accomplished by using an event-driven model inspired by the spiking behavior of biological neurons. Target of the Active Tuning optimization in RSNNs is the membrane potential of the neurons in the hidden layer. We show in two scenarios how RSNNs handle noisy inputs and that Active Tuning is a reliable method to increase their robustness as well as general prediction performance.
C1 [Ciurletti, Melvin; Traub, Manuel; Karlbauer, Matthias; Butz, Martin, V; Otte, Sebastian] Univ Tubingen, Comp Sci Dept, Neurocognit Modeling, Sand 14, D-72076 Tubingen, Germany.
RP Otte, S (corresponding author), Univ Tubingen, Comp Sci Dept, Neurocognit Modeling, Sand 14, D-72076 Tubingen, Germany.
EM melvin.ciurletti@student.uni-tuebingen.de;
   manuel.traub@uni-tuebingen.de; matthias.karlbauer@uni-tuebingen.de;
   martin.butz@uni-tuebingen.de; sebastian.otte@uni-tuebingen.de
CR Bellec G., 2018, ADV NEURAL INFORM PR
   Bellec G, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-17236-y
   Esser SK, 2016, P NATL ACAD SCI USA, V113, P11441, DOI 10.1073/pnas.1604850113
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Hubara I., 2016, ADV NEURAL INFORM PR, P4114, DOI DOI 10.5555/3157382.3157557
   Jaeger, 2001, 148 FRAUNH I AN INF
   Kingma DP., 2017, ARXIV
   Korsch HJ., 2008, CHAOS PROGRAM COLLEC, DOI [10.1007/978-3-540-74867-0, DOI 10.1007/978-3-540-74867-0]
   Koryakin D, 2012, NEURAL NETWORKS, V36, P35, DOI 10.1016/j.neunet.2012.08.008
   Otte, 2020, ARXIV201003958
   Otte S, 2016, NEUROCOMPUTING, V192, P128, DOI 10.1016/j.neucom.2016.01.088
   Paugam-Moisy H., 2012, HDB NATURAL COMPUTIN, V1, P1, DOI [DOI 10.1007/978-3-540-92910-9_10, 10.1007/978-3-540-92910-9_10]
   Press W., 2007, NUMERICAL RECIPES AR, V3rd, DOI DOI 10.2277/052143064X
   Schmichuber J, 2007, NEURAL COMPUT, V19, P757, DOI 10.1162/neco.2007.19.3.757
NR 14
TC 0
Z9 0
U1 1
U2 4
PY 2021
VL 12895
BP 220
EP 232
DI 10.1007/978-3-030-86383-8_18
UT WOS:000711936300018
DA 2023-11-16
ER

PT C
AU Galluppi, F
   Davies, S
   Furber, S
   Stewart, T
   Eliasmith, C
AF Galluppi, Francesco
   Davies, Sergio
   Furber, Steve
   Stewart, Terry
   Eliasmith, Chris
GP IEEE
TI Real Time On-Chip Implementation of Dynamical Systems with Spiking
   Neurons
SO 2012 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUN 10-15, 2012
CL Brisbane, AUSTRALIA
ID MODEL; CIRCUIT
AB Simulation of large-scale networks of spiking neurons has become appealing for understanding the computational principles of the nervous system by producing models based on biological evidence. In particular, networks that can assume a variety of (dynamically) stable states have been proposed as the basis for different behavioural and cognitive functions.
   This work focuses on implementing the Neural Engineering Framework (NEF), a formal method for mapping attractor networks and control-theoretic algorithms to biologically plausible networks of spiking neurons, on the SpiNNaker system, a massive programmable parallel architecture oriented to the simulation of networks of spiking neurons. We describe how to encode and decode analog values to patterns of neural spikes directly on chip. These methods take advantage of the full programmability of the ARM968 cores constituting the processing base of a SpiNNaker node, and exploit the fast Network-on-chip for spike communication.
   In this paper we focus on the fundamentals of representing, transforming and implementing dynamics in spiking networks. We show real time simulation results demonstrating the NEF principles and discuss advantages, precision and scalability. More generally, the present approach can be used to state and test hypotheses with large-scale spiking neural network models for a range of different cognitive functions and behaviours.
C1 [Galluppi, Francesco; Davies, Sergio; Furber, Steve] Univ Manchester, Adv Processor Technol Grp, Manchester M13 9PL, Lancs, England.
   [Stewart, Terry; Eliasmith, Chris] Univ Waterloo, Ctr Theoret Neurosci, Waterloo, ON N2L 3G1, Canada.
RP Galluppi, F (corresponding author), Univ Manchester, Adv Processor Technol Grp, Manchester M13 9PL, Lancs, England.
EM francesco.galluppi@cs.man.ac.uk
CR Amit D. J., 1992, MODELING BRAIN FUNCT
   [Anonymous], 2009, P C HIGH PERFORMANCE
   [Anonymous], 2010, 2010 IEEE INT S PARA
   [Anonymous], 2011, 45 ANN C INFORM SCI
   [Anonymous], P INT C APPL CONC SY, DOI DOI 10.1109/ACSD.2009.17
   Binzegger T, 2004, J NEUROSCI, V24, P8441, DOI 10.1523/JNEUROSCI.1400-04.2004
   CELEBRINI S, 1993, VISUAL NEUROSCI, V10, P811, DOI 10.1017/S0952523800006052
   Dehaene S, 1998, P NATL ACAD SCI USA, V95, P14529, DOI 10.1073/pnas.95.24.14529
   DeSchutter E, 2009, COMPUT NEUROSCI-MIT, P1
   Dethier J, 2011, I IEEE EMBS C NEUR E, P396, DOI 10.1109/NER.2011.5910570
   Eliasmith C, 2005, NEURAL COMPUT, V17, P1276, DOI 10.1162/0899766053630332
   Eliasmith C, 2002, NEUROCOMPUTING, V44, P1071, DOI 10.1016/S0925-2312(02)00418-6
   Eliasmith C, 2012, BUILD BRAIN IN PRESS
   Eliasmith C., 2003, NEURAL ENG COMPUTATI
   FUKUSHIMA K, 1992, PROG NEUROBIOL, V39, P609, DOI 10.1016/0301-0082(92)90016-8
   Furber S., 2006, AISB06 WORKSH GC5 AR
   Furber S B, 2006, ON CHIP INTERCHIP NE
   Galluppi Francesco, 2012, P CF
   Hynna KM, 2006, IEEE INT SYMP CIRC S, P3614
   Indiveri G, 2006, IEEE T NEURAL NETWOR, V17, P211, DOI 10.1109/TNN.2005.860850
   Izhikevich EM, 2008, P NATL ACAD SCI USA, V105, P3593, DOI 10.1073/pnas.0712231105
   Kuo PD, 2005, BIOL CYBERN, V93, P178, DOI 10.1007/s00422-005-0576-9
   LAZZARO J, 1993, IEEE T NEURAL NETWOR, V4, P523, DOI 10.1109/72.217193
   Lewis F., 1994, IEEE T AUTOMATIC CON, V39, P1773
   Maguire LP, 2007, NEUROCOMPUTING, V71, P13, DOI 10.1016/j.neucom.2006.11.029
   Nageswaran J. M., 2007, NEURAL NETWORKS, V22
   Plana LA, 2007, IEEE DES TEST COMPUT, V24, P454, DOI 10.1109/MDT.2007.149
   Rast Alexander, 2011, NEURAL NETW IN PRESS
   Schemmel J, 2010, IEEE INT SYMP CIRC S, P1947, DOI 10.1109/ISCAS.2010.5536970
   Singh R, 2006, J NEUROSCI, V26, P3667, DOI 10.1523/JNEUROSCI.4864-05.2006
   Stewart TC, 2011, 33 ANN C COGN SCI SO
   Stewart TC, 2009, FRONTIERS NEUROINFOR, V3
   Stewart TC, 2010, FRONT ARTIF INTEL AP, V221, P147, DOI 10.3233/978-1-60750-661-4-147
   Stewart TC, 2011, CONNECT SCI, V23, P145, DOI 10.1080/09540091.2011.571761
   Thomson AM, 2007, FRONTIERS NEUROSCIEN
   Wijekoon JHB, 2008, IEEE INT SYMP CIRC S, P1784, DOI 10.1109/ISCAS.2008.4541785
   Wills TJ, 2005, SCIENCE, V308, P873, DOI 10.1126/science.1108905
NR 37
TC 1
Z9 1
U1 0
U2 4
PY 2012
UT WOS:000309341302065
DA 2023-11-16
ER

PT J
AU Sarkar, ST
   Bhondekar, AP
   Macas, M
   Kumar, R
   Kaur, R
   Sharma, A
   Gulati, A
   Kumar, A
AF Sarkar, Sankho Turjo
   Bhondekar, Amol P.
   Macas, Martin
   Kumar, Ritesh
   Kaur, Rishemjit
   Sharma, Anupma
   Gulati, Ashu
   Kumar, Amod
TI Towards biological plausibility of electronic noses: A spiking neural
   network based approach for tea odour classification
SO NEURAL NETWORKS
DT Article
DE Electronic nose; McNemar's test; Spiking neural network; Tea; Spike
   latency coding; Dynamically evolving spiking neural networks
ID SIGNAL-PROCESSING TECHNIQUES; ARTIFICIAL OLFACTORY MUCOSA; NEURONAL
   NETWORK; RECOGNITION; SYSTEM; MODEL
AB The paper presents a novel encoding scheme for neuronal code generation for odour recognition using an electronic nose (EN). This scheme is based on channel encoding using multiple Gaussian receptive fields superimposed over the temporal EN responses. The encoded data is further applied to a spiking neural network (SNN) for pattern classification. Two forms of SNN, a back-propagation based SpikeProp and a dynamic evolving SNN are used to learn the encoded responses. The effects of information encoding on the performance of SNNs have been investigated. Statistical tests have been performed to determine the contribution of the SNN and the encoding scheme to overall odour discrimination. The approach has been implemented in odour classification of orthodox black tea (Kangra-Himachal Pradesh Region) thereby demonstrating a biomimetic approach for EN data analysis. (C) 2015 Elsevier Ltd. All rights reserved.
C1 [Sarkar, Sankho Turjo; Bhondekar, Amol P.; Kumar, Ritesh; Kaur, Rishemjit; Sharma, Anupma; Kumar, Amod] CSIR Cent Sci Instruments Org, Chandigarh, India.
   [Gulati, Ashu] CSIR Inst Himalayan Bioresource Technol, Palampur, Himachal Prades, India.
   [Sarkar, Sankho Turjo; Bhondekar, Amol P.; Kumar, Ritesh; Kaur, Rishemjit; Sharma, Anupma; Kumar, Amod] Acad Sci & Innovat Res, New Delhi, India.
   [Macas, Martin] Czech Tech Univ, CR-16635 Prague, Czech Republic.
RP Bhondekar, AP (corresponding author), CSIR Cent Sci Instruments Org, Chandigarh, India.
EM amol.bhondekar@gmail.com
CR Abdel-Aty-Zohdy HS, 2010, MIDWEST SYMP CIRCUIT, P81, DOI 10.1109/MWSCAS.2010.5548566
   Ache BW, 2005, NEURON, V48, P417, DOI 10.1016/j.neuron.2005.10.022
   Al Yamani J., 2011, FRONTIERS NEUROENGIN, V4
   Allen JN, 2008, IEEE INT SYMP CIRC S, P2178, DOI 10.1109/ISCAS.2008.4541883
   Ambros-Ingerson J., 1990, SCIENCE
   [Anonymous], 2011, GRADING SORTING PACK
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Borah S, 2007, J FOOD ENG, V79, P629, DOI 10.1016/j.jfoodeng.2006.02.022
   Bostanci B, 2013, ADV INTELL SYST, V201, P15, DOI 10.1007/978-81-322-1038-2_2
   Chen HT, 2011, IEEE T BIOMED CIRC S, V5, P160, DOI 10.1109/TBCAS.2010.2075928
   Covington JA, 2007, IET NANOBIOTECHNOL, V1, P15, DOI 10.1049/iet-nbt:20060015
   Delorme A, 2001, NEURAL NETWORKS, V14, P795, DOI 10.1016/S0893-6080(01)00049-1
   Gardner J. W., 2007, SOL STAT SENS ACT MI, P2465
   Gardner JW, 2009, IEEE SENS J, V9, P929, DOI 10.1109/JSEN.2009.2024856
   Gerstner W., 2002, SPIKING NEURON MODEL
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Gutierrez-Galvez A, 2006, SENSOR ACTUAT B-CHEM, V116, P29, DOI 10.1016/j.snb.2005.11.081
   Hojjat A., 2010, AUTOMATED EEG BASED
   Hsieh HY, 2013, IEEE T NEUR NET LEAR, V24, P2063, DOI 10.1109/TNNLS.2013.2271644
   Hsieh HY, 2012, 2012 IEEE ASIA PACIFIC CONFERENCE ON CIRCUITS AND SYSTEMS (APCCAS), P88, DOI 10.1109/APCCAS.2012.6418978
   Hsieh HY, 2012, IEEE T NEUR NET LEAR, V23, P1065, DOI 10.1109/TNNLS.2012.2195329
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Kasabov N, 2013, NEURAL NETWORKS, V41, P188, DOI 10.1016/j.neunet.2012.11.014
   Kaur R., 2012, SENSORS ACTUATORS B
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maass W, 2001, PULSED NEURAL NETWOR
   Martinelli E., 2011, FRONTIERS NEUROENGIN, V4
   Martinelli E, 2006, SENSOR ACTUAT B-CHEM, V119, P234, DOI 10.1016/j.snb.2005.12.029
   Masaru F, 2008, IEEE IJCNN, P840, DOI 10.1109/IJCNN.2008.4633895
   Natschlager T, 1998, NETWORK-COMP NEURAL, V9, P319, DOI 10.1088/0954-898X/9/3/003
   Nski A. K., 2011, ACTA NEUROBIOLOGIAE, V71, P409
   Pearce TC, 2001, NEUROCOMPUTING, V38, P299, DOI 10.1016/S0925-2312(01)00455-6
   PERSAUD K, 1982, NATURE, V299, P352, DOI 10.1038/299352a0
   Raman B., 2004, CHEMOSENSORY PROCESS, V2, P3
   Raman B, 2008, ANAL CHEM, V80, P8364, DOI 10.1021/ac8007048
   Raman B, 2007, IEEE SENS J, V7, P506, DOI 10.1109/JSEN.2007.891935
   Ratton L, 1997, SENSOR ACTUAT B-CHEM, V41, P105, DOI 10.1016/S0925-4005(97)80283-3
   Salzberg S. J. H. U., 1997, DATA MIN KNOWL DISC, V328, P317
   Smear M, 2011, NATURE, V479, P397, DOI 10.1038/nature10521
   SNIPPE HP, 1992, BIOL CYBERN, V66, P543, DOI 10.1007/BF00204120
   Thorpe S, 1998, COMPUTATIONAL NEUROSCIENCE: TRENDS IN RESEARCH, P113
   Wakamatsu T, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P951, DOI 10.1109/IJCNN.2011.6033325
   White J, 1999, NEUROCOMPUTING, V26-7, P919, DOI 10.1016/S0925-2312(98)00137-4
   White J, 1998, BIOL CYBERN, V78, P245, DOI 10.1007/s004220050430
   Wu QX, 2006, NEUROCOMPUTING, V69, P1912, DOI 10.1016/j.neucom.2005.11.023
NR 45
TC 20
Z9 22
U1 3
U2 43
PD NOV
PY 2015
VL 71
BP 142
EP 149
DI 10.1016/j.neunet.2015.07.014
UT WOS:000364160900013
DA 2023-11-16
ER

PT J
AU Bodyanskiy, Y
   Dolotov, A
   Vynokurova, O
AF Bodyanskiy, Ye
   Dolotov, A.
   Vynokurova, O.
TI Evolving spiking wavelet-neuro-fuzzy self-learning system
SO APPLIED SOFT COMPUTING
DT Article
DE Computational intelligence; Hybrid evolving system; Multilayered spiking
   neural network; Self-learning; Control systems theory; Wavelet; Fuzzy
   clustering
ID NETWORK
AB The paper introduces several modifications to self-learning fuzzy spiking neural network that is used as a base for evolving system design. The adaptive wavelet activation-membership functions are utilized to improve and generalize receptive neuron activation functions and the temporal Hebbian learning algorithm. The proposed evolving spiking wavelet-neuro-fuzzy self-learning system retains native features of spiking neurons and reveals evolving systems' capabilities in detecting overlapping clusters of irregular form. (C) 2013 Elsevier B. V. All rights reserved.
C1 [Bodyanskiy, Ye; Dolotov, A.; Vynokurova, O.] Kharkiv Natl Univ Radio Elect, UA-61166 Kharkov, Ukraine.
RP Bodyanskiy, Y (corresponding author), Kharkiv Natl Univ Radio Elect, 14 Lenin Ave,Off 511, UA-61166 Kharkov, Ukraine.
EM bodya@kture.kharkov.ua
CR [Anonymous], 2005, P 1 INT WORKSH GEN F
   [Anonymous], 1997, NEUROFUZZY SOFT COMP
   Bodyanskiy Y., 2008, INT J ARTIF INTELL M, V8, P9
   Bodyanskiy Y, 2008, PRO BIENN BALT EL C, P213, DOI 10.1109/BEC.2008.4657517
   Bodyanskiy Ye, 2009, SCI J RIGA TU, P66
   Bodyanskiy Ye., 2010, P 17 ZITT E W FUZZ C, P47
   Bodyanskiy Ye., 2009, IMAGE PROCESSING, P357
   Bodyanskiy Ye, 2008, SCI P RIG TU INF TEC, P27
   Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   Budka M, 2013, STUD COMPUT INTELL, V457, P177, DOI 10.1007/978-3-642-34300-1_17
   Butkiewicz BS, 2005, LECT NOTES COMPUT SC, V3528, P76
   COTTRELL M, 1986, BIOL CYBERN, V53, P405, DOI 10.1007/BF00318206
   De Berredo R. C., 2005, THESIS PONTIFICAL CA, P118
   Dolotov A., 2008, WISSENSCHAFTLICHE BE, V100, pS53
   Gerstner W., 2002, SPIKING NEURON MODEL
   Kasabov N, 2006, ADV SOFT COMP, P521, DOI 10.1007/3-540-34783-6_51
   Lughofer E, 2011, STUD FUZZ SOFT COMP, V266, P1, DOI 10.1007/978-3-642-18087-3
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maass W., 1998, PULSED NEURAL NETWOR, P408
   Maass W, 2007, LECT NOTES COMPUT SC, V4497, P507
   Meftah B, 2008, IEEE IJCNN, P681, DOI 10.1109/IJCNN.2008.4633868
   Mitaim S, 1997, 1997 IEEE INTERNATIONAL CONFERENCE ON NEURAL NETWORKS, VOLS 1-4, P537, DOI 10.1109/ICNN.1997.611726
   Mitaim S., 1996, P 5 IEEE INT C FUZZ, V2, P1213
   Nakamori Yo., 1996, FUZZY MODELLING PARA, P331
   Natschlager T, 1998, NETWORK-COMP NEURAL, V9, P319, DOI 10.1088/0954-898X/9/3/003
   Rutkowski L, 2008, COMPUTATIONAL INTELL, P514
NR 26
TC 9
Z9 10
U1 0
U2 10
PD JAN
PY 2014
VL 14
BP 252
EP 258
DI 10.1016/j.asoc.2013.05.020
PN B
UT WOS:000327528300009
DA 2023-11-16
ER

PT C
AU Bomberger, NA
   Waxman, AM
   Pait, FM
AF Bomberger, NA
   Waxman, AM
   Pait, FM
BE Dasarathy, BV
TI Spiking neural networks for higher-level information fusion
SO MULTISENSOR, MULTISOURCE INFORMATION FUSION: ARCHITECTURES, ALGORITHMS,
   AND APPLICATONS 2004
SE Proceedings of SPIE
DT Proceedings Paper
CT Conference on Multisensor, Multisource Information Fusion
CY APR 14-15, 2004
CL ORLANDO, FL
DE information fusion; fusion 2+; higher-level fusion; situation
   assessment; threat assessment; spiking neural networks; semantic
   knowledge representation; knowledge networks; knowledge hierarchy;
   associative learning
ID REPRESENTATION
AB This paper presents a novel approach to higher-level (2+) information fusion and knowledge representation using semantic networks composed of coupled spiking neuron nodes. Networks of spiking neurons have been shown to exhibit synchronization, in which sub-assemblies of nodes become phase locked to one another. This phase locking reflects the tendency of biological neural systems to produce synchronized neural assemblies, which have been hypothesized to be involved in feature binding. The approach in this paper embeds spiking neurons in a semantic network, in which a synchronized sub-assembly of nodes represents a hypothesis about a situation. Likewise, multiple synchronized assemblies that are out-of-phase with one another represent multiple hypotheses. The initial network is hand-coded, but additional semantic relationships can be established by associative learning mechanisms. This approach is demonstrated with a simulated scenario involving the tracking of suspected criminal vehicles between meeting places in an urban environment.
C1 ALPHATECH Inc, Fus Technol & Syst Div, Burlington, MA 01803 USA.
RP Bomberger, NA (corresponding author), ALPHATECH Inc, Fus Technol & Syst Div, Burlington, MA 01803 USA.
EM waxman@alphatech.com
CR Bartfai G, 2000, STUD FUZZ SOFT COMP, V48, P87
   ENDSLEY MR, 1995, HUM FACTORS, V37, P32, DOI 10.1518/001872095779049543
   Gerstner W., 2002, SPIKING NEURON MODEL
   Gray CM, 1999, NEURON, V24, P31, DOI 10.1016/S0896-6273(00)80820-X
   Hebb D., 2005, ORG BEHAV NEUROPSYCH
   HINMAN ML, 2002, 5 INT C INF FUS ANN
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Horn D, 1998, PULSED NEURAL NETWORKS, P297
   IVEY RT, 2003, 6 INT C INF FUS CAIR
   JORDAN MI, 1995, HDB BRAIN THEORY NEU, P579
   SHASTRI L, 1993, BEHAV BRAIN SCI, V16, P417, DOI 10.1017/S0140525X00030910
   Shastri L, 1999, APPL INTELL, V11, P79, DOI 10.1023/A:1008380614985
   SINGER W, 1995, HDB BRAIN THEORY NEU
   Steinberg AN, 1999, P SOC PHOTO-OPT INS, V3719, P430, DOI 10.1117/12.341367
   Tyler LK, 2001, TRENDS COGN SCI, V5, P244, DOI 10.1016/S1364-6613(00)01651-X
   von der Malsburg C, 1999, NEURON, V24, P95, DOI 10.1016/S0896-6273(00)80825-9
   Waxman A. M., 2002, 5 INT C INF FUS ANN
   WAXMAN AM, 2002, P 5 INT MIL SENS S G
   White F. E, 1987, DATA FUSION LEXICON
NR 19
TC 1
Z9 1
U1 0
U2 1
PY 2004
VL 5434
BP 249
EP 260
DI 10.1117/12.555425
UT WOS:000222944000025
DA 2023-11-16
ER

PT J
AU Vicente-Sola, A
   Manna, DL
   Kirkland, P
   Di Caterina, G
   Bihl, T
AF Vicente-Sola, Alex
   Manna, Davide L.
   Kirkland, Paul
   Di Caterina, Gaetano
   Bihl, Trevor
TI Keys to accurate feature extraction using residual spiking neural
   networks
SO NEUROMORPHIC COMPUTING AND ENGINEERING
DT Article
DE feature extraction; image classification; neural networks; neuromorphic;
   residual network; spiking
ID LOIHI
AB Spiking neural networks (SNNs) have become an interesting alternative to conventional artificial neural networks (ANN) thanks to their temporal processing capabilities and energy efficient implementations in neuromorphic hardware. However, the challenges involved in training SNNs have limited their performance in terms of accuracy and thus their applications. Improving learning algorithms and neural architectures for a more accurate feature extraction is therefore one of the current priorities in SNN research. In this paper we present a study on the key components of modern spiking architectures. We design a spiking version of the successful residual network architecture and provide an in-depth study on the possible implementations of spiking residual connections. This study shows how, depending on the use case, the optimal residual connection implementation may vary. Additionally, we empirically compare different techniques in image classification datasets taken from the best performing networks. Our results provide a state of the art guide to SNN design, which allows to make informed choices when trying to build the optimal visual feature extractor. Finally, our network outperforms previous SNN architectures in CIFAR-10 (94.14%) and CIFAR-100 (74.65%) datasets and matches the state of the art in DVS-CIFAR10 (72.98%), with less parameters than the previous state of the art and without the need for ANN-SNN conversion. Code available at: https://github.com/VicenteAlex/Spiking_ResNet.
C1 [Vicente-Sola, Alex; Manna, Davide L.; Kirkland, Paul; Di Caterina, Gaetano] Univ Strathclyde, Ctr Image & Signal Proc Elect & Elect Engn, Neuromorph Sensor Signal Proc Lab, Glasgow, Scotland.
   [Bihl, Trevor] Air Force Res Lab, Wright Patterson AFB, OH USA.
RP Vicente-Sola, A (corresponding author), Univ Strathclyde, Ctr Image & Signal Proc Elect & Elect Engn, Neuromorph Sensor Signal Proc Lab, Glasgow, Scotland.
EM alex.vicente-sola@strath.ac.uk
CR Abbott LF, 1999, BRAIN RES BULL, V50, P303, DOI 10.1016/S0361-9230(99)00161-6
   Bing Han, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P388, DOI 10.1007/978-3-030-58607-2_23
   Bing Han, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13555, DOI 10.1109/CVPR42600.2020.01357
   Cassidy AS, 2013, IEEE IJCNN
   Cooijmans T., 2016, ARXIV
   Davies M, 2021, P IEEE, V109, P911, DOI 10.1109/JPROC.2021.3067593
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Deng L, 2020, NEURAL NETWORKS, V121, P294, DOI 10.1016/j.neunet.2019.09.005
   Deng S., 2021, INT C LEARNING REPRE, P2328
   Esser SK, 2016, P NATL ACAD SCI USA, V113, P11441, DOI 10.1073/pnas.1604850113
   Anthony LFW, 2020, Arxiv, DOI arXiv:2007.03051
   Falkner S, 2018, PR MACH LEARN RES, V80
   Fang W., 2020, SPIKINGJELLY
   Fang W, 2021, ADV NEUR IN, V34
   Fang W, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2641, DOI 10.1109/ICCV48922.2021.00266
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huh D, 2018, ADV NEUR IN, V31
   Kaiser J, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00424
   Kim Y, 2021, FRONT NEUROSCI-SWITZ, V15, DOI 10.3389/fnins.2021.773954
   Kugele A, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00439
   Kusmierz L, 2017, CURR OPIN NEUROBIOL, V46, P170, DOI 10.1016/j.conb.2017.08.020
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lee C, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00119
   Li HM, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00309
   Mostafa H, 2018, IEEE T NEUR NET LEAR, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Mostafa H, 2018, NEURAL COMPUT, V30, P1542, DOI 10.1162/neco_a_01080
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Orchard G, 2021, IEEE WRK SIG PRO SYS, P254, DOI 10.1109/SiPS52927.2021.00053
   Paszke A, 2019, ADV NEUR IN, V32
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Severa W, 2019, NAT MACH INTELL, V1, P86, DOI 10.1038/s42256-018-0015-y
   Shrestha SB, 2018, ADV NEUR IN, V31
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   St”ckl C, 2020, Arxiv, DOI arXiv:2001.01682
   Stöckl C, 2021, NAT MACH INTELL, V3, DOI 10.1038/s42256-021-00311-4
   Vicente-Sola Alex, 2022, Figshare, DOI 10.6084/m9.figshare.20712535.v2
   Wang YX, 2021, IEEE T COGN DEV SYST, V13, P514, DOI 10.1109/TCDS.2020.2971655
   Wu JB, 2022, IEEE T PATTERN ANAL, V44, P7824, DOI 10.1109/TPAMI.2021.3114196
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Zheng HL, 2021, AAAI CONF ARTIF INTE, V35, P11062
NR 40
TC 1
Z9 1
U1 0
U2 0
PD DEC 1
PY 2022
VL 2
IS 4
AR 044001
DI 10.1088/2634-4386/ac8bef
UT WOS:001064852100001
DA 2023-11-16
ER

PT C
AU Mouraud, A
   Paugam-Moisy, H
   Puzenat, D
AF Mouraud, A
   Paugam-Moisy, H
   Puzenat, D
BE Fahringer, T
TI A distributed and multithreaded neural event driven simulation framework
SO PROCEEDINGS OF THE IASTED INTERNATIONAL CONFERENCE ON PARALLEL AND
   DISTRIBUTED COMPUTING AND NETWORKS
DT Proceedings Paper
CT IASTED International Conference on Parallel and Distributed Computing
   and Networks
CY FEB 14-16, 2006
CL Innsbruck, AUSTRIA
DE Spiking Neural Networks; Event-Driven Simulations; parallel computing;
   multi-threading; scheduling
ID SPIKING NEURONS; LARGE NETWORKS; SYNCHRONY; DYNAMICS; MODELS
AB In a Spiking Neural Networks (SNN), spike emissions are sparsely and irregularly distributed both in time and in the network architecture. Since a current feature of SNNs is a low average activity, efficient implementations of SNNs are usually based on an Event-Driven Simulation (EDS). On the other hand, simulations of large scale neural networks can take advantage of distributing the neurons on a set of processors (either workstation cluster or parallel computer). This article presents a large scale SNN simulation framework able to gather the benefits of EDS and parallel computing. Two levels of parallelism are combined: Distributed mapping of the neural topology, at the network level, and local multithreaded allocation of resources for simultaneous processing of events, at the neuron level. Based on the causality of events, a distributed solution is proposed for solving the complex problem of scheduling without synchronization barrier.
C1 Inst Cognit Sci, CNRS, UMR 5015, 67 Blvd Pinel, F-69675 Lyon, France.
   Univ Antilles Guyane, Lab GRIMAAG, Guadeloupe, France.
RP Mouraud, A (corresponding author), Inst Cognit Sci, CNRS, UMR 5015, 67 Blvd Pinel, F-69675 Lyon, France.
EM mouraud@isc.cnrs.fr; hpaugam@isc.cnrs.fr; dpuzenat@univ-ag.fr
CR [Anonymous], ADV NEURAL INFORM PR
   [Anonymous], 1998, BOOK GENESIS EXPLORI, DOI DOI 10.1007/978-1-4612-1634-63
   BONIFACE Y, 1999, P EUROPAR, P935
   Delorme A, 1999, NEUROCOMPUTING, V26-7, P989, DOI 10.1016/S0925-2312(99)00095-8
   Estévez PA, 2002, PARALLEL COMPUT, V28, P861, DOI 10.1016/S0167-8191(02)00078-9
   FERSHA A, 1995, PARALLEL DISTRIBUTED
   Gerstner W., 2002, SPIKING NEURON MODEL
   GRASSMANN C, 2002, ESANN 2002 P APR 200, P331
   GRASSMANN C, 1998, NC 98, P100
   Hellmich HH, 2005, IEEE IJCNN, P3261
   Hines ML, 1997, NEURAL COMPUT, V9, P1179, DOI 10.1162/neco.1997.9.6.1179
   Hopfield JJ, 2001, P NATL ACAD SCI USA, V98, P1282, DOI 10.1073/pnas.031567098
   Izhikevich EM, 2004, CEREB CORTEX, V14, P933, DOI 10.1093/cercor/bhh053
   JAHNKE A, 1997, INT C ART NEUR NETW, P1187
   Legenstein R, 2005, NEURAL COMPUT, V17, P2337, DOI 10.1162/0899766054796888
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maass W., 1999, PULSED NEURAL NETWOR
   Makino T, 2003, NEURAL COMPUT APPL, V11, P210, DOI 10.1007/s00521-003-0358-z
   Mattia M, 2000, NEURAL COMPUT, V12, P2305, DOI 10.1162/089976600300014953
   MEUNIER D, 2005, UNPUB INHIBITION SPI
   PAUGAMMOISY H, 1995, HDB BRAIN THEORY NEU, P605
   PREIS R, 2001, PDPTA 2001
   Reutimann J, 2003, NEURAL COMPUT, V15, P811, DOI 10.1162/08997660360581912
   ROCHEL O, 2003, EUR S ART NEUR NETW, P295
   Seiffert U, 2004, NEUROCOMPUTING, V57, P135, DOI 10.1016/j.neucom.2004.01.011
   Síma J, 2005, NEURAL COMPUT, V17, P2635, DOI 10.1162/089976605774320601
   Singer W, 1999, NEURON, V24, P49, DOI 10.1016/S0896-6273(00)80821-1
   Tallon-Baudry C, 2001, J NEUROSCI, V21, part. no., DOI 10.1523/JNEUROSCI.21-20-j0008.2001
NR 28
TC 4
Z9 4
U1 0
U2 1
PY 2006
BP 212
EP +
UT WOS:000236386800033
DA 2023-11-16
ER

PT J
AU Chakraborty, B
   Mukhopadhyay, S
AF Chakraborty, Biswadeep
   Mukhopadhyay, Saibal
TI Heterogeneous recurrent spiking neural network for spatio-temporal
   classification
SO FRONTIERS IN NEUROSCIENCE
DT Article
DE spiking neural network (SNN); action detection and recognition; spike
   timing dependent plasticity; heterogeneity; unsupervised learning;
   Bayesian Optimization (BO); leaky integrate and fire (LIF)
ID PLASTICITY
AB Spiking Neural Networks are often touted as brain-inspired learning models for the third wave of Artificial Intelligence. Although recent SNNs trained with supervised backpropagation show classification accuracy comparable to deep networks, the performance of unsupervised learning-based SNNs remains much lower. This paper presents a heterogeneous recurrent spiking neural network (HRSNN) with unsupervised learning for spatio-temporal classification of video activity recognition tasks on RGB (KTH, UCF11, UCF101) and event-based datasets (DVS128 Gesture). We observed an accuracy of 94.32% for the KTH dataset, 79.58% and 77.53% for the UCF11 and UCF101 datasets, respectively, and an accuracy of 96.54% on the event-based DVS Gesture dataset using the novel unsupervised HRSNN model. The key novelty of the HRSNN is that the recurrent layer in HRSNN consists of heterogeneous neurons with varying firing/relaxation dynamics, and they are trained via heterogeneous spike-time-dependent-plasticity (STDP) with varying learning dynamics for each synapse. We show that this novel combination of heterogeneity in architecture and learning method outperforms current homogeneous spiking neural networks. We further show that HRSNN can achieve similar performance to state-of-the-art backpropagation trained supervised SNN, but with less computation (fewer neurons and sparse connection) and less training data.
C1 [Chakraborty, Biswadeep; Mukhopadhyay, Saibal] Georgia Inst Technol, Dept Elect & Comp Engn, Atlanta, GA 30313 USA.
RP Chakraborty, B (corresponding author), Georgia Inst Technol, Dept Elect & Comp Engn, Atlanta, GA 30313 USA.
EM biswadeep@gatech.edu
CR Bi Y, 2020, IEEE T IMAGE PROCESS, V29, P9084, DOI 10.1109/TIP.2020.3023597
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Carvalho TP, 2009, NEURON, V61, P774, DOI 10.1016/j.neuron.2009.01.013
   Chakraborty B, 2021, FRONT NEUROSCI-SWITZ, V15, DOI 10.3389/fnins.2021.695357
   DEKLOET ER, 1987, PSYCHONEUROENDOCRINO, V12, P83, DOI 10.1016/0306-4530(87)90040-0
   Demin V, 2018, FRONT NEUROINFORM, V12, DOI 10.3389/fninf.2018.00079
   Eriksson D., 2021, UNCERTAINTY ARTIFICI, P493
   Escobar MJ, 2009, INT J COMPUT VISION, V82, P284, DOI 10.1007/s11263-008-0201-1
   Fang W, 2021, INCORPORATING LEARNA, P2661
   Feldman DE, 2012, NEURON, V75, P556, DOI 10.1016/j.neuron.2012.08.001
   Feydy Jean, 2019, 22 INT C ART INT STA, P2681
   George AM, 2020, IEEE IJCNN, DOI 10.1109/ijcnn48605.2020.9206681
   Gilson M, 2010, FRONT COMPUT NEUROSC, V4, DOI 10.3389/fncom.2010.00023
   Hofer SB, 2011, NAT NEUROSCI, V14, P1045, DOI 10.1038/nn.2876
   Frazier PI, 2018, Arxiv, DOI [arXiv:1807.02811, DOI 10.48550/ARXIV.1807.02811]
   Ivanov V., 2021, ADV NEURAL INF PROCE, V34, P25703
   Jin Y., 2018, ADV NEURAL INFORM PR
   Korte M, 2016, PHYSIOL REV, V96, P647, DOI 10.1152/physrev.00010.2015
   Lagorce X, 2017, IEEE T PATTERN ANAL, V39, P1346, DOI 10.1109/TPAMI.2016.2574707
   Lazar A, 2006, ESANN, P647
   Lee H, 2021, IEEE ACCESS, V9, P83901, DOI 10.1109/ACCESS.2021.3087509
   Legenstein R, 2007, NEURAL NETWORKS, V20, P323, DOI 10.1016/j.neunet.2007.04.017
   Liu Qianhui, 2021, IJCAI, P1743
   Lobo JL, 2020, NEURAL NETWORKS, V121, P88, DOI 10.1016/j.neunet.2019.09.004
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   Maro JM, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00275
   Meng Y, 2011, IEEE T NEURAL NETWOR, V22, P1952, DOI 10.1109/TNN.2011.2171044
   Nobukawa S, 2019, J ARTIF INTELL SOFT, V9, P283, DOI 10.2478/jaiscr-2019-0009
   Panda P, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00126
   Patravali J., 2021, PROC IEEECVF INT C C, P8484
   Perez-Nieves N, 2021, NAT COMMUN, V12, DOI 10.1038/s41467-021-26022-3
   Petitpré C, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-06033-3
   Pool RR, 2011, NEURAL COMPUT, V23, P1768, DOI 10.1162/NECO_a_00140
   Shamir M, 2006, NEURAL COMPUT, V18, P1951, DOI 10.1162/neco.2006.18.8.1951
   She XY, 2021, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.615756
   She Xueyuan, 2021, INT C LEARN REPR
   Shen G., 2021, ARXIV, DOI [10.2139/ssrn.4018613, DOI 10.2139/SSRN.4018613]
   Shrestha SB, 2018, ADV NEUR IN, V31
   Sjöström PJ, 2008, PHYSIOL REV, V88, P769, DOI 10.1152/physrev.00016.2007
   Soomro K, 2012, Arxiv, DOI arXiv:1212.0402
   Soomro K, 2017, IEEE I CONF COMP VIS, P696, DOI 10.1109/ICCV.2017.82
   Soures N, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00686
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Wang QY, 2019, IEEE WINT CONF APPL, P1826, DOI 10.1109/WACV.2019.00199
   Wang W, 2019, IEEE ACCESS, V7, P117165, DOI 10.1109/ACCESS.2019.2936604
   Wang ZJ, 2022, INT J INTELL SYST, V37, P2242, DOI 10.1002/int.22772
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Yang J., 2018, IOP C SERIES MAT SCI, DOI DOI 10.1088/1757-899X/394/5/052009
   Yin B., 2021, ARXIV PREPRINT, DOI [10.21203/rs.3.rs-1625930/v1, DOI 10.21203/RS.3.RS-1625930/V1]
   Zeldenrust F, 2021, PLOS COMPUT BIOL, V17, DOI 10.1371/journal.pcbi.1008673
   Zhang W., 2019, ADV NEUR IN
   Zheng H., 2020, ARXIV PREPRINT, DOI [10.1609/aaai.v35i12.17320, DOI 10.1609/AAAI.V35I12.17320]
   Zhou Y, 2020, NEUROCOMPUTING, V406, P12, DOI 10.1016/j.neucom.2020.04.079
NR 53
TC 0
Z9 0
U1 1
U2 8
PD JAN 30
PY 2023
VL 17
AR 994517
DI 10.3389/fnins.2023.994517
UT WOS:000929999700001
DA 2023-11-16
ER

PT J
AU Wang, ZH
   Gu, XZ
   Goh, RSM
   Zhou, JT
   Luo, T
AF Wang, Zhehui
   Gu, Xiaozhe
   Goh, Rick Siow Mong
   Zhou, Joey Tianyi
   Luo, Tao
TI Efficient Spiking Neural Networks With Radix Encoding
SO IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
DT Article; Early Access
DE Encoding; energy efficient; short spike train; speedup; spiking neural
   network (SNN)
ID INTELLIGENCE; PROCESSOR; DRIVEN
AB Spiking neural networks (SNNs) have advantages in latency and energy efficiency over traditional artificial neural networks (ANNs) due to their event-driven computation mechanism and the replacement of energy-consuming weight multiplication with addition. However, to achieve high accuracy, it usually requires long spike trains to ensure accuracy, usually more than 1000 time steps. This offsets the computation efficiency brought by SNNs because a longer spike train means a larger number of operations and larger latency. In this article, we propose a radix-encoded SNN, which has ultrashort spike trains. Specifically, it is able to use less than six time steps to achieve even higher accuracy than its traditional counterpart. We also develop a method to fit our radix encoding technique into the ANN-to-SNN conversion approach so that we can train radix-encoded SNNs more efficiently on mature platforms and hardware. Experiments show that our radix encoding can achieve 25x improvement in latency and 1.7% improvement in accuracy compared to the state-of-the-art method using the VGG-16 network on the CIFAR-10 dataset.
C1 [Wang, Zhehui; Goh, Rick Siow Mong; Zhou, Joey Tianyi; Luo, Tao] ASTAR, Inst High Performance Comp, Singapore 138632, Singapore.
   [Gu, Xiaozhe] Chinese Univ Hong Kong, Future Network Intelligence Inst FNii, Shenzhen 518172, Peoples R China.
RP Luo, T (corresponding author), ASTAR, Inst High Performance Comp, Singapore 138632, Singapore.
EM wang_zhehui@ihpc.a-star.edu.sg; gohsm@ihpc.a-star.edu.sg;
   joey_zhou@ihpc.a-star.edu.sg; luo_tao@ihpc.a-star.edu.sg
CR Pérez-Carrasco JA, 2013, IEEE T PATTERN ANAL, V35, P2706, DOI 10.1109/TPAMI.2013.71
   Bellec G., 2018, ADV NEURAL INFORM PR
   Chai T, 2014, GEOSCI MODEL DEV, V7, P1247, DOI 10.5194/gmd-7-1247-2014
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Diehl PU, 2015, IEEE IJCNN
   Hebb D., 2005, ORG BEHAV NEUROPSYCH
   Hu YF, 2020, Arxiv, DOI [arXiv:1805.01352, 10.48550/arXiv.1805.01352]
   Huh D, 2018, ADV NEUR IN, V31
   Hunsberger E, 2015, Arxiv, DOI arXiv:1510.08829
   Kaiser J, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00424
   Khan MM, 2008, IEEE IJCNN, P2849, DOI 10.1109/IJCNN.2008.4634199
   Kim J, 2018, NEUROCOMPUTING, V311, P373, DOI 10.1016/j.neucom.2018.05.087
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
   Lee C, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00119
   Luo T, 2020, IEEE T COMPUT AID D, V39, P438, DOI 10.1109/TCAD.2018.2889670
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   METROPOLIS N, 1949, J AM STAT ASSOC, V44, P335, DOI 10.2307/2280232
   Nambiar V. P., 2020, P IEEE AS SOL STAT C, P1
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Park J, 2020, IEEE J SOLID-ST CIRC, V55, P108, DOI 10.1109/JSSC.2019.2942367
   Pei J, 2019, NATURE, V572, P106, DOI 10.1038/s41586-019-1424-8
   Rathi N, 2020, Arxiv, DOI arXiv:2005.01807
   Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031
   Reynolds JJM, 2019, IEEE IJCNN
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Rueckauer Bodo, 2016, ARXIV, DOI DOI 10.3389/FNINS.2017.00682
   Schuman CD, 2019, IEEE IJCNN
   Schuster C, 2020, PUBLIC ADMIN REV, V80, P792, DOI 10.1111/puar.13246
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Shen Y., 2016, P COLING 2016 26 INT, P2526
   Shrestha SB, 2018, ADV NEUR IN, V31
   Sodhro AH, 2019, IEEE T IND INFORM, V15, P4235, DOI 10.1109/TII.2019.2902878
   Wang Z., 2020, 2020 IEEE 91 VEH TEC, P1, DOI DOI 10.1109/VTC2020-SPRING48590.2020.9128938
   Wilamowski B. M., 2011, IND ELECT SET, V5
   Wu YJ, 2019, AAAI CONF ARTIF INTE, P1311
   Yan ZL, 2021, AAAI CONF ARTIF INTE, V35, P10577
   Yang TJ, 2017, PROC CVPR IEEE, P6071, DOI 10.1109/CVPR.2017.643
   Yin PH, 2019, Arxiv, DOI arXiv:1903.05662
   Zhang L, 2019, AAAI CONF ARTIF INTE, P1319
NR 39
TC 1
Z9 1
U1 2
U2 17
PD 2022 AUG 16
PY 2022
DI 10.1109/TNNLS.2022.3195918
EA AUG 2022
UT WOS:000843784700001
DA 2023-11-16
ER

PT J
AU Misiunas, AVM
   Rapsevicius, V
   Samaitiene, R
   Meskauskas, T
AF Misiunas, Andrius Vytautas Misiukas
   Rapsevicius, Valdas
   Samaitiene, Ruta
   Meskauskas, Tadas
TI Electroencephalogram spike detection and classification by diagnosis
   with convolutional neural network
SO NONLINEAR ANALYSIS-MODELLING AND CONTROL
DT Article
DE electroencephalogram; convolutional neural network; machine learning;
   classification; epilepsy
ID EPILEPSY; SYSTEM
AB This work presents convolutional neural network (CNN) based methodology for electroencephalogram (EEG) classification by diagnosis: benign childhood epilepsy with centrotemporal spikes (rolandic epilepsy) (Group I) and structural focal epilepsy (Group II). Manual classification of these groups is sometimes difficult, especially, when no clinical record is available, thus presenting a need for an algorithm for automatic classification. The presented algorithm has the following steps: (i) EEG spike detection by morphological filter based algorithm; (ii) classification of EEG spikes using preprocessed EEG signal data from all channels in the vicinity of the spike detected; (iii) majority rule classifier application to all EEG spikes from a single patient. Classification based on majority rule allows us to achieve 80% average accuracy (despite the fact that from a single spike one would obtain only 58% accuracy).
C1 [Misiunas, Andrius Vytautas Misiukas; Rapsevicius, Valdas; Meskauskas, Tadas] Vilnius Univ, Inst Comp Sci, Didlaukio 47, LT-08303 Vilnius, Lithuania.
   [Samaitiene, Ruta] Vilnius Univ, Fac Med, Clin Childrens Dis, Santariskiu 4, LT-08406 Vilnius, Lithuania.
RP Misiunas, AVM (corresponding author), Vilnius Univ, Inst Comp Sci, Didlaukio 47, LT-08303 Vilnius, Lithuania.
EM andrius.misiukas@mif.vu.lt
CR Aicardi J, 2000, EPILEPTIC DISORD, V2, pS5
   Biggio B, 2015, IEEE SIGNAL PROC MAG, V32, P31, DOI 10.1109/MSP.2015.2426728
   Degen R, 1999, PEDIATR NEUROL, V20, P354, DOI 10.1016/S0887-8994(99)00004-1
   Juozapavicius A, 2011, NONLINEAR ANAL-MODEL, V16, P375
   Kim Y, 2014, IEEE ASME INT C ADV, P1747, DOI 10.1109/AIM.2014.6878336
   Korvel G, 2018, J AUDIO ENG SOC, V66, P1072, DOI 10.17743/jaes.2018.0066
   Kousarrizi MRN, 2009, 2009 3RD INTERNATIONAL CONFERENCE ON BIOINFORMATICS AND BIOMEDICAL ENGINEERING, VOLS 1-11, P1150
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
   Löfhede J, 2010, J NEURAL ENG, V7, DOI 10.1088/1741-2560/7/1/016007
   Misiunas AVM, 2019, AIP CONF PROC, V2164, DOI 10.1063/1.5130828
   Misiunas A. V. Misiukas, 2015, LIET MAT RINKINYS SE, V56, P60, DOI DOI 10.15388/LMR.A.2015.11
   Misiunas A. V. Misiukas, 2016, P LITHUANIAN MATH SO, V57, P47
   Misiunas AVM, 2019, LECT NOTES COMPUT SC, V11189, P441, DOI 10.1007/978-3-030-10692-8_50
   Misiunas AVM, 2019, BIOMED SIGNAL PROCES, V48, P118, DOI 10.1016/j.bspc.2018.10.006
   Nishida S., 1999, 14 WORLD C IFAC, V32, P4301
   Patel J, 2015, EXPERT SYST APPL, V42, P2162, DOI 10.1016/j.eswa.2014.10.031
   Pattinson J, 2017, CULT HIST MOD WAR, P1
   Sajda P, 2006, ANNU REV BIOMED ENG, V8, P537, DOI 10.1146/annurev.bioeng.8.061505.095802
   Sammut C., 2017, ENCY MACHINE LEARNIN, V2nd, DOI DOI 10.1007/978-1-4899-7687-1_68
   Scheffer IE, 2017, EPILEPSIA, V58, P512, DOI 10.1111/epi.13709
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tichonov J, 2018, ADV SCI TECHNOL-RES, V12, P29, DOI 10.12913/22998624/87041
NR 22
TC 0
Z9 0
U1 0
U2 6
PY 2020
VL 25
IS 4
BP 692
EP 704
DI 10.15388/namc.2020.25.18016
UT WOS:000546370200010
DA 2023-11-16
ER

PT J
AU Pietrzak, P
   Szczesny, S
   Huderek, D
   Przyborowski, L
AF Pietrzak, Pawel
   Szczesny, Szymon
   Huderek, Damian
   Przyborowski, Lukasz
TI Overview of Spiking Neural Network Learning Approaches and Their
   Computational Complexities
SO SENSORS
DT Review
DE spiking neural networks; learning algorithms; computational complexity;
   hardware
AB Spiking neural networks (SNNs) are subjects of a topic that is gaining more and more interest nowadays. They more closely resemble actual neural networks in the brain than their second-generation counterparts, artificial neural networks (ANNs). SNNs have the potential to be more energy efficient than ANNs on event-driven neuromorphic hardware. This can yield drastic maintenance cost reduction for neural network models, as the energy consumption would be much lower in comparison to regular deep learning models hosted in the cloud today. However, such hardware is still not yet widely available. On standard computer architectures consisting mainly of central processing units (CPUs) and graphics processing units (GPUs) ANNs, due to simpler models of neurons and simpler models of connections between neurons, have the upper hand in terms of execution speed. In general, they also win in terms of learning algorithms, as SNNs do not reach the same levels of performance as their second-generation counterparts in typical machine learning benchmark tasks, such as classification. In this paper, we review existing learning algorithms for spiking neural networks, divide them into categories by type, and assess their computational complexity.
C1 [Pietrzak, Pawel; Szczesny, Szymon; Huderek, Damian; Przyborowski, Lukasz] Poznan Univ Tech, Inst Comp Sci, Fac Comp & Telecommun, Piotrowo 3A St, PL-61138 Poznan, Poland.
RP Szczesny, S (corresponding author), Poznan Univ Tech, Inst Comp Sci, Fac Comp & Telecommun, Piotrowo 3A St, PL-61138 Poznan, Poland.
EM szymon.szczesny@put.poznan.pl
CR Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   Baby SA, 2017, PROCEEDINGS 2017 4TH IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION (ACPR), P316, DOI 10.1109/ACPR.2017.136
   Baydin AG, 2018, J MACH LEARN RES, V18
   Bing Han, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P388, DOI 10.1007/978-3-030-58607-2_23
   Bing Han, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13555, DOI 10.1109/CVPR42600.2020.01357
   Bohte S. M., 2000, 8th European Symposium on Artificial Neural Networks. ESANN"2000. Proceedings, P419
   Brown Tom, 2020, NEURIPS, V1, P3
   Bu T., 2022, OPTIMAL ANN SNN CONV
   Burbank KS, 2015, PLOS COMPUT BIOL, V11, DOI 10.1371/journal.pcbi.1004566
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   DeFelipe J, 2012, FRONT NEUROANAT, V6, DOI [10.3389/fnana.2012.00022, 10.3389/fnsyn.2012.00002, 10.3389/fnana.2012.00005]
   DeWolf T, 2021, SCI ROBOT, V6, DOI 10.1126/scirobotics.abk3268
   DeWolf T, 2020, FRONT NEUROROBOTICS, V14, DOI 10.3389/fnbot.2020.568359
   DeWolf T, 2016, P ROY SOC B-BIOL SCI, V283, DOI 10.1098/rspb.2016.2134
   Diehl PU, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON REBOOTING COMPUTING (ICRC)
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Dutta S, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-07418-y
   Eliasmith C., 2003, NEURAL ENG COMPUTATI
   Esser SK, 2015, BACKPROPAGATION ENER, P1117
   Falanga D, 2020, SCI ROBOT, V5, DOI 10.1126/scirobotics.aaz9712
   Fang W, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2641, DOI 10.1109/ICCV48922.2021.00266
   Höppner S, 2022, Arxiv, DOI arXiv:2103.08392
   Ivanov D., 2022, PREPRINT, DOI 10.3389/fnins.2022.959626
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2007, CEREB CORTEX, V17, P2443, DOI 10.1093/cercor/bhl152
   Juarez-Lora A, 2022, FRONT NEUROROBOTICS, V16, DOI 10.3389/fnbot.2022.904017
   Krizhevsky Alex, 2009, LEARNING MULTIPLE LA
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li HM, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00309
   Liu FX, 2021, FRONT NEUROSCI-SWITZ, V15, DOI 10.3389/fnins.2021.756876
   Lobov SA, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21082678
   Makarov VA, 2022, FRONT COMPUT NEUROSC, V16, DOI 10.3389/fncom.2022.859874
   Masquelier T, 2007, PLOS COMPUT BIOL, V3, P247, DOI 10.1371/journal.pcbi.0030031
   Mo LF, 2022, FRONT NEUROSCI-SWITZ, V16, DOI 10.3389/fnins.2022.838832
   Moreira O., 2020, P 2020 2 IEEE INT C, P1, DOI 10.1109/AICAS48895.2020.9073999
   Mozafari M, 2019, PATTERN RECOGN, V94, P87, DOI 10.1016/j.patcog.2019.05.015
   O'Connor P., 2016, DEEP SPIKING NETWORK
   Orchard G, 2015, FRONT NEUROSCI-SWITZ, V9, DOI [10.3389/fhins.2015.00437, 10.3389/fnins.2015.00437]
   Patel K, 2021, Arxiv, DOI arXiv:2106.08921
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Quiroga RQ, 2010, PSYCHOL REV, V117, P291, DOI 10.1037/a0016917
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Sandamirskaya Y, 2022, SCI ROBOT, V7, DOI 10.1126/scirobotics.abl8419
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Shalumov A, 2021, BIOINSPIR BIOMIM, V16, DOI 10.1088/1748-3190/ac290c
   Shrestha S. B., 2018, ADV NEURAL INFORM PR
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   St”ckl C, 2020, Arxiv, DOI arXiv:2001.01682
   Szczesny S, 2023, J EXP THEOR ARTIF IN, V35, P77, DOI 10.1080/0952813X.2021.1957024
   Szczesny S, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21093276
   Szczesny S, 2020, IEEE SENS J, V20, P5733, DOI 10.1109/JSEN.2020.2974701
   Tang GZ, 2019, IEEE INT C INT ROBOT, P4176, DOI [10.1109/iros40897.2019.8967864, 10.1109/IROS40897.2019.8967864]
   Tsur E. E., 2021, NEUROMORPHIC ENG SCI, DOI 10.1201/9781003143499
   Van Pottelbergh T, 2018, NEURAL COMPUT, V30, P987, DOI [10.1162/neco_a_01065, 10.1162/NECO_a_01065]
   Vigneron A, 2020, IEEE IJCNN, DOI 10.1109/ijcnn48605.2020.9207239
   Voela A, 2021, HYPATIA, V36, P101, DOI 10.1017/hyp.2020.49
   Wang CY, 2022, Arxiv, DOI arXiv:2207.02696
   WERBOS PJ, 1990, P IEEE, V78, P1550, DOI 10.1109/5.58337
   Yamazaki K, 2022, BRAIN SCI, V12, DOI 10.3390/brainsci12070863
   Yan YX, 2021, NEUROMORPH COMPUT EN, V1, DOI 10.1088/2634-4386/abf150
   Yousefzadeh A, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00665
   Zenke F, 2018, NEURAL COMPUT, V30, P1514, DOI 10.1162/neco_a_01086
   Zhong XY, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app12125980
   Zhou SB, 2020, IEEE ACCESS, V8, P76903, DOI 10.1109/ACCESS.2020.2990416
NR 67
TC 1
Z9 1
U1 16
U2 18
PD MAR
PY 2023
VL 23
IS 6
AR 3037
DI 10.3390/s23063037
UT WOS:000959978100001
DA 2023-11-16
ER

PT J
AU Guo, DQ
AF Guo, Daqing
TI Inhibition of rhythmic spiking by colored noise in neural systems
SO COGNITIVE NEURODYNAMICS
DT Article
DE Neural system; Spiking neuron model; Noise; Neuronal network motif
ID STOCHASTIC RESONANCE; NETWORK; ENSEMBLES
AB We study the effect of colored noise on the rhythmic spiking activity of neural systems in this paper. The phenomenon of the so-called inverse stochastic resonance, that is, noise with appropriate intensity suppresses the spiking activity in neural systems, is clearly observed in a special parameter regime. We find that the inhibition effect of colored noise is stronger than that of Gaussian white noise. Furthermore, our simulation results show that the inhibition effect of colored noise provides a useful mechanism for the generation of synchronized burst in type-2 mixed-feed-forward-feedback loop neuronal network motif, which indicates that such inhibition effect might have some biological implications.
C1 Univ Elect Sci & Technol China, Sch Elect Engn, Chengdu 610054, Peoples R China.
RP Guo, DQ (corresponding author), Univ Elect Sci & Technol China, Sch Elect Engn, Chengdu 610054, Peoples R China.
EM dqguo07@gmail.com
CR ADEY WR, 1972, INT J NEUROSCI, V3, P271, DOI 10.3109/00207457209147637
   Chialvo DR, 1997, PHYS REV E, V55, P1798, DOI 10.1103/PhysRevE.55.1798
   Chik DTW, 2001, PHYS REV E, V64, DOI 10.1103/PhysRevE.64.021913
   Collins JJ, 1996, J NEUROPHYSIOL, V76, P642, DOI 10.1152/jn.1996.76.1.642
   COLLINS JJ, 1995, NATURE, V376, P236, DOI 10.1038/376236a0
   Deco G, 2009, PROG NEUROBIOL, V88, P1, DOI 10.1016/j.pneurobio.2009.01.006
   Destexhe A, 2003, NAT REV NEUROSCI, V4, P739, DOI 10.1038/nrn1198
   Destexhe A, 2006, SCIENCE, V314, P85, DOI 10.1126/science.1127241
   Gailey PC, 1997, PHYS REV LETT, V79, P4701, DOI 10.1103/PhysRevLett.79.4701
   Gerstner W., 2002, SPIKING NEURON MODEL
   Guo DQ, 2009, PHYS REV E, V79, DOI 10.1103/PhysRevE.79.051921
   Gutkin BS, 2009, NATURWISSENSCHAFTEN, V96, P1091, DOI 10.1007/s00114-009-0570-5
   HANSEL D, 1992, PHYS REV LETT, V68, P718, DOI 10.1103/PhysRevLett.68.718
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Kloeden P.-E., 1994, NUMERICAL SOLUTION S
   Lee SG, 1999, PHYS REV E, V60, P826, DOI 10.1103/PhysRevE.60.826
   Li CG, 2006, PLOS COMPUT BIOL, V2, P925, DOI 10.1371/journal.pcbi.0020103
   Li CG, 2008, PHYS REV E, V78, DOI 10.1103/PhysRevE.78.037101
   Li QS, 2008, PHYS REV E, V77, DOI 10.1103/PhysRevE.77.036117
   Lindner B, 2002, PHYS REV E, V66, DOI 10.1103/PhysRevE.66.031916
   Milo R, 2002, SCIENCE, V298, P824, DOI 10.1126/science.298.5594.824
   Neiman AB, 2007, J NEUROPHYSIOL, V98, P2795, DOI 10.1152/jn.01289.2006
   Paydarfar D, 2006, J NEUROPHYSIOL, V96, P3338, DOI 10.1152/jn.00486.2006
   Pikovsky AS, 1997, PHYS REV LETT, V78, P775, DOI 10.1103/PhysRevLett.78.775
   Reigl M, 2004, BMC BIOL, V2, DOI 10.1186/1741-7007-2-25
   Sun XJ, 2008, CHAOS, V18, DOI 10.1063/1.2900402
   Tuckwell HC, 2011, J COMPUT NEUROSCI, V30, P361, DOI 10.1007/s10827-010-0260-5
   Tuckwell HC, 2009, PHYS REV E, V80, DOI 10.1103/PhysRevE.80.031907
NR 28
TC 50
Z9 52
U1 0
U2 14
PD SEP
PY 2011
VL 5
IS 3
BP 293
EP 300
DI 10.1007/s11571-011-9160-2
UT WOS:000295979000007
DA 2023-11-16
ER

PT J
AU Varma, PRK
   Sathiya, RR
   Vanitha, M
AF Varma, P. Ravi Kiran
   Sathiya, R. R.
   Vanitha, M.
TI Enhanced Elman spike neural network based intrusion attack detection in
   software defined Internet of Things network
SO CONCURRENCY AND COMPUTATION-PRACTICE & EXPERIENCE
DT Article
DE distributed DoS; enhanced Elman spike neural network; intrusion
   detection; IP flow records; mitigation process; software defined network
ID IDENTIFICATION
AB In this article, enhanced Elman spike neural network based intrusion attack detection in software defined IoT network is proposed. Initially, the data's are taken from CICDDoS 2019 and CICIDS 2018 benchmark datasets. Software defined network (SDN) secure defense system is detected the intrusion and distributed denial of service (DDoS) attacks on central controllers using multidimensional internet protocol (IP) flow analysis. Here, the enhanced Elman spike neural network (EESNN) classifies DDoS and intrusion attacks as normal and anomaly. The proposed EESNN-IAD-SDN method is executed in python language. The performance metrics, such as accuracy, specificity, F-measure, sensitivity, precision, recall is examined. The proposed EESNN-IAD-SDN method provides 13.93%, 13.26%, 14.35, and 13.73% higher accuracy in CICDDoS 2019 dataset compared with the existing methods, like GRU-IAD-SDN, LSTM-IAD-SDN, and GAN-IAD-SDN, respectively.
C1 [Varma, P. Ravi Kiran] Maharaj Vijayaram Gajapathi Raj Coll Engn, Dept Comp Sci & Engn, Vizianagaram, Andhra Pradesh, India.
   [Sathiya, R. R.] Amrita Vishwa Vidyapeetham, Amrita Sch Comp, Dept Comp Sci & Engn, Coimbatore, India.
   [Vanitha, M.] Saveetha Engn Coll, Dept Elect & Commun Engn, Chennai, India.
   [Varma, P. Ravi Kiran] Maharaj VijayaramGajapathi Raj Coll Engn Vizianaga, Dept Comp Sci & Engn, Vizianagaram, Andhra Pradesh, India.
RP Varma, PRK (corresponding author), Maharaj VijayaramGajapathi Raj Coll Engn Vizianaga, Dept Comp Sci & Engn, Vizianagaram, Andhra Pradesh, India.
EM ravikiranvarmap@gmail.com
CR Al-Jamali NAS, 2020, IEEE ACCESS, V8, P61246, DOI 10.1109/ACCESS.2020.2984311
   Assis MVO, 2021, J NETW COMPUT APPL, V177, DOI 10.1016/j.jnca.2020.102942
   Bhardwaj S, 2021, WIRELESS PERS COMMUN, DOI 10.1007/s11277-021-08920-3
   Costa LC, 2021, PERFORM EVALUATION, V147, DOI 10.1016/j.peva.2021.102194
   de Senneville BD, 2015, IEEE T MED IMAGING, V34, P974, DOI 10.1109/TMI.2014.2371995
   Dey A, 2020, 2020 2ND INTERNATIONAL CONFERENCE ON SUSTAINABLE TECHNOLOGIES FOR INDUSTRY 4.0 (STI), DOI 10.1109/STI50764.2020.9350411
   Francis SH, 2022, CIRC SYST SIGNAL PR, V41, P1751, DOI 10.1007/s00034-021-01850-2
   Gopalakrishnan K., 2020, PRINCIPLES INTERNET, P519
   Hajj S, 2021, T EMERG TELECOMMUN T, V32, DOI 10.1002/ett.4240
   Imtiaz SI, 2021, FUTURE GENER COMP SY, V115, P844, DOI 10.1016/j.future.2020.10.008
   Kassab W, 2020, J NETW COMPUT APPL, V163, DOI 10.1016/j.jnca.2020.102663
   Kim J, 2021, IEEE T INF FOREN SEC, V16, P3138, DOI 10.1109/TIFS.2021.3075845
   Li Y., 2021, IEEE INTERNET THINGS
   Novaes MP, 2021, FUTURE GENER COMP SY, V125, P156, DOI 10.1016/j.future.2021.06.047
   Novaes MP, 2020, IEEE ACCESS, V8, P83765, DOI 10.1109/ACCESS.2020.2992044
   Rajesh P, 2022, ENERGY SYST, V13, P939, DOI 10.1007/s12667-021-00452-w
   Rajesh P, 2021, J ENG DES TECHNOL, DOI 10.1108/JEDT-12-2020-0494
   Rehman SU, 2021, FUTURE GENER COMP SY, V118, P453, DOI 10.1016/j.future.2021.01.022
   Salam A., 2020, INTERNET THINGS SUST, P1, DOI [10.1007/978-3-030-35291-2_1, 10.1007/978-3-030-35291-2, DOI 10.1007/978-3-030-35291-2_1]
   Sarker IH, 2021, MOBILE NETW APPL, V26, P285, DOI 10.1007/s11036-020-01650-z
   Satheesh N, 2020, MICROPROCESS MICROSY, V79, DOI 10.1016/j.micpro.2020.103285
   Sengupta J, 2020, J NETW COMPUT APPL, V149, DOI 10.1016/j.jnca.2019.102481
   Shajin FH., 2020, J SOFT COMPUT ENG AP, V1, P7
   Srivastava A, 2021, COMPUT SCI REV, V39, DOI 10.1016/j.cosrev.2020.100359
   Susilo B, 2021, 2021 IEEE 11TH ANNUAL COMPUTING AND COMMUNICATION WORKSHOP AND CONFERENCE (CCWC), P807, DOI 10.1109/CCWC51732.2021.9375951
   unb.ca, CIC DAT DDOS 2019
   unb.ca, CIC DAT IDS 2018
   Wang JJ, 2020, IEEE COMMUN SURV TUT, V22, P1472, DOI 10.1109/COMST.2020.2965856
NR 28
TC 1
Z9 1
U1 2
U2 8
PD JAN 25
PY 2023
VL 35
IS 2
DI 10.1002/cpe.7503
EA NOV 2022
UT WOS:000888138600001
DA 2023-11-16
ER

PT J
AU Nobukawa, S
   Nishimura, H
   Wagatsuma, N
   Ando, S
   Yamanishi, T
AF Nobukawa, Sou
   Nishimura, Haruhiko
   Wagatsuma, Nobuhiko
   Ando, Satoshi
   Yamanishi, Teruya
TI Long-Tailed Characteristic of Spiking Pattern Alternation Induced by
   Log-Normal Excitatory Synaptic Distribution
SO IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
DT Article
DE Log-normal distribution; long-tailed distribution; pattern alternation;
   spiking neural network
ID FUNCTIONAL CONNECTIVITY; BINOCULAR-RIVALRY; NEURAL-NETWORKS; FIRING
   RATES; FREQUENCY; INHIBITION; MODULATION; CORTEX; SIGNAL; MODEL
AB Studies of structural connectivity at the synaptic level show that in synaptic connections of the cerebral cortex, the excitatory postsynaptic potential (EPSP) in most synapses exhibits sub-mV values, while a small number of synapses exhibit large EPSPs (greater than or similar to 1.0 [mV]). This means that the distribution of EPSP fits a log-normal distribution. While not restricting structural connectivity, skewed and long-tailed distributions have been widely observed in neural activities, such as the occurrences of spiking rates and the size of a synchronously spiking population. Many studies have been modeled this long-tailed EPSP neural activity distribution; however, its causal factors remain controversial. This study focused on the long-tailed EPSP distributions and interlateral synaptic connections primarily observed in the cortical network structures, thereby having constructed a spiking neural network consistent with these features. Especially, we constructed two coupled modules of spiking neural networks with excitatory and inhibitory neural populations with a log-normal EPSP distribution. We evaluated the spiking activities for different input frequencies and with/without strong synaptic connections. These coupled modules exhibited intermittent intermodule-alternative behavior, given moderate input frequency and the existence of strong synaptic and intermodule connections. Moreover, the power analysis, multiscale entropy analysis, and surrogate data analysis revealed that the long-tailed EPSP distribution and intermodule connections enhanced the complexity of spiking activity at large temporal scales and induced nonlinear dynamics and neural activity that followed the long-tailed distribution.
C1 [Nobukawa, Sou] Chiba Inst Technol, Dept Comp Sci, Chiba 2750016, Japan.
   [Nishimura, Haruhiko] Univ Hyogo, Grad Sch Appl Informat, Kobe, Hyogo 6500047, Japan.
   [Wagatsuma, Nobuhiko] Toho Univ, Fac Sci, Dept Informat Sci, Chiba 2748510, Japan.
   [Ando, Satoshi] JSOL Corp, Financial & Serv Ind Business Unit, Tokyo 1040053, Japan.
   [Yamanishi, Teruya] Fukui Univ Technol, AI & IoT Ctr, Fukui 9108505, Japan.
RP Nobukawa, S (corresponding author), Chiba Inst Technol, Dept Comp Sci, Chiba 2750016, Japan.
EM nobukawa@cs.it-chiba.ac.jp
CR Bartos M, 2007, NAT REV NEUROSCI, V8, P45, DOI 10.1038/nrn2044
   Bassett DS, 2017, NAT NEUROSCI, V20, P353, DOI 10.1038/nn.4502
   Battaglia FP, 2005, NEURAL NETWORKS, V18, P1280, DOI 10.1016/j.neunet.2005.08.011
   Bellec G., 2018, ADV NEURAL INFORM PR, P795
   Blake R, 2002, NAT REV NEUROSCI, V3, P13, DOI 10.1038/nrn701
   Börgers C, 2003, NEURAL COMPUT, V15, P509, DOI 10.1162/089976603321192059
   BORSELLINO A, 1972, KYBERNETIK, V10, P139, DOI 10.1007/BF00290512
   Bullmore ET, 2009, NAT REV NEUROSCI, V10, P186, DOI 10.1038/nrn2575
   Burkitt AN, 2006, BIOL CYBERN, V95, P97, DOI 10.1007/s00422-006-0082-8
   Buzsáki G, 2014, NAT REV NEUROSCI, V15, P264, DOI 10.1038/nrn3687
   CALLAWAY EM, 1991, P NATL ACAD SCI USA, V88, P745, DOI 10.1073/pnas.88.3.745
   Callaway EM, 1998, ANNU REV NEUROSCI, V21, P47, DOI 10.1146/annurev.neuro.21.1.47
   Costa M, 2002, PHYS REV LETT, V89, DOI 10.1103/PhysRevLett.89.068102
   Eguíluz VM, 2005, PHYS REV LETT, V94, DOI 10.1103/PhysRevLett.94.018102
   Fell J, 2000, ACTA NEUROBIOL EXP, V60, P87
   Fiser J, 2004, NATURE, V431, P573, DOI 10.1038/nature02907
   Garrett DD, 2011, J NEUROSCI, V31, P4496, DOI 10.1523/JNEUROSCI.5641-10.2011
   Garrett DD, 2010, J NEUROSCI, V30, P4914, DOI 10.1523/JNEUROSCI.5166-09.2010
   Gast R, 2020, NEURAL COMPUT, V32, P1615, DOI 10.1162/neco_a_01300
   Gerfen CR, 2018, J NEUROSCI RES, V96, P1467, DOI 10.1002/jnr.23978
   GILBERT CD, 1989, J NEUROSCI, V9, P2432
   GILBERT CD, 1983, J NEUROSCI, V3, P1116
   Goodman Dan FM, 2014, BMC NEUROSCI, V15, P1, DOI DOI 10.1186/1471-2202-15-S1-P199
   Guo DQ, 2010, IEEE T NEURAL NETWOR, V21, P895, DOI 10.1109/TNN.2010.2044419
   Hagmann P, 2008, PLOS BIOL, V6, P1479, DOI 10.1371/journal.pbio.0060159
   Hagmann P, 2007, PLOS ONE, V2, DOI 10.1371/journal.pone.0000597
   Hasenstaub A, 2005, NEURON, V47, P423, DOI 10.1016/j.neuron.2005.06.016
   Hirase H, 2001, P NATL ACAD SCI USA, V98, P9386, DOI 10.1073/pnas.161274398
   Hiratani N, 2013, FRONT COMPUT NEUROSC, V6, DOI 10.3389/fncom.2012.00102
   Hromadka T, 2008, PLOS BIOL, V6, P124, DOI 10.1371/journal.pbio.0060016
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Kada H, 2018, FRONT COMPUT NEUROSC, V12, DOI 10.3389/fncom.2018.00104
   Kanamaru T, 2003, PHYS REV E, V67, DOI 10.1103/PhysRevE.67.031916
   Kanamaru T, 2017, NEURAL COMPUT, V29, P1696, DOI 10.1162/NECO_a_00965
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Kossio FYK, 2018, PHYS REV LETT, V121, DOI 10.1103/PhysRevLett.121.058301
   Kovacic G, 2009, PHYS REV E, V80, DOI 10.1103/PhysRevE.80.021904
   Kriener B, 2014, FRONT COMPUT NEUROSC, V8, DOI 10.3389/fncom.2014.00136
   Kulkarni SR, 2018, NEURAL NETWORKS, V103, P118, DOI 10.1016/j.neunet.2018.03.019
   Lee B, 2018, CELL REP, V25, P1548, DOI 10.1016/j.celrep.2018.10.029
   Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508
   Lefort S, 2009, NEURON, V61, P301, DOI 10.1016/j.neuron.2008.12.020
   LEHKY SR, 1995, P ROY SOC B-BIOL SCI, V259, P71, DOI 10.1098/rspb.1995.0011
   LEVELT WJM, 1967, BRIT J PSYCHOL, V58, P143, DOI 10.1111/j.2044-8295.1967.tb01068.x
   Lin XH, 2017, NEUROCOMPUTING, V237, P59, DOI 10.1016/j.neucom.2016.08.087
   Lin ZT, 2018, NEUROCOMPUTING, V275, P94, DOI 10.1016/j.neucom.2017.05.009
   Martí D, 2018, PHYS REV E, V97, DOI 10.1103/PhysRevE.97.062314
   Mastrogiuseppe F, 2017, PLOS COMPUT BIOL, V13, DOI 10.1371/journal.pcbi.1005498
   McCormick DA, 1999, SCIENCE, V285, P541, DOI 10.1126/science.285.5427.541
   McIntosh AR, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.1000106
   Mizuseki K, 2014, PHILOS T R SOC B, V369, DOI 10.1098/rstb.2012.0530
   Mizuseki K, 2013, CELL REP, V4, P1010, DOI 10.1016/j.celrep.2013.07.039
   Morishima M, 2006, J NEUROSCI, V26, P4394, DOI 10.1523/JNEUROSCI.0252-06.2006
   Mozafari M, 2018, IEEE T NEUR NET LEAR, V29, P6178, DOI 10.1109/TNNLS.2018.2826721
   Nagao N, 2000, NEURAL PROCESS LETT, V12, P267, DOI 10.1023/A:1026511124944
   Neske GT, 2016, J NEUROPHYSIOL, V116, P351, DOI 10.1152/jn.00071.2016
   Nobukawa S, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-49286-8
   Nobukawa S, 2018, LECT NOTES COMPUT SC, V11301, P535, DOI 10.1007/978-3-030-04167-0_48
   Nobukawa S, 2019, NEUROIMAGE, V188, P357, DOI 10.1016/j.neuroimage.2018.12.008
   O'Connor DH, 2010, NEURON, V67, P1048, DOI 10.1016/j.neuron.2010.08.026
   Peyrache A, 2012, P NATL ACAD SCI USA, V109, P1731, DOI 10.1073/pnas.1109895109
   Rabinovich MI, 2006, REV MOD PHYS, V78, P1213, DOI 10.1103/RevModPhys.78.1213
   Riecke H, 2007, CHAOS, V17, DOI 10.1063/1.2743611
   Samura T, 2015, COGN NEURODYNAMICS, V9, P265, DOI 10.1007/s11571-015-9329-1
   Schreiber T, 1996, PHYS REV LETT, V77, P635, DOI 10.1103/PhysRevLett.77.635
   Shafi M, 2007, NEUROSCIENCE, V146, P1082, DOI 10.1016/j.neuroscience.2006.12.072
   Shanahan M, 2008, PHYS REV E, V78, DOI 10.1103/PhysRevE.78.041924
   She Q, 2016, SCI REP-UK, V6, DOI 10.1038/srep21468
   Song S, 2005, PLOS BIOL, V3, P507, DOI 10.1371/journal.pbio.0030068
   Sporns O, 2014, NAT NEUROSCI, V17, P652, DOI 10.1038/nn.3690
   Stam CJ, 2007, HUM BRAIN MAPP, V28, P1178, DOI 10.1002/hbm.20346
   Takahashi T, 2004, J NEUROL SCI, V225, P33, DOI 10.1016/j.jns.2004.06.016
   Takahashi T, 2018, CLIN NEUROPHYSIOL, V129, P222, DOI 10.1016/j.clinph.2017.11.004
   Takahashi T, 2017, CLIN NEUROPHYSIOL, V128, P1457, DOI 10.1016/j.clinph.2017.05.010
   Takahashi T, 2013, PROG NEURO-PSYCHOPH, V45, P258, DOI 10.1016/j.pnpbp.2012.05.001
   Tavanaei A, 2018, 2018 INT JOINT C NEU, DOI DOI 10.1109/IJCNN.2018.8489104
   Tavanaei A, 2018, NEURAL NETWORKS, V105, P294, DOI 10.1016/j.neunet.2018.05.018
   Teramae J.-N., 2012, SCI REP-UK, V2, P1
   Teramae J, 2016, AIP CONF PROC, V1738, DOI 10.1063/1.4951999
   Timme NM, 2016, PLOS COMPUT BIOL, V12, DOI 10.1371/journal.pcbi.1004858
   Traub RD., 1999, FAST OSCILLATIONS CO
   van den Heuvel M, 2008, J NEUROSCI, V28, P10844, DOI 10.1523/JNEUROSCI.2964-08.2008
   van den Heuvel MP, 2013, TRENDS COGN SCI, V17, P683, DOI 10.1016/j.tics.2013.09.012
   Vogels TP, 2005, J NEUROSCI, V25, P10786, DOI 10.1523/JNEUROSCI.3508-05.2005
   Wagatsuma N, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0080788
   Wagatsuma Nobuhiko, 2011, Front Comput Neurosci, V5, P31, DOI 10.3389/fncom.2011.00031
   WALKER P, 1975, PERCEPT PSYCHOPHYS, V18, P467, DOI 10.3758/BF03204122
   Whittington MA, 2000, INT J PSYCHOPHYSIOL, V38, P315, DOI 10.1016/S0167-8760(00)00173-2
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Yang AC, 2013, PROG NEURO-PSYCHOPH, V45, P253, DOI 10.1016/j.pnpbp.2012.09.015
   Zhang JW, 2019, J COMPUT NEUROSCI, V46, P211, DOI 10.1007/s10827-019-00712-w
NR 91
TC 9
Z9 9
U1 3
U2 11
PD AUG
PY 2021
VL 32
IS 8
BP 3525
EP 3537
DI 10.1109/TNNLS.2020.3015208
UT WOS:000681169500024
DA 2023-11-16
ER

PT J
AU Holker, R
   Susan, S
AF Holker, Ruchi
   Susan, Seba
TI Neuroscience-Inspired Parameter Selection of Spiking Neuron Using
   Hodgkin Huxley Model
SO INTERNATIONAL JOURNAL OF SOFTWARE SCIENCE AND COMPUTATIONAL
   INTELLIGENCE-IJSSCI
DT Article
DE Class 1 Neuron; Class 2 Neuron; Hodgkin-Huxley Model; Integrator Neuron;
   NEURON Simulator; Parameter Selection; Phasic Spiking Neuron; Spiking
   Neural Network; Spiking Neuron
ID INTEGRATE-AND-FIRE; PHASE-DIAGRAM; NETWORKS; SIMULATION
AB Spiking neural networks (SNN) are currently being researched to design an artificial brain to teach it how to think, perform, and learn like a human brain. This paper focuses on exploring optimal values of parameters of biological spiking neurons for the Hodgkin Huxley (HH) model. The HH model exhibits maximum number of neurocomputational properties as compared to other spiking models, as per previous research. This paper investigates the HH model parameters of Class 1, Class 2, phasic spiking, and integrator neurocomputational properties. For the simulation of spiking neurons, the NEURON simulator is used since it is easy to understand and code.
C1 [Holker, Ruchi] Delhi Technol Univ, Informat Technol Dept, Delhi, India.
   [Susan, Seba] Delhi Technol Univ, Delhi, India.
RP Holker, R (corresponding author), Delhi Technol Univ, Informat Technol Dept, Delhi, India.
CR Abbott LF, 1999, BRAIN RES BULL, V50, P303, DOI 10.1016/S0361-9230(99)00161-6
   Alonso LM, 2019, ELIFE, V8, DOI 10.7554/eLife.42722
   Beeman D, 2013, ENCY COMPUTATIONAL N
   Bohte Sander M., 2004, Natural Computing, V3, P195, DOI 10.1023/B:NACO.0000027755.02868.60
   Börgers C, 2017, TEXTS APPL MATH, V66, P51, DOI 10.1007/978-3-319-51171-9_8
   Bower J. M., 2014, GENESIS GEN NEURAL S
   Brette R, 2007, J COMPUT NEUROSCI, V23, P349, DOI 10.1007/s10827-007-0038-6
   Brette R, 2015, FRONT SYST NEUROSCI, V9, DOI 10.3389/fnsys.2015.00151
   Brunel N, 2007, BIOL CYBERN, V97, P341, DOI 10.1007/s00422-007-0189-6
   Burkitt AN, 2006, BIOL CYBERN, V95, P1, DOI 10.1007/s00422-006-0068-6
   Campbell K, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10020550
   Carlu M, 2020, J NEUROPHYSIOL, V123, P1042, DOI 10.1152/jn.00399.2019
   Cessac B, 2010, J PHYSIOL-PARIS, V104, P5, DOI 10.1016/j.jphysparis.2009.11.002
   Colwell LJ, 2009, PLOS COMPUT BIOL, V5, DOI 10.1371/journal.pcbi.1000265
   de Queiroz MS, 2006, NEUROCOMPUTING, V70, P14, DOI 10.1016/j.neucom.2006.07.002
   GERSTNER W, 1995, PHYS REV E, V51, P738, DOI 10.1103/PhysRevE.51.738
   Gerstner W., 2002, SPIKING NEURON MODEL, DOI [DOI 10.1017/CBO9780511815706, 10.1017/cbo9780511815706]
   Gewaltig M. O., 2012, COMPUTATIONAL SYSTEM, P533, DOI [10.1007/978-94-007-3858-4_18, DOI 10.1007/978-94-007-3858-4_18]
   Gruning A, 2014, ESANN
   Hansel D, 1998, NEURAL COMPUT, V10, P467, DOI 10.1162/089976698300017845
   Hernández OE, 2013, BMC MED EDUC, V13, DOI 10.1186/1472-6920-13-70
   Hines ML, 1997, NEURAL COMPUT, V9, P1179, DOI 10.1162/neco.1997.9.6.1179
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V116, P424, DOI 10.1113/jphysiol.1952.sp004716
   HODGKIN AL, 1948, J PHYSIOL-LONDON, V107, P165, DOI 10.1113/jphysiol.1948.sp004260
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V116, P497, DOI 10.1113/jphysiol.1952.sp004719
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V116, P449, DOI 10.1113/jphysiol.1952.sp004717
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V116, P473, DOI 10.1113/jphysiol.1952.sp004718
   Huguet G, 2017, FRONT COMPUT NEUROSC, V11, DOI 10.3389/fncom.2017.00003
   Izhikevich E. M., 2006, DYNAMICAL SYSTEMS NE, DOI DOI 10.7551/MITPRESS/2526.001.0001
   Izhikevich EM, 2000, INT J BIFURCAT CHAOS, V10, P1171, DOI 10.1142/S0218127400000840
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Jolivet R, 2004, J NEUROPHYSIOL, V92, P959, DOI 10.1152/jn.00190.2004
   Jolivet R, 2003, LECT NOTES COMPUT SC, V2714, P846
   Kawaguchi Y, 2002, J NEUROCYTOL, V31, P277, DOI 10.1023/A:1024126110356
   Lindner B, 2002, PHYS REV E, V66, DOI 10.1103/PhysRevE.66.031916
   Ly C, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0176963
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Mohamed A.W., 2019, INT J MACH LEARN CYB, P1
   Ori H, 2020, P NATL ACAD SCI USA, V117, P3575, DOI 10.1073/pnas.1916514117
   Paugam-Moisy H., 2012, HDB NATURAL COMPUTIN, V1, P1, DOI [DOI 10.1007/978-3-540-92910-9_10, 10.1007/978-3-540-92910-9_10]
   Ponulak F, 2011, ACTA NEUROBIOL EXP, V71, P409
   Prescott SA, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.1000198
   Serrano-Gotarredona T, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00002
   Seyed-allaei H, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00019
   Susan Seba, 2019, Computational Intelligence: Theories, Applications and Future DirectionsVolume II. ICCI-2017. Advances in Intelligent Systems and Computing (AISC 799), P545, DOI 10.1007/978-981-13-1135-2_41
   Susan Seba, 2019, Computational Intelligence: Theories, Applications and Future DirectionsVolume II. ICCI-2017. Advances in Intelligent Systems and Computing (AISC 799), P201, DOI 10.1007/978-981-13-1135-2_16
   Susan S, 2020, SOFT COMPUT, V24, P18219, DOI 10.1007/s00500-020-05080-7
   Tateno T, 2004, J NEUROPHYSIOL, V92, P2283, DOI 10.1152/jn.00109.2004
   Vreeken J, 2003, SPIKING NEURAL NETWO
   Wang HT, 2013, J THEOR BIOL, V328, P19, DOI 10.1016/j.jtbi.2013.03.003
   Wang HT, 2011, PHYS REV E, V83, DOI 10.1103/PhysRevE.83.021915
   Yamauchi S, 2011, FRONT COMPUT NEUROSC, V5, DOI [10.3389/fncom.2011.00042, 10.3389/fncom.2011.00029]
   Zhang, 2019, ARXIV190307864
NR 55
TC 2
Z9 2
U1 4
U2 11
PD APR-JUN
PY 2021
VL 13
IS 2
BP 89
EP 106
DI 10.4018/IJSSCI.2021040105
UT WOS:000713432200005
DA 2023-11-16
ER

PT J
AU Lien, HH
   Chang, TS
AF Lien, Hong-Han
   Chang, Tian-Sheuan
TI Sparse Compressed Spiking Neural Network Accelerator for Object
   Detection
SO IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS I-REGULAR PAPERS
DT Article
DE Object detection; Hardware; Convolution; Training; Neurons; Parallel
   processing; Topology; Spiking neural network; computer vision; object
   detection; VLSI hardware design
ID ON-CHIP; DESIGN; MEMORY
AB Spiking neural networks (SNNs), which are inspired by the human brain, have recently gained popularity due to their relatively simple and low-power hardware for transmitting binary spikes and highly sparse activation maps. However, because SNNs contain extra time dimension information, the SNN accelerator will require more buffers and take longer to infer, especially for the more difficult high-resolution object detection task. As a result, this paper proposes a sparse compressed spiking neural network accelerator that takes advantage of the high sparsity of activation maps and weights by utilizing the proposed gated one-to-all product for low power and highly parallel model execution. The experimental result of the neural network shows 71.5% mAP with mixed (1,3) time steps on the IVS 3cls dataset. The accelerator with the TSMC 28nm CMOS process can achieve 1024x576.29 frames per second processing when running at 500MHz with 35.88TOPS/W energy efficiency and 1.05mJ energy consumption per frame.
C1 [Lien, Hong-Han; Chang, Tian-Sheuan] Natl Yang Ming Chiao Tung Univ, Inst Elect, Hsinchu 30010, Taiwan.
RP Chang, TS (corresponding author), Natl Yang Ming Chiao Tung Univ, Inst Elect, Hsinchu 30010, Taiwan.
EM scott860228.ee08@nycu.edu.tw; tschang@nycu.edu.tw
CR Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Chen QY, 2022, IEEE T CIRCUITS-II, V69, P574, DOI 10.1109/TCSII.2021.3098633
   Chen Y., ARXIV210504916, V2021
   Datta G., 2021, ARXIV211212133
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Deng L, 2023, IEEE T NEUR NET LEAR, V34, P2791, DOI 10.1109/TNNLS.2021.3109064
   Frenkel C, 2019, IEEE T BIOMED CIRC S, V13, P145, DOI 10.1109/TBCAS.2018.2880425
   Gondimalla A, 2019, MICRO'52: THE 52ND ANNUAL IEEE/ACM INTERNATIONAL SYMPOSIUM ON MICROARCHITECTURE, P151, DOI 10.1145/3352460.3358291
   Guo JI, 2020, IEEE J EM SEL TOP C, V10, P388, DOI 10.1109/JETCAS.2020.3015753
   Han S, 2015, ADV NEUR IN, V28
   Khodamoradi Alireza, 2021, FPGA '21: The 2021 ACM/SIGDA International Symposium on Field-Programmable, P194, DOI 10.1145/3431920.3439283
   Kim S, 2020, AAAI CONF ARTIF INTE, V34, P11270
   Kundu S, 2021, IEEE WINT CONF APPL, P3952, DOI 10.1109/WACV48630.2021.00400
   Li G, 2022, IEEE T COMPUT AID D, V41, P1436, DOI 10.1109/TCAD.2021.3082868
   Lien HH, 2021, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS51556.2021.9401181
   Liu YX, 2013, OXID MED CELL LONGEV, V2013, DOI [10.1155/2013/146860, 10.1109/GEFS.2013.6601048]
   Loshchilov Ilya, 2017, ARXIV171105101
   Malladi KT, 2012, CONF PROC INT SYMP C, P37, DOI 10.1109/ISCA.2012.6237004
   Moradi S, 2018, IEEE T BIOMED CIRC S, V12, P106, DOI 10.1109/TBCAS.2017.2759700
   Narayanan S, 2020, ANN I S COM, P349, DOI 10.1109/ISCA45697.2020.00038
   Narayanan S, 2017, IEEE IJCNN, P2451, DOI 10.1109/IJCNN.2017.7966154
   Painkras E, 2013, IEEE J SOLID-ST CIRC, V48, P1943, DOI 10.1109/JSSC.2013.2259038
   Parashar A, 2017, 44TH ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA 2017), P27, DOI 10.1145/3079856.3080254
   Park J, 2019, ISSCC DIG TECH PAP I, V62, P140, DOI 10.1109/ISSCC.2019.8662398
   Rathi N, 2023, IEEE T NEUR NET LEAR, V34, P3174, DOI 10.1109/TNNLS.2021.3111897
   Rathi N, 2019, IEEE T COMPUT AID D, V38, P668, DOI 10.1109/TCAD.2018.2819366
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Schemmel J, 2010, IEEE INT SYMP CIRC S, P1947, DOI 10.1109/ISCAS.2010.5536970
   Shi Y, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00405
   Stuijt J, 2021, FRONT NEUROSCI-SWITZ, V15, DOI 10.3389/fnins.2021.664208
   Tsai CC, 2020, IEEE INT CONF MULTI, DOI 10.1109/icmew46912.2020.9106010
   Wang CY, 2020, IEEE COMPUT SOC CONF, P1571, DOI 10.1109/CVPRW50498.2020.00203
   Wu YJ, 2019, AAAI CONF ARTIF INTE, P1311
   Zheng H., 2020, ARXIV201105280
NR 35
TC 3
Z9 3
U1 3
U2 26
PD MAY
PY 2022
VL 69
IS 5
BP 2060
EP 2069
DI 10.1109/TCSI.2022.3149006
EA FEB 2022
UT WOS:000754272500001
DA 2023-11-16
ER

PT J
AU Shi, KL
   Heng, SZ
   Wang, XJ
   Liu, SY
   Cui, HY
   Chen, CS
   Zhu, YX
   Xu, WG
   Wan, CJ
   Wan, Q
AF Shi, Kailu
   Heng, Sizhuo
   Wang, Xiangjing
   Liu, Siyao
   Cui, Hangyuan
   Chen, Chunsheng
   Zhu, Yixin
   Xu, Weigao
   Wan, Changjin
   Wan, Qing
TI An Oxide Based Spiking Thermoreceptor for Low-Power Thermography Edge
   Detection
SO IEEE ELECTRON DEVICE LETTERS
DT Article
DE Firing; Image edge detection; Power demand; Temperature; Switches;
   Threshold voltage; Neurons; Spiking neural network; neuromorphic
   perceptual system; artificial spiking thermoreceptor; threshold switch
   memristor; edge detection
AB Thermoreceptors can encode thermal stimuli into spikes that are processed by the neural network to endow human with thermal perception. Such energy-efficiency and robust interaction with real-world have inspired the rise of the artificial spiking thermoreceptor (AST). However, monolithic spiking thermoreceptor is still rarely reported, whichmay due to the lack of device that can simultaneously implement temperature-sensing and spike-encoding functions. Here, we demonstrate an artificial spiking thermoreceptor based on Ag/TaOX/AlOX/ ITO threshold switching memristor to achieve human-like thermal perception. The device is able to encode thermal information into spikes at a low power consumption (<240 nW). These advantages consequently facilitate the demonstration of power efficient and accurate thermography edge detection based on the array of such AST and a pulse coupled neural network (PCNN).
C1 [Shi, Kailu; Heng, Sizhuo; Wang, Xiangjing; Liu, Siyao; Cui, Hangyuan; Chen, Chunsheng; Zhu, Yixin; Xu, Weigao; Wan, Changjin; Wan, Qing] Nanjing Univ, Sch Elect Sci & Engn, Nanjing 210023, Peoples R China.
   [Shi, Kailu; Heng, Sizhuo; Wang, Xiangjing; Liu, Siyao; Cui, Hangyuan; Chen, Chunsheng; Zhu, Yixin; Xu, Weigao; Wan, Changjin; Wan, Qing] Nanjing Univ, Sch Chem & Chem Engn, Nanjing 210023, Peoples R China.
RP Wan, CJ; Wan, Q (corresponding author), Nanjing Univ, Sch Elect Sci & Engn, Nanjing 210023, Peoples R China.; Wan, CJ; Wan, Q (corresponding author), Nanjing Univ, Sch Chem & Chem Engn, Nanjing 210023, Peoples R China.
EM cjwan@nju.edu.cn; wanqing@nju.edu.cn
CR Bao L, 2021, IEEE ELECTR DEVICE L, V42, P102, DOI 10.1109/LED.2020.3037779
   Bhatnagar P, 2022, NANO ENERGY, V91, DOI 10.1016/j.nanoen.2021.106676
   Chen CS, 2022, ADV MATER, V34, DOI 10.1002/adma.202201895
   Covi E, 2021, IEEE T ELECTRON DEV, V68, P4335, DOI 10.1109/TED.2021.3076029
   Deng XY, 2022, MULTIMED TOOLS APPL, V81, P27187, DOI 10.1007/s11042-022-12725-2
   Han JK, 2022, ADV FUNCT MATER, V32, DOI 10.1002/adfm.202204102
   Hua QL, 2019, GLOB CHALL, V3, DOI 10.1002/gch2.201900015
   Li FF, 2021, ACS NANO, V15, P16422, DOI 10.1021/acsnano.1c05836
   Midya R, 2019, ADV ELECTRON MATER, V5, DOI 10.1002/aelm.201900060
   Mu BY, 2021, SMALL, V17, DOI 10.1002/smll.202103837
   Nili H, 2016, NANOTECHNOLOGY, V27, DOI 10.1088/0957-4484/27/50/505210
   Park J, 2015, SCI ADV, V1, DOI 10.1126/sciadv.1500661
   Pickett MD, 2013, NAT MATER, V12, P114, DOI [10.1038/nmat3510, 10.1038/NMAT3510]
   Romanovsky AA, 2018, HAND CLINIC, V156, P3, DOI 10.1016/B978-0-444-63912-7.00001-1
   SPRAY DC, 1986, ANNU REV PHYSIOL, V48, P625
   Tang XL, 2014, 2014 11TH INTERNATIONAL COMPUTER CONFERENCE ON WAVELET ACTIVE MEDIA TECHNOLOGY AND INFORMATION PROCESSING (ICCWAMTIP), P169, DOI 10.1109/ICCWAMTIP.2014.7073383
   Tee BCK, 2015, SCIENCE, V350, P313, DOI 10.1126/science.aaa9306
   Wang W, 2021, ADV INTELL SYST-GER, V3, DOI 10.1002/aisy.202000224
   Wang W, 2019, NAT COMMUN, V10, DOI 10.1038/s41467-018-07979-0
   Wang ZX, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00783
   Wang ZR, 2017, NAT MATER, V16, P101, DOI [10.1038/nmat4756, 10.1038/NMAT4756]
   Wu QT, 2020, NANO LETT, V20, P8015, DOI 10.1021/acs.nanolett.0c02892
   Wu XD, 2021, ADV FUNCT MATER, V31, DOI 10.1002/adfm.202010824
   Wu ZH, 2020, ADV MATER, V32, DOI 10.1002/adma.202004398
   Yi W, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-07052-w
   Yoon JH, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-017-02572-3
   Zhang C, 2019, ADV FUNCT MATER, V29, DOI 10.1002/adfm.201808783
   Zhang XM, 2018, IEEE ELECTR DEVICE L, V39, P308, DOI 10.1109/LED.2017.2782752
   Zhu JX, 2022, ADV MATER, V34, DOI 10.1002/adma.202200481
NR 29
TC 3
Z9 3
U1 13
U2 18
PD DEC
PY 2022
VL 43
IS 12
BP 2196
EP 2199
DI 10.1109/LED.2022.3215693
UT WOS:000924865400047
DA 2023-11-16
ER

PT J
AU Iannella, N
   Kindermann, L
AF Iannella, N
   Kindermann, L
TI Finding iterative roots with a spiking neural network
SO INFORMATION PROCESSING LETTERS
DT Article
DE spiking neural network; iterative root; functional equations;
   distributed computing
AB In recent years, both multilayer perceptrons and networks of spiking neurons have been used in applications ranging from detailed models of specific cortical areas to image processing. A more challenging application is to find solutions to functional equations in order to gain insights to underlying phenomena. Finding the roots of real valued monotonically increasing function mappings is the solution to a particular class of functional equation. Furthermore, spiking neural network approaches in solving problems described by functional equations, may be an useful tool to provide important insights to how different regions of the brain may co-ordinate signaling within and between modalities, thus providing a possible basis to construct a theory of brain function. In this letter, we present for the first time a spiking neural network architecture based on integrate-and-fire units and delays, that is capable of calculating the functional or iterative root of nonlinear functions, by solving a particular class of functional equation. (c) 2005 Elsevier B.V. All rights reserved.
C1 RIKEN, Brain Sci Inst, Lab Visual Neurocomp, Wako, Saitama 3510198, Japan.
   RIKEN, Brain Sci Inst, Lab Math Neurosci, Wako, Saitama 3510198, Japan.
RP Iannella, N (corresponding author), RIKEN, Brain Sci Inst, Lab Visual Neurocomp, 2-1 Hirosawa, Wako, Saitama 3510198, Japan.
EM nicolang@brain.riken.go.jp
CR Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   Brunel N, 2000, J COMPUT NEUROSCI, V8, P183, DOI 10.1023/A:1008925309027
   Iannella N, 2004, MATH BIOSCI, V188, P117, DOI 10.1016/j.mbs.2003.10.002
   Iannella N, 2001, NEURAL NETWORKS, V14, P933, DOI 10.1016/S0893-6080(01)00080-6
   Izhikevich EM, 2000, INT J BIFURCAT CHAOS, V10, P1171, DOI 10.1142/S0218127400000840
   Izhikevich EM, 2001, SIAM REV, V43, P315, DOI 10.1137/S0036144500382064
   Kindermann L, 1998, ICONIP'98: THE FIFTH INTERNATIONAL CONFERENCE ON NEURAL INFORMATION PROCESSING JOINTLY WITH JNNS'98: THE 1998 ANNUAL CONFERENCE OF THE JAPANESE NEURAL NETWORK SOCIETY - PROCEEDINGS, VOLS 1-3, P713
   KINDERMANN L, 2000, P 7 INT C NEUR INF P, P565
   KUZMA M, 1990, ITERATIVE FUNCTIONAL
   RICE RE, 1980, AM MATH MON, V87, P252, DOI 10.2307/2321556
NR 11
TC 25
Z9 26
U1 0
U2 4
PD SEP 30
PY 2005
VL 95
IS 6
BP 545
EP 551
DI 10.1016/j.ipl.2005.05.022
UT WOS:000231637900005
DA 2023-11-16
ER

PT J
AU Stromatias, E
   Soto, M
   Serrano-Gotarredona, T
   Linares-Barranco, B
AF Stromatias, Evangelos
   Soto, Miguel
   Serrano-Gotarredona, Teresa
   Linares-Barranco, Bernabe
TI An Event-Driven Classifier for Spiking Neural Networks Fed with
   Synthetic or Dynamic Vision Sensor Data
SO FRONTIERS IN NEUROSCIENCE
DT Article
DE spiking neural networks; supervised learning; event driven processing;
   DVS sensors; convolutional neural networks; fully connected neural
   networks; neuromorphic
ID LARGE-SCALE MODEL
AB This paper introduces a novel methodology for training an event-driven classifier within a Spiking Neural Network (SNN) System capable of yielding good classification results when using both synthetic input data and real data captured from Dynamic Vision Sensor (DVS) chips. The proposed supervised method uses the spiking activity provided by an arbitrary topology of prior SNN layers to build histograms and train the classifier in the frame domain using the stochastic gradient descent algorithm. In addition, this approach can cope with leaky integrate-and-fire neuron models within the SNN, a desirable feature for real-world SNN applications, where neural activation must fade away after some time in the absence of inputs. Consequently, this way of building histograms captures the dynamics of spikes immediately before the classifier. We tested our method on the MNIST data set using different synthetic encodings and real DVS sensory data sets such as N-MNIST, MNIST-DVS, and Poker-DVS using the same network topology and feature maps. We demonstrate the effectiveness of our approach by achieving the highest classification accuracy reported on the N-MNIST (97.77%) and Poker-DVS (100%) real DVS data sets to date with a spiking convolutional network. Moreover, by using the proposed method we were able to retrain the output layer of a previously reported spiking neural network and increase its performance by 2%, suggesting that the proposed classifier can be used as the output layer in works where features are extracted using unsupervised spike-based learning methods. In addition, we also analyze SNN performance figures such as total event activity and network latencies, which are relevant for eventual hardware implementations. In summary, the paper aggregates unsupervised-trained SNNs with a supervised-trained SNN classifier, combining and applying them to heterogeneous sets of benchmarks, both synthetic and from real DVS chips.
C1 [Stromatias, Evangelos; Soto, Miguel; Serrano-Gotarredona, Teresa; Linares-Barranco, Bernabe] Univ Seville, CSIC, Inst Microelectron Sevilla CNM, Seville, Spain.
RP Linares-Barranco, B (corresponding author), Univ Seville, CSIC, Inst Microelectron Sevilla CNM, Seville, Spain.
EM bernabe@imse-cnm.csic.es
CR Anand R, 2009, SELF-DEFENSE IN INTERNATIONAL RELATIONS, P1, DOI 10.1057/9780230245747
   [Anonymous], 2012, 2012 INT JOINT C NEU, DOI [DOI 10.1109/IJCNN.2012.6252637, 10.1109/IJCNN.2012.6252637]
   Leñero-Bardallo JA, 2011, IEEE J SOLID-ST CIRC, V46, P1443, DOI 10.1109/JSSC.2011.2118490
   Pérez-Carrasco JA, 2013, IEEE T PATTERN ANAL, V35, P2706, DOI 10.1109/TPAMI.2013.71
   Bichler O, 2012, NEURAL NETWORKS, V32, P339, DOI 10.1016/j.neunet.2012.02.022
   Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16
   Camuñas-Mesa L, 2010, IEEE INT SYMP CIRC S, P249, DOI 10.1109/ISCAS.2010.5537918
   Camuñas-Mesa L, 2012, IEEE J SOLID-ST CIRC, V47, P504, DOI 10.1109/JSSC.2011.2167409
   Camuñas-Mesa L, 2011, IEEE T CIRCUITS-I, V58, P777, DOI 10.1109/TCSI.2010.2078851
   Ciresan D, 2012, PROC CVPR IEEE, P3642, DOI 10.1109/CVPR.2012.6248110
   Cohen GK, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00184
   DAN Y, 1992, SCIENCE, V256, P1570, DOI 10.1126/science.1317971
   Delbruck T., 2013, JAER OPEN SOURCE PRO
   Diehl P.U., 2015, 2015 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2015.7280696
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Eliasmith C, 2012, SCIENCE, V338, P1202, DOI 10.1126/science.1225266
   Esser S. K., 2015, ADV NEURAL INFORM PR, P1117
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   Gerstner W., 2002, SPIKING NEURON MODEL
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hunsberger E., 2015, ABS151008829 CORR
   Isaksson A, 2008, PATTERN RECOGN LETT, V29, P1960, DOI 10.1016/j.patrec.2008.06.018
   Izhikevich EM, 2008, P NATL ACAD SCI USA, V105, P3593, DOI 10.1073/pnas.0712231105
   Kheradpisheh S. R., 2016, ABS161101421 CORR
   Lagorce X, 2017, IEEE T PATTERN ANAL, V39, P1346, DOI 10.1109/TPAMI.2016.2574707
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee JM, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00191
   Lichtsteiner Patrick, 2008, IEEE Journal of Solid-State Circuits, V43, P566, DOI 10.1109/JSSC.2007.914337
   Liu Q, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00496
   Liu SC, 2010, IEEE INT SYMP CIRC S, P2027, DOI 10.1109/ISCAS.2010.5537164
   Maass W, 2004, J COMPUT SYST SCI, V69, P593, DOI 10.1016/j jcss.2004.04.001
   Mahowald M., 1994, ANALOG VLSI SYSTEM S
   Masquelier T, 2007, PLOS COMPUT BIOL, V3, P247, DOI 10.1371/journal.pcbi.0030031
   MEAD C, 1990, P IEEE, V78, P1629, DOI 10.1109/5.58356
   Merolla P., 2010, ABS10095473 CORR
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   MIT Technology Review, 2013, 10 BREAKTHR TECHN 20
   Neftci E., 2017, ABS161205596 CORR
   Neftci E, 2014, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00272
   Neftci EO, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00241
   Neil D, 2016, IEEE INT SYMP CIRC S, P2282, DOI 10.1109/ISCAS.2016.7539039
   Neil D, 2014, IEEE T VLSI SYST, V22, P2621, DOI 10.1109/TVLSI.2013.2294916
   Nowotny T, 2014, FRONT ROBOT AI, DOI 10.3389/frobt.2014.00005
   O'Connor P., 2016, ABS160208323 CORR
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   Orchard G, 2015, FRONT NEUROSCI-SWITZ, V9, DOI [10.3389/fhins.2015.00437, 10.3389/fnins.2015.00437]
   Orchard G, 2015, IEEE T PATTERN ANAL, V37, P2028, DOI 10.1109/TPAMI.2015.2392947
   Posch C, 2014, P IEEE, V102, P1470, DOI 10.1109/JPROC.2014.2346153
   Posch C, 2011, IEEE J SOLID-ST CIRC, V46, P259, DOI 10.1109/JSSC.2010.2085952
   Querlioz D, 2013, IEEE T NANOTECHNOL, V12, P288, DOI 10.1109/TNANO.2013.2250995
   Roclin D., 2013, NEUR NETW IJCNN 2013, P1
   Rueckauer B., 2016, ARXIV161204052 CORR
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Serrano-Gotarredona T, 2015, IEEE INT SYMP CIRC S, P2405, DOI 10.1109/ISCAS.2015.7169169
   Serrano-Gotarredona T, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00481
   Serrano-Gotarredona T, 2013, IEEE J SOLID-ST CIRC, V48, P827, DOI 10.1109/JSSC.2012.2230553
   SIEGERT AJF, 1951, PHYS REV, V81, P617, DOI 10.1103/PhysRev.81.617
   Soto M., 2017, SLOW POKER DVS DATA
   Stromatias E., 2013, 2013 INT JOINT C NEU, P1, DOI [DOI 10.1109/IJCNN.2013.6706927, 10.1109/ijcnn.2013.6706927]
   Stromatias E, 2015, IEEE IJCNN
   Stromatias E, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00222
   Tapson JC, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00104
   van Schaik A, 2015, NEUROCOMPUTING, V149, P233, DOI 10.1016/j.neucom.2014.01.071
NR 66
TC 67
Z9 72
U1 1
U2 20
PD JUN 28
PY 2017
VL 11
AR 350
DI 10.3389/fnins.2017.00350
UT WOS:000406553000001
DA 2023-11-16
ER

PT J
AU Zhang, AG
   Li, XM
   Gao, YM
   Niu, YZ
AF Zhang, Anguo
   Li, Xiumin
   Gao, Yueming
   Niu, Yuzhen
TI Event-Driven Intrinsic Plasticity for Spiking Convolutional Neural
   Networks
SO IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
DT Article
DE IP networks; Neurons; Biological neural networks; Computational
   modeling; Biological system modeling; Encoding; Biomembranes;
   Event-driven intrinsic plasticity (IP); input-driven IP; IP; self-driven
   IP; spiking neural network (SNN)
ID EXCITABILITY; NEURONS; SPARSE
AB The biologically discovered intrinsic plasticity (IP) learning rule, which changes the intrinsic excitability of an individual neuron by adaptively turning the firing threshold, has been shown to be crucial for efficient information processing. However, this learning rule needs extra time for updating operations at each step, causing extra energy consumption and reducing the computational efficiency. The event-driven or spike-based coding strategy of spiking neural networks (SNNs), i.e., neurons will only be active if driven by continuous spiking trains, employs all-or-none pulses (spikes) to transmit information, contributing to sparseness in neuron activations. In this article, we propose two event-driven IP learning rules, namely, input-driven and self-driven IP, based on basic IP learning. Input-driven means that IP updating occurs only when the neuron receives spiking inputs from its presynaptic neurons, whereas self-driven means that IP updating only occurs when the neuron generates a spike. A spiking convolutional neural network (SCNN) is developed based on the ANN2SNN conversion method, i.e., converting a well-trained rate-based artificial neural network to an SNN via directly mapping the connection weights. By comparing the computational performance of SCNNs with different IP rules on the recognition of MNIST, FashionMNIST, Cifar10, and SVHN datasets, we demonstrate that the two event-based IP rules can remarkably reduce IP updating operations, contributing to sparse computations and accelerating the recognition process. This work may give insights into the modeling of brain-inspired SNNs for low-power applications.
C1 [Zhang, Anguo; Gao, Yueming] Fuzhou Univ, Coll Phys & Informat Engn, Fuzhou 350108, Peoples R China.
   [Zhang, Anguo; Gao, Yueming] Key Lab Med Instrumentat & Pharmaceut Technol Fuj, Fuzhou 350116, Peoples R China.
   [Zhang, Anguo] Ruijie Networks Co Ltd, Res Inst Ruijie, Fuzhou 350002, Peoples R China.
   [Li, Xiumin] Chongqing Univ, Coll Automat, Chongqing 400030, Peoples R China.
   [Niu, Yuzhen] Fuzhou Univ, Coll Math & Comp Sci, Fujian Key Lab Network Comp & Intelligent Informa, Fuzhou 350108, Fujian, Peoples R China.
   [Niu, Yuzhen] Minist Educ, Key Lab Spatial Data Min & Informat Sharing, Fuzhou 350108, Fujian, Peoples R China.
RP Li, XM (corresponding author), Chongqing Univ, Coll Automat, Chongqing 400030, Peoples R China.
EM xmli@cqu.edu.cn
CR [Anonymous], ARXIV160902053
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Compte A, 2003, J NEUROPHYSIOL, V89, P2707, DOI 10.1152/jn.00845.2002
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Debanne D, 2019, CURR OPIN NEUROBIOL, V54, P73, DOI 10.1016/j.conb.2018.09.001
   Deng L, 2020, NEURAL NETWORKS, V121, P294, DOI 10.1016/j.neunet.2019.09.005
   Desai NS, 1999, NAT NEUROSCI, V2, P515, DOI 10.1038/9165
   Diehl P.U., 2015, 2015 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2015.7280696
   Diehl PU, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON REBOOTING COMPUTING (ICRC)
   Disterhoft JF, 2006, TRENDS NEUROSCI, V29, P587, DOI 10.1016/j.tins.2006.08.005
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   Han B, 2018, IEEE T MULTI-SCALE C, V4, P613, DOI 10.1109/TMSCS.2017.2737625
   Histed MH, 2009, NEURON, V63, P508, DOI 10.1016/j.neuron.2009.07.016
   Holt GR, 1997, NEURAL COMPUT, V9, P1001, DOI 10.1162/neco.1997.9.5.1001
   Jadi MP, 2014, P NATL ACAD SCI USA, V111, P6780, DOI 10.1073/pnas.1405300111
   Johnson AP, 2018, IEEE T CIRCUITS-I, V65, P687, DOI 10.1109/TCSI.2017.2726763
   Kaiser J., 2019, ARXIV190404805
   Krizhevsky Alex, 2009, LEARNING MULTIPLE LA
   Li CG, 2013, IEEE T AUTON MENT DE, V5, P62, DOI 10.1109/TAMD.2012.2211101
   Li CG, 2011, IEEE T AUTON MENT DE, V3, P277, DOI 10.1109/TAMD.2011.2159379
   Maass W, 2004, J COMPUT SYST SCI, V69, P593, DOI 10.1016/j jcss.2004.04.001
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   MUKHOPADHYAY AK, 2018, P IEEE SENSORS, P2018, DOI DOI 10.1109/ICSENS.2018.8589757
   Nataraj K, 2010, NEURON, V68, P750, DOI 10.1016/j.neuron.2010.09.033
   Naveros F, 2015, IEEE T NEUR NET LEAR, V26, P1567, DOI 10.1109/TNNLS.2014.2345844
   Neckar A, 2019, P IEEE, V107, P144, DOI 10.1109/JPROC.2018.2881432
   Neftci E, 2014, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00272
   Neftci EO, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00324
   Netzer Y., 2011, READING DIGITS NATUR, V2, P5
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   Rueckauer B., 2016, ARXIV161204052
   Rueckauer B, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351295
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Stromatias E, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00350
   Susi G., 2018, ARXIV180100864
   Touboul JD, 2011, J COMPUT NEUROSCI, V31, P485, DOI 10.1007/s10827-011-0327-y
   Triesch J, 2007, NEURAL COMPUT, V19, P885, DOI 10.1162/neco.2007.19.4.885
   Wang H, 2019, BRAIN TOPOGR, V32, P255, DOI 10.1007/s10548-018-0682-3
   Xiao H., 2017, ARXIV170807747
   Zhang AG, 2023, IEEE T COGN DEV SYST, V15, P337, DOI 10.1109/TCDS.2020.3041610
   Zhang AG, 2019, NEUROCOMPUTING, V365, P102, DOI 10.1016/j.neucom.2019.07.009
   Zhang ML, 2019, IEEE T NEUR NET LEAR, V30, P123, DOI 10.1109/TNNLS.2018.2833077
   Zhang ML, 2018, IEEE T COGN DEV SYST, V10, P151, DOI 10.1109/TCDS.2017.2651943
   Zhang W, 2003, NAT REV NEUROSCI, V4, P885, DOI 10.1038/nrn1248
NR 46
TC 16
Z9 16
U1 2
U2 17
PD MAY
PY 2022
VL 33
IS 5
BP 1986
EP 1995
DI 10.1109/TNNLS.2021.3084955
EA JUN 2021
UT WOS:000732090100001
DA 2023-11-16
ER

PT C
AU Ye, C
   Kornijcuk, V
   Kim, J
   Jeong, DS
AF Ye, ChangMin
   Kornijcuk, Vladimir
   Kim, Jeeson
   Jeong, Doo Seok
GP IEEE
TI FPGA implementation of sequence-to-sequence predicting spiking neural
   networks
SO 2020 17TH INTERNATIONAL SOC DESIGN CONFERENCE (ISOCC 2020)
SE International SoC Design Conference
DT Proceedings Paper
CT 17th International SoC Design Conference (ISOCC)
CY OCT 21-24, 2020
CL Yeosu, SOUTH KOREA
DE sequence-predicting spiking neural network; LbAP algorithm; rule-based
   event routing
AB We propose a hardware-efficient method to implement sequence-predicting spiking neural networks (SPSNN) on a field-programmable gate array board. The SPSNN is capable of sequence-to-sequence prediction (associative recall) when fully trained using the learning by backpropagating action potential (LbAP) algorithm. The key to the hardware-efficiency lies in the rule-based event (routing) method in place of conventional lookup-table-based methods which are memory-hungry methods, particularly, when both forward and inverse lookups should be considered.
C1 [Ye, ChangMin; Kornijcuk, Vladimir; Kim, Jeeson; Jeong, Doo Seok] Hanyang Univ, Div Mat Sci & Engn, Seoul, South Korea.
RP Jeong, DS (corresponding author), Hanyang Univ, Div Mat Sci & Engn, Seoul, South Korea.
EM dooseokj@hanyang.ac.kr
CR Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Gerstner W., 2002, SPIKING NEURON MODEL
   Kim D, 2020, IEEE ACCESS, V8, P110523, DOI 10.1109/ACCESS.2020.3001296
   Kornijcuk V, 2019, ADV MATER TECHNOL-US, V4, DOI 10.1002/admt.201800345
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Pedroni BU, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00357
NR 6
TC 0
Z9 0
U1 0
U2 2
PY 2020
BP 322
EP 323
DI 10.1109/ISOCC50952.2020.9332910
UT WOS:000680824100154
DA 2023-11-16
ER

PT C
AU Traub, M
   Legenstein, R
   Otte, S
AF Traub, Manuel
   Legenstein, Robert
   Otte, Sebastian
GP IEEE
TI Many-Joint Robot Arm Control with Recurrent Spiking Neural Networks
SO 2021 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS
   (IROS)
SE IEEE International Conference on Intelligent Robots and Systems
DT Proceedings Paper
CT IEEE/RSJ International Conference on Intelligent Robots and Systems
   (IROS)
CY SEP 27-OCT 01, 2021
CL ELECTR NETWORK
ID KINEMATICS
AB In the paper, we show how scalable, low-cost trunk-like robotic arms can be constructed using only basic 3D-printing equipment and simple electronics. The design is based on uniform, stackable joint modules with three degrees of freedom each. Moreover, we present an approach for controlling these robots with recurrent spiking neural networks. At first, a spiking forward model learns motor-pose correlations from movement observations. After training, intentions can be projected back through unrolled spike trains of the forward model essentially routing the intention-driven motor gradients towards the respective joints, which unfolds goal-direction navigation. We demonstrate that spiking neural networks can thus effectively control trunk-like robotic arms with up to 75 articulated degrees of freedom with near millimeter accuracy.
C1 [Traub, Manuel; Otte, Sebastian] Univ Tubingen, Neurocognit Modeling Grp, Comp Sci Dept, Sand 14, D-72076 Tubingen, Germany.
   [Legenstein, Robert] Graz Univ Technol, Fac Comp Sci & Biomed Engn, Inffeldgasse 16b, A-8010 Graz, Austria.
RP Traub, M (corresponding author), Univ Tubingen, Neurocognit Modeling Grp, Comp Sci Dept, Sand 14, D-72076 Tubingen, Germany.
EM manuel.traub@uni-tuebingen.de; robert.legenstein@igi.tugraz.at;
   sebastian.otte@uni-tuebingen.de
CR [Anonymous], 1997, NEURAL COMPUT, DOI 10.1162/neco.1997.9.8.1735
   Bartow A., 2013, P 12 WSEAS INT C SIG, P181
   Bayani S, 2015, IEEE ASME INT C ADV, P1271, DOI 10.1109/AIM.2015.7222713
   Bellec G., 2018, ADV NEURAL INFORM PR
   Bellec G, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-17236-y
   Bing ZS, 2018, FRONT NEUROROBOTICS, V12, DOI 10.3389/fnbot.2018.00035
   Bouganis A., 2010, P 2010 INT JOINT C N, P1, DOI DOI 10.1109/IJCNN.2010.5596525
   Crespi A, 2005, ROBOT AUTON SYST, V50, P163, DOI 10.1016/j.robot.2004.09.015
   Hannan MW, 2003, J ROBOTIC SYST, V20, P45, DOI 10.1002/rob.10070
   Kingma DP., 2017, ARXIV
   Kintel M., 2009, OPENSCAD PROGRAMMERS
   Maass W, 1998, PULSED NEURAL NETWORKS, P55
   Oja M, 2003, NEURAL COMPUTING SUR, V3, P1
   Otte S, 2018, LECT NOTES COMPUT SC, V11141, P748, DOI 10.1007/978-3-030-01424-7_73
   Otte S, 2017, LECT NOTES COMPUT SC, V10613, P262, DOI 10.1007/978-3-319-68600-4_31
   Otte S, 2016, LECT NOTES COMPUT SC, V9886, P149, DOI 10.1007/978-3-319-44778-0_18
   Park HY, 2020, ADV MATER, V32, DOI 10.1002/adma.202002120
   Reddi S. J., 2019, ARXIV190409237
   Rolf M, 2014, IEEE T NEUR NET LEAR, V25, P1147, DOI 10.1109/TNNLS.2013.2287890
   Roy K, 2019, NATURE, V575, P607, DOI 10.1038/s41586-019-1677-2
   Schumann CL, 2019, AIDS BEHAV, V23, P5, DOI 10.1007/s10461-017-1727-4
   Transeth AA, 2009, ROBOTICA, V27, P999, DOI 10.1017/S0263574709005414
NR 22
TC 0
Z9 0
U1 0
U2 0
PY 2021
BP 4918
EP 4925
DI 10.1109/IROS51168.2021.9636001
UT WOS:000755125503134
DA 2023-11-16
ER

PT J
AU Masquelier, T
AF Masquelier, Timothee
TI Back-propagation Now Works in Spiking Neural Networks!
SO ERCIM NEWS
DT Article
AB Back-propagation is THE learning algorithm behind the deep learning revolution. Until recently, it was not possible to use it in spiking neural networks (SNN), due to non-differentiability issues. But these issues can now be circumvented, signalling a new era for SNNs.
C1 [Masquelier, Timothee] Univ Toulouse 3, UMR5549, CNRS, Ctr Rech Cerveau & Cognit, Toulouse, France.
RP Masquelier, T (corresponding author), Univ Toulouse 3, UMR5549, CNRS, Ctr Rech Cerveau & Cognit, Toulouse, France.
EM timothee.masquelier@cnrs.fr
CR Kheradpisheh SR, 2020, INT J NEURAL SYST, V30, DOI 10.1142/S0129065720500276
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Pellegrini T, 2021, IEEE W SP LANG TECH, P97, DOI 10.1109/SLT48900.2021.9383587
   Wozniak S, 2020, NAT MACH INTELL, V2, P325, DOI 10.1038/s42256-020-0187-0
NR 4
TC 0
Z9 0
U1 0
U2 0
PD APR
PY 2021
IS 125
SI SI
BP 11
EP 12
UT WOS:000637044400004
DA 2023-11-16
ER

PT J
AU He, H
   Shang, YJ
   Yang, X
   Di, YZ
   Lin, JJ
   Zhu, YM
   Zheng, WH
   Zhao, JF
   Ji, MY
   Dong, LY
   Deng, N
   Lei, YL
   Chai, ZH
AF He, Hu
   Shang, Yingjie
   Yang, Xu
   Di, Yingze
   Lin, Jiajun
   Zhu, Yimeng
   Zheng, Wenhao
   Zhao, Jinfeng
   Ji, Mengyao
   Dong, Liya
   Deng, Ning
   Lei, Yunlin
   Chai, Zenghao
TI Constructing an Associative Memory System Using Spiking Neural Network
SO FRONTIERS IN NEUROSCIENCE
DT Article
DE spiking neural network; artificial intelligence; associative memory
   system; Hebb's rule; STDP
AB Development of computer science has led to the blooming of artificial intelligence (AI), and neural networks are the core of AI research. Although mainstream neural networks have done well in the fields of image processing and speech recognition, they do not perform well in models aimed at understanding contextual information. In our opinion, the reason for this is that the essence of building a neural network through parameter training is to fit the data to the statistical law through parameter training. Since the neural network built using this approach does not possess memory ability, it cannot reflect the relationship between data with respect to the causality. Biological memory is fundamentally different from the current mainstream digital memory in terms of the storage method. The information stored in digital memory is converted to binary code and written in separate storage units. This physical isolation destroys the correlation of information. Therefore, the information stored in digital memory does not have the recall or association functions of biological memory which can present causality. In this paper, we present the results of our preliminary effort at constructing an associative memory system based on a spiking neural network. We broke the neural network building process into two phases: the Structure Formation Phase and the Parameter Training Phase. The Structure Formation Phase applies a learning method based on Hebb's rule to provoke neurons in the memory layer growing new synapses to connect to neighbor neurons as a response to the specific input spiking sequences fed to the neural network. The aim of this phase is to train the neural network to memorize the specific input spiking sequences. During the Parameter Training Phase, STDP and reinforcement learning are employed to optimize the weight of synapses and thus to find a way to let the neural network recall the memorized specific input spiking sequences. The results show that our memory neural network could memorize different targets and could recall the images it had memorized.
C1 [He, Hu; Shang, Yingjie; Dong, Liya; Deng, Ning] Tsinghua Univ, Inst Microelect, Beijing, Peoples R China.
   [Yang, Xu; Di, Yingze; Lin, Jiajun; Zhu, Yimeng; Zheng, Wenhao; Zhao, Jinfeng; Ji, Mengyao; Lei, Yunlin; Chai, Zenghao] Beijing Inst Technol, Sch Comp Sci & Technol, Beijing, Peoples R China.
RP Yang, X (corresponding author), Beijing Inst Technol, Sch Comp Sci & Technol, Beijing, Peoples R China.
EM yangxu@tsinghua.edu.cn
CR [Anonymous], 1996, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.1996.517075
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bohte S. M., 2000, TECHNICAL REPORT
   Egmont-Petersen M, 2002, PATTERN RECOGN, V35, P2279, DOI 10.1016/S0031-3203(01)00178-9
   Fiesler E., 1994, INT C ART NEUR NETW, P26
   He H, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0212368
   Hebb D. O., 1988, NEUROCOMPUTING FDN R
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   HOPFIELD JJ, 1988, IEEE CIRCUIT DEVIC, V4, P3, DOI 10.1109/101.8118
   Indiveri G., 2003, CIRC SYST 2003 P 200, V4, P4, DOI DOI 10.1109/ISCAS.2003.1206342
   Jennings N.R., 2012, FDN MACHINE LEARNING
   Kasabov N, 2013, NEURAL NETWORKS, V41, P188, DOI 10.1016/j.neunet.2012.11.014
   Kasabov NK, 2017, IEEE T NEUR NET LEAR, V28, P887, DOI 10.1109/TNNLS.2016.2612890
   Kasabov NK, 2014, NEURAL NETWORKS, V52, P62, DOI 10.1016/j.neunet.2014.01.006
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   LeCun Yann, 2010, MNIST DATABASE HANDW
   Perez-Uribe A., 1999, STRUCTURE ADAPTABLE
   Plesser H. E., 2015, NEST NEURAL SIMULATI
   Quinlan PT, 1998, NEURAL NETWORKS, V11, P577, DOI 10.1016/S0893-6080(98)00033-1
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   SMIEJA FJ, 1993, CIRC SYST SIGNAL PR, V12, P331, DOI 10.1007/BF01189880
   Thorpe S, 1998, COMPUTATIONAL NEUROSCIENCE: TRENDS IN RESEARCH, P113
   Wade JJ, 2010, IEEE T NEURAL NETWOR, V21, P1817, DOI 10.1109/TNN.2010.2074212
   Zaknich A, 1998, IEEE T SIGNAL PROCES, V46, P1980, DOI 10.1109/78.700969
NR 25
TC 12
Z9 12
U1 1
U2 10
PD JUL 3
PY 2019
VL 13
AR 650
DI 10.3389/fnins.2019.00650
UT WOS:000473596900001
DA 2023-11-16
ER

PT J
AU Afifi, A
   Ayatollahi, A
   Raissi, F
AF Afifi, Ahmad
   Ayatollahi, Ahmad
   Raissi, Farshid
TI CMOL implementation of spiking neurons and spike-timing dependent
   plasticity
SO INTERNATIONAL JOURNAL OF CIRCUIT THEORY AND APPLICATIONS
DT Article
DE nanoelectronics; CMOL; spiking neurons; neuromorphic networks; STDP
   learning
ID SYNAPTIC PLASTICITY; STDP; CIRCUITS; ARCHITECTURES; SYNAPSES; NETWORKS;
   MODELS; POWER; CNN
AB Successful implementation of spiking neural networks onto CMOS-Molecular (CMOL) architecture has already been proposed, but the ability of dynamic learning has not yet been addressed. Here, we propose a spiking neural topology with spike-timing-dependent learning ability and provide its basic building blocks that are easily mapped onto CMOL architecture. The learning method modifies state of synaptic switches, using spatially and temporally local information which is available at the synapse when state modification is performed.
   The performance of the proposed topology is analyzed with regards to pre- and post-synaptic spike timing, and simulation results are provided for a synapse with spike-timing-dependent plasticity properties. Furthermore, its performance as spike-timing correlation learning and synchrony detection in a small feed-forward network is demonstrated as a case example. Copyright (C) 2010 John Wiley & Sons, Ltd.
C1 [Afifi, Ahmad; Ayatollahi, Ahmad] Iran Univ Sci & Technol, EE Dept, Tehran, Iran.
   [Raissi, Farshid] KN Toosi Univ Technol, ECE Dept, Tehran, Iran.
RP Afifi, A (corresponding author), Iran Univ Sci & Technol, EE Dept, Tehran, Iran.
EM ah_afifi@iust.ac.ir
CR Afifi A, 2009, IEICE ELECTRON EXPR, V6, P148, DOI 10.1587/elex.6.148
   [Anonymous], 2005, ACM J EMERG TECH COM, DOI DOI 10.1145/1084748.1084750
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Chen A, 2005, INT EL DEVICES MEET, P765
   Chicca E, 2003, IEEE T NEURAL NETWOR, V14, P1297, DOI 10.1109/TNN.2003.816367
   Csurgay AI, 2007, INT J CIRC THEOR APP, V35, P471, DOI 10.1002/cta.444
   Debanne D, 1998, J PHYSIOL-LONDON, V507, P237, DOI 10.1111/j.1469-7793.1998.237bu.x
   Gao CJ, 2007, IEEE T CIRCUITS-I, V54, P2502, DOI 10.1109/TCSI.2007.907830
   Gerstner W, 1998, PULSED NEURAL NETWORKS, P353
   Gerstner W., 2002, SPIKING NEURON MODEL
   Gorchetchnikov A, 2005, NEURAL NETWORKS, V18, P458, DOI 10.1016/j.neunet.2005.06.019
   Grossberg S., 1974, Progress theor Biol, V3, P51
   HEBB DO, 1988, ORG BEHAV, P60
   Hoekstra J, 2007, INT J CIRC THEOR APP, V35, P213, DOI 10.1002/cta.412
   Indiveri G, 2006, IEEE T NEURAL NETWOR, V17, P211, DOI 10.1109/TNN.2005.860850
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, NEURAL COMPUT, V15, P1511, DOI 10.1162/089976603321891783
   Kistler WM, 2000, NEURAL COMPUT, V12, P385, DOI 10.1162/089976600300015844
   LEE JH, 2005, IWANN 2005 SPAIN, P446
   Lee JH, 2007, INT J CIRC THEOR APP, V35, P239, DOI 10.1002/cta.410
   Likharev KK, 2005, LECT NOTES PHYS, V680, P447
   Maass W, 2004, J COMPUT SYST SCI, V69, P593, DOI 10.1016/j jcss.2004.04.001
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   Masoumi M, 2006, NANOTECHNOLOGY, V17, P89, DOI 10.1088/0957-4484/17/1/015
   Mozsáry A, 2007, INT J CIRC THEOR APP, V35, P149, DOI 10.1002/cta.385
   Rák A, 2009, INT J CIRC THEOR APP, V37, P587, DOI 10.1002/cta.569
   Ravinuthula V, 2009, INT J CIRC THEOR APP, V37, P631, DOI 10.1002/cta.488
   Sasaki K, 2006, IEICE T ELECTRON, VE89C, P1637, DOI 10.1093/ietele/e89-c.11.1637
   Saudargiene A, 2004, NEURAL COMPUT, V16, P595, DOI 10.1162/089976604772744929
   Snider GS, 2007, NANOTECHNOLOGY, V18, DOI 10.1088/0957-4484/18/36/365202
   Snider GS, 2008, 2008 IEEE INTERNATIONAL SYMPOSIUM ON NANOSCALE ARCHITECTURES, P85, DOI 10.1109/NANOARCH.2008.4585796
   Snider GS, 2007, NANOTECHNOLOGY, V18, DOI 10.1088/0957-4484/18/3/035204
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Strukov DB, 2005, NANOTECHNOLOGY, V16, P888, DOI 10.1088/0957-4484/16/6/045
   Strukov DB, 2005, NANOTECHNOLOGY, V16, P137, DOI 10.1088/0957-4484/16/1/028
   Türel Ö, 2004, INT J CIRC THEOR APP, V32, P277, DOI 10.1002/cta.282
   Türel Ö, 2003, INT J CIRC THEOR APP, V31, P37, DOI 10.1002/cta.223
   VOGELSTEIN RJ, 2003, P NIPS 03, P15
   Wagner R, 2009, INT J CIRC THEOR APP, V37, P87, DOI 10.1002/cta.498
   ZHANG W, 2008, 8 IEEE C NAN NANO 08, P737
NR 41
TC 9
Z9 10
U1 0
U2 15
PD APR
PY 2011
VL 39
IS 4
BP 357
EP 372
DI 10.1002/cta.638
UT WOS:000289372800001
DA 2023-11-16
ER

PT C
AU Zhao, XM
   Lin, XH
   Zhang, Z
AF Zhao, Xiaoman
   Lin, Xianghong
   Zhang, Zhen
BE Paul, R
TI A Semi-Supervised Multi-Spike Learning Algorithm for Deep Spiking Neural
   Networks
SO 2023 IEEE 13TH ANNUAL COMPUTING AND COMMUNICATION WORKSHOP AND
   CONFERENCE, CCWC
DT Proceedings Paper
CT IEEE 13th Annual Computing and Communication Workshop and Conference
   (CCWC)
CY MAR 08-11, 2023
CL ELECTR NETWORK
DE deep spiking neural network; semi-supervised learning; STDP; broadcast
   alignment; pattern classification
AB The deep spiking neural network (DSNN) model contains a mass of parameters, a high-performance deep model depends on a huge quantity of labeled data for solving specific problems, but collecting these labeled data is time-consuming and costly. Semi-supervised learning methods can overcome these difficulties by the unlabeled and labeled data. This paper proposes a semi-supervised multi- spike learning algorithm for DSNNs, in which the unsupervised learning rule based on spike timing-dependent plasticity is applied to adjust the synaptic weights through the unlabeled data, and the supervised learning rule based on broadcast alignment mechanism is applied to update the network weights through the labeled data. Applying spike train to encode image data, the proposed algorithm is validated on the MNIST digital image benchmark dataset. Compared with supervised learning using the labeled data, experiments indicate that the comparable classification accuracy can be achieved by the proposed semi-supervised method in DSNNs.
C1 [Zhao, Xiaoman; Lin, Xianghong; Zhang, Zhen] Northwest Normal Univ, Coll Comp Sci & Engn, Lanzhou, Peoples R China.
RP Zhao, XM (corresponding author), Northwest Normal Univ, Coll Comp Sci & Engn, Lanzhou, Peoples R China.
EM 2021212120@nwnu.edu.cn; linxh@nwnu.edu.cn; 2020211974@nwnu.edu.cn
CR Barber MJ, 2009, PHYS REV E, V80, DOI 10.1103/PhysRevE.80.026129
   Belkin M, 2006, J MACH LEARN RES, V7, P2399
   Bengio Y, 2016, Arxiv, DOI arXiv:1502.04156
   Bengio Y, 2017, NEURAL COMPUT, V29, P555, DOI 10.1162/NECO_a_00934
   Bohte Sander M., 2004, Natural Computing, V3, P195, DOI 10.1023/B:NACO.0000027755.02868.60
   Fatahi M, 2016, 2016 6TH INTERNATIONAL CONFERENCE ON COMPUTER AND KNOWLEDGE ENGINEERING (ICCKE), P153, DOI 10.1109/ICCKE.2016.7802132
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   GROSSBERG S, 1987, COGNITIVE SCI, V11, P23, DOI 10.1111/j.1551-6708.1987.tb00862.x
   Kang C, 2006, P 19 INT FLORIDA ART
   Lai JL, 2022, INFORM SCIENCES, V609, P465, DOI 10.1016/j.ins.2022.07.102
   Lee C, 2019, IEEE T COGN DEV SYST, V11, P384, DOI 10.1109/TCDS.2018.2833071
   Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508
   Lillicrap TP, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms13276
   Lin X, 2018, SPIKING NEURAL NETWO
   Liu D, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351591
   Merz C J, 1992, INT JOINT C NEURAL N
   O'Connor P., 2016, DEEP SPIKING NETWORK
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   Samadi A, 2017, NEURAL COMPUT, V29, P578, DOI 10.1162/NECO_a_00929
   Thiele J C, 2019, U.S. Patent Application, Patent No. [16/196,515, 16196515]
   Wang YX, 2021, IEEE T COGN DEV SYST, V13, P514, DOI 10.1109/TCDS.2020.2971655
   [王桢文 Wang Zhenwen], 2013, [计算机研究与发展, Journal of Computer Research and Development], V50, P2642
   Wu J N, 2013, CHINESE J BIOMEDICAL, V32, P588
   Zhu X., 2002, TECH REP
NR 24
TC 0
Z9 0
U1 1
U2 1
PY 2023
BP 976
EP 982
DI 10.1109/CCWC57344.2023.10099067
UT WOS:000995182600152
DA 2023-11-16
ER

PT C
AU Sun, CY
   Chen, QY
   Chen, K
   He, GQ
   Fu, YX
   Li, L
AF Sun, Congyi
   Chen, Qinyu
   Chen, Kai
   He, Guoqiang
   Fu, Yuxiang
   Li, Li
GP IEEE
TI Unsupervised Learning Based on Temporal Coding Using STDP in Spiking
   Neural Networks
SO 2022 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS (ISCAS 22)
SE IEEE International Symposium on Circuits and Systems
DT Proceedings Paper
CT IEEE International Symposium on Circuits and Systems (ISCAS)
CY MAY 28-JUN 01, 2022
CL Austin, TX
DE Temporal coding; Spiking neural network; Spiking-timing dependent
   plasticity
AB Spiking Neural Networks (SNNs) have been recognized as one of the next generation of Neural Networks (NNs), showing a great potential in a variety of applications. SpikingTiming Dependent Plasticity (STDP) underlies the brain's learning mechanisms, and trains SNNs with great energy efficiency. In this paper, we propose a low-cost spike-time based unsupervised learning method. It constructs a SNN with one fully-connected excitatory layer structure without inhibitory layer, and trains the SNN with STDP using a first-spike-based temporal coding scheme where input information is directly encoded into spike times. It only updates the synaptic weights connected to the neuron that first generates a spike in a forward propagation step, which reduces the frequency of the synaptic weight updates significantly. The forward propagation process can be stopped once a neuron fires whether in the training mode or the inference mode, by which many unnecessary computations are just avoided and the latency in the inference mode is reduced. The method was used to train on the classification task on MNIST dataset and achieved an accuracy of 90.4% with 800 excitatory neurons.
C1 [Sun, Congyi; Chen, Kai; He, Guoqiang; Fu, Yuxiang; Li, Li] Nanjing Univ, Sch Elect Sci & Engn, Nanjing, Peoples R China.
   [Chen, Qinyu] Univ Shanghai Sci & Technol, Inst Photon Chips, Shanghai, Peoples R China.
RP Fu, YX; Li, L (corresponding author), Nanjing Univ, Sch Elect Sci & Engn, Nanjing, Peoples R China.
EM yuxiangfu@nju.edu.cn; lili@nju.edu.cn
CR [Anonymous], 2016, ARXIV161101421
   Bear Mark F., 1994, Current Opinion in Neurobiology, V4, P389, DOI 10.1016/0959-4388(94)90101-5
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Ferr<prime>e P., 2018, FRONTIERS COMPUTATIO
   Gerstner W, 1996, NATURE, V383, P76, DOI 10.1038/383076a0
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee C, 2019, IEEE T COGN DEV SYST, V11, P384, DOI 10.1109/TCDS.2018.2833071
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   Mostafa H, 2018, IEEE T NEUR NET LEAR, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Rajendran B, 2013, IEEE T ELECTRON DEV, V60, P246, DOI 10.1109/TED.2012.2227969
   Sjöström PJ, 2001, NEURON, V32, P1149, DOI 10.1016/s0896-6273(01)00542-6
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Wang Q, 2017, NEUROCOMPUTING, V221, P146, DOI 10.1016/j.neucom.2016.09.071
   Zhang W, 2003, NAT REV NEUROSCI, V4, P885, DOI 10.1038/nrn1248
NR 16
TC 0
Z9 0
U1 1
U2 1
PY 2022
BP 2142
EP 2146
DI 10.1109/ISCAS48785.2022.9937812
UT WOS:000946638602072
DA 2023-11-16
ER

PT C
AU Kiselev, M
AF Kiselev, Mikhail
GP IEEE
TI Rate Coding vs. Temporal Coding - Is Optimum Between?
SO 2016 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUL 24-29, 2016
CL Vancouver, CANADA
DE spiking neural network; polychronization; STDP; rate coding; temporal
   coding; chaotic network; network self-organization
ID SPIKING NEURAL-NETWORKS; CODE
AB In this paper, we consider a novel approach to information representation in spiking neural networks. In a certain sense it is a combination of two well-known coding schemes - rate coding and temporal coding. Namely, it is based on asynchronous activity of ensembles of polychronous neuronal groups - groups of neurons firing in determined order with strictly fixed relative temporal delays. We demonstrate how a rate-coded input signal may be converted into this representation form by network structures which are formed as a result of network self-organization process based on STDP-style synaptic plasticity.
C1 [Kiselev, Mikhail] Chuvash State Univ, Dept Math & Informat Technol, Cheboxary, Russia.
RP Kiselev, M (corresponding author), Chuvash State Univ, Dept Math & Informat Technol, Cheboxary, Russia.
EM mkiselev@megaputer.ru
CR Brette R, 2015, FRONT SYST NEUROSCI, V9, DOI 10.3389/fnsys.2015.00151
   Butts DA, 2007, NATURE, V449, P92, DOI [10.1038/nature06105, 10.1038/natureO6105]
   Debanne D, 2013, NAT REV NEUROSCI, V14, P63, DOI 10.1038/nrn3361
   Gardner B, 2015, NEURAL COMPUT, V27, P2548, DOI 10.1162/NECO_a_00790
   Gerstner W., 2002, SPIKING NEURON MODEL
   Guise M, 2014, NEURAL COMPUT, V26, P2052, DOI 10.1162/NECO_a_00620
   Ikegaya Y, 2004, SCIENCE, V304, P559, DOI 10.1126/science.1093173
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Kiselev M. V., 2014, P C NEUR COMP THEOR, P264
   Kiselev M, 2013, LECT NOTES COMPUT SC, V7902, P510, DOI 10.1007/978-3-642-38679-4_51
   Kiselev MV, 2014, COMPUT INTEL NEUROSC, V2014, DOI 10.1155/2014/476580
   Mehta MR, 2002, NATURE, V417, P741, DOI 10.1038/nature00807
   Szatmáry B, 2010, PLOS COMPUT BIOL, V6, DOI 10.1371/journal.pcbi.1000879
NR 13
TC 15
Z9 17
U1 0
U2 6
PY 2016
BP 1355
EP 1359
UT WOS:000399925501072
DA 2023-11-16
ER

PT J
AU Lehmann, HM
   Hille, J
   Grassmann, C
   Issakov, V
AF Lehmann, Hendrik M.
   Hille, Julian
   Grassmann, Cyprian
   Issakov, Vadim
TI Direct Signal Encoding With Analog Resonate-and-Fire Neurons
SO IEEE ACCESS
DT Article
DE Neural networks; Resonant frequency; Encoding; Sensors; Codes;
   Biological neural networks; Oscillators; Neuromorphics; Sensor; spiking
   neural networks; neuromorphic hardware; signal encoding
ID SYSTEMS
AB Sensors are an essential element in a wide range of applications. As the number of sensors increases, so does the amount of data collected with them. This raises the challenge of efficiently processing this data. Spiking Neural Networks (SNNs) represents a promising approach to solve this problem through event-based, parallelized data processing. For SNNs to be genuinely efficient, some fundamental challenges arise, like converting analog signals to spike events. An emerging possibility is the use of Resonate-and-Fire (R&F) neurons, capable of reacting to specific frequency components of input signals. In this work, we present a possible analog implementation for a R&F neuron and show the practical encoding of analog signals into a spiking domain using actual measurements. The coding method allows analog sensor signals to be directly applied to SNNs for efficient data processing. In the future, this approach can potentially enable the direct integration of analog Spiking Neural Networks into sensors.
C1 [Lehmann, Hendrik M.; Issakov, Vadim] Tech Univ Carolo Wilhelmina Braunschweig, Inst CMOS Design, D-38106 Braunschweig, Germany.
   [Lehmann, Hendrik M.; Hille, Julian; Grassmann, Cyprian; Issakov, Vadim] Infineon Technol AG, D-85579 Neubiberg, Germany.
   [Hille, Julian] Tech Univ Munich, Dept Informat, D-80333 Munich, Germany.
RP Lehmann, HM (corresponding author), Tech Univ Carolo Wilhelmina Braunschweig, Inst CMOS Design, D-38106 Braunschweig, Germany.; Lehmann, HM (corresponding author), Infineon Technol AG, D-85579 Neubiberg, Germany.
EM hendrik.lehmann@infineon.com
CR Adrian ED, 1926, J PHYSIOL-LONDON, V61, P151, DOI 10.1113/jphysiol.1926.sp002281
   Auge D., 2020, RESONATE AND FIRE NE
   Auge D, 2021, LECT NOTES COMPUT SC, V12895, P245, DOI 10.1007/978-3-030-86383-8_20
   Auge D, 2021, NEURAL PROCESS LETT, V53, P4693, DOI 10.1007/s11063-021-10562-2
   Azam MA, 2018, J APPL PHYS, V124, DOI 10.1063/1.5042308
   Birkoben T, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-74219-1
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Debat G, 2021, FRONT COMPUT NEUROSC, V15, DOI 10.3389/fncom.2021.658764
   Diniz P. S. R., 2010, DIGIT SIGNAL PROCESS
   Frenkel C, 2019, IEEE T BIOMED CIRC S, V13, P145, DOI 10.1109/TBCAS.2018.2880425
   Furber S, 2016, J NEURAL ENG, V13, DOI 10.1088/1741-2560/13/5/051001
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   GRAY CM, 1989, NATURE, V338, P334, DOI 10.1038/338334a0
   Guglielmi E, 2020, IEEE J SOLID-ST CIRC, V55, P2094, DOI 10.1109/JSSC.2020.2973639
   Hille J., 2022, P INT C NEUR SYST JU, P1
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   Izhikevich EM, 2001, NEURAL NETWORKS, V14, P883, DOI 10.1016/S0893-6080(01)00078-8
   Kasabov N. K., 2019, TIME SPACE SPIKING N, DOI DOI 10.1007/978-3-662-57715-8
   Kornijcuk V, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00212
   Kugele A, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00439
   Lehmann HM, 2022, IEEE T CIRCUITS-I, V69, P4837, DOI 10.1109/TCSI.2022.3204433
   Lehmann HM, 2022, PRIME 2022: 17TH INTERNATIONAL CONFERENCE ON PHD RESEARCH IN MICROELECTRONICS AND ELECTRONICS, P293, DOI 10.1109/PRIME55000.2022.9816777
   Lehmann HM, 2021, IEEE INT CONF MICROW, P280, DOI 10.1109/COMCAS52219.2021.9629011
   Lichtsteiner P, 2008, IEEE J SOLID-ST CIRC, V43, P566, DOI 10.1109/JSSC.2007.914337
   Liu SC, 2010, CURR OPIN NEUROBIOL, V20, P288, DOI 10.1016/j.conb.2010.03.007
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Mayr C, 2019, Arxiv, DOI arXiv:1911.02385
   Nakada K., 2005, INT S NONL THEOR ITS, P82
   Neckar A, 2019, P IEEE, V107, P144, DOI 10.1109/JPROC.2018.2881432
   Orchard G, 2021, IEEE WRK SIG PRO SYS, P254, DOI 10.1109/SiPS52927.2021.00053
   Osborn L., 2017, MYOEL CONTR S U NEW, P188
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Rozenberg MJ, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-47348-5
   Rueckauer B, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351295
   Schaumann R., 2001, DESIGN ANALOG FILTER, V1
   Schemmel J, 2020, Arxiv, DOI arXiv:2003.11996
   Schenk C., 1993, HALBLEITERSCHALTUNGS
   Shayegannia M., 2007, PROC 4 IEEE GCC C, P1
   Srivastava KH, 2017, P NATL ACAD SCI USA, V114, P1171, DOI 10.1073/pnas.1611734114
   Stuijt J, 2021, FRONT NEUROSCI-SWITZ, V15, DOI 10.3389/fnins.2021.664208
   Su KL., 2012, ANALOG FILTERS
   THEUNISSEN F, 1995, J COMPUT NEUROSCI, V2, P149, DOI 10.1007/BF00961885
   Williams A. B., 2014, ANALOG FILTER CIRCUI, V1st Edition
NR 44
TC 0
Z9 0
U1 1
U2 1
PY 2023
VL 11
BP 50052
EP 50063
DI 10.1109/ACCESS.2023.3278098
UT WOS:001006282800001
DA 2023-11-16
ER

PT J
AU Gütig, R
AF Guetig, Robert
TI To spike, or when to spike?
SO CURRENT OPINION IN NEUROBIOLOGY
DT Review
ID NEURAL-NETWORKS; LEARNING ALGORITHM; GRADIENT DESCENT; REINFORCEMENT;
   PLASTICITY; DEPENDENCE; NEURONS; SPARSE; CORTEX; RULE
AB Recent experimental reports have suggested that cortical networks can operate in regimes were sensory information is encoded by relatively small populations of spikes and their precise relative timing. Combined with the discovery of spike timing dependent plasticity, these findings have sparked growing interest in the capabilities of neurons to encode and decode spike timing based neural representations. To address these questions, a novel family of methodologically diverse supervised learning algorithms for spiking neuron models has been developed. These models have demonstrated the high capacity of simple neural architectures to operate also beyond the regime of the well established independent rate codes and to utilize theoretical advantages of spike timing as an additional coding dimension.
C1 Max Planck Inst Expt Med, D-37075 Gottingen, Germany.
RP Gütig, R (corresponding author), Max Planck Inst Expt Med, Hermann Rein Str 3, D-37075 Gottingen, Germany.
EM guetig@em.mpg.de
CR Albers C., 2013, ADV NEURAL INFORM PR, V26, P1709
   [Anonymous], 2010, FRONT SYNAPTIC NEURO
   BARBER D, 2003, ADV NEURAL INFORM PR, V15, P149
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Bohte SM, ERROR BACKPROPAGATIO
   Booij O, 2005, INFORM PROCESS LETT, V95, P552, DOI 10.1016/j.ipl.2005.05.023
   Brea J, 2013, J NEUROSCI, V33, P9565, DOI 10.1523/JNEUROSCI.4098-12.2013
   Dayan P, 2001, THEORETICAL NEUROSCI, P34
   Fang H, 2010, NEURAL COMPUT
   Farries MA, 2007, J NEUROPHYSIOL, V98, P3648, DOI 10.1152/jn.00364.2007
   Feldman DE, 2012, NEURON, V75, P556, DOI 10.1016/j.neuron.2012.08.001
   Florian RV, 2007, NEURAL COMPUT, V19, P1468, DOI 10.1162/neco.2007.19.6.1468
   Florian RV, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0040233
   Gerstner W., 2002, SPIKING NEURON MODEL
   Ghosh-Dastidar S, 2009, NEURAL NETW
   Gollisch T, 2008, SCIENCE, V319, P1108, DOI 10.1126/science.1149639
   Gütig R, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0053063
   Gütig R, 2009, PLOS BIOL, V7, DOI 10.1371/journal.pbio.1000141
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Gutig R, 2012, COS SALT LAK CIT US, P58
   Houweling AR, 2008, NATURE, V451, P65, DOI 10.1038/nature06447
   Huber D, 2008, NATURE, V451, P61, DOI 10.1038/nature06445
   Izhikevich EM, 2007, CEREB CORTEX, V17, P2443, DOI 10.1093/cercor/bhl152
   Legenstein R, 2005, NEURAL COMPUT, V17, P2337, DOI 10.1162/0899766054796888
   Legenstein R, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.1000180
   Malenka RC, 1999, SCIENCE, V285, P1870, DOI 10.1126/science.285.5435.1870
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   Memmesheimer RM, 2012, COS SALT LAK CIT US, P113
   Pfister JP, 2006, NEURAL COMPUT, V18, P1318, DOI 10.1162/neco.2006.18.6.1318
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Rosenblatt F, 1962, PRINCIPLES NEURODYNA, P88
   Rubin R, 2010, PHYS REV LETT, V105, DOI 10.1103/PhysRevLett.105.218102
   Seung HS, 2003, NEURON, V40, P1063, DOI 10.1016/S0896-6273(03)00761-X
   Sjöström PJ, 2006, NEURON, V51, P227, DOI 10.1016/j.neuron.2006.06.017
   Sporea I, 2013, NEURAL COMPUT, V25, P473, DOI 10.1162/NECO_a_00396
   Stüttgen MC, 2008, NAT NEUROSCI, V11, P1091, DOI 10.1038/nn.2162
   Urbanczik R, 2009, NEURAL COMPUT, V21, P340, DOI 10.1162/neco.2008.09-07-605
   VanRullen R, 2005, TRENDS NEUROSCI, V28, P1, DOI 10.1016/j.tins.2004.10.010
   Victor JD, 1996, J NEUROPHYSIOL, V76, P1310, DOI 10.1152/jn.1996.76.2.1310
   Wolfe J, 2010, CURR OPIN NEUROBIOL, V20, P306, DOI 10.1016/j.conb.2010.03.006
   Xie XH, 2004, PHYS REV E, V69, DOI 10.1103/PhysRevE.69.041909
   Xu Y, 2013, NEURAL COMPUT, V25, P1472, DOI 10.1162/NECO_a_00450
   Xu Y, 2013, NEURAL NETWORKS, V43, P99, DOI 10.1016/j.neunet.2013.02.003
NR 43
TC 63
Z9 64
U1 0
U2 28
PD APR
PY 2014
VL 25
BP 134
EP 139
DI 10.1016/j.conb.2014.01.004
UT WOS:000335628800023
DA 2023-11-16
ER

PT C
AU Wang, YX
   Howard, N
AF Wang, Yingxu
   Howard, Newton
GP IEEE
TI The Spike Frequency Modulation (SFM) Theory for Neuroinformatics and
   Cognitive Cybernetics
SO 2019 IEEE INTERNATIONAL CONFERENCE ON SYSTEMS, MAN AND CYBERNETICS (SMC)
SE IEEE International Conference on Systems Man and Cybernetics Conference
   Proceedings
DT Proceedings Paper
CT IEEE International Conference on Systems, Man and Cybernetics (SMC)
CY OCT 06-09, 2019
CL Bari, ITALY
DE Neuroinformatics; cognitive cybernetics; neural signaling systems; spike
   frequency modulation; cognitive informatics; mathematical models; neural
   networks; cognitive systems; brain-machine interfaces
ID MODELS
AB One of the fundamental problems in neurology and neuroinformatics is whether the neural signals in human nerves systems are digital or analogue. This paper presents a novel neural signaling theory of Spike Frequency Modulation (SFM), which explains the nature of neural signals and their transformation in the nervous systems of the brain. Mathematical models of the unified signals of neural spikes across the sensory, associate and motor neurons are formally described. The time-divided mechanism for neural signal transmission and the space-divided mechanism for neural semantic representation in human nervous systems are rigorously explained. A set of experimental simulations demonstrates the SFM theory and the cognitive mechanisms of the neural pathways and networks. The SFM theory reveals the neurological and cognitive foundations of both natural and artificial neural networks for brain-inspired systems and engineering applications.
C1 [Wang, Yingxu] Univ Calgary, Schulich Sch Engn, Dept Elect & Comp Engn, Int Inst Cognit Informat & Cognit Comp, Calgary, AB, Canada.
   [Wang, Yingxu] Univ Calgary, Hotchkiss Brain Inst, Calgary, AB, Canada.
   [Howard, Newton] Univ Oxford, Computat Neurosci Lab, Oxford, England.
RP Wang, YX (corresponding author), Univ Calgary, Schulich Sch Engn, Dept Elect & Comp Engn, Int Inst Cognit Informat & Cognit Comp, Calgary, AB, Canada.; Wang, YX (corresponding author), Univ Calgary, Hotchkiss Brain Inst, Calgary, AB, Canada.
EM yingxu@ucalgary.ca; newton.howard@nds.ox.ac
CR Ascoli GA, 2003, NEUROINFORMATICS, V1, P1, DOI 10.1385/NI:1:1:001
   Beltrame F, 1999, IEEE Trans Inf Technol Biomed, V3, P239, DOI 10.1109/4233.788587
   Carter R., 2009, THE HUMAN BRAIN
   Chee-Ruiter CWJ, 1998, COMPUTATIONAL NEUROSCIENCE: TRENDS IN RESEARCH, P591
   Dayan P, 2005, THEORETICAL NEUROSCI
   Jirsa VK, 2004, NEUROINFORMATICS, V2, P183, DOI 10.1385/NI:2:2:183
   Lepora NF, 2013, BIOINSPIR BIOMIM, V8, DOI 10.1088/1748-3182/8/1/013001
   Marieb EN., 2017, HUMAN ANATOMY
   Sternberg R. J., 1998, SEARCH HUMAN MIND
   Wang Y., 2019, IEEE SYSTEM MAN CYBE, V5
   Wang Y., 2003, BRAIN MIND, V4, P151, DOI DOI 10.1023/A:1025401527570
   Wang YX, 2012, J ADV MATH APPL, V1, P206, DOI 10.1166/jama.2012.1015
   Wang YX, 2011, INT J COGN INFORM NA, V5, P75, DOI 10.4018/jcini.2011010105
   Wang YX, 2006, IEEE T SYST MAN CY C, V36, P203, DOI 10.1109/TSMCC.2006.871151
   Wiener N., 1948, CYBERNETICS CONTROL
   Wilson Robert Andrew, 2001, MIT ENCY COGNITIVE S
NR 16
TC 1
Z9 1
U1 0
U2 0
PY 2019
BP 2220
EP 2224
UT WOS:000521353902040
DA 2023-11-16
ER

PT J
AU Amirshahi, A
   Hashemi, M
AF Amirshahi, Alireza
   Hashemi, Matin
TI ECG Classification Algorithm Based on STDP and R-STDP Neural Networks
   for Real-Time Monitoring on Ultra Low-Power Personal Wearable Devices
SO IEEE TRANSACTIONS ON BIOMEDICAL CIRCUITS AND SYSTEMS
DT Article
DE Electrocardiography; Neurons; Biological neural networks; Feature
   extraction; Real-time systems; Energy consumption; Monitoring; Cardiac
   monitoring; Electrocardiogram (ECG) classification; embedded real-time
   systems; low power consumption; machine learning; spiking neural network
   (SNN); wearable devices
ID FEATURES; SYSTEM
AB This paper presents a novel ECG classification algorithm for inclusion as part of real-time cardiac monitoring systems in ultra low-power wearable devices. The proposed solution is based on spiking neural networks which are the third generation of neural networks. In specific, we employ spike-timing dependent plasticity (STDP), and reward-modulated STDP (R-STDP), in which the model weights are trained according to the timings of spike signals, and reward or punishment signals. Experiments show that the proposed solution is suitable for real-time operation, achieves comparable accuracy with respect to previous methods, and more importantly, its energy consumption in real-time classification of ECG signals is significantly smaller. In specific, energy consumption is 1.78$\boldsymbol {\mu }$J per beat, which is 2 to 9 orders of magnitude smaller than previous neural network based ECG classification methods.
C1 [Amirshahi, Alireza; Hashemi, Matin] Sharif Univ Technol, Learning & Intelligent Syst Lab, Dept Elect Engn, Tehran 11356, Iran.
RP Hashemi, M (corresponding author), Sharif Univ Technol, Learning & Intelligent Syst Lab, Dept Elect Engn, Tehran 11356, Iran.
EM alireza.amirshahi@ee.sharif.edu; matin@sharif.edu
CR [Anonymous], THESIS
   [Anonymous], 1997, MIT BIH ARRHYTHMIA D
   [Anonymous], INHIBITORY PLASTICIT
   [Anonymous], BRIAN 2 DOCUMENTATIO
   [Anonymous], THESIS
   [Anonymous], 2015, INT J COMPUT VISION, DOI DOI 10.1007/s11263-014-0788-3
   [Anonymous], 1987, AAMI REC PRACT TEST
   [Anonymous], P ACM SIGPLAN NOT
   [Anonymous], 2019, IEEE J BIOMED HEALTH
   [Anonymous], THEORETICAL NEUROSCI
   [Anonymous], IEEE T BIOMED CIRCUI
   [Anonymous], IEEE T BIOMED CIRCUI
   Bayasi N, 2015, IEEE INT SYMP CIRC S, P746, DOI 10.1109/ISCAS.2015.7168741
   Biagetti G, 2016, IEEE T CONSUM ELECTR, V62, P258, DOI 10.1109/TCE.2016.7613192
   Bing ZS, 2018, FRONT NEUROROBOTICS, V12, DOI 10.3389/fnbot.2018.00035
   Bote JM, 2018, IEEE J BIOMED HEALTH, V22, P429, DOI 10.1109/JBHI.2017.2671443
   Chou CY, 2018, IEEE T BIOMED CIRC S, V12, P801, DOI 10.1109/TBCAS.2018.2828031
   Crippa P, 2015, INT J SIMULAT SYST S, V16, P1
   Da He D, 2015, IEEE T BIOMED CIRC S, V9, P370, DOI 10.1109/TBCAS.2014.2346761
   Das A, 2018, NEURAL NETWORKS, V99, P134, DOI 10.1016/j.neunet.2017.12.015
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Foroozannejad MH, 2014, IEEE T COMPUT AID D, V33, P752, DOI 10.1109/TCAD.2014.2299958
   Foroozannejad MH, 2012, ACM T DES AUTOMAT EL, V17, DOI 10.1145/2209291.2209300
   Frémaux N, 2016, FRONT NEURAL CIRCUIT, V9, DOI 10.3389/fncir.2015.00085
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   Hashemi M, 2013, ACM T EMBED COMPUT S, V13, DOI 10.1145/2539036.2539042
   Hoekema R, 2001, IEEE T BIO-MED ENG, V48, P551, DOI 10.1109/10.918594
   Hu YH, 1997, IEEE T BIO-MED ENG, V44, P891, DOI 10.1109/10.623058
   Ince T, 2009, IEEE T BIO-MED ENG, V56, P1415, DOI 10.1109/TBME.2009.2013934
   Indiveri G., 2015, P EL DEV M IEDM 2015, p4.2.1, DOI [DOI 10.1109/IEDM.2015.7409623, 10.1109/iedm.2015.7409623]
   Jain SK, 2017, IEEE T BIOMED CIRC S, V11, P314, DOI 10.1109/TBCAS.2016.2592382
   Kachuee M., 2018, P IEEE INT C HEALTHC, P443
   Kasabov N, 2015, INFORM SCIENCES, V294, P565, DOI 10.1016/j.ins.2014.06.028
   Khayat M, 2017, STEEL COMPOS STRUCT, V23, P1, DOI 10.12989/scs.2017.23.1.001
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Kiranyaz S, 2016, IEEE T BIO-MED ENG, V63, P664, DOI 10.1109/TBME.2015.2468589
   Lee SY, 2015, IEEE J BIOMED HEALTH, V19, P236, DOI 10.1109/JBHI.2014.2310354
   Li PF, 2017, IEEE T BIO-MED ENG, V64, P78, DOI 10.1109/TBME.2016.2539421
   Ma Q., 2013, P IEEE TENCON 2013 X, P1
   Masquelier T, 2007, PLOS COMPUT BIOL, V3, P247, DOI 10.1371/journal.pcbi.0030031
   Meister M, 1999, NEURON, V22, P435, DOI 10.1016/S0896-6273(00)80700-X
   Moody GA, 2001, IEEE ENG MED BIOL, V20, P45, DOI 10.1109/51.932724
   Moradi S, 2014, IEEE T BIOMED CIRC S, V8, P98, DOI 10.1109/TBCAS.2013.2255873
   Mozafari M, 2018, IEEE T NEUR NET LEAR, V29, P6178, DOI 10.1109/TNNLS.2018.2826721
   Qiao N, 2017, IEEE SOI3DSUB MICRO
   Qiao N, 2016, BIOMED CIRC SYST C, P552, DOI 10.1109/BioCAS.2016.7833854
   Qiao N, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00141
   Rajpurkar P., 2017, ARXIV
   Shyu LY, 2004, IEEE T BIO-MED ENG, V51, P1269, DOI 10.1109/TBME.2004.824131
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Sopic D, 2018, IEEE T BIOMED CIRC S, V12, P982, DOI 10.1109/TBCAS.2018.2848477
   Srinivasa N, 2013, FRONT COMPUT NEUROSC, V7, DOI 10.3389/fncom.2013.00010
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Tang XC, 2018, IEEE T BIOMED CIRC S, V12, P751, DOI 10.1109/TBCAS.2018.2823275
   Teijeiro T, 2018, IEEE J BIOMED HEALTH, V22, P409, DOI 10.1109/JBHI.2016.2631247
   Wang XL, 2014, IEEE J BIOMED HEALTH, V18, P739, DOI 10.1109/JBHI.2013.2286157
   Zhang WB, 2018, 2018 INTERNATIONAL CONFERENCE ON BIG DATA AND ARTIFICIAL INTELLIGENCE (BDAI 2018), P47, DOI 10.1109/BDAI.2018.8546681
NR 57
TC 53
Z9 56
U1 4
U2 32
PD DEC
PY 2019
VL 13
IS 6
BP 1483
EP 1493
DI 10.1109/TBCAS.2019.2948920
UT WOS:000507321400034
DA 2023-11-16
ER

PT J
AU Kampakis, S
AF Kampakis, Stylianos
TI Investigating the computational power of spiking neurons with
   non-standard behaviors
SO NEURAL NETWORKS
DT Article
DE Computational power; Spiking; Non-standard behavior; Bursting;
   Oscillators; Rebound; Meta-learning; Rational analysis; Izhikevich
   neuron
ID NEURAL-NETWORK; REBOUND SPIKING; COUPLED NEURONS; INFORMATION; BURSTS;
   MODEL; OSCILLATION; UNIT
AB Spiking neural networks have been called the third generation of neural networks. Their main difference with respect to the previous two generations is the use of realistic neuron models. Their computational power has been well studied with respect to threshold gates and sigmoidal neurons. However, biologically realistic models of spiking neurons can produce behaviors that can be computationally relevant, but their power has not been assessed in the same way. This paper studies the computational power of neurons with different behaviors based on the previous analyses conducted by Maass and Schmitt. The studied behaviors are rebound spiking, resonance and bursting. The results of the analysis are presented. A theoretical motivation for this study is presented and a discussion is done on the possible implications of the findings for using networks of spiking neurons for performing computations. (c) 2013 Elsevier Ltd. All rights reserved.
C1 [Kampakis, Stylianos] Univ Edinburgh, Dept Informat, Edinburgh EH8 9YL, Midlothian, Scotland.
RP Kampakis, S (corresponding author), Ano Tzoumagias 1, Thessaloniki, Greece.
EM stylianos.kampakis@gmail.com
CR Abbott LF, 1999, BRAIN RES BULL, V50, P303, DOI 10.1016/S0361-9230(99)00161-6
   ANDERSON JR, 1991, BEHAV BRAIN SCI, V14, P471, DOI 10.1017/S0140525X00070801
   [Anonymous], 1972, PERCEPTRONS INTRO CO
   [Anonymous], 1997, IEEE T EVOLUTIONARY
   [Anonymous], 1982, VISION
   Balu R, 2007, J NEUROPHYSIOL, V97, P1959, DOI 10.1152/jn.01115.2006
   Bharat Rao R., 1995, Machine Learning. Proceedings of the Twelfth International Conference on Machine Learning, P471
   Bodyanskiy Y, 2008, PRO BIENN BALT EL C, P213, DOI 10.1109/BEC.2008.4657517
   Bohte S. M., 2001, IEEE T NEURAL NETWOR, V13
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Bouganis A., 2010, P 2010 INT JOINT C N, P1, DOI DOI 10.1109/IJCNN.2010.5596525
   Brazdil P., 2008, COGNITIVE TECHNOLOGI
   Buzsáki G, 2004, SCIENCE, V304, P1926, DOI 10.1126/science.1099745
   Cao JL, 2010, J NEUROSCI, V30, P16453, DOI 10.1523/JNEUROSCI.3177-10.2010
   Cybenko G., 1989, Mathematics of Control, Signals, and Systems, V2, P303, DOI 10.1007/BF02551274
   Dehaene S, 2004, CURR OPIN NEUROBIOL, V14, P218, DOI 10.1016/j.conb.2004.03.008
   Dethier J., 2011, ADV NEURAL INFORM PR
   Enomoto A, 2006, J NEUROSCI, V26, P3412, DOI 10.1523/JNEUROSCI.5274-05.2006
   Eyherabide HG, 2008, FRONT COMPUT NEUROSC, V2, DOI 10.3389/neuro.10.003.2008
   Felix RA, 2011, J NEUROSCI, V31, P12566, DOI 10.1523/JNEUROSCI.2450-11.2011
   Friston KJ, 2010, NAT REV NEUROSCI, V11, P127, DOI 10.1038/nrn2787
   Gerstner W., 2002, SPIKING NEURON MODEL
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   Goaillard JM, 2010, J NEUROSCI, V30, P4687, DOI 10.1523/JNEUROSCI.2998-09.2010
   Hagras H, 2004, IEEE INT CONF ROBOT, P4620, DOI 10.1109/ROBOT.2004.1302446
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   Hutcheon B, 2000, TRENDS NEUROSCI, V23, P216, DOI 10.1016/S0166-2236(00)01547-2
   Iannella N, 2001, NEURAL NETWORKS, V14, P933, DOI 10.1016/S0893-6080(01)00080-6
   Izhikevich EM, 2001, NEURAL NETWORKS, V14, P883, DOI 10.1016/S0893-6080(01)00078-8
   Izhikevich EM, 2003, TRENDS NEUROSCI, V26, P161, DOI 10.1016/S0166-2236(03)00034-1
   Izhikevich EM., 2007, DYNAMICAL SYSTEMS NE, DOI [DOI 10.1017/S0143385704000173, 10.7551/mitpress/2526.001.0001]
   Kampakis S., 2011, J SOFT COMPUTING
   Lavin A, 1996, J NEUROPHYSIOL, V75, P1432, DOI 10.1152/jn.1996.75.4.1432
   Li CYT, 2009, SCIENCE, V324, P643, DOI 10.1126/science.1169957
   Lisman JE, 1997, TRENDS NEUROSCI, V20, P38, DOI 10.1016/S0166-2236(96)10070-9
   LLINAS R, 1986, J PHYSIOL-LONDON, V376, P163, DOI 10.1113/jphysiol.1986.sp016147
   Ma WJ, 2006, NAT NEUROSCI, V9, P1432, DOI 10.1038/nn1790
   Maass W, 1996, NEURAL COMPUT, V8, P1, DOI 10.1162/neco.1996.8.1.1
   Maass W, 1997, NEURAL COMPUT, V9, P279, DOI 10.1162/neco.1997.9.2.279
   Maass W., 1995, Advances in Neural Information Processing Systems 7, P183
   Maass W, 1999, INFORM COMPUT, V153, P26, DOI 10.1006/inco.1999.2806
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maass W, 1997, ADV NEUR IN, V9, P211
   Maass W, 1996, ADV NEUR IN, V8, P211
   Maass W, 2001, PULSED NEURAL NETWOR
   Macready, 1995, SFITR9502010
   Marsat G, 2010, J COMP PHYSIOL A, V196, P315, DOI 10.1007/s00359-010-0514-8
   Masoller C, 2009, PHILOS T R SOC A, V367, P3255, DOI 10.1098/rsta.2009.0096
   MCCORMICK DA, 1992, J NEUROPHYSIOL, V68, P1384, DOI 10.1152/jn.1992.68.4.1384
   McCulloch Warren S., 1943, BULL MATH BIOPHYS, V5, P115, DOI 10.1007/BF02478259
   Meftah B, 2010, NEURAL PROCESS LETT, V32, P131, DOI 10.1007/s11063-010-9149-6
   Nieder A, 2009, ANNU REV NEUROSCI, V32, P185, DOI 10.1146/annurev.neuro.051508.135550
   Pedroarena C, 1997, P NATL ACAD SCI USA, V94, P724, DOI 10.1073/pnas.94.2.724
   Potjans W, 2009, NEURAL COMPUT, V21, P301, DOI 10.1162/neco.2008.08-07-593
   Rolls E, 2010, NOISY BRAIN
   Salinas E, 2006, NAT NEUROSCI, V9, P1349, DOI 10.1038/nn1106-1349
   Sancristóbal B, 2008, LECT NOTES COMPUT SC, V5164, P695, DOI 10.1007/978-3-540-87559-8_72
   Schmitt M, 1998, ANN MATH ARTIFICIAL, V24, P1
   Su HL, 2001, J NEUROSCI, V21, P4173, DOI 10.1523/JNEUROSCI.21-12-04173.2001
   Supèr H, 2011, J COGNITIVE NEUROSCI, V23, P491, DOI 10.1162/jocn.2010.21512
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   Wolpert D. H., 1996, NEURAL COMPUTATION, V8
NR 63
TC 3
Z9 3
U1 0
U2 19
PD JUL
PY 2013
VL 43
BP 41
EP 54
DI 10.1016/j.neunet.2013.01.011
UT WOS:000319237700005
DA 2023-11-16
ER

PT J
AU Kim, D
   Chakraborty, B
   She, XY
   Lee, E
   Kang, B
   Mukhopadhyay, S
AF Kim, Daehyun
   Chakraborty, Biswadeep
   She, Xueyuan
   Lee, Edward
   Kang, Beomseok
   Mukhopadhyay, Saibal
TI MONETA: A Processing-In-Memory-Based Hardware Platform for the Hybrid
   Convolutional Spiking Neural Network With Online Learning
SO FRONTIERS IN NEUROSCIENCE
DT Article
DE spiking neural network (SNN); processing-in-memory (PIM); convolutional
   spiking neural network; on-line learning; on-chip learning;
   spike-time-dependent plasticity (STDP); AI accelerator; hybrid network
ID NEURONS
AB We present a processing-in-memory (PIM)-based hardware platform, referred to as MONETA, for on-chip acceleration of inference and learning in hybrid convolutional spiking neural network. MONETAuses 8T static random-access memory (SRAM)-based PIM cores for vector matrix multiplication (VMM) augmented with spike-time-dependent-plasticity (STDP) based weight update. The spiking neural network (SNN)-focused data flow is presented to minimize data movement in MONETAwhile ensuring learning accuracy. MONETAsupports on-line and on-chip training on PIM architecture. The STDP-trained convolutional neural network within SNN (ConvSNN) with the proposed data flow, 4-bit input precision, and 8-bit weight precision shows only 1.63% lower accuracy in CIFAR-10 compared to the STDP accuracy implemented by the software. Further, the proposed architecture is used to accelerate a hybrid SNN architecture that couples off-chip supervised (back propagation through time) and on-chip unsupervised (STDP) training. We also evaluate the hybrid network architecture with the proposed data flow. The accuracy of this hybrid network is 10.84% higher than STDP trained accuracy result and 1.4% higher compared to the backpropagated training-based ConvSNN result with the CIFAR-10 dataset. Physical design of MONETAin 65 nm complementary metal-oxide-semiconductor (CMOS) shows 18.69 tera operation per second (TOPS)/W, 7.25 TOPS/W and 10.41 TOPS/W power efficiencies for the inference mode, learning mode, and hybrid learning mode, respectively.
C1 [Kim, Daehyun; Chakraborty, Biswadeep; She, Xueyuan; Lee, Edward; Kang, Beomseok; Mukhopadhyay, Saibal] Georgia Inst Technol, Dept Elect & Comp Engn, Atlanta, GA 30332 USA.
RP Kim, D (corresponding author), Georgia Inst Technol, Dept Elect & Comp Engn, Atlanta, GA 30332 USA.
EM daehyun.kim@gatech.edu
CR Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Buhler FN, 2017, SYMP VLSI CIRCUITS, pC30, DOI 10.23919/VLSIC.2017.8008536
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Chakraborty B, 2021, IEEE T IMAGE PROCESS, V30, P9014, DOI 10.1109/TIP.2021.3122092
   Chen GK, 2019, IEEE J SOLID-ST CIRC, V54, P992, DOI 10.1109/JSSC.2018.2884901
   Chen YH, 2016, ISSCC DIG TECH PAP I, V59, P262, DOI 10.1109/ISSCC.2016.7418007
   Chi P, 2016, CONF PROC INT SYMP C, P27, DOI 10.1109/ISCA.2016.13
   Chuang PY, 2020, DES AUT CON, DOI 10.1109/dac18072.2020.9218714
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng L, 2020, IEEE J SOLID-ST CIRC, V55, P2228, DOI 10.1109/JSSC.2020.2970709
   Diehl P.U., 2015, 2015 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2015.7280696
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Gerstner W., 2002, SPIKING NEURON MODEL
   He K., 2016, P IEEE C COMPUTER VI
   Imani M, 2019, PROCEEDINGS OF THE 2019 46TH INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA '19), P802, DOI 10.1145/3307650.3322237
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Kim D, 2020, IEEE SOLID-ST CIRC L, V3, P278, DOI 10.1109/LSSC.2020.3013448
   Kim S., 2020, AAAI CONF ARTIF INTE, V34, P11270
   Kim S, 2011, 2011 11TH INTERNATIONAL CONFERENCE ON CONTROL, AUTOMATION AND SYSTEMS (ICCAS), P1
   Ledinauskas E., 2020, ARXIV PREPRINT ARXIV, DOI [10.48550/ARXIV.2006.04436, DOI 10.48550/ARXIV.2006.04436]
   Lee C, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00119
   Lee C, 2019, IEEE T COGN DEV SYST, V11, P384, DOI 10.1109/TCDS.2018.2833071
   Lee C, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00435
   Long Y., 2020, P 2017 IEEE FAR E FO, P1, DOI DOI 10.1109/I2MTC43012.2020.9129204
   Long Y, 2019, IEEE J EXPLOR SOLID-, V5, P113, DOI 10.1109/JXCDC.2019.2923745
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Miquel J.R., 2021, ARXIV PREPRINT ARXIV, DOI [10.48550/ARXIV.2106.05624, DOI 10.48550/ARXIV.2106.05624]
   Narayanan S, 2020, ANN I S COM, P349, DOI 10.1109/ISCA45697.2020.00038
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Panda P, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00653
   Park J., 2019, 2019 IEEE INT SOLIDS, P140
   Peng XC, 2021, IEEE T COMPUT AID D, V40, P2306, DOI 10.1109/TCAD.2020.3043731
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Shafiee A, 2016, CONF PROC INT SYMP C, P14, DOI 10.1109/ISCA.2016.12
   She X., 2020, 2020 INT JOINT C NEU, P1
   She XY, 2021, PATTERN RECOGN, V118, DOI 10.1016/j.patcog.2021.108002
   She XY, 2019, DES AUT TEST EUROPE, P450, DOI [10.23919/DATE.2019.8714846, 10.23919/date.2019.8714846]
   Simonyan K., 2014, ARXIV, DOI [DOI 10.48550/ARXIV.1409.1556, 10.48550/arXiv.1409.1556]
   Singh S, 2020, ANN I S COM, P363, DOI 10.1109/ISCA45697.2020.00039
   Srinivasan G, 2018, ACM J EMERG TECH COM, V14, DOI 10.1145/3266229
   Sze Vivienne, 2020, SYNTHESIS LECT COMPU, V15, P1, DOI DOI 10.2200/S01004ED1V01Y202004CAC050
   Tavanaei A., 2016, PREPRINT
   Wang GR, 2021, FRONT NEUROSCI-SWITZ, V15, DOI 10.3389/fnins.2021.615279
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
NR 47
TC 2
Z9 2
U1 3
U2 16
PD APR 11
PY 2022
VL 16
AR 775457
DI 10.3389/fnins.2022.775457
UT WOS:000792454800001
DA 2023-11-16
ER

PT J
AU Zhang, GX
   Zhang, XH
   Rong, HN
   Paul, P
   Zhu, M
   Neri, F
   Ong, YS
AF Zhang, Gexiang
   Zhang, Xihai
   Rong, Haina
   Paul, Prithwineel
   Zhu, Ming
   Neri, Ferrante
   Ong, Yew-Soon
TI A Layered Spiking Neural System for Classification Problems
SO INTERNATIONAL JOURNAL OF NEURAL SYSTEMS
DT Article
DE Spiking neural networks; spiking neural P systems; layered weighted
   fuzzy spiking neural P systems; supervised learning
ID P SYSTEMS; NETWORK; BACKPROPAGATION; ALGORITHM
AB Biological brains have a natural capacity for resolving certain classification tasks. Studies on biologically plausible spiking neurons, architectures and mechanisms of artificial neural systems that closely match biological observations while giving high classification performance are gaining momentum. Spiking neural P systems (SN P systems) are a class of membrane computing models and third-generation neural networks that are based on the behavior of biological neural cells and have been used in various engineering applications. Furthermore, SN P systems are characterized by a highly flexible structure that enables the design of a machine learning algorithm by mimicking the structure and behavior of biological cells without the over-simplification present in neural networks. Based on this aspect, this paper proposes a novel type of SN P system, namely, layered SN P system (LSN P system), to solve classification problems by supervised learning. The proposed LSN P system consists of a multi-layer network containing multiple weighted fuzzy SN P systems with adaptive weight adjustment rules. The proposed system employs specific ascending dimension techniques and a selection method of output neurons for classification problems. The experimental results obtained using benchmark datasets from the UCI machine learning repository and MNIST dataset demonstrated the feasibility and effectiveness of the proposed LSN P system. More importantly, the proposed LSN P system presents the first SN P system that demonstrates sufficient performance for use in addressing real-world classification problems.
C1 [Zhang, Gexiang] Chengdu Univ Informat Technol, Sch Control Engn, Chengdu 610225, Peoples R China.
   [Zhang, Xihai] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Rong, Haina] Southwest Jiaotong Univ, Sch Elect Engn, Chengdu 610031, Peoples R China.
   [Paul, Prithwineel; Zhu, Ming] Chengdu Univ Informat Technol, Sch Control Engn, Chengdu 610225, Peoples R China.
   [Neri, Ferrante] Univ Surrey, NICE Grp, Dept Comp Sci, Surrey, England.
   [Ong, Yew-Soon] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore, Singapore.
RP Neri, F (corresponding author), Univ Surrey, NICE Grp, Dept Comp Sci, Surrey, England.
EM zhgxdylan@126.com; xihaizhang@tju.edu.cn; ronghaina@126.com;
   prithwineelpaul@gmail.com; zhuming@cuit.edu.cn; f.neri@surrey.ac.uk;
   ASYSOng@ntu.edu.sg
CR Adeli H, 2010, AUTOMATED EEG-BASED DIAGNOSIS OF NEUROLOGICAL DISORDERS: INVENTING THE FUTURE OF NEUROLOGY, P1
   Ahmadlou M, 2010, INTEGR COMPUT-AID E, V17, P197, DOI 10.3233/ICA-2010-0345
   Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   Alam KMR, 2020, NEURAL COMPUT APPL, V32, P8675, DOI 10.1007/s00521-019-04359-7
   [Anonymous], REGULARIZATION NEURA
   [Anonymous], 2017, REAL LIFE APPL MEMBR
   Bohte SM, 2005, INFORM PROCESS LETT, V95, P519, DOI 10.1016/j.ipl.2005.05.018
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Caraffini F, 2019, INFORM SCIENCES, V477, P186, DOI 10.1016/j.ins.2018.10.033
   Chen ZH, 2018, NEURAL COMPUT APPL, V29, P695, DOI 10.1007/s00521-016-2489-z
   Ciresan D, 2012, NEURAL NETWORKS, V32, P333, DOI 10.1016/j.neunet.2012.02.023
   Comsa IM, 2022, IEEE T NEUR NET LEAR, V33, P5939, DOI 10.1109/TNNLS.2021.3071976
   Cui J, 2020, IEEE T VEH TECHNOL, V69, P10494, DOI 10.1109/TVT.2020.3009165
   de la Cruz RTA, 2019, J MEMBRANE COMPUT, V1, P161, DOI 10.1007/s41965-019-00021-2
   Dekking F.M., 2005, MODERN INTRO PROBABI
   Diehl P.U., 2015, 2015 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2015.7280696
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Dora S, 2016, NEUROCOMPUTING, V171, P1216, DOI 10.1016/j.neucom.2015.07.086
   Dora S, 2019, IEEE T CYBERNETICS, V49, P989, DOI 10.1109/TCYB.2018.2791282
   Dora S, 2017, INFORM SCIENCES, V414, P19, DOI 10.1016/j.ins.2017.05.050
   Ghosh-Dastidar S, 2007, INTEGR COMPUT-AID E, V14, P187
   Ghosh-Dastidar S, 2009, ADV INTEL SOFT COMPU, V61, P167
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Gollisch T, 2008, SCIENCE, V319, P1108, DOI 10.1126/science.1149639
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554
   Ionescu M, 2006, FUND INFORM, V71, P279
   Jeyasothy A, 2019, Arxiv, DOI arXiv:1904.11367
   Jeyasothy A, 2021, EXPERT SYST APPL, V178, DOI 10.1016/j.eswa.2021.114985
   Khanna R, 2019, IEEE SENS J, V19, P4571, DOI 10.1109/JSEN.2019.2901271
   Kostal L, 2007, EUR J NEUROSCI, V26, P2693, DOI 10.1111/j.1460-9568.2007.05880.x
   Lazo PPL, 2021, J MEMBRANE COMPUT, V3, P149, DOI 10.1007/s41965-021-00072-4
   LeCun Y., MNIST DATABASE HANDW
   Lichman M., UC IRVINE MACHINE LE
   Lotter W, 2020, NAT MACH INTELL, V2, P210, DOI 10.1038/s42256-020-0170-9
   Lv ZQ, 2021, INT J NEURAL SYST, V31, DOI 10.1142/S0129065720500495
   Machingal P, 2020, IEEE IJCNN, DOI 10.1109/ijcnn48605.2020.9207620
   Martín-Vide C, 2003, THEOR COMPUT SCI, V296, P295, DOI 10.1016/S0304-3975(02)00659-X
   Muhammad K, 2021, IEEE T NEUR NET LEAR, V32, P507, DOI 10.1109/TNNLS.2020.2995800
   Nazari S, 2019, NEUROCOMPUTING, V330, P196, DOI 10.1016/j.neucom.2018.10.066
   Pan LQ, 2017, INT J NEURAL SYST, V27, DOI 10.1142/S0129065717500423
   Pan LQ, 2012, NEURAL COMPUT, V24, P805, DOI 10.1162/NECO_a_00238
   Päun G, 2000, J COMPUT SYST SCI, V61, P108, DOI 10.1006/jcss.1999.1693
   Peng H, 2019, IEEE T NEUR NET LEAR, V30, P1672, DOI 10.1109/TNNLS.2018.2872999
   Pereira DR, 2020, NEURAL COMPUT APPL, V32, P6393, DOI 10.1007/s00521-019-04146-4
   Plana LA, 2011, ACM J EMERG TECH COM, V7, DOI 10.1145/2043643.2043647
   Querlioz D, 2013, IEEE T NANOTECHNOL, V12, P288, DOI 10.1109/TNANO.2013.2250995
   Rafiei MH, 2017, IEEE T NEUR NET LEAR, V28, P3074, DOI 10.1109/TNNLS.2017.2682102
   Ridet JL, 1997, TRENDS NEUROSCI, V20, P570, DOI 10.1016/S0166-2236(97)01139-9
   Rigotti M, 2013, NATURE, V497, P585, DOI 10.1038/nature12160
   Rosenblatt F., 1988, PERCEPTION PROBABILI, P89
   Roy K, 2019, NATURE, V575, P607, DOI 10.1038/s41586-019-1677-2
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Salakhutdinov R, 2012, NEURAL COMPUT, V24, P1967, DOI 10.1162/NECO_a_00311
   Song T, 2019, IEEE T NANOBIOSCI, V18, P176, DOI 10.1109/TNB.2019.2896981
   Tavanaei A, 2019, NEUROCOMPUTING, V330, P39, DOI 10.1016/j.neucom.2018.11.014
   Verlan S, 2020, J MEMBRANE COMPUT, V2, P355, DOI 10.1007/s41965-020-00050-2
   Wade JJ, 2010, IEEE T NEURAL NETWOR, V21, P1817, DOI 10.1109/TNN.2010.2074212
   Wang J, 2013, INT J COMPUT MATH, V90, P857, DOI 10.1080/00207160.2012.743653
   Wang J, 2013, IEEE T FUZZY SYST, V21, P209, DOI 10.1109/TFUZZ.2012.2208974
   Wang J, 2010, NEURAL COMPUT, V22, P2615, DOI 10.1162/NECO_a_00022
   Wang RM, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00213
   Wang T, 2015, IEEE T POWER SYST, V30, P1182, DOI 10.1109/TPWRS.2014.2347699
   Wang ZQ, 2000, IEEE T NEURAL NETWOR, V11, P47, DOI 10.1109/72.822509
   Whittington JCR, 2019, TRENDS COGN SCI, V23, P235, DOI 10.1016/j.tics.2018.12.005
   Wu TF, 2021, IEEE T NEUR NET LEAR, V32, P2443, DOI 10.1109/TNNLS.2020.3005538
   Wu TF, 2018, INT J NEURAL SYST, V28, DOI 10.1142/S0129065718500132
   Wu TF, 2018, IEEE T NEUR NET LEAR, V29, P3349, DOI 10.1109/TNNLS.2017.2726119
   Zhang GX, 2021, INT J NEURAL SYST, V31, DOI 10.1142/S0129065720500550
   Zhang GX, 2014, INT J NEURAL SYST, V24, DOI 10.1142/S0129065714400061
   Zhu M, 2021, INT J NEURAL SYST, V31, DOI 10.1142/S0129065720500549
NR 72
TC 32
Z9 32
U1 13
U2 72
PD AUG
PY 2022
VL 32
IS 08
AR 2250023
DI 10.1142/S012906572250023X
UT WOS:000830051700005
DA 2023-11-16
ER

PT J
AU Zhong, XY
   Pan, HB
AF Zhong, Xueyan
   Pan, Hongbing
TI A Spike Neural Network Model for Lateral Suppression of
   Spike-Timing-Dependent Plasticity with Adaptive Threshold
SO APPLIED SCIENCES-BASEL
DT Article
DE Spike Neural Network; spike-timing-dependent plasticity; lateral
   inhibition; adaptive threshold; Leaky Integrate-and-Fire; pulse coding
ID STDP
AB Aiming at the practical constraints of high resource occupancy and complex calculations in the existing Spike Neural Network (SNN) image classification model, in order to seek a more lightweight and efficient machine vision solution, this paper proposes an adaptive threshold Spike Neural Network (SNN) model of lateral inhibition of Spike-Timing-Dependent Plasticity (STDP). The conversion from grayscale image to pulse sequence is completed by convolution normalization and first pulse time coding. The network self-classification is realized by combining the classical Spike-Timing-Dependent Plasticity algorithm (STDP) and lateral suppression algorithm. The occurrence of overfitting is effectively suppressed by introducing an adaptive threshold. The experimental results on the MNIST data set show that compared with the traditional SNN classification model, the complexity of the weight update algorithm is reduced from O(n(2)) to O(1), and the accuracy rate can still remain stable at about 96%. The provided model is conducive to the migration of software algorithms to the bottom layer of the hardware platform, and can provide a reference for the realization of edge computing solutions for small intelligent hardware terminals with high efficiency and low power consumption.
C1 [Zhong, Xueyan; Pan, Hongbing] Nanjing Univ, Sch Elect Sci & Engn, Nanjing 210023, Peoples R China.
   [Zhong, Xueyan] Nanjing Vocat Inst Railway Technol, Coll Intelligent Engn, Nanjing 210031, Peoples R China.
RP Zhong, XY (corresponding author), Nanjing Univ, Sch Elect Sci & Engn, Nanjing 210023, Peoples R China.; Zhong, XY (corresponding author), Nanjing Vocat Inst Railway Technol, Coll Intelligent Engn, Nanjing 210031, Peoples R China.
EM zhongxueyan1987@163.com; phb@nju.edu.cn
CR Amirshahi A, 2019, IEEE T BIOMED CIRC S, V13, P1483, DOI 10.1109/TBCAS.2019.2948920
   Andrzejak RG, 2001, PHYS REV E, V64, DOI 10.1103/PhysRevE.64.061907
   Borst A, 1999, NAT NEUROSCI, V2, P947, DOI 10.1038/14731
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Gao ZK, 2019, IEEE T NEUR NET LEAR, V30, P2755, DOI 10.1109/TNNLS.2018.2886414
   Hu SG, 2021, NEURAL COMPUT APPL, V33, P12317, DOI 10.1007/s00521-021-05832-y
   Hwang S, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11052059
   Kasabov NK, 2014, NEURAL NETWORKS, V52, P62, DOI 10.1016/j.neunet.2014.01.006
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Lammie C, 2019, IEEE T CIRCUITS-I, V66, P1558, DOI 10.1109/TCSI.2018.2881753
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee C, 2019, IEEE T COGN DEV SYST, V11, P384, DOI 10.1109/TCDS.2018.2833071
   Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508
   Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012
   Li XM, 2018, PHYSICA A, V491, P716, DOI 10.1016/j.physa.2017.08.053
   Liu SC, 2010, CURR OPIN NEUROBIOL, V20, P288, DOI 10.1016/j.conb.2010.03.007
   Michielli N, 2019, COMPUT BIOL MED, V106, P71, DOI 10.1016/j.compbiomed.2019.01.013
   Mohammadi Y, 2019, IRAN CONF ELECTR ENG, P1765, DOI [10.1109/iraniancee.2019.8786540, 10.1109/IranianCEE.2019.8786540]
   Mozafari M, 2018, IEEE T NEUR NET LEAR, V29, P6178, DOI 10.1109/TNNLS.2018.2826721
   Nuntalid N, 2011, LECT NOTES COMPUT SC, V7062, P451, DOI 10.1007/978-3-642-24955-6_54
   Orhan U, 2011, EXPERT SYST APPL, V38, P13475, DOI 10.1016/j.eswa.2011.04.149
   Petro B, 2020, IEEE T NEUR NET LEAR, V31, P358, DOI 10.1109/TNNLS.2019.2906158
   Sengupta N, 2017, INFORM SCIENCES, V406, P133, DOI 10.1016/j.ins.2017.04.017
   Shi MT, 2020, FRONT COMPUT NEUROSC, V14, DOI 10.3389/fncom.2020.00007
   Wang XW, 2020, NEURAL NETWORKS, V125, P258, DOI 10.1016/j.neunet.2020.02.011
   Xia Y, 2020, J ROBOT NETW ARTIF L, V7, P121, DOI 10.2991/jrnal.k.200528.010
   Xiang SY, 2021, IEEE J SEL TOP QUANT, V27, DOI 10.1109/JSTQE.2020.3005589
   [赵梓铭 Zhao Ziming], 2018, [计算机研究与发展, Journal of Computer Research and Development], V55, P327
   Zheng N, 2018, IEEE T NEUR NET LEAR, V29, P4287, DOI 10.1109/TNNLS.2017.2761335
NR 30
TC 2
Z9 2
U1 5
U2 25
PD JUN
PY 2022
VL 12
IS 12
AR 5980
DI 10.3390/app12125980
UT WOS:000817531600001
DA 2023-11-16
ER

PT C
AU Li, HY
   Hao, XY
   Wang, J
   Che, YQ
AF Li, Huiyan
   Hao, Xinyu
   Wang, Jiang
   Che, Yanqiu
BE Li, Z
   Sun, J
TI Automatic Fixed-point Design Method for Spiking Neuron Circuits
SO 2022 41ST CHINESE CONTROL CONFERENCE (CCC)
SE Chinese Control Conference
DT Proceedings Paper
CT 41st Chinese Control Conference (CCC)
CY JUL 25-27, 2022
CL Hefei, PEOPLES R CHINA
DE Fixed-point design; Hardware implementation; Spiking neurons
ID HARDWARE; NETWORKS
AB Improving hardware implementation efficiency of spiking neuron models is the basis of realizing high-performance spiking neural networks and is also beneficial to increasing the achievable network size. To this end, this work proposes an automatic fixed-point design method for designing spiking neuron circuits under any precision requirement to improve the realization efficiency. The performance of the method is evaluated on both phenomenological and biophysical neuron models and also a neural network with online learning. The results show that compared with the reference neuron model, the model designed with the proposed method can save 30% of resource consumption and increase 10% of the working frequency when the firing rate error is required to be 10%. The firing mode learning task can also be completed successfully without performance degradation with a neural network composed of the designed fixed-point neuron models, which demonstrates the effectiveness of the proposed method.
C1 [Li, Huiyan; Che, Yanqiu] Tianjin Univ Technol & Educ, Sch Automat & Elect Engn, Tianjin 300222, Peoples R China.
   [Hao, Xinyu; Wang, Jiang] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
RP Li, HY (corresponding author), Tianjin Univ Technol & Educ, Sch Automat & Elect Engn, Tianjin 300222, Peoples R China.
EM lhy2740@126.com; jiang.wang@tju.edu.cn
CR Farsa EZ, 2019, IEEE T CIRCUITS-II, V66, P1582, DOI 10.1109/TCSII.2019.2890846
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   Guo WZ, 2022, IEEE T NEUR NET LEAR, V33, P3988, DOI 10.1109/TNNLS.2021.3055421
   Heidarpour M, 2016, IEEE T CIRCUITS-I, V63, P1986, DOI 10.1109/TCSI.2016.2598161
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Huang X, 2021, IEEE INT ULTRA SYM, DOI 10.1109/IUS52206.2021.9593901
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Kreiser R, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00551
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maguire LP, 2007, NEUROCOMPUTING, V71, P13, DOI 10.1016/j.neucom.2006.11.029
   Patel K., 2021, ARXIV210608921
   Pérez J, 2018, NEURAL NETWORKS, V104, P15, DOI 10.1016/j.neunet.2018.04.002
   Ponulak F., 2005, RESUME NEW SUPERVISE
   Roy K, 2019, NATURE, V575, P607, DOI 10.1038/s41586-019-1677-2
   Roy S, 2004, DES AUT CON, P484, DOI 10.1145/996566.996701
NR 15
TC 0
Z9 0
U1 0
U2 0
PY 2022
BP 7094
EP 7099
UT WOS:000932071607038
DA 2023-11-16
ER

PT J
AU Huang, XH
   Zheng, ZG
   Hu, G
   Wu, S
   Rasch, MJ
AF Huang, Xuhui
   Zheng, Zhigang
   Hu, Gang
   Wu, Si
   Rasch, Malte J.
TI Different propagation speeds of recalled sequences in plastic spiking
   neural networks
SO NEW JOURNAL OF PHYSICS
DT Article
DE sequential activity recall; spiking neural network; spike-timing
   dependent plasticity
ID TIMING-DEPENDENT PLASTICITY; FEEDFORWARD NETWORKS; SYNCHRONOUS SPIKING;
   STABLE PROPAGATION; HIPPOCAMPAL REPLAY; SYNFIRE CHAINS; SPARSE CODE;
   REVERBERATION; REACTIVATION; NEURONS
AB Neural networks can generate spatiotemporal patterns of spike activity. Sequential activity learning and retrieval have been observed in many brain areas, and e.g. is crucial for coding of episodic memory in the hippocampus or generating temporal patterns during song production in birds. In a recent study, a sequential activity pattern was directly entrained onto the neural activity of the primary visual cortex (V1) of rats and subsequently successfully recalled by a local and transient trigger. It was observed that the speed of activity propagation in coordinates of the retinotopically organized neural tissue was constant during retrieval regardless how the speed of light stimulation sweeping across the visual field during training was varied. It is well known that spike-timing dependent plasticity (STDP) is a potential mechanism for embedding temporal sequences into neural network activity. How training and retrieval speeds relate to each other and how network and learning parameters influence retrieval speeds, however, is not well described. We here theoretically analyze sequential activity learning and retrieval in a recurrent neural network with realistic synaptic short-term dynamics and STDP. Testing multiple STDP rules, we confirm that sequence learning can be achieved by STDP. However, we found that a multiplicative nearest-neighbor (NN) weight update rule generated weight distributions and recall activities that best matched the experiments in V1. Using network simulations and mean-field analysis, we further investigated the learning mechanisms and the influence of network parameters on recall speeds. Our analysis suggests that a multiplicative STDP rule with dominant NN spike interaction might be implemented in V1 since recall speed was almost constant in an NMDA-dominant regime. Interestingly, in an AMPA-dominant regime, neural circuits might exhibit recall speeds that instead follow the change in stimulus speeds. This prediction could be tested in experiments.
C1 [Huang, Xuhui; Zheng, Zhigang; Hu, Gang] Beijing Normal Univ, Dept Phys, Beijing 100875, Peoples R China.
   [Huang, Xuhui] Chinese Acad Sci, Brainnetome Ctr, Inst Automat, Beijing, Peoples R China.
   [Huang, Xuhui] Chinese Acad Sci, Natl Lab Pattern Recognit, Inst Automat, Beijing, Peoples R China.
   [Wu, Si; Rasch, Malte J.] Beijing Normal Univ, State Key Lab Cognit Neurosci & Learning, Beijing 100875, Peoples R China.
   [Wu, Si; Rasch, Malte J.] Beijing Normal Univ, IDG McGovern Inst Brain Res, Beijing 100875, Peoples R China.
   [Wu, Si; Rasch, Malte J.] Beijing Normal Univ, Ctr Collaborat & Innovat Brain & Learning Sci, Beijing 100875, Peoples R China.
RP Huang, XH (corresponding author), Beijing Normal Univ, Dept Phys, Beijing 100875, Peoples R China.
EM wusi@bnu.edu.cn; malte.rasch@bnu.edu.cn
CR Abbott LF, 1996, CEREB CORTEX, V6, P406, DOI 10.1093/cercor/6.3.406
   Aviel Y, 2004, NEUROCOMPUTING, V58, P123, DOI 10.1016/j.neucom.2004.01.032
   Bashan A, 2012, NAT COMMUN, V3, DOI 10.1038/ncomms1705
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Byrnes S, 2011, NEURAL COMPUT, V23, P2567, DOI 10.1162/NECO_a_00184
   Carr MF, 2011, NAT NEUROSCI, V14, P147, DOI 10.1038/nn.2732
   Chance FS, 1998, J NEUROSCI, V18, P4785
   Contreras EJB, 2013, NEURON, V79, P555, DOI 10.1016/j.neuron.2013.06.013
   Dan Y, 2006, PHYSIOL REV, V86, P1033, DOI 10.1152/physrev.00030.2005
   Dave AS, 2000, SCIENCE, V290, P812, DOI 10.1126/science.290.5492.812
   Davidson TJ, 2009, NEURON, V63, P497, DOI 10.1016/j.neuron.2009.07.027
   Diba K, 2007, NAT NEUROSCI, V10, P1241, DOI 10.1038/nn1961
   Diesmann M, 1999, NATURE, V402, P529, DOI 10.1038/990101
   Eagleman SL, 2012, P NATL ACAD SCI USA, V109, P19450, DOI 10.1073/pnas.1212059109
   Euston DR, 2007, SCIENCE, V318, P1147, DOI 10.1126/science.1148979
   Fiete IR, 2010, NEURON, V65, P563, DOI 10.1016/j.neuron.2010.02.003
   Froemke RC, 2002, NATURE, V416, P433, DOI 10.1038/416433a
   Gerstner W., 2002, SPIKING NEURON MODEL
   Gjorgjieva J, 2011, P NATL ACAD SCI USA, V108, P19383, DOI 10.1073/pnas.1105933108
   Hahnloser RHR, 2002, NATURE, V419, P65, DOI 10.1038/nature00974
   Han F, 2008, NEURON, V60, P321, DOI 10.1016/j.neuron.2008.08.026
   Hanuschkin A, 2011, J COMPUT NEUROSCI, V31, P509, DOI 10.1007/s10827-011-0318-z
   Holcman D, 2006, PLOS COMPUT BIOL, V2, P174, DOI 10.1371/journal.pcbi.0020023
   Hosaka R, 2008, NEURAL COMPUT, V20, P415, DOI 10.1162/neco.2007.11-05-043
   Huang XH, 2014, CHINESE PHYS B, V23, DOI 10.1088/1674-1056/23/10/108703
   Ikegaya Y, 2004, SCIENCE, V304, P559, DOI 10.1126/science.1093173
   Ivanov PC, 2014, UNDERST COMPLEX SYST, P203, DOI 10.1007/978-3-319-03518-5_10
   Izhikevich EM, 2004, CEREB CORTEX, V14, P933, DOI 10.1093/cercor/bhh053
   Izhikevich EM, 2003, NEURAL COMPUT, V15, P1511, DOI 10.1162/089976603321891783
   Ji DY, 2007, NAT NEUROSCI, V10, P100, DOI 10.1038/nn1825
   Karlsson MP, 2009, NAT NEUROSCI, V12, P913, DOI 10.1038/nn.2344
   Kistler WM, 2002, NEURAL COMPUT, V14, P987, DOI 10.1162/089976602753633358
   Klampfl S, 2013, J NEUROSCI, V33, P11515, DOI 10.1523/JNEUROSCI.5044-12.2013
   Kumar A, 2008, J NEUROSCI, V28, P5268, DOI 10.1523/JNEUROSCI.2542-07.2008
   Kumar A, 2010, NAT REV NEUROSCI, V11, P615, DOI 10.1038/nrn2886
   Lehn H, 2009, J NEUROSCI, V29, P3475, DOI 10.1523/JNEUROSCI.5370-08.2009
   Li MR, 2006, PHYS REV E, V74, DOI 10.1103/PhysRevE.74.011918
   Litvak V, 2003, J NEUROSCI, V23, P3006
   Long MA, 2010, NATURE, V468, P394, DOI 10.1038/nature09514
   Luczak A, 2007, P NATL ACAD SCI USA, V104, P347, DOI 10.1073/pnas.0605643104
   MARGOLIASH D, 1992, J NEUROSCI, V12, P4309
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   Masuda N, 2007, J COMPUT NEUROSCI, V22, P327, DOI 10.1007/s10827-007-0022-1
   Mongillo G, 2008, SCIENCE, V319, P1543, DOI 10.1126/science.1150769
   Nádasdy Z, 1999, J NEUROSCI, V19, P9497
   O'Neill J, 2008, NAT NEUROSCI, V11, P209, DOI 10.1038/nn2037
   Pastalkova E, 2008, SCIENCE, V321, P1322, DOI 10.1126/science.1159775
   Pfister JP, 2006, J NEUROSCI, V26, P9673, DOI 10.1523/JNEUROSCI.1425-06.2006
   Rubin J, 2001, PHYS REV LETT, V86, P364, DOI 10.1103/PhysRevLett.86.364
   Sjostrom J., 2010, SCHOLARPEDIA 52 REVI, V5, P1362, DOI [10.4249%2Fscholarpedia.1362, DOI 10.4249/SCHOLARPEDIA.1362]
   Sjöström PJ, 2001, NEURON, V32, P1149, DOI 10.1016/s0896-6273(01)00542-6
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Stoop R, 2013, PHYS REV LETT, V110, DOI 10.1103/PhysRevLett.110.108105
   Suri RE, 2002, BIOL CYBERN, V87, P440, DOI 10.1007/s00422-002-0355-9
   Swadlow H. A., 2012, SCHOLARPEDIA, V7, DOI [10.4249/scholarpedia.1451, DOI 10.4249/SCHOLARPEDIA.1451]
   Takahashi YK, 2009, PHYS REV E, V79, DOI 10.1103/PhysRevE.79.051904
   Tsodyks M, 1998, NEURAL COMPUT, V10, P821, DOI 10.1162/089976698300017502
   Tsodyks MV, 1997, P NATL ACAD SCI USA, V94, P719, DOI 10.1073/pnas.94.2.719
   van Rossum MCW, 2000, J NEUROSCI, V20, P8812
   Varela JA, 1999, J NEUROSCI, V19, P4293
   Varela JA, 1997, J NEUROSCI, V17, P7926
   Wang HX, 2005, NAT NEUROSCI, V8, P187, DOI 10.1038/nn1387
   Wang ST, 2006, PHYS REV LETT, V96, DOI 10.1103/PhysRevLett.96.018103
   Wang XJ, 2002, NEURON, V36, P955, DOI 10.1016/S0896-6273(02)01092-9
   WILSON MA, 1994, SCIENCE, V265, P676, DOI 10.1126/science.8036517
   Xu SJ, 2012, NAT NEUROSCI, V15, P449, DOI 10.1038/nn.3036
   Yuan WJ, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0084644
NR 67
TC 6
Z9 6
U1 1
U2 25
PD MAR 6
PY 2015
VL 17
AR 035006
DI 10.1088/1367-2630/17/3/035006
UT WOS:000352899100005
DA 2023-11-16
ER

PT C
AU Howard, G
   Bull, L
   Lanzi, PL
AF Howard, Gerard
   Bull, Larry
   Lanzi, Pier-Luca
GP IEEE
TI A Spiking Neural Representatin for XCSF
SO 2010 IEEE CONGRESS ON EVOLUTIONARY COMPUTATION (CEC)
SE IEEE Congress on Evolutionary Computation
DT Proceedings Paper
CT 2010 IEEE World Congress on Computational Intelligence
CY JUL 18-23, 2010
CL Barcelona, SPAIN
AB This paper presents a Learning Classifier System (LCS) where each traditional rule is represented by a spiking neural network, a type of network with dynamic internal state. The evolutionary design process exploits parameter self-adaptation and a constructionist approach, providing the system with a flexible knowledge representation. It is shown how this approach allows for the evolution of networks of appropriate complexity to emerge whilst solving a continuous maze environment. Additionally, we extend the system to allow for temporal state decomposition. We evaluate our spiking neural LCS against one that uses Multi Layer Perceptron rules.
C1 [Howard, Gerard; Bull, Larry] Univ West England, Dept Comp Sci, Bristol BS16 1QY, Avon, England.
   [Lanzi, Pier-Luca] Politecn Milan, Dipartimento Elect & Informazione, Milan, Italy.
RP Howard, G (corresponding author), Univ West England, Dept Comp Sci, Bristol BS16 1QY, Avon, England.
EM gerard2.howard@uwc.ac.uk; larry.bull@uwe.ac.uk; pierluca.lanzi@polimi.it
CR Ahluwalia M, 1999, GECCO-99: PROCEEDINGS OF THE GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P11
   [Anonymous], 2001, EVOLUTIONARY ROBOTIC
   [Anonymous], GEN EV COMP C GECCO
   Boyan J. A., 1995, Advances in Neural Information Processing Systems 7, P369
   Buhmann MD., 2003, C MO AP C M, P11, DOI 10.1017/CBO9780511543241
   Bull L., 2002, Parallel Problem Solving from Nature - PPSN VII. 7th International Conference. Proceedings (Lecture Notes in Computer Science Vol.2439), P558
   Bull L, 2003, IEEE C EVOL COMPUTAT, P991
   Bull L., 2009, INT J PARAL IN PRESS
   Bull L., 2000, AN AN 6 INT C SIM AD
   Bull L., 2002, P 4 ANN C GEN EV COM, P905
   Dam HH, 2008, IEEE T KNOWL DATA EN, V20, P26, DOI 10.1109/TKDE.2007.190671
   Dolan C. P., 1987, Genetic Algorithms and their Applications: Proceedings of the Second International Conference on Genetic Algorithms, P123
   Federici D, 2005, P CEC 2005 IEEE C EV
   Floreano D., 2005, P 8 INT C ART LIF DE
   Floreano D, 2008, EVOL INTELL, V1, P47, DOI 10.1007/s12065-007-0002-4
   Gerstner W., 2002, SPIKING NEURON MODEL
   Giani A, 1995, P 4 ANN C EV PROGR E
   HOLLAND JH, 1976, PROG THEOR BIOL, V4, P263
   Howard D., 2009, GECCO 2009
   Howard D., 2008, GECCO 2008, P1977
   Hurst J., 2002, Parallel Problem Solving from Nature - PPSN VII. 7th International Conference. Proceedings (Lecture Notes in Computer Science Vol.2439), P588
   Hurst J, 2006, ARTIF LIFE, V12, P353, DOI 10.1162/artl.2006.12.3.353
   Indiveri G., 2001, IEEE T NEUR IN PRESS
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Korkin M, 1998, P 2 INT C EV SYST BI
   Lanzi P. L., IEEE C EV COMP CEC 2, P2270
   Lanzi PL, 2005, IEEE C EVOL COMPUTAT, P2032
   Lanzi PL, 1999, GECCO-99: PROCEEDINGS OF THE GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P345
   LIN LJ, 1992, MACH LEARN, V8, P625
   MAHADEVAN S, 1992, ARTIF INTELL, V55, P311, DOI 10.1016/0004-3702(92)90058-6
   O'Hara T, 2005, IEEE C EVOL COMPUTAT, P2046
   OHARA T, 2004, UWELCSG04004
   Port R.F., 1995, MIND MOTION EXPLORAT
   Quartz SR, 1997, BEHAV BRAIN SCI, V20, P537
   Rocha M., EPIA 2003, P24
   Rumelhart DE, 1986, PARALLEL DISTRIBUTED, V1, DOI DOI 10.7551/MITPRESS/5236.001.0001
   Santamaria JC, 1997, ADAPT BEHAV, V6, P163, DOI 10.1177/105971239700600201
   Schlessinger E., 2005, P EUR C ART LIF ECAL
   THAM CK, 1995, ROBOT AUTON SYST, V15, P247, DOI 10.1016/0921-8890(95)00005-Z
   Valenzuela-Rendon M, 1991, P 4 INT C GENETIC AL, P346
   Wilson S. W., 2001, 4 INT WORKSH LEARN C
   Wilson SW, 1995, EVOL COMPUT, V3, P149, DOI 10.1162/evco.1995.3.2.149
   Wilson Stewart W, 2000, LEARNING CLASSIFIER, P209, DOI [DOI 10.1007/3-540-45027-011, 10.1007/3-540-45027-0_11, DOI 10.1007/3-540-45027-0_11]
NR 43
TC 1
Z9 1
U1 0
U2 0
PY 2010
UT WOS:000287375801007
DA 2023-11-16
ER

PT J
AU Han, JH
   Li, ZL
   Zheng, WM
   Zhang, YH
AF Han, Jianhui
   Li, Zhaolin
   Zheng, Weimin
   Zhang, Youhui
TI Hardware Implementation of Spiking Neural Networks on FPGA
SO TSINGHUA SCIENCE AND TECHNOLOGY
DT Article
DE Spiking Neural Network (SNN); Field-Programmable Gate Arrays (FPGA);
   digital circuit; low-power; MNIST
AB Inspired by real biological neural models, Spiking Neural Networks (SNNs) process information with discrete spikes and show great potential for building low-power neural network systems. This paper proposes a hardware implementation of SNN based on Field-Programmable Gate Arrays (FPGA). It features a hybrid updating algorithm, which combines the advantages of existing algorithms to simplify hardware design and improve performance. The proposed design supports up to 16384 neurons and 16.8 million synapses but requires minimal hardware resources and archieves a very low power consumption of 0.477W. A test platform is built based on the proposed design using a Xilinx FPGA evaluation board, upon which we deploy a classification task on the MNIST dataset. The evaluation results show an accuracy of 97.06% and a frame rate of 161 frames per second.
C1 [Han, Jianhui] Tsinghua Univ, Inst Microelect, Beijing 100084, Peoples R China.
   [Li, Zhaolin] Tsinghua Univ, Res Inst Informat Technol, Beijing 100084, Peoples R China.
   [Zheng, Weimin; Zhang, Youhui] Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
RP Li, ZL (corresponding author), Tsinghua Univ, Res Inst Informat Technol, Beijing 100084, Peoples R China.; Zhang, YH (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
EM hanjh16@mails.tsinghua.edu.cn; lzl73@mail.tsinghua.edu.cn;
   zwm-dcs@mail.tsinghua.edu.cn; zyh02@tsinghua.edu.cn
CR [Anonymous], XIL ZYNQ 7000 SOC ZC
   [Anonymous], NVIDIA TESL P100 WOR
   [Anonymous], NVIDIA SYST MAN INT
   Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Cheung Kit, 2012, Artificial Neural Networks and Machine Learning - ICANN 2012. Proceedings of the 22nd International Conference on Artificial Neural Networks, P113, DOI 10.1007/978-3-642-33269-2_15
   Ciresan DC, 2010, NEURAL COMPUT, V22, P3207, DOI 10.1162/NECO_a_00052
   Diehl P.U., 2015, 2015 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2015.7280696
   Farquhar E, 2006, IEEE INT SYMP CIRC S, P4114, DOI 10.1109/ISCAS.2006.1693534
   Han S., 2015, ARXIV PREPRINT 15100
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Khan MM, 2008, IEEE IJCNN, P2849, DOI 10.1109/IJCNN.2008.4634199
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Liu M, 2009, L N INST COMP SCI SO, V3, P44
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Mohamed AR, 2012, IEEE T AUDIO SPEECH, V20, P14, DOI 10.1109/TASL.2011.2109382
   Moore SW, 2012, ANN IEEE SYM FIELD P, P133, DOI 10.1109/FCCM.2012.32
   Neil D, 2014, IEEE T VLSI SYST, V22, P2621, DOI 10.1109/TVLSI.2013.2294916
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   Paszke Adam, 2017, NIPS W
   Pfeil T, 2016, PHYS REV X, V6, DOI 10.1103/PhysRevX.6.021023
NR 21
TC 47
Z9 47
U1 6
U2 84
PD AUG
PY 2020
VL 25
IS 4
BP 479
EP 486
DI 10.26599/TST.2019.9010019
UT WOS:000517499500004
DA 2023-11-16
ER

PT J
AU Rathi, N
   Roy, K
AF Rathi, Nitin
   Roy, Kaushik
TI DIET-SNN: A Low-Latency Spiking Neural Network With Direct Input
   Encoding and Leakage and Threshold Optimization
SO IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
DT Article
DE Neurons; Training; Encoding; Backpropagation; Task analysis;
   Computational modeling; Biological neural networks; Backpropagation
   through time (BPTT); convolutional neural networks; spiking neural
   networks (SNNs); supervised learning
ID ON-CHIP
AB Bioinspired spiking neural networks (SNNs), operating with asynchronous binary signals (or spikes) distributed over time, can potentially lead to greater computational efficiency on event-driven hardware. The state-of-the-art SNNs suffer from high inference latency, resulting from inefficient input encoding and suboptimal settings of the neuron parameters (firing threshold and membrane leak). We propose DIET-SNN, a low-latency deep spiking network trained with gradient descent to optimize the membrane leak and the firing threshold along with other network parameters (weights). The membrane leak and threshold of each layer are optimized with end-to-end backpropagation to achieve competitive accuracy at reduced latency. The input layer directly processes the analog pixel values of an image without converting it to spike train. The first convolutional layer converts analog inputs into spikes where leaky-integrate-and-fire (LIF) neurons integrate the weighted inputs and generate an output spike when the membrane potential crosses the trained firing threshold. The trained membrane leak selectively attenuates the membrane potential, which increases activation sparsity in the network. The reduced latency combined with high activation sparsity provides massive improvements in computational efficiency. We evaluate DIET-SNN on image classification tasks from CIFAR and ImageNet datasets on VGG and ResNet architectures. We achieve top-1 accuracy of 69% with five timesteps (inference latency) on the ImageNet dataset with 12x less compute energy than an equivalent standard artificial neural network (ANN). In addition, DIET-SNN performs 20-500x faster inference compared to other state-of-the-art SNN models.
C1 [Rathi, Nitin; Roy, Kaushik] Purdue Univ, Sch Elect & Comp Engn, W Lafayette, IN 47907 USA.
RP Rathi, N (corresponding author), Purdue Univ, Sch Elect & Comp Engn, W Lafayette, IN 47907 USA.
EM rathi2@purdue.edu; kaushik@purdue.edu
CR Almomani A, 2019, CLUSTER COMPUT, V22, P419, DOI 10.1007/s10586-018-02891-0
   [Anonymous], 2019, ARXIV190109948
   [Anonymous], 2019, CORR
   Bellec G., 2018, ADV NEURAL INFORM PR
   Bing Han, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13555, DOI 10.1109/CVPR42600.2020.01357
   Bohte S. M., 2000, 8th European Symposium on Artificial Neural Networks. ESANN"2000. Proceedings, P419
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Chakraborty I, 2020, NAT MACH INTELL, V2, P43, DOI 10.1038/s42256-019-0134-0
   Chankyu Lee, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12374), P366, DOI 10.1007/978-3-030-58526-6_22
   Chen T., 2016, ARXIV160406174
   Chen YH, 2016, CONF PROC INT SYMP C, P367, DOI 10.1109/ISCA.2016.40
   Comsa JM, 2020, INT CONF ACOUST SPEE, P8529, DOI [10.1109/icassp40776.2020.9053856, 10.1109/ICASSP40776.2020.9053856]
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Diehl P.U., 2015, 2015 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2015.7280696
   Fang W., 2020, ARXIV200705785
   Frady E. P, 2020, ARXIV200412691
   Freund K, 2019, GOOGLE CLOUD DOUBLES
   Gerstner W., 2002, SPIKING NEURON MODEL
   Han S., 2016, P INT C LEARN REPR, DOI DOI 10.48550/ARXIV.1510.00149
   He YH, 2018, LECT NOTES COMPUT SC, V11211, P815, DOI 10.1007/978-3-030-01234-2_48
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Horowitz M, 2014, ISSCC DIG TECH PAP I, V57, P10, DOI 10.1109/ISSCC.2014.6757323
   Huh D, 2018, ADV NEUR IN, V31
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90
   Kheradpisheh SR, 2020, INT J NEURAL SYST, V30, DOI 10.1142/S0129065720500276
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
   Krizhevsky Alex, 2009, LEARNING MULTIPLE LA
   Li D, 2016, PROCEEDINGS OF 2016 IEEE INTERNATIONAL CONFERENCES ON BIG DATA AND CLOUD COMPUTING (BDCLOUD 2016) SOCIAL COMPUTING AND NETWORKING (SOCIALCOM 2016) SUSTAINABLE COMPUTING AND COMMUNICATIONS (SUSTAINCOM 2016) (BDCLOUD-SOCIALCOM-SUSTAINCOM 2016), P477, DOI 10.1109/BDCloud-SocialCom-SustainCom.2016.76
   Li G., 2019, ARXIV190701167
   Lillicrap TP, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms13276
   Lu S, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00535
   MAINEN ZF, 1995, SCIENCE, V268, P1503, DOI 10.1126/science.7770778
   Mostafa H, 2018, IEEE T NEUR NET LEAR, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Painkras E, 2013, IEEE J SOLID-ST CIRC, V48, P1943, DOI 10.1109/JSSC.2013.2259038
   Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32
   Rathi Nitin, 2020, INT C LEARN REPR
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Samadi A, 2017, NEURAL COMPUT, V29, P578, DOI 10.1162/NECO_a_00929
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Shen YM, 2017, ANN IEEE SYM FIELD P, P93, DOI 10.1109/FCCM.2017.47
   Shrestha SB, 2018, ADV NEUR IN, V31
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Taherkhani A, 2018, IEEE T NEUR NET LEAR, V29, P5394, DOI 10.1109/TNNLS.2018.2797801
   Wu YJ, 2019, AAAI CONF ARTIF INTE, P1311
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Yin B, 2020, IEEE INTERNET THINGS, V7, P8748, DOI [10.1109/JIOT.2020.2996562, 10.1145/3407197.3407225]
   Zhang W, 2020, ARXIV PREPRINT ARXIV
   Zheng H., 2020, ARXIV201105280
NR 50
TC 43
Z9 43
U1 4
U2 19
PD JUN
PY 2023
VL 34
IS 6
BP 3174
EP 3182
DI 10.1109/TNNLS.2021.3111897
EA SEP 2021
UT WOS:000732232200001
DA 2023-11-16
ER

PT C
AU Enriquez-Gaytan, J
   Gómez-Castañeda, F
   Moreno-Cadenas, JA
   Flores-Nava, LM
AF Enriquez-Gaytan, J.
   Gomez-Castaneda, F.
   Moreno-Cadenas, J. A.
   Flores-Nava, L. M.
GP IEEE
TI Spiking Neural Network Trained by Metaheuristics for Gas Sensing
SO 2017 INTERNATIONAL CONFERENCE ON ELECTRONICS, COMMUNICATIONS AND
   COMPUTERS (CONIELECOMP)
SE International Conference on Electronics Communications and Computers
   CONIELECOMP
DT Proceedings Paper
CT 27th International Conference on Electronics, Communications and
   Computers (CONIELECOMP)
CY FEB 22-24, 2017
CL Univ Americas Pubela, Cholula, MEXICO
HO Univ Americas Pubela
DE Spiking neural networks (SNN-Izhikevich's model); bio-inspired signal
   processing; Metaheuristic algorithm; Artificial Bee Colony algorithm
   (ABC algorithm)
AB In this manuscript, we propose a bio-inspired method of signal processing in a task for the detection of gases (Methane and Iso-Butane) using an array of four gas sensors by Figaro and Spiking Neural Network (SNN-Izhikevich's model), supported by the synaptic current model. Our SNN is trained by the metaheuristic algorithm of Artificial Bee Colony algorithm (ABC algorithm) in a supervised manner. This is demonstrated using Matlab.
C1 [Enriquez-Gaytan, J.; Gomez-Castaneda, F.; Moreno-Cadenas, J. A.; Flores-Nava, L. M.] IPN, CINVESTAV, Dept Elect Engn, Mexico City, DF, Mexico.
RP Enriquez-Gaytan, J (corresponding author), IPN, CINVESTAV, Dept Elect Engn, Mexico City, DF, Mexico.
EM jenriquezg@cinvestav.mx; fgomez@cinvestav.mx; jmoreno@cinvestav.mx;
   lmflores@cinvestav.mx
CR [Anonymous], 2005, TR06
   Dayan P., 2001, THEORETICAL NEUROSCI
   Fountas Z., 2011, SPIKING NEURAL NETWO
   Hagan MT., 1997, NEURAL NETWORK DESIG
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2010, PHILOS T R SOC A, V368, P5061, DOI 10.1098/rsta.2010.0130
   Kandel E.R., 2012, PRINCIPLES NEURAL SC
   Karaboga D, 2007, LECT NOTES COMPUT SC, V4529, P789, DOI 10.1007/978-3-540-72950-1_77
   Karaboga D, 2014, ARTIF INTELL REV, V42, P21, DOI 10.1007/s10462-012-9328-0
   Ortiz I. C. Matadamas, 2014, APLICACION REDES NEU
   Perez-Garcia A. N., 2014, INT C EL ENG COMP SC
   Ramirez L. G. Corona, 2015, SENSORES ACTUADORES
   Sörensen K, 2015, INT T OPER RES, V22, P3, DOI 10.1111/itor.12001
   Vazquez RA, 2015, COMPUT INTEL NEUROSC, V2015, DOI 10.1155/2015/947098
   Wu QX, 2007, LECT NOTES ARTIF INT, V4682, P26
   Yigitbasi Elif Deniz, 2013, International Journal of Information and Electronics Engineering, V3, P634, DOI 10.7763/IJIEE.2013.V3.394
NR 17
TC 0
Z9 0
U1 2
U2 3
PY 2017
UT WOS:000403233800009
DA 2023-11-16
ER

PT C
AU Yue, Y
   Baltes, M
   Abujahar, N
   Sun, T
   Smith, CD
   Bihl, T
   Liu, J
AF Yue, Ye
   Baltes, Marc
   Abujahar, Nidal
   Sun, Tao
   Smith, Charles D.
   Bihl, Trevor
   Liu, Jundong
GP IEEE
TI HYBRID SPIKING NEURAL NETWORKS FINE-TUNING FOR HIPPOCAMPUS SEGMENTATION
SO 2023 IEEE 20TH INTERNATIONAL SYMPOSIUM ON BIOMEDICAL IMAGING, ISBI
SE IEEE International Symposium on Biomedical Imaging
DT Proceedings Paper
CT 20th IEEE International Symposium on Biomedical Imaging (ISBI)
CY APR 18-21, 2023
CL Cartagena, COLOMBIA
DE Spiking neural network; image segmentation; hippocampus; brain; U-Net;
   ANN-SNN conversion
AB Over the past decade, artificial neural networks (ANNs) have made tremendous advances, in part due to the increased availability of annotated data. However, ANNs typically require significant power and memory consumptions to reach their full potential. Spiking neural networks (SNNs) have recently emerged as a low-power alternative to ANNs due to their sparsity nature.
   SNN, however, are not as easy to train as ANNs. In this work, we propose a hybrid SNN training scheme and apply it to segment human hippocampi from magnetic resonance images. Our approach takes ANN-SNN conversion as an initialization step and relies on spike-based backpropagation to fine-tune the network. Compared with the conversion and direct training solutions, our method has advantages in both segmentation accuracy and training efficiency. Experiments demonstrate the effectiveness of our model in achieving the design goals.
C1 [Yue, Ye; Baltes, Marc; Abujahar, Nidal; Sun, Tao; Liu, Jundong] Ohio Univ, Sch Elect Engn & Comp Sci, Athens, OH 45701 USA.
   [Smith, Charles D.] Univ Kentucky, Dept Neurol, Lexington, KY USA.
   [Bihl, Trevor] Wright State Univ, Dept Biomed, Ind & Human Factors Engn, Dayton, OH 45435 USA.
RP Liu, J (corresponding author), Ohio Univ, Sch Elect Engn & Comp Sci, Athens, OH 45701 USA.
EM liuj1@ohio.edu
CR Bellec G, 2018, ADV NEUR IN, V31
   Chen YN, 2017, LECT NOTES COMPUT SC, V10541, P88, DOI 10.1007/978-3-319-67389-9_11
   Chen YN, 2017, I S BIOMED IMAGING, P192, DOI 10.1109/ISBI.2017.7950499
   Coupé P, 2011, NEUROIMAGE, V54, P940, DOI 10.1016/j.neuroimage.2010.09.018
   Davies M, 2021, P IEEE, V109, P911, DOI 10.1109/JPROC.2021.3067593
   Diehl PU, 2015, IEEE IJCNN
   Ho ND, 2021, DES AUT CON, P793, DOI 10.1109/DAC18074.2021.9586266
   Hobbs KH, 2016, I S BIOMED IMAGING, P19, DOI 10.1109/ISBI.2016.7493201
   Hunsberger E, 2015, Arxiv, DOI arXiv:1510.08829
   Kim Y, 2021, Arxiv, DOI arXiv:2110.07742
   Kim Y, 2021, FRONT NEUROSCI-SWITZ, V15, DOI 10.3389/fnins.2021.773954
   Li Yan, 2021, arXiv
   Liu QH, 2020, IEEE T NEUR NET LEAR, V31, P5300, DOI 10.1109/TNNLS.2020.2966058
   Manna Davide Liberato, 2022, NEUROMORPHIC COMPUTI, V1
   Patel K, 2021, Arxiv, DOI arXiv:2106.08921
   Rasmussen D, 2019, NEUROINFORMATICS, V17, P611, DOI 10.1007/s12021-019-09424-z
   Rathi Nitin, 2020, ENABLING DEEP SPIKIN
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Roy K, 2019, NATURE, V575, P607, DOI 10.1038/s41586-019-1677-2
   Rueckauer Bodo, 2016, ARXIV, DOI DOI 10.3389/FNINS.2017.00682
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Shi BB, 2017, PATTERN RECOGN, V63, P487, DOI 10.1016/j.patcog.2016.09.032
   Shreya S., 2018, PROC IEEE INT C EMER, P1, DOI [10.1109/ICEE44586.2018.8937873, DOI 10.1109/ICEE44586.2018.8937873]
   Song YT, 2015, LECT NOTES COMPUT SC, V9351, P190, DOI 10.1007/978-3-319-24574-4_23
   Tong T, 2013, NEUROIMAGE, V76, P11, DOI 10.1016/j.neuroimage.2013.02.069
   Vicente-Sola Alex, 2022, NEUROMORPHIC COMPUTI, P1
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
NR 27
TC 0
Z9 0
U1 0
U2 0
PY 2023
DI 10.1109/ISBI53787.2023.10230610
UT WOS:001062050500287
DA 2023-11-16
ER

PT C
AU Medini, C
   Vijayan, A
   Zacharia, RM
   Rajagopal, LP
   Nair, B
   Diwakar, S
AF Medini, Chaitanya
   Vijayan, Asha
   Zacharia, Ritu Maria
   Rajagopal, Lekshmi Priya
   Nair, Bipin
   Diwakar, Shyam
BE Mauri, JL
   Thampi, SM
   Wozniak, M
   Marques, O
   Krishnaswamy, D
   Sahni, S
   Callegari, C
   Takagi, H
   Bojkovic, ZS
   Vinod, M
   Prasad, NR
   Calero, JMA
   Rodrigues, J
   Que, XY
   Meghanathan, N
   Sandhu, R
   Au, E
TI Spike Encoding for Pattern Recognition: Comparing Cerebellum Granular
   Layer Encoding and BSA algorithms
SO 2015 INTERNATIONAL CONFERENCE ON ADVANCES IN COMPUTING, COMMUNICATIONS
   AND INFORMATICS (ICACCI)
DT Proceedings Paper
CT International Conference on Advances in Computing, Communications and
   Informatics ICACCI
CY AUG 10-13, 2015
CL SCMS Grp of Inst, Aluva, INDIA
HO SCMS Grp of Inst
DE nature-inspired computing; classification; machine learning; encoding;
   spiking neuron
ID MODEL; INFORMATION
AB Spiking neural encoding models allow classification of real world tasks to suit for brain-machine interfaces in addition to serving as internal models. We developed a new spike encoding model inspired from cerebellum granular layer and tested different classification techniques like SVM, Naive Bayes, MLP for training spiking neural networks to perform pattern recognition tasks on encoded datasets. As a precursor to spiking network-based pattern recognition, in this study, real world datasets were encoded into spike trains. The objective of this study was to encode information from datasets into spiking neuron patterns that were relevant for spiking neural networks and for conventional machine learning algorithms. In this initial study, we present a new approach similar to cerebellum granular layer encoding and compared it with BSA encoding techniques. We have also compared the efficiency of the encoded dataset with different datasets and with standard machine learning algorithms.
C1 [Medini, Chaitanya; Vijayan, Asha; Zacharia, Ritu Maria; Rajagopal, Lekshmi Priya; Nair, Bipin; Diwakar, Shyam] Amrita Univ, Amrita Vishwa Vidyapeetham, Amrita Sch Biotechnol, Clappana PO, Kollam, Kerala, India.
RP Medini, C (corresponding author), Amrita Univ, Amrita Vishwa Vidyapeetham, Amrita Sch Biotechnol, Clappana PO, Kollam, Kerala, India.
EM krishnachaitanya@am.amrita.edu; ashavijayan@am.amrita.edu;
   ritu.mariaz@gmail.com; lekshmipriya310@gmail.com; bipin@amrita.edu;
   shyam@amrita.edu
CR Albus J. S., 1975, J DYN SYST MEAS CONT
   Albus J. S., 1971, THEORY CEREBELLAR FU, V10, P25
   [Anonymous], 2012, MATL SIGN PROC TOOLB
   [Anonymous], 2005, 20 NAT C ART INT 17
   [Anonymous], 2010, THEORY SPIKE TIMING
   Brunel N, 2004, NEURON, V43, P745, DOI 10.1016/S0896-6273(04)00528-8
   Carrillo RR, 2008, BIOSYSTEMS, V94, P18, DOI 10.1016/j.biosystems.2008.05.008
   Cerminara NL, 2009, J PHYSIOL-LONDON, V587, P429, DOI 10.1113/jphysiol.2008.163337
   Clopath C, 2012, PLOS COMPUT BIOL, V8, DOI 10.1371/journal.pcbi.1002448
   Diwakar S., 2014, 6 INT C NEUR COMP TH
   Diwakar S, 2009, J NEUROPHYSIOL, V101, P519, DOI 10.1152/jn.90382.2008
   Gao Y, 2003, I IEEE EMBS C NEUR E, P189
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Han J., 2006, DATA MINING CONCEPTS
   Iakymchuk T., 2014, 19 WORLD C INT FED A, P701
   Jin X, 2010, COMPUT SCI ENG, V12, P91, DOI 10.1109/MCSE.2010.112
   Koyama S., 2011, BMC NEUROSCI S1, V12, pP177
   Lichman M., 2013, UCI MACHINE LEARNING
   Lichman M, 2013, P 3 INT C INN COMP T
   MARR D, 1969, J PHYSIOL-LONDON, V202, P437, DOI 10.1113/jphysiol.1969.sp008820
   McKennoch S, 2006, IEEE IJCNN, P3970
   Medini C., 2010, UCI MACHINE LEARNING
   Medini C., 2010, IMPROVED SPIKING NEU
   Medini C., 2013, BRAIN COMPUTER INTER, V2012
   Medini C, 2012, COMPUT INTEL NEUROSC, V2012, DOI 10.1155/2012/359529
   Memmesheimer RM, 2014, NEURON, V82, P925, DOI 10.1016/j.neuron.2014.03.026
   Mitsuo K, 2003, PROG BRAIN RES, V142, P171
   Naud R, 2008, BIOL CYBERN, V99, P335, DOI 10.1007/s00422-008-0264-7
   Nuntalid N., 2011, EVOLVING PROBABILIST, P451
   Nuntalid N, 2011, LECT NOTES COMPUT SC, V7062, P451, DOI 10.1007/978-3-642-24955-6_54
   Purves D., 2004, BIOSYSTEMS
   Rolls ET, 2011, PROG NEUROBIOL, V95, P448, DOI 10.1016/j.pneurobio.2011.08.002
   Schrauwen B, 2003, IEEE IJCNN, P2825
   Schweighofer N, 1998, EUR J NEUROSCI, V10, P95, DOI 10.1046/j.1460-9568.1998.00007.x
   The MathWorks Inc, 2012, TECHNIQUES DESIGNING
   Wolpert DM, 1998, NEURAL NETWORKS, V11, P1317, DOI 10.1016/S0893-6080(98)00066-5
NR 37
TC 4
Z9 4
U1 2
U2 2
PY 2015
BP 1619
EP 1625
UT WOS:000380475900270
DA 2023-11-16
ER

PT C
AU Villa, AEP
   Asai, Y
   Iglesias, J
   Chibirova, OK
   Cabessa, J
   Dutoit, P
   Shaposhnyk, V
AF Villa, Alessandro E. P.
   Asai, Yoshiyuki
   Iglesias, Javier
   Chibirova, Olga K.
   Cabessa, Jeremie
   Dutoit, Pierre
   Shaposhnyk, Vladyslav
BE Wang, R
   Gu, F
TI Dynamical Systems and Accurate Temporal Information Transmission in
   Neural Networks
SO ADVANCES IN COGNITIVE NEURODYNAMICS (II)
DT Proceedings Paper
CT 2nd International Conference on Cognitive Neurodynamics (ICCN 2009)
CY NOV 15-19, 2009
CL E China Univ Sci & Technol, Hangzhou, PEOPLES R CHINA
HO E China Univ Sci & Technol
ID PATTERNS
AB We simulated the activity of hierarchically organized spiking neural networks characterized by an initial developmental phase featuring cell death followed by spike timing dependent synaptic plasticity in presence of background noise. Upstream networks receiving spatiotemporally organized external inputs projected to downstream networks disconnected from external inputs. The observation of precise firing sequences, formed by recurrent patterns of spikes intervals above chance levels, suggested the build-up of an unsupervised connectivity able to sustain and preserve temporal information processing.
C1 [Villa, Alessandro E. P.] Univ Joseph Fourier, Grenoble Inst Neurosci, Neuroheurist Res Grp, Grenoble, France.
   [Villa, Alessandro E. P.] HUGE, Dept Psychiat, Sleep Res Lab, Geneva, Switzerland.
   [Villa, Alessandro E. P.] Univ Lausanne, Neuroheurist Res Grp, ISI HEC, Lausanne, Switzerland.
RP Villa, AEP (corresponding author), Univ Joseph Fourier, Grenoble Inst Neurosci, Neuroheurist Res Grp, Grenoble, France.
EM Alessandro.Villa@neuroheuristic.org
CR [Anonymous], 1991, CORTICONICS
   Asai Y, 2008, NEURAL NETWORKS, V21, P799, DOI 10.1016/j.neunet.2008.06.014
   Asai Y, 2008, J BIOL PHYS, V34, P325, DOI 10.1007/s10867-008-9093-0
   Chechik G, 1999, NEURAL COMPUT, V11, P2061, DOI 10.1162/089976699300016089
   Chibirova O, 2008, LECT NOTES COMPUT SC, V5216, P296
   Iglesias J, 2008, LECT NOTES COMPUT SC, V5164, P646, DOI 10.1007/978-3-540-87559-8_67
   Iglesias J, 2008, INT J NEURAL SYST, V18, P267, DOI 10.1142/S0129065708001580
   Innocenti GM, 2005, NAT REV NEUROSCI, V6, P955, DOI 10.1038/nrn1790
   Roberts PD, 2002, BIOL CYBERN, V87, P392, DOI 10.1007/s00422-002-0361-y
   Segundo JP, 2003, INT J BIFURCAT CHAOS, V13, P2035, DOI 10.1142/S0218127403007886
   SHATZ CJ, 1990, NEURON, V5, P745, DOI 10.1016/0896-6273(90)90333-B
   Tetko IV, 2001, J NEUROSCI METH, V105, P1, DOI 10.1016/S0165-0270(00)00336-8
   Villa AEP, 2000, CONC ADV BRAIN RES, V3, P1
NR 13
TC 0
Z9 0
U1 0
U2 1
PY 2011
BP 61
EP 65
DI 10.1007/978-90-481-9695-1_8
UT WOS:000394916700008
DA 2023-11-16
ER

PT J
AU Liu, X
   Liu, HJ
   Tang, YG
   Gao, Q
AF Liu, Xian
   Liu, Huijun
   Tang, Yinggan
   Gao, Qing
TI Fuzzy PID control of epileptiform spikes in a neural mass model
SO NONLINEAR DYNAMICS
DT Article
DE Neural mass model; Epileptiform spikes; Fuzzy PID control
ID DEEP BRAIN-STIMULATION; MATHEMATICAL-MODEL; DYNAMICS; EPILEPSY; EEG;
   MECHANISMS; RESPONSES; MEG/EEG
AB In this paper, the problem of controlling epileptiform spikes in a neural mass model is addressed. Considering the complication and nonlinearity of the neural mass model, a fuzzy PID controller is designed so that epileptiform spikes are quenched and the output waveform tracks an expected one. The tracking effect is analyzed by numerical simulation for a regular network of coupled neural populations. The effect of important model parameters on the control energy and the effect of the types of controlled populations on the ability to realize the tracking purpose are analyzed for the same network.
C1 [Liu, Xian; Liu, Huijun; Tang, Yinggan; Gao, Qing] Yanshan Univ, Key Lab Ind Comp Control Engn Hebei Prov, Inst Elect Engn, Qinhuangdao 066004, Peoples R China.
RP Liu, X (corresponding author), Yanshan Univ, Key Lab Ind Comp Control Engn Hebei Prov, Inst Elect Engn, Qinhuangdao 066004, Peoples R China.
EM liuxian@ysu.edu.cn
CR [Anonymous], FUZZY CONTROLLER DES
   [Anonymous], 1981, ELECT FIELDS BRAIN
   Benabid AL, 2007, EXPERT REV MED DEVIC, V4, P895, DOI 10.1586/17434440.4.6.895
   Çetin S, 2010, NONLINEAR DYNAM, V61, P465, DOI 10.1007/s11071-010-9662-1
   Cona F, 2011, NEUROIMAGE, V57, P1045, DOI 10.1016/j.neuroimage.2011.05.007
   David O, 2003, NEUROIMAGE, V20, P1743, DOI 10.1016/j.neuroimage.2003.07.015
   David O, 2006, NEUROIMAGE, V31, P1580, DOI 10.1016/j.neuroimage.2006.02.034
   David O, 2006, NEUROIMAGE, V30, P1255, DOI 10.1016/j.neuroimage.2005.10.045
   Ding L, 2010, NONLINEAR DYNAM, V60, P131, DOI 10.1007/s11071-009-9585-x
   Goodfellow M, 2012, NEUROIMAGE, V59, P2644, DOI 10.1016/j.neuroimage.2011.08.060
   Goodfellow M, 2011, NEUROIMAGE, V55, P920, DOI 10.1016/j.neuroimage.2010.12.074
   Guclu R, 2009, NONLINEAR DYNAM, V58, P553, DOI 10.1007/s11071-009-9500-5
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Ivancevic T, 2009, NONLINEAR DYNAM, V56, P23, DOI 10.1007/s11071-008-9376-9
   JANSEN BH, 1993, BIOL CYBERN, V68, P275, DOI 10.1007/BF00224863
   JANSEN BH, 1995, BIOL CYBERN, V73, P357, DOI 10.1007/BF00199471
   Khosravi S, 2012, NONLINEAR DYNAM, V69, P1825, DOI 10.1007/s11071-012-0389-z
   Krauss GL, 2007, ACTA NEUROCHIR SUPPL, V97, P347
   Lin CCK, 2012, J NEURAL ENG, V9, DOI 10.1088/1741-2560/9/2/026026
   Lopes da Silva F H, 1976, Prog Brain Res, V45, P281
   Lytton WW, 2008, NAT REV NEUROSCI, V9, P626, DOI 10.1038/nrn2416
   Passim K M, 2001, FUZZY CONTROL
   Pollo C, 2007, ACTA NEUROCHIR SUPPL, V97, P311
   Ren CE, 2012, NONLINEAR DYNAM, V67, P941, DOI 10.1007/s11071-011-0036-0
   Rubchinsky LL, 2012, NONLINEAR DYNAM, V68, P329, DOI 10.1007/s11071-011-0223-z
   Sen Bhattacharya B, 2011, NEURAL NETWORKS, V24, P631, DOI 10.1016/j.neunet.2011.02.009
   Shirahata T, 2011, ACTA BIOL HUNG, V62, P211, DOI 10.1556/ABiol.62.2011.2.11
   Sunderam S, 2010, EPILEPSY BEHAV, V17, P6, DOI 10.1016/j.yebeh.2009.10.017
   van Albada SJ, 2009, J THEOR BIOL, V257, P664, DOI 10.1016/j.jtbi.2008.12.013
   van Albada SJ, 2009, J THEOR BIOL, V257, P642, DOI 10.1016/j.jtbi.2008.12.018
   Wendling F, 2000, BIOL CYBERN, V83, P367, DOI 10.1007/s004220000160
   Zavaglia M., 2010, COMPUT INTELL NEUROS, V10, P1155
NR 32
TC 23
Z9 24
U1 6
U2 38
PD JAN
PY 2013
VL 71
IS 1-2
BP 13
EP 23
DI 10.1007/s11071-012-0638-1
UT WOS:000312787000002
DA 2023-11-16
ER

PT C
AU Kulkarni, SR
   Babu, AV
   Rajendran, B
AF Kulkarni, Shruti R.
   Babu, Anakha V.
   Rajendran, Bipin
GP IEEE
TI Spiking Neural Networks - Algorithms, Hardware Implementations and
   Applications
SO 2017 IEEE 60TH INTERNATIONAL MIDWEST SYMPOSIUM ON CIRCUITS AND SYSTEMS
   (MWSCAS)
SE Midwest Symposium on Circuits and Systems Conference Proceedings
DT Proceedings Paper
CT 60th IEEE International Midwest Symposium on Circuits and Systems
   (MWSCAS)
CY AUG 06-09, 2017
CL Tufts Univ, Medford Somerville Campus, Boston, MA
HO Tufts Univ, Medford Somerville Campus
ID NEURONS; DEVICE; CIRCUIT; SYNAPSE; RESUME; MODEL
AB Spiking Neural Networks (SNNs) are the third generation of artificial neural networks that closely mimic the time encoding and information processing aspects of the human brain. It has been postulated that these networks are more efficient for realizing cognitive computing systems compared to second generation networks that are widely used in machine learning algorithms today. In this paper, we review the learning algorithms, hardware demonstrations and potential applications of SNN based learning systems.
C1 [Kulkarni, Shruti R.; Babu, Anakha V.; Rajendran, Bipin] New Jersey Inst Technol, Dept Elect & Comp Engn, Newark, NJ 07102 USA.
RP Kulkarni, SR (corresponding author), New Jersey Inst Technol, Dept Elect & Comp Engn, Newark, NJ 07102 USA.
EM srk68@njit.edu; av442@njit.edu; bipin@njit.edu
CR Abbott LF, 1999, BRAIN RES BULL, V50, P303, DOI 10.1016/S0361-9230(99)00161-6
   Alibart F, 2012, ADV FUNCT MATER, V22, P609, DOI 10.1002/adfm.201101935
   Allred JM, 2016, IEEE IJCNN, P2492, DOI 10.1109/IJCNN.2016.7727509
   Ambrogio S, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00056
   [Anonymous], EL DEV M IEDM 2014 I
   Anwani N., 2015, NEUR NETW IJCNN 2015
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Boahen K, 2005, SCI AM, V292, P56, DOI 10.1038/scientificamerican0505-56
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Booij O, 2005, INFORM PROCESS LETT, V95, P552, DOI 10.1016/j.ipl.2005.05.023
   Bora Ashish, 2014, 2014 International Joint Conference on Neural Networks (IJCNN), P2079, DOI 10.1109/IJCNN.2014.6889892
   Brette R., 2012, SIMULATING SPIKING N
   Burr GW, 2008, IBM J RES DEV, V52, P449, DOI 10.1147/rd.524.0449
   Burr GW, 2017, ADV PHYS-X, V2, P89, DOI 10.1080/23746149.2016.1259585
   Covi E, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00482
   Diehl P.U., 2015, 2015 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2015.7280696
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Fidjeland Andreas K, 2010, 2010 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2010.5596678
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   Ge CJ, 2017, INFORM SCIENCES, V399, P30, DOI 10.1016/j.ins.2017.03.006
   Gehlhaar J, 2014, ACM SIGPLAN NOTICES, V49, P317, DOI 10.1145/2541940.2564710
   Hebb D. O., 2005, THE ORGANIZATION OF
   Hunsberger E., 2016, TRAINING SPIKING DEE
   Hwu T., 2017, IEEE T COGN DEV SYST, VPP, P1, DOI DOI 10.1109/ISCAS.2017.8050981
   Indiveri Giacomo, 2011, Front Neurosci, V5, P118, DOI 10.3389/fnins.2011.00118
   Jackson BL, 2013, ACM J EMERG TECH COM, V9, DOI 10.1145/2463585.2463588
   Jerry Matthew, 2016, 2016 74th Annual Device Research Conference (DRC), P1, DOI 10.1109/DRC.2016.7548503
   Jo SH, 2010, NANO LETT, V10, P1297, DOI 10.1021/nl904092h
   Kasabov NK, 2014, NEURAL NETWORKS, V52, P62, DOI 10.1016/j.neunet.2014.01.006
   Kibong Moon, 2016, 2016 International Symposium on VLSI Technology, Systems and Application (VLSI-TSA), P1, DOI 10.1109/VLSI-TSA.2016.7480499
   Kim S, 2011, 2011 11TH INTERNATIONAL CONFERENCE ON CONTROL, AUTOMATION AND SYSTEMS (ICCAS), P1
   Kraft F. M., 2006, P EPFL LATSIS S 2006, P97
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
   Kulkarni S., 2017, SUPERVISED LEARNING
   Kuzum D, 2012, NANO LETT, V12, P2179, DOI 10.1021/nl201040y
   Lazar A. A., 2005, PROCEEDINGS OF CONFE
   Lee JM, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00191
   Lee WW, 2017, IEEE T NEUR NET LEAR, V28, P849, DOI 10.1109/TNNLS.2015.2509479
   Lin J, 2016, INT EL DEVICES MEET
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Mandal S, 2014, SCI REP-UK, V4, DOI 10.1038/srep05333
   McKennoch S, 2006, IEEE IJCNN, P3970
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Mohemmed A, 2012, INT J NEURAL SYST, V22, DOI 10.1142/S0129065712500128
   Nageswaran JM, 2009, NEURAL NETWORKS, V22, P791, DOI 10.1016/j.neunet.2009.06.028
   Ostwal V., 2015, 2015 International Symposium on VLSI Technology, Systems and Applications (VLSI-TSA), P1, DOI 10.1109/VLSI-TSA.2015.7117569
   Panwar N, 2014, IEEE DEVICE RES CONF, P135, DOI 10.1109/DRC.2014.6872334
   Panzeri S, 2010, TRENDS NEUROSCI, V33, P111, DOI 10.1016/j.tins.2009.12.001
   Patterson D, 1997, IEEE MICRO, V17, P34, DOI 10.1109/40.592312
   Ponulak F., 2006, PROC EPFL LATSIS S D, P119
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Prasad C., 2015, NEUR NETW IJCNN 2015
   Purves D., 2001, NEUROSCIENCE
   Qiao N, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00141
   Rajendran B, 2007, IEEE T ELECTRON DEV, V54, P707, DOI 10.1109/TED.2007.891300
   Rajesh Manjesh B, 2016, 2016 INT C CIRCUIT P, P1, DOI DOI 10.1109/ICCPCT.2016.7530116
   Reid David, 2014, PLoS One, V9, pe103656, DOI 10.1371/journal.pone.0103656
   Rueckauer B., 2016, ARXIV161204052
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707
   Shetty C, 2015, IEEE INT SYMP CIRC S, P1905, DOI 10.1109/ISCAS.2015.7169038
   Suri M, 2011, 2011 IEEE INTERNATIONAL ELECTRON DEVICES MEETING (IEDM)
   Taherkhani A, 2015, IEEE T NEUR NET LEAR, V26, P3137, DOI 10.1109/TNNLS.2015.2404938
   Tandon P., 2016, NEURAL INFORM PROCES
   Tuma T, 2016, NAT NANOTECHNOL, V11, P693, DOI [10.1038/NNANO.2016.70, 10.1038/nnano.2016.70]
   VONNEUMANN J, 1993, IEEE ANN HIST COMPUT, V15, P28
   Wu Yonghui, 2016, GOOGLES NEURAL MACHI
   Yu QF, 2013, PLOS ONE, V8, DOI [10.1371/journal.pone.0059627, 10.1371/journal.pone.0078318]
   Yu SM, 2011, IEEE T ELECTRON DEV, V58, P2729, DOI 10.1109/TED.2011.2147791
   Zhang Chen, 2015, P 2015 ACMSIGDA INT, P161, DOI 10.1145/2684746.2689060
NR 70
TC 10
Z9 10
U1 0
U2 7
PY 2017
BP 426
EP 431
UT WOS:000424694700107
DA 2023-11-16
ER

PT J
AU Fernando, S
   Kumarasinghe, N
AF Fernando, Subha
   Kumarasinghe, Nishantha
TI Modeling honeybee communication using network of spiking neural networks
   to simulate nectar reporting behavior
SO ARTIFICIAL LIFE AND ROBOTICS
DT Article
DE Honeybee foraging; Spiking neurons; Swarm cognition; Swarm intelligence
ID TREMBLE DANCE; BEES; NEURONS
AB The paper presents the findings of the research that attempted to mathematically model the cognitive behavior that could arise due to the interaction between honeybees in a colony during forager recruitment process. The model defines a honeybee as a spiking neural network, and colony as a network of spiking neural networks. The proposed mathematical model has been evaluated by analyzing the cognitive behavior generated by the main network which represents honeybees' interaction as interactions of component networks (i.e. spiking neural networks). Accordingly, behavior of the component network, that represents an unemployed forager in the colony, was examined under different scenarios by setting networks' parameters to simulate ecological situations in the colony. The reporting of different level of quantity of nectar sources by scouts to the colony, an attempt made by a scout to attract more unemployed foragers for foraging, and influence by dancing foragers to attract other unemployed foragers for foraging are those ecological colony states that have been tested in this research. The results of all these cases have supported that the proposed mathematical model can sufficiently simulate the unemployed forager's behavior during recruitment process.
C1 [Fernando, Subha] Univ Moratuwa, Dept Computat Math, Moratuwa, Sri Lanka.
   [Kumarasinghe, Nishantha] Gen Sir John Kotelawala Def Univ, Ctr Behav Neurosci & Computat, Colombo, Sri Lanka.
RP Fernando, S (corresponding author), Univ Moratuwa, Dept Computat Math, Moratuwa, Sri Lanka.
EM subhaf@uom.lk; drkumarasinghe2015@yahoo.com
CR Abbott LF, 2000, NAT NEUROSCI, V3, P1178, DOI 10.1038/81453
   Ahmed H., 2012, 2012585 QUEENS U SCH, VK7L3N6
   [Anonymous], 1974, SOCIAL BEHAV BEES
   Bailis P, 2010, LECT NOTES COMPUTER, V6234
   Blum C, 2005, PHYS LIFE REV, V2, P353, DOI 10.1016/j.plrev.2005.10.001
   Bressan JMA, 2015, FRONT NEUROANAT, V8, DOI 10.3389/fnana.2014.00166
   Fernando S, 2014, INT J COMPUT APPL, V104, P45
   Fernando S, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P1336, DOI 10.1109/IJCNN.2011.6033379
   Gil Mariana, 2010, Commun Integr Biol, V3, P95
   Izhikevich E. M., 2007, DYNAMICAL SYSTEMS NE
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Karaboga D, 2009, ARTIF INTELL REV, V31, P61, DOI 10.1007/s10462-009-9127-4
   Krink T, SWARM INTELLIGENCE I
   Kumarasinghe, 2015, INT J COMPUT APPL, V130, P33
   Myerscough MR, 2003, P ROY SOC B-BIOL SCI, V270, P577, DOI 10.1098/rspb.2002.2293
   Passino KM, 2006, BEHAV ECOL SOCIOBIOL, V59, P427, DOI 10.1007/s00265-005-0067-y
   Perk CG, 2006, J NEUROPHYSIOL, V95, P1147, DOI 10.1152/jn.01220.2004
   Quintavalle A, 2013, BIOSCIENCES MASTER R, P1
   Seeley TD, 2000, J COMP PHYSIOL A, V186, P813, DOI 10.1007/s003590000134
   SEELEY TD, 1992, BEHAV ECOL SOCIOBIOL, V31, P375, DOI 10.1007/BF00170604
   Sinakevitch I, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0014536
   Tautz J, 2003, P NATL ACAD SCI USA, V100, P7343, DOI 10.1073/pnas.1232346100
   Thom C, 2003, J EXP BIOL, V206, P2111, DOI 10.1242/jeb.00398
   Trianni V., 2011, ADV ARTIFICIAL LIFE, P270
   Trianni V, 2011, SWARM INTELL-US, V5, P3, DOI 10.1007/s11721-010-0050-8
   Yahya H, 2007, MIRACLE HONEYBEE
NR 26
TC 0
Z9 0
U1 1
U2 3
PD JUN
PY 2018
VL 23
IS 2
BP 241
EP 248
DI 10.1007/s10015-017-0418-6
UT WOS:000444164800011
DA 2023-11-16
ER

PT J
AU Zhu, GL
   Hua, X
   Yu, GJ
   Chai, ZL
AF Zhu, Guoliang
   Hua, Xia
   Yu, Gongjian
   Chai, Zhilei
TI SNN Simulation Performance Prediction: A Nonempirical Method
SO JOURNAL OF CIRCUITS SYSTEMS AND COMPUTERS
DT Article
DE Nest simulator; spiking neural networks; performance model
ID PROCESSOR; NETWORKS; MODELS
AB As a third generation artificial neural network, spiking neuron network is expected to expand the artificial intelligence world. However, as a more detailed simulation of brain, a single run of spiking neural networks (SNNs) simulation can take hours to days. To get a better prediction of SNN simulation performance, existing work requires gathering result of actual runs to conduct accurate modeling. In this paper, we propose a nonempirical SNN simulation performance prediction method, prototyped in a hybrid CPU-FPGA cluster. Experiments show that our method, without actual simulation run, can get comparable accuracy with orders of magnitude less runtime cost.
C1 [Zhu, Guoliang; Hua, Xia; Yu, Gongjian; Chai, Zhilei] Jiangnan Univ, Wuxi 214002, Jiangsu, Peoples R China.
RP Chai, ZL (corresponding author), Jiangnan Univ, Wuxi 214002, Jiangsu, Peoples R China.
EM zlchai@jiangnan.edu.cn
CR Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   Balaji A., ARXIV200909298
   Balaji A, 2019, IEEE COMPUT ARCHIT L, V18, P149, DOI 10.1109/LCA.2019.2951507
   Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Brain, BRAIN SNN SIMULATOR
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Falsafi B., 1997, ACM Transactions on Modeling and Computer Simulation, V7, P104, DOI 10.1145/244804.244808
   Furber SB, 2013, IEEE T COMPUT, V62, P2454, DOI 10.1109/TC.2012.142
   GENESIS, GENESIS SNN SIMULATO
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   Heath T., 2005, P 10 ACM SIGPLAN S P, P186, DOI [DOI 10.1145/1065944.1065969.(AVAILABLE, 10.1145/1065944.1065969]
   Helias M, 2012, FRONT NEUROINFORM, V6, DOI 10.3389/fninf.2012.00026
   igi, CSIM SNN SIMULATOR
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Kim S., ARXIV190306530
   Kunkel S, 2012, FRONT NEUROINFORM, V5, DOI 10.3389/fninf.2011.00035
   Lee D, 2018, CONF PROC INT SYMP C, P275, DOI 10.1109/ISCA.2018.00032
   Liu J., 2009, IEEE CIRC SYST INT C, P1, DOI DOI 10.1109/CAS-ICTD.2009.4960772
   Ma D, 2017, J SYST ARCHITECT, V77, P43, DOI 10.1016/j.sysarc.2017.01.003
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Moore SW, 2012, ANN IEEE SYM FIELD P, P133, DOI 10.1109/FCCM.2012.32
   Moradi S, 2018, IEEE T BIOMED CIRC S, V12, P106, DOI 10.1109/TBCAS.2017.2759700
   Morrison A, 2007, NEURAL COMPUT, V19, P1437, DOI 10.1162/neco.2007.19.6.1437
   Nandakumar SR, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-64878-5
   nba, SNNAP SNN SIMULATOR
   Nest, NEST SNN SIMULATOR
   Neuron, NEURON SNN SIMULATOR
   Pakkenberg B, 2003, EXP GERONTOL, V38, P95, DOI 10.1016/S0531-5565(02)00151-1
   Pei J, 2019, NATURE, V572, P106, DOI 10.1038/s41586-019-1424-8
   Purves D., 2008, COGN NEUROSCI-UK
   Purves D., 2018, NEUROSCIENCE
   Qiao H, 2003, IEEE T SYST MAN CY B, V33, P925, DOI 10.1109/TSMCB.2002.804368
   Qin XL, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON PROGNOSTICS AND HEALTH MANAGEMENT (ICPHM), P1, DOI [10.1109/ICPHM.2017.7998297, 10.1109/ATNAC.2017.8215431]
   Riesenhuber M, 1999, NAT NEUROSCI, V2, P1019, DOI 10.1038/14819
   Rudolph-Lilith M, 2012, NEURAL COMPUT, V24, P1426, DOI 10.1162/NECO_a_00278
   Schenck W., 2014, SC14 C SUP NEW ORL L
   Schuller I.K., 2015, NEUROMORPHIC COMPUTI
   She XY, 2019, DES AUT TEST EUROPE, P450, DOI [10.23919/DATE.2019.8714846, 10.23919/date.2019.8714846]
   Shrestha S. B., ARXIV181008646
   Smith, 2019, KEYNOTE FCRC
   Song SH, 2020, 21ST ACM SIGPLAN/SIGBED CONFERENCE ON LANGUAGES, COMPILERS, AND TOOLS FOR EMBEDDED SYSTEMS (LCTES '20), P38, DOI 10.1145/3372799.3394364
   Tikidji-Hamburyan RA, 2017, FRONT NEUROINFORM, V11, DOI 10.3389/fninf.2017.00046
   van Albada SJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00291
   Wu JC, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351221
   Wu JB, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00836
   Yudanov D., 2010, P INT JOINT C NEUR N, P1, DOI [10.1109/IJCNN.2010.5596334, DOI 10.1109/IJCNN.2010.5596334]
NR 46
TC 0
Z9 0
U1 0
U2 4
PD JUL 15
PY 2022
VL 31
IS 10
AR 2250183
DI 10.1142/S0218126622501833
UT WOS:000818084400009
DA 2023-11-16
ER

PT J
AU Han, LX
   Huang, P
   Wang, YJ
   Zhou, Z
   Zhang, YZ
   Liu, XY
   Kang, JF
AF Han, Lixia
   Huang, Peng
   Wang, Yijiao
   Zhou, Zheng
   Zhang, Yizhou
   Liu, Xiaoyan
   Kang, Jinfeng
TI Efficient Discrete Temporal Coding Spike-Driven In-Memory Computing
   Macro for Deep Neural Network Based on Nonvolatile Memory
SO IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS I-REGULAR PAPERS
DT Article
DE Encoding; In-memory computing; Capacitors; Biological neural networks;
   Neurons; Delays; Nonvolatile memory; In-memory computing; deep neural
   network; non-volatile memory; discrete temporal coding; spike-driven
   macro
ID CHIP
AB Nonvolatile memory (NVM) based neural network can directly perform in situ computation in memory to significantly reduce energy consumption resulting from the data movement. However, the energy consumption by the analog-to-digital converter (ADC) restricts the efficiency of the mixed-signal in-memory computing macro. The rate coding spike-driven in-memory computing macro can increase the energy efficiency via eliminating the ADC, but the improvement is limited because substantial energy is consumed for the coding of multiple spikes. In this work, we propose a discrete temporal coding spike-driven in-memory computing macro, including input coding scheme, weight mapping method, and improved leaky integrate-and-fire (LIF) neuron circuit, to perform the efficient forward inference of deep neural networks based on NVM array. We then optimize the designment of the proposed in-memory computing macro to mitigate the neural network accuracy loss due to the nonlinearity of the LIF neuron and voltage drop caused by interconnect resistance. Because the temporal coding scheme reduces spike numbers and the improved-LIF circuit simultaneously integrates two bit-lines current corresponding to positive and negative weight, the proposed macro achieves 46.63TOPS/W energy efficiency and 1.92TOPS throughput for 3bit temporal coding precision.
C1 [Han, Lixia; Huang, Peng; Zhou, Zheng; Zhang, Yizhou; Liu, Xiaoyan; Kang, Jinfeng] Peking Univ, Sch Integrated Circuits, Beijing 100871, Peoples R China.
   [Wang, Yijiao] Beihang Univ, Fert Beijing Inst, Sch Integrated Circuit Sci & Engn, MIIT Key Lab Spintron, Beijing 100191, Peoples R China.
RP Huang, P (corresponding author), Peking Univ, Sch Integrated Circuits, Beijing 100871, Peoples R China.; Wang, YJ (corresponding author), Beihang Univ, Fert Beijing Inst, Sch Integrated Circuit Sci & Engn, MIIT Key Lab Spintron, Beijing 100191, Peoples R China.
EM phwang@pku.edu.cn; yijiaowang@buaa.edu.cn
CR Ambrogio S, 2019, INT EL DEVICES MEET, DOI 10.1109/iedm19573.2019.8993482
   Bo Li, 2015, 2015 Asia-Pacific Microwave Conference (APMC). Proceedings, P1, DOI 10.1109/APMC.2015.7411621
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Cai H, 2022, IEEE T CIRCUITS-I, V69, P1519, DOI 10.1109/TCSI.2022.3140769
   Cai W., 2022, SCI CHINA INFORM SCI, V65, P1
   Canziani Alfredo, 2016, ARXIV
   Chen LR, 2017, DES AUT TEST EUROPE, P19, DOI 10.23919/DATE.2017.7926952
   Chen YH, 2016, ISSCC DIG TECH PAP I, V59, P262, DOI 10.1109/ISSCC.2016.7418007
   Cheng-Xin Xue, 2020, 2020 IEEE International Solid- State Circuits Conference - (ISSCC), P244, DOI 10.1109/ISSCC19947.2020.9063078
   Emer, 2017, PROC VLSI SHORT COUR, P1
   Giacomin E, 2019, IEEE T CIRCUITS-I, V66, P643, DOI 10.1109/TCSI.2018.2872455
   Gokmen T, 2019, INT EL DEVICES MEET, DOI 10.1109/iedm19573.2019.8993573
   Guo ZX, 2021, P IEEE, V109, P1398, DOI 10.1109/JPROC.2021.3084997
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Han LX, 2021, INT RELIAB PHY SYM, DOI 10.1109/IRPS46558.2021.9405200
   Han RZ, 2019, IEEE T CIRCUITS-I, V66, P1692, DOI 10.1109/TCSI.2018.2885574
   Hao Y, 2021, SCI CHINA INFORM SCI, V64, DOI 10.1007/s11432-021-3235-7
   Horowitz M, 2014, ISSCC DIG TECH PAP I, V57, P10, DOI 10.1109/ISSCC.2014.6757323
   Huang YX, 2022, IEEE T CIRCUITS-I, V69, P518, DOI 10.1109/TCSI.2021.3124553
   JIANG H, 2018, IEEE INT SYMP CIRC S
   Joshi V, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-16108-9
   Jouppi NP, 2017, 44TH ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA 2017), P1, DOI 10.1145/3079856.3080246
   Kim W, 2019, S VLSI TECH, pT66, DOI [10.23919/vlsit.2019.8776551, 10.23919/VLSIT.2019.8776551]
   Li ZQ, 2020, INT J GEOGR INF SCI, V34, P1378, DOI 10.1080/13658816.2020.1720692
   Liu Q, 2020, ISSCC DIG TECH PAP I, P500, DOI 10.1109/ISSCC19947.2020.9062953
   Luo J, 2019, INT EL DEVICES MEET, DOI 10.1109/iedm19573.2019.8993535
   Mackin C, 2019, ADV ELECTRON MATER, V5, DOI 10.1002/aelm.201900026
   Merrikh-Bayat F, 2018, IEEE T NEUR NET LEAR, V29, P4782, DOI 10.1109/TNNLS.2017.2778940
   Mochida R, 2018, 2018 IEEE SYMPOSIUM ON VLSI TECHNOLOGY, P175, DOI 10.1109/VLSIT.2018.8510676
   Moons B, 2017, ISSCC DIG TECH PAP I, P246, DOI 10.1109/ISSCC.2017.7870353
   Park S, 2020, DES AUT CON, DOI [10.1109/dac18072.2020.9218689, 10.1007/s00779-020-01476-2]
   Si X, 2019, IEEE T CIRCUITS-I, V66, P4172, DOI 10.1109/TCSI.2019.2928043
   SOLIMAN T, 2020, IEDM, P29, DOI DOI 10.1109/IEDM13553.2020.9372124
   Tao TM, 2021, IEEE T CIRCUITS-I, V68, P1906, DOI 10.1109/TCSI.2021.3060798
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   Waldrop MM, 2016, NATURE, V530, P144, DOI 10.1038/530144a
   Xiang Y., 2019, IEDM, P38, DOI [10.1109/IEDM19573.2019.8993508, DOI 10.1109/IEDM19573.2019.8993508]
   Xiang YC, 2020, IEEE T ELECTRON DEV, V67, P2329, DOI 10.1109/TED.2020.2987439
   Xue CX, 2021, ISSCC DIG TECH PAP I, V64, P246, DOI 10.1109/ISSCC42613.2021.9365769
   Yan BN, 2019, S VLSI TECH, pT86, DOI [10.23919/vlsit.2019.8776485, 10.23919/VLSIT.2019.8776485]
   Yao P, 2020, NATURE, V577, P641, DOI 10.1038/s41586-020-1942-4
   Yin SH, 2020, IEEE J SOLID-ST CIRC, V55, P1733, DOI 10.1109/JSSC.2019.2963616
   Yu S., 2015, IEDM, P451, DOI 10.1109/IEDM.2015.7409718
   Yu SM, 2018, P IEEE, V106, P260, DOI 10.1109/JPROC.2018.2790840
   Zhang L, 2019, AAAI CONF ARTIF INTE, P1319
   Zhang SH, 2020, DES AUT TEST EUROPE, P1426, DOI 10.23919/DATE48585.2020.9116323
   Zhang XM, 2020, INT EL DEVICES MEET, DOI 10.1109/IEDM13553.2020.9371937
NR 47
TC 1
Z9 1
U1 9
U2 27
PD NOV
PY 2022
VL 69
IS 11
BP 4487
EP 4498
DI 10.1109/TCSI.2022.3194918
EA AUG 2022
UT WOS:000840476000001
DA 2023-11-16
ER

PT J
AU Kim, CS
   Kim, T
   Min, KK
   Kim, S
   Park, BG
AF Kim, Chae Soo
   Kim, Taehyung
   Min, Kyung Kyu
   Kim, Sungjun
   Park, Byung-Gook
TI 3D Integrable W/SiN<i><sub>x</sub></i>/<i>n</i>-Si/<i>p</i>-Si 1D1R
   Unipolar Resistive Random Access Memory Synapse for Suppressing Reverse
   Leakage in Spiking Neural Network
SO JOURNAL OF NANOSCIENCE AND NANOTECHNOLOGY
DT Article
DE Resistive Random Access Memory; 1 Diode-1 RRAM; Spiking Neural Network;
   3D Structure
ID DEVICES
AB In this paper, we pose reverse leakage current issue which occurs when resistive random access memory (RRAM) is used as synapse for spiking neural networks (SNNs). To prevent this problem, 1 diode-1 RRAM (1D1R) synapse is suggested and simulated to examine their current rectifying chracteristics, Furthermore, high density of 1 K 3D 1D1R synapse array structure and its process flow are proposed.
C1 [Kim, Chae Soo; Kim, Taehyung; Min, Kyung Kyu; Park, Byung-Gook] Seoul Natl Univ, Dept Elect & Comp Engn, Seoul 08826, South Korea.
   [Kim, Sungjun] Chungbuk Natl Univ, Sch Elect Engn, Cheongju 28644, South Korea.
RP Park, BG (corresponding author), Seoul Natl Univ, Dept Elect & Comp Engn, Seoul 08826, South Korea.
CR Aluguri R, 2016, IEEE J ELECTRON DEVI, V4, P294, DOI 10.1109/JEDS.2016.2594190
   Ambrogio S, 2016, IEEE T ELECTRON DEV, V63, P1508, DOI 10.1109/TED.2016.2526647
   [Anonymous], 2003, P 2003 INT S CIRC SY
   Burr GW, 2014, J VAC SCI TECHNOL B, V32, DOI 10.1116/1.4889999
   Ielmini D, 2018, MICROELECTRON ENG, V190, P44, DOI 10.1016/j.mee.2018.01.009
   Indiveri G, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00073
   Jang J, 2009, 2009 SYMPOSIUM ON VLSI TECHNOLOGY, DIGEST OF TECHNICAL PAPERS, P192
   Kim S, 2017, CURR APPL PHYS, V17, P146, DOI 10.1016/j.cap.2016.11.017
   Kuegeler C, 2009, SOLID STATE ELECTRON, V53, P1287, DOI 10.1016/j.sse.2009.09.034
   Kwon MW, 2017, J NANOSCI NANOTECHNO, V17, P3038, DOI 10.1166/jnn.2017.14025
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Park S, 2012, 2012 IEEE INTERNATIONAL ELECTRON DEVICES MEETING (IEDM)
   Pavlidis NG, 2005, IEEE IJCNN, P2190
   Querlioz D, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P1775, DOI 10.1109/IJCNN.2011.6033439
   Xin JG, 2001, IEEE IJCNN, P1772, DOI 10.1109/IJCNN.2001.938430
NR 15
TC 4
Z9 4
U1 3
U2 26
PD AUG
PY 2020
VL 20
IS 8
BP 4735
EP 4739
DI 10.1166/jnn.2020.17806
UT WOS:000518698800023
DA 2023-11-16
ER

PT J
AU Vaila, R
   Chiasson, J
   Saxena, V
AF Vaila, Ruthvik
   Chiasson, John
   Saxena, Vishal
TI A Deep Unsupervised Feature Learning Spiking Neural Network With
   Binarized Classification Layers for the EMNIST Classification
SO IEEE TRANSACTIONS ON EMERGING TOPICS IN COMPUTATIONAL INTELLIGENCE
DT Article
DE Neurons; Feature extraction; Backpropagation; Biological neural
   networks; Synapses; Encoding; Membrane potentials; STDP; Spiking
   Networks; Surrogate Gradients; EMNIST; Binary Activations; Reduced
   Multiplications
AB End user AI is trained on large server farms with data collected from the users. With ever increasing demand for IoT devices, there is a need for deep learning approaches that can be implemented at the Edge in an energy efficient manner. In this work we approach this using spiking neural networks. The unsupervised learning technique of spike timing dependent plasticity (STDP) and binary activations are used to extract features from spiking input data. Gradient descent (backpropagation) is used only on the output layer to perform training for classification. The accuracies obtained for the balanced EMNIST data set compare favorably with other approaches. The effect of the stochastic gradient descent (SGD) approximations on learning capabilities of our network are also explored.
C1 [Vaila, Ruthvik] Boise State Univ, Dept Elect & Comp Engn, Boise, ID 83706 USA.
   [Chiasson, John] Boise State Univ, ECE, Boise, ID 83706 USA.
   [Saxena, Vishal] Univ Delaware, ECE, Newark, DE 19716 USA.
RP Vaila, R (corresponding author), Boise State Univ, Dept Elect & Comp Engn, Boise, ID 83706 USA.
EM ruthvikvaila@u.boisestate.edu; JohnChiasson@boisestate.edu;
   vsaxena@udel.edu
CR [Anonymous], 2015, ARXIV PREPRINT ARXIV
   Anwani N., 2015, P INT JOINT C NEUR N, P1, DOI DOI 10.1109/IJCNN.2015
   Baldominos A, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9153169
   Bekolay T, 2014, FRONT NEUROINFORM, V7, DOI 10.3389/fninf.2013.00048
   Bichler O., 2019, IEEE IJCNN
   Cohen G, 2017, IEEE IJCNN, P2921, DOI 10.1109/IJCNN.2017.7966217
   Comsa I. M., 2019, ARXIV190713223
   Conradt Jorg, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P780, DOI 10.1109/ICCVW.2009.5457625
   Delorme A, 2001, NEUROCOMPUTING, V38, P539, DOI 10.1016/S0925-2312(01)00403-9
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Frenkel C., 2019, LEARNING FEEDBACK DI
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   Gardner B, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0161335
   GROSSBERG S, 1987, COGNITIVE SCI, V11, P23, DOI 10.1111/j.1551-6708.1987.tb00862.x
   Hines M., 2013, NEURON SIMUL ENV, P1
   Hubara I, 2018, J MACH LEARN RES, V18
   Jaeger Dieter, ENCY COMPUTATIONAL N, DOI [10.1007/978-1-4614-7320-6_795-1, DOI 10.1007/978-1-4614-7320-6_795-1]
   Jiao X, 2018, DES AUT TEST EUROPE, P1223, DOI 10.23919/DATE.2018.8342202
   Jin Y., 2018, ADV NEURAL INFORM PR, P7005, DOI DOI 10.48550/ARXIV.1805.07866
   Kheradpisheh S. R, COMMUNICATION
   Kheradpisheh S. Reza, 2019, ARXIV191009495
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Kheradpisheh SR, 2016, NEUROCOMPUTING, V205, P382, DOI 10.1016/j.neucom.2016.04.029
   Kiselev M, 2016, IEEE IJCNN, P1355, DOI 10.1109/IJCNN.2016.7727355
   Krizhevsky A., 2012, THE CIFAR 10 DATASET
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y., 1998, MNIST DATABASE HANDW
   Lee C, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00435
   Lee EH, 2017, INT CONF ACOUST SPEE, P5900, DOI 10.1109/ICASSP.2017.7953288
   Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508
   Lillicrap TP, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms13276
   Liu JQ, 2018, 2018 51ST ANNUAL IEEE/ACM INTERNATIONAL SYMPOSIUM ON MICROARCHITECTURE (MICRO), P655, DOI [10.1109/MICR0.2018.00059, 10.1109/MICRO.2018.00059]
   Masquelier T, 2017, THESIS U TOULOUSE 3
   Masquelier T, 2008, PLOS ONE, V3, DOI 10.1371/journal.pone.0001377
   Mostafa Hesham, 2018, IEEE Trans Neural Netw Learn Syst, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Mozafari M, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00625
   Neftci EO, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00324
   Nielsen M. A, 2015, NEURAL NETW DEEP LEA
   Panda P., 2019, SCALABLE EFFICIENT A
   Posch C, 2010, IEEE INT SYMP CIRC S, P1392, DOI 10.1109/ISCAS.2010.5537265
   Rocke A., 2017, PAULISPACE JUN
   Rouhani BD, 2016, I SYMPOS LOW POWER E, P112, DOI 10.1145/2934583.2934599
   Rueckauer B., 2016, ARXIV161204052
   Sakemi Y, 2020, SUPERVISED LEARNING
   Saxena Vishal, 2018, Journal of Low Power Electronics and Applications, V8, DOI 10.3390/jlpea8040034
   Shawon A., 2018, 2018 INT C BANGL, V2018, P1, DOI 10.1109/ICBSLP.2018.8554900
   Tavanaei A, 2018, 2018 INT JOINT C NEU, DOI DOI 10.1109/IJCNN.2018.8489104
   Vaila R., 2019, NICE WORKSHOP SERIES
   Vaila R., 2020, ARXIV200211843
   Vaila R., 2020, ARXIV200211044
   Vaila R., 2019, ARXIV PREPRINT ARXIV
   Vaila R., 2019, P INT C NEUR SYST, P1
   Whittington JCR, 2019, TRENDS COGN SCI, V23, P235, DOI 10.1016/j.tics.2018.12.005
   Wu XY, 2019, IEEE T NANOTECHNOL, V18, P149, DOI 10.1109/TNANO.2018.2871680
   Wu XY, 2015, IEEE T CIRCUITS-II, V62, P1088, DOI 10.1109/TCSII.2015.2456372
   Wu XY, 2015, IEEE J EM SEL TOP C, V5, P254, DOI 10.1109/JETCAS.2015.2433552
NR 56
TC 12
Z9 12
U1 2
U2 21
PD FEB
PY 2022
VL 6
IS 1
BP 124
EP 135
DI 10.1109/TETCI.2020.3035164
UT WOS:000745512200016
DA 2023-11-16
ER

PT C
AU Wang, F
   Severa, WM
   Rothganger, F
AF Wang, Felix
   Severa, William M.
   Rothganger, Fred
GP ACM
TI Acquisition and Representation of Spatio-Temporal Signals in
   Polychronizing Spiking Neural Networks
SO PROCEEDINGS OF THE 2019 7TH ANNUAL NEURO-INSPIRED COMPUTATIONAL ELEMENTS
   WORKSHOP (NICE 2019)
DT Proceedings Paper
CT 7th Annual Neuro-Inspired Computational Elements Workshop (NICE)
CY MAR 26-28, 2019
CL Albany, NY
DE Spiking neural networks; polychronization; signal processing;
   spatio-temporal coding; internal model
ID RECOGNITION
AB The ability of an intelligent agent to process complex signals such as those found in audio or video depends heavily on the nature of the internal representation of the relevant information. This work explores the mechanisms underlying this process by investigating theories inspired by the function of the neocortex. In particular, we focus on the phenomenon of polychronization, which describes the self-organization in a spiking neural network resulting from the interplay between network structure, driven spiking activity, and synaptic plasticity. What emerges are groups of neurons that exhibit reproducible, time-locked patterns of spiking activity. We propose that this representation is well suited to spatio-temporal signal processing, as it naturally resembles patterns found in real-world signals. We explore the computational properties of this approach and demonstrate the ability of a simple polychronizing network to learn different spatio-temporal signals.
C1 [Wang, Felix; Severa, William M.; Rothganger, Fred] Sandia Natl Labs, POB 5800, Albuquerque, NM 87185 USA.
RP Wang, F (corresponding author), Sandia Natl Labs, POB 5800, Albuquerque, NM 87185 USA.
EM felwang@sandia.gov; wmsever@sandia.gov; frothga@sandia.gov
CR Barlow H, 2001, NETWORK-COMP NEURAL, V12, P241, DOI 10.1088/0954-898X/12/3/301
   Braitenberg V., 1991, ANATOMY CORTEX STAT
   Conant RC., 1970, INT J SYST SCI, V1, P89, DOI [DOI 10.1080/00207727008920220, 10.1080/00207727008920220]
   DAVIS SB, 1980, IEEE T ACOUST SPEECH, V28, P357, DOI 10.1109/TASSP.1980.1163420
   Edelman G. M., 1987, NEURAL DARWINISM THE
   Garofolo JS, 1993, TIMIT ACOUSTIC PHONE, DOI DOI 10.35111/17GK-BN40
   Guise M, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00009
   HULL JJ, 1994, IEEE T PATTERN ANAL, V16, P550, DOI 10.1109/34.291440
   Izhikevich EM, 2004, CEREB CORTEX, V14, P933, DOI 10.1093/cercor/bhh053
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   LEE KF, 1989, IEEE T ACOUST SPEECH, V37, P1641, DOI 10.1109/29.46546
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   MILLER GA, 1956, PSYCHOL REV, V63, P81, DOI 10.1037/h0043158
   Moore B. C. J., 2012, INTRO PSYCHOL HEARIN
   Morris RGM, 1999, BRAIN RES BULL, V50, P437, DOI 10.1016/S0361-9230(99)00182-3
   Paugam-Moisy H, 2008, NEUROCOMPUTING, V71, P1143, DOI 10.1016/j.neucom.2007.12.027
   Prut Y, 1998, J NEUROPHYSIOL, V79, P2857, DOI 10.1152/jn.1998.79.6.2857
   Rolston JD, 2007, NEUROSCIENCE, V148, P294, DOI 10.1016/j.neuroscience.2007.05.025
   Szatmáry B, 2010, PLOS COMPUT BIOL, V6, DOI 10.1371/journal.pcbi.1000879
   Tetzlaff C, 2012, BIOL CYBERN, V106, P715, DOI 10.1007/s00422-012-0529-z
   Wang F, 2015, PROCEDIA COMPUT SCI, V61, P322, DOI 10.1016/j.procs.2015.09.149
NR 22
TC 0
Z9 0
U1 0
U2 1
PY 2020
DI 10.1145/3320288.3320291
UT WOS:000850472000011
DA 2023-11-16
ER

PT C
AU Rocke, P
   McGinley, B
   Morgan, F
   Maher, J
AF Rocke, Patrick
   McGinley, Brian
   Morgan, Fearghal
   Maher, John
BE Diniz, PC
   Marques, E
   Bertels, K
   Fernandes, MM
   Cardoso, JMP
TI Reconfigurable hardware evolution platform for a spiking neural network
   robotics controller
SO RECONFIGURABLE COMPUTING: ARCHITECTURES, TOOLS AND APPLICATIONS
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 3rd International Workshop on Applied Reconfigurable Computing
CY MAR 27-29, 2007
CL Mangaratiba, BRAZIL
DE reconfigurable hardware; FPAA; genetic algorithm; spiking neural
   networks; evolutionary computation
AB This paper describes a platform for the hardware evolution of Spiking Neural Network (SNN) based robotics controllers on multiple Field Programmable Analogue Arrays (FPAAs). The SNN robotics controller, evolved using a GA, performs obstacle avoidance and navigation. A robotics simulator is used to evaluate the performance of the evolved hardware SNN. Simulated sonar data is input to FPAA neurons and the SNN returns motor control data to the simulator. Initial results indicate the emergence of effective navigation behavior.
C1 [Rocke, Patrick; McGinley, Brian; Morgan, Fearghal; Maher, John] NUI Galway, Dept Elect Engn, Res Grp, BIRC, Galway, Ireland.
RP Rocke, P (corresponding author), NUI Galway, Dept Elect Engn, Res Grp, BIRC, Galway, Ireland.
EM patrick.rocke@nuigalway.ie; brian.mcginley@nuigalway.ie;
   fearghal.morgan@nuigalway.ie; john.maher@nuigalway.ie
CR Bellis S, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON FIELD-PROGRAMMABLE TECHNOLOGY, PROCEEDINGS, P449, DOI 10.1109/FPT.2004.1393322
   BERENSON D, UNPUB 2005 NASADOD C
   FLOREANO D, EVOLUTIONARY BITS SP
   FLOREANO D, EVOLVING NEURAL NEUR
   Gerstner W., 2002, SPIKING NEURON MODEL
   Haykin S., 1994, NEURAL NETWORKS COMP
   Holland JH., 1975, ADAPTATION NATURAL A
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   MAHER J, 2006, IEEE S FIELD PROGR C
   MCGINLEY B, 2005, WORKSH LIF PERC SYST
   NOLFT S, 1994, 4 INT WORKSH ART LIF
   ROCKE P, 2005, 2005 INT C REC COMP, P11
   Terry MA, 2006, LECT NOTES COMPUT SC, V3907, P332
   UPEGUI C, METHODOLGY EVOLVING
   URZELAI J, EVOLUTION ADAPTIVE S
   XINYAO, 1999, P IEEE, V87
NR 16
TC 9
Z9 9
U1 0
U2 3
PY 2007
VL 4419
BP 373
EP +
UT WOS:000245913900036
DA 2023-11-16
ER

PT J
AU Stöckel, A
   Jenzen, C
   Thies, M
   Rückert, U
AF Stoeckel, Andreas
   Jenzen, Christoph
   Thies, Michael
   Rueckert, Ulrich
TI Binary Associative Memories as a Benchmark for Spiking Neuromorphic
   Hardware
SO FRONTIERS IN COMPUTATIONAL NEUROSCIENCE
DT Article
DE neuromorphic hardware; spiking neural networks; benchmark; associative
   memory
ID NETWORK; MODELS; GENERATION; CELLS
AB Large-scale neuromorphic hardware platforms, specialized computer systems for energy efficient simulation of spiking neural networks, are being developed around the world, for example as part of the European Human Brain Project (HBP). Due to conceptual differences, a universal performance analysis of these systems in terms of runtime, accuracy and energy efficiency is non-trivial, yet indispensable for further hard-and software development. In this paper we describe a scalable benchmark based on a spiking neural network implementation of the binary neural associative memory. We treat neuromorphic hardware and software simulators as black-boxes and execute exactly the same network description across all devices. Experiments on the HBP platforms under varying configurations of the associative memory show that the presented method allows to test the quality of the neuron model implementation, and to explain significant deviations from the expected reference output.
C1 [Stoeckel, Andreas; Jenzen, Christoph; Thies, Michael; Rueckert, Ulrich] Bielefeld Univ, Fac Technol, Cognitron & Sensor Syst, Cluster Excellence Cognit Interact Technol, Bielefeld, Germany.
   [Stoeckel, Andreas] Univ Waterloo, Ctr Theoret Neurosci, Waterloo, ON, Canada.
RP Stöckel, A; Jenzen, C (corresponding author), Bielefeld Univ, Fac Technol, Cognitron & Sensor Syst, Cluster Excellence Cognit Interact Technol, Bielefeld, Germany.
EM astoecke@uwaterloo.ca; cjenzen@techfak.uni-bielefeld.de
CR [Anonymous], FRONT NEUR C NEUR 20
   [Anonymous], 2007, SCHOLARPEDIA, DOI DOI 10.4249/SCH0LARPEDIA.1903
   [Anonymous], THESIS
   [Anonymous], 1991, NEURONAL NETWORKS HI
   [Anonymous], 1961, KYBERNETIK
   [Anonymous], THESIS
   [Anonymous], 2015, 2015 INT JOINT C NEU, DOI DOI 10.1109/IJCNN.2015.7280625
   [Anonymous], 2017, ARXIV170301909
   [Anonymous], THESIS
   [Anonymous], THESIS
   [Anonymous], ASS MEMORY SYSTEM TH
   BHALLA US, 1993, J NEUROPHYSIOL, V69, P1948, DOI 10.1152/jn.1993.69.6.1948
   Bill J, 2010, FRONT COMPUT NEUROSC, V4, DOI 10.3389/fncom.2010.00129
   Braitenberg V., 1998, CORTEX STAT GEOMETRY, DOI 10.1007/978-3-662-03733-1
   Brette R, 2005, J NEUROPHYSIOL, V94, P3637, DOI 10.1152/jn.00686.2005
   BRILLINGER DR, 1988, BIOL CYBERN, V59, P189, DOI 10.1007/BF00318010
   Brüderle D, 2011, BIOL CYBERN, V104, P263, DOI 10.1007/s00422-011-0435-9
   Davison Andrew P, 2008, Front Neuroinform, V2, P11, DOI 10.3389/neuro.11.011.2008
   Diamond A, 2016, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00491
   Ehrlich M, 2010, ARTIFICIAL NEURAL NETWORKS AND INTELLIGENT INFORMATION PROCESSING, P43
   Feng WC, 2007, COMPUTER, V40, P50, DOI 10.1109/MC.2007.445
   Fieres J, 2008, IEEE IJCNN, P969, DOI 10.1109/IJCNN.2008.4633916
   Friedmann S, 2017, IEEE T BIOMED CIRC S, V11, P128, DOI 10.1109/TBCAS.2016.2579164
   Furber S., 2010, 2010 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2010.5596364
   Furber SB, 2013, IEEE T COMPUT, V62, P2454, DOI 10.1109/TC.2012.142
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   Gerstner W, 2018, SCHOLARPEDIA, V3, P1343
   Gray CM, 1996, SCIENCE, V274, P109, DOI 10.1126/science.274.5284.109
   Hasler J, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00118
   Hebb D.O., 1949, ORG BEHAV NEUROPSYCH
   HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554
   Lansner A, 2009, TRENDS NEUROSCI, V32, P178, DOI 10.1016/j.tins.2008.12.002
   Marc-Oliver G., 2007, SCHOLARPEDIA, DOI [DOI 10.4249/SCHOLARPEDIA.1430, 10.4249/scholarpedia.1430]
   Markram H, 2012, SCI AM, V306, P50, DOI 10.1038/scientificamerican0612-50
   McCulloch Warren S., 1943, BULL MATH BIOPHYS, V5, P115, DOI 10.1007/BF02478259
   Mundy A., 2015, P INT JOINT C NEUR N, P1, DOI DOI 10.1109/IJCNN.2015.7280390
   NELDER JA, 1965, COMPUT J, V7, P308, DOI 10.1093/comjnl/7.4.308
   Painkras E, 2013, IEEE J SOLID-ST CIRC, V48, P1943, DOI 10.1109/JSSC.2013.2259038
   PALM G, 1980, BIOL CYBERN, V36, P19, DOI 10.1007/BF00337019
   Palm G, 2013, NEURAL NETWORKS, V37, P163, DOI 10.1016/j.neunet.2012.08.013
   Petrovici MA, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0108590
   Pfeil T, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00011
   RUECKERT U, 1991, ARTIFICIAL NEURAL NETWORKS, VOLS 1 AND 2, P1195
   Schemmel J, 2010, IEEE INT SYMP CIRC S, P1947, DOI 10.1109/ISCAS.2010.5536970
   Schmuker M, 2014, P NATL ACAD SCI USA, V111, P2081, DOI 10.1073/pnas.1303053111
   Scholze S, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00117
   Schwenker F, 1996, NEURAL NETWORKS, V9, P445, DOI 10.1016/0893-6080(95)00112-3
   Sharp T., 2013, 2013 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2013.6706988
   Stromatias E., 2013, 2013 INT JOINT C NEU, P1, DOI [DOI 10.1109/IJCNN.2013.6706927, 10.1109/ijcnn.2013.6706927]
   Teukolsky S. A., 2007, NUMERICAL RECIPES AR
   Thanasoulis V, 2014, IEEE INT SYMP CIRC S, P265, DOI 10.1109/ISCAS.2014.6865116
   WILLSHAW DJ, 1969, NATURE, V222, P960, DOI 10.1038/222960a0
   Yavuz E, 2016, SCI REP-UK, V6, DOI 10.1038/srep18854
NR 53
TC 2
Z9 2
U1 0
U2 3
PD AUG 22
PY 2017
VL 11
AR 71
DI 10.3389/fncom.2017.00071
UT WOS:000409014200001
DA 2023-11-16
ER

PT J
AU Zhang, SQ
   Zhang, ZY
   Zhou, ZH
AF Zhang, Shao-Qun
   Zhang, Zhao-Yu
   Zhou, Zhi-Hua
TI Bifurcation Spiking Neural Network
SO JOURNAL OF MACHINE LEARNING RESEARCH
DT Article
DE Spiking Neural Network; Leaky Integrate-and-Fire model; Control Rates;
   Eigenvalues; Bifurcation Dynamical System
ID MODEL
AB Spiking neural networks (SNNs) have attracted much attention due to their great potential for modeling time-dependent signals. The performance of SNNs depends not only on picking an apposite architecture and searching optimal connection weights as well as conventional deep neural networks, but also on the careful tuning of many hyper-parameters within fundamental spiking neural models. However, so far, there has been less systematic work on analyzing SNNs' dynamical characteristics, especially ones relative to these internal hyper-parameters, which leads to whether SNNs are adequate for modeling actual data relies on fortune. In this work, we provide a theoretical framework for investigating spiking neural models from the perspective of dynamical systems. As a result, we point out that the LIF model with control rate hyper-parameters is a bifurcation dynamical system. This point explains why the performance of SNNs is so sensitive to the setting of control rate hyper-parameters, leading to a recommendation that diverse and adaptive eigenvalues are beneficial to improve the performance of SNNs. Inspired by this insight, we develop the Bifurcation Spiking Neural Network (BSNN) with supervised implementation, and theoretically show that BSNN is an adaptive dynamical system. Experiments validate the effectiveness of BSNN on several benchmark data sets, showing that BSNN achieves superior performance to existing SNNs and is robust to the setting of control rates.
C1 [Zhang, Shao-Qun; Zhang, Zhao-Yu; Zhou, Zhi-Hua] Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing 210023, Peoples R China.
RP Zhang, SQ (corresponding author), Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing 210023, Peoples R China.
EM ZHANGSQ@LAMDA.NJU.EDU.CN; ZHANGZHAOYU@LAMDA.NJU.EDU.CN;
   ZHOUZH@LAMDA.NJU.EDU.CN
CR Anumula J, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00023
   Beck A, 2013, SIAM J OPTIMIZ, V23, P2037, DOI 10.1137/120887679
   Bergstra J, 2012, J MACH LEARN RES, V13, P281
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Burkitt AN, 2006, BIOL CYBERN, V95, P97, DOI 10.1007/s00422-006-0082-8
   Burkitt AN, 2006, BIOL CYBERN, V95, P1, DOI 10.1007/s00422-006-0068-6
   Carlson KD, 2014, FRONT NEUROSCI-SWITZ, V8, DOI 10.3389/fnins.2014.00010
   Chou C.-N., 2019, P 10 INN THEOR COMP
   Cohen G, 2017, IEEE IJCNN, P2921, DOI 10.1109/IJCNN.2017.7966217
   Cohen GK, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00184
   Dumont G, 2017, PLOS COMPUT BIOL, V13, DOI 10.1371/journal.pcbi.1005691
   Eldan R., 2016, C LEARN THEOR, P907
   GERSTNER W, 1995, PHYS REV E, V51, P738, DOI 10.1103/PhysRevE.51.738
   Gerstner W., 2002, SPIKING NEURON MODEL
   Hirsch MW, 2012, DIFF EQUAT+
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Hohn N, 2001, PHYS REV E, V63, DOI 10.1103/PhysRevE.63.031902
   Huh D, 2018, ADV NEUR IN, V31
   Hunsberger E., 2015, ARXIV PREPRINT ARXIV
   Hunter D, 2012, IEEE T IND INFORM, V8, P228, DOI 10.1109/TII.2012.2187914
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Jin Y., 2018, ADV NEURAL INFORM PR, P7005, DOI DOI 10.48550/ARXIV.1805.07866
   Kulkarni SR, 2018, NEURAL NETWORKS, V103, P118, DOI 10.1016/j.neunet.2018.03.019
   Kuznetsov Y A, 2013, ELEMENTS APPL BIFURC
   Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508
   Lobo JL, 2020, NEURAL NETWORKS, V121, P88, DOI 10.1016/j.neunet.2019.09.004
   Lorenzo PR, 2017, PROCEEDINGS OF THE 2017 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE (GECCO'17), P481, DOI 10.1145/3071178.3071208
   Lotfi Rezaabad A., 2020, INT C NEUR SYST, P1, DOI DOI 10.1145/3407197.3407211
   McKennoch S, 2006, IEEE IJCNN, P3970
   Neftci EO, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00324
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   OConnor P., 2016, ARXIV160208323
   OConnor Peter, 2019, 22 INT C ARTIFICIAL, V89
   Onuki A., 2002, PHASE TRANSITION DYN
   Orchard G, 2015, FRONT NEUROSCI-SWITZ, V9, DOI [10.3389/fhins.2015.00437, 10.3389/fnins.2015.00437]
   Pillow JW, 2005, J NEUROSCI, V25, P11003, DOI 10.1523/JNEUROSCI.3305-05.2005
   Quiroga RQ, 2005, NATURE, V435, P1102, DOI 10.1038/nature03687
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Shimokawa T, 1999, PHYS REV E, V59, P3427, DOI 10.1103/PhysRevE.59.3427
   Shrestha SB, 2018, ADV NEUR IN, V31
   Snoek J., 2012, ADV NEURAL INFORM PR, V25, P1
   VanRullen R, 2005, TRENDS NEUROSCI, V28, P1, DOI 10.1016/j.tins.2004.10.010
   Weinberger, 2013, ADV NEURAL INFORM PR, P1538
   Wu J.-H., 2021, ARXIV211106027
   Yu Q, 2014, NEUROCOMPUTING, V138, P3, DOI 10.1016/j.neucom.2013.06.052
   Zhang S.-Q., 2021, ARXIV211106222
   Zhang SQ, 2021, NEURAL COMPUT, V33, P2951, DOI 10.1162/neco_a_01431
   Zhang SQ, 2020, FRONT ARTIF INTEL AP, V325, P1714, DOI 10.3233/FAIA200284
   Zhang WR, 2019, ADV NEUR IN, V32
   Zhou ZH, 2021, SCI CHINA INFORM SCI, V64, DOI 10.1007/s11432-020-2885-6
NR 50
TC 3
Z9 3
U1 1
U2 5
PY 2021
VL 22
BP 1
EP 21
UT WOS:000726629200001
DA 2023-11-16
ER

PT C
AU Allen, JN
   Abdel-Aty-Zohdy, HS
   Ewing, RL
AF Allen, JN
   Abdel-Aty-Zohdy, HS
   Ewing, RL
GP ieee
TI Electronic nose inhibition in a spiking neural network for noise
   cancellation
SO PROCEEDINGS OF THE 2004 IEEE SYMPOSIUM ON COMPUTATIONAL INTELLIGENCE IN
   BIOINFORMATICS AND COMPUTATIONAL BIOLOGY
DT Proceedings Paper
CT IEEE Symposium on Computational Intelligence in Bioinformatics and
   Computational Biology
CY OCT 07-08, 2004
CL La Jolla, CA
AB An olfaction detection spiking neural network that detects binary odor patterns is analyzed and implemented. This paper presents a new method for inhibiting spiking neural networks by modulating a detection threshold. Interference noise from active odors is measured by a single inhibitory neuron. The inhibition neuron changes the detection threshold to create tolerance for a system with multiple odors present. A digital implementation of the inhibition is simulated. Comparative results prove that threshold modulation reduces false-positive detection error in high noise scenarios where fifteen odors are active simultaneously.
C1 Oakland Univ, Microelect Syst Design Lab, Rochester, MN USA.
RP Allen, JN (corresponding author), Oakland Univ, Microelect Syst Design Lab, Rochester, MN USA.
CR ABDELATYZOHDY HS, 1997, P 40 IEEE INT MIDW S
   ALLEN J, 2002, P 45 IEEE INT MIDW S
   BUCK L, 1996, CELL, V100, P611
   Dutta R, 2003, NEURAL NETWORKS, V16, P847, DOI 10.1016/S0893-6080(03)00092-3
   FUSI S, 1999, THESIS JERUSALEM U J
   Kermani BG, 1999, IEEE T BIO-MED ENG, V46, P429, DOI 10.1109/10.752940
   ROCHEL O, 2002, EUROSENSORS
   YOUSSIF R, 2002, P 45 IEEE INT MIDW S
NR 8
TC 5
Z9 5
U1 0
U2 1
PY 2004
BP 129
EP 133
UT WOS:000225765700020
DA 2023-11-16
ER

PT C
AU Hulea, M
   Caruntu, CF
AF Hulea, Mircea
   Caruntu, Constantin Florin
GP IEEE
TI Spiking neural network for controlling the artificial muscles of a
   humanoid robotic arm
SO 2014 18TH INTERNATIONAL CONFERENCE SYSTEM THEORY, CONTROL AND COMPUTING
   (ICSTCC)
SE International Conference on System Theory Control and Computing
DT Proceedings Paper
CT 18th International Conference on System Theory, Control and Computing
   (ICSTCC)
CY OCT 17-19, 2014
CL Sinaia, ROMANIA
DE human-like robotic arm; spiking neural networks; analogue bio-inspired
   neuron; flexinol actuator wires
ID MOTOR; CIRCUITS; LOCOMOTION; MOVEMENT
AB One of the main functions of the human nervous system is the muscles control. The complexity of this function increases for hand and fingers control because of the high diversity and accuracy of the possible motions. Starting from the control mechanisms of the natural muscle we developed a structure of spiking neurons implemented in PCB hardware that is able to drive the elbow of a human-like robotic arm. In order to increase the biological plausibility of the designed robotic arm driving system, the artificial neural network controls artificial muscles that are implemented with Flexinol actuator wires. From our knowledge the control of the actuator wires using spiking neural networks was not performed previously.
   The results show that the excitatory spiking neurons are able to flex and straighten the artificial elbow by stimulating two antagonistic actuator wires. Moreover, by using inhibitory neurons that modulate the neural excitatory activity the arm mobile segment can be driven to specific positions.
   Our final goal is the development of a biologically plausible neural structure that is able to easily drive the hand and fingers of a human-like robotic arm by controlling the artificial muscles implemented with actuator wires. This will give important clues for future developments of spiking neural networks that drive robotic arms for performing complex tasks.
C1 [Hulea, Mircea] Gheorghe Asachi Tech Univ Iasi, Dept Comp Sci & Engn, Fac Automat Control & Comp Engn, Iasi, Romania.
   [Caruntu, Constantin Florin] Gheorghe Asachi Tech Univ Iasi, Dept Automat Control & Appl Informat, Fac Automat Control & Comp Engn, Iasi, Romania.
RP Hulea, M (corresponding author), Gheorghe Asachi Tech Univ Iasi, Dept Comp Sci & Engn, Fac Automat Control & Comp Engn, Iasi, Romania.
EM mhulea@tuiasi.ro; caruntuc@ac.tuiasi.ro
CR Bouganis A., 2010, P 2010 INT JOINT C N, P1, DOI DOI 10.1109/IJCNN.2010.5596525
   CHEVALIER G, 1990, TRENDS NEUROSCI, V13, P277, DOI 10.1016/0166-2236(90)90109-N
   DeLong MR, 2007, ARCH NEUROL-CHICAGO, V64, P20, DOI 10.1001/archneur.64.1.20
   Fagg AH, 2009, IEEE T NEUR SYS REH, V17, P487, DOI 10.1109/TNSRE.2009.2029313
   Franklin G. F., 1994, FEEDBACK CONTROL DYN, V3rd
   Gamez D, 2012, BIOINSPIR BIOMIM, V7, DOI 10.1088/1748-3182/7/2/025008
   Goulding M, 2009, NAT REV NEUROSCI, V10, P507, DOI 10.1038/nrn2608
   Grillner S, 2003, NAT REV NEUROSCI, V4, P573, DOI 10.1038/nrn1137
   Grillner S., 1990, SELECTION INITIATION
   Grillner S, 2008, BRAIN RES REV, V57, P2, DOI 10.1016/j.brainresrev.2007.06.027
   Guertin PA, 2009, BRAIN RES REV, V62, P45, DOI 10.1016/j.brainresrev.2009.08.002
   Hulea M., 2011, P 15 C SYST THEOR CO, P282
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Joshi P, 2005, NEURAL COMPUT, V17, P1715, DOI 10.1162/0899766054026684
   Leisman G, 2013, REV NEUROSCIENCE, V24, P9, DOI 10.1515/revneuro-2012-0067
   Nakata Y, 2012, IEEE INT CONF ROBOT, P3153, DOI 10.1109/ICRA.2012.6225362
   Roggen D, 2003, 2003 NASA/DOD CONFERENCE ON EVOLVABLE HARDWARE, P189
   Takakusaki K, 2004, NEUROSCI RES, V50, P137, DOI 10.1016/j.neures.2004.06.015
   Teh Y. H., 2008, THESIS AUSTR NATL U
   Wang Xu, COORDINATED HUNTING
   Zehr EP, 2004, NEUROSCIENTIST, V10, P347, DOI 10.1177/1073858404264680
NR 21
TC 9
Z9 10
U1 0
U2 4
PY 2014
BP 163
EP 168
UT WOS:000704338900029
DA 2023-11-16
ER

PT C
AU Gardner, B
   Grüning, A
AF Gardner, Brian
   Gruening, Andre
BE Mladenov, V
   KoprinkovaHristova, P
   Palm, G
   Villa, AEP
   Appollini, B
   Kasabov, N
TI Learning Temporally Precise Spiking Patterns through Reward Modulated
   Spike-Timing-Dependent Plasticity
SO ARTIFICIAL NEURAL NETWORKS AND MACHINE LEARNING - ICANN 2013
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 23rd International Conference on Artificial Neural Networks (ICANN)
CY SEP 10-OCT 13, 2013
CL Techn Univ Sofia, Sofia, BULGARIA
HO Techn Univ Sofia
DE Neuronal Plasticity; Stochastic Neuron; Synapses
ID NEURAL-NETWORKS
AB Precise neuronal spike timing plays an important role in many aspects of cognitive processing. Here, we explore how a spiking neural network can learn to generate temporally precise spikes in response to a spatio-temporal pattern, through spike-timing-dependent plasticity modulated by a delayed reward signal. An escape noise neuron is implemented as the readout to incorporate the effect of background noise on spike timing. We compare the performance of two different escape rate functions that drive spiking in the readout neuron: the Arrhenius & Current (A&C) and Exponential (EXP) model. Our results show that the network can learn to reproduce target spike patterns containing between 1 and 10 spikes with 10 ms temporal accuracy. We also demonstrate the superior performance of the A&C model over the EXP model for the parameters we consider, especially when reproducing a large number of target spikes.
C1 [Gardner, Brian; Gruening, Andre] Univ Surrey, Dept Comp, Guildford GU2 7XH, Surrey, England.
RP Gardner, B (corresponding author), Univ Surrey, Dept Comp, Guildford GU2 7XH, Surrey, England.
EM b.gardner@surrey.ac.uk; a.gruning@surrey.ac.u
CR Bohte Sander M., 2004, Natural Computing, V3, P195, DOI 10.1023/B:NACO.0000027755.02868.60
   Chance FS, 2002, NEURON, V35, P773, DOI 10.1016/S0896-6273(02)00820-6
   El-Laithy K., 2011, COMPUTATIONAL INTELL, V4
   Farries MA, 2007, J NEUROPHYSIOL, V98, P3648, DOI 10.1152/jn.00364.2007
   Florian RV, 2007, NEURAL COMPUT, V19, P1468, DOI 10.1162/neco.2007.19.6.1468
   Frémaux N, 2010, J NEUROSCI, V30, P13326, DOI 10.1523/JNEUROSCI.6249-09.2010
   Gerstner W., 2002, SPIKING NEURON MODEL
   Grüning A, 2012, NEURAL PROCESS LETT, V36, P117, DOI 10.1007/s11063-012-9225-1
   Izhikevich EM, 2007, CEREB CORTEX, V17, P2443, DOI 10.1093/cercor/bhl152
   Legenstein R, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.1000180
   Morrison A, 2008, BIOL CYBERN, V98, P459, DOI 10.1007/s00422-008-0233-1
   Pfister JP, 2006, NEURAL COMPUT, V18, P1318, DOI 10.1162/neco.2006.18.6.1318
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Sporea I, 2013, NEURAL COMPUT, V25, P473, DOI 10.1162/NECO_a_00396
   Urbanczik R, 2009, NAT NEUROSCI, V12, P250, DOI 10.1038/nn.2264
   van Rossum MCW, 2001, NEURAL COMPUT, V13, P751, DOI 10.1162/089976601300014321
   van Rossum MCW, 2000, J NEUROSCI, V20, P8812
NR 17
TC 6
Z9 6
U1 0
U2 1
PY 2013
VL 8131
BP 256
EP 263
UT WOS:000342695200032
DA 2023-11-16
ER

PT C
AU Zhang, YD
   Zeng, ZG
   Wen, SP
AF Zhang, Yide
   Zeng, Zhigang
   Wen, Shiping
GP IEEE
TI Implementation of Memristive Neural Networks with Spike-rate-dependent
   Plasticity Synapses
SO PROCEEDINGS OF THE 2014 INTERNATIONAL JOINT CONFERENCE ON NEURAL
   NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUL 06-11, 2014
CL Beijing, PEOPLES R CHINA
ID NEURONS; MEMORY; MODEL
AB The property of changing resistance according to applied currents of memristors makes them candidates for emulating synapses in artificial neural networks. In this paper, we introduce a memristive synapse design into neural network circuits. Combined with modified integrate-andfire (I&F) complementary metal-oxide-semiconducter (CMOS) neurons, the memristive neural network shows similarities to its biological counterpart, in respect of biologically realistic, current-controlled spikes and adaptive synaptic plasticity. Then, the spike-rate-dependent plasticity (SRDP) of the synapse, an extended protocol of the Hebbian learning rule, is originally implemented by the circuit. And some advanced neural activities including learning, associative memory and forgetting are realized based on the SRDP rule. These activities are comprehensively validated on a neural network circuit inspired by famous Pavlov's dog-experiment with simulations and quantitative analyses.
C1 [Zhang, Yide; Zeng, Zhigang; Wen, Shiping] Huazhong Univ Sci & Technol, Sch Automat, Wuhan 430074, Peoples R China.
   [Zhang, Yide; Zeng, Zhigang; Wen, Shiping] Educ Minist China, Key Lab Image Proc & Intelligent Control, Wuhan 430074, Peoples R China.
RP Zhang, YD (corresponding author), Huazhong Univ Sci & Technol, Sch Automat, Wuhan 430074, Peoples R China.
EM edwardchang@hust.edu.cn; hustzgzeng@gmail.com; wenshiping226@126.com
CR [Anonymous], NEUROCOMPUTING
   [Anonymous], 1989, ANALOG VLSI NEURAL S
   [Anonymous], BSIMV3 MANUAL
   [Anonymous], 1927, CONDITIONAL REFLEXES
   Bartolozzi C, 2007, NEURAL COMPUT, V19, P2581, DOI 10.1162/neco.2007.19.10.2581
   Bear Mark F., 1994, Current Opinion in Neurobiology, V4, P389, DOI 10.1016/0959-4388(94)90101-5
   Beiu V, 2003, IEEE T NEURAL NETWOR, V14, P1217, DOI 10.1109/TNN.2003.816365
   Benderli S, 2009, ELECTRON LETT, V45, P377, DOI 10.1049/el.2009.3511
   Bichler O, 2013, NEURAL COMPUT, V25, P549, DOI 10.1162/NECO_a_00377
   BIENENSTOCK EL, 1982, J NEUROSCI, V2, P32, DOI 10.1523/jneurosci.02-01-00032.1982
   Biolek Z, 2009, RADIOENGINEERING, V18, P210
   Brader JM, 2007, NEURAL COMPUT, V19, P2881, DOI 10.1162/neco.2007.19.11.2881
   Cantley KD, 2012, IEEE T NEUR NET LEAR, V23, P565, DOI 10.1109/TNNLS.2012.2184801
   Cederström L, 2013, IEEE INT SYMP CIRC S, P2323, DOI 10.1109/ISCAS.2013.6572343
   Chevaleyre V, 2003, NEURON, V38, P461, DOI 10.1016/S0896-6273(03)00235-6
   Chicca E, 2003, IEEE T NEURAL NETWOR, V14, P1297, DOI 10.1109/TNN.2003.816367
   CHUA LO, 1976, P IEEE, V64, P209, DOI 10.1109/PROC.1976.10092
   CHUA LO, 1971, IEEE T CIRCUITS SYST, VCT18, P507, DOI 10.1109/TCT.1971.1083337
   Dayan P., 2001, THEORETICAL NEUROSCI
   Grossberg S, 2012, NEURAL NETWORKS, V27, P1, DOI [10.1016/j.neunet.2011.10.011, 10.1016/j.neunet.2012.09.017]
   Hebb D., 2005, ORG BEHAV NEUROPSYCH
   Hopfield JJ, 1990, NETWORK-COMP NEURAL, V1, P27, DOI 10.1088/0954-898X/1/1/003
   Indiveri G, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00073
   Jo SH, 2010, NANO LETT, V10, P1297, DOI 10.1021/nl904092h
   Joglekar YN, 2009, EUR J PHYS, V30, P661, DOI 10.1088/0143-0807/30/4/001
   Kim H, 2012, IEEE T CIRCUITS-I, V59, P148, DOI 10.1109/TCSI.2011.2161360
   Li SZ, 2013, J MATER CHEM C, V1, P5292, DOI 10.1039/c3tc30575a
   Linares-Barranco B., 2009, NAT PRECED
   Masquelier T, 2007, PLOS COMPUT BIOL, V3, P247, DOI 10.1371/journal.pcbi.0030031
   Masquelier T, 2009, NEURAL COMPUT, V21, P1259, DOI 10.1162/neco.2008.06-08-804
   PEREZCARRASCO JA, 2010, P 2010 IEEE INT S CI, P1659
   Pershin YV, 2010, NEURAL NETWORKS, V23, P881, DOI 10.1016/j.neunet.2010.05.001
   Prodromakis T, 2011, IEEE T ELECTRON DEV, V58, P3099, DOI 10.1109/TED.2011.2158004
   Querlioz D, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P1775, DOI 10.1109/IJCNN.2011.6033439
   Rachmuth G, 2011, P NATL ACAD SCI USA, V108, pE1266, DOI 10.1073/pnas.1106161108
   Richardson MJE, 2010, PHYS REV LETT, V105, DOI 10.1103/PhysRevLett.105.178102
   Rubin J, 2001, PHYS REV LETT, V86, P364, DOI 10.1103/PhysRevLett.86.364
   Shin S, 2010, IEEE T COMPUT AID D, V29, P590, DOI 10.1109/TCAD.2010.2042891
   Snider GS, 2007, NANOTECHNOLOGY, V18, DOI 10.1088/0957-4484/18/36/365202
   Snider G, 2011, COMPUTER, V44, P21, DOI 10.1109/MC.2011.48
   Snider G, 2011, NANOTECHNOLOGY, V22, DOI 10.1088/0957-4484/22/1/015201
   Snider GS, 2008, 2008 IEEE INTERNATIONAL SYMPOSIUM ON NANOSCALE ARCHITECTURES, P85, DOI 10.1109/NANOARCH.2008.4585796
   STAFSTROM CE, 1984, J NEUROPHYSIOL, V52, P264, DOI 10.1152/jn.1984.52.2.264
   Strukov DB, 2008, NATURE, V453, P80, DOI 10.1038/nature06932
   Wang ZQ, 2012, ADV FUNCT MATER, V22, P2759, DOI 10.1002/adfm.201103148
   Ziegler M, 2012, ADV FUNCT MATER, V22, P2744, DOI 10.1002/adfm.201200244
NR 46
TC 25
Z9 29
U1 0
U2 11
PY 2014
BP 2226
EP 2233
UT WOS:000371465702047
DA 2023-11-16
ER

PT C
AU Yousefzadeh, A
   Hoseini, S
   Holanda, P
   Leroux, S
   Werner, T
   Serrano-Gotarredona, T
   Barranco, BL
   Dhoedt, B
   Simoens, P
AF Yousefzadeh, Amirreza
   Hoseini, Sahar
   Holanda, Priscila
   Leroux, Sam
   Werner, Thilo
   Serrano-Gotarredona, Teresa
   Barranco, Bernabe Linares
   Dhoedt, Bart
   Simoens, Pieter
GP IEEE
TI Conversion of Synchronous Artificial Neural Network to Asynchronous
   Spiking Neural Network using sigma-delta quantization
SO 2019 IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE CIRCUITS
   AND SYSTEMS (AICAS 2019)
DT Proceedings Paper
CT 1st IEEE International Conference on Artificial Intelligence Circuits
   and Systems (AICAS)
CY MAR 18-20, 2019
CL Hsinchu, TAIWAN
ID PROCESSOR
AB Artificial Neural Networks (ANNs) show great performance in several data analysis tasks including visual and auditory applications. However, direct implementation of these algorithms without considering the sparsity of data requires high processing power, consume vast amounts of energy and suffer from scalability issues. Inspired by biology, one of the methods which can reduce power consumption and allow scalability in the implementation of neural networks is asynchronous processing and communication by means of action potentials, so-called spikes. In this work, we use the well-known sigma-delta quantization method and introduce an easy and straightforward solution to convert an Artificial Neural Network to a Spiking Neural Network which can be implemented asynchronously in a neuromorphic platform. Briefly, we used asynchronous spikes to communicate the quantized output activations of the neurons. Despite the fact that our proposed mechanism is simple and applicable to a wide range of different ANNs, it outperforms the state-of-the-art implementations from the accuracy and energy consumption point of view. All source code for this project is available upon request for the academic purpose(1).
C1 [Yousefzadeh, Amirreza; Holanda, Priscila; Leroux, Sam; Werner, Thilo; Dhoedt, Bart; Simoens, Pieter] Univ Ghent, Imec, IDLab, Ghent, Belgium.
   [Hoseini, Sahar; Serrano-Gotarredona, Teresa; Barranco, Bernabe Linares] CSIC, Inst Microelectron Sevilla, Seville, Spain.
   [Hoseini, Sahar; Serrano-Gotarredona, Teresa; Barranco, Bernabe Linares] Univ Seville, Seville, Spain.
RP Barranco, BL (corresponding author), CSIC, Inst Microelectron Sevilla, Seville, Spain.; Barranco, BL (corresponding author), Univ Seville, Seville, Spain.
EM Bernabe@imse-cnm.csic.es
CR BOSER BE, 1988, IEEE J SOLID-ST CIRC, V23, P1298, DOI 10.1109/4.90025
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Diehl P.U., 2015, 2015 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2015.7280696
   Farabet C, 2012, FRONT NEUROSCI-SWITZ, V6, DOI 10.3389/fnins.2012.00032
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   Grady D., VISION THING MAINLY
   Indiveri Giacomo, 2011, Front Neurosci, V5, P118, DOI 10.3389/fnins.2011.00118
   Kheradpisheh S. R., CORR
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee J., CORR
   Leroux S., CORR
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   MINK JW, 1981, AM J PHYSIOL, V241, pR203, DOI 10.1152/ajpregu.1981.241.3.R203
   Moons B, 2017, IEEE J SOLID-ST CIRC, V52, P903, DOI 10.1109/JSSC.2016.2636225
   Mostafa H., 2017, 2017 IEEE INT S CIRC, P1, DOI [10.1109/ISCAS.2017.8050527, DOI 10.1109/ISCAS.2017.8050527]
   Mostafaei H, 2019, IEEE T IND ELECTRON, V66, P5567, DOI 10.1109/TIE.2018.2869345
   Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32
   Rueckauer B, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351295
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Sengupta A., CORR
   Serrano-Gotarredona T, 2013, IEEE J SOLID-ST CIRC, V48, P827, DOI 10.1109/JSSC.2012.2230553
   Stromatias E, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00350
   Van Rullen R, 2001, NEURAL COMPUT, V13, P1255, DOI 10.1162/08997660152002852
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Yousefzadeh A, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351562
NR 25
TC 16
Z9 17
U1 0
U2 1
PY 2019
BP 81
EP 85
DI 10.1109/aicas.2019.8771624
UT WOS:000493095400019
DA 2023-11-16
ER

PT J
AU McKinstry, JL
   Edelman, GM
AF McKinstry, Jeffrey L.
   Edelman, Gerald M.
TI Temporal sequence learning in winner-take-all networks of spiking
   neurons demonstrated in a brain-based device
SO FRONTIERS IN NEUROROBOTICS
DT Article
DE neurorobotics; sequence learning; spiking network; winner-take-all;
   motor control and learning/plasticity; spike-timing dependent
   plasticity; sensorimotor control; large-scale spiking neural networks
ID MOTOR; MOVEMENTS; MODEL
AB Animal behavior often involves a temporally ordered sequence of actions learned from experience. Here we describe simulations of interconnected networks of spiking neurons that learn to generate patterns of activity in correct temporal order. The simulation consists of large-scale networks of thousands of excitatory and inhibitory neurons that exhibit short-term synaptic plasticity and spike timing dependent synaptic plasticity. The neural architecture within each area is arranged to evoke vvinner-take-all (WTA) patterns of neural activity that persist for tens of milliseconds. In order to generate and switch between consecutive firing patterns in correct temporal order, a reentrant exchange of signals between these areas was necessary. To demonstrate the capacity of this arrangement, we used the simulation to train a brain based device responding to visual input by autonomously generating temporal sequences of motor actions.
C1 [McKinstry, Jeffrey L.; Edelman, Gerald M.] Inst Neurosci, San Diego, CA 92037 USA.
RP McKinstry, JL (corresponding author), Inst Neurosci, 800 Silverado St,Suite 302, San Diego, CA 92037 USA.
EM mckinstry@nsi.edu
CR [Anonymous], 1988, C PROGRAMMING LANGUA
   [Anonymous], 1978, MINDFUL BRAIN
   Averbeck BB, 2002, P NATL ACAD SCI USA, V99, P13172, DOI 10.1073/pnas.162485599
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Binshtok AM, 2006, J NEUROSCI, V26, P708, DOI 10.1523/JNEUROSCI.4409-05.2006
   Chen YQ, 2013, FRONT COMPUT NEUROSC, V7, DOI 10.3389/fncom.2013.00016
   Chersi F, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0027652
   Dayan P., 2001, THEORETICAL NEUROSCI
   Edelman GM, 2007, SCIENCE, V318, P1103, DOI 10.1126/science.1148677
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2008, P NATL ACAD SCI USA, V105, P3593, DOI 10.1073/pnas.0712231105
   Izhikevich EM, 2010, PHILOS T R SOC A, V368, P5061, DOI 10.1098/rsta.2010.0130
   Jones LM, 2007, P NATL ACAD SCI USA, V104, P18772, DOI 10.1073/pnas.0705546104
   Liu JK, 2009, J NEUROSCI, V29, P13172, DOI 10.1523/JNEUROSCI.2358-09.2009
   McKinstry JL, 2008, NEURAL NETWORKS, V21, P553, DOI 10.1016/j.neunet.2008.01.004
   Myme CIO, 2003, J NEUROPHYSIOL, V90, P771, DOI 10.1152/jn.00070.2003
   Nakajima T, 2009, J NEUROPHYSIOL, V101, P1883, DOI 10.1152/jn.90636.2008
   Rhodes BJ, 2004, HUM MOVEMENT SCI, V23, P699, DOI 10.1016/j.humov.2004.10.008
   Rutishauser U, 2009, NEURAL COMPUT, V21, P478, DOI 10.1162/neco.2008.03-08-734
   Salinas E, 2009, J NEUROSCI, V29, P4369, DOI 10.1523/JNEUROSCI.0164-09.2009
   Seidemann E, 1996, J NEUROSCI, V16, P752
   Tanji J, 2001, ANNU REV NEUROSCI, V24, P631, DOI 10.1146/annurev.neuro.24.1.631
   Verduzco-Flores SO, 2012, J COMPUT NEUROSCI, V32, P403, DOI 10.1007/s10827-011-0360-x
   ZUCKER RS, 1989, ANNU REV NEUROSCI, V12, P13, DOI 10.1146/annurev.ne.12.030189.000305
NR 24
TC 9
Z9 9
U1 0
U2 3
PY 2013
VL 7
AR 10
DI 10.3389/fnbot.2013.00010
UT WOS:000209437600010
DA 2023-11-16
ER

PT C
AU Pu, JR
   Nambiar, VP
   Do, AT
   Goh, WL
AF Pu, Junran
   Nambiar, Vishnu P.
   Anh Tuan Do
   Goh, Wang Ling
GP IEEE
TI Block-Based Spiking Neural Network Hardware with Deme Genetic Algorithm
SO 2019 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS (ISCAS)
SE IEEE International Symposium on Circuits and Systems
DT Proceedings Paper
CT IEEE International Symposium on Circuits and Systems (IEEE ISCAS)
CY MAY 26-29, 2019
CL Sapporo, JAPAN
DE spiking neural network hardware; spiking neuron model; spatial
   architecture; deme genetic algorithm
AB Hardware implementation of spiking neural networks (SNN) has been the focus of many previous works due to its higher execution speed. A block-based SNN architecture with a simple spiking neuron model is proposed in this paper. Compared to traditional spiking neuron models, the proposed model simplifies the equation of the membrane potential for ease of hardware implementation. The block-based SNN architecture also makes the hardware implementation more scalable and simplifies floorplanning. Deme genetic algorithm (GA) was applied for training the SNN model, and a population encoding scheme was used for spike time conversion. Two case studies were carried out to verify the functionality of the proposed model, namely number recognition and Fisher Iris classification. Experimental results showed that the proposed SNN model with deme GA was able to achieve comparable or higher classification accuracy than previous works.
C1 [Pu, Junran; Goh, Wang Ling] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore, Singapore.
   [Nambiar, Vishnu P.; Anh Tuan Do] ASTAR, Inst Microelect IME, IC Design Dept, Singapore, Singapore.
RP Pu, JR (corresponding author), Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore, Singapore.
EM junran001@e.ntu.edu.sg; vishnu_paramasivam@ime.a-star.edu.sg;
   doat@ime.a-star.edu.sg; ewlgoh@ntu.edu.sg
CR [Anonymous], 2015, SPLST
   [Anonymous], 1991, TOUCHSTONE DELTA SYS
   [Anonymous], 2015, 2015 INT JOINT C NEU, DOI DOI 10.1109/IJCNN.2015.7280625
   [Anonymous], 1996, GALIB C LIB GENETIC
   [Anonymous], 1989, GENETIC ALGORITHMS S
   Dua D., 2017, UCI MACHINE LEARNING
   Gerstner W., 2002, SPIKING NEURON MODEL
   Ghosh-Dastidar S, 2007, INTEGR COMPUT-AID E, V14, P187
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Jin X, 2008, IEEE IJCNN, P2812, DOI 10.1109/IJCNN.2008.4634194
   Kumar S, 2002, IEEE COMP SOC ANN, P117, DOI 10.1109/ISVLSI.2002.1016885
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Olshausen BA, 2004, CURR OPIN NEUROBIOL, V14, P481, DOI 10.1016/j.conb.2004.07.007
   Xin JG, 2001, IEEE IJCNN, P1772, DOI 10.1109/IJCNN.2001.938430
NR 14
TC 5
Z9 5
U1 0
U2 0
PY 2019
UT WOS:000483076402114
DA 2023-11-16
ER

PT J
AU Cessac, B
AF Cessac, B.
TI A VIEW OF NEURAL NETWORKS AS DYNAMICAL SYSTEMS
SO INTERNATIONAL JOURNAL OF BIFURCATION AND CHAOS
DT Review
DE Neural networks; dynamical systems; synaptic plasticity; linear
   response; chaos
ID LONG-TERM POTENTIATION; MEAN-FIELD THEORY; FIRE NEURONS; RECURRENT
   NETWORKS; SPIKING NEURONS; CHAOS; MODEL; SYNCHRONIZATION; TRANSMISSION;
   BIFURCATION
AB We present some recent investigations resulting from the modeling of neural networks as dynamical systems, and deal with the following questions, adressed in the context of specific models.
   (i) Characterizing the collective dynamics;
   (ii) Statistical analysis of spike trains;
   (iii) Interplay between dynamics and network structure;
   (iv) Effects of synaptic plasticity.
C1 [Cessac, B.] Univ Nice Sophia Antipolis, CNRS, Lab JA Dieudonne, UMR 6621, Nice, France.
   [Cessac, B.] INRIA, EPI NeuroMathComp, F-06902 Sophia Antipolis, France.
RP Cessac, B (corresponding author), Univ Nice Sophia Antipolis, CNRS, Lab JA Dieudonne, UMR 6621, Nice, France.
CR ABBOTT LF, 1993, PHYS REV E, V48, P1483, DOI 10.1103/PhysRevE.48.1483
   ABELES M, 1993, CONCEPT NEUROSCI, V4, P131
   Abeles M., 1982, LOCAL CORTICAL CIRCU, DOI DOI 10.1007/978-3-642-81708-3
   Adrian ED, 1926, J PHYSIOL-LONDON, V61, P151, DOI 10.1113/jphysiol.1926.sp002281
   Afraimovich VS, 2007, NONLINEARITY, V20, P1761, DOI 10.1088/0951-7715/20/7/011
   AMARI S, 1972, IEEE T SYST MAN CYB, VSMC2, P643, DOI 10.1109/TSMC.1972.4309193
   AMARI SI, 1977, SIAM J APPL MATH, V33, P95, DOI 10.1137/0133008
   [Anonymous], 2001, ION CHANNELS EXCITAB
   [Anonymous], 1990, ZETA FUNCTIONS PERIO
   [Anonymous], PUBLICATIONS MATH IH
   [Anonymous], 1991, CORTICONICS
   [Anonymous], P 11 EUR S ART NEUR
   [Anonymous], 2006, ARXIVQBIO0611072
   [Anonymous], PHYS REV E
   Arabzadeh E, 2006, J NEUROSCI, V26, P9216, DOI 10.1523/JNEUROSCI.1491-06.2006
   ARTOLA A, 1990, NATURE, V347, P69, DOI 10.1038/347069a0
   Ashwin P, 2005, NONLINEARITY, V18, P2035, DOI 10.1088/0951-7715/18/5/009
   Atay FM, 2006, PHYSICA D, V224, P35, DOI 10.1016/j.physd.2006.09.018
   Barahona M, 2002, PHYS REV LETT, V89, DOI 10.1103/PhysRevLett.89.054101
   Barbieri R, 2004, NEURAL COMPUT, V16, P277, DOI 10.1162/089976604322742038
   BARJAVEL Rene, 1944, VOYAGEUR IMPRUDENT
   BEN AROUS G, 1997, ANN PROBAB, V25, P1367
   BENAROUS G, 1995, PROBAB THEORY REL, V103, P431
   Bi GQ, 2001, ANNU REV NEUROSCI, V24, P139, DOI 10.1146/annurev.neuro.24.1.139
   BIENENSTOCK EL, 1982, J NEUROSCI, V2, P32, DOI 10.1523/jneurosci.02-01-00032.1982
   Blanchard P, 2000, J STAT PHYS, V98, P375, DOI 10.1023/A:1018639308981
   Blanchard P, 2009, UNDERST COMPLEX SYST, P1, DOI 10.1007/978-3-540-87829-2_1
   BLISS TVP, 1973, J PHYSIOL-LONDON, V232, P357, DOI 10.1113/jphysiol.1973.sp010274
   Boccaletti S, 2006, PHYS REP, V424, P175, DOI 10.1016/j.physrep.2005.10.009
   Bowen R, 2008, LECT NOTES MATH, V470, P1, DOI 10.1007/978-3-540-77695-6
   BOWEN R, 1975, EQUILIBRIUM STATES E, V470
   Bressloff PC, 2000, SIAM J APPL MATH, V60, P820, DOI 10.1137/S0036139998339643
   Bressloff PC, 2000, NEURAL COMPUT, V12, P91, DOI 10.1162/089976600300015907
   BRESSLOFF PC, 2003, EPILEPSY DYNAMIC DIS, pCH7
   Brette R, 2005, J NEUROPHYSIOL, V94, P3637, DOI 10.1152/jn.00686.2005
   Brette R, 2007, J COMPUT NEUROSCI, V23, P349, DOI 10.1007/s10827-007-0038-6
   Brunel N, 1998, J THEOR BIOL, V195, P87, DOI 10.1006/jtbi.1998.0782
   Brunel N, 2003, NEURAL COMPUT, V15, P2281, DOI 10.1162/089976603322362365
   Brunel N, 1999, NEURAL COMPUT, V11, P1621, DOI 10.1162/089976699300016179
   Cessac B, 2007, NONLINEARITY, V20, P2883, DOI 10.1088/0951-7715/20/12/007
   Cessac B, 2008, J MATH BIOL, V56, P311, DOI 10.1007/s00285-007-0117-3
   Cessac B, 2006, CHAOS, V16, DOI 10.1063/1.2126813
   Cessac B, 2007, EUR PHYS J-SPEC TOP, V142, P7, DOI 10.1140/epjst/e2007-00003-5
   Cessac B, 2004, PHYS REV E, V70, DOI 10.1103/PhysRevE.70.056111
   CESSAC B, 1994, PHYSICA D, V74, P24, DOI 10.1016/0167-2789(94)90024-8
   CESSAC B, 1995, J PHYS I, V5, P409, DOI 10.1051/jp1:1995135
   CESSAC B, 1994, EUROPHYS LETT, V26, P577, DOI 10.1209/0295-5075/26/8/004
   CESSAC B, 2009, J STAT PHYS IN PRESS
   Cessac B, 2007, PHYSICA D, V225, P13, DOI 10.1016/j.physd.2006.09.034
   Cessac B, 2008, FRONT COMPUT NEUROSC, V2, DOI 10.3389/neuro.10.002.2008
   CHAZOTTES J, 2009, ENCY COMPLE IN PRESS
   Chazottes JR, 1998, J STAT PHYS, V90, P697, DOI 10.1023/A:1023220802597
   Chizhov AV, 2007, PHYS REV E, V75, DOI 10.1103/PhysRevE.75.011924
   Chow CC, 2000, NEURAL COMPUT, V12, P1643, DOI 10.1162/089976600300015295
   Coombes S, 1999, PHYS REV E, V60, P2086, DOI 10.1103/PhysRevE.60.2086
   Coombes S, 1999, PHYS LETT A, V255, P49, DOI 10.1016/S0375-9601(99)00172-3
   Cooper L., 2004, THEORY CORTICAL PLAS
   Cosandier-Rimélé D, 2007, IEEE T BIO-MED ENG, V54, P380, DOI 10.1109/TBME.2006.890489
   COSNARD M, 1985, LECT NOTES MATH, V1163, P23
   COSNARD M, 1993, MATH APPL BIOL MED
   CRISANTI A, 1987, PHYS REV A, V36, P4922, DOI 10.1103/PhysRevA.36.4922
   CRISANTI A, 1988, PHYS REV A, V37, P4865, DOI 10.1103/PhysRevA.37.4865
   CRONIN J, 1987, MATH ASPECTS H HUXLE
   Dauce E, 1998, NEURAL NETWORKS, V11, P521, DOI 10.1016/S0893-6080(97)00131-7
   Dayan P., 2001, THEORETICAL NEUROSCI
   DEALMEIDA JRL, 1978, J PHYS A-MATH GEN, V11, P983, DOI 10.1088/0305-4470/11/5/028
   Delorme A, 2001, NEUROCOMPUTING, V38, P539, DOI 10.1016/S0925-2312(01)00403-9
   Diesmann M, 1999, NATURE, V402, P529, DOI 10.1038/990101
   DUDEK SM, 1993, J NEUROSCI, V13, P2910
   ECKMANN JP, 1985, REV MOD PHYS, V57, P617, DOI 10.1103/RevModPhys.57.617
   Ermentrout B, 1998, REP PROG PHYS, V61, P353, DOI 10.1088/0034-4885/61/4/002
   ERMENTROUT GB, 1984, SIAM J MATH ANAL, V15, P215, DOI 10.1137/0515019
   ERNST U, 1995, PHYS REV LETT, V74, P1570, DOI 10.1103/PhysRevLett.74.1570
   FAUGERAS O, 2008, FRONT NEUROSCI UNPUB
   FITZHUGH R, 1966, J GEN PHYSIOL, V49, P989, DOI 10.1085/jgp.49.5.989
   Fourcaud N, 2002, NEURAL COMPUT, V14, P2057, DOI 10.1162/089976602320264015
   Fourcaud-Trocmé N, 2003, J NEUROSCI, V23, P11628
   GAMBAUDO J, 1988, CHAOS THEORIE EXPERI
   Gao Y, 2008, ENTROPY-SWITZ, V10, P71, DOI 10.3390/entropy-e10020071
   GERSTNER W, 1995, PHYS REV E, V51, P738, DOI 10.1103/PhysRevE.51.738
   Gerstner W, 2002, BIOL CYBERN, V87, P404, DOI 10.1007/s00422-002-0353-y
   Gerstner W., 2002, SPIKING NEURON MODEL
   GIRKO VL, 1985, THEOR PROBAB APPL+, V29, P694, DOI 10.1137/1129095
   Gong PL, 2007, PHYS REV LETT, V98, DOI 10.1103/PhysRevLett.98.048104
   Grammont F, 1999, EXP BRAIN RES, V128, P118, DOI 10.1007/s002210050826
   GRIMBERT F, 2005, RR5597 INRIA
   Grinstein G, 2005, P NATL ACAD SCI USA, V102, P9948, DOI 10.1073/pnas.0504127102
   GUCKENHEIMER J, 1993, B MATH BIOL, V55, P937, DOI 10.1007/BF02460693
   Guckenheimer J, 2002, SIAM J APPL DYN SYST, V1, P105, DOI 10.1137/S1111111101394040
   Guionnet A, 1997, PROBAB THEORY REL, V109, P183, DOI 10.1007/s004400050130
   Gupta, 2003, STATIC DYNAMIC NEURA, DOI 10.1002/0471427950
   Hansel D, 1998, NEURAL COMPUT, V10, P467, DOI 10.1162/089976698300017845
   Hasegawa H, 2005, PHYS REV E, V72, DOI 10.1103/PhysRevE.72.056139
   Hebb D.O., 1949, ORG BEHAV NEUROPSYCH
   Hertz J, 1998, THEORETICAL ASPECTS OF NEURAL COMPUTATION, P135
   HIRSCH MW, 1989, NEURAL NETWORKS, V2, P331, DOI 10.1016/0893-6080(89)90018-X
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Hoppenstaedt F, 1997, WEAKLY CONNECTED NEU
   HUNT BR, 1992, B AM MATH SOC, V27, P217, DOI 10.1090/S0273-0979-1992-00328-2
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, NEURAL COMPUT, V15, P1511, DOI 10.1162/089976603321891783
   Jahnke S, 2008, PHYS REV LETT, V100, DOI 10.1103/PhysRevLett.100.048102
   Jalan S, 2006, CHAOS, V16, DOI 10.1063/1.2336415
   JANSEN BH, 1995, BIOL CYBERN, V73, P357, DOI 10.1007/BF00199471
   JAYNES ET, 1957, PHYS REV, V106, P620, DOI 10.1103/PhysRev.106.620
   Johnson DH, 2004, J COMPUT NEUROSCI, V16, P69, DOI 10.1023/B:JCNS.0000004842.04535.7c
   JOHNSON KO, 1980, J NEUROPHYSIOL, V43, P1793, DOI 10.1152/jn.1980.43.6.1793
   JOLIVET R, 2006, INTEGRATE AND FIRE M
   Jost J, 2002, PHYS REV E, V65, DOI 10.1103/PhysRevE.65.016201
   Katok A., 1998, INTRO MODERN THEORY
   Keener J., 1998, INTERD APPL, DOI 10.1007/b98841
   KEENER JP, 1981, SIAM J APPL MATH, V41, P503, DOI 10.1137/0141042
   Keller G., 1998, EQUILIBRIUM STATES E
   KIRKPATRICK S, 1978, PHYS REV B, V17, P4384, DOI 10.1103/PhysRevB.17.4384
   Kirst C, 2009, FRONT NEUROSCI-SWITZ, V3, P2, DOI 10.3389/neuro.01.009.2009
   Kirst C, 2009, PHYS REV LETT, V102, DOI 10.1103/PhysRevLett.102.068101
   KNIGHT BW, 1972, J GEN PHYSIOL, V59, P734, DOI 10.1085/jgp.59.6.734
   Koch Christof, 1999, P1
   Lago-Fernández LF, 2000, PHYS REV LETT, V84, P2758, DOI 10.1103/PhysRevLett.84.2758
   Lapique L., 1907, J PHYSL PATHOL GEN, V9, P620, DOI DOI 10.1007/S00422-007-0189-6
   LEVY WB, 1983, NEUROSCIENCE, V8, P791, DOI 10.1016/0306-4522(83)90010-6
   Litvak V, 2003, J NEUROSCI, V23, P3006
   M?zard M., 1987, SPIN GLASS THEORY, V9
   MACKAY RS, 1986, PHYSICA D, V19, P206, DOI 10.1016/0167-2789(86)90020-5
   MAHON S, 2003, J PHYSL, V1, P947
   Malenka RC, 1999, SCIENCE, V285, P1870, DOI 10.1126/science.285.5435.1870
   MALSBURG CV, 1973, KYBERNETIK, V14, P85, DOI 10.1007/BF00288907
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   Mattia M, 2002, PHYS REV E, V66, DOI 10.1103/PhysRevE.66.051917
   Memmesheimer RM, 2006, PHYSICA D, V224, P182, DOI 10.1016/j.physd.2006.09.037
   MILLER KD, 1989, SCIENCE, V245, P605, DOI 10.1126/science.2762813
   MILNOR J, 1985, COMMUN MATH PHYS, V99, P177, DOI 10.1007/BF01212280
   MIROLLO RE, 1990, SIAM J APPL MATH, V50, P1645, DOI 10.1137/0150098
   MOLGEDEY L, 1992, PHYS REV LETT, V69, P3717, DOI 10.1103/PhysRevLett.69.3717
   Moynot O, 2002, PROBAB THEORY REL, V123, P41, DOI 10.1007/s004400100182
   NAGUMO J, 1962, P IRE, V50, P2061, DOI 10.1109/JRPROC.1962.288235
   Nelson M., 1995, H HUXLEY MODEL, P27
   Nemenman I, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.1000025
   Nirenberg S, 2003, P NATL ACAD SCI USA, V100, P7348, DOI 10.1073/pnas.1131895100
   Nishikawa T, 2003, PHYS REV LETT, V91, DOI 10.1103/PhysRevLett.91.014101
   OSBONE L, 2008, ARXIVORG08033837
   PERRINET L, 2001, NEUROCOMPUTING, V38
   Pillow JW, 2008, NATURE, V454, P995, DOI 10.1038/nature07140
   Pillow JW, 2005, J NEUROSCI, V25, P11003, DOI 10.1523/JNEUROSCI.3305-05.2005
   Rao RPN, 2001, NEURAL COMPUT, V13, P2221, DOI 10.1162/089976601750541787
   Renart A, 2004, MATH COMP BIOL SER, P431
   Rieke F., 1996, SPIKES EXPLORING NEU
   ROSTROGONZALEZ H, 2009, COMP NEUR M CNS
   ROTTERDAM A, 1982, B MATH BIOL, V44, P283
   Roudi Y, 2009, PLOS COMPUT BIOL, V5, DOI 10.1371/journal.pcbi.1000380
   Rudolph M, 2006, NEURAL COMPUT, V18, P2146, DOI 10.1162/neco.2006.18.9.2146
   Ruelle D, 2005, COMMUN MATH PHYS, V258, P445, DOI 10.1007/s00220-004-1267-4
   Ruelle D, 1999, J STAT PHYS, V95, P393, DOI 10.1023/A:1004593915069
   RUELLE D, 1971, COMMUN MATH PHYS, V20, P167, DOI 10.1007/BF01646553
   Ruelle D., 1978, THERMODYNAMIC FORMAL
   Schneidman E, 2006, NATURE, V440, P1007, DOI 10.1038/nature04701
   Segev R, 2004, NAT NEUROSCI, V7, P1155, DOI 10.1038/nn1323
   Senn W, 2000, SIAM J APPL MATH, V61, P1143
   SINANOVIC A, 2006, SIGNAL PROCESS UNPUB
   Siri B, 2008, NEURAL COMPUT, V20, P2937, DOI 10.1162/neco.2008.05-07-530
   Siri B, 2007, J PHYSIOL-PARIS, V101, P136, DOI 10.1016/j.jphysparis.2007.10.003
   SOMPOLINSKY H, 1982, PHYS REV B, V25, P6860, DOI 10.1103/PhysRevB.25.6860
   SOMPOLINSKY H, 1988, PHYS REV LETT, V61, P259, DOI 10.1103/PhysRevLett.61.259
   Soula H, 2006, NEURAL COMPUT, V18, P60, DOI 10.1162/089976606774841567
   Soula H, 2007, NEURAL COMPUT, V19, P3262, DOI 10.1162/neco.2007.19.12.3262
   THEUNISSEN F, 1995, J COMPUT NEUROSCI, V2, P149, DOI 10.1007/BF00961885
   Thomson AM, 2007, FRONT NEUROSCI-SWITZ, V1, P19, DOI 10.3389/neuro.01.1.1.002.2007
   Timme M, 2002, PHYS REV LETT, V89, DOI 10.1103/PhysRevLett.89.258701
   TOUBOUL J, 2007, J PHYSL PAR IN PRESS
   Touboul J, 2008, SIAM J APPL MATH, V68, P1045, DOI 10.1137/070687268
   TREVES A, 1993, NETWORK-COMP NEURAL, V4, P259, DOI 10.1088/0954-898X/4/3/002
   VAN VREESWIJK C, 1998, NEURAL COMPUT, V10, P1321
   vanVreeswijk C, 1996, SCIENCE, V274, P1724, DOI 10.1126/science.274.5293.1724
   VANVREESWIJK C, 2004, WHAT IS NEURAL CODE
   VANVREESWIJK C, 1997, COMPUT NEUROSCI, V97
   Xie XH, 2002, NEURAL COMPUT, V14, P2627, DOI 10.1162/089976602760408008
   Ya S., 1972, RUSS MATH SURV+, V27, P21, DOI 10.1070/RM1972v027n04ABEH001383
   [No title captured]
NR 178
TC 19
Z9 21
U1 0
U2 20
PD JUN
PY 2010
VL 20
IS 6
BP 1585
EP 1629
DI 10.1142/S0218127410026721
UT WOS:000281734300002
DA 2023-11-16
ER

PT J
AU Ekelmans, P
   Kraynyukovas, N
   Tchumatchenko, T
AF Ekelmans, Pierre
   Kraynyukovas, Nataliya
   Tchumatchenko, Tatjana
TI Targeting operational regimes of interest in recurrent neural networks
SO PLOS COMPUTATIONAL BIOLOGY
DT Article
ID IN-VIVO; VISUAL-CORTEX; NEURONS; MOUSE; EXCITATION; INTEGRATION;
   CIRCUITS; COLUMN; MODEL; ORGANIZATION
AB Neural computations emerge from local recurrent neural circuits or computational units such as cortical columns that comprise hundreds to a few thousand neurons. Continuous progress in connectomics, electrophysiology, and calcium imaging require tractable spiking network models that can consistently incorporate new information about the network structure and reproduce the recorded neural activity features. However, for spiking networks, it is challenging to predict which connectivity configurations and neural properties can generate fundamental operational states and specific experimentally reported nonlinear cortical computations. Theoretical descriptions for the computational state of cortical spiking circuits are diverse, including the balanced state where excitatory and inhibitory inputs balance almost perfectly or the inhibition stabilized state (ISN) where the excitatory part of the circuit is unstable. It remains an open question whether these states can co-exist with experimentally reported nonlinear computations and whether they can be recovered in biologically realistic implementations of spiking networks. Here, we show how to identify spiking network connectivity patterns underlying diverse nonlinear computations such as XOR, bistability, inhibitory stabilization, supersaturation, and persistent activity. We establish a mapping between the stabilized supralinear network (SSN) and spiking activity which allows us to pinpoint the location in parameter space where these activity regimes occur. Notably, we find that biologically-sized spiking networks can have irregular asynchronous activity that does not require strong excitation-inhibition balance or large feedforward input and we show that the dynamic firing rate trajectories in spiking networks can be precisely targeted without error-driven training algorithms.
   Author summaryBiological neural networks must be able to execute diverse nonlinear operations on signals in order to perform complex information processing. While nonlinear transformations have been observed experimentally or in specific theoretical models, a comprehensive theory linking the parameters of a network of spiking neurons to its computations is still lacking. We show that spiking networks can be accurately approximated with a mathematically tractable model, the Stabilized Supralinear Network. Using the mapping we derived between these two frameworks, we show that spiking networks have a rich repertoire of nonlinear regimes at their disposal and link the existence of such regimes to precise conditions on parameters. Notably, we show that classical excitatory-inhibitory networks of leaky integrate-and-fire neurons support nonlinear transformations without the need for synaptic plasticity, intricate wiring diagrams or a complex system of different cell types. The capacity of a network to reliably perform such operations has profound functional implications as they can be the basis permitting the execution of complex computations.
C1 [Ekelmans, Pierre; Kraynyukovas, Nataliya; Tchumatchenko, Tatjana] Max Planck Inst Brain Res, Theory Neural Dynam Grp, Frankfurt, Germany.
   [Ekelmans, Pierre] Frankfurt Inst Adv Studies, Frankfurt, Germany.
   [Kraynyukovas, Nataliya; Tchumatchenko, Tatjana] Univ Klinikum Bonn, Inst Expt Epileptol & Cognit Res, Life & Brain Ctr, Bonn, Germany.
   [Tchumatchenko, Tatjana] Johannes Gutenberg Univ Mainz, Inst Physiol Chem, Med Ctr Johannes Gutenberg, Mainz, Germany.
RP Tchumatchenko, T (corresponding author), Max Planck Inst Brain Res, Theory Neural Dynam Grp, Frankfurt, Germany.; Tchumatchenko, T (corresponding author), Univ Klinikum Bonn, Inst Expt Epileptol & Cognit Res, Life & Brain Ctr, Bonn, Germany.; Tchumatchenko, T (corresponding author), Johannes Gutenberg Univ Mainz, Inst Physiol Chem, Med Ctr Johannes Gutenberg, Mainz, Germany.
EM tatjana.tchumatchenko@uni-mainz.de
CR Ahmadian Y, 2021, NEURON, V109, P3373, DOI 10.1016/j.neuron.2021.07.031
   Ahmadian Y, 2013, NEURAL COMPUT, V25, P1994, DOI 10.1162/NECO_a_00472
   Allen Institute for Brain Science, 2019, SYNAPTIC PHYSL COARS
   AMIT DJ, 1991, NETWORK-COMP NEURAL, V2, P259, DOI 10.1088/0954-898X/2/3/003
   Amit DJ, 1997, CEREB CORTEX, V7, P237, DOI 10.1093/cercor/7.3.237
   Atallah BV, 2012, NEURON, V73, P159, DOI 10.1016/j.neuron.2011.12.013
   Baker C, 2020, PLOS COMPUT BIOL, V16, DOI 10.1371/journal.pcbi.1008192
   Barnes SJ, 2015, J NEUROSCI, V35, P9024, DOI 10.1523/JNEUROSCI.4583-14.2015
   Becker S, 2022, PLOS COMPUT BIOL, V18, DOI 10.1371/journal.pcbi.1010543
   Billeh YN, 2020, NEURON, V106, P388, DOI 10.1016/j.neuron.2020.01.040
   Bock DD, 2011, NATURE, V471, P177, DOI 10.1038/nature09802
   Brunel N, 1999, NEURAL COMPUT, V11, P1621, DOI 10.1162/089976699300016179
   Brunel N, 2000, J COMPUT NEUROSCI, V8, P183, DOI 10.1023/A:1008925309027
   Buice MA, 2010, NEURAL COMPUT, V22, P377, DOI 10.1162/neco.2009.02-09-960
   Busse L, 2009, NEURON, V64, P931, DOI 10.1016/j.neuron.2009.11.004
   Carandini M, 2012, NAT REV NEUROSCI, V13, P51, DOI 10.1038/nrn3136
   Cavanagh SE, 2020, FRONT NEURAL CIRCUIT, V14, DOI 10.3389/fncir.2020.615626
   Cavanagh SE, 2016, ELIFE, V5, DOI 10.7554/eLife.18937
   Chen GF, 2019, NAT COMMUN, V10, DOI 10.1038/s41467-019-08550-1
   Cossell L, 2015, NATURE, V518, P399, DOI 10.1038/nature14182
   de Kock CPJ, 2021, COMMUN BIOL, V4, DOI 10.1038/s42003-021-02241-8
   Destexhe A, 2001, NEUROSCIENCE, V107, P13, DOI 10.1016/S0306-4522(01)00344-X
   FUSTER JM, 1971, SCIENCE, V173, P652, DOI 10.1126/science.173.3997.652
   Gentet LJ, 2012, NAT NEUROSCI, V15, P607, DOI 10.1038/nn.3051
   Gerstner W., 2002, SPIKING NEURON MODEL
   Graham JW., 2015, CELL, V163
   Guest JM., 2021, BIORXIV
   Haider B, 2006, J NEUROSCI, V26, P4535, DOI 10.1523/JNEUROSCI.5297-05.2006
   Hengen KB, 2013, NEURON, V80, P335, DOI 10.1016/j.neuron.2013.08.038
   Herculano-Houzel S, 2013, FRONT NEUROANAT, V7, DOI 10.3389/fnana.2013.00035
   Hofer SB, 2011, NAT NEUROSCI, V14, P1045, DOI 10.1038/nn.2876
   Jercog D, 2017, ELIFE, V6, DOI 10.7554/eLife.22425
   KATZ B, 1965, PROC R SOC SER B-BIO, V161, P483, DOI 10.1098/rspb.1965.0016
   Khajeh R, 2022, PLOS COMPUT BIOL, V18, DOI 10.1371/journal.pcbi.1008836
   Khan AG, 2018, NAT NEUROSCI, V21, P851, DOI 10.1038/s41593-018-0143-z
   Ko H, 2011, NATURE, V473, P87, DOI 10.1038/nature09880
   Kraynyukovaa N, 2018, P NATL ACAD SCI USA, V115, P3464, DOI 10.1073/pnas.1700080115
   Kusmierz L, 2020, PHYS REV LETT, V125, DOI 10.1103/PhysRevLett.125.028101
   Lefort S, 2009, NEURON, V61, P301, DOI 10.1016/j.neuron.2008.12.020
   Li YT, 2013, NAT NEUROSCI, V16, P1324, DOI 10.1038/nn.3494
   Lien AD, 2013, NAT NEUROSCI, V16, P1315, DOI 10.1038/nn.3488
   Margrie TW, 2002, PFLUG ARCH EUR J PHY, V444, P491, DOI 10.1007/s00424-002-0831-z
   Markram H, 2015, CELL, V163, P456, DOI 10.1016/j.cell.2015.09.029
   Marshel JH, 2019, SCIENCE, V365, P558, DOI 10.1126/science.aaw5202
   Meyer HS, 2010, CEREB CORTEX, V20, P2277, DOI 10.1093/cercor/bhq067
   Miller KD., 2020, BIORXIV
   Minsky M., 1969, PERCEPTRONS INTRO CO
   Mongillo G, 2012, PHYS REV LETT, V108, DOI 10.1103/PhysRevLett.108.158101
   Montbrió E, 2015, PHYS REV X, V5, DOI 10.1103/PhysRevX.5.021028
   Motta A, 2019, SCIENCE, V366, P1093, DOI 10.1126/science.aay3134
   Murray JD, 2014, NAT NEUROSCI, V17, P1661, DOI 10.1038/nn.3862
   Oberlaender M, 2012, CEREB CORTEX, V22, P2375, DOI 10.1093/cercor/bhr317
   Okun M, 2008, NAT NEUROSCI, V11, P535, DOI 10.1038/nn.2105
   Ozeki H, 2009, NEURON, V62, P578, DOI 10.1016/j.neuron.2009.03.028
   Persi E, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1001078
   Petersen CCH, 2017, NEURON, V95, P1266, DOI 10.1016/j.neuron.2017.06.049
   Pfeffer CK, 2013, NAT NEUROSCI, V16, P1068, DOI 10.1038/nn.3446
   Piscopo DM, 2013, J NEUROSCI, V33, P4642, DOI 10.1523/JNEUROSCI.5187-12.2013
   Poo C, 2011, NEURON, V72, P41, DOI 10.1016/j.neuron.2011.08.015
   Powell DJ, 2021, CURR BIOL, V31, P4831, DOI 10.1016/j.cub.2021.08.042
   Priebe NJ, 2004, NAT NEUROSCI, V7, P1113, DOI 10.1038/nn1310
   Purves D., 2001, NEUROSCIENCE
   Rancz EA, 2011, NAT NEUROSCI, V14, P527, DOI 10.1038/nn.2765
   Renart A, 2010, SCIENCE, V327, P587, DOI 10.1126/science.1179850
   Ricciardi L.M., 1977, DIFFUSION PROCESSES
   Ringach D, 2004, COGNITIVE SCI, V28, P147, DOI 10.1016/j.cogsci.2003.11.003
   Rosenbaum R, 2014, PHYS REV X, V4, DOI 10.1103/PhysRevX.4.021039
   Roxin A, 2011, J NEUROSCI, V31, P16217, DOI 10.1523/JNEUROSCI.1677-11.2011
   Rubin DB, 2015, NEURON, V85, P402, DOI 10.1016/j.neuron.2014.12.026
   Rupprecht P, 2018, NEURON, V100, P669, DOI 10.1016/j.neuron.2018.09.013
   Sadeh S, 2021, NAT REV NEUROSCI, V22, P21, DOI 10.1038/s41583-020-00390-z
   Sanzeni A, 2020, PLOS COMPUT BIOL, V16, DOI 10.1371/journal.pcbi.1008165
   Sanzeni A, 2020, ELIFE, V9, DOI 10.7554/eLife.54875
   Schwalger T, 2017, PLOS COMPUT BIOL, V13, DOI 10.1371/journal.pcbi.1005507
   Seeman SC, 2018, ELIFE, V7, DOI 10.7554/eLife.37349
   SOFTKY WR, 1993, J NEUROSCI, V13, P334
   Stobb M, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0037292
   Stringer C, 2019, NATURE, V571, P361, DOI 10.1038/s41586-019-1346-5
   Stringer C, 2019, SCIENCE, V364, P255, DOI 10.1126/science.aav7893
   Tan AYY, 2011, J NEUROSCI, V31, P12339, DOI 10.1523/JNEUROSCI.2039-11.2011
   Tchumatchenko T, 2010, PHYS REV LETT, V104, DOI 10.1103/PhysRevLett.104.058102
   Tsodyks MV, 1997, J NEUROSCI, V17, P4382
   van Vreeswijk C, 1998, NEURAL COMPUT, V10, P1321, DOI 10.1162/089976698300017214
   vanVreeswijk C, 1996, SCIENCE, V274, P1724, DOI 10.1126/science.274.5293.1724
   WILSON HR, 1972, BIOPHYS J, V12, P1, DOI 10.1016/S0006-3495(72)86068-5
   WOOLSEY TA, 1970, BRAIN RES, V17, P205, DOI 10.1016/0006-8993(70)90079-X
   Wu YK, 2021, ELIFE, V10, DOI [10.7554/eLife.71263, 10.7554/eLife.71263.sa0, 10.7554/eLife.71263.sa1, 10.7554/eLife.71263.sa2]
NR 87
TC 0
Z9 0
U1 1
U2 1
PD MAY
PY 2023
VL 19
IS 5
AR e1011097
DI 10.1371/journal.pcbi.1011097
UT WOS:000987879500003
DA 2023-11-16
ER

PT C
AU Zajzon, B
   Duarte, R
   Morrison, A
AF Zajzon, Barna
   Duarte, Renato
   Morrison, Abigail
GP IEEE
TI Transferring State Representations in Hierarchical Spiking Neural
   Networks
SO 2018 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUL 08-13, 2018
CL Rio de Janeiro, BRAZIL
DE stimulus representation; state transfer; modularity; spiking neural
   networks
AB Hierarchical modularity is a parsimonious design principle in many complex systems and underlies various key structural and functional aspects of neurobiological systems, whose modules are recurrent networks of spiking neurons. An essential requirement for such systems to adequately function is the ability to transfer information across multiple modules in a reliable and efficient manner. In this work, we study the characteristics of emergent stimulus representations in recurrent, spiking neural networks and the features that allow efficient information transfer among multiple, interacting sub-networks. We find that the specificity of structural mappings between the modules is strictly required for information to propagate to a sufficient depth, in a sequential setup. Conserved topography not only improves computational performance in all scenarios analyzed, but it proves to be more robust against noise and interference effects, results in less variability in the neural responses and increases memory capacity.
C1 [Zajzon, Barna; Duarte, Renato; Morrison, Abigail] Julich Res Ctr, Inst Neurosci & Med INM 6, Inst Adv Simulat IAS 6, Julich, Germany.
   [Zajzon, Barna; Duarte, Renato; Morrison, Abigail] Julich Res Ctr, JARA Inst Brain Struct Funct Relationships JBI 1, Julich, Germany.
   [Zajzon, Barna] Rhein Westfal TH Aachen, Dept Psychiat Psychotherapy & Psychosomat, Aachen, Germany.
   [Morrison, Abigail] Ruhr Univ Bochum, Fac Psychol, Inst Cognit Neurosci, Bochum, Germany.
RP Zajzon, B (corresponding author), Julich Res Ctr, Inst Neurosci & Med INM 6, Inst Adv Simulat IAS 6, Julich, Germany.; Zajzon, B (corresponding author), Julich Res Ctr, JARA Inst Brain Struct Funct Relationships JBI 1, Julich, Germany.; Zajzon, B (corresponding author), Rhein Westfal TH Aachen, Dept Psychiat Psychotherapy & Psychosomat, Aachen, Germany.
EM b.zajzon@fz-juelich.de
CR [Anonymous], 2016, NAT METHODS, DOI DOI 10.1038/nmeth.3707
   [Anonymous], 2017, NEST 2 12 10
   Duarte R., 2017, BIORXIV
   Duarte R, 2017, NEURAL MICROCIRCUIT
   Duarte R, 2017, CURR OPIN NEUROBIOL, V43, P156, DOI 10.1016/j.conb.2017.02.007
   Duarte RCF, 2014, FRONT COMPUT NEUROSC, V8, DOI 10.3389/fncom.2014.00124
   Enel P, 2016, PLOS COMPUT BIOL, V12, DOI 10.1371/journal.pcbi.1004967
   Gallicchio C, 2017, NEUROCOMPUTING, V268, P87, DOI 10.1016/j.neucom.2016.12.089
   Jaeger H., 2002, ADV NEURAL INF PROCE, V15, P609
   Lukosevicius M, 2009, COMPUT SCI REV, V3, P127, DOI 10.1016/j.cosrev.2009.03.005
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Maass W, 2004, J PHYSIOL-PARIS, V98, P315, DOI 10.1016/j.jphysparis.2005.09.020
   Rigotti M, 2013, NATURE, V497, P585, DOI 10.1038/nature12160
   van den Broek D, 2017, BEST SPIKE FILTER KE
   Vogels TP, 2009, NAT NEUROSCI, V12, P483, DOI 10.1038/nn.2276
NR 15
TC 0
Z9 0
U1 0
U2 1
PY 2018
UT WOS:000585967401120
DA 2023-11-16
ER

PT J
AU Song, S
   Jeon, B
   Kim, M
   Kim, JJ
AF Song, Seunghwan
   Jeon, Bosung
   Kim, Munhyeon
   Kim, Jae-Joon
TI Efficient Convolutional Processing of Spiking Neural Network With
   Weight-Sharing Filters
SO IEEE ELECTRON DEVICE LETTERS
DT Article
DE Convolution; Logic gates; Synapses; Tin; Neural networks; Convolutional
   neural networks; Silicon; Charge trap flash (CTF); efficient
   convolutional processing; spiking neural network (SNN)
AB The importance of implementing an efficient convolutional neural network (CNN) is increasing. A weight-sharing spiking CNN inference system (WS-SCNN) employing efficient convolution layers (ECLs) is proposed and modeled to enable the compact convolutional processing of the spiking neural network (SNN) inference. The proposed ECL efficiently maps convolutional features between inputs and filter weights. The ECL does not replicate the synaptic filter array with respect to input sliding, which minimizes the number of synaptic devices required to implement hardware SNNs. A four-bit weight quantization capability of a fabricated charge-trap flash synaptic device is used to verify the accurate multiplication and summation of weights in the ECL. Moreover, a nine-layer WS-SCNN consisting of multiple ECLs is modeled, and the benefits of the WS-SCNN in terms of the area and energy are evaluated. Simulation results show that the WS-SCNN has 5.68 and 103.5 times higher energy and area efficiency than conventional SCNN systems, respectively.
C1 [Song, Seunghwan; Jeon, Bosung; Kim, Munhyeon; Kim, Jae-Joon] Seoul Natl Univ, Dept Elect & Comp Engn, Seoul 08826, South Korea.
   [Song, Seunghwan; Jeon, Bosung; Kim, Munhyeon; Kim, Jae-Joon] Seoul Natl Univ, Interuniv Semicond Res Ctr, Seoul 08826, South Korea.
RP Kim, JJ (corresponding author), Seoul Natl Univ, Dept Elect & Comp Engn, Seoul 08826, South Korea.; Kim, JJ (corresponding author), Seoul Natl Univ, Interuniv Semicond Res Ctr, Seoul 08826, South Korea.
EM kimjaejoon@snu.ac.kr
CR Brown S. D., 2012, FIELD PROGRAMMABLE G, V180
   bsim, BSIM CMG TECHNICAL M
   Choi HS, 2020, IEEE ELECTR DEVICE L, V41, P1653, DOI 10.1109/LED.2020.3025587
   Diehl P.U., 2015, 2015 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2015.7280696
   Esser S. K., P NATL ACAD SCI USA, V113
   Ho ND, 2021, DES AUT CON, P793, DOI 10.1109/DAC18074.2021.9586266
   Hwang S, 2022, IEEE ELECTR DEVICE L, V43, P549, DOI 10.1109/LED.2022.3149029
   Hwang S, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-60572-8
   Ielmini D, 2020, ADV INTELL SYST-GER, V2, DOI 10.1002/aisy.202000040
   Kim C, 2017, ISSCC DIG TECH PAP I, P202, DOI 10.1109/ISSCC.2017.7870331
   Kim H, 2018, IEEE ELECTR DEVICE L, V39, P630, DOI 10.1109/LED.2018.2809661
   Kim J, 2018, I SYMPOS LOW POWER E, P13, DOI 10.1145/3218603.3218639
   Kim TH, 2022, IEEE T ELECTRON DEV, V69, P3151, DOI 10.1109/TED.2022.3169112
   Kim Y, 2015, ACM J EMERG TECH COM, V11, DOI 10.1145/2700234
   LeCun Y, 1995, HDB BRAIN THEORY NEU, V3361, P1995, DOI 10.5555/303568.303704
   Li YS, 2021, ADV INTELL SYST-GER, V3, DOI 10.1002/aisy.202000137
   Malavena G, 2019, IEEE T ELECTRON DEV, V66, P4727, DOI 10.1109/TED.2019.2940602
   Song S, 2022, IEEE ELECTR DEVICE L, V43, P1657, DOI 10.1109/LED.2022.3197239
   Sugizaki T., 2003, 2003 Symposium on VLSI Technology. Digest of Technical Papers (IEEE Cat. No.03CH37407), P27, DOI 10.1109/VLSIT.2003.1221069
   Tiomkin E., IND TEMPERATURE NAND
   Yu SM, 2021, IEEE T CIRCUITS-I, V68, P2753, DOI 10.1109/TCSI.2021.3072200
   Zhong YN, 2022, NAT ELECTRON, V5, P672, DOI 10.1038/s41928-022-00838-3
NR 22
TC 0
Z9 0
U1 5
U2 5
PD JUN
PY 2023
VL 44
IS 6
BP 1007
EP 1010
DI 10.1109/LED.2023.3265065
UT WOS:001001401500033
DA 2023-11-16
ER

PT C
AU Ahmadi, A
   Zwolinski, M
AF Ahmadi, Arash
   Zwolinski, Mark
GP IEEE
TI A Modified Izhikevich Model For Circuit Implementation of Spiking Neural
   Networks
SO 2010 FIRST IEEE LATIN AMERICAN SYMPOSIUM ON CIRCUITS AND SYSTEMS
   (LASCAS)
SE IEEE Latin American Symposium on Circuits and Systems
DT Proceedings Paper
CT 1st IEEE Latin American Symposium on Circuits and Systems (LASCAS)
CY FEB 24-26, 2010
CL BAHAMAS
DE Neural networks; Neural network hardware
AB The Izhikevich neuron model reproduces the spiking and bursting behaviour of certain types of cortical neurons. This model has a second order non-linearity that makes it difficult to implement in hardware. We propose a simplified version of the model that has a piecewise-linear relationship. This modification simplifies the hardware implementation but demonstrates similar dynamic behaviour.
C1 [Ahmadi, Arash; Zwolinski, Mark] Univ Southampton, Sch Elect & Comp Sci, Southampton SO17 1BJ, Hants, England.
RP Ahmadi, A (corresponding author), Univ Southampton, Sch Elect & Comp Sci, Southampton SO17 1BJ, Hants, England.
EM aa5@ecs.soton.ac.uk; mzc@ecs.soton.ac.uk
CR [Anonymous], J PHYSL, DOI DOI 10.1007/BF02459568
   Brunel N, 2007, BIOL CYBERN, V97, P337, DOI 10.1007/s00422-007-0190-0
   Cassidy A, 2007, 2007 IEEE BIOMEDICAL CIRCUITS AND SYSTEMS CONFERENCE, P75, DOI 10.1109/BIOCAS.2007.4463312
   Furber S, 2006, P AISB WORKSH GC5 AR, P29
   Gerstner W., 2002, SPIKING NEURON MODEL
   Glackin B, 2005, LECT NOTES COMPUT SC, V3512, P552
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM., 2007, DYNAMICAL SYSTEMS NE, DOI [DOI 10.1017/S0143385704000173, 10.7551/mitpress/2526.001.0001]
   Mokhtar M, 2008, LECT NOTES COMPUT SC, V5216, P362
   Renaud S, 2007, IEEE INT SYMP CIRC S, P3355, DOI 10.1109/ISCAS.2007.378286
   Shayani H., 2008, NASA ESA C AD HARDW, P236
   Shayani H., 2008, 16 EUR S ART NEUR NE, P197
   Wijekoon JHB, 2008, NEURAL NETWORKS, V21, P524, DOI 10.1016/j.neunet.2007.12.037
NR 13
TC 1
Z9 1
U1 0
U2 0
PY 2010
BP 192
EP 195
UT WOS:000392285500039
DA 2023-11-16
ER

PT C
AU Daffron, C
   Chan, J
   Disney, A
   Bechtel, L
   Wagner, R
   Dean, ME
   Rose, GS
   Plank, JS
   Birdwell, JD
   Schuman, CD
AF Daffron, Christopher
   Chan, Jason
   Disney, Adam
   Bechtel, Luke
   Wagner, Ryan
   Dean, Mark E.
   Rose, Garrett S.
   Plank, James S.
   Birdwell, J. Douglas
   Schuman, Catherine D.
GP IEEE
TI Extensions and Enhancements for the DANNA Neuromorphic Architecture
SO SOUTHEASTCON 2016
SE IEEE SoutheastCon-Proceedings
DT Proceedings Paper
CT SoutheastCon
CY MAR 30-APR 03, 2016
CL Norfolk, VA
ID SPIKING; NETWORK
AB Dynamic Adaptive Neural Network Arrays (DANNAs) are neuromorphic systems that have been developed for hardware implementation. They feature highly adaptive and programmable structural elements, which model artificial neural networks with spiking behavior. In this paper, we highlight the current hardware implementations of DANNA, including their features and functionalities. We conclude with future directions.
C1 [Daffron, Christopher; Chan, Jason; Disney, Adam; Bechtel, Luke; Wagner, Ryan; Dean, Mark E.; Rose, Garrett S.; Plank, James S.; Birdwell, J. Douglas] Univ Tennessee, Dept Elect Engn & Comp Sci, Knoxville, TN 37996 USA.
   [Schuman, Catherine D.] Oak Ridge Natl Lab, Computat Data Analyt, Oak Ridge, TN USA.
RP Daffron, C (corresponding author), Univ Tennessee, Dept Elect Engn & Comp Sci, Knoxville, TN 37996 USA.
CR Anand R, 2009, SELF-DEFENSE IN INTERNATIONAL RELATIONS, P1, DOI 10.1057/9780230245747
   [Anonymous], 2014, P 2014 BIOM SCI ENG
   Bartolozzi C, 2007, NEURAL COMPUT, V19, P2581, DOI 10.1162/neco.2007.19.10.2581
   Cassidy A, 2007, 2007 IEEE BIOMEDICAL CIRCUITS AND SYSTEMS CONFERENCE, P75, DOI 10.1109/BIOCAS.2007.4463312
   Dean Mark E., 2014, Unconventional Computation and Natural Computation. 13th International Conference. Proceedings: LNCS 8553, P129, DOI 10.1007/978-3-319-08123-6_11
   Furber SB, 2013, IEEE T COMPUT, V62, P2454, DOI 10.1109/TC.2012.142
   Glackin B, 2005, LECT NOTES COMPUT SC, V3512, P552
   Hu M, 2012, DES AUT CON, P498
   Jackson BL, 2013, ACM J EMERG TECH COM, V9, DOI 10.1145/2463585.2463588
   Kunkel S, 2012, FRONT NEUROINFORM, V5, DOI 10.3389/fninf.2011.00035
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Renaud S, 2007, IEEE INT SYMP CIRC S, P3355, DOI 10.1109/ISCAS.2007.378286
   Snook J., 2009, US Patent, Patent No. [7,533,071, 7533071]
   Soltiz M, 2013, IEEE T COMPUT, V62, P1597, DOI 10.1109/TC.2013.75
   Vogelstein RJ, 2007, IEEE T NEURAL NETWOR, V18, P253, DOI 10.1109/TNN.2006.883007
NR 15
TC 1
Z9 1
U1 0
U2 0
PY 2016
UT WOS:000387067900120
DA 2023-11-16
ER

PT C
AU Kushawaha, RK
   Kumar, S
   Banerjee, B
   Velmurugan, R
AF Kushawaha, Ravi Kumar
   Kumar, Saurabh
   Banerjee, Biplab
   Velmurugan, Rajbabu
GP IEEE COMP SOC
TI Distilling Spikes: Knowledge Distillation in Spiking Neural Networks
SO 2020 25TH INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION (ICPR)
SE International Conference on Pattern Recognition
DT Proceedings Paper
CT 25th International Conference on Pattern Recognition (ICPR)
CY JAN 10-15, 2021
CL ELECTR NETWORK
AB Spiking Neural Networks (SNN) are energy-efficient computing architectures that exchange spikes for processing information, unlike classical Artificial Neural Networks (ANN). Due to this, SNNs are better suited for real-life deployments. However, similar to ANNs, SNNs also benefit from deeper architectures to obtain improved performance. Furthermore, like the deep ANNs, the memory, compute and power requirements of SNNs also increase with model size, and model compression becomes a necessity. Knowledge distillation is a model compression technique that enables transferring the learning of a large machine learning model to a smaller model with minimal loss in performance. In this paper, we propose techniques for knowledge distillation in spiking neural networks for the task of image classification. We present ways to distill spikes from a larger SNN, also called the teacher network, to a smaller one, also called the student network, while minimally impacting the classification accuracy. We demonstrate the effectiveness of the proposed method with detailed experiments on three standard datasets while proposing novel distillation methodologies and loss functions. We also present a multi-stage knowledge distillation technique for SNNs using an intermediate network to obtain higher performance from the student network. Our approach is expected to open up new avenues for deploying high performing large SNN models on resource-constrained hardware platforms.
C1 [Kushawaha, Ravi Kumar; Kumar, Saurabh; Banerjee, Biplab; Velmurugan, Rajbabu] Indian Inst Technol, Mumbai, Maharashtra, India.
RP Kushawaha, RK (corresponding author), Indian Inst Technol, Mumbai, Maharashtra, India.
EM rkkush2397@gmail.com; saurabhkm@iitb.ac.in; bbanerjee@iitb.ac.in;
   rajbabu@ee.iitb.ac.in
CR Ahn S, 2019, PROC CVPR IEEE, P9155, DOI 10.1109/CVPR.2019.00938
   Ankit A, 2017, DES AUT CON, DOI 10.1145/3061639.3062311
   Bucilua C., 2006, P 12 ACM SIGKDD INT, P535, DOI DOI 10.1145/1150402.1150464
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Chen G., 2017, P 31 INT C NEUR INF, P742
   Cheng Yu, 2017, ARXIV171009282
   Diamond A, 2016, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00491
   Feng M, 2009, PROC IEEE INT CONF S, P105, DOI 10.1109/ICSM.2009.5306329
   Gerstner W., 2002, SPIKING NEURON MODEL
   Harris CR, 2020, NATURE, V585, P357, DOI 10.1038/s41586-020-2649-2
   Hinton Geoffrey, 2015, ARXIV150302531
   Huang MK, 2018, INTERSPEECH, P3703, DOI 10.21437/Interspeech.2018-1589
   Hunter JD, 2007, COMPUT SCI ENG, V9, P90, DOI 10.1109/MCSE.2007.55
   Kempter R, 1999, PHYS REV E, V59, P4498, DOI 10.1103/PhysRevE.59.4498
   Kim S, 2020, AAAI CONF ARTIF INTE, V34, P11270
   Kim Yoon, 2016, ARXIV160607947
   Kumar S., 2019, ARXIV190810559
   Lee JM, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00191
   Lin CK, 2018, ACM SIGPLAN NOTICES, V53, P78, DOI [10.1145/3192366.3192371, 10.1145/3296979.3192371]
   Liu Xiaodong, 2019, ARXIV190409482
   Mirzadeh Seyed Iman, 2020, ARXIV190203393, P5191, DOI DOI 10.1609/AAAI.V34I04.5963
   Neil D, 2016, P 31 ANN ACM S APPL
   Paszke A., 2019, ADV NEURAL INFORM PR
   Paugam-Moisy H., 2012, HDB NATURAL COMPUTIN, V1, P1, DOI [DOI 10.1007/978-3-540-92910-9_10, 10.1007/978-3-540-92910-9_10]
   Rossum, 1995, PYTHON REFERENCE MAN
   Samaddar R, 2019, GLOBAL GOVERNANCE AND INDIA'S NORTH-EAST: LOGISTICS, INFRASTRUCTURE AND SOCIETY, P1
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Tian Yonglong, 2019, ICLR
   Virgilio CD, 2020, NEURAL NETWORKS, V122, P130, DOI 10.1016/j.neunet.2019.09.037
   Wu YJ, 2019, AAAI CONF ARTIF INTE, P1311
   Xiao H., 2017, ARXIV170807747
   Yin SY, 2017, DES AUT CON, DOI 10.1145/3061639.3062232
NR 32
TC 5
Z9 5
U1 1
U2 11
PY 2021
BP 4536
EP 4543
DI 10.1109/ICPR48806.2021.9412147
UT WOS:000678409204088
DA 2023-11-16
ER

PT J
AU Pérez, J
   Alcázar, M
   Sánchez, I
   Cabrera, JA
   Nybacka, M
   Castillo, JJ
AF Perez, Javier
   Alcazar, Manuel
   Sanchez, Ignacio
   Cabrera, Juan A.
   Nybacka, Mikael
   Castillo, Juan J.
TI On-line learning applied to spiking neural network for antilock braking
   systems
SO NEUROCOMPUTING
DT Article
DE Antilock brake system; Spiking neural network; On-line learning;
   Supervised learning; Vehicle dynamics; Vehicle safety
ID MODEL
AB Computationally replicating the behaviour of the cerebral cortex to perform the control tasks of daily life in a human being is a challenge today. First, it is necessary to know the structure and connections between the elements of the neural network that perform movement control. Next, a mathematical neural model that adequately resembles biological neurons has to be developed. Finally, a suitable learning model that allows adapting neural network response to changing conditions in the environment is also required. Spiking Neural Networks (SNN) are currently the closest approximation to biological neural networks. SNNs make use of temporal spike trains to deal with inputs and outputs, thus allowing a faster and more complex computation. In this paper, a controller based on an SNN is proposed to perform the control of an anti-lock braking system (ABS) in vehicles. To this end, two neural networks are used to regulate the braking force. The first one is devoted to estimating the optimal slip while the second one is in charge of setting the optimal braking pressure. The latter resembles biological reflex arcs to ensure stability during operation. This neural structure is used to control the fast regulation cycles that occur during ABS operation. Furthermore, an algorithm has been developed to train the network while driving. On-line learning is proposed to update the response of the controller. Hence, to cope with real conditions, a control algorithm based on neural networks that learn by making use of neural plasticity, similar to what occurs in biological systems, has been implemented. Neural connections are modulated using Spike-Timing-Dependent Plasticity (STDP) by means of a supervised learning structure using the slip error as input. Road-type detection has been included in the same neural structure. To validate and to evaluate the performance of the proposed algorithm, simulations as well as experiments in a real vehicle were carried out. The algorithm proved to be able to adapt to changes in adhesion conditions rapidly. This way, the capability of spiking neural networks to perform the full control logic of the ABS has been verified.
C1 [Perez, Javier; Alcazar, Manuel; Sanchez, Ignacio; Cabrera, Juan A.; Castillo, Juan J.] Univ Malaga, Dept Mech Engn, Malaga 29071, Spain.
   [Nybacka, Mikael] KTH Royal Inst Technol, Dept Engn Mech, SE-10044 Stockholm, Sweden.
   [Nybacka, Mikael] KTH Royal Inst Technol, Integrated Transport Res Lab, SE-10044 Stockholm, Sweden.
RP Cabrera, JA (corresponding author), Univ Malaga, Dept Mech Engn, Malaga 29071, Spain.
EM jcabrera@uma.es
CR Amirkhani AOL, 2022, IEEE ACCESS, V10, P58736, DOI 10.1109/ACCESS.2022.3179700
   Arriandiaga A, 2020, IEEE T NEUR NET LEAR, V31, P3920, DOI 10.1109/TNNLS.2019.2947380
   Buchanan K.A., 2010, FRONT SYNAPTIC NEURO, V2, P1
   Comsa I.M., 2019, IEEE T NEUR NET LEAR, P1
   DeWolf T, 2020, FRONT NEUROROBOTICS, V14, DOI 10.3389/fnbot.2020.568359
   Doya K, 1999, NEURAL NETWORKS, V12, P961, DOI 10.1016/S0893-6080(99)00046-5
   Feldman A.G., 2020, J NEUROPHYSIOL
   Feldman AG., 2015, REFERENT CONTROL ACT, DOI 10.1007/978-1-4939-2736-4
   Fernández JP, 2021, IEEE T VEH TECHNOL, V70, P1255, DOI 10.1109/TVT.2021.3055142
   Guan XC, 2020, IEEE ACCESS, V8, P17673, DOI 10.1109/ACCESS.2020.2968240
   Haggerty SE, 2018, FRONT SYST NEUROSCI, V12, DOI 10.3389/fnsys.2018.00004
   HENNEMAN E, 1957, SCIENCE, V126, P1345, DOI 10.1126/science.126.3287.1345
   Ivanov V, 2015, IEEE T VEH TECHNOL, V64, P3878, DOI 10.1109/TVT.2014.2361860
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2007, CEREB CORTEX, V17, P2443, DOI 10.1093/cercor/bhl152
   Jain LC, 2002, STUD FUZZ SOFT COMP, V84, P137
   Jing HH, 2011, IEEE T VEH TECHNOL, V60, P1470, DOI 10.1109/TVT.2011.2125806
   Kandel ER, 2001, SCIENCE, V294, P1030, DOI 10.1126/science.1067020
   Kiencke U., 2005, AUTOMOTIVE CONTROL S
   Lagani G, 2021, NEURAL NETWORKS, V143, P719, DOI 10.1016/j.neunet.2021.08.003
   Lin CM, 2003, IEEE T NEURAL NETWOR, V14, P351, DOI 10.1109/TNN.2002.806950
   Liu C, 2021, IEEE ACCESS, V9, P17071, DOI 10.1109/ACCESS.2021.3053280
   Nandakumar SR, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-64878-5
   Oniz Y, 2015, NEUROCOMPUTING, V149, P690, DOI 10.1016/j.neucom.2014.07.061
   Pacejka HB, 2012, TIRE AND VEHICLE DYNAMICS, 3RD EDITION, P1, DOI 10.1016/B978-0-08-097016-5.00001-2
   Fernández JP, 2021, NEUROCOMPUTING, V463, P237, DOI 10.1016/j.neucom.2021.08.005
   Pretagostini F, 2020, IEEE ACCESS, V8, P10951, DOI 10.1109/ACCESS.2020.2965644
   Radac MB, 2018, NEUROCOMPUTING, V275, P317, DOI 10.1016/j.neucom.2017.08.036
   Ranjan JAK, 2020, J SUPERCOMPUT, V76, P6545, DOI 10.1007/s11227-019-02881-y
   Raychaudhuri T, 1995, CONTROL 95, P369
   Reif K., 2014, BRAKES BRAKE CONTROL
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Sabanovi E., 2020, IDENTIFICATION ROAD
   Sassella A, 2022, NONLINEAR ANAL-HYBRI, V46, DOI 10.1016/j.nahs.2022.101220
   Savitski D, 2016, INT J AUTO TECH-KOR, V17, P327, DOI 10.1007/s12239-016-0033-x
   Shulz D.E., 2013, SPIKE TIMING DEPENDE
   Su J, 2021, IEEE ACCESS, V9, P51950, DOI 10.1109/ACCESS.2021.3068159
   Taherkhani A, 2018, IEEE T NEUR NET LEAR, V29, P5394, DOI 10.1109/TNNLS.2018.2797801
   Tan HW, 2019, ADV INTELL SYST-GER, V1, DOI 10.1002/aisy.201900036
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Tavernini D, 2020, IEEE T IND ELECTRON, V67, P3990, DOI 10.1109/TIE.2019.2916387
   Taylor P., 2010, VEH SYST DYN INT J V, P37
   Voutsas K, 2007, IEEE T NEURAL NETWOR, V18, P1785, DOI 10.1109/TNN.2007.899623
   Wang HW, 2021, IEEE ACCESS, V9, P40349, DOI 10.1109/ACCESS.2021.3064960
   Wang XW, 2020, NEURAL NETWORKS, V125, P258, DOI 10.1016/j.neunet.2020.02.011
NR 46
TC 0
Z9 0
U1 2
U2 2
PD NOV 28
PY 2023
VL 559
AR 126784
DI 10.1016/j.neucom.2023.126784
EA SEP 2023
UT WOS:001083784000001
DA 2023-11-16
ER

PT J
AU Yang, SM
   Deng, B
   Li, HY
   Liu, C
   Wang, J
   Yu, HT
   Qin, YM
AF Yang, Shuangming
   Deng, Bin
   Li, Huiyan
   Liu, Chen
   Wang, Jiang
   Yu, Haitao
   Qin, Yingmei
TI FPGA implementation of hippocampal spiking network and its real-time
   simulation on dynamical neuromodulation of oscillations
SO NEUROCOMPUTING
DT Article
DE Field programmable gate array (FPGA); Spiking neural network;
   High-performance neurocomputing; Hippocampal oscillations; Real-time
   implementation
ID NEURAL-NETWORK; THETA RHYTHM; BASAL GANGLIA; NAVIGATION; MODELS; MEMORY
AB Neural information is represented and transmitted among single neurons by a series of all-or-none neural codes with certain oscillation dynamics. Real-time implementation of the hippocampal spiking network is a promising avenue to investigate the complexity underlying spatiotemporal information encoding and the emergent coherence that arises with the properly coupling of large number of neurons. This paper presents a real-time scalable hardware platform for implementing hippocampal spiking neural network (HSNN) with 10 K neurons, which introduces a novel network-on-chip architecture for the randomly connected spiking neural networks (SNNs). The effects of endogenous surroundings and neural heterogeneity are taken into consideration in the hardware design, which replicates more relevant biological dynamics in comparison with the state-of-the-art studies. Based on the hardware synthesis and theoretical analysis, it is demonstrated that the proposed implementation is able to mimic hippocampal oscillation modulation dynamics under external stimuli, which is vital for the reasonable design of noninvasive electrotherapeutic strategies. The proposed implementation is meaningful for both the efficient hardware implementation of the randomly connected SNNs and the dynamic investigation of the HSNNs. (C) 2017 Elsevier B.V. All rights reserved.
C1 [Yang, Shuangming; Deng, Bin; Liu, Chen; Wang, Jiang; Yu, Haitao] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Li, Huiyan; Qin, Yingmei] Tianjin Univ Technol & Educ, Sch Automat & Elect Engn, Tianjin 300222, Peoples R China.
RP Yu, HT (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
EM htyu@tju.edu.cn
CR Anastassiou CA, 2010, J NEUROSCI, V30, P1925, DOI 10.1523/JNEUROSCI.3635-09.2010
   [Anonymous], 2010, 2010 IEEE INT S PARA
   Axmacher N, 2010, P NATL ACAD SCI USA, V107, P3228, DOI 10.1073/pnas.0911531107
   Barbieri R, 2000, NEUROCOMPUTING, V32, P629, DOI 10.1016/S0925-2312(00)00225-3
   Berger TW, 2012, IEEE T NEUR SYS REH, V20, P198, DOI 10.1109/TNSRE.2012.2189133
   Bonabi SY, 2014, FRONT NEUROSCI-SWITZ, V8, DOI 10.3389/fnins.2014.00379
   Bray LCJ, 2010, FRONT NEURAL CIRCUIT, V4, DOI 10.3389/fncir.2010.00122
   Bush D, 2010, PLOS COMPUT BIOL, V6, DOI 10.1371/journal.pcbi.1000839
   Buzsáki G, 2005, HIPPOCAMPUS, V15, P827, DOI 10.1002/hipo.20113
   Buzsáki G, 2013, NAT NEUROSCI, V16, P130, DOI 10.1038/nn.3304
   Cassidy A, 2008, 2008 IEEE BIOMEDICAL CIRCUITS AND SYSTEMS CONFERENCE - INTELLIGENT BIOMEDICAL SYSTEMS (BIOCAS), P289, DOI 10.1109/BIOCAS.2008.4696931
   Eichenbaum H, 2000, NAT REV NEUROSCI, V1, P41, DOI 10.1038/35036213
   Hájos N, 2004, J NEUROSCI, V24, P9127, DOI 10.1523/JNEUROSCI.2113-04.2004
   Hartley T, 2014, PHILOS T R SOC B, V369, DOI 10.1098/rstb.2012.0510
   Hasselmo ME, 2001, NEUROCOMPUTING, V38, P633, DOI 10.1016/S0925-2312(01)00411-8
   Himavathi S, 2007, IEEE T NEURAL NETWOR, V18, P880, DOI 10.1109/TNN.2007.891626
   Huhn Z, 2005, HIPPOCAMPUS, V15, P950, DOI 10.1002/hipo.20112
   Igarashi J, 2011, NEURAL NETWORKS, V24, P950, DOI 10.1016/j.neunet.2011.06.008
   Iyer SS, 2005, IBM J RES DEV, V49, P333, DOI 10.1147/rd.492.0333
   Javitt DC, 2008, NAT REV DRUG DISCOV, V7, P68, DOI 10.1038/nrd2463
   Johnson EL, 2015, CURR OPIN NEUROBIOL, V31, P18, DOI 10.1016/j.conb.2014.07.021
   Kodogiannis VS, 2013, INT J NEURAL SYST, V23, DOI 10.1142/S012906571350024X
   Lega B, 2016, CEREB CORTEX, V26, P268, DOI 10.1093/cercor/bhu232
   Lega BC, 2012, HIPPOCAMPUS, V22, P748, DOI 10.1002/hipo.20937
   Maguire LP, 2007, NEUROCOMPUTING, V71, P13, DOI 10.1016/j.neucom.2006.11.029
   Marshall L, 2005, BMC NEUROSCI, V6, DOI 10.1186/1471-2202-6-23
   Mejias JF, 2012, PHYS REV LETT, V108, DOI 10.1103/PhysRevLett.108.228102
   Memmesheimer RM, 2010, P NATL ACAD SCI USA, V107, P11092, DOI 10.1073/pnas.0909615107
   Minkovich K, 2014, IEEE T NEUR NET LEAR, V25, P316, DOI 10.1109/TNNLS.2013.2276056
   Misra J, 2010, NEUROCOMPUTING, V74, P239, DOI 10.1016/j.neucom.2010.03.021
   Mokhtar M, 2008, LECT NOTES COMPUT SC, V5216, P362
   Oren I, 2006, J NEUROSCI, V26, P9923, DOI 10.1523/JNEUROSCI.1580-06.2006
   Palop JJ, 2007, NEURON, V55, P697, DOI 10.1016/j.neuron.2007.07.025
   Palop JJ, 2006, NATURE, V443, P768, DOI 10.1038/nature05289
   Petrovici MA, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0108590
   Pevzner A, 2016, FRONT SYST NEUROSCI, V10, DOI 10.3389/fnsys.2016.00030
   Reato D, 2010, J NEUROSCI, V30, P15067, DOI 10.1523/JNEUROSCI.2059-10.2010
   Recce M, 2000, NEUROCOMPUTING, V32, P225, DOI 10.1016/S0925-2312(00)00168-5
   Taxidis J, 2013, FRONT COMPUT NEUROSC, V7
   Wang RC, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00014
   Wang XJ, 1996, J NEUROSCI, V16, P6402
   Williams JH, 1997, J NEUROPHYSIOL, V78, P2631, DOI 10.1152/jn.1997.78.5.2631
   Xin Y, 2014, MICROELECTRON J, V45, P690, DOI 10.1016/j.mejo.2014.03.018
   Yang SM, 2016, NEUROCOMPUTING, V177, P274, DOI 10.1016/j.neucom.2015.11.026
   Yang SM, 2015, NEURAL NETWORKS, V71, P62, DOI 10.1016/j.neunet.2015.07.017
   Zamanlooy B, 2014, IEEE T VLSI SYST, V22, P39, DOI 10.1109/TVLSI.2012.2232321
NR 46
TC 23
Z9 24
U1 2
U2 28
PD MAR 22
PY 2018
VL 282
BP 262
EP 276
DI 10.1016/j.neucom.2017.12.031
UT WOS:000424893200025
DA 2023-11-16
ER

PT J
AU Ding, YQ
   Zuo, L
   Yang, KS
   Chen, ZS
   Hu, J
   Xiahou, T
AF Ding, Yongqi
   Zuo, Lin
   Yang, Kunshan
   Chen, Zhongshu
   Hu, Jian
   Xiahou, Tangfan
TI An improved probabilistic spiking neural network with enhanced
   discriminative ability
SO KNOWLEDGE-BASED SYSTEMS
DT Article
DE Spiking neural network; Probabilistic firing mechanism; Attention
   mechanism; Leaky integrate-and-fire neuron
ID COMPUTATION; POTENTIALS; DEEPER
AB The non-differentiability of the spike activity has been a hindrance to the development of high-performance spiking neural networks (SNNs). Current learning algorithms mainly focus on achieving attractive SNNs based on surrogate gradient or conversion, yet their performance is still limited. The probability-based SNNs use the probabilistic mechanism to smooth out spike activity, showing a promising way for training SNNs. This work optimizes the probabilistic mechanism and proposes the probabilistic firing mechanism (PFM) for spiking neurons. PFM enables differentiable spike activity and can be adapted to a variety of spiking neurons. In addition, to eliminate the negative influence of probabilistic uncertainty, the attention discrimination mechanism (ADM) is proposed, which enables the neurons to respond efficiently by adaptively distinguishing the salient elements of the input current. By fusing PFM, ADM, and Leaky Integrate-and-Fire (LIF) neurons, we constructed the Probabilistic Attention Leaky Integrate-and-Fire (PALIF) neuron and Probabilistic Attention Spiking Neural Network (PASNN). Ablation studies confirm the effectiveness of PFM and ADM, and indicate that PASNN is suitable for low-latency scenarios. Experiments on both static image and neuromorphic datasets, including CIFAR10, CIFAR100, N-MNIST, and CIFAR10-DVS, demonstrate that PASNN achieves competitive performance in terms of accuracy and inference speed.
C1 [Ding, Yongqi; Zuo, Lin; Yang, Kunshan; Hu, Jian] Univ Elect Sci & Technol China, Sch Informat & Software Engn, Chengdu, Sichuan, Peoples R China.
   [Chen, Zhongshu; Xiahou, Tangfan] Univ Elect Sci & Technol China, Sch Mech & Elect Engn, Chengdu, Sichuan, Peoples R China.
   [Zuo, Lin] Xiyuan Ave, Chengdu 611731, Sichuan, Peoples R China.
RP Zuo, L (corresponding author), Xiyuan Ave, Chengdu 611731, Sichuan, Peoples R China.
EM linzuo@uestc.edu.cn
CR Abro WA, 2022, KNOWL-BASED SYST, V242, DOI 10.1016/j.knosys.2022.108318
   Andrew A.M., 2003, KYBERNETES, P32
   Bing Han, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13555, DOI 10.1109/CVPR42600.2020.01357
   Bu T., 2022, P INT C LEARN REPR I
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Cheng X, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1519
   Deng S., 2022, P INT C LEARN REPR I
   Ding J., 2021, P 30 INT JOINT C ART
   Ding XH, 2021, PROC CVPR IEEE, P13728, DOI 10.1109/CVPR46437.2021.01352
   Fang W, 2021, ADV NEUR IN, V34
   Fang W, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2641, DOI 10.1109/ICCV48922.2021.00266
   Feng L, 2022, P 31 INT JOINT C ART, P2471, DOI DOI 10.24963/IJCAI
   Gidon A, 2020, SCIENCE, V367, P83, DOI 10.1126/science.aax6239
   Guo YF, 2023, PATTERN RECOGN, V142, DOI 10.1016/j.patcog.2023.109639
   Guo Y, 2022, LECT NOTES COMPUT SC, V13672, P52, DOI 10.1007/978-3-031-19775-8_4
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/TPAMI.2019.2913372, 10.1109/CVPR.2018.00745]
   Hu RH, 2019, IEEE T NEUR NET LEAR, V30, P1984, DOI 10.1109/TNNLS.2018.2875471
   Jang H, 2022, IEEE T NEUR NET LEAR, V33, P2034, DOI 10.1109/TNNLS.2022.3144296
   Kasabov N, 2010, NEURAL NETWORKS, V23, P16, DOI 10.1016/j.neunet.2009.08.010
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Kim Y, 2021, NEURAL NETWORKS, V144, P686, DOI 10.1016/j.neunet.2021.09.022
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Kugele A, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00439
   Kundu S, 2021, IEEE WINT CONF APPL, P3952, DOI 10.1109/WACV48630.2021.00400
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li HM, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00309
   Li Y., 2021, INT C MACHINE LEARNI, V139, P6316
   Liu GS, 2022, IEEE T CYBERNETICS, DOI 10.1109/TCYB.2022.3198259
   Liu QH, 2022, INT CONF ACOUST SPEE, P8922, DOI 10.1109/ICASSP43922.2022.9746865
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Magee JC, 2000, NAT REV NEUROSCI, V1, P181, DOI 10.1038/35044552
   Orchard G, 2015, FRONT NEUROSCI-SWITZ, V9, DOI [10.3389/fhins.2015.00437, 10.3389/fnins.2015.00437]
   Paszke A, 2019, ADV NEUR IN, V32
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   SPRUSTON N, 1994, TRENDS NEUROSCI, V17, P161, DOI 10.1016/0166-2236(94)90094-9
   Sun Y., 2023, ARXIV
   Sutskever I, 2014, ADV NEUR IN, V27
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang C., 2022, ARXIV
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang Y., 2022, P INT JOINT C ART IN, P2501
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Wu ZZ, 2022, IEEE T NEUR NET LEAR, V33, P6249, DOI 10.1109/TNNLS.2021.3073016
   Xie X., 2023, IEEE T NEUR NET LEAR, P1
   Xie XR, 2017, IEEE T NEUR NET LEAR, V28, P1411, DOI 10.1109/TNNLS.2016.2541339
   Yan ZL, 2021, AAAI CONF ARTIF INTE, V35, P10577
   Yao M, 2023, IEEE T PATTERN ANAL, V45, P9393, DOI 10.1109/TPAMI.2023.3241201
   Yao M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10201, DOI 10.1109/ICCV48922.2021.01006
   Yu C., 2022, ARXIV
   Yu Q, 2022, IEEE T NEUR NET LEAR, V33, P1714, DOI 10.1109/TNNLS.2020.3043415
   Zhan QG, 2022, IEEE T CYBERNETICS, V52, P13323, DOI 10.1109/TCYB.2021.3079097
   Zhang ML, 2022, IEEE T NEUR NET LEAR, V33, P1947, DOI 10.1109/TNNLS.2021.3110991
   Zhao DC, 2022, NEURAL NETWORKS, V154, P68, DOI 10.1016/j.neunet.2022.06.036
   Zheng HL, 2021, AAAI CONF ARTIF INTE, V35, P11062
   Zhou Z., 2023, P INT C LEARN REPR I
   Zhu R., 2022, ARXIV
   Zuo L, 2022, RELIAB ENG SYST SAFE, V225, DOI 10.1016/j.ress.2022.108561
   Zuo L, 2020, NEUROCOMPUTING, V408, P1, DOI 10.1016/j.neucom.2020.01.109
NR 62
TC 0
Z9 0
U1 2
U2 2
PD NOV 25
PY 2023
VL 280
AR 111024
DI 10.1016/j.knosys.2023.111024
EA SEP 2023
UT WOS:001088593000001
DA 2023-11-16
ER

PT C
AU Cardoso, MC
   Silva, M
   Vellasco, MMBR
   Cataldo, E
AF Cardoso, Marcelo C.
   Silva, Marco
   Vellasco, Marley M. B. R.
   Cataldo, Edson
BE Roy, A
   Angelov, P
   Alimi, A
   Venayagamoorthy, K
   Trafalis, T
TI Quantum-Inspired Features and Parameter Optimization of Spiking Neural
   Networks for a Case Study from Atmospheric
SO INNS CONFERENCE ON BIG DATA 2015 PROGRAM
SE Procedia Computer Science
DT Proceedings Paper
CT INNS Conference on Big Data
CY AUG 08-10, 2015
CL San Francisco, CA
DE Atmospheric Discharges; Spiking Neural Network; Clustering; Evolutionary
   Algorithms
AB Identified cluster of atmospheric discharges, sufficiently near from transmissions line, could be an important alarm to support real time decisions. Lightning are important events that affect the electrical power system operation, which are often responsible for transmission lines outages, and can trigger a sequence of events that lead to system collapse. The Brazilian lightning network detection monitors nearly 18 million events monthly and all this data must be processed and analyzed. This paper uses a hybrid model named the Quantum binary-real evolving Spiking Neural Network (QbrSNN) for clustering problem, where the features and parameters of a spiking neural network (SNN) are optimized using the Quantum-Inspired Evolutionary Algorithm with representation Binary-Real (QIEA-BR). The proposed model is applied to atmospheric discharges data, with a significantly higher clustering accuracy than traditional techniques.
C1 [Cardoso, Marcelo C.; Silva, Marco; Vellasco, Marley M. B. R.] Pontif Catholic Univ Rio de Janeiro PUC Rio, Rio De Janeiro, Brazil.
   [Cataldo, Edson] Univ Fed Fluminense, Rio De Janeiro, Brazil.
RP Cardoso, MC (corresponding author), Pontif Catholic Univ Rio de Janeiro PUC Rio, Rio De Janeiro, Brazil.
EM mcascardo1@gmail.com; mabs21@ele.puc-rio.br; marley@ele.puc-rio.br;
   ecataldo@im.uff.br
CR Bohte S. M., 2003, THESIS I PROGRAMMING
   da Cruz A. V. A., 2006, QUANTUM INSPIRED EVO, P2630
   de Pinho Anderson Guimaraes, 2009, Proceedings of the 2009 World Congress on Nature & Biologically Inspired Computing (NaBIC 2009), P445, DOI 10.1109/NABIC.2009.5393327
   Everitt B.S., 2001, CLUSTER ANAL, V4th ed.
   Hamed H. N. A., 2011, NUMERICAL ANAL SCI C, P133
   Hamed HNA, 2009, 2009 INTERNATIONAL CONFERENCE OF SOFT COMPUTING AND PATTERN RECOGNITION, P695, DOI 10.1109/SoCPaR.2009.139
   Han KH, 2002, IEEE T EVOLUT COMPUT, V6, P580, DOI 10.1109/TEVC.2002.804320
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   Jusevicius M. A. R., 2007, 9 INT S LIGHTN PROT
   Kaufman L, 2009, FINDING GROUPS DATA
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Natschlager T, 1998, NETWORK-COMP NEURAL, V9, P319, DOI 10.1088/0954-898X/9/3/003
   Silva M, 2014, IEEE IJCNN, P2391, DOI 10.1109/IJCNN.2014.6889566
   Tan PN., 2013, INTRO DATA MINING, P487
   de Lima GRT, 2013, COMPUT GEOSCI-UK, V57, P158, DOI 10.1016/j.cageo.2013.04.016
NR 15
TC 3
Z9 3
U1 0
U2 3
PY 2015
VL 53
BP 74
EP 81
DI 10.1016/j.procs.2015.07.281
UT WOS:000360311000009
DA 2023-11-16
ER

PT C
AU McDaid, L
   Harkin, J
   Hall, S
   Dowrick, T
   Chen, Y
   Marsland, J
AF McDaid, L.
   Harkin, J.
   Hall, S.
   Dowrick, T.
   Chen, Y.
   Marsland, J.
BE Dimitrov, DP
   Mladenov, V
   Jordanova, S
   Mastorakis, N
TI EMBRACE: Emulating Biologically-Inspired architectures on Hardware
SO PROCEEDINGS OF THE 9TH WSEAS INTERNATIONAL CONFERENCE ON NEURAL NETWORKS
   (NN' 08): ADVANCED TOPICS ON NEURAL NETWORKS
SE Artificial Intelligence Series-WSEAS
DT Proceedings Paper
CT 9th WSEAS International Conference on Neural Networks (NN 08)
CY MAY 02-04, 2008
CL Tech Univ Sofia, Sofia, BULGARIA
HO Tech Univ Sofia
DE synapse; Network-on-Chip; Spiking Neural Networks; reconfigurable;
   neuron; mixed-mode
ID INTERCONNECT; SYNAPSES; NEURONS; DESIGN
AB This paper highlights and discusses the current challenges in the implementation of large scale Spiking Neural Networks (SNNs) in hardware. A mixed-mode approach to realising scalable SNNs on a reconfigurable hardware platform is presented. The approach uses compact low power analogue spiking neuron cells, with a weight storage capability, interconnected using Network on Chip (NoC) routers. Results presented show that this route to hardware implementation is promising.
C1 [McDaid, L.; Harkin, J.] Univ Ulster, Intelligent Syst Res Ctr, Coleraine BT52 1SA, Londonderry, North Ireland.
   [Hall, S.; Dowrick, T.; Chen, Y.; Marsland, J.] Univ Liverpool, Dept Elect Engn Elect, Liverpool L69 3BX, Merseyside, England.
RP McDaid, L (corresponding author), Univ Ulster, Intelligent Syst Res Ctr, Coleraine BT52 1SA, Londonderry, North Ireland.
EM lj.mcdaid@ulster.ac.uk
CR BAINBRIDGE WJ, 2004, P DATE 04 PAR FEB, V3, P274
   Bartolozzi C, 2006, NEUROCOMPUTING, V69, P1971, DOI 10.1016/j.neucom.2005.06.022
   Chang MCF, 2001, P IEEE, V89, P456, DOI 10.1109/5.920578
   Chen GQ, 2005, IEEE INT SYMP CIRC S, P2514
   CHEN Y, 3 INT S NEUR NETW IS
   CHEN Y, WCCI IN PRESS
   Chicca E, 2003, IEEE T NEURAL NETWOR, V14, P1297, DOI 10.1109/TNN.2003.816367
   CHICCA E, 2004, ISCAS 04 MAY, V5, P357
   Davis JA, 2001, P IEEE, V89, P305, DOI 10.1109/5.915376
   DeHon A, 2004, IEEE T VLSI SYST, V12, P1038, DOI 10.1109/TVLSI.2004.827562
   Diorio C, 1996, IEEE T ELECTRON DEV, V43, P1972, DOI 10.1109/16.543035
   Farquhar E, 2005, IEEE T CIRCUITS-I, V52, P477, DOI 10.1109/TCSI.2004.842871
   FLOYD BA, 2002, IEEE J SOLID STATE C, V37
   GOOSSENS KGW, 2008, INT S NETW CHIP APR
   Huang DW, 2003, IEEE J SEL TOP QUANT, V9, P614, DOI 10.1109/JSTQE.2003.812506
   Indiveri G, 2006, IEEE T NEURAL NETWOR, V17, P211, DOI 10.1109/TNN.2005.860850
   Indiveri G, 2003, PROCEEDINGS OF THE 2003 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL IV, P820
   Joyner JW, 2004, IEEE T VLSI SYST, V12, P367, DOI 10.1109/TVLSI.2004.825835
   KANAZAWA Y, 2003, SICE ANN C FUK AUG 4
   LINARES-BARRANCO B, 1991, IEEE J SOLID-ST CIRC, V26, P956, DOI 10.1109/4.92015
   Maguire LP, 2007, NEUROCOMPUTING, V71, P13, DOI 10.1016/j.neucom.2006.11.029
   MEINDL JD, 2001, INT EL DEV M
   Murray A F, 1997, Int J Neural Syst, V8, P559, DOI 10.1142/S0129065797000525
   Naeemi A, 2005, IEEE ELECTR DEVICE L, V26, P544, DOI 10.1109/LED.2005.852744
   NOUSIAS I, 2006, NASA ESA ADAPTIVE HA, P420
   Patel GN, 1997, ELECTRON LETT, V33, P997, DOI 10.1049/el:19970686
   PEARSON MJ, 2007, IEEE T NEURAL NETS, V18
   ROSATO JJ, FUTURE FAB INT, V8
   SCHEMMEL J, 2004, IEEE INT P NEUR NETW, V3, P1711
   Simoni MF, 1999, IEEE T CIRCUITS-II, V46, P967, DOI 10.1109/82.775396
   SMITH KC, 1988, COMPUTER, V21, P17, DOI 10.1109/2.48
   Tsodyks MV, 1997, P NATL ACAD SCI USA, V94, P719, DOI 10.1073/pnas.94.2.719
   Tuffy F, 2007, NEUROCOMPUTING, V71, P30, DOI 10.1016/j.neucom.2006.11.027
   WANG P, 2004, IEEE T VERY LARGE SC, V12
   WEIXIONG Z, 2002, SOLID STATE ELECT, V22, P268
   Wolf W, 2005, DES AUT TEST EUROPE, P86, DOI 10.1109/DATE.2005.217
   YOUNG JL, 2004, IEEE INT S CIRC SYST, V4, P744
   Zhu JH, 2003, LECT NOTES COMPUT SC, V2778, P1062
NR 38
TC 1
Z9 1
U1 0
U2 1
PY 2008
BP 167
EP +
UT WOS:000257699300031
DA 2023-11-16
ER

PT J
AU Im, J
   Kim, J
   Yoo, HN
   Baek, JW
   Kwon, D
   Oh, S
   Kim, J
   Hwang, J
   Park, BG
   Lee, JH
AF Im, Jiseong
   Kim, Jaehyeon
   Yoo, Ho-Nam
   Baek, Jong-Won
   Kwon, Dongseok
   Oh, Seongbin
   Kim, Jangsaeng
   Hwang, Joon
   Park, Byung-Gook
   Lee, Jong-Ho
TI On-Chip Trainable Spiking Neural Networks Using Time-To-First-Spike
   Encoding
SO IEEE ACCESS
DT Article
DE Neurons; Firing; Hardware; Training; System-on-chip; Flash memories;
   Power demand; Spiking neural networks (SNNs); time-to-first-spike
   (TTFS); on-chip training; synaptic devices; NAND flash
ID MODEL
AB Artificial Neural Networks (ANNs) have shown remarkable performance in various fields. However, ANN relies on the von-Neumann architecture, which consumes a lot of power. Hardware-based spiking neural networks (SNNs) inspired by a human brain have become an alternative with significantly low power consumption. In this paper, we propose on-chip trainable SNNs using a time-to-first-spike (TTFS) method. We modify the learning rules of conventional SNNs using TTFS to be suitable for on-chip learning. Vertical NAND flash memory cells fabricated by a device manufacturer are used as synaptic devices. The entire learning process considering the hardware implementation is also demonstrated. The performance of the proposed network is evaluated through the MNIST classification in system-level simulation using Python. The proposed SNNs show an accuracy of 96% for a network size of 784 - 400 - 10. We also investigate the effect of non-ideal cell characteristics (such as pulse-to-pulse and device-to-device variations) on inference accuracy. Our networks demonstrate excellent immunity for various device variations compared with the networks using off-chip training.
C1 [Kwon, Dongseok] Seoul Natl Univ, Dept Elect & Comp Engn, SNU, Seoul 08826, South Korea.
   [Lee, Jong-Ho] Seoul Natl Univ, Dept Elect & Comp Engn, Seoul 08826, South Korea.
   Seoul Natl Univ, ISRC, Seoul 08826, South Korea.
RP Lee, JH (corresponding author), Seoul Natl Univ, Dept Elect & Comp Engn, Seoul 08826, South Korea.
EM jhl@snu.ac.kr
CR Abbott LF, 1999, BRAIN RES BULL, V50, P303, DOI 10.1016/S0361-9230(99)00161-6
   Akashdeep, 2017, EXPERT SYST APPL, V88, P249, DOI 10.1016/j.eswa.2017.07.005
   Ambrosi J., 2018, P IEEE INT C REB COM, P1, DOI [10.1109/icrc.2018.8638612, DOI 10.1109/ICRC.2018.8638612]
   [Anonymous], 2008, IEEE IEDM
   Arozi M, 2020, SYMMETRY-BASEL, V12, DOI 10.3390/sym12040541
   Burkitt AN, 2006, BIOL CYBERN, V95, P1, DOI 10.1007/s00422-006-0068-6
   Chen PY, 2015, ICCAD-IEEE ACM INT, P194, DOI 10.1109/ICCAD.2015.7372570
   Dai YT, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-017-02527-8
   Dodge S, 2019, ACM T APPL PERCEPT, V16, DOI 10.1145/3306241
   Drakaki M., 2005, INT C TECHN AUT ICTA, P322
   Duchi J, 2009, J MACH LEARN RES, V10, P2899
   Franco S., 2002, DESIGN OPERATIONAL A
   Hasan R, 2017, MICROELECTRON J, V66, P31, DOI 10.1016/j.mejo.2017.05.005
   Kheradpisheh SR, 2020, INT J NEURAL SYST, V30, DOI 10.1142/S0129065720500276
   Kim H, 2017, NANOTECHNOLOGY, V28, DOI 10.1088/1361-6528/aa86f8
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
   Kwon D, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00423
   Kwon D, 2019, IEEE T ELECTRON DEV, V66, P395, DOI 10.1109/TED.2018.2879821
   LANG JH, 1985, IEEE T COMPUT, V34, P475, DOI 10.1109/TC.1985.1676588
   Lin CH, 1998, XI BRAZILIAN SYMPOSIUM ON INTEGRATED CIRCUIT DESIGN, PROCEEDINGS, P195, DOI 10.1109/SBCCI.1998.715440
   Liu KF, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00835
   Mostafa Hesham, 2018, IEEE Trans Neural Netw Learn Syst, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Oh S., 2020, ARXIV200605033
   Petrenko S, 2018, BIG DATA TECHNOLOGIE, P115
   Querlioz D, 2012, IEEE INT SYMP NANO, P203
   Roy K, 2019, NATURE, V575, P607, DOI 10.1038/s41586-019-1677-2
   Taherkhani A, 2020, NEURAL NETWORKS, V122, P253, DOI 10.1016/j.neunet.2019.09.036
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Yu SM, 2018, P IEEE, V106, P260, DOI 10.1109/JPROC.2018.2790840
   Yu SM, 2012, 2012 IEEE INTERNATIONAL ELECTRON DEVICES MEETING (IEDM)
NR 30
TC 0
Z9 0
U1 4
U2 11
PY 2022
VL 10
BP 31263
EP 31272
DI 10.1109/ACCESS.2022.3160271
UT WOS:000773248300001
DA 2023-11-16
ER

PT C
AU Menescal, DD
   de Castro, LN
AF Menescal, Diego Duarte
   de Castro, Leandro Nunes
BE Omatu, S
   Mehmood, R
   Sitek, P
   Cicerone, S
   Rodriguez, S
TI On the Generation of Desired Outputs for Spike Neural Networks (SNN)
SO 19TH INTERNATIONAL SYMPOSIUM ON DISTRIBUTED COMPUTING AND ARTIFICIAL
   INTELLIGENCE
SE Lecture Notes in Networks and Systems
DT Proceedings Paper
CT 19th International Symposium on Distributed Computing and Artificial
   Intelligence
CY JUL 13-15, 2022
CL L'Aquila, ITALY
DE Spike neural network; Desired output; Spike generation
AB In supervised learning algorithms, it is necessary to define an error function for the parameter adjustment process to take place. This function generation requires the input feature vectors and their respective desired outputs. In the context of neural networks, the network outputs are compared with the desired outputs so as to compute the error function. For standard networks, such as Perceptron, Adaline and others, the desired outputs are basically a class label or output value that will be directly used to calculate the network error. In the case of bioinspired networks, such as those using Leaky Integrate-and-Fire (LIF) neurons, their output are electrical impulses (spikes). In such cases, the electrical impulse has a built-in temporal dependence that does not occur for Perceptron neurons, thus representing a challenge to calculate the desired output values (spikes) for Spike Neural Networks (SNN). The purpose of this paper is to define an analytical solution to build the desired spikes for each category in a classification problem for a SNN. The computational challenge encountered to represent the dynamics of spike generation in bioinspired neurons will also be discussed, which has a direct impact on the objective of the proposed solution.
C1 [Menescal, Diego Duarte; de Castro, Leandro Nunes] Univ Prebiteriana Mackenzie, Comp & Informat Fac, Nat Comp & Machine Learning Lab LCoN, BR-01302907 Sao Paulo, SP, Brazil.
RP Menescal, DD (corresponding author), Univ Prebiteriana Mackenzie, Comp & Informat Fac, Nat Comp & Machine Learning Lab LCoN, BR-01302907 Sao Paulo, SP, Brazil.
EM dmenescal@outlook.com
CR Dayan P, 2005, THEORETICAL NEUROSCI
   de Carvalho ACPLF, 2009, STUD COMPUT INTELL, V205, P177
   Kaur P., 2021, BIOINSPIRED NEUROCOM, V903
   Kinaneva D., 2021, 3 INT C HUMAN COMPUT, P1
   Kreuz T, 2013, J NEUROPHYSIOL, V109, P1457, DOI 10.1152/jn.00873.2012
   Maass W., 1999, PULSED NEURAL NETWOR
   Miller P., 2018, INTRO COURSE COMPUTA
   Python Programming Language Homepage, US
   Velichko A, 2020, IEEE T CIRCUITS-II, V67, P3477, DOI 10.1109/TCSII.2020.2997117
   Wang XW, 2020, NEURAL NETWORKS, V125, P258, DOI 10.1016/j.neunet.2020.02.011
NR 10
TC 0
Z9 0
U1 1
U2 1
PY 2023
VL 583
BP 100
EP 110
DI 10.1007/978-3-031-20859-1_11
UT WOS:000929025600011
DA 2023-11-16
ER

PT J
AU Sun, Z
   Cutsuridis, V
   Caiafa, CF
   Solé-Casals, J
AF Sun, Zhe
   Cutsuridis, Vassilis
   Caiafa, Cesar F.
   Sole-Casals, Jordi
TI Brain Simulation and Spiking Neural Networks
SO COGNITIVE COMPUTATION
DT Editorial Material
C1 [Sun, Zhe] Juntendo Univ, Fac Hlth Data Sci, Urayasu, Chiba 2790013, Japan.
   [Sun, Zhe] Ctr Adv Photon Image Proc Res Team, RIKEN, Wako, Saitama 3510198, Japan.
   [Cutsuridis, Vassilis] Univ Lincoln, Sch Comp Sci, Lincoln LN6 7TS, England.
   [Caiafa, Cesar F.] CONICET CCT La Plata CIC PBA UNLP, Inst Argentino Radioastron, Casilla Correo N5, RA-1894 Villa Elisa, BA, Argentina.
   [Sole-Casals, Jordi] Univ Vic, Cent Univ Catalonia, Data & Signal Proc Res Grp, Vic 08500, Catalonia, Spain.
   [Sole-Casals, Jordi] Univ Cambridge, Dept Psychiat, Cambridge CB2 0SZ, England.
RP Sun, Z (corresponding author), Juntendo Univ, Fac Hlth Data Sci, Urayasu, Chiba 2790013, Japan.; Sun, Z (corresponding author), Ctr Adv Photon Image Proc Res Team, RIKEN, Wako, Saitama 3510198, Japan.
EM z.sun.kc@juntendo.ac.jp; vcutsuridis@lincoln.ac.uk; ccaiafa@fi.uba.ar;
   jordi.sole@uvic.cat
CR Cakan C, 2023, COGN COMPUT, V15, P1132, DOI 10.1007/s12559-021-09931-9
   Chen H, 2022, CONF TECHNOL APPL, P1, DOI [10.1109/TAAI57707.2022.00010, 10.1145/3491102.3501831]
   Crook-Rumsey M, 2023, COGN COMPUT, V15, P1273, DOI 10.1007/s12559-022-10075-7
   Kobayashi T, 2022, CHIN J COMMUN, V15, P378, DOI 10.1080/17544750.2021.1987283
   Kopsick JD, 2023, COGN COMPUT, V15, P1190, DOI 10.1007/s12559-021-09954-2
   Luboeinski J, 2023, COGN COMPUT, V15, P1211, DOI 10.1007/s12559-022-10021-7
   Salustri M, 2023, COGN COMPUT, V15, P1231, DOI 10.1007/s12559-022-10034-2
   Shaw R, 2023, COGN COMPUT, V15, P1243, DOI 10.1007/s12559-022-10023-5
   Xue X, 2022, COGN COMPUT, P1
   Yang YZ, 2023, POLYM BULL, V80, P1817, DOI [10.1007/s00289-022-04158-6, 10.1109/ICSICT55466.2022.9963420]
   Zhang J, 2023, COGN COMPUT, V15, P1106, DOI 10.1007/s12559-021-09981-z
NR 11
TC 0
Z9 0
U1 2
U2 2
PD JUL
PY 2023
VL 15
IS 4
BP 1103
EP 1105
DI 10.1007/s12559-023-10156-1
EA JUN 2023
UT WOS:001004530700001
DA 2023-11-16
ER

PT C
AU Yudanov, D
   Reznik, L
AF Yudanov, Dmitri
   Reznik, Leon
GP IEEE
TI Scalable Multi-Precision Simulation of Spiking Neural Networks on GPU
   with OpenCL
SO 2012 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUN 10-15, 2012
CL Brisbane, AUSTRALIA
DE spiking neural network simulation; high precision; GPU implementation;
   OpenCL
ID NEURONS
AB Biologically-realistic multi-precision spiking neural network (SNN) simulation is designed and implemented on a new GPU device Radeon (TM) HD 7970 using OpenCL framework. The implementation aims to investigate the role of time precision in simulated SNNs. Simulation methods and GPU platforms are reviewed. Simulation model and process are presented and analyzed. The GPU model is capable of simulating a SNN with up to two million neurons. GPU and CPU results are directly verified and found to match exactly.
C1 [Yudanov, Dmitri] AMD, Austin, TX 78741 USA.
   [Reznik, Leon] Rochester Inst Technol, Dept Comp Sci, Rochester, NY 14623 USA.
RP Yudanov, D (corresponding author), AMD, Austin, TX 78741 USA.
EM dxy7370@amd.com; lr@cs.rit.edu
CR [Anonymous], 2010, SOLVING ORDINARY DIF
   [Anonymous], 2011, HETEROGENEOUS COMPUT
   [Anonymous], CEREBRAL PLASTICITY
   [Anonymous], BMC NEUROSCI
   [Anonymous], 2010, ALGORITHMS THEORY CO
   [Anonymous], HETEROGENEOUS COMPUT
   Ariav G, 2003, J NEUROSCI, V23, P7750
   Arrabales Raul, 2009, 2009 IEEE Symposium on Computational Intelligence and Games (CIG), P217, DOI 10.1109/CIG.2009.5286473
   Azouz R, 1999, J NEUROSCI, V19, P2209
   Brette R, 2007, J COMPUT NEUROSCI, V23, P349, DOI 10.1007/s10827-007-0038-6
   BULIRSCH R, 1966, NUMER MATH, V8, P1, DOI 10.1007/BF02165234
   Daga M., 2011, Proceedings of the 2011 Symposium on Application Accelerators in High-Performance Computing (SAAHPC 2011), P141, DOI 10.1109/SAAHPC.2011.29
   Fountas Z, 2011, IEEE CONF COMPU INTE, P350, DOI 10.1109/CIG.2011.6032027
   George AA, 2011, J NEUROSCI, V31, P14721, DOI 10.1523/JNEUROSCI.1424-11.2011
   Grinblat G L, 2011, SIMULATION, V88, P299
   Hanuschkin A, 2010, FRONTIERS NEUROINFOR, V4
   Hoffmann J, 2010, LECT NOTES COMPUT SC, V6352, P184, DOI 10.1007/978-3-642-15819-3_23
   Houston M, 2011, FUS DEV SUMM AMD GRA
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Jeffrey A, 2008, HDB MATH FORMULAS IN
   Kayser C, 2010, P NATL ACAD SCI USA, V107, P16976, DOI 10.1073/pnas.1012656107
   Kofman E, 2001, SIMUL-T SOC MOD SIM, V18, P123
   KONISHI M, 1993, SCI AM, V268, P66, DOI 10.1038/scientificamerican0493-66
   Lee S H, 40202010 STS U VIRG
   Lesica NA, 2008, J NEUROSCI, V28, P5412, DOI 10.1523/JNEUROSCI.0073-08.2008
   MAINEN ZF, 1995, SCIENCE, V268, P1503, DOI 10.1126/science.7770778
   Martinez G. L, 2011, THESIS
   Merrill DG, 2010, PACT 2010: PROCEEDINGS OF THE NINETEENTH INTERNATIONAL CONFERENCE ON PARALLEL ARCHITECTURES AND COMPILATION TECHNIQUES, P545, DOI 10.1145/1854273.1854344
   Pallipuran V K, 2011, J SUPERCOMPUT, P1
   Parker G. E., 1996, Neural, Parallel & Scientific Computations, V4, P97
   Poggio T, 2010, CNS GPU BASED FRAMEW
   Richert M., 2011, FRONTIERS NEUROINFOR, V5
   Sanders Jason, 2010, CUDA EXAMPLE INTRO G
   Schoppa NE, 2006, NEURON, V49, P271, DOI 10.1016/j.neuron.2005.11.038
   Sharpee TO, 2011, CURR OPIN NEUROBIOL, V21, P761, DOI 10.1016/j.conb.2011.05.027
   Stewart R D, 2011, J COMPUTATIONAL NEUR, V30, P1
   Stewart RD, 2009, J COMPUT NEUROSCI, V27, P115, DOI 10.1007/s10827-008-0131-5
   Thibeault C. M., 2011, Proceedings of the ISCA 3rd International Conference on Bioinformatics and Computational Biology, P146
   Yudanov D., 2010, P INT JOINT C NEUR N, P1, DOI [10.1109/IJCNN.2010.5596334, DOI 10.1109/IJCNN.2010.5596334]
   Zheng G, 2009, J COMPUT NEUROSCI, V26, P409, DOI 10.1007/s10827-008-0119-1
NR 40
TC 0
Z9 0
U1 0
U2 2
PY 2012
UT WOS:000309341300078
DA 2023-11-16
ER

PT C
AU Xie, XR
   Qu, H
   Liu, GS
   Liu, LS
AF Xie, Xiurui
   Qu, Hong
   Liu, Guisong
   Liu, Lingshuang
BE Loo, CK
   Yap, KS
   Wong, KW
   Teoh, A
   Huang, K
TI Recognizing Human Actions by Using the Evolving Remote Supervised Method
   of Spiking Neural Networks
SO NEURAL INFORMATION PROCESSING (ICONIP 2014), PT I
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 21st International Conference on Neural Information Processing (ICONIP)
CY NOV 03-06, 2014
CL Kuching, MALAYSIA
DE Human actions recognition; Spiking neural networks; Particle swarm
   optimization; Remote supervised learning method
AB This paper proposes a novel approach based on spiking neural networks to recognize human actions in videos. In our method, a star skeleton detector is designed to extract spatial features of input videos, and a classifier using evolving ReSuMe algorithm is proposed, with scale and shift invariance, to recognize input patterns. In learning algorithm, the remote supervised learning method(ReSuMe) is improved by the particle swarm optimization(PSO) algorithm. Experimental results on KTH and Weizmann dataset prove that our method achieves a significant improvement in performance compared with traditional ReSuMe and other method based on neural networks.
C1 [Xie, Xiurui; Qu, Hong; Liu, Guisong; Liu, Lingshuang] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Sichuan, Peoples R China.
RP Xie, XR (corresponding author), Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Sichuan, Peoples R China.
EM hongqu@uestc.edu.cn
CR Chen H.-S., 2006, INT WORKSHOP VISUAL, P171, DOI DOI 10.1145/1178782.1178808
   Escobar MJ, 2009, INT J COMPUT VISION, V82, P284, DOI 10.1007/s11263-008-0201-1
   Gerstner W, 2002, SPIKING NERUAL MODEL
   Han J, 2012, MOR KAUF D, P1
   Meng Y, 2011, IEEE T NEURAL NETWOR, V22, P1952, DOI 10.1109/TNN.2011.2171044
   Petridis V, 2009, MED C CONTR AUTOMAT, P406, DOI 10.1109/MED.2009.5164575
   Poli Riccardo, 2008, Journal of Artificial Evolution & Applications, DOI [10.1155/2008/685175, 10.1155/2008/761459]
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Schüldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462
   Viola P, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P734
NR 10
TC 1
Z9 1
U1 0
U2 4
PY 2014
VL 8834
BP 366
EP 373
UT WOS:000432659500046
DA 2023-11-16
ER

PT J
AU Arriandiaga, A
   Portillo, E
   Espinosa-Ramos, JI
   Kasabov, NK
AF Arriandiaga, Ander
   Portillo, Eva
   Espinosa-Ramos, Josafath Israel
   Kasabov, Nikola K.
TI Pulsewidth Modulation-Based Algorithm for Spike Phase Encoding and
   Decoding of Time-Dependent Analog Data
SO IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
DT Article
DE Encoding; Neurons; Pulse width modulation; Decoding; Signal processing
   algorithms; Heuristic algorithms; Data models; Analog data; data
   compression; spike encoding; spike series decoding; spiking neural
   networks (SNNs); streaming data
ID NEURAL-NETWORK METHODOLOGY; LOSSLESS COMPRESSION; EEG DATA;
   CLASSIFICATION; COMMUNICATION; INFORMATION; PATTERNS; NEURONS; CODES
AB This article proposes a new spike encoding and decoding algorithm for analog data. The algorithm uses the pulsewidth modulation principles to achieve a high reconstruction accuracy of the signal, along with a high level of data compression. Two benchmark data sets are used to illustrate the method: stock index time series and human voice data. Applications of the method for spiking neural network (SNN) modeling and neuromorphic implementations are discussed. The proposed method would allow the development of new applications of SNNs as regression techniques for predictive time-series modeling.
C1 [Arriandiaga, Ander; Portillo, Eva] Univ Basque Country, Dept Automat Control & Syst Engn, Fac Engn, Bilbao 48080, Spain.
   [Espinosa-Ramos, Josafath Israel; Kasabov, Nikola K.] Auckland Univ Technol, Res Inst, Knowledge Engn & Discovery, Auckland 1010, New Zealand.
RP Arriandiaga, A (corresponding author), Univ Basque Country, Dept Automat Control & Syst Engn, Fac Engn, Bilbao 48080, Spain.
EM ander.arriandiaga@ehu.eus
CR Al-Bahadili H, 2008, COMPUT MATH APPL, V56, P143, DOI 10.1016/j.camwa.2007.11.043
   Andrzejak RG, 2001, PHYS REV E, V64, DOI 10.1103/PhysRevE.64.061907
   [Anonymous], [No title captured]
   [Anonymous], 1994, 754 ANSIIEEE
   [Anonymous], 2017, ARXIV170301909
   [Anonymous], 2017, P 2017 IEEE BIOM CIR
   Bohte Sander M., 2004, Natural Computing, V3, P195, DOI 10.1023/B:NACO.0000027755.02868.60
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Bose P, 2016, IEEE T GEOSCI REMOTE, V54, P6563, DOI 10.1109/TGRS.2016.2586602
   Burtscher M, 2007, IEEE DATA COMPR CONF, P293
   Capecci E, 2015, NEURAL NETWORKS, V68, P62, DOI 10.1016/j.neunet.2015.03.009
   Cattani A., 2015, ARXIV150403954
   Diehl PU, 2015, IEEE IJCNN
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Doborjeh M. G., 2017, 2020 INT
   Doborjeh MG, 2016, IEEE T BIO-MED ENG, V63, P1830, DOI 10.1109/TBME.2015.2503400
   Eliasmith C., 2016, ARXIV161105141
   Engelson V., 2000, Proceedings DCC 2000. Data Compression Conference, DOI 10.1109/DCC.2000.838221
   Esser SK, 2016, P NATL ACAD SCI USA, V113, P11441, DOI 10.1073/pnas.1604850113
   Finelli LA, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.1000062
   Fowler J.E., 1994, P 1994 S VOLUME VISU, P43
   Gao T, 2016, EVOL SYST-GER, V7, P277, DOI 10.1007/s12530-016-9144-x
   Gerstner W., 2002, SPIKING NEURON MODEL, V1st, P11
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   Gruning A., 2014, 22 EUR S ART NEUR NE
   Gütig R, 2009, PLOS BIOL, V7, DOI 10.1371/journal.pbio.1000141
   Guyonneau R, 2004, J PHYSIOL-PARIS, V98, P487, DOI 10.1016/j.jphysparis.2005.09.004
   Havenith MN, 2011, J NEUROSCI, V31, P8570, DOI 10.1523/JNEUROSCI.2817-10.2011
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Hough M., 2009, P INT C ROB ART LIF, P1
   Huang J, 2011, IEEE T CIRCUITS-I, V58, P1178, DOI 10.1109/TCSI.2010.2094350
   Isenburg M, 2005, COMPUT AIDED DESIGN, V37, P869, DOI 10.1016/j.cad.2004.09.015
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Jamali M, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms13229
   Kasabov N, 2017, IEEE T COGN DEV SYST, V9, P293, DOI 10.1109/TCDS.2016.2636291
   Kasabov N, 2016, NEURAL NETWORKS, V78, P1, DOI 10.1016/j.neunet.2015.09.011
   Kasabov N, 2015, INFORM SCIENCES, V294, P565, DOI 10.1016/j.ins.2014.06.028
   Kasabov N, 2014, NEUROCOMPUTING, V134, P269, DOI 10.1016/j.neucom.2013.09.049
   Kasabov NK, 2017, IEEE T NEUR NET LEAR, V28, P887, DOI 10.1109/TNNLS.2016.2612890
   Kominek J., 2003, CMU ARTIC DATABASES
   Lazzaro J., 1995, Proceedings. Sixteenth Conference on Advanced Research in VLSI, P158, DOI 10.1109/ARVLSI.1995.515618
   Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508
   Lindstrom P, 2006, IEEE T VIS COMPUT GR, V12, P1245, DOI 10.1109/TVCG.2006.143
   Lindstrom P, 2014, IEEE T VIS COMPUT GR, V20, P2674, DOI 10.1109/TVCG.2014.2346458
   Lopes-dos-Santos V, 2015, J NEUROPHYSIOL, V113, P1015, DOI 10.1152/jn.00380.2014
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Mehrotra K., 1997, ELEMENTS ARTIFICIAL
   Meng XY, 2016, AUTOMATICA, V70, P173, DOI 10.1016/j.automatica.2016.03.012
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Montemurro MA, 2007, J NEUROPHYSIOL, V98, P1871, DOI 10.1152/jn.00593.2007
   Nadasdy Z, 2009, FRONT SYST NEUROSCI, V3, DOI 10.3389/neuro.06.006.2009
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   Panzeri S, 2010, TRENDS NEUROSCI, V33, P111, DOI 10.1016/j.tins.2009.12.001
   Perrinet L., 2002, 10th European Symposium on Artificial Neural Networks. ESANN'2002. Proceedings, P313
   Ratanaworabhan P, 2006, IEEE DATA COMPR CONF, P133
   Reid D., 2015, 11 INT C INT COMP TH
   Reid D, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0103656
   Rumbell T, 2014, IEEE T NEUR NET LEAR, V25, P894, DOI 10.1109/TNNLS.2013.2283140
   Schrauwen B, 2003, IEEE IJCNN, P2825
   Shannon CE, 1998, P IEEE, V86, P447, DOI 10.1109/JPROC.1998.659497
   Shin JH, 2010, IEEE T NEURAL NETWOR, V21, P1697, DOI 10.1109/TNN.2010.2050600
   Shrestha SB, 2017, NEURAL NETWORKS, V86, P54, DOI 10.1016/j.neunet.2016.10.011
   Storn R, 1997, J GLOBAL OPTIM, V11, P341, DOI 10.1023/A:1008202821328
   Sun J, 2012, ADV IND CONTROL, P25, DOI 10.1007/978-1-4471-2885-4_2
   Tu EM, 2017, IEEE T NEUR NET LEAR, V28, P1305, DOI 10.1109/TNNLS.2016.2536742
   Walter F, 2016, NEURAL PROCESS LETT, V44, P103, DOI 10.1007/s11063-015-9478-6
   Wang ZZ, 2016, NEUROCOMPUTING, V173, P1203, DOI 10.1016/j.neucom.2015.08.078
   Yu Q, 2016, IEEE T NEUR NET LEAR, V27, P621, DOI 10.1109/TNNLS.2015.2416771
NR 69
TC 2
Z9 2
U1 3
U2 20
PD OCT
PY 2020
VL 31
IS 10
BP 3920
EP 3931
DI 10.1109/TNNLS.2019.2947380
UT WOS:000576436600012
DA 2023-11-16
ER

PT C
AU Milo, V
   Pedretti, G
   Carboni, R
   Calderoni, A
   Ramaswamy, N
   Ambrogio, S
   Ielmini, D
AF Milo, V.
   Pedretti, G.
   Carboni, R.
   Calderoni, A.
   Ramaswamy, N.
   Ambrogio, S.
   Ielmini, D.
GP IEEE
TI Demonstration of hybrid CMOS/RRAM neural networks with spike
   time/rate-dependent plasticity
SO 2016 IEEE INTERNATIONAL ELECTRON DEVICES MEETING (IEDM)
SE IEEE International Electron Devices Meeting
DT Proceedings Paper
CT 62nd Annual IEEE International Electron Devices Meeting (IEDM)
CY DEC 03-07, 2016
CL San Francisco, CA
ID SYNAPSES
AB Neural networks with resistive-switching memory (RRAM) synapses can mimic learning and recognition in the human brain, thus overcoming the major limitations of von Neumann computing architectures. While most researchers aim at supervised learning of a pre-determined set of patterns, unsupervised learning of patterns might be attractive for brain-inspired robot/drone navigation. Here we demonstrate neural networks with CMOS/RRAM synapses capable of unsupervised learning by spike-time dependent plasticity (STDP) and spike-rate dependent plasticity (SRDP). First, STDP learning in a RRAM synaptic network is demonstrated. Then we present a 4-transistor/1-resistor synapse capable of SRDP, finally demonstrating SRDP learning, update, and recognition of patterns at the level of neural network.
C1 [Milo, V.; Pedretti, G.; Carboni, R.; Ambrogio, S.; Ielmini, D.] Politecn Milan, DEIB, Piazza L da Vinci 32, I-20133 Milan, Italy.
   [Milo, V.; Pedretti, G.; Carboni, R.; Ambrogio, S.; Ielmini, D.] IU NET, Piazza L da Vinci 32, I-20133 Milan, Italy.
   [Calderoni, A.; Ramaswamy, N.] Micron Technol, Boise, ID USA.
RP Milo, V (corresponding author), Politecn Milan, DEIB, Piazza L da Vinci 32, I-20133 Milan, Italy.; Milo, V (corresponding author), IU NET, Piazza L da Vinci 32, I-20133 Milan, Italy.
EM valerio.milo@polimi.it
CR Ambrogio S., 2016, S VLSI, P196
   Ambrogio S, 2016, IEEE T ELECTRON DEV, V63, P1508, DOI 10.1109/TED.2016.2526647
   Balatti S, 2015, IEEE T ELECTRON DEV, V62, P3365, DOI 10.1109/TED.2015.2463104
   Burr GW, 2015, IEEE T ELECTRON DEV, V62, P3498, DOI 10.1109/TED.2015.2439635
   Garbin D, 2015, IEEE T ELECTRON DEV, V62, P2494, DOI 10.1109/TED.2015.2440102
   Kuzum D, 2012, NANO LETT, V12, P2179, DOI 10.1021/nl201040y
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Prezioso M, 2016, SCI REP-UK, V6, DOI 10.1038/srep21331
   Rachmuth G, 2011, P NATL ACAD SCI USA, V108, pE1266, DOI 10.1073/pnas.1106161108
   Sangkil Kim, 2015, 2015 IEEE MTT-S International Microwave Symposium (IMS2015), P1, DOI 10.1109/MWSYM.2015.7166723
   Wang ZQ, 2015, FRONT NEUROSCI-SWITZ, V8, DOI 10.3389/fnins.2014.00438
NR 11
TC 46
Z9 46
U1 0
U2 12
PY 2016
AR 16.8
UT WOS:000399108800110
DA 2023-11-16
ER

PT J
AU Oniz, Y
   Kaynak, O
AF Oniz, Yesim
   Kaynak, Okyay
TI Control of a direct drive robot using fuzzy spiking neural networks with
   variable structure systems-based learning algorithm
SO NEUROCOMPUTING
DT Article
DE Direct drive robot; Spiking neural networks; Variable structure systems
   based learning algorithm; Robot trajectory control
ID TRACKING CONTROL; NONLINEAR-SYSTEMS; CLASSIFICATION; PREDICTION
AB In this work, a sliding mode theory based supervised training algorithm that implements fuzzy reasoning on a spiking neural network has been developed and tested on the trajectory control problem of a two-degrees-of-freedom direct drive robotic manipulator. To describe the generation of a new spike train from the incoming spike trains Spike Response Model has been utilized and the Lyapunov stability method has been adopted in the derivation of the update rules for the neurocontroller parameters. The results of the real-time experiments indicate that stable online tuning and fast learning speed are the prominent characteristics of the proposed algorithm.(C) 2014 Elsevier B.V. All rights reserved.
C1 [Oniz, Yesim; Kaynak, Okyay] Bogazici Univ, Dept Elect & Elect Engn, Istanbul, Turkey.
   [Kaynak, Okyay] Harbin Inst Technol, Harbin, Peoples R China.
RP Oniz, Y (corresponding author), Bogazici Univ, Dept Elect & Elect Engn, Istanbul, Turkey.
EM yesim.oniz@boun.edu.tr; okyay.kaynak@boun.edu.tr
CR Abiyev R., 2012, P 2012 IEEE ASME INT, P1030
   Ahmed S, 2012, EVOL SYST, V3, P179, DOI 10.1007/s12530-012-9053-6
   [Anonymous], 1991, APPL NONLINEAR CONTR
   Ardalani-Farsa M, 2010, NEUROCOMPUTING, V73, P2540, DOI 10.1016/j.neucom.2010.06.004
   Batllori R, 2011, PROCEDIA COMPUT SCI, V6, DOI 10.1016/j.procs.2011.08.060
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Chen CW, 2013, J VIB CONTROL, V19, P1333, DOI 10.1177/1077546312442232
   Gandhi T, 2011, NEUROCOMPUTING, V74, P3051, DOI 10.1016/j.neucom.2011.04.029
   Gerstner W., 2002, SPIKING NEURON MODEL
   Ghosh-Dastidar S, 2007, INTEGR COMPUT-AID E, V14, P187
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   *INT MOT INC, 1992, DIR DRIV MAN RES DEV
   Johansson RS, 2004, NAT NEUROSCI, V7, P170, DOI 10.1038/nn1177
   Juang CF, 2002, IEEE T FUZZY SYST, V10, P155, DOI 10.1109/91.995118
   Kasabov N, 2014, NEUROCOMPUTING, V134, P269, DOI 10.1016/j.neucom.2013.09.049
   Kayacan E, 2012, IEEE T IND ELECTRON, V59, P3510, DOI 10.1109/TIE.2011.2182017
   Li ZK, 2013, IEEE T AUTOMAT CONTR, V58, P518, DOI 10.1109/TAC.2012.2208295
   LIN CT, 1995, FUZZY SET SYST, V70, P183, DOI 10.1016/0165-0114(94)00216-T
   Lin D, 2010, FUZZY SET SYST, V161, P2066, DOI 10.1016/j.fss.2010.03.006
   Maass W, 1997, NEURAL COMPUT, V9, P279, DOI 10.1162/neco.1997.9.2.279
   McCulloch WS., 1943, B MATH BIOPHYS, V5, P115, DOI DOI 10.1007/BF02478259
   McFall KS, 2013, J FRANKLIN I, V350, P300, DOI 10.1016/j.jfranklin.2012.11.003
   Mohareri O, 2012, NEUROCOMPUTING, V88, P54, DOI 10.1016/j.neucom.2011.06.035
   Oniz Y, 2009, IEEE T SYST MAN CY B, V39, P551, DOI 10.1109/TSMCB.2008.2007966
   Pan YP, 2013, NEUROCOMPUTING, V99, P15, DOI 10.1016/j.neucom.2012.05.011
   Parma GG, 1998, ELECTRON LETT, V34, P97, DOI 10.1049/el:19980062
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Sadati N, 2008, NEUROCOMPUTING, V71, P2702, DOI 10.1016/j.neucom.2007.06.019
   Shin JH, 2010, IEEE T NEURAL NETWOR, V21, P1697, DOI 10.1109/TNN.2010.2050600
   SLOTINE JJ, 1983, INT J CONTROL, V38, P465, DOI 10.1080/00207178308933088
   Topalov AV, 2011, NEUROCOMPUTING, V74, P1883, DOI 10.1016/j.neucom.2010.07.035
   Topalov AV, 2004, J PROCESS CONTR, V14, P581, DOI 10.1016/j.jprocont.2003.10.005
   Topalov AV, 2001, IEEE T SYST MAN CY B, V31, P445, DOI 10.1109/3477.931542
   Utkin V. I., 1999, SLIDING MODE CONTROL, V9
   VanRullen R, 2005, TRENDS NEUROSCI, V28, P1, DOI 10.1016/j.tins.2004.10.010
   Vreeken J., 2002, UUCS2003008
   Wang XQ, 2008, NEUROCOMPUTING, V71, P655, DOI 10.1016/j.neucom.2007.08.025
   Wang XQ, 2014, NEUROCOMPUTING, V134, P230, DOI 10.1016/j.neucom.2013.07.055
   Wehr M, 1996, NATURE, V384, P162, DOI 10.1038/384162a0
   Werbos P., 1974, REGRESSION NEW TOOLS
   Wu QX, 2013, NEUROCOMPUTING, V116, P3, DOI 10.1016/j.neucom.2012.01.046
   Yin S., 2014, DATA BASED TECHNIQUE
   Yin S, 2014, IEEE T IND ELECTRON, V61, P6418, DOI 10.1109/TIE.2014.2301773
   Yu SH, 2004, FUZZY SET SYST, V148, P469, DOI 10.1016/j.fss.2003.12.004
   Zak S.H., 2003, SYSTEMS CONTROL, V388
   Zhang H, 2013, INT J CONTROL, V86, P1824, DOI 10.1080/00207179.2013.797107
   Zhang H, 2013, IEEE T IND INFORM, V9, P337, DOI 10.1109/TII.2012.2225434
NR 47
TC 20
Z9 23
U1 4
U2 55
PD FEB 3
PY 2015
VL 149
BP 690
EP 699
DI 10.1016/j.neucom.2014.07.061
PN B
UT WOS:000346550300023
DA 2023-11-16
ER

PT C
AU Wang, XQ
   Hou, ZG
   Lv, F
   Tan, M
   Wang, YJ
AF Wang, Xiuqing
   Hou, Zeng-Guang
   Lv, Feng
   Tan, Min
   Wang, Yongji
BE Huang, T
   Zeng, Z
   Li, C
   Leung, CS
TI A Target-Reaching Controller for Mobile Robots Using Spiking Neural
   Networks
SO NEURAL INFORMATION PROCESSING, ICONIP 2012, PT IV
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 19th International Conference on Neural Information Processing (ICONIP)
CY NOV 11-15, 2012
CL Doha, QATAR
DE Mobile robot; spiking neural networks; navigation controller; target
   reaching; obstacle-avoidance; wall-following
AB Autonomous navigation plays important role in mobile robots. In this paper, a navigation controller based on spiking neural networks (SNNs) for mobile robots is presented. The proposed target-reaching navigation controller, in which the reactive architecture is used, is composed of three sub-controllers: the obstacle-avoidance controller and the wall-following controller using spiking neural networks (SNNs), and the goal-reaching controller. The experimental results show that the navigation controller can control the mobile robot to reach the target successfully while avoiding the obstacle and following the wall to get rid of the deadlock caused by local minimum. The proposed navigation controller does not require accurate mathematical models of the environment, and is suitable to unknown and unstructured environment.
C1 [Wang, Xiuqing; Lv, Feng] Hebei Normal Univ, Vocat & Tech Inst, Shijiazhuang 050031, Hebei, Peoples R China.
   [Hou, Zeng-Guang; Tan, Min] Chinese Acad Sci, Inst Automat, State Key Lab Management & Control Complex Syst, Beijing 100090, Peoples R China.
   [Wang, Yongji] Chinese Acad Sci, Inst Software, State Key Lab Comp Sci, Beijing 100190, Peoples R China.
RP Wang, XQ (corresponding author), Hebei Normal Univ, Vocat & Tech Inst, Shijiazhuang 050031, Hebei, Peoples R China.
EM xiuqingwang2004@yahoo.com.cn; zengguang.hou@ia.ac.cn;
   lvfeng@mail.tsinghua.edu.cn; min.tan@ia.ac.cn; ywang@itechs.iscas.ac.cn
CR Alamdari ARSA, 2005, PROCEEDINGS OF WORLD ACADEMY OF SCIENCE, ENGINEERING AND TECHNOLOGY, VOL 6, P49
   [Anonymous], 1993, NEURAL NETWORK PERCE
   Arkin R. C., 1990, Robotics and Autonomous Systems, V6, P105, DOI 10.1016/S0921-8890(05)80031-4
   Floreano D, 2005, ARTIF LIFE, V11, P121, DOI 10.1162/1064546053278900
   Floreano D., 2001, LNCS, P38
   Hagras H, 2004, IEEE INT CONF ROBOT, P4620, DOI 10.1109/ROBOT.2004.1302446
   Kasabov Nikola, 2012, Advances in Computational Intelligence. IEEE World Congress on Computational Intelligence (WCCI 2012). Plenary/Invited Lectures, P234, DOI 10.1007/978-3-642-30687-7_12
   Kasabov N, 2010, NEURAL NETWORKS, V23, P16, DOI 10.1016/j.neunet.2009.08.010
   Kasabov Nikola, 2009, Natural Computing, V8, P199, DOI 10.1007/s11047-008-9066-z
   Maass W., 1999, PULSED NEURAL NETWOR
   Mohareri O, 2012, NEUROCOMPUTING, V88, P54, DOI 10.1016/j.neucom.2011.06.035
   Na YK, 2003, AUTON ROBOT, V15, P193, DOI 10.1023/A:1025597227189
   Rossomando FG, 2011, CONTROL ENG PRACT, V19, P215, DOI 10.1016/j.conengprac.2010.11.011
   Schliebs S, 2009, NEURAL NETWORKS, V22, P623, DOI 10.1016/j.neunet.2009.06.038
   Soula H., 2005, P AM ASS ART INT AAA, P1
   Vreeken Jilles, 2002, UUCS2003008
   Wang X-L, 2007, THESIS
   Wang XQ, 2008, NEUROCOMPUTING, V71, P655, DOI 10.1016/j.neucom.2007.08.025
   Wang XQ, 2009, 2009 INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND COMPUTATIONAL INTELLIGENCE, VOL I, PROCEEDINGS, P194, DOI 10.1109/AICI.2009.448
   Wang XQ, 2008, ICNC 2008: FOURTH INTERNATIONAL CONFERENCE ON NATURAL COMPUTATION, VOL 4, PROCEEDINGS, P125, DOI 10.1109/ICNC.2008.718
   Wysoski SG, 2008, NEUROCOMPUTING, V71, P2563, DOI 10.1016/j.neucom.2007.12.038
   Xiuqing Wang, 2011, 2011 10th IEEE International Conference on Cognitive Informatics & Cognitive Computing (ICCI-CC 2011), P348, DOI 10.1109/COGINF.2011.6016164
   Ye J, 2008, NEUROCOMPUTING, V71, P1561, DOI 10.1016/j.neucom.2007.04.014
NR 23
TC 3
Z9 3
U1 0
U2 2
PY 2012
VL 7666
BP 652
EP 659
UT WOS:000345091300079
DA 2023-11-16
ER

PT J
AU Di Paolo, EA
AF Di Paolo, EA
TI Spike-timing dependent plasticity for evolved robots
SO ADAPTIVE BEHAVIOR
DT Article
DE evolutionary robotics; spiking neural networks; spike-timing dependent
   plasticity; activity-dependent synaptic scaling; neural noise;
   robustness
ID EXPERIENCE
AB Plastic spiking neural networks are synthesized for phototactic robots using evolutionary techniques. Synaptic plasticity asymmetrically depends on the precise relative timing between presynaptic and postsynaptic spikes at the millisecond range and on longer-term activity-dependent regulatory scaling. Comparative studies have been carried out for different kinds of plastic neural networks with low and high levels of neural noise. In all cases, the evolved controllers are highly robust against internal synaptic decay and other perturbations. The importance of the precise timing of spikes is demonstrated by randomizing the spike trains. In the low neural noise scenario, weight values undergo rhythmic changes at the mesoscale due to bursting, but during periods of high activity they are finely regulated at the microscale by synchronous or entrained firing. Spike train randomization results in loss of performance in this case. In contrast, in the high neural noise scenario, robots are robust to loss of information in the timing of the spike trains, demonstrating the counterintuitive results that plasticity, which is dependent on precise spike timing, can work even in its absence, provided the behavioral strategies make use of robust longer-term invariants of sensorimotor interaction. A comparison with a rate-based model of synaptic plasticity shows that under similarly noisy conditions, asymmetric spike-timing dependent plasticity achieves better performance by means of efficient reduction in weight variance over time. Performance also presents negative sensitivity to reduced levels of noise, showing that random firing has a functional value.
C1 Univ Sussex, Sch Cognit & Comp Sci, Brighton BN1 9QH, E Sussex, England.
RP Di Paolo, EA (corresponding author), Univ Sussex, Sch Cognit & Comp Sci, Brighton BN1 9QH, E Sussex, England.
EM ezequiel@cogs.susx.ac.uk
CR Abbott LF, 2000, NAT NEUROSCI, V3, P1178, DOI 10.1038/81453
   Abbott LF, 1996, CEREB CORTEX, V6, P406, DOI 10.1093/cercor/6.3.406
   Beer R. D., 1996, From Animals to Animats 4. Proceedings of the Fourth International Conference on Simulation of Adaptive Behavior, P421
   Beer R.D., 1990, INTELLIGENCE ADAPTIV
   Beer Randall D., 1992, Adaptive Behavior, V1, P91, DOI 10.1177/105971239200100105
   Bi GQ, 2001, ANNU REV NEUROSCI, V24, P139, DOI 10.1146/annurev.neuro.24.1.139
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Brody CD, 1999, NEURAL COMPUT, V11, P1537, DOI 10.1162/089976699300016133
   CHECHIK G, 2002, UNPUB SPIKE TIMING D
   DIPAOLO EA, 1990, ANIMALS ANIMATS, V6
   Floreano D, 2000, NEURAL NETWORKS, V13, P431, DOI 10.1016/S0893-6080(00)00032-0
   FLOREANO D, 2001, EVOLUTIONARY ROBOTIC, V4
   Gerstner W, 1997, P NATL ACAD SCI USA, V94, P12740, DOI 10.1073/pnas.94.24.12740
   HARVEY I, 1994, ANIMALS ANIMATS, V3, P292
   Hopfield JJ, 2001, P NATL ACAD SCI USA, V98, P1282, DOI 10.1073/pnas.031567098
   Horn D, 1998, NEURAL COMPUT, V10, P1, DOI 10.1162/089976698300017863
   Husbands P, 1998, CONNECT SCI, V10, P185, DOI 10.1080/095400998116404
   Jakobi N, 1997, ADAPT BEHAV, V6, P325, DOI 10.1177/105971239700600205
   Kempter R, 1999, PHYS REV E, V59, P4498, DOI 10.1103/PhysRevE.59.4498
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   Mehta MR, 2002, NATURE, V417, P741, DOI 10.1038/nature00807
   Mehta MR, 2000, NEURON, V25, P707, DOI 10.1016/S0896-6273(00)81072-7
   Mehta MR, 1997, P NATL ACAD SCI USA, V94, P8918, DOI 10.1073/pnas.94.16.8918
   Rao RPN, 2001, NEURAL COMPUT, V13, P2221, DOI 10.1162/089976601750541787
   Rubin J, 2001, PHYS REV LETT, V86, P364, DOI 10.1103/PhysRevLett.86.364
   Ruppin E, 2002, NAT REV NEUROSCI, V3, P132, DOI 10.1038/nrn729
   Slocum AC, 2000, FROM ANIM ANIMAT, P430
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Stopfer M, 1997, NATURE, V390, P70, DOI 10.1038/36335
   SUTTON RS, 1988, MACH LEARN, V3, P220
   Tuckwell H.C., 1988, INTRO THEORETICAL NE, V2
   Turrigiano GG, 1999, TRENDS NEUROSCI, V22, P221, DOI 10.1016/S0166-2236(98)01341-1
   Turrigiano GG, 1998, NATURE, V391, P892, DOI 10.1038/36103
   van Rossum MCW, 2000, J NEUROSCI, V20, P8812
   Yao HS, 2001, NEURON, V32, P315, DOI 10.1016/S0896-6273(01)00460-3
NR 37
TC 39
Z9 40
U1 1
U2 10
PY 2002
VL 10
IS 3-4
BP 243
EP 263
DI 10.1177/1059712302010003006
UT WOS:000183615500007
DA 2023-11-16
ER

PT S
AU Eriksson, J
   Torres, O
   Mitchell, A
   Tucker, G
   Lindsay, K
   Halliday, D
   Rosenberg, J
   Moreno, JM
   Villa, AEP
AF Eriksson, J
   Torres, O
   Mitchell, A
   Tucker, G
   Lindsay, K
   Halliday, D
   Rosenberg, J
   Moreno, JM
   Villa, AEP
BE Tyrrell, AM
   Haddow, PC
   Torresen, J
TI Spiking neural networks for reconfigurable POEtic tissue
SO EVOLVABLE SYSTEMS: FROM BIOLOGY TO HARDWARE, PROCEEDINGS
SE Lecture Notes in Computer Science
DT Article; Proceedings Paper
CT 5th International Conference on Evolvable Systems
CY MAR 17-20, 2003
CL TRONDHEIM, NORWAY
ID SYNAPTIC PLASTICITY; SIMULATION; MEMORY; LONG
AB Vertebrate and most invertebrate organisms interact with their environment through processes of adaptation and learning. Such processes are generally controlled by complex networks of nerve cells, or neurons, and their interactions. Neurons are characterized by all-or-none discharges - the spikes and the time series corresponding to the sequences of the discharges - the spike trains - carry most of the information used for intercellular communication. This paper describes biologically inspired spiking neural network models suitable for digital hardware implementation. We consider bio-realism, hardware friendliness, and performance as factors which influence the ability of these models to integrate into a flexible computational substrate inspired by evolutionary, developmental and learning aspects of living organisms. Both software and hardware simulations have been used to assess and compare the different models to determine the most suitable spiking neural network model.
C1 Univ York, York YO10 5DD, N Yorkshire, England.
   Univ Glasgow, Glasgow, Lanark, Scotland.
   Tech Univ Catalunya, Barcelona, Spain.
   Univ Lausanne, Lab Neuroheurist, Lausanne, Switzerland.
   Univ Grenoble 1, Grenoble, France.
RP Eriksson, J (corresponding author), Univ York, York YO10 5DD, N Yorkshire, England.
EM acmll@ohm.york.ac.uk
CR [Anonymous], 1998, PULSED NEURAL NETWOR
   [Anonymous], 1998, SYNAPTIC ORG BRAIN
   Bohte S, 2000, P EUR S ART NEUR NET, P419
   CHRISTODOULOU C, 1992, P INT JOINT C NEUR N, V3, P165
   Del Giudice P, 2001, NEUROCOMPUTING, V38, P1175, DOI 10.1016/S0925-2312(01)00557-4
   Fusi S, 2000, NEURAL COMPUT, V12, P2227, DOI 10.1162/089976600300014917
   Fusi S, 2001, NEUROCOMPUTING, V38, P1223, DOI 10.1016/S0925-2312(01)00571-9
   HARTMANN G, 1997, P 6 INT C MICR NEUR, P130
   Hill SL, 1997, NETWORK-COMP NEURAL, V8, P165, DOI 10.1088/0954-898X/8/2/004
   Jahnke A., 1996, Proceedings of the Fifth International Conference on Microelectronics for Neural Networks and Fuzzy Systems. MicroNeuro'96, P232, DOI 10.1109/MNNFS.1996.493796
   Mattia M, 2000, NEURAL COMPUT, V12, P2305, DOI 10.1162/089976600300014953
   MAYA S, 2000, LNCS, V1896, P270
   ROBERTS PD, IN PRESS BIOL CYBERN
   ROGGEN D, UNPUB 5 INT C EV SYS
   TEMPESTI G, UNPUB 5 INT C EV SYS
   TYRRELL AM, 5 INT C EV SYST ICES
   Villa AEP, 2000, CONC ADV BRAIN RES, V3, P1
NR 17
TC 11
Z9 12
U1 0
U2 1
PY 2003
VL 2606
BP 165
EP 173
UT WOS:000182975200015
DA 2023-11-16
ER

PT C
AU Kulkarni, SR
   Alexiades, JM
   Rajendran, B
AF Kulkarni, Shruti R.
   Alexiades, John M.
   Rajendran, Bipin
GP IEEE
TI Learning and Real-time Classification of Hand-written Digits With
   Spiking Neural Networks
SO 2017 24TH IEEE INTERNATIONAL CONFERENCE ON ELECTRONICS, CIRCUITS AND
   SYSTEMS (ICECS)
SE IEEE International Conference on Electronics Circuits and Systems
DT Proceedings Paper
CT 24th IEEE International Conference on Electronics, Circuits and Systems
   (ICECS)
CY DEC 05-08, 2017
CL Batumi, GEORGIA
DE Spiking neural networks; classification; supervised learning; GPU based
   acceleration; real-time processing
AB We describe a novel spiking neural network (SNN) for automated, real-time handwritten digit classification and its implementation on a GP-GPU platform. Information processing within the network, from feature extraction to classification is implemented by mimicking the basic aspects of neuronal spike initiation and propagation in the brain. The feature extraction layer of the SNN uses fixed synaptic weight maps to extract the key features of the image and the classifier layer uses the recently developed NormAD approximate gradient descent based supervised learning algorithm for spiking neural networks to adjust the synaptic weights. On the standard MNIST database images of handwritten digits, our network achieves an accuracy of 99.80% on the training set and 98.06% on the test set, with nearly 7x fewer parameters compared to the state-of-the-art spiking networks. We further use this network in a GPU based user-interface system demonstrating real-time SNN simulation to infer digits written by different users. On a test set of 500 such images, this real-time platform achieves an accuracy exceeding 97% while making a prediction within an SNN emulation time of less than 100 ms.
C1 [Kulkarni, Shruti R.; Alexiades, John M.; Rajendran, Bipin] New Jersey Inst Technol, Dept Elect & Comp Engn, Newark, NJ 07102 USA.
RP Kulkarni, SR (corresponding author), New Jersey Inst Technol, Dept Elect & Comp Engn, Newark, NJ 07102 USA.
EM srk68@njit.edu; jma59@njit.edu; bipin@njit.edu
CR [Anonymous], 2016, ARXIV161105141
   [Anonymous], OPTIMIZING PARALLEL
   Anwani N., 2015, P INT JOINT C NEUR N, P1, DOI DOI 10.1109/IJCNN.2015
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Coates A., 2013, 30 INT C MACH LEARN
   Culjak I., 2012, 2012 35th International Convention on Information and Communication Technology, Electronics and Microelectronics, P1725
   Diehl P.U., 2015, 2015 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2015.7280696
   Fidjeland Andreas K, 2010, 2010 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2010.5596678
   Krichmar JL, 2015, ACM J EMERG TECH COM, V11, DOI 10.1145/2629509
   Lee JM, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00191
   Lee WW, 2017, IEEE T NEUR NET LEAR, V28, P849, DOI 10.1109/TNNLS.2015.2509479
   LiWan Matthew Zeiler, 2013, P 30 INT C MACH LEAR, P1058
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Mohemmed A, 2012, INT J NEURAL SYST, V22, DOI 10.1142/S0129065712500128
   Nageswaran JM, 2009, NEURAL NETWORKS, V22, P791, DOI 10.1016/j.neunet.2009.06.028
   Naveros F, 2017, FRONT NEUROINFORM, V11, DOI 10.3389/fninf.2017.00007
   Olshausen BA, 2003, J COGNITIVE NEUROSCI, V15, P154, DOI 10.1162/089892903321107891
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Rueckauer B., 2016, ARXIV161204052
   Yavuz E, 2016, SCI REP-UK, V6, DOI 10.1038/srep18854
   Yudanov D, 2010, IEEE IJCNN
NR 21
TC 5
Z9 5
U1 1
U2 2
PY 2017
BP 128
EP 131
UT WOS:000426974200030
DA 2023-11-16
ER

PT C
AU Kulkarni, SR
   Baghini, MS
AF Kulkarni, Shruti R.
   Baghini, Maryam Shojaei
BE Wang, H
   Yuen, SY
   Wang, L
   Shao, L
   Wang, X
TI Spiking Neural Network based ASIC for Character Recognition
SO 2013 NINTH INTERNATIONAL CONFERENCE ON NATURAL COMPUTATION (ICNC)
DT Proceedings Paper
CT 9th International Conference on Natural Computation (ICNC)
CY JUL 23-25, 2013
CL Shenyang, PEOPLES R CHINA
DE Spiking Neural networks; character recognition; ASIC design
AB Spiking neural networks are the recent models of artificial neural networks. These networks use biologically similar neuron models as their basic computation units. This paper presents and compares a custom spiking neural network (SNN) with a conventional nearest neighbour classifier for hand written character recognition. The classifiers are designed and simulated in 90nm CMOS technology. The two algorithms are compared in terms of their success rates and their hardware requirements (based on the area and power estimates). The classification performance of the SNN is also compared with that of second generation feedforward neural network, with the same set of images. The robustness of SNN is demonstrated in this work by its ability to classify the 30 out of 32 noisy characters images presented as compared to the nearest neighbour algorithm, which correctly classified only 20 of them. The feedforward neural network using backpropagation algorithm was able to correctly identify 29 out of 32 noisy images in MATLAB. In terms of hardware, the ASIC realizing the nearest neighbour classifier dissipates power of 1.2mW and an area of 380 m x 380 m, while the SNN dissipates 16.7mW power and an area of 1mm x 1mm. The higher area and power requirements for the SNN stem from its inherent parallel architecture. Earlier works have focused on realization of a single spiking neuron and its variants while this work brings about the application using networks of these neurons and their suitability for custom realization.
C1 [Kulkarni, Shruti R.; Baghini, Maryam Shojaei] Indian Inst Technol, Dept Elect Engn, Bombay, Maharashtra, India.
RP Kulkarni, SR (corresponding author), Indian Inst Technol, Dept Elect Engn, Bombay, Maharashtra, India.
EM shruti123k@yahoo.co.in; mshojaei@ee.iitb.ac.in
CR Abbott LF, 1999, BRAIN RES BULL, V50, P303, DOI 10.1016/S0361-9230(99)00161-6
   Bhuiyan MA, 2009, CIMSVP 2009: IEEE SYMPOSIUM ON COMPUTATIONAL INTELLIGENCE FOR MULTIMEDIA SIGNAL AND VISION PROCESSING, P29
   Blumenstein M, 2003, PROC INT CONF DOC, P137
   Gaurav D. D., 2012, CORR
   Gerstner W, 2002, BIOL CYBERN, V87, P404, DOI 10.1007/s00422-002-0353-y
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Gupta A, 2007, IEEE IJCNN, P53, DOI 10.1109/IJCNN.2007.4370930
   Hong L, 2010, J APPL SCI, V10, P1841
   Jolivet R, 2003, LECT NOTES COMPUT SC, V2714, P846
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Mamtha H.R., 2011, P INT C SOFTW COMP A, V9
   Nagare A. P., 2011, INT J COMPUTER APPL, V25, P36
   PANCHEV C, 2002, P INT C ART NEUR NET, P896
   Paugam-moisy H., 2010, COMPUTING SPIKING NE
   Singh B, 2011, J PATTERN RECOGNIT R, V6, P269, DOI 10.13176/11.302
   Suruchi G. D., 2012, INT J ENG INNOVATIVE, V1
   Vamvakas G, 2010, PATTERN RECOGN, V43, P2807, DOI 10.1016/j.patcog.2010.02.018
NR 17
TC 0
Z9 0
U1 0
U2 4
PY 2013
BP 194
EP 199
UT WOS:000341627900033
DA 2023-11-16
ER

PT J
AU Adineh-vand, A
   Karimi, G
   Khazaei, M
AF Adineh-vand, Ayoub
   Karimi, Gholamreza
   Khazaei, Mozafar
TI Digital Implementation of a Spiking Convolutional Neural Network for
   Tumor Detection
SO INFORMACIJE MIDEM-JOURNAL OF MICROELECTRONICS ELECTRONIC COMPONENTS AND
   MATERIALS
DT Article
DE Brain tissue; MRI images; Spiking Neural Network; Digital
   Implementation; STDP
ID MODEL; FPGA
AB The structural variation of the brain tissue creates challenges for detection of tumors in MRI images. In this paper, an architecture for spiking convolutional neural networks (SCNNs) is implemented in an embedded system and their potential is evaluated in terms of hardware utilization and power consumption in complex applications such as tumor detection. Accordingly, the structure of the proposed SCNN is implemented on a field-programmable gate array (FPGA) using fixed point arithmetic. To evaluate the speed, accuracy and flexibility of the proposed SCNN, lzhikevich neuron model is used with the spike-timing-dependent plasticity (STDP) learning rule. The suggested neural network is explored for digital implementation possibility and costs. Results of the hardware synthesis and digital implementation are presented on an FPGA.
C1 [Adineh-vand, Ayoub; Karimi, Gholamreza] Razi Univ, Fac Engn, Elect Engn Dept, Kermanshah, Iran.
   [Khazaei, Mozafar] Kermanshah Univ Med Sci, Reprod Res Ctr, Kermanshah, Iran.
RP Karimi, G (corresponding author), Razi Univ, Fac Engn, Elect Engn Dept, Kermanshah, Iran.
EM ghkarimi@razi.ac.ir
CR Azghadi MR, 2012, 2012 INT JOINT C NEU, P1, DOI [10.1109/IJCNN.2012.6252820, DOI 10.1109/IJCNN.2012.6252820]
   Farsa E. Zaman, 2019, IEEE T CIRCUITS SY 2
   Farsa EZ, 2015, J COMPUT ELECTRON, V14, P707, DOI 10.1007/s10825-015-0709-x
   FITZHUGH R, 1961, BIOPHYS J, V1, P445, DOI 10.1016/S0006-3495(61)86902-6
   Gilan A. A., 2019, IEEE T CIRCUITS SY 2
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Indiveri G, 2006, IEEE T NEURAL NETWOR, V17, P211, DOI 10.1109/TNN.2005.860850
   Guerra-Hernandez EI, 2017, IEEE ACCESS, V5, P8301, DOI 10.1109/ACCESS.2017.2696985
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Jiménez-Fernández A, 2017, IEEE T NEUR NET LEAR, V28, P804, DOI 10.1109/TNNLS.2016.2583223
   Karimi G, 2018, INT J CIRC THEOR APP, V46, P965, DOI 10.1002/cta.2457
   Markram H, 2011, FRONT NEURAL CIRCUIT, V5, DOI 10.3389/fncir.2011.00006
   Motamedi M, 2016, ASIA S PACIF DES AUT, P575, DOI 10.1109/ASPDAC.2016.7428073
   NAGUMO J, 1962, P IRE, V50, P2061, DOI 10.1109/JRPROC.1962.288235
   Naka T, 2019, IEEE T CIRCUITS-II, V66, P1247, DOI 10.1109/TCSII.2018.2876974
   Paugam-Moisy H., 2012, HDB NATURAL COMPUTIN, V1, P1, DOI [DOI 10.1007/978-3-540-92910-9_10, 10.1007/978-3-540-92910-9_10]
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Ponulak F, 2011, ACTA NEUROBIOL EXP, V71, P409
   Pullini A., 2017, IEEE T CIRCUITS SYST
   Putnam A, 2014, CONF PROC INT SYMP C, P13, DOI 10.1109/ISCA.2014.6853195
   Rahman A, 2016, DES AUT TEST EUROPE, P1393
   ROSE RM, 1989, PROC R SOC SER B-BIO, V237, P267, DOI 10.1098/rspb.1989.0049
   Tapiador-Morales R, 2019, IEEE T BIOMED CIRC S, V13, P159, DOI 10.1109/TBCAS.2018.2880012
   Yang SM, 2016, NEUROCOMPUTING, V177, P274, DOI 10.1016/j.neucom.2015.11.026
   Zhang Chen, 2015, P 2015 ACMSIGDA INT, P161, DOI 10.1145/2684746.2689060
NR 26
TC 4
Z9 4
U1 0
U2 4
PD DEC
PY 2019
VL 49
IS 4
BP 193
EP 201
DI 10.33180/InfMIDEM2019.401
UT WOS:000510400300001
DA 2023-11-16
ER

PT J
AU Zhang, ML
   Qu, H
   Xie, XR
   Kurths, J
AF Zhang, Malu
   Qu, Hong
   Xie, Xiurui
   Kurths, Juergen
TI Supervised learning in spiking, neural networks with noise-threshold
SO NEUROCOMPUTING
DT Article
DE Spiking neurons; Noise-threshold; Supervised learning; Spiking neural
   networks (SNNs); Anti-noise capability
ID PRECISION; NEURONS; INFORMATION; SIGNALS; TRAINS; MODEL
AB With a similar capability of processing spikes as biological neural systems, networks of spiking neurons are expected to achieve a performance similar to that of living brains. Despite the achievement of spiking neuron based applications, most of them assume noise-free condition for learning and testing. This assumption, though fairly general, ignores the fact that noise widely exists in spiking neural networks (SNNs) and the neural response can be significantly disturbed by noise. Therefore, how to deal with noise is an important issue in the applications of SNNs. Here, by analyzing strategies employed to make spiking neurons robust to noise, also inspired by biological neurons, we propose a strategy that train spiking neurons with a dynamic firing threshold named noise-threshold. The noise-threshold can be applied by the existing supervised learning methods to improve the noise tolerance of them. Experimental results show that, with a combination of noise-threshold, the anti-noise capability of the existing supervised learning methods improves significantly, and the trained neuron can precisely and reliably reproduce target sequences of spikes even under highly noisy conditions. More importantly, the SNNs-based computational model equipped with a noise-threshold is more robust and can achieve a good performance even with different types of noise. Therefore, the noise-threshold is significant to practical applications and theoretical researches of SNNs.
C1 [Zhang, Malu; Qu, Hong; Xie, Xiurui] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 610054, Peoples R China.
   [Kurths, Juergen] Potsdam Inst Climate Impact Res PIK, D-14473 Potsdam, Germany.
   [Kurths, Juergen] Humboldt Univ, Dept Phys, D-12489 Berlin, Germany.
   [Kurths, Juergen] Univ Aberdeen, Inst Complex Syst & Math Biol, Aberdeen AB24 3UE, Scotland.
RP Qu, H (corresponding author), Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 610054, Peoples R China.
EM hongqu@uestc.edu.cn
CR Albers C., ARXIV14076525
   [Anonymous], PLOS ONE
   Azouz R, 2000, P NATL ACAD SCI USA, V97, P8110, DOI 10.1073/pnas.130200797
   Bair W, 1996, NEURAL COMPUT, V8, P1185, DOI 10.1162/neco.1996.8.6.1185
   Berry MJ, 1998, ADV NEUR IN, V10, P110
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Borst A, 1999, NAT NEUROSCI, V2, P947, DOI 10.1038/14731
   Brunel N, 1999, NEURAL COMPUT, V11, P1621, DOI 10.1162/089976699300016179
   Chicca E., 2000, P WORLD C NEUR
   Dora S, 2016, NEUROCOMPUTING, V171, P1216, DOI 10.1016/j.neucom.2015.07.086
   Florian R.V., 2013, PLOS ONE, V7
   Fusi S, 2000, IEEE IJCNN, P121, DOI 10.1109/IJCNN.2000.861291
   Gerstner W., 2002, SPIKING NEURON MODEL
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Gollisch T, 2008, SCIENCE, V319, P1108, DOI 10.1126/science.1149639
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Henze DA, 2001, NEUROSCIENCE, V105, P121, DOI 10.1016/S0306-4522(01)00167-1
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Hu J, 2013, NEURAL COMPUT, V25, P450, DOI 10.1162/NECO_a_00395
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Kempter R, 1999, ADV NEUR IN, V11, P125
   Kerr D, 2015, NEUROCOMPUTING, V158, P268, DOI 10.1016/j.neucom.2015.01.011
   Maass W, 1997, NEURAL COMPUT, V9, P279, DOI 10.1162/neco.1997.9.2.279
   Maass W., 2006, NEURAL NETWORKS, V10, P1659
   Maass W., NOISY SPIKING NEURON
   MAINEN ZF, 1995, SCIENCE, V268, P1503, DOI 10.1126/science.7770778
   Manwani A, 1999, NEURAL COMPUT, V11, P1797, DOI 10.1162/089976699300015972
   Memmesheimer RM, 2014, NEURON, V82, P925, DOI 10.1016/j.neuron.2014.03.026
   Mohemmed A, 2012, INT J NEURAL SYST, V22, DOI 10.1142/S0129065712500128
   Montemurro MA, 2007, J NEUROPHYSIOL, V98, P1871, DOI 10.1152/jn.00593.2007
   Nadasdy Z, 2009, FRONT SYST NEUROSCI, V3, DOI 10.3389/neuro.06.006.2009
   Ponulak F., 2006, THESIS
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Qu H, 2015, NEUROCOMPUTING, V151, P310, DOI 10.1016/j.neucom.2014.09.034
   Qu H, 2009, IEEE T NEURAL NETWOR, V20, P1724, DOI 10.1109/TNN.2009.2029858
   Reinagel P, 2000, J NEUROSCI, V20, P5392, DOI 10.1523/JNEUROSCI.20-14-05392.2000
   Rieke F., SPIKES EXPLORING NEU
   Rostro-Gonzalez H, 2015, NEUROCOMPUTING, V170, P47, DOI 10.1016/j.neucom.2015.03.090
   Rostro-Gonzalez H., 2015, NEUROCOMPUTING
   Schneidman E, 1998, NEURAL COMPUT, V10, P1679, DOI 10.1162/089976698300017089
   Schneidman E., THESIS
   Schreiber S, 2003, NEUROCOMPUTING, V52-4, P925, DOI 10.1016/S0925-2312(02)00838-X
   Shmiel T, 2005, P NATL ACAD SCI USA, V102, P18655, DOI 10.1073/pnas.0509346102
   Taherkhani A., 2015, IEEE T NEURAL NETW L
   Taherkhani A., 2014, P ESANN
   Taherkhani A., 2015, NEURAL NETWORKS IJCN, P1
   Uzzell VJ, 2004, J NEUROPHYSIOL, V92, P780, DOI 10.1152/jn.01171.2003
   van Rossum MCW, 2003, J NEUROPHYSIOL, V89, P2406, DOI 10.1152/jn.01106.2002
   vanVreeswijk C, 1996, SCIENCE, V274, P1724, DOI 10.1126/science.274.5293.1724
   Nguyen VA, 2012, IEEE T NEUR NET LEAR, V23, P971, DOI 10.1109/TNNLS.2012.2191419
   Wang JL, 2014, NEUROCOMPUTING, V144, P526, DOI 10.1016/j.neucom.2014.04.017
   Wang WW, 2012, IEEE T NEUR NET LEAR, V23, P1574, DOI 10.1109/TNNLS.2012.2208477
   Wang ZZ, 2016, NEUROCOMPUTING, V173, P1203, DOI 10.1016/j.neucom.2015.08.078
   Wei S, 2011, NEUROCOMPUTING, V74, P1485, DOI 10.1016/j.neucom.2011.01.005
   Xu Y, 2013, NEURAL COMPUT, V25, P1472, DOI 10.1162/NECO_a_00450
   Yu Q., 2015, IEEE T NEURAL NETW L
   Yu Q, 2013, IEEE T NEUR NET LEAR, V24, P1539, DOI 10.1109/TNNLS.2013.2245677
   Zhang M., 2015, P 18 AS PAC S INT EV, V1
NR 58
TC 50
Z9 50
U1 1
U2 35
PD JAN 5
PY 2017
VL 219
BP 333
EP 349
DI 10.1016/j.neucom.2016.09.044
UT WOS:000390734300031
DA 2023-11-16
ER

PT C
AU Tao, XL
   Michel, HE
AF Tao, XL
   Michel, HE
BE Saito, TT
TI Novel artificial neural networks for remote-sensing data classification
SO Optics and Photonics in Global Homeland Security
SE PROCEEDINGS OF THE SOCIETY OF PHOTO-OPTICAL INSTRUMENTATION ENGINEERS
   (SPIE)
DT Proceedings Paper
CT Conference on Optics and Photonics in Global Homeland Security
CY MAR 29-APR 01, 2005
CL Orlando, FL
DE normalized radial basis function (NRBF); spiking neural network;
   remote-sensing data classification
ID SPIKING NEURONS
AB This paper discusses two novel artificial neural network architectures applied to multi-class classification problems of remote-sensing data. These approaches are 1) a spiking neural-network model for the partitioning of data into clusters, and 2) a neuron model based on complex-valued weights (CVN). In the former model, the learning process is based on the Spike Timing-Dependent Plasticity rule under the Hebbian Learning framework. With temporally encoded inputs, the synaptic efficiencies of the delays between the pre- and post-synaptic spikes can store the information of different data clusters. With the encoding method using Gaussian receptive fields, the model was applied to the remote-sensing data. The result showed that it could provide more useful information than using traditional clustering method such as K-means. The CVN model has proved to be more powerful than traditional neuron models in solving the XOR problem and image processing problems. This paper discusses an implementation of the complex-valued neuron in NRBF neural networks to improve the NRBF structure. The complex-valued weights are used in the supervised learning part of an NRBF neural network. This classifier was tested with satellite multi-spectral image data and results show that this neural network model is more accurate and powerful than the conventional NRBF model.
C1 Univ Massachusetts, Dept Elect & Comp Engn, Dartmouth, MA 02747 USA.
RP Tao, XL (corresponding author), Univ Massachusetts, Dept Elect & Comp Engn, 285 Old Westport Rd, Dartmouth, MA 02747 USA.
CR Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   Cha I, 1996, IEEE T IMAGE PROCESS, V5, P964, DOI 10.1109/83.503912
   Gerstner W, 2002, BIOL CYBERN, V87, P404, DOI 10.1007/s00422-002-0353-y
   Ghosh J., 2001, RADIAL BASIS FUNCTIO, V67
   Hebb D., 2005, ORG BEHAV NEUROPSYCH
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   MOODY J, 1988, P 1988 CONN MOD SUM
   Moody J, 1989, NEURAL COMPUT, V1, P281, DOI 10.1162/neco.1989.1.2.281
   Natschlager T, 1998, NETWORK-COMP NEURAL, V9, P319, DOI 10.1088/0954-898X/9/3/003
   ROBERT A, 1997, REMOTE SENSING MODEL
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   TAO X, 2003, P SPIE
   TAO X, 2004, P IC AI 04 JUN 21 24
NR 14
TC 2
Z9 2
U1 0
U2 3
PY 2005
VL 5781
BP 127
EP 138
DI 10.1117/12.609117
UT WOS:000230404300016
DA 2023-11-16
ER

PT C
AU Zou, CL
   Cui, XX
   Kuang, YS
   Wang, Y
   Wang, XA
AF Zou, Chenglong
   Cui, Xiaoxin
   Kuang, Yisong
   Wang, Yuan
   Wang, Xinan
GP IEEE
TI A Hybrid Spiking Recurrent Neural Network on Hardware for Efficient
   Emotion Recognition
SO 2022 IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE CIRCUITS
   AND SYSTEMS (AICAS 2022): INTELLIGENT TECHNOLOGY IN THE POST-PANDEMIC
   ERA
DT Proceedings Paper
CT IEEE International Conference on Artificial Intelligence Circuits and
   Systems (AICAS) - Intelligent Technology in the Post-Pandemic Era
CY JUN 13-15, 2022
CL Incheon, SOUTH KOREA
DE AI; RNN; SNN; emotion recognition
AB In recent years, neuromorphic engineering based on spiking neural networks (SNNs) for real-time and low-power artificial intelligence (AI) tasks has attracted a lot of interest. However, most of the previous implementations on hardware of these algorithms concentrate on traditional feedforward fully-connected/convolutional neural network (CNNs) architectures which are used for vision image processing. Their applications in temporal text tasks using recurrent neural networks (RNNs) is less discussed. In this paper, we point out main difficulties of RNNs implementation on conventional neuromorphic systems and propose a hardware-oriented spiking RNN architecture for emotion recognition, which absorbs the external dynamics of traditional RNN and internal dynamics of SNN. Experimental results on two emotion recognition datasets show our spiking RNNs achieve comparable accuracies with other deep learning models and efficient run-time performance.
C1 [Zou, Chenglong; Cui, Xiaoxin; Kuang, Yisong; Wang, Yuan] Peking Univ, Sch Integrated Circuits, Key Lab Microelect Devices & Circuits, Beijing 100871, Peoples R China.
   [Zou, Chenglong; Wang, Xinan] Peking Univ, Sch ECE, Key Lab Integrated Microsyst, Shenzhen Grad Sch, Shenzhen 518055, Peoples R China.
RP Cui, XX; Wang, Y (corresponding author), Peking Univ, Sch Integrated Circuits, Key Lab Microelect Devices & Circuits, Beijing 100871, Peoples R China.
EM cuixx@pku.edu.cn; wangyuan@pku.edu.cn
CR Amir A, 2017, PROC CVPR IEEE, P7388, DOI 10.1109/CVPR.2017.781
   BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181
   Bengio Y, 2013, Arxiv, DOI arXiv:1305.2982
   Bouvier M, 2019, ACM J EMERG TECH COM, V15, DOI 10.1145/3304103
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Deng L, 2020, NEURAL NETWORKS, V121, P294, DOI 10.1016/j.neunet.2019.09.005
   Diehl PU, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON REBOOTING COMPUTING (ICRC)
   Diehl PU, 2015, IEEE IJCNN
   Esser SK, 2016, P NATL ACAD SCI USA, V113, P11441, DOI 10.1073/pnas.1604850113
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Kuang YS, 2021, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS51556.2021.9401195
   Levy O, 2014, ADV NEUR IN, V27
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Yousefzadeh A, 2019, 2019 IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE CIRCUITS AND SYSTEMS (AICAS 2019), P81, DOI [10.1109/AICAS.2019.8771624, 10.1109/aicas.2019.8771624]
   Zhang Ye, 2016, Arxiv, DOI [arXiv:1510.03820, DOI 10.48550/ARXIV.1510.03820]
   Zou C., 2020, IEEE INT SYMP CIRC S, P1, DOI DOI 10.1109/iscas45731.2020.9180918
   Zou CL, 2021, FRONT NEUROSCI-SWITZ, V15, DOI 10.3389/fnins.2021.694170
NR 18
TC 0
Z9 0
U1 0
U2 6
PY 2022
BP 332
EP 335
DI 10.1109/AICAS54282.2022.9869950
UT WOS:000859273200084
DA 2023-11-16
ER

PT C
AU Morie, T
AF Morie, Takashi
GP IEEE
TI CMOS Circuits and Nanodevices for Spike Based Neural Computing
SO 2015 IEEE INTERNATIONAL MEETING FOR FUTURE OF ELECTRON DEVICES, KANSAI
   (IMFEDK)
DT Proceedings Paper
CT IEEE International Meeting for Future of Electron Devices, Kansai
   (IMFEDK)
CY JUN 04-05, 2015
CL Kyoto, JAPAN
DE spiking neuron; pulse-coupled oscillator; coupled Markov random field
   model; multiply-and-accumulation calculation; CMOS circuit; nanodisk
   array
AB This paper describes hardware implementation of two integrate-and-fire type neuron models for spike based computing: pulse-coupled phase oscillator networks and spiking neural networks. A coupled Markov random field model for image region segmentation can be implemented using a pulse-coupled phase oscillator network. Multiply-and-accumulation calculation can be performed using rise timing of responses in an integrate-and-fire type spiking neuron model. Both oscillator and neuron models can be implemented by CMOS circuits consisting of capacitors with current sources or resistors. For constructing large-scale networks, nanodisk array structures are used for realizing high resistance.
C1 [Morie, Takashi] Kyushu Inst Technol, Grad Sch Life Sci & Syst Engn, Kitakyushu, Fukuoka 8080196, Japan.
RP Morie, T (corresponding author), Kyushu Inst Technol, Grad Sch Life Sci & Syst Engn, Kitakyushu, Fukuoka 8080196, Japan.
EM morie@brain.kyutech.ac.jp
CR Huang CH, 2009, JPN J APPL PHYS, V48, DOI 10.1143/JJAP.48.04C187
   Igarashi M, 2010, APPL PHYS EXPRESS, V3, DOI 10.1143/APEX.3.085202
   Maass W., 1999, PULSED NEURAL NETWOR
   Matsuzaka K., 2012, NONLINEAR THEORY ITS, V3, P180
   Matsuzaka K, 2015, ELECTRON LETT, V51, P46, DOI 10.1049/el.2014.2105
   Morie T, 2014, ASIA S PACIF DES AUT, P185, DOI 10.1109/ASPDAC.2014.6742887
NR 6
TC 1
Z9 1
U1 0
U2 0
PY 2015
UT WOS:000380445900044
DA 2023-11-16
ER

PT J
AU Yu, HT
   Guo, XM
   Wang, J
   Deng, B
   Wei, XL
AF Yu, Haitao
   Guo, Xinmeng
   Wang, Jiang
   Deng, Bin
   Wei, Xile
TI Vibrational resonance in adaptive small-world neuronal networks with
   spike-timing-dependent plasticity
SO PHYSICA A-STATISTICAL MECHANICS AND ITS APPLICATIONS
DT Article
DE Vibrational resonance; Spike-timing-dependent plasticity; Neural
   network; Small-world network
ID STOCHASTIC RESONANCE; EXCITABLE SYSTEMS; COHERENCE; NOISE;
   SYNCHRONIZATION; CONNECTIVITY; ENHANCEMENT; MECHANISMS; MEDIA
AB The phenomenon of vibrational resonance is investigated in adaptive Newman-Watts small-world neuronal networks, where the strength of synaptic connections between neurons is modulated based on spike-timing-dependent plasticity. Numerical results demonstrate that there exists appropriate amplitude of high-frequency driving which is able to optimize the neural ensemble response to the weak low-frequency periodic signal. The effect of networked vibrational resonance can be significantly affected by spike-timing-dependent plasticity. It is shown that spike-timing-dependent plasticity with dominant depression can always improve the efficiency of vibrational resonance, and a small adjusting rate can promote the transmission of weak external signal in small-world neuronal networks. In addition, the network topology plays an important role in the vibrational resonance in spike-timing-dependent plasticity-induced neural systems, where the system response to the subthreshold signal is maximized by an optimal network structure. Furthermore, it is demonstrated that the introduction of inhibitory synapses can considerably weaken the phenomenon of vibrational resonance in the hybrid small-world neuronal networks with spike-timing-dependent plasticity. (C) 2015 Elsevier B.V. All rights reserved.
C1 [Yu, Haitao; Guo, Xinmeng; Wang, Jiang; Deng, Bin; Wei, Xile] Tianjin Univ, Sch Elect Engn & Automat, Tianjin 300072, Peoples R China.
RP Wang, J (corresponding author), Tianjin Univ, Sch Elect Engn & Automat, Tianjin 300072, Peoples R China.
EM eejwang@tju.edu.cn
CR Bayati M, 2012, PHYS REV E, V86, DOI 10.1103/PhysRevE.86.011925
   BENZI R, 1981, J PHYS A-MATH GEN, V14, pL453, DOI 10.1088/0305-4470/14/11/006
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Bordet M, 2012, ELECTRON LETT, V48, P903, DOI 10.1049/el.2012.1343
   Chizhevsky VN, 2008, PHYS REV E, V77, DOI 10.1103/PhysRevE.77.051126
   Deng B, 2010, CHAOS, V20, DOI 10.1063/1.3324700
   Feldman DE, 2005, SCIENCE, V310, P810, DOI 10.1126/science.1115807
   Fino E, 2009, NEUROSCIENCE, V160, P744, DOI 10.1016/j.neuroscience.2009.03.015
   Gammaitoni L, 1998, REV MOD PHYS, V70, P223, DOI 10.1103/RevModPhys.70.223
   Gitterman M, 2001, J PHYS A-MATH GEN, V34, pL355, DOI 10.1088/0305-4470/34/24/101
   Haenggi P., 1996, Nonlinear physics of complex systems. Current status and future trends, P294
   Hu DL, 2014, COMPUT BIOL MED, V45, P80, DOI 10.1016/j.compbiomed.2013.11.022
   Karmarkar UR, 2002, BIOL CYBERN, V87, P373, DOI 10.1007/s00422-002-0351-0
   Kleberg FI, 2014, FRONT COMPUT NEUROSC, V8, DOI 10.3389/fncom.2014.00053
   Landa PS, 2000, J PHYS A-MATH GEN, V33, pL433, DOI 10.1088/0305-4470/33/45/103
   Lee S, 2009, PLOS COMPUT BIOL, V5, DOI 10.1371/journal.pcbi.1000602
   Li XM, 2009, CHAOS, V19, DOI 10.1063/1.3076394
   Lindner B, 2004, PHYS REP, V392, P321, DOI 10.1016/j.physrep.2003.10.015
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   Nevian T, 2006, J NEUROSCI, V26, P11001, DOI 10.1523/JNEUROSCI.1749-06.2006
   Newman MEJ, 1999, PHYS LETT A, V263, P341, DOI 10.1016/S0375-9601(99)00757-4
   Nishiyama M, 2000, NATURE, V408, P584, DOI 10.1038/35046067
   Ozer M, 2009, PHYS LETT A, V373, P964, DOI 10.1016/j.physleta.2009.01.034
   Perc M, 2005, NEW J PHYS, V7, DOI 10.1088/1367-2630/7/1/252
   Perc M, 2005, PHYS REV E, V71, DOI 10.1103/PhysRevE.71.026229
   Perc M, 2007, PHYS REV E, V76, DOI 10.1103/PhysRevE.76.066203
   Perc M, 2007, CHAOS SOLITON FRACT, V31, P280, DOI 10.1016/j.chaos.2005.10.018
   Rajasekar S, 2011, CHAOS, V21, DOI 10.1063/1.3610213
   Ren QS, 2012, PHYS REV E, V86, DOI 10.1103/PhysRevE.86.056103
   Ren QS, 2012, PHYS REV E, V85, DOI 10.1103/PhysRevE.85.022901
   Shin CW, 2006, PHYS REV E, V74, DOI 10.1103/PhysRevE.74.045101
   Song S, 2001, NEURON, V32, P339, DOI 10.1016/S0896-6273(01)00451-2
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Sun XJ, 2008, CHAOS, V18, DOI 10.1063/1.2900402
   Tzounopoulos T, 2007, NEURON, V54, P291, DOI 10.1016/j.neuron.2007.03.026
   Ullner E, 2003, PHYS LETT A, V312, P348, DOI 10.1016/S0375-9601(03)00681-9
   Uzuntarla M, 2015, COMMUN NONLINEAR SCI, V22, P367, DOI 10.1016/j.cnsns.2014.08.040
   van Rossum MCW, 2000, J NEUROSCI, V20, P8812
   Wang CJ, 2014, INT J MOD PHYS B, V28, DOI 10.1142/S0217979214501033
   Wang QY, 2010, INT J MOD PHYS B, V24, P1201, DOI 10.1142/S0217979210055317
   Wang QY, 2008, PHYS LETT A, V372, P5681, DOI 10.1016/j.physleta.2008.07.005
   Wang QY, 2012, CHAOS, V22, DOI 10.1063/1.4767719
   Wang QY, 2012, COMMUN NONLINEAR SCI, V17, P3979, DOI 10.1016/j.cnsns.2012.02.019
   Wang QY, 2009, CHAOS, V19, DOI 10.1063/1.3133126
   WIESENFELD K, 1995, NATURE, V373, P33, DOI 10.1038/373033a0
   Wu D, 2011, PHYS REV E, V84, DOI 10.1103/PhysRevE.84.021102
   Xue M, 2013, EUR PHYS J B, V86, DOI 10.1140/epjb/e2013-30782-3
   Yang JH, 2010, CHAOS, V20, DOI 10.1063/1.3481343
   Yang LJ, 2012, PHYS REV E, V86, DOI 10.1103/PhysRevE.86.016209
   Yu HT, 2015, PHYSICA A, V419, P307, DOI 10.1016/j.physa.2014.10.031
   Yu HT, 2012, CHAOS, V22, DOI 10.1063/1.4729462
   Zhigulin VP, 2003, PHYS REV E, V67, DOI 10.1103/PhysRevE.67.021901
NR 52
TC 12
Z9 12
U1 2
U2 41
PD OCT 15
PY 2015
VL 436
BP 170
EP 179
DI 10.1016/j.physa.2015.05.037
UT WOS:000357704500015
DA 2023-11-16
ER

PT J
AU Delbruck, T
   van Schaik, A
   Hasler, J
AF Delbruck, Tobi
   van Schaik, Andre
   Hasler, Jennifer
TI Research topic: neuromorphic engineering systems and applications. A
   snapshot of neuromorphic systems engineering
SO FRONTIERS IN NEUROSCIENCE
DT Editorial Material
DE neuromorphic engineering; neural networks; event-based; spiking neural
   networks; dynamic vision sensor; floating gate; neural simulation;
   synaptic plasticity
C1 [Delbruck, Tobi] Univ Zurich, Inst Neuroinformat, Zurich, Switzerland.
   [Delbruck, Tobi] ETH, Zurich, Switzerland.
   [van Schaik, Andre] Univ Western Sydney, MARCS Inst, Sydney, NSW, Australia.
   [Hasler, Jennifer] Georgia Inst Technol, Sch Elect & Comp Engn, Atlanta, GA 30332 USA.
RP van Schaik, A (corresponding author), Univ Western Sydney, MARCS Inst, Sydney, NSW, Australia.
EM a.vanschaik@uws.edu.au
CR Ambroise M, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00215
   Brandli C, 2014, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00275
   Camuñas-Mesa LA, 2014, FRONT NEUROSCI-SWITZ, V8, DOI 10.3389/fnins.2014.00048
   Carlson KD, 2014, FRONT NEUROSCI-SWITZ, V8, DOI 10.3389/fnins.2014.00010
   Clady X, 2014, FRONT NEUROSCI-SWITZ, V8, DOI 10.3389/fnins.2014.00009
   Coath M, 2014, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00278
   Delbruck T, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00223
   Gupta P, 2014, FRONT NEUROSCI-SWITZ, V8, DOI 10.3389/fnins.2014.00054
   Marr B, 2014, FRONT NEUROSCI-SWITZ, V8, DOI 10.3389/fnins.2014.00086
   Neftci E, 2014, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00272
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   Rea F, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00234
   Sandamirskaya Y, 2014, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00276
   Wang RCM, 2014, FRONT NEUROSCI-SWITZ, V8, DOI 10.3389/fnins.2014.00051
NR 14
TC 3
Z9 3
U1 1
U2 26
PD DEC 19
PY 2014
VL 8
AR 424
DI 10.3389/fnins.2014.00424
UT WOS:000346562100001
DA 2023-11-16
ER

PT J
AU Siddique, N
   Widrow, B
   Maguire, L
AF Siddique, Nazmul
   Widrow, Bernard
   Maguire, Liam
TI Special Issue: Spiking Neural Networks INTRODUCTION
SO INTERNATIONAL JOURNAL OF NEURAL SYSTEMS
DT Editorial Material
CR Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Goudar V, 2014, NAT NEUROSCI, V17, P487, DOI 10.1038/nn.3679
   Halassa MM, 2010, ANNU REV PHYSIOL, V72, P335, DOI 10.1146/annurev-physiol-021909-135843
   Henneberger C, 2010, NATURE, V463, P232, DOI 10.1038/nature08673
   Kumar A, 2010, NAT REV NEUROSCI, V11, P615, DOI 10.1038/nrn2886
   Owen SF, 2013, NATURE, V500, P458, DOI 10.1038/nature12330
NR 6
TC 0
Z9 0
U1 0
U2 13
PD AUG
PY 2014
VL 24
IS 5
SI SI
AR 1403002
DI 10.1142/S0129065714030026
UT WOS:000336922900001
DA 2023-11-16
ER

PT C
AU Liu, Q
   Furber, S
AF Liu, Qian
   Furber, Steve
BE Hirose, A
   Ozawa, S
   Doya, K
   Ikeda, K
   Lee, M
   Liu, D
TI Noisy Softplus: A Biology Inspired Activation Function
SO NEURAL INFORMATION PROCESSING, ICONIP 2016, PT IV
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 23rd International Conference on Neural Information Processing (ICONIP)
CY OCT 16-21, 2016
CL Kyoto, JAPAN
DE Noisy softplus; Biologically-inspired; Spiking neural network;
   Activation function; LIF neurons
AB The Spiking Neural Network (SNN) has not achieved the recognition/classification performance of its non-spiking competitor, the Artificial Neural Network(ANN), particularly when used in deep neural networks. The mapping of a well-trained ANN to an SNN is a hot topic in this field, especially using spiking neurons with biological characteristics. This paper proposes a new biologically-inspired activation function, Noisy Softplus, which is well-matched to the response function of LIF (Leaky Integrate-and-Fire) neurons. A convolutional network (ConvNet) was trained on the MNIST database with Noisy Softplus units and converted to an SNN while maintaining a close classification accuracy. This result demonstrates the equivalent recognition capability of the more biologically-realistic SNNs and bring biological features to the activation units in ANNs.
C1 [Liu, Qian; Furber, Steve] Univ Manchester, Adv Processor Technol Grp, Manchester M13 9PL, Lancs, England.
RP Liu, Q (corresponding author), Univ Manchester, Adv Processor Technol Grp, Manchester M13 9PL, Lancs, England.
EM qian.liu-3@manchester.ac.uk; steve.furber@manchester.ac.uk
CR Buesing L, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1002211
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Davison Andrew P, 2008, Front Neuroinform, V2, P11, DOI 10.3389/neuro.11.011.2008
   Diehl P.U., 2016, CONVERSION ARTIFICIA
   Diehl PU, 2015, IEEE IJCNN
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   Glorot X., 2011, PMLR, Vvol 15, P315
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   Hunsberger E., 2015, SPIKING DEEP NETWORK
   Jug F, 2012, SWISS SOC NEUROSCI, V1
   Liu Q., 2016, FRONT NEURO IN PRESS
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Neftci E, 2014, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00272
   OConnor P, 2016, DEEP SPIKING NETWORK
   Stromatias E., 2015, INT JOINT C NEUR NET
NR 15
TC 21
Z9 24
U1 0
U2 12
PY 2016
VL 9950
BP 405
EP 412
DI 10.1007/978-3-319-46681-1_49
UT WOS:000389804700049
DA 2023-11-16
ER

PT J
AU Wang, H
   Li, YF
   Zhang, Y
AF Wang, Huan
   Li, Yan-Fu
   Zhang, Ying
TI Bioinspired spiking spatiotemporal attention framework for lithium-ion
   batteries state-of-health estimation
SO RENEWABLE & SUSTAINABLE ENERGY REVIEWS
DT Article
DE Lithium-ion batteries; Prognostic and health management; Capacity
   prediction; Spiking neural network
ID NETWORK
AB State-of-health (SOH) estimation of batteries is crucial for ensuring the safety of energy storage systems. Prediction models based on external information (current, voltage, etc.) and artificial neural networks (ANN) are effective solutions. However, external information easily interferes, and the ANN-based model has data dependence, high energy consumption, and insufficient cognitive ability. This motivates us to utilize precise battery physical and chemical degradation information and brain-inspired spiking neural networks (SNNs) for accurate SOH estimation. Therefore, this study proposes a bioinspired spiking spatiotemporal attention neural network (SSA-Net) framework for battery health state monitoring by utilizing full-life-cycle electrochemical impedance spectroscopy (EIS). SSA-Net perfectly models brain neurons' information transmission mechanism and neuron dynamics, thereby endowing it with efficient spatiotemporal feature processing capabilities and low power consumption. Based on the designed spiking residual architecture, SSA-Net constructs a deep spiking information encoding framework achieving high gradient transfer efficiency. More importantly, this study proposes a novel SNN-based spiking spatiotemporal attention module, which realizes the enhancement of useful spiking features and discards worthless information through an adaptive spiking feature selection mechanism. Experimental results show that SSA-Net effectively extracts electrochemical features associated with battery degradation, facilitating precise modeling of the nonlinear relationship between EIS data and SOH and achieving competitive performance.
C1 [Wang, Huan; Li, Yan-Fu; Zhang, Ying] Tsinghua Univ, Dept Ind Engn, Beijing 100084, Peoples R China.
RP Li, YF (corresponding author), Tsinghua Univ, Dept Ind Engn, Beijing 100084, Peoples R China.
EM liyanfu@tsinghua.edu.cn
CR Bian C, 2020, ENERGY, V191, DOI 10.1016/j.energy.2019.116538
   Cheng G, 2021, ENERGY, V232, DOI 10.1016/j.energy.2021.121022
   Christensen PA, 2021, RENEW SUST ENERG REV, V148, DOI 10.1016/j.rser.2021.111240
   Ding P, 2021, RENEW SUST ENERG REV, V148, DOI 10.1016/j.rser.2021.111287
   Fang W., 2021, ADV NEURAL INFORM PR, V34, P21056
   Fang WAC, SPIKINGJELLY
   Fang W, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2641, DOI 10.1109/ICCV48922.2021.00266
   Feng XN, 2019, IEEE T VEH TECHNOL, V68, P8583, DOI 10.1109/TVT.2019.2927120
   Gerstner W, 2002, SPIKING NEURON MODEL, DOI [10.1017/CBO9780511815706, DOI 10.1017/CBO9780511815706]
   Han XB, 2015, J POWER SOURCES, V278, P802, DOI 10.1016/j.jpowsour.2014.12.101
   Horowitz M, 2014, ISSCC DIG TECH PAP I, V57, P10, DOI 10.1109/ISSCC.2014.6757323
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/TPAMI.2019.2913372, 10.1109/CVPR.2018.00745]
   Jones PK, 2022, NAT COMMUN, V13, DOI 10.1038/s41467-022-32422-w
   Khumprom P, 2019, ENERGIES, V12, DOI 10.3390/en12040660
   Li PH, 2022, RENEW SUST ENERG REV, V156, DOI 10.1016/j.rser.2021.111843
   Li PH, 2020, J POWER SOURCES, V459, DOI 10.1016/j.jpowsour.2020.228069
   Li Y, 2019, RENEW SUST ENERG REV, V113, DOI 10.1016/j.rser.2019.109254
   Lu JH, 2023, NAT COMMUN, V14, DOI 10.1038/s41467-023-38458-w
   Lu JH, 2022, ENERGY STORAGE MATER, V50, P139, DOI 10.1016/j.ensm.2022.05.007
   Ma J, 2015, SCI CHINA TECHNOL SC, V58, P2038, DOI 10.1007/s11431-015-5961-6
   Meng HX, 2023, RELIAB ENG SYST SAFE, V236, DOI 10.1016/j.ress.2023.109288
   Meng HX, 2019, RENEW SUST ENERG REV, V116, DOI 10.1016/j.rser.2019.109405
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Nejad S, 2016, J POWER SOURCES, V316, P183, DOI 10.1016/j.jpowsour.2016.03.042
   Pan HH, 2018, ENERGY, V160, P466, DOI 10.1016/j.energy.2018.06.220
   Pei J, 2019, NATURE, V572, P106, DOI 10.1038/s41586-019-1424-8
   Rauf H, 2022, RENEW SUST ENERG REV, V156, DOI 10.1016/j.rser.2021.111903
   Shen S, 2019, J ENERGY STORAGE, V25, DOI 10.1016/j.est.2019.100817
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Tian JP, 2022, ENERGY STORAGE MATER, V51, P372, DOI 10.1016/j.ensm.2022.06.053
   Tian JP, 2022, ENERGY STORAGE MATER, V50, P718, DOI 10.1016/j.ensm.2022.06.007
   Tian JP, 2020, IEEE T POWER ELECTR, V35, P10363, DOI 10.1109/TPEL.2020.2978493
   Ungurean L, 2020, INT J ENERG RES, V44, P6767, DOI 10.1002/er.5413
   Wang H, ARXIV
   Wang H, 2023, RELIAB ENG SYST SAFE, V233, DOI 10.1016/j.ress.2023.109102
   Wang H, 2022, KNOWL-BASED SYST, V239, DOI 10.1016/j.knosys.2021.107978
   Wang H, 2022, IEEE T NEUR NET LEAR, V33, P4757, DOI 10.1109/TNNLS.2021.3060494
   Yao JC, 2023, ENERGY, V271, DOI 10.1016/j.energy.2023.127033
   Zhang CL, 2022, FRONT ENERGY RES, V10, DOI 10.3389/fenrg.2022.1013800
   Zhang CL, 2022, IEEE T VEH TECHNOL, V71, P2601, DOI 10.1109/TVT.2021.3138959
   Zhang Y, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-16779-4
   Zhang Y, 2022, RENEW SUST ENERG REV, V161, DOI 10.1016/j.rser.2022.112282
   Zhao SS, 2022, J ENERGY STORAGE, V52, DOI 10.1016/j.est.2022.104901
   Zhou ZK, 2020, J CLEAN PROD, V267, DOI 10.1016/j.jclepro.2020.121882
   Zraibi B, 2021, IEEE T VEH TECHNOL, V70, P4252, DOI 10.1109/TVT.2021.3071622
NR 45
TC 0
Z9 0
U1 2
U2 2
PD DEC
PY 2023
VL 188
AR 113728
DI 10.1016/j.rser.2023.113728
UT WOS:001086610500001
DA 2023-11-16
ER

PT C
AU Saleh, AY
   Hamed, HNB
   Shamsuddin, SM
   Ibrahim, AO
AF Saleh, Abdulrazak Yahya
   Hamed, Haza Nuzly Bin Abdull
   Shamsuddin, Siti Mariyam
   Ibrahim, Ashraf Osman
BE Saeed, F
   Gazem, N
   Patnaik, S
   Balaid, ASS
   Mohammed, F
TI A New Hybrid K-Means Evolving Spiking Neural Network Model Based on
   Differential Evolution
SO RECENT TRENDS IN INFORMATION AND COMMUNICATION TECHNOLOGY
SE Lecture Notes on Data Engineering and Communications Technologies
DT Proceedings Paper
CT 2nd International Conference of Reliable Information and Communication
   Technology (IRICT)
CY APR 23-24, 2017
CL Johor, MALAYSIA
DE Clustering; K-means; Differential evolution; Spiking neural network;
   Evolving spiking neural networks; K-DESNN
ID ALGORITHM; NEURONS; OPTIMIZATION
AB Clustering is one of the essential unsupervised learning techniques in Data Mining. In this paper, a new hybrid (K-DESNN) approach to combine differential evolution and K-means evolving spiking neural network model (K-means ESNN) for clustering problems has been proposed. The proposed model examines that ESNN improves by using K-DESNN model. This approach improves the flexibility of the ESNN algorithm in producing better solutions which is utilized to conquer the K-means disadvantages. Various UCI machine learning data sets have been utilized for evaluating the performance of this model. The results have shown that K-DESNN is much better than the original K-means ESNN in the number of pre-synaptic neurons measure and clustering accuracy performance.
C1 [Saleh, Abdulrazak Yahya] Univ Malaysia Sarawak, FSKPM Fac, Kota Samarahan 94300, Sarawak, Malaysia.
   [Hamed, Haza Nuzly Bin Abdull] UTM, Fac Comp, Soft Comp Res Grp, Skudai 81310, Johor, Malaysia.
   [Shamsuddin, Siti Mariyam] UTM, UTM Big Data Ctr, Skudai 81310, Johor, Malaysia.
   [Ibrahim, Ashraf Osman] Alzaiem Alazhari Univ, Fac Comp & Technol, Khartoum, Sudan.
   [Ibrahim, Ashraf Osman] Nile Coll, Khartoum 11111, Sudan.
RP Saleh, AY (corresponding author), Univ Malaysia Sarawak, FSKPM Fac, Kota Samarahan 94300, Sarawak, Malaysia.
EM Abdulrazakalhababi@gmail.com; haza@utm.my; sitimariyams@gmail.com;
   ashrafosman2@gmail.com
CR Abbass HA, 2001, IEEE C EVOL COMPUTAT, P207, DOI 10.1109/CEC.2001.934391
   Alsabti K., 1997, EFFICIENT K MEANS CL, V43, P5
   [Anonymous], INT J ARTIF INTELL
   [Anonymous], P 2 INT WORKSH WEB D
   Belatreche A, 2006, NEW MATH NAT COMPUT, V2, P237, DOI 10.1142/S179300570600049X
   Berkhin P, 2006, GROUPING MULTIDIMENSIONAL DATA: RECENT ADVANCES IN CLUSTERING, P25
   Bock H.-H., 2007, SELECTED CONTRIBUTIO, P161, DOI [10.1007/978-3-540-73560-1_15, DOI 10.1007/978-3-540-73560-1_15, DOI 10.1093/humrep/dew218]
   Bohte SM, 2005, INFORM PROCESS LETT, V95, P519, DOI 10.1016/j.ipl.2005.05.018
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   Brette R, 2007, J COMPUT NEUROSCI, V23, P349, DOI 10.1007/s10827-007-0038-6
   Das S, 2011, IEEE T EVOLUT COMPUT, V15, P4, DOI 10.1109/TEVC.2010.2059031
   Fayyad U, 1996, AI MAG, V17, P37
   Firouzi B., 2008, WORLD ACAD SCI ENG T, V36, P605
   Godara S., 2016, INDIAN J SCI TECHNOL, V9
   Gong WY, 2011, SOFT COMPUT, V15, P645, DOI 10.1007/s00500-010-0591-1
   Hamed HNA, 2015, LECT NOTES COMPUT SC, V9377, P382, DOI 10.1007/978-3-319-25393-0_42
   Hamed HNA, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P2653, DOI 10.1109/IJCNN.2011.6033565
   Hamed HNA, 2009, LECT NOTES COMPUT SC, V5864, P611, DOI 10.1007/978-3-642-10684-2_68
   He ZY, 2005, LECT NOTES ARTIF INT, V3801, P157
   Huang ZX, 1998, DATA MIN KNOWL DISC, V2, P283, DOI 10.1023/A:1009769707641
   Huang ZX, 1999, IEEE T FUZZY SYST, V7, P446, DOI 10.1109/91.784206
   Ilonen J, 2003, NEURAL PROCESS LETT, V17, P93, DOI 10.1023/A:1022995128597
   Jain AK, 2010, PATTERN RECOGN LETT, V31, P651, DOI 10.1016/j.patrec.2009.09.011
   Kasabov Nikola, 2012, Advances in Computational Intelligence. IEEE World Congress on Computational Intelligence (WCCI 2012). Plenary/Invited Lectures, P234, DOI 10.1007/978-3-642-30687-7_12
   Kasabov N, 2006, 2006 INTERNATIONAL SYMPOSIUM ON EVOLVING FUZZY SYSTEMS, PROCEEDINGS, P8, DOI 10.1109/ISEFS.2006.251185
   Kasabov N, 2014, NEUROCOMPUTING, V134, P269, DOI 10.1016/j.neucom.2013.09.049
   Kasabov N, 2013, NEURAL NETWORKS, V41, P188, DOI 10.1016/j.neunet.2012.11.014
   Kasabov NK, 2014, NEURAL NETWORKS, V52, P62, DOI 10.1016/j.neunet.2014.01.006
   Kotsiantis S., 2004, WSEAS T INF SCI APPL, V1, P73
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   MacQueen J, 1967, 5 BERK S MATH STAT P, DOI DOI 10.1007/S11665-016-2173-6
   Mandloi M., 2014, SURVEY CLUSTERING AL
   Michlovsky Z., 2009, STRING PATTERN RECOG, P611
   Nazeer KAA, 2010, ELECT ENG COMPUTING, P433
   Noman N, 2008, IEEE T EVOLUT COMPUT, V12, P107, DOI 10.1109/TEVC.2007.895272
   Patel VR, 2011, COMM COM INF SC, V250, P307
   Pham DT, 2005, P I MECH ENG C-J MEC, V219, P103, DOI 10.1243/095440605X8298
   Saleh A.Y., 2014, INT J ADV SOFT COMPU, V6
   Saleh A.Y., 2014, INT C REC TRENDS INF, P13
   Schliebs S, 2013, EVOL SYST-GER, V4, P87, DOI 10.1007/s12530-013-9074-9
   Schliebs S, 2009, LECT NOTES COMPUT SC, V5506, P1229, DOI 10.1007/978-3-642-02490-0_149
   Schliebs S, 2009, NEURAL NETWORKS, V22, P623, DOI 10.1016/j.neunet.2009.06.038
   Singh A., 2013, INT J COMPUT APPL, V67, P13, DOI DOI 10.5120/11430-6785
   Soltic S, 2008, IEEE IJCNN, P2091, DOI 10.1109/IJCNN.2008.4634085
   Storn R, 1997, J GLOBAL OPTIM, V11, P341, DOI 10.1023/A:1008202821328
   Sun Y, 2002, PATTERN RECOGN LETT, V23, P875, DOI 10.1016/S0167-8655(01)00163-5
   Thakare Y, 2015, INT J COMPUT APPL, V110
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   Thorpe S., 1997, CAN HUMAN VISUAL SYS
   Wu J., 2012, ADV K MEANS CLUSTERI, DOI 10.1007/978-3-642-29807-3
   Wu XD, 2008, KNOWL INF SYST, V14, P1, DOI 10.1007/s10115-007-0114-2
   Wysoski SG, 2006, LECT NOTES COMPUT SC, V4179, P1133
   Wysoski SG, 2006, LECT NOTES COMPUT SC, V4131, P61
   Zaki M. J., 2014, DATA MINING ANAL FUN, DOI DOI 10.1017/CBO9780511810114
NR 55
TC 0
Z9 0
U1 0
U2 3
PY 2018
VL 5
BP 571
EP 583
DI 10.1007/978-3-319-59427-9_60
UT WOS:000432202300060
DA 2023-11-16
ER

PT C
AU Nguyen, DA
   Tran, XT
   Dang, KN
   Iacopi, F
AF Duy-Anh Nguyen
   Xuan-Tu Tran
   Dang, Khanh N.
   Iacopi, Francesca
BE Tran, XT
   Bui, DH
TI A lightweight Max-Pooling method and architecture for Deep Spiking
   Convolutional Neural Networks
SO APCCAS 2020: PROCEEDINGS OF THE 2020 IEEE ASIA PACIFIC CONFERENCE ON
   CIRCUITS AND SYSTEMS (APCCAS 2020)
DT Proceedings Paper
CT 16th IEEE Asia Pacific Conference on Circuits and Systems (IEEE APCCAS)
   / IEEE Asia Pacific Conference on Postgraduate Research in
   Microelectronics and Electronics (PrimeAsia)
CY DEC 08-10, 2020
CL Halong, VIETNAM
DE Deep Convolutional Spiking Neural Networks; ANN-to-SNN conversion;
   Spiking Max Pooling
AB The training of Deep Spiking Neural Networks (DSNNs) is facing many challenges due to the non-differentiable nature of spikes. The conversion of a traditional Deep Neural Networks (DNNs) to its DSNNs counterpart is currently one of the prominent solutions, as it leverages many state-of-the-art pretrained models and training techniques. However, the conversion of max-pooling layer is a non-trivia task. The state-of-the-art conversion methods either replace the max-pooling layer with other pooling mechanisms or use a max-pooling method based on the cumulative number of output spikes. This incurs both memory storage overhead and increases computational complexity, as one inference in DSNNs requires many timesteps, and the number of output spikes after each layer needs to be accumulated. In this paper1, we propose a novel max-pooling mechanism that is not based on the number of output spikes but is based on the membrane potential of the spiking neurons. Simulation results show that our approach still preserves classification accuracies on MNIST and CIFAR10 dataset. Hardware implementation results show that our proposed hardware block is lightweight with an area cost of 15.3kGEs, at a maximum frequency of 300 MHz.
C1 [Duy-Anh Nguyen; Xuan-Tu Tran; Dang, Khanh N.] Vietnam Natl Univ, SISLAB, Univ Engn & Technol, Hanoi, Vietnam.
   [Xuan-Tu Tran] Univ Technol Sydney, Ultimo, Australia.
   [Duy-Anh Nguyen] UTS VNU Joint Technol & Innovat Res Ctr JTIRC, Hanoi, Vietnam.
RP Tran, XT (corresponding author), Vietnam Natl Univ, SISLAB, Univ Engn & Technol, Hanoi, Vietnam.; Tran, XT (corresponding author), Univ Technol Sydney, Ultimo, Australia.
EM tutx@vnu.edu.vn
CR Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Diehl PU, 2015, IEEE IJCNN
   Guo SH, 2020, IEEE EMBED SYST LETT, V12, P21, DOI 10.1109/LES.2019.2919244
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
NR 5
TC 2
Z9 2
U1 0
U2 0
PY 2020
BP 209
EP 212
DI 10.1109/apccas50809.2020.9301703
UT WOS:000682771300055
DA 2023-11-16
ER

PT C
AU Alnajjar, F
   Murase, K
AF Alnajjar, F
   Murase, K
BE Murase, K
   Sekiyama, K
   Kubota, N
   Naniwa, T
   Sitte, J
TI Self-organization of spiking neural network generating autonomous
   behavior in a miniature mobile robot
SO PROCEEDINGS OF THE 3RD INTERNATIONAL SYMPOSIUM ON AUTONOMOUS MINIROBOTS
   FOR RESEARCH AND EDUTAINMENT (AMIRE 2005)
DT Proceedings Paper
CT 3rd International Symposium on Autonomous Minirobots for Research and
   Edutainment
CY SEP 20-22, 2005
CL Fukui, JAPAN
AB Purpose of this study is to develop self-organization algorithm of spiking neural network applicable to autonomous robots. We first formulated a spiking neural network model whose inputs and outputs were analog. We then implemented it into a miniature mobile robot Khepera. In order to see whether or not a solution(s) for the given task exists with the spiking neural network, the robot was evolved with the genetic algorithm (GA) in an environment. The robot acquired the obstacle-avoidance and navigation task successfully, exhibiting the presence of the solution. Then, a self-organization algorithm based on the use-dependent synaptic potentiation and depotentiation was formulated and implemented into the robot. In the environment, the robot gradually organized the network and the obstacle avoidance behavior was formed. The time needed for the training was much less than with genetic evolution, approximately one fifth (1/5).
C1 Univ Fukui, Dept Human & Artificial Intelligence Syst, Fukui 9108507, Japan.
RP Alnajjar, F (corresponding author), Univ Fukui, Dept Human & Artificial Intelligence Syst, Fukui 9108507, Japan.
EM fady@synapse.his.fukui-u.ac.jp
CR Floreano D., 2001, LNCS, P38
   Gerstner W., 2002, SPIKING NEURON MODEL
   Islam M, 2002, IEICE T INF SYST, VE85D, P1118
   MAASS W, 1996, AUSTR C NEUR NETW
   Ruf B, 1998, IEEE T NEURAL NETWOR, V9, P575, DOI 10.1109/72.668899
   Sala D. M., 1998, Australian Journal of Intelligent Information Processing Systems, V5, P161
NR 6
TC 1
Z9 1
U1 0
U2 0
PY 2006
BP 255
EP +
DI 10.1007/3-540-29344-2_38
UT WOS:000233560000038
DA 2023-11-16
ER

PT J
AU Bing, ZS
   Baumann, I
   Jiang, ZY
   Huang, K
   Cai, CX
   Knoll, A
AF Bing, Zhenshan
   Baumann, Ivan
   Jiang, Zhuangyi
   Huang, Kai
   Cai, Caixia
   Knoll, Alois
TI Supervised Learning in SNN via Reward-Modulated Spike-Timing-Dependent
   Plasticity for a Target Reaching Vehicle
SO FRONTIERS IN NEUROROBOTICS
DT Article
DE spiking neural network; R-STDP; supervised learning; end-to-end control;
   autonomous locomotion
ID NEURAL-NETWORK; MOBILE ROBOT; NAVIGATION; MODEL; STDP
AB Spiking neural networks (SNNs) offer many advantages over traditional artificial neural networks (ANNs) such as biological plausibility, fast information processing, and energy efficiency. Although SNNs have been used to solve a variety of control tasks using the Spike-Timing-Dependent Plasticity (STDP) learning rule, existing solutions usually involve hard-coded network architectures solving specific tasks rather than solving different kinds of tasks generally. This results in neglecting one of the biggest advantages of ANNs, i.e., being general-purpose and easy-to-use due to their simple network architecture, which usually consists of an input layer, one or multiple hidden layers and an output layer. This paper addresses the problem by introducing an end-to-end learning approach of spiking neural networks constructed with one hidden layer and reward-modulated Spike-Timing-Dependent Plasticity (R-STDP) synapses in an all-to-all fashion. We use the supervised reward-modulated Spike-Timing-Dependent-Plasticity learning rule to train two different SNN-based sub-controllers to replicate a desired obstacle avoiding and goal approaching behavior, provided by pre-generated datasets. Together they make up a target-reaching controller, which is used to control a simulated mobile robot to reach a target area while avoiding obstacles in its path. We demonstrate the performance and effectiveness of our trained SNNs to achieve target reaching tasks in different unknown scenarios.
C1 [Bing, Zhenshan; Baumann, Ivan; Jiang, Zhuangyi; Cai, Caixia; Knoll, Alois] Tech Univ Munich, Chair Robot Artificial Intelligence & Embedded Sy, Dept Informat, Munich, Germany.
   [Huang, Kai] Sun Yat Sen Univ, Dept Data & Comp Sci, Guangzhou, Guangdong, Peoples R China.
   [Huang, Kai] Peng Cheng Lab, Shenzhen, Peoples R China.
RP Cai, CX (corresponding author), Tech Univ Munich, Chair Robot Artificial Intelligence & Embedded Sy, Dept Informat, Munich, Germany.
EM ccxtum@gmail.com
CR Alnajjar F, 2008, IEEE IJCNN, P2207, DOI 10.1109/IJCNN.2008.4634103
   Ambrosano A., 2016, RETINA COLOR OPPONEN
   [Anonymous], 2014, SURVEY PUBLIC OPINIO, DOI DOI 10.1109/ICCVE.2014.45
   [Anonymous], 2012, ADV ARTIF NEURAL SYS, DOI DOI 10.1155/2012/713581
   [Anonymous], 2016, INT C EV BAS CONTR C
   [Anonymous], 1992, P THEOR PRAX FUZZ LO
   [Anonymous], 2000, THE BRAIN EXPLAINED
   Beyeler M, 2015, NEURAL NETWORKS, V72, P75, DOI 10.1016/j.neunet.2015.09.005
   Bicho E, 1998, IEEE IND ELEC, P1176, DOI 10.1109/IECON.1998.724266
   Bing ZS, 2018, FRONT NEUROROBOTICS, V12, DOI 10.3389/fnbot.2018.00035
   Bingsheng Zhang, 2018, 2018 IEEE International Conference on Internet of Things (iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom) and IEEE Smart Data (SmartData). Proceedings
   Blum H., 2017, ROB SCI SYST 2017 C
   Bouganis A., 2010, P 2010 INT JOINT C N, P1, DOI DOI 10.1109/IJCNN.2010.5596525
   BROOKS RA, 1986, IEEE T ROBOTIC AUTOM, V2, P14, DOI 10.1109/JRA.1986.1087032
   Carrillo RR, 2008, BIOSYSTEMS, V94, P18, DOI 10.1016/j.biosystems.2008.05.008
   Cassidy AS, 2014, INT CONF HIGH PERFOR, P27, DOI 10.1109/SC.2014.8
   Clawson TS, 2016, IEEE DECIS CONTR P, P3381, DOI 10.1109/CDC.2016.7798778
   Cyr A, 2012, ADAPT BEHAV, V20, P257, DOI 10.1177/1059712312442231
   DeSouza GN, 2002, IEEE T PATTERN ANAL, V24, P237, DOI 10.1109/34.982903
   Echeveste R, 2015, NEURAL COMPUT, V27, P672, DOI 10.1162/NECO_a_00707
   Esser S. K., 2015, ADV NEURAL INFORM PR, P1117
   Foderaro G, 2010, IEEE DECIS CONTR P, P911, DOI 10.1109/CDC.2010.5717260
   Frémaux N, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003024
   Friudenberg P, 2018, IEEE ACCESS, V6, P16948, DOI 10.1109/ACCESS.2018.2802468
   Gerstner W., 2002, SPIKING NEURON MODEL
   Helgadóttir LI, 2013, I IEEE EMBS C NEUR E, P891, DOI 10.1109/NER.2013.6696078
   Huang WH, 2006, ROBOT AUTON SYST, V54, P288, DOI 10.1016/j.robot.2005.11.004
   Indiveri GC, 1999, IEEE T CIRCUITS-II, V46, P1337, DOI 10.1109/82.803473
   Izhikevich EM, 2007, CEREB CORTEX, V17, P2443, DOI 10.1093/cercor/bhl152
   Kaiser J, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON SIMULATION, MODELING, AND PROGRAMMING FOR AUTONOMOUS ROBOTS (SIMPAR), P127, DOI 10.1109/SIMPAR.2016.7862386
   Kruse T, 2013, ROBOT AUTON SYST, V61, P1726, DOI 10.1016/j.robot.2013.05.007
   Lewis M. A., 2000, Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065), P494, DOI 10.1109/ROBOT.2000.844103
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Mazumder P, 2016, INTEGRATION, V54, P109, DOI 10.1016/j.vlsi.2016.01.002
   Milde MB, 2017, FRONT NEUROROBOTICS, V11, DOI 10.3389/fnbot.2017.00028
   Neftci EO, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00324
   Nichols E, 2013, IEEE T CYBERNETICS, V43, P115, DOI 10.1109/TSMCB.2012.2200674
   Ponulak F, 2011, ACTA NEUROBIOL EXP, V71, P409
   Rohmer E, 2013, IEEE INT C INT ROBOT, P1321, DOI 10.1109/IROS.2013.6696520
   Rothman JS, 2014, PROG MOL BIOL TRANSL, V123, P305, DOI 10.1016/B978-0-12-397897-4.00004-8
   Ru D, 2014, 2014 IEEE 14TH INTERNATIONAL CONFERENCE ON NANOTECHNOLOGY (IEEE-NANO), P873, DOI 10.1109/NANO.2014.6968000
   Shim MS, 2017, IEEE IJCNN, P3098, DOI 10.1109/IJCNN.2017.7966242
   Spuler M., 2015, P INT JOINT C NEURAL, V2015, DOI [10.1109/IJCNN.2015.7280521, DOI 10.1109/IJCNN.2015.7280521]
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   Vasilaki E, 2009, PLOS COMPUT BIOL, V5, DOI 10.1371/journal.pcbi.1000586
   Wang XQ, 2008, NEUROCOMPUTING, V71, P655, DOI 10.1016/j.neucom.2007.08.025
   Wang XQ, 2014, NEUROCOMPUTING, V134, P230, DOI 10.1016/j.neucom.2013.07.055
   Wang XQ, 2009, 2009 INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND COMPUTATIONAL INTELLIGENCE, VOL I, PROCEEDINGS, P194, DOI 10.1109/AICI.2009.448
   Wysoski SG, 2010, NEURAL NETWORKS, V23, P819, DOI 10.1016/j.neunet.2010.04.009
   Zhang X, 2013, IEEE DECIS CONTR P, P6798, DOI 10.1109/CDC.2013.6760966
   2017, IEEE IJCNN, P2243, DOI DOI 10.1109/IJCNN.2017.7966127
NR 51
TC 20
Z9 20
U1 2
U2 13
PD MAY 3
PY 2019
VL 13
AR 18
DI 10.3389/fnbot.2019.00018
UT WOS:000466985500001
DA 2023-11-16
ER

PT J
AU Yang, B
   Qin, L
   Peng, H
   Guo, CG
   Luo, XH
   Wang, J
AF Yang, Bo
   Qin, Lang
   Peng, Hong
   Guo, Chenggang
   Luo, Xiaohui
   Wang, Jun
TI SDDC-Net: A U-shaped deep spiking neural P convolutional network for
   retinal vessel segmentation
SO DIGITAL SIGNAL PROCESSING
DT Article
DE Deep learning; Retinal vessels; Spiking neural P system; Convolutional
   neural network; Dilated convolution
ID BLOOD-VESSELS; MATCHED-FILTER; MODEL; SYSTEMS
AB As an essential step in the early diagnosis of retinopathy, the blood vessels morphological attributes assist specialists to obtain pathological information efficiently. Most existing deep learning methods are based on U-shaped convolutional neural networks for the segmentation of blood vessels and have made substantial progress. However, the variance between vessel images remains challenging for segmentation algorithms, as demonstrated by poor cross-validation performance between different datasets. In this paper, a novel U-shaped deep convolutional network is proposed for retinal vessel segmentation, namely spiking neural P-type Dual-channel dilated convolutional network (SDDC-Net). We redesign the classical U-shaped convolution network based on the spiking neural P system computational mechanism for the first time. Distinct from the conventional convolutional neural network, SDDC-Net integrates the spiking neural P system convolutional neurons into the classic encoder-decoder architecture. We employ dilated convolution into an encoder, which improves both capabilities of perceiving more contexts and perceptual sensitivity of thin blood. We evaluate this model on three public datasets (DRIVE, STARE, CHASE_DB1), which indicates the more sensitive detection of thin vessels compared to most existing methods, showing higher sensitivity and F1 metrics. Compared to the baseline U-Net, our sensitivity, accuracy, and F1 score metrics on DRIVE dataset surpass by 10.66%, 1.73%, and 1.47% respectively. We also evaluate the effectiveness of spiking neural P system and dilated convolution in the ablation experiments, which demonstrates that accuracy increases with few drops in specificity. The cross-validation experiments show that our model has not only effective segmentation ability but also excellent generalization ability.(c) 2023 Elsevier Inc. All rights reserved.
C1 [Yang, Bo; Qin, Lang; Peng, Hong; Guo, Chenggang; Luo, Xiaohui] Xihua Univ, Sch Comp & Software Engn, Chengdu 610039, Peoples R China.
   [Wang, Jun] Xihua Univ, Sch Elect Engn & Elect Informat, Chengdu 610039, Peoples R China.
RP Guo, CG (corresponding author), Xihua Univ, Sch Comp & Software Engn, Chengdu 610039, Peoples R China.
EM chenggang.guo90@hotmail.com
CR Al-Diri B, 2009, IEEE T MED IMAGING, V28, P1488, DOI 10.1109/TMI.2009.2017941
   Al-Rawi M, 2007, COMPUT BIOL MED, V37, P262, DOI 10.1016/j.compbiomed.2006.03.003
   Azzopardi G, 2015, MED IMAGE ANAL, V19, P46, DOI 10.1016/j.media.2014.08.002
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Cai SJ, 2020, QUANT IMAG MED SURG, V10, P1275, DOI 10.21037/qims-19-1090
   Cai YL, 2022, INFORM SCIENCES, V587, P473, DOI 10.1016/j.ins.2021.12.058
   Campochiaro PA, 2021, PROG RETIN EYE RES, V83, DOI 10.1016/j.preteyeres.2020.100921
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Deng XY, 2022, BIOMED SIGNAL PROCES, V73, DOI 10.1016/j.bspc.2021.103467
   Fraz MM, 2012, COMPUT METH PROG BIO, V108, P600, DOI 10.1016/j.cmpb.2011.08.009
   Gegundez-Arias ME, 2021, COMPUT METH PROG BIO, V205, DOI 10.1016/j.cmpb.2021.106081
   Girshick R., 2014, P IEEE C COMP VIS PA, P580
   Orlando JI, 2017, IEEE T BIO-MED ENG, V64, P16, DOI 10.1109/TBME.2016.2535311
   Ionescu M, 2006, FUND INFORM, V71, P279
   Jin QG, 2019, KNOWL-BASED SYST, V178, P149, DOI 10.1016/j.knosys.2019.04.025
   Khan TM, 2022, BIOMED SIGNAL PROCES, V71, DOI 10.1016/j.bspc.2021.103169
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li B, 2021, SIGNAL PROCESS, V178, DOI 10.1016/j.sigpro.2020.107793
   Li B, 2021, INT J NEURAL SYST, V31, DOI 10.1142/S0129065720500501
   Li B, 2020, KNOWL-BASED SYST, V196, DOI 10.1016/j.knosys.2020.105794
   Li LZ, 2020, IEEE WINT CONF APPL, P3645, DOI 10.1109/WACV45572.2020.9093621
   Li QL, 2016, IEEE T MED IMAGING, V35, P109, DOI 10.1109/TMI.2015.2457891
   Liskowski P, 2016, IEEE T MED IMAGING, V35, P2369, DOI 10.1109/TMI.2016.2546227
   Liu Q, 2023, IEEE T NEUR NET LEAR, V34, P6227, DOI 10.1109/TNNLS.2021.3134792
   Liu Q, 2022, KNOWL-BASED SYST, V235, DOI 10.1016/j.knosys.2021.107656
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Long LF, 2022, KNOWL-BASED SYST, V253, DOI 10.1016/j.knosys.2022.109568
   Long LF, 2022, NEURAL NETWORKS, V152, P300, DOI 10.1016/j.neunet.2022.04.030
   Long LF, 2022, INT J NEURAL SYST, V32, DOI 10.1142/S0129065722500204
   Martinez-Perez ME, 2007, MED IMAGE ANAL, V11, P47, DOI 10.1016/j.media.2006.11.004
   Pan LQ, 2009, INT J COMPUT COMMUN, V4, P273, DOI 10.15837/ijccc.2009.3.2435
   Paum G, 2007, J UNIVERS COMPUT SCI, V13, P1707
   Paun Gh, 2010, OXFORD HDB MEMBRANE
   Peng H, 2021, COMPUT VIS IMAGE UND, V210, DOI 10.1016/j.cviu.2021.103228
   Peng H, 2020, NEURAL NETWORKS, V127, P110, DOI 10.1016/j.neunet.2020.04.014
   Peng H, 2020, INT J NEURAL SYST, V30, DOI 10.1142/S0129065720500082
   Peng H, 2020, KNOWL-BASED SYST, V188, DOI 10.1016/j.knosys.2019.105064
   Peng H, 2019, KNOWL-BASED SYST, V163, P875, DOI 10.1016/j.knosys.2018.10.016
   Peng H, 2019, IEEE T NEUR NET LEAR, V30, P1672, DOI 10.1109/TNNLS.2018.2872999
   Peng H, 2017, NEURAL NETWORKS, V95, P66, DOI 10.1016/j.neunet.2017.08.003
   Rangayyan RM, 2007, CAN CON EL COMP EN, P717
   Ronneberger O, 2015, P INT C MED IM COMP, P234
   Roychowdhury S, 2015, IEEE T BIO-MED ENG, V62, P1738, DOI 10.1109/TBME.2015.2403295
   Salazar-Gonzalez A, 2014, IEEE J BIOMED HEALTH, V18, P1874, DOI 10.1109/JBHI.2014.2302749
   Soares JVB, 2006, IEEE T MED IMAGING, V25, P1214, DOI 10.1109/TMI.2006.879967
   Staal J, 2004, IEEE T MED IMAGING, V23, P501, DOI 10.1109/TMI.2004.825627
   Ting DSW, 2019, PROG RETIN EYE RES, V72, DOI 10.1016/j.preteyeres.2019.04.003
   Wang DY, 2020, IEEE J BIOMED HEALTH, V24, P3384, DOI 10.1109/JBHI.2020.3002985
   Wang PQ, 2018, IEEE WINT CONF APPL, P1451, DOI 10.1109/WACV.2018.00163
   Wang X, 2022, NEUROCOMPUTING, V486, P135, DOI 10.1016/j.neucom.2021.11.017
   Wu YC, 2019, LECT NOTES COMPUT SC, V11764, P264, DOI 10.1007/978-3-030-32239-7_30
   Xian RH, 2023, INT J NEURAL SYST, V33, DOI 10.1142/S0129065722500605
   Yan ZQ, 2018, IEEE T BIO-MED ENG, V65, P1912, DOI 10.1109/TBME.2018.2828137
   You XG, 2011, PATTERN RECOGN, V44, P2314, DOI 10.1016/j.patcog.2011.01.007
   Alom MZ, 2018, Arxiv, DOI [arXiv:1802.06955, DOI 10.48550/ARXIV.1802.06955]
   Zeng ZT, 2019, IEEE ACCESS, V7, P21420, DOI 10.1109/ACCESS.2019.2896920
   Zhang B, 2010, COMPUT BIOL MED, V40, P438, DOI 10.1016/j.compbiomed.2010.02.008
   Zhang J, 2014, COMPUT MED IMAG GRAP, V38, P517, DOI 10.1016/j.compmedimag.2014.05.010
   Zhang Y, 2022, BIOMED SIGNAL PROCES, V73, DOI 10.1016/j.bspc.2021.103472
   Zhang Y, 2022, EXPERT SYST APPL, V195, DOI 10.1016/j.eswa.2022.116526
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhao SW, 2022, J MEMBRANE COMPUT, V4, P87, DOI 10.1007/s41965-022-00094-6
   Zhao YT, 2015, IEEE T MED IMAGING, V34, P1797, DOI 10.1109/TMI.2015.2409024
NR 64
TC 4
Z9 4
U1 16
U2 17
PD MAY
PY 2023
VL 136
AR 104002
DI 10.1016/j.dsp.2023.104002
EA MAR 2023
UT WOS:000957719900001
DA 2023-11-16
ER

PT C
AU Yang, SM
   Wang, J
   Deng, B
   Li, HY
   Che, YQ
AF Yang, Shuangming
   Wang, Jiang
   Deng, Bin
   Li, Huiyan
   Che, Yanqiu
GP IEEE
TI Digital Implementation of the Retinal Spiking Neural Network under Light
   Stimulation
SO 2019 9TH INTERNATIONAL IEEE/EMBS CONFERENCE ON NEURAL ENGINEERING (NER)
SE International IEEE EMBS Conference on Neural Engineering
DT Proceedings Paper
CT 9th IEEE/EMBS International Conference on Neural Engineering (NER)
CY MAR 20-23, 2019
CL San Francisco, CA
ID EFFICIENT FPGA IMPLEMENTATION; AMACRINE CELLS; GANGLION; ARCHITECTURE
AB The visual system is one of the most important pathways of obtaining information for human being and other animals. The retina is responsible for initial processing of visual information and transmitting signals to the second processing system by using the spiking activity patterns. This paper implements a retinal spiking neural network based on field-programmable gate array (FPGA), and uses different scopes of light stimulation to stimulate the digital retinal network and induce different spiking activities. The retina neural network contains 96 neurons, which uses Hodgkin-Huxley type neuron model to build neural network using three-layer feedforward neural network structure. The neural network is implemented using Cyclone IV EP4CE115 FPGA, and uses OV7620 camera to obtain external signals. The state machine control the input information of the retina system, and the firing patterns are finally displayed on oscilloscope device. Experimental results show that the proposed digital retinal network can generate the dual-peak response of the retinal ganglion cells. This work is meaningful for the design of the retina prostheses and is helpful for the investigation of the underlying mechanisms of the retinal activities.
C1 [Li, Huiyan] Tianjin Univ Technol & Educ, Sch Automat & Elect Engn, Tianjin 300222, Peoples R China.
   [Yang, Shuangming; Wang, Jiang; Deng, Bin] Tianjin Univ, Sch Elect & Informat Engn, Tianjin, Peoples R China.
   [Che, Yanqiu] Penn State Univ, Coll Med, Dept Neurosurg, Hershey, PA 17033 USA.
   [Che, Yanqiu] Penn State Univ, Ctr Neural Engn, Hershey, PA 17033 USA.
RP Li, HY (corresponding author), Tianjin Univ Technol & Educ, Sch Automat & Elect Engn, Tianjin 300222, Peoples R China.
EM yangshuangming@tju.edu.cn; jiangwang@tju.edu.cn; dengbin@tju.edu.cn;
   lhy2740@126.com; yche@pennstatehealth.psu.edu
CR Breuninger T, 2011, J NEUROSCI, V31, P6504, DOI 10.1523/JNEUROSCI.0616-11.2011
   Chan YC, 2008, VISION RES, V48, P2466, DOI 10.1016/j.visres.2008.08.010
   Choi TYW, 2005, IEEE T CIRCUITS-I, V52, P1049, DOI 10.1109/TCSI.2005.849136
   DAWSON WW, 1973, SCIENCE, V181, P747, DOI 10.1126/science.181.4101.747
   EHINGER B, 1969, Z ZELLFORSCH MIK ANA, V97, P285, DOI 10.1007/BF00344763
   He QH, 2011, EUR J NEUROSCI, V33, P36, DOI 10.1111/j.1460-9568.2010.07484.x
   Helmstaedter M, 2013, NATURE, V500, P168, DOI 10.1038/nature12346
   MacNeil MA, 1998, NEURON, V20, P971, DOI 10.1016/S0896-6273(00)80478-X
   Mustafi D, 2009, PROG RETIN EYE RES, V28, P289, DOI 10.1016/j.preteyeres.2009.05.003
   Pang JJ, 2007, VISION RES, V47, P384, DOI 10.1016/j.visres.2006.09.021
   Puller C, 2011, J COMP NEUROL, V519, P759, DOI 10.1002/cne.22546
   Roehlecke C, 2011, MOL VIS, V17, P876
   Snellman J, 2008, PROG RETIN EYE RES, V27, P450, DOI 10.1016/j.preteyeres.2008.03.003
   STERLING P, 1988, J NEUROSCI, V8, P623
   Volder J. E., 1959, ELECT COMPUTERS IRE, VEC-8, P330, DOI DOI 10.1109/TEC.1959.5222693
   Wang L, 2011, NEURAL REGEN RES, V6, P1254, DOI 10.3969/j.issn.1673-5374.2011.16.010
   WASSLE H, 1991, PHYSIOL REV, V71, P447, DOI 10.1152/physrev.1991.71.2.447
   Yang S., 2018, IEEE T CYBERNETICS
   Yang S., SCI REPORTS, V7, P40152
   Yang SM, 2018, NEUROCOMPUTING, V314, P394, DOI 10.1016/j.neucom.2018.07.006
   Yang SM, 2018, NEUROCOMPUTING, V282, P262, DOI 10.1016/j.neucom.2017.12.031
   Yang SM, 2018, PHYSICA A, V494, P484, DOI 10.1016/j.physa.2017.11.155
   Yang SM, 2017, NEURAL NETWORKS, V94, P220, DOI 10.1016/j.neunet.2017.07.012
   Yang SM, 2016, NEUROCOMPUTING, V177, P274, DOI 10.1016/j.neucom.2015.11.026
   Yang SM, 2015, NEURAL NETWORKS, V71, P62, DOI 10.1016/j.neunet.2015.07.017
   Zamanlooy B, 2014, IEEE T VLSI SYST, V22, P39, DOI 10.1109/TVLSI.2012.2232321
NR 26
TC 4
Z9 4
U1 0
U2 1
PY 2019
BP 542
EP 545
DI 10.1109/ner.2019.8716932
UT WOS:000469933200133
DA 2023-11-16
ER

PT C
AU Thiruvarudchelvan, V
   Bossomaier, T
AF Thiruvarudchelvan, Vaenthan
   Bossomaier, Terry
GP IEEE
TI Towards Realtime Stance Classification by Spiking Neural Network
SO 2012 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUN 10-15, 2012
CL Brisbane, AUSTRALIA
ID EVENT-DRIVEN SIMULATION; NEURONS
AB Spiking neural networks are a popular area of current research in both artificial intelligence and neuroscience. Unlike second generation networks like the multilayer perceptron (MLP), they simulate rather than emulate neuronal interactions. Spiking networks have been shown to be theoretically more powerful than earlier generation networks, and have repeatedly been suggested as ideal for realtime problems due to their time-basis. Because of their sparse nature, real neural networks are also extremely power-efficient, a pressing concern in computing today. This raises the possibility of applying sparse spiking networks for power-saving. To investigate these ideas, we wish to apply a spiking network to realtime data classification. As a first step, we use a feedforward network with the SpikeProp algorithm to classify offline skeleton data derived from a depth camera. Classifier networks were successfully trained, but we found SpikeProp considerably more complex to apply than backpropagation. There is considerable potential for optimization and power efficiency, and we hope to compare the performance of our system with more established learning techniques in a realtime setting.
C1 [Thiruvarudchelvan, Vaenthan; Bossomaier, Terry] Charles Sturt Univ, Ctr Res Complex Syst, Bathurst, NSW 2795, Australia.
RP Thiruvarudchelvan, V (corresponding author), Charles Sturt Univ, Ctr Res Complex Syst, Bathurst, NSW 2795, Australia.
EM vthiru@csu.edu.au; tbossomaier@csu.edu.au
CR Albers S, 2010, COMMUN ACM, V53, P86, DOI 10.1145/1735223.1735245
   Ananthanarayanan R., 2009, SC 09 P C HIGH PERF
   [Anonymous], 2011, IISU DEV GUID VER 2
   [Anonymous], ADV NEURAL INFORM PR
   [Anonymous], 2010 INT JOINT C NEU
   BIALEK W, 1991, SCIENCE, V252, P1854, DOI 10.1126/science.2063199
   Boahen K, 2005, SCI AM, V292, P56, DOI 10.1038/scientificamerican0505-56
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Carroll J., 2011, PERFORMANCE IN PRESS
   Carter J, 2010, COMPUTER, V43, P76, DOI 10.1109/MC.2010.198
   Delorme A, 2003, NETWORK-COMP NEURAL, V14, P613, DOI 10.1088/0954-898X/14/4/301
   Donofrio D, 2009, COMPUTER, V42, P62, DOI 10.1109/MC.2009.353
   Fidjeland Andreas K, 2010, 2010 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2010.5596678
   Gerstner W., 2002, SPIKING NEURON MODEL
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Jin X, 2010, COMPUT SCI ENG, V12, P91, DOI 10.1109/MCSE.2010.112
   Kim NS, 2003, COMPUTER, V36, P68, DOI 10.1109/MC.2003.1250885
   Laughlin SB, 2003, SCIENCE, V301, P1870, DOI 10.1126/science.1089662
   Laughlin SB, 1998, NAT NEUROSCI, V1, P36, DOI 10.1038/236
   Lobb CJ, 2005, Workshop on Principles of Advanced and Distributed Simulation, Proceedings, P16, DOI 10.1109/PADS.2005.18
   Maass W, 1999, INFORM COMPUT, V153, P26, DOI 10.1006/inco.1999.2806
   Mattia M, 2000, NEURAL COMPUT, V12, P2305, DOI 10.1162/089976600300014953
   McKennoch S, 2006, IEEE IJCNN, P3970
   MOORE S, 2002, THESIS U BATH
   Mouraud A., 2006, P PDCN06 PAR DISTR C
   Ros E, 2006, NEURAL COMPUT, V18, P2959, DOI 10.1162/neco.2006.18.12.2959
   Rumelhart DE, 1986, PARALLEL DISTRIBUTED, V1, DOI DOI 10.7551/MITPRESS/5236.001.0001
   Schrauwen B., 2004, NEUR NETW 2004 P 200, V1, p[xlvii, 3302]
   Swanson S, 2011, IEEE COMMUN MAG, V49, P112, DOI 10.1109/MCOM.2011.5741155
   Takase Haruhiko, 2009, Proceedings 2009 International Joint Conference on Neural Networks (IJCNN 2009 - Atlanta), P3062, DOI 10.1109/IJCNN.2009.5178756
   Wakamatsu T, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P951, DOI 10.1109/IJCNN.2011.6033325
   Werbos P., 1974, REGRESSION NEW TOOLS
NR 33
TC 0
Z9 0
U1 0
U2 1
PY 2012
UT WOS:000309341300006
DA 2023-11-16
ER

PT J
AU Meftah, B
   Lezoray, O
   Benyettou, A
AF Meftah, B.
   Lezoray, O.
   Benyettou, A.
TI Segmentation and Edge Detection Based on Spiking Neural Network Model
SO NEURAL PROCESSING LETTERS
DT Article
DE Spiking neuron networks; Segmentation; Edge detection; Hebbian learning
ID IMAGE SEGMENTATION; NEURONS
AB The process of segmenting images is one of the most critical ones in automatic image analysis whose goal can be regarded as to find what objects are present in images. Artificial neural networks have been well developed so far. First two generations of neural networks have a lot of successful applications. Spiking neuron networks (SNNs) are often referred to as the third generation of neural networks which have potential to solve problems related to biological stimuli. They derive their strength and interest from an accurate modeling of synaptic interactions between neurons, taking into account the time of spike emission. SNNs overcome the computational power of neural networks made of threshold or sigmoidal units. Based on dynamic event-driven processing, they open up new horizons for developing models with an exponential capacity of memorizing and a strong ability to fast adaptation. Moreover, SNNs add a new dimension, the temporal axis, to the representation capacity and the processing abilities of neural networks. In this paper, we present how SNN can be applied with efficacy in image segmentation and edge detection. Results obtained confirm the validity of the approach.
C1 [Meftah, B.] Univ Mascara, Equipe EDTEC, Mascara, Algeria.
   [Lezoray, O.] Univ Caen Basse Normandie, GREYC UMR CNRS 6072, F-14050 Caen, France.
   [Benyettou, A.] Univ Mohamed Boudiaf USTO, Lab Signal Image & Parole SIMPA, Oran, Algeria.
RP Meftah, B (corresponding author), Univ Mascara, Equipe EDTEC, Mascara, Algeria.
EM meftahb@yahoo.fr
CR [Anonymous], 2000, HDB MEDICAL IMAGING, DOI 10.1117/3.831079.ch3
   Averbeck BB, 2006, NAT REV NEUROSCI, V7, P358, DOI 10.1038/nrn1888
   Bohte Sander M., 2004, Natural Computing, V3, P195, DOI 10.1023/B:NACO.0000027755.02868.60
   Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   Buhmann JM, 2005, NEURAL COMPUT, V17, P1010, DOI 10.1162/0899766053491913
   Butts DA, 2007, NATURE, V449, P92, DOI [10.1038/nature06105, 10.1038/natureO6105]
   Chicurel M, 2002, SCIENCE, V295, P606, DOI 10.1126/science.295.5555.606
   Dayan P., 2001, THEORETICAL NEUROSCI
   Deng YN, 2001, IEEE T PATTERN ANAL, V23, P800, DOI 10.1109/34.946985
   Freixenet J, 2002, LECT NOTES COMPUT SC, V2352, P408, DOI 10.1007/3-540-47977-5_27
   Gerstner W, 2002, BIOL CYBERN, V87, P404, DOI 10.1007/s00422-002-0353-y
   Gerstner W., 2002, SPIKING NEURON MODEL
   Ghosh-Dastidar S, 2009, ADV INTEL SOFT COMPU, V61, P167
   GIRAU B, 2006, EUR S ART NEUR NETW, P173
   Gupta A, 2009, IEEE IJCNN, P1189
   Leibold C, 2001, NEURAL NETWORKS, V14, P805, DOI 10.1016/S0893-6080(01)00081-8
   Liew AWC, 2005, IEEE T FUZZY SYST, V13, P444, DOI 10.1109/TFUZZ.2004.841748
   Ma Z, 2010, COMPUT METHOD BIOMEC, V13, P235, DOI 10.1080/10255840903131878
   MAASS W, 2001, RELEVANCE NEURAL NET
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   MEFTAH B, 2008, IEEE WORLD C COMP IN, P682
   Melkemi KE, 2006, PATTERN RECOGN LETT, V27, P1230, DOI 10.1016/j.patrec.2005.07.021
   Meurie C, 2005, INT J ROBOT AUTOM, V20, P63, DOI 10.2316/Journal.206.2005.2.206-2780
   Natschlager T, 1998, NETWORK-COMP NEURAL, V9, P319, DOI 10.1088/0954-898X/9/3/003
   OSTER M, 2004, P 11 IEEE INT C EL C, V11, P203
   Paugam-Moisy H., 2009, HDB NATURAL COMPUTIN, P40
   Rowcliffe P, 2002, LECT NOTES COMPUT SC, V2415, P69
   Senthilkumaran N., 2009, International Journal of Recent Trends in Engineering, V1, P250
   Simoes AD, 2008, LECT NOTES ARTIF INT, V5249, P227, DOI 10.1007/978-3-540-88190-2_28
   Stein RB, 2005, NAT REV NEUROSCI, V6, P389, DOI 10.1038/nrn1668
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   Wu QX, 2008, NEUROCOMPUTING, V71, P2055, DOI 10.1016/j.neucom.2007.10.020
NR 32
TC 49
Z9 58
U1 3
U2 23
PD OCT
PY 2010
VL 32
IS 2
BP 131
EP 146
DI 10.1007/s11063-010-9149-6
UT WOS:000283099900002
DA 2023-11-16
ER

PT J
AU Jin, X
   Luján, M
   Plana, LA
   Davies, S
   Temple, S
   Furber, SB
AF Jin, Xin
   Lujan, Mikel
   Plana, Luis A.
   Davies, Sergio
   Temple, Steve
   Furber, Steve B.
TI MODELING SPIKING NEURAL NETWORKS ON SPINNAKER
SO COMPUTING IN SCIENCE & ENGINEERING
DT Editorial Material
C1 [Furber, Steve B.] Univ Manchester, Sch Comp Sci, Adv Processor Technol Res Grp, Manchester M13 9PL, Lancs, England.
EM jinxa@cs.man.ac.uk; mikel.lujan@manchester.ac.uk;
   luis.plana@manchester.ac.uk; daviess@cs.man.ac.uk; temples@cs.man.ac.uk;
   steve.furber@manchester.ac.uk
CR HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Izhikevich EM, 2004, CEREB CORTEX, V14, P933, DOI 10.1093/cercor/bhh053
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   JIN X, 2008, P 2008 INT JOINT C N, P2812
   Khan MM, 2008, IEEE IJCNN, P2849, DOI 10.1109/IJCNN.2008.4634199
   Markram H, 2006, NAT REV NEUROSCI, V7, P153, DOI 10.1038/nrn1848
   Merolla PA, 2007, IEEE T CIRCUITS-I, V54, P301, DOI 10.1109/TCSI.2006.887474
   Plana LA, 2007, IEEE DES TEST COMPUT, V24, P454, DOI 10.1109/MDT.2007.149
   Sporns O, 2000, CEREB CORTEX, V10, P127, DOI 10.1093/cercor/10.2.127
NR 9
TC 63
Z9 71
U1 2
U2 15
PD SEP-OCT
PY 2010
VL 12
IS 5
BP 91
EP 97
DI 10.1109/MCSE.2010.112
UT WOS:000281289200015
DA 2023-11-16
ER

PT J
AU Song, T
   Pan, LQ
   Wu, TF
   Zheng, P
   Wong, MLD
   Rodriguez-Paton, A
AF Song, Tao
   Pan, Linqiang
   Wu, Tingfang
   Zheng, Pan
   Wong, M. L. Dennis
   Rodriguez-Paton, Alfonso
TI Spiking Neural P Systems With Learning Functions
SO IEEE TRANSACTIONS ON NANOBIOSCIENCE
DT Article
DE Bio-inspired computing; membrane computing; spiking neural P system;
   learning; letter classification
ID SYNAPSES WORKING; NETWORKS; RECOGNITION; NEURONS; RULES; MODEL;
   CLASSIFICATION
AB Spiking neural P systems (SN P systems) are a class of distributed and parallel neural-like computing models, inspired from the way neurons communicate by means of spikes. In this paper, a new variant of the systems, called SN P systems with learning functions, is introduced. Such systems can dynamically strengthen and weaken connections among neurons during the computation. A class of specific SN P systems with simple Hebbian learning function is constructed to recognize English letters. The experimental results show that the SN P systems achieve average accuracy rate 98.76% in the test case without noise. In the test cases with low, medium, and high noises, the SN P systems outperform back propagation neural networks and probabilistic neural networks. Moreover, comparing with spiking neural networks, SN P systems perform a little better in recognizing letters with noise. The result of this paper is promising in terms of the fact that it is the first attempt to use SN P systems in pattern recognition after many theoretical advancements of SN P systems, and SN P systems exhibit the feasibility for tackling pattern recognition problems.
C1 [Song, Tao; Pan, Linqiang; Wu, Tingfang] Huazhong Univ Sci & Technol, Sch Automat, Educ Minist China, Key Lab Image Proc & Intelligent Control, Wuhan 430074, Hubei, Peoples R China.
   [Song, Tao] China Univ Petr, Coll Comp & Commun Engn, Qingdao 266580, Shandong, Peoples R China.
   [Zheng, Pan] Univ Canterbury, Dept Accounting & Informat Syst, Christchurch 8041, New Zealand.
   [Wong, M. L. Dennis] Heriot Watt Univ Malaysia, Sch Engn & Phys Sci, Kuala Lumpur 62200, Malaysia.
   [Rodriguez-Paton, Alfonso] Univ Politecn Madrid, Dept Inteligencia Artificial, Campus Montegancedo, E-28660 Madrid, Spain.
RP Pan, LQ (corresponding author), Huazhong Univ Sci & Technol, Sch Automat, Educ Minist China, Key Lab Image Proc & Intelligent Control, Wuhan 430074, Hubei, Peoples R China.
EM lqpan@mail.hust.edu.cn
CR [Anonymous], INT J FOUND COMPUT S
   [Anonymous], BIOSYSTEMS
   [Anonymous], NEUROCOMPUTING
   [Anonymous], 2015, DEEP LEARNING, DOI [DOI 10.1038/NATURE14539, 10.1038/nature14539]
   [Anonymous], P IEEE INT C MACH VI
   [Anonymous], FUNDAM INFORM
   [Anonymous], 2008, WORLD APPL SCI J
   [Anonymous], 2016, IEEE T NEURAL NETWOR
   [Anonymous], COMPUT KNOWL TECHNOL
   [Anonymous], 2004, OXFORD HDB COMPUTATI
   [Anonymous], 2010, SPIKING NEURAL P SYS
   [Anonymous], P 5 BRAINST WEEK MEM
   [Anonymous], NEURAL NETW
   Buonomano DV, 1999, NEURAL COMPUT, V11, P103, DOI 10.1162/089976699300016836
   Burges CJC, 1998, DATA MIN KNOWL DISC, V2, P121, DOI 10.1023/A:1009715923555
   Carpenter G., 1991, PATTERN RECOGNITION
   CARPENTER GA, 1989, NEURAL NETWORKS, V2, P243, DOI 10.1016/0893-6080(89)90035-X
   Cavaliere M, 2009, THEOR COMPUT SCI, V410, P2352, DOI 10.1016/j.tcs.2009.02.031
   Deco G, 1998, NETWORK-COMP NEURAL, V9, P303, DOI 10.1088/0954-898X/9/3/002
   Eurich CW, 2000, NEURAL COMPUT, V12, P1519, DOI 10.1162/089976600300015240
   Ghosh-Dastidar S, 2007, INTEGR COMPUT-AID E, V14, P187
   Gong MG, 2015, IEEE T NEUR NET LEAR, V26, P3263, DOI 10.1109/TNNLS.2015.2469673
   Goodman DFM, 2009, FRONT NEUROSCI-SWITZ, V3, P192, DOI 10.3389/neuro.01.026.2009
   Gupta A, 2007, IEEE IJCNN, P53, DOI 10.1109/IJCNN.2007.4370930
   Hagan MT., 1997, NEURAL NETWORK DESIG
   Haibo He, 2009, IEEE Transactions on Knowledge and Data Engineering, V21, P1263, DOI 10.1109/TKDE.2008.239
   Hopcroft John E., 1979, INTRO AUTOMATA THEOR
   Ibarra OH, 2009, THEOR COMPUT SCI, V410, P2982, DOI 10.1016/j.tcs.2009.03.004
   Ionescu M, 2006, FUND INFORM, V71, P279
   Ishdorj TO, 2010, THEOR COMPUT SCI, V411, P2345, DOI 10.1016/j.tcs.2010.01.019
   Kang M, 2008, INFORM SCIENCES, V178, P3802, DOI 10.1016/j.ins.2008.05.011
   Kasabov NK, 2014, NEURAL NETWORKS, V52, P62, DOI 10.1016/j.neunet.2014.01.006
   Leporati Alberto, 2009, Natural Computing, V8, P681, DOI 10.1007/s11047-008-9091-y
   LI JH, 1989, IEEE T CIRCUITS SYST, V36, P1405, DOI 10.1109/31.41297
   Liu CL, 2003, PATTERN RECOGN, V36, P2271, DOI 10.1016/S0031-3203(03)00085-2
   Liu YT, 2016, IEEE T NEUR NET LEAR, V27, P347, DOI 10.1109/TNNLS.2015.2496330
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maass W, 2001, PULSED NEURAL NETWOR
   MOLLER MF, 1993, NEURAL NETWORKS, V6, P525, DOI 10.1016/S0893-6080(05)80056-5
   Ni Z, 2013, IEEE T NEUR NET LEAR, V24, P913, DOI 10.1109/TNNLS.2013.2247627
   Pan LQ, 2011, SCI CHINA INFORM SCI, V54, P1596, DOI 10.1007/s11432-011-4303-y
   Pan LQ, 2010, LECT NOTES COMPUT SC, V5957, P436
   Pan LQ, 2009, INT J COMPUT COMMUN, V4, P273, DOI 10.15837/ijccc.2009.3.2435
   Panchev C, 2006, LECT NOTES COMPUT SC, V4131, P750
   Paum G, 2007, J UNIVERS COMPUT SCI, V13, P1707
   Päun G, 2000, J COMPUT SYST SCI, V61, P108, DOI 10.1006/jcss.1999.1693
   Paun Gh, 2010, OXFORD HDB MEMBRANE
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Roy S, 2016, IEEE T NEUR NET LEAR, V27, P1572, DOI 10.1109/TNNLS.2015.2447011
   SIEGELMANN HT, 1995, J COMPUT SYST SCI, V50, P132, DOI 10.1006/jcss.1995.1013
   Sipser Michael, 2012, INTRO THEORY COMPUTA
   Song T., 2018, NEURAL PROCESS LETT, P1, DOI 0.1007/s11063-018-9947-9
   Song T, 2016, IEEE T NANOBIOSCI, V15, P666, DOI 10.1109/TNB.2016.2598879
   Song T, 2016, INFORM SCIENCES, V372, P380, DOI 10.1016/j.ins.2016.08.055
   Song T, 2015, IEEE T NANOBIOSCI, V14, P465, DOI 10.1109/TNB.2015.2402311
   Song T, 2015, IEEE T NANOBIOSCI, V14, P38, DOI 10.1109/TNB.2014.2367506
   Song T, 2014, THEOR COMPUT SCI, V529, P82, DOI 10.1016/j.tcs.2014.01.001
   Song T, 2013, IEEE T NANOBIOSCI, V12, P255, DOI 10.1109/TNB.2013.2271278
   Song T, 2013, INFORM SCIENCES, V219, P197, DOI 10.1016/j.ins.2012.07.023
   Stimberg M, 2014, FRONT NEUROINFORM, V8, DOI 10.3389/fninf.2014.00006
   Wang J, 2010, NEURAL COMPUT, V22, P2615, DOI 10.1162/NECO_a_00022
   Wang T, 2015, IEEE T POWER SYST, V30, P1182, DOI 10.1109/TPWRS.2014.2347699
   Wasserman P D, 1993, ADV METHODS NEURAL C
   Zeng XX, 2012, IEEE T NANOBIOSCI, V11, P366, DOI 10.1109/TNB.2012.2211034
   Zhang GX, 2014, INT J NEURAL SYST, V24, DOI 10.1142/S0129065714400061
   Zhang HG, 2014, IEEE T NEUR NET LEAR, V25, P1229, DOI 10.1109/TNNLS.2014.2317880
   Zhong XN, 2015, NEUROCOMPUTING, V149, P116, DOI 10.1016/j.neucom.2014.01.060
   Zhou Bolei, 2014, NEURIPS
NR 68
TC 83
Z9 83
U1 5
U2 47
PD APR
PY 2019
VL 18
IS 2
BP 176
EP 190
DI 10.1109/TNB.2019.2896981
UT WOS:000466225100008
DA 2023-11-16
ER

PT J
AU Renaud, S
   Tomas, J
   Lewis, N
   Bornat, Y
   Daouzli, A
   Rudolph, M
   Destexhe, A
   Saïghi, S
AF Renaud, S.
   Tomas, J.
   Lewis, N.
   Bornat, Y.
   Daouzli, A.
   Rudolph, M.
   Destexhe, A.
   Saighi, S.
TI PAX: A mixed hardware/software simulation platform for spiking neural
   networks
SO NEURAL NETWORKS
DT Article
DE Spiking neural networks; Integrated circuits; Hardware simulation;
   Conductance-based neuron models; Spike-Timing-Dependent Plasticity
ID NEURONS; MODEL; SYNAPSES; SYSTEM
AB Many hardware-based solutions now exist for the simulation of bio-like neural networks. Less conventional than software-based systems, these types of simulators generally combine digital and analog forms of computation. In this paper we present a mixed hardware-software platform, specifically designed for the simulation of spiking neural networks, using conductance-based models of neurons and synaptic connections with dynamic adaptation rules (Spike-Timing-Dependent Plasticity). The neurons and networks are configurable, and are computed in 'biological real time' by which we mean that the difference between simulated time and simulation time is guaranteed lower than 50 mu s. After presenting the issues and context involved in the design and use of hardware-based spiking neural networks, we describe the analog neuromimetic integrated circuits which form the core of the platform. We then explain the organization and computation principles of the modules within the platform, and present experimental results which validate the system. Designed as a tool for computational neuroscience, the platform is exploited Ill collaborative research projects together with neurobiology and computer science partners. (C) 2010 Elsevier Ltd. All rights reserved.
C1 [Renaud, S.; Tomas, J.; Lewis, N.; Bornat, Y.; Daouzli, A.; Saighi, S.] Univ Bordeaux, IMS, ENSEIRB, CNRS,UMR5218, F-33405 Talence, France.
   [Rudolph, M.; Destexhe, A.] CNRS, UNIC, UPR2191, F-91198 Gif Sur Yvette, France.
RP Renaud, S (corresponding author), Univ Bordeaux, IMS, ENSEIRB, CNRS,UMR5218, 351 Cours Liberat, F-33405 Talence, France.
EM sylvie.renaud@ims-bordeaux.fr
CR Akay M., 2007, HDB NEURAL ENG 2
   Badoual M, 2006, INT J NEURAL SYST, V16, P79, DOI 10.1142/S0129065706000524
   Binczak S, 2006, NEURAL NETWORKS, V19, P684, DOI 10.1016/j.neunet.2005.07.011
   BORNAT Y, 2005, P 20 C DES CIRC INT
   Brette R, 2005, J NEUROPHYSIOL, V94, P3637, DOI 10.1152/jn.00686.2005
   Brette R, 2007, J COMPUT NEUROSCI, V23, P349, DOI 10.1007/s10827-007-0038-6
   CONNORS BW, 1990, TRENDS NEUROSCI, V13, P99, DOI 10.1016/0166-2236(90)90185-D
   Destexhe A, 2001, NEUROSCIENCE, V107, P13, DOI 10.1016/S0306-4522(01)00344-X
   DESTEXHE A, 1994, NEURAL COMPUT, V6, P14, DOI 10.1162/neco.1994.6.1.14
   Farquhar E, 2005, IEEE T CIRCUITS-I, V52, P477, DOI 10.1109/TCSI.2004.842871
   Fieres J, 2006, IEEE IJCNN, P21
   FITZHUGH R, 1961, BIOPHYS J, V1, P445, DOI 10.1016/S0006-3495(61)86902-6
   Froemke RC, 2002, NATURE, V416, P433, DOI 10.1038/416433a
   Gerstner W., 2002, SPIKING NEURON MODEL
   Glackin B, 2005, LECT NOTES COMPUT SC, V3512, P552
   Graas EL, 2004, NEUROINFORMATICS, V2, P417, DOI 10.1385/NI:2:4:417
   Gupta A, 2000, SCIENCE, V287, P273, DOI 10.1126/science.287.5451.273
   Hasler P, 2007, IEEE INT SYMP CIRC S, P3359, DOI 10.1109/ISCAS.2007.378287
   Hines ML, 1997, NEURAL COMPUT, V9, P1179, DOI 10.1162/neco.1997.9.6.1179
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Indiveri G, 2006, IEEE T NEURAL NETWOR, V17, P211, DOI 10.1109/TNN.2005.860850
   Indiveri G, 2007, IEEE INT SYMP CIRC S, P3371, DOI 10.1109/ISCAS.2007.378290
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Johansson C, 2007, NEURAL NETWORKS, V20, P48, DOI 10.1016/j.neunet.2006.05.029
   Jung R, 2001, IEEE T NEUR SYS REH, V9, P319, DOI 10.1109/7333.948461
   Le Masson G, 2002, NATURE, V417, P854, DOI 10.1038/nature00825
   Liu SC, 2004, IEEE T NEURAL NETWOR, V15, P1305, DOI 10.1109/TNN.2004.832725
   MAHOWALD M, 1991, NATURE, V354, P515, DOI 10.1038/354515a0
   McCormick DA, 1997, ANNU REV NEUROSCI, V20, P185, DOI 10.1146/annurev.neuro.20.1.185
   Migliore M, 2006, J COMPUT NEUROSCI, V21, P119, DOI 10.1007/s10827-006-7949-5
   MORRIS C, 1981, BIOPHYS J, V35, P193, DOI 10.1016/S0006-3495(81)84782-0
   RENAUD S, 1999, IEEE T BIOMEDICAL EN, V46, P638
   Renaud S, 2007, IEEE INT SYMP CIRC S, P3355, DOI 10.1109/ISCAS.2007.378286
   Renaud-Le Masson S, 2004, INFORM SCIENCES, V161, P57, DOI 10.1016/j.ins.2003.03.007
   Schemmel J, 2007, IEEE INT SYMP CIRC S, P3367, DOI 10.1109/ISCAS.2007.378289
   Sorensen M, 2004, J NEUROSCI, V24, P5427, DOI 10.1523/JNEUROSCI.4449-03.2004
   Vogelstein RJ, 2004, 2004 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL 5, PROCEEDINGS, P385
   Zou Q, 2006, NETWORK-COMP NEURAL, V17, P211, DOI 10.1080/09548980600711124
NR 38
TC 15
Z9 17
U1 0
U2 5
PD SEP
PY 2010
VL 23
IS 7
BP 905
EP 916
DI 10.1016/j.neunet.2010.02.006
UT WOS:000281005300013
DA 2023-11-16
ER

PT J
AU Yan, YL
   Chu, HM
   Jin, Y
   Huan, YX
   Zou, Z
   Zheng, LR
AF Yan, Yulong
   Chu, Haoming
   Jin, Yi
   Huan, Yuxiang
   Zou, Zhuo
   Zheng, Lirong
TI Backpropagation With Sparsity Regularization for Spiking Neural Network
   Learning
SO FRONTIERS IN NEUROSCIENCE
DT Article
DE spiking neural network; backpropagation; sparsity regularization;
   spiking sparsity; synaptic sparsity
ID INTELLIGENCE; PERFORMANCE
AB The spiking neural network (SNN) is a possible pathway for low-power and energy-efficient processing and computing exploiting spiking-driven and sparsity features of biological systems. This article proposes a sparsity-driven SNN learning algorithm, namely backpropagation with sparsity regularization (BPSR), aiming to achieve improved spiking and synaptic sparsity. Backpropagation incorporating spiking regularization is utilized to minimize the spiking firing rate with guaranteed accuracy. Backpropagation realizes the temporal information capture and extends to the spiking recurrent layer to support brain-like structure learning. The rewiring mechanism with synaptic regularization is suggested to further mitigate the redundancy of the network structure. Rewiring based on weight and gradient regulates the pruning and growth of synapses. Experimental results demonstrate that the network learned by BPSR has synaptic sparsity and is highly similar to the biological system. It not only balances the accuracy and firing rate, but also facilitates SNN learning by suppressing the information redundancy. We evaluate the proposed BPSR on the visual dataset MNIST, N-MNIST, and CIFAR10, and further test it on the sensor dataset MIT-BIH and gas sensor. Results bespeak that our algorithm achieves comparable or superior accuracy compared to related works, with sparse spikes and synapses.
C1 [Yan, Yulong; Chu, Haoming; Jin, Yi; Huan, Yuxiang; Zou, Zhuo; Zheng, Lirong] Fudan Univ, Sch Informat Sci & Technol, Shanghai, Peoples R China.
RP Zou, Z; Zheng, LR (corresponding author), Fudan Univ, Sch Informat Sci & Technol, Shanghai, Peoples R China.
EM zhuo@fudan.edu.cn; lrzheng@fudan.edu.cn
CR Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   Allred J.M., 2020, PREPRINT
   Amirshahi A, 2019, IEEE T BIOMED CIRC S, V13, P1483, DOI 10.1109/TBCAS.2019.2948920
   [Anonymous], 2019, P AAAI C ARTIFICIAL
   Bartol TM, 2015, ELIFE, V4, DOI 10.7554/eLife.10778
   Bauer FC, 2019, IEEE T BIOMED CIRC S, V13, P1575, DOI 10.1109/TBCAS.2019.2953001
   Bialas M, 2022, IEEE T NEUR NET LEAR, V33, P5215, DOI 10.1109/TNNLS.2021.3069683
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Chen HT, 2020, PROC CVPR IEEE, P1465, DOI 10.1109/CVPR42600.2020.00154
   Cheng ZQ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1897, DOI 10.1145/3343031.3350898
   Cho SG, 2019, IEEE CUST INTEGR CIR, DOI 10.1109/CICC.2019.8780116
   Comsa IM, 2022, IEEE T NEUR NET LEAR, V33, P5939, DOI 10.1109/TNNLS.2021.3071976
   Cook SJ, 2019, NATURE, V571, P63, DOI 10.1038/s41586-019-1352-7
   Corradi F, 2019, IEEE IJCNN
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Dempsey WP, 2022, P NATL ACAD SCI USA, V119, DOI 10.1073/pnas.2107661119
   Diehl P.U., 2015, 2015 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2015.7280696
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Ding C, 2021, 2021 IEEE 3RD INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE CIRCUITS AND SYSTEMS (AICAS), DOI [10.1109/ICAICE54393.2021.00008, 10.1109/AICAS51828.2021.9458445]
   Finnerty A., 2017, REDUCE POWER COST CO
   Frenkel C., 2021, ARXIV PREPRINT ARXIV
   Guo WZ, 2020, ELECTRONICS-SWITZ, V9, DOI 10.3390/electronics9071059
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   He Kaiming, 2015, IEEE I CONF COMP VIS, P1026, DOI DOI 10.1109/ICCV.2015.123
   Hubara I., 2016, ADV NEURAL INFORM PR, VVolume 29
   Imam N, 2020, NAT MACH INTELL, V2, P181, DOI 10.1038/s42256-020-0159-4
   Ioffe S., 2015, PR MACH LEARN RES, P448
   Jin Y., 2018, ADV NEURAL INF PROCE, V31, P1
   Kaiser J, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00424
   Kim S, 2020, AAAI CONF ARTIF INTE, V34, P11270
   Kingma DP., 2017, ARXIV
   Kolagasioglu E., 2018, ENERGY EFFICIENT FEA
   Krizhevsky Alex, 2009, LEARNING MULTIPLE LA
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Liang MX, 2021, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS51556.2021.9401607
   Lillicrap TP, 2020, NAT REV NEUROSCI, V21, P335, DOI 10.1038/s41583-020-0277-3
   Loshchilov Ilya, 2017, ARXIV171105101
   Luo LQ, 2021, SCIENCE, V373, P1103, DOI 10.1126/science.abg7285
   Marisa T, 2017, IEEE T BIOMED CIRC S, V11, P267, DOI 10.1109/TBCAS.2016.2619858
   Milo R, 2004, SCIENCE, V303, P1538, DOI 10.1126/science.1089167
   Moody GA, 2001, IEEE ENG MED BIOL, V20, P45, DOI 10.1109/51.932724
   Mozafari M, 2018, IEEE T NEUR NET LEAR, V29, P6178, DOI 10.1109/TNNLS.2018.2826721
   Nguyen T.N.N., 2021, PREPRINT
   Orchard G, 2015, FRONT NEUROSCI-SWITZ, V9, DOI [10.3389/fhins.2015.00437, 10.3389/fnins.2015.00437]
   Paszke A, 2019, ADV NEUR IN, V32
   Pei J, 2019, NATURE, V572, P106, DOI 10.1038/s41586-019-1424-8
   Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32
   Rathi N, 2019, IEEE T COMPUT AID D, V38, P668, DOI 10.1109/TCAD.2018.2819366
   Roy K, 2019, NATURE, V575, P607, DOI 10.1038/s41586-019-1677-2
   Rueckauer B, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351295
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Shi Y, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00405
   Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270
   Stöckl C, 2021, NAT MACH INTELL, V3, DOI 10.1038/s42256-021-00311-4
   Tang H, 2020, NEUROCOMPUTING, V407, P300, DOI 10.1016/j.neucom.2020.05.031
   Tang P.T.P., 2017, ARXIV PREPRINT ARXIV
   Thorpe S, 1998, COMPUTATIONAL NEUROSCIENCE: TRENDS IN RESEARCH, P113
   Vaila R., 2019, P INT C NEUR SYST, P1
   Vergara A, 2013, SENSOR ACTUAT B-CHEM, V185, P462, DOI 10.1016/j.snb.2013.05.027
   Wu YC, 2020, ELECTRON LETT, V56, P1230, DOI 10.1049/el.2020.2224
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Yan YL, 2021, 2021 IEEE 3RD INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE CIRCUITS AND SYSTEMS (AICAS), DOI 10.1109/AICAS51828.2021.9458461
   Yan ZL, 2021, BIOMED SIGNAL PROCES, V63, DOI 10.1016/j.bspc.2020.102170
   Zhang W., 2019, P ADV NEUR INF PROC, V32, P1
NR 64
TC 4
Z9 4
U1 5
U2 20
PD APR 14
PY 2022
VL 16
AR 760298
DI 10.3389/fnins.2022.760298
UT WOS:000796233600001
DA 2023-11-16
ER

PT J
AU Sporea, I
   Grüning, A
AF Sporea, Ioana
   Gruening, Andre
TI Supervised Learning in Multilayer Spiking Neural Networks
SO NEURAL COMPUTATION
DT Article
ID ERROR-BACKPROPAGATION; CLASSIFICATION; REINFORCEMENT; PROPAGATION;
   NEURONS
AB We introduce a supervised learning algorithm for multilayer spiking neural networks. The algorithm overcomes a limitation of existing learning algorithms: it can be applied to neurons firing multiple spikes in artificial neural networks with hidden layers. It can also, in principle, be used with any linearizable neuron model and allows different coding schemes of spike train patterns. The algorithm is applied successfully to classic linearly nonseparable benchmarks such as the XOR problem and the Iris data set, as well as to more complex classification and mapping problems. The algorithm has been successfully tested in the presence of noise, requires smaller networks than reservoir computing, and results in faster convergence than existing algorithms for similar tasks such as SpikeProp.
C1 [Sporea, Ioana; Gruening, Andre] Univ Surrey, Dept Comp, Guildford GU2 7XH, Surrey, England.
RP Sporea, I (corresponding author), Univ Surrey, Dept Comp, Guildford GU2 7XH, Surrey, England.
EM i.nica@surrey.ac.uk; a.gruning@surrey.ac.uk
CR [Anonymous], P 15 PRORISC WORKSH
   [Anonymous], 2001, HDB BIOL PHYS
   Bohte S., 2010, ADV NEURAL INFORM PR, V23, P253
   Bohte SM, 2011, LECT NOTES COMPUT SC, V6791, P60, DOI 10.1007/978-3-642-21735-7_8
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Booij O, 2005, INFORM PROCESS LETT, V95, P552, DOI 10.1016/j.ipl.2005.05.023
   deCharms RC, 1996, NATURE, V381, P610, DOI 10.1038/381610a0
   Elias J. G., 2002, PULSED NEURAL NETWOR
   Fisher RA, 1936, ANN EUGENIC, V7, P179, DOI 10.1111/j.1469-1809.1936.tb02137.x
   Fitzsimonds RM, 1997, NATURE, V388, P439, DOI 10.1038/41267
   Gerstner W., 2002, SPIKING NEURON MODEL
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   Glackin C, 2011, NEURAL NETWORKS, V24, P247, DOI 10.1016/j.neunet.2010.11.008
   Grüning A, 2007, NEURAL COMPUT, V19, P3108, DOI 10.1162/neco.2007.19.11.3108
   Grüning A, 2012, NEURAL PROCESS LETT, V36, P117, DOI 10.1007/s11063-012-9225-1
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Harris KD, 2008, TRENDS NEUROSCI, V31, P130, DOI 10.1016/j.tins.2007.12.002
   Hebb D., 2005, ORG BEHAV NEUROPSYCH
   Johansson RS, 2004, NAT NEUROSCI, V7, P170, DOI 10.1038/nn1177
   KNUDSEN EI, 1994, J NEUROSCI, V14, P3985
   Knudsen EI, 2002, NATURE, V417, P322, DOI 10.1038/417322a
   Legenstein R, 2005, NEURAL COMPUT, V17, P2337, DOI 10.1162/0899766054796888
   Maass W, 1997, NEURAL COMPUT, V9, P279, DOI 10.1162/neco.1997.9.2.279
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Masaru F, 2008, IEEE IJCNN, P840, DOI 10.1109/IJCNN.2008.4633895
   Mauk MD, 2004, ANNU REV NEUROSCI, V27, P307, DOI 10.1146/annurev.neuro.27.070203.144247
   McKennoch S, 2009, NEURAL COMPUT, V21, P9, DOI 10.1162/neco.2008.09-07-610
   Neuenschwander S, 1996, NATURE, V379, P728, DOI 10.1038/379728a0
   Ponulak F., 2006, RESUME PROOF CONVERG
   Ponulak F, 2008, INT J APPL MATH COMP, V18, P117, DOI 10.2478/v10006-008-0011-1
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Roelfsema P. R., 2005, NEURAL COMPUT, V17, P1
   Rojas R, 1996, NEURAL NETWORKS, P149, DOI 10.1007/978-3-642-61068-4{\_}7
   Rostro-Gonzalez H., 2010, REVERSE ENG SPIKING
   Ruf B, 1997, NEURAL PROCESS LETT, V5, P9, DOI 10.1023/A:1009697008681
   Rumelhart DE, 1986, PARALLEL DISTRIBUTED, V1, DOI DOI 10.7551/MITPRESS/5236.001.0001
   Shepherd JD, 2006, NEURON, V52, P475, DOI 10.1016/j.neuron.2006.08.034
   Sporea I, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P1090, DOI 10.1109/IJCNN.2011.6033344
   Takase Haruhiko, 2009, Proceedings 2009 International Joint Conference on Neural Networks (IJCNN 2009 - Atlanta), P3062, DOI 10.1109/IJCNN.2009.5178756
   Tao HZW, 2000, J NEUROSCI, V20, P3233, DOI 10.1523/JNEUROSCI.20-09-03233.2000
   THORPE SJ, 1989, CONNECTIONISM IN PERSPECTIVE, P63
   Tino P, 2005, LECT NOTES COMPUT SC, V3611, P666
   Urbanczik R, 2009, NAT NEUROSCI, V12, P250, DOI 10.1038/nn.2264
   van Rossum MCW, 2001, NEURAL COMPUT, V13, P751, DOI 10.1162/089976601300014321
   Wade JJ, 2010, IEEE T NEURAL NETWOR, V21, P1817, DOI 10.1109/TNN.2010.2074212
   WATT AJ, 2010, FRONTIERS SYNAPTIC N, V2
   Wehr M, 1996, NATURE, V384, P162, DOI 10.1038/384162a0
   Xin JG, 2001, IEEE IJCNN, P1772, DOI 10.1109/IJCNN.2001.938430
NR 48
TC 94
Z9 100
U1 0
U2 46
PD FEB
PY 2013
VL 25
IS 2
BP 473
EP 509
DI 10.1162/NECO_a_00396
UT WOS:000313403600006
DA 2023-11-16
ER

PT C
AU Antiqueira, L
   Zhao, L
AF Antiqueira, Lucas
   Zhao, Liang
BE Engelbrecht, A
   Filho, CJAB
   Neto, FBD
TI Structural relationships between spiking neural networks and functional
   samples
SO 2013 1ST BRICS COUNTRIES CONGRESS ON COMPUTATIONAL INTELLIGENCE AND 11TH
   BRAZILIAN CONGRESS ON COMPUTATIONAL INTELLIGENCE (BRICS-CCI & CBIC)
DT Proceedings Paper
CT 1st BRICS Countries Congress on Computational Intelligence / 11th
   Brazilian Congress on Computational Intelligence (BRICS-CCI and CBIC)
CY SEP 08-11, 2013
CL Recife, BRAZIL
DE computer simulation; modeling; network theory (graphs); spatial networks
ID COMPLEX NETWORKS; SCALE-FREE; BRAIN; RECORDINGS; EMERGENCE; TOPOLOGY;
   EEG
AB Models of spiking neural networks have a great potential to become a crucial tool in the development of complex network theory. Of particular interest, these models can be used to better understand the important class of brain functional networks, which are frequently studied in the context of computational network analysis. A fundamental question is whether functional connectivity sampling via surface multichannel recordings is able to reproduce the main connectivity features of the underlying spatial neural network. In this work we address this problem through computational modeling using the integrate-and-fire spiking neuron model, which enabled us to relate neural connectivity and the respective mesoscopic dynamics. Functional samples were then compared to an idealized spatial neural network model in terms of established topological network measurements. Results show that some measurements (e.g., betweenness centrality) are able to fairly approximate functional and spatial networks. Therefore, under specific circumstances of sampling size and simulation approach, it is possible to say that functional networks are able to reproduce connectivity features of the underlying neural network.
C1 [Antiqueira, Lucas; Zhao, Liang] Univ Sao Paulo, ICMC, Inst Math & Comp Sci, BR-13560970 Sao Carlos, SP, Brazil.
RP Antiqueira, L (corresponding author), Univ Sao Paulo, ICMC, Inst Math & Comp Sci, POB 668, BR-13560970 Sao Carlos, SP, Brazil.
EM lantiq@icmc.usp.br; zhao@icmc.usp.br
CR Achlioptas D, 2009, J ACM, V56, DOI 10.1145/1538902.1538905
   Antiqueira L, 2010, NEUROIMAGE, V53, P439, DOI 10.1016/j.neuroimage.2010.06.018
   Antiqueira L, 2009, NEW J PHYS, V11, DOI 10.1088/1367-2630/11/1/013058
   Arenas A, 2008, PHYS REP, V469, P93, DOI 10.1016/j.physrep.2008.09.002
   Azevedo FAC, 2009, J COMP NEUROL, V513, P532, DOI 10.1002/cne.21974
   Barabási AL, 1999, SCIENCE, V286, P509, DOI 10.1126/science.286.5439.509
   Boas PRV, 2010, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2010/03/P03009
   Bullmore ET, 2012, NAT REV NEUROSCI, V13, P336, DOI 10.1038/nrn3214
   Buzsáki G, 2012, NAT REV NEUROSCI, V13, P407, DOI 10.1038/nrn3241
   Clauset A, 2005, PHYS REV LETT, V94, DOI 10.1103/PhysRevLett.94.018701
   Costa LD, 2007, ADV PHYS, V56, P167, DOI 10.1080/00018730601170527
   Costa LD, 2011, ADV PHYS, V60, P329, DOI 10.1080/00018732.2011.572452
   Dall'Asta L, 2006, THEOR COMPUT SCI, V355, P6, DOI 10.1016/j.tcs.2005.12.009
   Deco G, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.1000092
   Erdos P., 1959, PUBL MATH DEBRECEN, V6, P290
   FREEMAN LC, 1977, SOCIOMETRY, V40, P35, DOI 10.2307/3033543
   GILBERT EN, 1959, ANN MATH STAT, V30, P1141, DOI 10.1214/aoms/1177706098
   Han JDJ, 2005, NAT BIOTECHNOL, V23, P839, DOI 10.1038/nbt1116
   Joudaki Amir, 2012, PLoS One, V7, pe35673, DOI 10.1371/journal.pone.0035673
   Kaiser M, 2004, PHYS REV E, V69, DOI 10.1103/PhysRevE.69.036103
   Kaiser M, 2011, NEUROIMAGE, V57, P892, DOI 10.1016/j.neuroimage.2011.05.025
   Koschützki D, 2005, LECT NOTES COMPUT SC, V3418, P16
   Kramer MA, 2011, J NEUROSCI, V31, P15757, DOI 10.1523/JNEUROSCI.2287-11.2011
   Latapy M, 2008, IEEE INFOCOM SER, P2333
   Latora V, 2001, PHYS REV LETT, V87, DOI 10.1103/PhysRevLett.87.198701
   Lee SH, 2006, PHYS REV E, V73, DOI 10.1103/PhysRevE.73.016102
   Newman M, 2010, NETWORKS INTRO, DOI 10.1093/acprof:oso/9780199206650.001.0001
   Newman MEJ, 2002, PHYS REV LETT, V89, DOI 10.1103/PhysRevLett.89.208701
   Nunez P. L., 2006, ELECT FIELDS BRAIN N, V2nd Edn
   Olejniczak P, 2006, J CLIN NEUROPHYSIOL, V23, P186, DOI 10.1097/01.wnp.0000220079.61973.6c
   Palva S, 2012, TRENDS COGN SCI, V16, P219, DOI 10.1016/j.tics.2012.02.004
   Pereda E, 2005, PROG NEUROBIOL, V77, P1, DOI 10.1016/j.pneurobio.2005.10.003
   Pipa G, 2011, FRONT COMPUT NEUROSC, V5, DOI [10.3389/fncom.2011.00023, 10.3389/fncom.2011.00004]
   Roth A, 2009, COMPUT NEUROSCI-MIT, P139
   Rubinov M, 2011, NEUROIMAGE, V56, P2068, DOI 10.1016/j.neuroimage.2011.03.069
   Shkarayev MS, 2009, EPL-EUROPHYS LETT, V88, DOI 10.1209/0295-5075/88/50001
   Stam CJ, 2010, INT J PSYCHOPHYSIOL, V77, P186, DOI 10.1016/j.ijpsycho.2010.06.024
   Stumpf MPH, 2005, P NATL ACAD SCI USA, V102, P4221, DOI 10.1073/pnas.0501179102
   Trappenberg TP., 2002, FUNDAMENTALS COMPUTA
   Watts DJ, 1998, NATURE, V393, P440, DOI 10.1038/30918
   WAXMAN BM, 1988, IEEE J SEL AREA COMM, V6, P1617, DOI 10.1109/49.12889
NR 41
TC 3
Z9 3
U1 0
U2 3
PY 2013
BP 46
EP 54
DI 10.1109/BRICS-CCI-CBIC.2013.19
UT WOS:000346422500008
DA 2023-11-16
ER

PT C
AU Yusuf, ZM
   Hamed, HNA
   Yusuf, LM
   Isa, MA
AF Yusuf, Zulhairi Mi
   Hamed, Haza Nuzly Abdull
   Yusuf, Lizawati Mi
   Isa, Mohd Adham
GP IEEE
TI Evolving Spiking Neural Network (ESNN) and Harmony Search Algorithm
   (HSA) for parameter optimization
SO PROCEEDINGS OF THE 2017 6TH INTERNATIONAL CONFERENCE ON ELECTRICAL
   ENGINEERING AND INFORMATICS (ICEEI'17)
SE International Conference on Electrical Engineering and Informatics
DT Proceedings Paper
CT 6th International Conference on Electrical Engineering and Informatics
   (ICEEI) - Sustainable Society Through Digital Innovation
CY NOV 25-27, 2017
CL Langkawi, MALAYSIA
DE Evolving Spiking Neural Network (ESNN); Harmony Search Algorithm;
   Parameter Optimization; Modulation Factor; Proportion Factor; Similarity
   Factors
AB Spiking Neural Network (SNN) acts as a part of the third generation of Artificial Neural Networks (ANNs). Evolving Spiking Neural Network (ESNN) is one of the most broadly utilized among in SNN models in numerous current research works. During the classification process, ESNN model acts as a classifier and three parameters are used in this article. However, the parameters are needed to set manually before the classification process. To solve the stated problems, ESNN required an optimizer that able to optimize the three parameters such as similarity value, modulation factor and proportion factor. The best estimations of parameters are adaptively chosen by Harmony Search Algorithm (HSA) to abstain from choosing appropriate values for specific issues through the trial-and-error approach. Therefore, this article proposed the integration of ESNN as a classifier and HSA as an optimizer for parameter optimization. The experimental results give favorable accuracy rates via the hybrid of ESNN and HSA.
C1 [Yusuf, Zulhairi Mi; Hamed, Haza Nuzly Abdull; Yusuf, Lizawati Mi; Isa, Mohd Adham] Univ Teknol Malaysia, Fac Comp, Johor Baharu 81310, Johor, Malaysia.
RP Yusuf, ZM (corresponding author), Univ Teknol Malaysia, Fac Comp, Johor Baharu 81310, Johor, Malaysia.
EM zulhairi.miyusuf@gmail.com; haza@utm.my; lizawati@utm.my;
   mohadham@utm.my
CR Abdull Hamed HN., 2012, NOVEL INTEGRATED MET
   [Anonymous], 2007, EVOLVING CONNECTIONI
   Ehrlich H. C., LNCS, V4128, P1148
   Gao X., 2016, RAKENTEIDEN MEKANIIK, V49, P119
   Geem ZW, 2001, SIMULATION, V76, P60, DOI 10.1177/003754970107600201
   Gerstner W., 2002, SPIKING NEURON MODEL
   Hamed HNA, 2009, 2009 INTERNATIONAL CONFERENCE OF SOFT COMPUTING AND PATTERN RECOGNITION, P695, DOI 10.1109/SoCPaR.2009.139
   KEEL (Knowledge Extraction based on Evolutionary Learning), 2005, IR PIM IND DIAB HEAR
   MAASS W, 1996, AUSTR C NEUR NETW
   Saleh A.Y., 2014, INT J ADV SOFT COMPU, V6
   Schliebs S, 2009, LECT NOTES COMPUT SC, V5506, P1229, DOI 10.1007/978-3-642-02490-0_149
   Theodossiou N., 2011, P 3 INT CEMEPE SECOT
   Thorpe S, 1998, COMPUTATIONAL NEUROSCIENCE: TRENDS IN RESEARCH, P113
   Wysoski SG, 2010, NEURAL NETWORKS, V23, P819, DOI 10.1016/j.neunet.2010.04.009
NR 14
TC 0
Z9 0
U1 0
U2 0
PY 2017
UT WOS:000428741200008
DA 2023-11-16
ER

PT J
AU Chakraborty, A
   Panda, S
   Chakrabarti, S
AF Chakraborty, Ayan
   Panda, Sashmita
   Chakrabarti, Saswat
TI Action Potential Parameters and Spiking Behavior of Cortical Neurons: A
   Statistical Analysis for Designing Spiking Neural Networks
SO IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS
DT Article
DE Action potential (AP); energy per spike; hypothesis testing; information
   gain; interspike interval (ISI); k-means clustering; Kullback-Leibler
   (KL) divergence; spiking frequency
ID FIRING FREQUENCY
AB Cortical neurons exhibit several spiking dynamics both in in-vivo and in-vitro experiments. Neural spikes or action potentials (APs) are also observed in various shapes and forms. Statistical correlation between AP parameters and associated spiking behavior of a neuron is discussed in this article. Three fundamental parameters: 1) width; 2) height; and 3) energy of an AP along with spiking frequency and interspike interval (ISI) are extracted for 91 human cortical neurons selected from Allen Institute for Brain Science (AIBS) database. It has been shown that neurons firing narrow, short, and low-energy APs have higher spiking frequency compared to the neurons with wide and taller APs. For a rise in excitation, it has been presented that information gain for neurons firing wider spikes is less compared to information gain for neurons firing narrow spikes. It has been shown that neurons with low spiking frequency and high spiking frequency dissipate energy of similar order for total spiking activity for similar excitation. Implications of the statistical inferences drawn are explained for a computational model of a spiking neuron. The effect of changing AP width on the overall dynamics of a spiking neural network is also highlighted. The key findings of this study will be useful for designing spiking neural networks for various cognitive applications.
C1 [Chakraborty, Ayan; Panda, Sashmita; Chakrabarti, Saswat] Indian Inst Technol Kharagpur, GS Sanyal Sch Telecommun, Kharagpur 721302, India.
   [Chakrabarti, Saswat] Indian Inst Technol Kharagpur, Fac Biotechnol & Biosci, Kharagpur 721302, India.
RP Chakraborty, A (corresponding author), Indian Inst Technol Kharagpur, GS Sanyal Sch Telecommun, Kharagpur 721302, India.
EM chakraborty.ayan.1991@gmail.com; sashmita@iitkgp.ac.in;
   saswat@ece.iitkgp.ac.in
CR Adonias GL, 2020, FRONT COMPUT NEUROSC, V14, DOI 10.3389/fncom.2020.556628
   Adonias GL, 2020, IEEE T NANOBIOSCI, V19, P224, DOI 10.1109/TNB.2020.2975942
   Aghababaiyan K, 2018, IEEE T NANOBIOSCI, V17, P78, DOI 10.1109/TNB.2018.2800899
   [Anonymous], 2016, ALL CELL TYP DAT
   Baranauskas G, 2003, NAT NEUROSCI, V6, P258, DOI 10.1038/nn1019
   Bean BP, 2007, NAT REV NEUROSCI, V8, P451, DOI 10.1038/nrn2148
   Berger T, 2010, IEEE T INFORM THEORY, V56, P852, DOI 10.1109/TIT.2009.2037089
   Boddum K, 2017, NEUROPHARMACOLOGY, V118, P102, DOI 10.1016/j.neuropharm.2017.02.024
   Burman I., 2021, P IEEE NAT C COMM NC, P1
   Chakraborty A, 2021, I IEEE EMBS C NEUR E, P77, DOI 10.1109/NER49283.2021.9441251
   Chakraborty A, 2021, I IEEE EMBS C NEUR E, P734, DOI 10.1109/NER49283.2021.9441230
   Chistiakova M, 2019, J NEUROSCI, V39, P6865, DOI 10.1523/JNEUROSCI.3039-18.2019
   Cook ND, 2008, NEUROSCIENCE, V153, P556, DOI 10.1016/j.neuroscience.2008.02.042
   Dayan P., 2001, THEORETICAL NEUROSCI
   Druckmann S, 2013, CEREB CORTEX, V23, P2994, DOI 10.1093/cercor/bhs290
   Ganguly C, 2020, IEEE T NEUR SYS REH, V28, P772, DOI 10.1109/TNSRE.2020.2975203
   Ghavami Siavash, 2018, IEEE Transactions on Molecular, Biological, and Multi-Scale Communications, V4, P221, DOI 10.1109/TMBMC.2019.2937291
   Goldental A, 2014, FRONT COMPUT NEUROSC, V8, DOI 10.3389/fncom.2014.00052
   Gouwens NW, 2019, NAT NEUROSCI, V22, P1182, DOI 10.1038/s41593-019-0417-0
   Hernáth F, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-49197-8
   Hore A., 2021, P IEEE ADV COMM TECH, P1
   Ikeda K, 2005, NEURAL COMPUT, V17, P2719, DOI 10.1162/089976605774320593
   Lin XH, 2023, IEEE T COGN DEV SYST, V15, P16, DOI 10.1109/TCDS.2021.3140115
   Mehta MR, 2002, NATURE, V417, P741, DOI 10.1038/nature00807
   Nowak LG, 2003, J NEUROPHYSIOL, V89, P1541, DOI 10.1152/jn.00580.2002
   Ramezani H, 2018, IEEE T NANOBIOSCI, V17, P260, DOI 10.1109/TNB.2018.2838056
   Sun SH, 2021, J PHYSIOL-LONDON, V599, P2211, DOI 10.1113/JP280844
   Tateno T, 2004, J NEUROPHYSIOL, V92, P2283, DOI 10.1152/jn.00109.2004
   Veletic M, 2019, P IEEE, V107, P1425, DOI 10.1109/JPROC.2019.2915199
   Wijesinghe P, 2018, IEEE TETCI, V2, P345, DOI 10.1109/TETCI.2018.2829924
   Wu JB, 2023, IEEE T NEUR NET LEAR, V34, P446, DOI 10.1109/TNNLS.2021.3095724
NR 31
TC 0
Z9 0
U1 2
U2 2
PD JUN
PY 2023
VL 15
IS 2
BP 808
EP 818
DI 10.1109/TCDS.2022.3185028
UT WOS:001005746000043
DA 2023-11-16
ER

PT J
AU Bousoulas, P
   Tsioustas, C
   Hadfield, J
   Aslanidis, V
   Limberopoulos, S
   Tsoukalas, D
AF Bousoulas, P.
   Tsioustas, C.
   Hadfield, J.
   Aslanidis, V
   Limberopoulos, S.
   Tsoukalas, D.
TI Low Power Stochastic Neurons From SiO<sub>2</sub>-Based Bilayer
   Conductive Bridge Memristors for Probabilistic Spiking Neural Network
   Applications-Part II: Modeling
SO IEEE TRANSACTIONS ON ELECTRON DEVICES
DT Article
DE Biological neuron; numerical modeling; spiking neural networks;
   stochastic firing
ID NANOPARTICLES; SIZE; TEMPERATURE; SYNAPSES; FILMS
AB A deep understanding of the underlying resistive switching mechanism for the implementation of volatile memristive properties is regarded as of great importance for enhancing their performance. Along these lines, a 2-D dynamical model is introduced to interpret the whole memristive pattern within the bilayer configuration, as well as the crucial of the dense layer of the Pt nanoparticles (NPs) on the local thermal distribution. Moreover, the probabilistic leaky-integrate-and-fire (LIF) neuron properties were simulated by considering a simple RC circuit in order to perform Bayesian extrapolation within a spiking neural network. A classification application is consequently demonstrated by using the liver tumor dataset. The advantageous capabilities of the stochastic-based spike neural networks (SNNs) are highlighted in striking contrast with the conventional artificial neural networks (ANNs), as well as the deterministic-based SNNs, in terms of prediction accuracy and power consumption.
C1 [Bousoulas, P.; Tsioustas, C.; Hadfield, J.; Aslanidis, V; Limberopoulos, S.; Tsoukalas, D.] Natl Tech Univ Athens, Sch Appl Math & Phys Sci, Athens 15780, Greece.
RP Bousoulas, P (corresponding author), Natl Tech Univ Athens, Sch Appl Math & Phys Sci, Athens 15780, Greece.
EM panbous@mail.ntua.gr
CR [Anonymous], 2011, INT J DATABASE MANAG, DOI DOI 10.5121/IJDMS.2011.3207
   Asoro MA, 2009, MICROSC MICROANAL, V15, P706, DOI 10.1017/S1431927609097013
   Avramescu ML, 2017, ENVIRON SCI POLLUT R, V24, P1553, DOI 10.1007/s11356-016-7932-2
   Azouz R, 2000, P NATL ACAD SCI USA, V97, P8110, DOI 10.1073/pnas.130200797
   Begoli E, 2019, NAT MACH INTELL, V1, P20, DOI 10.1038/s42256-018-0004-1
   Bousoulas P, 2021, APPL PHYS LETT, V118, DOI 10.1063/5.0044647
   Bousoulas P, 2016, J APPL PHYS, V120, DOI 10.1063/1.4964872
   Bousoulas P., LOW POWER STOCHAST 1
   Bousoulas P, 2020, NANOTECHNOLOGY, V31, DOI 10.1088/1361-6528/aba3a1
   Cai FX, 2019, NAT ELECTRON, V2, P290, DOI 10.1038/s41928-019-0270-x
   Cannon RC, 2010, PLOS COMPUT BIOL, V6, DOI 10.1371/journal.pcbi.1000886
   Carneiro JO, 2019, COATINGS, V9, DOI 10.3390/coatings9080468
   Chang SH, 2008, APPL PHYS LETT, V92, DOI 10.1063/1.2924304
   Choi S, 2017, NANO LETT, V17, P3113, DOI 10.1021/acs.nanolett.7b00552
   CORDERO GG, 2016, SEMICOND SCI TECH, V31
   Ham D, 2021, NAT ELECTRON, V4, P635, DOI 10.1038/s41928-021-00646-1
   Huang HM, 2020, ADV INTELL SYST-GER, V2, DOI 10.1002/aisy.202000149
   Isacoff EY, 2013, NEURON, V80, P658, DOI 10.1016/j.neuron.2013.10.040
   Lacy F, 2009, IEEE SENS J, V9, P1111, DOI 10.1109/JSEN.2009.2026514
   Li HT, 2017, IEEE T CIRCUITS-I, V64, P2263, DOI 10.1109/TCSI.2017.2709812
   Li JX, 2020, MATER HORIZ, V7, P71, DOI 10.1039/c9mh01206k
   Luo WH, 2008, J PHYS CHEM C, V112, P2359, DOI 10.1021/jp0770155
   Morrison A, 2008, BIOL CYBERN, V98, P459, DOI 10.1007/s00422-008-0233-1
   Nanda KK, 2002, PHYS REV A, V66, DOI 10.1103/PhysRevA.66.013208
   Naundorf B, 2006, NATURE, V440, P1060, DOI 10.1038/nature04610
   Nessler B, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003037
   Papakonstantinopoulos C, 2021, ACS APPL ELECTRON MA, V3, P2729, DOI 10.1021/acsaelm.1c00302
   Pouget A, 2013, NAT NEUROSCI, V16, P1170, DOI 10.1038/nn.3495
   RAVINDRA NM, 1986, J APPL PHYS, V60, P1139, DOI 10.1063/1.337358
   Sakellaropoulos D, 2020, IEEE ELECTR DEVICE L, V41, P1013, DOI 10.1109/LED.2020.2997565
   Serb A, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms12611
   Sheny DS, 2013, SPECTROCHIM ACTA A, V114, P267, DOI 10.1016/j.saa.2013.05.028
   Sun HT, 2014, ADV FUNCT MATER, V24, P5679, DOI 10.1002/adfm.201401304
   Yang YC, 2012, NAT COMMUN, V3, DOI 10.1038/ncomms1737
   Zhao Y, 2018, IEEE T ELECTRON DEV, V65, P4290, DOI 10.1109/TED.2018.2865225
NR 35
TC 5
Z9 5
U1 1
U2 12
PD MAY
PY 2022
VL 69
IS 5
BP 2368
EP 2376
DI 10.1109/TED.2022.3160140
EA MAR 2022
UT WOS:000777275400001
DA 2023-11-16
ER

PT J
AU Gelen, AG
   Atasoy, A
AF Gelen, Aykut Gorkem
   Atasoy, Ayten
TI SPAYK: An environment for spiking neural network simulation
SO TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES
DT Article
DE Spiking neural network; STDP based learning; supervised classification;
   unsupervised pattern recognition
AB In research areas such as mobile robotics and computer vision, energy and computational efficiency have become critical. This has greatly increased interest in high-efficiency neuromorphic hardware and spiking neural networks. Because neuromorphic hardware is not yet widely available, spiking neural network studies are conducted by simulations. There are numerous simulators available today, each designed for a specific purpose. In this paper, a novel and open -source package (SPAYK) for simulating spiking neural networks is presented. SPAYK has been proposed to speed up spiking neural network research. In the majority of simulators, networks are expressed with differential equations and require advanced neuroscience knowledge since such simulators are generally designed for brain and neuroscience research. SPAYK, on the other hand, is specifically designed as a framework to easily design spiking neural networks for practical problems. SPAYK is an easy-to-use Python package. There are three fundamental classes in the core: the model class for creating neuron groups, the organization class for simulating tissues, and the learning class for synaptic plasticity. While developing and testing the SPAYK environment, various experiments were carried out. This study includes three of these experiments. In the first experiment, we investigated the behavior of a group of Izhikevich neurons for visual stimuli. Also, a single Izhikevich neuron has been trained to respond to a particular label in a supervised manner with synaptic plasticity. In the second experiment, a well-known experiment was repeated to validate SPAYK. In this experiment, a neuron trained by synaptic plasticity can recognize repetitive patterns in a spike train. In the third experiment, a similar neuron was simulated with stimuli with multiple labels adapted from the MNIST dataset. It has been shown that the neuron can classify a particular label by synaptic plasticity. All these experiments and the SPAYK environment are presented as open-source tools.
C1 [Gelen, Aykut Gorkem] Erzincan Binali Yildirim Univ, Dept Elect & Elect Engn, Erzincan, Turkiye.
   [Atasoy, Ayten] Karadeniz Tech Univ, Dept Elect & Elect Engn, Trabzon, Turkiye.
RP Gelen, AG (corresponding author), Erzincan Binali Yildirim Univ, Dept Elect & Elect Engn, Erzincan, Turkiye.
EM aykut.gelen@erzincan.edu.tr
CR [Anonymous], 2002, NEURAL ENG COMPUTATI
   Bekolay T, 2014, FRONT NEUROINFORM, V7, DOI 10.3389/fninf.2013.00048
   Cao ZQ, 2015, NEURAL COMPUT APPL, V26, P1839, DOI 10.1007/s00521-015-1848-5
   Cheng X, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1519
   Choudhary T, 2018, INT CONF INTEL INFOR, P1
   Fang WAC, SPIKINGJELLY
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   Gerstner W, 2018, SCHOLARPEDIA, V3, P1343
   Goodman Dan, 2008, Front Neuroinform, V2, P5, DOI 10.3389/neuro.11.005.2008
   HEBB D. O., 1949
   Hines ML, 2001, NEUROSCIENTIST, V7, P123, DOI 10.1177/107385840100700207
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Kim S, 2020, AAAI CONF ARTIF INTE, V34, P11270
   Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508
   Li XM, 2020, NEURAL PLAST, V2020, DOI 10.1155/2020/8851351
   Liu JX, 2018, PROCEEDINGS OF 2018 10TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND COMPUTING (ICMLC 2018), P230, DOI 10.1145/3195106.3195115
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Marc-Oliver G., 2007, SCHOLARPEDIA, DOI [DOI 10.4249/SCHOLARPEDIA.1430, 10.4249/scholarpedia.1430]
   Masquelier T, 2008, PLOS ONE, V3, DOI 10.1371/journal.pone.0001377
   Mozafari M, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00625
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Skocik MJ, 2014, IEEE T NEUR NET LEAR, V25, P1474, DOI 10.1109/TNNLS.2013.2294016
   Tikidji-Hamburyan RA, 2017, FRONT NEUROINFORM, V11, DOI 10.3389/fninf.2017.00046
   Vitay J, 2015, FRONT NEUROINFORM, V9, DOI 10.3389/fninf.2015.00019
   Wang XW, 2020, NEURAL NETWORKS, V125, P258, DOI 10.1016/j.neunet.2020.02.011
   Wilson MA, 1989, GENESIS SYSTEM SIMUL, P485
   Wu JB, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00836
   Yan ZL, 2021, BIOMED SIGNAL PROCES, V63, DOI 10.1016/j.bspc.2020.102170
NR 29
TC 0
Z9 0
U1 5
U2 6
PY 2023
VL 31
IS 2
BP 462
EP 480
DI 10.55730/1300-0632.3995
UT WOS:000957927500014
DA 2023-11-16
ER

PT C
AU Liu, QH
   Xing, D
   Feng, L
   Tang, HJ
   Pan, G
AF Liu, Qianhui
   Xing, Dong
   Feng, Lang
   Tang, Huajin
   Pan, Gang
GP IEEE
TI EVENT-BASED MULTIMODAL SPIKING NEURAL NETWORK WITH ATTENTION MECHANISM
SO 2022 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)
SE International Conference on Acoustics Speech and Signal Processing
   ICASSP
DT Proceedings Paper
CT 47th IEEE International Conference on Acoustics, Speech and Signal
   Processing (ICASSP)
CY MAY 22-27, 2022
CL Singapore, SINGAPORE
DE spiking neural networks; multimodal learning; dynamic vision sensors;
   dynamic audio sensors
AB Human brain can effectively integrate visual and auditory information. Dynamic Vision Sensor (DVS) and Dynamic Audio Sensor (DAS) are event-based sensors imitating the mechanism of human retina and cochlea. Since the sensors record the visual and auditory input as asynchronous discrete events, they are inherently suitable to cooperate with the spiking neural network (SNN). Existing works of SNNs for processing events mainly focus on unimodality, however, audiovisual multimodal SNNs are still limited. In this paper, we propose an end-to-end event-based multimodal spiking neural network. The network consists of visual and auditory uni-modal subnetworks and a novel attention-based cross-modal subnetwork for fusion. The attention mechanism measures the significance of each modality and allocates the weights to two modalities. We evaluate our proposed multimodal network on an event-based audiovisual joint dataset (MNIST-DVS and N-TIDIGITS datasets). Experimental results show the performance improvement of this multimodal network and the effectiveness of our proposed attention mechanism.
C1 [Liu, Qianhui; Xing, Dong; Feng, Lang; Tang, Huajin; Pan, Gang] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou, Peoples R China.
   [Liu, Qianhui] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore, Singapore.
   [Tang, Huajin; Pan, Gang] Zhejiang Lab, Hangzhou, Peoples R China.
RP Pan, G (corresponding author), Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou, Peoples R China.; Pan, G (corresponding author), Zhejiang Lab, Hangzhou, Peoples R China.
EM qianhuiliu@zju.edu.cn; dongxing@zju.edu.cn; langfeng@zju.edu.cn;
   htang@zju.edu.cn; gpan@zju.edu.cn
CR Anumula J, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00023
   Chen Zehao, 2021, P IEEECVF C COMPUTER, P14760
   Gu PJ, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1366
   Jin Y., 2018, ADV NEURAL INFORM PR, P7005, DOI DOI 10.48550/ARXIV.1805.07866
   Kawashima R, 1999, NEUROIMAGE, V10, P209, DOI 10.1006/nimg.1999.0452
   Li Xiaoya, 2019, ISCAS, P1
   Lichtsteiner P, 2008, IEEE J SOLID-ST CIRC, V43, P566, DOI 10.1109/JSSC.2007.914337
   Liu QH, 2020, AAAI CONF ARTIF INTE, V34, P1308
   Liu QH, 2020, IEEE T NEUR NET LEAR, V31, P5300, DOI 10.1109/TNNLS.2020.2966058
   Liu Qianhui, 2021, IJCAI, P1743
   Liu SC, 2014, IEEE T BIOMED CIRC S, V8, P453, DOI 10.1109/TBCAS.2013.2281834
   Orchard G, 2015, IEEE T PATTERN ANAL, V37, P2028, DOI 10.1109/TPAMI.2015.2392947
   Rathi N, 2021, IEEE TETCI, V5, P143, DOI 10.1109/TETCI.2018.2872014
   Roy K, 2019, NATURE, V575, P607, DOI 10.1038/s41586-019-1677-2
   Shrestha SB., 2018, ADV NEURAL INFORM PR, V31, P1412
   Wang HD, 2021, 2021 THE 6TH INTERNATIONAL CONFERENCE ON INTEGRATED CIRCUITS AND MICROSYSTEMS (ICICM 2021), P430, DOI 10.1109/ICICM54364.2021.9660297
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Xiao R, 2020, IEEE T NEUR NET LEAR, V31, P3649, DOI 10.1109/TNNLS.2019.2945630
   Zhang ML, 2020, IEEE J-STSP, V14, P592, DOI 10.1109/JSTSP.2020.2983547
   Zhang ML, 2019, IEEE T NEUR NET LEAR, V30, P123, DOI 10.1109/TNNLS.2018.2833077
   Zhang Malu, 2021, IEEE T NEURAL NETWOR
   Zhang WR, 2019, ADV NEUR IN, V32
NR 22
TC 2
Z9 2
U1 0
U2 8
PY 2022
BP 8922
EP 8926
DI 10.1109/ICASSP43922.2022.9746865
UT WOS:000864187909047
DA 2023-11-16
ER

PT J
AU Siddique, NH
   McDaid, LJ
   Kasabov, N
   Widrow, B
AF Siddique, N. H.
   McDaid, L. J.
   Kasabov, N.
   Widrow, B.
TI Special Issue: Spiking Neural Networks INTRODUCTION
SO INTERNATIONAL JOURNAL OF NEURAL SYSTEMS
DT Editorial Material
C1 [Siddique, N. H.; McDaid, L. J.] Univ Ulster, Coleraine BT52 1SA, Londonderry, North Ireland.
   [Kasabov, N.] Auckland Univ Technol, Auckland, New Zealand.
   [Widrow, B.] Stanford Univ, Stanford, CA 94305 USA.
RP Siddique, NH (corresponding author), Univ Ulster, Coleraine BT52 1SA, Londonderry, North Ireland.
CR Christodoulou C, 2002, NEURAL NETWORKS, V15, P891, DOI 10.1016/S0893-6080(02)00034-5
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Halassa MM, 2010, ANNU REV PHYSIOL, V72, P335, DOI 10.1146/annurev-physiol-021909-135843
   Henneberger C, 2010, NATURE, V463, P232, DOI 10.1038/nature08673
   Iglesias J, 2008, INT J NEURAL SYST, V18, P267, DOI 10.1142/S0129065708001580
   Kasinski A., 2006, International Journal of Applied Mathematics and Computer Science, V16, P101
   Perea G, 2009, TRENDS NEUROSCI, V32, P421, DOI 10.1016/j.tins.2009.05.001
   Rossello JL, 2009, INT J NEURAL SYST, V19, P465, DOI 10.1142/S0129065709002166
NR 9
TC 1
Z9 1
U1 0
U2 8
PD DEC
PY 2010
VL 20
IS 6
BP V
EP VII
DI 10.1142/S0129065710002590
UT WOS:000284647300001
DA 2023-11-16
ER

PT C
AU Meftah, B
   Benyettou, A
   Lezoray, O
   QingXiang, W
AF Meftah, B.
   Benyettou, A.
   Lezoray, O.
   QingXiang, W.
GP IEEE
TI Image Clustering with Spiking Neuron Network
SO 2008 IEEE INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS, VOLS 1-8
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks
CY JUN 01-08, 2008
CL Hong Kong, PEOPLES R CHINA
AB The process of segmenting images is one of the most critical ones in automatic image analysis whose goal can be regarded as to find what objects are presented in images. Artificial neural networks have been well developed. First two generations of neural networks have a lot of successful applications. Spiking Neuron Networks (SNNs) are often referred to as the 3(rd) generation of neural networks which have potential to solve problems related to biological stimuli. They derive their strength and interest from an accurate modeling of synaptic interactions between neurons, taking into account the time of spike emission. SNNs overcome the computational power of neural networks made of threshold or sigmoidal units. Moreover, SNNs add a new dimension, the temporal axis, to the representation capacity and the processing abilities of neural networks. In this paper, we present how SNN can be applied with efficacy in image segmentation.
C1 [Meftah, B.] Ctr Univ Mustapha Stambouli, Equipe EDTEC LRSBG, Mascara, Algeria.
   [Benyettou, A.] Univ Mohamed Boudiaf, Lab Signal Image & Parole SIMPA, Oran, Algeria.
   [Lezoray, O.] Univ Caen, CNRS, GREYC, UMR 6072, F-14050 Caen, France.
   [QingXiang, W.] Univ Ulster Magee Derry, Sch Comp & Intelligent Syst, Londonderry BT48 7JL, North Ireland.
RP Meftah, B (corresponding author), Ctr Univ Mustapha Stambouli, Equipe EDTEC LRSBG, Mascara, Algeria.
CR [Anonymous], 1988, CAMBRIDGE STUD MATH
   Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   BRAGA TBL, 2000, ARTIFICIAL NEURAL NE
   Dayan P., 2004, THEORETICAL NEUROSCI
   Gerstner W., 2002, SPIKING NEURON MODEL
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Maass W, 2001, THEOR COMPUT SCI, V261, P157, DOI 10.1016/S0304-3975(00)00137-7
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Meurie C, 2005, INT J ROBOT AUTOM, V20, P63, DOI 10.2316/Journal.206.2005.2.206-2780
   OSTER M, 2004, P 11 IEEE INT C EL C, V11, P203
   Paugam-Moisy H, 2006, SPIKING NEURON NETWO
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   Zhang YJ, 2001, ISSPA 2001: SIXTH INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND ITS APPLICATIONS, VOLS 1 AND 2, PROCEEDINGS, P148, DOI 10.1109/ISSPA.2001.949797
NR 13
TC 5
Z9 6
U1 0
U2 3
PY 2008
BP 681
EP 685
DI 10.1109/IJCNN.2008.4633868
UT WOS:000263827200111
DA 2023-11-16
ER

PT C
AU Hori, S
   Zapata, M
   Madrenas, J
   Morie, T
   Tamukoh, H
AF Hori, Sansei
   Zapata, Mireya
   Madrenas, Jordi
   Morie, Takashi
   Tamukoh, Hakaru
BE Lintas, A
   Rovetta, S
   Verschure, PFMJ
   Villa, AEP
TI An Implementation of a Spiking Neural Network Using Digital Spiking
   Silicon Neuron Model on a SIMD Processor
SO ARTIFICIAL NEURAL NETWORKS AND MACHINE LEARNING - ICANN 2017, PT I
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 26th International Conference on Artificial Neural Networks (ICANN)
CY SEP 11-14, 2017
CL Alghero, ITALY
DE SNN; DSSN; SIMD processor; FPGA
C1 [Hori, Sansei; Morie, Takashi; Tamukoh, Hakaru] Kyushu Inst Technol, Grad Sch Life Sci & Syst Engn, Kitakyushu, Fukuoka, Japan.
   [Zapata, Mireya; Madrenas, Jordi] Univ Politecn Cataluna, Dept Elect Engn, Barcelona, Spain.
RP Hori, S (corresponding author), Kyushu Inst Technol, Grad Sch Life Sci & Syst Engn, Kitakyushu, Fukuoka, Japan.
EM hori-sansei@edu.brain.kyutech.ac.jp; mireya.zapata@upc.edu;
   jordi.madrenas@upc.edu; morie@brain.kyutech.ac.jp;
   tamukoh@brain.kyutech.ac.jp
CR Li J, 2012, FRONT NEUROSCI-SWITZ, V6, DOI 10.3389/fnins.2012.00183
   Zapata M, 2016, LECT NOTES COMPUT SC, V9886, P365, DOI 10.1007/978-3-319-44778-0_43
NR 2
TC 1
Z9 1
U1 0
U2 0
PY 2017
VL 10613
BP 437
EP 438
PN I
UT WOS:000449802500062
DA 2023-11-16
ER

PT C
AU Kuroe, Y
   Iima, H
   Maeda, Y
AF Kuroe, Yasuaki
   Iima, Hitoshi
   Maeda, Yutaka
BE Merelo, JJ
   Garibaldi, J
   Barranco, AL
   Madani, K
   Warwick, K
TI Learning Method of Recurrent Spiking Neural Networks to Realize Various
   Firing Patterns using Particle Swarm Optimization
SO IJCCI: PROCEEDINGS OF THE 11TH INTERNATIONAL JOINT CONFERENCE ON
   COMPUTATIONAL INTELLIGENCE
DT Proceedings Paper
CT 11th International Joint Conference on Computational Intelligence
   (IJCCI)
CY SEP 17-19, 2019
CL Vienna, AUSTRIA
DE Spiking Neural Network; Learning Method; Particle Swarm Optimization;
   Burst Firing; Periodic Firing
AB Recently it has been reported that artificial spiking neural networks (SNNs) are computationally more powerful than the conventional neural networks. In biological neural networks of living organisms, various firing patterns of nerve cells have been observed, typical examples of which are burst firings and periodic firings. In this paper we propose a learning method which can realize various firing patterns for recurrent SNNs (RSSNs). We have already proposed learning methods of RSNNs in which the learning problem is formulated such that the number of spikes emitted by a neuron and their firing instants coincide with given desired ones. In this paper, in addition to that, we consider several desired properties of a target RSNN and proposes cost functions for realizing them. Since the proposed cost functions are not differentiable with respect to the learning parameters, we propose a learning method based on the particle swarm optimization.
C1 [Kuroe, Yasuaki; Maeda, Yutaka] Kansai Univ, Fac Engn Sci, Suita, Osaka, Japan.
   [Kuroe, Yasuaki; Iima, Hitoshi] Kyoto Inst Technol, Fac Informat & Human Sci, Kyoto, Japan.
RP Kuroe, Y (corresponding author), Kansai Univ, Fac Engn Sci, Suita, Osaka, Japan.; Kuroe, Y (corresponding author), Kyoto Inst Technol, Fac Informat & Human Sci, Kyoto, Japan.
CR [Anonymous], 1988, PARALLEL DISTRIBUTED
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   GERSTNER W, 1993, BIOL CYBERN, V69, P503, DOI 10.1007/BF01185422
   GERSTNER W, 1993, ADV NEURAL INFORM PR, V6, P363
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2007, DYNAMICAL SYSTEMS NE
   Kennedy J., 1995, SWARM INTELL-US, V4, P1942
   Kuroe Y., 2010, P INT JOINT C NEUR N, P2561
   Kuroe Y., 1992, ISCIE CONTROL INFORM, V36, P634
   Lee JM, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00191
   Maass W, 1997, NEURAL COMPUT, V9, P279, DOI 10.1162/neco.1997.9.2.279
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Mass W, 1996, COMPUTATIONAL COMPLE, V7
   MASS W, 1998, PULSED NEURAL NETS
   Rumelhart D.E., 1986, PARALLEL DISTRIBUTED, V1, DOI DOI 10.7551/MITPRESS/5236.001.0001
   Selvaratnam K., 2000, Transactions of the Institute of Systems, Control and Information Engineers, V13, P95, DOI 10.5687/iscie.13.3_95
NR 16
TC 0
Z9 0
U1 0
U2 0
PY 2019
BP 479
EP 486
DI 10.5220/0008164704790486
UT WOS:000571773900052
DA 2023-11-16
ER

PT C
AU Zhan, GG
   Song, ZT
   Fang, T
   Zhang, Y
   Le, S
   Zhang, XZ
   Wang, SY
   Lin, YF
   Jia, J
   Zhang, LH
   Kang, XY
AF Zhan, Gege
   Song, Zuoting
   Fang, Tao
   Zhang, Yuan
   Le, Song
   Zhang, Xueze
   Wang, Shouyan
   Lin, Yifang
   Jia, Jie
   Zhang, Lihua
   Kang, Xiaoyang
GP IEEE
TI Applications of Spiking Neural Network in Brain Computer Interface
SO 2021 9TH IEEE INTERNATIONAL WINTER CONFERENCE ON BRAIN-COMPUTER
   INTERFACE (BCI)
SE International Winter Workshop on Brain-Computer Interface
DT Proceedings Paper
CT 9th IEEE International Winter Conference on Brain-Computer Interface
   (BCI)
CY FEB 22-24, 2021
CL Korea Univ Inst Artificial Intelligence, ELECTR NETWORK
HO Korea Univ Inst Artificial Intelligence
DE Spiking neural network (SNN); BCI; EEG; Brain Disease; Motor Imagery
ID EEG DATA; CLASSIFICATION; RECOGNITION; METHODOLOGY; NEUCUBE; PATTERN;
   MODELS
AB Spiking neural network (SNN) is regarded as the third generation of the artificial neural network, which takes biologically plausible spiking neurons as the basic computing unit. Due to its ability to capture the rich dynamics of biological neurons and to represent and integrate different information dimensions, such as time, frequency and phase, SNN provides a powerful tool for modeling complex information processing in the brain. EEG can help us better understand brain activity and structure, and shows great potential in implementing Brain-Computer Interface (BCI). In this review, we mainly summarize the application of the SNN model in EEG signal processing. These applications are grouped into four categories, each of which is further explored using examples from previous studies.
C1 [Zhan, Gege; Song, Zuoting; Fang, Tao; Zhang, Yuan; Le, Song; Zhang, Xueze; Zhang, Lihua; Kang, Xiaoyang] Fudan Univ, Acad Engn & Technol,Minist Educ, Engn Res Ctr AI & Robot,MOE Frontiers Ctr Brain S, Shanghai Engn Res Ctr AI & Robot,Inst AI & Robot, Shanghai, Peoples R China.
   [Wang, Shouyan] Fudan Univ, Inst Sci & Technol Brain Inspired Intelligence, Shanghai, Peoples R China.
   [Lin, Yifang; Jia, Jie] Fudan Univ, Huashan Hosp, Dept Rehabil Med, Shanghai, Peoples R China.
   [Zhang, Lihua; Kang, Xiaoyang] Ji Hua Lab, Foshan, Guangdong, Peoples R China.
   [Kang, Xiaoyang] Res Ctr Intelligent Sensing, Zhejiang Lab, Hangzhou 311100, Peoples R China.
RP Kang, XY (corresponding author), Fudan Univ, Acad Engn & Technol,Minist Educ, Engn Res Ctr AI & Robot,MOE Frontiers Ctr Brain S, Shanghai Engn Res Ctr AI & Robot,Inst AI & Robot, Shanghai, Peoples R China.; Kang, XY (corresponding author), Ji Hua Lab, Foshan, Guangdong, Peoples R China.; Kang, XY (corresponding author), Res Ctr Intelligent Sensing, Zhejiang Lab, Hangzhou 311100, Peoples R China.
EM xiaoyang_kang@fudan.edu.cn
CR Behrenbeck J, 2019, J NEURAL ENG, V16, DOI 10.1088/1741-2552/aafabc
   Beyeler M, 2013, NEURAL NETWORKS, V48, P109, DOI 10.1016/j.neunet.2013.07.012
   Capecci E, 2016, IEEE IJCNN, P1360, DOI 10.1109/IJCNN.2016.7727356
   Capecci E, 2015, NEURAL NETWORKS, V68, P62, DOI 10.1016/j.neunet.2015.03.009
   Carino-Escobar R, 2016, LECT NOTES COMPUT SC, V9713, P245, DOI 10.1007/978-3-319-41009-8_26
   Doborjeh MG, 2018, EVOL SYST-GER, V9, P195, DOI 10.1007/s12530-017-9178-8
   Doborjeh MG, 2016, IEEE IJCNN, P1373, DOI 10.1109/IJCNN.2016.7727358
   Doborjeh MG, 2016, IEEE T BIO-MED ENG, V63, P1830, DOI 10.1109/TBME.2015.2503400
   Doborjeh ZG, 2018, COGN COMPUT, V10, P35, DOI 10.1007/s12559-017-9517-x
   Gerstner W., 2002, SPIKING NEURON MODEL
   Ghosh-Dastidar S, 2007, INTEGR COMPUT-AID E, V14, P187
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Goel P, 2006, LECT NOTES ARTIF INT, V4251, P825
   Goel P, 2008, INT J KNOWL-BASED IN, V12, P295, DOI 10.3233/KES-2008-12404
   Gütig R, 2014, CURR OPIN NEUROBIOL, V25, P134, DOI 10.1016/j.conb.2014.01.004
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Kasabov N. K., 2019, SPRINGER SERIES BIO, P169
   Kasabov N, 2015, INFORM SCIENCES, V294, P565, DOI 10.1016/j.ins.2014.06.028
   Kasabov NK, 2014, NEURAL NETWORKS, V52, P62, DOI 10.1016/j.neunet.2014.01.006
   Kawano H, 2016, LECT NOTES COMPUT SC, V9950, P221, DOI 10.1007/978-3-319-46681-1_27
   Kulkarni SR, 2018, NEURAL NETWORKS, V103, P118, DOI 10.1016/j.neunet.2018.03.019
   Lin Xiang-hong, 2009, Acta Electronica Sinica, V37, P1270
   Luo Y., 2020, ENVIRON SCI POLLUT R, VPP, P1, DOI DOI 10.1007/s11356-020-08864-4
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Mashford BS, 2017, IBM J RES DEV, V61, DOI 10.1147/JRD.2017.2663978
   Niranjani AN, 2017, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON INTELLIGENT SUSTAINABLE SYSTEMS (ICISS 2017), P901, DOI 10.1109/ISS1.2017.8389309
   Nuntalid N, 2011, LECT NOTES COMPUT SC, V7062, P451, DOI 10.1007/978-3-642-24955-6_54
   Tan C, 2020, NEURAL PROCESS LETT, V52, P1675, DOI 10.1007/s11063-020-10322-8
   Virgilio GCD, 2019, LECT NOTES COMPUT SC, V11524, P14, DOI 10.1007/978-3-030-21077-9_2
   Virgilio CD, 2020, NEURAL NETWORKS, V122, P130, DOI 10.1016/j.neunet.2019.09.037
   Wang HT, 2020, IEEE ACCESS, V8, P86850, DOI 10.1109/ACCESS.2020.2992631
   Wang XW, 2020, NEURAL NETWORKS, V125, P258, DOI 10.1016/j.neunet.2020.02.011
NR 32
TC 3
Z9 3
U1 3
U2 16
PY 2021
BP 288
EP 293
DI 10.1109/BCI51272.2021.9385361
UT WOS:000669665700062
DA 2023-11-16
ER

PT J
AU Pande, S
   Morgan, F
   Smit, G
   Bruintjes, T
   Rutgers, J
   McGinley, B
   Cawley, S
   Harkin, J
   McDaid, L
AF Pande, Sandeep
   Morgan, Fearghal
   Smit, Gerard
   Bruintjes, Tom
   Rutgers, Jochem
   McGinley, Brian
   Cawley, Seamus
   Harkin, Jim
   McDaid, Liam
TI Fixed latency on-chip interconnect for hardware spiking neural network
   architectures
SO PARALLEL COMPUTING
DT Article
DE Network on Chip (NoC); Spiking Neural Networks (SNN); Synaptic
   connectivity; Latency jitter
ID NEURONS; PLATFORM; DESIGN
AB Information in a Spiking Neural Network (SNN) is encoded as the relative timing between spikes. Distortion in spike timings can impact the accuracy of SNN operation by modifying the precise firing time of neurons within the SNN. Maintaining the integrity of spike timings is crucial for reliable operation of SNN applications. A packet switched Network on Chip (NoC) infrastructure offers scalable connectivity for spike communication in hardware SNN architectures. However, shared resources in NoC architectures can result in unwanted variation in spike packet transfer latency. This packet latency jitter distorts the timing information conveyed on the synaptic connections in the SNN, resulting in unreliable application behaviour.
   This paper presents a SystemC simulation based analysis of the synaptic information distortion in NoC based hardware SNNs. The paper proposes a fixed spike transfer latency ring topology interconnect for spike communication between neural tiles, using a novel time-stamped spike broadcast flow control scheme. The proposed architectural technique is evaluated using spike rates employed in previously reported mesh topology NoC based hardware SNN applications, which exhibited spike latency jitter over NoC paths. Results indicate that the proposed interconnect offers fixed spike transfer latency and eliminates the associated information distortion.
   The paper presents the micro-architecture of the proposed ring router. The FPGA validated ring interconnect architecture has been synthesised using 65 nm low-power CMOS technology. Silicon area comparisons for various ring sizes are presented. Scalability of the proposed architecture has been addressed by employing a hierarchical NoC architecture. (C) 2013 Elsevier B.V. All rights reserved.
C1 [Pande, Sandeep; Morgan, Fearghal; McGinley, Brian; Cawley, Seamus] Natl Univ Ireland, Galway, Ireland.
   [Smit, Gerard; Bruintjes, Tom; Rutgers, Jochem] Univ Twente, NL-7500 AE Enschede, Netherlands.
RP Pande, S (corresponding author), Natl Univ Ireland, Galway, Ireland.
EM sandeep.pande@nuigalway.ie; fearghal.morgan@nuigalway.ie;
   g.j.m.smit@utwente.nl; t.m.bruintje-s@utwente.nl;
   j.h.rutgers@utwente.nl; brian.mcginley@nuigalway.ie;
   s.cawley6@nuigalway.ie; jg.harkin@ulster.ac.uk; lj.mcdaid@ulster.ac.uk
CR [Anonymous], ACM COMPUTING SURVEY
   [Anonymous], 1999, NEURAL NETWORKS COMP
   Auda G, 1999, Int J Neural Syst, V9, P129, DOI 10.1142/S0129065799000125
   Benini L, 2001, ISSS'01: 14TH INTERNATIONAL SYMPOSIUM ON SYSTEM SYNTHESIS, P33, DOI 10.1109/ISSS.2001.957909
   Benini L, 2002, COMPUTER, V35, P70, DOI 10.1109/2.976921
   Bohte SM, 2005, INFORM PROCESS LETT, V95, P519, DOI 10.1016/j.ipl.2005.05.018
   Bolotin E, 2004, J SYST ARCHITECT, V50, P105, DOI 10.1016/j.sysarc.2003.07.004
   Cawley S, 2011, GENET PROGRAM EVOL M, V12, P257, DOI 10.1007/s10710-011-9130-9
   Dall'Osso M, 2003, PR IEEE COMP DESIGN, P536, DOI 10.1109/ICCD.2003.1240952
   Dally W., 2003, PRINCIPLES PRACTICES
   Ehrlich M, 2007, P INT C SENS CIRC IN
   Emery R, 2009, 2009 3RD ACM/IEEE INTERNATIONAL SYMPOSIUM ON NETWORKS-ON-CHIP, P144, DOI 10.1109/NOCS.2009.5071462
   Furber S, 2009, NINTH INTERNATIONAL CONFERENCE ON APPLICATION OF CONCURRENCY TO SYSTEM DESIGN, PROCEEDINGS, P3, DOI 10.1109/ACSD.2009.17
   Gerstner W, 1997, P NATL ACAD SCI USA, V94, P12740, DOI 10.1073/pnas.94.24.12740
   Gerstner W., 2002, SPIKING NEURON MODEL
   Glackin B, 2005, LECT NOTES COMPUTER, V3512, P1
   Goossens K, 2005, IEEE DES TEST COMPUT, V22, P414, DOI 10.1109/MDT.2005.99
   HAPPEL BLM, 1994, NEURAL NETWORKS, V7, P985, DOI 10.1016/S0893-6080(05)80155-8
   Harkin J, 2009, INT J RECONFIGURABLE, V2009, DOI 10.1155/2009/908740
   Kepa K, 2009, I C FIELD PROG LOGIC, P403, DOI 10.1109/FPL.2009.5272250
   Kim John, 2009, Proceedings of the 2009 2nd International Workshop on Network on Chip Architectures (NoCArc 2009), P5, DOI 10.1145/1645213.1645217
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maguire LP, 2007, NEUROCOMPUTING, V71, P13, DOI 10.1016/j.neucom.2006.11.029
   Pande Sandeep, 2010, Proceedings 2010 International Symposium on System-on-Chip - SOC, P139, DOI 10.1109/ISSOC.2010.5625566
   Pande S., 2013, NEURAL PROCESS LETT, V1370-4621, P1
   Pearson MJ, 2007, IEEE T NEURAL NETWOR, V18, P1472, DOI 10.1109/TNN.2007.891203
   Rieke F., 1999, SPIKES EXPLORING NEU
   Ronco E, 1995, CSC95026 U GLASG, V1, P1
   Ros E, 2006, IEEE T NEURAL NETWOR, V17, P1050, DOI 10.1109/TNN.2006.875980
   Salminen E, 2007, DSD 2007: 10TH EUROMICRO CONFERENCE ON DIGITAL SYSTEM DESIGN ARCHITECTURES, METHODS AND TOOLS, PROCEEDINGS, P503, DOI 10.1109/DSD.2007.4341515
   Schemmel J, 2008, IEEE IJCNN, P431, DOI 10.1109/IJCNN.2008.4633828
   Strogatz SH, 2001, NATURE, V410, P268, DOI 10.1038/35065725
   Theocharides T, 2004, IEEE INT SOC CONF, P191, DOI 10.1109/SOCC.2004.1362404
   Thorpe S, 1996, NATURE, V381, P520, DOI 10.1038/381520a0
   Upegui A, 2005, MICROPROCESS MICROSY, V29, P211, DOI 10.1016/j.micpro.2004.08.012
   Vainbrand D, 2011, MICROPROCESS MICROSY, V35, P152, DOI 10.1016/j.micpro.2010.08.005
   Vogelstein RJ, 2007, IEEE T NEURAL NETWOR, V18, P253, DOI 10.1109/TNN.2006.883007
   Watts DJ, 1998, NATURE, V393, P440, DOI 10.1038/30918
   Wiklund D., 2003, Proceedings International Parallel and Distributed Processing Symposium, DOI 10.1109/IPDPS.2003.1213180
NR 39
TC 24
Z9 24
U1 2
U2 18
PD SEP
PY 2013
VL 39
IS 9
SI SI
BP 357
EP 371
DI 10.1016/j.parco.2013.04.010
UT WOS:000324960800002
DA 2023-11-16
ER

PT S
AU Hosaka, R
   Ikeguchi, T
   Aihara, K
AF Hosaka, Ryosuke
   Ikeguchi, Tohru
   Aihara, Kazuyuki
BE King, I
   Wang, J
   Chan, L
   Wang, DL
TI Self-organizing rhythmic patterns with spatio-temporal spikes in class I
   and class II neural networks
SO NEURAL INFORMATION PROCESSING, PT 1, PROCEEDINGS
SE LECTURE NOTES IN COMPUTER SCIENCE
DT Article; Proceedings Paper
CT 13th International Conference on Neural Informational Processing
CY OCT 03-06, 2006
CL Hong Kong, PEOPLES R CHINA
ID NEURONS; OSCILLATIONS
AB Regularly spiking neurons are classified into two categories, Class I and Class II, by their firing properties for constant inputs. To investigate how the firing properties of single neurons affect to ensemble rhythmic activities in neural networks, we constructed different types of neural networks whose excitatory neurons are the Class I neurons or the Class II neurons. The networks were driven by random inputs and developed with STDP learning. As a result, the Class I and the Class II neural networks generate different types of rhythmic activities: the Class I neural network generates slow rhythmic activities, and the Class II neural network generates fast rhythmic activities.
C1 Saitama Univ, Grad Sch Sci & Engn, Sakura Ku, Urawa, Saitama 3388570, Japan.
   JST, ERATO, Aihara Complex Modelling Project, Shibuya Ku, Tokyo 1510064, Japan.
   Univ Tokyo, Inst Ind Sci, Meguro Ku, Tokyo 1538505, Japan.
RP Hosaka, R (corresponding author), Saitama Univ, Grad Sch Sci & Engn, Sakura Ku, 255 Shimo Ohkubo, Urawa, Saitama 3388570, Japan.
EM hosaka@nls.ics.saitama-u.ac.jp
CR Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Buzsáki G, 2004, SCIENCE, V304, P1926, DOI 10.1126/science.1099745
   CONNORS BW, 1990, TRENDS NEUROSCI, V13, P99, DOI 10.1016/0166-2236(90)90185-D
   Gibson JR, 1999, NATURE, V402, P75, DOI 10.1038/47035
   Gray CM, 1996, SCIENCE, V274, P109, DOI 10.1126/science.274.5284.109
   HODGKIN AL, 1948, J PHYSIOL-LONDON, V107, P165, DOI 10.1113/jphysiol.1948.sp004260
   HOSAKA R, 2006, UNPUB NEURAL COMPUTA
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   MIYAKAWA H, 2003, BIOPHYSICS NEURONS
   Nicolelis MAL, 2001, ADV NEURAL POPULATIO
   OKEEFE J, 1993, HIPPOCAMPUS, V3, P317, DOI 10.1002/hipo.450030307
   Rinzel J., 1989, METHODS NEURONAL MOD, P251
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
NR 14
TC 0
Z9 0
U1 0
U2 1
PY 2006
VL 4232
BP 39
EP 48
UT WOS:000241790100005
DA 2023-11-16
ER

PT J
AU Fukai, T
AF Fukai, T
TI Oscillations for rapid selection of neural activities based on spike
   timing
SO NEUROREPORT
DT Article
DE oscillatory neural activities; spike-timing code; recurrent inhibition;
   competitive behaviour; integrate-and-fire neurones; computer simulations
ID INTEGRATION; NEURONS; CORTEX; MODEL
AB In cortical information processing, neuronal inputs are transformed into sequences of action potentials. However, the neural codes used for embedding information in the spike trains remain unclear. Here a neural network consisting of recurrent inhibitory connections is shown to achieve selective activation and inactivation of neurones very efficiently according to spike timing rather than firing rates, when they are stimulated by periodic spike trains. Oscillatory neural activities serve as an accurate clock for the spike-timing code utilized in the rapid selection of neural activities. These results suggest that differences in spike timing of <1 ms can be of functional significance in certain neural information processing.
C1 RIKEN,FRONTIER RES PROGRAM,LAB NEURAL MODELING,WAKO,SAITAMA 35191,JAPAN.
RP Fukai, T (corresponding author), TOKAI UNIV,DEPT ELECTR,KITAKANAME 1117,HIRATSUKA,KANAGAWA 25912,JAPAN.
CR ABELES M, 1993, J NEUROPHYSIOL, V70, P1629, DOI 10.1152/jn.1993.70.4.1629
   [Anonymous], 1988, INTRO THEORETIC NEUR
   BLAND BH, 1993, PROG NEUROBIOL, V41, P157, DOI 10.1016/0301-0082(93)90007-F
   COHEN MA, 1983, IEEE T SYST MAN CYB, V13, P815, DOI 10.1109/TSMC.1983.6313075
   COULTRIP R, 1992, NEURAL NETWORKS, V5, P47, DOI 10.1016/S0893-6080(05)80006-1
   DEVRIES SH, 1993, CELL, V72, P139, DOI 10.1016/S0092-8674(05)80033-9
   ENGEL AK, 1992, TRENDS NEUROSCI, V15, P218, DOI 10.1016/0166-2236(92)90039-B
   ERWIN E, 1995, NEURAL COMPUT, V7, P425, DOI 10.1162/neco.1995.7.3.425
   FUKAI T, 1995, TOKAI U RIKEN PREPRI
   Gray C M, 1994, J Comput Neurosci, V1, P11, DOI 10.1007/BF00962716
   Hikosaka O, 1991, Curr Opin Neurobiol, V1, P638, DOI 10.1016/S0959-4388(05)80042-X
   HOPFIELD JJ, 1984, P NATL ACAD SCI-BIOL, V81, P3088, DOI 10.1073/pnas.81.10.3088
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   KASKI S, 1994, NEURAL NETWORKS, V7, P973, DOI 10.1016/S0893-6080(05)80154-6
   MAINEN ZF, 1995, SCIENCE, V268, P1503, DOI 10.1126/science.7770778
   NICOLELIS MAL, 1995, SCIENCE, V268, P1353, DOI 10.1126/science.7761855
   OKEEFE J, 1993, HIPPOCAMPUS, V3, P317, DOI 10.1002/hipo.450030307
   SKAGGS WE, 1995, U ARIZONA PREPRINT
   SOFTKY WR, 1993, J NEUROSCI, V13, P334
   Tamori Y., 1993, Society for Neuroscience Abstracts, V19, P547
   TANAKA S, 1990, NEURAL NETWORKS, V3, P625, DOI 10.1016/0893-6080(90)90053-N
   von der Malsburg C, 1973, Kybernetik, V14, P85
   WILSON HR, 1972, BIOPHYS J, V12, P1, DOI 10.1016/S0006-3495(72)86068-5
NR 23
TC 4
Z9 4
U1 0
U2 1
PD DEC 29
PY 1995
VL 7
IS 1
BP 273
EP 277
DI 10.1097/00001756-199512000-00065
UT WOS:A1995TX36900065
DA 2023-11-16
ER

PT C
AU Ostrau, C
   Homburg, J
   Klarhorst, C
   Thies, M
   Rückert, U
AF Ostrau, Christoph
   Homburg, Jonas
   Klarhorst, Christian
   Thies, Michael
   Rueckert, Ulrich
BE Farkas, I
   Masulli, P
   Wermter, S
TI Benchmarking Deep Spiking Neural Networks on Neuromorphic Hardware
SO ARTIFICIAL NEURAL NETWORKS AND MACHINE LEARNING, ICANN 2020, PT II
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 29th International Conference on Artificial Neural Networks (ICANN)
CY SEP 15-18, 2020
CL Bratislava, SLOVAKIA
DE Spiking neural networks; Neural architecture search; Benchmark
ID ARCHITECTURE
AB With more and more event-based neuromorphic hardware systems being developed at universities and in industry, there is a growing need for assessing their performance with domain specific measures. In this work, we use the methodology of converting pre-trained non-spiking to spiking neural networks to evaluate the performance loss and measure the energy-per-inference for three neuromorphic hardware systems (BrainScaleS, Spikey, SpiNNaker) and common simulation frameworks for CPU (NEST) and CPU/GPU (GeNN). For analog hardware we further apply a re-training technique known as hardware-in-the-loop training to cope with device mismatch. This analysis is performed for five different networks, including three networks that have been found by an automated optimization with a neural architecture search framework. We demonstrate that the conversion loss is usually below one percent for digital implementations, and moderately higher for analog systems with the benefit of much lower energy-per-inference costs.
C1 [Ostrau, Christoph; Homburg, Jonas; Klarhorst, Christian; Thies, Michael; Rueckert, Ulrich] Bielefeld Univ, Tech Fac, Bielefeld, Germany.
RP Ostrau, C (corresponding author), Bielefeld Univ, Tech Fac, Bielefeld, Germany.
EM costrau@techfak.uni-bielefeld.de
CR Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Davies M, 2019, NAT MACH INTELL, V1, P386, DOI 10.1038/s42256-019-0097-1
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Davison Andrew P, 2008, Front Neuroinform, V2, P11, DOI 10.3389/neuro.11.011.2008
   Diehl P.U., 2015, 2015 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2015.7280696
   Furber SB, 2013, IEEE T COMPUT, V62, P2454, DOI 10.1109/TC.2012.142
   Goltz J., 2019, FAST DEEP NEUROMORPH, DOI [10.1145/3381755.3381770, DOI 10.1145/3381755.3381770]
   Homburg JD, 2019, LECT NOTES COMPUT SC, V11507, P735, DOI 10.1007/978-3-030-20518-8_61
   Jordan J., 2019, NEST 2 18 0, DOI DOI 10.5281/ZENODO.2605422
   Knight JC, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00941
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Marc-Oliver G., 2007, SCHOLARPEDIA, DOI [DOI 10.4249/SCHOLARPEDIA.1430, 10.4249/scholarpedia.1430]
   Moradi S, 2018, IEEE T BIOMED CIRC S, V12, P106, DOI 10.1109/TBCAS.2017.2759700
   Neckar A, 2019, P IEEE, V107, P144, DOI 10.1109/JPROC.2018.2881432
   Ostrau C., 2020, NEURO INSPIRED COMPU, P1, DOI [10.1145/3381755.3381772, DOI 10.1145/3381755.3381772]
   Ostrau C., 2019, C P 2019 INT C HIGH, DOI [10.1109/HPCS48598.2019.9188207, DOI 10.1109/HPCS48598.2019.9188207]
   Petrovici MA, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0108590
   Pfeil T, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00011
   Rhodes O, 2020, PHILOS T R SOC A, V378, DOI 10.1098/rsta.2019.0160
   Schemmel J, 2010, IEEE INT SYMP CIRC S, P1947, DOI 10.1109/ISCAS.2010.5536970
   Schmitt S, 2017, IEEE IJCNN, P2227, DOI 10.1109/IJCNN.2017.7966125
   Stöckel A, 2017, FRONT COMPUT NEUROSC, V11, DOI 10.3389/fncom.2017.00071
   van Albada SJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00291
   Yavuz E, 2016, SCI REP-UK, V6, DOI 10.1038/srep18854
NR 25
TC 3
Z9 3
U1 1
U2 3
PY 2020
VL 12397
BP 610
EP 621
DI 10.1007/978-3-030-61616-8_49
UT WOS:000713797800049
DA 2023-11-16
ER

PT J
AU Rosenbaum, R
   Tchumatchenko, T
   Moreno-Bote, R
AF Rosenbaum, Robert
   Tchumatchenko, Tatjana
   Moreno-Bote, Ruben
TI Correlated neuronal activity and its relationship to coding, dynamics
   and network architecture
SO FRONTIERS IN COMPUTATIONAL NEUROSCIENCE
DT Editorial Material
DE neuronal correlations; neural synchrony; neural coding; spike train
   analysis; neuronal networks; noise correlation
C1 [Rosenbaum, Robert] Univ Notre Dame, Dept Appl & Computat Math & Stat, Notre Dame, IN 46556 USA.
   [Rosenbaum, Robert] Ctr Neural Basis Cognit, Pittsburgh, PA USA.
   [Tchumatchenko, Tatjana] Max Planck Inst Brain Res, Dept Theory Neural Dynam, Frankfurt, Germany.
   [Moreno-Bote, Ruben] Parc Sanitari St Joan de Deu, Res Unit, Barcelona, Spain.
   [Moreno-Bote, Ruben] Univ Barcelona, Barcelona, Spain.
   [Moreno-Bote, Ruben] Ctr Invest Biomed Red Salud Mental CIBERSAM, Barcelona, Spain.
RP Rosenbaum, R (corresponding author), Univ Notre Dame, Dept Appl & Computat Math & Stat, Notre Dame, IN 46556 USA.
EM robertr@pitt.edu
CR Abeles M., 1982, LOCAL CORTICAL CIRCU, DOI DOI 10.1007/978-3-642-81708-3
   Alvarado-Rojas C, 2013, FRONT COMPUT NEUROSC, V7, DOI 10.3389/fncom.2013.00140
   Barreiro AK, 2014, FRONT COMPUT NEUROSC, V8, DOI 10.3389/fncom.2014.00010
   Bird AD, 2014, FRONT COMPUT NEUROSC, V8, DOI 10.3389/fncom.2014.00002
   Bolhasani E, 2013, FRONT COMPUT NEUROSC, V7, DOI 10.3389/fncom.2013.00108
   Cohen MR, 2011, NAT NEUROSCI, V14, P811, DOI 10.1038/nn.2842
   de la Rocha J, 2007, NATURE, V448, P802, DOI 10.1038/nature06028
   Dipoppa M, 2013, FRONT COMPUT NEUROSC, V7, DOI 10.3389/fncom.2013.00139
   Engel AK, 2001, TRENDS COGN SCI, V5, P16, DOI 10.1016/S1364-6613(00)01568-0
   Finger H, 2014, FRONT COMPUT NEUROSC, V7, DOI 10.3389/fncom.2013.00195
   FUNAHASHI S, 1989, J NEUROPHYSIOL, V61, P331, DOI 10.1152/jn.1989.61.2.331
   Grytskyy D, 2013, FRONT COMPUT NEUROSC, V7, DOI 10.3389/fncom.2013.00131
   Helias M, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003428
   Jahnke S, 2013, FRONT COMPUT NEUROSC, V7, DOI 10.3389/fncom.2013.00153
   Kilpatrick ZP, 2013, FRONT COMPUT NEUROSC, V7, DOI 10.3389/fncom.2013.00082
   Moreno R, 2002, PHYS REV LETT, V89, DOI 10.1103/PhysRevLett.89.288101
   Moreno-Bote R, 2006, PHYS REV LETT, V96, DOI 10.1103/PhysRevLett.96.028101
   Ostojic S, 2009, J NEUROSCI, V29, P10234, DOI 10.1523/JNEUROSCI.1275-09.2009
   Renart A, 2010, SCIENCE, V327, P587, DOI 10.1126/science.1179850
   Rosenbaum R, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00058
   Rosenbaum RJ, 2010, FRONT COMPUT NEUROSC, V4, DOI 10.3389/fncom.2010.00009
   Salinas E, 2001, NAT REV NEUROSCI, V2, P539, DOI 10.1038/35086012
   Schwalger T, 2013, FRONT COMPUT NEUROSC, V7, DOI 10.3389/fncom.2013.00164
   SINGER W, 1995, ANNU REV NEUROSCI, V18, P555, DOI 10.1146/annurev.ne.18.030195.003011
   Tchumatchenko T, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1002239
   Tchumatchenko T, 2010, PHYS REV LETT, V104, DOI 10.1103/PhysRevLett.104.058102
   Torre E, 2013, FRONT COMPUT NEUROSC, V7, DOI 10.3389/fncom.2013.00132
   Wimmer K, 2014, NAT NEUROSCI, V17, P431, DOI 10.1038/nn.3645
   Zanin M, 2013, FRONT COMPUT NEUROSC, V7, DOI 10.3389/fncom.2013.00125
   Zhou PC, 2013, FRONT COMPUT NEUROSC, V7, DOI 10.3389/fncom.2013.00113
NR 30
TC 8
Z9 8
U1 1
U2 17
PD AUG 27
PY 2014
VL 8
AR 102
DI 10.3389/fncom.2014.00102
UT WOS:000341953900001
DA 2023-11-16
ER

PT J
AU Capecci, E
   Lobo, JL
   Lana, I
   Espinosa-Ramos, JI
   Kasabov, N
AF Capecci, Elisa
   Lobo, Jesus L.
   Lana, Ibai
   Espinosa-Ramos, Josafath I.
   Kasabov, Nikola
TI Modelling gene interaction networks from time-series gene expression
   data using evolving spiking neural networks
SO EVOLVING SYSTEMS
DT Article
DE Artificial intelligence; Evolving spiking neural networks;
   Transcriptome; Gene expression; Microarray; Data analysis; Gene
   interaction networks; Nickel allergy; Allergic contact dermatitis
ID SPATIOTEMPORAL DATA; CLASSIFICATION; BRAIN; NEUCUBE; ARRAY; SET
AB The genetic mechanisms responsible for the differentiation, metabolism, morphology and function of a cell in both normal and abnormal conditions can be uncovered by the analysis of transcriptomes. Mining big data such as the information encoded in nucleic acids, proteins, and metabolites has challenged researchers for several years now. Even though bioinformatics and system biology techniques have improved greatly and many improvements have been done in these fields of research, most of the processes that influence gene interaction over time are still unknown. In this study, we apply state-of-the art spiking neural network techniques to model, analyse and extract information about the regulatory processes of gene expression over time. A case study of microarray profiling in human skin during elicitation of eczema is used to examine the temporal association of genes involved in the inflammatory response, by means of a gene interaction network. Spiking neural network techniques are able to learn the interaction between genes using information encoded from the time-series gene expression data as spikes. The temporal interaction is learned, and the patterns of activity extracted and analysed with a gene interaction network. Results demonstrated that useful knowledge can be extracted from the data by using spiking neural network, unlocking some of the possible mechanisms involved in the regulatory process of gene expression.
C1 [Capecci, Elisa; Espinosa-Ramos, Josafath I.; Kasabov, Nikola] Auckland Univ Technol, Knowledge Engn & Discovery Res Inst, AUT Tower,Cnr Rutland & Wakefield St, Auckland, New Zealand.
   [Lobo, Jesus L.; Lana, Ibai] Tecnalia Res & Innovat, P Tecnol Bizkaia, Ed 700, Derio 48160, Spain.
RP Capecci, E (corresponding author), Auckland Univ Technol, Knowledge Engn & Discovery Res Inst, AUT Tower,Cnr Rutland & Wakefield St, Auckland, New Zealand.
EM elisa.capecci@gmail.com; jesus.lopez@tecnalia.com;
   iba.lana@tecnalia.com; vjier1979@gmail.com; nkasabov@aut.ac.nz
CR Angelov P, 2013, INFORM SCIENCES, V222, P163, DOI 10.1016/j.ins.2012.08.006
   [Anonymous], EVOLVING SYSTEMS
   [Anonymous], 2010, EVOLVING INTELLIGENT
   [Anonymous], 1996, SPECTRAL GRAPH THEOR
   [Anonymous], 2005, P 1 INT WORKSH GEN F
   [Anonymous], 2018, STAT MACH LEARN TOOL
   [Anonymous], REV MANAGERIAL SCI, DOI DOI 10.1080/08927014.2017.1351958
   [Anonymous], 2002, MICROARRAYS INTEGRAT
   [Anonymous], 2004, STAT MICROARRAYS
   Argyriou A., 2007, ADV NEURAL INFORM PR, V19, P41
   Barrett T, 2013, NUCLEIC ACIDS RES, V41, pD991, DOI 10.1093/nar/gks1193
   Beer DG, 2002, NAT MED, V8, P816, DOI 10.1038/nm733
   Beleites C, 2013, ANAL CHIM ACTA, V760, P25, DOI 10.1016/j.aca.2012.11.007
   Bradley P. S., 1998, Machine Learning. Proceedings of the Fifteenth International Conference (ICML'98), P82
   Capano V, 2015, SCI REP-UK, V5, DOI 10.1038/srep09895
   Capecci E, 2015, NEURAL NETWORKS, V68, P62, DOI 10.1016/j.neunet.2015.03.009
   Causton HC, 2009, MICROARRAY GENE EXPR
   DeVries T, 2017, INT C LEARN REPR
   Dobbin KK, 2008, CLIN CANCER RES, V14, P108, DOI 10.1158/1078-0432.CCR-07-0443
   Dobbin KK, 2007, BIOSTATISTICS, V8, P101, DOI 10.1093/biostatistics/kxj036
   Edgar R, 2002, NUCLEIC ACIDS RES, V30, P207, DOI 10.1093/nar/30.1.207
   Espinosa-Ramos JI, 2019, IEEE T COGN DEV SYST, V11, P63, DOI 10.1109/TCDS.2017.2776863
   Ezkurdia I, 2014, HUM MOL GENET, V23, P5866, DOI 10.1093/hmg/ddu309
   Feuk L, 2006, NAT REV GENET, V7, P85, DOI 10.1038/nrg1767
   Fürnkranz J, 2002, J MACH LEARN RES, V2, P721, DOI 10.1162/153244302320884605
   GERSTNER W, 1995, PHYS REV E, V51, P738, DOI 10.1103/PhysRevE.51.738
   Gerstner W, 2001, PLAUSIBLE NEURAL NET
   Gerstner W, 2012, SCIENCE, V338, P60, DOI 10.1126/science.1227356
   Ghosh-Dastidar S, 2007, INTEGR COMPUT-AID E, V14, P187
   He X., 2006, ADV NEURAL INFORM PR, V18, P507, DOI DOI 10.3233/IDT-120147
   Hebb D.O., 1949, ORG BEHAV NEUROPSYCH
   Huang SJ, 2018, CANCER GENOM PROTEOM, V15, P41, DOI 10.21873/cgp.20063
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Kasabov Nikola, 2012, Artificial Neural Networks in Pattern Recognition. Proceedings of the 5th INNS IAPR TC 3 GIRPR Workshop, ANNPR 2012, P225, DOI 10.1007/978-3-642-33212-8_21
   Kasabov N, 2016, NEURAL NETWORKS, V78, P1, DOI 10.1016/j.neunet.2015.09.011
   Kasabov N, 2015, INFORM SCIENCES, V294, P565, DOI 10.1016/j.ins.2014.06.028
   Kasabov N, 2013, NEURAL NETWORKS, V41, P188, DOI 10.1016/j.neunet.2012.11.014
   Kasabov NK, 2014, NEURAL NETWORKS, V52, P62, DOI 10.1016/j.neunet.2014.01.006
   Kelly JG, 2010, ANAL BIOANAL CHEM, V398, P2191, DOI 10.1007/s00216-010-4179-5
   Koefoed L, 2018, 2018 INT JOINT C NEU, P1, DOI [10.1109/IJCNN.2018.8489634, DOI 10.1109/IJCNN.2018.8489634]
   Kumar C, 2009, FEBS LETT, V583, P1703, DOI 10.1016/j.febslet.2009.03.035
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Marks S, 2017, EVOL SYST-GER, V8, P193, DOI 10.1007/s12530-016-9170-8
   McLachlan G, 2005, WILEY SERIES PROBABI
   McLachlan G. J., 2005, ANAL MICROARRAY GENE
   MOLLER MF, 1993, NEURAL NETWORKS, V6, P525, DOI 10.1016/S0893-6080(05)80056-5
   Mortazavi A, 2008, NAT METHODS, V5, P621, DOI 10.1038/nmeth.1226
   Mukherjee S, 2003, J COMPUT BIOL, V10, P119, DOI 10.1089/106652703321825928
   Nuntalid N, 2011, LECT NOTES COMPUT SC, V7062, P451, DOI 10.1007/978-3-642-24955-6_54
   Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7
   Panda S, 2003, TRENDS CELL BIOL, V13, P151, DOI 10.1016/S0962-8924(03)00006-0
   Pedersen MB, 2007, J INVEST DERMATOL, V127, P2585, DOI 10.1038/sj.jid.5700902
   Pertea M, 2010, GENOME BIOL, V11, DOI 10.1186/gb-2010-11-5-206
   Radovic M, 2017, BMC BIOINFORMATICS, V18, DOI 10.1186/s12859-016-1423-9
   Roffo G, 2018, FEATURE SELECTION LI
   Roffo G, 2017, IEEE I CONF COMP VIS, P1407, DOI 10.1109/ICCV.2017.156
   Roffo G, 2015, IEEE I CONF COMP VIS, P4202, DOI 10.1109/ICCV.2015.478
   Schrauwen B, 2003, IEEE IJCNN, P2825
   Sebastiani P, 2003, STAT SCI, V18, P33, DOI 10.1214/ss/1056397486
   Shen EH, 2012, TRENDS NEUROSCI, V35, P711, DOI 10.1016/j.tins.2012.09.005
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Sun XL, 2010, BMC BIOINFORMATICS, V11, DOI 10.1186/1471-2105-11-607
   Tapia JE, 2013, IEEE T INF FOREN SEC, V8, P488, DOI 10.1109/TIFS.2013.2242063
   Thorpe S, 1998, COMPUTATIONAL NEUROSCIENCE: TRENDS IN RESEARCH, P113
   Tomasev N, 2015, STUD COMPUT INTELL, V584, P231, DOI 10.1007/978-3-662-45620-0_11
   Trevisan J, 2014, J BIOPHOTONICS, V7, P254, DOI 10.1002/jbio.201300190
   Trevisan J, 2010, ANALYST, V135, P3266, DOI 10.1039/c0an00586j
   Tu EM, 2017, IEEE T NEUR NET LEAR, V28, P1305, DOI 10.1109/TNNLS.2016.2536742
   Tu EM, 2014, IEEE IJCNN, P638, DOI 10.1109/IJCNN.2014.6889717
   Tu EM, 2014, NEUROCOMPUTING, V143, P109, DOI 10.1016/j.neucom.2014.05.067
   Verleysen M, 2005, LECT NOTES COMPUT SC, V3512, P758
   Wang XW, 2008, BMC SYST BIOL, V2, DOI 10.1186/1752-0509-2-58
   Yixiong Chen, 2013, Neural Information Processing. 20th International Conference, ICONIP 2013. Proceedings: LNCS 8228, P70, DOI 10.1007/978-3-642-42051-1_10
   Zhou F, 2012, PROC CVPR IEEE, P127, DOI 10.1109/CVPR.2012.6247667
NR 74
TC 0
Z9 0
U1 1
U2 10
PD DEC
PY 2020
VL 11
IS 4
BP 599
EP 613
DI 10.1007/s12530-019-09269-6
UT WOS:000576195800005
DA 2023-11-16
ER

PT J
AU Zenke, F
   Vogels, TP
AF Zenke, Friedemann
   Vogels, Tim P.
TI The Remarkable Robustness of Surrogate Gradient Learning for Instilling
   Complex Function in Spiking Neural Networks
SO NEURAL COMPUTATION
DT Article
ID MODELS; INTELLIGENCE; DYNAMICS; POWER
AB Brains process information in spiking neural networks. Their intricate connections shape the diverse functions these networks perform. Yet how network connectivity relates to function is poorly understood, and the functional capabilities of models of spiking networks are still rudimentary. The lack of both theoretical insight and practical algorithms to find the necessary connectivity poses a major impediment to both studying information processing in the brain and building efficient neuromorphic hardware systems. The training algorithms that solve this problem for artificial neural networks typically rely on gradient descent. But doing so in spiking networks has remained challenging due to the nondifferentiable nonlinearity of spikes. To avoid this issue, one can employ surrogate gradients to discover the required connectivity. However, the choice of a surrogate is not unique, raising the question of how its implementation influences the effectiveness of the method. Here, we use numerical simulations to systematically study how essential design parameters of surrogate gradients affect learning performance on a range of classification problems. We show that surrogate gradient learning is robust to different shapes of underlying surrogate derivatives, but the choice of the derivative's scale can substantially affect learning performance. When we combine surrogate gradients with suitable activity regularization techniques, spiking networks perform robust information processing at the sparse activity limit. Our study provides a systematic account of the remarkable robustness of surrogate gradient learning and serves as a practical guide to model functional spiking neural networks.
C1 [Zenke, Friedemann; Vogels, Tim P.] Univ Oxford, Ctr Neural Circuits & Behav, Oxford OX1 3SR, England.
   [Zenke, Friedemann] Friedrich Miescher Inst Biomed Res, CH-4058 Basel, Switzerland.
   [Vogels, Tim P.] IST Austria, A-3400 Klosterneuburg, Austria.
RP Zenke, F (corresponding author), Univ Oxford, Ctr Neural Circuits & Behav, Oxford OX1 3SR, England.
EM friedemann.zenke@fmi.ch; tim.vogels@ist.ac.at
CR Amir A, 2017, PROC CVPR IEEE, P7388, DOI 10.1109/CVPR.2017.781
   [Anonymous], 2019, BIOL INSPIRED ALTERN
   Barrett DGT, 2019, CURR OPIN NEUROBIOL, V55, P55, DOI 10.1016/j.conb.2019.01.007
   Bellec G., 2018, ADV NEURAL INFORM PR, P795
   Boahen K, 2017, COMPUT SCI ENG, V19, P14, DOI 10.1109/MCSE.2017.33
   Bohte SM, 2011, LECT NOTES COMPUT SC, V6791, P60, DOI 10.1007/978-3-642-21735-7_8
   Cramer B., 2020, ARXIV200607239
   Cramer B, 2022, IEEE T NEUR NET LEAR, V33, P2744, DOI 10.1109/TNNLS.2020.3044364
   CRICK F, 1989, NATURE, V337, P129, DOI 10.1038/337129a0
   Cueva C. J., 2019, BIORXIV, DOI [10.1101/504936, DOI 10.1101/504936]
   Esser SK, 2016, P NATL ACAD SCI USA, V113, P11441, DOI 10.1073/pnas.1604850113
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   Gütig R, 2016, SCIENCE, V351, DOI 10.1126/science.aab4113
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Hochreiter S, 1998, INT J UNCERTAIN FUZZ, V6, P107, DOI 10.1142/S0218488598000094
   Huh Dongsung, 2018, ADV NEURAL INFORM PR, P1440
   Hunsberger Eric, 2015, COMPUT SCI
   Kaiming He, 2015, 2015 IEEE International Conference on Computer Vision (ICCV). Proceedings, P1026, DOI 10.1109/ICCV.2015.123
   Kingma DP., 2017, ARXIV
   LeCun Y., 1998, MNIST DATABASE HANDW
   Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508
   Maheswaranathan N, 2019, DEEP LEARNING MODELS, DOI 10.1101/340943
   Mante V, 2013, NATURE, V503, P78, DOI 10.1038/nature12742
   McClure P, 2016, FRONT COMPUT NEUROSC, V10, DOI 10.3389/fncom.2016.00131
   McIntosh LT, 2016, ADV NEUR IN, V29
   Michaels J. A., 2019, NEURAL NETWORK MODEL, DOI DOI 10.1101/742189
   Mishkin D., 2016, IEEE T NEUR NET LEAR, V29, P3227, DOI 10.1109/TNNLS. 2017.2726060
   Murray JM, 2019, ELIFE, V8, DOI 10.7554/eLife.43299
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Neftci EO, 2018, ISCIENCE, V5, P52, DOI 10.1016/j.isci.2018.06.010
   Orchard G, 2015, FRONT NEUROSCI-SWITZ, V9, DOI [10.3389/fhins.2015.00437, 10.3389/fnins.2015.00437]
   Paszke A, 2019, ADV NEUR IN, V32
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Pospisil DA, 2018, ELIFE, V7, DOI 10.7554/eLife.38242
   Richards BA, 2019, NAT NEUROSCI, V22, P1761, DOI 10.1038/s41593-019-0520-2
   Roy K, 2019, NATURE, V575, P607, DOI 10.1038/s41586-019-1677-2
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707
   Schemmel J, 2010, IEEE INT SYMP CIRC S, P1947, DOI 10.1109/ISCAS.2010.5536970
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Shrestha S. B., 2018, ADV NEURAL INFORM PR
   Sterling P., 2017, PRINCIPLES NEURAL DE
   Stroud JP, 2018, NAT NEUROSCI, V21, P1774, DOI 10.1038/s41593-018-0276-0
   Sussillo D, 2013, NEURAL COMPUT, V25, P626, DOI 10.1162/NECO_a_00409
   Tanaka Hidenori, 2019, Adv Neural Inf Process Syst, V32, P8537
   Vogels TP, 2005, J NEUROSCI, V25, P10786, DOI 10.1523/JNEUROSCI.3508-05.2005
   Wang J, 2018, NAT NEUROSCI, V21, P102, DOI 10.1038/s41593-017-0028-6
   Warden P., 2018, ARXIV
   Williams RJ, 1989, NEURAL COMPUT, V1, P270, DOI 10.1162/neco.1989.1.2.270
   Williamson RC, 2019, CURR OPIN NEUROBIOL, V55, P40, DOI 10.1016/j.conb.2018.12.009
   Wozniak S, 2020, NAT MACH INTELL, V2, P325, DOI 10.1038/s42256-020-0187-0
   Xuan Huang, 2021, 2021 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech), P867, DOI 10.1109/DASC-PICom-CBDCom-CyberSciTech52372.2021.00144
   Yamins DLK, 2016, NAT NEUROSCI, V19, P356, DOI 10.1038/nn.4244
   Yamins DLK, 2014, P NATL ACAD SCI USA, V111, P8619, DOI 10.1073/pnas.1403112111
   Zenke F, 2018, NEURAL COMPUT, V30, P1514, DOI 10.1162/neco_a_01086
   Zimmer Romain, 2019, ARXIV191110124
NR 55
TC 56
Z9 56
U1 3
U2 12
PD MAR
PY 2021
VL 33
IS 4
BP 899
EP 925
DI 10.1162/neco_a_01367
UT WOS:000663433900003
DA 2023-11-16
ER

PT C
AU Kato, H
   Kimura, T
   Ikeguchi, T
AF Kato, Hideyuki
   Kimura, Takayuki
   Ikeguchi, Tohru
BE In, V
   Longhini, P
   Palacious, A
TI Self-Organized Neural Network Structure Depending on the STDP Learning
   Rules
SO APPLICATIONS OF NONLINEAR DYNAMICS-MODEL AND DESIGN OF COMPLEX SYSTEMS
SE Understanding Complex Systems Springer Complexity
DT Proceedings Paper
CT International Conference on Applied Nonlinear Dynamics
CY SEP 24-27, 2007
CL Poipu Beach, HI
ID NEURONS; EPSPS
AB Complex systems are widely studied in various fields of science. A neural network is one of the typical examples of the complex systems. Recent studies in neuroscience reported that the neural networks are dynamically self-organized by the spike-timing dependent synaptic plasticity (STDP). Although the neural networks change their structure using the STDP dynamically, neural networks are often analyzed in a static state. Thus, in this paper, we analyze neural network structure from a dynamical point of view. Then, we show that the self-organized neural network to which the STDP learning rule is applied generates the small-world effect and randomness of the inter-spike intervals (ISIs) in the self-organized neural network increases as the small-world effect becomes higher.
C1 [Kato, Hideyuki] Saitama Univ, Sakura Ku, 255 Shimo Okubo, Saitama 3388570, Japan.
RP Kato, H (corresponding author), Saitama Univ, Sakura Ku, 255 Shimo Okubo, Saitama 3388570, Japan.
EM kato@nls.ics.saitama-u.ac.jp
CR Abbott LF, 2000, NAT NEUROSCI, V3, P1178, DOI 10.1038/81453
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   GUTIG SRR, 2003, J NEUROSCI, V23, P3687
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   SOFTKY WR, 1993, J NEUROSCI, V13, P334
   Watts DJ, 1998, NATURE, V393, P440, DOI 10.1038/30918
NR 7
TC 2
Z9 2
U1 0
U2 1
PY 2009
BP 413
EP +
DI 10.1007/978-3-540-85632-0_36
UT WOS:000264750300036
DA 2023-11-16
ER

PT C
AU Meng, QY
   Xiao, MQ
   Yan, S
   Wang, Y
   Lin, ZC
   Luo, ZQ
AF Meng, Qingyan
   Xiao, Mingqing
   Yan, Shen
   Wang, Yisen
   Lin, Zhouchen
   Luo, Zhi-Quan
GP IEEE COMP SOC
TI Training High-Performance Low-Latency Spiking Neural Networks by
   Differentiation on Spike Representation
SO 2022 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION
   (CVPR)
SE IEEE Conference on Computer Vision and Pattern Recognition
DT Proceedings Paper
CT IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CY JUN 18-24, 2022
CL New Orleans, LA
AB Spiking Neural Network (SNN) is a promising energy-efficient AI model when implemented on neuromorphic hardware. However, it is a challenge to efficiently train SNNs due to their non-differentiability. Most existing methods either suffer from high latency (i.e., long simulation time steps), or cannot achieve as high performance as Artificial Neural Networks (ANNs). In this paper, we propose the Differentiation on Spike Representation (DSR) method, which could achieve high performance that is competitive to ANNs yet with low latency. First, we encode the spike trains into spike representation using (weighted) firing rate coding. Based on the spike representation, we systematically derive that the spiking dynamics with common neural models can be represented as some sub-differentiable mapping. With this viewpoint, our proposed DSR method trains SNNs through gradients of the mapping and avoids the common non-differentiability problem in SNN training. Then we analyze the error when representing the specific mapping with the forward computation of the SNN. To reduce such error, we propose to train the spike threshold in each layer, and to introduce a new hyperparameter for the neural models. With these components, the DSR method can achieve state-of-the-art SNN performance with low latency on both static and neuromorphic datasets, including CIFAR-10, CIFAR-100, ImageNet, and DVS-CIFAR10.
C1 [Meng, Qingyan; Luo, Zhi-Quan] Chinese Univ Hong Kong, Shenzhen, Peoples R China.
   [Meng, Qingyan; Luo, Zhi-Quan] Shenzhen Res Inst Big Data, Shenzhen, Peoples R China.
   [Xiao, Mingqing; Wang, Yisen; Lin, Zhouchen] Peking Univ, Sch Artificial Intelligence, Key Lab Machine Percept MoE, Beijing, Peoples R China.
   [Yan, Shen] Peking Univ, Ctr Data Sci, Beijing, Peoples R China.
   [Wang, Yisen; Lin, Zhouchen] Peking Univ, Inst Artificial Intelligence, Beijing, Peoples R China.
   [Lin, Zhouchen] Peng Cheng Lab, Shenzhen, Guangdong, Peoples R China.
RP Lin, ZC (corresponding author), Peking Univ, Sch Artificial Intelligence, Key Lab Machine Percept MoE, Beijing, Peoples R China.; Lin, ZC (corresponding author), Peking Univ, Inst Artificial Intelligence, Beijing, Peoples R China.; Lin, ZC (corresponding author), Peng Cheng Lab, Shenzhen, Guangdong, Peoples R China.
EM qingyanmeng@link.cuhk.edu.cn; mingqing_xiao@pku.edu.cn;
   yanshen@pku.edu.cn; yisen.wang@pku.edu.cn; zlin@pku.edu.cn;
   luozq@cuhk.edu.cn
CR [Anonymous], 2018, NEURIPS
   [Anonymous], 2000, ESANN
   Bellec G., 2018, NEURIPS
   Burkitt AN, 2006, BIOL CYBERN, V95, P1, DOI 10.1007/s00422-006-0068-6
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Caporale N, 2008, ANNU REV NEUROSCI, V31, P25, DOI 10.1146/annurev.neuro.31.060407.125639
   Choi J., 2018, ARXIV PREPRINT ARXIV
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng L, 2020, NEURAL NETWORKS, V121, P294, DOI 10.1016/j.neunet.2019.09.005
   Deng S., 2021, ICLR
   Diehl P.U., 2015, 2015 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2015.7280696
   Ding Jianhao, 2021, IJCAI
   Esser Steve K, 2015, NEURIPS
   Esser SK, 2016, P NATL ACAD SCI USA, V113, P11441, DOI 10.1073/pnas.1604850113
   Fang Wei, 2021, ICCV
   Han Bing, 2020, CVPR
   Han Bing, 2020, ECCV
   He K., 2016, P IEEE C COMP VIS PA, DOI DOI 10.1007/978-3-319-46493-0_38
   HEBB D. O., 1949
   Huh Dongsung, 2018, NEURIPS
   Hunsberger Eric, 2015, COMPUT SCI
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Kim Soo Ye, 2020, AAAI
   Krizhevsky Alex, 2009, LEARNING MULTIPLE LA
   Lee C, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00119
   Legenstein R, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.1000180
   Li H, 2017, PROT CONTR MOD POW, V2, DOI 10.1186/s41601-017-0040-6
   Li Yuhang, 2021, ICML
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Mostafa H, 2018, IEEE T NEUR NET LEAR, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Panzeri S, 2001, NEURAL COMPUT, V13, P1311, DOI 10.1162/08997660152002870
   Pei J, 2019, NATURE, V572, P106, DOI 10.1038/s41586-019-1424-8
   Rathi Nitin, 2020, INT C LEARN REPR
   Rathi Nitin, 2020, ARXIV200803658
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Stöckl C, 2021, NAT MACH INTELL, V3, DOI 10.1038/s42256-021-00311-4
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Wang LL, 2020, PHYTOPATHOL RES, V2, DOI 10.1186/s42483-020-00048-9
   Wang Y., 2018, IEEE T NEUR NET LEAR
   Wei Fangyun, 2021, NEURIPS
   Wu Hao, 2021, AAAI
   WU JB, 2021, TNNLS, DOI DOI 10.1109/TNNLS.2021.3095724
   Wu YF, 2018, J NANOFLUIDS, V7, P1, DOI 10.1166/jon.2018.1437
   Wu Yu, 2019, AAAI
   Wunderlich TC, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-91786-z
   Xiao M., 2021, NEURIPS
   Yan Zhanglu, 2021, AAAI
   Yang Yi-Rui, 2021, ICML
   Zheng H., 2021, AAAI
   Zhou Shibo, 2021, AAAI
NR 54
TC 10
Z9 10
U1 1
U2 5
PY 2022
BP 12434
EP 12443
DI 10.1109/CVPR52688.2022.01212
UT WOS:000870759105051
DA 2023-11-16
ER

PT C
AU Wang, F
AF Wang, Felix
BE Dagli, CH
TI Simulation Tool for Asynchronous Cortical Streams (STACS): Interfacing
   with Spiking Neural Networks
SO COMPLEX ADAPTIVE SYSTEMS, 2015
SE Procedia Computer Science
DT Proceedings Paper
CT Conference on Engineering Cyber Physical Systems - Machine Learning,
   Data Analytics and Smart Systems Architecting
CY NOV 02-04, 2015
CL San Jose, CA
DE simulation; spiking; neural network; asynchronous communication;
   closed-loop; embodied cognition; biological models
ID NEURONS
AB We present a Simulation Tool for Asynchronous Cortical Streams (STACS) for studying spiking neural networks exhibiting adaptation in a closed-loop system. The goal is to develop a more complete understanding of the emergent behaviors at the network level, and attention is given to methods of analysis at this scale. In particular, STACS facilitates the development of network level metrics of spiking activity. At the same time, emphasis is placed on biologically faithful models of spiking and plasticity with respect to the underlying neural substrate. The essential component, however, is the ability of the neural system in interfacing with the environment. This is because behaviors such as learning and adaptation are inherently closed-loop processes that involve the interaction between an intelligent agent and its environment, here, embodied cognition. To this end, STACS utilizes a portable communication protocol, YARP, for interfacing and interacting with a wide range of external devices, both sensory and motor, as well as the ability to create user-defined methods. In doing so, we may capture and respond to real world input to a neural network, simulating experimentation of live cortical cultures such as on multielectrode arrays. (C) 2015 The Authors. Published by Elsevier B.V.
C1 [Wang, Felix] Univ Illinois, Beckman Inst, 405 N Mathews Ave, Urbana, IL 61801 USA.
RP Wang, F (corresponding author), Univ Illinois, Beckman Inst, 405 N Mathews Ave, Urbana, IL 61801 USA.
EM fywang2@illinois.edu
CR Brette R, 2007, J COMPUT NEUROSCI, V23, P349, DOI 10.1007/s10827-007-0038-6
   BROWN R, 1988, COMMUN ACM, V31, P1220, DOI 10.1145/63039.63045
   Clopath C, 2010, FRONTIERS SYNAPTIC N, V2, P1
   Engel AK, 2001, NAT REV NEUROSCI, V2, P704, DOI 10.1038/35094565
   Gleeson P, 2010, PLOS COMPUTATIONAL B
   Hellwig B, 2000, BIOL CYBERN, V82, P111, DOI 10.1007/PL00007964
   Hines ML, 1997, NEURON BOOK
   Izhikevich EM, 2005, IEEE T NEURAL NETWOR, V94, P3637
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   KALE LV, 1993, SIGPLAN NOTICES, V28, P91, DOI 10.1145/167962.165874
   Karypis G, 1999, SIAM REV, V41, P278, DOI 10.1137/S0036144598334138
   Marc-Oliver G., 2007, SCHOLARPEDIA, DOI [DOI 10.4249/SCHOLARPEDIA.1430, 10.4249/scholarpedia.1430]
   Metta G., 2006, International Journal of Advanced Robotic Systems, V3, P43
   Minkovich K, 2014, IEEE T NEURAL NETWOR, V25
   Oksendal B. K., 2002, STOCHASTIC DIFFERENT
   Plesser HE, 2007, LECT NOTES COMPUT SC, V4641, P672
   Vainbrand D, 2011, MICROPROCESS MICROSY, V35, P152, DOI 10.1016/j.micpro.2010.08.005
   Wiener N., 1948, CYBERNETICS CONTROL
NR 18
TC 2
Z9 2
U1 0
U2 2
PY 2015
VL 61
BP 322
EP 327
DI 10.1016/j.procs.2015.09.149
UT WOS:000373845000046
DA 2023-11-16
ER

PT J
AU Okreghe, CO
   Zamani, M
   Demosthenous, A
AF Okreghe, Christian O.
   Zamani, Majid
   Demosthenous, Andreas
TI A Deep Neural Network-Based Spike Sorting With Improved Channel
   Selection and Artefact Removal
SO IEEE ACCESS
DT Article
DE Sorting; Recording; Feature extraction; Classification algorithms;
   Convolutional neural networks; Deep learning; Electroencephalography;
   Artefact removal; channel selection; convolutional neural network (CNN);
   deep learning; deep spike detection (DSD); extracellular recordings;
   real-time sorting; spike sorting
ID NEURONS
AB In order to implement highly efficient brain-machine interface (BMI) systems, high-channel count sensing is often used to record extracellular action potentials. However, the extracellular recordings are typically severely contaminated by artefacts and various noise sources, rendering the separation of multi-unit neural recordings an immensely challenging task. Removing artefact and noise from neural events can improve the spike sorting performance and classification accuracy. This paper presents a deep learning technique called deep spike detection (DSD) with a strong learning ability of high-dimensional vectors for neural channel selection and artefacts removal from the selected neural channel. The proposed method significantly improves spike detection compared to the conventional methods by sequentially diminishing the noise level and discarding the active artefacts in the recording channels. The simulated and experimental results show that there is considerably better performance when the extracellular raw recordings are cleaned prior to assigning individual spikes to the neurons that generated them. The DSD achieves an overall classification accuracy of 91.53% and outperformes Wave_clus by 3.38% on the simulated dataset with various noise levels and artefacts.
C1 [Okreghe, Christian O.; Zamani, Majid; Demosthenous, Andreas] UCL, Dept Elect & Elect Engn, London WC1E 7JE, England.
RP Okreghe, CO (corresponding author), UCL, Dept Elect & Elect Engn, London WC1E 7JE, England.
EM christian.okreghe.16@ucl.ac.uk
CR Aflalo T, 2015, SCIENCE, V348, P906, DOI 10.1126/science.aaa5417
   Arsene C, 2020, Arxiv, DOI arXiv:1908.10417
   Averbeck BB, 2006, NAT REV NEUROSCI, V7, P358, DOI 10.1038/nrn1888
   Baldazzi G, 2020, J NEURAL ENG, V17, DOI 10.1088/1741-2552/abc741
   Buneo Christopher A, 2016, CRCNS, V1.0, DOI 10.6080/K0CZ353K
   Carlson D, 2019, CURR OPIN NEUROBIOL, V55, P90, DOI 10.1016/j.conb.2019.02.007
   Chaudhary U, 2016, NAT REV NEUROL, V12, P513, DOI 10.1038/nrneurol.2016.113
   Christie BP, 2015, J NEURAL ENG, V12, DOI 10.1088/1741-2560/12/1/016009
   Chung JE, 2017, NEURON, V95, P1381, DOI 10.1016/j.neuron.2017.08.030
   Collinger JL, 2013, LANCET, V381, P557, DOI 10.1016/S0140-6736(12)61816-9
   Dehnen G, 2021, BRAIN SCI, V11, DOI 10.3390/brainsci11060761
   Einevoll GT, 2012, CURR OPIN NEUROBIOL, V22, P11, DOI 10.1016/j.conb.2011.10.001
   Fabietti Marcos, 2021, Brain Inform, V8, P14, DOI 10.1186/s40708-021-00135-3
   Golub MD, 2015, ELIFE, V4, DOI [10.7554/elife.10015, 10.7554/eLife.10015]
   Issar D, 2020, J NEUROPHYSIOL, V123, P1472, DOI 10.1152/jn.00641.2019
   Kechris C, 2021, IEEE ENG MED BIO, P890, DOI 10.1109/EMBC46164.2021.9630585
   Klaes C, 2015, J NEUROSCI, V35, P15466, DOI 10.1523/JNEUROSCI.2747-15.2015
   Lawlor PN, 2018, J COMPUT NEUROSCI, V45, P173, DOI 10.1007/s10827-018-0696-6
   Lecoq J, 2021, NAT METHODS, V18, P1401, DOI 10.1038/s41592-021-01285-2
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Li ZH, 2020, BRAIN SCI, V10, DOI 10.3390/brainsci10110835
   Lopes F, 2021, IEEE ACCESS, V9, P149955, DOI 10.1109/ACCESS.2021.3125728
   Mirzaei S, 2020, IEEE ENG MED BIO, P894, DOI 10.1109/EMBC44109.2020.9176515
   Mzurikwao D, 2019, 2019 IEEE SECOND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND KNOWLEDGE ENGINEERING (AIKE), P195, DOI 10.1109/AIKE.2019.00042
   Leite NMN, 2018, IEEE INT C BIOINFORM, P2605, DOI 10.1109/BIBM.2018.8621080
   Pachitariu M., 2016, BIORXIV, DOI DOI 10.1101/061481
   Park IY, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10010301
   Pedreira C, 2012, J NEUROSCI METH, V211, P58, DOI 10.1016/j.jneumeth.2012.07.010
   Perich Matthew G, 2018, CRCNS, V1.0, DOI 10.6080/K0FT8J72
   Quiroga R. Q., 2009, WAVE CLUS UNSUPERVIS
   Quiroga RQ, 2004, NEURAL COMPUT, V16, P1661, DOI 10.1162/089976604774201631
   Rácz M, 2020, J NEURAL ENG, V17, DOI 10.1088/1741-2552/ab4896
   Saif-ur-Rehman M, 2021, J NEURAL ENG, V18, DOI 10.1088/1741-2552/abc8d4
   Saif-ur-Rehman M, 2019, J NEURAL ENG, V16, DOI 10.1088/1741-2552/ab1e63
   Schafer RJ, 2011, SCIENCE, V332, P1568, DOI 10.1126/science.1199892
   Schirrmeister RT, 2017, HUM BRAIN MAPP, V38, P5391, DOI 10.1002/hbm.23730
   Seong C., 2021, IEEE T BIOMED CIRC S, V15, P1441
   Shi Y, 2013, J NEUROPHYSIOL, V109, P2097, DOI 10.1152/jn.00223.2012
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Steinmetz NA, 2021, SCIENCE, V372, P258, DOI 10.1126/science.abf4588
   Sun WT, 2020, NEUROCOMPUTING, V404, P108, DOI 10.1016/j.neucom.2020.04.029
   Wouters J, 2021, J NEURAL ENG, V18, DOI 10.1088/1741-2552/ac0f4a
   Yang BH, 2018, BIOMED SIGNAL PROCES, V43, P148, DOI 10.1016/j.bspc.2018.02.021
   Yang K., 2017, PROC INT C CLOUD TEC, V17
   Yi JH, 2022, IEEE OPEN J CIRCUITS, V3, P168, DOI 10.1109/OJCAS.2022.3184302
   Zamani Majid, 2022, Annu Int Conf IEEE Eng Med Biol Soc, V2022, P4884, DOI 10.1109/EMBC48229.2022.9871487
   Zamani M, 2020, IEEE T BIOMED CIRC S, V14, P221, DOI 10.1109/TBCAS.2020.2969910
   Zamani M, 2018, IEEE T BIOMED CIRC S, V12, P665, DOI 10.1109/TBCAS.2018.2825421
   Zamani M, 2014, IEEE T NEUR SYS REH, V22, P716, DOI 10.1109/TNSRE.2014.2309678
NR 49
TC 1
Z9 1
U1 4
U2 5
PY 2023
VL 11
BP 15131
EP 15143
DI 10.1109/ACCESS.2023.3242643
UT WOS:000937101100001
DA 2023-11-16
ER

PT C
AU Ko, CW
   Lin, YD
   Chung, HW
   Jan, GJ
AF Ko, CW
   Lin, YD
   Chung, HW
   Jan, GJ
BE Chang, HK
   Zhang, YT
TI An EEG spike detection algorithm using artificial neural network with
   multi-channel correlation
SO PROCEEDINGS OF THE 20TH ANNUAL INTERNATIONAL CONFERENCE OF THE IEEE
   ENGINEERING IN MEDICINE AND BIOLOGY SOCIETY, VOL 20, PTS 1-6: BIOMEDICAL
   ENGINEERING TOWARDS THE YEAR 2000 AND BEYOND
SE PROCEEDINGS OF ANNUAL INTERNATIONAL CONFERENCE OF THE IEEE ENGINEERING
   IN MEDICINE AND BIOLOGY SOCIETY
DT Proceedings Paper
CT 20th Annual International Conference of the
   IEEE-Engineering-in-Medicine-and-Biology-Society
CY OCT 29-NOV 01, 1998
CL HONG KONG, PEOPLES R CHINA
DE spike detection; electroencephalography; radial basis function;
   artificial neural network; incidence matrix
AB An automatic spike detection algorithm for classification of multi-channel electroencephalographic (EEG) signals based on artificial neural network is presented. Radial basis function (RBF) neural network was chosen for single channel recognition, with model optimization using receiver operating characteristics analysis. Waveform simplification was employed for high noise immunity. Feature extraction with as few as three parameters was used as preparation for the inputs to the neural network. Identification of multi-channel geometric correlation was performed to further lower the false-positive rate by using an incidence matrix. Threshold value for spike classification was chosen for simultaneous maximization of detection sensitivity and selectivity. Evaluation with visual analysis in this preliminary study showed a 83% sensitivity using 16-channel continuous EEG records of four patients, while a high false positive rate was found, which was believed to arise from the extensive and exhaustive visual analysis process. The computation time required for spike detection was significantly less than that needed for online display of the signals on the monitor. We believe that the algorithm proposed in this study is robust and that the simple structure of RBF neural network yields high potential for real-time implementation.
C1 Natl Taiwan Univ, Dept Elect Engn, Taipei 10764, Taiwan.
RP Ko, CW (corresponding author), Natl Taiwan Univ, Dept Elect Engn, Rm 238,1,Sect 4,Roosevelt Rd, Taipei 10764, Taiwan.
EM r85165@cctwin.ee.ntu.edu.tw
CR BODENSTEIN G, 1977, P IEEE, V65, P642, DOI 10.1109/PROC.1977.10543
   Gabor AJ, 1996, ELECTROEN CLIN NEURO, V99, P257, DOI 10.1016/0013-4694(96)96001-0
   GOTMAN J, 1992, ELECTROEN CLIN NEURO, V83, P12, DOI 10.1016/0013-4694(92)90127-4
   GOTMAN J, 1976, ELECTROEN CLIN NEURO, V41, P513, DOI 10.1016/0013-4694(76)90063-8
   GOTMAN J, 1996, TREATMENT EPILEPSY P, P280
   KOOI KA, 1966, NEUROLOGY, V16, P59, DOI 10.1212/WNL.16.1.59
   PAURI F, 1992, ELECTROEN CLIN NEURO, V82, P1, DOI 10.1016/0013-4694(92)90175-H
   WEBBER WRS, 1994, ELECTROEN CLIN NEURO, V91, P194, DOI 10.1016/0013-4694(94)90069-8
   WEBBER WRS, 1993, ELECTROEN CLIN NEURO, V87, P364, DOI 10.1016/0013-4694(93)90149-P
NR 9
TC 18
Z9 19
U1 0
U2 1
PY 1998
VL 20
BP 2070
EP 2073
PN 1-6
UT WOS:000079210400568
DA 2023-11-16
ER

PT J
AU Bertens, P
   Lee, SW
AF Bertens, Paul
   Lee, Seong-Whan
TI Network of evolvable neural units can learn synaptic learning rules and
   spiking dynamics
SO NATURE MACHINE INTELLIGENCE
DT Article
ID T-MAZE; EVOLUTION; NEURONS; NEUROTRANSMITTER; SYNCHRONIZATION;
   NEUROEVOLUTION; PLASTICITY
AB Although deep neural networks have seen great success in recent years through various changes in overall architectures and optimization strategies, their fundamental underlying design remains largely unchanged. Computational neuroscience may provide more biologically realistic models of neural processing mechanisms, but they are still high-level abstractions of empirical behaviour. Here we propose an evolvable neural unit (ENU) that can evolve individual somatic and synaptic compartment models of neurons in a scalable manner. We demonstrate that ENUs can evolve to mimic integrate-and-fire neurons and synaptic spike-timing-dependent plasticity. Furthermore, by constructing a network where an ENU takes the place of each synapse and neuron, we evolve an agent capable of learning to solve a T-maze environment task. This network independently discovers spiking dynamics and reinforcement-type learning rules, opening up a new path towards biologically inspired artificial intelligence.
   Bertens and Lee propose an evolvable neural unit, a recurrent neural network-based module that can evolve individual somatic and synaptic compartment models of neurons. By constructing networks of these evolvable neural units, they can evolve agents that learn synaptic update rules and the spiking dynamics of neurons.
C1 [Bertens, Paul; Lee, Seong-Whan] Korea Univ, Dept Brain & Cognit Engn, Seoul, South Korea.
   [Lee, Seong-Whan] Korea Univ, Dept Artificial Intelligence, Seoul, South Korea.
RP Lee, SW (corresponding author), Korea Univ, Dept Brain & Cognit Engn, Seoul, South Korea.; Lee, SW (corresponding author), Korea Univ, Dept Artificial Intelligence, Seoul, South Korea.
EM sw.lee@korea.ac.kr
CR Abbott LF, 2000, NAT NEUROSCI, V3, P1178, DOI 10.1038/81453
   Abbott LF, 1999, BRAIN RES BULL, V50, P303, DOI 10.1016/S0361-9230(99)00161-6
   [Anonymous], 2010, FRONT SYNAPTIC NEURO
   Arnold DV, 2002, IEEE T EVOLUT COMPUT, V6, P30, DOI 10.1023/A:1015059928466
   Back T., 1991, P 4 INT C GEN ALG MO, V2
   Back T., 1996, EVOLUTIONARY ALGORIT
   Bellec G., 2018, ADV NEURAL INFORM PR
   BENGIO A, 1992, MONUMENT HIST, P2
   Blynel J, 2003, LECT NOTES COMPUT SC, V2611, P593
   Börgers C, 2003, NEURAL COMPUT, V15, P509, DOI 10.1162/089976603321192059
   Buhry L, 2011, NEURAL COMPUT, V23, P2599, DOI 10.1162/NECO_a_00170
   Carlson D, 2013, 2013 9TH IEEE INTERNATIONAL CONFERENCE ON DISTRIBUTED COMPUTING IN SENSOR SYSTEMS (IEEE DCOSS 2013), P1, DOI 10.1109/DCOSS.2013.73
   Carlson KD, 2014, FRONT NEUROSCI-SWITZ, V8, DOI 10.3389/fnins.2014.00010
   CATTERALL WA, 1995, ANNU REV BIOCHEM, V64, P493, DOI 10.1146/annurev.biochem.64.1.493
   Cho K., 2014, ARXIV14061078, V1406, P1078
   Collingridge GL, 2004, NAT REV NEUROSCI, V5, P952, DOI 10.1038/nrn1556
   Deacon RMJ, 2006, NAT PROTOC, V1, P7, DOI 10.1038/nprot.2006.2
   Di Paolo EA, 2003, PHILOS T R SOC A, V361, P2299, DOI 10.1098/rsta.2003.1256
   Doya K, 2002, NEURAL NETWORKS, V15, P495, DOI 10.1016/S0893-6080(02)00044-8
   Fell J, 2011, NAT REV NEUROSCI, V12, P105, DOI 10.1038/nrn2979
   Flagel SB, 2011, NATURE, V469, P53, DOI 10.1038/nature09588
   Floreano D, 2006, INT J INTELL SYST, V21, P1005, DOI 10.1002/int.20173
   Frémaux N, 2016, FRONT NEURAL CIRCUIT, V9, DOI 10.3389/fncir.2015.00085
   Hassabis D, 2017, NEURON, V95, P245, DOI 10.1016/j.neuron.2017.06.011
   Hausknecht M, 2014, IEEE T COMP INTEL AI, V6, P355, DOI 10.1109/TCIAIG.2013.2294713
   Hebb D., 2005, ORG BEHAV NEUROPSYCH
   Hilfiker S, 1999, PHILOS T ROY SOC B, V354, P269, DOI 10.1098/rstb.1999.0378
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Hollenbeck PJ, 2005, J CELL SCI, V118, P5411, DOI [10.1242/jcs.02745, 10.1242/jcs.053850]
   HORNIK K, 1991, NEURAL NETWORKS, V4, P251, DOI 10.1016/0893-6080(91)90009-T
   Igel C, 2003, IEEE C EVOL COMPUTAT, P2588
   Jain AK, 1996, COMPUTER, V29, P31, DOI 10.1109/2.485891
   Kepecs A, 2002, J NEUROSCI, V22, P9053
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lehman J, 2018, GECCO'18: PROCEEDINGS OF THE 2018 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P450, DOI 10.1145/3205455.3205474
   Levitan I, 2015, NEURON CELL MOL BIOL
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   MCCORMICK DA, 1989, J NEUROPHYSIOL, V62, P1018, DOI 10.1152/jn.1989.62.5.1018
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Mountcastle VB, 1997, BRAIN, V120, P701, DOI 10.1093/brain/120.4.701
   Mouret Jean-Baptiste, 2014, GROWING ADAPTIVE MAC, P251, DOI DOI 10.1007/978-3-642-55337-0_9
   Nedergaard M, 2002, NAT REV NEUROSCI, V3, P748, DOI 10.1038/nrn916
   Oliphant T. E., 2006, A GUIDE TO NUMPY, V1
   Paszke A., 2017, OPEN REV
   Risi S, 2010, LECT NOTES ARTIF INT, V6226, P533, DOI 10.1007/978-3-642-15193-4_50
   Rounds E. L., 2016, INT C PAR PROBL SOLV, P537
   Sacramento J, 2018, ADV NEUR IN, V31
   Salimans Tim, 2017, ABS170303864 CORR
   Soltoggio A., 2008, ARTIFICIAL LIFE, VXI, P569
   Soltoggio A, 2007, IEEE C EVOL COMPUTAT, P2471, DOI 10.1109/CEC.2007.4424781
   Spruston N, 2008, NAT REV NEUROSCI, V9, P206, DOI 10.1038/nrn2286
   Stanley KO, 2019, NAT MACH INTELL, V1, P24, DOI 10.1038/s42256-018-0006-z
   Sutskever Ilya, 2013, INT C MACH LEARN, P1139
   Vale RD, 2003, CELL, V112, P467, DOI 10.1016/S0092-8674(03)00111-9
   Venkadesh S, 2018, FRONT NEUROINFORM, V12, DOI 10.3389/fninf.2018.00008
   Wierstra D, 2014, J MACH LEARN RES, V15, P949
NR 57
TC 5
Z9 5
U1 1
U2 16
PD DEC
PY 2020
VL 2
IS 12
DI 10.1038/s42256-020-00267-x
UT WOS:000600016400010
DA 2023-11-16
ER

PT J
AU Ju, PH
AF Ju, Ping-Hua
TI HOT HEAVY RAIL STEEL SURFACE ONLINE FAULTS DETECTING BASED ON FUZZY
   SPIKING NEURAL NETWORK
SO METALURGIA INTERNATIONAL
DT Article
DE hot-rolled heavy rail; faults detecting; feature extraction; neural
   network
AB Aiming at slow hot-rolled heavy rail surface faults detecting and low precision currently, this paper research a suit of system which is based on the fuzzy spiking neural network. In the system several high speed linear CCD cameras are used to collect pictures of hot-rolled heavy rail. According to the geometric characteristics of the heavy rail and its defect characteristics of high-frequency region, some image processing technologies are applied in workstation. The system is optically proposes too dark and sun regional overlapping fusion method and image correlation between pixel lines' algorithm to get sum face faults and fuzzy spiking neural network to make classification for the characteristics of low SVM training algorithm., Using fuzzy spiking neural network for classification on detection of hot heavy rail surface, greatly improve the speed and accuracy of online testing, and the detection correction rate is over than 80 percent.
C1 Chongqing Univ, Natl Key Lab Mech Transmiss, Chongqing 400044, Peoples R China.
RP Ju, PH (corresponding author), Chongqing Univ, Natl Key Lab Mech Transmiss, Chongqing 400044, Peoples R China.
CR Bassiou N, 2007, COMPUT VIS IMAGE UND, V107, P108, DOI 10.1016/j.cviu.2006.11.012
   Bodyanskiy Ye., 2008 INT BIENN BALT
   Glackin C, 2011, NEURAL NETWORKS, V24, P247, DOI 10.1016/j.neunet.2010.11.008
   Guan Xin, 2009, Railway Computer Application, V18, P27
   [韩思奇 Han Siqi], 2002, [系统工程与电子技术, System engineering & electronic technology], V24, P91
   Qi J.H., 2011, J UNIV SCI TECHNOL B, V33, P12
   TAO Gong-ming, 2010, STEEL ROLLING, V27, P32
   Wang Ai-ping, 2012, Journal of System Simulation, V24, P81
   WANG FENG-CHAO, 2010, OPTICS J, V30, P713
   ZHANG XUE-WU, 2011, OPTICS J, V31, P1
NR 10
TC 0
Z9 0
U1 0
U2 8
PY 2012
VL 17
IS 12
BP 48
EP 52
UT WOS:000309145500011
DA 2023-11-16
ER

PT C
AU Hong, CF
   Wang, J
   Che, YQ
AF Hong, Chaofei
   Wang, Jiang
   Che, Yanqiu
GP IEEE
TI Information Transmission through Temporal Structure in Synchronous
   spikes
SO 2019 9TH INTERNATIONAL IEEE/EMBS CONFERENCE ON NEURAL ENGINEERING (NER)
SE International IEEE EMBS Conference on Neural Engineering
DT Proceedings Paper
CT 9th IEEE/EMBS International Conference on Neural Engineering (NER)
CY MAR 20-23, 2019
CL San Francisco, CA
ID SPIKING; PROPAGATION; NETWORKS
AB Neuronal gamma-band synchronization is a common phenomenon found in cortical networks, which is considered as a potential mechanism for communication among brain areas. How neural assemblies transit information within the narrow time window of each gamma cycle is still an open question. Previous modeling studies have demonstrated that precise spike timing can robustly carry information with the propagation of strongly synchronized spikes. Here we show that the temporal structure of loosely synchronized spikes within each gamma cycle can also effectively carry information in the noisy cortical networks. The relative spiking phase of the synchronous spikes are significantly more consistent under the same stimulus compared to those in random stimuli. Moreover, there is an optimal conduction delay distribution for the network to maximize the information transmission. Our work suggests that the loosely synchronized spikes in the gamma cycles may provide a fundamental mechanism for neural communication using temporal codes.
C1 [Hong, Chaofei; Wang, Jiang] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Che, Yanqiu] Penn State Coll Med, Dept Neurosurg, Hershey, PA 17033 USA.
RP Hong, CF (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
EM hongchf@tju.edu.cn; jiangwang@tju.edu.cn; yche@pennstatehealth.psu.edu
CR Andersen P., 2007, HIPPOCAMPUS BOOK
   Bruno RM, 2006, SCIENCE, V312, P1622, DOI 10.1126/science.1124593
   Burkitt AN, 2006, BIOL CYBERN, V95, P1, DOI 10.1007/s00422-006-0068-6
   Dayan P., 2000, J CHEM INF MODEL
   Diesmann M, 1999, NATURE, V402, P529, DOI 10.1038/990101
   Fries P, 2007, TRENDS NEUROSCI, V30, P309, DOI 10.1016/j.tins.2007.05.005
   Fries P, 2015, NEURON, V88, P220, DOI 10.1016/j.neuron.2015.09.034
   Goodman Dan, 2008, Front Neuroinform, V2, P5, DOI 10.3389/neuro.11.005.2008
   Kremkow J, 2010, J NEUROSCI, V30, P15760, DOI 10.1523/JNEUROSCI.3874-10.2010
   Kumar A, 2008, J NEUROSCI, V28, P5268, DOI 10.1523/JNEUROSCI.2542-07.2008
   Kumar A, 2010, NAT REV NEUROSCI, V11, P615, DOI 10.1038/nrn2886
   Wu G. K., 2008, NEURON
NR 12
TC 0
Z9 0
U1 0
U2 3
PY 2019
BP 1118
EP 1121
DI 10.1109/ner.2019.8717154
UT WOS:000469933200271
DA 2023-11-16
ER

PT C
AU Zuo, L
   Ma, LY
   Xiao, YQ
   Zhang, ML
   Qu, H
AF Zuo, Lin
   Ma, Linyao
   Xiao, Yanqing
   Zhang, Malu
   Qu, Hong
BE Liu, D
   Xie, S
   Li, Y
   Zhao, D
   ElAlfy, ESM
TI A Dynamic Region Generation Algorithm for Image Segmentation Based on
   Spiking Neural Network
SO NEURAL INFORMATION PROCESSING (ICONIP 2017), PT III
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 24th International Conference on Neural Information Processing (ICONIP)
CY NOV 14-18, 2017
CL Guangzhou, PEOPLES R CHINA
DE Spiking neural network; Image segmentation; Pattern recognition
ID RULE
AB We propose a dynamic region generation algorithm for image segmentation based on spiking neural network inspired by human visual cortex that shows the tremendous capacity of processing image. The network structure generated by the proposed algorithm is automatically and dynamically. An image can be decomposed into several different shape and size of regions that look like superpixels. Merging these regions based on the color space similarity can extract contour. Dynamic network architecture brings stronger computing power. Dynamic generation method leads to more flexible network. Experimental results on BCDS300 dataset confirm that our approach achieves satisfactory segmentation results for different images compared with SLIC.
C1 [Zuo, Lin; Ma, Linyao; Xiao, Yanqing; Zhang, Malu; Qu, Hong] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 610054, Peoples R China.
RP Qu, H (corresponding author), Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 610054, Peoples R China.
EM hongqu@uestc.edu.com
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Afifi A, 2009, 2009 EUROPEAN CONFERENCE ON CIRCUIT THEORY AND DESIGN, VOLS 1 AND 2, P563, DOI 10.1109/ECCTD.2009.5275035
   Alpert S, 2012, IEEE T PATTERN ANAL, V34, P315, DOI 10.1109/TPAMI.2011.130
   Ang C. H., 2011, PROC INT C FIELD PRO, P1, DOI [10.1109/FPT.2011.6132701, DOI 10.1109/FPT.2011.6132701]
   De Berredo R.C., 2005, THESIS
   Gerstner W., 2002, SPIKING NEURON MODEL
   Ghosh-Dastidar S, 2007, INTEGR COMPUT-AID E, V14, P187
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Hosoya T, 2005, NATURE, V436, P71, DOI 10.1038/nature03689
   Kerr D, 2015, NEUROCOMPUTING, V158, P268, DOI 10.1016/j.neucom.2015.01.011
   Lin XH, 2014, LECT NOTES COMPUT SC, V8588, P248, DOI 10.1007/978-3-319-09333-8_27
   Masquelier T, 2009, NEURAL COMPUT, V21, P1259, DOI 10.1162/neco.2008.06-08-804
   Meng Y, 2011, IEEE T NEURAL NETWOR, V22, P1952, DOI 10.1109/TNN.2011.2171044
   Qu H, 2015, NEUROCOMPUTING, V151, P310, DOI 10.1016/j.neucom.2014.09.034
   Qu H, 2009, IEEE T NEURAL NETWOR, V20, P1724, DOI 10.1109/TNN.2009.2029858
   Sun Q.Y., 2015, FRUIT IMAGE SEGMENTA
   van de Sande KEA, 2011, IEEE I CONF COMP VIS, P1879, DOI 10.1109/ICCV.2011.6126456
   Wolters A, 2003, J NEUROPHYSIOL, V89, P2339, DOI 10.1152/jn.00900.2002
   Wu QX, 2013, NEUROCOMPUTING, V116, P3, DOI 10.1016/j.neucom.2012.01.046
   Xie XR, 2017, IEEE T NEUR NET LEAR, V28, P1411, DOI 10.1109/TNNLS.2016.2541339
   Zhang M., 2017, IEEE T COGN DEV SYST
NR 21
TC 0
Z9 0
U1 0
U2 1
PY 2017
VL 10636
BP 816
EP 824
DI 10.1007/978-3-319-70090-8_83
PN III
UT WOS:000576767300083
DA 2023-11-16
ER

PT C
AU Xiao, R
   Yan, R
   Tang, HJ
   Tan, KC
AF Xiao, Rong
   Yan, Rui
   Tang, Huajin
   Tan, Kay Chen
BE Sun, F
   Liu, H
   Hu, D
TI A Spiking Neural Network Model for Sound Recognition
SO COGNITIVE SYSTEMS AND SIGNAL PROCESSING, ICCSIP 2016
SE Communications in Computer and Information Science
DT Proceedings Paper
CT 3rd International Conference on Cognitive Systems and Information
   Processing (ICCSIP)
CY NOV 19-23, 2016
CL Beijing, PEOPLES R CHINA
DE Temporal coding; Temporal learning; Time-frequency information; Spiking
   neural network; Sound recognition
ID OSCILLATIONS; CODE; CLASSIFICATION
AB This paper presents a spiking neural network (SNN) model of leaky integrate-and-fire (LIF) neurons for sound recognition, which provides a way to simulate the brain processes. Neural coding and learning by processing external stimulus and recognizing different patterns are important parts in SNN model. Based on features extracted from the time-frequency representation of sound, we present a time-frequency encoding method which can retain the adequate information of original sound and generate spikes from represented features. The generated spikes are further used to train the SNN model with plausible supervised synaptic learning rule to efficiently perform various classification tasks. By testing the encoding and learning methods in RWCP database, experiments demonstrate that the proposed SNN model can achieve the robust performance for sound recognition across a variety of noise conditions.
C1 [Xiao, Rong; Yan, Rui; Tang, Huajin] Sichuan Univ, Coll Comp Sci, Neuromorph Comp Res Ctr, Chengdu, Sichuan, Peoples R China.
   [Tan, Kay Chen] City Univ Hong Kong, Dept Comp Sci, Kowloon Tong, Hong Kong, Peoples R China.
RP Tang, HJ (corresponding author), Sichuan Univ, Coll Comp Sci, Neuromorph Comp Res Ctr, Chengdu, Sichuan, Peoples R China.
EM rxiao@stu.scu.edu.cn; ryan@scu.edu.cn; htang@scu.edu.cn;
   kaytan@cityu.edu.hk
CR Benchenane K, 2010, NEURON, V66, P921, DOI 10.1016/j.neuron.2010.05.013
   Bohte Sander M., 2004, Natural Computing, V3, P195, DOI 10.1023/B:NACO.0000027755.02868.60
   Brody CD, 2003, NEURON, V37, P843, DOI 10.1016/S0896-6273(03)00120-X
   Cowling M, 2003, PATTERN RECOGN LETT, V24, P2895, DOI 10.1016/S0167-8655(03)00147-8
   Dennis J, 2013, INT CONF ACOUST SPEE, P803, DOI 10.1109/ICASSP.2013.6637759
   Gerstner W., 2002, SPIKING NEURON MODEL
   Goldhor R. S., 1993, ICASSP-93. 1993 IEEE International Conference on Acoustics, Speech, and Signal Processing (Cat. No.92CH3252-4), P149, DOI 10.1109/ICASSP.1993.319077
   Gütig R, 2014, CURR OPIN NEUROBIOL, V25, P134, DOI 10.1016/j.conb.2014.01.004
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Heil P, 1997, J NEUROPHYSIOL, V77, P2616, DOI 10.1152/jn.1997.77.5.2616
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Hu J, 2016, IEEE COMPUT INTELL M, V11, P56, DOI 10.1109/MCI.2016.2532268
   Hu J, 2013, NEURAL COMPUT, V25, P450, DOI 10.1162/NECO_a_00395
   Indiveri G, 2015, P IEEE, V103, P1379, DOI 10.1109/JPROC.2015.2444094
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Le Mouel C, 2014, J COMPUT NEUROSCI, V37, P333, DOI 10.1007/s10827-014-0505-9
   Leutgeb S, 2005, CURR OPIN NEUROBIOL, V15, P738, DOI 10.1016/j.conb.2005.10.002
   Liu L., 1999, THESIS
   Masquelier T, 2009, NEURAL COMPUT, V21, P1259, DOI 10.1162/neco.2008.06-08-804
   Mehta MR, 2002, NATURE, V417, P741, DOI 10.1038/nature00807
   Meister M, 1999, NEURON, V22, P435, DOI 10.1016/S0896-6273(00)80700-X
   Natarajan R, 2008, NEURAL COMPUT, V20, P2325, DOI 10.1162/neco.2008.01-07-436
   O'Shaughnessy D, 2008, PATTERN RECOGN, V41, P2965, DOI 10.1016/j.patcog.2008.05.008
   Orchard G, 2015, IEEE T PATTERN ANAL, V37, P2028, DOI 10.1109/TPAMI.2015.2392947
   Panzeri S, 2010, TRENDS NEUROSCI, V33, P111, DOI 10.1016/j.tins.2009.12.001
   Perez-Orive J, 2002, SCIENCE, V297, P359, DOI 10.1126/science.1070502
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Tiesinga P, 2008, NAT REV NEUROSCI, V9, P97, DOI 10.1038/nrn2315
   Valero X, 2012, IEEE T MULTIMEDIA, V14, P1684, DOI 10.1109/TMM.2012.2199972
   VanRullen R, 2005, TRENDS NEUROSCI, V28, P1, DOI 10.1016/j.tins.2004.10.010
   Nguyen VA, 2012, IEEE T NEUR NET LEAR, V23, P971, DOI 10.1109/TNNLS.2012.2191419
   WOODARD JP, 1992, IEEE T SIGNAL PROCES, V40, P1833, DOI 10.1109/78.143457
   Yu Q, 2016, IEEE T NEUR NET LEAR, V27, P621, DOI 10.1109/TNNLS.2015.2416771
   Yu QF, 2013, PLOS ONE, V8, DOI [10.1371/journal.pone.0059627, 10.1371/journal.pone.0078318]
   Zhao B., 2015, IEEE T NEURAL NETWOR, V26, P24
NR 35
TC 7
Z9 7
U1 1
U2 7
PY 2017
VL 710
BP 584
EP 594
DI 10.1007/978-981-10-5230-9_57
UT WOS:000432311900057
DA 2023-11-16
ER

PT C
AU Yousefzadeh, A
   Masquelier, T
   Serrano-Gotarredona, T
   Linares-Barranco, B
AF Yousefzadeh, A.
   Masquelier, T.
   Serrano-Gotarredona, T.
   Linares-Barranco, B.
GP IEEE
TI Hardware Implementation of Convolutional STDP for On-line Visual Feature
   Learning
SO 2017 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS (ISCAS)
SE IEEE International Symposium on Circuits and Systems
DT Proceedings Paper
CT IEEE International Symposium on Circuits and Systems (ISCAS)
CY MAY 28-31, 2017
CL Baltimore, MD
DE Neuromorphic Systems; Spike Time Dependent Plasticity (STDP); Spiking
   Neural Networks; Hardware Implementation of Neural Systems; Learning
   Systems
AB We present a highly hardware friendly STDP (Spike Timing Dependent Plasticity) learning rule for training Spiking Convolutional Cores in Unsupervised mode and training Fully Connected Classifiers in Supervised Mode. Examples are given for a 2-layer Spiking Neural System which learns in real time features from visual scenes obtained with spiking DVS (Dynamic Vision Sensor) Cameras.
C1 [Yousefzadeh, A.; Serrano-Gotarredona, T.; Linares-Barranco, B.] CSIC, Inst Microelect Sevilla, Seville, Spain.
   [Yousefzadeh, A.; Serrano-Gotarredona, T.; Linares-Barranco, B.] Univ Seville, Seville, Spain.
   [Masquelier, T.] Univ Toulouse, CNRS, CERCO UMR 5549, F-31300 Toulouse, France.
RP Yousefzadeh, A (corresponding author), CSIC, Inst Microelect Sevilla, Seville, Spain.; Yousefzadeh, A (corresponding author), Univ Seville, Seville, Spain.
EM reza@imse-cnm.csic.es; bernabe@imse-cnm.csic.es
CR Pérez-Carrasco JA, 2013, IEEE T PATTERN ANAL, V35, P2706, DOI 10.1109/TPAMI.2013.71
   Bichler O, 2012, NEURAL NETWORKS, V32, P339, DOI 10.1016/j.neunet.2012.02.022
   Corradi F., JAER
   Iakymchuk T, 2014, IEEE INT SYMP CIRC S, P1556, DOI 10.1109/ISCAS.2014.6865445
   Markran H., 1997, SCIENCE, V275
   Masquelier T, 2016, SCI REP-UK, V6, DOI 10.1038/srep24086
   Masquelier Timothee, 2007, PLOS COMPUT BIOL
   Oster M., 2009, NEURAL COMPUT
   Serrano-Gotarredona Rafael, 2009, IEEE T NEURAL NETWOR
   Serrano-Gotarredona T., 2013, IEEE J SOLID STATE C, V48
   Yousefzadeh A., 2016, REAL TIME VIDEOS
NR 11
TC 0
Z9 0
U1 0
U2 1
PY 2017
BP 2323
EP 2326
UT WOS:000439261800099
DA 2023-11-16
ER

PT J
AU Rácz, M
   Liber, C
   Németh, E
   Fiáth, R
   Rokai, J
   Harmati, I
   Ulbert, I
   Márton, G
AF Racz, Melinda
   Liber, Csaba
   Nemeth, Erik
   Fiath, Richard
   Rokai, Janos
   Harmati, Istvan
   Ulbert, Istvan
   Marton, Gergely
TI Spike detection and sorting with deep learning
SO JOURNAL OF NEURAL ENGINEERING
DT Article
DE spike detection; spike sorting; spike prediction; LSTM; recurrent neural
   network; convolutional neural network
ID DISCHARGE; SHIFT
AB Objective. The extraction and identification of single-unit activities in intracortically recorded electric signals have a key role in basic neuroscience, but also in applied fields, like in the development of high-accuracy brain-computer interfaces. The purpose of this paper is to present our current results on the detection, classification and prediction of neural activities based on multichannel action potential recordings. Approach. Throughout our investigations, a deep learning approach utilizing convolutional neural networks and a combination of recurrent and convolutional neural networks was applied, with the latter used in case of spike detection and the former used for cases of sorting and predicting spiking activities. Main results. In our experience, the algorithms applied prove to be useful in accomplishing the tasks mentioned above: our detector could reach an average recall of 69%, while we achieved an average accuracy of 89% in classifying activities produced by more than 20 distinct neurons. Significance. Our findings support the concept of creating real-time, high-accuracy action potential based BCIs in the future, providing a flexible and robust algorithmic background for further development.
C1 [Racz, Melinda; Liber, Csaba; Harmati, Istvan] Budapest Univ Technol & Econ, Dept Elect Engn & Informat, Budapest, Hungary.
   [Racz, Melinda; Nemeth, Erik; Fiath, Richard; Rokai, Janos; Ulbert, Istvan; Marton, Gergely] Pazmany Peter Catholic Univ, Fac Informat Technol & Bion, Budapest, Hungary.
   [Fiath, Richard; Ulbert, Istvan; Marton, Gergely] Hungarian Acad Sci, Inst Cognit Neurosci & Psychol, Res Ctr Nat Sci, Budapest, Hungary.
   [Marton, Gergely] Obuda Univ, Doctoral Sch Mat Sci & Technol, Budapest, Hungary.
   [Rokai, Janos] Semmelweis Univ, Karoly Racz Sch PhD Studies, Budapest, Hungary.
RP Rácz, M (corresponding author), Budapest Univ Technol & Econ, Dept Elect Engn & Informat, Budapest, Hungary.; Rácz, M (corresponding author), Pazmany Peter Catholic Univ, Fac Informat Technol & Bion, Budapest, Hungary.
EM racz.melinda@ppke.hu
CR ABELES M, 1977, P IEEE, V65, P762, DOI 10.1109/PROC.1977.10559
   [Anonymous], 1997, NEURAL COMPUT, DOI 10.1162/neco.1997.9.8.1735
   Arora A, 2018, J NEURAL ENG, V15, DOI 10.1088/1741-2552/aae131
   Biffi E., 2008, 4th IET International Conference on Advances in Medical, Signal and Information Processing, MEDSIP 2008, DOI 10.1049/cp:20080434
   Chah E, 2011, J NEURAL ENG, V8, DOI 10.1088/1741-2560/8/1/016006
   Chaure FJ, 2018, J NEUROPHYSIOL, V120, P1859, DOI 10.1152/jn.00339.2018
   Chen Y, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.82
   Choi JH, 2006, IEEE T BIO-MED ENG, V53, P738, DOI 10.1109/TBME.2006.870239
   Craik A, 2019, J NEURAL ENG, V16, DOI 10.1088/1741-2552/ab0ab5
   Fee MS, 1996, J NEUROSCI METH, V69, P175, DOI 10.1016/S0165-0270(96)00050-7
   Fiáth R, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-018-36816-z
   Fiáth R, 2018, BIOSENS BIOELECTRON, V106, P86, DOI 10.1016/j.bios.2018.01.060
   FUKUSHIMA K, 1980, BIOL CYBERN, V36, P193, DOI 10.1007/BF00344251
   GEORGOPOULOS AP, 1986, SCIENCE, V233, P1416, DOI 10.1126/science.3749885
   GEORGOPOULOS AP, 1983, EXP BRAIN RES, V49, P327
   Hochberg LR, 2012, NATURE, V485, P372, DOI 10.1038/nature11076
   Houston B, 2019, J NEURAL ENG, V16, DOI 10.1088/1741-2552/aae67f
   Hulata E, 2002, J NEUROSCI METH, V117, P1, DOI 10.1016/S0165-0270(02)00032-8
   Kim KH, 2000, IEEE T BIO-MED ENG, V47, P1406, DOI 10.1109/10.871415
   Lewicki MS, 1998, NETWORK-COMP NEURAL, V9, pR53, DOI 10.1088/0954-898X/9/4/001
   Li J, 2019, J NEURAL ENG, V16, DOI 10.1088/1741-2552/aaeaae
   Márton G, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0145307
   Oweiss KG, 2002, NEUROCOMPUTING, V44, P1133, DOI 10.1016/S0925-2312(02)00436-8
   Pachitariu M., 2016, NIPS P, V29, P4448
   Quiroga RQ, 2004, NEURAL COMPUT, V16, P1661, DOI 10.1162/089976604774201631
   Rey HG, 2015, BRAIN RES BULL, V119, P106, DOI 10.1016/j.brainresbull.2015.04.007
   Saif-ur-Rehman M, 2019, J NEURAL ENG, V16, DOI 10.1088/1741-2552/ab1e63
   Schirrmeister RT, 2017, HUM BRAIN MAPP, V38, P5391, DOI 10.1002/hbm.23730
   SCHWARTZ AB, 1988, J NEUROSCI, V8, P2913
   Schwartz AB, 2006, NEURON, V52, P205, DOI 10.1016/j.neuron.2006.09.019
   Stevenson IH, 2011, NAT NEUROSCI, V14, P139, DOI 10.1038/nn.2731
   Takahashi S, 2003, NEUROSCI RES, V46, P265, DOI 10.1016/S0168-0102(03)00103-2
   Vargas-Irwin C, 2007, J NEUROSCI METH, V164, P1, DOI 10.1016/j.jneumeth.2007.03.025
   Vogelstein R J, 2005, C P IEEE ENG MED BIO, V1, P546, DOI [10.1109/IEMBS.2004.1403215, DOI 10.1109/IEMBS.2004.1403215]
   Waldert S, 2009, J PHYSIOL-PARIS, V103, P244, DOI 10.1016/j.jphysparis.2009.08.007
   Wang GL, 2006, IEEE T BIO-MED ENG, V53, P1195, DOI 10.1109/TBME.2006.873397
   WERBOS PJ, 1988, NEURAL NETWORKS, V1, P339, DOI 10.1016/0893-6080(88)90007-X
   Wodlinger B, 2015, J NEURAL ENG, V12, DOI 10.1088/1741-2560/12/1/016011
   Wood F, 2004, P ANN INT IEEE EMBS, V26, P4009
   Xu HJ, 2018, J NEURAL ENG, V15, DOI 10.1088/1741-2552/aa9451
   Yang K, 2017, J PHYS CONF SER, V910, DOI 10.1088/1742-6596/910/1/012062
NR 41
TC 33
Z9 33
U1 3
U2 34
PD FEB
PY 2020
VL 17
IS 1
AR 016038
DI 10.1088/1741-2552/ab4896
UT WOS:000520420400001
DA 2023-11-16
ER

PT J
AU Ullah, S
   Henke, M
   Narisetti, N
   Panzarova, K
   Trtilek, M
   Hejatko, J
   Gladilin, E
AF Ullah, Sajid
   Henke, Michael
   Narisetti, Narendra
   Panzarova, Klara
   Trtilek, Martin
   Hejatko, Jan
   Gladilin, Evgeny
TI Towards Automated Analysis of Grain Spikes in Greenhouse Images Using
   Neural Network Approaches: A Comparative Investigation of Six Methods
SO SENSORS
DT Article
DE high-throughput plant image analysis; spike detection; spike
   segmentation; deep learning; automated plant phenotyping
ID CLASSIFICATION
AB Automated analysis of small and optically variable plant organs, such as grain spikes, is highly demanded in quantitative plant science and breeding. Previous works primarily focused on the detection of prominently visible spikes emerging on the top of the grain plants growing in field conditions. However, accurate and automated analysis of all fully and partially visible spikes in greenhouse images renders a more challenging task, which was rarely addressed in the past. A particular difficulty for image analysis is represented by leaf-covered, occluded but also matured spikes of bushy crop cultivars that can hardly be differentiated from the remaining plant biomass. To address the challenge of automated analysis of arbitrary spike phenotypes in different grain crops and optical setups, here, we performed a comparative investigation of six neural network methods for pattern detection and segmentation in RGB images, including five deep and one shallow neural network. Our experimental results demonstrate that advanced deep learning methods show superior performance, achieving over 90% accuracy by detection and segmentation of spikes in wheat, barley and rye images. However, spike detection in new crop phenotypes can be performed more accurately than segmentation. Furthermore, the detection and segmentation of matured, partially visible and occluded spikes, for which phenotypes substantially deviate from the training set of regular spikes, still represent a challenge to neural network models trained on a limited set of a few hundreds of manually labeled ground truth images. Limitations and further potential improvements of the presented algorithmic frameworks for spike image analysis are discussed. Besides theoretical and experimental investigations, we provide a GUI-based tool (SpikeApp), which shows the application of pre-trained neural networks to fully automate spike detection, segmentation and phenotyping in images of greenhouse-grown plants.
C1 [Ullah, Sajid; Henke, Michael; Hejatko, Jan] Masaryk Univ, CEITEC Cent European Inst Technol, Plant Sci Core Facil, Brno 60200, Czech Republic.
   [Narisetti, Narendra; Gladilin, Evgeny] Leibniz Inst Plant Genet & Crop Plant Res IPK, D-06466 Gatersleben, Germany.
   [Panzarova, Klara; Trtilek, Martin] Spol Sro, PSI Photon Syst Instruments, Drasov 66424, Czech Republic.
RP Ullah, S (corresponding author), Masaryk Univ, CEITEC Cent European Inst Technol, Plant Sci Core Facil, Brno 60200, Czech Republic.; Gladilin, E (corresponding author), Leibniz Inst Plant Genet & Crop Plant Res IPK, D-06466 Gatersleben, Germany.
EM sajid.ullah@ceitec.muni.cz; mhenke@uni-goettingen.de;
   narisetti@ipk-gatersleben.de; panzarova@psi.cz; martin@psi.cz;
   hejatko@sci.muni.cz; gladilin@ipk-gatersleben.de
CR Alharbi N, 2018, PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION APPLICATIONS AND METHODS (ICPRAM 2018), P346, DOI 10.5220/0006580403460355
   Azad Reza, 2020, Computer Vision - ECCV 2020 Workshops. Proceedings. Lecture Notes in Computer Science (LNCS 12535), P251, DOI 10.1007/978-3-030-66415-2_16
   Bi Kun, 2010, Transactions of the Chinese Society of Agricultural Engineering, V26, P212
   Bochkovskiy A., 2020, PREPRINT
   Chang T, 1993, IEEE T IMAGE PROCESS, V2, P429, DOI 10.1109/83.242353
   Chen L. C., 2014, 14127062 ARXIV
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Christodoulou CI, 2003, IEEE T MED IMAGING, V22, P902, DOI 10.1109/TMI.2003.815066
   Costa C, 2019, FRONT PLANT SCI, V9, DOI 10.3389/fpls.2018.01933
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Everingham M., PASCAL VISUAL OBJECT
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Grillo O, 2017, COMPUT ELECTRON AGR, V141, P223, DOI 10.1016/j.compag.2017.07.024
   Guo ZF, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-31977-3
   Hasan MM, 2018, PLANT METHODS, V14, DOI 10.1186/s13007-018-0366-8
   He Kaiming, 2016, PROC CVPR IEEE
   Holschneider M., 1990, WAVELETS, P286, DOI [DOI 10.1007/978-3-642-75988-8_28, 10.1007/978-3-642-75988-8_28]
   Huang J, 2017, PROC CVPR IEEE, P3296, DOI 10.1109/CVPR.2017.351
   King DB, 2015, ACS SYM SER, V1214, P1
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
   Li QY, 2017, PLANT METHODS, V13, DOI 10.1186/s13007-017-0231-1
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Minervini M, 2015, IEEE SIGNAL PROC MAG, V32, P126, DOI 10.1109/MSP.2015.2405111
   Misra T, 2020, PLANT METHODS, V16, DOI 10.1186/s13007-020-00582-9
   Narisetti N, 2020, FRONT PLANT SCI, V11, DOI 10.3389/fpls.2020.00666
   Pieruschka R, 2019, PLANT PHENOMICS, V2019, DOI 10.34133/2019/7507131
   Pound M. P., 2017, P IEEE CVF INT C COM
   Redmon J., ARXIV
   Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031
   Ronneberger O., 2015, INT C MED IM COMP CO, P234
   Salehi SSM, 2017, LECT NOTES COMPUT SC, V10541, P379, DOI 10.1007/978-3-319-67389-9_44
   Tan CW, 2020, FRONT PLANT SCI, V11, DOI 10.3389/fpls.2020.00259
   Tzutalin, 2015, LAB FREE SOFTW MIT L
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
NR 34
TC 1
Z9 1
U1 2
U2 17
PD NOV
PY 2021
VL 21
IS 22
AR 7441
DI 10.3390/s21227441
UT WOS:000726906200001
DA 2023-11-16
ER

PT J
AU Contoyiannis, YF
   Kosmidis, EK
   Diakonos, FK
   Kampitakis, M
   Potirakis, SM
AF Contoyiannis, Yiannis F.
   Kosmidis, Efstratios K.
   Diakonos, Fotios K.
   Kampitakis, Myron
   Potirakis, Stelios M.
TI A hybrid artificial neural network for the generation of critical
   fluctuations and inter-spike intervals
SO CHAOS SOLITONS & FRACTALS
DT Article
DE Criticality; Intermittency; Kink-antikink solitons; Artificial neural
   network; Membrane potential of biological neurons; Spikes
AB The recently introduced [Contoyiannis et al., 2021] hybrid artificial neural network can simulate the dynamics of membrane potential fluctuations of real neurons based on fundamental principles of Physics. Here, we propose a temporal description of the membrane potential fluctuations, which resembles the soliton solutions in phi(4) field theory. Within this framework, kink-antikink dynamics are associated with spike generation. Furthermore, we show that the simulation can also reproduce the distribution of inter-spike intervals of biological neurons in their critical state [Kosmidis et al., 2018]. A proposal for the intermittency origin of these fluctuations is discussed.
C1 [Contoyiannis, Yiannis F.; Potirakis, Stelios M.] Univ West Attica, Dept Elect & Elect Engn, Ancient Olive Grove Campus, GR-12241 Egaleo, Greece.
   [Contoyiannis, Yiannis F.; Diakonos, Fotios K.] Univ Athens, Dept Phys, Athens, Greece.
   [Kosmidis, Efstratios K.] Aristotle Univ Thessaloniki, Dept Med, Lab Physiol, Thessaloniki, Greece.
   [Kampitakis, Myron] Hellen Elect Distribut Network Operator SA, Network Major Installat Dept, 72 Athinon Ave, GR-18547 Faliro, Greece.
   [Potirakis, Stelios M.] Natl Observ Athens, Inst Astron Astrophys Space Applicat & Remote Sens, Metaxa and Vasileos Pavlou, GR-15236 Athens, Greece.
RP Potirakis, SM (corresponding author), Univ West Attica, Dept Elect & Elect Engn, Ancient Olive Grove Campus, GR-12241 Egaleo, Greece.
EM yiaconto@uniwa.gr; kosmidef@auth.gr; fdiakono@phys.uoa.gr;
   m.kampitakis@deddie.gr; spoti@uniwa.gr
CR Amit D. J., 1989, MODELING BRAIN FUNCT
   [Anonymous], 1985, QUANTUM FIELD THEORY, DOI DOI 10.1017/CBO9780511813900
   [Anonymous], 1993, QUANTUM FIELD THEORY
   [Anonymous], 2007, NEUROQUANTOLOGY, DOI DOI 10.14704/NQ.2007.5.3.137
   Avramiea AE, 2022, J NEUROSCI, V42, P2221, DOI 10.1523/JNEUROSCI.1095-21.2022
   Bezaire MJ, 2013, HIPPOCAMPUS, V23, P751, DOI 10.1002/hipo.22141
   Bornholdt S, 2000, PHYS REV LETT, V84, P6114, DOI 10.1103/PhysRevLett.84.6114
   Brochini L, 2016, SCI REP-UK, V6, DOI 10.1038/srep35831
   Contoyiannis YF, 2021, PHYSICA A, V577, DOI 10.1016/j.physa.2021.126073
   Contoyiannis YF, 2007, PHYS REV E, V76, DOI 10.1103/PhysRevE.76.031138
   Contoyiannis YF, 2002, PHYS REV LETT, V89, DOI 10.1103/PhysRevLett.89.035701
   Dahmen D, 2019, P NATL ACAD SCI USA, V116, P13051, DOI 10.1073/pnas.1818972116
   DEWILDE P, 1997, NEURAL NETWORK MODEL
   Eftaxias K, 2018, COMPLEXITY OF SEISMIC TIME SERIES: MEASUREMENT AND APPLICATION, P437, DOI 10.1016/B978-0-12-813138-1.00013-4
   Friedman N, 2012, PHYS REV LETT, V108, DOI 10.1103/PhysRevLett.108.208102
   Georgiev D, 2004, BIOMED REV, V15, P67, DOI DOI 10.14748/BMR.V15.103
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Ivancevic VG, 2013, ARXIV13050613V1 Q BI
   Kinouchi O, 2006, NAT PHYS, V2, P348, DOI 10.1038/nphys289
   Kosmidis EK, 2018, EUR J NEUROSCI, V48, P2343, DOI 10.1111/ejn.14117
   Landau LD, 1969, STAT PHYS 2, V9
   Landmann S., SELF ORG CRITICALITY
   Levina A, 2007, NAT PHYS, V3, P857, DOI 10.1038/nphys758
   Lovecchio E, 2012, FRONT PHYSIOL, V3, DOI 10.3389/fphys.2012.00098
   METROPOLIS N, 1953, J CHEM PHYS, V21, P1087, DOI 10.1063/1.1699114
   Naundorf B, 2006, NATURE, V440, P1060, DOI 10.1038/nature04610
   Potirakis SM, 2019, PHYSICA A, V528, DOI 10.1016/j.physa.2019.121360
   Rubin R, 2017, P NATL ACAD SCI USA, V114, pE9366, DOI 10.1073/pnas.1705841114
   Schuster H.G., 1989, DETERMINIST CHAOS IN
   Tetzlaff C, 2010, PLOS COMPUT BIOL, V6, DOI 10.1371/journal.pcbi.1001013
   Tkacik G, 2015, P NATL ACAD SCI USA, V112, P11508, DOI 10.1073/pnas.1514188112
   Villarrubia G, 2018, NEUROCOMPUTING, V272, P10, DOI 10.1016/j.neucom.2017.04.075
   Werner G, 2007, BIOSYSTEMS, V90, P496, DOI 10.1016/j.biosystems.2006.12.001
   Wilting J, 2019, CURR OPIN NEUROBIOL, V58, P105, DOI 10.1016/j.conb.2019.08.002
NR 34
TC 1
Z9 1
U1 0
U2 3
PD JUN
PY 2022
VL 159
AR 112115
DI 10.1016/j.chaos.2022.112115
EA APR 2022
UT WOS:000798992800004
DA 2023-11-16
ER

PT C
AU Meftah, B
   Lezoray, O
   Lecluse, M
   Benyettou, A
AF Meftah, Boudjelal
   Lezoray, Olivier
   Lecluse, Michel
   Benyettou, Abdelkader
BE Diamantaras, K
   Duch, W
   Iliadis, LS
TI Cell Microscopic Segmentation with Spiking Neuron Networks
SO ARTIFICIAL NEURAL NETWORKS-ICANN 2010, PT I
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 20th International Conference on Artificial Neural Networks
CY SEP 15-18, 2010
CL Thessaloniki, GREECE
DE Cell microscopic images; Hebbian learning; Segmentation; Spiking Neuron
   Networks
ID NUCLEI
AB Spiking Neuron Networks (SNNs) overcome the computational power of neural networks made of thresholds or sigmoidal units. Indeed, SNNs add a new dimension, the temporal axis, to the representation capacity and the processing abilities of neural networks. In this paper, we present how SNN can be applied with efficacy for cell microscopic image segmentation. Results obtained confirm the validity of the approach. The strategy is performed on cytological color images. Quantitative measures are used to evaluate the resulting segmentations.
C1 [Meftah, Boudjelal] Univ Mascara, Equipe EDTEC, Mascara, Algeria.
   [Meftah, Boudjelal; Lezoray, Olivier] Univ Caen Basse Normandie, CNRS 6072, UMR, GREYC, F-14050 Caen, France.
   [Lecluse, Michel] Ctr Hospitalier Public Cotentin, Serv anatom cytolog pathologiques, F-50130 Octeville, France.
   [Benyettou, Abdelkader] Univ Mohamed Boudiaf, Lab Signal Image Parole, Oran, Algeria.
RP Meftah, B (corresponding author), Univ Mascara, Equipe EDTEC, Mascara, Algeria.
CR Adiga PSU, 2001, PATTERN RECOGN, V34, P1449, DOI 10.1016/S0031-3203(00)00076-5
   Anoraganingrum D., 1999, Proceedings 10th International Conference on Image Analysis and Processing, P1043, DOI 10.1109/ICIAP.1999.797734
   Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   Borsotti M, 1998, PATTERN RECOGN LETT, V19, P741, DOI 10.1016/S0167-8655(98)00052-X
   DEBERREDO RC, 2005, THESIS U MINAS GERAI
   Di Ruberto C, 2000, INT C PATT RECOG, P397, DOI 10.1109/ICPR.2000.903568
   GERSTNER W, 1995, PHYS REV E, V51, P738, DOI 10.1103/PhysRevE.51.738
   Gerstner W., 2002, SPIKING NEURON MODEL
   Glory E, 2005, LECT NOTES COMPUT SC, V3708, P227
   Gupta A, 2009, IEEE IJCNN, P1189
   Karlsson A, 2003, LECT NOTES COMPUT SC, V2749, P595
   Knesel EA, 1996, ACTA CYTOL, V40, P60
   Lezoray O, 2002, IEEE T IMAGE PROCESS, V11, P783, DOI 10.1109/TIP.2002.800889
   Lin G, 2003, CYTOM PART A, V56A, P23, DOI 10.1002/cyto.a.10079
   LUDEMIR BTB, 2000, ARTIFICIAL NEURAL NE
   Maass W, 1997, NEURAL COMPUT, V9, P279, DOI 10.1162/neco.1997.9.2.279
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   MAASS W, 2001, RELEVANCE NEURAL NET
   Meurie C, 2005, INT J ROBOT AUTOM, V20, P63, DOI 10.2316/Journal.206.2005.2.206-2780
   Mouroutis T, 1998, BIOIMAGING, V6, P79, DOI 10.1002/1361-6374(199806)6:2<79::AID-BIO3>3.0.CO;2-#
   MURASHOV D, 2004, P 7 INT C PATT REC I, V3, P814
   Natschlager T, 1998, NETWORK-COMP NEURAL, V9, P319, DOI 10.1088/0954-898X/9/3/003
   OSTER M, 2004, P 11 IEEE INT C EL C, V11, P203
   PAPANICOLAOU ON, 1942, SCIENCE, V95, P432
   Patten SF, 1996, ACTA CYTOL, V40, P45
   PAUGAMMOISY H, 2009, HDB NATURAL IN PRESS
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   WU BJ, 2001, INTRO NEURAL DYNAMIC
   Wu HS, 2000, J MICROSC-OXFORD, V197, P296, DOI 10.1046/j.1365-2818.2000.00653.x
NR 29
TC 3
Z9 4
U1 0
U2 1
PY 2010
VL 6352
BP 117
EP +
PN I
UT WOS:000287889800016
DA 2023-11-16
ER

PT C
AU Cherdo, Y
   Miramond, B
   Pegatoquet, A
AF Cherdo, Yann
   Miramond, Benoit
   Pegatoquet, Alain
GP IEEE
TI Time series prediction and anomaly detection with recurrent spiking
   neural networks
SO 2023 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS, IJCNN
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUN 18-23, 2023
CL Broadbeach, AUSTRALIA
DE Spiking Neural Networks; CNN; LSTM; RNN; Unsupervised Anomaly detection;
   time-series prediction; Surrogate Gradient Descent
AB In the recent years, Spiking Neural Networks have gain much attention from the research community. They can now be trained using the powerful gradient descent and have drifted from the neuroscience to the Machine Learning community. An abundant literature shows that they can perform well on classical Artificial Intelligence tasks such as image or signal classification while consuming less energy than state-of-the-art models like Convolutional Neural Networks. Yet, there is very little work about their performance on unsupervised anomaly detection and time-series prediction. Indeed, the processing of such temporal data requires different encoding and decoding mechanisms and rises questions about their capacity to model a dynamical signal with long term temporal dependencies. In this paper, we propose for the first time a Sparse Recurrent Spiking Neural Network with specific encoding and decoding mechanisms to successfully predict time-series and do Unsupervised Anomaly Detection. We also provide a framework to describe in detail our model computational costs and fairly compare them with state-of-the-art models. Despite improvable performances, we show that our model perform well on these tasks and open a door for further studies of such applications for Spiking Neural Networks.
C1 [Cherdo, Yann] Renault Software, LEAT, CNRS UMR 7248, Biot, France.
   [Miramond, Benoit; Pegatoquet, Alain] Univ Cote Azur, LEAT, CNRS UMR 7248, Biot, France.
RP Cherdo, Y (corresponding author), Renault Software, LEAT, CNRS UMR 7248, Biot, France.
EM yann.cherdo@univ-cotedazur.fr; benoit.miramond@univ-cotedazur.fr;
   alain.pegatoquet@univ-cotedazur.fr
CR Ahmad S., 2019, ARXIV190311257
   Ahmad S., 2015, ARXIV150307469
   Ahmad S, 2017, NEUROCOMPUTING, V262, P134, DOI 10.1016/j.neucom.2017.04.070
   [Anonymous], 2020, METEONET OPEN REFERE
   [Anonymous], 2015, P EUR S ANN
   Auge D, 2021, NEURAL PROCESS LETT, V53, P4693, DOI 10.1007/s11063-021-10562-2
   Bellec G., 2018, ADV NEURAL INFORM PR
   BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181
   Bontemps L, 2016, LECT NOTES COMPUT SC, V10018, P141, DOI 10.1007/978-3-319-48057-2_9
   Calvin W. H., 1998, CEREBRAL CODE THINKI
   Chauhan S., 2015, DSAA
   Cherdo Y., 2020, ICASSP
   Cho K., 2014, ARXIV, P103, DOI [10.3115/v1/w14-4012, 10.3115/v1/W14-4012]
   Cordone L., 2022, IJCNN
   Cordone L, 2021, IEEE IJCNN, DOI 10.1109/IJCNN52387.2021.9533514
   Dampfhoffer M., 2022, INT C ART NEUR NETW
   Datta G., 2022, ARXIV221012613
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Den`eve S., 2016, NATURE NEUROSCIENCE
   Dennler N., 2021, C ART INT CIRC SYST
   Eshraghian J. K., ARXIV210912894, V2021
   Filonov P., 2016, ARXIV161206676
   Gerstner W, 2002, SPIKING NEURON MODEL, DOI [10.1017/CBO9780511815706, DOI 10.1017/CBO9780511815706]
   Gerz F., 2022, IJCNN IEEE
   GUHA S, 2016, ICML, V48
   Hagenaars J., 2021, ADV NEURAL INFORM PR
   Hawkins J, 2004, INTELLIGENCE
   Hochreiter S., 1997, LONG SHORT TERM MEMO, V9, P1735
   Izhikevich E. M., 2003, IEEE T NEURAL NETWOR
   Kejariwal A., 2015, TWITTER ANOMALYDETEC
   L<spacing diaeresis>angkvist M., 2014, PATTERN RECOGNITION
   Laptev N., 2015, ACM SIGKDD
   Lavin A., 2015, IEEE
   Lemaire E., 2022, ARXIV221013107
   Li C., 2022, FRONTIERS NEUROSCIEN
   Liu Q., 2022, KNOWLEDGE BASED SYST
   Long L., 2022, KNOWLEDGE BASED SYST
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maciag P. S., 2021, NEURAL NETWORKS
   Munir M., 2019, IEEE ACCESS
   Neftci Emre O., 2019, IEEE SIGNAL PROCESSI
   Pavlidis N., 2005, P IEEE IJCNN
   Pytorch, US
   Rao A, 2022, NAT MACH INTELL, V4, P467, DOI 10.1038/s42256-022-00480-w
   Stanway A., 2013, ETSY SKYLINE ONLINE
   Thorpe S, 1998, COMPUTATIONAL NEUROSCIENCE: TRENDS IN RESEARCH, P113
   Wang W., 2020, ARXIV200109220
   Wei D., 2021, APPL ENERGY
   Yahoo! webscope research, S5 LAB AN DET DAT VE
   Yan Z., 2021, BIOMEDICAL SIGNAL PR
   Yin B., 2020, INT C NEUR SYST
   Yin B., 2021, NATURE MACHINE INTEL
NR 52
TC 0
Z9 0
U1 1
U2 1
PY 2023
DI 10.1109/IJCNN54540.2023.10191614
UT WOS:001046198704068
DA 2023-11-16
ER

PT C
AU Cai, RT
   Wu, QX
   Wang, P
   Sun, HH
   Wang, ZC
AF Cai, Rongtai
   Wu, Qingxiang
   Wang, Ping
   Sun, Honghai
   Wang, Zichen
BE Zhang, Y
   Zhou, ZH
   Zhang, C
   Li, Y
TI Moving Target Detection and Classification Using Spiking Neural Networks
SO INTELLIGENT SCIENCE AND INTELLIGENT DATA ENGINEERING, ISCIDE 2011
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 2nd Sino-Foreign-Interchange Workshop on Intelligence Science and
   Intelligent Data Engineering (IScIDE)
CY OCT 23-25, 2011
CL Xian, PEOPLES R CHINA
DE Spiking Neural Network; Hebb learning rule; object detection and
   classification; visual surveillance
AB We proposed a spiking neural network (SNN) to detect moving target in video streams and classify them into real categorization in this paper. The proposed SNN uses spike trains to encoding information such as the gray value of pixels or feature parameters of the target, detects moving target by simulating the visual cortex for motion detection in biological system with axonal delays and classify them into different categorizations according to their distance to categorization's centers found by Hebb learning rule. The experimental results show that the proposed SNN is promising in intelligence computation and applicable in general visual surveillance system.
C1 [Cai, Rongtai; Wu, Qingxiang; Wang, Ping] Fujian Normal Univ, Sch Phys Opt Elect & Informat, Fuzhou 350108, Fujian, Peoples R China.
   [Sun, Honghai; Wang, Zichen] Chinese Acad Sci, Changchun Inst Opt, Fine Mech & Phys, Changchun 130033, Peoples R China.
RP Cai, RT (corresponding author), Fujian Normal Univ, Sch Phys Opt Elect & Informat, Fuzhou 350108, Fujian, Peoples R China.
EM gjrtcai@163.com
CR Bothe S.M., 2003, THESIS
   Caunce A., 2010, P BRIT MACH VIS C
   Dayan P., 2001, THEORETICAL NEUROSCI
   Gerstner W., 2002, SPIKING NEURON MODEL
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Kohn M, 2010, CONSTELLATIONS, V17, P572, DOI 10.1111/j.1467-8675.2010.00615.x
   Lalonde J F, 2011, THESIS
   Lin JW, 2002, TRENDS NEUROSCI, V25, P449, DOI 10.1016/S0166-2236(02)02212-9
   Lipton A. J., 1998, 4 IEEE WORKSH APPL C
   Maass W., 1999, PULSED NEURAL NETWOR
   Natschlager T., 1999, THESIS
   Thorpe SJ, 2000, ISCAS 2000: IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS - PROCEEDINGS, VOL IV, P405, DOI 10.1109/ISCAS.2000.858774
   Wu QX, 2008, LECT NOTES COMPUT SC, V5227, P76
NR 13
TC 2
Z9 2
U1 0
U2 5
PY 2012
VL 7202
BP 210
EP 217
UT WOS:000320469100027
DA 2023-11-16
ER

PT J
AU Stanojevic, A
   Cherubini, G
   Wozniak, S
   Eleftheriou, E
AF Stanojevic, Ana
   Cherubini, Giovanni
   Wozniak, Stanislaw
   Eleftheriou, Evangelos
TI Time-encoded multiplication-free spiking neural networks: application to
   data classification tasks
SO NEURAL COMPUTING & APPLICATIONS
DT Article
DE Spiking neural network; Temporal coding; Low-complexity data
   classification; Multiplication-free inference; Backpropagation through
   time
ID STORAGE
AB Spiking neural networks (SNNs) are mimicking computationally powerful biologically inspired models in which neurons communicate through sequences of spikes, regarded here as sparse binary sequences of zeros and ones. In neuroscience it is conjectured that time encoding, where the information is carried by the temporal position of spikes, is playing a crucial role at least in some parts of the brain where estimation of the spiking rate with a large latency cannot take place. Motivated by the efficiency of temporal coding, compared with the widely used rate coding, the goal of this paper is to develop and train an energy-efficient time-coded deep spiking neural network system. To ensure that the similarity among input stimuli is translated into a correlation of the spike sequences, we introduce correlative temporal encoding and extended correlative temporal encoding techniques to map analog input information into input spike patterns. Importantly, we propose an implementation where all multiplications in the system are replaced with at most a few additions. As a more efficient alternative to both rate-coded SNNs and artificial neural networks, such system represents a preferable solution for the implementation of neuromorphic hardware. We consider data classification tasks where input spike patterns are presented to a feed-forward architecture with leaky-integrate-and-fire neurons. The SNN is trained by backpropagation through time with the objective to match sequences of output spikes with those of specifically designed target spike patterns, each corresponding to exactly one class. During inference the target spike pattern with the smallest van Rossum distance from the output spike pattern determines the class. Extensive simulations indicate that the proposed system achieves a classification accuracy at par with that of state-of-the-art machine learning models.
C1 [Stanojevic, Ana; Cherubini, Giovanni; Wozniak, Stanislaw; Eleftheriou, Evangelos] IBM Res Zurich, Ruschlikon, Switzerland.
   [Stanojevic, Ana] Ecole Polytech Fed Lausanne EPFL, Lausanne, Switzerland.
   [Eleftheriou, Evangelos] Axelera AI, Zurich, Switzerland.
RP Stanojevic, A (corresponding author), IBM Res Zurich, Ruschlikon, Switzerland.; Stanojevic, A (corresponding author), Ecole Polytech Fed Lausanne EPFL, Lausanne, Switzerland.
EM ans@zurich.ibm.com
CR Bellec G, 2018, ADV NEUR IN, V31
   Bellec G, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-17236-y
   Bohnstingl T, 2022, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2022.3153985
   Bohte SM, 2011, LECT NOTES COMPUT SC, V6791, P60, DOI 10.1007/978-3-642-21735-7_8
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Boybat I, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-04933-y
   Bulat A, 2019, ARXIV
   Cao ZQ, 2015, NEURAL COMPUT APPL, V26, P1839, DOI 10.1007/s00521-015-1848-5
   Cherubini G, 2016, COMPUTER, V49, P43, DOI 10.1109/MC.2016.117
   Comsa IM, 2022, IEEE T NEUR NET LEAR, V33, P5939, DOI 10.1109/TNNLS.2021.3071976
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Courbariaux M., 2016, ARXIV
   Dasgupta S, 2017, SCIENCE, V358, P793, DOI 10.1126/science.aam9868
   Devlin J., 2018, ARXIV, DOI 10.18653/v1/N19-1423
   Eichler K, 2017, NATURE, V548, P175, DOI 10.1038/nature23455
   Fabre-Thorpe M, 1998, NEUROREPORT, V9, P303, DOI 10.1097/00001756-199801260-00023
   Farsa EZ, 2019, IEEE T CIRCUITS-II, V66, P1582, DOI 10.1109/TCSII.2019.2890846
   Furui S, 2012, IEEE SIGNAL PROC MAG, V29, P16, DOI 10.1109/MSP.2012.2209906
   Garain A, 2021, NEURAL COMPUT APPL, V33, P12591, DOI 10.1007/s00521-021-05910-1
   Gardner B, 2015, ARXIV
   Gardner B, 2015, NEURAL COMPUT, V27, P2548, DOI 10.1162/NECO_a_00790
   Geifman YG, 2018, US
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   Ghosh-Dastidar S, 2009, ADV INTEL SOFT COMPU, V61, P167
   Gollisch T, 2008, SCIENCE, V319, P1108, DOI 10.1126/science.1149639
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Greene Derek, 2006, P 23 INT C MACH LEAR, P377
   Horowitz M, 2014, ISSCC DIG TECH PAP I, V57, P10, DOI 10.1109/ISSCC.2014.6757323
   Hu SG, 2021, NEURAL COMPUT APPL, V33, P12317, DOI 10.1007/s00521-021-05832-y
   HUBEL DH, 1959, J PHYSIOL-LONDON, V148, P574, DOI 10.1113/jphysiol.1959.sp006308
   Huh D., 2017, ARXIV
   Hunsberger E., 2015, ARXIV
   Jankowski Mikolaj, 2020, IEEE INT WORK SIGN P, P1
   Jimenez-Romero C, 2017, NEURAL COMPUT APPL, V28, pS755, DOI 10.1007/s00521-016-2398-1
   Johansson RS, 2004, NAT NEUROSCI, V7, P170, DOI 10.1038/nn1177
   Kingma DP., 2017, ARXIV
   Krizhevsky Alex, 2009, LEARNING MULTIPLE LA
   Kubke MF, 2002, J NEUROSCI, V22, P7671
   Lin M., 2020, ADV NEURAL INFORM PR, V33, P7474
   Lin Mingbao, 2021, ARXIV
   Liu JX, 2021, NEURAL COMPUT APPL, V33, P11753, DOI 10.1007/s00521-021-05817-x
   Liu ZC, 2018, LECT NOTES COMPUT SC, V11219, P747, DOI 10.1007/978-3-030-01267-0_44
   Luo YH, 2022, NEURAL COMPUT APPL, V34, P9967, DOI 10.1007/s00521-022-06984-1
   Manning C., 1999, FDN STAT NATURAL LAN
   Marimuthu, 2010, ARXIV
   Mostafa H, 2018, IEEE T NEUR NET LEAR, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Ponulak F, 2011, ACTA NEUROBIOL EXP, V71, P409
   Qaiser S., 2018, INT J COMPUTER APPL, V181, P25, DOI [10.5120/ijca2018917395, DOI 10.5120/IJCA2018917395]
   Ranjan JAK, 2020, J SUPERCOMPUT, V76, P6545, DOI 10.1007/s11227-019-02881-y
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Ryali C. K., 2020, PROC 37 INT C MACH L, P8295
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Sebastian A, 2018, J APPL PHYS, V124, DOI 10.1063/1.5042413
   She XY, 2019, DES AUT TEST EUROPE, P450, DOI [10.23919/DATE.2019.8714846, 10.23919/date.2019.8714846]
   SIMONS G, 1971, ANN MATH STAT, V42, P1735, DOI 10.1214/aoms/1177693172
   Sjöström PJ, 2008, PHYSIOL REV, V88, P769, DOI 10.1152/physrev.00016.2007
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Stanojevic A, 2020, 27193 NBER, P1, DOI DOI 10.1353/ECA.2020.0000
   Strubell E., 2019, ARXIV
   Sze V, 2019, NEURIPS, V138
   Togaçar M, 2021, NEURAL COMPUT APPL, V33, P6147, DOI 10.1007/s00521-020-05388-3
   van Rossum MCW, 2001, NEURAL COMPUT, V13, P751, DOI 10.1162/089976601300014321
   Venkatesan V, 2018, IEEE INT CONGR BIG, P232, DOI 10.1109/BigDataCongress.2018.00040
   Volobuev, 2011, NAT SCI, V3, P53
   Wang Y, 2021, P IEEECVF INT C COMP, P5360
   Wozniak S, 2020, NAT MACH INTELL, V2, P325, DOI 10.1038/s42256-020-0187-0
   Xiao H., 2017, ARXIV170807747
   Xu ZH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5178, DOI 10.1109/ICCV48922.2021.00515
   Zambrano D, 2017, ARXIV
   Zechun Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P143, DOI 10.1007/978-3-030-58568-6_9
   Zenke F, 2018, NEURAL COMPUT, V30, P1514, DOI 10.1162/neco_a_01086
   Zhang AG, 2019, NEUROCOMPUTING, V365, P102, DOI 10.1016/j.neucom.2019.07.009
   Zhao DC, 2020, FRONT COMPUT NEUROSC, V14, DOI 10.3389/fncom.2020.576841
NR 74
TC 1
Z9 1
U1 5
U2 23
PD MAR
PY 2023
VL 35
IS 9
SI SI
BP 7017
EP 7033
DI 10.1007/s00521-022-07910-1
EA DEC 2022
UT WOS:000894483400004
DA 2023-11-16
ER

PT C
AU Yao, HY
   Huang, HP
   Huang, YC
   Lo, CC
AF Yao, Huang-Yu
   Huang, Hsuan-Pei
   Huang, Yu-Chi
   Lo, Chung-Chuan
GP IEEE
TI Flyintel - a Platform for Robot Navigation based on a Brain-Inspired
   Spiking Neural Network
SO 2019 IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE CIRCUITS
   AND SYSTEMS (AICAS 2019)
DT Proceedings Paper
CT 1st IEEE International Conference on Artificial Intelligence Circuits
   and Systems (AICAS)
CY MAR 18-20, 2019
CL Hsinchu, TAIWAN
DE spiking neural network; navigation; robotics; Drosophila; central
   complex
AB Spiking neural networks (SNN) are regarded by many as the "third generation network" that will solve computation problems in a more biologically realistic way. In our project, we design a robotic platform controlled by a user-defined SNN in order to develop a next generation artificial intelligence robot with high flexibility. This paper describes the preliminary progress of the project. We first implement a basic simple decision network and the robot is able to perform a basic but vital foraging and risk-avoiding task. Next, we implement the neural network of the fruit fly central complex in order to endow the robot with spatial orientation memory, a crucial function underlying the ability of spatial navigation.
C1 [Yao, Huang-Yu; Huang, Hsuan-Pei; Huang, Yu-Chi; Lo, Chung-Chuan] Natl Tsing Hua Univ, Inst Syst Neurosci, Hsinchu 30013, Taiwan.
RP Yao, HY (corresponding author), Natl Tsing Hua Univ, Inst Syst Neurosci, Hsinchu 30013, Taiwan.
EM huangyu_yao@lolab-nthu.org; peihuang@lolab-nthu.org; hyc@lolab-nthu.org;
   cclo@mx.nthu.edu.tw
CR Fernandes E, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL TECHNOLOGY (ICIT), P3320, DOI 10.1109/ICIT.2015.7125590
   Huang YC, 2019, FRONT NEUROINFORM, V12, DOI 10.3389/fninf.2018.00099
   Kreiser R, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351509
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Meng ZH, 2017, IEEE/SICE I S SYS IN, P651, DOI 10.1109/SII.2017.8279295
   Moser EI, 2008, ANNU REV NEUROSCI, V31, P69, DOI 10.1146/annurev.neuro.31.061307.090723
   Su TS, 2017, NAT COMMUN, V8, DOI 10.1038/s41467-017-00191-6
   Tang G., 2018, ICONS 18
NR 8
TC 1
Z9 2
U1 0
U2 6
PY 2019
BP 219
EP 220
UT WOS:000493095400051
DA 2023-11-16
ER

PT C
AU Pellegrini, T
   Zimmer, R
   Masquelier, T
AF Pellegrini, Thomas
   Zimmer, Romain
   Masquelier, Timothee
GP IEEE
TI LOW-ACTIVITY SUPERVISED CONVOLUTIONAL SPIKING NEURAL NETWORKS APPLIED TO
   SPEECH COMMANDS RECOGNITION
SO 2021 IEEE SPOKEN LANGUAGE TECHNOLOGY WORKSHOP (SLT)
SE IEEE Workshop on Spoken Language Technology
DT Proceedings Paper
CT IEEE Spoken Language Technology Workshop (SLT)
CY JAN 19-22, 2021
CL ELECTR NETWORK
DE Spiking neural networks; surrogate gradient; speech command recognition
AB Deep Neural Networks (DNNs) are the current state-of-the-art models in many speech related tasks. There is a growing interest, though, for more biologically realistic, hardware friendly and energy efficient models, named Spiking Neural Networks (SNNs). Recently, it has been shown that SNNs can be trained efficiently, in a supervised manner, using backpropagation with a surrogate gradient trick. In this work, we report speech command (SC) recognition experiments using supervised SNNs. We explored the Leaky-Integrate-Fire (LIF) neuron model for this task, and show that a model comprised of stacked dilated convolution spiking layers can reach an error rate very close to standard DNNs on the Google SC v1 dataset: 5.5%, while keeping a very sparse spiking activity, below 5%, thank to a new regularization term. We also show that modeling the leakage of the neuron membrane potential is useful, since the LIF model outperformed its non-leaky model counterpart significantly.
C1 [Pellegrini, Thomas; Zimmer, Romain] Univ Toulouse, IRIT, Toulouse, France.
   [Zimmer, Romain; Masquelier, Timothee] Univ Toulouse 3, CNRS, CERCO UMR 5549, Toulouse, France.
RP Pellegrini, T (corresponding author), Univ Toulouse, IRIT, Toulouse, France.
CR Adrian ED, 1926, J PHYSIOL-LONDON, V61, P151, DOI 10.1113/jphysiol.1926.sp002281
   Bellec G., 2018, ADV NEURAL INFORM PR
   de Andrade Douglas Coimbra, 2018, NEURAL ATTENTION MOD
   Gerstner W., 2002, SPIKING NEURON MODEL
   Gerstner W, 2009, SCIENCE, V326, P379, DOI 10.1126/science.1181936
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Kobayashi R, 2009, FRONT COMPUT NEUROSC, V3, DOI 10.3389/neuro.10.009.2009
   Liu L., 2019, ARXIV PREPRINT ARXIV
   Maass W, 1999, INFORM COMPUT, V153, P26, DOI 10.1006/inco.1999.2806
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Olshausen BA, 2003, J COGNITIVE NEUROSCI, V15, P154, DOI 10.1162/089892903321107891
   Santos, 2015, LIBROSA V0 4 0
   STEIN RB, 1967, BIOPHYS J, V7, P37, DOI 10.1016/S0006-3495(67)86574-3
   Tavanaei A, 2017, LECT NOTES COMPUT SC, V10639, P899, DOI 10.1007/978-3-319-70136-3_95
   Warden Pete, 2018, SPEECH COMMANDS DATA
   Wu JC, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351221
   Wu JB, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00199
   Zhang ML, 2019, AAAI CONF ARTIF INTE, P1327
   Zhang Y, 2015, IEEE T NEUR NET LEAR, V26, P2635, DOI 10.1109/TNNLS.2015.2388544
   Zimmer Romain, 2019, ARXIV PREPRINT ARXIV
NR 20
TC 17
Z9 17
U1 0
U2 8
PY 2021
BP 97
EP 103
DI 10.1109/SLT48900.2021.9383587
UT WOS:000663633300014
DA 2023-11-16
ER

PT C
AU Zhang, ZM
   Wu, QX
   Wang, X
   Sun, QY
AF Zhang, Zhenmin
   Wu, Qingxiang
   Wang, Xuan
   Sun, Qiyan
BE Xiao, Z
   Tong, Z
   Li, K
   Wang, X
   Li, K
TI Training Spiking Neural Networks With the Improved Grey-Level
   Co-occurrence Matrix Algorithm for Texture Analysis
SO 2015 11TH INTERNATIONAL CONFERENCE ON NATURAL COMPUTATION (ICNC)
DT Proceedings Paper
CT 11th International Conference on Natural Computation (ICNC) / 12th
   International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)
CY AUG 15-17, 2015
CL Zhangjiajie, PEOPLES R CHINA
DE spiking neural network; GLCM algorithm; Feature Extraction; Texture
   Classification
AB Texture refers to the tactile impression, such as rough, silky, bumpy, and other texture terms. The Grey-Level Co-occurrence Matrix (GLCM) algorithm is widely used in visual images for texture feature extraction, image structure characterization analysis and texture classification. The GLCM can not only give the statistics of pixel gray values occur in an image, but also give multiple characteristics of the images. Since the primate brain, which is constructed with spiking neurons, has excellent performance in terms of image feature extraction, the improved GLCM algorithm is used to train a spiking neural network and also to simulate the brain's ability about extract key information and utilize these extracted feature information to classify different texture image. Experimental results in this article show that this combination of the GLCM and spiking neural network can effectively extract image features, and the texture classification results is also to achieve satisfactory effect.
C1 [Zhang, Zhenmin; Wu, Qingxiang; Wang, Xuan; Sun, Qiyan] Fujian Normal Univ, Coll Photon & Elect Engn, Fuzhou, Peoples R China.
RP Wu, QX (corresponding author), Fujian Normal Univ, Coll Photon & Elect Engn, Fuzhou, Peoples R China.
CR [Anonymous], SYSTEMS MAN CYBERNET
   Beck MW, 2014, ECOL INDIC, V45, P195, DOI 10.1016/j.ecolind.2014.04.002
   Benco M, 2007, RADIOENGINEERING, V16, P64
   Benco M, 2014, INT J ADV ROBOT SYST, V11, DOI 10.5772/58692
   Bergounioux M, 2014, ADV IMAG ELECT PHYS, V181, P35, DOI 10.1016/B978-0-12-800091-5.00002-1
   Brodatz P., 1966, TEXTURE PHOTOGRAPHIC
   Brownstone RM, 2015, NEURON, V86, P9, DOI 10.1016/j.neuron.2015.03.056
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Kosik KS, 2013, NATURE, V503, P31, DOI 10.1038/503031a
   Kuebler E.S., 2013, IEEE IJCNN
   Mallat B. S., 2007, MALLAT WAVELET TOUR, V31, P85
   Masland RH, 2001, NAT NEUROSCI, V4, P877, DOI 10.1038/nn0901-877
   Mirzapour F, 2013, IRAN CONF ELECTR ENG
   Nazemi A, 2015, NEUROCOMPUTING, V152, P369, DOI 10.1016/j.neucom.2014.10.054
   Ojala T., 2002, OUTEX NEW FRAMEWORK, V1, P706
   Sadtler PT, 2014, NATURE, V512, P423, DOI 10.1038/nature13665
   Saroja GAS, 2013, 2013 IEEE CONFERENCE ON INFORMATION AND COMMUNICATION TECHNOLOGIES (ICT 2013), P1319
   Schultz SK, 2001, AM J PSYCHIAT, V158, P662, DOI 10.1176/appi.ajp.158.4.662
   Tamura H., 1978, TEXTURAL FEATURES CO, V8, P473
   Van de Wouwer G, 1999, IEEE T IMAGE PROCESS, V8, P592, DOI 10.1109/83.753747
   Wu Q.X., 2006, KNOWLEDGE REPRESENTA, V4, P2801
   Wu QX, 2013, NEUROCOMPUTING, V116, P3, DOI 10.1016/j.neucom.2012.01.046
   Wu QX, 2009, LECT NOTES ARTIF INT, V5755, P21
   Xue MS, 2014, NATURE, V511, P596, DOI 10.1038/nature13321
   Zhang J, 2008, HPCC 2008: 10TH IEEE INTERNATIONAL CONFERENCE ON HIGH PERFORMANCE COMPUTING AND COMMUNICATIONS, PROCEEDINGS, P782, DOI 10.1109/HPCC.2008.55
NR 25
TC 0
Z9 0
U1 1
U2 3
PY 2015
BP 1069
EP 1074
UT WOS:000380617000185
DA 2023-11-16
ER

PT J
AU Kim, DW
   Yi, WS
   Choi, JY
   Ashiba, K
   Baek, JU
   Jun, HS
   Kim, JJ
   Park, JG
AF Kim, Dong Won
   Yi, Woo Seok
   Choi, Jin Young
   Ashiba, Kei
   Baek, Jong Ung
   Jun, Han Sol
   Kim, Jae Joon
   Park, Jea Gun
TI Double MgO-Based Perpendicular Magnetic Tunnel Junction for Artificial
   Neuron
SO FRONTIERS IN NEUROSCIENCE
DT Article
DE neuromorphic; MRAM; spiking neuron; spiking neural network; artificial
   neuron
ID SPIKING; NETWORKS; MODEL
AB A perpendicular spin transfer torque (p-STT)-based neuron was developed for a spiking neural network (SNN). It demonstrated the integration behavior of a typical neuron in an SNN; in particular, the integration behavior corresponding to magnetic resistance change gradually increased with the input spike number. This behavior occurred when the spin electron directions between double Co2Fe6B2 free and pinned layers in the p-STT-based neuron were switched from parallel to antiparallel states. In addition, a neuron circuit for integrate-and-fire operation was proposed. Finally, pattern-recognition simulation was performed for a single-layer SNN.
C1 [Kim, Dong Won; Baek, Jong Ung; Jun, Han Sol; Park, Jea Gun] Hanyang Univ, Dept Nanoscale Semicond Engn, Seoul, South Korea.
   [Yi, Woo Seok; Kim, Jae Joon] Pohang Univ Sci & Technol, Dept Creat IT Engn, Pohang, South Korea.
   [Choi, Jin Young] Hanyang Univ, MRAM Ctr, Dept Elect & Comp Engn, Seoul, South Korea.
   [Ashiba, Kei; Park, Jea Gun] SUMCO Corp, Wafer Engn Dept, Imari, Japan.
RP Park, JG (corresponding author), Hanyang Univ, Dept Nanoscale Semicond Engn, Seoul, South Korea.; Park, JG (corresponding author), SUMCO Corp, Wafer Engn Dept, Imari, Japan.
EM parkjgl@hanyang.ac.kr
CR [Anonymous], TECH DIG INT ELECT D
   [Anonymous], PROC CVPR IEEE
   Burr G., 2014, P IEEE INT EL DEV M
   Du ZD, 2015, PROCEEDINGS OF THE 48TH ANNUAL IEEE/ACM INTERNATIONAL SYMPOSIUM ON MICROARCHITECTURE (MICRO-48), P494, DOI 10.1145/2830772.2830789
   Dutta S, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-07418-y
   Gentet LJ, 2000, BIOPHYS J, V79, P314, DOI 10.1016/S0006-3495(00)76293-X
   Grollier J, 2016, P IEEE, V104, P2024, DOI 10.1109/JPROC.2016.2597152
   Hansen M, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00091
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/TPAMI.2019.2913372, 10.1109/CVPR.2018.00745]
   Indiveri G, 2013, NANOTECHNOLOGY, V24, DOI 10.1088/0957-4484/24/38/384010
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Johnson AP, 2018, IEEE T CIRCUITS-I, V65, P687, DOI 10.1109/TCSI.2017.2726763
   Kondo K, 2018, J PHYS D APPL PHYS, V51, DOI 10.1088/1361-6463/aad592
   Kurenkov A, 2019, ADV MATER, V31, DOI 10.1002/adma.201900636
   Lee DY, 2016, SCI REP-UK, V6, DOI 10.1038/srep38125
   Lee DY, 2016, NANOSCALE RES LETT, V11, DOI 10.1186/s11671-016-1637-9
   Lee JM, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00191
   Lee SE, 2016, NPG ASIA MATER, V8, DOI 10.1038/am.2016.162
   Liyanagedera CM, 2017, PHYS REV APPL, V8, DOI 10.1103/PhysRevApplied.8.064017
   MacLaren JM, 2001, J APPL PHYS, V89, P6895, DOI 10.1063/1.1357839
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Mizrahi A, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-03963-w
   Monroe D, 2014, COMMUN ACM, V57, P13, DOI 10.1145/2601069
   Querlioz D, 2015, P IEEE, V103, P1398, DOI 10.1109/JPROC.2015.2437616
   Sengupta A, 2016, SCI REP-UK, V6, DOI 10.1038/srep30039
   Sharmin S, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-11732-w
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Sourikopoulos I, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00123
   Srinivasan G, 2017, DES AUT TEST EUROPE, P530, DOI 10.23919/DATE.2017.7927045
   Suzuki M, 2016, ACTA MATER, V106, P155, DOI 10.1016/j.actamat.2016.01.011
   Szegedy C., 2015, 2015 IEEE C COMPUTER, P1, DOI [10.1109/CVPR.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Torrejon J, 2017, NATURE, V547, P428, DOI 10.1038/nature23011
   Tuma T, 2016, NAT NANOTECHNOL, V11, P693, DOI [10.1038/NNANO.2016.70, 10.1038/nnano.2016.70]
   Victora RH, 2003, IEEE T MAGN, V39, P710, DOI 10.1109/TMAG.2003.808998
   Zahari F, 2015, AIMS MATER SCI, V2, P203, DOI 10.3934/matersci.2015.3.203
   Zhang DM, 2016, IEEE INT SYMP NANO, P173, DOI 10.1145/2950067.2950105
   Ziegler M, 2015, IEEE T BIOMED CIRC S, V9, P197, DOI 10.1109/TBCAS.2015.2410811
NR 38
TC 13
Z9 13
U1 0
U2 9
PD APR 30
PY 2020
VL 14
AR 309
DI 10.3389/fnins.2020.00309
UT WOS:000536180200001
DA 2023-11-16
ER

PT J
AU Lin, J
   Yuan, JS
AF Lin, Jie
   Yuan, Jiann-Shiun
TI Analysis and Simulation of Capacitor-Less ReRAM-Based Stochastic Neurons
   for the in-Memory Spiking Neural Network
SO IEEE TRANSACTIONS ON BIOMEDICAL CIRCUITS AND SYSTEMS
DT Article; Proceedings Paper
CT IEEE Biomedical Circuits and Systems Conference (BioCAS)
CY OCT 19-21, 2017
CL Torino, ITALY
DE Neuromorphic; resistive random-access memory (ReRAM); spiking neural
   network; stochastic neuron; unsupervised learning
ID DESIGN; ENERGY; NOISE; MODEL
AB The stochastic neuron is a key for event-based probabilistic neural networks. We propose a stochastic neuron using a metal-oxide resistive random-access memory (ReRAM). The ReRAM's conducting filament with built-in stochasticity is used to mimic the neuron's membrane capacitor, which temporally integrates input spikes. A capacitor-less neuron circuit is designed, laid out, and simulated. The output spiking train of the neuron obeys the Poisson distribution. Using the 65-nm CMOS technology node, the area of the neuron is 14 x 5 mu m(2), which is one ninth the size of a 1-pF capacitor. The average power consumption of the neuron is 1.289 mu W. We introduce the neural array-A modified one-transistor-one-ReRAM (1T1R) crossbar that integrates the ReRAM neurons with ReRAM synapses to form a compact and energy efficient in-memory spiking neural network. A spiking deep belief network (DBN) with a noisy rectified linear unit (NReLU) is trained and mapped to the spiking DBN using the proposed ReRAM neurons. Simulation results show that the ReRAM neuron-based DBN is able to recognize the handwritten digits with 94.7% accuracy and is robust against theReRAMprocess variation effect.
C1 [Lin, Jie; Yuan, Jiann-Shiun] Univ Cent Florida, Dept Elect & Comp Engn, Orlando, FL 32826 USA.
RP Yuan, JS (corresponding author), Univ Cent Florida, Dept Elect & Comp Engn, Orlando, FL 32826 USA.
EM ljie@knights.ucf.edu; yuanj@mail.ucf.edu
CR ABBOTT LF, 1993, PHYS REV E, V48, P1483, DOI 10.1103/PhysRevE.48.1483
   Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   Al-Shedivat M, 2015, IEEE J EM SEL TOP C, V5, P242, DOI 10.1109/JETCAS.2015.2435512
   Ankit A, 2017, DES AUT CON, DOI 10.1145/3061639.3062311
   [Anonymous], P EUR WORKSH CMOS VA, DOI DOI 10.1109/VARI.2014.6957074
   [Anonymous], 2015, INT J COMPUT VISION, DOI DOI 10.1007/s11263-014-0788-3
   [Anonymous], P 2011 INT EL DEV M
   [Anonymous], 2003, BSIM4 3 0 MOSFET MOD
   [Anonymous], P WORLD C NEUR
   [Anonymous], 2016, 2016 IEE INT C REB C, DOI [10.1109/ICRC.2016.7738691, DOI 10.1109/ICRC.2016.7738691]
   [Anonymous], ADV NEURAL INFORM PR, DOI DOI 10.1103/PHYSREVAPPLIED.9.044036
   [Anonymous], P 2016 IEEE INT EL D
   Chechik G, 1999, NEURAL COMPUT, V11, P2061, DOI 10.1162/089976699300016089
   Cui JW, 2016, IEEE INT SYMP CIRC S, P121, DOI 10.1109/ISCAS.2016.7527185
   Diehl P.U., 2015, 2015 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2015.7280696
   Dong XY, 2012, IEEE T COMPUT AID D, V31, P994, DOI 10.1109/TCAD.2012.2185930
   Fang Z, 2010, IEEE ELECTR DEVICE L, V31, P476, DOI 10.1109/LED.2010.2041893
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hu M, 2016, DES AUT CON, DOI 10.1145/2897937.2898010
   Indiveri G, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00073
   Jiang ZZ, 2014, INT CONF SIM SEMI PR, P41, DOI 10.1109/SISPAD.2014.6931558
   Jo SH, 2009, NANO LETT, V9, P496, DOI 10.1021/nl803669s
   Jolivet R, 2006, J COMPUT NEUROSCI, V21, P35, DOI 10.1007/s10827-006-7074-5
   Kwon MW, 2017, J NANOSCI NANOTECHNO, V17, P3038, DOI 10.1166/jnn.2017.14025
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee MJ, 2011, NAT MATER, V10, P625, DOI [10.1038/NMAT3070, 10.1038/nmat3070]
   Li S., 2016, PROC 53ND ACM EDAC I, P1
   Lin J, 2017, J LOW POWER ELECTRON, V13, P497, DOI 10.1166/jolpe.2017.1503
   Lin J, 2016, 2016 13TH IEEE INTERNATIONAL CONFERENCE ON SOLID-STATE AND INTEGRATED CIRCUIT TECHNOLOGY (ICSICT), P713, DOI 10.1109/ICSICT.2016.7999020
   Lin J, 2016, J LOW POWER ELECTRON, V12, P218, DOI 10.1166/jolpe.2016.1445
   Maass W, 2014, P IEEE, V102, P860, DOI 10.1109/JPROC.2014.2310593
   McPherson J, 2003, APPL PHYS LETT, V82, P2121, DOI 10.1063/1.1565180
   Moreno-Bote R, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003522
   Nair V., 2010, PROC 27 INT C INT C
   Naous R, 2016, IEEE T NANOTECHNOL, V15, P15, DOI 10.1109/TNANO.2015.2493960
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   Palma G, 2013, PROCEEDINGS OF THE 2013 IEEE/ACM INTERNATIONAL SYMPOSIUM ON NANOSCALE ARCHITECTURES (NANOARCH), P95, DOI 10.1109/NanoArch.2013.6623051
   Posch C, 2011, IEEE J SOLID-ST CIRC, V46, P259, DOI 10.1109/JSSC.2010.2085952
   Roy K, 2003, P IEEE, V91, P305, DOI 10.1109/JPROC.2002.808156
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Tuma T, 2016, NAT NANOTECHNOL, V11, P693, DOI [10.1038/NNANO.2016.70, 10.1038/nnano.2016.70]
   Wong HSP, 2012, P IEEE, V100, P1951, DOI 10.1109/JPROC.2012.2190369
   Yu SM, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00186
   Zhao CY, 2016, IEEE T MULTI-SCALE C, V2, P265, DOI 10.1109/TMSCS.2016.2607164
NR 44
TC 16
Z9 19
U1 0
U2 11
PD OCT
PY 2018
VL 12
IS 5
BP 1004
EP 1017
DI 10.1109/TBCAS.2018.2843286
UT WOS:000448032000005
DA 2023-11-16
ER

PT J
AU Song, S
   Kim, M
   Jeon, B
   Ryu, D
   Kim, S
   Lee, K
   Lee, JH
   Kim, JJ
   Shim, W
   Kwon, D
   Park, BG
AF Song, Seunghwan
   Kim, Munhyeon
   Jeon, Bosung
   Ryu, Donghyun
   Kim, Sihyun
   Lee, Kitae
   Lee, Jong-Ho
   Kim, Jae-Joon
   Shim, Wonbo
   Kwon, Daewoong
   Park, Byung-Gook
TI Spiking Neural Network With Weight-Sharing Synaptic Array for
   Multi-input Processing
SO IEEE ELECTRON DEVICE LETTERS
DT Article
DE FeFETs; Neurons; Parallel processing; Logic gates; Integrated circuit
   modeling; Biological neural networks; Transistors; Spiking neural
   networks (SNNs); multi-input processing SNN system; ferroelectric FETs
ID FLASH
AB A multi-input processing spiking neural network inference system (MSS) is proposed to enhance the parallel processing capabilities of the spiking neural network (SNN) inference relative to the conventional SNN inference. Processing multiple input samples using MSS with shared synaptic arrays for compute-in-memory drastically reduces the number of synaptic arrays assigned for parallel processing, thereby minimizing the effort required by parallel processing networks. A shared weight array is evaluated using the 4-bit quantization capabilities of a manufactured ferroelectric field-effect-transistor as a synaptic device. A batch action was implemented as a multi-input on a 3-layer fully connected network to verify the MSS. The benefits of MSS in energy consumption and area throughput are rigorously investigated and estimated based on the number of multi-input processing. It is proven that the simultaneously processing of multiple input samples using the proposed MSS boosts the energy and area efficiency by up to 9.12 and 242 times, respectively.
C1 [Song, Seunghwan; Kim, Munhyeon; Jeon, Bosung; Ryu, Donghyun; Kim, Sihyun; Lee, Kitae; Lee, Jong-Ho; Kim, Jae-Joon; Park, Byung-Gook] Seoul Natl Univ, Dept Elect & Comp Engn, Seoul 08826, South Korea.
   [Song, Seunghwan; Kim, Munhyeon; Jeon, Bosung; Ryu, Donghyun; Kim, Sihyun; Lee, Kitae; Lee, Jong-Ho; Kim, Jae-Joon; Park, Byung-Gook] Seoul Natl Univ, Interuniv Semicond Res Ctr, Seoul 08826, South Korea.
   [Shim, Wonbo] Seoul Natl Univ Sci & Technol, Dept Elect & Informat Engn, Seoul 01811, South Korea.
   [Kwon, Daewoong] Inha Univ, Dept Elect & Comp Engn, Incheon 22212, South Korea.
   [Kwon, Daewoong] Inha Univ, 3D Convergence Ctr, Incheon 22212, South Korea.
RP Park, BG (corresponding author), Seoul Natl Univ, Dept Elect & Comp Engn, Seoul 08826, South Korea.; Park, BG (corresponding author), Seoul Natl Univ, Interuniv Semicond Res Ctr, Seoul 08826, South Korea.; Shim, W (corresponding author), Seoul Natl Univ Sci & Technol, Dept Elect & Informat Engn, Seoul 01811, South Korea.; Kwon, D (corresponding author), Inha Univ, Dept Elect & Comp Engn, Incheon 22212, South Korea.; Kwon, D (corresponding author), Inha Univ, 3D Convergence Ctr, Incheon 22212, South Korea.
EM wbshim@seoultech.ac.kr; dw79kwon@inha.ac.kr; bgpark@snu.ac.kr
CR [Anonymous], 2017, INT EL DEVICES MEET
   [Anonymous], BSIM CMG TECHNICAL M
   Bengio Y., 2007, LARGE SCALE KERNEL M, V34, P1, DOI DOI 10.1038/NATURE14539
   Cruz-Albrecht JM, 2012, IEEE T BIOMED CIRC S, V6, P246, DOI 10.1109/TBCAS.2011.2174152
   Diehl PU, 2015, IEEE IJCNN
   Dutta S, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00634
   Hadidi R, 2018, IEEE ROBOT AUTOM LET, V3, P3709, DOI 10.1109/LRA.2018.2856261
   Han JH, 2020, TSINGHUA SCI TECHNOL, V25, P479, DOI 10.26599/TST.2019.9010019
   Huang YP, 2019, Arxiv, DOI arXiv:1811.06965
   Hwang S, 2018, IEEE ELECTR DEVICE L, V39, P1441, DOI 10.1109/LED.2018.2853635
   Kim CH, 2018, IEEE T ELECTRON DEV, V65, P1774, DOI 10.1109/TED.2018.2817266
   Kim H, 2018, IEEE ELECTR DEVICE L, V39, P630, DOI 10.1109/LED.2018.2809661
   Kim TH, 2020, IEEE T NANOTECHNOL, V19, P475, DOI 10.1109/TNANO.2020.2996814
   LeCun Y, 1995, HDB BRAIN THEORY NEU, V3361, P1995, DOI 10.5555/303568.303704
   Lee J, 2019, IEEE ELECTR DEVICE L, V40, P1358, DOI 10.1109/LED.2019.2928335
   Mulaosmanovic H, 2017, S VLSI TECH, pT176, DOI 10.23919/VLSIT.2017.7998165
   Ni K, 2018, INT EL DEVICES MEET
   Ni K, 2018, IEEE ELECTR DEVICE L, V39, P1656, DOI 10.1109/LED.2018.2872347
   Painkras E, 2013, IEEE J SOLID-ST CIRC, V48, P1943, DOI 10.1109/JSSC.2013.2259038
   Regev A, 2020, 2020 2ND IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE CIRCUITS AND SYSTEMS (AICAS 2020), P145, DOI [10.1109/AICAS48895.2020.9073840, 10.1109/aicas48895.2020.9073840]
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Wu J, 2017, NATL KEY LAB NOVEL S, V5, P495, DOI DOI 10.1007/978-3-642-28661-2-5
NR 22
TC 2
Z9 2
U1 4
U2 23
PD OCT
PY 2022
VL 43
IS 10
BP 1657
EP 1660
DI 10.1109/LED.2022.3197239
UT WOS:000861441600020
DA 2023-11-16
ER

PT J
AU Naveros, F
   Garrido, JA
   Carrillo, RR
   Ros, E
   Luque, NR
AF Naveros, Francisco
   Garrido, Jesus A.
   Carrillo, Richard R.
   Ros, Eduardo
   Luque, Niceto R.
TI Event- and Time-Driven Techniques Using Parallel CPU-GPU Co-processing
   for Spiking Neural Networks (vol 11, 7, 2017)
SO FRONTIERS IN NEUROINFORMATICS
DT Correction
DE event- and time-driven techniques; CPU; GPU; look-up table; spiking
   neural models; bi-fixed-step integration methods
C1 [Naveros, Francisco; Garrido, Jesus A.; Carrillo, Richard R.; Ros, Eduardo] Univ Granada, Res Ctr Informat & Commun Technol, Dept Comp Architecture & Technol, Granada, Spain.
   [Luque, Niceto R.] Vis Inst, Aging Vis & Act Lab, Paris, France.
   [Luque, Niceto R.] Pierre & Marie Curie Univ, CNRS, INSERM, Paris, France.
RP Ros, E (corresponding author), Univ Granada, Res Ctr Informat & Commun Technol, Dept Comp Architecture & Technol, Granada, Spain.; Luque, NR (corresponding author), Vis Inst, Aging Vis & Act Lab, Paris, France.; Luque, NR (corresponding author), Pierre & Marie Curie Univ, CNRS, INSERM, Paris, France.
EM eros@ugr.es; niceto.luque@inserm.fr
CR Naveros F, 2017, FRONT NEUROINFORM, V11, DOI 10.3389/fninf.2017.00007
NR 1
TC 5
Z9 5
U1 1
U2 9
PD MAY 4
PY 2018
VL 12
AR 24
DI 10.3389/fninf.2018.00024
UT WOS:000431446300001
DA 2023-11-16
ER

PT C
AU Taeckens, E
   Dong, R
   Shah, S
AF Taeckens, Elijah
   Dong, Ryan
   Shah, Sahil
GP IEEE
TI A Biologically Plausible Spiking Neural Network for Decoding Kinematics
   in the Hippocampus and Premotor Cortex
SO 2023 11TH INTERNATIONAL IEEE/EMBS CONFERENCE ON NEURAL ENGINEERING, NER
SE International IEEE EMBS Conference on Neural Engineering
DT Proceedings Paper
CT 11th International IEEE EMBS Conference on Neural Engineering (IEEE/EMBS
   NER)
CY APR 24-27, 2023
CL IEEE Engn Med & Biol Soc, Baltimore, MD
HO IEEE Engn Med & Biol Soc
AB This work presents a spiking neural network for predicting kinematics from neural data towards accurate and energy-efficient brain machine interface. A brain machine interface is a technological system that interprets neural signals to allow motor impaired patients to control prosthetic devices. Spiking neural networks have the potential to improve brain machine interface technology due to their low power cost and close similarity to biological neural structures. The SNN in this study uses the leaky integrate-and-fire model to simulate the behavior of neurons, and learns using a local learning method that uses surrogate gradient to learn the parameters of the network. The network implements a novel continuous time output encoding scheme that allows for regression-based learning. The SNN is trained and tested offline on neural and kinematic data recorded from the premotor cortex of a primate and the hippocampus of a rat. The model is evaluated by finding the correlation between the predicted kinematic data and true kinematic data, and achieves peak Pearson Correlation Coefficients of 0.77 for the premotor cortex recordings and 0.80 for the hippocampus recordings. The accuracy of the model is benchmarked against a Kalman filter decoder and a LSTM network, as well as a spiking neural network trained with backpropagation to compare the effects of local learning.
C1 [Taeckens, Elijah; Dong, Ryan; Shah, Sahil] Univ Maryland, Dept Elect & Comp Engn, College Pk, MD 20742 USA.
RP Shah, S (corresponding author), Univ Maryland, Dept Elect & Comp Engn, College Pk, MD 20742 USA.
EM sshah389@umd.edu
CR Arthur JV, 2011, IEEE T CIRCUITS-I, V58, P1034, DOI 10.1109/TCSI.2010.2089556
   Boi F, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00563
   Chowdhury S. N., 2022, 20 IEEE INTERREGIONA
   Dethier J, 2013, J NEURAL ENG, V10, DOI 10.1088/1741-2560/10/3/036008
   Gehrig M, 2020, IEEE INT CONF ROBOT, P4195, DOI [10.1109/icra40945.2020.9197133, 10.1109/ICRA40945.2020.9197133]
   Glaser Joshua I, 2020, eNeuro, V7, DOI 10.1523/ENEURO.0506-19.2020
   Indiveri G, 2003, PROCEEDINGS OF THE 2003 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL IV, P820
   Jackson A, 2006, IEEE T NEUR SYS REH, V14, P187, DOI 10.1109/TNSRE.2006.875547
   Kaiser J, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00424
   Kim SP, 2008, J NEURAL ENG, V5, P455, DOI 10.1088/1741-2560/5/4/010
   Lawlor PN, 2018, J COMPUT NEUROSCI, V45, P173, DOI 10.1007/s10827-018-0696-6
   Liao JW, 2022, 2022 IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE CIRCUITS AND SYSTEMS (AICAS 2022): INTELLIGENT TECHNOLOGY IN THE POST-PANDEMIC ERA, P134, DOI 10.1109/AICAS54282.2022.9869846
   MEAD C, 1990, P IEEE, V78, P1629, DOI 10.1109/5.58356
   Mizuseki K, 2009, MULTIUNIT RECORDINGS
   Mizuseki K, 2009, NEURON, V64, P267, DOI 10.1016/j.neuron.2009.08.037
   Musallam S, 2004, SCIENCE, V305, P258, DOI 10.1126/science.1097938
   N. S. C. I. S. Center, 2022, TRAUMATIC SPINAL COR
   Neftci EO, 2019, Arxiv, DOI arXiv:1901.09948
   Perich M. G, 2018, EXTRACELLULAR NEURAL
   Shah S, 2019, I IEEE EMBS C NEUR E, P1138, DOI [10.1109/NER.2019.8717137, 10.1109/ner.2019.8717137]
   Shaikh S., 2022, HDB BIOCHIPS, P869
   Snoek J., 2012, ADV NEURAL INFORM PR, V25, P1
   Stuijt J, 2021, BRAIN EVENT DRIVEN F
   Sussillo D, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms13749
NR 24
TC 0
Z9 0
U1 0
U2 0
PY 2023
DI 10.1109/NER52421.2023.10123745
UT WOS:001009053700033
DA 2023-11-16
ER

PT J
AU Woodward, A
   Froese, T
   Ikegami, T
AF Woodward, Alexander
   Froese, Tom
   Ikegami, Takashi
TI Neural coordination can be enhanced by occasional interruption of normal
   firing patterns: A self-optimizing spiking neural network model
SO NEURAL NETWORKS
DT Article
DE Self-optimization; Hopfield network; Spiking neurons; Global neural
   coordination; Psychedelics; Altered states of consciousness
ID OPTIMIZATION; NEURONS; PLASTICITY; MEMORY; TIME
AB The state space of a conventional Hopfield network typically exhibits many different attractors of which only a small subset satisfies constraints between neurons in a globally optimal fashion. It has recently been demonstrated that combining Hebbian learning with occasional alterations of normal neural states avoids this problem by means of self-organized enlargement of the best basins of attraction. However, so far it is not clear to what extent this process of self-optimization is also operative in real brains. Here we demonstrate that it can be transferred to more biologically plausible neural networks by implementing a self-optimizing spiking neural network model. In addition, by using this spiking neural network to emulate a Hopfield network with Hebbian learning, we attempt to make a connection between rate-based and temporal coding based neural systems. Although further work is required to make this model more realistic, it already suggests that the efficacy of the self-optimizing process is independent from the simplifying assumptions of a conventional Hopfield network. We also discuss natural and cultural processes that could be responsible for occasional alteration of neural firing patterns in actual brains. (C) 2014 Elsevier Ltd. All rights reserved.
C1 [Woodward, Alexander; Ikegami, Takashi] Univ Tokyo, Grad Sch Arts & Sci, Tokyo 1538902, Japan.
   [Froese, Tom] Univ Nacl Autonoma Mexico, Inst Invest Matemat Aplicadas & Sistemas, Dept Ciencias Computac, Mexico City 04510, DF, Mexico.
   [Froese, Tom] Univ Nacl Autonoma Mexico, Ctr Ciencias Complejidad, Mexico City 04510, DF, Mexico.
RP Woodward, A (corresponding author), Univ Tokyo, Grad Sch Arts & Sci, Tokyo 1538902, Japan.
EM alex.w.nz@gmail.com
CR [Anonymous], 1908, RITES PASSAGE
   Bailey CH, 2000, NAT REV NEUROSCI, V1, P11, DOI 10.1038/35036191
   BEER RD, 1995, ADAPT BEHAV, V3, P469, DOI 10.1177/105971239500300405
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Carhart-Harris RL, 2014, FRONT HUM NEUROSCI, V8, DOI 10.3389/fnhum.2014.00020
   Davies AP, 2011, ARTIF LIFE, V17, P167, DOI 10.1162/artl_a_00030
   Froese T, 2013, ADAPT BEHAV, V21, P199, DOI 10.1177/1059712313483145
   Gerstner W., 2002, SPIKING NEURON MODEL
   HOPFIELD JJ, 1985, BIOL CYBERN, V52, P141
   HOPFIELD JJ, 1984, P NATL ACAD SCI-BIOL, V81, P3088, DOI 10.1073/pnas.81.10.3088
   HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554
   Huang YY, 2004, P NATL ACAD SCI USA, V101, P859, DOI 10.1073/pnas.2237201100
   Koch C, 2000, NAT NEUROSCI, V3, P1171, DOI 10.1038/81444
   Kryzhanovsky B, 2008, LECT NOTES ARTIF INT, V5097, P89, DOI 10.1007/978-3-540-69731-2_10
   Kupferschmidt K, 2014, SCIENCE, V345, P18, DOI 10.1126/science.345.6192.18
   Maass W, 1997, NETWORK-COMP NEURAL, V8, P355, DOI 10.1088/0954-898X/8/4/002
   Maass W, 1997, NEURAL COMPUT, V9, P279, DOI 10.1162/neco.1997.9.2.279
   Muthukumaraswamy S. D., 2013, J NEUROSCIENCE, V33
   Nozawa H, 1992, CHAOS, V2, P377, DOI 10.1063/1.165880
   Rojas R, 1996, NEURAL NETWORKS, P149, DOI 10.1007/978-3-642-61068-4{\_}7
   Tanaka H, 2009, IEICE T FUND ELECTR, VE92A, P1690, DOI 10.1587/transfun.E92.A.1690
   Tino P, 2005, LECT NOTES COMPUT SC, V3611, P666
   Turner V., 1991, RITUAL PROCESS STRUC
   Varela F. J., 1999, J CONSCIOUSNESS STUD, V6, P2
   Watson RA, 2011, ADAPT BEHAV, V19, P227, DOI 10.1177/1059712311412797
   Watson RA, 2011, ARTIF LIFE, V17, P147, DOI 10.1162/artl_a_00029
   Watson RA, 2011, COMPLEXITY, V16, P17, DOI 10.1002/cplx.20346
   Wilson S, 2012, BRIT J PSYCHIAT, V200, P273, DOI 10.1192/bjp.bp.111.104091
NR 28
TC 15
Z9 15
U1 0
U2 19
PD FEB
PY 2015
VL 62
SI SI
BP 39
EP 46
DI 10.1016/j.neunet.2014.08.011
UT WOS:000350919500007
DA 2023-11-16
ER

PT J
AU Bennett, A
   White, A
AF Bennett, Adam
   White, Anthony
TI Synfire circuits: Constraint programming technique for combining
   functional groupings of spiking neurons
SO BIOLOGICALLY INSPIRED COGNITIVE ARCHITECTURES
DT Article
DE Artificial intelligence; Spiking neural network; Constraint programming;
   Modular neural network
ID CHAINS
AB Existing training techniques for spiking neuronal networks tend to be monolithic in nature and scale poorly to larger networks. This paper presents a technique for combining multiple functional neural groupings into a more complex composite network. This is accomplished by ensuring that four axioms hold true for the composite network. The axioms were designed to ensure that incoming signals arrive simultaneously to any component groupings. A number of experiments were conducted in which an algorithm implementing the axioms was used to combine component groupings into more complex networks; these experiments show the practical utility of the technique and reinforce by demonstration the correctness of the axioms.
C1 [Bennett, Adam; White, Anthony] Carleton Univ, 1125 Colonel By Dr, Ottawa, ON K1S 5B6, Canada.
RP Bennett, A (corresponding author), Carleton Univ, 1125 Colonel By Dr, Ottawa, ON K1S 5B6, Canada.
EM adamebennett@cmail.carleton.ca; arpwhite@scs.carleton.ca
CR Bennett A., 2018, THESIS
   BLODGETT HC, 1947, J EXP PSYCHOL, V37, P412, DOI 10.1037/h0059305
   Brooks R. A., 1990, Robotics and Autonomous Systems, V6, P3, DOI 10.1016/S0921-8890(05)80025-9
   Ferreira C, 2002, SOFT COMPUTING AND INDUSTRY, P635
   Gerstein GL, 2012, J NEUROSCI METH, V206, P54, DOI 10.1016/j.jneumeth.2012.02.003
   Hayon G, 2005, J COMPUT NEUROSCI, V18, P41, DOI 10.1007/s10827-005-5479-1
   Jeanson F., 2013, THESIS
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Prudhornme C., 2017, 6241 TASC LS2N CNRS
   Ranhel J., 2011, Proceedings 2011 IEEE Symposium on Foundations of Computational Intelligence (FOCI 2011), P66, DOI 10.1109/FOCI.2011.5949465
   Ranhel J, 2012, IEEE T NEUR NET LEAR, V23, P916, DOI 10.1109/TNNLS.2012.2190421
   Wernick W, 1942, T AM MATH SOC, V51, P117, DOI 10.2307/1989982
NR 12
TC 0
Z9 0
U1 0
U2 0
PD AUG
PY 2018
VL 25
BP 66
EP 71
DI 10.1016/j.bica.2018.07.008
UT WOS:000447096500009
DA 2023-11-16
ER

PT C
AU Adam, K
AF Adam, Karen
GP IEEE
TI A TIME ENCODING APPROACH TO TRAINING SPIKING NEURAL NETWORKS
SO 2022 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)
SE International Conference on Acoustics Speech and Signal Processing
   ICASSP
DT Proceedings Paper
CT 47th IEEE International Conference on Acoustics, Speech and Signal
   Processing (ICASSP)
CY MAY 22-27, 2022
CL Singapore, SINGAPORE
DE Time encoding; spiking neural networks; learning
ID BAND-LIMITED SIGNALS; FINITE-RATE; RECONSTRUCTION; LOIHI
AB While Spiking Neural Networks (SNNs) have been gaining in popularity, it seems that the algorithms used to train them are not powerful enough to solve the same tasks as those tackled by classical Artificial Neural Networks (ANNs).
   In this paper, we provide an extra tool to help us understand and train SNNs by using theory from the field of time encoding. Time encoding machines (TEMs) can be used to model integrate-and-fire neurons and have well-understood reconstruction properties.
   We will see how one can take inspiration from the field of TEMs to interpret the spike times of SNNs as constraints on the SNNs' weight matrices. More specifically, we study how to train one-layer SNNs by solving a set of linear constraints, and how to train two-layer SNNs by leveraging the all-or-none and asynchronous properties of the spikes emitted by SNNs. These properties of spikes result in an alternative to backpropagation which is not possible in the case of simultaneous and graded activations as in classical ANNs.
C1 [Adam, Karen] Ecole Polytech Fed Lausanne EPFL, Sch Comp & Commun Sci, Lausanne, Switzerland.
RP Adam, K (corresponding author), Ecole Polytech Fed Lausanne EPFL, Sch Comp & Commun Sci, Lausanne, Switzerland.
CR Adam K, 2020, INT CONF ACOUST SPEE, P9264, DOI [10.1109/icassp40776.2020.9053294, 10.1109/ICASSP40776.2020.9053294]
   Adam K, 2020, IEEE T SIGNAL PROCES, V68, P1105, DOI 10.1109/TSP.2020.2967182
   Adam Karen, 2021, ARXIV210414511
   Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   Alexandru R, 2020, IEEE T SIGNAL PROCES, V68, P747, DOI 10.1109/TSP.2019.2961301
   Boerlin M, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003258
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Comsa I.M., 2020, ICASSP 2020 2020 IEE, DOI DOI 10.1109/ICASSP40776.2020.9053856
   Cordone Lo<spacing diaeresis>ic, 2021, ARXIV210412579
   Davies M, 2021, P IEEE, V109, P911, DOI 10.1109/JPROC.2021.3067593
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Gontier D, 2014, APPL COMPUT HARMON A, V36, P63, DOI 10.1016/j.acha.2013.02.002
   Hilton M, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P5474, DOI 10.1109/ICASSP39728.2021.9414759
   Kamath Abijith Jagannath, 2021, ARXIV210703344
   Lazar AA, 2004, NEUROCOMPUTING, V58, P53, DOI 10.1016/j.neucom.2004.01.022
   Lazar AA, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL VI, PROCEEDINGS, P709
   Lazar AA, 2008, NEURAL COMPUT, V20, P2715, DOI 10.1162/neco.2008.06-07-559
   Lazar AA, 2010, IEEE T INFORM THEORY, V56, P821, DOI 10.1109/TIT.2009.2037040
   Ma CX, 2021, IEEE IJCNN, DOI 10.1109/IJCNN52387.2021.9534390
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Pacholska Michalina, 2020, ARXIV200104933
   Pfister JP, 2006, J NEUROSCI, V26, P9673, DOI 10.1523/JNEUROSCI.1425-06.2006
   Rudresh S, 2020, INT CONF ACOUST SPEE, P5585, DOI [10.1109/ICASSP40776.2020.9053120, 10.1109/icassp40776.2020.9053120]
   Thao NT, 2021, IEEE T SIGNAL PROCES, V69, P341, DOI 10.1109/TSP.2020.3043809
   VanRullen R, 2005, TRENDS NEUROSCI, V28, P1, DOI 10.1016/j.tins.2004.10.010
   Vetterli M, 2002, IEEE T SIGNAL PROCES, V50, P1417, DOI 10.1109/TSP.2002.1003065
   Wunderlich TC, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-91786-z
NR 27
TC 0
Z9 0
U1 1
U2 2
PY 2022
BP 5957
EP 5961
DI 10.1109/ICASSP43922.2022.9746319
UT WOS:000864187906049
DA 2023-11-16
ER

PT C
AU Dellaferrera, G
   Martinelli, F
   Cernak, M
AF Dellaferrera, Giorgia
   Martinelli, Flavio
   Cernak, Milos
GP IEEE
TI A BIN ENCODING TRAINING OF A SPIKING NEURAL NETWORK BASED VOICE ACTIVITY
   DETECTION
SO 2020 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL
   PROCESSING
SE International Conference on Acoustics Speech and Signal Processing
   ICASSP
DT Proceedings Paper
CT IEEE International Conference on Acoustics, Speech, and Signal
   Processing (ICASSP)
CY MAY 04-08, 2020
CL Barcelona, SPAIN
DE spiking neural networks; voice activity detection; bin encoding;
   supervised learning
AB Advances of deep learning for Artificial Neural Networks (ANNs) have led to significant improvements in the performance of digital signal processing systems implemented on digital chips. Although recent progress in low-power chips is remarkable, neuromorphic chips that run Spiking Neural Networks (SNNs) based applications offer an even lower power consumption, as a consequence of the ensuing sparse spike-based coding scheme. In this work, we develop a SNN-based Voice Activity Detection (VAD) system that belongs to the building blocks of any audio and speech processing system. We propose to use the bin encoding, a novel method to convert log mel filterbank bins of single-time frames into spike patterns. We integrate the proposed scheme in a bilayer spiking architecture which was evaluated on the QUT-NOISE-TIMIT corpus. Our approach shows that SNNs enable an ultra low-power implementation of a VAD classifier that consumes only 3.8 mu W, while achieving state-of-the-art performance. The code is freely available on Code Ocean [1].
C1 [Dellaferrera, Giorgia; Martinelli, Flavio; Cernak, Milos] Ecole Polytech Fed Lausanne EPFL, Lausanne, Switzerland.
   [Dellaferrera, Giorgia; Martinelli, Flavio] Logitech Europe SA, Lausanne, Switzerland.
RP Dellaferrera, G (corresponding author), Ecole Polytech Fed Lausanne EPFL, Lausanne, Switzerland.; Dellaferrera, G (corresponding author), Logitech Europe SA, Lausanne, Switzerland.
CR Benyassine A, 1997, IEEE COMMUN MAG, V35, P64, DOI 10.1109/35.620527
   Blouw Peter, 2018, ABS181201739 CORR
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1007/BF00994018
   Davidson M, 2018, INT J OCCUP ENV HEAL, V24, P75, DOI 10.1080/10773525.2018.1517234
   Dean D., 2010, INTERSPEECH
   Dellaferrera G., 2020, BIN ENCODING SNN VAD
   Dong MX, 2018, FRONT MOL NEUROSCI, V11, DOI 10.3389/fnmol.2018.00257
   Ghaemmaghami H., 2015, INTERSPEECH 2015
   Ghaemmaghami H, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 3 AND 4, P3118
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Hazan H., 2018, ABS180709374 CORR
   Hunsberger E., 2016, TRAINING SPIKING DEE, V11
   Li J., 2004, P ICASSP
   Meoni G, 2018, 2018 14TH CONFERENCE ON PHD RESEARCH IN MICROELECTRONICS AND ELECTRONICS (PRIME 2018), P41, DOI 10.1109/PRIME.2018.8430328
   Price M, 2018, IEEE J SOLID-ST CIRC, V53, P66, DOI 10.1109/JSSC.2017.2752838
   Ramírez J, 2004, SPEECH COMMUN, V42, P271, DOI 10.1016/j.specom.2003.10.002
   Silva DA, 2017, TELECOMMUN INF TECH, P37, DOI 10.1007/978-3-319-53753-5_4
   Sohn J, 1999, IEEE SIGNAL PROC LET, V6, P1, DOI 10.1109/97.736233
   Tavanaei A., 2017, ABS170603170 CORR
   Tavanaei A., 2016, ABS160600802 CORR
   Verstraeten D., 2005, ISOLATED WORD RECOGN, V01, P435
   Verstraeten D, 2006, IEEE IJCNN, P1050
   Wu JB, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00836
   Yang MH, 2018, ISSCC DIG TECH PAP I, P346, DOI 10.1109/ISSCC.2018.8310326
   Yu Q., 2019, ABS190201094 CORR
NR 25
TC 6
Z9 6
U1 1
U2 3
PY 2020
BP 3207
EP 3211
DI 10.1109/icassp40776.2020.9054761
UT WOS:000615970403091
DA 2023-11-16
ER

PT J
AU Wang, Y
   Deng, YL
   Cao, LH
   Zhang, JH
   Yang, L
AF Wang, Ye
   Deng, Yaling
   Cao, Lihong
   Zhang, Jiahong
   Yang, Lei
TI Retrospective memory integration accompanies reconfiguration of neural
   cell assemblies
SO HIPPOCAMPUS
DT Article
DE associative inference; associative memory; cell assembly; recurrent
   spiking neural network; retrospective memory integration
ID MEDIAL TEMPORAL-LOBE; HIPPOCAMPAL SYSTEM; MODEL; INFORMATION;
   CONSOLIDATION; MECHANISMS; SUPPORTS; NEURONS; SPIKES; REPLAY
AB Memory is a dynamic process that is based on and can be altered by experiences. Integrating memories of multiple experiences (memory integration) is the basis of flexible and complex decision-making. However, the mechanism of memory integration in neural networks of the brain remains poorly understood. In this study, we built a recurrent spiking network model and investigated the neural mechanism of memory integration before a decision is made (retrospective memory integration) at the neural circuit level. Our simulations suggest that retrospective memory integration accompanies reconfiguration of neural cell assemblies. Additionally, partially blocking neural network plasticity leads to failure of memory integration. These findings can potentially guide the experimental investigation of the neural mechanism of retrospective memory integration and serve as the basis for developing new artificial intelligence algorithms.
C1 [Wang, Ye; Deng, Yaling; Cao, Lihong; Zhang, Jiahong] Commun Univ China, State Key Lab Media Convergence & Commun, Beijing 100024, Peoples R China.
   [Wang, Ye; Deng, Yaling; Cao, Lihong] Commun Univ China, Neurosci & Intelligent Media Inst, Beijing, Peoples R China.
   [Yang, Lei] Pacific Northwest Res Inst, 720 Broadway, Seattle, WA 98122 USA.
RP Wang, Y (corresponding author), Commun Univ China, State Key Lab Media Convergence & Commun, Beijing 100024, Peoples R China.
EM yewang@cuc.edu.cn
CR Barron HC, 2020, CELL, V183, P228, DOI 10.1016/j.cell.2020.08.035
   Biderman N, 2020, TRENDS COGN SCI, V24, P542, DOI 10.1016/j.tics.2020.04.004
   Bunsey M, 1996, NATURE, V379, P255, DOI 10.1038/379255a0
   Buzsáki G, 2010, NEURON, V68, P362, DOI 10.1016/j.neuron.2010.09.023
   Cai DJ, 2016, NATURE, V534, P115, DOI 10.1038/nature17955
   Carr MF, 2011, NAT NEUROSCI, V14, P147, DOI 10.1038/nn.2732
   Cichon J, 2015, NATURE, V520, P180, DOI 10.1038/nature14251
   Collin SHP, 2015, NAT NEUROSCI, V18, P1562, DOI 10.1038/nn.4138
   De Falco E, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms13408
   DeVito LM, 2010, HIPPOCAMPUS, V20, P208, DOI 10.1002/hipo.20610
   Diekelmann S, 2010, NAT REV NEUROSCI, V11, P114, DOI 10.1038/nrn2762
   Eichenbaum H, 2000, NAT REV NEUROSCI, V1, P41, DOI 10.1038/35036213
   Ellenbogen JM, 2007, P NATL ACAD SCI USA, V104, P7723, DOI 10.1073/pnas.0700094104
   Gershman SJ, 2014, J EXP PSYCHOL GEN, V143, P182, DOI 10.1037/a0030844
   Hasselmo ME, 1997, BEHAV BRAIN RES, V89, P1, DOI 10.1016/S0166-4328(97)00048-X
   Hebb D.O., 1949, ORG BEHAV NEUROPSYCH
   Howard MW, 2005, PSYCHOL REV, V112, P75, DOI 10.1037/0033-295X.112.1.75
   Howard MW, 2002, J MEM LANG, V46, P85, DOI 10.1006/jmla.2001.2798
   Ison MJ, 2015, NEURON, V87, P220, DOI 10.1016/j.neuron.2015.06.016
   Joo HR, 2018, NAT REV NEUROSCI, V19, P744, DOI 10.1038/s41583-018-0077-1
   Josselyn SA, 2020, SCIENCE, V367, P39, DOI 10.1126/science.aaw4325
   Josselyn SA, 2015, NAT REV NEUROSCI, V16, P521, DOI 10.1038/nrn4000
   Karlsson MP, 2009, NAT NEUROSCI, V12, P913, DOI 10.1038/nn.2344
   Kastellakis G, 2016, CELL REP, V17, P1491, DOI 10.1016/j.celrep.2016.10.015
   Koster R, 2018, NEURON, V99, P1342, DOI 10.1016/j.neuron.2018.08.009
   Kumaran D, 2016, TRENDS COGN SCI, V20, P512, DOI 10.1016/j.tics.2016.05.004
   Kumaran D, 2012, PSYCHOL REV, V119, P573, DOI 10.1037/a0028681
   Litwin-Kumar A, 2014, NAT COMMUN, V5, DOI 10.1038/ncomms6319
   MCNAUGHTON BL, 1987, TRENDS NEUROSCI, V10, P408, DOI 10.1016/0166-2236(87)90011-7
   Moscovitch M, 2016, ANNU REV PSYCHOL, V67, P105, DOI 10.1146/annurev-psych-113011-143733
   Nicola W, 2019, NAT NEUROSCI, V22, P1168, DOI 10.1038/s41593-019-0415-2
   Nicolás B, 2021, NEUROIMAGE, V226, DOI 10.1016/j.neuroimage.2020.117558
   Pfister JP, 2006, J NEUROSCI, V26, P9673, DOI 10.1523/JNEUROSCI.1425-06.2006
   Poirazi P, 2020, NAT REV NEUROSCI, V21, P303, DOI 10.1038/s41583-020-0301-7
   Pokorny C, 2020, CEREB CORTEX, V30, P952, DOI 10.1093/cercor/bhz140
   Preston AR, 2004, HIPPOCAMPUS, V14, P148, DOI 10.1002/hipo.20009
   Quiroga RQ, 2019, CELL, V179, P1015, DOI 10.1016/j.cell.2019.10.016
   Rolls ET, 2013, FRONT SYST NEUROSCI, V7, DOI [10.3389/fnsys.2013.00074, 10.3389/fncel.2013.00098]
   Schlichting ML, 2015, CURR OPIN BEHAV SCI, V1, P1, DOI 10.1016/j.cobeha.2014.07.005
   Schlichting ML, 2014, HIPPOCAMPUS, V24, P1248, DOI 10.1002/hipo.22310
   Shohamy D, 2008, NEURON, V60, P378, DOI 10.1016/j.neuron.2008.09.023
   Shohamy D, 2015, CURR OPIN BEHAV SCI, V5, P85, DOI 10.1016/j.cobeha.2015.08.010
   Tompary A, 2017, NEURON, V96, P228, DOI 10.1016/j.neuron.2017.09.005
   TREVES A, 1994, HIPPOCAMPUS, V4, P374, DOI 10.1002/hipo.450040319
   van Kesteren MTR, 2012, TRENDS NEUROSCI, V35, P211, DOI 10.1016/j.tins.2012.02.001
   Wang Y, 2019, COMPUT INTEL NEUROSC, V2019, DOI 10.1155/2019/2367075
   Wong FS, 2019, ELIFE, V8, DOI 10.7554/eLife.47085
   Xie WZ, 2020, NAT HUM BEHAV, V4, P937, DOI 10.1038/s41562-020-0901-2
   Zeithamova D, 2012, NEURON, V75, P168, DOI 10.1016/j.neuron.2012.05.010
   Zeithamova D, 2012, FRONT HUM NEUROSCI, V6, DOI 10.3389/fnhum.2012.00070
   Zeithamova D, 2010, J NEUROSCI, V30, P14676, DOI 10.1523/JNEUROSCI.3250-10.2010
   Zenke F, 2015, NAT COMMUN, V6, DOI 10.1038/ncomms7922
NR 52
TC 1
Z9 1
U1 3
U2 13
PD MAR
PY 2022
VL 32
IS 3
BP 179
EP 192
DI 10.1002/hipo.23399
EA DEC 2021
UT WOS:000732511100001
DA 2023-11-16
ER

PT J
AU Wang, J
   Li, TF
   Sun, C
   Yan, RQ
   Chen, XF
AF Wang, Jun
   Li, Tianfu
   Sun, Chuang
   Yan, Ruqiang
   Chen, Xuefeng
TI Improved spiking neural network for intershaft bearing fault diagnosis
SO JOURNAL OF MANUFACTURING SYSTEMS
DT Article
DE Spiking neural network; Intershaft bearing; Fault diagnosis; Data
   encoding; Surrogate gradient
ID MACHINERY; ATTENTION; NEURONS
AB The intershaft bearing is located between the high and low-pressure rotors of the aero-engine, where the working environment is harsh, the load variation range is large, and the lubrication and heat dissipation are poor. The fault of the intershaft bearing is sudden and will cause the engine to hold the shaft and break the shaft, which is more harmful than the ordinary bearings. In recent years, bearing intelligent fault diagnosis methods based on deep learning have been widely applied. However, most of the existing methods are mainly proposed based on second-generation neural networks. Spiking neural network (SNN), also known as the third-generation neural network, mimics the dynamics of the biological brain and is more powerful for processing time-series information. As we know, vibration data is a typical time-series data, SNN would have stronger feature extraction potential for it. In this paper, we propose an improved spiking neural network (ISNN) for intershaft bearing fault diagnosis. Specifically, we propose an encoding method to encode raw data into spike sequences and demonstrate that the encoding method is accurate and efficient. Then we derive the gradient relation in the ISNN and mathematically prove the replacement of invalid gradients in it. Furthermore, we compensate for the loss of information in forward propagation and simplify the process of backpropagation by constructing suitable spiking neurons. Finally, we test the ISNN on the fault dataset of intershaft bearings, and the results show that the proposed ISNN outperforms previous SNNs and typical second-generation neural networks.
C1 [Wang, Jun; Li, Tianfu; Sun, Chuang; Yan, Ruqiang; Chen, Xuefeng] Xi An Jiao Tong Univ, Dept Mech Engn, Xian 710049, Shaanxi, Peoples R China.
   [Li, Tianfu] Ecole Polytech Fed Lausanne, Lab Intelligent Maintenance & Operat Syst, CH-1015 Lausanne, Switzerland.
RP Sun, C (corresponding author), Xi An Jiao Tong Univ, Dept Mech Engn, Xian 710049, Shaanxi, Peoples R China.
EM ch.sun@xjtu.edu.cn
CR An ZH, 2020, ISA T, V100, P155, DOI 10.1016/j.isatra.2019.11.010
   Andrew Alex M., 2003, KYBERNETES, V32
   [Anonymous], 1994, WAVELETS
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Cao HR, 2022, J MANUF SYST, V62, P186, DOI 10.1016/j.jmsy.2021.11.016
   Deng L, 2020, NEURAL NETWORKS, V121, P294, DOI 10.1016/j.neunet.2019.09.005
   Ge Y, 2022, J MANUF SYST, V63, P177, DOI 10.1016/j.jmsy.2022.03.009
   GERSTNER W, 1995, PHYS REV E, V51, P738, DOI 10.1103/PhysRevE.51.738
   He WH, 2020, NEURAL NETWORKS, V132, P108, DOI 10.1016/j.neunet.2020.08.001
   He Y, 2020, IEEE ACCESS, V8, P203058, DOI 10.1109/ACCESS.2020.3034305
   Heil P, 2004, CURR OPIN NEUROBIOL, V14, P461, DOI 10.1016/j.conb.2004.07.002
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Hu RH, 2019, IEEE T NEUR NET LEAR, V30, P1984, DOI 10.1109/TNNLS.2018.2875471
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Jalayer M, 2021, COMPUT IND, V125, DOI 10.1016/j.compind.2020.103378
   Jiang ZN, 2017, APPL SCI-BASEL, V7, DOI 10.3390/app7090937
   Jin DZ, 2002, PHYS REV LETT, V89, DOI 10.1103/PhysRevLett.89.208102
   Kun Feng, 2021, 2021 GLOBAL RELIABIL, P1
   Lazar AA, 2004, NEUROCOMPUTING, V58, P53, DOI 10.1016/j.neucom.2004.01.022
   Li TF, 2022, IEEE T SYST MAN CY-S, V52, P2302, DOI 10.1109/TSMC.2020.3048950
   Li TF, 2021, IEEE T IND ELECTRON, V68, P12739, DOI 10.1109/TIE.2020.3040669
   MOUNTCASTLE VB, 1957, J NEUROPHYSIOL, V20, P408, DOI 10.1152/jn.1957.20.4.408
   Mu XK, 2021, J MANUF SYST, V61, P112, DOI 10.1016/j.jmsy.2021.08.010
   Muruganatham B, 2013, MECH SYST SIGNAL PR, V35, P150, DOI 10.1016/j.ymssp.2012.08.019
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Nik Dennler, 2021, 2021 IEEE 3 INT C AR, P1
   Pan J, 2018, IEEE T IND ELECTRON, V65, P4973, DOI 10.1109/TIE.2017.2767540
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Senanayake DA, 2021, IEEE T NEUR NET LEAR, V32, P4588, DOI 10.1109/TNNLS.2020.3023941
   Slepova LO, 2018, PROCEEDINGS OF THE 2018 IEEE CONFERENCE OF RUSSIAN YOUNG RESEARCHERS IN ELECTRICAL AND ELECTRONIC ENGINEERING (EICONRUS), P992, DOI 10.1109/EIConRus.2018.8317256
   Sun C, 2018, IEEE T IND INFORM, V14, P3261, DOI 10.1109/TII.2018.2819674
   Taherkhani A, 2018, IEEE T NEUR NET LEAR, V29, P5394, DOI 10.1109/TNNLS.2018.2797801
   Tang JX, 2016, IEEE T NEUR NET LEAR, V27, P809, DOI 10.1109/TNNLS.2015.2424995
   Thorpe S, 1996, NATURE, V381, P520, DOI 10.1038/381520a0
   Verstraete D, 2017, SHOCK VIB, V2017, DOI 10.1155/2017/5067651
   Wang H, 2022, IEEE T NEUR NET LEAR, V33, P4757, DOI 10.1109/TNNLS.2021.3060494
   Wang XW, 2020, NEURAL NETWORKS, V125, P258, DOI 10.1016/j.neunet.2020.02.011
   Wen L, 2020, NEURAL COMPUT APPL, V32, P6111, DOI 10.1007/s00521-019-04097-w
   Wu YJ, 2019, AAAI CONF ARTIF INTE, P1311
   Xie ZL, 2022, J MANUF SYST, V62, P301, DOI 10.1016/j.jmsy.2021.12.003
   Xu Z, 2021, J MANUF SYST, V58, P132, DOI 10.1016/j.jmsy.2020.08.002
   Yan Y, 2014, NAT NEUROSCI, V17, P1380, DOI 10.1038/nn.3805
   Ye Z, 2021, J MANUF SYST, V59, P467, DOI 10.1016/j.jmsy.2021.03.022
   Yu XL, 2022, IEEE T IND INFORM, V18, P185, DOI 10.1109/TII.2021.3070324
   Zhang DC, 2020, IEEE T INSTRUM MEAS, V69, P2996, DOI 10.1109/TIM.2019.2929669
   Zhang JL, 2019, IEEE ASIAN SOLID STA, P213, DOI [10.1109/A-SSCC47793.2019.9056903, 10.1109/a-sscc47793.2019.9056903]
   Zhang KY, 2020, J MANUF SYST, V55, P273, DOI 10.1016/j.jmsy.2020.04.016
   Zhang TL, 2022, IEEE T NEUR NET LEAR, V33, P7621, DOI 10.1109/TNNLS.2021.3085966
   Zhang YH, 2021, MEASUREMENT, V171, DOI 10.1016/j.measurement.2020.108774
   Zhao B, 2021, J MANUF SYST, V59, P565, DOI 10.1016/j.jmsy.2021.03.024
   Zuo L, 2022, RELIAB ENG SYST SAFE, V225, DOI 10.1016/j.ress.2022.108561
   Zuo L, 2021, J MANUF SYST, V61, P714, DOI 10.1016/j.jmsy.2020.07.003
NR 53
TC 7
Z9 7
U1 23
U2 36
PD OCT
PY 2022
VL 65
BP 208
EP 219
DI 10.1016/j.jmsy.2022.09.003
EA SEP 2022
UT WOS:000911575300009
DA 2023-11-16
ER

PT C
AU Lu, B
   Wen, K
   Wang, C
AF Lu, Bo
   Wen, Kai
   Wang, Chuan
GP IEEE
TI Quantum Computation Using Coherent Ising Machines Based on Spiking
   Neural Networks
SO 2022 14TH INTERNATIONAL CONFERENCE ON WIRELESS COMMUNICATIONS AND SIGNAL
   PROCESSING, WCSP
SE International Conference on Wireless Communications and Signal
   Processing
DT Proceedings Paper
CT 14th IEEE International Conference on Wireless Communications and Signal
   Processing (WCSP)
CY NOV 01-03, 2022
CL Nanjing, PEOPLES R CHINA
DE Coherent Ising machines; spiking neural networks; combinatorial
   optimization; quantum computation
ID OPTIMIZATION; SIMULATION; NEURONS
AB Coherent Ising machines (CIMs) are a promising approach for quantum computation. It exhibits excellent performance in solving the combinatorial optimization problems, however, the existence of amplitude heterogeneity limits the applications of CIMs. Here in this work, we present a system of the spiking neural network composed of antisymmetrically coupled degenerate optical parametric oscillator pulses and dissipative pulses. It works based on the principles of CIM during solving the combinatorial optimization problems. Moreover, a nonlinear transfer function is chosen to mitigate the amplitude inhomogeneities and destabilize the resulting local minima according to the dynamical behavior of spiking neurons.
C1 [Lu, Bo; Wang, Chuan] Beijing Normal Univ Beijing, Sch Artificial Intelligence, Beijing, Peoples R China.
   [Wen, Kai] Beijing QBoson Quantum Technol Co Ltd, Beijing, Peoples R China.
RP Lu, B (corresponding author), Beijing Normal Univ Beijing, Sch Artificial Intelligence, Beijing, Peoples R China.
EM lubo@mail.bnu.edu.cn; wenk@boseq.com; wangchuan@bnu.edu.cn
CR Arora S, 2009, COMPUTATIONAL COMPLEXITY: A MODERN APPROACH, P1, DOI 10.1017/CBO9780511804090
   BARAHONA F, 1988, OPER RES, V36, P493, DOI 10.1287/opre.36.3.493
   Böhm F, 2021, COMMUN PHYS-UK, V4, DOI 10.1038/s42005-021-00655-8
   Böhm F, 2019, NAT COMMUN, V10, DOI 10.1038/s41467-019-11484-3
   Böhm F, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-07328-1
   Brette R, 2007, J COMPUT NEUROSCI, V23, P349, DOI 10.1007/s10827-007-0038-6
   Hamerly R, 2019, SCI ADV, V5, DOI 10.1126/sciadv.aau0823
   Inagaki T, 2021, NAT COMMUN, V12, DOI 10.1038/s41467-021-22576-4
   Inagaki T, 2016, SCIENCE, V354, P603, DOI 10.1126/science.aah4243
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Kadowaki T, 1998, PHYS REV E, V58, P5355, DOI 10.1103/PhysRevE.58.5355
   Kitchen DB, 2004, NAT REV DRUG DISCOV, V3, P935, DOI 10.1038/nrd1549
   Leleu T, 2019, PHYS REV LETT, V122, DOI 10.1103/PhysRevLett.122.040607
   Leleu T, 2017, PHYS REV E, V95, DOI 10.1103/PhysRevE.95.022118
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   McMahon PL, 2016, SCIENCE, V354, P614, DOI 10.1126/science.aah5178
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Okawachi Y, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-17919-6
   Pierangeli D, 2019, PHYS REV LETT, V122, DOI 10.1103/PhysRevLett.122.213902
   Saccone M, 2022, NAT PHYS, V18, P517, DOI 10.1038/s41567-022-01538-7
   Soler-Dominguez A, 2017, ACM COMPUT SURV, V50, DOI 10.1145/3054133
   Wang Z, 2013, PHYS REV A, V88, DOI 10.1103/PhysRevA.88.063853
   Yu Q., 2012, 2012 INT JOINT C NEU, P1
NR 23
TC 0
Z9 0
U1 3
U2 3
PY 2022
BP 813
EP 817
DI 10.1109/WCSP55476.2022.10039429
UT WOS:000972901000148
DA 2023-11-16
ER

PT J
AU Booij, O
   Nguyen, HT
AF Booij, O
   Nguyen, HT
TI A gradient descent rule for spiking neurons emitting multiple spikes
SO INFORMATION PROCESSING LETTERS
DT Article
DE spiking neural networks; temporal pattern recognition;
   error-backpropagation; parallel processing
ID NETWORKS
AB A supervised learning rule for Spiking Neural Networks (SNNs) is presented that can cope with neurons that spike multiple times. The rule is developed by extending the existing SpikeProp algorithm which could only be used for one spike per neuron. The problem caused by the discontinuity in the spike process is counteracted with a simple but effective rule, which makes the learning process more efficient. Our learning rule is successfully tested on a classification task of Poisson spike trains. We also applied the algorithm on a temporal version of the XOR problem and show that it is possible to learn this classical problem using only one spiking neuron making use of a hair-trigger situation. (c) 2005 Elsevier B.V. All rights reserved.
C1 Univ Amsterdam, Fac Sci, NL-1098 SJ Amsterdam, Netherlands.
RP Booij, O (corresponding author), Univ Amsterdam, Fac Sci, Kruislaan 403, NL-1098 SJ Amsterdam, Netherlands.
EM obooij@science.uva.nl; hieu@science.uva.nl
CR [Anonymous], 1986, PARALLEL DISTRIBUTED
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Gerstner W., 2002, SPIKING NEURON MODEL
   Heeger D., 2000, POISSON MODEL SPIKE
   Hopfield JJ, 2001, P NATL ACAD SCI USA, V98, P1282, DOI 10.1073/pnas.031567098
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   Maass W, 1997, ADV NEUR IN, V9, P211
   Maass W., 1999, PULSED NEURAL NETWOR
   MOORE S, 2002, THESIS U BATH
   Natschlager T, 1998, NETWORK-COMP NEURAL, V9, P319, DOI 10.1088/0954-898X/9/3/003
   NATSCHLAGER T, 2004, ADV NEURAL INFORMATI, V16
   Ruf B, 1997, LECT NOTES COMPUT SC, V1240, P380, DOI 10.1007/BFb0032496
   Schrauwen B, 2004, IEEE IJCNN, P471, DOI 10.1109/IJCNN.2004.1379954
   XIN J, 2001, P IEEE INT JOINT C N
   [No title captured]
NR 15
TC 113
Z9 133
U1 0
U2 10
PD SEP 30
PY 2005
VL 95
IS 6
BP 552
EP 558
DI 10.1016/j.ipl.2005.05.023
UT WOS:000231637900006
DA 2023-11-16
ER

PT C
AU Poulsen, TM
   Moore, RK
AF Poulsen, Thomas M.
   Moore, Roger K.
GP IEEE
TI Sound localization through evolutionary learning applied to spiking
   neural networks
SO 2007 IEEE SYMPOSIUM ON FOUNDATIONS OF COMPUTATIONAL INTELLIGENCE, VOLS 1
   AND 2
DT Proceedings Paper
CT IEEE Symposium on Foundations of Computational Intelligence
CY APR 01-05, 2007
CL Honolulu, HI
ID NEURONS
AB A biologically based learning framework is established to study neural modeling with respect to sound source localization. This involves a 2-dimensional environment wherein agents must locate sound sources that are periodically resituated whilst emitting pulses at regular intervals. Agents employ a spiking neural model that controls movement on the basis of binaural inputs, and evolutionary learning (EL) is applied to evolve neural connectivity and weights. It is demonstrated that agents are successfully able to locate sound sources and that the simulative framework can be extended to address questions pertaining to the evolution of spiking neural networks.
C1 [Poulsen, Thomas M.] Univ Sheffield, Speech & Hearing Grp, Dept Comp Sci, Sheffield S10 2TN, S Yorkshire, England.
   [Moore, Roger K.] Univ Sheffield, Dept Comp Sci, Speech & Hearing Grp, Sheffield S10 2TN, S Yorkshire, England.
RP Poulsen, TM (corresponding author), Univ Sheffield, Speech & Hearing Grp, Dept Comp Sci, Sheffield S10 2TN, S Yorkshire, England.
CR Beer Randall D., 1992, Adaptive Behavior, V1, P91, DOI 10.1177/105971239200100105
   BEER RD, 1996, P 4 INT C SIM AD BEH, P421
   Di Paolo EA, 2000, ADAPT BEHAV, V8, P27, DOI 10.1177/105971230000800103
   FLOREANO D, 2001, EVOLUTIONARY ROBOTIC, V4
   Gabbiani F, 1996, NATURE, V384, P564, DOI 10.1038/384564a0
   Gerstner W., 2002, SPIKING NEURON MODEL
   Kistler WM, 1997, NEURAL COMPUT, V9, P1015, DOI 10.1162/neco.1997.9.5.1015
   Machens CK, 2003, NAT NEUROSCI, V6, P341, DOI 10.1038/nn1036
   Rokem A, 2006, J NEUROPHYSIOL, V95, P2541, DOI 10.1152/jn.00891.2005
   VANSTEVENINCK RD, 1988, PROC R SOC SER B-BIO, V234, P379
   WERNER GM, 1991, P 2 INT C ART LIF, P659
   Wright BD, 2002, ADV NEUR IN, V14, P309
NR 12
TC 4
Z9 4
U1 0
U2 0
PY 2007
BP 350
EP +
DI 10.1109/FOCI.2007.371495
UT WOS:000248503700052
DA 2023-11-16
ER

PT C
AU Ross, M
   Berberian, N
   Cyr, A
   Thériault, F
   Chartier, S
AF Ross, Matt
   Berberian, Nareg
   Cyr, Andre
   Theriault, Frederic
   Chartier, Sylvain
BE Lintas, A
   Rovetta, S
   Verschure, PFMJ
   Villa, AEP
TI Learning Distance-Behavioural Preferences Using a Single Sensor in a
   Spiking Neural Network
SO ARTIFICIAL NEURAL NETWORKS AND MACHINE LEARNING - ICANN 2017, PT I
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 26th International Conference on Artificial Neural Networks (ICANN)
CY SEP 11-14, 2017
CL Alghero, ITALY
DE Spiking neural networks; Spike timing dependent plasticity; Robotic
   simulation; Sensory calibration
ID NEURONS
AB Actions from autonomous agents demand adaptive rules rather than being hard coded. Contrary to using multiple pre-calibrated sensors, utilizing a single non-calibrated sensor in combination with neural elements could provide flexibility through learning, to effectively cope with changing environments. The objective of this study was to design an adaptive system with the potential capability of learning behavioural preferences in relation to distinct distances from a wall using only a single ultrasonic sensor. Using spike-timing dependent plasticity (STDP) as a learning mechanism in a spiking neural network (SNN), the agent displayed the correct behaviour and was successful in learning the desired behavioural preference at a medium distance. However, the agent treated far and close distances as ambiguous inputs from the sensory environment, despite the presentation of reinforcement cues during learning.
C1 [Ross, Matt; Berberian, Nareg; Cyr, Andre; Chartier, Sylvain] Univ Ottawa, Sch Psychol, 136 Jean Jacques Lussier, Ottawa, ON K1N 6N5, Canada.
   [Theriault, Frederic] Cegep Vieux Montreal, Comp Sci, 255 Ontario St E, Montreal, PQ H2X 1X6, Canada.
RP Ross, M (corresponding author), Univ Ottawa, Sch Psychol, 136 Jean Jacques Lussier, Ottawa, ON K1N 6N5, Canada.
EM mross094@uottawa.ca; nberb062@uottawa.ca
CR [Anonymous], 2010, SPIKING NEURAL NETWO
   Pérez-Carrasco JA, 2010, IEEE T NEURAL NETWOR, V21, P609, DOI 10.1109/TNN.2009.2039943
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Bryan Kolb I. W., 2014, INTRO BRAIN BEHAV, VFourth
   Caporale N, 2008, ANNU REV NEUROSCI, V31, P25, DOI 10.1146/annurev.neuro.31.060407.125639
   Cyr A, 2015, J ROBOT, V2015, DOI 10.1155/2015/643869
   Cyr A, 2014, FRONT NEUROROBOTICS, V8, P1, DOI 10.3389/fnbot.2014.00021
   Cyr A, 2009, NEURAL COMPUT APPL, V18, P431, DOI 10.1007/s00521-009-0254-2
   Haykin S., 2009, NEURAL NETWORKS LEAR
   Humphries MD, 2007, PHILOS T R SOC B, V362, P1627, DOI 10.1098/rstb.2007.2057
   Iakymchuk T, 2015, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-015-0059-4
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Ross M., 2017, LEARNING DISTANCE BE
   Theriault F., 2017, LEARNING DISTANCE BE
   Wenstrup JJ, 2011, NEUROSCI BIOBEHAV R, V35, P2073, DOI 10.1016/j.neubiorev.2010.12.015
NR 15
TC 0
Z9 0
U1 0
U2 1
PY 2017
VL 10613
BP 110
EP 118
DI 10.1007/978-3-319-68600-4_14
PN I
UT WOS:000449802500014
DA 2023-11-16
ER

PT J
AU Chen, QY
   He, GQ
   Wang, XY
   Xu, J
   Shen, SR
   Chen, H
   Fu, YX
   Li, L
AF Chen, Qinyu
   He, Guoqiang
   Wang, Xinyuan
   Xu, Jin
   Shen, Sirui
   Chen, Hui
   Fu, Yuxiang
   Li, Li
TI A 67.5 μJ/Prediction Accelerator for Spiking Neural Networks in Image
   Segmentation
SO IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS II-EXPRESS BRIEFS
DT Article
DE Image segmentation; spiking neural network; VLSI
ID PROCESSOR
AB Spiking Neural Networks (SNNs) is promising to enable low power and high performance edge computing hardware design and have recently attracted attentions of researchers. Compared to Artificial Neural Networks (ANNs), SNNs, which present more realistic brain-inspired computing models, are developed as an alternative to ANNs. However, the temporal primitive of SNNs causes irregular and repeated data accesses, leading to high latency and extra power consumption. In this work, we propose an efficient architecture for SNNs by exploiting event-based characteristics. A reconfigurable spiking neuron processing unit is proposed to support a variety of spike-layers. Furthermore, to reduce the cycles needed per frame, an efficient dataflow with fast-filtering mechanism is introduced to leverage the sparsity of discrete spikes. The results show that this design achieves 67.5 mu J/image prediction energy with a throughput of 2.2K FPS. The core size is 0.89 mm(2) under 28-nm technology, with 90.98% computing hardware utilization and a competitive accuracy 97.10% on a driving dataset.
C1 [Chen, Qinyu; He, Guoqiang; Wang, Xinyuan; Xu, Jin; Shen, Sirui; Chen, Hui; Fu, Yuxiang; Li, Li] Nanjing Univ, Sch Elect & Engn, Nanjing 210000, Peoples R China.
RP Fu, YX; Li, L (corresponding author), Nanjing Univ, Sch Elect & Engn, Nanjing 210000, Peoples R China.
EM cqy@smail.nju.edu.cn; yuxiangfu@nju.edu.cn; lili@nju.edu.cn
CR Cavigelli L., 2015, P 25 EDITION GREAT L, P199, DOI [10.1145/2742060.2743766, DOI 10.1145/2742060.2743766]
   Chen GK, 2019, IEEE J SOLID-ST CIRC, V54, P992, DOI 10.1109/JSSC.2018.2884901
   Chen Q., 2021, PROC IEEE INT S CIRC, P1, DOI DOI 10.1038/NMETH.1318
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Farsa EZ, 2019, IEEE T CIRCUITS-II, V66, P1582, DOI 10.1109/TCSII.2019.2890846
   Gao C, 2018, PROCEEDINGS OF THE 2018 ACM/SIGDA INTERNATIONAL SYMPOSIUM ON FIELD-PROGRAMMABLE GATE ARRAYS (FPGA'18), P21, DOI 10.1145/3174243.3174261
   Kaiser J, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00424
   Narayanan S, 2020, ANN I S COM, P349, DOI 10.1109/ISCA45697.2020.00038
   Park J, 2020, IEEE J SOLID-ST CIRC, V55, P108, DOI 10.1109/JSSC.2019.2942367
   Roy A, 2017, I SYMPOS LOW POWER E
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Whatmough PN, 2017, ISSCC DIG TECH PAP I, P242, DOI 10.1109/ISSCC.2017.7870351
NR 12
TC 7
Z9 7
U1 1
U2 11
PD FEB
PY 2022
VL 69
IS 2
BP 574
EP 578
DI 10.1109/TCSII.2021.3098633
UT WOS:000748372000067
DA 2023-11-16
ER

PT C
AU Park, S
   Kim, S
   Na, B
   Yoon, S
AF Park, Seongsik
   Kim, Seijoon
   Na, Byunggook
   Yoon, Sungroh
GP IEEE
TI T2FSNN: Deep Spiking Neural Networks with Time-to-first-spike Coding
SO PROCEEDINGS OF THE 2020 57TH ACM/EDAC/IEEE DESIGN AUTOMATION CONFERENCE
   (DAC)
SE Design Automation Conference DAC
DT Proceedings Paper
CT 57th ACM/IEEE Design Automation Conference (DAC)
CY JUL 20-24, 2020
CL ELECTR NETWORK
DE Biological neural networks; Neuromorphics; Supervised learning; Image
   classification
AB Spiking neural networks (SNNs) have gained considerable interest due to their energy-efficient characteristics, yet lack of a scalable training algorithm has restricted their applicability in practical machine learning problems. The deep neural network-to-SNN conversion approach has been widely studied to broaden the applicability of SNNs. Most previous studies, however, have not fully utilized spatio-temporal aspects of SNNs, which has led to inefficiency in terms of number of spikes and inference latency. In this paper, we present T2FSNN, which introduces the concept of time-to-first-spike coding into deep SNNs using the kernel-based dynamic threshold and dendrite to overcome the aforementioned drawback. In addition, we propose gradient-based optimization and early firing methods to further increase the efficiency of the T2FSNN. According to our results, the proposed methods can reduce inference latency and number of spikes to 22% and less than 1%, compared to those of burst coding, which is the state-of-the-art result on the CIFAR-100.
C1 [Yoon, Sungroh] Seoul Natl Univ, Dept Elect & Comp Engn, ASRI, INMC, Seoul 08826, South Korea.
   Seoul Natl Univ, Inst Engn Res, Seoul 08826, South Korea.
RP Yoon, S (corresponding author), Seoul Natl Univ, Dept Elect & Comp Engn, ASRI, INMC, Seoul 08826, South Korea.
EM sryoon@snu.ac.kr
CR Adrian ED, 1926, J PHYSIOL-LONDON, V61, P49, DOI 10.1113/jphysiol.1926.sp002273
   [Anonymous], 2015, NIPS
   Butts DA, 2007, NATURE, V449, P92, DOI [10.1038/nature06105, 10.1038/natureO6105]
   Diehl PU, 2015, IEEE IJCNN
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   Jin Y., 2018, NIPS
   Kim J, 2018, NEUROCOMPUTING, V311, P373, DOI 10.1016/j.neucom.2018.05.087
   Kim Soo Ye, 2020, AAAI
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Montemurro MA, 2008, CURR BIOL, V18, P375, DOI 10.1016/j.cub.2008.02.023
   Park S., 2018, AAAI
   Park S, 2019, PERSPECT CONTEMP KOR, P1, DOI 10.1007/s12083-019-00780-w
   Rueckauer B, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351295
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Tan MX, 2019, PR MACH LEARN RES, V97
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   Wu Yu, 2019, AAAI
   Zhang L, 2019, AAAI
NR 19
TC 72
Z9 78
U1 6
U2 51
PY 2020
DI 10.1109/dac18072.2020.9218689
UT WOS:000628528400194
DA 2023-11-16
ER

PT J
AU Kim, D
   She, XY
   Rahman, NM
   Chekuri, VCK
   Mukhopadhyay, S
AF Kim, Daehyun
   She, Xueyuan
   Rahman, Nael Mizanur
   Chekuri, Venkata Chaitanya Krishna
   Mukhopadhyay, Saibal
TI Processing-In-Memory-Based On-Chip Learning With Spike-Time-Dependent
   Plasticity in 65-nm CMOS
SO IEEE SOLID-STATE CIRCUITS LETTERS
DT Article
DE Accelerator; on-chip learning; spike-timing-dependent plasticity (STDP);
   spiking neural network (SNN)
AB A processing-in-memory (PIM)-based accelerator is presented in 65-nm CMOS for on-chip learning in spiking neural network using timing-based stochastic spike-timing-dependent plasticity (STDP). The design uses mixed-signal processing in the 8T-SRAM array for spike accumulation and all-digital computation for neuron dynamics and synaptic weight updates. The 0.39-mm(2) and 14.83-mW test chip demonstrates 100K images/second learning rate and 148.3 nJ/image learning energy.
C1 [Kim, Daehyun; She, Xueyuan; Rahman, Nael Mizanur; Chekuri, Venkata Chaitanya Krishna; Mukhopadhyay, Saibal] Georgia Inst Technol, Sch Elect & Comp Engn, Atlanta, GA 30332 USA.
RP Kim, D (corresponding author), Georgia Inst Technol, Sch Elect & Comp Engn, Atlanta, GA 30332 USA.
EM daehyun.kim@gatech.edu
CR Buhler FN, 2017, SYMP VLSI CIRCUITS, pC30, DOI 10.23919/VLSIC.2017.8008536
   Jung Kuk Kim, 2015, 2015 Symposium on VLSI Circuits (VLSI Circuits), pC50, DOI 10.1109/VLSIC.2015.7231323
   Kim S, 2011, 2011 11TH INTERNATIONAL CONFERENCE ON CONTROL, AUTOMATION AND SYSTEMS (ICCAS), P1
   Park J, 2019, ISSCC DIG TECH PAP I, V62, P140, DOI 10.1109/ISSCC.2019.8662398
   She XY, 2019, DES AUT TEST EUROPE, P450, DOI [10.23919/DATE.2019.8714846, 10.23919/date.2019.8714846]
NR 5
TC 8
Z9 8
U1 0
U2 3
PY 2020
VL 3
BP 278
EP 281
DI 10.1109/LSSC.2020.3013448
UT WOS:000723378200071
DA 2023-11-16
ER

PT J
AU Mirsadeghi, M
   Shalchian, M
   Kheradpisheh, SR
   Masquelier, T
AF Mirsadeghi, Maryam
   Shalchian, Majid
   Kheradpisheh, Saeed Reza
   Masquelier, Timothee
TI Spike time displacement-based error backpropagation in convolutional
   spiking neural networks
SO NEURAL COMPUTING & APPLICATIONS
DT Article
DE StiDi-BP algorithm; Convolutional spiking neural networks; Real-valued
   weights; Binary weights
AB In this paper, we introduce a supervised learning algorithm, which avoids backward recursive gradient computation, for training deep convolutional spiking neural networks (SNNs) with single-spike-based temporal coding. The algorithm employs a linear approximation to compute the derivative of the spike latency with respect to the membrane potential, and it uses spiking neurons with piecewise linear postsynaptic potential to reduce the computational cost and the complexity of neural processing. To evaluate the performance of the proposed algorithm in deep architectures, we employ it in convolutional SNNs for the image classification task. For two popular benchmarks of MNIST and Fashion-MNIST datasets, the network reaches accuracies of, respectively, 99.2 and 92.8%. The trade-off between memory storage capacity and computational cost with accuracy is analyzed by applying two sets of weights: real-valued weights that are updated in the backward pass and their signs, binary weights, that are employed in the feedforward process. We evaluate the binary CSNN on two datasets of MNIST and Fashion-MNIST and obtain acceptable performance with a negligible accuracy drop with respect to real-valued weights (about 0.6 and 0.8% drops, respectively).
C1 [Mirsadeghi, Maryam; Shalchian, Majid] Amirkabir Univ Technol, Dept Elect Engn, Tehran, Iran.
   [Kheradpisheh, Saeed Reza] Shahid Beheshti Univ, Fac Math Sci, Dept Comp & Data Sci, Tehran, Iran.
   [Masquelier, Timothee] Univ Toulouse 3, CerCo UMR 5549, CNRS, Toulouse, France.
RP Shalchian, M (corresponding author), Amirkabir Univ Technol, Dept Elect Engn, Tehran, Iran.
EM shalchian@aut.ac.ir
CR Bohte SM, 2011, LECT NOTES COMPUT SC, V6791, P60, DOI 10.1007/978-3-642-21735-7_8
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Cho J, 2021, ELECTRONICS-SWITZ, V10, DOI 10.3390/electronics10030230
   Comsa IM., 2021, IEEE T NEUR NET LEAR, V35, P1
   Deng S., 2021, INT C LEARN REPR
   Esser S, 2015, BACKPROPAGATION ENER, P28
   Esser SK, 2016, P NATL ACAD SCI USA, V113, P11441, DOI 10.1073/pnas.1604850113
   Fang W, 2021, INCORPORATING LEARNA, P2661
   Han X., 2017, ARXIV
   Huh D, 2018, ADV NEUR IN, V31
   Jolliffe IT, 2016, PHILOS T R SOC A, V374, DOI 10.1098/rsta.2015.0202
   Kheradpisheh SR, 2022, IEEE ACCESS, V10, P70769, DOI 10.1109/ACCESS.2022.3187033
   Kheradpisheh SR, 2022, NEURAL PROCESS LETT, V54, P1255, DOI 10.1007/s11063-021-10680-x
   Kheradpisheh SR, 2020, INT J NEURAL SYST, V30, DOI 10.1142/S0129065720500276
   Kheradpisheh SR, 2016, NEUROCOMPUTING, V205, P382, DOI 10.1016/j.neucom.2016.04.029
   Kundu S, 2021, IEEE WINT CONF APPL, P3952, DOI 10.1109/WACV48630.2021.00400
   Laydevant J, 2021, P IEEE CVF C COMP VI, P4640
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee C, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00119
   Lee JM, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00191
   Liu SY, 2015, PROCEEDINGS 3RD IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION ACPR 2015, P730, DOI 10.1109/ACPR.2015.7486599
   Masquelier T, 2018, FRONT COMPUT NEUROSC, V12, DOI 10.3389/fncom.2018.00074
   Mirsadeghi M, 2021, NEUROCOMPUTING, V427, P131, DOI 10.1016/j.neucom.2020.11.052
   Mostafa H, 2018, IEEE T NEUR NET LEAR, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Mozafari M, 2018, IEEE T NEUR NET LEAR, V29, P6178, DOI 10.1109/TNNLS.2018.2826721
   Muramatsu N, 2021, ARXIV
   Neftci EO, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00324
   Nomura O, 2022, IEEE T CIRCUITS-II, V69, P3640, DOI 10.1109/TCSII.2022.3184313
   Rathi N., 2020, ARXIV
   Roweis S, 2002, ADV NEUR IN
   Rueckauer B, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351295
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Shrestha SB., 2018, ADV NEURAL INFORM PR, V31, P1412
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Szegedy C., 2015, 2015 IEEE C COMPUTER, P1, DOI [10.1109/CVPR.2015.7298594, DOI 10.1109/CVPR.2015.7298594]
   Wang YX, 2021, IEEE T COGN DEV SYST, V13, P514, DOI 10.1109/TCDS.2020.2971655
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Zenke F, 2018, NEURAL COMPUT, V30, P1514, DOI 10.1162/neco_a_01086
   Zhang ML, 2022, IEEE T NEUR NET LEAR, V33, P1947, DOI 10.1109/TNNLS.2021.3110991
   Zhang W, 2020, 34 C NEURAL INFORM P
   Zhou SB, 2021, AAAI CONF ARTIF INTE, V35, P11143
NR 42
TC 0
Z9 0
U1 6
U2 6
PD JUL
PY 2023
VL 35
IS 21
BP 15891
EP 15906
DI 10.1007/s00521-023-08567-0
EA APR 2023
UT WOS:000975306700002
DA 2023-11-16
ER

PT C
AU Wang, RC
   Jin, C
   McEwan, A
   van Schaik, A
AF Wang, Runchun
   Jin, Craig
   McEwan, Alistair
   van Schaik, Andre
GP IEEE
TI A programmable axonal propagation delay circuit for time-delay spiking
   neural networks
SO 2011 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS (ISCAS)
SE IEEE International Symposium on Circuits and Systems
DT Proceedings Paper
CT IEEE International Symposium on Circuits and Systems (ISCAS)
CY MAY 15-18, 2011
CL Rio de Janeiro, BRAZIL
AB We present an implementation of a programmable axonal propagation delay circuit which uses one first-order log-domain low-pass filter. Delays may be programmed in the 5-50ms range. It is designed to be a building block for time-delay spiking neural networks. It consists of a leaky-integrate-and-fire core, a spike generator circuit, and a delay adaptation circuit.
C1 [Wang, Runchun; Jin, Craig; McEwan, Alistair; van Schaik, Andre] Univ Sydney, Sch Elect & Informat Engn, Sydney, NSW 2006, Australia.
RP Wang, RC (corresponding author), Univ Sydney, Sch Elect & Informat Engn, Sydney, NSW 2006, Australia.
EM rwan6647@uni.sydney.edu.au
CR Eurich CW, 2000, NEUROCOMPUTING, V32, P741, DOI 10.1016/S0925-2312(00)00239-3
   Gerstner W, 1996, NATURE, V383, P76, DOI 10.1038/383076a0
   Horio Y., 1998, IEEE INT C EL CIRC S, P301
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Lazarro J., 1992, IEEE INT S CIRC SYST, P2220
   MINCH BA, 1995, ADV NEURAL INFORMATI, V7, P739
   Python D., 2001, IEEE J SOLID STATE C, V36
   STANFORD LR, 1987, SCIENCE, V238, P358, DOI 10.1126/science.3659918
   Stevens B, 1998, J NEUROSCI, V18, P9303
NR 9
TC 12
Z9 17
U1 0
U2 1
PY 2011
BP 869
EP 872
UT WOS:000297265301027
DA 2023-11-16
ER

PT J
AU Zhang, T
   Xiang, SY
   Liu, WZ
   Han, YA
   Guo, XX
   Hao, Y
AF Zhang, Tao
   Xiang, Shuiying
   Liu, Wenzhuo
   Han, Yanan
   Guo, Xingxing
   Hao, Yue
TI Hybrid Spiking Fully Convolutional Neural Network for Semantic
   Segmentation
SO ELECTRONICS
DT Article
DE spiking convolutional neural network; semantic segmentation; surrogate
   gradient; supervised training
ID INTELLIGENCE; PROCESSOR
AB The spiking neural network (SNN) exhibits distinct advantages in terms of low power consumption due to its event-driven nature. However, it is limited to simple computer vision tasks because the direct training of SNNs is challenging. In this study, we propose a hybrid architecture called the spiking fully convolutional neural network (SFCNN) to expand the application of SNNs in the field of semantic segmentation. To train the SNN, we employ the surrogate gradient method along with backpropagation. The accuracy of mean intersection over union (mIoU) for the VOC2012 dataset is higher than that of existing spiking FCNs by almost 30%. The accuracy of mIoU can reach 39.6%. Moreover, the proposed hybrid SFCNN achieved excellent segmentation performance for other datasets such as COCO2017, DRIVE, and Cityscapes. Our hybrid SFCNN is a valuable and interesting contribution to extending the functionality of SNNs, especially for power-constrained applications.
C1 [Zhang, Tao; Xiang, Shuiying; Liu, Wenzhuo; Han, Yanan; Guo, Xingxing] Xidian Univ, State Key Lab Integrated Serv Networks, Xian 710071, Peoples R China.
   [Xiang, Shuiying; Hao, Yue] Xidian Univ, State Key Discipline Lab Wide Bandgap Semicond Tec, Xian 710071, Peoples R China.
RP Xiang, SY (corresponding author), Xidian Univ, State Key Lab Integrated Serv Networks, Xian 710071, Peoples R China.; Xiang, SY (corresponding author), Xidian Univ, State Key Discipline Lab Wide Bandgap Semicond Tec, Xian 710071, Peoples R China.
EM 21011210532@stu.xidian.edu.cn; syxiang@xidian.edu.cn;
   22011211053@stu.xidian.edu.cn; yhao@xidian.edu.cn; xxguo@xidian.edu.cn;
   ynanhan@stu.xidian.edu.cn
CR Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   Al-Rawi M, 2007, COMPUT BIOL MED, V37, P262, DOI 10.1016/j.compbiomed.2006.03.003
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Bu T, 2023, Arxiv, DOI arXiv:2303.04347
   Chaurasia A, 2017, 2017 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING (VCIP)
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Ding JH, 2021, Arxiv, DOI arXiv:2105.11654
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Ferré P, 2018, FRONT COMPUT NEUROSC, V12, DOI 10.3389/fncom.2018.00024
   Geng QC, 2021, IEEE T IMAGE PROCESS, V30, P2436, DOI 10.1109/TIP.2020.3046921
   github, US
   Horowitz M, 2014, ISSCC DIG TECH PAP I, V57, P10, DOI 10.1109/ISSCC.2014.6757323
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Huynh PK, 2022, Arxiv, DOI arXiv:2202.08897
   Kim S, 2020, AAAI CONF ARTIF INTE, V34, P11270
   Kim Y, 2022, NEUROMORPH COMPUT EN, V2, DOI 10.1088/2634-4386/ac9b86
   Le NQK, 2022, J CHEM INF MODEL, DOI 10.1021/acs.jcim.2c01034
   Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508
   Li SX, 2021, IEEE T CIRCUITS-I, V68, P1543, DOI 10.1109/TCSI.2021.3052885
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Lu Y, 2019, LECT NOTES COMPUT SC, V11554, P97, DOI 10.1007/978-3-030-22796-8_11
   Midya R, 2019, ADV ELECTRON MATER, V5, DOI 10.1002/aelm.201900060
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Park S, 2020, DES AUT CON, DOI [10.1109/dac18072.2020.9218689, 10.1007/s00779-020-01476-2]
   Paszke A, 2019, ADV NEUR IN, V32
   Rathi N, 2020, Arxiv, DOI arXiv:2008.03658
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Roy K, 2019, NATURE, V575, P607, DOI 10.1038/s41586-019-1677-2
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Safa A, 2021, SOFTW IMPACTS, V10, DOI 10.1016/j.simpa.2021.100131
   Shastri BJ, 2021, NAT PHOTONICS, V15, P102, DOI 10.1038/s41566-020-00754-y
   Shelhamer E., 2015, P IEEE C COMP VIS PA, P3431
   Sze V, 2017, P IEEE, V105, P2295, DOI 10.1109/JPROC.2017.2761740
   Xiang SY, 2021, IEEE J SEL TOP QUANT, V27, DOI 10.1109/JSTQE.2020.3005589
   Xiang SY, 2019, IEEE J SEL TOP QUANT, V25, DOI 10.1109/JSTQE.2019.2911565
   Yuan QT, 2023, BRIEF BIOINFORM, V24, DOI 10.1093/bib/bbac630
   Zenke F, 2021, NEURAL COMPUT, V33, P899, DOI 10.1162/neco_a_01367
NR 38
TC 0
Z9 0
U1 3
U2 3
PD SEP
PY 2023
VL 12
IS 17
AR 3565
DI 10.3390/electronics12173565
UT WOS:001061054300001
DA 2023-11-16
ER

PT C
AU Capecci, E
   Espinosa-Ramos, JI
   Mammone, N
   Kasabov, N
   Duun-Henriksen, J
   Kjaer, TW
   Campolo, M
   La Foresta, F
   Morabito, FC
AF Capecci, Elisa
   Espinosa-Ramos, Josafath I.
   Mammone, Nadia
   Kasabov, Nikola
   Duun-Henriksen, Jonas
   Kjaer, Troels Wesenberg
   Campolo, Maurizio
   La Foresta, Fabio
   Morabito, Francesco C.
GP IEEE
TI Modelling Absence Epilepsy Seizure Data in the NeuCube Evolving Spiking
   Neural Network Architecture
SO 2015 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUL 12-17, 2015
CL Killarney, IRELAND
DE Spiking Neural Networks; EEG; NeuCube; Epilepsy; Childhood Absence
   Seizures
AB Epilepsy is the most diffuse brain disorder that can affect people's lives even on its early stage. In this paper, we used for the first time the spiking neural networks (SNN) framework called NeuCube for the analysis of electroencephalography (EEG) data recorded from a person affected by Absence Epileptic (AE), using permutation entropy (PE) features. Our results demonstrated that the methodology constitutes a valuable tool for the analysis and understanding of functional changes in the brain in term of its spiking activity and connectivity. Future applications of the model aim at personalised modelling of epileptic data for the analysis and the event prediction.
C1 [Capecci, Elisa; Kasabov, Nikola] Auckland Univ Technol, Knowledge Engn & Discovery Res Inst, AUT Tower,Level 7,Cnr Rutland & Wakefield St, Auckland 1010, New Zealand.
   [Espinosa-Ramos, Josafath I.] Natl Polytech Inst, Ctr Res Comp, Mexico City 07738, DF, Mexico.
   [Mammone, Nadia] IRCCS Ctr Neurolesi Bonino Pulejo, Messina, Italy.
   [Duun-Henriksen, Jonas] HypoSafe AS, DK-2800 Lyngby, Denmark.
   [Kjaer, Troels Wesenberg] Roskilde Univ Hosp, Dept Neurol, Neurophysiol Ctr, DK-4000 Roskilde, Denmark.
   [Campolo, Maurizio; La Foresta, Fabio; Morabito, Francesco C.] Mediterranean Univ Reggio Calabria, DICEAM Dept, I-89060 Reggio Di Calabria, Italy.
RP Capecci, E (corresponding author), Auckland Univ Technol, Knowledge Engn & Discovery Res Inst, AUT Tower,Level 7,Cnr Rutland & Wakefield St, Auckland 1010, New Zealand.
EM ecapecci@aut.ac.nz; vjier@prodigy.net.mx; nadiamammone@tiscali.it
CR [Anonymous], 2011, IEEE INT S MEDICAL M
   [Anonymous], 2012 INT JOINT C NEU, DOI DOI 10.1109/IJCNN.2012.6252439
   [Anonymous], 2007, RETRIEVED
   [Anonymous], 1988, COPLANAR STEREOTAXIS
   Benuskova L., 2007, TOP BIOMED ENG
   Capecci E., 2014, SMART INNOVATION SYS
   Capecci E., 2015, NEURAL NETW IN PRESS
   Chan V, 2007, IEEE T CIRCUITS-I, V54, P48, DOI 10.1109/TCSI.2006.887979
   Crunelli V, 2002, NAT REV NEUROSCI, V3, P371, DOI 10.1038/nrn811
   Doborjeh MG, 2014, 2014 IEEE SYMPOSIUM ON EVOLVING AND AUTONOMOUS LEARNING SYSTEMS (EALS), P73, DOI 10.1109/EALS.2014.7009506
   Duun-Henriksen J, 2012, PEDIATR NEUROL, V46, P287, DOI 10.1016/j.pediatrneurol.2012.02.018
   Duun-Henriksen J, 2012, CLIN NEUROPHYSIOL, V123, P84, DOI 10.1016/j.clinph.2011.06.001
   Espinosa-Ramos J. I., 2015, NEURAL NETWORK UNPUB
   Gerstner W, 2001, PLAUSIBLE NEURAL NET
   Kasabov Nikola, 2012, Artificial Neural Networks in Pattern Recognition. Proceedings of the 5th INNS IAPR TC 3 GIRPR Workshop, ANNPR 2012, P225, DOI 10.1007/978-3-642-33212-8_21
   Kasabov N., 2014, INFORM SCI
   Kasabov N., 2014, NEUROCOMPUT IN PRESS
   Kasabov N, 2013, NEURAL NETWORKS, V41, P188, DOI 10.1016/j.neunet.2012.11.014
   Kasabov N, 2010, NEURAL NETWORKS, V23, P16, DOI 10.1016/j.neunet.2009.08.010
   Kasabov NK, 2014, NEURAL NETWORKS, V52, P62, DOI 10.1016/j.neunet.2014.01.006
   Koessler L, 2009, NEUROIMAGE, V46, P64, DOI 10.1016/j.neuroimage.2009.02.006
   Lancaster JL, 2000, HUM BRAIN MAPP, V10, P120, DOI 10.1002/1097-0193(200007)10:3<120::AID-HBM30>3.0.CO;2-8
   Mammone N, 2012, INT J NEURAL SYST, V22, DOI 10.1142/S0129065712500244
   Manning JPA, 2003, TRENDS PHARMACOL SCI, V24, P542, DOI 10.1016/j.tips.2003.08.006
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Thorpe S, 1998, COMPUTATIONAL NEUROSCIENCE: TRENDS IN RESEARCH, P113
   Tu EM, 2014, IEEE IJCNN, P638, DOI 10.1109/IJCNN.2014.6889717
   Yixiong Chen, 2013, Neural Information Processing. 20th International Conference, ICONIP 2013. Proceedings: LNCS 8228, P70, DOI 10.1007/978-3-642-42051-1_10
NR 28
TC 0
Z9 0
U1 1
U2 1
PY 2015
UT WOS:000370730603062
DA 2023-11-16
ER

PT J
AU Lobo, JL
   Lana, I
   Del Ser, J
   Bilbao, MN
   Kasabov, N
AF Lobo, Jesus L.
   Lana, Ibai
   Del Ser, Javier
   Bilbao, Miren Nekane
   Kasabov, Nikola
TI Evolving Spiking Neural Networks for online learning over drifting data
   streams
SO NEURAL NETWORKS
DT Review
DE Spiking Neural Networks; Data reduction; Online learning; Concept drift
ID SELF-GENERATING PROTOTYPES; PRINCIPLES
AB Nowadays huge volumes of data are produced in the form of fast streams, which are further affected by non-stationary phenomena. The resulting lack of stationarity in the distribution of the produced data calls for efficient and scalable algorithms for online analysis capable of adapting to such changes (concept drift). The online learning field has lately turned its focus on this challenging scenario, by designing incremental learning algorithms that avoid becoming obsolete after a concept drift occurs. Despite the noted activity in the literature, a need for new efficient and scalable algorithms that adapt to the drift still prevails as a research topic deserving further effort. Surprisingly, Spiking Neural Networks, one of the major exponents of the third generation of artificial neural networks, have not been thoroughly studied as an online learning approach, even though they are naturally suited to easily and quickly adapting to changing environments. This work covers this research gap by adapting Spiking Neural Networks to meet the processing requirements that online learning scenarios impose. In particular the work focuses on limiting the size of the neuron repository and making the most of this limited size by resorting to data reduction techniques. Experiments with synthetic and real data sets are discussed, leading to the empirically validated assertion that, by virtue of a tailored exploitation of the neuron repository, Spiking Neural Networks adapt better to drifts, obtaining higher accuracy scores than naive versions of Spiking Neural Networks for online learning environments. (C) 2018 Elsevier Ltd. All rights reserved.
C1 [Lobo, Jesus L.; Lana, Ibai; Del Ser, Javier] TECNALIA, Div ICT, Parque Tecnol Bizkaia, Derio 48160, Spain.
   [Del Ser, Javier; Bilbao, Miren Nekane] Univ Basque Country UPV EHU, Bilbao 48013, Spain.
   [Del Ser, Javier] BCAM, Bilbao 48009, Spain.
   [Kasabov, Nikola] Auckland Univ Technol, KEDRI, Auckland 1010, New Zealand.
RP Lobo, JL (corresponding author), TECNALIA, Div ICT, Parque Tecnol Bizkaia, Derio 48160, Spain.
EM jesus.lopez@tecnalia.com
CR Aggarwal Charu C, 2006, P 32 INT C VERY LARG, P607
   Alippi C, 2008, IEEE T NEURAL NETWOR, V19, P2053, DOI 10.1109/TNN.2008.2003998
   Alippi Cesare, 2014, INTELLIGENCE EMBEDDE
   Alnajjar F, 2008, IEEE IJCNN, P2207, DOI 10.1109/IJCNN.2008.4634103
   [Anonymous], 2018, EVOL SYST-GER, DOI DOI 10.1007/s12530-016-9168-2
   [Anonymous], 2007, EVOLVING CONNECTIONI
   Baena-Garcia M., 2006, 4 INT WORKSHOP KNOWL, V6, P77
   Barddal JP, 2017, J SYST SOFTWARE, V127, P278, DOI 10.1016/j.jss.2016.07.005
   Belatreche A, 2007, SOFT COMPUT, V11, P239, DOI [10.1007/s00500-006-0065-7, 10.1007/S00500-006-0065-7]
   Bifet A, 2006, LECT NOTES ARTIF INT, V4265, P29
   Bifet A, 2009, LECT NOTES ARTIF INT, V5828, P23, DOI 10.1007/978-3-642-05224-8_4
   Bifet A, 2007, PROCEEDINGS OF THE SEVENTH SIAM INTERNATIONAL CONFERENCE ON DATA MINING, P443
   Bohte S. M., 2000, 8th European Symposium on Artificial Neural Networks. ESANN"2000. Proceedings, P419
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324
   Cohen E., 2003, P 22 ACM SIGMOD SIGA, P223, DOI DOI 10.1145/773153.773175
   Dawid AP, 1999, BERNOULLI, V5, P125, DOI 10.2307/3318616
   Derrac J., 2010, MEMET COMPUT, V2, P183, DOI DOI 10.1007/S12293-010-0048-1
   Ditzler G, 2015, IEEE COMPUT INTELL M, V10, P12, DOI 10.1109/MCI.2015.2471196
   Domingos P, 2003, J COMPUT GRAPH STAT, V12, P945, DOI 10.1198/1061860032544
   Domingos P., 2000, Proceedings. KDD-2000. Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P71, DOI 10.1145/347090.347107
   Elwell R, 2011, IEEE T NEURAL NETWOR, V22, P1517, DOI 10.1109/TNN.2011.2160459
   Escalante HJ, 2016, APPL SOFT COMPUT, V40, P569, DOI 10.1016/j.asoc.2015.12.015
   Fayed HA, 2007, PATTERN RECOGN, V40, P1498, DOI 10.1016/j.patcog.2006.10.018
   Gama J, 2004, LECT NOTES ARTIF INT, V3171, P286
   Gama J, 2014, ACM COMPUT SURV, V46, DOI 10.1145/2523813
   García S, 2012, IEEE T PATTERN ANAL, V34, P417, DOI 10.1109/TPAMI.2011.142
   Gerstner W., 2002, SPIKING NEURON MODEL
   Gomes HM, 2017, MACH LEARN, V106, P1469, DOI 10.1007/s10994-017-5642-8
   Gomes HM, 2017, ACM COMPUT SURV, V50, DOI 10.1145/3054925
   Gonçalves PM, 2013, PATTERN RECOGN LETT, V34, P1018, DOI 10.1016/j.patrec.2013.02.005
   GROSSBERG S, 1988, NEURAL NETWORKS, V1, P17, DOI 10.1016/0893-6080(88)90021-4
   Harries Michael, 1999, SPLICE 2 COMP EVALUA
   HART PE, 1968, IEEE T INFORM THEORY, V14, P515, DOI 10.1109/TIT.1968.1054155
   Hu WW, 2016, IEEE T CYBERNETICS, V46, P2719, DOI 10.1109/TCYB.2015.2487318
   Kasabov N, 2016, NEURAL NETWORKS, V78, P1, DOI 10.1016/j.neunet.2015.09.011
   Kasabov N, 2013, NEURAL NETWORKS, V41, P188, DOI 10.1016/j.neunet.2012.11.014
   Kasabov NK, 2014, NEURAL NETWORKS, V52, P62, DOI 10.1016/j.neunet.2014.01.006
   Khamassi I., 2017, CEUR WORKSHOP P, V1958
   Khamassi I, 2015, COGN COMPUT, V7, P772, DOI 10.1007/s12559-015-9341-0
   Klinkenberg R., 2004, Intelligent Data Analysis, V8, P281
   Krawczyk B, 2017, INFORM FUSION, V37, P132, DOI 10.1016/j.inffus.2017.02.004
   Kukar M, 2007, MACHINE LEARNING DAT
   Li J, 2015, INT J SENS NETW, V17, P163, DOI 10.1504/IJSNET.2015.068179
   Lobo J. L., 2017, APPL SOFT COMPUTING
   Meena L, 2015, LECT NOTES COMPUT SC, V9489, P671, DOI 10.1007/978-3-319-26532-2_74
   Minku LL, 2012, IEEE T KNOWL DATA EN, V24, P619, DOI 10.1109/TKDE.2011.58
   Minku LL, 2010, IEEE T KNOWL DATA EN, V22, P730, DOI 10.1109/TKDE.2009.156
   Ng W, 2008, LECT NOTES COMPUT SC, V4947, P204, DOI 10.1007/978-3-540-78568-2_17
   Oliveira DVR, 2012, PROC INT C TOOLS ART, P904, DOI 10.1109/ICTAI.2012.126
   Ponulak F., 2005, RESUME NEW SUPERVISE, V42
   Ponulak F, 2008, INT J APPL MATH COMP, V18, P117, DOI 10.2478/v10006-008-0011-1
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Ruiqin Chang, 2011, Journal of Computers, V6, P1493, DOI 10.4304/jcp.6.7.1493-1500
   Schliebs S, 2013, EVOL SYST-GER, V4, P87, DOI 10.1007/s12530-013-9074-9
   Soltic S, 2008, IEEE IJCNN, P2091, DOI 10.1109/IJCNN.2008.4634085
   Soltic S, 2010, INT J NEURAL SYST, V20, P437, DOI 10.1142/S012906571000253X
   Thorpe S, 1998, COMPUTATIONAL NEUROSCIENCE: TRENDS IN RESEARCH, P113
   Thorpe SJ, 1997, ADV NEUR IN, V9, P901
   TOMEK I, 1976, IEEE T SYST MAN CYB, V6, P769, DOI 10.1109/tsmc.1976.4309452
   TOMEK I, 1976, IEEE T SYST MAN CYB, V6, P448
   Triguero I, 2012, IEEE T SYST MAN CY C, V42, P86, DOI 10.1109/TSMCC.2010.2103939
   Triguero I, 2010, IEEE T NEURAL NETWOR, V21, P1984, DOI 10.1109/TNN.2010.2087415
   VITTER JS, 1985, ACM T MATH SOFTWARE, V11, P37, DOI 10.1145/3147.3165
   Wang JL, 2017, IEEE T NEUR NET LEAR, V28, P30, DOI 10.1109/TNNLS.2015.2501322
   Wang JL, 2014, NEUROCOMPUTING, V144, P526, DOI 10.1016/j.neucom.2014.04.017
   Wang S, 2018, IEEE T NEUR NET LEAR, V29, P4802, DOI 10.1109/TNNLS.2017.2771290
   Webb GI, 2016, DATA MIN KNOWL DISC, V30, P964, DOI 10.1007/s10618-015-0448-4
   WILSON DL, 1972, IEEE T SYST MAN CYB, VSMC2, P408, DOI 10.1109/TSMC.1972.4309137
   Wysoski SG, 2006, LECT NOTES COMPUT SC, V4179, P1133
   Wysoski SG, 2010, NEURAL NETWORKS, V23, P819, DOI 10.1016/j.neunet.2010.04.009
   Zhou ZH, 2014, IEEE COMPUT INTELL M, V9, P62, DOI 10.1109/MCI.2014.2350953
   Zliobaite I., 2010, ABS10104784 CORR
   Zliobaite I, 2016, STUD BIG DATA, V16, P91, DOI 10.1007/978-3-319-26989-4_4
NR 74
TC 45
Z9 47
U1 0
U2 47
PD DEC
PY 2018
VL 108
BP 1
EP 19
DI 10.1016/j.neunet.2018.07.014
UT WOS:000450298900001
DA 2023-11-16
ER

PT C
AU Kuang, ZB
   Wang, J
   Yang, SM
   Yi, GS
   Deng, B
   Wei, XL
AF Kuang, Zaibo
   Wang, Jiang
   Yang, Shuangming
   Yi, Guosheng
   Deng, Bin
   Wei, Xile
GP IEEE
TI Digital Implementation of the Spiking Neural Network and Its Digit
   Recognition
SO PROCEEDINGS OF THE 2019 31ST CHINESE CONTROL AND DECISION CONFERENCE
   (CCDC 2019)
SE Chinese Control and Decision Conference
DT Proceedings Paper
CT 31st Chinese Control And Decision Conference (CCDC)
CY JUN 03-05, 2019
CL Nanchang, PEOPLES R CHINA
DE Spiking neural network; Real-time implementation; FPGA; STDP; Digit
   recognition
ID EFFICIENT FPGA IMPLEMENTATION
AB Motivated by biological principles of neural systems, spiking neural network (SNN) shows a tremendous potential in solving pattern recognition and cognitive tasks in recent years. In this study, a biologically inspired SNN composed of three layers is implemented on a reconfigurable FPGA with high computational efficiency and low hardware cost. The proposed SNN is consists of spiking neurons simulated by leaky-integrate-and-fire neuron model. In addition, spiking-time-dependent-plasticity based on event-driven is utilized to train the constructed network. The real-time hardware realization of the proposed SNN demonstrates powerful and efficient learning scheme. Results on different datasets shows that the proposed SNN implementation has the merit of capability of coping with pattern recognition tasks. Furthermore, the proposed implementation with remarkable performance could be applied and embed in bio-inspired neuromorphic platform such as robots for recognition tasks and on-line applications.
C1 [Kuang, Zaibo; Wang, Jiang; Yang, Shuangming; Yi, Guosheng; Deng, Bin; Wei, Xile] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
RP Wang, J (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
EM jiangwang@tju.edu.cn
CR Berger TW, 2012, IEEE T NEUR SYS REH, V20, P198, DOI 10.1109/TNSRE.2012.2189133
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Lee SW, 2015, PLOS BIOL, V13, DOI 10.1371/journal.pbio.1002137
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   O'reilly R.C., 2000, COMPUTATIONAL EXPLOR
   Rumelhart D.E., 1986, PARALLEL DISTRIBUTED, V1, DOI [10.21236/ada164453, 10.1016/b978-1-4832-1446-7.50035-2]
   Tal D, 2014, NEURAL COMPUT, V9, P305
   Xin Y, 2014, MICROELECTRON J, V45, P690, DOI 10.1016/j.mejo.2014.03.018
   Yang S., 2018, IEEE T CYBERNETICS
   Yang SM, 2018, NEUROCOMPUTING, V314, P394, DOI 10.1016/j.neucom.2018.07.006
   Yang SM, 2018, NEUROCOMPUTING, V282, P262, DOI 10.1016/j.neucom.2017.12.031
   Yang SM, 2018, PHYSICA A, V494, P484, DOI 10.1016/j.physa.2017.11.155
   Yang SM, 2017, NEURAL NETWORKS, V94, P220, DOI 10.1016/j.neunet.2017.07.012
   Yang SM, 2017, SCI REP-UK, V7, DOI 10.1038/srep40152
   Yang SM, 2016, NEUROCOMPUTING, V177, P274, DOI 10.1016/j.neucom.2015.11.026
   Yang SM, 2015, NEURAL NETWORKS, V71, P62, DOI 10.1016/j.neunet.2015.07.017
NR 17
TC 3
Z9 3
U1 3
U2 11
PY 2019
BP 3621
EP 3625
DI 10.1109/ccdc.2019.8832952
UT WOS:000555859003155
DA 2023-11-16
ER

PT C
AU Hsieh, HY
   Li, PY
   Yang, CH
   Tang, KT
AF Hsieh, Hung-Yi
   Li, Pin-Yi
   Yang, Cheng-Han
   Tang, Kea-Tiong
GP IEEE
TI A High Learning Capability Probabilistic Spiking Neural Network Chip
SO 2018 INTERNATIONAL SYMPOSIUM ON VLSI DESIGN, AUTOMATION AND TEST
   (VLSI-DAT)
SE International Symposium on VLSI Design Automation and Test
DT Proceedings Paper
CT International Symposium on VLSI Design, Automation and Test (VLSI-DAT)
CY APR 16-19, 2018
CL Hsinchu, TAIWAN
DE Online learning; Probabilistic Spiking Neural Network;
   Switehed-capacitor; Learning capability
ID IMPLEMENTATION; HARDWARE
AB This paper presents an analog implementation of probabilistic spiking neural network for portable or biomedical applications which require learning or classification. Online learning adjusts weights by spike based computation. The weight is saved in the long-term synaptic memory. The circuit primarily uses the switched-capacitor structures and was fabricated using 0.18 mu m CMOS technology. This chip consumes less than 10 mu W under a 1V supply and the core area of the chip occupies 0.43mm(2). The chip can learn 80 random patterns with the area under curve of 0.8. The result indicates the chip is appropriate for portable or biomedical applications.
C1 [Hsieh, Hung-Yi; Li, Pin-Yi; Yang, Cheng-Han; Tang, Kea-Tiong] Natl Tsing Hua Univ, Dept Elect Engn, Neuromorph & Biomed Engn Lab, Hsinchu 30013, Taiwan.
RP Hsieh, HY (corresponding author), Natl Tsing Hua Univ, Dept Elect Engn, Neuromorph & Biomed Engn Lab, Hsinchu 30013, Taiwan.
EM hyhsieh@larc.ee.nthu.edu.tw; pinyili@foxmial.com; jason950374@gmail.com;
   kttang@ee.nthu.edu.tw
CR Cauwenberghs G, 1999, IEEE T CIRCUITS-II, V46, P240, DOI 10.1109/82.754858
   Fawcett T, 2006, PATTERN RECOGN LETT, V27, P861, DOI 10.1016/j.patrec.2005.10.010
   Hsieh HY, 2013, IEEE T NEUR NET LEAR, V24, P2063, DOI 10.1109/TNNLS.2013.2271644
   Hsieh HY, 2012, IEEE T NEUR NET LEAR, V23, P1065, DOI 10.1109/TNNLS.2012.2195329
   Kasabov N, 2010, NEURAL NETWORKS, V23, P16, DOI 10.1016/j.neunet.2009.08.010
   Misra J, 2010, NEUROCOMPUTING, V74, P239, DOI 10.1016/j.neucom.2010.03.021
   Mitra S, 2009, IEEE T BIOMED CIRC S, V3, P32, DOI 10.1109/TBCAS.2008.2005781
   Sun Q, 2011, IEEE T NEURAL NETWOR, V22, P858, DOI 10.1109/TNN.2011.2125986
   Sun YW, 2012, COMPUT BIOL MED, V42, P751, DOI 10.1016/j.compbiomed.2012.04.007
   Zhang R, 2012, IEEE T NEUR NET LEAR, V23, P330, DOI 10.1109/TNNLS.2011.2178315
NR 10
TC 2
Z9 2
U1 0
U2 0
PY 2018
UT WOS:000450113800018
DA 2023-11-16
ER

PT C
AU Maliavko, AA
   Gavrilov, AV
AF Maliavko, Aleksandr A.
   Gavrilov, Andrey V.
GP IEEE
TI Towards Development of Self-Learning and Self-Modification Spiking
   Neural Network as Model of Brain
SO 2016 13TH INTERNATIONAL SCIENTIFIC-TECHNICAL CONFERENCE ON ACTUAL
   PROBLEMS OF ELECTRONIC INSTRUMENT ENGINEERING (APEIE), VOL 2
SE International Conference on Actual Problems of Electronic Instrument
   Engineering
DT Proceedings Paper
CT 13th International Scientific-Technical Conference on Actual Problems of
   Electronics Instrument Engineering (APEIE)
CY OCT 03-06, 2016
CL Novosibirsk, RUSSIA
DE Artificial intelligence; Neural networks; Brain; Machine learning
AB In this paper novel architecture of artificial intelligence based on spike neural networks is proposed. This architecture includes some perception, some action modules and control module supervising learning of others. Control module is based on generation and learning to generate basic emotions: positive and negative. These emotions are supervising learning and behavior of whole system.
C1 [Maliavko, Aleksandr A.; Gavrilov, Andrey V.] Novosibirsk State Tech Univ, Novosibirsk, Russia.
RP Maliavko, AA (corresponding author), Novosibirsk State Tech Univ, Novosibirsk, Russia.
CR Anokhin Pyotr K., 1974, BIOL NEUROPHYSIOLOGY
   [Anonymous], INTELLIGENCE
   [Anonymous], 2000, HYBRID NEURAL SYSTEM
   [Anonymous], DESIGNING SOLUTIONS
   [Anonymous], P INT C INT INF ENG
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Gavrilov AV, 2003, KORUS 2003: 7TH KOREA-RUSSIA INTERNATIONAL SYMPOSIUM ON SCIENCE AND TECHNOLOGY, VOL 2, PROCEEDINGS, P143
   Smith LS, 2010, ADV EXP MED BIOL, V657, P167, DOI 10.1007/978-0-387-79100-5_9
   Starzyk JA, 2006, IEEE T NEURAL NETWOR, V17, P1460, DOI 10.1109/TNN.2006.883008
NR 9
TC 2
Z9 3
U1 0
U2 4
PY 2016
BP 461
EP 463
UT WOS:000392625500106
DA 2023-11-16
ER

PT J
AU Wu, J
   Furber, S
AF Wu, Jian
   Furber, Steve
TI A Multicast Routing Scheme for a Universal Spiking Neural Network
   Architecture
SO COMPUTER JOURNAL
DT Article
DE neural network; routing; event-driven; multicast; fault-tolerance
ID SIMULATION
AB A multicast routing infrastructure is proposed as a core feature of SpiNNaker, a massively parallel computer for the real-time simulation of large-scale spiking neural networks. The infrastructure is implemented using a communications router, based on an event-driven routing scheme, on each multicore processing node in the system. The design considerations emphasize the difference between the requirements of neural network communications and those of conventional computer networks and on-chip networks. The focus of the design is on neural modelling flexibility, power-efficiency, fault-tolerance and the communication throughput of the router.
C1 [Wu, Jian; Furber, Steve] Univ Manchester, Sch Comp Sci, Adv Processor Technol Grp, Manchester M13 9PL, Lancs, England.
RP Wu, J (corresponding author), Univ Manchester, Sch Comp Sci, Adv Processor Technol Grp, Oxford Rd, Manchester M13 9PL, Lancs, England.
EM wuj@cs.man.ac.uk
CR Agis R, 2007, INT J ELECTRON, V94, P469, DOI 10.1080/00207210701308625
   [Anonymous], P 10 SIAM C PAR PROC
   [Anonymous], 1988, PARALLEL DISTRIBUTED
   Bainbridge J, 2002, IEEE MICRO, V22, P16, DOI 10.1109/MM.2002.1044296
   Boahen KA, 2000, IEEE T CIRCUITS-II, V47, P416, DOI 10.1109/82.842110
   Boucheny C, 2005, LECT NOTES COMPUT SC, V3512, P136
   Cong B, 1997, PROCEEDINGS OF THE IEEE 1997 AEROSPACE AND ELECTRONICS CONFERENCE - NAECON 1997, VOLS 1 AND 2, P231, DOI 10.1109/NAECON.1997.618084
   Dally W., 2003, PRINCIPLES PRACTICES
   Danese G, 2002, IEEE MICRO, V22, P20, DOI 10.1109/MM.2002.1013301
   Furber S, 2006, P AISB WORKSH GC5 AR, P29
   Furber S, 2008, STUD COMPUT INTELL, V115, P763, DOI 10.1098/rsif.2006.0177
   GELENBE E, 1993, NEURAL COMPUT, V5, P154, DOI 10.1162/neco.1993.5.1.154
   Gelenbe E, 1989, NEURAL COMPUT, V1, P502, DOI 10.1162/neco.1989.1.4.502
   Gerstner W., 2002, SPIKING NEURON MODEL
   HAMMERSTROM D, 1990, P INT JOINT C NEUR N, P537
   Hu JC, 2004, DES AUT CON, P260
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   JAHNKE A, 1997, LECT NOTES COMPUTER, V1327, P1187
   JIN X, 2008, P IJCNN 08 HONG KONG, P2813
   Kumar A, 2007, PR IEEE COMP DESIGN, P63, DOI 10.1109/ICCD.2007.4601881
   Plana Luis A., 2008, 2008 2nd ACM/IEEE International Symposium on Networks-on-Chip (NOCS '08), P215, DOI 10.1109/NOCS.2008.4492744
   Plana LA, 2007, IEEE DES TEST COMPUT, V24, P454, DOI 10.1109/MDT.2007.149
   Plesser HE, 2007, LECT NOTES COMPUT SC, V4641, P672
   RAJEEV D, 1998, IEEE T PARALL DISTR, V9, P1004
   Ramacher U., 1995, Proceedings 9th International Parallel Processing Symposium (Cat. No.95TH8052), P774, DOI 10.1109/IPPS.1995.395862
   Sparso J, 2001, PRINCIPLES OF ASYNCHRONOUS CIRCUIT DESIGN: A SYSTEMS PERSPECTIVE, P3
   SUN Q, 2007, P 2007 INT C PAR PRO, P21
   TUCKER LW, 1988, COMPUTER, V21, P26, DOI 10.1109/2.74
   Tutsch D, 2001, LECT NOTES COMPUT SC, V2093, P478
   Wu J., 2006, P 18 UK AS FOR, V2, P16
   STRUCTURE TYPICAL NE
NR 31
TC 14
Z9 14
U1 4
U2 8
PD MAR
PY 2010
VL 53
IS 3
BP 280
EP 288
DI 10.1093/comjnl/bxp024
UT WOS:000274974800004
DA 2023-11-16
ER

PT C
AU Daddinounou, S
   Vatajelu, EI
AF Daddinounou, Salah
   Vatajelu, Elena Ioana
BE Kubatova, H
   Steininger, A
   Jenihhin, M
   Garbolino, T
   Fiser, P
   Belohoubek, J
   Borecky, J
TI Synaptic Control for Hardware Implementation of Spike Timing Dependent
   Plasticity
SO 2022 25TH INTERNATIONAL SYMPOSIUM ON DESIGN AND DIAGNOSTICS OF
   ELECTRONIC CIRCUITS AND SYSTEMS (DDECS)
SE IEEE International Symposium on Design and Diagnostics of Electronic
   Circuits & Systems
DT Proceedings Paper
CT 25th International Symposium on Design and Diagnostics of Electronic
   Circuits and Systems (DDECS)
CY APR 06-08, 2022
CL Prague, CZECH REPUBLIC
DE Spiking Neural Networks (SNN); unsupervised learning; Magnetic Tunnel
   Junction (MTJ); neuromorphics.
ID NETWORK
AB Spiking neural networks (SNN) are biologically plausible networks. Compared to formal neural networks, they come with huge benefits related to their asynchronous processing and massively parallel architecture. Recent developments in neuromorphics aim to implement these SNNs in hardware to fully exploit their potential in terms of low energy consumption. In this paper, the plasticity of a multi-state conductance synapse in SNN is shown. The synapse is a compound of multiple Magnetic Tunnel Junction (MTJ) devices connected in parallel. The network performs learning by potentiation and depression of the synapses. In this paper we show how these two mechanisms can be obtained in hardware-implemented SNNs. We present a methodology to achieve the Spike Timing Dependent Plasticity (STDP) learning rule in hardware by carefully engineering the post- and pre-synaptic signals. We demonstrate synaptic plasticity as a function of the relative spiking time of input and output neurons only.
C1 [Daddinounou, Salah; Vatajelu, Elena Ioana] Univ Grenoble Alpes, CNRS, Grenoble INP, TIMA, F-38000 Grenoble, France.
RP Daddinounou, S (corresponding author), Univ Grenoble Alpes, CNRS, Grenoble INP, TIMA, F-38000 Grenoble, France.
EM salah.daddinounou@univ-grenoble-alpes.fr; elena-ioana.vatajelu@cnrs.fr
CR Andreeva NV, 2020, BIONANOSCIENCE, V10, P824, DOI 10.1007/s12668-020-00778-2
   Ballard Z, 2021, NAT MACH INTELL, V3, P556, DOI 10.1038/s42256-021-00360-9
   Courbariaux M, 2016, Arxiv, DOI arXiv:1602.02830
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Furber SB, 2013, IEEE T COMPUT, V62, P2454, DOI 10.1109/TC.2012.142
   Kok M., 2020, THESIS U TWENTE
   LECUN Y, 1989, P ADV NEUR INF PROC, V2, P1
   Maranhao G, 2021, IET CIRC DEVICE SYST, V15, P237, DOI 10.1049/cds2.12018
   Markovic D, 2020, NAT REV PHYS, V2, P499, DOI 10.1038/s42254-020-0208-2
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Querlioz D, 2013, IEEE T NANOTECHNOL, V12, P288, DOI 10.1109/TNANO.2013.2250995
   Strong SP, 1998, PHYS REV LETT, V80, P197, DOI 10.1103/PhysRevLett.80.197
   Zhang DM, 2016, IEEE T BIOMED CIRC S, V10, P828, DOI 10.1109/TBCAS.2016.2533798
   Zhang Y, 2015, IEEE T ELECTRON DEV, V62, P2048, DOI 10.1109/TED.2015.2414721
NR 15
TC 2
Z9 2
U1 2
U2 6
PY 2022
BP 106
EP 111
UT WOS:000835725500019
DA 2023-11-16
ER

PT J
AU Eshraghian, JK
   Wang, XX
   Lu, WD
AF Eshraghian, Jason K.
   Wang, Xinxin
   Lu, Wei D.
TI Memristor-Based Binarized Spiking Neural Networks
SO IEEE NANOTECHNOLOGY MAGAZINE
DT Article
ID MEMORY
C1 [Eshraghian, Jason K.; Wang, Xinxin; Lu, Wei D.] Univ Michigan, Dept Elect Engn & Comp Sci, Ann Arbor, MI 48109 USA.
RP Eshraghian, JK (corresponding author), Univ Michigan, Dept Elect Engn & Comp Sci, Ann Arbor, MI 48109 USA.
EM jeshraghian@gmail.com; xinxinw@umich.edu; wluee@eecs.umich.edu
CR Amodei D., 2019, AI AND COMPUTE
   [Anonymous], 2013, RELIABILITY MAINTAIN, DOI DOI 10.1109/IEDM.2013.6724674
   Azghadi MR, 2020, ADV INTELL SYST-GER, V2, DOI 10.1002/aisy.201900189
   Bakir MS, 2008, IEEE CUST INTEGR CIR, P663, DOI 10.1109/CICC.2008.4672173
   Banbury Colby R., 2020, ARXIV200304821
   Bengio Yoshua, 2013, ABS13083432 CORR
   Bergstra J., 2011, P 24 INT C NEURAL IN, P2546
   BLAKEMOR.C, 1970, NATURE, V228, P37, DOI 10.1038/228037a0
   Brown Tom, 2020, NEURIPS, V1, P3
   Cai FX, 2019, NAT ELECTRON, V2, P290, DOI 10.1038/s41928-019-0270-x
   Chanthbouala A, 2012, NAT MATER, V11, P860, DOI [10.1038/NMAT3415, 10.1038/nmat3415]
   Chen B, 2015, 2015 IEEE INTERNATIONAL ELECTRON DEVICES MEETING (IEDM)
   Chen YC, 2021, ELEC COMP C, P1645, DOI 10.1109/ECTC32696.2021.00261
   Choi BJ, 2005, J APPL PHYS, V98, DOI 10.1063/1.2001146
   Correll JM, 2020, IEEE J EXPLOR SOLID-, V6, P36, DOI 10.1109/JXCDC.2020.2992228
   Davies M., 2017, ARXIV170505475
   Dhar P, 2020, NAT MACH INTELL, V2, P423, DOI 10.1038/s42256-020-0219-9
   Dong S, 2018, PROCEEDINGS OF THE 2018 ACM/SPEC INTERNATIONAL CONFERENCE ON PERFORMANCE ENGINEERING (ICPE '18), P96, DOI 10.1145/3184407.3184423
   Ebong I, 2010, INT C MICROELECTRON, P292, DOI 10.1109/ICM.2010.5696142
   Eshraghian J. K., ARXIV210912894, V2021
   Eshraghian JK, 2021, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS51556.2021.9401672
   Eshraghian JK, 2019, 2019 IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE CIRCUITS AND SYSTEMS (AICAS 2019), P267, DOI [10.1109/aicas.2019.8771550, 10.1109/AICAS.2019.8771550]
   Eshraghian JK, 2018, IEEE T VLSI SYST, V26, P2816, DOI 10.1109/TVLSI.2018.2829918
   Esser Steven K, 2019, ARXIV190208153
   Fang W, 2021, INCORPORATING LEARNA, P2661
   Frenkel, ARXIV210601288, V2021
   Gou JP, 2021, INT J COMPUT VISION, V129, P1789, DOI 10.1007/s11263-021-01453-z
   Gupta S, 2015, PR MACH LEARN RES, V37, P1737
   Hady FT, 2017, P IEEE, V105, P1822, DOI 10.1109/JPROC.2017.2731776
   Han S., 2016, P INT C LEARN REPR, DOI DOI 10.48550/ARXIV.1510.00149
   Hu M, 2016, DES AUT CON, DOI 10.1145/2897937.2898010
   Hubara I, 2016, ADV NEUR IN, V29
   Hunsberger Eric, 2015, COMPUT SCI
   Ielmini D, 2018, NAT ELECTRON, V1, P333, DOI 10.1038/s41928-018-0092-2
   Jamieson K, 2016, JMLR WORKSH CONF PRO, V51, P240
   Jouppi NP, 2017, 44TH ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA 2017), P1, DOI 10.1145/3079856.3080246
   Kang SM, 2021, IEEE T CIRCUITS-I, V68, P4837, DOI 10.1109/TCSI.2021.3126555
   King DB, 2015, ACS SYM SER, V1214, P1
   Knag, P 2016 IEEE S VLSI C, P1, DOI [10.1109/VLSIC.2016.7573526, DOI 10.1109/VLSIC.2016.7573526]
   Knag P, 2015, IEEE J SOLID-ST CIRC, V50, P1070, DOI 10.1109/JSSC.2014.2386892
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
   Lapique L., 1907, J PHYSL PATHOL GEN, V9, P620, DOI DOI 10.1007/S00422-007-0189-6
   Lask, 2020, ARXIV201203837
   Li YY, 2020, ADV MATER, V32, DOI 10.1002/adma.202003984
   Li ZY, 2021, IEEE J SOLID-ST CIRC, V56, P1105, DOI 10.1109/JSSC.2020.3045369
   Lin CY, 2020, APPL MATER TODAY, V21, DOI 10.1016/j.apmt.2020.100848
   Lin CY, 2020, SMALL, V16, DOI 10.1002/smll.202003964
   Loshchilov Ilya, 2016, ARXIV160803983
   Menzel S, 2012, J APPL PHYS, V111, DOI 10.1063/1.3673239
   Naveros F, 2015, IEEE T NEUR NET LEAR, V26, P1567, DOI 10.1109/TNNLS.2014.2345844
   Perez-Nieves N, 2021, NAT COMMUN, V12, DOI 10.1038/s41467-021-26022-3
   Polino Antonio, 2018, ARXIV180205668
   Rich D., 2020, NANOCHIPS 2030, P127, DOI DOI 10.1007/978-3-030-18338-7_9
   Richards BA, 2019, NAT NEUROSCI, V22, P1761, DOI 10.1038/s41593-019-0520-2
   Sebastian A, 2020, NAT NANOTECHNOL, V15, P529, DOI 10.1038/s41565-020-0655-z
   Shafiee A, 2016, CONF PROC INT SYMP C, P14, DOI 10.1109/ISCA.2016.12
   Shim WB, 2020, INT RELIAB PHY SYM, DOI 10.1109/irps45951.2020.9129252
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Strukov DB, 2008, NATURE, V453, P80, DOI 10.1038/nature06932
   Thompson N. C., 2020, ARXIV200705558
   Uhlich S., ARXIV190511452
   Umuroglu Y, 2017, FPGA'17: PROCEEDINGS OF THE 2017 ACM/SIGDA INTERNATIONAL SYMPOSIUM ON FIELD-PROGRAMMABLE GATE ARRAYS, P65, DOI 10.1145/3020078.3021744
   Valov I, 2011, NANOTECHNOLOGY, V22, DOI 10.1088/0957-4484/22/25/254003
   WAN WE, 2020, ISSCC DIG TECH PAP I, P498, DOI DOI 10.1109/ISSCC19947.2020.9062979
   Wang QW, 2019, INT EL DEVICES MEET, DOI 10.1109/iedm19573.2019.8993641
   Wang XX, 2022, IEEE T CIRCUITS-II, V69, P559, DOI 10.1109/TCSII.2021.3097035
   Wang XX, 2020, IEEE T CIRCUITS-I, V67, P4224, DOI 10.1109/TCSI.2020.3000468
   WERBOS PJ, 1990, P IEEE, V78, P1550, DOI 10.1109/5.58337
   Wolf Lasse F, 2020, PROC ICML WORKSHOP C
   Yao P, 2020, NATURE, V577, P641, DOI 10.1038/s41586-020-1942-4
   Zenke F, 2021, NEURAL COMPUT, V33, P899, DOI 10.1162/neco_a_01367
   Zhang JL, 2017, FPGA'17: PROCEEDINGS OF THE 2017 ACM/SIGDA INTERNATIONAL SYMPOSIUM ON FIELD-PROGRAMMABLE GATE ARRAYS, P25, DOI 10.1145/3020078.3021698
   Zhang T., 2019, ARXIV191004540
   Zhu JY, 2018, DES AUT TEST EUROPE, P241, DOI 10.23919/DATE.2018.8342010
   Zidan MA, 2018, NAT ELECTRON, V1, P22, DOI 10.1038/s41928-017-0006-8
   Zidan MA, 2013, MICROELECTRON J, V44, P176, DOI 10.1016/j.mejo.2012.10.001
NR 76
TC 25
Z9 25
U1 0
U2 19
PD APR
PY 2022
VL 16
IS 2
BP 14
EP 23
DI 10.1109/MNANO.2022.3141443
EA JAN 2022
UT WOS:000750450600001
DA 2023-11-16
ER

PT C
AU Gupta, STP
   Linares-Serrano, P
   Sen Bhattacharya, B
   Serrano-Gotarredona, T
AF Gupta, Shriya T. P.
   Linares-Serrano, Pablo
   Sen Bhattacharya, Basabdatta
   Serrano-Gotarredona, Teresa
GP IEEE
TI Foveal-pit inspired filtering of DVS spike response
SO 2021 55TH ANNUAL CONFERENCE ON INFORMATION SCIENCES AND SYSTEMS (CISS)
DT Proceedings Paper
CT 55th Annual Conference on Information Sciences and Systems (CISS)
CY MAR 24-26, 2021
CL ELECTR NETWORK
DE dynamic vision sensor; neural filtering; spiking neural network;
   classification; difference of gaussian; convolution; foveal-pit
AB In this paper, we present results of processing Dynamic Vision Sensor (DVS) recordings of visual patterns with a retinal model based on foveal-pit inspired Difference of Gaussian (DoG) filters. A DVS sensor was stimulated with varying number of vertical white and black bars of different spatial frequencies moving horizontally at a constant velocity. The output spikes generated by the DVS sensor were applied as input to a set of DoG filters inspired by the receptive field structure of the primate visual pathway. In particular, these filters mimic the receptive fields of the midget and parasol ganglion cells (spiking neurons of the retina) that sub-serve the photo-receptors of the fovealpit. The features extracted with the foveal-pit model are used for further classification using a spiking convolutional neural network trained with a backpropagation variant adapted for spiking neural networks.
C1 [Gupta, Shriya T. P.] Microsoft Corp Pvt Ltd, Microsoft Res & Dev Ctr, Bangalore, Karnataka, India.
   [Linares-Serrano, Pablo; Serrano-Gotarredona, Teresa] CSIC, CNM, IMSE, Inst Microelect Sevilla, Seville, Spain.
   [Linares-Serrano, Pablo; Serrano-Gotarredona, Teresa] Univ Seville, Seville, Spain.
   [Sen Bhattacharya, Basabdatta] BITS Pilani Goa Campus, Dept Comp Sci, Pilani, Goa, India.
RP Gupta, STP (corresponding author), Microsoft Corp Pvt Ltd, Microsoft Res & Dev Ctr, Bangalore, Karnataka, India.
EM shriyatp99@gmail.com; pablolinareserrano@gmail.com;
   basabdattab@goa.bits-pilani.ac.in; terese@imse-cnm.csic.es
CR [Anonymous], 2015, DEEP LEARNING, DOI [DOI 10.1038/NATURE14539, 10.1038/nature14539]
   Bekolay T, 2014, FRONT NEUROINFORM, V7, DOI 10.3389/fninf.2013.00048
   Bengio Y., 2007, LARGE SCALE KERNEL M, V34, P1
   Camuñas-Mesa L, 2012, IEEE J SOLID-ST CIRC, V47, P504, DOI 10.1109/JSSC.2011.2167409
   Diehl PU, 2015, IEEE IJCNN
   Gerstner W., 2002, SPIKING NEURON MODEL
   Gupta S, 2020, SYMP VLSI CIRCUITS, DOI 10.1109/vlsicircuits18222.2020.9162772
   Hunsberger E., 2016, ARXIV PREPRINT ARXIV
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012
   Mostafa Hesham, 2018, IEEE Trans Neural Netw Learn Syst, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Neftci EO, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00324
   Orchard G, 2015, FRONT NEUROSCI-SWITZ, V9, DOI [10.3389/fhins.2015.00437, 10.3389/fnins.2015.00437]
   Sen Bhattacharya B, 2010, IEEE T NEURAL NETWOR, V21, P1087, DOI 10.1109/TNN.2010.2048339
   Sen-Bhattacharya B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00454
   Serrano-Gotarredona T, 2013, IEEE J SOLID-ST CIRC, V48, P827, DOI 10.1109/JSSC.2012.2230553
   Stromatias E, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00350
   Van Rullen R, 2001, NEURAL COMPUT, V13, P1255, DOI 10.1162/08997660152002852
   Virtanen P, 2020, NAT METHODS, V17, P261, DOI 10.1038/s41592-019-0686-2
   Young A. R., 2019, IEEE ACCESS, V7
NR 21
TC 0
Z9 0
U1 0
U2 0
PY 2021
DI 10.1109/CISS50987.2021.9400245
UT WOS:000671822600037
DA 2023-11-16
ER

PT J
AU Pyle, R
   Rosenbaum, R
AF Pyle, Ryan
   Rosenbaum, Robert
TI Spatiotemporal Dynamics and Reliable Computations in Recurrent Spiking
   Neural Networks
SO PHYSICAL REVIEW LETTERS
DT Article
ID CORTICAL CIRCUITS; MODEL; SYSTEMS; NEURONS; CORTEX; CONNECTIVITY;
   MOVEMENTS; PATTERNS; MEMORY; WAVES
AB Randomly connected networks of excitatory and inhibitory spiking neurons provide a parsimonious model of neural variability, but are notoriously unreliable for performing computations. We show that this difficulty is overcome by incorporating the well-documented dependence of connection probability on distance. Spatially extended spiking networks exhibit symmetry-breaking bifurcations and generate spatiotemporal patterns that can be trained to perform dynamical computations under a reservoir computing framework.
C1 [Pyle, Ryan; Rosenbaum, Robert] Univ Notre Dame, Dept Appl & Computat Math & Stat, Notre Dame, IN 46556 USA.
   [Rosenbaum, Robert] Univ Notre Dame, Interdisciplinary Ctr Network Sci & Applicat, Notre Dame, IN 46556 USA.
RP Pyle, R (corresponding author), Univ Notre Dame, Dept Appl & Computat Math & Stat, Notre Dame, IN 46556 USA.
CR Abbott LF, 2016, NAT NEUROSCI, V19, P350, DOI 10.1038/nn.4241
   Amit DJ, 1997, CEREB CORTEX, V7, P237, DOI 10.1093/cercor/7.3.237
   [Anonymous], 1989, METHODS SOLUTION APP
   Bressloff PC, 2012, J PHYS A-MATH THEOR, V45, DOI 10.1088/1751-8113/45/3/033001
   Brunel N, 2000, J COMPUT NEUROSCI, V8, P183, DOI 10.1023/A:1008925309027
   Churchland MM, 2012, NATURE, V487, P51, DOI 10.1038/nature11129
   Coombes S, 2005, BIOL CYBERN, V93, P91, DOI 10.1007/s00422-005-0574-y
   Ercsey-Ravasz M, 2013, NEURON, V80, P184, DOI 10.1016/j.neuron.2013.07.036
   Ermentrout B, 1998, REP PROG PHYS, V61, P353, DOI 10.1088/0034-4885/61/4/002
   Gerstner W., 2002, SPIKING NEURON MODEL
   Hennequin G, 2014, NEURON, V82, P1394, DOI 10.1016/j.neuron.2014.04.045
   Izhikevich EM, 2007, DYNAMICAL SYSTEMS NE
   Kapitula T, 2004, DISCRETE CONT DYN-A, V10, P857, DOI 10.3934/dcds.2004.10.857
   Keane A, 2015, J NEUROSCI, V35, P1591, DOI 10.1523/JNEUROSCI.1669-14.2015
   Knoblauch K., 2014, MICRO MESO MACROCONN, P45
   Lajoie G, 2013, PHYS REV E, V87, DOI 10.1103/PhysRevE.87.052901
   Ledoux E, 2011, FRONT COMPUT NEUROSC, V5, DOI 10.3389/fncom.2011.00025
   Levy RB, 2012, J NEUROSCI, V32, P5609, DOI 10.1523/JNEUROSCI.5158-11.2012
   Lim S, 2013, NAT NEUROSCI, V16, P1306, DOI 10.1038/nn.3492
   Lindner B, 2004, PHYS REP, V392, P321, DOI 10.1016/j.physrep.2003.10.015
   Lund JS, 2003, CEREB CORTEX, V13, P15, DOI 10.1093/cercor/13.1.15
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Monteforte M, 2012, PHYS REV X, V2, DOI 10.1103/PhysRevX.2.041007
   Ostojic S, 2014, NAT NEUROSCI, V17, P594, DOI 10.1038/nn.3658
   Pyle R, 2016, PHYS REV E, V93, DOI 10.1103/PhysRevE.93.040302
   Renart A, 2010, SCIENCE, V327, P587, DOI 10.1126/science.1179850
   Ricard MR, 2009, J NONLINEAR SCI, V19, P467, DOI 10.1007/s00332-009-9041-6
   Richardson MJE, 2007, PHYS REV E, V76, DOI 10.1103/PhysRevE.76.021919
   Richardson MJE, 2009, PHYS REV E, V80, DOI 10.1103/PhysRevE.80.021928
   Rosenbaum R, 2016, FRONT COMPUT NEUROSC, V10, DOI 10.3389/fncom.2016.00039
   Rosenbaum R, 2014, PHYS REV X, V4, DOI 10.1103/PhysRevX.4.021039
   Roxin A, 2005, PHYS REV LETT, V94, DOI 10.1103/PhysRevLett.94.238103
   Sadeh S, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0127547
   Sadeh S, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0114237
   Shadlen Michael N., 1994, Current Opinion in Neurobiology, V4, P569, DOI 10.1016/0959-4388(94)90059-0
   Shenoy KV, 2013, ANNU REV NEUROSCI, V36, P337, DOI 10.1146/annurev-neuro-062111-150509
   SOFTKY WR, 1993, J NEUROSCI, V13, P334
   SOMPOLINSKY H, 1988, PHYS REV LETT, V61, P259, DOI 10.1103/PhysRevLett.61.259
   Sussillo D, 2009, NEURON, V63, P544, DOI 10.1016/j.neuron.2009.07.018
   van Vreeswijk C, 1998, NEURAL COMPUT, V10, P1321, DOI 10.1162/089976698300017214
   vanVreeswijk C, 1996, SCIENCE, V274, P1724, DOI 10.1126/science.274.5293.1724
   Vogels TP, 2011, SCIENCE, V334, P1569, DOI 10.1126/science.1211095
NR 42
TC 31
Z9 31
U1 1
U2 34
PD JAN 6
PY 2017
VL 118
IS 1
AR 018103
DI 10.1103/PhysRevLett.118.018103
UT WOS:000391474900016
DA 2023-11-16
ER

PT J
AU BERGSTRO.RM
AF BERGSTRO.RM
TI UNIT SPIKE ACTIVITY IN COELENTERAN NEURAL NETWORK
SO NATURWISSENSCHAFTEN
DT Note
CR HORRIDGE GA, 1954, J EXP BIOL, V31, P594
   Lentz TL., 1968, PRIMITIVE NERVOUS SY
   ROBSON EA, 1969, J EXP BIOL, V50, P151
NR 3
TC 3
Z9 3
U1 0
U2 1
PY 1971
VL 58
IS 3
BP 153
EP &
DI 10.1007/BF00593116
UT WOS:A1971I927400018
DA 2023-11-16
ER

PT J
AU Neftci, EO
   Mostafa, H
   Zenke, F
AF Neftci, Emre O.
   Mostafa, Hesham
   Zenke, Friedemann
TI Surrogate Gradient Learning in Spiking Neural Networks: Bringing the
   Power of Gradient-based optimization to spiking neural networks
SO IEEE SIGNAL PROCESSING MAGAZINE
DT Article
DE Neural networks; Fault tolerance; Energy efficiency; Biological system
   modeling
ID ALGORITHM
AB Spiking neural networks (SNNs) are nature's versatile solution to fault-tolerant, energy-efficient signal processing. To translate these benefits into hardware, a growing number of neuromorphic spiking NN processors have attempted to emulate biological NNs. These developments have created an imminent need for methods and tools that enable such systems to solve real-world signal processing problems. Like conventional NNs, SNNs can be trained on real, domain-specific data; however, their training requires the overcoming of a number of challenges linked to their binary and dynamical nature. This article elucidates step-by-step the problems typically encountered when training SNNs and guides the reader through the key concepts of synaptic plasticity and data-driven learning in the spiking setting. Accordingly, it gives an overview of existing approaches and provides an introduction to surrogate gradient (SG) methods, specifically, as a particularly flexible and efficient method to overcome the aforementioned challenges.
C1 [Neftci, Emre O.] Univ Calif Irvine, Dept Cognit Sci & Comp Sci, Irvine, CA 92697 USA.
   [Mostafa, Hesham] Intels Artificial Intelligence Prod Grp, Off CTO, Santa Clara, CA USA.
   [Zenke, Friedemann] Friedrich Miescher Inst Biomed Res, Basel, Switzerland.
RP Neftci, EO (corresponding author), Univ Calif Irvine, Dept Cognit Sci & Comp Sci, Irvine, CA 92697 USA.
EM eneftci@uci.edu; hesham.mostafa@intel.com; friedemann.zenke@fmi.ch
CR Abbott LF, 2016, NAT NEUROSCI, V19, P350, DOI 10.1038/nn.4241
   ACKLEY DH, 1985, COGNITIVE SCI, V9, P147
   [Anonymous], 2016, BINARIZED NEURAL NET
   [Anonymous], 2016, ADV NEURAL INFORM PR
   [Anonymous], 2016, DEEP LEARNING
   Anwani N., 2015, P INT JOINT C NEUR N, P1, DOI DOI 10.1109/IJCNN.2015
   Baldi P, 2016, NEURAL NETWORKS, V83, P51, DOI 10.1016/j.neunet.2016.07.006
   Bellec G., 2018, ADV NEURAL INFORM PR, P795
   Bellec G., 2019, BIOL INSPIRED ALTERN
   Bengio Yoshua, 2013, ABS13083432 CORR
   Boahen K, 2017, COMPUT SCI ENG, V19, P14, DOI 10.1109/MCSE.2017.33
   Bohte SM, 2011, LECT NOTES COMPUT SC, V6791, P60, DOI 10.1007/978-3-642-21735-7_8
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Brea J, 2013, J NEUROSCI, V33, P9565, DOI 10.1523/JNEUROSCI.4098-12.2013
   Esser SK, 2016, P NATL ACAD SCI USA, V113, P11441, DOI 10.1073/pnas.1604850113
   Gardner B, 2015, NEURAL COMPUT, V27, P2548, DOI 10.1162/NECO_a_00790
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   Gilra A, 2017, ELIFE, V6, DOI 10.7554/eLife.28295
   Guerguiev J, 2017, ELIFE, V6, DOI 10.7554/eLife.22901
   Gütig R, 2014, CURR OPIN NEUROBIOL, V25, P134, DOI 10.1016/j.conb.2014.01.004
   Huh Dongsung, 2018, ADV NEURAL INFORM PR, P1440
   Hunsberger E., 2015, SPIKING DEEP NETWORK
   Kaiser J., 2018, SYNAPTIC PLASTICITY
   Lee JM, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00191
   Lillicrap TP, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms13276
   Memmesheimer RM, 2014, NEURON, V82, P925, DOI 10.1016/j.neuron.2014.03.026
   Mostafa H, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00608
   Mostafa Hesham, 2018, IEEE Trans Neural Netw Learn Syst, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Mostafa H, 2018, NEURAL COMPUT, V30, P1542, DOI 10.1162/neco_a_01080
   Neftci EO, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00324
   Nicola W, 2017, NAT COMMUN, V8, DOI 10.1038/s41467-017-01827-3
   OConnor P., 2017, TEMPORALLY EFFICIENT
   Pfister JP, 2006, NEURAL COMPUT, V18, P1318, DOI 10.1162/neco.2006.18.6.1318
   Rezende DJ, 2014, FRONT COMPUT NEUROSC, V8, DOI 10.3389/fncom.2014.00038
   Shrestha S. B., 2018, ADV NEURAL INFORM PR
   Shrestha S. B., 2018, P 32 INT C NEUR INF, P1419, DOI DOI 10.5555/3326943.3327073
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Williams RJ, 1989, NEURAL COMPUT, V1, P270, DOI 10.1162/neco.1989.1.2.270
   Wozniak S., 2018, DEEP NETWORKS INCORP
   Zenke F, 2018, NEURAL COMPUT, V30, P1514, DOI 10.1162/neco_a_01086
NR 40
TC 377
Z9 382
U1 10
U2 65
PD NOV
PY 2019
VL 36
IS 6
BP 51
EP 63
DI 10.1109/MSP.2019.2931595
UT WOS:000497692800001
HC Y
HP N
DA 2023-11-16
ER

PT C
AU Zhang, Z
   Wu, QX
   Wang, X
   Sun, QY
AF Zhang, Zhenmin
   Wu, Qingxiang
   Wang, Xuan
   Sun, Qiyan
BE Xiao, Z
   Tong, Z
   Li, K
   Wang, X
   Li, K
TI Training Spiking Neural Networks With the Improved Grey-Level
   Co-occurrence Matrix Algorithm for Texture Analysis
SO 2015 11TH INTERNATIONAL CONFERENCE ON NATURAL COMPUTATION (ICNC)
DT Proceedings Paper
CT 11th International Conference on Natural Computation (ICNC) / 12th
   International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)
CY AUG 15-17, 2015
CL Zhangjiajie, PEOPLES R CHINA
DE spiking neural network; GLCM algorithm; Feature Extraction; Texture
   Classification
AB Texture refers to the tactile impression, such as rough, silky, bumpy, and other texture terms. The Grey-Level Co-occurrence Matrix (GLCM) algorithm is widely used in visual images for texture feature extraction, image structure characterization analysis and texture classification. The GLCM can not only give the statistics of pixel gray values occur in an image, but also give multiple characteristics of the images. Since the primate brain, which is constructed with spiking neurons, has excellent performance in terms of image feature extraction, the improved GLCM algorithm is used to train a spiking neural network and also to simulate the brain's ability about extract key information and utilize these extracted feature information to classify different texture image. Experimental results in this article show that this combination of the GLCM and spiking neural network can effectively extract image features, and the texture classification results is also to achieve satisfactory effect.
C1 [Zhang, Zhenmin; Wu, Qingxiang; Wang, Xuan; Sun, Qiyan] Fujian Normal Univ, Coll Photon & Elect Engn, Fuzhou, Peoples R China.
RP Wu, QX (corresponding author), Fujian Normal Univ, Coll Photon & Elect Engn, Fuzhou, Peoples R China.
CR [Anonymous], SYSTEMS MAN CYBERNET
   Beck MW, 2014, ECOL INDIC, V45, P195, DOI 10.1016/j.ecolind.2014.04.002
   Benco M., 2014, INT J ADV ROBOTIC SY, V11
   Benco M, 2007, RADIOENGINEERING, V16, P64
   Bergounioux M, 2014, ADV IMAG ELECT PHYS, V181, P35, DOI 10.1016/B978-0-12-800091-5.00002-1
   Brodatz P., 1966, TEXTURE PHOTOGRAPHIC
   Brownstone RM, 2015, NEURON, V86, P9, DOI 10.1016/j.neuron.2015.03.056
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Kosik KS, 2013, NATURE, V503, P31, DOI 10.1038/503031a
   Kuebler E.S., 2013, IEEE IJCNN
   Mallat B. S., 2007, MALLAT WAVELET TOUR, V31, P85
   Masland RH, 2001, NAT NEUROSCI, V4, P877, DOI 10.1038/nn0901-877
   Mirzapour F, 2013, IRAN CONF ELECTR ENG
   Nazemi A, 2015, NEUROCOMPUTING, V152, P369, DOI 10.1016/j.neucom.2014.10.054
   Ojala T., 2002, OUTEX NEW FRAMEWORK, V1, P706
   Sadtler PT, 2014, NATURE, V512, P423, DOI 10.1038/nature13665
   Saroja GAS, 2013, 2013 IEEE CONFERENCE ON INFORMATION AND COMMUNICATION TECHNOLOGIES (ICT 2013), P1319
   Schultz SK, 2001, AM J PSYCHIAT, V158, P662, DOI 10.1176/appi.ajp.158.4.662
   Tamura H., 1978, TEXTURAL FEATURES CO, V8, P473
   Van de Wouwer G, 1999, IEEE T IMAGE PROCESS, V8, P592, DOI 10.1109/83.753747
   Wu Q.X., 2006, KNOWLEDGE REPRESENTA, V4, P2801
   Wu QX, 2013, NEUROCOMPUTING, V116, P3, DOI 10.1016/j.neucom.2012.01.046
   Wu QX, 2009, LECT NOTES ARTIF INT, V5755, P21
   Xue MS, 2014, NATURE, V511, P596, DOI 10.1038/nature13321
   Zhang J, 2008, HPCC 2008: 10TH IEEE INTERNATIONAL CONFERENCE ON HIGH PERFORMANCE COMPUTING AND COMMUNICATIONS, PROCEEDINGS, P782, DOI 10.1109/HPCC.2008.55
NR 25
TC 0
Z9 0
U1 1
U2 1
PY 2015
BP 825
EP 830
UT WOS:000380617000143
DA 2023-11-16
ER

PT C
AU Hu, ZH
   Wang, T
   Hu, XL
AF Hu, Zhanhao
   Wang, Tao
   Hu, Xiaolin
BE Liu, D
   Xie, S
   Li, Y
   Zhao, D
   ElAlfy, ESM
TI An STDP-Based Supervised Learning Algorithm for Spiking Neural Networks
SO NEURAL INFORMATION PROCESSING (ICONIP 2017), PT II
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 24th International Conference on Neural Information Processing (ICONIP)
CY NOV 14-18, 2017
CL Guangzhou, PEOPLES R CHINA
DE STDP; SNN; Supervised learning
AB Compared with rate-based artificial neural networks, Spiking Neural Networks (SNN) provide a more biological plausible model for the brain. But how they perform supervised learning remains elusive. Inspired by recent works of Bengio et al., we propose a supervised learning algorithm based on Spike-Timing Dependent Plasticity (STDP) for a hierarchical SNN consisting of Leaky Integrate-and-fire (LIF) neurons. A time window is designed for the presynaptic neuron and only the spikes in this window take part in the STDP updating process. The model is trained on the MNIST dataset. The classification accuracy approach that of a Multilayer Perceptron (MLP) with similar architecture trained by the standard back-propagation algorithm.
C1 [Hu, Zhanhao] Tsinghua Univ, Dept Phys, Beijing, Peoples R China.
   [Wang, Tao] Huawei Technol, Beijing, Peoples R China.
   [Hu, Xiaolin] Tsinghua Univ, Ctr Brain Inspired Comp Res CBICR, Tsinghua Natl Lab Informat Sci & Technol TNList, Dept Comp Sci & Technol, Beijing, Peoples R China.
RP Hu, XL (corresponding author), Tsinghua Univ, Ctr Brain Inspired Comp Res CBICR, Tsinghua Natl Lab Informat Sci & Technol TNList, Dept Comp Sci & Technol, Beijing, Peoples R China.
EM xlhu@tsinghua.edu.cn
CR [Anonymous], 2016, ARXIV161101421
   [Anonymous], 2015, ARXIV150905936
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Booij O, 2005, INFORM PROCESS LETT, V95, P552, DOI 10.1016/j.ipl.2005.05.023
   Clopath C, 2010, NAT NEUROSCI, V13, P344, DOI 10.1038/nn.2479
   Dayan P., 2001, THEORETICAL NEUROSCI
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Querlioz D, 2013, IEEE T NANOTECHNOL, V12, P288, DOI 10.1109/TNANO.2013.2250995
   Scellier B, 2017, FRONT COMPUT NEUROSC, V11, DOI 10.3389/fncom.2017.00024
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Xie XH, 2000, ADV NEUR IN, V12, P199
   Xie XR, 2017, IEEE T NEUR NET LEAR, V28, P1411, DOI 10.1109/TNNLS.2016.2541339
NR 17
TC 5
Z9 5
U1 1
U2 3
PY 2017
VL 10635
BP 92
EP 100
DI 10.1007/978-3-319-70096-0_10
PN II
UT WOS:000576766300010
DA 2023-11-16
ER

PT C
AU Valova, I
   Gueorguieva, N
   Georgiev, G
AF Valova, I
   Gueorguieva, N
   Georgiev, G
GP IEEE
TI Modeling weakly connected networks of neural oscillators with spiking
   neurons
SO INTERNATIONAL CONFERENCE ON SYSTEMS, MAN AND CYBERNETICS, VOL 1-4,
   PROCEEDINGS
SE IEEE International Conference on Systems Man and Cybernetics Conference
   Proceedings
DT Proceedings Paper
CT IEEE International Conference on Systems, Man and Cybernetics
CY OCT 10-12, 2005
CL Waikoloa, HI
DE spiking neurons; neural oscillations; biologically inspired neuron
   models
ID OLFACTORY-BULB; DENDRITES
AB The goal of this research is to investigate the relationships between synaptic organizations (anatomy) of the neural networks and the dynamical properties (function) of weakly connected networks of neural oscillators. It is shown how certain parameters of the spiking neuron model can be used to represent these dynamics. The two proposed modesl are based on the two main cell types in the olfactory bulb, the mitral and granule cells. The dynamics that have been simulated include the reciprocal and lateral inhibition of mitral cells by granule cells, as well as the saturation of mitral cells. The simulations show how certain spike inputs to mitral cells correspond to cortex recognition and discrimination in the olfactory bulb.
C1 Univ Massachusetts Dartmouth, N Dartmouth, MA 02747 USA.
RP Valova, I (corresponding author), Univ Massachusetts Dartmouth, 285 Old Westport Rd, N Dartmouth, MA 02747 USA.
EM ivalova@umassd.edu; natachag@csi.cuny.edu; georgiev@uwosh.edu
CR Abbott LF, 2000, NAT NEUROSCI, V3, P1178, DOI 10.1038/81453
   Christodoulou C, 2002, NEURAL NETWORKS, V15, P891, DOI 10.1016/S0893-6080(02)00034-5
   Dayan P., 2001, THEORETICAL NEUROSCI
   GERSTNER W, POPULATION DYNAMICS, V12, P43
   Gerstner W., 2002, SPIKING NEURON MODEL
   Horn D, 2000, ADV NEUR IN, V12, P129
   Kay LM, 1999, NAT NEUROSCI, V2, P1003, DOI 10.1038/14801
   Kempter R, 1999, PHYS REV E, V59, P4498, DOI 10.1103/PhysRevE.59.4498
   Margrie TW, 2001, P NATL ACAD SCI USA, V98, P319, DOI 10.1073/pnas.011523098
   MORI K, 1987, PROG NEUROBIOL, V29, P275, DOI 10.1016/0301-0082(87)90024-4
   Rieke F., 1999, SPIKES EXPLORING NEU
   RUF B, 1998, THESIS TECHNICAL U G
   Schoppa NE, 1999, NAT NEUROSCI, V2, P1106, DOI 10.1038/16033
   Shen GYY, 1999, J NEUROPHYSIOL, V82, P3006, DOI 10.1152/jn.1999.82.6.3006
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   VALOVA I, 2004, P ART NEUR NETW ENG, P3
NR 16
TC 0
Z9 0
U1 0
U2 1
PY 2005
BP 810
EP 815
UT WOS:000235210800134
DA 2023-11-16
ER

PT C
AU Chaturvedi, S
   Kurshid, AA
AF Chaturvedi, S.
   Kurshid, A. A.
BE Bahrami, M
TI ASIC Implementation for Improved Character Recognition and
   Classification using SNN Model
SO PROCEEDINGS OF THE 2015 INTERNATIONAL CONFERENCE ON SOFT COMPUTING AND
   SOFTWARE ENGINEERING (SCSE'15)
SE Procedia Computer Science
DT Proceedings Paper
CT International Conference on Soft Computing and Software Engineering
   (SCSE)
CY MAR 05-06, 2015
CL Univ California, Berkeley, CA
HO Univ California
DE Artificial Neural Network (ANN); Spiking Neural Network (SNN);
   Leaky-Integrate & Fire Neuron Model; Izhikev ch Model; Pattern
   Classification& Recognition; Application Specific Integrated Circuits
   (ASIC)
AB The third generation of spiking neural networks raises the level of biological realism by using individual spikes. So instead of using rate coding, these neurons use pulse coding mechanisms where neurons receive and do send out individual pulses, allowing multiplexing of information.This work depicts how Spiking neural network model is used for character recognition and classification. Here, we adapt to the technique of using ASIC for large scale simulations of the Izhikevich model and use RTL Clock gating approach for reducing the dynamic power. Here the focus is on how power consumption and system cost can be reduced for large production run. The full custom biologically plausible spiking neural network model is implemented on ASIC with 90 nm Process. The Izhikevich spiking neuron model is best suited for large scale cortical simulations due to its accuracy, efficiency, power and simulation time. The classification efficiency of SNN based on MATLAB simulations is demonstrated in this work by its ability to classify the 27 characters correctly out of 30 noisy character images presented. The ASIC realizing the English character classification and recognition dissipates power of 2.8 mW and an area of 120312 lam'. This work brings about the application of using networks of these spiking neurons for character recognition and their suitability for custom realization with reduced power consumption. (C) 2015 The Authors. Published by Elsevier B.V.
C1 [Chaturvedi, S.] GHRCE, Elect, Nagpur, MS, India.
   [Kurshid, A. A.] RCOEM, Elect, Nagpur, MS, India.
RP Chaturvedi, S (corresponding author), GHRCE, Elect, Nagpur, MS, India.
EM soni2569@gmail.com
CR Chaturvedi Soni, 2014, CIIT INT J DIGITAL I, V06
   Chaturvedi Soni, 2014 INT C WIR COMM
   Chaturvedi Soni, 2013, INT J EMERGING TREND, V1
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Gupta A., 2007, IEEE NEUR NETW C ORL
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Kulkarni Shruti R, 2013, 9 INT C NAT COMP ICN
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Meftah B., 2012, SCI, V427, P525
   Sharma Abhishek, 2013, INT J ENG TRENDS TEC, V4
   Vazquez Roberto A., 2010, 7 INT C EL ENG COMP
NR 11
TC 3
Z9 3
U1 0
U2 1
PY 2015
VL 62
BP 151
EP 158
DI 10.1016/j.procs.2015.08.428
UT WOS:000364544900025
DA 2023-11-16
ER

PT C
AU Nguyen, VT
   Trinh, QK
   Zhang, RY
   Nakashima, Y
AF Nguyen, Van-Tinh
   Quang-Kien Trinh
   Zhang, Renyuan
   Nakashima, Yasuhiko
GP IEEE
TI XNOR-BSNN: In-Memory Computing Model for Deep Binarized Spiking Neural
   Network
SO 2021 INTERNATIONAL CONFERENCE ON HIGH PERFORMANCE BIG DATA AND
   INTELLIGENT SYSTEMS (HPBD&IS)
DT Proceedings Paper
CT International Conference on High Performance Big Data and Intelligent
   Systems (HPBD and IS)
CY DEC 05-07, 2021
CL Macau, PEOPLES R CHINA
DE In-memory computing; Binary Spiking Neural Network; residual connection
AB This paper proposes a residual binarized spiking neural network (B-SNN) model suited for inmemory computing (IMC) implementation. While in most of the prior arts, due to the nature of spike represented unipolar format, the B-SNN were implemented using either complex or non-regular logic that is not suited for IMC and/or makes the network inflexible. In this work, we present a B-SNN model that permits the direct adoption of a unipolar format spike on the XNOR array, i.e., allows fully exploiting IMC's potential benefit based on the highly regular and simple array structure. Also, instead of indirectly taking the B-SNN model from the trained BNN, we propose a residual model for deep B-SNN networks. The system simulation shows that our trained network achieves reasonably good accuracy (59.11%) on CIFAR100 with very low inference latency (only 8 time-steps BSNN).
C1 [Nguyen, Van-Tinh; Zhang, Renyuan; Nakashima, Yasuhiko] NARA Inst Sci & Technol, Sch Informat Sci, Ikoma, Japan.
   [Quang-Kien Trinh] Le Quy Don Tech Univ, Dept Microelect & Microproc, Hanoi, Vietnam.
RP Nguyen, VT (corresponding author), NARA Inst Sci & Technol, Sch Informat Sci, Ikoma, Japan.
EM nguyen.van_tinh.np3@is.naist.jp; kien.trinh@lqdtu.edu.vn;
   rzhang@is.naist.jp; nakashim@is.naist.jp
CR Abu Lebdeh M, 2017, IEEE T CIRCUITS-I, V64, P2427, DOI 10.1109/TCSI.2017.2706299
   Chuang PY, 2020, DES AUT CON, DOI 10.1109/dac18072.2020.9218714
   Deng L., 2019, ARXIV PREPRINT ARXIV
   Esser SK, 2016, P NATL ACAD SCI USA, V113, P11441, DOI 10.1073/pnas.1604850113
   Guo WZ, 2021, FRONT NEUROSCI-SWITZ, V15, DOI [10.3389/fnins.2021.638474, 10.1007/s11704-020-9230-x]
   Hubara I, 2018, J MACH LEARN RES, V18
   Kheradpisheh S., 2020, ARXIV PREPRINT ARXIV
   Kim Y, 2020, ARXIV PREPRINT ARXIV
   Krizhevsky A, 2009, HDB SYST AUTOIMMUNE, V1, P1
   Lu S, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00535
   Pham T., 2021, 2021 IEEE INT S CIRC
   Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32
   Sebastian A, 2020, NAT NANOTECHNOL, V15, P529, DOI 10.1038/s41565-020-0655-z
   She XY, 2019, IEEE IJCNN
   Srinivasan G, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00189
   Tinh N. V., 2021, IEEE ACCESS, V9
   Wang YX, 2021, IEEE T COGN DEV SYST, V13, P514, DOI 10.1109/TCDS.2020.2971655
   Wu YJ, 2019, AAAI CONF ARTIF INTE, P1311
NR 18
TC 0
Z9 0
U1 3
U2 4
PY 2021
BP 17
EP 21
DI 10.1109/HPBDIS53214.2021.9658467
UT WOS:000814979500004
DA 2023-11-16
ER

PT J
AU Guo, YF
   Huang, XH
   Ma, Z
AF Guo, Yufei
   Huang, Xuhui
   Ma, Zhe
TI Direct learning-based deep spiking neural networks: a review
SO FRONTIERS IN NEUROSCIENCE
DT Review
DE spiking neural network; brain-inspired computation; direct learning;
   deep neural network; energy efficiency; spatial-temporal processing
ID GRADIENT DESCENT; ALGORITHM
AB The spiking neural network (SNN), as a promising brain-inspired computational model with binary spike information transmission mechanism, rich spatially-temporal dynamics, and event-driven characteristics, has received extensive attention. However, its intricately discontinuous spike mechanism brings difficulty to the optimization of the deep SNN. Since the surrogate gradient method can greatly mitigate the optimization difficulty and shows great potential in directly training deep SNNs, a variety of direct learning-based deep SNN works have been proposed and achieved satisfying progress in recent years. In this paper, we present a comprehensive survey of these direct learning-based deep SNN works, mainly categorized into accuracy improvement methods, efficiency improvement methods, and temporal dynamics utilization methods. In addition, we also divide these categorizations into finer granularities further to better organize and introduce them. Finally, the challenges and trends that may be faced in future research are prospected.
C1 [Guo, Yufei; Huang, Xuhui; Ma, Zhe] Intelligent Sci & Technol Acad CASIC, Beijing, Peoples R China.
   [Guo, Yufei; Huang, Xuhui; Ma, Zhe] Sci Res Lab Aerosp Intelligent Syst & Technol, Beijing, Peoples R China.
RP Ma, Z (corresponding author), Intelligent Sci & Technol Acad CASIC, Beijing, Peoples R China.; Ma, Z (corresponding author), Sci Res Lab Aerosp Intelligent Syst & Technol, Beijing, Peoples R China.
EM mazhe_thu@163.com
CR Barchid S, 2023, Arxiv, DOI arXiv:2304.10211
   Bellec G, 2018, ADV NEUR IN, V31
   Bing Han, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P388, DOI 10.1007/978-3-030-58607-2_23
   Biswas S, 2022, Arxiv, DOI arXiv:2211.10754
   Bittar A, 2022, Arxiv, DOI arXiv:2212.01187
   Bittar A, 2022, FRONT NEUROSCI-SWITZ, V16, DOI 10.3389/fnins.2022.865897
   Bohte SM, 2011, LECT NOTES COMPUT SC, V6791, P60, DOI 10.1007/978-3-642-21735-7_8
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Booij O, 2005, INFORM PROCESS LETT, V95, P552, DOI 10.1016/j.ipl.2005.05.023
   Bu T, 2023, Arxiv, DOI arXiv:2303.04347
   Bu T, 2022, AAAI CONF ARTIF INTE, P11
   Chen Y., 2021, ARXIV PREPRINT ARXIV
   Chen Y, 2022, INT CONF ACOUST SPEE, P8927, DOI 10.1109/ICASSP43922.2022.9746774
   Cheng X, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1519
   Chowdhury SS, 2021, IEEE IJCNN, DOI 10.1109/IJCNN52387.2021.9534111
   Chowdhury SS, 2022, LECT NOTES COMPUT SC, V13671, P709, DOI 10.1007/978-3-031-20083-0_42
   Cordone L., 2022, ARXIV
   Deng S., 2022, ARXIV
   Ding Jianchuan, 2022, ADV NEURAL INFORM PR
   Duan C., 2022, ADV NEURAL INFORM PR
   Dupeyroux J, 2021, IEEE INT CONF ROBOT, P96, DOI 10.1109/ICRA48506.2021.9560937
   Fang W., 2021, ADV NEURAL INFORM PR, V34, P21056
   Fang W, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2641, DOI 10.1109/ICCV48922.2021.00266
   Feng L, 2022, P 31 INT JOINT C ART, P2471, DOI DOI 10.24963/IJCAI
   Gao Y, 2023, IEEE T PATTERN ANAL, V45, P7764, DOI 10.1109/TPAMI.2022.3224051
   Guo Y., 2022, ADV NEURAL INFORM PR
   Guo YF, 2023, PATTERN RECOGN, V142, DOI 10.1016/j.patcog.2023.109639
   Guo YF, 2022, LECT NOTES COMPUT SC, V13671, P36, DOI 10.1007/978-3-031-20083-0_3
   Guo Y, 2022, LECT NOTES COMPUT SC, V13672, P52, DOI 10.1007/978-3-031-19775-8_4
   Guo YF, 2022, PROC CVPR IEEE, P326, DOI 10.1109/CVPR52688.2022.00042
   Hagenaars J., 2021, ADV NEURAL INF PROCE, V34, P7167
   Han B, 2023, Arxiv, DOI arXiv:2211.12219
   Han CS, 2022, 37TH ANNUAL ACM SYMPOSIUM ON APPLIED COMPUTING, P1048, DOI 10.1145/3477314.3507085
   Hao YZ, 2020, NEURAL NETWORKS, V121, P387, DOI 10.1016/j.neunet.2019.09.007
   Hong CF, 2020, IEEE T NEUR NET LEAR, V31, P1285, DOI 10.1109/TNNLS.2019.2919662
   Hu YF, 2023, Arxiv, DOI arXiv:2112.08954
   Ikegawa S, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22082876
   Kim S, 2020, AAAI CONF ARTIF INTE, V34, P11270
   Kim Y, 2022, LECT NOTES COMPUT SC, V13684, P36, DOI 10.1007/978-3-031-20053-3_3
   Kim Y, 2022, LECT NOTES COMPUT SC, V13672, P102, DOI 10.1007/978-3-031-19775-8_7
   Kim Y, 2021, FRONT NEUROSCI-SWITZ, V15, DOI 10.3389/fnins.2021.773954
   Kosta AK, 2022, Arxiv, DOI arXiv:2209.11741
   Kushawaha RK, 2021, INT C PATT RECOG, P4536, DOI 10.1109/ICPR48806.2021.9412147
   Leng L., 2022, ADV NEURAL INFORM PR
   Li Wenshuo, 2022, P IEEECVF C COMPUTER, P783, DOI [10.1109/CVPR52688.2022.00086, DOI 10.1109/CVPR52688.2022.00086]
   Li Yan, 2021, arXiv
   Li Y., 2022, ARXIV
   Li Y., 2022, ARXIV PREPRINT ARXIV
   Li Y., 2021, INT C MACHINE LEARNI, V139, P6316
   Liu FX, 2022, AAAI CONF ARTIF INTE, P1692
   Lobov SA, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00088
   Luo XL, 2022, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2022.3164930
   Luo YH, 2021, Arxiv, DOI arXiv:2003.07584
   Meng Q., 2022, P IEEE CVF C COMP VI, P12444, DOI [10.1109/CVPR52688.2022.01212, DOI 10.1109/CVPR52688.2022.01212]
   Na BYG, 2022, Arxiv, DOI arXiv:2201.12738
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Nomura O, 2022, IEEE T CIRCUITS-II, V69, P3640, DOI 10.1109/TCSII.2022.3184313
   Parameshwara CM, 2021, IEEE INT C INT ROBOT, P3414, DOI 10.1109/IROS51168.2021.9636506
   Patel K, 2021, Arxiv, DOI arXiv:2106.08921
   Pellegrini T, 2021, IEEE W SP LANG TECH, P97, DOI 10.1109/SLT48900.2021.9383587
   Ponghiran W, 2022, AAAI CONF ARTIF INTE, P8001
   Ponulak F, 2011, ACTA NEUROBIOL EXP, V71, P409
   Rancon U, 2021, Arxiv, DOI arXiv:2109.13751
   Rathi N, 2020, Arxiv, DOI arXiv:2008.03658
   Roy K, 2019, NATURE, V575, P607, DOI 10.1038/s41586-019-1677-2
   Sadovsky E, 2023, 2023 33RD INTERNATIONAL CONFERENCE RADIOELEKTRONIKA, RADIOELEKTRONIKA, DOI 10.1109/RADIOELEKTRONIKA57919.2023.10109082
   She Xueyuan, 2021, INT C LEARN REPR
   Shen G., 2023, PREPRINT, DOI [10.48550/arXiv.2301.12356, DOI 10.48550/ARXIV.2301.12356]
   Stagsted RK, 2020, ROBOTICS: SCIENCE AND SYSTEMS XVI
   Takuya S, 2021, PROC IEEE COOL CHIPS, DOI 10.1109/COOLCHIPS52128.2021.9410323
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Tavanaei A, 2017, LECT NOTES COMPUT SC, V10639, P899, DOI 10.1007/978-3-319-70136-3_95
   Tavanaei A, 2017, NEUROCOMPUTING, V240, P191, DOI 10.1016/j.neucom.2017.01.088
   Viale A, 2022, IEEE INT C INT ROBOT, P79, DOI 10.1109/IROS47612.2022.9981034
   Wang Siwei, 2022, ADV NEURAL INFORM PR
   Wang XW, 2020, NEURAL NETWORKS, V125, P258, DOI 10.1016/j.neunet.2020.02.011
   Wang XT, 2023, Arxiv, DOI arXiv:2303.11127
   Wang Y., 2022, INT JOINT C ART INT, DOI [10.24963/ijcai.2022/347, DOI 10.24963/IJCAI.2022/347]
   Wu J, 2019, IEEE
   Wu JC, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351221
   Wu JB, 2022, IEEE T PATTERN ANAL, V44, P7824, DOI 10.1109/TPAMI.2021.3114196
   Wu JB, 2023, IEEE T NEUR NET LEAR, V34, P446, DOI 10.1109/TNNLS.2021.3095724
   Wu JB, 2019, INTERSPEECH, P3667
   Wu JB, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00199
   Wu JB, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00836
   Wu YJ, 2019, AAAI CONF ARTIF INTE, P1311
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Xu Q, 2023, Arxiv, DOI arXiv:2304.09500
   Xu Q, 2023, Arxiv, DOI arXiv:2304.05627
   Xu Y, 2013, NEURAL NETWORKS, V43, P99, DOI 10.1016/j.neunet.2013.02.003
   Yamazaki K, 2022, BRAIN SCI, V12, DOI 10.3390/brainsci12070863
   Yang Q., 2022, ARXIV PREPRINT ARXIV
   Yao M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10201, DOI 10.1109/ICCV48922.2021.01006
   Yao X., 2022, 36 C NEUR INF PROC S
   Yin B, 2020, IEEE INTERNET THINGS, V7, P8748, DOI [10.1109/JIOT.2020.2996562, 10.1145/3407197.3407225]
   Yin BJ, 2021, NAT MACH INTELL, V3, P905, DOI 10.1038/s42256-021-00397-w
   Yu L, 2023, IEEE T PATTERN ANAL, V45, P8660, DOI 10.1109/TPAMI.2022.3227448
   Yu Q, 2022, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2022.3165527
   Yu Q, 2022, IEEE T NEUR NET LEAR, V33, P1134, DOI 10.1109/TNNLS.2020.3040969
   Zambrano D, 2016, Arxiv, DOI arXiv:1609.02053
   Zenke F, 2018, NEURAL COMPUT, V30, P1514, DOI 10.1162/neco_a_01086
   Zhang DZ, 2022, Arxiv, DOI arXiv:2204.07050
   Zhang JQ, 2022, PROC CVPR IEEE, P8791, DOI 10.1109/CVPR52688.2022.00860
   Zhang ML, 2022, IEEE T NEUR NET LEAR, V33, P1947, DOI 10.1109/TNNLS.2021.3110991
   Zhang ML, 2020, NEUROCOMPUTING, V409, P103, DOI 10.1016/j.neucom.2020.03.079
   Zhang ML, 2020, IEEE J-STSP, V14, P592, DOI 10.1109/JSTSP.2020.2983547
   Zhang ML, 2019, AAAI CONF ARTIF INTE, P1327
   Zhang W, 2020, ADV NEURAL INFORM PR, V33, P12022, DOI DOI 10.48550/ARXIV.2002.10085
   Zheng H., 2021, P AAAI, DOI [10.1609/aaai.v35i12.17320, DOI 10.1609/AAAI.V35I12.17320]
   Zhou S., 2021, P AAAI C ART INT, DOI [10.1609/aaai.v35i12.17329, DOI 10.1609/AAAI.V35I12.17329]
   Zhou SB, 2020, IEEE ACCESS, V8, P76903, DOI 10.1109/ACCESS.2020.2990416
   Zhu L, 2022, PROC CVPR IEEE, P3584, DOI 10.1109/CVPR52688.2022.00358
   Zhu R.J., 2022, ARXIV
   Zhu Yaoyu, 2022, 36 C NEURAL INFORM P
   Zimmer R, 2019, Arxiv, DOI arXiv:1911.10124
   Zou SH, 2023, Arxiv, DOI arXiv:2303.09681
NR 116
TC 1
Z9 1
U1 20
U2 20
PD JUN 16
PY 2023
VL 17
AR 1209795
DI 10.3389/fnins.2023.1209795
UT WOS:001016587700001
DA 2023-11-16
ER

PT J
AU Büchel, J
   Lenz, G
   Hu, YL
   Sheik, S
   Sorbaro, M
AF Buechel, Julian
   Lenz, Gregor
   Hu, Yalun
   Sheik, Sadique
   Sorbaro, Martino
TI Adversarial attacks on spiking convolutional neural networks for
   event-based vision
SO FRONTIERS IN NEUROSCIENCE
DT Article
DE spiking convolutional neural networks; adversarial examples;
   neuromorphic engineering; robust AI; dynamic vision sensors
ID MEMORY
AB Event-based dynamic vision sensors provide very sparse output in the form of spikes, which makes them suitable for low-power applications. Convolutional spiking neural networks model such event-based data and develop their full energy-saving potential when deployed on asynchronous neuromorphic hardware. Event-based vision being a nascent field, the sensitivity of spiking neural networks to potentially malicious adversarial attacks has received little attention so far. We show how white-box adversarial attack algorithms can be adapted to the discrete and sparse nature of event-based visual data, and demonstrate smaller perturbation magnitudes at higher success rates than the current state-of-the-art algorithms. For the first time, we also verify the effectiveness of these perturbations directly on neuromorphic hardware. Finally, we discuss the properties of the resulting perturbations, the effect of adversarial training as a defense strategy, and future directions.
C1 [Buechel, Julian] IBM Res, Zurich, Switzerland.
   [Lenz, Gregor; Sheik, Sadique] SynSense, Zurich, Switzerland.
   [Hu, Yalun] SynSense, Chengdu, Peoples R China.
   [Sorbaro, Martino] Swiss Fed Inst Technol, AI Ctr, Zurich, Switzerland.
   [Sorbaro, Martino] Univ Zurich, Inst Neuroinformat, ETH, Zurich, Switzerland.
RP Büchel, J (corresponding author), IBM Res, Zurich, Switzerland.; Sorbaro, M (corresponding author), Swiss Fed Inst Technol, AI Ctr, Zurich, Switzerland.; Sorbaro, M (corresponding author), Univ Zurich, Inst Neuroinformat, ETH, Zurich, Switzerland.
EM msorbaro@ethz.ch
CR Akhtar N, 2018, IEEE ACCESS, V6, P14410, DOI 10.1109/ACCESS.2018.2807385
   Amir A, 2017, PROC CVPR IEEE, P7388, DOI 10.1109/CVPR.2017.781
   Bagheri A., 2018, 2018 IEEE 19 INT WOR
   Balkanski E, 2020, Arxiv, DOI arXiv:2010.11782
   Bengio Y, 2013, Arxiv, DOI arXiv:1308.3432
   Biggio B, 2018, PATTERN RECOGN, V84, P317, DOI 10.1016/j.patcog.2018.07.023
   Brown TB, 2018, Arxiv, DOI arXiv:1712.09665
   Cherupally SK, 2022, SEMICOND SCI TECH, V37, DOI 10.1088/1361-6641/ac461f
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Esser SK, 2016, P NATL ACAD SCI USA, V113, P11441, DOI 10.1073/pnas.1604850113
   Eykholt K, 2018, PROC CVPR IEEE, P1625, DOI 10.1109/CVPR.2018.00175
   Furber SB, 2013, IEEE T COMPUT, V62, P2454, DOI 10.1109/TC.2012.142
   Giraud C, 2004, INT FED INFO PROC, V153, P159
   Khaddam-Aljameh R., 2021, 2021 Symposium on VLSI Technology
   Kim Y, 2014, CONF PROC INT SYMP C, P361, DOI 10.1109/ISCA.2014.6853210
   Liang L, 2023, IEEE T NEUR NET LEAR, V34, P2569, DOI 10.1109/TNNLS.2021.3106961
   Liu Q., 2019, P IEEECVF C COMPUTER
   Marcovecchio A, 2022, INT J CLIMATOL, V42, P81, DOI 10.1002/joc.7233
   Modas A, 2019, PROC CVPR IEEE, P9079, DOI 10.1109/CVPR.2019.00930
   Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282
   Moradi S, 2018, IEEE T BIOMED CIRC S, V12, P106, DOI 10.1109/TBCAS.2017.2759700
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Orchard G, 2015, FRONT NEUROSCI-SWITZ, V9, DOI [10.3389/fhins.2015.00437, 10.3389/fnins.2015.00437]
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Sharmin Saima, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12374), P399, DOI 10.1007/978-3-030-58526-6_24
   Sorbaro M, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00662
   Stutz D, 2021, Arxiv, DOI arXiv:2006.13977
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Zhang HY, 2019, PR MACH LEARN RES, V97
NR 29
TC 1
Z9 1
U1 6
U2 8
PD DEC 22
PY 2022
VL 16
AR 1068193
DI 10.3389/fnins.2022.1068193
UT WOS:000909696500001
DA 2023-11-16
ER

PT J
AU Kino, H
   Fukushima, T
   Tanaka, T
AF Kino, Hisashi
   Fukushima, Takafumi
   Tanaka, Tetsu
TI Generation of STDP With Non-Volatile Tunnel-FET Memory for Large-Scale
   and Low-Power Spiking Neural Networks
SO IEEE JOURNAL OF THE ELECTRON DEVICES SOCIETY
DT Article
DE Nonvolatile memory; Logic gates; Synapses; Field effect transistors;
   Programming; MONOS devices; Delays; Spiking neural network; tunnel FET;
   MONOS; spike-timing-dependent plasticity; synaptic device
ID TIMING-DEPENDENT PLASTICITY; FLOATING-GATE
AB Spiking neural networks (SNNs) have attracted considerable attention as next-generation neural networks. As SNNs consist of devices that have spike-timing-dependent plasticity (STDP) characteristics, STDP is one of the critical characteristics we need to consider to implement an SNN. In this study, we generated the STDP of a biological synapse with non-volatile tunnel-field-effect-transistor (tunnel FET) memory that has a charge-storage layer and a tunnel FET structure. Tunnel FET is a promising structure to reduce the operation voltage owing to its steep sub-threshold slope. Therefore, the nonvolatile tunnel-FET memory we propose enables the implementation of low-operation-voltage SNNs. This article reports the I - V, programming, and both symmetric and asymmetric STDP characteristics of a non-volatile tunnel-FET memory with p-channel-MOS-like operation.
C1 [Kino, Hisashi] Tohoku Univ, Frontier Res Inst Interdisciplinary Sci, Sendai, Miyagi 9808579, Japan.
   [Fukushima, Takafumi; Tanaka, Tetsu] Tohoku Univ, Grad Sch Engn, Dept Mech Syst Engn, Sendai, Miyagi 9808579, Japan.
   [Tanaka, Tetsu] Tohoku Univ, Grad Sch Biomed Engn, Dept Biomed Engn, Sendai, Miyagi 9808579, Japan.
RP Kino, H (corresponding author), Tohoku Univ, Frontier Res Inst Interdisciplinary Sci, Sendai, Miyagi 9808579, Japan.
EM kino@lbc.mech.tohoku.ac.jp
CR Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Du Y, 2019, IEEE T COMPUT AID D, V38, P1811, DOI 10.1109/TCAD.2018.2859237
   Gopalakrishnan R, 2014, IEEE IJCNN, P4296, DOI 10.1109/IJCNN.2014.6889631
   Izhikevich EM, 2008, P NATL ACAD SCI USA, V105, P3593, DOI 10.1073/pnas.0712231105
   Jo SH, 2010, NANO LETT, V10, P1297, DOI 10.1021/nl904092h
   Kang DH, 2015, NEUROCOMPUTING, V155, P153, DOI 10.1016/j.neucom.2014.12.036
   Kato K, 2020, APPL PHYS EXPRESS, V13, DOI 10.35848/1882-0786/ab9875
   Kim CH, 2018, IEEE T ELECTRON DEV, V65, P1774, DOI 10.1109/TED.2018.2817266
   Kim H, 2016, IEEE ELECTR DEVICE L, V37, P249, DOI 10.1109/LED.2016.2521863
   Kim M, 2015, IEEE T ELECTRON DEV, V62, P9, DOI 10.1109/TED.2014.2371038
   Kino H., 2017, INT C SOL STAT DEV M, P791
   Kino H., 2019, INT C SOLID STATE DE, P673
   Kino H, 2020, 2020 IEEE ELECTRON DEVICES TECHNOLOGY AND MANUFACTURING CONFERENCE (EDTM 2020), DOI 10.1109/edtm47692.2020.9118027
   Kino H, 2020, JPN J APPL PHYS, V59, DOI 10.35848/1347-4065/ab6867
   Kino H, 2019, IEEE J ELECTRON DEVI, V7, P1225, DOI 10.1109/JEDS.2019.2936180
   Kino H, 2018, JPN J APPL PHYS, V57, DOI 10.7567/JJAP.57.04FE07
   Kurenkov A, 2019, ADV MATER, V31, DOI 10.1002/adma.201900636
   Matsuura K, 2018, IEEE J ELECTRON DEVI, V6, P1246, DOI 10.1109/JEDS.2018.2883133
   Ohno T, 2011, NAT MATER, V10, P591, DOI [10.1038/NMAT3054, 10.1038/nmat3054]
   Pankaala Mikko, 2009, Proceedings 2009 International Joint Conference on Neural Networks (IJCNN 2009 - Atlanta), P2409, DOI 10.1109/IJCNN.2009.5178879
   Park J, 2017, IEEE T ELECTRON DEV, V64, P2438, DOI 10.1109/TED.2017.2685519
   Pei YL, 2011, IEEE T NANOTECHNOL, V10, P528, DOI 10.1109/TNANO.2010.2050331
   Sangkil Kim, 2015, 2015 IEEE MTT-S International Microwave Symposium (IMS2015), P1, DOI 10.1109/MWSYM.2015.7166723
   She XY, 2019, IEEE IJCNN
   Smith AW, 2014, NEUROCOMPUTING, V124, P210, DOI 10.1016/j.neucom.2013.07.007
   Tan HW, 2019, ADV INTELL SYST-GER, V1, DOI 10.1002/aisy.201900036
   Wang W, 2019, FARADAY DISCUSS, V213, P453, DOI 10.1039/c8fd00097b
NR 27
TC 2
Z9 2
U1 4
U2 14
PY 2020
VL 8
BP 1266
EP 1271
DI 10.1109/JEDS.2020.3025336
UT WOS:000587913200024
DA 2023-11-16
ER

PT J
AU Pang, LL
   Liu, JX
   Harkin, J
   Martin, G
   McElholm, M
   Javed, A
   McDaid, L
AF Pang, Lili
   Liu, Junxiu
   Harkin, Jim
   Martin, George
   McElholm, Malachy
   Javed, Aqib
   McDaid, Liam
TI Case Study-Spiking Neural Network Hardware System for Structural Health
   Monitoring
SO SENSORS
DT Article
DE structural health monitoring; damage state classification; spiking
   neural networks; feature extraction; artificial neural networks
ID MACHINE; CLASSIFICATION; CAPACITY; MODEL
AB This case study provides feasibility analysis of adapting Spiking Neural Networks (SNN) based Structural Health Monitoring (SHM) system to explore low-cost solution for inspection of structural health of damaged buildings which survived after natural disaster that is, earthquakes or similar activities. Various techniques are used to detect the structural health status of a building for performance benchmarking, including different feature extraction methods and classification techniques (e.g., SNN, K-means and artificial neural network etc.). The SNN is utilized to process the sensory data generated from full-scale seven-story reinforced concrete building to verify the classification performances. Results show that the proposed SNN hardware has high classification accuracy, reliability, longevity and low hardware area overhead.
C1 [Pang, Lili] Nanjing Inst Technol, Sch Innovat & Entrepreneurship, Ind Ctr, Nanjing 211167, Peoples R China.
   [Liu, Junxiu; Harkin, Jim; Martin, George; McElholm, Malachy; Javed, Aqib; McDaid, Liam] Ulster Univ, Sch Comp Engn & Intelligent Syst, Derry BT48 7JL, North Ireland.
RP Pang, LL (corresponding author), Nanjing Inst Technol, Sch Innovat & Entrepreneurship, Ind Ctr, Nanjing 211167, Peoples R China.; Liu, JX (corresponding author), Ulster Univ, Sch Comp Engn & Intelligent Syst, Derry BT48 7JL, North Ireland.
EM panglili@njit.edu.cn; j.liu1@ulster.ac.uk; jg.harkin@ulster.ac.uk;
   gs.martin@ulster.ac.uk; m.mcelholm@ulster.ac.uk; javed-a@ulster.ac.uk;
   lj.mcdaid@ulster.ac.uk
CR Abdo MAB., 2014, STRUCTURAL HLTH MONI
   Amezquita-Sanchez JP, 2018, SCI IRAN, V25, P2913, DOI 10.24200/sci.2018.21136
   Amezquita-Sanchez JP, 2015, SCI IRAN, V22, P1931
   Azam SE, 2019, STRUCT CONTROL HLTH, V26, DOI 10.1002/stc.2288
   Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Bouzenad A, 2019, INVENTIONS-BASEL, V4, DOI 10.3390/inventions4010017
   Bull LA, 2019, MECH SYST SIGNAL PR, V134, DOI 10.1016/j.ymssp.2019.106294
   Chen T.W., 2009, P ICASSP IEEE INT C
   de Oliveira MA, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18010152
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   González AF, 2019, J PHARM PHARMACOGN R, V7, P12
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Harkin J, 2009, INT J RECONFIGURABLE, V2009, DOI 10.1155/2009/908740
   Hernandez E, 2018, EARTHQ ENG STRUCT D, V47, P2561, DOI 10.1002/eqe.3099
   Higgins I, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0180174
   Hsu TY, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18051437
   Jang S, 2010, SMART STRUCT SYST, V6, P439, DOI 10.12989/sss.2010.6.5_6.439
   Javed A, 2020, IEEE INT SYMP CIRC S
   Karayannis CG, 2016, CONSTR BUILD MATER, V105, P227, DOI 10.1016/j.conbuildmat.2015.12.019
   Kasabov N, 2016, NEURAL NETWORKS, V78, P1, DOI 10.1016/j.neunet.2015.09.011
   Kasabov N, 2013, NEURAL NETWORKS, V41, P188, DOI 10.1016/j.neunet.2012.11.014
   Kasabov NK, 2017, IEEE T NEUR NET LEAR, V28, P887, DOI 10.1109/TNNLS.2016.2612890
   Lee JM, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00191
   Liu J., 2015, P IEEE INT S CIRC SY
   Liu J., 2017, LECT NOTES ARTIFICIA
   Liu JX, 2019, IEEE T NEUR NET LEAR, V30, P865, DOI 10.1109/TNNLS.2018.2854291
   Medhi M, 2019, J NONDESTRUCT EVAL, V38, DOI 10.1007/s10921-019-0601-x
   Mesquita E, 2018, ENG STRUCT, V161, P108, DOI 10.1016/j.engstruct.2018.02.013
   Moaveni B, 2011, J STRUCT ENG-ASCE, V137, P705, DOI 10.1061/(ASCE)ST.1943-541X.0000300
   Moaveni B, 2010, STRUCT SAF, V32, P347, DOI 10.1016/j.strusafe.2010.03.006
   Naeem M, 2015, IEEE T NEUR NET LEAR, V26, P2370, DOI 10.1109/TNNLS.2014.2382334
   Notley S., 2018, ARXIV180502294
   Oh BK, 2017, APPL SOFT COMPUT, V58, P576, DOI 10.1016/j.asoc.2017.05.029
   Park SW, 2015, MEASUREMENT, V59, P352, DOI 10.1016/j.measurement.2014.09.063
   Roy K, 2019, NATURE, V575, P607, DOI 10.1038/s41586-019-1677-2
   Sun YW, 2012, COMPUT BIOL MED, V42, P751, DOI 10.1016/j.compbiomed.2012.04.007
   Wang JF, 2017, J PERFORM CONSTR FAC, V31, DOI 10.1061/(ASCE)CF.1943-5509.0000952
   Wang L., 2015, LECT NOTES ARTIFICIA
   Worden K, 2007, PHILOS T R SOC A, V365, P515, DOI 10.1098/rsta.2006.1938
   Zhang Y.Z., 2019, ZIDONGHUA XUEBAO, DOI [10.16383/j.aas.c180685, DOI 10.16383/J.AAS.C180685]
NR 40
TC 10
Z9 10
U1 4
U2 14
PD SEP
PY 2020
VL 20
IS 18
AR 5126
DI 10.3390/s20185126
UT WOS:000580727900001
DA 2023-11-16
ER

PT J
AU Lee, IH
   Cho, UI
AF Lee, IH
   Cho, UI
TI Realization of spiking by an excitable chemical system
SO PHYSICAL CHEMISTRY CHEMICAL PHYSICS
DT Article
ID NEURAL-NETWORK MODELS; PATTERN-RECOGNITION; REACTOR NETWORKS; TEMPORAL
   CODE; NEURONS; IMPLEMENTATION; OSCILLATIONS; INFORMATION; PROPAGATION;
   BEHAVIOR
AB It is theoretically demonstrated that an excitable chemical system can function as a spiking neuron and a chemical spiking neuron network can be constructed. The Oregonator is used as a model for a chemical spiking neuron. The chemical spiking neuron modeled by the Oregonator is similar in behavior to the spiking neuron model of Maass even though the Oregonator has a different excitation mechanism from the spiking neuron model. In the spiking neuron network, information is encoded and processed by using the timing of the spikes in spiking neurons. It is shown that chemical spiking neuron networks can be constructed by the unidirectional selective coupling of the spiking chemical neurons. The chemical spiking neuron network can process information encoded temporally like the spiking neuron network with a restriction in the range of the weight of couplings between the chemical neurons.
C1 Yonsei Univ, Dept Chem, Seoul 120749, South Korea.
RP Cho, UI (corresponding author), Yonsei Univ, Dept Chem, Seoul 120749, South Korea.
CR Buonomano DV, 1999, NEURAL COMPUT, V11, P103, DOI 10.1162/089976699300016836
   Dechert G, 1996, J PHYS CHEM-US, V100, P19043, DOI 10.1021/jp9616066
   Deco G, 1997, PHYS REV LETT, V79, P4697, DOI 10.1103/PhysRevLett.79.4697
   DEKEPPER P, 1990, J PHYS CHEM-US, V94, P6525, DOI 10.1021/j100380a004
   Diesmann M, 1999, NATURE, V402, P529, DOI 10.1038/990101
   DOLNIK M, 1989, J PHYS CHEM-US, V93, P2764, DOI 10.1021/j100344a015
   DOLNIK M, 1992, J PHYS CHEM-US, V96, P3218, DOI 10.1021/j100187a009
   Fetz EE, 1997, SCIENCE, V278, P1901, DOI 10.1126/science.278.5345.1901
   FIELD RJ, 1974, J CHEM PHYS, V60, P1877, DOI 10.1063/1.1681288
   Fields R.J, 1974, FARADAY CHEM SOC, V9, P21, DOI [10.1039/fs9740900021, DOI 10.1039/FS9740900021]
   FINKEOVA J, 1990, J PHYS CHEM-US, V94, P4110, DOI 10.1021/j100373a042
   Furukawa S, 2000, J NEUROSCI, V20, P1216, DOI 10.1523/JNEUROSCI.20-03-01216.2000
   GERSTNER W, 1995, PHYS REV E, V51, P738, DOI 10.1103/PhysRevE.51.738
   HJELMFELT A, 1995, PHYSICA D, V84, P180, DOI 10.1016/0167-2789(95)00014-U
   HJELMFELT A, 1993, J PHYS CHEM-US, V97, P7988, DOI 10.1021/j100132a030
   HJELMFELT A, 1993, SCIENCE, V260, P335, DOI 10.1126/science.260.5106.335
   HJELMFELT A, 1991, P NATL ACAD SCI USA, V88, P10983, DOI 10.1073/pnas.88.24.10983
   HJELMFELT A, 1992, P NATL ACAD SCI USA, V89, P383, DOI 10.1073/pnas.89.1.383
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Hohmann W, 1999, J PHYS CHEM A, V103, P7606, DOI 10.1021/jp991480n
   Hohmann W, 1998, J PHYS CHEM A, V102, P3103, DOI 10.1021/jp980377f
   Hohmann W, 1996, J CHEM SOC FARADAY T, V92, P2873, DOI 10.1039/ft9969202873
   Hohmann W, 1997, J PHYS CHEM A, V101, P7364, DOI 10.1021/jp971939i
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   LAPLANTE JP, 1995, J PHYS CHEM-US, V99, P10063, DOI 10.1021/j100025a001
   LEBENDER D, 1994, J PHYS CHEM-US, V98, P7533, DOI 10.1021/j100082a023
   Maass W, 1997, NEURAL COMPUT, V9, P279, DOI 10.1162/neco.1997.9.2.279
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   MAINEN ZF, 1995, SCIENCE, V268, P1503, DOI 10.1126/science.7770778
   MCCLURKIN JW, 1991, SCIENCE, V253, P675, DOI 10.1126/science.1908118
   MIDDLEBROOKS JC, 1994, SCIENCE, V264, P842, DOI 10.1126/science.8171339
   OKAMOTO M, 1988, BIOL CYBERN, V58, P295, DOI 10.1007/BF00363938
   PACAULT A, 1976, ACCOUNTS CHEM RES, V9, P438, DOI 10.1021/ar50108a003
   Parodi O, 1996, BIOL CYBERN, V74, P497, DOI 10.1007/BF00209421
   Pavlasek J, 1997, BIOL CYBERN, V77, P359, DOI 10.1007/s004220050396
   Prut Y, 1998, J NEUROPHYSIOL, V79, P2857, DOI 10.1152/jn.1998.79.6.2857
   Reich DS, 2000, J NEUROSCI, V20, P1964
   Riehle A, 1997, SCIENCE, V278, P1950, DOI 10.1126/science.278.5345.1950
   RUOFF P, 1986, J CHEM PHYS, V84, P1413, DOI 10.1063/1.450484
   RUOFF P, 1982, CHEM PHYS LETT, V90, P76, DOI 10.1016/0009-2614(82)83328-9
   RUOFF P, 1983, NATURWISSENSCHAFTEN, V70, P306, DOI 10.1007/BF00404839
   Steinbock O, 1996, J PHYS CHEM-US, V100, P18970, DOI 10.1021/jp961209v
   Thorpe SJ, 1997, ADV NEUR IN, V9, P901
   Watanabe M, 1998, BIOL CYBERN, V78, P87, DOI 10.1007/s004220050416
NR 44
TC 2
Z9 2
U1 0
U2 3
PY 2001
VL 3
IS 1
BP 94
EP 98
DI 10.1039/b006784i
UT WOS:000166521800012
DA 2023-11-16
ER

PT J
AU Belatreche, A
   Maguire, LP
   McGinnity, M
AF Belatreche, Ammar
   Maguire, Liam P.
   McGinnity, Martin
TI Advances in design and application of spiking neural networks
SO SOFT COMPUTING
DT Article; Proceedings Paper
CT 6th International FLINS Conference on Applied Artificial Intelligence
CY SEP 01-03, 2004
CL Blankenberge, BELGIUM
DE spiking neurons; spike response model; integrate-and-fire model; dynamic
   synapse; evolutionary strategy; temporal coding; supervised learning
AB This paper presents new findings in the design and application of biologically plausible neural networks based on spiking neuron models, which represent a more plausible model of real biological neurons where time is considered as an important feature for information encoding and processing in the brain. The design approach consists of an evolutionary strategy based supervised training algorithm, newly developed by the authors, and the use of different biologically plausible neuronal models. A dynamic synapse (DS) based neuron model, a biologically more detailed model, and the spike response model (SRM) are investigated in order to demonstrate the efficacy of the proposed approach and to further our understanding of the computing capabilities of the nervous system. Unlike the conventional synapse, represented as a static entity with a fixed weight, employed in conventional and SRM-based neural networks, a DS is weightless and its strength changes upon the arrival of incoming input spikes. Therefore its efficacy depends on the temporal structure of the impinging spike trains. In the proposed approach, the training of the network free parameters is achieved using an evolutionary strategy where, instead of binary encoding, real values are used to encode the static and DS parameters which underlie the learning process. The results show that spiking neural networks based on both types of synapse are capable of learning non-linearly separable data by means of spatio-temporal encoding. Furthermore, a comparison of the obtained performance with classical neural networks (multi-layer perceptrons) is presented.
C1 Univ Ulster, Fac Engn, Sch Comp & Intelligent Syst, Intelligent Syst Engn Lab, Derry BT48 7JL, North Ireland.
RP Belatreche, A (corresponding author), Univ Ulster, Fac Engn, Sch Comp & Intelligent Syst, Intelligent Syst Engn Lab, Magee Campus,Northland Rd, Derry BT48 7JL, North Ireland.
EM a.belatreche@ulster.ac.uk
CR Bäck T, 1993, EVOL COMPUT, V1, P1, DOI 10.1162/evco.1993.1.1.1
   Belatreche A, 2004, APPLIED COMPUTATIONAL INTELLIGENCE, P205, DOI 10.1142/9789812702661_0040
   Belatreche A, 2003, PROCEEDINGS OF THE 7TH JOINT CONFERENCE ON INFORMATION SCIENCES, P1524
   Beyer HG, 1995, EVOL COMPUT, V3, P311, DOI 10.1162/evco.1995.3.3.311
   Bohte S. M., 2000, 8th European Symposium on Artificial Neural Networks. ESANN"2000. Proceedings, P419
   Dibazar AA, 2003, IEEE IJCNN, P3146
   Fisher RA, 1936, ANN EUGENIC, V7, P179, DOI 10.1111/j.1469-1809.1936.tb02137.x
   Fuhrmann G, 2002, J NEUROPHYSIOL, V87, P140, DOI 10.1152/jn.00258.2001
   Gautrais J, 1998, BIOSYSTEMS, V48, P57, DOI 10.1016/S0303-2647(98)00050-1
   George S, 2003, IEEE IJCNN, P666
   GERSTNER W, 1995, PHYS REV E, V51, P738, DOI 10.1103/PhysRevE.51.738
   Gerstner W., 2002, SPIKING NEURON MODEL
   Goldberg D.E., 1989, GENETIC ALGORITHMS S, DOI [10.1111/j.1365-2486.2009.02080.x, DOI 10.1111/J.1365-2486.2009.02080.X]
   Heittmann A, 2004, 2004 47TH MIDWEST SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL II, CONFERENCE PROCEEDINGS, P373
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Holland JH., 1975, ADAPTATION NATURAL A
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maass W., 1999, PULSED NEURAL NETWOR
   Mangasarian O.L., 1990, CANC DIAGNOSIS VIA L
   Markram H, 1996, NATURE, V382, P807, DOI 10.1038/382807a0
   Pantic L, 2003, NETWORK-COMP NEURAL, V14, P17, DOI 10.1088/0954-898X/14/1/302
   Rao RPN, 2001, NEURAL COMPUT, V13, P2221, DOI 10.1162/089976601750541787
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Thorpe S, 1996, NATURE, V381, P520, DOI 10.1038/381520a0
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   Tsodyks M, 1998, NEURAL COMPUT, V10, P821, DOI 10.1162/089976698300017502
   WOLDBERG WW, 1990, P NATL ACAD SCI USA, V87, P9193
   Yao X, 1999, IEEE T EVOLUT COMPUT, V3, P82, DOI 10.1109/4235.771163
   Zador AM, 1997, NEURON, V19, P1, DOI 10.1016/S0896-6273(00)80341-4
NR 30
TC 60
Z9 65
U1 0
U2 17
PD FEB
PY 2007
VL 11
IS 3
BP 239
EP 248
DI 10.1007/s00500-006-0065-7
UT WOS:000241456100005
DA 2023-11-16
ER

PT C
AU Gibson, T
   Henderson, JA
   Wiles, J
AF Gibson, Tingting (Amy)
   Henderson, James A.
   Wiles, Janet
GP IEEE
TI Predicting Temporal Sequences Using an Event-based Spiking Neural
   Network Incorporating Learnable Delays
SO PROCEEDINGS OF THE 2014 INTERNATIONAL JOINT CONFERENCE ON NEURAL
   NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUL 06-11, 2014
CL Beijing, PEOPLES R CHINA
DE spiking neural networks; transmission delays; delay learning;
   spike-delay-variance learning; dynamic vision sensor
ID TIME-SERIES; DRIVEN SIMULATION; NEURONS; MULTISTEP; SPIKENET; MODELS
AB This paper presents a novel paradigm for a spiking neural network to forecast temporal sequences. The key to the approach is a new model of a spiking neuron that can make multi-step predictions, using learnable temporal delays at both dendrites and axons. This model is able to learn the temporal structure of space-time events, adaptable to multiple scales, with the neurons able to function asynchronously to predict future events in a video sequence. This approach contrasts with conventional neural network approaches that use fixed time steps and iterative prediction. Simulations were conducted to compare the new model to a conventional iterative paradigm on motion sequences from a frame-free event-driven Dynamic Vision Sensor (DVS128, 16k pixels), showing that the new approach consistently has a low prediction error while the iterative paradigm is affected by propagated errors.
C1 [Gibson, Tingting (Amy); Henderson, James A.; Wiles, Janet] Univ Queensland, Sch ITEE, Brisbane, Qld 4072, Australia.
RP Wiles, J (corresponding author), Univ Queensland, Sch ITEE, Brisbane, Qld 4072, Australia.
EM j.wiles@uq.edu.au
CR [Anonymous], ADV NEURAL INFORM PR
   [Anonymous], 1988, SUPERVISED LEARNING
   [Anonymous], 2002, 9EMES RENCONTRES INT
   Barbounis TG, 2006, IEEE T ENERGY CONVER, V21, P273, DOI 10.1109/TEC.2005.847954
   Berthouze L, 2006, NEURAL PROCESS LETT, V23, P27, DOI 10.1007/s11063-005-2838-x
   Boucheny C, 2005, LECT NOTES COMPUT SC, V3512, P136
   Brader JM, 2007, NEURAL COMPUT, V19, P2881, DOI 10.1162/neco.2007.19.11.2881
   Brandli C, 2014, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00275
   Burgsteiner H, 2007, APPL INTELL, V26, P99, DOI 10.1007/s10489-006-0007-1
   Campolo M, 2003, HYDROLOG SCI J, V48, P381, DOI 10.1623/hysj.48.3.381.45286
   Chandy K. M., 2006, GARTN APPL INT WEB S
   Chang FJ, 2007, HYDROLOG SCI J, V52, P114, DOI 10.1623/hysj.52.1.114
   Cheng CT, 2008, J HYDROL, V361, P118, DOI 10.1016/j.jhydrol.2008.07.040
   Debanne D, 2004, NAT REV NEUROSCI, V5, P304, DOI 10.1038/nrn1397
   Delorme A, 2003, NETWORK-COMP NEURAL, V14, P613, DOI 10.1088/0954-898X/14/4/301
   Drazen D, 2011, EXP FLUIDS, V51, P1465, DOI 10.1007/s00348-011-1207-y
   ELMAN JL, 1990, COGNITIVE SCI, V14, P179, DOI 10.1207/s15516709cog1402_1
   Eun Yeong Ahn, 2011, Proceedings 2011 IEEE Symposium on Computational Intelligence for Multimedia, Signal and Vision Processing (CIMSIVP 2011), P52, DOI 10.1109/CIMSIVP.2011.5949251
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Gibson T., 2014, P 2014 INT IN PRESS
   Guerra FA, 2008, CHAOS SOLITON FRACT, V35, P967, DOI 10.1016/j.chaos.2006.05.077
   Hill T, 1996, MANAGE SCI, V42, P1082, DOI 10.1287/mnsc.42.7.1082
   Kayacan E, 2010, EXPERT SYST APPL, V37, P1784, DOI 10.1016/j.eswa.2009.07.064
   Koeth F, 2013, BIOL INSPIR COGN ARC, V6, P8, DOI 10.1016/j.bica.2013.05.010
   Lichtsteiner P, 2008, IEEE J SOLID-ST CIRC, V43, P566, DOI 10.1109/JSSC.2007.914337
   Liu SC, 2010, IEEE INT SYMP CIRC S, P2027, DOI 10.1109/ISCAS.2010.5537164
   Mattia M, 2000, NEURAL COMPUT, V12, P2305, DOI 10.1162/089976600300014953
   Menezes JMP, 2008, NEUROCOMPUTING, V71, P3335, DOI 10.1016/j.neucom.2008.01.030
   Mirzaee H, 2009, CHAOS SOLITON FRACT, V41, P1975, DOI 10.1016/j.chaos.2008.08.016
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   Pal SK, 2008, STUD COMPUT INTELL, V103, P23
   Palmer A, 2006, TOURISM MANAGE, V27, P781, DOI 10.1016/j.tourman.2005.05.006
   Panchev C, 2005, LECT NOTES ARTIF INT, V3575, P182
   Pao HT, 2007, ENERG CONVERS MANAGE, V48, P907, DOI 10.1016/j.enconman.2006.08.016
   Reutimann J, 2003, NEURAL COMPUT, V15, P811, DOI 10.1162/08997660360581912
   Serrano-Gotarredona T., MNIST DVS DATABASE
   Sterne P, 2012, NEURAL COMPUT, V24, P2053, DOI 10.1162/NECO_a_00306
   Stratton P, 2010, NEUROIMAGE, V52, P1070, DOI 10.1016/j.neuroimage.2010.01.027
   Sutskever I, 2011, ICML
   Teräsvirta T, 2005, INT J FORECASTING, V21, P755, DOI 10.1016/j.ijforecast.2005.04.010
   Thorpe SJ, 2004, NEUROCOMPUTING, V58, P857, DOI 10.1016/j.neucom.2004.01.138
   Tolnai S, 2009, J NEUROPHYSIOL, V102, P1206, DOI 10.1152/jn.00275.2009
   WAIBEL A, 1989, IEEE T ACOUST SPEECH, V37, P328, DOI 10.1109/29.21701
   Wright P. W., 2012, NEUR NETW IJCNN 2012, P1
   Zhang GP, 2001, COMPUT OPER RES, V28, P381, DOI 10.1016/S0305-0548(99)00123-9
NR 45
TC 5
Z9 5
U1 0
U2 1
PY 2014
BP 3213
EP 3220
UT WOS:000371465703048
DA 2023-11-16
ER

PT C
AU Rafe, AW
   Garcia, JA
   Raffe, WL
AF Rafe, Andrew W.
   Garcia, Jaime A.
   Raffe, William L.
GP IEEE
TI Exploration Of Encoding And Decoding Methods For Spiking Neural Networks
   On The Cart Pole And Lunar Lander Problems Using Evolutionary Training
SO 2021 IEEE CONGRESS ON EVOLUTIONARY COMPUTATION (CEC 2021)
SE IEEE Congress on Evolutionary Computation
DT Proceedings Paper
CT IEEE Congress on Evolutionary Computation (IEEE CEC)
CY JUN 28-JUL 01, 2021
CL ELECTR NETWORK
DE genetic algorithm; spiking neurons; spiking neural network; spike train;
   reinforcement learning
ID MODEL
AB Spiking Neural Networks are increasingly drawing interest due to their potential for large efficiency gains when used with neuromorphic computers. However, when attempting to replicate the successes of Artificial Neural Networks, challenges are faced due to their vastly different architectures and therefore differing methods for training and optimisation. There has been minimal analysis of the differences between encoding and decoding methods and the effect of state space exposure periods on the performance of these networks. The core contribution of this paper is the detailed analysis of decoding methods, state exposure periods, and a learned input encoding method of an evolved Spiking Neural Network within the Reinforcement Learning context. This is demonstrated using the Cart Pole and Lunar Lander Reinforcement Learning problems. The paper discovers a negative correlation between the generation to reach the goal and the state space exposure period over all decoding methods tested. The state exposure period is also found to influence the number of random actions taken due to the decoding methods being unable to select an action. This paper explores the differences in temporal and rate-based decoding as well as identifying benefits in resetting networks to their default states between episode steps. Additionally, the novel input encoder, is effective at pre-processing state information using the same evolutionary algorithm as the rest of the network.
C1 [Rafe, Andrew W.; Garcia, Jaime A.; Raffe, William L.] Univ Technol Sydney, Sch Comp Sci, Sydney, NSW, Australia.
RP Rafe, AW (corresponding author), Univ Technol Sydney, Sch Comp Sci, Sydney, NSW, Australia.
EM andrew.w.rafe@student.uts.edu.au; jaime.garcia@uts.edu.au;
   william.raffe@uts.edu.au
CR Bing ZS, 2018, IEEE INT CONF ROBOT, P4725
   Brockman Greg, 2016, arXiv
   Burkitt AN, 2006, BIOL CYBERN, V95, P1, DOI 10.1007/s00422-006-0068-6
   Hebb D.O., 1949, ORG BEHAV NEUROPSYCH
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508
   Li M, 2017, FRONT CELL NEUROSCI, V11, DOI 10.3389/fncel.2017.00236
   Markowska-Kaczmar U, 2015, SOFT COMPUT, V19, P3465, DOI 10.1007/s00500-014-1515-2
   VanRullen R, 2005, TRENDS NEUROSCI, V28, P1, DOI 10.1016/j.tins.2004.10.010
   Wiklendt L, 2009, NEURAL COMPUT APPL, V18, P369, DOI 10.1007/s00521-008-0187-1
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Yee E., 2011, Proceedings of the 2011 11th International Conference on Hybrid Intelligent Systems (HIS 2011), P411, DOI 10.1109/HIS.2011.6122141
   Zhang L., 2018, IEEE IJCNN, P1
NR 15
TC 1
Z9 1
U1 0
U2 2
PY 2021
BP 498
EP 505
DI 10.1109/CEC45853.2021.9504921
UT WOS:000703866100063
DA 2023-11-16
ER

PT J
AU Johnson, MG
   Chartier, S
AF Johnson, Melissa G.
   Chartier, Sylvain
TI Spike neural models Part I: The Hodgkin-Huxley model
SO QUANTITATIVE METHODS FOR PSYCHOLOGY
DT Article
DE Spiking neural networks; neural models; Hodkgin-Huxley model
AB Artificial neural networks, or ANNs, have grown a lot since their inception back in the 1940s. But no matter the changes, one of the most important components of neural networks is still the node, which represents the neuron. Within spiking neural networks, the node is especially important because it contains the functions and properties of neurons that are necessary for their network. One important aspect of neurons is the ionic flow which produces action potentials, or spikes. Forces of diffusion and electrostatic pressure work together with the physical properties of the cell to move ions around changing the cell membrane potential which ultimately produces the action potential. This tutorial reviews the Hodkgin-Huxley model and shows how it simulates the ionic flow of the giant squid axon via four differential equations. The model is implemented in Matlab using Euler's Method to approximate the differential equations. By using Euler's method, an extra parameter is created, the time step. This new parameter needs to be carefully considered or the results of the node may be impaired.
C1 [Johnson, Melissa G.; Chartier, Sylvain] Univ Ottawa, Ottawa, ON, Canada.
RP Johnson, MG (corresponding author), Univ Ottawa, Ottawa, ON, Canada.
EM mjohn140@uottawa.ca
CR ABBOTT LF, 1990, LECT NOTES PHYS, V368, P5
   Carlson N. R, 2016, PHYSL BEHAV
   DasGupta B., 1994, MATH RES, V79, P641
   Denker J., 1987, Complex Systems, V1, P877
   Dreiseitl S, 2002, J BIOMED INFORM, V35, P352, DOI 10.1016/S1532-0464(03)00034-0
   DuBois ML, 2010, CELL BIO RES PROG, P1
   Gerstner W., 2002, SPIKING NEURON MODEL
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V116, P424, DOI 10.1113/jphysiol.1952.sp004716
   HODGKIN AL, 1952, PROC R SOC SER B-BIO, V140, P177, DOI 10.1098/rspb.1952.0054
   HODGKIN AL, 1952, COLD SPRING HARB SYM, V17, P43, DOI 10.1101/SQB.1952.017.01.007
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V116, P449, DOI 10.1113/jphysiol.1952.sp004717
   HODGKIN AL, 1955, J PHYSIOL-LONDON, V128, P28, DOI 10.1113/jphysiol.1955.sp005290
   HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554
   Kuebler E. S., 2014, BMC NEUROSCI, V15, P1, DOI [DOI 10.1186/1471-2202-15-S1-P26][PMCID, 10.1186/1471-2202-15-S1-P26, DOI 10.1186/1471-2202-15-S1-P26]
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   McCulloch WS., 1943, B MATH BIOPHYS, V5, P115, DOI DOI 10.1007/BF02478259
   ROSENBLATT F, 1958, PSYCHOL REV, V65, P386, DOI 10.1037/h0042519
   Shen Y, 2013, J BIOMOL NMR, V56, P227, DOI 10.1007/s10858-013-9741-y
   Sutskever I., 2014, ADV NEURAL INFORM PR, P3104
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Zamani M., 2010, PROC INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2010.5596806
NR 23
TC 3
Z9 4
U1 0
U2 3
PY 2017
VL 13
IS 2
BP 105
EP 119
DI 10.20982/tqmp.13.2.p105
UT WOS:000429407100001
DA 2023-11-16
ER

PT C
AU Mohemmed, A
   Schliebs, S
   Matsuda, S
   Kasabov, N
AF Mohemmed, Ammar
   Schliebs, Stefan
   Matsuda, Satoshi
   Kasabov, Nikola
BE Iliadis, L
   Jayne, C
TI Method for Training a Spiking Neuron to Associate Input-Output Spike
   Trains
SO ENGINEERING APPLICATIONS OF NEURAL NETWORKS, PT I
SE IFIP Advances in Information and Communication Technology
DT Proceedings Paper
CT 12th INNS EANN-SIG International Conference (EANN 2011)/7th IFIP 12 5
   International Conference (AIAI 2011)
CY SEP 15-18, 2011
CL Corfu, GREECE
DE Spiking Neural Networks; Supervised Learning; Spatio-temporal patterns
AB We propose a novel supervised learning rule allowing the training of a precise input-output behavior to a spiking neuron. A single neuron can be trained to associate (map) different output spike trains to different multiple input spike trains. Spike trains are transformed into continuous functions through appropriate kernels and then Delta rule is applied. The main advantage of the method is its algorithmic simplicity promoting its straightforward application to building spiking neural networks (SNN) for engineering problems. We experimentally demonstrate on a synthetic benchmark problem the suitability of the method for spatio-temporal classification. The obtained results show promising efficiency and precision of the proposed method.
C1 [Mohemmed, Ammar; Schliebs, Stefan; Matsuda, Satoshi; Kasabov, Nikola] Knowledge Engn Discovery Res Inst, Auckland 1010, New Zealand.
RP Mohemmed, A (corresponding author), Knowledge Engn Discovery Res Inst, 350 Queen St, Auckland 1010, New Zealand.
EM amohemme@aut.ac.nz; sschlieb@aut.ac.nz; matsuda.satoshi@nihon-u.ac.jp;
   nkasabov@aut.ac.nz
CR Bell CC, 1997, NATURE, V387, P278, DOI 10.1038/387278a0
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Bohte S. M., 2000, 8th European Symposium on Artificial Neural Networks. ESANN"2000. Proceedings, P419
   Florian R. V., 2010, CHRONOTRON NEURON LE
   Gerstner W., 2002, SPIKING NEURON MODEL
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Kasinski A., 2006, International Journal of Applied Mathematics and Computer Science, V16, P101
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Marc-Oliver G., 2007, SCHOLARPEDIA, DOI [DOI 10.4249/SCHOLARPEDIA.1430, 10.4249/scholarpedia.1430]
   Nordlie E, 2009, PLOS COMPUT BIOL, V5, DOI 10.1371/journal.pcbi.1000456
   Ponulak F., 2005, RESUME NEW SUPERVISE
   Ponulak F, 2008, INT J APPL MATH COMP, V18, P117, DOI 10.2478/v10006-008-0011-1
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   van Rossum MCW, 2001, NEURAL COMPUT, V13, P751, DOI 10.1162/089976601300014321
   Victor JD, 1997, NETWORK-COMP NEURAL, V8, P127, DOI 10.1088/0954-898X/8/2/003
NR 15
TC 14
Z9 14
U1 2
U2 2
PY 2011
VL 363
BP 219
EP 228
UT WOS:000309322700025
DA 2023-11-16
ER

PT C
AU Ratnasingam, S
   McGinnity, TM
AF Ratnasingam, Sivalogeswaran
   McGinnity, T. M.
GP IEEE
TI A Spiking Neural Network for Tactile Form Based Object Recognition
SO 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUL 31-AUG 05, 2011
CL San Jose, CA
DE Tactile object recognition; robotic object recognition; tactile form
   perception
ID NEURONS; TOUCH
AB This paper proposes a biologically plausible system for object recognition based on tactile form perception. A spiking neural network, an encoding scheme for converting the input values into spike trains, a method for converting the output spike pattern into reliable features for object recognition and a training approach for the spiking neural network are proposed. Three separate spiking neural networks are used in this recognition system. Three features, based on the output firing pattern of the three networks, are projected onto a three dimensional space. Each class of objects forms a cluster in the three-dimensional feature space. During the training the firing threshold of the hidden layer is modified in such a way that the cluster formed by an object is small and does not overlap with neighbouring clusters. The system has been tested with a number of objects for recognition based on shape. In addition, the system has also been tested for the ability to recognise objects of the same shape but different size. The results show the proposed system gives good performance in recognising objects based on tactile form perception.
C1 [Ratnasingam, Sivalogeswaran; McGinnity, T. M.] Univ Ulster, Sch Comp & Intelligent Syst, Intelligent Syst Res Ctr, Coleraine BT52 1SA, Londonderry, North Ireland.
RP Ratnasingam, S (corresponding author), Univ Ulster, Sch Comp & Intelligent Syst, Intelligent Syst Res Ctr, Magee Campus, Coleraine BT52 1SA, Londonderry, North Ireland.
EM s.ratnasingam@ulster.ac.uk; tm.mcginnity@ulster.ac.uk
CR Arbib MA, 2000, NEURAL NETWORKS, V13, P975, DOI 10.1016/S0893-6080(00)00070-8
   Azouz R, 2000, P NATL ACAD SCI USA, V97, P8110, DOI 10.1073/pnas.130200797
   BIEDERMAN I, 1987, PSYCHOL REV, V94, P115, DOI 10.1037/0033-295X.94.2.115
   BURKE D, 1988, J PHYSIOL-LONDON, V402, P347, DOI 10.1113/jphysiol.1988.sp017208
   Calvert G. A., 2004, HDB MULTISENSORY PRO, V1st ed.
   CHAVEZNORIEGA LE, 1990, EXP BRAIN RES, V79, P633
   Dario P, 2000, 2000 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2000), VOLS 1-3, PROCEEDINGS, P1, DOI 10.1109/IROS.2000.894573
   DeLaurentis K. J., 2000, DEV SHAPE MEMORY ALL
   Desai NS, 1999, NAT NEUROSCI, V2, P515, DOI 10.1038/9165
   Heidemann G, 2004, IEEE INT CONF ROBOT, P813, DOI 10.1109/ROBOT.2004.1307249
   Ittyerah M, 2007, BRIT J PSYCHOL, V98, P589, DOI 10.1348/000712606X171531
   James TW, 2007, CAN J EXP PSYCHOL, V61, P219, DOI 10.1037/cjep2007023
   Johnsson M, 2005, LECT NOTES COMPUT SC, V3561, P386
   Johnsson M., 2007, AUTONOMOUS ROBOTIC S, P239
   Johnsson M., 2006, P 9 SCAND C ART INT, P127
   Johnsson M, 2007, ROBOT AUTON SYST, V55, P720, DOI 10.1016/j.robot.2007.05.003
   Johnsson M, 2010, J ROBOT, V2010, DOI 10.1155/2010/860790
   KLATZKY RL, 1987, J EXP PSYCHOL GEN, V116, P356
   LEDERMAN SJ, 1990, COGNITIVE PSYCHOL, V22, P421, DOI 10.1016/0010-0285(90)90009-S
   Natale L., 2006, 6 INT C EP ROB PAR F, P20
   Okamura AM, 1997, IEEE INT CONF ROBOT, P2485, DOI 10.1109/ROBOT.1997.619334
   Petriu E. M., 2003, IEEE T INSTRUMENTATI, V53, P1425
   Ratnasingam S., 2011, IEEE S SERI IN PRESS
   Wade J., 2010, THESIS U ULSTER
NR 24
TC 4
Z9 4
U1 0
U2 3
PY 2011
BP 880
EP 885
UT WOS:000297541201001
DA 2023-11-16
ER

PT J
AU Wang, Y
   Duan, SK
   Chen, F
AF Wang, Yuan
   Duan, Shukai
   Chen, Feng
TI Efficient asynchronous federated neuromorphic learning of spiking neural
   networks
SO NEUROCOMPUTING
DT Article
DE Asynchronous federated learning; Spiking Neural Network; Average spike
   rate; Model stalenss
ID POWER
AB Spiking Neural Networks (SNNs) can be trained on resource-constrained devices at low computational costs. There has been little attention to training them on a large-scale distributed system like federated learning. Federated Learning (FL) can be exploited to perform collaborative training for higher accuracy, involving multiple resource-constrained devices. In this paper, we introduce SNNs into asynchronous federated learning (AFL), which adapts to the statistical heterogeneity of users and complex communication environments. A novel fusion weight based on information age and average spike rate is designed, which aims to reduce the impact of model staleness. Numerical experiments validate SNNs on federated learning with MNIST, FashionMNIST, CIFAR10 and SVHN benchmarks, achieving better accuracy and desirable convergence under Non-IID settings.
C1 [Wang, Yuan; Duan, Shukai; Chen, Feng] Southwest Univ, Coll Artificial Intelligence, Chongqing 400715, Peoples R China.
RP Chen, F (corresponding author), Southwest Univ, Coll Artificial Intelligence, Chongqing 400715, Peoples R China.
EM wangyuan_stu@163.com; duansk@swu.edu.cn; fengchen.uestc@gmail.com
CR Bing Han, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13555, DOI 10.1109/CVPR42600.2020.01357
   Chen M., 2019, INT C LEARN RE UNPUB
   Chen RZ, 2018, IEEE IJCNN, P404
   Diehl PU, 2015, IEEE IJCNN
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Gu B, 2022, IEEE T NEUR NET LEAR, V33, P6103, DOI 10.1109/TNNLS.2021.3072238
   Jang H, 2019, IEEE SIGNAL PROC MAG, V36, P64, DOI 10.1109/MSP.2019.2935234
   Kairouz P, 2021, FOUND TRENDS MACH LE, V14, P1, DOI 10.1561/2200000083
   Karimireddy SP, 2020, PR MACH LEARN RES, V119
   Kheradpisheh SR, 2022, NEURAL PROCESS LETT, V54, P1255, DOI 10.1007/s11063-021-10680-x
   Kim S, 2020, AAAI CONF ARTIF INTE, V34, P11270
   Kim Y, 2021, FRONT NEUROSCI-SWITZ, V15, DOI 10.3389/fnins.2021.773954
   Lee C, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00119
   Lee J, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00143
   Li QB, 2021, PROC CVPR IEEE, P10708, DOI 10.1109/CVPR46437.2021.01057
   Li T., 2020, P MACHINE LEARNING S, V2, P429, DOI DOI 10.48550/ARXIV.1812.06127
   Lu YL, 2020, IEEE T IND INFORM, V16, P2134, DOI 10.1109/TII.2019.2942179
   McMahan HB, 2017, PR MACH LEARN RES, V54, P1273
   Mostafa H, 2018, IEEE T NEUR NET LEAR, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Nguyen J, 2022, Arxiv, DOI arXiv:2106.06639
   Rathi N, 2023, IEEE T NEUR NET LEAR, V34, P3174, DOI 10.1109/TNNLS.2021.3111897
   Roy K, 2019, NATURE, V575, P607, DOI 10.1038/s41586-019-1677-2
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Shi C, 2021, IEEE T CIRCUITS-II, V68, P1581, DOI 10.1109/TCSII.2021.3063784
   Skatchkovsky N, 2020, INT CONF ACOUST SPEE, P8524, DOI [10.1109/ICASSP40776.2020.9053861, 10.1109/icassp40776.2020.9053861]
   Tavanaei A, 2019, NEUROCOMPUTING, V330, P39, DOI 10.1016/j.neucom.2018.11.014
   Venkatesha Y, 2021, IEEE T SIGNAL PROCES, V69, P6183, DOI 10.1109/TSP.2021.3121632
   Wang HB, 2022, IEEE T BIOMED CIRC S, V16, P636, DOI 10.1109/TBCAS.2022.3189240
   Wang ZY, 2022, IEEE T WIREL COMMUN, V21, P6961, DOI 10.1109/TWC.2022.3153495
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Xie C, 2020, Arxiv, DOI arXiv:1903.03934
   Xie K, 2022, IEEE T VEH TECHNOL, V71, P9980, DOI 10.1109/TVT.2022.3178808
   Yang Q, 2019, ACM T INTEL SYST TEC, V10, DOI 10.1145/3298981
   Yu H, 2019, AAAI CONF ARTIF INTE, P5693
   Zhao B, 2015, IEEE T NEUR NET LEAR, V26, P1963, DOI 10.1109/TNNLS.2014.2362542
   Zheng HL, 2020, Arxiv, DOI arXiv:2011.05280
   Zhou YH, 2022, IEEE T PARALL DISTR, V33, P192, DOI 10.1109/TPDS.2021.3090331
NR 39
TC 0
Z9 0
U1 1
U2 1
PD NOV 7
PY 2023
VL 557
AR 126686
DI 10.1016/j.neucom.2023.126686
EA AUG 2023
UT WOS:001073646100001
DA 2023-11-16
ER

PT C
AU Venceslai, V
   Marchisio, A
   Alouani, I
   Martina, M
   Shafique, M
AF Venceslai, Valerio
   Marchisio, Alberto
   Alouani, Ihsen
   Martina, Maurizio
   Shafique, Muhammad
GP IEEE
TI NeuroAttack: Undermining Spiking Neural Networks Security through
   Externally Triggered Bit-Flips
SO 2020 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN) held as part
   of the IEEE World Congress on Computational Intelligence (IEEE WCCI)
CY JUL 19-24, 2020
CL ELECTR NETWORK
DE Machine Learning; Spiking Neural Networks; Reliability; Adversarial
   Attacks; Fault-Injection Attacks; Deep Neural Networks; DNN; SNN;
   Security; Resilience; Cross-Layer
AB Due to their proven efficiency, machine-learning systems are deployed in a wide range of complex real-life problems. More specifically, Spiking Neural Networks (SNNs) emerged as a promising solution to the accuracy, resourceutilization, and energy-efficiency challenges in machine-learning systems. While these systems are going mainstream, they have inherent security and reliability issues. In this paper, we propose NeuroAttack, a cross-layer attack that threatens the SNNs integrity by exploiting low-level reliability issues through a high-level attack. Particularly, we trigger a fault-injection based sneaky hardware backdoor through a carefully crafted adversarial input noise. Our results on Deep Neural Networks (DNNs) and SNNs show a serious integrity threat to state-of-the art machine-learning techniques.
C1 [Venceslai, Valerio; Marchisio, Alberto; Shafique, Muhammad] Tech Univ Wien, Vienna, Austria.
   [Venceslai, Valerio; Martina, Maurizio] Politecn Torino, Turin, Italy.
   [Alouani, Ihsen] Univ Polytech Hauts De France, Valenciennes, France.
RP Venceslai, V (corresponding author), Tech Univ Wien, Vienna, Austria.; Venceslai, V (corresponding author), Politecn Torino, Turin, Italy.
EM s254591@studenti.polito.it; alberto.marchisio@tuwien.ac.at;
   ihsen.alouani@uphf.fr; maurizio.martina@polito.it;
   muhammad.shafiquel@tuwien.ac.at
CR Abbassi I. H., 2018, DATE
   [Anonymous], 2017, CORR
   [Anonymous], 2019, CORR
   Beeman D, 2013, ENCY COMPUTATIONAL N
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Bouvier M., 2019, SPIKING NEURAL NETWO
   Cardaliaguet P., 1992, NEURAL NETWORKS
   Clements J., 2018, CORR
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Gibson G. J., 1989, ICASSP
   Goodfellow Ian J., 2015, EXPLAINING HARNESSIN
   Han Song, 2016, ICLR
   Hanif M. A., 2018, DATE
   Hanif M. A., 2019, PHILOS T ROYAL SOC A
   Hanif MA, 2020, IEEE INT ON LINE, DOI 10.1109/iolts50870.2020.9159734
   Hanif MA, 2018, J LOW POWER ELECTRON, V14, P520, DOI 10.1166/jolpe.2018.1575
   Hanif MA, 2018, IEEE INT ON LINE, P257, DOI 10.1109/IOLTS.2018.8474192
   Hazan H, 2018, FRONT NEUROINFORM, V12, DOI 10.3389/fninf.2018.00089
   Hoang L.-H., 2020, DATE
   Izhikevich E. M., 2003, IEEE T NEURAL NETWOR
   Kim Y, 2014, CONF PROC INT SYMP C, P361, DOI 10.1109/ISCA.2014.6853210
   Krizhevsky Alex, 2009, LEARNING MULTIPLE LA
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee C., 2018, FRONTIERS NEUROSCIEN
   Lee J. Y., 2016, CORR, Vabs/1603.03827
   Liu YN, 2017, ICCAD-IEEE ACM INT, P131, DOI 10.1109/ICCAD.2017.8203770
   Liu YQ, 2018, 25TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2018), DOI 10.14722/ndss.2018.23291
   Marchisio Alberto, 2019, 2019 IEEE Computer Society Annual Symposium on VLSI (ISVLSI), P553, DOI 10.1109/ISVLSI.2019.00105
   Marchisio A., 2020, IJCNN
   Marchisio A., 2018, IJCNN
   Marchisio A., 2020, DATE
   Marchisio A., 2020, DAC
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Neftci E. O., 2019, SIGNAL PROCESSING MA
   Neggaz M. A., 2018, ICCD
   Neggaz M. A., 2019, IEEE DESIGN TEST
   Rakin A. S., 2019, CORR
   Reagen B, 2018, DES AUT CON, DOI 10.1145/3195970.3195997
   Reagen B, 2016, CONF PROC INT SYMP C, P267, DOI 10.1109/ISCA.2016.32
   Rodriguez J., 2019, FDT
   Rueckauer B., FRONTIERS NEUROSCIEN
   Vaila R., 2019, CORR
   Wang ZZ, 2014, INT J NEURAL SYST, V24, DOI 10.1142/S0129065714400048
   Zhang J. J., 2019, DAC
NR 44
TC 13
Z9 13
U1 0
U2 3
PY 2020
DI 10.1109/ijcnn48605.2020.9207351
UT WOS:000626021405125
DA 2023-11-16
ER

PT C
AU Yudanov, D
   Shaaban, M
   Melton, R
   Reznik, L
AF Yudanov, Dmitri
   Shaaban, Muhammad
   Melton, Roy
   Reznik, Leon
GP IEEE
TI GPU-Based Simulation of Spiking Neural Networks with Real-Time
   Performance & High Accuracy
SO 2010 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS IJCNN 2010
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT World Congress on Computational Intelligence (WCCI 2010)
CY 2010
CL Barcelona, SPAIN
DE GPU; CUDA; STDP; spiking neural network; high accuracy; parallel
   computing; shared memory
ID NEURONS; MODEL
AB A novel GPU-based simulation of spiking neural networks is implemented as a hybrid system using Parker-Sochacki numerical integration method with adaptive order. Full single-precision floating-point accuracy for all model variables is achieved. The implementation is validated with exact matching of all neuron potential traces from GPU-based simulation versus those of a reference CPU-based simulation. A network of 4096 Izhikevich neurons simulated on an NVIDIA GTX260 device achieves real-time performance with a speedup of 9 compared to simulation executed on Opteron 285, 2.6-GHz device.
C1 [Yudanov, Dmitri; Shaaban, Muhammad; Melton, Roy; Reznik, Leon] Rochester Inst Technol, Dept Comp Engn, Rochester, NY 14623 USA.
RP Yudanov, D (corresponding author), Rochester Inst Technol, Dept Comp Engn, Rochester, NY 14623 USA.
EM dxy7370@gmail.com; meseec@rit.edu; Roy.Melton@mail.rit.edu;
   lr@cs.rit.edu
CR [Anonymous], 2008, EFFICIENT SPARSE MAT
   [Anonymous], 2010, CUDA DATA PARALLEL P
   Brette R, 2007, J COMPUT NEUROSCI, V23, P349, DOI 10.1007/s10827-007-0038-6
   Coddington E., 1955, THEORY ORDINARY DIFF
   Fidjeland AK, 2009, IEEE INT CONF ASAP, P137, DOI 10.1109/ASAP.2009.24
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2008, P NATL ACAD SCI USA, V105, P3593, DOI 10.1073/pnas.0712231105
   Lapique L., 1907, J PHYSL PATHOL GEN, V9, P620, DOI DOI 10.1007/S00422-007-0189-6
   Nageswaran J., 2009, NEURAL NETWORKS  JUL
   Nageswaran Jayram Moorkanikara, 2009, 2009 IEEE International Symposium on Circuits and Systems - ISCAS 2009, P1917, DOI 10.1109/ISCAS.2009.5118157
   Parker G. E., 1996, Neural, Parallel & Scientific Computations, V4, P97
   Picard E., 1922, TRAITE ANAL, V3
   Stewart RD, 2009, J COMPUT NEUROSCI, V27, P115, DOI 10.1007/s10827-008-0131-5
   2008, NVIDIA CUDA PROGRAMM
   2009, NVIDIA CUDA C PROGRA
NR 16
TC 1
Z9 1
U1 0
U2 0
PY 2010
UT WOS:000287421402142
DA 2023-11-16
ER

PT C
AU Jeanson, F
   White, A
AF Jeanson, Francis
   White, Anthony
BE Soule, T
TI Evolving Axonal Delay Neural Networks for Robot Control
SO PROCEEDINGS OF THE FOURTEENTH INTERNATIONAL CONFERENCE ON GENETIC AND
   EVOLUTIONARY COMPUTATION CONFERENCE
DT Proceedings Paper
CT 14th International Conference on Genetic and Evolutionary Computation
   Conference (GECCO)
CY JUL 07-11, 2012
CL Philadelphia, PA
DE Neural Coding; Spiking Neural Network; Axonal Delays; Coincidence
   Detection; Neural Adaptation; Embodied Cognition
ID VISUAL-CORTEX; CAT; OSCILLATIONS; DYNAMICS; NEURONS
AB This paper investigates the dynamical and control properties of a discrete spiking neural network model with axonal delays. After examining contemporary work on spike timing as a mechanism for neural coding, we introduce a simple axonal delay network model which, via coincidence detection, demonstrates the presence of biologically observed regimes such as sustained firing and the emergence of synchrony. We establish delay criteria allowing for the classification of three distinct regimes including global synchrony, complex firing, and dissipation. We then proceed to test this model in a robot light seeking task. Results show that evolving network delays is sufficient for solving the task. We conclude by hypothesizing that global synchronous firing is more suited to reactive behaviours while complex firing patterns may serve as an organizing mechanism for more indirect processing.
C1 [Jeanson, Francis; White, Anthony] Carleton Univ, Ottawa, ON K1S 5B6, Canada.
RP Jeanson, F (corresponding author), Carleton Univ, 1125 Colonel Dr, Ottawa, ON K1S 5B6, Canada.
EM fjeanson@connect.carleton.ca; arpwhite@scs.carleton.ca
CR [Anonymous], 1991, CORTICONICS
   BOUYER JJ, 1981, ELECTROEN CLIN NEURO, V51, P244, DOI 10.1016/0013-4694(81)90138-3
   Braitenberg V., 1984, VEHICLES EXPT SYNTHE
   BROOKS RA, 1991, ARTIF INTELL, V47, P139, DOI 10.1016/0004-3702(91)90053-M
   Brunel N, 2000, J COMPUT NEUROSCI, V8, P183, DOI 10.1023/A:1008925309027
   Buszaki G., 2006, RHYTHMS BRAIN
   ECKHORN R, 1988, BIOL CYBERN, V60, P121, DOI 10.1007/BF00202899
   Edelman G. M., 1987, NEURAL DARWINISM THE
   Engel AK, 2001, TRENDS COGN SCI, V5, P16, DOI 10.1016/S1364-6613(00)01568-0
   Fujii H, 1996, NEURAL NETWORKS, V9, P1303, DOI 10.1016/S0893-6080(96)00054-8
   Izhikevich EM, 2004, CEREB CORTEX, V14, P933, DOI 10.1093/cercor/bhh053
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Konig P, 1996, TRENDS NEUROSCI, V19, P130, DOI 10.1016/S0166-2236(96)80019-1
   Kriener B, 2008, NEURAL COMPUT, V20, P2185, DOI 10.1162/neco.2008.02-07-474
   Michel O., 1995, KHEPERA SIMULATOR PA
   MURTHY VN, 1992, P NATL ACAD SCI USA, V89, P5670, DOI 10.1073/pnas.89.12.5670
   NOLFI S, 1996, P WORKSH EV COMP MAC
   Perkel D. H., 1968, Bulletin Neurosci Res Progr, V6, P221
   PETERS A, 1993, CEREB CORTEX, V3, P49, DOI 10.1093/cercor/3.1.49
   Ren M, 2007, SCIENCE, V316, P758, DOI 10.1126/science.1135468
   Scheier C., 1995, ADV ARTIFICIAL LIFE, P862
   SPERLING G, 1960, PSYCHOL MONOGR, V74, P1, DOI 10.1037/h0093759
   Stevens CF, 1999, NEURON, V22, P139, DOI 10.1016/S0896-6273(00)80685-6
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   THORPE SJ, 1989, CONNECTIONISM IN PERSPECTIVE, P63
   von der Marlsburg C., 1981, CORRELATION THEORY B, P2
   Ward LM, 2003, TRENDS COGN SCI, V7, P553, DOI 10.1016/j.tics.2003.10.012
NR 28
TC 5
Z9 5
U1 2
U2 4
PY 2012
BP 121
EP 128
DI 10.1145/2330163.2330181
UT WOS:000309611100016
DA 2023-11-16
ER

PT J
AU Gupta, S
   Singal, G
   Garg, D
   Jagannathan, S
AF Gupta, Surbhi
   Singal, Gaurav
   Garg, Deepak
   Jagannathan, Sarangapani
TI QC_SANE: Robust Control in DRL Using Quantile Critic With Spiking Actor
   and Normalized Ensemble
SO IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
DT Article
DE Artificial neural networks; Neurons; Uncertainty; Task analysis;
   Robustness; Statistics; Sociology; Actor critic; deep reinforcement
   learning (DRL); ensemble; reinforcement learning (RL); robust control;
   spiking neural network (SNN)
AB Recently introduced deep reinforcement learning (DRL) techniques in discrete-time have resulted in significant advances in online games, robotics, and so on. Inspired from recent developments, we have proposed an approach referred to as Quantile Critic with Spiking Actor and Normalized Ensemble (QC_SANE) for continuous control problems, which uses quantile loss to train critic and a spiking neural network (NN) to train an ensemble of actors. The NN does an internal normalization using a scaled exponential linear unit (SELU) activation function and ensures robustness. The empirical study on multijoint dynamics with contact (MuJoCo)-based environments shows improved training and test results than the state-of-the-art approach: population coded spiking actor network (PopSAN).
C1 [Gupta, Surbhi; Garg, Deepak] Bennett Univ, Dept Comp Sci Engn, Greater Noida 201310, Uttar Pradesh, India.
   [Singal, Gaurav] Netaji Subhas Univ Technol, Dept Comp Sci Engn, New Delhi 110078, India.
   [Jagannathan, Sarangapani] Missouri Univ Sci & Technol, Dept Elect & Comp Engn, Rolla, MO 65409 USA.
RP Singal, G (corresponding author), Netaji Subhas Univ Technol, Dept Comp Sci Engn, New Delhi 110078, India.
EM gsurbhi1993@gmail.com; gaurav.singal@nsut.ac.in;
   deepakgarg108@gmail.com; sarangap@mst.edu
CR Comsa JM, 2020, INT CONF ACOUST SPEE, P8529, DOI [10.1109/icassp40776.2020.9053856, 10.1109/ICASSP40776.2020.9053856]
   Garg, 2019, ARXIV190904121
   Gupta S, 2021, ARCH COMPUT METHOD E, V28, P4715, DOI 10.1007/s11831-021-09552-3
   Haarnoja T, 2018, PR MACH LEARN RES, V80
   Hans A., 2010, 2010 Ninth International Conference on Machine Learning and Applications (ICMLA 2010), P401, DOI 10.1109/ICMLA.2010.66
   Hessel M, 2018, AAAI CONF ARTIF INTE, P3215
   Hill Ashley, 2018, STABLE BASELINES
   Kaul, 2020, ARXIV200105209
   Klambauer G., 2017, P 31 INT C NEUR INF, DOI DOI 10.5555/3294771.3294864
   Kuznetsov A, 2020, INT C MACHINE LEARNI, P5556
   Leahy, 2020, ARXIV201009042
   Lillicrap T.P., 2016, CONTINUOUS CONTROL D
   Lopez-Paz D, 2018, ARXIV181100908
   Ma XY, 2020, VEH TECHNOL CONFE, DOI 10.1109/VTC2020-Fall49728.2020.9348512
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Michmizos, 2020, ARXIV201009635
   Mnih Volodymyr, 2013, PLAYING ATARI DEEP R
   Ota K, 2020, P INT C MACH LEARN, P7424
   Patel D, 2019, NEURAL NETWORKS, V120, P108, DOI 10.1016/j.neunet.2019.08.009
   Rathi Nitin, 2020, INT C LEARN REPR
   Schneider J., 2020, ARXIV201109588
   Silver D, 2014, PR MACH LEARN RES, V32
   Takeuchi, 2006, NONPARAMETRIC QUANTI
   Tieck JCV, 2021, IEEE ROBOT AUTOM LET, V6, P2894, DOI 10.1109/LRA.2020.3034067
   van Hasselt H, 2016, AAAI CONF ARTIF INTE, P2094
   Wang ZY, 2016, PR MACH LEARN RES, V48
   Wattenhofer, 2019, ARXIV190611941
   Zhang ST, 2019, AAAI CONF ARTIF INTE, P5789
   Zhou, 2017, ARXIV171208987
NR 29
TC 0
Z9 0
U1 5
U2 12
PD SEP
PY 2023
VL 34
IS 9
BP 6656
EP 6662
DI 10.1109/TNNLS.2021.3129525
EA DEC 2021
UT WOS:000733470100001
DA 2023-11-16
ER

PT C
AU Xue, FZ
   Zhang, Y
   Zhou, HJ
   Li, XM
AF Xue, Fangzheng
   Zhang, Yang
   Zhou, Hongjun
   Li, Xiumin
BE DelgadoGarcia, JM
   Pan, X
   SanchezCampusano, R
   Wang, R
TI Effects of Temporal Integration on Computational Performance of Spiking
   Neural Network
SO ADVANCES IN COGNITIVE NEURODYNAMICS (VI)
SE Advances in Cognitive Neurodynamics
DT Proceedings Paper
CT 6th International Conference on Cognitive Neurodynamics (ICCN)
CY AUG 01-05, 2017
CL Carmona, SPAIN
DE Spiking neural network; Time constant; Time delay; Computational
   performance
ID TIME
AB In spiking neural networks (SNN), information is considered to be encoded mainly in the temporal patterns of their firing activity. Temporal integration of information plays a crucial role in a variety of cognitive processes, such as sensory discrimination, decision-making, or interval timing. However, it is rarely considered in traditional computational SNN models. In this paper, we investigate the influence of temporal integration on the computational performance of liquid state machine (LSM) from two aspects: the synaptic decay constant and time delay from presynaptic neurons to the output neurons. LSM is a biologically spiking neural network model for real-time computing on time-varying inputs, where the high dimensionality of dynamical spikes is transformed into smoothly changing states through synaptic integration into the readout neuron. Our experimental results show that increasing the decay constant of synapses from SNN to the output neuron can remarkably improve the computational performance due to the enhancement of temporal integration. Moreover, transmission delays have an even larger impact on the richness of dynamical states, which in turn significantly increase the computational accuracy of SNN. These results may have important implications for the modeling of spiking neural networks with excellent computational performance.
C1 [Xue, Fangzheng; Zhang, Yang; Li, Xiumin] Chongqing Univ, Coll Automat, Chongqing, Peoples R China.
   [Zhou, Hongjun] Chongqing Univ, Sch Econ & Business Adm, Chongqing, Peoples R China.
RP Li, XM (corresponding author), Chongqing Univ, Coll Automat, Chongqing, Peoples R China.
EM xmli@cqu.edu.cn
CR Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Joshi P, 2004, LECT NOTES COMPUT SC, V3141, P258
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Maass W, 2001, THEOR COMPUT SCI, V261, P157, DOI 10.1016/S0304-3975(00)00137-7
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maass W., 1997, Proceedings of the Tenth Annual Conference on Computational Learning Theory, P54, DOI 10.1145/267460.267477
   Maass W., 2002, DBLP, P213
   Maass W., 1999, COMPLEXITY LEARNING
   Maass W, 2007, PLOS COMPUT BIOL, V3, P15, DOI 10.1371/journal.pcbi.0020165
   Schmitt M., 1998, COMPUTING BOOLEAN FU
   Schrauwen B, 2008, NEURAL NETWORKS, V21, P511, DOI 10.1016/j.neunet.2007.12.009
   Verstraeten D, 2005, INFORM PROCESS LETT, V95, P521, DOI 10.1016/j.ipl.2005.05.019
NR 12
TC 0
Z9 0
U1 0
U2 1
PY 2018
BP 127
EP 133
DI 10.1007/978-981-10-8854-4_16
UT WOS:000552577400016
DA 2023-11-16
ER

PT J
AU Kim, J
   Kim, CH
   Woo, SY
   Kang, WM
   Seo, YT
   Lee, S
   Oh, S
   Bae, JH
   Park, BG
   Lee, JH
AF Kim, Jangsaeng
   Kim, Chul-Heung
   Woo, Sung Yun
   Kang, Won-Mook
   Seo, Young-Tak
   Lee, Soochang
   Oh, Seongbin
   Bae, Jong-Ho
   Park, Byung-Gook
   Lee, Jong-Ho
TI Initial synaptic weight distribution for fast learning speed and high
   recognition rate in STDP-based spiking neural network
SO SOLID-STATE ELECTRONICS
DT Article
DE Initial synaptic weight distribution; Spike-timing-dependent plasticity
   (STDP); Homeostasis functionality; NOR flash memory; Spiking neural
   networks
ID MEMORY
AB We analyze that the initial synaptic weight distribution affects the performance, such as the learning speed, recognition rate and the power consumption in the spiking neural networks (SNNs) based on spike-timing-dependent plasticity (STDP) learning rule. A thin-film transistor (TFT)-type NOR flash memory is used as a synaptic device. In this fully connected two-layer neuromorphic system using the proposed pulse scheme, the results with and without the homeostasis functionality were analyzed separately. In addition, power consumption of the network in various initial synaptic weight distributions, and recognition rate that varies with the number of output neurons are also investigated. In pattern recognition for 28 x 28 MNIST handwritten patterns, higher performance is achieved in various aspects when the initial synaptic weights are distributed near the maximum value.
C1 [Kim, Jangsaeng; Kim, Chul-Heung; Woo, Sung Yun; Kang, Won-Mook; Seo, Young-Tak; Lee, Soochang; Oh, Seongbin; Park, Byung-Gook; Lee, Jong-Ho] Seoul Natl Univ, Dept Elect & Comp Engn, Seoul 151742, South Korea.
   [Kim, Jangsaeng; Kim, Chul-Heung; Woo, Sung Yun; Kang, Won-Mook; Seo, Young-Tak; Lee, Soochang; Oh, Seongbin; Park, Byung-Gook; Lee, Jong-Ho] Seoul Natl Univ, ISRC, Seoul 151742, South Korea.
   [Bae, Jong-Ho] Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA.
RP Lee, JH (corresponding author), Seoul Natl Univ, Dept Elect & Comp Engn, Seoul 151742, South Korea.; Lee, JH (corresponding author), Seoul Natl Univ, ISRC, Seoul 151742, South Korea.
EM jhl@snu.ac.kr
CR [Anonymous], 2010, 2010 INT JOINT C NEU
   [Anonymous], 2012, IEDM
   [Anonymous], 2015, DEEP LEARNING, DOI [DOI 10.1038/NATURE14539, 10.1038/nature14539]
   Burr GW, 2015, IEEE T ELECTRON DEV, V62, P3498, DOI 10.1109/TED.2015.2439635
   Fernandez-Redondo M., 2000, Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium, P543, DOI 10.1109/IJCNN.2000.860828
   Fernandez-Redondo M., 2001, 9th European Symposium on Artificial Neural Networks. ESANN'2001. Proceedings, P119
   Indiveri G, 2015, P IEEE, V103, P1379, DOI 10.1109/JPROC.2015.2444094
   Kang WM, 2019, IEEE IJCNN
   Kim CH, 2018, IEEE T ELECTRON DEV, V65, P1774, DOI 10.1109/TED.2018.2817266
   Lee S, 2019, J NANOSCI NANOTECHNO, V19, P6050, DOI 10.1166/jnn.2019.17025
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Querlioz D, 2013, IEEE T NANOTECHNOL, V12, P288, DOI 10.1109/TNANO.2013.2250995
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Sidler S, 2017, LECT NOTES COMPUT SC, V10613, P281, DOI 10.1007/978-3-319-68600-4_33
   Suri M, 2011, 2011 IEEE INTERNATIONAL ELECTRON DEVICES MEETING (IEDM)
   Yam JYF, 2000, NEUROCOMPUTING, V30, P219, DOI 10.1016/S0925-2312(99)00127-7
   Zamarreño-Ramos C, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00026
NR 17
TC 3
Z9 3
U1 2
U2 37
PD MAR
PY 2020
VL 165
AR 107742
DI 10.1016/j.sse.2019.107742
UT WOS:000510833600008
DA 2023-11-16
ER

PT J
AU Bohte, SM
   Kok, JN
AF Bohte, SM
   Kok, JN
TI Applications of spiking neural networks
SO INFORMATION PROCESSING LETTERS
DT Editorial Material
DE distributed computing
C1 CWI, NL-1098 SJ Amsterdam, Netherlands.
   Leiden Univ, LIACS, NL-2300 RA Leiden, Netherlands.
RP Bohte, SM (corresponding author), CWI, Kruislaan 413, NL-1098 SJ Amsterdam, Netherlands.
EM s.m.bohte@cwi.nl; joost@liacs.nl
CR [Anonymous], 2005, ADV NEURAL INF PROCE
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Deneve S., 2005, ADV NEURAL INFORM PR, V17, P353
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Zemel R, 2005, ADV NEURAL INFORM PR, V17, P1609
NR 5
TC 16
Z9 16
U1 0
U2 6
PD SEP 30
PY 2005
VL 95
IS 6
BP 519
EP 520
DI 10.1016/j.ipl.2005.05.018
UT WOS:000231637900001
DA 2023-11-16
ER

PT J
AU Jeon, M
   Kang, T
   Lee, JJ
   Lee, W
AF Jeon, Mingi
   Kang, Taewook
   Lee, Jae-Jin
   Lee, Woojoo
TI A Study on the Low-Power Operation of the Spike Neural Network Using the
   Sensory Adaptation Method
SO MATHEMATICS
DT Article
DE artificial neural networks; spiking neural networks; neuromorphic;
   frequency adaptation
ID MODEL; PROCESSOR; LOIHI
AB Motivated by the idea that there should be a close relationship between biological significance and low power driving of spike neural networks (SNNs), this paper aims to focus on spike-frequency adaptation, which deviates significantly from existing biological meaningfulness, and develop a new spike-frequency adaptation with more biological characteristics. As a result, this paper proposes the sensory adaptation method that reflects the mechanisms of the human sensory organs, and studies network architectures and neuron models for the proposed method. Next, this paper introduces a dedicated SNN simulator that can selectively apply the conventional spike-frequency adaptation and the proposed method, and provides the results of functional verification and effectiveness evaluation of the proposed method. Through intensive simulation, this paper reveals that the proposed method can produce a level of training and testing performance similar to the conventional method while significantly reducing the number of spikes to 32.66% and 45.63%, respectively. Furthermore, this paper contributes to SNN research by showing an example based on in-depth analysis that embedding biological meaning in SNNs may be closely related to the low-power driving characteristics of SNNs.
C1 [Jeon, Mingi; Lee, Woojoo] Chung Ang Univ, Sch Elect & Elect Engn, 84 Heukseok Ro, Seoul 06974, South Korea.
   [Kang, Taewook; Lee, Jae-Jin] Elect & Telecommun Res Inst, 218 Gajeong Ro, Daejeon 34129, South Korea.
RP Lee, W (corresponding author), Chung Ang Univ, Sch Elect & Elect Engn, 84 Heukseok Ro, Seoul 06974, South Korea.
EM space@cau.ac.kr
CR Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   Barbier T, 2021, IEEE COMPUT SOC CONF, P1377, DOI 10.1109/CVPRW53098.2021.00152
   Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Davies M., 2019, P NEURO INSPIRED COM
   Davies M, 2021, P IEEE, V109, P911, DOI 10.1109/JPROC.2021.3067593
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Doborjeh M, 2021, NEURAL NETWORKS, V144, P522, DOI 10.1016/j.neunet.2021.09.013
   Edalatpanah S.A., 2021, ARXIV, DOI DOI 10.48550/ARXIV.2106.12178
   Guo SS, 2019, PR GR LAK SYMP VLSI, P63, DOI 10.1145/3299874.3317966
   Ha GE, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms13791
   Han K., 2019, INT C INDOOR POSIT, P1, DOI [10.1109/IPIN.2019.8911751, DOI 10.1109/ipin.2019.8911751]
   Han K, 2021, IEEE INTERNET THINGS, V8, P4642, DOI 10.1109/JIOT.2020.3027479
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Kang T, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3187719
   Kang T, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2021.3082273
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Khodaverdian Zeinab, 2021, 2021 7th International Conference on Web Research (ICWR), P191, DOI 10.1109/ICWR51868.2021.9443133
   Kulkarni SR, 2017, IEEE I C ELECT CIRC, P128, DOI 10.1109/ICECS.2017.8292015
   Lee W, 2019, IEEE T COMPUT AID D, V38, P1758, DOI 10.1109/TCAD.2018.2859240
   Li SX, 2021, IEEE T CIRCUITS-I, V68, P1543, DOI 10.1109/TCSI.2021.3052885
   Liu DQ, 2017, NEUROCOMPUTING, V249, P212, DOI 10.1016/j.neucom.2017.04.003
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   MORRIS C, 1981, BIOPHYS J, V35, P193, DOI 10.1016/S0006-3495(81)84782-0
   Morrison A, 2007, NEURAL COMPUT, V19, P1437, DOI 10.1162/neco.2007.19.6.1437
   Nessler B, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003037
   Peron SP, 2009, BIOL CYBERN, V100, P505, DOI 10.1007/s00422-009-0304-y
   Peykani P., 2021, BIG DATA COMPUT VISI, V1, P170
   Pfister JP, 2006, J NEUROSCI, V26, P9673, DOI 10.1523/JNEUROSCI.1425-06.2006
   Querlioz D, 2013, IEEE T NANOTECHNOL, V12, P288, DOI 10.1109/TNANO.2013.2250995
   Shukla R., 2021, BIG DATA COMPUT VISI, V1, P1
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
NR 35
TC 0
Z9 0
U1 0
U2 1
PD NOV
PY 2022
VL 10
IS 22
AR 4191
DI 10.3390/math10224191
UT WOS:000887502800001
DA 2023-11-16
ER

PT J
AU Li, M
   Ruan, HB
   Qi, Y
   Guo, TT
   Wang, P
   Pan, G
AF Li, Ming
   Ruan, Haibo
   Qi, Yu
   Guo, Tiantian
   Wang, Ping
   Pan, Gang
TI Odor Recognition with a Spiking Neural Network for Bioelectronic Nose
SO SENSORS
DT Article
DE odor recognition; bioelectronic nose; spiking neural network
ID DISCRIMINATION; MODEL; MACHINE; NEURONS
AB Electronic noses recognize odors using sensor arrays, and usually face difficulties for odor complicacy, while animals have their own biological sensory capabilities for various types of odors. By implanting electrodes into the olfactory bulb of mammalian animals, odors may be recognized by decoding the recorded neural signals, in order to construct a bioelectronic nose. This paper proposes a spiking neural network (SNN)-based odor recognition method from spike trains recorded by the implanted electrode array. The proposed SNN-based approach exploits rich timing information well in precise time points of spikes. To alleviate the overfitting problem, we design a new SNN learning method with a voltage-based regulation strategy. Experiments are carried out using spike train signals recorded from the main olfactory bulb in rats. Results show that our SNN-based approach achieves the state-of-the-art performance, compared with other methods. With the proposed voltage regulation strategy, it achieves about 15% improvement compared with a classical SNN model.
C1 [Li, Ming; Ruan, Haibo; Qi, Yu; Pan, Gang] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310027, Zhejiang, Peoples R China.
   [Guo, Tiantian; Wang, Ping] Zhejiang Univ, Dept Biomed Engn, Hangzhou 310027, Zhejiang, Peoples R China.
   [Pan, Gang] Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310027, Zhejiang, Peoples R China.
RP Qi, Y; Pan, G (corresponding author), Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310027, Zhejiang, Peoples R China.; Pan, G (corresponding author), Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310027, Zhejiang, Peoples R China.
EM lming@zju.edu.cn; hbruan@zju.edu.cn; qiyu@zju.edu.cn;
   21315047@zju.edu.cn; cnpwang@zju.edu.cn; gpan@zju.edu.cn
CR Adrian ED, 1926, J PHYSIOL-LONDON, V61, P49, DOI 10.1113/jphysiol.1926.sp002273
   Bi GQ, 2001, ANNU REV NEUROSCI, V24, P139, DOI 10.1146/annurev.neuro.24.1.139
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Breer H, 2003, ANAL BIOANAL CHEM, V377, P427, DOI 10.1007/s00216-003-2113-9
   Burkitt AN, 2006, BIOL CYBERN, V95, P1, DOI 10.1007/s00422-006-0068-6
   Di Pietrantonio F, 2015, BIOSENS BIOELECTRON, V67, P516, DOI 10.1016/j.bios.2014.09.027
   Dong Q, 2013, BIOSENS BIOELECTRON, V49, P263, DOI 10.1016/j.bios.2013.05.035
   GARDNER JW, 1994, SENSOR ACTUAT B-CHEM, V18, P211
   Guo TT, 2016, SENSOR ACTUAT B-CHEM, V225, P34, DOI 10.1016/j.snb.2015.11.010
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Indiveri G, 2006, IEEE T NEURAL NETWOR, V17, P211, DOI 10.1109/TNN.2005.860850
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Lee SH, 2010, BIOTECHNOL BIOPROC E, V15, P22, DOI [10.1007/s12257-009-3077-1, 10.1007/S12257-009-3077-1]
   Ma X, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00044
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Olshausen BA, 2003, J COGNITIVE NEUROSCI, V15, P154, DOI 10.1162/089892903321107891
   Pan G, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00555
   PERSAUD K, 1982, NATURE, V299, P352, DOI 10.1038/299352a0
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Qi Y, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1597
   Serruya MD, 2002, NATURE, V416, P141, DOI 10.1038/416141a
   Sun XY, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0167497
   Wasilewski T, 2018, SENSOR ACTUAT B-CHEM, V257, P511, DOI 10.1016/j.snb.2017.10.086
   Wasilewski T, 2017, BIOSENS BIOELECTRON, V87, P480, DOI 10.1016/j.bios.2016.08.080
   Wu JB, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00836
   Wu ZH, 2016, IEEE INTELL SYST, V31, P44, DOI 10.1109/MIS.2016.105
   Wu ZH, 2013, IEEE INTELL SYST, V28, P28, DOI 10.1109/MIS.2013.137
   You KJ, 2011, IEEE T BIO-MED ENG, V58, P1208, DOI 10.1109/TBME.2010.2103312
   Yu Q, 2016, IEEE T NEUR NET LEAR, V27, P621, DOI 10.1109/TNNLS.2015.2416771
   Yu Q, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0078318
   Yu YP, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0147754
   Zhang SM, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-018-36885-0
NR 35
TC 8
Z9 8
U1 3
U2 37
PD MAR 1
PY 2019
VL 19
IS 5
AR 993
DI 10.3390/s19050993
UT WOS:000462540400012
DA 2023-11-16
ER

PT S
AU Florian, RV
AF Florian, Razvan V.
BE Nolfi, S
   Baldassarre, G
   Calabretta, R
   Hallam, JCT
   Marocco, D
   Meyer, JA
   Miglino, O
   Parisi, D
TI Spiking neural controllers for pushing objects around
SO FROM ANIMALS TO ANIMATS 9, PROCEEDINGS
SE Lecture Notes in Computer Science
DT Article; Proceedings Paper
CT 9th International Conference on Simulation of Adaptive Behavior
CY SEP 25-29, 2006
CL Italian Natl Res Council, Rome, ITALY
HO Italian Natl Res Council
ID TIMING-DEPENDENT PLASTICITY; NEURONS
AB We evolve spiking neural networks that implement a seek-push-release drive for a simple simulated agent interacting with objects. The evolved agents display minimally-cognitive behavior, by switching as a function of context between the three sub-behaviors and by being able to discriminate relative object size. The neural controllers have either static synapses or synapses featuring spike-timing-dependent plasticity (STDP). Both types of networks are able to solve the task with similar efficacy, but networks with plastic synapses evolved faster. In the evolved networks, plasticity plays a minor role during the interaction with the environment and is used mostly to tune synapses when networks start to function.
C1 Ctr Cognit & Neural Studies, Cluj Napoca 400504, Romania.
   Univ Babes Bolyai, Inst Interdisciplinary Expt Res, Cluj Napoca 400271, Romania.
RP Florian, RV (corresponding author), Ctr Cognit & Neural Studies, Str Saturn 24, Cluj Napoca 400504, Romania.
EM florian@coneural.org
CR [Anonymous], P AAAI SPRING S DEV
   BEER R, 1996, P 4 INT C SIMULATION, P721
   Bi GQ, 2002, BIOL CYBERN, V87, P319, DOI 10.1007/s00422-002-0349-7
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   DAMPER R, 1998, P 1998 IEEE INT S CI
   Damper RI, 2003, LECT NOTES COMPUT SC, V2611, P616
   Damper RI, 2000, ROBOT AUTON SYST, V31, P247, DOI 10.1016/S0921-8890(99)00122-0
   Dan Y, 2004, NEURON, V44, P23, DOI 10.1016/j.neuron.2004.09.007
   DasGupta B, 1996, NEURAL COMPUT, V8, P805, DOI 10.1162/neco.1996.8.4.805
   Di Paolo EA, 2003, PHILOS T R SOC A, V361, P2299, DOI 10.1098/rsta.2003.1256
   Di Paolo EA, 2002, ADAPT BEHAV, V10, P243, DOI 10.1177/1059712302010003006
   DIPAOLO EA, 2002, EPSRCBBSRC INT WORKS
   Federici D, 2005, NEURAL NETWORKS, V18, P746, DOI 10.1016/j.neunet.2005.06.006
   Federici D, 2005, P CEC 2005 IEEE C EV
   FLOREANO D, 2001, EVOLUTIONARY ROBOTIC, V4
   FLOREANO D, 2002, P 3 INT S HUM ART IN
   Floreano D, 2002, P 8 INT C ART LIF
   Florian RV, 2005, Seventh International Symposium on Symbolic and Numeric Algorithms for Scientific Computing, Proceedings, P299
   FLORIAN RV, 2003, 0301 CTR COGN NEUR S
   FLORIAN RV, 2003, CONEURAL0302 CTR COG
   FLORIAN RV, 2003, CONEURAL0303 CTR COG
   FRENCH RLB, 2001, P GEN EV COMP C GECC, P1099
   FRENCH RLB, 2002, P 7 INT C SIM AD BEH, P335
   Gerstner W., 2002, SPIKING NEURON MODEL
   HAGRAS HAK, 2004, P 2004 IEEE INT C RO
   Katada Y, 2004, LECT NOTES COMPUT SC, V3242, P952
   Katada Y, 2003, 2003 IEEE INTERNATIONAL SYMPOSIUM ON COMPUTATIONAL INTELLIGENCE IN ROBOTICS AND AUTOMATION, VOLS I-III, PROCEEDINGS, P318
   MAAS W, 1999, PULSED NEURAL NETWOR
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   MURESAN RC, 2004, INT C INT ENG SYST S
   Pfeifer R., 1999, UNDERSTANDING INTELL
   ROGGEN D, 2003, 2003 NASA DOD C EVOL, P199
   Ruppin E, 2002, NAT REV NEUROSCI, V3, P132, DOI 10.1038/nrn729
   SAGGIE K, 2003, LECT NOTES COMPUTER, V2801
   SAGGIEWEXLER K, 2005, ARTIF LIFE, V12, P1
   Schnitger G., 1994, THEORETICAL ADV NEUR, P127
   Slocum AC, 2000, FROM ANIM ANIMAT, P430
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   SOULA H, 2004, LAST MINUTE RESULTS
   SOULA H, 2003, P INT C ART NEUR NET
   TURNEY P, 1996, 13 INT C MACH LEARN, P135
   Van Leeuwen M, 2003, EVOLVING VISION BASE
NR 43
TC 8
Z9 8
U1 0
U2 4
PY 2006
VL 4095
BP 570
EP 581
UT WOS:000242125300047
DA 2023-11-16
ER

PT J
AU Guo, L
   Shi, HY
   Chen, YG
   Yu, HL
AF Guo, Lei
   Shi, Hongyi
   Chen, Yunge
   Yu, Hongli
TI Anti-interference ability of deep spiking neural network
SO JOURNAL OF INTEGRATIVE NEUROSCIENCE
DT Article
DE Deep spiking neural network; synaptic plasticity; anti-interference;
   electric field; firing rate; correlation
ID MODEL
AB Organisms have the advantages of self-adaptive mechanisms and an anti-interference ability. To investigate the anti-interference ability of a deep spiking neural network that simulates a biological neural system, the correlation between membrane potential and firing rate is interpreted as an anti-interference index so as to investigate the anti-interference ability of a deep spiking neural network under the regulation of synaptic plasticity in the presence of different amplitudes of an electric field. When the relative variation rate of firing rate is less than 10% or the correlation between the membrane potential is greater than half, the influence of electric field on neural network is relatively small. Otherwise, the influence is relatively large. Simulation results show that: based on the regulation of synaptic plasticity, within a certain electric field interference range, the relative rate of variation of cell firing rates is small compared with non-interference, while correlation between the membrane potential in each layer is large when compared to non-interference.
C1 [Guo, Lei] Hebei Univ Technol, Coll Elect Engn, Dept Biomed Engn, Tianjin 300130, Peoples R China.
   [Shi, Hongyi] Hebei Univ Technol, Sch Elect Engn, State Key Lab Reliable & Intelligence Elect Equip, Tianjin 300130, Peoples R China.
   [Chen, Yunge] Hebei Univ Technol, Sch Elect Engn, Key Lab Electromagnet Field & Elect Apparat Relia, Tianjin 300130, Peoples R China.
RP Guo, L (corresponding author), Hebei Univ Technol, Coll Elect Engn, Dept Biomed Engn, Tianjin 300130, Peoples R China.
EM 2004008@hebut.edu.cn
CR Bédard C, 2006, PHYS REV E, V73, DOI 10.1103/PhysRevE.73.051911
   Chang Xiao-long, 2014, Journal of Shanghai Jiaotong University, V48, P1485
   [陈云芝 Chen Yunzhi], 2014, [中国医学物理学杂志, Chinese Journal of Medical Physics], V31, P4820
   Chen YZ, 2015, J BIOMEDICAL ENG, V39, P697
   Guo L, 2018, J INTEGR NEUROSCI, V17, P1
   Izhikevich EM, 2004, CEREB CORTEX, V14, P933, DOI 10.1093/cercor/bhh053
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   [巨政权 Ju Zhengquan], 2012, [微电子学与计算机, Microelectronics & Computer], V29, P89
   [蔺想红 Lin Xianghong], 2015, [电子学报, Acta Electronica Sinica], V43, P577
   Litwin-Kumar A, 2014, NAT COMMUN, V5, DOI 10.1038/ncomms6319
   Nobukawa S, 2015, J ARTIF INTELL SOFT, V5, P109, DOI 10.1515/jaiscr-2015-0023
   Radman T, 2009, BRAIN STIMUL, V2, P215, DOI 10.1016/j.brs.2009.03.007
   Rich S, 2015, BMC NEUROSCI, V16, pP303
   Wang ML, 2015, ACTA PHYS SINICA, V64, P416
   Wei Y, 2014, J NEUROSCI, V34, P15804, DOI 10.1523/JNEUROSCI.3929-12.2014
   Yang Tianpeng, 2013, Journal of Beijing University of Aeronautics and Astronautics, V39, P697
   Yang YP, 2013, J BEIJING U AERONAUT, V39, P705
   Yu Kai, 2014, Application Research of Computers, V31, P70, DOI 10.3969/j.issn.1001-3695.2014.01.016
   [于凯 Yu Kai], 2013, [天津大学学报. 自然科学与工程技术版, Journal of Tianjin University], V46, P726
   Yuan L, 2014, CHINESE ENG SCI, V16, P76
   [张昭昭 Zhang Zhaozhao], 2014, [信息与控制, Information and Control], V43, P181
NR 21
TC 0
Z9 0
U1 1
U2 13
PY 2018
VL 17
IS 4
BP 307
EP 311
DI 10.31083/j.jin.2018.04.0407
UT WOS:000450613300001
DA 2023-11-16
ER

PT C
AU Garrido, JA
   Carrillo, RR
   Luque, NR
   Ros, E
AF Garrido, Jesus A.
   Carrillo, Richard R.
   Luque, Niceto R.
   Ros, Eduardo
BE Cabestany, J
   Rojas, I
   Joya, G
TI Event and Time Driven Hybrid Simulation of Spiking Neural Networks
SO Advances in Computational Intelligence, IWANN 2011, Pt I
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 11th International Work-Conference on Artificial Neural Networks (IWANN)
CY JUN 08-10, 2011
CL Torremolinos, SPAIN
DE spiking neural networks; simulation; time-driven; event-driven
ID NEURONS; INTEGRATE; DYNAMICS; MODELS
AB Emerging research areas in neuroscience are requiring simulation of large and detailed spiking neural networks. Although event-driven methods have been recently proposed to simulate these networks, they still present some drawbacks. To obtain the advantages of an event-driven simulation method and a traditional time-driven method, we present a hybrid method. This method efficiently simulates neural networks composed of several neural models: highly active neurons or neurons defined by very-complex model are simulated using a time-driven method whereas other neurons are simulated using an event-driven method based in lookup tables. To perform a comparative study of this hybrid method in terms of speed and accuracy, a model of the cerebellar granular layer has been simulated. The performance results showed that a hybrid simulation can provide considerable advantages when the network is composed of neurons with different characteristics.
C1 [Garrido, Jesus A.; Luque, Niceto R.; Ros, Eduardo] Univ Granada, Dept Comp Architecture & Technol, CITIC, ETSI Informat & Telecomunicac, E-18071 Granada, Spain.
   [Carrillo, Richard R.] Univ Almeria, Dept Comp Architecture & Elect, Almeria, Spain.
RP Garrido, JA (corresponding author), Univ Granada, Dept Comp Architecture & Technol, CITIC, ETSI Informat & Telecomunicac, E-18071 Granada, Spain.
EM jgarrido@atc.ugr.es; rcarrillo@atc.ugr.es; nluque@atc.ugr.es;
   eros@atc.ugr.es
CR [Anonymous], ADV NEURAL INFORM PR
   [Anonymous], 1998, BOOK GENESIS EXPLORI, DOI DOI 10.1007/978-1-4612-1634-63
   Brette R, 2006, NEURAL COMPUT, V18, P2004, DOI 10.1162/neco.2006.18.8.2004
   Coenen OJMD, 2001, AUTON ROBOT, V11, P291, DOI 10.1023/A:1012403510221
   D'Haene M, 2009, NEURAL COMPUT, V21, P1068, DOI 10.1162/neco.2008.02-08-707
   Delorme A, 1999, NEUROCOMPUTING, V26-7, P989, DOI 10.1016/S0925-2312(99)00095-8
   Gerstner W., 2002, SPIKING NEURON MODEL
   Hines ML, 1997, NEURAL COMPUT, V9, P1179, DOI 10.1162/neco.1997.9.6.1179
   KANDEL E., 1981, PRINCIPLES NEUROSCIE
   Makino T, 2003, NEURAL COMPUT APPL, V11, P210, DOI 10.1007/s00521-003-0358-z
   O'reilly R.C., 2000, COMPUTATIONAL EXPLOR
   Reutimann J, 2003, NEURAL COMPUT, V15, P811, DOI 10.1162/08997660360581912
   Ros E, 2006, NEURAL COMPUT, V18, P2959, DOI 10.1162/neco.2006.18.12.2959
NR 13
TC 10
Z9 10
U1 1
U2 5
PY 2011
VL 6691
BP 554
EP 561
UT WOS:000353417500069
DA 2023-11-16
ER

PT J
AU Evans, BD
   Stringer, SM
AF Evans, Benjamin D.
   Stringer, Simon M.
TI Transformation-invariant visual representations in self-organizing
   spiking neural networks
SO FRONTIERS IN COMPUTATIONAL NEUROSCIENCE
DT Article
DE transformation-invariant visual object recognition; integrate and fire;
   spiking neural net; continuous transformation learning; trace learning;
   inferior temporal cortex
ID TIMING-DEPENDENT PLASTICITY; OBJECT RECOGNITION; NEURONAL RESPONSES;
   SINGLE NEURONS; CORTEX; SYSTEM; SYNCHRONIZATION; SENSITIVITY;
   STATISTICS; FEATURES
AB The ventral visual pathway achieves object and face recognition by building transformation-invariant representations from elementary visual features. In previous computer simulation studies with rate-coded neural networks, the development of transformation-invariant representations has been demonstrated using either of two biologically plausible learning mechanisms, Trace learning and Continuous Transformation (CT) learning. However, it has not previously been investigated how transformation-invariant representations may be learned in a more biologically accurate spiking neural network. A key issue is how the synaptic connection strengths in such a spiking network might self-organize through Spike-Time Dependent Plasticity (STDP) where the change in synaptic strength is dependent on the relative times of the spikes emitted by the presynaptic and postsynaptic neurons rather than simply correlated activity driving changes in synaptic efficacy. Here we present simulations with conductance-based integrate-and-fire (IF) neurons using a STDP learning rule to address these gaps in our understanding. It is demonstrated that with the appropriate selection of model parameters and training regime, the spiking network model can utilize either Trace-like or CT-like learning mechanisms to achieve transform-invariant representations.
C1 [Evans, Benjamin D.; Stringer, Simon M.] Univ Oxford, Dept Expt Psychol, Ctr Theoret Neurosci & Artificial Intelligence, Oxford OX1 3UD, England.
RP Evans, BD (corresponding author), Univ Oxford, Dept Expt Psychol, Ctr Theoret Neurosci & Artificial Intelligence, S Parks Rd, Oxford OX1 3UD, England.
EM benjamin.evans@psy.ox.ac.uk
CR Abbott LF, 2000, NAT NEUROSCI, V3, P1178, DOI 10.1038/81453
   Amit DJ, 1997, NETWORK-COMP NEURAL, V8, P373, DOI 10.1088/0954-898X/8/4/003
   [Anonymous], 2005, INFORM THEORY INFERE
   Bell AJ, 1997, VISION RES, V37, P3327, DOI 10.1016/S0042-6989(97)00121-1
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   BIALEK W, 1991, SCIENCE, V252, P1854, DOI 10.1126/science.2063199
   Booth MCA, 1998, CEREB CORTEX, V8, P510, DOI 10.1093/cercor/8.6.510
   Dan Y, 2006, PHYSIOL REV, V86, P1033, DOI 10.1152/physrev.00030.2005
   David SV, 2004, J NEUROSCI, V24, P6991, DOI 10.1523/JNEUROSCI.1422-04.2004
   DESIMONE R, 1991, J COGNITIVE NEUROSCI, V3, P1, DOI 10.1162/jocn.1991.3.1.1
   Diesmann M, 1999, NATURE, V402, P529, DOI 10.1038/990101
   Elliffe MCM, 2002, BIOL CYBERN, V86, P59, DOI 10.1007/s004220100284
   Felsen G, 2005, PLOS BIOL, V3, P1819, DOI 10.1371/journal.pbio.0030342
   Felsen G, 2005, NAT NEUROSCI, V8, P1643, DOI 10.1038/nn1608
   FERSTER D, 1995, SCIENCE, V270, P756, DOI 10.1126/science.270.5237.756
   FOLDIAK P, 1993, COMPUTATION AND NEURAL SYSTEMS, P55
   Földiák P, 1991, NEURAL COMPUT, V3, P194, DOI 10.1162/neco.1991.3.2.194
   Fries P, 2002, J NEUROSCI, V22, P3739
   FUKUSHIMA K, 1988, NEURAL NETWORKS, V1, P119, DOI 10.1016/0893-6080(88)90014-7
   Gerstner W., 2006, SPIKING NEURON MODEL
   Hebb D.O., 1949, ORG BEHAV NEUROPSYCH
   HUBEL DH, 1968, J PHYSIOL-LONDON, V195, P215, DOI 10.1113/jphysiol.1968.sp008455
   ITO M, 1995, J NEUROPHYSIOL, V73, P218, DOI 10.1152/jn.1995.73.1.218
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Kreiter AK, 1996, J NEUROSCI, V16, P2381
   KUWABARA N, 1993, J NEUROPHYSIOL, V69, P1713, DOI 10.1152/jn.1993.69.5.1713
   Maass W., 1999, PULSED NEURAL NETWOR
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   Masquelier T, 2009, J NEUROSCI, V29, P13484, DOI 10.1523/JNEUROSCI.2207-09.2009
   MCCORMICK DA, 1985, J NEUROPHYSIOL, V54, P782, DOI 10.1152/jn.1985.54.4.782
   McCulloch Warren S., 1943, BULL MATH BIOPHYS, V5, P115, DOI 10.1007/BF02478259
   Mel BW, 1997, NEURAL COMPUT, V9, P777, DOI 10.1162/neco.1997.9.4.777
   Michler F, 2009, J NEUROPHYSIOL, V102, P953, DOI 10.1152/jn.90651.2008
   Op De Beeck H, 2000, J COMP NEUROL, V426, P505, DOI 10.1002/1096-9861(20001030)426:4<505::AID-CNE1>3.0.CO;2-M
   Perrinet L, 2001, NEUROCOMPUTING, V38, P817, DOI 10.1016/S0925-2312(01)00460-X
   Perrinet L., 2003, THESIS U P SABATIER
   Quiroga RQ, 2005, NATURE, V435, P1102, DOI 10.1038/nature03687
   Riesenhuber M, 1999, NAT NEUROSCI, V2, P1019, DOI 10.1038/14819
   Rolls E. T., 1998, NEURAL NETWORKS BRAI
   Rolls ET, 1997, EXP BRAIN RES, V114, P149, DOI 10.1007/PL00005615
   Rolls ET, 2000, NEURAL COMPUT, V12, P2547, DOI 10.1162/089976600300014845
   Síma J, 2003, NEURAL COMPUT, V15, P2727, DOI 10.1162/089976603322518731
   Simoncelli EP, 2003, CURR OPIN NEUROBIOL, V13, P144, DOI 10.1016/S0959-4388(03)00047-3
   Stringer SM, 2006, BIOL CYBERN, V94, P128, DOI 10.1007/s00422-005-0030-z
   Stringer SM, 2000, NEURAL NETWORKS, V13, P305, DOI 10.1016/S0893-6080(00)00017-4
   Tanaka K, 1996, NEURAL NETWORKS, V9, P1459, DOI 10.1016/S0893-6080(96)00045-7
   TANAKA K, 1991, J NEUROPHYSIOL, V66, P170, DOI 10.1152/jn.1991.66.1.170
   Thorpe S, 1996, NATURE, V381, P520, DOI 10.1038/381520a0
   Thorpe SJ, 2000, ISCAS 2000: IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS - PROCEEDINGS, VOL IV, P405, DOI 10.1109/ISCAS.2000.858774
   TOVEE MJ, 1994, J NEUROPHYSIOL, V72, P1049, DOI 10.1152/jn.1994.72.3.1049
   Troyer TW, 1998, J NEUROSCI, V18, P5908
   van Hateren JH, 1998, P ROY SOC B-BIOL SCI, V265, P359, DOI 10.1098/rspb.1998.0303
   van Rossum MCW, 2000, J NEUROSCI, V20, P8812
   Van Rullen R, 2001, NEURAL COMPUT, V13, P1255, DOI 10.1162/08997660152002852
   Wallis G, 1997, PROG NEUROBIOL, V51, P167, DOI 10.1016/S0301-0082(96)00054-8
NR 55
TC 18
Z9 18
U1 0
U2 11
PD JUL 25
PY 2012
VL 6
AR 46
DI 10.3389/fncom.2012.00046
UT WOS:000306699100001
DA 2023-11-16
ER

PT J
AU Shrestha, A
   Ahmed, K
   Wang, YZ
   Widemann, DP
   Moody, AT
   Van Essen, BC
   Qiu, QR
AF Shrestha, Amar
   Ahmed, Khadeer
   Wang, Yanzhi
   Widemann, David P.
   Moody, Adam T.
   Van Essen, Brian C.
   Qiu, Qinru
TI Modular Spiking Neural Circuits for Mapping Long Short-Term Memory on a
   Neurosynaptic Processor
SO IEEE JOURNAL ON EMERGING AND SELECTED TOPICS IN CIRCUITS AND SYSTEMS
DT Article; Proceedings Paper
CT IEEE/ACM 36th International Conference on Computer-Aided Design (ICCAD)
CY NOV 13-16, 2017
CL Irvine, CA
DE Spiking neural networks; recurrent neural networks; long short-term
   memory; neuromorphic hardware
ID NETWORKS
AB Due to the distributed and asynchronous nature of neural computation through low-energy spikes, brain-inspired hardware systems offer high energy efficiency and massive parallelism. One such platform is the IBM TrueNorth neurosynaptic system. Recently, TrueNorth compatible representation learning algorithms have emerged, achieving close to the state-of-the-art performance in various data sets. However, its application in temporal sequence processing models, such as recurrent neural networks (RNNs), is still only at the proof of concept level. There is an inherent difficulty in capturing temporal dynamics of an RNN using spiking neurons, which is only exasperated by the hardware constraints in connectivity and synaptic weight resolution. This paper presents a design flow that overcomes these difficulties and maps a special case of recurrent networks called long short-term memory (LSTM) onto a spike-based platform. The framework utilizes various approximation techniques, such as activation discretization, weight quantization, and scaling and rounding, spiking neural circuits that implement the complex gating mechanisms, and a store-and-release technique to enable neuron synchronization and faithful storage. While the presented techniques can be applied to map LSTM to any spiking neural network (SNN) simulator/emulator, here we choose the TrueNorth chip as the target platform by adhering to its hardware constraints. Three LSTM applications, parity check, extended Reber grammar, and question classification, are evaluated. The tradeoffs among accuracy, performance, and energy tradeoffs achieved on TrueNorth are demonstrated. This is compared with the performance on an SNN platform without hardware constraints, which represents the upper bound of the achievable accuracy.
C1 [Shrestha, Amar; Ahmed, Khadeer; Wang, Yanzhi; Qiu, Qinru] Syracuse Univ, Dept Elect Engn & Comp Sci, Syracuse, NY 13224 USA.
   [Ahmed, Khadeer] Synopsys, Mountain View, CA 94043 USA.
   [Widemann, David P.; Moody, Adam T.; Van Essen, Brian C.] Lawrence Livermore Natl Lab, Livermore, CA 94551 USA.
RP Shrestha, A (corresponding author), Syracuse Univ, Dept Elect Engn & Comp Sci, Syracuse, NY 13224 USA.
EM amshrest@syr.edu; kahmed@syr.edu; ywang393@syr.edu; widemann1@llnl.gov;
   moody20@llnl.gov; vanessen1@llnl.gov; qiqiu@syr.edu
CR Ahmed K, 2016, IEEE IJCNN, P4286, DOI 10.1109/IJCNN.2016.7727759
   Amir A, 2013, IEEE IJCNN
   [Anonymous], 2015, DRAW RECURRENT NEURA
   [Anonymous], 2016, BINARIZED NEURAL NET
   [Anonymous], EFFECTIVE QUANTIZATI
   [Anonymous], 2015, BENCHMARKING LSTM NE
   [Anonymous], RECURRENT NEURAL NET
   [Anonymous], 2016, QUANTIZED NEURAL NET
   [Anonymous], 2015, TENSORFLOW LARGE SCA
   [Anonymous], 2016, 2016 IEE INT C REB C, DOI [10.1109/ICRC.2016.7738691, DOI 10.1109/ICRC.2016.7738691]
   [Anonymous], 2016, TERNARY WEIGHT NETWO
   [Anonymous], 2015, IMPROVING PERFORMANC
   BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181
   Bengio Yoshua, 2013, ABS13083432 CORR
   Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Cassidy AS, 2014, INT CONF HIGH PERFOR, P27, DOI 10.1109/SC.2014.8
   Chen QW, 2017, DES AUT TEST EUROPE, P205, DOI 10.23919/DATE.2017.7926983
   Chung J., 2014, ARXIV
   Diehl P.U., 2015, 2015 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2015.7280696
   Diehl PU, 2016, IEEE IJCNN, P4278, DOI 10.1109/IJCNN.2016.7727758
   Esser SK, 2016, P NATL ACAD SCI USA, V113, P11441, DOI 10.1073/pnas.1604850113
   Gers FA, 2003, J MACH LEARN RES, V3, P115, DOI 10.1162/153244303768966139
   Hinton G., 2012, RMSPROP DIVIDE GRADI
   Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.1.1, 10.1007/978-3-642-24797-2]
   Khan MM, 2008, IEEE IJCNN, P2849, DOI 10.1109/IJCNN.2008.4634199
   Laudani A, 2015, COMPUT INTEL NEUROSC, V2015, DOI 10.1155/2015/818243
   Li X., 2002, ACL, DOI DOI 10.3115/1072228.1072378
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Pennington J., 2014, P 2014 C EMP METH NA, P1532
   Preissl R, 2012, INT CONF HIGH PERFOR
   Shrestha A, 2017, ICCAD-IEEE ACM INT, P631, DOI 10.1109/ICCAD.2017.8203836
   Sutskever I., 2014, CORR, P3104
   Venugopalan S, 2015, IEEE I CONF COMP VIS, P4534, DOI 10.1109/ICCV.2015.515
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Zhou SC, 2017, J COMPUT SCI TECH-CH, V32, P667, DOI 10.1007/s11390-017-1750-y
NR 35
TC 5
Z9 5
U1 1
U2 16
PD DEC
PY 2018
VL 8
IS 4
BP 782
EP 795
DI 10.1109/JETCAS.2018.2856117
UT WOS:000454224200010
DA 2023-11-16
ER

PT C
AU Kulkarni, SR
   Alexiades, JM
   Rajendran, B
AF Kulkarni, Shruti R.
   Alexiades, John M.
   Rajendran, Bipin
GP IEEE
TI Live Demonstration: Image Classification Using Bio-inspired Spiking
   Neural Networks
SO 2018 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS (ISCAS)
SE IEEE International Symposium on Circuits and Systems
DT Proceedings Paper
CT IEEE International Symposium on Circuits and Systems (ISCAS)
CY MAY 27-30, 2018
CL Florence, ITALY
AB We present a live demonstration of an image classification system using bio-inspired Spiking Neural Networks. Our network is three-layered and is trained with the images from the MNIST database, achieving an accuracy of 98:06%. Synapses connecting the output layer neurons obey the spike based weight-adaptation rule using the supervised learning algorithm called NormAD. This network, implemented on a graphical processing unit (GPU), is used to classify digits drawn by users on a touch-screen interface in real-time. The spike propagation maps generated and displayed by the platform reveal key insights about information processing mechanisms of the brain.
C1 [Kulkarni, Shruti R.; Alexiades, John M.; Rajendran, Bipin] New Jersey Inst Technol, Dept Elect & Comp Engn, Newark, NJ 07102 USA.
RP Rajendran, B (corresponding author), New Jersey Inst Technol, Dept Elect & Comp Engn, Newark, NJ 07102 USA.
EM bipin@njit.edu
CR Kulkarni SR, 2017, IEEE I C ELECT CIRC, P128, DOI 10.1109/ICECS.2017.8292015
NR 1
TC 0
Z9 0
U1 0
U2 1
PY 2018
DI 10.1109/ISCAS.2018.8351810
UT WOS:000451218704038
DA 2023-11-16
ER

PT C
AU Slepova, LO
   Skripnik, TN
AF Slepova, Liudmila O.
   Skripnik, Tatiana N.
GP IEEE
TI Neurotechnologies in Signal Processing for Implementation of Interaction
   Between Electronic Sensors and the Nervous System
SO PROCEEDINGS OF THE 2019 IEEE CONFERENCE OF RUSSIAN YOUNG RESEARCHERS IN
   ELECTRICAL AND ELECTRONIC ENGINEERING (EICONRUS)
SE IEEE NW Russia Young Researchers in Electrical and Electronic
   Engineering Conference
DT Proceedings Paper
CT IEEE Conference of Russian Young Researchers in Electrical and
   Electronic Engineering (EIConRus)
CY JAN 28-31, 2019
CL St Petersburg Electrotechn Univ, RUSSIA
HO St Petersburg Electrotechn Univ
DE action potential; biological model; odor classification; spiking neural
   network; pattern recognition; STDP; neural prostheses
AB Over the years of research and development in the field of neurotechnologies, a great variety of artificial neural networks has been created, to some extent simulating the functionality of biological neural systems. The neuron of the spike neural network is considered. The priority areas of use of this type of neural networks are: spatial navigation and environmental analysis; analysis of smells and dynamic graphic and sound information; neuroprosthetics and neural interfaces. There are a huge number of simplified neural models, but most of them still require large resources. The paper discusses the main advantages and disadvantages of the considered neural networks.
C1 [Slepova, Liudmila O.] ITMO Univ, Fac Control Syst & Robot, St Petersburg, Russia.
   [Skripnik, Tatiana N.] St Petersburg State Marine Tech Univ, Dept Marine Elect, St Petersburg, Russia.
RP Skripnik, TN (corresponding author), St Petersburg State Marine Tech Univ, Dept Marine Elect, St Petersburg, Russia.
EM marine_electronics@corp.smtu.ru
CR Dang B, 2018, AIP CONF PROC, V2034, DOI 10.1063/1.5067350
   Ivanov AV, 2018, PROCEEDINGS OF THE 2018 IEEE CONFERENCE OF RUSSIAN YOUNG RESEARCHERS IN ELECTRICAL AND ELECTRONIC ENGINEERING (EICONRUS), P882, DOI 10.1109/EIConRus.2018.8317229
   Kotlyarevskaya MV, 2018, PROCEEDINGS OF THE 2018 IEEE CONFERENCE OF RUSSIAN YOUNG RESEARCHERS IN ELECTRICAL AND ELECTRONIC ENGINEERING (EICONRUS), P894, DOI 10.1109/EIConRus.2018.8317232
   Popov AV, 2018, PROCEEDINGS OF THE 2018 IEEE CONFERENCE OF RUSSIAN YOUNG RESEARCHERS IN ELECTRICAL AND ELECTRONIC ENGINEERING (EICONRUS), P962, DOI 10.1109/EIConRus.2018.8317249
   Slepova LO, 2018, PROCEEDINGS OF THE 2018 IEEE CONFERENCE OF RUSSIAN YOUNG RESEARCHERS IN ELECTRICAL AND ELECTRONIC ENGINEERING (EICONRUS), P992, DOI 10.1109/EIConRus.2018.8317256
   Zhilenkov Anton, 2016, Vibroengineering Procedia. 22nd International Conference on Vibroengineering, P17
   Zhilenkov AA, 2018, PROCEEDINGS OF THE 2018 IEEE CONFERENCE OF RUSSIAN YOUNG RESEARCHERS IN ELECTRICAL AND ELECTRONIC ENGINEERING (EICONRUS), P1040, DOI 10.1109/EIConRus.2018.8317267
   Zhilenkov AA, 2017, IEEE NW RUSS YOUNG, P1100, DOI 10.1109/EIConRus.2017.7910747
NR 8
TC 0
Z9 0
U1 0
U2 1
PY 2019
BP 1237
EP 1239
DI 10.1109/eiconrus.2019.8657125
UT WOS:000469452600294
DA 2023-11-16
ER

PT J
AU Wang, RCM
   Hamilton, TJ
   Tapson, JC
   van Schaik, A
AF Wang, Runchun M.
   Hamilton, Tara J.
   Tapson, Jonathan C.
   van Schaik, Andre
TI A mixed-signal implementation of a polychronous spiking neural network
   with delay adaptation
SO FRONTIERS IN NEUROSCIENCE
DT Article
DE mixed-signal implementation; polychronous spiking neural network; analog
   implementation; multiplexed neuron array; neuromorphic engineering
ID COMPUTATION; MODEL; CMOS
AB We present a mixed-signal implementation of a re-configurable polychronous spiking neural network capable of storing and recalling spatio-temporal patterns. The proposed neural network contains one neuron array and one axon array. Spike Timing Dependent Delay Plasticity is used to fine-tune delays and add dynamics to the network. In our mixed-signal implementation, the neurons and axons have been implemented as both analog and digital circuits. The system thus consists of one FPGA, containing the digital neuron array and the digital axon array, and one analog IC containing the analog neuron array and the analog axon array. The system can be easily configured to use different combinations of each. We present and discuss the experimental results of all combinations of the analog and digital axon arrays and the analog and digital neuron arrays. The test results show that the proposed neural network is capable of successfully recalling more than 85% of stored patterns using both analog and digital circuits.
C1 [Wang, Runchun M.; Hamilton, Tara J.; Tapson, Jonathan C.; van Schaik, Andre] Univ Western Sydney, MARCS Inst, Sydney, NSW, Australia.
RP Wang, RCM (corresponding author), Univ Western Sydney, MARCS Inst, Locked Bag 1797, Penrith, NSW 2751, Australia.
EM mark.wang@uws.edu.au
CR [Anonymous], 2012, IEEE INT JOINT C NEU, DOI [DOI 10.1109/IJCNN.2012.6252636, DOI 10.5167/UZH-75361]
   Arthur JV, 2004, IEEE IJCNN, P1699
   Basu A, 2010, IEEE T BIOMED CIRC S, V4, P311, DOI 10.1109/TBCAS.2010.2055157
   Boahen KA, 2000, IEEE T CIRCUITS-II, V47, P416, DOI 10.1109/82.842110
   Brink S, 2013, IEEE T BIOMED CIRC S, V7, P71, DOI 10.1109/TBCAS.2012.2197858
   Cauwenberghs G, 1996, ISCAS 96: 1996 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS - CIRCUITS AND SYSTEMS CONNECTING THE WORLD, VOL 3, P334, DOI 10.1109/ISCAS.1996.541601
   Dowrick T, 2013, NEUROCOMPUTING, V108, P79, DOI 10.1016/j.neucom.2012.12.004
   Gao CJ, 2007, IEEE T CIRCUITS-I, V54, P2502, DOI 10.1109/TCSI.2007.907830
   Gerstner W, 1996, NATURE, V383, P76, DOI 10.1038/383076a0
   Goldberg DH, 2001, NEURAL NETWORKS, V14, P781, DOI 10.1016/S0893-6080(01)00057-0
   Harkin J, 2009, INT J RECONFIGURABLE, V2009, DOI 10.1155/2009/908740
   Harkin J, 2008, I C FIELD PROG LOGIC, P482
   Hasler J, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00118
   HORIO Y, 1990, 1990 IEEE INTERNATIONAL SYMP ON CIRCUITS AND SYSTEMS, VOLS 1-4, P2986, DOI 10.1109/ISCAS.1990.112638
   Hussain S., NEUROCOMPUT IN PRESS, P1
   Hussain S, 2012, 2012 IEEE ASIA PACIFIC CONFERENCE ON CIRCUITS AND SYSTEMS (APCCAS), P304, DOI 10.1109/APCCAS.2012.6419032
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Johansson C, 2007, NEURAL NETWORKS, V20, P48, DOI 10.1016/j.neunet.2006.05.029
   Levy WB, 1996, NEURAL COMPUT, V8, P531, DOI 10.1162/neco.1996.8.3.531
   Liu S.C., 2002, ANALOG VLSI CIRCUITS
   Masuda N, 2003, NEURAL COMPUT, V15, P103, DOI 10.1162/089976603321043711
   Mihalas S, 2009, NEURAL COMPUT, V21, P704, DOI 10.1162/neco.2008.12-07-680
   Minkovich K, 2012, IEEE T NEUR NET LEAR, V23, P889, DOI 10.1109/TNNLS.2012.2191795
   Mirhassani M, 2007, MICROELECTRON ENG, V84, P300, DOI 10.1016/j.mee.2006.02.014
   Python D, 2001, IEEE J SOLID-ST CIRC, V36, P1067, DOI 10.1109/4.933462
   Saighi S., 2010, 2010 INT JOINT C NEU, P1, DOI [10.1109/IJCNN.2010.5596979, DOI 10.1109/IJCNN.2010.5596979]
   Schemmel J, 2008, IEEE IJCNN, P431, DOI 10.1109/IJCNN.2008.4633828
   Scholze S, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00117
   Sheik Sadique, 2013, Biomimetic and Biohybrid Systems. Second International Conference, Living Machines 2013. Proceedings. LNCS 8064, P262, DOI 10.1007/978-3-642-39802-5_23
   Van Rullen R, 2001, NEURAL COMPUT, V13, P1255, DOI 10.1162/08997660152002852
   Vogelstein RJ, 2007, IEEE T NEURAL NETWOR, V18, P253, DOI 10.1109/TNN.2006.883007
   Wang R., 2013, 2013 IEEE INT S CIRC, P2
   Wang R., 2011, 2011 7 INT C INT SEN, P97, DOI [10.1109/ISSNIP.2011.6146572, DOI 10.1109/ISSNIP.2011.6146572]
   Wang RC, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00014
   Weste N. H. E., 2005, CMOS VLSI DESIGN CIR, V3rd
   Yu T, 2010, IEEE INT SYMP CIRC S, P2558, DOI 10.1109/ISCAS.2010.5537114
   Zaveri MS, 2011, NEURAL NETWORKS, V24, P291, DOI 10.1016/j.neunet.2010.12.003
NR 38
TC 21
Z9 22
U1 1
U2 1
PD MAR 18
PY 2014
VL 8
DI 10.3389/fnins.2014.00051
UT WOS:000346435500002
DA 2023-11-16
ER

PT C
AU Sporea, I
   Grüning, A
AF Sporea, Ioana
   Gruening, Andre
GP IEEE
TI Reference time in SpikeProp
SO 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUL 31-AUG 05, 2011
CL San Jose, CA
ID SPIKING NEURONS; NETWORKS
AB Although some studies have been done on the learning algorithm for spiking neural networks SpikeProp, little has been mentioned about the required input bias neuron that sets the reference time start. This paper examines the importance of the reference time in neural networks based on temporal encoding. The findings refute previous assumptions about the reference start time.
C1 [Sporea, Ioana; Gruening, Andre] Univ Surrey, Dept Comp, FEPS, Guildford GU2 7XH, Surrey, England.
RP Sporea, I (corresponding author), Univ Surrey, Dept Comp, FEPS, Guildford GU2 7XH, Surrey, England.
EM i.nica@surrey.ac.uk; a.gruning@surrey.ac.uk
CR [Anonymous], P 15 PRORISC WORKSH
   [Anonymous], 2001, HDB BIOL PHYS
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Booij O, 2005, INFORM PROCESS LETT, V95, P552, DOI 10.1016/j.ipl.2005.05.023
   Fujita M., 2008, P IEEE INT JOINT C N, P840
   Maass W, 1997, NEURAL COMPUT, V9, P279, DOI 10.1162/neco.1997.9.2.279
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maass W, 1997, ADV NEUR IN, V9, P211
   McKennoch S, 2006, IEEE IJCNN, P3970
   Minsky M., 1969, PERCEPTRONS INTRO CO
   MOORE S, 2002, THESIS U BATH
   Rojas R, 1996, NEURAL NETWORKS, P149, DOI 10.1007/978-3-642-61068-4{\_}7
   Takase Haruhiko, 2009, Proceedings 2009 International Joint Conference on Neural Networks (IJCNN 2009 - Atlanta), P3062, DOI 10.1109/IJCNN.2009.5178756
   THORPE SJ, 1989, CONNECTIONISM IN PERSPECTIVE, P63
   Xin JG, 2001, IEEE IJCNN, P1772, DOI 10.1109/IJCNN.2001.938430
NR 15
TC 2
Z9 2
U1 0
U2 0
PY 2011
BP 1090
EP 1092
UT WOS:000297541201031
DA 2023-11-16
ER

PT C
AU Camuñas-Mesa, LA
   Linares-Barranco, B
   Serrano-Gotarredona, T
AF Camunas-Mesa, Luis A.
   Linares-Barranco, Bernabe
   Serrano-Gotarredona, Teresa
GP IEEE
TI Implementation of a tunable spiking neuron for STDP with memristors in
   FDSOI 28nm
SO 2020 2ND IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE
   CIRCUITS AND SYSTEMS (AICAS 2020)
DT Proceedings Paper
CT 2nd IEEE International Conference on Artificial Intelligence Circuits
   and Systems (AICAS)
CY AUG 31-SEP 04, 2020
CL ELECTR NETWORK
DE Spiking Neural Networks; Event-driven processing; STDP; Memristors
ID ARCHITECTURE
AB Hybrid memristor-CMOS techniques have been recently proposed to build large-scale neural networks with learning capabilities. The intrinsic characteristics of memristors make them specially suited to implement synaptic connections between layers of spiking neurons, undergoing STDP learning (Spike-Timing-Dependent Plasticity) mechanisms when processing spikes with particular shapes. In a previous work, we proposed a tunable spiking neuron circuit which can generate spikes with controllable shape. In this work, the spike generator circuit has been implemented in FDSOI 28nm technology, and it has demonstrated its capability to produce spikes with pulse widths in the range between 8 mu s and 100ms.
C1 [Camunas-Mesa, Luis A.; Linares-Barranco, Bernabe; Serrano-Gotarredona, Teresa] CSIC, Inst Microelect Sevilla IMSE CNM, Seville, Spain.
   [Camunas-Mesa, Luis A.; Linares-Barranco, Bernabe; Serrano-Gotarredona, Teresa] Univ Seville, Seville, Spain.
RP Camuñas-Mesa, LA (corresponding author), CSIC, Inst Microelect Sevilla IMSE CNM, Seville, Spain.; Camuñas-Mesa, LA (corresponding author), Univ Seville, Seville, Spain.
EM camunas@imse-cnm.csic.es; bernabe@imse-cnm.csic.es;
   terese@imse-cnm.csic.es
CR Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Bouvier M, 2019, ACM J EMERG TECH COM, V15, DOI 10.1145/3304103
   Camuñas-Mesa LA, 2019, MATERIALS, V12, DOI 10.3390/ma12172745
   CHUA LO, 1971, IEEE T CIRCUITS SYST, VCT18, P507, DOI 10.1109/TCT.1971.1083337
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Frenkel C, 2019, IEEE T BIOMED CIRC S, V13, P145, DOI 10.1109/TBCAS.2018.2880425
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   GERSTNER W, 1993, BIOL CYBERN, V69, P503, DOI 10.1007/BF01185422
   Indiveri G, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00073
   Linares-Barranco B., 2009, NATURE PRECEDINGS
   Masquelier T, 2008, PLOS ONE, V3, DOI 10.1371/journal.pone.0001377
   Masquelier T, 2009, NEURAL COMPUT, V21, P1259, DOI 10.1162/neco.2008.06-08-804
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Moradi S, 2018, IEEE T BIOMED CIRC S, V12, P106, DOI 10.1109/TBCAS.2017.2759700
   Neckar A, 2019, P IEEE, V107, P144, DOI 10.1109/JPROC.2018.2881432
   Schemmel J, 2010, IEEE INT SYMP CIRC S, P1947, DOI 10.1109/ISCAS.2010.5536970
   Serrano-Gotarredona T, 2012, IEEE I C ELECT CIRC, P949, DOI 10.1109/ICECS.2012.6463504
   Shen JC, 2016, SCI CHINA INFORM SCI, V59, DOI 10.1007/s11432-015-5511-7
   Strukov DB, 2008, NATURE, V453, P80, DOI 10.1038/nature06932
   Zamarreño-Ramos C, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00026
NR 21
TC 3
Z9 3
U1 1
U2 2
PY 2020
BP 94
EP 98
DI 10.1109/aicas48895.2020.9073994
UT WOS:000720328700020
DA 2023-11-16
ER

PT J
AU Fidjeland, AK
   Gamez, D
   Shanahan, MP
   Lazdins, E
AF Fidjeland, Andreas K.
   Gamez, David
   Shanahan, Murray P.
   Lazdins, Edgars
TI Three Tools for the Real-Time Simulation of Embodied Spiking Neural
   Networks Using GPUs
SO NEUROINFORMATICS
DT Article
DE Simulation; 3D visualization; Spike encoding; Robotics; Spiking neural
   networks; iCub; GPU
ID 2-DIMENSIONAL LIMB MOVEMENTS; MUSCLE-SPINDLE FEEDBACK; NEURONS; BRAIN;
   INFORMATION; HUMANS; MODEL; SKIN; ARCHITECTURE; AFFERENTS
AB This paper presents a toolbox of solutions that enable the user to construct biologically-inspired spiking neural networks with tens of thousands of neurons and millions of connections that can be simulated in real time, visualized in 3D and connected to robots and other devices. NeMo is a high performance simulator that works with a variety of neural and oscillator models and performs parallel simulations on either GPUs or multi-core processors. SpikeStream is a visualization and analysis environment that works with NeMo and can construct networks, store them in a database and visualize their activity in 3D. The iSpike library provides biologically-inspired conversion between real data and spike representations to support work with robots, such as the iCub. Each of the tools described in this paper can be used independently with other software, and they also work well together.
C1 [Fidjeland, Andreas K.; Gamez, David; Shanahan, Murray P.; Lazdins, Edgars] Univ London Imperial Coll Sci Technol & Med, London SW7 2AZ, England.
RP Fidjeland, AK (corresponding author), Univ London Imperial Coll Sci Technol & Med, 180 Queens Gate, London SW7 2AZ, England.
EM andreas.fidjeland@imperial.ac.uk
CR Aleksander I., 2005, WORLD MY MIND MY MIN
   Anand R, 2009, SELF-DEFENSE IN INTERNATIONAL RELATIONS, P1, DOI 10.1057/9780230245747
   Andreou A., 1994, MIDW S CIRC SYST, P97
   [Anonymous], 2011, FRONT NEUROINFORM
   Bergenheim M, 2000, EXP BRAIN RES, V134, P301, DOI 10.1007/s002210000471
   Bernardet U, 2010, NEUROINFORMATICS, V8, P113, DOI 10.1007/s12021-010-9069-7
   Bernhard F, 2006, LECT NOTES COMPUT SC, V3994, P236
   Bhowmik D., 2012, P INT JOINT C NEUR N
   Binzegger T, 2004, J NEUROSCI, V24, P8441, DOI 10.1523/JNEUROSCI.1400-04.2004
   Bolduc M, 1998, COMPUT VIS IMAGE UND, V69, P170, DOI 10.1006/cviu.1997.0560
   Bouganis A., 2010, P IEEE INT JOINT C N, P4104
   Bower J. M., 2003, HDB BRAIN THEORY NEU, P475
   Brette R, 2007, J COMPUT NEUROSCI, V23, P349, DOI 10.1007/s10827-007-0038-6
   Buchmann T., 2011, THESIS IMPERIAL COLL
   Cannata Giorgio, 2008, 2008 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems (MFI 2008), P434, DOI 10.1109/MFI.2008.4648033
   Carnevale N.T., 2006, NEURON BOOK, DOI DOI 10.1017/CBO9780511541612
   Clark A., 2008, SUPERSIZING MIND EMB
   Collins DF, 1996, J PHYSIOL-LONDON, V496, P857, DOI 10.1113/jphysiol.1996.sp021733
   Collins DF, 2005, J NEUROPHYSIOL, V94, P1699, DOI 10.1152/jn.00191.2005
   Davison A.P., 2008, FRONTIERS NEUROINFOR, V2
   Djurfeldt M, 2008, IBM J RES DEV, V52, P31, DOI 10.1147/rd.521.0031
   EDIN BB, 1995, J PHYSIOL-LONDON, V487, P243, DOI 10.1113/jphysiol.1995.sp020875
   ENROTHCUGELL C, 1966, J PHYSIOL-LONDON, V187, P517, DOI 10.1113/jphysiol.1966.sp008107
   FERRELL WR, 1987, J PHYSIOL-LONDON, V386, P63, DOI 10.1113/jphysiol.1987.sp016522
   Fidjeland A.K., 2011, NEMO MANUAL
   Fidjeland AK, 2010, P IEEE WORLD C COMP, P536
   Fidjeland AK, 2009, IEEE INT CONF ASAP, P137, DOI 10.1109/ASAP.2009.24
   Fitzpatrick P, 2008, ROBOT AUTON SYST, V56, P29, DOI 10.1016/j.robot.2007.09.014
   Fountas Z, 2011, IEEE CONF COMPU INTE, P350, DOI 10.1109/CIG.2011.6032027
   Gamez D, 2012, BIOINSPIR BIOMIM, V7, DOI 10.1088/1748-3182/7/2/025008
   Gamez D., 2011, ISPIKE MANUAL
   Gamez D, 2006, P IEEE 5 CHAPT C ADV, P85
   Gamez D., 2011, SPIKESTREAM MANUAL
   Gamez D, 2007, LECT NOTES COMPUT SC, V4668, P360
   Gamez D, 2011, CONSCIOUS COGN, V20, P1403, DOI 10.1016/j.concog.2011.05.016
   Gamez D, 2010, CONSCIOUS COGN, V19, P294, DOI 10.1016/j.concog.2009.11.001
   GEORGOPOULOS AP, 1986, SCIENCE, V233, P1416, DOI 10.1126/science.3749885
   Goodman DFM, 2010, NEUROINFORMATICS, V8, P183, DOI 10.1007/s12021-010-9082-x
   Goodman DFM, 2009, FRONT NEUROSCI-SWITZ, V3, P192, DOI 10.3389/neuro.01.026.2009
   Grill-Spector K, 2004, ANNU REV NEUROSCI, V27, P649, DOI 10.1146/annurev.neuro.27.070203.144220
   Grillner S, 2005, TRENDS NEUROSCI, V28, P364, DOI 10.1016/j.tins.2005.05.004
   Hagmann P, 2008, PLOS BIOL, V6, P1479, DOI 10.1371/journal.pbio.0060159
   Hammarlund P, 1998, J COMPUT NEUROSCI, V5, P443, DOI 10.1023/A:1008893429695
   Han B., 2010, P INT JOINT C NEUR N, P3050
   Hellwig B, 2000, BIOL CYBERN, V82, P111, DOI 10.1007/PL00007964
   Hoshi T, 2006, IEEE INT CONF ROBOT, P3463, DOI 10.1109/ROBOT.2006.1642231
   Ijspeert AJ, 2007, SCIENCE, V315, P1416, DOI 10.1126/science.1138353
   Indiveri G, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00073
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2008, P NATL ACAD SCI USA, V105, P3593, DOI 10.1073/pnas.0712231105
   Jones KE, 2001, J PHYSIOL-LONDON, V536, P635, DOI 10.1111/j.1469-7793.2001.0635c.xd
   JURGENS R, 1981, BIOL CYBERN, V39, P87, DOI 10.1007/BF00336734
   Kit Cheung, 2009, Proceedings of the 2009 International Conference on Field-Programmable Technology (FPT 2009), P247, DOI 10.1109/FPT.2009.5377667
   Krichmar JL, 2005, P NATL ACAD SCI USA, V102, P2111, DOI 10.1073/pnas.0409792102
   Kuramoto Y., 1984, CHEM OSCILLATIONS WA
   Linares-Barranco A, 2007, IEEE INT SYMP CIRC S, P1192, DOI 10.1109/ISCAS.2007.378265
   Liu JD, 2006, INT J AUTOM COMPUT, V3, P336, DOI 10.1007/s11633-006-0336-x
   Lyon R. F., 1982, Proceedings of ICASSP 82. IEEE International Conference on Acoustics, Speech and Signal Processing, P1282
   MACEFIELD G, 1990, J PHYSIOL-LONDON, V429, P113, DOI 10.1113/jphysiol.1990.sp018247
   Maguire LP, 2007, NEUROCOMPUTING, V71, P13, DOI 10.1016/j.neucom.2006.11.029
   Marc-Oliver G., 2007, SCHOLARPEDIA, DOI [DOI 10.4249/SCHOLARPEDIA.1430, 10.4249/scholarpedia.1430]
   Markram H, 2006, NAT REV NEUROSCI, V7, P153, DOI 10.1038/nrn1848
   Marques H., 2010, INT C HUM ROB, P391, DOI DOI 10.1109/ICHR.2010.5686344
   Masquelier T, 2007, PLOS COMPUT BIOL, V3, P247, DOI 10.1371/journal.pcbi.0030031
   Metta G., 2008, P WORKSH PERF METR I
   Meuth Ryan J., 2007, 22nd IEEE International Symposium on Intelligent Control, ISIC 2007. Part of IEEE Multi-conference on Systems and Control, P524, DOI 10.1109/ISIC.2007.4450940
   Morrison A, 2008, BIOL CYBERN, V98, P459, DOI 10.1007/s00422-008-0233-1
   Nageswaran JM, 2009, NEURAL NETWORKS, V22, P791, DOI 10.1016/j.neunet.2009.06.028
   Noë A, 2004, J CONSCIOUSNESS STUD, V11, P3
   Nowotny T., 2011, BMC NEUROSCI, V12, P239
   Rast A, 2011, NEURAL NETWORKS, V24, P961, DOI 10.1016/j.neunet.2011.06.014
   Ribot-Ciscar E, 2003, EXP BRAIN RES, V149, P512, DOI 10.1007/s00221-003-1384-x
   ROBINSON DA, 1964, J PHYSIOL-LONDON, V174, P245, DOI 10.1113/jphysiol.1964.sp007485
   Roll JP, 2000, EXP BRAIN RES, V134, P311, DOI 10.1007/s002210000472
   Roll JP, 2004, EXP BRAIN RES, V157, P359, DOI 10.1007/s00221-004-1853-x
   Rossant C, 2011, FRONT NEUROSCI-SWITZ, V5, DOI [10.3389/fnins.2011.00009, 10.3389/fninf.2011.00009]
   Schemmel J, 2010, IEEE INT SYMP CIRC S, P1947, DOI 10.1109/ISCAS.2010.5536970
   SCHWARTZ EL, 1980, VISION RES, V20, P645, DOI 10.1016/0042-6989(80)90090-5
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Sporns O., 2007, SCHOLARPEDIA, V2, P4695, DOI [DOI 10.4249/SCHOLARPEDIA.4695, 10.4249/scholarpedia.4695]
   Thomas D.B., 2009, P IEEE S FIELD PROGR
   Tiesel JP, 2009, IEEE IJCNN, P754
   Tononi G, 2008, BIOL BULL-US, V215, P216, DOI 10.2307/25470707
   Vogels TP, 2005, J NEUROSCI, V25, P10786, DOI 10.1523/JNEUROSCI.3508-05.2005
   Yudanov D., 2010, P INT JOINT C NEUR N
NR 85
TC 4
Z9 4
U1 0
U2 8
PD JUL
PY 2013
VL 11
IS 3
BP 267
EP 290
DI 10.1007/s12021-012-9174-x
UT WOS:000322259300002
DA 2023-11-16
ER

PT J
AU Gholami, M
   Farsa, EZ
   Karimi, G
AF Gholami, Morteza
   Farsa, Edris Zaman
   Karimi, Gholamreza
TI Reconfigurable field-programmable gate array-based on-chip learning
   neuromorphic digital implementation for nonlinear function approximation
SO INTERNATIONAL JOURNAL OF CIRCUIT THEORY AND APPLICATIONS
DT Article
DE digital hardware; field-programmable gate array (FPGA); hardware
   implementation; neuromorphic; on-chip learning; spiking neural networks
   (SNNs)
ID SPIKING NEURAL-NETWORKS; LOW-ERROR; HARDWARE; MODEL; PROCESSOR; NEURONS;
   DESIGN
AB Hardware implementations of spiking neural networks, which are known as neuromorphic architectures, provide an explicit understanding of brain performance. As a result, biological features of the brain may well inspire the next generation of computers and electronic systems used in such areas as signal processing, image processing, function approximation, and pattern recognition. Approximating nonlinear functions has many uses in computer science and applied mathematics. The sigmoid is the most universal activation function in neural networks by which the relationship between biological and artificial neurons is defined. It is a suitable option for predicting the probability of anything from 0 to 1 as output. In this paper, a spiking neural network using Izhikevich neurons and a gradient descent learning algorithm are propounded to approximate the sigmoid and other nonlinear functions. The flexibility of the spiking network is demonstrated by showing the average relative errors in the approximation process. A time- and cost-efficient digital neuromorphic implementation on the base of on-chip learning method for approximating the sigmoid function is also discussed. The paper reports the results of the hardware synthesis and the spiking network's physical implementation on a field-programmable gate array. The maximum frequency and throughput of the implemented network were 83.209 MHz and 9.86 Mb/s, respectively.
C1 [Gholami, Morteza; Karimi, Gholamreza] Razi Univ, Elect Engn Dept, Fac Engn, Kermanshah, Iran.
   [Farsa, Edris Zaman] Islamic Azad Univ, Dept Comp Engn, Sanandaj Branch, Sanandaj, Iran.
RP Karimi, G (corresponding author), Razi Univ, Elect Engn Dept, Fac Engn, Kermanshah, Iran.
EM ghkarimi@razi.ac.ir
CR Abdoli B, 2020, INT J CIRC THEOR APP, V48, P2141, DOI 10.1002/cta.2877
   Amin HH, 2005, LECT NOTES COMPUT SC, V3644, P621
   Amiri M, 2019, INT J CIRC THEOR APP, V47, P483, DOI 10.1002/cta.2596
   Armato A, 2011, MICROPROCESS MICROSY, V35, P557, DOI 10.1016/j.micpro.2011.05.007
   Asgari H, 2020, IEEE T CIRCUITS-II, V67, P2697, DOI 10.1109/TCSII.2020.2968588
   Asgari H, 2020, INT J CIRC THEOR APP, V48, P724, DOI 10.1002/cta.2753
   Azghadi MR, 2017, IEEE T BIOMED CIRC S, V11, P434, DOI 10.1109/TBCAS.2016.2618351
   Bajger M, 2008, J SIGNAL PROCESS SYS, V52, P137, DOI 10.1007/s11265-007-0140-z
   Basu A, 2010, IEEE T BIOMED CIRC S, V4, P311, DOI 10.1109/TBCAS.2010.2055157
   Bonabi SY, 2014, FRONT NEUROSCI-SWITZ, V8, DOI 10.3389/fnins.2014.00379
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   DeBole MV, 2019, COMPUTER, V52, P20, DOI 10.1109/MC.2019.2903009
   Farsa EZ, 2019, IEEE T CIRCUITS-II, V66, P1582, DOI 10.1109/TCSII.2019.2890846
   Farsa EZ, 2015, J COMPUT ELECTRON, V14, P707, DOI 10.1007/s10825-015-0709-x
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   Heidarpur M, 2019, IEEE T CIRCUITS-I, V66, P2651, DOI 10.1109/TCSI.2019.2899356
   Iannella N, 2001, NEURAL NETWORKS, V14, P933, DOI 10.1016/S0893-6080(01)00080-6
   Indiveri G, 2006, IEEE T NEURAL NETWOR, V17, P211, DOI 10.1109/TNN.2005.860850
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Karimi G, 2018, INT J CIRC THEOR APP, V46, P965, DOI 10.1002/cta.2457
   Kugele A, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00439
   Lammie C, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351532
   Ma D, 2017, J SYST ARCHITECT, V77, P43, DOI 10.1016/j.sysarc.2017.01.003
   Maguire LP, 2007, NEUROCOMPUTING, V71, P13, DOI 10.1016/j.neucom.2006.11.029
   Nawrocki RA, 2016, IEEE T ELECTRON DEV, V63, P3819, DOI 10.1109/TED.2016.2598413
   Neil D, 2014, IEEE T VLSI SYST, V22, P2621, DOI 10.1109/TVLSI.2013.2294916
   Pani D, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00090
   Ponulak F, 2011, ACTA NEUROBIOL EXP, V71, P409
   Rajendran B, 2019, IEEE SIGNAL PROC MAG, V36, P97, DOI 10.1109/MSP.2019.2933719
   Schumann CL, 2019, AIDS BEHAV, V23, P5, DOI 10.1007/s10461-017-1727-4
   Serrano-Gotarredona T, 2013, IEEE CIRC SYST MAG, V13, P74, DOI 10.1109/MCAS.2013.2256271
   Young AR, 2019, IEEE ACCESS, V7, P135606, DOI 10.1109/ACCESS.2019.2941772
NR 32
TC 8
Z9 8
U1 0
U2 12
PD AUG
PY 2021
VL 49
IS 8
BP 2425
EP 2435
DI 10.1002/cta.3075
EA JUN 2021
UT WOS:000663839200001
DA 2023-11-16
ER

PT J
AU Volanis, G
   Antonopoulos, A
   Makris, Y
   Hatzopoulos, AA
AF Volanis, Georgios
   Antonopoulos, Angelos
   Makris, Yiorgos
   Hatzopoulos, Alkis A.
TI Toward Silicon-Based Cognitive Neuromorphic ICs-A Survey
SO IEEE DESIGN & TEST
DT Article
DE Neurons; Integrated circuit modeling; Computers; Silicon; Neuromorphics;
   Biological neural networks
ID SPIKING NEURONS; NEURAL-NETWORKS; ON-CHIP; INTEGRATION; SYSTEM; ARRAY
C1 [Volanis, Georgios; Makris, Yiorgos] Univ Texas Dallas, Elect Engn, Richardson, TX 75080 USA.
   [Antonopoulos, Angelos] Univ Texas Dallas, Richardson, TX 75080 USA.
   [Hatzopoulos, Alkis A.] Aristotle Univ Thessaloniki, Dept Elect & Comp Engn, Elect Lab, GR-54006 Thessaloniki, Greece.
RP Volanis, G (corresponding author), Univ Texas Dallas, Dept Elect Engn, Richardson, TX 75080 USA.
EM gxv130830@utdallas.edu
CR Alvado L, 2004, NEUROCOMPUTING, V58, P109, DOI 10.1016/j.neucom.2004.01.030
   Ambrogio S, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00056
   [Anonymous], 1989, ANALOG VLSI NEURAL S
   [Anonymous], 2001, INT PROC WORLD C NEU
   [Anonymous], 1950, J CLIN PSYCHOL, V6, P307
   Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Brader JM, 2007, NEURAL COMPUT, V19, P2881, DOI 10.1162/neco.2007.19.11.2881
   Chan V, 2007, IEEE T CIRCUITS-I, V54, P48, DOI 10.1109/TCSI.2006.887979
   Chicca E, 2014, P IEEE, V102, P1367, DOI 10.1109/JPROC.2014.2313954
   Cruz-Albrecht JM, 2013, NANOTECHNOLOGY, V24, DOI 10.1088/0957-4484/24/38/384011
   Diehl PU, 2015, IEEE IJCNN
   Fong XY, 2016, IEEE T COMPUT AID D, V35, P1, DOI 10.1109/TCAD.2015.2481793
   Häfliger P, 2007, IEEE T NEURAL NETWOR, V18, P551, DOI 10.1109/TNN.2006.884676
   Hasler J, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00118
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Indiveri G, 2006, IEEE T NEURAL NETWOR, V17, P211, DOI 10.1109/TNN.2005.860850
   Indiveri G, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00118
   Indiveri G, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00073
   Indiveri G, 2009, COGN COMPUT, V1, P119, DOI 10.1007/s12559-008-9003-6
   Lichtsteiner P, 2008, IEEE J SOLID-ST CIRC, V43, P566, DOI 10.1109/JSSC.2007.914337
   MAHOWALD M, 1991, NATURE, V354, P515, DOI 10.1038/354515a0
   Maliuk D, 2015, IEEE T NEUR NET LEAR, V26, P1721, DOI 10.1109/TNNLS.2014.2354406
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Mitra S, 2009, IEEE T BIOMED CIRC S, V3, P32, DOI 10.1109/TBCAS.2008.2005781
   Mostafa H, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00357
   Painkras E, 2013, IEEE J SOLID-ST CIRC, V48, P1943, DOI 10.1109/JSSC.2013.2259038
   Poon CS, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00108
   Qiao N, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00141
   Rigotti M, 2010, NEUROIMAGE, V52, P833, DOI 10.1016/j.neuroimage.2010.01.047
   Schemmel J, 2008, IEEE IJCNN, P431, DOI 10.1109/IJCNN.2008.4633828
   Serrano-Gotarredona R, 2009, IEEE T NEURAL NETWOR, V20, P1417, DOI 10.1109/TNN.2009.2023653
   Thomas A, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00241
   van Schaik A, 2001, NEURAL NETWORKS, V14, P617, DOI 10.1016/S0893-6080(01)00067-3
   Vogelstein RJ, 2007, IEEE T NEURAL NETWOR, V18, P253, DOI 10.1109/TNN.2006.883007
   Walter F, 2015, NEURAL NETWORKS, V72, P152, DOI 10.1016/j.neunet.2015.07.004
   Wang RM, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00180
NR 36
TC 7
Z9 7
U1 0
U2 15
PD MAY-JUN
PY 2016
VL 33
IS 3
BP 91
EP 102
DI 10.1109/MDAT.2016.2545159
UT WOS:000375552800010
DA 2023-11-16
ER

PT J
AU Guo, SH
   Wang, L
   Chen, BZ
   Dou, Q
AF Guo, Shasha
   Wang, Lei
   Chen, Baozi
   Dou, Qiang
TI An Overhead-Free Max-Pooling Method for SNN
SO IEEE EMBEDDED SYSTEMS LETTERS
DT Article
DE Neurons; Microsoft Windows; Mathematical model; Training; Computational
   modeling; Biological neural networks; Task analysis; Approximate; max
   pooling; overhead; spiking neural network (SNN)
AB Spiking neural networks (SNNs) have been shown to be accurate, fast, and efficient in classical machine vision tasks, such as object recognition or detection. It is typical to convert a pretrained deep neural network into an SNN since training SNN is not easy. The max-pooling (MP) function is widely adopted in most state-of-the-art deep neural networks. To maintain the accuracy of the SNN obtained through conversion, this function is an important element to be implemented. However, it is difficult due to the dynamic characteristics of spikes. As far as we know, existing solutions adopt additional technologies except the spiking neuron model to implement MP or approximate MP, which introduce overhead of memory storage and computation. In this letter, we propose a novel method that utilizes only the spiking neuron model to approximate MP. Our method does not incur any overhead. We validate our method with three datasets and six networks including three oxford visual geometry group-like networks. And the experimental results show that the performance (accuracy and convergence rate) of our method is as good as or even better than that of the existing method.
C1 [Guo, Shasha; Wang, Lei; Chen, Baozi; Dou, Qiang] Natl Univ Def Technol, Coll Comp Sci & Technol, Changsha 410073, Peoples R China.
RP Guo, SH (corresponding author), Natl Univ Def Technol, Coll Comp Sci & Technol, Changsha 410073, Peoples R China.
EM guoshasha13@nudt.edu.cn; arrowya@gmail.com
CR [Anonymous], 2011, P NIPS WORKSHOP DEEP
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Chen RS, 2019, POSTGRAD MED, V131, P73, DOI 10.1080/00325481.2019.1552824
   Diehl PU, 2015, IEEE IJCNN
   Esser SK, 2016, P NATL ACAD SCI USA, V113, P11441, DOI 10.1073/pnas.1604850113
   Hu J, 2013, NEURAL COMPUT, V25, P450, DOI 10.1162/NECO_a_00395
   Hunsberger Eric, 2015, COMPUT SCI
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
   Krizhevsky Alex, 2012, LEARNING MULTIPLE LA
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li CY, 2017, OMICS, V21, P749, DOI 10.1089/omi.2017.0120
   Lin ZT, 2017, ELECTRON LETT, V53, P1347, DOI 10.1049/el.2017.2219
   Masquelier T, 2007, PLOS COMPUT BIOL, V3, P247, DOI 10.1371/journal.pcbi.0030031
   Orchard G, 2015, IEEE T PATTERN ANAL, V37, P2028, DOI 10.1109/TPAMI.2015.2392947
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Simonyan K., 2014, VERY DEEP CONVOLUTIO
   Yu AJ, 2002, NEURAL COMPUT, V14, P2857, DOI 10.1162/089976602760805313
NR 18
TC 6
Z9 6
U1 0
U2 12
PD MAR
PY 2020
VL 12
IS 1
BP 21
EP 24
DI 10.1109/LES.2019.2919244
UT WOS:000519894400006
DA 2023-11-16
ER

PT C
AU Du, YW
   Jin, J
   Wang, Q
   Fan, JY
AF Du, Yuwei
   Jin, Jing
   Wang, Qiang
   Fan, Jianyin
GP IEEE
TI EMG-Based Continuous Motion Decoding of Upper Limb with Spiking Neural
   Network
SO 2022 IEEE INTERNATIONAL INSTRUMENTATION AND MEASUREMENT TECHNOLOGY
   CONFERENCE (I2MTC 2022)
SE IEEE Instrumentation and Measurement Technology Conference
DT Proceedings Paper
CT IEEE International Instrumentation and Measurement Technology Conference
   (I2MTC)
CY MAY 16-19, 2022
CL Ottawa, CANADA
DE electromyography; continuous motion; spiking neural network
AB Surface electromyography (EMG), generated during muscle activities of human beings, allows intuitive control for human-robot interaction to happen. Decoding human movement intention from EMG accurately and instantaneously is one of the most important parts of the whole control task. Spiking neural network (SNN) with spiking neurons is more computationally powerful than networks with non-spiking neurons and contains temporal information (time-dependency). Compared with discrete motion classification task, motion regression is more meaningful and helpful for the underlying applications including assisting human beings' activities of daily living (ADLs). We proposed a novel method deploying SNN in human motion regression task. An SNN is built to decode elbow joint angle from preprocessed surface EMG signals and achieved satisfying accuracy compared with long short-term memory. According to the experiment results, SNN is competent to decode motion information from surface EMG.
C1 [Du, Yuwei; Jin, Jing; Wang, Qiang; Fan, Jianyin] Harbin Inst Technol, Dept Control Sci & Engn, Harbin, Peoples R China.
RP Du, YW (corresponding author), Harbin Inst Technol, Dept Control Sci & Engn, Harbin, Peoples R China.
EM hitduyw@163.com; injinghit@hit.edu.cn; wanggiang@hit.edu.cn;
   fanjianyin_q@foxmail.com
CR Artemiadis PK, 2007, 2007 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-9, P495
   Artemiadis PK, 2010, IEEE T INF TECHNOL B, V14, P582, DOI 10.1109/TITB.2010.2040832
   Shrestha SB, 2018, Arxiv, DOI arXiv:1810.08646
   Bao TZ, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3036654
   Donati E, 2019, IEEE T BIOMED CIRC S, V13, P795, DOI 10.1109/TBCAS.2019.2925454
   Gehrig M, 2020, IEEE INT CONF ROBOT, P4195, DOI [10.1109/icra40945.2020.9197133, 10.1109/ICRA40945.2020.9197133]
   Jiang N, 2014, IEEE T NEUR SYS REH, V22, P549, DOI 10.1109/TNSRE.2013.2287383
   Khan SM, 2020, IEEE REV BIOMED ENG, V13, P248, DOI 10.1109/RBME.2019.2950897
   Ma JX, 2015, IEEE T HUM-MACH SYST, V45, P74, DOI 10.1109/THMS.2014.2358634
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Pizzolato S, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0186132
   Wen RS, 2020, IEEE ROBOT AUTOM LET, V5, P2762, DOI 10.1109/LRA.2020.2974439
   Zhuang Y, 2019, IEEE T IND INFORM, V15, P1211, DOI 10.1109/TII.2018.2875729
NR 13
TC 0
Z9 0
U1 4
U2 7
PY 2022
DI 10.1109/I2MTC48687.2022.9806710
UT WOS:000844585400239
DA 2023-11-16
ER

PT J
AU Han, RX
   Wang, J
   Miao, R
   Deng, B
   Qin, YM
   Yu, HT
   Wei, XL
AF Han, Ruixue
   Wang, Jiang
   Miao, Rui
   Deng, Bin
   Qin, Yingmei
   Yu, Haitao
   Wei, Xile
TI Propagation of Collective Temporal Regularity in Noisy Hierarchical
   Networks
SO IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
DT Article
DE Feedforward neural network; information entropy; Izhikevich model;
   spike-timing-dependent plasticity (STDP); spiking regularity; synaptic
   dynamics; topology-dependent coherence resonance (CR)
ID TIMING-DEPENDENT PLASTICITY; HIGH-CONDUCTANCE STATE; SPIKING ACTIVITY;
   SYNAPTIC PLASTICITY; SYNCHRONOUS SPIKING; SIGNAL PROPAGATION;
   EXCITABLE-MEMBRANES; COHERENCE RESONANCE; GABAERGIC SYNAPSES; NEURONAL
   NETWORKS
AB Neuronal communication between different brain areas is achieved in terms of spikes. Consequently, spike-time regularity is closely related to many cognitive tasks and timing precision of neural information processing. A recent experiment on primate parietal cortex reports that spike-time regularity increases consistently from primary sensory to higher cortical regions. This observation conflicts with the influential view that spikes in the neocortex are fundamentally irregular. To uncover the underlying network mechanism, we construct a multilayered feedforward neural information transmission pathway and investigate how spike-time regularity evolves across subsequent layers. Numerical results reveal that despite the obviously irregular spiking patterns in previous several layers, neurons in downstream layers can generate rather regular spikes, which depends on the network topology. In particular, we find that collective temporal regularity in deeper layers exhibits resonance-like behavior with respect to both synaptic connection probability and synaptic weight, i.e., the optimal topology parameter maximizes the spike-timing regularity. Furthermore, it is demonstrated that synaptic properties, including inhibition, synaptic transient dynamics, and plasticity, have significant impacts on spike-timing regularity propagation. The emergence of the increasingly regular spiking (RS) patterns in higher parietal regions can, thus, be viewed as a natural consequence of spiking activity propagation between different brain areas. Finally, we validate an important function served by increased RS: promoting reliable propagation of spike-rate signals across downstream layers.
C1 [Han, Ruixue; Wang, Jiang; Miao, Rui; Deng, Bin; Yu, Haitao; Wei, Xile] Tianjin Univ, Sch Elect & Automat Engn, Tianjin 300072, Peoples R China.
   [Qin, Yingmei] Tianjin Univ Technol & Educ Tianjin, Sch Automat & Elect Engn, Tianjin 300222, Peoples R China.
RP Wei, XL (corresponding author), Tianjin Univ, Sch Elect & Automat Engn, Tianjin 300072, Peoples R China.
EM ruixuehantju@sina.com; jiangwang@tju.edu.cn; 361812194@qq.com;
   dengbin@tju.edu.cn; eeymqin@tju.edu.cn; htyu@tju.edu.cn;
   xilewei@tju.edu.cn
CR August DA, 1999, J COMPUT NEUROSCI, V6, P71, DOI 10.1023/A:1008861001091
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Braitenberg V., 1991, J ANAT, V179, P203
   Brunel N, 2000, J COMPUT NEUROSCI, V8, P183, DOI 10.1023/A:1008925309027
   Bullmore ET, 2009, NAT REV NEUROSCI, V10, P186, DOI 10.1038/nrn2575
   Caporale N, 2008, ANNU REV NEUROSCI, V31, P25, DOI 10.1146/annurev.neuro.31.060407.125639
   Carron R, 2013, FRONT SYST NEUROSCI, V7, DOI 10.3389/fnsys.2013.00112
   Câteau H, 2001, NEURAL NETWORKS, V14, P675, DOI 10.1016/S0893-6080(01)00065-X
   CONNORS BW, 1990, TRENDS NEUROSCI, V13, P99, DOI 10.1016/0166-2236(90)90185-D
   Debanne D, 1999, J NEUROSCI, V19, P10664, DOI 10.1523/JNEUROSCI.19-24-10664.1999
   Debanne D, 1996, P NATL ACAD SCI USA, V93, P11225, DOI 10.1073/pnas.93.20.11225
   Destexhe A, 2003, NAT REV NEUROSCI, V4, P739, DOI 10.1038/nrn1198
   Destexhe A, 1994, J Comput Neurosci, V1, P195, DOI 10.1007/BF00961734
   Destexhe A, 2009, J COMPUT NEUROSCI, V27, P493, DOI 10.1007/s10827-009-0164-4
   Diesmann M, 1999, NATURE, V402, P529, DOI 10.1038/990101
   Dorval AD, 2008, J NEUROSCI METH, V173, P129, DOI 10.1016/j.jneumeth.2008.05.013
   Feinerman O, 2005, J NEUROPHYSIOL, V94, P3406, DOI 10.1152/jn.00264.2005
   Felleman DJ, 1991, CEREB CORTEX, V1, P1, DOI 10.1093/cercor/1.1.1
   Fetz E., 1991, NORMAL ALTERED STATE
   Gerstner W, 2000, NEURAL COMPUT, V12, P43, DOI 10.1162/089976600300015899
   Gilson M, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0025339
   Gjorgjieva J, 2011, P NATL ACAD SCI USA, V108, P19383, DOI 10.1073/pnas.1105933108
   Gong YB, 2009, J CHEM PHYS, V130, DOI 10.1063/1.3125512
   GRAY CM, 1989, NATURE, V338, P334, DOI 10.1038/338334a0
   Guo DQ, 2009, PHYS REV E, V79, DOI 10.1103/PhysRevE.79.051921
   Gütig R, 2003, J NEUROSCI, V23, P3697
   Haas JS, 2006, J NEUROPHYSIOL, V96, P3305, DOI 10.1152/jn.00551.2006
   Hahn G, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003811
   HESTRIN S, 1990, NEURON, V5, P247, DOI 10.1016/0896-6273(90)90162-9
   Hiratani N, 2015, PLOS COMPUT BIOL, V11, DOI 10.1371/journal.pcbi.1004227
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Jahnke S, 2013, FRONT COMPUT NEUROSC, V7, DOI 10.3389/fncom.2013.00153
   Jiruska P, 2013, J PHYSIOL-LONDON, V591, P787, DOI 10.1113/jphysiol.2012.239590
   Karbowski J, 2002, PHYS REV E, V65, DOI 10.1103/PhysRevE.65.031902
   Kistler WM, 2000, NEURAL COMPUT, V12, P385, DOI 10.1162/089976600300015844
   Kleberg FI, 2014, FRONT COMPUT NEUROSC, V8, DOI 10.3389/fncom.2014.00053
   Kodangattil JN, 2013, J PHYSIOL-LONDON, V591, P4699, DOI 10.1113/jphysiol.2013.257873
   Kremkow J, 2010, J NEUROSCI, V30, P15760, DOI 10.1523/JNEUROSCI.3874-10.2010
   Kuhn A, 2004, J NEUROSCI, V24, P2345, DOI 10.1523/JNEUROSCI.3349-03.2004
   Kumar A, 2008, J NEUROSCI, V28, P5268, DOI 10.1523/JNEUROSCI.2542-07.2008
   Kumar A, 2008, NEURAL COMPUT, V20, P1, DOI 10.1162/neco.2008.20.1.1
   Kumar A, 2011, FRONT SYST NEUROSCI, V5, DOI 10.3389/fnsys.2011.00086
   Kumar A, 2010, NAT REV NEUROSCI, V11, P615, DOI 10.1038/nrn2886
   Lee AK, 2002, NEURON, V36, P1183, DOI 10.1016/S0896-6273(02)01096-6
   Lee S, 2009, PLOS COMPUT BIOL, V5, DOI 10.1371/journal.pcbi.1000602
   Li MR, 2006, PHYS REV E, V74, DOI 10.1103/PhysRevE.74.011918
   Li QS, 2008, PHYS REV E, V77, DOI 10.1103/PhysRevE.77.036117
   Litvak V, 2003, J NEUROSCI, V23, P3006
   Luz Y, 2012, PLOS COMPUT BIOL, V8, DOI 10.1371/journal.pcbi.1002334
   Maffei A, 2011, NEURAL PLAST, V2011, DOI 10.1155/2011/254724
   Magee JC, 1997, SCIENCE, V275, P209, DOI 10.1126/science.275.5297.209
   Maimon G, 2009, NEURON, V62, P426, DOI 10.1016/j.neuron.2009.03.021
   Maquet P, 2001, SCIENCE, V294, P1048, DOI 10.1126/science.1062856
   MCCORMICK DA, 1985, J NEUROPHYSIOL, V54, P782, DOI 10.1152/jn.1985.54.4.782
   Mehring C, 2003, BIOL CYBERN, V88, P395, DOI 10.1007/s00422-002-0384-4
   Nádasdy Z, 1999, J NEUROSCI, V19, P9497
   Ozer M, 2008, PHYS LETT A, V372, P6498, DOI 10.1016/j.physleta.2008.09.007
   Ozer M, 2009, EPL-EUROPHYS LETT, V86, DOI 10.1209/0295-5075/86/40008
   PERKEL DH, 1967, BIOPHYS J, V7, P391, DOI 10.1016/S0006-3495(67)86596-2
   Pikovsky A, 2002, PHYS REV LETT, V88, DOI 10.1103/PhysRevLett.88.050601
   Pikovsky AS, 1997, PHYS REV LETT, V78, P775, DOI 10.1103/PhysRevLett.78.775
   RAMAN IM, 1992, NEURON, V9, P173, DOI 10.1016/0896-6273(92)90232-3
   Renart A, 2010, SCIENCE, V327, P587, DOI 10.1126/science.1179850
   Reyes AD, 2003, NAT NEUROSCI, V6, P593, DOI 10.1038/nn1056
   Richardson MJE, 2004, PHYS REV E, V69, DOI 10.1103/PhysRevE.69.051918
   Riehle A, 1997, SCIENCE, V278, P1950, DOI 10.1126/science.278.5345.1950
   Rieke F., 1999, SPIKES EXPLORING NEU
   Rubin J, 2001, PHYS REV LETT, V86, P364, DOI 10.1103/PhysRevLett.86.364
   Salinas E, 2000, J NEUROSCI, V20, P6193, DOI 10.1523/JNEUROSCI.20-16-06193.2000
   Scannell JW, 1999, CEREB CORTEX, V9, P277, DOI 10.1093/cercor/9.3.277
   Schmid G, 2004, PHYS BIOL, V1, P61, DOI 10.1088/1478-3967/1/2/002
   Schmid G, 2004, PHYSICA A, V344, P665, DOI 10.1016/j.physa.2004.06.049
   Shadlen MN, 1998, J NEUROSCI, V18, P3870
   SHANNON CE, 1948, BELL SYST TECH J, V27, P379, DOI 10.1002/j.1538-7305.1948.tb01338.x
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Steele PM, 1999, J NEUROPHYSIOL, V81, P1559, DOI 10.1152/jn.1999.81.4.1559
   Stickgold R, 2000, NAT NEUROSCI, V3, P1237, DOI 10.1038/81756
   Sun XJ, 2014, SCI CHINA TECHNOL SC, V57, P879, DOI 10.1007/s11431-014-5529-x
   Svirskis G, 2000, BIOPHYS J, V79, P629, DOI 10.1016/S0006-3495(00)76321-1
   Tallon-Baudry C, 2009, FRONT BIOSCI-LANDMRK, V14, P321, DOI 10.2741/3246
   Thorpe SJ, 2001, SCIENCE, V291, P260, DOI 10.1126/science.1058249
   TSODYKS MV, 1995, NETWORK-COMP NEURAL, V6, P111, DOI 10.1088/0954-898X/6/2/001
   VAADIA E, 1995, NATURE, V373, P515, DOI 10.1038/373515a0
   van Rossum MCW, 2002, J NEUROSCI, V22, P1956, DOI 10.1523/JNEUROSCI.22-05-01956.2002
   van Rossum MCW, 2000, J NEUROSCI, V20, P8812
   van Vreeswijk C, 1998, NEURAL COMPUT, V10, P1321, DOI 10.1162/089976698300017214
   vanVreeswijk C, 1996, SCIENCE, V274, P1724, DOI 10.1126/science.274.5293.1724
   Varela F, 2001, NAT REV NEUROSCI, V2, P229, DOI 10.1038/35067550
   Vogels TP, 2013, FRONT NEURAL CIRCUIT, V7, DOI 10.3389/fncir.2013.00119
   Vogels TP, 2011, SCIENCE, V334, P1569, DOI 10.1126/science.1211095
   Vogels TP, 2009, NAT NEUROSCI, V12, P483, DOI 10.1038/nn.2276
   Vogels TP, 2005, J NEUROSCI, V25, P10786, DOI 10.1523/JNEUROSCI.3508-05.2005
   Wang MS, 2004, CHEMPHYSCHEM, V5, P1602, DOI 10.1002/cphc.200400255
   Wang ST, 2006, PHYS REV LETT, V96, DOI 10.1103/PhysRevLett.96.018103
   Wang YQ, 2000, PHYS REV E, V61, P740, DOI 10.1103/PhysRevE.61.740
   Womelsdorf T, 2006, NATURE, V439, P733, DOI 10.1038/nature04258
   Woodin MA, 2003, NEURON, V39, P807, DOI 10.1016/S0896-6273(03)00507-5
   Xu SJ, 2012, NAT NEUROSCI, V15, P449, DOI 10.1038/nn.3036
   Yilmaz E, 2013, PHYS LETT A, V377, P1301, DOI 10.1016/j.physleta.2013.03.007
NR 100
TC 5
Z9 5
U1 2
U2 24
PD JAN
PY 2017
VL 28
IS 1
BP 191
EP 205
DI 10.1109/TNNLS.2015.2502993
UT WOS:000391725000016
DA 2023-11-16
ER

PT J
AU Schæfer, M
   Schoenauer, T
   Wolff, C
   Hartmann, G
   Klar, H
   Rückert, U
AF Schæfer, M
   Schoenauer, T
   Wolff, C
   Hartmann, G
   Klar, H
   Rückert, U
TI Simulation of spiking neural networks -: architectures and
   implementations
SO NEUROCOMPUTING
DT Article; Proceedings Paper
CT 8th European Symposium on Artificial Neural Networks (ESANN)
CY APR 26-28, 2001
CL BRUGGE, BELGIUM
DE spiking neural networks; PCNN simulation; accelerators; vision networks
ID VISUAL-CORTEX; ORIENTATION COLUMNS; PRINCIPLES; EMERGENCE; CELLS
AB The fast simulation of large networks of spiking neurons is a major task for the examination of biology-inspired vision systems. Networks of this type label features by synchronization of spikes and there is strong demand to simulate these effects in real world environments. As the calculations for one model neuron are complex, the digital simulation of large networks is not efficient using existing simulation systems. Consequently, it is necessary to develop special simulation techniques. This article introduces a wide range of concepts for the different parts of digital simulator systems for large vision networks and presents accelerators based on these foundations. (C) 2002 Elsevier Science B.V. All rights reserved.
C1 Univ Paderborn, Heinz Nixdorf Inst, D-33102 Paderborn, Germany.
   Tech Univ Berlin, Inst Microelect & Solid State Elect, D-10587 Berlin, Germany.
RP Wolff, C (corresponding author), Univ Paderborn, Heinz Nixdorf Inst, D-33102 Paderborn, Germany.
CR [Anonymous], 1998, PULSED NEURAL NETWOR
   BIENENSTOCK EL, 1982, J NEUROSCI, V2, P32, DOI 10.1523/jneurosci.02-01-00032.1982
   Eckhorn R, 1990, NEURAL COMPUT, V2, P293, DOI 10.1162/neco.1990.2.3.293
   ECKHORN R, 1989, P ICNN, V1, P723
   FRANK G, 1995, P INT C NEUR NETW IC, V4, P2014
   FRANK G, 1997, HNI VERLAGSSCHRIFTEN, V26
   FRENCH AS, 1970, IEEE T BIO-MED ENG, VBM17, P248, DOI 10.1109/TBME.1970.4502739
   Gerstner W, 1996, NATURE, V383, P76, DOI 10.1038/383076a0
   GRAY CM, 1989, P NATL ACAD SCI USA, V86, P1698, DOI 10.1073/pnas.86.5.1698
   HARTMANN G, 1990, PARALLEL PROCESSING IN NEURAL SYSTEMS AND COMPUTERS, P361
   HARTMANN G, 1997, SPIKE128K ACCELERATO, P130
   HEEMSKERK JNH, 1995, THESIS LEIDEN U RIJK
   HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837
   HUBEL DH, 1982, NATURE, V299, P515, DOI 10.1038/299515a0
   JAHNKE A, 1997, INT C ART NEUR NETW, P1187
   JAHNKE A, 1996, MICRONEURO 96, P232
   Kempter R, 1999, PHYS REV E, V59, P4498, DOI 10.1103/PhysRevE.59.4498
   KEMPTER R, 1997, NATURWISSENSCHATLICH, V17
   LINSKER R, 1986, P NATL ACAD SCI USA, V83, P8779, DOI 10.1073/pnas.83.22.8779
   LINSKER R, 1986, P NATL ACAD SCI USA, V83, P7508, DOI 10.1073/pnas.83.19.7508
   LINSKER R, 1986, P NATL ACAD SCI USA, V83, P8390, DOI 10.1073/pnas.83.21.8390
   MOHRAZ K, 1997, P IMACS WORLD C 97 B, V6, P523
   NIERBUR E, 1994, ADV NEURAL INFORMATI, V6, P904
   Preis R, 1997, ADVANCES IN COMPUTATIONAL MECHANICS WITH PARALLEL AND DISTRIBUTED PROCESSING, P63, DOI 10.4203/ccp.45.3.1
   ROTH U, 1997, MICRONEURO 97, P31
   ROTH U, 1995, IWANN 95, P720
   SCHFER M, 1999, MICRONEURO 99, P316
   SCHOENAUER T, 1998, VIDYNN 98
   SCHOENAUER T, 1998, ICCIN 98 INT C COMP, P17
   SCHOENAUER T, 2000, INT JOINT C NEUR NET
   SINGER W, 1995, SCIENCE, V270, P758, DOI 10.1126/science.270.5237.758
   WEITZEL L, 1997, CAIP 97
   WOLFF C, 1999, DSP DTSCH 99, P267
   WOLFF C, 1999, MICRONEURO 99, P324
NR 34
TC 26
Z9 32
U1 1
U2 7
PD OCT
PY 2002
VL 48
BP 647
EP 679
AR PII S0925-2312(01)00633-6
DI 10.1016/S0925-2312(01)00633-6
UT WOS:000178464600038
DA 2023-11-16
ER

PT C
AU Hart, CB
AF Hart, Corey B.
BE Dagli, CH
TI Variable Time Delays and Representational Capacity in Sparsely Connected
   Populations of Spiking Neurons
SO COMPLEX ADAPTIVE SYSTEMS: EMERGING TECHNOLOGIES FOR EVOLVING SYSTEMS:
   SOCIO-TECHNICAL, CYBER AND BIG DATA
SE Procedia Computer Science
DT Proceedings Paper
CT Complex Adaptive Systems Conference
CY NOV 13-15, 2013
CL Baltimore, MD
DE Artificial neural networks; spiking neurons; neural assembly computing
ID COMPUTATION; NETWORKS
AB Successive generations of artificial neural networks have leveraged their multiplicity of connections and weights for significant improvements in information processing capability and memory capacity. The most recent generation of artificial neural networks, third generation networks, consist of spiking neuron models that attempt to mimic the complex dynamic features exhibited by real biological neurons in the hopes of improvements in computational and representational capacities. While the theoretical capabilities of these networks are impressive, understanding the nature and extent of their computational advantages, and the appropriate network architectures and algorithms necessary for their successful exploitation, have lagged far behind the theory. With this in mind, we herein explore the representational capacity of two related forms of neural networks: synfire chains, and polychronic networks. We find that the computational capacity of such cellular assembly based networks increases with the size of between-neural-pool time delays and that for relatively small changes in time delay, linear increases in network representational capacities are obtained. (C) 2013 The Authors. Published by Elsevier B.V.
C1 Lockheed Martin IS&GS, Adv Technol & Innovat, King Of Prussia, PA 19406 USA.
RP Hart, CB (corresponding author), Lockheed Martin IS&GS, Adv Technol & Innovat, 230 Mall Blvd, King Of Prussia, PA 19406 USA.
EM corey.hart@lmco.com
CR Abeles M., 1982, LOCAL CORTICAL CIRCU, DOI DOI 10.1007/978-3-642-81708-3
   GROSSBERG S, 1973, STUD APPL MATH, V52, P213
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Minsky M.L, 1969, PERCEPTRONS
   Paugham-Moisy S., 2006, SPIKING NEURAL NETWO
   Rahnel J, 2012, IEEE T NEURAL NETWOR, V23, P916
   Rosenblatt F., 1970, 854601 CORN AER LAB
NR 9
TC 0
Z9 0
U1 0
U2 0
PY 2013
VL 20
BP 22
EP 26
DI 10.1016/j.procs.2013.09.233
UT WOS:000342564700002
DA 2023-11-16
ER

PT J
AU Liu, JK
   She, ZS
AF Liu, Jian K.
   She, Zhen-Su
TI A Spike-Timing Pattern Based Neural Network Model for the Study of
   Memory Dynamics
SO PLOS ONE
DT Article
AB It is well accepted that the brain's computation relies on spatiotemporal activity of neural networks. In particular, there is growing evidence of the importance of continuously and precisely timed spiking activity. Therefore, it is important to characterize memory states in terms of spike-timing patterns that give both reliable memory of firing activities and precise memory of firing timings. The relationship between memory states and spike-timing patterns has been studied empirically with large-scale recording of neuron population in recent years. Here, by using a recurrent neural network model with dynamics at two time scales, we construct a dynamical memory network model which embeds both fast neural and synaptic variation and slow learning dynamics. A state vector is proposed to describe memory states in terms of spike-timing patterns of neural population, and a distance measure of state vector is defined to study several important phenomena of memory dynamics: partial memory recall, learning efficiency, learning with correlated stimuli. We show that the distance measure can capture the timing difference of memory states. In addition, we examine the influence of network topology on learning ability, and show that local connections can increase the network's ability to embed more memory states. Together theses results suggest that the proposed system based on spike-timing patterns gives a productive model for the study of detailed learning and memory dynamics.
RP Liu, JK (corresponding author), Univ Calif Los Angeles, Dept Math, Los Angeles, CA 90024 USA.
EM liujk@ucla.edu; she@pku.edu.cn
CR BIALEK W, 1991, SCIENCE, V252, P1854, DOI 10.1126/science.2063199
   Börgers C, 2005, P NATL ACAD SCI USA, V102, P7002, DOI 10.1073/pnas.0502366102
   Borst A, 1999, NAT NEUROSCI, V2, P947, DOI 10.1038/14731
   Brunel N, 2000, J COMPUT NEUROSCI, V8, P183, DOI 10.1023/A:1008925309027
   Buonomano DV, 2009, NAT REV NEUROSCI, V10, P113, DOI 10.1038/nrn2558
   BUONOMANO DV, 1995, SCIENCE, V267, P1028, DOI 10.1126/science.7863330
   Buonomano DV, 2005, J NEUROPHYSIOL, V94, P2275, DOI 10.1152/jn.01250.2004
   Buonomano DV, 2000, J NEUROSCI, V20, P1129, DOI 10.1523/JNEUROSCI.20-03-01129.2000
   Buzsáki G, 2007, NATURE, V446, P267, DOI 10.1038/446267a
   Dan Y, 2006, PHYSIOL REV, V86, P1033, DOI 10.1152/physrev.00030.2005
   de Almeida L, 2007, LEARN MEMORY, V14, P795, DOI 10.1101/lm.730207
   DESTEXHE A, 1994, NEURAL COMPUT, V6, P14, DOI 10.1162/neco.1994.6.1.14
   EDELMAN GM, 1993, NEURON, V10, P115, DOI 10.1016/0896-6273(93)90304-A
   Fröhlich F, 2008, J NEUROSCI, V28, P1709, DOI 10.1523/JNEUROSCI.4263-07.2008
   Fusi S, 2005, NEURON, V45, P599, DOI 10.1016/j.neuron.2005.02.001
   GOLOMB D, 1994, J NEUROPHYSIOL, V72, P1109, DOI 10.1152/jn.1994.72.3.1109
   Gupta A, 2000, SCIENCE, V287, P273, DOI 10.1126/science.287.5451.273
   Hahnloser RHR, 2002, NATURE, V419, P65, DOI 10.1038/nature00974
   Harris KD, 2005, NAT REV NEUROSCI, V6, P399, DOI 10.1038/nrn1669
   Harris KD, 2003, NATURE, V424, P552, DOI 10.1038/nature01834
   Izhikevich EM, 2004, CEREB CORTEX, V14, P933, DOI 10.1093/cercor/bhh053
   Izhikevich EM, 2003, TRENDS NEUROSCI, V26, P161, DOI 10.1016/S0166-2236(03)00034-1
   Izhikevich EM, 2007, CEREB CORTEX, V17, P2443, DOI 10.1093/cercor/bhl152
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Jacobs A. L., 2009, P NATL ACAD SCI US
   LIU JK, 2009, FRONT SYST NEUROSCI
   Markram H, 1998, P NATL ACAD SCI USA, V95, P5323, DOI 10.1073/pnas.95.9.5323
   Mauk MD, 2004, ANNU REV NEUROSCI, V27, P307, DOI 10.1146/annurev.neuro.27.070203.144247
   Mehring C, 2003, BIOL CYBERN, V88, P395, DOI 10.1007/s00422-002-0384-4
   Morrison A, 2008, BIOL CYBERN, V98, P459, DOI 10.1007/s00422-008-0233-1
   Muniak MA, 2007, J NEUROSCI, V27, P11687, DOI 10.1523/JNEUROSCI.1486-07.2007
   Nelson SB, 2008, NEURON, V60, P477, DOI 10.1016/j.neuron.2008.10.020
   Pastalkova E, 2008, SCIENCE, V321, P1322, DOI 10.1126/science.1159775
   Rabinovich MI, 2006, REV MOD PHYS, V78, P1213, DOI 10.1103/RevModPhys.78.1213
   Salinas E, 2001, NAT REV NEUROSCI, V2, P539, DOI 10.1038/35086012
   Siri B, 2008, NEURAL COMPUT, V20, P2937, DOI 10.1162/neco.2008.05-07-530
   Siri B, 2007, J PHYSIOL-PARIS, V101, P136, DOI 10.1016/j.jphysparis.2007.10.003
   Song S, 2005, PLOS BIOL, V3, P507, DOI 10.1371/journal.pbio.0030068
   THEUNISSEN F, 1995, J COMPUT NEUROSCI, V2, P149, DOI 10.1007/BF00961885
   Tiesinga P, 2008, NAT REV NEUROSCI, V9, P97, DOI 10.1038/nrn2315
   Vogels TP, 2005, ANNU REV NEUROSCI, V28, P357, DOI 10.1146/annurev.neuro.28.061604.135637
NR 41
TC 6
Z9 6
U1 0
U2 8
PD JUL 24
PY 2009
VL 4
IS 7
AR e6247
DI 10.1371/journal.pone.0006247
UT WOS:000268318900001
DA 2023-11-16
ER

PT C
AU Shrestha, A
   Ahmed, K
   Wang, YZ
   Qiu, QR
AF Shrestha, Amar
   Ahmed, Khadeer
   Wang, Yanzhi
   Qiu, Qinru
GP IEEE
TI Stable Spike-Timing Dependent Plasticity Rule for Multilayer
   Unsupervised and Supervised Learning
SO 2017 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY MAY 14-19, 2017
CL Anchorage, AK
DE spiking neural network; STDP; digit recognition; unsupervised learning;
   supervised learning; quantized STDP
ID SYNAPTIC PLASTICITY; NETWORKS; MODEL
AB Spike-Timing Dependent Plasticity (STDP), the canonical learning rule for spiking neural networks (SNN), is gaining tremendous interest because of its simplicity, efficiency and biological plausibility. However, to date, multilayer feed-forward networks of spiking neurons are either only partially trained using STDP or pre-trained using traditional deep neural networks which are converted to deep spiking neural networks or a two-layer network where STDP learnt features are manually labelled. In this work, we present a low-cost, simplified, yet stable STDP rule for layer-wise unsupervised and supervised training of a multilayer feed-forward SNN. We propose to approximate Bayesian neuron using Stochastic Integrate and Fire (SIF) neuron model and introduce a supervised learning approach using teacher neurons to train the classification layer with one neuron per class. A SNN is trained for classification of handwritten digits with multiple layers of spiking neurons, including both the feature extraction and classification layer, using the proposed STDP rule. Our method achieves comparable to better accuracy on MNIST dataset than manually labelled two layer networks for the same sized hidden layer. We also analyze the parameter space to provide rationales for parameter fine-tuning and provide additional methods to improve noise resilience and input intensity variations. We further propose a Quantized 2-Power Shift (Q2PS) STDP rule, which reduces the implementation cost of digital hardware while achieves comparable performance.
C1 [Shrestha, Amar; Ahmed, Khadeer; Wang, Yanzhi; Qiu, Qinru] Syracuse Univ, Dept Elect Engn & Comp Sci, Syracuse, NY 13244 USA.
RP Shrestha, A (corresponding author), Syracuse Univ, Dept Elect Engn & Comp Sci, Syracuse, NY 13244 USA.
EM amshrest@syr.ed; khahmed@syr.ed; ywang393@syr.ed; qiqiu@syr.ed
CR Abbott LF, 2000, NAT NEUROSCI, V3, P1178, DOI 10.1038/81453
   Afshar S, 2015, IEEE T BIOMED CIRC S, V9, P188, DOI 10.1109/TBCAS.2015.2416391
   Ahmed K., 2016, NEUR NETW IJCNN 2016
   Ahmed K., 2016, VLSI ISVLSI 2016 IEE
   [Anonymous], 2013, EVENT DRIVEN CONTRAS
   [Anonymous], 2015, NEUROMORPHIC ENG SYS
   [Anonymous], 2009, P C HIGH PERFORMANCE
   Burbank KS, 2012, PLOS COMPUT BIOL, V8, DOI 10.1371/journal.pcbi.1002393
   Castellani GC, 2005, LEARN MEMORY, V12, P423, DOI 10.1101/lm.80705
   Diehl PU, 2015, IEEE IJCNN
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Gerstner W., 2002, SPIKING NEURON MODEL
   Gilson M, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0025339
   Javed F, 2010, AM J CLIN NUTR, V91, P907, DOI 10.3945/ajcn.2009.28512
   Kasabov N, 2013, NEURAL NETWORKS, V41, P188, DOI 10.1016/j.neunet.2012.11.014
   Kempter R, 1999, PHYS REV E, V59, P4498, DOI 10.1103/PhysRevE.59.4498
   Kepecs J. T. A., 2002, ADV NEUR INF PROC SY
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lesica NA, 2007, NEURON, V55, P479, DOI 10.1016/j.neuron.2007.07.013
   Levy N, 2001, NEURAL NETWORKS, V14, P815, DOI 10.1016/S0893-6080(01)00044-2
   Lisman J., 2010, SPIKE TIMING DEPENDE, V26, P53
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Masquelier T, 2007, PLOS COMPUT BIOL, V3, P247, DOI 10.1371/journal.pcbi.0030031
   Matias FS, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0140504
   Nessler B, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003037
   Querlioz D., 2011, NEUR NETW IJCNN 2011
   Sengupta S., 2015, ARXIV150307490
   Shouval HZ, 2002, P NATL ACAD SCI USA, V99, P10831, DOI 10.1073/pnas.152343099
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Teramae J, 2014, P IEEE, V102, P500, DOI 10.1109/JPROC.2014.2306254
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   van Rossum MCW, 2000, J NEUROSCI, V20, P8812
NR 32
TC 27
Z9 27
U1 0
U2 10
PY 2017
BP 1999
EP 2006
UT WOS:000426968702034
DA 2023-11-16
ER

PT C
AU Galán-Prado, F
   Rosselló, JL
AF Galan-Prado, Fabio
   Rossello, Josep L.
BE Rojas, I
   Joya, G
   Catala, A
TI Smart Hardware Implementation of Spiking Neural Networks
SO ADVANCES IN COMPUTATIONAL INTELLIGENCE, IWANN 2017, PT I
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 14th International Work-Conference on Artificial Neural Networks (IWANN)
CY JUN 14-16, 2017
CL Cadiz, SPAIN
DE Neuromorphic hardware; Spiking Neural Network; FPGA
ID NEURONS
AB During last years a lot of attention have been focused to the hardware implementation of Artificial Neural Networks (ANN) to efficiently exploit the inherent parallelism associated to these systems. From the different types of ANN, the Spiking Neural Networks (SNN) arise as a promising bio-inspired model that is able to emulate the expected neural behavior with a high confidence. Many works are centered in using analog circuitry to reproduce SNN with a high degree of precision, while minimizing the area and the energy costs. Nevertheless, the reliability and flexibility of these systems is lower if compared with digital implementations. In this paper we present a new, low-cost bio-inspired digital neural model for SNN along with an auxiliary Computer Aided Design (CAD) tool for the efficient implementation of high-volume SNN.
C1 [Galan-Prado, Fabio; Rossello, Josep L.] Univ Illes Balears, Phys Dept, Elect Engn Grp, Palma De Mallorca, Spain.
RP Galán-Prado, F (corresponding author), Univ Illes Balears, Phys Dept, Elect Engn Grp, Palma De Mallorca, Spain.
EM fabio.galan@uib.es; j.rossello@uib.es
CR Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   Cassidy A, 2007, 2007 IEEE BIOMEDICAL CIRCUITS AND SYSTEMS CONFERENCE, P75, DOI 10.1109/BIOCAS.2007.4463312
   Cassidy AS, 2013, NEURAL NETWORKS, V45, P4, DOI 10.1016/j.neunet.2013.05.011
   Kaulmann Tim, 2007, 7th International Conference on Hybrid Intelligent Systems, HIS 2007, P302
   London M, 2010, NATURE, V466, P123, DOI 10.1038/nature09086
   Misra J, 2010, NEUROCOMPUTING, V74, P239, DOI 10.1016/j.neucom.2010.03.021
   Morris-Reich A, 2017, PAL CRIT ST ANTISEM, P1, DOI 10.1007/978-3-319-49953-6_1
   Natschlager T, 1998, NETWORK-COMP NEURAL, V9, P319, DOI 10.1088/0954-898X/9/3/003
   Omondi AR, 2006, FPGA IMPLEMENTATIONS OF NEURAL NETWORKS, P1, DOI 10.1007/0-387-28487-7_1
   Rosselló JL, 2016, INT J NEURAL SYST, V26, DOI 10.1142/S0129065715500367
   Rosselló JL, 2014, INT J NEURAL SYST, V24, DOI 10.1142/S0129065714300034
   Schrauwen B, 2008, NEURAL NETWORKS, V21, P511, DOI 10.1016/j.neunet.2007.12.009
   Segev I., 1998, A BRADFORD BOOK, V2
   Soltic S, 2010, INT J NEURAL SYST, V20, P437, DOI 10.1142/S012906571000253X
   Steinmetz PN, 2000, J COMPUT NEUROSCI, V9, P133, DOI 10.1023/A:1008967807741
   Wysoski SG, 2008, NEUROCOMPUTING, V71, P2563, DOI 10.1016/j.neucom.2007.12.038
NR 16
TC 2
Z9 2
U1 1
U2 7
PY 2017
VL 10305
BP 560
EP 568
DI 10.1007/978-3-319-59153-7_48
UT WOS:000443108200048
DA 2023-11-16
ER

PT C
AU Sun, HQ
   Qi, Y
   Wang, YM
AF Sun, Huaqin
   Qi, Yu
   Wang, Yueming
BE Ying, X
TI Delving into Temporal-Spectral Connections in Spike-LFP Decoding by
   Transformer Networks
SO HUMAN BRAIN AND ARTIFICIAL INTELLIGENCE, HBAI 2022
SE Communications in Computer and Information Science
DT Proceedings Paper
CT International Workshop on Human Brain and Artificial Intelligence (HBAI)
CY JUL 23, 2022
CL Vienna, AUSTRIA
DE Brain-computer interfaces; Spike-LFP fusion; Neural decoding
AB Invasive brain-computer interfaces (iBCIs) have demonstrated great potential in neural function restoration by decoding intention from brain signals for external device control. Spike trains and local field potentials (LFPs) are two typical intracortical neural signals with good complementarity from time and frequency domains. However, existing studies mostly focused on a single type of signal, and the interaction between the two signals has not been well studied. This study proposes a temporal-spectral transformer network (TSNet) to model the temporal (with spikes), spectral (with LFPs), and mutual (with both signals) connections in spike-LFPs towards robust neural decoding. Experiments with clinical neural signals demonstrate that the attention-based connection model enables the dynamic temporal-spectral compensation in spike and LFP signals, which improves the robustness against temporal shifts and noises in neural decoding.
C1 [Sun, Huaqin; Wang, Yueming] Zhejiang Univ, Qiushi Acad Adv Studies, Hangzhou, Peoples R China.
   [Qi, Yu] Zhejiang Univ, Sch Med, Affiliated Mental Hlth Ctr, Hangzhou, Peoples R China.
   [Qi, Yu] Zhejiang Univ, Sch Med, Hangzhou Peoples Hosp 7, Hangzhou, Peoples R China.
   [Qi, Yu] Zhejiang Univ, Sch Med, MOE Frontier Sci Ctr Brain Sci & Brain Machine In, Hangzhou, Peoples R China.
RP Wang, YM (corresponding author), Zhejiang Univ, Qiushi Acad Adv Studies, Hangzhou, Peoples R China.
EM sunhuaqin@zju.edu.cn; qiyu@zju.edu.cn; ymingwang@zju.edu.cn
NR 0
TC 1
Z9 1
U1 1
U2 1
PY 2023
VL 1692
BP 15
EP 29
DI 10.1007/978-981-19-8222-4_2
UT WOS:000925059700002
DA 2023-11-16
ER

PT C
AU Liu, Y
   Cheng, L
AF Liu, Yang
   Cheng, Long
GP IEEE
TI Spiking-Neural-Network Based Fugl-Meyer Hand Gesture Recognition For
   Wearable Hand Rehabilitation Robot
SO 2018 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUL 08-13, 2018
CL Rio de Janeiro, BRAZIL
DE spiking neural networks; surface electromyography; Fugl-Meyer
   assessment; hand gesture recogniton; SpikeProp
ID CLASSIFICATION; STROKE
AB Hand rehabilitation robot can assist the patients in completing rehabilitation exercises. Usually these rehabilitation exercises are designed according to Fugl-Meyer Assessment(FMA). Surface electromyography(sEMG) signal is the most commonly used physiological signal to identify the patient's movement intention. However, recognizing the hand gesture based on the sEMG signal is still a challenging problem due to the low amplitude and non-stationary characteristics of the sEMG signal. In this paper, eight standard hand movements in FMA are selected for the active exercises by hand rehabilitation robots. A total of 15 volunteers' sEMG signals are collected in the course of the experiment. Four time domain features, integral EMG(IEGM), root mean square(RMS), zero crossings(ZC) and energy percentage(EP), are used to identify hand gestures. A feedforward spiking neural network receives the above time domain feature data, and combines the population coding with the Spikeprop learning algorithm to realize the accurate recognition of hand gestures. The experimental results show that: (1) the spiking neural network can achieve a satisfactory classification accuracy by using only 15 neurons; (2) the classification accuracy using all four features are highest with an accuracy of 96.5%; (3) under the same number of neurons, the classification accuracy of the spiking neural network is higher than that of the multilayer perceptron, radial basis function network and support vector machine. This demonstrates the fact that spiking neural networks can achieve a satisfactory classification accuracy with a smaller network size.
C1 [Liu, Yang; Cheng, Long] Chinese Acad Sci, Inst Automat, State Key Lab Management & Control Complex Syst, Beijing 100190, Peoples R China.
   [Liu, Yang; Cheng, Long] Univ Chinese Acad Sci, Beijing, Peoples R China.
RP Cheng, L (corresponding author), Chinese Acad Sci, Inst Automat, State Key Lab Management & Control Complex Syst, Beijing 100190, Peoples R China.; Cheng, L (corresponding author), Univ Chinese Acad Sci, Beijing, Peoples R China.
EM long.cheng@ia.ac.cn
CR [Anonymous], 2012, HDB NATURAL COMPUTIN, DOI 10.1007/978-3-540-92910-9_22
   Atzori M, 2016, FRONT NEUROROBOTICS, V10, DOI 10.3389/fnbot.2016.00009
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Cao ZQ, 2015, NEURAL COMPUT APPL, V26, P1839, DOI 10.1007/s00521-015-1848-5
   Chen M, 2017, IEEE ANN INT CONF CY, P1472, DOI 10.1109/CYBER.2017.8446436
   Chen Y., 2013, P 20 INT C NEUR INF, V8228, P70
   Dobkin BH, 2005, NEW ENGL J MED, V352, P1677, DOI 10.1056/NEJMcp043511
   Gladstone DJ, 2002, NEUROREHAB NEURAL RE, V16, P232, DOI 10.1177/154596802401105171
   González-Izal M, 2012, J ELECTROMYOGR KINES, V22, P501, DOI 10.1016/j.jelekin.2012.02.019
   Guo SX, 2015, SENSORS-BASEL, V15, P9022, DOI 10.3390/s150409022
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Khadivi A, 2005, INT CONF ACOUST SPEE, P385
   Kleine BU, 2001, J APPL PHYSIOL, V91, P1588, DOI 10.1152/jappl.2001.91.4.1588
   Liu SY, 2017, IEEE IND ELEC, P5701, DOI 10.1109/IECON.2017.8216989
   Lobov S, 2015, SENSORS-BASEL, V15, P27894, DOI 10.3390/s151127894
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Mesa I, 2014, EXPERT SYST APPL, V41, P5190, DOI 10.1016/j.eswa.2014.03.014
   Naik GR, 2010, IEEE T INF TECHNOL B, V14, P301, DOI 10.1109/TITB.2009.2037752
   Ostwald SK, 2008, J NEUROSCI NURS, V40, P173, DOI 10.1097/01376517-200806000-00008
   Panzeri S, 2015, TRENDS COGN SCI, V19, P162, DOI 10.1016/j.tics.2015.01.002
   Peng L, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND BIOMIMETICS (ROBIO), P1486, DOI 10.1109/ROBIO.2016.7866537
   Rong Y, 2013, NEUROPHYSIOLOGY+, V45, P39, DOI 10.1007/s11062-013-9335-z
   Shrestha SB, 2017, NEURAL NETWORKS, V96, P33, DOI 10.1016/j.neunet.2017.08.010
   Tu EM, 2017, IEEE T NEUR NET LEAR, V28, P1305, DOI 10.1109/TNNLS.2016.2536742
   Wang WQ, 2016, IEEE T SYST MAN CY-S, V46, P980, DOI 10.1109/TSMC.2016.2531653
   Xie XR, 2017, NEUROCOMPUTING, V241, P152, DOI 10.1016/j.neucom.2017.01.086
   Zhang F, 2016, IEEE T HUM-MACH SYST, V46, P761, DOI 10.1109/THMS.2016.2562510
   Zhu XZ, 2008, FBIE: 2008 INTERNATIONAL SEMINAR ON FUTURE BIOMEDICAL INFORMATION ENGINEERING, PROCEEDINGS, P160, DOI 10.1109/FBIE.2008.38
NR 28
TC 0
Z9 0
U1 0
U2 4
PY 2018
UT WOS:000585967401072
DA 2023-11-16
ER

PT C
AU Xie, JL
   Zhao, QJ
   Zhao, JY
AF Xie, Jinli
   Zhao, Qinjun
   Zhao, Jianyu
BE Cong, F
   Leung, A
   Wei, Q
TI Burst and Correlated Firing in Spiking Neural Network with Global
   Inhibitory Feedback
SO ADVANCES IN NEURAL NETWORKS, PT I
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 14th International Symposium on Neural Networks (ISNN)
CY JUN 21-26, 2017
CL JAPAN
DE Burst; Correlation; Inhibitory feedback; Leaky integrate-and-fire neuron
ID PRIMARY VISUAL-CORTEX; NEURONAL CORRELATION; INFORMATION; COMPUTATION
AB Burst and correlated firing activities are observed experimentally in a variety of brain areas, which transmit and communicate information predominantly through spikes. The firing mode of spiking neurons relies on specific network characteristics. The inhibitory feedback is thought to be crucial to the burst firing. However, the effects of inhibitory feedback, and in particular the resulting bursting, on neural correlations need further studies. In order to understand how inhibitory feedback circuit modulates correlations and burst, we carry out numerical simulations of spiking neural network with global inhibitory feedback. Owing to the feedback inhibition, the neurons fire correlated action potentials of a long time scale and exhibit bursting fire pattern. We also found that, with constant output firing rate, the burst firing enhanced network correlations. These results suggest that in the spiking neural network with globally inhibitory feedback the shifts in the feedback strength can induce changes in burst probability, and then effect the correlated firing activities.
C1 [Xie, Jinli; Zhao, Qinjun; Zhao, Jianyu] Univ Jinan, Sch Elect Engn, Jinan 250022, Shandong, Peoples R China.
RP Xie, JL (corresponding author), Univ Jinan, Sch Elect Engn, Jinan 250022, Shandong, Peoples R China.
EM cse_xiejl@ujn.edu.cn; cse_zhaoqj@ujn.edu.cn; cse_zjy@ujn.edu.cn
CR Averbeck BB, 2006, NAT REV NEUROSCI, V7, P358, DOI 10.1038/nrn1888
   Chan HK, 2016, FRONT COMPUT NEUROSC, V10, DOI 10.3389/fncom.2016.00042
   de la Rocha J, 2007, NATURE, V448, P802, DOI 10.1038/nature06028
   Dipoppa M, 2013, FRONT COMPUT NEUROSC, V7, DOI 10.3389/fncom.2013.00139
   Gerkin RC, 2013, P NATL ACAD SCI USA, V110, P17083, DOI 10.1073/pnas.1303830110
   Kepecs A, 2003, NETWORK-COMP NEURAL, V14, P103, DOI 10.1088/0954-898X/14/1/306
   Kim SY, 2015, COGN NEURODYNAMICS, V9, P411, DOI 10.1007/s11571-015-9334-4
   Kohn A, 2005, J NEUROSCI, V25, P3661, DOI 10.1523/JNEUROSCI.5106-04.2005
   Krahe R, 2004, NAT REV NEUROSCI, V5, P13, DOI 10.1038/nrn1296
   Miller KJ, 2007, J NEUROSCI, V27, P2424, DOI 10.1523/JNEUROSCI.3886-06.2007
   Okun M, 2008, NAT NEUROSCI, V11, P535, DOI 10.1038/nn.2105
   Sah N, 2013, EUR J NEUROSCI, V38, P2542, DOI 10.1111/ejn.12262
   Sherman SM, 2002, PHILOS T R SOC B, V357, P1695, DOI 10.1098/rstb.2002.1161
   Smith MA, 2008, J NEUROSCI, V28, P12591, DOI 10.1523/JNEUROSCI.2929-08.2008
   Stephen C, NAT COMMUN
   Xie JL, 2012, NEUROCOMPUTING, V83, P146, DOI 10.1016/j.neucom.2011.12.004
   Xie JL, 2013, COGN NEURODYNAMICS, V7, P325, DOI 10.1007/s11571-013-9241-5
NR 17
TC 0
Z9 0
U1 0
U2 0
PY 2017
VL 10261
BP 529
EP 535
DI 10.1007/978-3-319-59072-1_62
UT WOS:000439963900062
DA 2023-11-16
ER

PT C
AU Glackin, C
   McDaid, L
   Maguire, L
   Sayers, H
AF Glackin, Cornelius
   McDaid, Liam
   Maguire, Liam
   Sayers, Heather
BE Kurkova, V
   Neruda, R
   Koutnik, J
TI Implementing fuzzy reasoning on a spiking neural network
SO ARTIFICIAL NEURAL NETWORKS - ICANN 2008, PT II
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 18th International Conference on Artificial Neural Networks (ICANN 2008)
CY SEP 03-06, 2008
CL Prague, CZECH REPUBLIC
DE spiking neuron model; dynamic synapse; supervised learning; receptive
   field; fuzzy reasoning
AB This paper presents a supervised training algorithm that implements fuzzy reasoning on a. spiking neural network. Neuron selectivity is facilitated using receptive fields that enable individual neurons to be responsive to certain spike train frequencies. The receptive fields behave in a similar manner as fuzzy membership functions. The network is supervised but, learning only occurs locally as in the biological case. The connectivity of the hidden and output layers is representative of a fuzzy rule base. The advantages and disadvantages of the network topology for the IRIS classification task are demonstrated and directions of current and future work are discussed.
C1 [Glackin, Cornelius; McDaid, Liam; Maguire, Liam; Sayers, Heather] Univ Ulster, Fac Engn, Sch Comp & Intelligent Syst, Coleraine BT48 7JL, Londonderry, North Ireland.
RP Glackin, C (corresponding author), Univ Ulster, Fac Engn, Sch Comp & Intelligent Syst, Magee Campus, Coleraine BT48 7JL, Londonderry, North Ireland.
EM glackin-c1@ulster.ac.uk; lj.mcdaid@ulster.ac.uk;
   lp.maguire@ulster.ac.uk; hm.sayers@ulster.ac.uk
CR Abbott LF, 2000, NAT NEUROSCI, V3, P1178, DOI 10.1038/81453
   Abdelbar AM, 2006, NEURAL COMPUT APPL, V15, P1, DOI 10.1007/s00521-005-0001-2
   BARLOW HB, 1953, J PHYSIOL-LONDON, V119, P69, DOI 10.1113/jphysiol.1953.sp004829
   Belatreche A, 2003, P IEEE CYB INT CHALL, P39
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   CARNELL A, 2005, 13 EUR S ART NEUR NE
   Dunn J. C., 1973, Journal of Cybernetics, V3, P32, DOI 10.1080/01969727308546046
   Fisher RA, 1936, ANN EUGENIC, V7, P179, DOI 10.1111/j.1469-1809.1936.tb02137.x
   KASINSKI A, 2005, COMP SUPERVISED LEAR
   MAASS W, 1996, EL C COMP COMPL ECCC, V3
   Natschläger T, 2001, NETWORK-COMP NEURAL, V12, P75, DOI 10.1088/0954-898X/12/1/305
   Pfister JP, 2003, LECT NOTES COMPUT SC, V2714, P92
   Ruf B, 1997, NEURAL PROCESS LETT, V5, P9, DOI 10.1023/A:1009697008681
   Sougné J, 2000, PERSP NEURAL COMP, P23
   Tsodyks M, 1998, NEURAL COMPUT, V10, P821, DOI 10.1162/089976698300017502
   ZADEH LA, 1965, INFORM CONTROL, V8, P338, DOI 10.1016/S0019-9958(65)90241-X
NR 16
TC 8
Z9 8
U1 0
U2 0
PY 2008
VL 5164
BP 258
EP 267
UT WOS:000259567200027
DA 2023-11-16
ER

PT C
AU Chakraborty, B
   Mukhopadhyay, S
AF Chakraborty, Biswadeep
   Mukhopadhyay, Saibal
GP IEEE
TI Brain-Inspired Spiking Neural Network for Online Unsupervised Time
   Series Prediction
SO 2023 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS, IJCNN
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUN 18-23, 2023
CL Broadbeach, AUSTRALIA
DE spiking neural network; recurrent; STDP; Wasserstein distance;
   persistent homologies; online time series prediction
ID OPTIMIZATION
AB Energy and data-efficient online time series prediction for predicting evolving dynamical systems are critical in several fields, especially edge AI applications that need to update continuously based on streaming data. However, current Deep Neural Network (DNN)-based supervised online learning models require a large amount of training data and cannot quickly adapt when the underlying system changes. Moreover, these models require continuous retraining with incoming data making them highly inefficient. We present a novel Continuous Learning-based Unsupervised Recurrent Spiking Neural Network Model (CLURSNN), trained with spike timing dependent plasticity (STDP) to solve these issues. CLURSNN makes online predictions by reconstructing the underlying dynamical system using Random Delay Embedding by measuring the membrane potential of neurons in the recurrent layer of the recurrent spiking neural network (RSNN) with the highest betweenness centrality. We also use topological data analysis to propose a novel methodology using the Wasserstein Distance between the persistent homologies of the predicted and observed time series as a loss function. We show that the proposed online time series prediction methodology outperforms state-of-the-art DNN models when predicting an evolving Lorenz63 dynamical system.
C1 [Chakraborty, Biswadeep; Mukhopadhyay, Saibal] Georgia Inst Technol, Atlanta, GA 30332 USA.
RP Chakraborty, B (corresponding author), Georgia Inst Technol, Atlanta, GA 30332 USA.
EM biswadeep@gatech.edu; saibal.mukhopadhyay@ece.gatech.edu
CR Aljundi R, 2019, PROC CVPR IEEE, P11246, DOI 10.1109/CVPR.2019.01151
   Anava O., 2013, C LEARN THEOR, P172
   Bhatnagar Aadyot, 2021, ARXIV210909265
   Bubenik P., 2020, TOPOLOGICAL DATA ANA, P97, DOI DOI 10.1007/978-3-030-43408-3_4
   Chakraborty B, 2021, FRONT NEUROSCI-SWITZ, V15, DOI 10.3389/fnins.2021.695357
   Chakraborty B, 2021, IEEE T IMAGE PROCESS, V30, P9014, DOI 10.1109/TIP.2021.3122092
   Chakraborty Biswadeep, 2023, 11 INT C LEARN UNPUB
   Chakraborty Biswadeep, 2022, ARXIV221104297
   Chang ZQ, 2021, IEEE INTERNET THINGS, V8, P13849, DOI 10.1109/JIOT.2021.3088875
   Cui LZ, 2018, INT J MACH LEARN CYB, V9, P1399, DOI 10.1007/s13042-018-0834-5
   FREEMAN LC, 1977, SOCIOMETRY, V40, P35, DOI 10.2307/3033543
   Gama J, 2014, ACM COMPUT SURV, V46, DOI 10.1145/2523813
   Gidea M, 2018, PHYSICA A, V491, P820, DOI 10.1016/j.physa.2017.09.028
   Laptev N, 2015, S5 LABELED ANOMALY D
   Leite D, 2020, EVOL SYST-GER, V11, P181, DOI 10.1007/s12530-020-09334-5
   Li SY, 2019, ADV NEUR IN, V32
   Liu CH, 2016, AAAI CONF ARTIF INTE, P1867
   LORENZ EN, 1963, J ATMOS SCI, V20, P130, DOI 10.1175/1520-0469(1963)020<0130:DNF>2.0.CO;2
   Ma HF, 2018, P NATL ACAD SCI USA, V115, pE9994, DOI 10.1073/pnas.1802987115
   Mahyar I, 2018, PHYSICA A, V497, P166, DOI 10.1016/j.physa.2017.12.145
   Margin DA, 2022, INT BLACK SEA CONF, P208, DOI [10.1109/BLACKSEACOM54372.2022.9858322, 10.1109/BlackSeaCom54372.2022.9858322]
   Petro B, 2020, IEEE T NEUR NET LEAR, V31, P358, DOI 10.1109/TNNLS.2019.2906158
   Petropoulos F, 2022, INT J FORECASTING, V38, P705, DOI 10.1016/j.ijforecast.2021.11.001
   Reid David, 2014, PLoS One, V9, pe103656, DOI 10.1371/journal.pone.0103656
   Pool RR, 2011, NEURAL COMPUT, V23, P1768, DOI 10.1162/NECO_a_00140
   Sahoo Doyen, 2017, ARXIV171103705
   She Xueyuan, 2021, INT C LEARN REPR
   Williams RJ, 1989, NEURAL COMPUT, V1, P270, DOI 10.1162/neco.1989.1.2.270
   Yoo S, 2022, IEEE INT MEM WORKSH, P17, DOI 10.1109/IMW52921.2022.9779247
   Yue ZH, 2022, AAAI CONF ARTIF INTE, P8980
NR 30
TC 0
Z9 0
U1 2
U2 2
PY 2023
DI 10.1109/IJCNN54540.2023.10191645
UT WOS:001046198704099
DA 2023-11-16
ER

PT J
AU Karakida, R
   Igarashi, Y
   Nagata, K
   Okada, M
AF Karakida, Ryo
   Igarashi, Yasuhiko
   Nagata, Kenji
   Okada, Masato
TI Inter-Layer Correlation in a Feed-Forward Network with Intra-Layer
   Common Noise
SO JOURNAL OF THE PHYSICAL SOCIETY OF JAPAN
DT Article
DE feed-forward neural network; common noise; synchronous firing; spike
   correlation
ID HIGHER-ORDER INTERACTIONS; PRIMARY VISUAL-CORTEX; NEURAL POPULATION;
   CORTICAL ACTIVITY; DISCHARGE
AB Neural networks generate correlated neural activities. In a multi-layer network, experimental studies have shown that spike correlations appear within a layer and between different layers. It is input common among neurons in each layer that realizes such correlated activities. Theoretical studies have demonstrated that common input given to neurons within a layer, which we call "intra-layer common noise'', generates spike correlation within the layer, which is "intralayer correlation'', in a feed-forward network. However, it has not been studied whether the common noise can generate spike correlation between different layers, which is "inter-layer correlation''. In this study, we constructed a theory of inter-layer correlation and calculated the theoretical values of the inter-layer correlation in a multi-layer feedforward network with intra-layer common noise. Our theory revealed that the common noise generates the inter-layer correlation, which coincided with results of simulation.
C1 [Karakida, Ryo; Igarashi, Yasuhiko; Nagata, Kenji; Okada, Masato] Univ Tokyo, Grad Sch Frontier Sci, Kashiwa, Chiba 2778561, Japan.
   [Igarashi, Yasuhiko] Japan Soc Promot Sci, Chiyoda Ku, Tokyo 1028472, Japan.
   [Okada, Masato] RIKEN, Brain Sci Inst, Wako, Saitama 3510198, Japan.
RP Karakida, R (corresponding author), Univ Tokyo, Grad Sch Frontier Sci, Kashiwa, Chiba 2778561, Japan.
EM okada@k.u-tokyo.ac.jp
CR ABELES M, 1993, J NEUROPHYSIOL, V70, P1629, DOI 10.1152/jn.1993.70.4.1629
   Amari S, 2003, NEURAL COMPUT, V15, P127, DOI 10.1162/089976603321043720
   Ganmor E, 2011, P NATL ACAD SCI USA, V108, P9679, DOI 10.1073/pnas.1019641108
   GEORGOPOULOS AP, 1986, SCIENCE, V233, P1416, DOI 10.1126/science.3749885
   Hamaguchi K, 2005, NEURAL COMPUT, V17, P2034, DOI 10.1162/0899766054322937
   Hansen BJ, 2012, NEURON, V76, P590, DOI 10.1016/j.neuron.2012.08.029
   Ikegaya Y, 2004, SCIENCE, V304, P559, DOI 10.1126/science.1093173
   Kohn A, 2005, J NEUROSCI, V25, P3661, DOI 10.1523/JNEUROSCI.5106-04.2005
   Macke JH, 2011, PHYS REV LETT, V106, DOI 10.1103/PhysRevLett.106.208102
   Montani F, 2009, PHILOS T R SOC A, V367, P3297, DOI 10.1098/rsta.2009.0082
   Ohiorhenuan IE, 2010, NATURE, V466, P617, DOI 10.1038/nature09178
   Rosenbaum RJ, 2010, FRONT COMPUT NEUROSC, V4, DOI 10.3389/fncom.2010.00009
   Schneidman E, 2006, NATURE, V440, P1007, DOI 10.1038/nature04701
   Shimazaki H, 2012, PLOS COMPUT BIOL, V8, DOI 10.1371/journal.pcbi.1002385
   Tchumatchenko T, 2010, PHYS REV LETT, V104, DOI 10.1103/PhysRevLett.104.058102
   Yamana M, 2005, J PHYS SOC JPN, V74, P2260, DOI 10.1143/JPSJ.74.2260
   Yu JN, 2010, NEURON, V68, P1187, DOI 10.1016/j.neuron.2010.11.027
   Yu S, 2011, J NEUROSCI, V31, P17514, DOI 10.1523/JNEUROSCI.3127-11.2011
   ZOHARY E, 1994, NATURE, V370, P140, DOI 10.1038/370140a0
NR 19
TC 0
Z9 0
U1 0
U2 9
PD JUN
PY 2013
VL 82
IS 6
DI 10.7566/JPSJ.82.064007
UT WOS:000319809400024
DA 2023-11-16
ER

PT C
AU Cardarilli, GC
   Cristini, A
   Di Nunzio, L
   Re, M
   Salerno, M
   Susi, G
AF Cardarilli, Gian Carlo
   Cristini, Alessandro
   Di Nunzio, Luca
   Re, Marco
   Salerno, Mario
   Susi, Gianluca
BE Matthews, MB
TI Spiking Neural Networks based on LIF with Latency: Simulation and
   Synchronization Effects
SO 2013 ASILOMAR CONFERENCE ON SIGNALS, SYSTEMS AND COMPUTERS
SE Conference Record of the Asilomar Conference on Signals Systems and
   Computers
DT Proceedings Paper
CT 47th Asilomar Conference on Signals, Systems, and Computers
CY NOV 03-06, 2013
CL Pacific Grove, CA
DE LIF with Latency Model; Spiking Neural Networks; Synchronization
   Effects; Jitter; intelligent DSP applications
ID EVENT-DRIVEN SIMULATION; ANTEROVENTRAL COCHLEAR NUCLEUS; SYNCHRONOUS
   SPIKING; SYNAPTIC INPUT; TEMPORAL INFORMATION; STABLE PROPAGATION;
   ELECTRIC FISH; FIRING RATES; NEURONS; MODEL
AB In this paper, a work on spiking neural networks based on a model of a kind of Leaky Integrate-and-Fire (LIF) neuron with latency is presented. Efficient simulations are carried out through an ad hoc event-driven approach, highlighting some particular effects of synchrony in a simple feedforward network topology. These results are consistent with literature results and, thanks to the implementation of the biologically plausible latency effect in the model, new results have emerged from the simulations. The authors plan to apply these results in the near future to applications in which this kind of neural networks and Digital Signal Processing (DSP) applications can be merged to obtain powerful nonlinear DSP techniques. In the plan of the authors is also the definition of a hardware prototype of the network based on analog/digital techniques.
C1 [Cardarilli, Gian Carlo; Cristini, Alessandro; Di Nunzio, Luca; Re, Marco; Salerno, Mario; Susi, Gianluca] Univ Roma Tor Vergata, Dept Elect Engn, I-00133 Rome, Italy.
RP Cardarilli, GC (corresponding author), Univ Roma Tor Vergata, Dept Elect Engn, Via Politecn 1, I-00133 Rome, Italy.
EM cardarilli@ieee.org; alessandro.cristini84@gmail.com;
   di.nunzio@ing.uniroma2.it; re@ieee.org; salerno@eln.uniroma2.it;
   gianluca.susi@uniroma2.it
CR Aertsen A, 1996, J PHYSIOLOGY-PARIS, V90, P243, DOI 10.1016/S0928-4257(97)81432-5
   [Anonymous], 1968, NEUROSCI RES PROGR B
   Belatreche A, 2007, SOFT COMPUT, V11, P239, DOI [10.1007/s00500-006-0065-7, 10.1007/S00500-006-0065-7]
   Brette R, 2007, J COMPUT NEUROSCI, V23, P349, DOI 10.1007/s10827-007-0038-6
   Burkitt AN, 2006, BIOL CYBERN, V95, P97, DOI 10.1007/s00422-006-0082-8
   Burkitt AN, 2006, BIOL CYBERN, V95, P1, DOI 10.1007/s00422-006-0068-6
   Burkitt AN, 1999, NEURAL COMPUT, V11, P871, DOI 10.1162/089976699300016485
   CARR CE, 1993, ANNU REV NEUROSCI, V16, P223, DOI 10.1146/annurev.ne.16.030193.001255
   CARR CE, 1986, J NEUROSCI, V6, P107
   Câteau H, 2001, NEURAL NETWORKS, V14, P675, DOI 10.1016/S0893-6080(01)00065-X
   Citri A, 2008, NEUROPSYCHOPHARMACOL, V33, P18, DOI 10.1038/sj.npp.1301559
   D'Haene M, 2009, NEURAL COMPUT, V21, P1068, DOI 10.1162/neco.2008.02-08-707
   Diesmann M, 1999, NATURE, V402, P529, DOI 10.1038/990101
   Edelman G. M., 1987, NEURAL DARWINISM THE
   FitzHugh R., 1955, B MATH BIOPHYS, V17, P257, DOI [10.1007/BF02477753, DOI 10.1007/BF02477753]
   Guo YX, 1997, J NEUROSCI, V17, P1761
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   JORIS PX, 1994, J NEUROPHYSIOL, V71, P1037, DOI 10.1152/jn.1994.71.3.1037
   JORIS PX, 1994, J NEUROPHYSIOL, V71, P1022, DOI 10.1152/jn.1994.71.3.1022
   Kistler WM, 2002, NEURAL COMPUT, V14, P987, DOI 10.1162/089976602753633358
   Kumar A, 2008, J NEUROSCI, V28, P5268, DOI 10.1523/JNEUROSCI.2542-07.2008
   Kumar A, 2010, NAT REV NEUROSCI, V11, P615, DOI 10.1038/nrn2886
   Litvak V, 2003, J NEUROSCI, V23, P3006
   Maass W, 2002, PHYS NEUR N, P373
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Marsalek PR, 1997, P NATL ACAD SCI USA, V94, P735, DOI 10.1073/pnas.94.2.735
   Mattia M, 2000, NEURAL COMPUT, V12, P2305, DOI 10.1162/089976600300014953
   Paolini AG, 2001, HEARING RES, V159, P101, DOI 10.1016/S0378-5955(01)00327-6
   Pearson MJ, 2007, IEEE T NEURAL NETWOR, V18, P1472, DOI 10.1109/TNN.2007.891203
   Ponulak F, 2011, ACTA NEUROBIOL EXP, V71, P409
   Ros E, 2006, NEURAL COMPUT, V18, P2959, DOI 10.1162/neco.2006.18.12.2959
   Rosselló JL, 2012, INT J NEURAL SYST, V22, DOI 10.1142/S0129065712500141
   Salerno M., 2013, ACEEE INT J INFORM T, V3
   Salerno M, 2011, BIOINFORMATICS 2011, P116
   van Rossum MCW, 2002, J NEUROSCI, V22, P1956, DOI 10.1523/JNEUROSCI.22-05-01956.2002
   Vogels TP, 2005, J NEUROSCI, V25, P10786, DOI 10.1523/JNEUROSCI.3508-05.2005
NR 37
TC 5
Z9 5
U1 0
U2 2
PY 2013
BP 1838
EP 1842
UT WOS:000341772900332
DA 2023-11-16
ER

EF
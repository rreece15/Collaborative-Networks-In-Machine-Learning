FN Clarivate Analytics Web of Science
VR 1.0
PT C
AU Koohestan-Mahalian, F
   Cotter, NE
AF Koohestan-Mahalian, Fatemeh
   Cotter, Neil E.
BE Matthews, MB
TI Exact Characterization of Phase Locking in a Linear Recurrent Spiking
   Neural Network
SO 2020 54TH ASILOMAR CONFERENCE ON SIGNALS, SYSTEMS, AND COMPUTERS
SE Conference Record of the Asilomar Conference on Signals Systems and
   Computers
DT Proceedings Paper
CT 54th Asilomar Conference on Signals, Systems, and Computers
CY NOV 01-05, 2020
CL ELECTR NETWORK
DE spiking neuron; response surface; phase locking; recurrent; spike time
ID MODEL
AB This paper presents a graphical method for determining the phase-tracking behavior of a linear spiking neuron with a recurrent connection. By employing what we refer to as the response surface method, we show that the response of the neuron to a steady input spike train is of three types: convergence to a stable fixed point, divergence from an unstable fixed point and finite firing, or chaotic firing. We also present a matrix formula for spiking times in a recurrent linear spiking neural network.
C1 [Koohestan-Mahalian, Fatemeh; Cotter, Neil E.] Univ Utah, Elect & Comp Engn Dept, Salt Lake City, UT 84112 USA.
RP Koohestan-Mahalian, F (corresponding author), Univ Utah, Elect & Comp Engn Dept, Salt Lake City, UT 84112 USA.
EM Fatima.mahalian@gmail.com; necotter@ece.utah.edu
CR Abbott LF, 1999, BRAIN RES BULL, V50, P303, DOI 10.1016/S0361-9230(99)00161-6
   [Anonymous], 2001, HDB BIOL PHYS
   Bressloff PC, 2000, NEURAL COMPUT, V12, P91, DOI 10.1162/089976600300015907
   Esser SK, 2016, P NATL ACAD SCI USA, V113, P11441, DOI 10.1073/pnas.1604850113
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   KEENER JP, 1981, SIAM J APPL MATH, V41, P503, DOI 10.1137/0141042
   Paugam-Moisy H, 2012, HDB NATURAL COMPUTIN, V1, P1
NR 9
TC 0
Z9 0
U1 0
U2 0
PY 2020
BP 802
EP 807
DI 10.1109/IEEECONF51394.2020.9443314
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Engineering, Electrical & Electronic; Imaging Science &
   Photographic Technology; Telecommunications
DA 2023-11-11
ER

PT J
AU Jiang, CM
   Yang, L
   Zhang, YL
AF Jiang, Chunming
   Yang, Le
   Zhang, Yilei
TI A Spiking Neural Network With Spike-Timing-Dependent Plasticity for
   Surface Roughness Analysis
SO IEEE SENSORS JOURNAL
DT Article
DE Neurons; Surface roughness; Rough surfaces; Sensors; Films; Membrane
   potentials; Biological system modeling; Surface roughness
   discrimination; tactile sensor; spiking neural network;
   spike-timing-dependent plasticity
ID DISCRIMINATION; TEXTURE; STDP
AB Spiking neural network (SNN) utilizes spike trains for information processing among neurons, which is more biologically plausible and widely regarded as the third-generation artificial neural network (ANN). It has the potential for effectively processing spatial-temporal information and has the characteristics of lower power consumption and smaller calculation load compared with conventional ANNs. In this work, we demonstrate the feasibility of applying SNN to classify tactile signals collected by a bionic artificial fingertip that touches a group of real-world metal surfaces with different roughness levels. A two-layer SNN is adopted and trained using an unsupervised learning method with spike-timing-dependent plasticity (STDP). Experiments show that the trained SNN can categorize the input tactile signals into different surface roughness of metal textures with more than 80% accuracy. This work lays the foundation of applying SNNs to more complex tactile signal processing in robotics, manufacturing, and other engineering fields.
C1 [Jiang, Chunming; Zhang, Yilei] Univ Canterbury, Dept Mech Engn, Christchurch 8041, New Zealand.
   [Yang, Le] Univ Canterbury, Dept Comp Engn, Christchurch 8041, New Zealand.
RP Zhang, YL (corresponding author), Univ Canterbury, Dept Mech Engn, Christchurch 8041, New Zealand.
EM chunming.jiang@pg.canbterbury.ac.nz; le.yang@canterbury.ac.nz;
   yilei.zhang@canterbury.ac.nz
CR [Anonymous], 2009, ROBOTICS SCI SYSTEMS
   [Anonymous], 2012, PIEZOELECTRIC CERAMI
   Brzosko Z, 2015, ELIFE, V4, DOI 10.7554/eLife.09685
   Cooper LN, 2012, NAT REV NEUROSCI, V13, P798, DOI 10.1038/nrn3353
   Deng L, 2020, NEURAL NETWORKS, V121, P294, DOI 10.1016/j.neunet.2019.09.005
   Diehl PU, 2015, IEEE IJCNN
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Hao YZ, 2020, NEURAL NETWORKS, V121, P387, DOI 10.1016/j.neunet.2019.09.007
   Hebb DO, 1950, J CLIN PSYCHOL, V6, P307
   Heeger D., 2000, HANDOUT U STANDFORD, V5, P76
   Isett BR, 2018, NEURON, V97, P418, DOI 10.1016/j.neuron.2017.12.021
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Kroemer O, 2011, IEEE T ROBOT, V27, P545, DOI 10.1109/TRO.2011.2121130
   Liang QZ, 2017, IEEE SENS J, V17, P7954, DOI 10.1109/JSEN.2017.2763245
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Mayol-Cuevas WW, 1998, IEEE SYS MAN CYBERN, P4246, DOI 10.1109/ICSMC.1998.727512
   Mozafari M, 2018, IEEE T NEUR NET LEAR, V29, P6178, DOI 10.1109/TNNLS.2018.2826721
   Muhammad HB, 2011, MICROELECTRON ENG, V88, P1811, DOI 10.1016/j.mee.2011.01.045
   Oddo CM, 2011, IEEE T ROBOT, V27, P522, DOI 10.1109/TRO.2011.2116930
   Pozo K, 2010, NEURON, V66, P337, DOI 10.1016/j.neuron.2010.04.028
   Qin LH, 2017, SENSOR ACTUAT A-PHYS, V264, P133, DOI 10.1016/j.sna.2017.07.054
   Rolls ET, 2011, PROG NEUROBIOL, V95, P448, DOI 10.1016/j.pneurobio.2011.08.002
   Sun QQ, 2009, J NEUROPHYSIOL, V102, P2955, DOI 10.1152/jn.00562.2009
   Taherkhani A, 2020, NEURAL NETWORKS, V122, P253, DOI 10.1016/j.neunet.2019.09.036
   Tang W, 2013, APPL SURF SCI, V273, P199, DOI 10.1016/j.apsusc.2013.02.013
   Virgilio CD, 2020, NEURAL NETWORKS, V122, P130, DOI 10.1016/j.neunet.2019.09.037
   Wang Y., 2018, IEEE T NEUR NET LEAR
   Yeung LC, 2004, P NATL ACAD SCI USA, V101, P14943, DOI 10.1073/pnas.0405555101
   Yi ZK, 2017, NEUROCOMPUTING, V244, P102, DOI 10.1016/j.neucom.2017.03.025
   Yi ZK, 2017, SENSOR ACTUAT A-PHYS, V255, P46, DOI 10.1016/j.sna.2016.12.021
   Zhang JC, 2009, P NATL ACAD SCI USA, V106, P13028, DOI 10.1073/pnas.0900546106
   Zhang W, 2003, NAT REV NEUROSCI, V4, P885, DOI 10.1038/nrn1248
NR 32
TC 2
Z9 2
U1 7
U2 28
PD JAN 1
PY 2022
VL 22
IS 1
BP 438
EP 445
DI 10.1109/JSEN.2021.3120845
WC Engineering, Electrical & Electronic; Instruments & Instrumentation;
   Physics, Applied
DA 2023-11-11
ER

PT C
AU Liu, D
   Yue, S
AF Liu, Daqi
   Yue, Shigang
GP IEEE
TI Visual Pattern Recognition using Unsupervised Spike Timing Dependent
   Plasticity Learning
SO 2016 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUL 24-29, 2016
CL Vancouver, CANADA
ID NEURONS; INTEGRATE; NETWORKS; NOISE
AB Neuroscience study shows mammalian brain only use millisecond scale time window to process complicated real-life recognition scenarios. However, such speed cannot be achieved by traditional rate-based spiking neural network (SNN). Compared with spiking rate, the specific spiking timing (also called spiking pattern) may convey much more information. In this paper, by using modified rank order coding scheme, the generated absolute analog features have been encoded into the first spike wave with specific spatiotemporal structural information. An intuitive yet powerful feed-forward spiking neural network framework has been proposed, along with its own unsupervised spike-timing- dependent plasticity (STDP) learning rule with dynamic post-synaptic potential threshold. Compared with other state-ofart spiking algorithms, the proposed method uses biologically plausible STDP learning method to learn the selectivity while the dynamic post-synaptic potential threshold guarantees no training sample will be ignored during the learning procedure. Furthermore, unlike the complicated frameworks used in those state-of-art spiking algorithms, the proposed intuitive spiking neural network is not time-consuming and quite capable of on-line learning. A satisfactory experimental result has been achieved on classic MNIST handwritten character database.
C1 [Liu, Daqi; Yue, Shigang] Univ Lincoln, Sch Comp Sci, Brayford Pool LN6 7TS, Lincoln, England.
RP Liu, D (corresponding author), Univ Lincoln, Sch Comp Sci, Brayford Pool LN6 7TS, Lincoln, England.
EM dliu@lincoln.ac.uk; syue@lincoln.ac.uk
CR [Anonymous], SPIKING NEURAL MODEL
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Brunel N, 2001, PHYS REV LETT, V86, P2186, DOI 10.1103/PhysRevLett.86.2186
   Delorme A, 1999, NEUROCOMPUTING, V26-7, P989, DOI 10.1016/S0925-2312(99)00095-8
   Delorme A, 2001, NEUROCOMPUTING, V38, P539, DOI 10.1016/S0925-2312(01)00403-9
   Delorme A, 2001, NEURAL NETWORKS, V14, P795, DOI 10.1016/S0893-6080(01)00049-1
   Gerstner W, 2000, NEURAL COMPUT, V12, P43, DOI 10.1162/089976600300015899
   Gerstner W, 1996, NATURE, V383, P76, DOI 10.1038/383076a0
   Hebb D. O., 1949, ORG BEHAV A NEUROPSY
   Kempter R, 1999, PHYS REV E, V59, P4498, DOI 10.1103/PhysRevE.59.4498
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   Masquelier T, 2008, PLOS ONE, V3, DOI 10.1371/journal.pone.0001377
   Masquelier T, 2007, PLOS COMPUT BIOL, V3, P247, DOI 10.1371/journal.pcbi.0030031
   Rieke F., 1996, SPIKES EXPLORING NEU
   Rubin JE, 2005, J NEUROPHYSIOL, V93, P2600, DOI 10.1152/jn.00803.2004
   Serre T., 2005, THEORY OBJECT RECOGN
   Serre T, 2007, IEEE T PATTERN ANAL, V29, P411, DOI 10.1109/TPAMI.2007.56
   Shadlen Michael N., 1994, Current Opinion in Neurobiology, V4, P569, DOI 10.1016/0959-4388(94)90059-0
   Sjöström PJ, 2001, NEURON, V32, P1149, DOI 10.1016/s0896-6273(01)00542-6
   Thériault C, 2013, IEEE T IMAGE PROCESS, V22, P764, DOI 10.1109/TIP.2012.2222900
   Thorpe S, 1996, NATURE, V381, P520, DOI 10.1038/381520a0
   van Rossum MCW, 2000, J NEUROSCI, V20, P8812
   Yu Q, 2013, IEEE T NEUR NET LEAR, V24, P1539, DOI 10.1109/TNNLS.2013.2245677
NR 25
TC 8
Z9 8
U1 0
U2 1
PY 2016
BP 285
EP 292
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Nasser, H
   Cessac, B
AF Nasser, Hassan
   Cessac, Bruno
TI Parameter Estimation for Spatio-Temporal Maximum Entropy Distributions:
   Application to Neural Spike Trains
SO ENTROPY
DT Article
DE neural coding; Gibbs distribution; maximum entropy; convex duality;
   spatio-temporal constraints; large-scale analysis; spike train; MEA
   recordings
ID TEMPORAL CORRELATIONS; INFORMATION; POPULATION; NETWORKS
AB We propose a numerical method to learn maximum entropy (MaxEnt) distributions with spatio-temporal constraints from experimental spike trains. This is an extension of two papers, [10] and [4], which proposed the estimation of parameters where only spatial constraints were taken into account. The extension we propose allows one to properly handle memory effects in spike statistics, for large-sized neural networks.
C1 [Nasser, Hassan; Cessac, Bruno] INRIA, F-06560 Sophia Antipolis, France.
RP Nasser, H (corresponding author), INRIA, 2004 Route Lucioles, F-06560 Sophia Antipolis, France.
EM Hassan.Nasser@inria.fr; Bruno.Cessac@inria.fr
CR Amari S, 2001, IEEE T INFORM THEORY, V47, P1701, DOI 10.1109/18.930911
   [Anonymous], 2007, ARXIV07122437
   [Anonymous], 2000, P C COMPUTATIONAL NA
   [Anonymous], 1975, LECT NOTES MATH
   Berger AL, 1996, COMPUT LINGUIST, V22, P39
   Cessac B., 2013, RR8329 INRIA
   Chazottes J., 2008, ISR J MATH, V131
   Chen S.F., 1999, ENT LANG MOD
   Chichilnisky EJ, 2001, NETWORK-COMP NEURAL, V12, P199, DOI 10.1088/0954-898X/12/2/306
   Collins M, 2002, MACH LEARN, V48, P253, DOI 10.1023/A:1013912006537
   CSISZAR I, 1974, IEEE T INFORM THEORY, V20, P122, DOI 10.1109/TIT.1974.1055146
   Dudik M., 2004, P 17 ANN C COMP LEAR
   Fernández R, 2005, J STAT PHYS, V118, P555, DOI 10.1007/s10955-004-8821-5
   Ferrea E, 2012, FRONT NEURAL CIRCUIT, V6, DOI 10.3389/fncir.2012.00080
   Ganmor E, 2011, P NATL ACAD SCI USA, V108, P9679, DOI 10.1073/pnas.1019641108
   Ganmor E, 2011, J NEUROSCI, V31, P3044, DOI 10.1523/JNEUROSCI.3682-10.2011
   Garibaldi U., 1985, P 5 NAT C HIST PHYS, V9, P341
   Georgii H. O., 1988, GIBBS MEASURES PHASE
   Gikhman II., 1979, THEORY STOCHASTIC PR, DOI [10.1007/978-1-4615-8065-2, DOI 10.1007/978-1-4615-8065-2]
   Hill DN, 2011, J NEUROSCI, V31, P8699, DOI 10.1523/JNEUROSCI.0971-11.2011
   Jaynes E.T., 1985, MACROSCOPIC PREDICTI, P254
   JAYNES ET, 1957, PHYS REV, V106, P620, DOI 10.1103/PhysRev.106.620
   JAYNES ET, 1980, ANNU REV PHYS CHEM, V31, P579, DOI 10.1146/annurev.pc.31.100180.003051
   Jaynes ET., 1979, MAXIMUM ENTROPY FORM, P15
   Kappen HJ, 1998, ADV NEUR IN, V10, P280
   Keller G., 1998, EQUILIBRIUM STATES E
   Li ZH, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0070894
   Litke AM, 2004, IEEE T NUCL SCI, V51, P1434, DOI 10.1109/TNS.2004.832706
   Marre O, 2009, PHYS REV LETT, V102, DOI 10.1103/PhysRevLett.102.138101
   Marre O, 2012, J NEUROSCI, V32, P14859, DOI 10.1523/JNEUROSCI.0723-12.2012
   Mayer V., 2010, MEMOIR AM MATH SOC, V203
   Nakahara H., 2001, ADV NEURAL INFORM PR, P253
   Nasser H, 2013, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2013/03/P03006
   Otten M, 2010, J CHEM PHYS, V133, DOI 10.1063/1.3455333
   Pillow JW, 2008, NATURE, V454, P995, DOI 10.1038/nature07140
   Quiroga RQ, 2004, NEURAL COMPUT, V16, P1661, DOI 10.1162/089976604774201631
   Rosenfeld R., 1994, TECHNICAL REPORT
   Ruelle D., 1978, THERMODYNAMIC FORMAL
   Ruelle D., 1969, STAT MECH, DOI 10.1142/4090
   Schaub M.T., 2010, ARXIV10091828
   Schneidman E, 2006, NATURE, V440, P1007, DOI 10.1038/nature04701
   Stevenson IH, 2011, NAT NEUROSCI, V14, P139, DOI 10.1038/nn.2731
   Strong SP, 1998, PHYS REV LETT, V80, P197, DOI 10.1103/PhysRevLett.80.197
   Tang A, 2008, J NEUROSCI, V28, P505, DOI 10.1523/JNEUROSCI.3359-07.2008
   Tkacik G., 2009, PHYS REV LETT
   Tkacik G, 2013, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2013/03/P03011
   Truccolo W, 2010, NAT NEUROSCI, V13, P105, DOI 10.1038/nn.2455
   Vasquez JC, 2012, J PHYSIOL-PARIS, V106, P120, DOI 10.1016/j.jphysparis.2011.11.001
   Zhou YQ, 2003, PROCEEDINGS OF THE 2003 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING, P153
NR 49
TC 16
Z9 16
U1 0
U2 10
PD APR
PY 2014
VL 16
IS 4
BP 2244
EP 2277
DI 10.3390/e16042244
WC Physics, Multidisciplinary
DA 2023-11-11
ER

PT C
AU Yaqoob, M
   Wróbel, B
AF Yaqoob, Muhammad
   Wrobel, Borys
GP IEEE
TI Very Small Spiking Neural Networks Evolved to Recognize a Pattern in a
   Continuous Input Stream
SO 2017 IEEE SYMPOSIUM SERIES ON COMPUTATIONAL INTELLIGENCE (SSCI)
DT Proceedings Paper
CT IEEE Symposium Series on Computational Intelligence (IEEE SSCI)
CY NOV 27-DEC 01, 2017
CL Honolulu, HI
DE artificial evolution; complex networks; evolutionary algorithm; minimal
   cognition; spiking neural networks; temporal pattern recognition
ID REPRESENTATION; MODELS; DELAYS; TIME
AB We obtained, with artificial evolution, very small (one or two interneurons, one output neuron) spiking neural networks (SNNs) recognizing a simple temporal pattern in a continuous input stream. The patterns the network evolved to recognize consisted of three different signals. In other words, the task was equivalent to searching in a stream (sequence) of three symbols (say, ABBCACBC..) for a specific subsequence (ABC). The fitness function we used rewarded spiking after the occurrence of the correct pattern (subsequence), and penalized spikes elsewhere. We found out that the networks did not go below two interneurons when they evolved to solve this task with a brief interval of silence between signals. However-surprisingly-for a longer interval of silences between signals the task could be accomplished with just one interneuron. We then analyzed how the spiking networks work by mapping the states of the network onto states of Finite State Machines-a general model of computation on time series. Our long term goal is to understand the mechanisms governing the neural networks that accomplish computational tasks in a way that is robust to noise and damage.
C1 [Yaqoob, Muhammad; Wrobel, Borys] Adam Mickiewicz Univ, Evolving Syst Lab, Poznan, Poland.
   [Wrobel, Borys] IOPAN, Sopot, Poland.
RP Yaqoob, M (corresponding author), Adam Mickiewicz Univ, Evolving Syst Lab, Poznan, Poland.
EM yaqoob@evosys.org; wrobel@evosys.org
CR Ahissar E, 2001, NEURON, V32, P185, DOI 10.1016/S0896-6273(01)00466-4
   [Anonymous], 2016, 2016 IEE INT C REB C, DOI [DOI 10.1109/ICRC.2016.7738691, 10.1109/ICRC.2016.7738691]
   BIALEK W, 1991, SCIENCE, V252, P1854, DOI 10.1126/science.2063199
   deCharms RC, 2000, ANNU REV NEUROSCI, V23, P613, DOI 10.1146/annurev.neuro.23.1.613
   Gerstner W, 1996, NATURE, V383, P76, DOI 10.1038/383076a0
   Huxter J, 2003, NATURE, V425, P828, DOI 10.1038/nature02058
   Isaacson JS, 2010, CURR OPIN NEUROBIOL, V20, P328, DOI 10.1016/j.conb.2010.02.004
   Joachimczak M, 2012, BIOSYSTEMS, V109, P498, DOI 10.1016/j.biosystems.2012.05.014
   Joris P, 2007, TRENDS NEUROSCI, V30, P70, DOI 10.1016/j.tins.2006.12.004
   Laurent G, 1996, TRENDS NEUROSCI, V19, P489, DOI 10.1016/S0166-2236(96)10054-0
   Maex R, 2009, NEURAL NETWORKS, V22, P1105, DOI 10.1016/j.neunet.2009.07.022
   Natschläger T, 2002, THEOR COMPUT SCI, V287, P251, DOI 10.1016/S0304-3975(02)00099-3
   Naud R, 2008, BIOL CYBERN, V99, P335, DOI 10.1007/s00422-008-0264-7
   Rieke F., 1999, SPIKES EXPLORING NEU
   Rutishauser U, 2009, NEURAL COMPUT, V21, P478, DOI 10.1162/neco.2008.03-08-734
   Savage J E., 1997, MODELS COMPUTATION E
   Steuber V, 2004, J COMPUT NEUROSCI, V17, P149, DOI 10.1023/B:JCNS.0000037678.26155.b5
   Steuber V, 2002, NEUROCOMPUTING, V44, P183, DOI 10.1016/S0925-2312(02)00388-0
   Steuber V, 1999, NEUROCOMPUTING, V26-7, P271, DOI 10.1016/S0925-2312(99)00021-1
   Steuber V, 2006, NETWORK-COMP NEURAL, V17, P173, DOI 10.1080/09548980500520328
   Thorpe S, 1996, NATURE, V381, P520, DOI 10.1038/381520a0
   Tino P, 2005, LECT NOTES COMPUT SC, V3611, P666
   Wrobel B., 2014, SCI, V557, P187, DOI DOI 10.1007/978-3-642-55337-0_6
   Wrobel B., 2014, LNICST, V134, P135, DOI DOI 10.1007/978-3-319-06944-9_10
NR 24
TC 4
Z9 4
U1 0
U2 0
PY 2017
BP 3496
EP 3503
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
DA 2023-11-11
ER

PT J
AU Pang, CCC
   Upton, ARM
   Shine, G
   Kamath, MV
AF Pang, CCC
   Upton, ARM
   Shine, G
   Kamath, MV
TI A comparison of algorithms for detection of spikes in the
   electroencephalogram
SO IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING
DT Article
DE Classification; EEG; neural networks; spike detection
ID NEURAL-NETWORKS; AUTOMATIC RECOGNITION; SEIZURE DETECTION
AB Identification of the short transient waveform, called a spike, in the cortical electroencephalogram (EEG) plays an important role during diagnosis of neurological disorders such as epilepsy. It has been suggested that artificial neural networks (ANN) can be employed for spike detection in the EEG, if suitable features are provided as input to an ANN. In.this paper, we explore the performance of neural network-based classifiers using features selected by four previous investigations. Of these, three algorithms model the spike by mathematical parameters and use them as features for classification while the fourth algorithm uses raw EEG to train the classifier. The objective of this paper is to examine if there is any inherent advantage to any particular set of features, subject to the condition that the same data are used for all feature selection algorithms. Our results suggest that artificial neural networks trained with features selected using any one of the above three algorithms as well as raw EEG directly fed to the ANN will yield similar results.
C1 McMaster Univ, Dept Med, Hamilton, ON L8N 3Z5, Canada.
   McMaster Univ, Dept Elect & Comp Engn, Hamilton, ON L8N 3Z5, Canada.
RP Kamath, MV (corresponding author), McMaster Univ, Dept Med, Room 3E25,Hlth Sci Bldg, Hamilton, ON L8N 3Z5, Canada.
EM kamathm@mcmail.cis.mcmaster.ca
CR DAUBECHIES I, 1990, IEEE T INFORM THEORY, V36, P961, DOI 10.1109/18.57199
   Demuth H., 2003, NEURAL NETWORK TOOLB
   EBERHART RC, 1990, NEURAL NETWORK PC TO
   GOTMAN J, 1979, ELECTROEN CLIN NEURO, V46, P510, DOI 10.1016/0013-4694(79)90004-X
   GOTMAN J, 1982, ELECTROEN CLIN NEURO, V54, P530, DOI 10.1016/0013-4694(82)90038-4
   Gotman J, 1997, ELECTROEN CLIN NEURO, V103, P356, DOI 10.1016/S0013-4694(97)00003-9
   HAUKIN S, 1994, NEURAL NETWORKS COMP
   HJORTH B, 1973, ELECTROENCEPHALOGR, V34, P306
   KALAYCI T, 1995, IEEE ENG MED BIOL, V14, P160, DOI 10.1109/51.376754
   KILOH LG, 1981, CLIN ELECTROENCEPHAL
   KLOPPEL B, 1994, NEUROPSYCHOBIOLOGY, V29, P33, DOI 10.1159/000119060
   Kurth C, 2000, ANN BIOMED ENG, V28, P1362, DOI 10.1114/1.1331312
   NIEDERMEYER Ernst., 1982, ELECTROEN CLIN NEURO
   Osorio I, 1998, EPILEPSIA, V39, P615, DOI 10.1111/j.1528-1157.1998.tb01430.x
   Özdamar Ö, 1998, COMPUT BIOMED RES, V31, P122, DOI 10.1006/cbmr.1998.1475
   PANG CC, 2001, THESIS MCMASTER U HA
   Qu H, 1997, IEEE T BIO-MED ENG, V44, P115, DOI 10.1109/10.552241
   Tarassenko L, 1998, IEE P-SCI MEAS TECH, V145, P270, DOI 10.1049/ip-smt:19982328
   WALMSLEY M, 1984, IEEE T BIO-MED ENG, V31, P720, DOI 10.1109/TBME.1984.325397
   Webber WRS, 1996, ELECTROEN CLIN NEURO, V98, P250, DOI 10.1016/0013-4694(95)00277-4
   Weng W, 1996, NEURAL NETWORKS, V9, P1223, DOI 10.1016/0893-6080(96)00032-9
NR 21
TC 80
Z9 84
U1 0
U2 5
PD APR
PY 2003
VL 50
IS 4
BP 521
EP 526
DI 10.1109/TBME.2003.809479
WC Engineering, Biomedical
DA 2023-11-11
ER

PT J
AU Lobov, SA
   Mikhaylov, AN
   Shamshin, M
   Makarov, VA
   Kazantsev, VB
AF Lobov, Sergey A.
   Mikhaylov, Alexey N.
   Shamshin, Maxim
   Makarov, Valeri A.
   Kazantsev, Victor B.
TI Spatial Properties of STDP in a Self-Learning Spiking Neural Network
   Enable Controlling a Mobile Robot
SO FRONTIERS IN NEUROSCIENCE
DT Article
DE spiking neural networks; spike-timing-dependent plasticity; learning;
   neurorobotics; neuroanimat; synaptic competition; neural competition;
   memristive devices
ID MODEL; NEURONS; BRAINS
AB Development of spiking neural networks (SNNs) controlling mobile robots is one of the modern challenges in computational neuroscience and artificial intelligence. Such networks, being replicas of biological ones, are expected to have a higher computational potential than traditional artificial neural networks (ANNs). The critical problem is in the design of robust learning algorithms aimed at building a "living computer" based on SNNs. Here, we propose a simple SNN equipped with a Hebbian rule in the form of spike-timing-dependent plasticity (STDP). The SNN implements associative learning by exploiting the spatial properties of STDP. We show that a LEGO robot controlled by the SNN can exhibit classical and operant conditioning. Competition of spike-conducting pathways in the SNN plays a fundamental role in establishing associations of neural connections. It replaces the irrelevant associations by new ones in response to a change in stimuli. Thus, the robot gets the ability to relearn when the environment changes. The proposed SNN and the stimulation protocol can be further enhanced and tested in developing neuronal cultures, and also admit the use of memristive devices for hardware implementation.
C1 [Lobov, Sergey A.; Mikhaylov, Alexey N.; Shamshin, Maxim; Makarov, Valeri A.; Kazantsev, Victor B.] Lobachevsky State Univ Nizhny Novgorod, Neurotechnol Dept, Nizhnii Novgorod, Russia.
   [Lobov, Sergey A.; Kazantsev, Victor B.] Innopolis Univ, Ctr Technol Robot & Mechatron Components, Neurosci & Cognit Technol Lab, Innopolis, Russia.
   [Makarov, Valeri A.] Univ Complutense Madrid, Fac Ciencias Matemat, Inst Matemat Interdisciplinar, Madrid, Spain.
RP Lobov, SA (corresponding author), Lobachevsky State Univ Nizhny Novgorod, Neurotechnol Dept, Nizhnii Novgorod, Russia.; Lobov, SA (corresponding author), Innopolis Univ, Ctr Technol Robot & Mechatron Components, Neurosci & Cognit Technol Lab, Innopolis, Russia.
EM lobov@neuro.nnov.ru
CR Bakkum DJ, 2008, J NEURAL ENG, V5, P310, DOI 10.1088/1741-2560/5/3/004
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Calvo Tapia C, 2020, COMMUN NONLINEAR SCI, V82, DOI 10.1016/j.cnsns.2019.105065
   Tapia CC, 2018, PHYS REV E, V97, DOI 10.1103/PhysRevE.97.052308
   Chou TS, 2015, FRONT NEUROROBOTICS, V9, DOI 10.3389/fnbot.2015.00006
   Clopath C, 2010, NAT NEUROSCI, V13, P344, DOI 10.1038/nn.2479
   Dauth S, 2017, J NEUROPHYSIOL, V117, P1320, DOI 10.1152/jn.00575.2016
   Dayan P., 2001, THEORETICAL NEUROSCI
   DeMarse TB, 2001, AUTON ROBOT, V11, P305, DOI 10.1023/A:1012407611130
   Du C, 2015, ADV FUNCT MATER, V25, P4290, DOI 10.1002/adfm.201501427
   Emelyanov AV, 2019, MICROELECTRON ENG, V215, DOI 10.1016/j.mee.2019.110988
   Esser SK, 2016, P NATL ACAD SCI USA, V113, P11441, DOI 10.1073/pnas.1604850113
   Gladkov A, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-15506-2
   Gong PL, 2009, PLOS COMPUT BIOL, V5, DOI 10.1371/journal.pcbi.1000611
   Gorban AN, 2019, PHYS LIFE REV, V29, P55, DOI 10.1016/j.plrev.2018.09.005
   HEBB D. O., 1949
   Hong S., 2010, P 14 INT C COMP SUPP
   Houk J., 1995, MODEL INF PROCESS BA, V13
   Hull C. L., 1943, PRINCIPLES BEHAV INT
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2002, BIOSYSTEMS, V67, P95, DOI 10.1016/S0303-2647(02)00067-9
   Izhikevich EM, 2007, CEREB CORTEX, V17, P2443, DOI 10.1093/cercor/bhl152
   Kandel E, 2000, PRINCIPLES NEURAL SC
   Kim S, 2015, NANO LETT, V15, P2203, DOI 10.1021/acs.nanolett.5b00697
   KOHONEN T, 1982, BIOL CYBERN, V43, P59, DOI 10.1007/BF00337288
   Kuzum D, 2013, NANOTECHNOLOGY, V24, DOI 10.1088/0957-4484/24/38/382001
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li C, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351877
   Liu JX, 2019, IEEE T NEUR NET LEAR, V30, P865, DOI 10.1109/TNNLS.2018.2854291
   Lobov S., 2017, P 5 INT C NEUR EL IN
   Lobov SA, 2017, MATH MODEL NAT PHENO, V12, P109, DOI 10.1051/mmnp/201712409
   Lobov SA, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20020500
   Malishev E, 2015, J PHYS CONF SER, V643, DOI 10.1088/1742-6596/643/1/012025
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   Meyer J.-A., 1991, P 1 INT C SIM AD BEH
   MILO V, 2017, INT EL DEVICES MEET
   Minnekhanov AA, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-47263-9
   Morrison A, 2008, BIOL CYBERN, V98, P459, DOI 10.1007/s00422-008-0233-1
   Palmer JHC, 2014, FRONT COMPUT NEUROSC, V8, DOI 10.3389/fncom.2014.00079
   Pamies D, 2014, EXP BIOL MED, V239, P1096, DOI 10.1177/1535370214537738
   Pavlov IP, 2010, ANN NEUROSCI, V17, P136, DOI 10.5214/ans.0972-7531.1017309
   Pfister JP, 2006, J NEUROSCI, V26, P9673, DOI 10.1523/JNEUROSCI.1425-06.2006
   Pimashkin A, 2016, COGN NEURODYNAMICS, V10, P287, DOI 10.1007/s11571-016-9380-6
   Pimashkin A, 2013, FRONT NEURAL CIRCUIT, V7, DOI 10.3389/fncir.2013.00087
   Potter S, 1997, SIGHT SOUND, P4
   Reger BD, 2000, ARTIF LIFE, V6, P307, DOI 10.1162/106454600300103656
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Shahaf G, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.1000228
   Sjöström PJ, 2001, NEURON, V32, P1149, DOI 10.1016/s0896-6273(01)00542-6
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Tan ZH, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-00849-7
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Tsodyks M, 1998, NEURAL COMPUT, V10, P821, DOI 10.1162/089976698300017502
   Tyukin I, 2019, B MATH BIOL, V81, P4856, DOI 10.1007/s11538-018-0415-5
   Wade J. J., 2008, PROCEEDINGS OF THE 2
   Wang FZ, 2019, J APPL PHYS, V125, DOI 10.1063/1.5042281
   Wang W, 2018, SCI ADV, V4, DOI 10.1126/sciadv.aat4752
   Xu Y, 2013, NEURAL NETWORKS, V43, P99, DOI 10.1016/j.neunet.2013.02.003
   Yu SM, 2011, IEEE T ELECTRON DEV, V58, P2729, DOI 10.1109/TED.2011.2147791
   Zamarreño-Ramos C, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00026
   Ziegler M, 2012, ADV FUNCT MATER, V22, P2744, DOI 10.1002/adfm.201200244
NR 62
TC 61
Z9 61
U1 2
U2 32
PD FEB 26
PY 2020
VL 14
AR 88
DI 10.3389/fnins.2020.00088
WC Neurosciences
DA 2023-11-11
ER

PT J
AU Gardner, B
   Grüning, A
AF Gardner, Brian
   Gruening, Andre
TI Supervised Learning With First-to-Spike Decoding in Multilayer Spiking
   Neural Networks
SO FRONTIERS IN COMPUTATIONAL NEUROSCIENCE
DT Article
DE spiking neural networks; multilayer SNN; supervised learning;
   backpropagation; temporal coding; classification; MNIST
ID BACKPROPAGATION; CLASSIFICATION; PLASTICITY; NEURONS; MODELS; RULE
AB Experimental studies support the notion of spike-based neuronal information processing in the brain, with neural circuits exhibiting a wide range of temporally-based coding strategies to rapidly and efficiently represent sensory stimuli. Accordingly, it would be desirable to apply spike-based computation to tackling real-world challenges, and in particular transferring such theory to neuromorphic systems for low-power embedded applications. Motivated by this, we propose a new supervised learning method that can train multilayer spiking neural networks to solve classification problems based on a rapid, first-to-spike decoding strategy. The proposed learning rule supports multiple spikes fired by stochastic hidden neurons, and yet is stable by relying on first-spike responses generated by a deterministic output layer. In addition to this, we also explore several distinct, spike-based encoding strategies in order to form compact representations of presented input data. We demonstrate the classification performance of the learning rule as applied to several benchmark datasets, including MNIST. The learning rule is capable of generalizing from the data, and is successful even when used with constrained network architectures containing few input and hidden layer neurons. Furthermore, we highlight a novel encoding strategy, termed "scanline encoding," that can transform image data into compact spatiotemporal patterns for subsequent network processing. Designing constrained, but optimized, network structures and performing input dimensionality reduction has strong implications for neuromorphic applications.
C1 [Gardner, Brian] Univ Surrey, Dept Comp Sci, Guildford, Surrey, England.
   [Gruening, Andre] Univ Appl Sci, Fac Elect Engn & Comp Sci, Stralsund, Germany.
RP Gardner, B (corresponding author), Univ Surrey, Dept Comp Sci, Guildford, Surrey, England.
EM b.gardner@surrey.ac.uk
CR Albers C, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0148948
   Bagheri A, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P2986, DOI 10.1109/ICASSP.2018.8462410
   Bellec G, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-17236-y
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Booij O, 2005, INFORM PROCESS LETT, V95, P552, DOI 10.1016/j.ipl.2005.05.023
   Brea J, 2013, J NEUROSCI, V33, P9565, DOI 10.1523/JNEUROSCI.4098-12.2013
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Diehl PU, 2015, IEEE IJCNN
   Fisher RA, 1936, ANN EUGENIC, V7, P179, DOI 10.1111/j.1469-1809.1936.tb02137.x
   Florian RV, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0040233
   Frémaux N, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003024
   Friedmann S, 2017, IEEE T BIOMED CIRC S, V11, P128, DOI 10.1109/TBCAS.2016.2579164
   Gardner B, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0161335
   Gardner B, 2015, NEURAL COMPUT, V27, P2548, DOI 10.1162/NECO_a_00790
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   Gerstner W., 2002, SPIKING NEURON MODEL, DOI [DOI 10.1017/CBO9780511815706, 10.1017/cbo9780511815706]
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   Gollisch T, 2008, SCIENCE, V319, P1108, DOI 10.1126/science.1149639
   Grüning A, 2012, NEURAL PROCESS LETT, V36, P117, DOI 10.1007/s11063-012-9225-1
   Gruning A., 2020, ARXIV PREPRINT ARXIV
   Gruning A., 2014, COMPUT INTELL-US
   Gütig R, 2014, CURR OPIN NEUROBIOL, V25, P134, DOI 10.1016/j.conb.2014.01.004
   Gütig R, 2009, PLOS BIOL, V7, DOI 10.1371/journal.pbio.1000141
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Hinton G., 2012, RMSPROP DIVIDE GRADI
   Hung CP, 2005, SCIENCE, V310, P863, DOI 10.1126/science.1117593
   Jang H, 2019, IEEE SIGNAL PROC MAG, V36, P64, DOI 10.1109/MSP.2019.2935234
   Kheradpisheh SR, 2020, INT J NEURAL SYST, V30, DOI 10.1142/S0129065720500276
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Kiani R, 2005, J NEUROPHYSIOL, V94, P1587, DOI 10.1152/jn.00540.2004
   Klikauer T, 2016, TRIPLEC-COMMUN CAPIT, V14, P260
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508
   Lin CK, 2018, COMPUTER, V51, P52, DOI 10.1109/MC.2018.157113521
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Memmesheimer RM, 2014, NEURON, V82, P925, DOI 10.1016/j.neuron.2014.03.026
   Mohemmed A, 2012, INT J NEURAL SYST, V22, DOI 10.1142/S0129065712500128
   Morrison A, 2008, BIOL CYBERN, V98, P459, DOI 10.1007/s00422-008-0233-1
   Mostafa Hesham, 2018, IEEE Trans Neural Netw Learn Syst, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Neftci E, 2014, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00272
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   Pfister JP, 2006, NEURAL COMPUT, V18, P1318, DOI 10.1162/neco.2006.18.6.1318
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Rezende DJ, 2014, FRONT COMPUT NEUROSC, V8, DOI 10.3389/fncom.2014.00038
   Simard PY, 2003, PROC INT CONF DOC, P958
   Simeone O., 2020, ARXIV PREPRINT ARXIV
   Sporea I, 2013, NEURAL COMPUT, V25, P473, DOI 10.1162/NECO_a_00396
   Tavanaei A, 2019, NEUROCOMPUTING, V330, P39, DOI 10.1016/j.neucom.2018.11.014
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   Urbanczik R, 2009, NEURAL COMPUT, V21, P340, DOI 10.1162/neco.2008.09-07-605
   van Rossum MCW, 2000, J NEUROSCI, V20, P8812
   VanRullen R, 2005, TRENDS NEUROSCI, V28, P1, DOI 10.1016/j.tins.2004.10.010
   WOLBERG WH, 1990, P NATL ACAD SCI USA, V87, P9193, DOI 10.1073/pnas.87.23.9193
   Xiao Han, 2017, ARXIV170807747, P4321
   Yu Q, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0078318
   Zenke F., 2020, REMARKABLE ROBUSTNES, DOI [10.1101/2020.06.29.176925, DOI 10.1101/2020.06.29.176925]
   Zenke F, 2018, NEURAL COMPUT, V30, P1514, DOI 10.1162/neco_a_01086
NR 59
TC 1
Z9 1
U1 1
U2 10
PD APR 12
PY 2021
VL 15
AR 617862
DI 10.3389/fncom.2021.617862
WC Mathematical & Computational Biology; Neurosciences
DA 2023-11-11
ER

PT J
AU Luo, YL
   Wan, L
   Liu, JX
   Harkin, J
   McDaid, L
   Cao, Y
   Ding, XM
AF Luo, Yuling
   Wan, Lei
   Liu, Junxiu
   Harkin, Jim
   McDaid, Liam
   Cao, Yi
   Ding, Xuemei
TI Low Cost Interconnected Architecture for the Hardware Spiking Neural
   Networks
SO FRONTIERS IN NEUROSCIENCE
DT Article
DE interconnected architecture; spiking neural networks; Networks-on-Chip;
   system scalability; arbitration scheme
ID ON-CHIP; ROUTING ALGORITHM; ADAPTIVE NETWORK; SYSTEM
AB A novel low cost interconnected architecture (LCIA) is proposed in this paper, which is an efficient solution for the neuron interconnections for the hardware spiking neural networks (SNNs). It is based on an all-to-all connection that takes each paired input and output nodes of multi-layer SNNs as the source and destination of connections. The aim is to maintain an efficient routing performance under low hardware overhead. A Networks-on-Chip (NoC) router is proposed as the fundamental component of the LCIA, where an effective scheduler is designed to address the traffic challenge due to irregular spikes. The router can find requests rapidly, make the arbitration decision promptly, and provide equal services to different network traffic requests. Experimental results show that the LCIA can manage the intercommunication of the multi-layer neural networks efficiently and have a low hardware overhead which can maintain the scalability of hardware SNNs.
C1 [Luo, Yuling; Wan, Lei; Liu, Junxiu] Guangxi Normal Univ, Fac Elect Engn, Guilin, Peoples R China.
   [Harkin, Jim; McDaid, Liam; Ding, Xuemei] Univ Ulster, Sch Comp Engn & Intelligent Syst, Coleraine, Londonderry, North Ireland.
   [Cao, Yi] Univ Edinburgh, Business Sch, Management Sci & Business Econ Grp, Edinburgh, Midlothian, Scotland.
   [Ding, Xuemei] Fujian Normal Univ, Coll Math & Informat, Fuzhou, Fujian, Peoples R China.
RP Liu, JX (corresponding author), Guangxi Normal Univ, Fac Elect Engn, Guilin, Peoples R China.
EM j.liu@ieee.org
CR Abdali E. M, 2017, P 12 INT S REC COMM, P1, DOI [10.1109/ReCoSoC.2017.8016160, DOI 10.1109/RECOSOC.2017.8016160]
   Agarwal A., 2009, J ENG COMPUT ARCHITE, V3, P21
   Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   Ang CH, 2012, ELECTRON LETT, V48, P145, DOI 10.1049/el.2011.3651
   Basu A, 2010, IEEE T BIOMED CIRC S, V4, P311, DOI 10.1109/TBCAS.2010.2055157
   Benini L, 2002, COMPUTER, V35, P70, DOI 10.1109/2.976921
   Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Billaudelle S, 2016, ARXIV150502142
   Carrillo S, 2013, IEEE T PARALL DISTR, V24, P2451, DOI 10.1109/TPDS.2012.289
   Carrillo S, 2012, NEURAL NETWORKS, V33, P42, DOI 10.1016/j.neunet.2012.04.004
   Carrillo S, 2010, LECT NOTES COMPUT SC, V6274, P133, DOI 10.1007/978-3-642-15323-5_12
   Cawley S, 2011, GENET PROGRAM EVOL M, V12, P257, DOI 10.1007/s10710-011-9130-9
   Charleston-Villalobos S, 2011, MED BIOL ENG COMPUT, V49, P15, DOI 10.1007/s11517-010-0663-5
   Cios K. J., 1997, NEUROCOMPUTING, V16, P259, DOI [10.1007/978-1-59745-520-6_8, DOI 10.1007/978-1-59745-520-6_8]
   Cui YH, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-26436-y
   Dally W. J., 2004, PRINCIPLES PRACTICES
   Dally WJ, 2001, DES AUT CON, P684, DOI 10.1109/DAC.2001.935594
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Emery R, 2009, 2009 3RD ACM/IEEE INTERNATIONAL SYMPOSIUM ON NETWORKS-ON-CHIP, P144, DOI 10.1109/NOCS.2009.5071462
   Fidjeland Andreas K, 2010, 2010 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2010.5596678
   Furber SB, 2013, IEEE T COMPUT, V62, P2454, DOI 10.1109/TC.2012.142
   Gerstner W., 2002, SPIKING NEURON MODEL, DOI [DOI 10.1017/CBO9780511815706, 10.1017/cbo9780511815706]
   Graas EL, 2004, NEUROINFORMATICS, V2, P417, DOI 10.1385/NI:2:4:417
   Harkin J, 2009, INT J RECONFIGURABLE, V2009, DOI 10.1155/2009/908740
   Hu JC, 2004, ICCAD-2004: INTERNATIONAL CONFERENCE ON COMPUTER AIDED DESIGN, IEEE/ACM DIGEST OF TECHNICAL PAPERS, P354, DOI 10.1109/ICCAD.2004.1382601
   Jin X, 2010, COMPUT SCI ENG, V12, P91, DOI 10.1109/MCSE.2010.112
   Jin-xiang Wang, 2010, 2010 10th IEEE International Conference on Solid-State and Integrated Circuit Technology (ICSICT), P382, DOI 10.1109/ICSICT.2010.5667710
   Jordan J, 2018, FRONT NEUROINFORM, V12, DOI 10.3389/fninf.2018.00002
   Jovanovic S, 2009, MICROPROCESS MICROSY, V33, P24, DOI 10.1016/j.micpro.2008.08.004
   Kepa K, 2009, I C FIELD PROG LOGIC, P403, DOI 10.1109/FPL.2009.5272250
   KLEFENZ F, 1992, CERN REPORT, V92, P799
   Kulkarni S., 2012, 2012 4th International Conference on Computational Intelligence and Communication Networks (CICN 2012), P804, DOI 10.1109/CICN.2012.26
   Kwon H, 2017, 11 IEEE ACM INT S NE, V2017, P1, DOI [10.1145/3130218.3130230, DOI 10.1145/3130218.3130230]
   Kwon H., 2018, COMPUTING RES REPOSI, P1
   Lagorce X, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00206
   Liu JX, 2016, IEEE T COMPUT AID D, V35, P260, DOI 10.1109/TCAD.2015.2459050
   Liu JX, 2015, MICROPROCESS MICROSY, V39, P358, DOI 10.1016/j.micpro.2015.06.002
   Luo YL, 2018, NEURAL PROCESS LETT, V48, P1777, DOI 10.1007/s11063-018-9797-5
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Moctezuma JC, 2015, MICROPROCESS MICROSY, V39, P693, DOI 10.1016/j.micpro.2015.09.003
   Mohammadi R, 2015, LECT NOTES COMP SCI, V9491, P356
   Moscibroda T, 2009, CONF PROC INT SYMP C, P196, DOI 10.1145/1555815.1555781
   Nageswaran Jayram Moorkanikara, 2009, Proceedings 2009 International Joint Conference on Neural Networks (IJCNN 2009 - Atlanta), P2145, DOI 10.1109/IJCNN.2009.5179043
   Painkras E, 2013, THESIS, P19
   Painkras E, 2013, IEEE J SOLID-ST CIRC, V48, P1943, DOI 10.1109/JSSC.2013.2259038
   Pande S, 2013, NEURAL PROCESS LETT, V38, P131, DOI 10.1007/s11063-012-9274-5
   Park D, 1999, J TRANSP ENG, V125, P515, DOI 10.1061/(ASCE)0733-947X(1999)125:6(515)
   Perrinet LU, 2008, PROC SPIE, V7000, DOI 10.1117/12.787076
   Rast AD, 2008, IEEE IJCNN, P2727, DOI 10.1109/IJCNN.2008.4634181
   Sabogal S., 2017, P 31 ANN AIAAUSU C S
   Schemmel J, 2008, IEEE IJCNN, P431, DOI 10.1109/IJCNN.2008.4633828
   Schmitt S, 2017, IEEE IJCNN, P2227, DOI 10.1109/IJCNN.2017.7966125
   Schuman CD., 2017, ARXIV
   Upegui A, 2005, MICROPROCESS MICROSY, V29, P211, DOI 10.1016/j.micpro.2004.08.012
   van Albada SJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00291
   Walter F, 2017, IEEE INT SYMP CIRC S, P2715
   Wan L., 2016, SIGNALS SYSTEMS C, P1
   Wang MC, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P3184, DOI 10.1109/IJCNN.2011.6033643
   Wang R, 2015, COMPUT SCI, V7, P1
   Wu QX, 2008, NEUROCOMPUTING, V71, P2055, DOI 10.1016/j.neucom.2007.10.020
   Wysoski SG, 2008, NEUROCOMPUTING, V71, P2563, DOI 10.1016/j.neucom.2007.12.038
   Zhang Z, 2008, DES AUT CON, P441
   Zheng SQ, 2007, IEEE T PARALL DISTR, V18, P84, DOI 10.1109/TPDS.2007.253283
NR 63
TC 11
Z9 11
U1 0
U2 4
PD NOV 21
PY 2018
VL 12
AR 857
DI 10.3389/fnins.2018.00857
WC Neurosciences
DA 2023-11-11
ER

PT J
AU Ben Abdallah, A
   Dang, KN
AF Ben Abdallah, Abderazek
   Dang, Khanh N.
TI Toward Robust Cognitive 3D Brain-Inspired Cross-Paradigm System
SO FRONTIERS IN NEUROSCIENCE
DT Article
DE spiking neural network; neuromorphic; 3D-ICs; fault-tolerance; mapping
   algorithm
ID 3-D ICS; ALGORITHM; DESIGN
AB Spiking Neuromorphic systems have been introduced as promising platforms for energy-efficient spiking neural network (SNNs) execution. SNNs incorporate neuronal and synaptic states in addition to the variant time scale into their computational model. Since each neuron in these networks is connected to many others, high bandwidth is required. Moreover, since the spike times are used to encode information in SNN, a precise communication latency is also needed, although SNN is tolerant to the spike delay variation in some limits when it is seen as a whole. The two-dimensional packet-switched network-on-chip was proposed as a solution to provide a scalable interconnect fabric in large-scale spike-based neural networks. The 3D-ICs have also attracted a lot of attention as a potential solution to resolve the interconnect bottleneck. Combining these two emerging technologies provides a new horizon for IC design to satisfy the high requirements of low power and small footprint in emerging AI applications. Moreover, although fault-tolerance is a natural feature of biological systems, integrating many computation and memory units into neuromorphic chips confronts the reliability issue, where a defective part can affect the overall system's performance. This paper presents the design and simulation of R-NASH-a reliable three-dimensional digital neuromorphic system geared explicitly toward the 3D-ICs biological brain's three-dimensional structure, where information in the network is represented by sparse patterns of spike timing and learning is based on the local spike-timing-dependent-plasticity rule. Our platform enables high integration density and small spike delay of spiking networks and features a scalable design. R-NASH is a design based on the Through-Silicon-Via technology, facilitating spiking neural network implementation on clustered neurons based on Network-on-Chip. We provide a memory interface with the host CPU, allowing for online training and inference of spiking neural networks. Moreover, R-NASH supports fault recovery with graceful performance degradation.
C1 [Ben Abdallah, Abderazek; Dang, Khanh N.] Univ Aizu, Grad Sch Comp Sci & Engn, Adapt Syst Lab, Aizu Wakamatsu, Fukushima, Japan.
   [Dang, Khanh N.] Vietnam Natl Univ, VNU Univ Engn & Technol, VNU Key Lab Smart Integrated Syst SISLAB, Hanoi, Vietnam.
RP Ben Abdallah, A (corresponding author), Univ Aizu, Grad Sch Comp Sci & Engn, Adapt Syst Lab, Aizu Wakamatsu, Fukushima, Japan.
EM benab@u-aizu.ac.jp
CR Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   Arka AI, 2021, ACM T DES AUTOMAT EL, V26, DOI 10.1145/3424239
   Bamford SA, 2010, IEEE T NEURAL NETWOR, V21, P286, DOI 10.1109/TNN.2009.2036912
   Banerjee K, 2001, P IEEE, V89, P602, DOI 10.1109/5.929647
   Ben Ahmed A, 2014, J PARALLEL DISTR COM, V74, P2229, DOI 10.1016/j.jpdc.2014.01.002
   Ben Ahmed A, 2013, J SUPERCOMPUT, V66, P1507, DOI 10.1007/s11227-013-0940-9
   Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Chen GK, 2019, IEEE J SOLID-ST CIRC, V54, P992, DOI 10.1109/JSSC.2018.2884901
   Dang Khanh N., 2019, 2019 International Conference on Internet of Things, Embedded Systems and Communications (IINTEC). Proceedings, P155, DOI 10.1109/IINTEC48298.2019.9112123
   Dang KN, 2020, IEEE T VLSI SYST, V28, P672, DOI 10.1109/TVLSI.2019.2948878
   Dang KN, 2020, IEEE T EMERG TOP COM, V8, P577, DOI 10.1109/TETC.2017.2762407
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Deb K, 2002, IEEE T EVOLUT COMPUT, V6, P182, DOI 10.1109/4235.996017
   Diehl PU, 2015, IEEE IJCNN
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Eliasmith C, 2012, SCIENCE, V338, P1202, DOI 10.1126/science.1225266
   Frenkel C, 2019, IEEE T BIOMED CIRC S, V13, P999, DOI 10.1109/TBCAS.2019.2928793
   Frenkel C, 2019, IEEE T BIOMED CIRC S, V13, P145, DOI 10.1109/TBCAS.2018.2880425
   Furber S, 2016, J NEURAL ENG, V13, DOI 10.1088/1741-2560/13/5/051001
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   Goldwyn JH, 2011, PHYS REV E, V83, DOI 10.1103/PhysRevE.83.041908
   Hazan H, 2018, FRONT NEUROINFORM, V12, DOI 10.3389/fninf.2018.00089
   HSIAO MY, 1970, IBM J RES DEV, V14, P395, DOI 10.1147/rd.144.0395
   Ikechukwu OM, 2021, IEEE ACCESS, V9, P64331, DOI 10.1109/ACCESS.2021.3071089
   Ikechukwu OM, 2020, INT CONF BIG DATA, P133, DOI 10.1109/BigComp48618.2020.00-86
   Jin X., 2010, PARALLEL SIMULATION
   Joseph J.M., 2021, INORG NANO-MET CHEM, P1
   Jung Kuk Kim, 2015, 2015 Symposium on VLSI Circuits (VLSI Circuits), pC50, DOI 10.1109/VLSIC.2015.7231323
   Dang KN, 2022, IEEE T COMPUT AID D, V41, P799, DOI 10.1109/TCAD.2021.3069370
   Kim S, 2011, 2011 11TH INTERNATIONAL CONFERENCE ON CONTROL, AUTOMATION AND SYSTEMS (ICCAS), P1
   Lee C, 2019, IEEE T COGN DEV SYST, V11, P384, DOI 10.1109/TCDS.2018.2833071
   Lee HG, 2007, ACM T DES AUTOMAT EL, V12, DOI 10.1145/1255456.1255460
   Levin J.A., 2014, Patent No. US, Patent No. [2014/0351190 A1, 20140351190]
   Mahmoodi H, 2009, IEEE T VLSI SYST, V17, P33, DOI 10.1109/TVLSI.2008.2008453
   Merolla P, 2014, IEEE T CIRCUITS-I, V61, P820, DOI 10.1109/TCSI.2013.2284184
   Panth S, 2014, I SYMPOS LOW POWER E, P171, DOI 10.1145/2627369.2627642
   Purves D., 2008, NEUROSCIENCE
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Schemmel J, 2010, IEEE INT SYMP CIRC S, P1947, DOI 10.1109/ISCAS.2010.5536970
   Scholze S, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00117
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Shi C, 2021, IEEE T CIRCUITS-II, V68, P1581, DOI 10.1109/TCSII.2021.3063784
   Stimberg M, 2019, ELIFE, V8, DOI 10.7554/eLife.47314
   Vu TH, 2019, ACM J EMERG TECH COM, V15, DOI 10.1145/3340963
   Waldrop MM, 2016, NATURE, V530, P144, DOI 10.1038/530144a
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Yin SY, 2017, DES AUT CON, DOI 10.1145/3061639.3062232
   Zamarreño-Ramos C, 2013, IEEE T BIOMED CIRC S, V7, P82, DOI 10.1109/TBCAS.2012.2195725
   Zhao MR, 2020, APPL PHYS REV, V7, DOI 10.1063/1.5124915
NR 49
TC 3
Z9 3
U1 1
U2 6
PD JUN 25
PY 2021
VL 15
AR 690208
DI 10.3389/fnins.2021.690208
WC Neurosciences
DA 2023-11-11
ER

PT C
AU Jaoudi, Y
   Yakopcic, C
   Taha, T
AF Jaoudi, Yassine
   Yakopcic, Chris
   Taha, Tarek
GP IEEE
TI Conversion of an Unsupervised Anomaly Detection System to Spiking Neural
   Network for Car Hacking Identification
SO 2020 11TH INTERNATIONAL GREEN AND SUSTAINABLE COMPUTING WORKSHOPS (IGSC)
DT Proceedings Paper
CT 11th International Green and Sustainable Computing Workshop (IGSC)
CY OCT 19-22, 2020
CL ELECTR NETWORK
DE Autoencoder; Spiking Neural Network; Intrusion detection; Controller
   area network; Conversion; Loihi; Neuromorphic processor
AB Across industry, there is an increasing availability of streaming, time-varying data, where it is important to detect anomalous behavior. These data are found in an enormous number of sensor-based applications, in cybersecurity (where anomalous behavior could indicate an attack), and in finance. Spiking Neural Networks (SNNs) have come under the spotlight for machine learning applications due to the extreme energy efficiency of their implementation on neuromorphic processors like the Intel Loihi research chip. In this paper we explore the applicability of spiking neural networks for in vehicle cyberattack detection. We show exemplary results by converting an autoencoder model to spiking form. We present a learning model comparison that shows the proposed SNN autoencoder outperforms a One Class Support Vector Machine and an Isolation Forest. Furthermore, only a slight reduction in accuracy is observed when compared to a traditional autoencoder.
C1 [Jaoudi, Yassine; Yakopcic, Chris; Taha, Tarek] Univ Dayton, Dept Elect & Comp Engn, Dayton, OH 45469 USA.
RP Jaoudi, Y (corresponding author), Univ Dayton, Dept Elect & Comp Engn, Dayton, OH 45469 USA.
EM jaoudiy1@udayton.edu; cyakopcic1@udayton.edu; tarek.taha@udayton.edu
CR Ahmad S, 2017, NEUROCOMPUTING, V262, P134, DOI 10.1016/j.neucom.2017.04.070
   [Anonymous], 2013, DEF CON
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Fu TC, 2011, ENG APPL ARTIF INTEL, V24, P164, DOI 10.1016/j.engappai.2010.09.007
   Habeeb RAA, 2019, INT J INFORM MANAGE, V45, P289, DOI 10.1016/j.ijinfomgt.2018.08.006
   Kang MJ, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0155781
   Lee H, 2017, ANN CONF PRIV SECUR, P57, DOI 10.1109/PST.2017.00017
   Müter M, 2011, IEEE INT VEH SYM, P1110, DOI 10.1109/IVS.2011.5940552
   Prasad NR, 2009, CMC-COMPUT MATER CON, V14, P1, DOI 10.1145/1541880.1541882
   Rasmussen D, 2019, NEUROINFORMATICS, V17, P611, DOI 10.1007/s12021-019-09424-z
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Seo E., 2018 16 ANN C 1 SECU, P1, DOI DOI 10.1109/PST.2018.8514157
   Song HM, 2016, 2016 INTERNATIONAL CONFERENCE ON INFORMATION NETWORKING (ICOIN), P63, DOI 10.1109/ICOIN.2016.7427089
   Taylor A, 2016, PROCEEDINGS OF 3RD IEEE/ACM INTERNATIONAL CONFERENCE ON DATA SCIENCE AND ADVANCED ANALYTICS, (DSAA 2016), P130, DOI 10.1109/DSAA.2016.20
   Wang CD, 2018, IEEE ACCESS, V6, P9091, DOI 10.1109/ACCESS.2018.2799210
   Zang D, 2018, CHIN CONT DECIS CONF, P1059, DOI 10.1109/CCDC.2018.8407286
NR 16
TC 0
Z9 0
U1 0
U2 1
PY 2020
WC Computer Science, Theory & Methods; Green & Sustainable Science &
   Technology; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Romanov, EL
   Novitskaya, YV
AF Romanov, Evgeniy L.
   Novitskaya, Yuliya V.
GP IEEE
TI Program Toolkit for Auditory Scenes' Analysis
SO 2016 13TH INTERNATIONAL SCIENTIFIC-TECHNICAL CONFERENCE ON ACTUAL
   PROBLEMS OF ELECTRONIC INSTRUMENT ENGINEERING (APEIE), VOL 2
SE International Conference on Actual Problems of Electronic Instrument
   Engineering
DT Proceedings Paper
CT 13th International Scientific-Technical Conference on Actual Problems of
   Electronics Instrument Engineering (APEIE)
CY OCT 03-06, 2016
CL Novosibirsk, RUSSIA
DE Auditory scene; spiking neural network; Java; spectrum; cochleagram
AB In this paper the architecture of program toolkit for auditory scenes' analysis and the features of spiking neural networks and neuromorphic structures using are proposed. The programming examples and the test results of main components are given.
C1 [Romanov, Evgeniy L.; Novitskaya, Yuliya V.] Novosibirsk State Tech Univ, Novosibirsk, Russia.
RP Romanov, EL (corresponding author), Novosibirsk State Tech Univ, Novosibirsk, Russia.
CR Aleksandrov Yu.I., 2008, NEURON SIGNAL PROCES
   [Anonymous], SWARM ALGORITHMS
   [Anonymous], API DAT PAR JAV EL R
   Gavrilov Andrey V., 2016, P 11 INT FOR STRAT T
   Karpov Y.G., 2010, MODEL CHECKING VERIF
   Kashchenko S.A., 2013, MODELS WAVE MEMORY
   Ma N., EFFICIENT IMPLEMENTA
   Nicholls JG, 2003, NEURON BRAIN
   Romanov E. L., 2016, ROB ART INT P 7 ALL, P155
   Wang D., COMPUTATIONAL AUDITO
NR 10
TC 0
Z9 0
U1 0
U2 0
PY 2016
BP 464
EP 470
WC Engineering, Electrical & Electronic; Instruments & Instrumentation
DA 2023-11-11
ER

PT J
AU Li, XM
   Chen, Q
   Xue, FZ
AF Li, Xiumin
   Chen, Qing
   Xue, Fangzheng
TI Bursting dynamics remarkably improve the performance of neural networks
   on liquid computing
SO COGNITIVE NEURODYNAMICS
DT Article
DE Spiking; Bursting; Liquid computing
ID PANCREATIC BETA-CELLS; STOCHASTIC RESONANCE; SELECTIVE COMMUNICATION;
   SPIKING NEURONS; SYNCHRONIZATION; INFORMATION; PLASTICITY; STATES;
   ORDER; MODEL
AB Burst firings are functionally important behaviors displayed by neural circuits, which plays a primary role in reliable transmission of electrical signals for neuronal communication. However, with respect to the computational capability of neural networks, most of relevant studies are based on the spiking dynamics of individual neurons, while burst firing is seldom considered. In this paper, we carry out a comprehensive study to compare the performance of spiking and bursting dynamics on the capability of liquid computing, which is an effective approach for intelligent computation of neural networks. The results show that neural networks with bursting dynamic have much better computational performance than those with spiking dynamics, especially for complex computational tasks. Further analysis demonstrate that the fast firing pattern of bursting dynamics can obviously enhance the efficiency of synaptic integration from pre-neurons both temporally and spatially. This indicates that bursting dynamic can significantly enhance the complexity of network activity, implying its high efficiency in information processing.
C1 [Li, Xiumin; Chen, Qing; Xue, Fangzheng] Chongqing Univ, Key Lab Dependable Serv Comp Cyber Phys Soc, Minist Educ, Chongqing 400044, Peoples R China.
   [Li, Xiumin; Chen, Qing; Xue, Fangzheng] Chongqing Univ, Coll Automat, Chongqing 400044, Peoples R China.
RP Li, XM (corresponding author), Chongqing Univ, Key Lab Dependable Serv Comp Cyber Phys Soc, Minist Educ, Chongqing 400044, Peoples R China.; Li, XM (corresponding author), Chongqing Univ, Coll Automat, Chongqing 400044, Peoples R China.
EM xmli@cqu.edu.cn
CR BENZI R, 1981, J PHYS A-MATH GEN, V14, pL453, DOI 10.1088/0305-4470/14/11/006
   Bertram R, 2000, J BIOSCIENCE, V25, P197, DOI 10.1007/BF03404915
   Burgsteiner H., 2005, THESIS
   Burgsteiner Harald, 2005, P 9 INT C ENG APPL N, P129
   Dambre J, 1995, AM J HYPERTENS, V8
   Gammaitoni L, 1998, REV MOD PHYS, V70, P223, DOI 10.1103/RevModPhys.70.223
   Gerstner W., 2002, SPIKING NEURON MODEL
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2002, BIOSYSTEMS, V67, P95, DOI 10.1016/S0303-2647(02)00067-9
   Izhikevich EM, 2003, TRENDS NEUROSCI, V26, P161, DOI 10.1016/S0166-2236(03)00034-1
   Jaeger H., 2001, GMD REPORT 148
   Joshi P, 2004, LECT NOTES COMPUT SC, V3141, P258
   Kim SY, 2015, COGN NEURODYNAMICS, V9, P411, DOI 10.1007/s11571-015-9334-4
   Kim SJ, 2015, INT J SECUR APPL, V9, P9
   Kosko B, 2004, PHYS REV E, V70, DOI 10.1103/PhysRevE.70.031911
   Li XM, 2007, PHYS REV E, V76, DOI 10.1103/PhysRevE.76.041902
   Lisman JE, 1997, TRENDS NEUROSCI, V20, P38, DOI 10.1016/S0166-2236(96)10070-9
   Llinás RR, 2006, J NEUROPHYSIOL, V95, P3297, DOI 10.1152/jn.00166.2006
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   MAASS W, 2002, NEURAL INFORM PROCES, P213
   Maass W, 2007, PLOS COMPUT BIOL, V3, P15, DOI 10.1371/journal.pcbi.0020165
   Maass W, 2007, LECT NOTES COMPUT SC, V4497, P507
   Meng P, 2013, COGN NEURODYNAMICS, V7, P197, DOI 10.1007/s11571-012-9226-9
   Moulins M, 1987, CRUSTACEAN STOMATOGA, P330
   Norton D, 2010, NEUROCOMPUTING, V73, P2893, DOI 10.1016/j.neucom.2010.08.005
   Saha AA, 2003, SIGNAL PROCESS, V83, P1193, DOI 10.1016/S0165-1684(03)00039-2
   Schrauwen B, 2008, NEUROCOMPUTING, V71, P1159, DOI 10.1016/j.neucom.2007.12.020
   Sherman SM, 2001, TRENDS NEUROSCI, V24, P122, DOI 10.1016/S0166-2236(00)01714-8
   Shew WL, 2011, J NEUROSCI, V31, P55, DOI 10.1523/JNEUROSCI.4637-10.2011
   Shi X, 2008, COGN NEURODYNAMICS, V2, P195, DOI 10.1007/s11571-008-9055-z
   Sohal VS, 2001, NEURON, V31, P3, DOI 10.1016/S0896-6273(01)00349-X
   WIESENFELD K, 1995, NATURE, V373, P33, DOI 10.1038/373033a0
   Xue FZ, 2013, NEUROCOMPUTING, V122, P324, DOI 10.1016/j.neucom.2013.06.019
NR 34
TC 6
Z9 6
U1 0
U2 13
PD OCT
PY 2016
VL 10
IS 5
BP 415
EP 421
DI 10.1007/s11571-016-9387-z
WC Neurosciences
DA 2023-11-11
ER

PT C
AU Galluppi, F
   Davies, S
   Furber, S
   Stewart, T
   Eliasmith, C
AF Galluppi, Francesco
   Davies, Sergio
   Furber, Steve
   Stewart, Terry
   Eliasmith, Chris
GP IEEE
TI Real Time On-Chip Implementation of Dynamical Systems with Spiking
   Neurons
SO 2012 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUN 10-15, 2012
CL Brisbane, AUSTRALIA
ID MODEL; CIRCUIT
AB Simulation of large-scale networks of spiking neurons has become appealing for understanding the computational principles of the nervous system by producing models based on biological evidence. In particular, networks that can assume a variety of (dynamically) stable states have been proposed as the basis for different behavioural and cognitive functions.
   This work focuses on implementing the Neural Engineering Framework (NEF), a formal method for mapping attractor networks and control-theoretic algorithms to biologically plausible networks of spiking neurons, on the SpiNNaker system, a massive programmable parallel architecture oriented to the simulation of networks of spiking neurons. We describe how to encode and decode analog values to patterns of neural spikes directly on chip. These methods take advantage of the full programmability of the ARM968 cores constituting the processing base of a SpiNNaker node, and exploit the fast Network-on-chip for spike communication.
   In this paper we focus on the fundamentals of representing, transforming and implementing dynamics in spiking networks. We show real time simulation results demonstrating the NEF principles and discuss advantages, precision and scalability. More generally, the present approach can be used to state and test hypotheses with large-scale spiking neural network models for a range of different cognitive functions and behaviours.
C1 [Galluppi, Francesco; Davies, Sergio; Furber, Steve] Univ Manchester, Adv Processor Technol Grp, Manchester M13 9PL, Lancs, England.
   [Stewart, Terry; Eliasmith, Chris] Univ Waterloo, Ctr Theoret Neurosci, Waterloo, ON N2L 3G1, Canada.
RP Galluppi, F (corresponding author), Univ Manchester, Adv Processor Technol Grp, Manchester M13 9PL, Lancs, England.
EM francesco.galluppi@cs.man.ac.uk
CR Amit D. J., 1992, MODELING BRAIN FUNCT
   [Anonymous], 2009, P C HIGH PERFORMANCE
   [Anonymous], 2010, 2010 IEEE INT S PARA
   [Anonymous], 2011, 45 ANN C INFORM SCI
   [Anonymous], P INT C APPL CONC SY, DOI DOI 10.1109/ACSD.2009.17
   Binzegger T, 2004, J NEUROSCI, V24, P8441, DOI 10.1523/JNEUROSCI.1400-04.2004
   CELEBRINI S, 1993, VISUAL NEUROSCI, V10, P811, DOI 10.1017/S0952523800006052
   Dehaene S, 1998, P NATL ACAD SCI USA, V95, P14529, DOI 10.1073/pnas.95.24.14529
   DeSchutter E, 2009, COMPUT NEUROSCI-MIT, P1
   Dethier J, 2011, I IEEE EMBS C NEUR E, P396, DOI 10.1109/NER.2011.5910570
   Eliasmith C, 2005, NEURAL COMPUT, V17, P1276, DOI 10.1162/0899766053630332
   Eliasmith C, 2002, NEUROCOMPUTING, V44, P1071, DOI 10.1016/S0925-2312(02)00418-6
   Eliasmith C, 2012, BUILD BRAIN IN PRESS
   Eliasmith C., 2003, NEURAL ENG COMPUTATI
   FUKUSHIMA K, 1992, PROG NEUROBIOL, V39, P609, DOI 10.1016/0301-0082(92)90016-8
   Furber S., 2006, AISB06 WORKSH GC5 AR
   Furber S B, 2006, ON CHIP INTERCHIP NE
   Galluppi Francesco, 2012, P CF
   Hynna KM, 2006, IEEE INT SYMP CIRC S, P3614
   Indiveri G, 2006, IEEE T NEURAL NETWOR, V17, P211, DOI 10.1109/TNN.2005.860850
   Izhikevich EM, 2008, P NATL ACAD SCI USA, V105, P3593, DOI 10.1073/pnas.0712231105
   Kuo PD, 2005, BIOL CYBERN, V93, P178, DOI 10.1007/s00422-005-0576-9
   LAZZARO J, 1993, IEEE T NEURAL NETWOR, V4, P523, DOI 10.1109/72.217193
   Lewis F., 1994, IEEE T AUTOMAT CONTR, V39, P1773
   Maguire LP, 2007, NEUROCOMPUTING, V71, P13, DOI 10.1016/j.neucom.2006.11.029
   Nageswaran J. M., 2007, NEURAL NETWORKS, V22
   Plana LA, 2007, IEEE DES TEST COMPUT, V24, P454, DOI 10.1109/MDT.2007.149
   Rast Alexander, 2011, NEURAL NETW IN PRESS
   Schemmel J, 2010, IEEE INT SYMP CIRC S, P1947, DOI 10.1109/ISCAS.2010.5536970
   Singh R, 2006, J NEUROSCI, V26, P3667, DOI 10.1523/JNEUROSCI.4864-05.2006
   Stewart TC, 2011, 33 ANN C COGN SCI SO
   Stewart TC, 2009, FRONTIERS NEUROINFOR, V3
   Stewart TC, 2010, FRONT ARTIF INTEL AP, V221, P147, DOI 10.3233/978-1-60750-661-4-147
   Stewart TC, 2011, CONNECT SCI, V23, P145, DOI 10.1080/09540091.2011.571761
   Thomson AM, 2007, FRONTIERS NEUROSCIEN
   Wijekoon JHB, 2008, IEEE INT SYMP CIRC S, P1784, DOI 10.1109/ISCAS.2008.4541785
   Wills TJ, 2005, SCIENCE, V308, P873, DOI 10.1126/science.1108905
NR 37
TC 1
Z9 1
U1 0
U2 4
PY 2012
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT J
AU Sarkar, ST
   Bhondekar, AP
   Macas, M
   Kumar, R
   Kaur, R
   Sharma, A
   Gulati, A
   Kumar, A
AF Sarkar, Sankho Turjo
   Bhondekar, Amol P.
   Macas, Martin
   Kumar, Ritesh
   Kaur, Rishemjit
   Sharma, Anupma
   Gulati, Ashu
   Kumar, Amod
TI Towards biological plausibility of electronic noses: A spiking neural
   network based approach for tea odour classification
SO NEURAL NETWORKS
DT Article
DE Electronic nose; McNemar's test; Spiking neural network; Tea; Spike
   latency coding; Dynamically evolving spiking neural networks
ID SIGNAL-PROCESSING TECHNIQUES; ARTIFICIAL OLFACTORY MUCOSA; NEURONAL
   NETWORK; RECOGNITION; SYSTEM; MODEL
AB The paper presents a novel encoding scheme for neuronal code generation for odour recognition using an electronic nose (EN). This scheme is based on channel encoding using multiple Gaussian receptive fields superimposed over the temporal EN responses. The encoded data is further applied to a spiking neural network (SNN) for pattern classification. Two forms of SNN, a back-propagation based SpikeProp and a dynamic evolving SNN are used to learn the encoded responses. The effects of information encoding on the performance of SNNs have been investigated. Statistical tests have been performed to determine the contribution of the SNN and the encoding scheme to overall odour discrimination. The approach has been implemented in odour classification of orthodox black tea (Kangra-Himachal Pradesh Region) thereby demonstrating a biomimetic approach for EN data analysis. (C) 2015 Elsevier Ltd. All rights reserved.
C1 [Sarkar, Sankho Turjo; Bhondekar, Amol P.; Kumar, Ritesh; Kaur, Rishemjit; Sharma, Anupma; Kumar, Amod] CSIR Cent Sci Instruments Org, Chandigarh, India.
   [Gulati, Ashu] CSIR Inst Himalayan Bioresource Technol, Palampur, Himachal Prades, India.
   [Sarkar, Sankho Turjo; Bhondekar, Amol P.; Kumar, Ritesh; Kaur, Rishemjit; Sharma, Anupma; Kumar, Amod] Acad Sci & Innovat Res, New Delhi, India.
   [Macas, Martin] Czech Tech Univ, CR-16635 Prague, Czech Republic.
RP Bhondekar, AP (corresponding author), CSIR Cent Sci Instruments Org, Chandigarh, India.
EM amol.bhondekar@gmail.com
CR Abdel-Aty-Zohdy HS, 2010, MIDWEST SYMP CIRCUIT, P81, DOI 10.1109/MWSCAS.2010.5548566
   Ache BW, 2005, NEURON, V48, P417, DOI 10.1016/j.neuron.2005.10.022
   Al Yamani J., 2011, FRONTIERS NEUROENGIN, V4
   Allen JN, 2008, IEEE INT SYMP CIRC S, P2178, DOI 10.1109/ISCAS.2008.4541883
   Ambros-Ingerson J., 1990, SCIENCE
   [Anonymous], 2011, GRADING SORTING PACK
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Borah S, 2007, J FOOD ENG, V79, P629, DOI 10.1016/j.jfoodeng.2006.02.022
   Bostanci B, 2013, ADV INTELL SYST, V201, P15, DOI 10.1007/978-81-322-1038-2_2
   Chen HT, 2011, IEEE T BIOMED CIRC S, V5, P160, DOI 10.1109/TBCAS.2010.2075928
   Covington JA, 2007, IET NANOBIOTECHNOL, V1, P15, DOI 10.1049/iet-nbt:20060015
   Delorme A, 2001, NEURAL NETWORKS, V14, P795, DOI 10.1016/S0893-6080(01)00049-1
   Gardner J. W., 2007, SOL STAT SENS ACT MI, P2465
   Gardner JW, 2009, IEEE SENS J, V9, P929, DOI 10.1109/JSEN.2009.2024856
   Gerstner W., 2002, SPIKING NEURON MODEL
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Gutierrez-Galvez A, 2006, SENSOR ACTUAT B-CHEM, V116, P29, DOI 10.1016/j.snb.2005.11.081
   Hojjat A., 2010, AUTOMATED EEG BASED
   Hsieh HY, 2013, IEEE T NEUR NET LEAR, V24, P2063, DOI 10.1109/TNNLS.2013.2271644
   Hsieh HY, 2012, 2012 IEEE ASIA PACIFIC CONFERENCE ON CIRCUITS AND SYSTEMS (APCCAS), P88, DOI 10.1109/APCCAS.2012.6418978
   Hsieh HY, 2012, IEEE T NEUR NET LEAR, V23, P1065, DOI 10.1109/TNNLS.2012.2195329
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Kasabov N, 2013, NEURAL NETWORKS, V41, P188, DOI 10.1016/j.neunet.2012.11.014
   Kaur R., 2012, SENSORS ACTUATORS B
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maass W, 2001, PULSED NEURAL NETWOR
   Martinelli E., 2011, FRONTIERS NEUROENGIN, V4
   Martinelli E, 2006, SENSOR ACTUAT B-CHEM, V119, P234, DOI 10.1016/j.snb.2005.12.029
   Masaru F, 2008, IEEE IJCNN, P840, DOI 10.1109/IJCNN.2008.4633895
   Natschlager T, 1998, NETWORK-COMP NEURAL, V9, P319, DOI 10.1088/0954-898X/9/3/003
   Nski A. K., 2011, ACTA NEUROBIOLOGIAE, V71, P409
   Pearce TC, 2001, NEUROCOMPUTING, V38, P299, DOI 10.1016/S0925-2312(01)00455-6
   PERSAUD K, 1982, NATURE, V299, P352, DOI 10.1038/299352a0
   Raman B., 2004, CHEMOSENSORY PROCESS, V2, P3
   Raman B, 2008, ANAL CHEM, V80, P8364, DOI 10.1021/ac8007048
   Raman B, 2007, IEEE SENS J, V7, P506, DOI 10.1109/JSEN.2007.891935
   Ratton L, 1997, SENSOR ACTUAT B-CHEM, V41, P105, DOI 10.1016/S0925-4005(97)80283-3
   Salzberg S. J. H. U., 1997, DATA MIN KNOWL DISC, V328, P317
   Smear M, 2011, NATURE, V479, P397, DOI 10.1038/nature10521
   SNIPPE HP, 1992, BIOL CYBERN, V66, P543, DOI 10.1007/BF00204120
   Thorpe S, 1998, COMPUTATIONAL NEUROSCIENCE: TRENDS IN RESEARCH, P113
   Wakamatsu T, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P951, DOI 10.1109/IJCNN.2011.6033325
   White J, 1999, NEUROCOMPUTING, V26-7, P919, DOI 10.1016/S0925-2312(98)00137-4
   White J, 1998, BIOL CYBERN, V78, P245, DOI 10.1007/s004220050430
   Wu QX, 2006, NEUROCOMPUTING, V69, P1912, DOI 10.1016/j.neucom.2005.11.023
NR 45
TC 20
Z9 22
U1 4
U2 43
PD NOV
PY 2015
VL 71
BP 142
EP 149
DI 10.1016/j.neunet.2015.07.014
WC Computer Science, Artificial Intelligence; Neurosciences
DA 2023-11-11
ER

PT C
AU Ciurletti, M
   Traub, M
   Karlbauer, M
   Butz, MV
   Otte, S
AF Ciurletti, Melvin
   Traub, Manuel
   Karlbauer, Matthias
   Butz, Martin, V
   Otte, Sebastian
BE Farkas, I
   Masulli, P
   Otte, S
   Wermter, S
TI Signal Denoising with Recurrent Spiking Neural Networks and Active
   Tuning
SO ARTIFICIAL NEURAL NETWORKS AND MACHINE LEARNING, ICANN 2021, PT V
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 30th International Conference on Artificial Neural Networks (ICANN)
CY SEP 14-17, 2021
CL ELECTR NETWORK
DE Recurrent spiking neural networks; Signal denoising; Active Tuning;
   Temporal gradients
AB Active Tuning is an optimization paradigm specifically designed to increase the robustness and generalization ability of temporal forward models like recurrent neural networks (RNNs). This work explores how the Active Tuning method can be used to optimize the internal dynamics of recurrent spiking neural networks (RSNNs). Active Tuning decouples the network from direct influence of the data stream and instead tunes its internal dynamics. This is based on the temporal gradient signals from propagating the error between outputs and observations backwards through time. Meanwhile, the network is running in a closed-loop prediction cycle, where the own output is used as the next input. As modern ANNs often demand excessive amounts of computational resources, spiking neural networks (SNNs) aim for the energy efficiency demonstrated by the human brain. This is accomplished by using an event-driven model inspired by the spiking behavior of biological neurons. Target of the Active Tuning optimization in RSNNs is the membrane potential of the neurons in the hidden layer. We show in two scenarios how RSNNs handle noisy inputs and that Active Tuning is a reliable method to increase their robustness as well as general prediction performance.
C1 [Ciurletti, Melvin; Traub, Manuel; Karlbauer, Matthias; Butz, Martin, V; Otte, Sebastian] Univ Tubingen, Comp Sci Dept, Neurocognit Modeling, Sand 14, D-72076 Tubingen, Germany.
RP Otte, S (corresponding author), Univ Tubingen, Comp Sci Dept, Neurocognit Modeling, Sand 14, D-72076 Tubingen, Germany.
EM melvin.ciurletti@student.uni-tuebingen.de;
   manuel.traub@uni-tuebingen.de; matthias.karlbauer@uni-tuebingen.de;
   martin.butz@uni-tuebingen.de; sebastian.otte@uni-tuebingen.de
CR Bellec G., 2018, ADV NEURAL INFORM PR, P787
   Bellec G, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-17236-y
   Esser SK, 2016, P NATL ACAD SCI USA, V113, P11441, DOI 10.1073/pnas.1604850113
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Hubara I., 2016, P 30 INT C NEUR INF, P4114, DOI DOI 10.5555/3157382.3157557
   Jaeger, 2001, 148 FRAUNH I AN INF
   Kingma D. P., 2014, ARXIV, DOI DOI 10.48550/ARXIV.1412.6980
   Korsch HJ., 2008, CHAOS PROGRAM COLLEC, DOI [10.1007/978-3-540-74867-0, DOI 10.1007/978-3-540-74867-0]
   Koryakin D, 2012, NEURAL NETWORKS, V36, P35, DOI 10.1016/j.neunet.2012.08.008
   Otte, 2020, ARXIV201003958
   Otte S, 2016, NEUROCOMPUTING, V192, P128, DOI 10.1016/j.neucom.2016.01.088
   Paugam-Moisy H, 2012, HDB NATURAL COMPUTIN, V1, P1, DOI DOI 10.1007/978-3-540-92910-9_10
   Press W., 1986, NUMERICAL RECIPES AR, DOI DOI 10.2277/052143064X
   Schmichuber J, 2007, NEURAL COMPUT, V19, P757, DOI 10.1162/neco.2007.19.3.757
NR 14
TC 0
Z9 0
U1 1
U2 4
PY 2021
VL 12895
BP 220
EP 232
DI 10.1007/978-3-030-86383-8_18
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods; Imaging Science & Photographic Technology
DA 2023-11-11
ER

PT J
AU Bodyanskiy, Y
   Dolotov, A
   Vynokurova, O
AF Bodyanskiy, Ye
   Dolotov, A.
   Vynokurova, O.
TI Evolving spiking wavelet-neuro-fuzzy self-learning system
SO APPLIED SOFT COMPUTING
DT Article
DE Computational intelligence; Hybrid evolving system; Multilayered spiking
   neural network; Self-learning; Control systems theory; Wavelet; Fuzzy
   clustering
ID NETWORK
AB The paper introduces several modifications to self-learning fuzzy spiking neural network that is used as a base for evolving system design. The adaptive wavelet activation-membership functions are utilized to improve and generalize receptive neuron activation functions and the temporal Hebbian learning algorithm. The proposed evolving spiking wavelet-neuro-fuzzy self-learning system retains native features of spiking neurons and reveals evolving systems' capabilities in detecting overlapping clusters of irregular form. (C) 2013 Elsevier B. V. All rights reserved.
C1 [Bodyanskiy, Ye; Dolotov, A.; Vynokurova, O.] Kharkiv Natl Univ Radio Elect, UA-61166 Kharkov, Ukraine.
RP Bodyanskiy, Y (corresponding author), Kharkiv Natl Univ Radio Elect, 14 Lenin Ave,Off 511, UA-61166 Kharkov, Ukraine.
EM bodya@kture.kharkov.ua
CR [Anonymous], 2005, P 1 INT WORKSH GEN F
   [Anonymous], 1997, NEUROFUZZY SOFT COMP
   Bodyanskiy Y., 2008, INT J ARTIF INTELL M, V8, P9
   Bodyanskiy Y, 2008, PRO BIENN BALT EL C, P213, DOI 10.1109/BEC.2008.4657517
   Bodyanskiy Ye, 2009, SCI J RIGA TU, P66
   Bodyanskiy Ye., 2010, P 17 ZITT E W FUZZ C, P47
   Bodyanskiy Ye., 2009, IMAGE PROCESSING, P357
   Bodyanskiy Ye, 2008, SCI P RIG TU INF TEC, P27
   Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   Budka M, 2013, STUD COMPUT INTELL, V457, P177, DOI 10.1007/978-3-642-34300-1_17
   Butkiewicz BS, 2005, LECT NOTES COMPUT SC, V3528, P76
   COTTRELL M, 1986, BIOL CYBERN, V53, P405, DOI 10.1007/BF00318206
   De Berredo R. C., 2005, THESIS PONTIFICAL CA, P118
   Dolotov A., 2008, WISSENSCHAFTLICHE BE, V100, pS53
   Gerstner W., 2002, SPIKING NEURON MODEL
   Kasabov N, 2006, ADV SOFT COMP, P521, DOI 10.1007/3-540-34783-6_51
   Lughofer E, 2011, STUD FUZZ SOFT COMP, V266, P1, DOI 10.1007/978-3-642-18087-3
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maass W., 1998, PULSED NEURAL NETWOR, P408
   Maass W, 2007, LECT NOTES COMPUT SC, V4497, P507
   Meftah B, 2008, IEEE IJCNN, P681, DOI 10.1109/IJCNN.2008.4633868
   Mitaim S, 1997, 1997 IEEE INTERNATIONAL CONFERENCE ON NEURAL NETWORKS, VOLS 1-4, P537, DOI 10.1109/ICNN.1997.611726
   Mitaim S., 1996, P 5 IEEE INT C FUZZ, V2, P1213
   Nakamori Yo., 1996, FUZZY MODELLING PARA, P331
   Natschlager T, 1998, NETWORK-COMP NEURAL, V9, P319, DOI 10.1088/0954-898X/9/3/003
   Rutkowski L, 2008, COMPUTATIONAL INTELL, P514
NR 26
TC 9
Z9 10
U1 0
U2 10
PD JAN
PY 2014
VL 14
BP 252
EP 258
DI 10.1016/j.asoc.2013.05.020
PN B
WC Computer Science, Artificial Intelligence; Computer Science,
   Interdisciplinary Applications
DA 2023-11-11
ER

PT C
AU Bomberger, NA
   Waxman, AM
   Pait, FM
AF Bomberger, NA
   Waxman, AM
   Pait, FM
BE Dasarathy, BV
TI Spiking neural networks for higher-level information fusion
SO MULTISENSOR, MULTISOURCE INFORMATION FUSION: ARCHITECTURES, ALGORITHMS,
   AND APPLICATONS 2004
SE Proceedings of SPIE
DT Proceedings Paper
CT Conference on Multisensor, Multisource Information Fusion
CY APR 14-15, 2004
CL ORLANDO, FL
DE information fusion; fusion 2+; higher-level fusion; situation
   assessment; threat assessment; spiking neural networks; semantic
   knowledge representation; knowledge networks; knowledge hierarchy;
   associative learning
ID REPRESENTATION
AB This paper presents a novel approach to higher-level (2+) information fusion and knowledge representation using semantic networks composed of coupled spiking neuron nodes. Networks of spiking neurons have been shown to exhibit synchronization, in which sub-assemblies of nodes become phase locked to one another. This phase locking reflects the tendency of biological neural systems to produce synchronized neural assemblies, which have been hypothesized to be involved in feature binding. The approach in this paper embeds spiking neurons in a semantic network, in which a synchronized sub-assembly of nodes represents a hypothesis about a situation. Likewise, multiple synchronized assemblies that are out-of-phase with one another represent multiple hypotheses. The initial network is hand-coded, but additional semantic relationships can be established by associative learning mechanisms. This approach is demonstrated with a simulated scenario involving the tracking of suspected criminal vehicles between meeting places in an urban environment.
C1 ALPHATECH Inc, Fus Technol & Syst Div, Burlington, MA 01803 USA.
RP Bomberger, NA (corresponding author), ALPHATECH Inc, Fus Technol & Syst Div, Burlington, MA 01803 USA.
EM waxman@alphatech.com
CR Bartfai G, 2000, STUD FUZZ SOFT COMP, V48, P87
   ENDSLEY MR, 1995, HUM FACTORS, V37, P32, DOI 10.1518/001872095779049543
   Gerstner W., 2002, SPIKING NEURON MODEL
   Gray CM, 1999, NEURON, V24, P31, DOI 10.1016/S0896-6273(00)80820-X
   Hebb D. O., 1949, ORG BEHAV
   HINMAN ML, 2002, 5 INT C INF FUS ANN
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Horn D, 1998, PULSED NEURAL NETWORKS, P297
   IVEY RT, 2003, 6 INT C INF FUS CAIR
   JORDAN MI, 1995, HDB BRAIN THEORY NEU, P579
   SHASTRI L, 1993, BEHAV BRAIN SCI, V16, P417, DOI 10.1017/S0140525X00030910
   Shastri L, 1999, APPL INTELL, V11, P79, DOI 10.1023/A:1008380614985
   SINGER W, 1995, HDB BRAIN THEORY NEU
   Steinberg AN, 1999, P SOC PHOTO-OPT INS, V3719, P430, DOI 10.1117/12.341367
   Tyler LK, 2001, TRENDS COGN SCI, V5, P244, DOI 10.1016/S1364-6613(00)01651-X
   von der Malsburg C, 1999, NEURON, V24, P95, DOI 10.1016/S0896-6273(00)80825-9
   Waxman A. M., 2002, 5 INT C INF FUS ANN
   WAXMAN AM, 2002, P 5 INT MIL SENS S G
   White F. E, 1987, DATA FUSION LEXICON
NR 19
TC 1
Z9 1
U1 0
U2 1
PY 2004
VL 5434
BP 249
EP 260
DI 10.1117/12.555425
WC Computer Science, Artificial Intelligence; Instruments &
   Instrumentation; Imaging Science & Photographic Technology
DA 2023-11-11
ER

PT J
AU Vicente-Sola, A
   Manna, DL
   Kirkland, P
   Di Caterina, G
   Bihl, T
AF Vicente-Sola, Alex
   Manna, Davide L.
   Kirkland, Paul
   Di Caterina, Gaetano
   Bihl, Trevor
TI Keys to accurate feature extraction using residual spiking neural
   networks
SO NEUROMORPHIC COMPUTING AND ENGINEERING
DT Article
DE feature extraction; image classification; neural networks; neuromorphic;
   residual network; spiking
ID LOIHI
AB Spiking neural networks (SNNs) have become an interesting alternative to conventional artificial neural networks (ANN) thanks to their temporal processing capabilities and energy efficient implementations in neuromorphic hardware. However, the challenges involved in training SNNs have limited their performance in terms of accuracy and thus their applications. Improving learning algorithms and neural architectures for a more accurate feature extraction is therefore one of the current priorities in SNN research. In this paper we present a study on the key components of modern spiking architectures. We design a spiking version of the successful residual network architecture and provide an in-depth study on the possible implementations of spiking residual connections. This study shows how, depending on the use case, the optimal residual connection implementation may vary. Additionally, we empirically compare different techniques in image classification datasets taken from the best performing networks. Our results provide a state of the art guide to SNN design, which allows to make informed choices when trying to build the optimal visual feature extractor. Finally, our network outperforms previous SNN architectures in CIFAR-10 (94.14%) and CIFAR-100 (74.65%) datasets and matches the state of the art in DVS-CIFAR10 (72.98%), with less parameters than the previous state of the art and without the need for ANN-SNN conversion. Code available at: https://github.com/VicenteAlex/Spiking_ResNet.
C1 [Vicente-Sola, Alex; Manna, Davide L.; Kirkland, Paul; Di Caterina, Gaetano] Univ Strathclyde, Ctr Image & Signal Proc Elect & Elect Engn, Neuromorph Sensor Signal Proc Lab, Glasgow, Scotland.
   [Bihl, Trevor] Air Force Res Lab, Wright Patterson AFB, OH USA.
RP Vicente-Sola, A (corresponding author), Univ Strathclyde, Ctr Image & Signal Proc Elect & Elect Engn, Neuromorph Sensor Signal Proc Lab, Glasgow, Scotland.
EM alex.vicente-sola@strath.ac.uk
CR Abbott LF, 1999, BRAIN RES BULL, V50, P303, DOI 10.1016/S0361-9230(99)00161-6
   Bing Han, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P388, DOI 10.1007/978-3-030-58607-2_23
   Bing Han, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13555, DOI 10.1109/CVPR42600.2020.01357
   Cassidy AS, 2013, IEEE IJCNN
   Cooijmans T., 2016, CORR
   Davies M, 2021, P IEEE, V109, P911, DOI 10.1109/JPROC.2021.3067593
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Deng L, 2020, NEURAL NETWORKS, V121, P294, DOI 10.1016/j.neunet.2019.09.005
   Deng S., 2021, INT C LEARNING REPRE, P2328
   Esser SK, 2016, P NATL ACAD SCI USA, V113, P11441, DOI 10.1073/pnas.1604850113
   Anthony LFW, 2020, Arxiv, DOI [arXiv:2007.03051, DOI 10.48550/ARXIV.2007.03051]
   Falkner S, 2018, PR MACH LEARN RES, V80
   Fang W, 2021, ADV NEUR IN, V34
   Fang W, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2641, DOI 10.1109/ICCV48922.2021.00266
   Fang Wei, 2020, SPIKINGJELLY
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Huh D, 2018, ADV NEUR IN, V31
   Kaiser J, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00424
   Kim Y, 2021, FRONT NEUROSCI-SWITZ, V15, DOI 10.3389/fnins.2021.773954
   Kugele A, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00439
   Kusmierz L, 2017, CURR OPIN NEUROBIOL, V46, P170, DOI 10.1016/j.conb.2017.08.020
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lee C, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00119
   Li HM, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00309
   Mostafa H, 2018, IEEE T NEUR NET LEAR, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Mostafa H, 2018, NEURAL COMPUT, V30, P1542, DOI 10.1162/neco_a_01080
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Orchard G, 2021, IEEE WRK SIG PRO SYS, P254, DOI 10.1109/SiPS52927.2021.00053
   Paszke A, 2019, ADV NEUR IN, V32
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Severa W, 2019, NAT MACH INTELL, V1, P86, DOI 10.1038/s42256-018-0015-y
   Shrestha SB, 2018, ADV NEUR IN, V31
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   St”ckl C, 2020, Arxiv, DOI arXiv:2001.01682
   Stöckl C, 2021, NAT MACH INTELL, V3, DOI 10.1038/s42256-021-00311-4
   Vicente-Sola Alex, 2022, Figshare, DOI 10.6084/m9.figshare.20712535.v2
   Wang YX, 2021, IEEE T COGN DEV SYST, V13, P514, DOI 10.1109/TCDS.2020.2971655
   Wu JB, 2022, IEEE T PATTERN ANAL, V44, P7824, DOI 10.1109/TPAMI.2021.3114196
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Zheng HL, 2021, AAAI CONF ARTIF INTE, V35, P11062
NR 40
TC 1
Z9 1
U1 0
U2 0
PD DEC 1
PY 2022
VL 2
IS 4
AR 044001
DI 10.1088/2634-4386/ac8bef
WC Engineering, Electrical & Electronic; Physics, Applied
DA 2023-11-11
ER

PT C
AU Mouraud, A
   Paugam-Moisy, H
   Puzenat, D
AF Mouraud, A
   Paugam-Moisy, H
   Puzenat, D
BE Fahringer, T
TI A distributed and multithreaded neural event driven simulation framework
SO PROCEEDINGS OF THE IASTED INTERNATIONAL CONFERENCE ON PARALLEL AND
   DISTRIBUTED COMPUTING AND NETWORKS
DT Proceedings Paper
CT IASTED International Conference on Parallel and Distributed Computing
   and Networks
CY FEB 14-16, 2006
CL Innsbruck, AUSTRIA
DE Spiking Neural Networks; Event-Driven Simulations; parallel computing;
   multi-threading; scheduling
ID SPIKING NEURONS; LARGE NETWORKS; SYNCHRONY; DYNAMICS; MODELS
AB In a Spiking Neural Networks (SNN), spike emissions are sparsely and irregularly distributed both in time and in the network architecture. Since a current feature of SNNs is a low average activity, efficient implementations of SNNs are usually based on an Event-Driven Simulation (EDS). On the other hand, simulations of large scale neural networks can take advantage of distributing the neurons on a set of processors (either workstation cluster or parallel computer). This article presents a large scale SNN simulation framework able to gather the benefits of EDS and parallel computing. Two levels of parallelism are combined: Distributed mapping of the neural topology, at the network level, and local multithreaded allocation of resources for simultaneous processing of events, at the neuron level. Based on the causality of events, a distributed solution is proposed for solving the complex problem of scheduling without synchronization barrier.
C1 Inst Cognit Sci, CNRS, UMR 5015, 67 Blvd Pinel, F-69675 Lyon, France.
   Univ Antilles Guyane, Lab GRIMAAG, Guadeloupe, France.
RP Mouraud, A (corresponding author), Inst Cognit Sci, CNRS, UMR 5015, 67 Blvd Pinel, F-69675 Lyon, France.
EM mouraud@isc.cnrs.fr; hpaugam@isc.cnrs.fr; dpuzenat@univ-ag.fr
CR [Anonymous], ADV NEURAL INFORM PR
   [Anonymous], 1998, BOOK GENESIS EXPLORI, DOI DOI 10.1007/978-1-4612-1634-63
   BONIFACE Y, 1999, P EUROPAR, P935
   Delorme A, 1999, NEUROCOMPUTING, V26-7, P989, DOI 10.1016/S0925-2312(99)00095-8
   Estévez PA, 2002, PARALLEL COMPUT, V28, P861, DOI 10.1016/S0167-8191(02)00078-9
   FERSHA A, 1995, PARALLEL DISTRIBUTED
   Gerstner W., 2002, SPIKING NEURON MODEL
   GRASSMANN C, 2002, ESANN 2002 P APR 200, P331
   GRASSMANN C, 1998, NC 98, P100
   Hellmich HH, 2005, IEEE IJCNN, P3261
   Hines ML, 1997, NEURAL COMPUT, V9, P1179, DOI 10.1162/neco.1997.9.6.1179
   Hopfield JJ, 2001, P NATL ACAD SCI USA, V98, P1282, DOI 10.1073/pnas.031567098
   Izhikevich EM, 2004, CEREB CORTEX, V14, P933, DOI 10.1093/cercor/bhh053
   JAHNKE A, 1997, INT C ART NEUR NETW, P1187
   Legenstein R, 2005, NEURAL COMPUT, V17, P2337, DOI 10.1162/0899766054796888
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   MAASS W., 1999, PULSED NEURAL NETWOR
   Makino T, 2003, NEURAL COMPUT APPL, V11, P210, DOI 10.1007/s00521-003-0358-z
   Mattia M, 2000, NEURAL COMPUT, V12, P2305, DOI 10.1162/089976600300014953
   MEUNIER D, 2005, UNPUB INHIBITION SPI
   PAUGAMMOISY H, 1995, HDB BRAIN THEORY NEU, P605
   PREIS R, 2001, PDPTA 2001
   Reutimann J, 2003, NEURAL COMPUT, V15, P811, DOI 10.1162/08997660360581912
   ROCHEL O, 2003, EUR S ART NEUR NETW, P295
   Seiffert U, 2004, NEUROCOMPUTING, V57, P135, DOI 10.1016/j.neucom.2004.01.011
   Síma J, 2005, NEURAL COMPUT, V17, P2635, DOI 10.1162/089976605774320601
   Singer W, 1999, NEURON, V24, P49, DOI 10.1016/S0896-6273(00)80821-1
   Tallon-Baudry C, 2001, J NEUROSCI, V21, part. no., DOI 10.1523/JNEUROSCI.21-20-j0008.2001
NR 28
TC 4
Z9 4
U1 0
U2 1
PY 2006
BP 212
EP +
WC Computer Science, Theory & Methods
DA 2023-11-11
ER

PT J
AU Chakraborty, B
   Mukhopadhyay, S
AF Chakraborty, Biswadeep
   Mukhopadhyay, Saibal
TI Heterogeneous recurrent spiking neural network for spatio-temporal
   classification
SO FRONTIERS IN NEUROSCIENCE
DT Article
DE spiking neural network (SNN); action detection and recognition; spike
   timing dependent plasticity; heterogeneity; unsupervised learning;
   Bayesian Optimization (BO); leaky integrate and fire (LIF)
ID PLASTICITY
AB Spiking Neural Networks are often touted as brain-inspired learning models for the third wave of Artificial Intelligence. Although recent SNNs trained with supervised backpropagation show classification accuracy comparable to deep networks, the performance of unsupervised learning-based SNNs remains much lower. This paper presents a heterogeneous recurrent spiking neural network (HRSNN) with unsupervised learning for spatio-temporal classification of video activity recognition tasks on RGB (KTH, UCF11, UCF101) and event-based datasets (DVS128 Gesture). We observed an accuracy of 94.32% for the KTH dataset, 79.58% and 77.53% for the UCF11 and UCF101 datasets, respectively, and an accuracy of 96.54% on the event-based DVS Gesture dataset using the novel unsupervised HRSNN model. The key novelty of the HRSNN is that the recurrent layer in HRSNN consists of heterogeneous neurons with varying firing/relaxation dynamics, and they are trained via heterogeneous spike-time-dependent-plasticity (STDP) with varying learning dynamics for each synapse. We show that this novel combination of heterogeneity in architecture and learning method outperforms current homogeneous spiking neural networks. We further show that HRSNN can achieve similar performance to state-of-the-art backpropagation trained supervised SNN, but with less computation (fewer neurons and sparse connection) and less training data.
C1 [Chakraborty, Biswadeep; Mukhopadhyay, Saibal] Georgia Inst Technol, Dept Elect & Comp Engn, Atlanta, GA 30313 USA.
RP Chakraborty, B (corresponding author), Georgia Inst Technol, Dept Elect & Comp Engn, Atlanta, GA 30313 USA.
EM biswadeep@gatech.edu
CR Bi Y, 2020, IEEE T IMAGE PROCESS, V29, P9084, DOI 10.1109/TIP.2020.3023597
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Carvalho TP, 2009, NEURON, V61, P774, DOI 10.1016/j.neuron.2009.01.013
   Chakraborty B, 2021, FRONT NEUROSCI-SWITZ, V15, DOI 10.3389/fnins.2021.695357
   DEKLOET ER, 1987, PSYCHONEUROENDOCRINO, V12, P83, DOI 10.1016/0306-4530(87)90040-0
   Demin V, 2018, FRONT NEUROINFORM, V12, DOI 10.3389/fninf.2018.00079
   Eriksson D., 2021, UNCERTAINTY ARTIFICI, P493
   Escobar MJ, 2009, INT J COMPUT VISION, V82, P284, DOI 10.1007/s11263-008-0201-1
   Fang W., 2021, P IEEE CVF INT C COM, P2661, DOI 10.1109/ICCV48922.2021.00266
   Feldman DE, 2012, NEURON, V75, P556, DOI 10.1016/j.neuron.2012.08.001
   Feydy J., 2019, 22 INT C ARTIFICIAL, P2681
   George AM, 2020, IEEE IJCNN, DOI 10.1109/ijcnn48605.2020.9206681
   Gilson M, 2010, FRONT COMPUT NEUROSC, V4, DOI 10.3389/fncom.2010.00023
   Hofer SB, 2011, NAT NEUROSCI, V14, P1045, DOI 10.1038/nn.2876
   Frazier PI, 2018, Arxiv, DOI [arXiv:1807.02811, DOI 10.48550/ARXIV.1807.02811]
   Ivanov V., 2021, ADV NEURAL INF PROCE, V34, P25703
   Jin Y., 2018, ADV NEURAL INFORM PR
   Korte M, 2016, PHYSIOL REV, V96, P647, DOI 10.1152/physrev.00010.2015
   Lagorce X, 2017, IEEE T PATTERN ANAL, V39, P1346, DOI 10.1109/TPAMI.2016.2574707
   Lazar A, 2006, ESANN, P647
   Lee H, 2021, IEEE ACCESS, V9, P83901, DOI 10.1109/ACCESS.2021.3087509
   Legenstein R, 2007, NEURAL NETWORKS, V20, P323, DOI 10.1016/j.neunet.2007.04.017
   Liu Qianhui, 2021, IJCAI, P1743
   Lobo JL, 2020, NEURAL NETWORKS, V121, P88, DOI 10.1016/j.neunet.2019.09.004
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   Maro JM, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00275
   Meng Y, 2011, IEEE T NEURAL NETWOR, V22, P1952, DOI 10.1109/TNN.2011.2171044
   Nobukawa S, 2019, J ARTIF INTELL SOFT, V9, P283, DOI 10.2478/jaiscr-2019-0009
   Panda P, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00126
   Patravali J., 2021, PROC IEEECVF INT C C, P8484
   Perez-Nieves N, 2021, NAT COMMUN, V12, DOI 10.1038/s41467-021-26022-3
   Petitpré C, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-06033-3
   Pool RR, 2011, NEURAL COMPUT, V23, P1768, DOI 10.1162/NECO_a_00140
   Shamir M, 2006, NEURAL COMPUT, V18, P1951, DOI 10.1162/neco.2006.18.8.1951
   She XY, 2021, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.615756
   She Xueyuan, 2021, INT C LEARN REPR
   Shen G., 2021, ARXIV, DOI [10.2139/ssrn.4018613, DOI 10.2139/SSRN.4018613]
   Shrestha SB, 2018, ADV NEUR IN, V31
   Sjöström PJ, 2008, PHYSIOL REV, V88, P769, DOI 10.1152/physrev.00016.2007
   Soomro K, 2012, Arxiv, DOI arXiv:1212.0402
   Soomro K, 2017, IEEE I CONF COMP VIS, P696, DOI 10.1109/ICCV.2017.82
   Soures N, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00686
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Wang QY, 2019, IEEE WINT CONF APPL, P1826, DOI 10.1109/WACV.2019.00199
   Wang W, 2019, IEEE ACCESS, V7, P117165, DOI 10.1109/ACCESS.2019.2936604
   Wang ZJ, 2022, INT J INTELL SYST, V37, P2242, DOI 10.1002/int.22772
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Yang J., 2018, IOP C SERIES MAT SCI, DOI DOI 10.1088/1757-899X/394/5/052009
   Yin B., 2021, ARXIV PREPRINT, DOI [10.21203/rs.3.rs-1625930/v1, DOI 10.21203/RS.3.RS-1625930/V1]
   Zeldenrust F, 2021, PLOS COMPUT BIOL, V17, DOI 10.1371/journal.pcbi.1008673
   Zhang W., 2019, ADV NEUR IN
   Zheng H., 2020, ARXIV PREPRINT, DOI [10.1609/aaai.v35i12.17320, DOI 10.1609/AAAI.V35I12.17320]
   Zhou Y, 2020, NEUROCOMPUTING, V406, P12, DOI 10.1016/j.neucom.2020.04.079
NR 53
TC 0
Z9 0
U1 1
U2 8
PD JAN 30
PY 2023
VL 17
AR 994517
DI 10.3389/fnins.2023.994517
WC Neurosciences
DA 2023-11-11
ER

PT J
AU Wang, ZH
   Gu, XZ
   Goh, RSM
   Zhou, JT
   Luo, T
AF Wang, Zhehui
   Gu, Xiaozhe
   Goh, Rick Siow Mong
   Zhou, Joey Tianyi
   Luo, Tao
TI Efficient Spiking Neural Networks With Radix Encoding
SO IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
DT Article; Early Access
DE Encoding; energy efficient; short spike train; speedup; spiking neural
   network (SNN)
ID INTELLIGENCE; PROCESSOR; DRIVEN
AB Spiking neural networks (SNNs) have advantages in latency and energy efficiency over traditional artificial neural networks (ANNs) due to their event-driven computation mechanism and the replacement of energy-consuming weight multiplication with addition. However, to achieve high accuracy, it usually requires long spike trains to ensure accuracy, usually more than 1000 time steps. This offsets the computation efficiency brought by SNNs because a longer spike train means a larger number of operations and larger latency. In this article, we propose a radix-encoded SNN, which has ultrashort spike trains. Specifically, it is able to use less than six time steps to achieve even higher accuracy than its traditional counterpart. We also develop a method to fit our radix encoding technique into the ANN-to-SNN conversion approach so that we can train radix-encoded SNNs more efficiently on mature platforms and hardware. Experiments show that our radix encoding can achieve 25x improvement in latency and 1.7% improvement in accuracy compared to the state-of-the-art method using the VGG-16 network on the CIFAR-10 dataset.
C1 [Wang, Zhehui; Goh, Rick Siow Mong; Zhou, Joey Tianyi; Luo, Tao] ASTAR, Inst High Performance Comp, Singapore 138632, Singapore.
   [Gu, Xiaozhe] Chinese Univ Hong Kong, Future Network Intelligence Inst FNii, Shenzhen 518172, Peoples R China.
RP Luo, T (corresponding author), ASTAR, Inst High Performance Comp, Singapore 138632, Singapore.
EM wang_zhehui@ihpc.a-star.edu.sg; gohsm@ihpc.a-star.edu.sg;
   joey_zhou@ihpc.a-star.edu.sg; luo_tao@ihpc.a-star.edu.sg
CR Pérez-Carrasco JA, 2013, IEEE T PATTERN ANAL, V35, P2706, DOI 10.1109/TPAMI.2013.71
   Bellec G., 2018, ADV NEURAL INFORM PR, P787
   Chai T, 2014, GEOSCI MODEL DEV, V7, P1247, DOI 10.5194/gmd-7-1247-2014
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Diehl PU, 2015, IEEE IJCNN
   Hebb D. O., 1949, ORG BEHAV
   Hu YF, 2020, Arxiv, DOI [arXiv:1805.01352, 10.48550/arXiv.1805.01352]
   Huh D, 2018, ADV NEUR IN, V31
   Hunsberger E, 2015, Arxiv, DOI arXiv:1510.08829
   Kaiser J, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00424
   Khan MM, 2008, IEEE IJCNN, P2849, DOI 10.1109/IJCNN.2008.4634199
   Kim J, 2018, NEUROCOMPUTING, V311, P373, DOI 10.1016/j.neucom.2018.05.087
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
   Lee C, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00119
   Luo T, 2020, IEEE T COMPUT AID D, V39, P438, DOI 10.1109/TCAD.2018.2889670
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   METROPOLIS N, 1949, J AM STAT ASSOC, V44, P335, DOI 10.2307/2280232
   Nambiar V. P., 2020, P IEEE AS SOL STAT C, P1
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Park J, 2020, IEEE J SOLID-ST CIRC, V55, P108, DOI 10.1109/JSSC.2019.2942367
   Pei J, 2019, NATURE, V572, P106, DOI 10.1038/s41586-019-1424-8
   Rathi N, 2020, Arxiv, DOI arXiv:2005.01807
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Reynolds JJM, 2019, IEEE IJCNN
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Rueckauer Bodo, 2016, ARXIV, DOI DOI 10.3389/FNINS.2017.00682
   Schulz C, 2019, 2019 EUROPEAN CONFERENCE ON MOBILE ROBOTS (ECMR), DOI [10.1109/ecmr.2019.8870966, 10.1109/vlsi-tsa.2019.8804650]
   Schuster C, 2020, PUBLIC ADMIN REV, V80, P792, DOI 10.1111/puar.13246
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Shen Y., 2016, P COLING 2016 26 INT, P2526
   Shrestha SB, 2018, ADV NEUR IN, V31
   Sodhro AH, 2019, IEEE T IND INFORM, V15, P4235, DOI 10.1109/TII.2019.2902878
   Wang Z., 2020, 2020 IEEE 91 VEHICUL, P1, DOI DOI 10.1109/VTC2020-SPRING48590.2020.9128938
   Wilamowski B. M., 2011, IND ELECT SET, V5
   Wu YJ, 2019, AAAI CONF ARTIF INTE, P1311
   Yan ZL, 2021, AAAI CONF ARTIF INTE, V35, P10577
   Yang TJ, 2017, PROC CVPR IEEE, P6071, DOI 10.1109/CVPR.2017.643
   Yin PH, 2019, Arxiv, DOI arXiv:1903.05662
   Zhang L, 2019, AAAI CONF ARTIF INTE, P1319
NR 39
TC 1
Z9 1
U1 2
U2 17
PD 2022 AUG 16
PY 2022
DI 10.1109/TNNLS.2022.3195918
EA AUG 2022
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Computer Science, Theory & Methods; Engineering,
   Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Misiunas, AVM
   Rapsevicius, V
   Samaitiene, R
   Meskauskas, T
AF Misiunas, Andrius Vytautas Misiukas
   Rapsevicius, Valdas
   Samaitiene, Ruta
   Meskauskas, Tadas
TI Electroencephalogram spike detection and classification by diagnosis
   with convolutional neural network
SO NONLINEAR ANALYSIS-MODELLING AND CONTROL
DT Article
DE electroencephalogram; convolutional neural network; machine learning;
   classification; epilepsy
ID EPILEPSY; SYSTEM
AB This work presents convolutional neural network (CNN) based methodology for electroencephalogram (EEG) classification by diagnosis: benign childhood epilepsy with centrotemporal spikes (rolandic epilepsy) (Group I) and structural focal epilepsy (Group II). Manual classification of these groups is sometimes difficult, especially, when no clinical record is available, thus presenting a need for an algorithm for automatic classification. The presented algorithm has the following steps: (i) EEG spike detection by morphological filter based algorithm; (ii) classification of EEG spikes using preprocessed EEG signal data from all channels in the vicinity of the spike detected; (iii) majority rule classifier application to all EEG spikes from a single patient. Classification based on majority rule allows us to achieve 80% average accuracy (despite the fact that from a single spike one would obtain only 58% accuracy).
C1 [Misiunas, Andrius Vytautas Misiukas; Rapsevicius, Valdas; Meskauskas, Tadas] Vilnius Univ, Inst Comp Sci, Didlaukio 47, LT-08303 Vilnius, Lithuania.
   [Samaitiene, Ruta] Vilnius Univ, Fac Med, Clin Childrens Dis, Santariskiu 4, LT-08406 Vilnius, Lithuania.
RP Misiunas, AVM (corresponding author), Vilnius Univ, Inst Comp Sci, Didlaukio 47, LT-08303 Vilnius, Lithuania.
EM andrius.misiukas@mif.vu.lt
CR Aicardi J, 2000, EPILEPTIC DISORD, V2, pS5
   Biggio B, 2015, IEEE SIGNAL PROC MAG, V32, P31, DOI 10.1109/MSP.2015.2426728
   Degen R, 1999, PEDIATR NEUROL, V20, P354, DOI 10.1016/S0887-8994(99)00004-1
   Juozapavicius A, 2011, NONLINEAR ANAL-MODEL, V16, P375
   Kim Y, 2014, IEEE ASME INT C ADV, P1747, DOI 10.1109/AIM.2014.6878336
   Korvel G, 2018, J AUDIO ENG SOC, V66, P1072, DOI 10.17743/jaes.2018.0066
   Kousarrizi MRN, 2009, 2009 3RD INTERNATIONAL CONFERENCE ON BIOINFORMATICS AND BIOMEDICAL ENGINEERING, VOLS 1-11, P1150
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
   Löfhede J, 2010, J NEURAL ENG, V7, DOI 10.1088/1741-2560/7/1/016007
   Misiunas AVM, 2019, AIP CONF PROC, V2164, DOI 10.1063/1.5130828
   Misiunas A. V. Misiukas, 2015, LIET MAT RINKINYS SE, V56, P60, DOI DOI 10.15388/LMR.A.2015.11
   Misiunas A. V. Misiukas, 2016, P LITHUANIAN MATH SO, V57, P47
   Misiunas AVM, 2019, LECT NOTES COMPUT SC, V11189, P441, DOI 10.1007/978-3-030-10692-8_50
   Misiunas AVM, 2019, BIOMED SIGNAL PROCES, V48, P118, DOI 10.1016/j.bspc.2018.10.006
   Nishida S., 1999, 14 WORLD C IFAC, V32, P4301
   Patel J, 2015, EXPERT SYST APPL, V42, P2162, DOI 10.1016/j.eswa.2014.10.031
   Pattinson J, 2017, CULT HIST MOD WAR, P1
   Sajda P, 2006, ANNU REV BIOMED ENG, V8, P537, DOI 10.1146/annurev.bioeng.8.061505.095802
   Sammut C., 2017, ENCY MACHINE LEARNIN, V2nd, DOI DOI 10.1007/978-1-4899-7687-1_68
   Scheffer IE, 2017, EPILEPSIA, V58, P512, DOI 10.1111/epi.13709
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Tichonov J, 2018, ADV SCI TECHNOL-RES, V12, P29, DOI 10.12913/22998624/87041
NR 22
TC 0
Z9 0
U1 0
U2 6
PY 2020
VL 25
IS 4
BP 692
EP 704
DI 10.15388/namc.2020.25.18016
WC Mathematics, Applied; Mathematics, Interdisciplinary Applications;
   Mechanics
DA 2023-11-11
ER

PT J
AU Pietrzak, P
   Szczesny, S
   Huderek, D
   Przyborowski, L
AF Pietrzak, Pawel
   Szczesny, Szymon
   Huderek, Damian
   Przyborowski, Lukasz
TI Overview of Spiking Neural Network Learning Approaches and Their
   Computational Complexities
SO SENSORS
DT Review
DE spiking neural networks; learning algorithms; computational complexity;
   hardware
AB Spiking neural networks (SNNs) are subjects of a topic that is gaining more and more interest nowadays. They more closely resemble actual neural networks in the brain than their second-generation counterparts, artificial neural networks (ANNs). SNNs have the potential to be more energy efficient than ANNs on event-driven neuromorphic hardware. This can yield drastic maintenance cost reduction for neural network models, as the energy consumption would be much lower in comparison to regular deep learning models hosted in the cloud today. However, such hardware is still not yet widely available. On standard computer architectures consisting mainly of central processing units (CPUs) and graphics processing units (GPUs) ANNs, due to simpler models of neurons and simpler models of connections between neurons, have the upper hand in terms of execution speed. In general, they also win in terms of learning algorithms, as SNNs do not reach the same levels of performance as their second-generation counterparts in typical machine learning benchmark tasks, such as classification. In this paper, we review existing learning algorithms for spiking neural networks, divide them into categories by type, and assess their computational complexity.
C1 [Pietrzak, Pawel; Szczesny, Szymon; Huderek, Damian; Przyborowski, Lukasz] Poznan Univ Tech, Inst Comp Sci, Fac Comp & Telecommun, Piotrowo 3A St, PL-61138 Poznan, Poland.
RP Szczesny, S (corresponding author), Poznan Univ Tech, Inst Comp Sci, Fac Comp & Telecommun, Piotrowo 3A St, PL-61138 Poznan, Poland.
EM szymon.szczesny@put.poznan.pl
CR Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   Baby SA, 2017, PROCEEDINGS 2017 4TH IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION (ACPR), P316, DOI 10.1109/ACPR.2017.136
   Baydin AG, 2018, J MACH LEARN RES, V18
   Bing Han, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P388, DOI 10.1007/978-3-030-58607-2_23
   Bing Han, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13555, DOI 10.1109/CVPR42600.2020.01357
   Bohte S. M., 2000, 8th European Symposium on Artificial Neural Networks. ESANN"2000. Proceedings, P419
   Brown T., 2020, PROC ADV NEUR INF PR, P1877
   Bu T., 2022, OPTIMAL ANN SNN CONV
   Burbank KS, 2015, PLOS COMPUT BIOL, V11, DOI 10.1371/journal.pcbi.1004566
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   DeFelipe J, 2012, FRONT NEUROANAT, V6, DOI [10.3389/fnana.2012.00022, 10.3389/fnsyn.2012.00002, 10.3389/fnana.2012.00005]
   DeWolf T, 2021, SCI ROBOT, V6, DOI 10.1126/scirobotics.abk3268
   DeWolf T, 2020, FRONT NEUROROBOTICS, V14, DOI 10.3389/fnbot.2020.568359
   DeWolf T, 2016, P ROY SOC B-BIOL SCI, V283, DOI 10.1098/rspb.2016.2134
   Diehl PU, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON REBOOTING COMPUTING (ICRC)
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Dutta S, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-07418-y
   Eliasmith C., 2003, NEURAL ENG COMPUTATI
   Esser SK., 2015, BACKPROPAGATION ENER, P1117
   Falanga D, 2020, SCI ROBOT, V5, DOI 10.1126/scirobotics.aaz9712
   Fang W, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2641, DOI 10.1109/ICCV48922.2021.00266
   Höppner S, 2022, Arxiv, DOI arXiv:2103.08392
   Ivanov D., 2022, PREPRINT, DOI 10.3389/fnins.2022.959626
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2007, CEREB CORTEX, V17, P2443, DOI 10.1093/cercor/bhl152
   Juarez-Lora A, 2022, FRONT NEUROROBOTICS, V16, DOI 10.3389/fnbot.2022.904017
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li HM, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00309
   Liu FX, 2021, FRONT NEUROSCI-SWITZ, V15, DOI 10.3389/fnins.2021.756876
   Lobov SA, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21082678
   Makarov VA, 2022, FRONT COMPUT NEUROSC, V16, DOI 10.3389/fncom.2022.859874
   Masquelier T, 2007, PLOS COMPUT BIOL, V3, P247, DOI 10.1371/journal.pcbi.0030031
   Mo LF, 2022, FRONT NEUROSCI-SWITZ, V16, DOI 10.3389/fnins.2022.838832
   Moreira O., 2020, P 2020 2 IEEE INT C, P1, DOI 10.1109/AICAS48895.2020.9073999
   Mozafari M, 2019, PATTERN RECOGN, V94, P87, DOI 10.1016/j.patcog.2019.05.015
   OConnor P., 2016, DEEP SPIKING NETWORK
   Orchard G, 2015, FRONT NEUROSCI-SWITZ, V9, DOI [10.3389/fnins.2015.00437, 10.3389/fhins.2015.00437]
   Patel K, 2021, Arxiv, DOI arXiv:2106.08921
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Quiroga RQ, 2010, PSYCHOL REV, V117, P291, DOI 10.1037/a0016917
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Sandamirskaya Y, 2022, SCI ROBOT, V7, DOI 10.1126/scirobotics.abl8419
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Shalumov A, 2021, BIOINSPIR BIOMIM, V16, DOI 10.1088/1748-3190/ac290c
   Shrestha S. B., 2018, ADV NEURAL INFORM PR
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   St”ckl C, 2020, Arxiv, DOI arXiv:2001.01682
   Szczesny S, 2023, J EXP THEOR ARTIF IN, V35, P77, DOI 10.1080/0952813X.2021.1957024
   Szczesny S, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21093276
   Szczesny S, 2020, IEEE SENS J, V20, P5733, DOI 10.1109/JSEN.2020.2974701
   Tang GZ, 2019, IEEE INT C INT ROBOT, P4176, DOI [10.1109/IROS40897.2019.8967864, 10.1109/iros40897.2019.8967864]
   Tsur E. E., 2021, NEUROMORPHIC ENG SCI, DOI 10.1201/9781003143499
   Van Pottelbergh T, 2018, NEURAL COMPUT, V30, P987, DOI [10.1162/neco_a_01065, 10.1162/NECO_a_01065]
   Vigneron A, 2020, IEEE IJCNN, DOI 10.1109/ijcnn48605.2020.9207239
   Voela A, 2021, HYPATIA, V36, P101, DOI 10.1017/hyp.2020.49
   Wang CY, 2022, Arxiv, DOI [arXiv:2207.02696, DOI 10.48550/ARXIV.2207.02696]
   WERBOS PJ, 1990, P IEEE, V78, P1550, DOI 10.1109/5.58337
   Yamazaki K, 2022, BRAIN SCI, V12, DOI 10.3390/brainsci12070863
   Yan YX, 2021, NEUROMORPH COMPUT EN, V1, DOI 10.1088/2634-4386/abf150
   Yousefzadeh A, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00665
   Zenke F, 2018, NEURAL COMPUT, V30, P1514, DOI 10.1162/neco_a_01086
   Zhong XY, 2022, APPL SCI-BASEL, V12, DOI 10.3390/app12125980
   Zhou SB, 2020, IEEE ACCESS, V8, P76903, DOI 10.1109/ACCESS.2020.2990416
NR 67
TC 1
Z9 1
U1 15
U2 17
PD MAR
PY 2023
VL 23
IS 6
AR 3037
DI 10.3390/s23063037
WC Chemistry, Analytical; Engineering, Electrical & Electronic; Instruments
   & Instrumentation
DA 2023-11-11
ER

PT J
AU Guo, DQ
AF Guo, Daqing
TI Inhibition of rhythmic spiking by colored noise in neural systems
SO COGNITIVE NEURODYNAMICS
DT Article
DE Neural system; Spiking neuron model; Noise; Neuronal network motif
ID STOCHASTIC RESONANCE; NETWORK; ENSEMBLES
AB We study the effect of colored noise on the rhythmic spiking activity of neural systems in this paper. The phenomenon of the so-called inverse stochastic resonance, that is, noise with appropriate intensity suppresses the spiking activity in neural systems, is clearly observed in a special parameter regime. We find that the inhibition effect of colored noise is stronger than that of Gaussian white noise. Furthermore, our simulation results show that the inhibition effect of colored noise provides a useful mechanism for the generation of synchronized burst in type-2 mixed-feed-forward-feedback loop neuronal network motif, which indicates that such inhibition effect might have some biological implications.
C1 Univ Elect Sci & Technol China, Sch Elect Engn, Chengdu 610054, Peoples R China.
RP Guo, DQ (corresponding author), Univ Elect Sci & Technol China, Sch Elect Engn, Chengdu 610054, Peoples R China.
EM dqguo07@gmail.com
CR ADEY WR, 1972, INT J NEUROSCI, V3, P271, DOI 10.3109/00207457209147637
   Chialvo DR, 1997, PHYS REV E, V55, P1798, DOI 10.1103/PhysRevE.55.1798
   Chik DTW, 2001, PHYS REV E, V64, DOI 10.1103/PhysRevE.64.021913
   Collins JJ, 1996, J NEUROPHYSIOL, V76, P642, DOI 10.1152/jn.1996.76.1.642
   COLLINS JJ, 1995, NATURE, V376, P236, DOI 10.1038/376236a0
   Deco G, 2009, PROG NEUROBIOL, V88, P1, DOI 10.1016/j.pneurobio.2009.01.006
   Destexhe A, 2003, NAT REV NEUROSCI, V4, P739, DOI 10.1038/nrn1198
   Destexhe A, 2006, SCIENCE, V314, P85, DOI 10.1126/science.1127241
   Gailey PC, 1997, PHYS REV LETT, V79, P4701, DOI 10.1103/PhysRevLett.79.4701
   Gerstner W., 2002, SPIKING NEURON MODEL
   Guo DQ, 2009, PHYS REV E, V79, DOI 10.1103/PhysRevE.79.051921
   Gutkin BS, 2009, NATURWISSENSCHAFTEN, V96, P1091, DOI 10.1007/s00114-009-0570-5
   HANSEL D, 1992, PHYS REV LETT, V68, P718, DOI 10.1103/PhysRevLett.68.718
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Kloeden P.-E., 1994, NUMERICAL SOLUTION S
   Lee SG, 1999, PHYS REV E, V60, P826, DOI 10.1103/PhysRevE.60.826
   Li CG, 2006, PLOS COMPUT BIOL, V2, P925, DOI 10.1371/journal.pcbi.0020103
   Li CG, 2008, PHYS REV E, V78, DOI 10.1103/PhysRevE.78.037101
   Li QS, 2008, PHYS REV E, V77, DOI 10.1103/PhysRevE.77.036117
   Lindner B, 2002, PHYS REV E, V66, DOI 10.1103/PhysRevE.66.031916
   Milo R, 2002, SCIENCE, V298, P824, DOI 10.1126/science.298.5594.824
   Neiman AB, 2007, J NEUROPHYSIOL, V98, P2795, DOI 10.1152/jn.01289.2006
   Paydarfar D, 2006, J NEUROPHYSIOL, V96, P3338, DOI 10.1152/jn.00486.2006
   Pikovsky AS, 1997, PHYS REV LETT, V78, P775, DOI 10.1103/PhysRevLett.78.775
   Reigl M, 2004, BMC BIOL, V2, DOI 10.1186/1741-7007-2-25
   Sun XJ, 2008, CHAOS, V18, DOI 10.1063/1.2900402
   Tuckwell HC, 2011, J COMPUT NEUROSCI, V30, P361, DOI 10.1007/s10827-010-0260-5
   Tuckwell HC, 2009, PHYS REV E, V80, DOI 10.1103/PhysRevE.80.031907
NR 28
TC 50
Z9 52
U1 0
U2 14
PD SEP
PY 2011
VL 5
IS 3
BP 293
EP 300
DI 10.1007/s11571-011-9160-2
WC Neurosciences
DA 2023-11-11
ER

PT J
AU Varma, PRK
   Sathiya, RR
   Vanitha, M
AF Varma, P. Ravi Kiran
   Sathiya, R. R.
   Vanitha, M.
TI Enhanced Elman spike neural network based intrusion attack detection in
   software defined Internet of Things network
SO CONCURRENCY AND COMPUTATION-PRACTICE & EXPERIENCE
DT Article
DE distributed DoS; enhanced Elman spike neural network; intrusion
   detection; IP flow records; mitigation process; software defined network
ID IDENTIFICATION
AB In this article, enhanced Elman spike neural network based intrusion attack detection in software defined IoT network is proposed. Initially, the data's are taken from CICDDoS 2019 and CICIDS 2018 benchmark datasets. Software defined network (SDN) secure defense system is detected the intrusion and distributed denial of service (DDoS) attacks on central controllers using multidimensional internet protocol (IP) flow analysis. Here, the enhanced Elman spike neural network (EESNN) classifies DDoS and intrusion attacks as normal and anomaly. The proposed EESNN-IAD-SDN method is executed in python language. The performance metrics, such as accuracy, specificity, F-measure, sensitivity, precision, recall is examined. The proposed EESNN-IAD-SDN method provides 13.93%, 13.26%, 14.35, and 13.73% higher accuracy in CICDDoS 2019 dataset compared with the existing methods, like GRU-IAD-SDN, LSTM-IAD-SDN, and GAN-IAD-SDN, respectively.
C1 [Varma, P. Ravi Kiran] Maharaj Vijayaram Gajapathi Raj Coll Engn, Dept Comp Sci & Engn, Vizianagaram, Andhra Pradesh, India.
   [Sathiya, R. R.] Amrita Vishwa Vidyapeetham, Amrita Sch Comp, Dept Comp Sci & Engn, Coimbatore, India.
   [Vanitha, M.] Saveetha Engn Coll, Dept Elect & Commun Engn, Chennai, India.
   [Varma, P. Ravi Kiran] Maharaj VijayaramGajapathi Raj Coll Engn Vizianaga, Dept Comp Sci & Engn, Vizianagaram, Andhra Pradesh, India.
RP Varma, PRK (corresponding author), Maharaj VijayaramGajapathi Raj Coll Engn Vizianaga, Dept Comp Sci & Engn, Vizianagaram, Andhra Pradesh, India.
EM ravikiranvarmap@gmail.com
CR Al-Jamali NAS, 2020, IEEE ACCESS, V8, P61246, DOI 10.1109/ACCESS.2020.2984311
   Assis MVO, 2021, J NETW COMPUT APPL, V177, DOI 10.1016/j.jnca.2020.102942
   Bhardwaj S, 2021, WIRELESS PERS COMMUN, DOI 10.1007/s11277-021-08920-3
   Costa LC, 2021, PERFORM EVALUATION, V147, DOI 10.1016/j.peva.2021.102194
   de Senneville BD, 2015, IEEE T MED IMAGING, V34, P974, DOI 10.1109/TMI.2014.2371995
   Dey A, 2020, 2020 2ND INTERNATIONAL CONFERENCE ON SUSTAINABLE TECHNOLOGIES FOR INDUSTRY 4.0 (STI), DOI 10.1109/STI50764.2020.9350411
   Francis SH, 2022, CIRC SYST SIGNAL PR, V41, P1751, DOI 10.1007/s00034-021-01850-2
   Gopalakrishnan K., 2020, PRINCIPLES INTERNET, P519
   Hajj S, 2021, T EMERG TELECOMMUN T, V32, DOI 10.1002/ett.4240
   Imtiaz SI, 2021, FUTURE GENER COMP SY, V115, P844, DOI 10.1016/j.future.2020.10.008
   Kassab W, 2020, J NETW COMPUT APPL, V163, DOI 10.1016/j.jnca.2020.102663
   Kim J, 2021, IEEE T INF FOREN SEC, V16, P3138, DOI 10.1109/TIFS.2021.3075845
   Li Y., 2021, IEEE INTERNET THINGS
   Novaes MP, 2021, FUTURE GENER COMP SY, V125, P156, DOI 10.1016/j.future.2021.06.047
   Novaes MP, 2020, IEEE ACCESS, V8, P83765, DOI 10.1109/ACCESS.2020.2992044
   Rajesh P, 2022, ENERGY SYST, V13, P939, DOI 10.1007/s12667-021-00452-w
   Rajesh P, 2021, J ENG DES TECHNOL, DOI 10.1108/JEDT-12-2020-0494
   Rehman SU, 2021, FUTURE GENER COMP SY, V118, P453, DOI 10.1016/j.future.2021.01.022
   Salam A., 2020, INTERNET THINGS SUST, P1, DOI [10.1007/978-3-030-35291-2_1, 10.1007/978-3-030-35291-2, DOI 10.1007/978-3-030-35291-2_1]
   Sarker IH, 2021, MOBILE NETW APPL, V26, P285, DOI 10.1007/s11036-020-01650-z
   Satheesh N, 2020, MICROPROCESS MICROSY, V79, DOI 10.1016/j.micpro.2020.103285
   Sengupta J, 2020, J NETW COMPUT APPL, V149, DOI 10.1016/j.jnca.2019.102481
   Shajin FH., 2020, J SOFT COMPUT ENG AP, V1, P7
   Srivastava A, 2021, COMPUT SCI REV, V39, DOI 10.1016/j.cosrev.2020.100359
   Susilo B, 2021, 2021 IEEE 11TH ANNUAL COMPUTING AND COMMUNICATION WORKSHOP AND CONFERENCE (CCWC), P807, DOI 10.1109/CCWC51732.2021.9375951
   unb.ca, CIC DAT DDOS 2019
   unb.ca, CIC DAT IDS 2018
   Wang JJ, 2020, IEEE COMMUN SURV TUT, V22, P1472, DOI 10.1109/COMST.2020.2965856
NR 28
TC 1
Z9 1
U1 2
U2 8
PD JAN 25
PY 2023
VL 35
IS 2
DI 10.1002/cpe.7503
EA NOV 2022
WC Computer Science, Software Engineering; Computer Science, Theory &
   Methods
DA 2023-11-11
ER

PT J
AU Holker, R
   Susan, S
AF Holker, Ruchi
   Susan, Seba
TI Neuroscience-Inspired Parameter Selection of Spiking Neuron Using
   Hodgkin Huxley Model
SO INTERNATIONAL JOURNAL OF SOFTWARE SCIENCE AND COMPUTATIONAL
   INTELLIGENCE-IJSSCI
DT Article
DE Class 1 Neuron; Class 2 Neuron; Hodgkin-Huxley Model; Integrator Neuron;
   NEURON Simulator; Parameter Selection; Phasic Spiking Neuron; Spiking
   Neural Network; Spiking Neuron
ID INTEGRATE-AND-FIRE; PHASE-DIAGRAM; NETWORKS; SIMULATION
AB Spiking neural networks (SNN) are currently being researched to design an artificial brain to teach it how to think, perform, and learn like a human brain. This paper focuses on exploring optimal values of parameters of biological spiking neurons for the Hodgkin Huxley (HH) model. The HH model exhibits maximum number of neurocomputational properties as compared to other spiking models, as per previous research. This paper investigates the HH model parameters of Class 1, Class 2, phasic spiking, and integrator neurocomputational properties. For the simulation of spiking neurons, the NEURON simulator is used since it is easy to understand and code.
C1 [Holker, Ruchi] Delhi Technol Univ, Informat Technol Dept, Delhi, India.
   [Susan, Seba] Delhi Technol Univ, Delhi, India.
RP Holker, R (corresponding author), Delhi Technol Univ, Informat Technol Dept, Delhi, India.
CR Abbott LF, 1999, BRAIN RES BULL, V50, P303, DOI 10.1016/S0361-9230(99)00161-6
   Alonso LM, 2019, ELIFE, V8, DOI 10.7554/eLife.42722
   Beeman D, 2013, ENCY COMPUTATIONAL N
   Bohte Sander M., 2004, Natural Computing, V3, P195, DOI 10.1023/B:NACO.0000027755.02868.60
   Börgers C, 2017, TEXTS APPL MATH, V66, P51, DOI 10.1007/978-3-319-51171-9_8
   Bower J. M., 2014, GENESIS GEN NEURAL S
   Brette R, 2007, J COMPUT NEUROSCI, V23, P349, DOI 10.1007/s10827-007-0038-6
   Brette R, 2015, FRONT SYST NEUROSCI, V9, DOI 10.3389/fnsys.2015.00151
   Brunel N, 2007, BIOL CYBERN, V97, P341, DOI 10.1007/s00422-007-0189-6
   Burkitt AN, 2006, BIOL CYBERN, V95, P1, DOI 10.1007/s00422-006-0068-6
   Campbell K, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10020550
   Carlu M, 2020, J NEUROPHYSIOL, V123, P1042, DOI 10.1152/jn.00399.2019
   Cessac B, 2010, J PHYSIOL-PARIS, V104, P5, DOI 10.1016/j.jphysparis.2009.11.002
   Colwell LJ, 2009, PLOS COMPUT BIOL, V5, DOI 10.1371/journal.pcbi.1000265
   de Queiroz MS, 2006, NEUROCOMPUTING, V70, P14, DOI 10.1016/j.neucom.2006.07.002
   GERSTNER W, 1995, PHYS REV E, V51, P738, DOI 10.1103/PhysRevE.51.738
   Gerstner W., 2002, SPIKING NEURON MODEL, DOI [DOI 10.1017/CBO9780511815706, 10.1017/cbo9780511815706]
   Gewaltig M. O., 2012, COMPUTATIONAL SYSTEM, P533, DOI [10.1007/978-94-007-3858-4_18, DOI 10.1007/978-94-007-3858-4_18]
   Gruning A, 2014, ESANN
   Hansel D, 1998, NEURAL COMPUT, V10, P467, DOI 10.1162/089976698300017845
   Hernández OE, 2013, BMC MED EDUC, V13, DOI 10.1186/1472-6920-13-70
   Hines ML, 1997, NEURAL COMPUT, V9, P1179, DOI 10.1162/neco.1997.9.6.1179
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V116, P424, DOI 10.1113/jphysiol.1952.sp004716
   HODGKIN AL, 1948, J PHYSIOL-LONDON, V107, P165, DOI 10.1113/jphysiol.1948.sp004260
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V116, P497, DOI 10.1113/jphysiol.1952.sp004719
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V116, P449, DOI 10.1113/jphysiol.1952.sp004717
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V116, P473, DOI 10.1113/jphysiol.1952.sp004718
   Huguet G, 2017, FRONT COMPUT NEUROSC, V11, DOI 10.3389/fncom.2017.00003
   Izhikevich EM, 2000, INT J BIFURCAT CHAOS, V10, P1171, DOI 10.1142/S0218127400000840
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2007, DYNAMICAL SYSTEMS NE, DOI 10.7551/mitpress/2526.001.0001
   Jolivet R, 2004, J NEUROPHYSIOL, V92, P959, DOI 10.1152/jn.00190.2004
   Jolivet R, 2003, LECT NOTES COMPUT SC, V2714, P846
   Kawaguchi Y, 2002, J NEUROCYTOL, V31, P277, DOI 10.1023/A:1024126110356
   Lindner B, 2002, PHYS REV E, V66, DOI 10.1103/PhysRevE.66.031916
   Ly C, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0176963
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Mohamed AAR, 2019, IEEE PES INNOV SMART, DOI [10.1109/isgteurope.2019.8905643, 10.1007/s13042-019-01053-x, 10.1109/indicon47234.2019.9029043]
   Ori H, 2020, P NATL ACAD SCI USA, V117, P3575, DOI 10.1073/pnas.1916514117
   Paugam-Moisy H, 2012, HDB NATURAL COMPUTIN, V1, P1, DOI DOI 10.1007/978-3-540-92910-9_10
   Ponulak F, 2011, ACTA NEUROBIOL EXP, V71, P409
   Prescott SA, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.1000198
   Serrano-Gotarredona T, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00002
   Seyed-allaei H, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00019
   Susan Seba, 2019, Computational Intelligence: Theories, Applications and Future DirectionsVolume II. ICCI-2017. Advances in Intelligent Systems and Computing (AISC 799), P545, DOI 10.1007/978-981-13-1135-2_41
   Susan Seba, 2019, Computational Intelligence: Theories, Applications and Future DirectionsVolume II. ICCI-2017. Advances in Intelligent Systems and Computing (AISC 799), P201, DOI 10.1007/978-981-13-1135-2_16
   Susan S, 2020, SOFT COMPUT, V24, P18219, DOI 10.1007/s00500-020-05080-7
   Tateno T, 2004, J NEUROPHYSIOL, V92, P2283, DOI 10.1152/jn.00109.2004
   Vreeken J, 2003, SPIKING NEURAL NETWO
   Wang HT, 2013, J THEOR BIOL, V328, P19, DOI 10.1016/j.jtbi.2013.03.003
   Wang HT, 2011, PHYS REV E, V83, DOI 10.1103/PhysRevE.83.021915
   Yamauchi S, 2011, FRONT COMPUT NEUROSC, V5, DOI [10.3389/fncom.2011.00042, 10.3389/fncom.2011.00029]
   Zhang J., 2019, ARXIV PREPRINT ARXIV
NR 55
TC 2
Z9 2
U1 4
U2 11
PD APR-JUN
PY 2021
VL 13
IS 2
BP 89
EP 106
DI 10.4018/IJSSCI.2021040105
WC Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Computer Science, Software Engineering
DA 2023-11-11
ER

PT J
AU Shi, KL
   Heng, SZ
   Wang, XJ
   Liu, SY
   Cui, HY
   Chen, CS
   Zhu, YX
   Xu, WG
   Wan, CJ
   Wan, Q
AF Shi, Kailu
   Heng, Sizhuo
   Wang, Xiangjing
   Liu, Siyao
   Cui, Hangyuan
   Chen, Chunsheng
   Zhu, Yixin
   Xu, Weigao
   Wan, Changjin
   Wan, Qing
TI An Oxide Based Spiking Thermoreceptor for Low-Power Thermography Edge
   Detection
SO IEEE ELECTRON DEVICE LETTERS
DT Article
DE Firing; Image edge detection; Power demand; Temperature; Switches;
   Threshold voltage; Neurons; Spiking neural network; neuromorphic
   perceptual system; artificial spiking thermoreceptor; threshold switch
   memristor; edge detection
AB Thermoreceptors can encode thermal stimuli into spikes that are processed by the neural network to endow human with thermal perception. Such energy-efficiency and robust interaction with real-world have inspired the rise of the artificial spiking thermoreceptor (AST). However, monolithic spiking thermoreceptor is still rarely reported, whichmay due to the lack of device that can simultaneously implement temperature-sensing and spike-encoding functions. Here, we demonstrate an artificial spiking thermoreceptor based on Ag/TaOX/AlOX/ ITO threshold switching memristor to achieve human-like thermal perception. The device is able to encode thermal information into spikes at a low power consumption (<240 nW). These advantages consequently facilitate the demonstration of power efficient and accurate thermography edge detection based on the array of such AST and a pulse coupled neural network (PCNN).
C1 [Shi, Kailu; Heng, Sizhuo; Wang, Xiangjing; Liu, Siyao; Cui, Hangyuan; Chen, Chunsheng; Zhu, Yixin; Xu, Weigao; Wan, Changjin; Wan, Qing] Nanjing Univ, Sch Elect Sci & Engn, Nanjing 210023, Peoples R China.
   [Shi, Kailu; Heng, Sizhuo; Wang, Xiangjing; Liu, Siyao; Cui, Hangyuan; Chen, Chunsheng; Zhu, Yixin; Xu, Weigao; Wan, Changjin; Wan, Qing] Nanjing Univ, Sch Chem & Chem Engn, Nanjing 210023, Peoples R China.
RP Wan, CJ; Wan, Q (corresponding author), Nanjing Univ, Sch Elect Sci & Engn, Nanjing 210023, Peoples R China.; Wan, CJ; Wan, Q (corresponding author), Nanjing Univ, Sch Chem & Chem Engn, Nanjing 210023, Peoples R China.
EM cjwan@nju.edu.cn; wanqing@nju.edu.cn
CR Bao L, 2021, IEEE ELECTR DEVICE L, V42, P102, DOI 10.1109/LED.2020.3037779
   Bhatnagar P, 2022, NANO ENERGY, V91, DOI 10.1016/j.nanoen.2021.106676
   Chen CS, 2022, ADV MATER, V34, DOI 10.1002/adma.202201895
   Covi E, 2021, IEEE T ELECTRON DEV, V68, P4335, DOI 10.1109/TED.2021.3076029
   Deng XY, 2022, MULTIMED TOOLS APPL, V81, P27187, DOI 10.1007/s11042-022-12725-2
   Han JK, 2022, ADV FUNCT MATER, V32, DOI 10.1002/adfm.202204102
   Hua QL, 2019, GLOB CHALL, V3, DOI 10.1002/gch2.201900015
   Li FF, 2021, ACS NANO, V15, P16422, DOI 10.1021/acsnano.1c05836
   Midya R, 2019, ADV ELECTRON MATER, V5, DOI 10.1002/aelm.201900060
   Mu BY, 2021, SMALL, V17, DOI 10.1002/smll.202103837
   Nili H, 2016, NANOTECHNOLOGY, V27, DOI 10.1088/0957-4484/27/50/505210
   Park J, 2015, SCI ADV, V1, DOI 10.1126/sciadv.1500661
   Pickett MD, 2013, NAT MATER, V12, P114, DOI [10.1038/NMAT3510, 10.1038/nmat3510]
   Romanovsky AA, 2018, HAND CLINIC, V156, P3, DOI 10.1016/B978-0-444-63912-7.00001-1
   SPRAY DC, 1986, ANNU REV PHYSIOL, V48, P625
   Tang XL, 2014, 2014 11TH INTERNATIONAL COMPUTER CONFERENCE ON WAVELET ACTIVE MEDIA TECHNOLOGY AND INFORMATION PROCESSING (ICCWAMTIP), P169, DOI 10.1109/ICCWAMTIP.2014.7073383
   Tee BCK, 2015, SCIENCE, V350, P313, DOI 10.1126/science.aaa9306
   Wang W, 2021, ADV INTELL SYST-GER, V3, DOI 10.1002/aisy.202000224
   Wang W, 2019, NAT COMMUN, V10, DOI 10.1038/s41467-018-07979-0
   Wang ZX, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00783
   Wang ZR, 2017, NAT MATER, V16, P101, DOI [10.1038/nmat4756, 10.1038/NMAT4756]
   Wu QT, 2020, NANO LETT, V20, P8015, DOI 10.1021/acs.nanolett.0c02892
   Wu XD, 2021, ADV FUNCT MATER, V31, DOI 10.1002/adfm.202010824
   Wu ZH, 2020, ADV MATER, V32, DOI 10.1002/adma.202004398
   Yi W, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-07052-w
   Yoon JH, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-017-02572-3
   Zhang C, 2019, ADV FUNCT MATER, V29, DOI 10.1002/adfm.201808783
   Zhang XM, 2018, IEEE ELECTR DEVICE L, V39, P308, DOI 10.1109/LED.2017.2782752
   Zhu JX, 2022, ADV MATER, V34, DOI 10.1002/adma.202200481
NR 29
TC 3
Z9 3
U1 13
U2 18
PD DEC
PY 2022
VL 43
IS 12
BP 2196
EP 2199
DI 10.1109/LED.2022.3215693
WC Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Lien, HH
   Chang, TS
AF Lien, Hong-Han
   Chang, Tian-Sheuan
TI Sparse Compressed Spiking Neural Network Accelerator for Object
   Detection
SO IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS I-REGULAR PAPERS
DT Article
DE Object detection; Hardware; Convolution; Training; Neurons; Parallel
   processing; Topology; Spiking neural network; computer vision; object
   detection; VLSI hardware design
ID ON-CHIP; DESIGN; MEMORY
AB Spiking neural networks (SNNs), which are inspired by the human brain, have recently gained popularity due to their relatively simple and low-power hardware for transmitting binary spikes and highly sparse activation maps. However, because SNNs contain extra time dimension information, the SNN accelerator will require more buffers and take longer to infer, especially for the more difficult high-resolution object detection task. As a result, this paper proposes a sparse compressed spiking neural network accelerator that takes advantage of the high sparsity of activation maps and weights by utilizing the proposed gated one-to-all product for low power and highly parallel model execution. The experimental result of the neural network shows 71.5% mAP with mixed (1,3) time steps on the IVS 3cls dataset. The accelerator with the TSMC 28nm CMOS process can achieve 1024x576.29 frames per second processing when running at 500MHz with 35.88TOPS/W energy efficiency and 1.05mJ energy consumption per frame.
C1 [Lien, Hong-Han; Chang, Tian-Sheuan] Natl Yang Ming Chiao Tung Univ, Inst Elect, Hsinchu 30010, Taiwan.
RP Chang, TS (corresponding author), Natl Yang Ming Chiao Tung Univ, Inst Elect, Hsinchu 30010, Taiwan.
EM scott860228.ee08@nycu.edu.tw; tschang@nycu.edu.tw
CR Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Chen QY, 2022, IEEE T CIRCUITS-II, V69, P574, DOI 10.1109/TCSII.2021.3098633
   Chen Y., ARXIV210504916, V2021
   Datta G., 2021, ARXIV211212133
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Deng L, 2023, IEEE T NEUR NET LEAR, V34, P2791, DOI 10.1109/TNNLS.2021.3109064
   Frenkel C, 2019, IEEE T BIOMED CIRC S, V13, P145, DOI 10.1109/TBCAS.2018.2880425
   Gondimalla A, 2019, MICRO'52: THE 52ND ANNUAL IEEE/ACM INTERNATIONAL SYMPOSIUM ON MICROARCHITECTURE, P151, DOI 10.1145/3352460.3358291
   Guo JI, 2020, IEEE J EM SEL TOP C, V10, P388, DOI 10.1109/JETCAS.2020.3015753
   Han S, 2015, ADV NEUR IN, V28
   Khodamoradi Alireza, 2021, FPGA '21: The 2021 ACM/SIGDA International Symposium on Field-Programmable, P194, DOI 10.1145/3431920.3439283
   Kim S, 2020, AAAI CONF ARTIF INTE, V34, P11270
   Kundu S, 2021, IEEE WINT CONF APPL, P3952, DOI 10.1109/WACV48630.2021.00400
   Li G, 2022, IEEE T COMPUT AID D, V41, P1436, DOI 10.1109/TCAD.2021.3082868
   Lien HH, 2021, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS51556.2021.9401181
   Liu YL, 2013, IEEE INT WORKS GENET, P1, DOI [10.1155/2013/146860, 10.1109/GEFS.2013.6601048]
   Loshchilov Ilya, 2017, ARXIV171105101
   Malladi KT, 2012, CONF PROC INT SYMP C, P37, DOI 10.1109/ISCA.2012.6237004
   Moradi S, 2018, IEEE T BIOMED CIRC S, V12, P106, DOI 10.1109/TBCAS.2017.2759700
   Narayanan S, 2020, ANN I S COM, P349, DOI 10.1109/ISCA45697.2020.00038
   Narayanan S, 2017, IEEE IJCNN, P2451, DOI 10.1109/IJCNN.2017.7966154
   Painkras E, 2013, IEEE J SOLID-ST CIRC, V48, P1943, DOI 10.1109/JSSC.2013.2259038
   Parashar A, 2017, 44TH ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA 2017), P27, DOI 10.1145/3079856.3080254
   Park J, 2019, ISSCC DIG TECH PAP I, V62, P140, DOI 10.1109/ISSCC.2019.8662398
   Rathi N, 2023, IEEE T NEUR NET LEAR, V34, P3174, DOI 10.1109/TNNLS.2021.3111897
   Rathi N, 2019, IEEE T COMPUT AID D, V38, P668, DOI 10.1109/TCAD.2018.2819366
   Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690
   Schemmel J, 2010, IEEE INT SYMP CIRC S, P1947, DOI 10.1109/ISCAS.2010.5536970
   Shi Y, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00405
   Stuijt J, 2021, FRONT NEUROSCI-SWITZ, V15, DOI 10.3389/fnins.2021.664208
   Tsai CC, 2020, IEEE INT CONF MULTI, DOI 10.1109/icmew46912.2020.9106010
   Wang CY, 2020, IEEE COMPUT SOC CONF, P1571, DOI 10.1109/CVPRW50498.2020.00203
   Wu YJ, 2019, AAAI CONF ARTIF INTE, P1311
   Zheng H., 2020, ARXIV201105280
NR 35
TC 3
Z9 3
U1 3
U2 26
PD MAY
PY 2022
VL 69
IS 5
BP 2060
EP 2069
DI 10.1109/TCSI.2022.3149006
EA FEB 2022
WC Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Nobukawa, S
   Nishimura, H
   Wagatsuma, N
   Ando, S
   Yamanishi, T
AF Nobukawa, Sou
   Nishimura, Haruhiko
   Wagatsuma, Nobuhiko
   Ando, Satoshi
   Yamanishi, Teruya
TI Long-Tailed Characteristic of Spiking Pattern Alternation Induced by
   Log-Normal Excitatory Synaptic Distribution
SO IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
DT Article
DE Log-normal distribution; long-tailed distribution; pattern alternation;
   spiking neural network
ID FUNCTIONAL CONNECTIVITY; BINOCULAR-RIVALRY; NEURAL-NETWORKS; FIRING
   RATES; FREQUENCY; INHIBITION; MODULATION; CORTEX; SIGNAL; MODEL
AB Studies of structural connectivity at the synaptic level show that in synaptic connections of the cerebral cortex, the excitatory postsynaptic potential (EPSP) in most synapses exhibits sub-mV values, while a small number of synapses exhibit large EPSPs (greater than or similar to 1.0 [mV]). This means that the distribution of EPSP fits a log-normal distribution. While not restricting structural connectivity, skewed and long-tailed distributions have been widely observed in neural activities, such as the occurrences of spiking rates and the size of a synchronously spiking population. Many studies have been modeled this long-tailed EPSP neural activity distribution; however, its causal factors remain controversial. This study focused on the long-tailed EPSP distributions and interlateral synaptic connections primarily observed in the cortical network structures, thereby having constructed a spiking neural network consistent with these features. Especially, we constructed two coupled modules of spiking neural networks with excitatory and inhibitory neural populations with a log-normal EPSP distribution. We evaluated the spiking activities for different input frequencies and with/without strong synaptic connections. These coupled modules exhibited intermittent intermodule-alternative behavior, given moderate input frequency and the existence of strong synaptic and intermodule connections. Moreover, the power analysis, multiscale entropy analysis, and surrogate data analysis revealed that the long-tailed EPSP distribution and intermodule connections enhanced the complexity of spiking activity at large temporal scales and induced nonlinear dynamics and neural activity that followed the long-tailed distribution.
C1 [Nobukawa, Sou] Chiba Inst Technol, Dept Comp Sci, Chiba 2750016, Japan.
   [Nishimura, Haruhiko] Univ Hyogo, Grad Sch Appl Informat, Kobe, Hyogo 6500047, Japan.
   [Wagatsuma, Nobuhiko] Toho Univ, Fac Sci, Dept Informat Sci, Chiba 2748510, Japan.
   [Ando, Satoshi] JSOL Corp, Financial & Serv Ind Business Unit, Tokyo 1040053, Japan.
   [Yamanishi, Teruya] Fukui Univ Technol, AI & IoT Ctr, Fukui 9108505, Japan.
RP Nobukawa, S (corresponding author), Chiba Inst Technol, Dept Comp Sci, Chiba 2750016, Japan.
EM nobukawa@cs.it-chiba.ac.jp
CR Bartos M, 2007, NAT REV NEUROSCI, V8, P45, DOI 10.1038/nrn2044
   Bassett DS, 2017, NAT NEUROSCI, V20, P353, DOI 10.1038/nn.4502
   Battaglia FP, 2005, NEURAL NETWORKS, V18, P1280, DOI 10.1016/j.neunet.2005.08.011
   Bellec G., 2018, ADV NEURAL INFORM PR, P795
   Blake R, 2002, NAT REV NEUROSCI, V3, P13, DOI 10.1038/nrn701
   Börgers C, 2003, NEURAL COMPUT, V15, P509, DOI 10.1162/089976603321192059
   BORSELLINO A, 1972, KYBERNETIK, V10, P139, DOI 10.1007/BF00290512
   Bullmore ET, 2009, NAT REV NEUROSCI, V10, P186, DOI 10.1038/nrn2575
   Burkitt AN, 2006, BIOL CYBERN, V95, P97, DOI 10.1007/s00422-006-0082-8
   Buzsáki G, 2014, NAT REV NEUROSCI, V15, P264, DOI 10.1038/nrn3687
   CALLAWAY EM, 1991, P NATL ACAD SCI USA, V88, P745, DOI 10.1073/pnas.88.3.745
   Callaway EM, 1998, ANNU REV NEUROSCI, V21, P47, DOI 10.1146/annurev.neuro.21.1.47
   Costa M, 2002, PHYS REV LETT, V89, DOI 10.1103/PhysRevLett.89.068102
   Eguíluz VM, 2005, PHYS REV LETT, V94, DOI 10.1103/PhysRevLett.94.018102
   Fell J, 2000, ACTA NEUROBIOL EXP, V60, P87
   Fiser J, 2004, NATURE, V431, P573, DOI 10.1038/nature02907
   Garrett DD, 2011, J NEUROSCI, V31, P4496, DOI 10.1523/JNEUROSCI.5641-10.2011
   Garrett DD, 2010, J NEUROSCI, V30, P4914, DOI 10.1523/JNEUROSCI.5166-09.2010
   Gast R, 2020, NEURAL COMPUT, V32, P1615, DOI 10.1162/neco_a_01300
   Gerfen CR, 2018, J NEUROSCI RES, V96, P1467, DOI 10.1002/jnr.23978
   GILBERT CD, 1989, J NEUROSCI, V9, P2432
   GILBERT CD, 1983, J NEUROSCI, V3, P1116
   Goodman Dan FM, 2014, BMC NEUROSCI, V15, P1, DOI DOI 10.1186/1471-2202-15-S1-P199
   Guo DQ, 2010, IEEE T NEURAL NETWOR, V21, P895, DOI 10.1109/TNN.2010.2044419
   Hagmann P, 2008, PLOS BIOL, V6, P1479, DOI 10.1371/journal.pbio.0060159
   Hagmann P, 2007, PLOS ONE, V2, DOI 10.1371/journal.pone.0000597
   Hasenstaub A, 2005, NEURON, V47, P423, DOI 10.1016/j.neuron.2005.06.016
   Hirase H, 2001, P NATL ACAD SCI USA, V98, P9386, DOI 10.1073/pnas.161274398
   Hiratani N, 2013, FRONT COMPUT NEUROSC, V6, DOI 10.3389/fncom.2012.00102
   Hromadka T, 2008, PLOS BIOL, V6, P124, DOI 10.1371/journal.pbio.0060016
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Kada H, 2018, FRONT COMPUT NEUROSC, V12, DOI 10.3389/fncom.2018.00104
   Kanamaru T, 2003, PHYS REV E, V67, DOI 10.1103/PhysRevE.67.031916
   Kanamaru T, 2017, NEURAL COMPUT, V29, P1696, DOI 10.1162/NECO_a_00965
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Kossio FYK, 2018, PHYS REV LETT, V121, DOI 10.1103/PhysRevLett.121.058301
   Kovacic G, 2009, PHYS REV E, V80, DOI 10.1103/PhysRevE.80.021904
   Kriener B, 2014, FRONT COMPUT NEUROSC, V8, DOI 10.3389/fncom.2014.00136
   Kulkarni SR, 2018, NEURAL NETWORKS, V103, P118, DOI 10.1016/j.neunet.2018.03.019
   Lee B, 2018, CELL REP, V25, P1548, DOI 10.1016/j.celrep.2018.10.029
   Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508
   Lefort S, 2009, NEURON, V61, P301, DOI 10.1016/j.neuron.2008.12.020
   LEHKY SR, 1995, P ROY SOC B-BIOL SCI, V259, P71, DOI 10.1098/rspb.1995.0011
   LEVELT WJM, 1967, BRIT J PSYCHOL, V58, P143, DOI 10.1111/j.2044-8295.1967.tb01068.x
   Lin XH, 2017, NEUROCOMPUTING, V237, P59, DOI 10.1016/j.neucom.2016.08.087
   Lin ZT, 2018, NEUROCOMPUTING, V275, P94, DOI 10.1016/j.neucom.2017.05.009
   Martí D, 2018, PHYS REV E, V97, DOI 10.1103/PhysRevE.97.062314
   Mastrogiuseppe F, 2017, PLOS COMPUT BIOL, V13, DOI 10.1371/journal.pcbi.1005498
   McCormick DA, 1999, SCIENCE, V285, P541, DOI 10.1126/science.285.5427.541
   McIntosh AR, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.1000106
   Mizuseki K, 2014, PHILOS T R SOC B, V369, DOI 10.1098/rstb.2012.0530
   Mizuseki K, 2013, CELL REP, V4, P1010, DOI 10.1016/j.celrep.2013.07.039
   Morishima M, 2006, J NEUROSCI, V26, P4394, DOI 10.1523/JNEUROSCI.0252-06.2006
   Mozafari M, 2018, IEEE T NEUR NET LEAR, V29, P6178, DOI 10.1109/TNNLS.2018.2826721
   Nagao N, 2000, NEURAL PROCESS LETT, V12, P267, DOI 10.1023/A:1026511124944
   Neske GT, 2016, J NEUROPHYSIOL, V116, P351, DOI 10.1152/jn.00071.2016
   Nobukawa S, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-49286-8
   Nobukawa S, 2018, LECT NOTES COMPUT SC, V11301, P535, DOI 10.1007/978-3-030-04167-0_48
   Nobukawa S, 2019, NEUROIMAGE, V188, P357, DOI 10.1016/j.neuroimage.2018.12.008
   O'Connor DH, 2010, NEURON, V67, P1048, DOI 10.1016/j.neuron.2010.08.026
   Peyrache A, 2012, P NATL ACAD SCI USA, V109, P1731, DOI 10.1073/pnas.1109895109
   Rabinovich MI, 2006, REV MOD PHYS, V78, P1213, DOI 10.1103/RevModPhys.78.1213
   Riecke H, 2007, CHAOS, V17, DOI 10.1063/1.2743611
   Samura T, 2015, COGN NEURODYNAMICS, V9, P265, DOI 10.1007/s11571-015-9329-1
   Schreiber T, 1996, PHYS REV LETT, V77, P635, DOI 10.1103/PhysRevLett.77.635
   Shafi M, 2007, NEUROSCIENCE, V146, P1082, DOI 10.1016/j.neuroscience.2006.12.072
   Shanahan M, 2008, PHYS REV E, V78, DOI 10.1103/PhysRevE.78.041924
   She Q, 2016, SCI REP-UK, V6, DOI 10.1038/srep21468
   Song S, 2005, PLOS BIOL, V3, P507, DOI 10.1371/journal.pbio.0030068
   Sporns O, 2014, NAT NEUROSCI, V17, P652, DOI 10.1038/nn.3690
   Stam CJ, 2007, HUM BRAIN MAPP, V28, P1178, DOI 10.1002/hbm.20346
   Takahashi T, 2004, J NEUROL SCI, V225, P33, DOI 10.1016/j.jns.2004.06.016
   Takahashi T, 2018, CLIN NEUROPHYSIOL, V129, P222, DOI 10.1016/j.clinph.2017.11.004
   Takahashi T, 2017, CLIN NEUROPHYSIOL, V128, P1457, DOI 10.1016/j.clinph.2017.05.010
   Takahashi T, 2013, PROG NEURO-PSYCHOPH, V45, P258, DOI 10.1016/j.pnpbp.2012.05.001
   Tavanaei A, 2018, 2018 INT JOINT C NEU, DOI DOI 10.1109/IJCNN.2018.8489104
   Tavanaei A, 2018, NEURAL NETWORKS, V105, P294, DOI 10.1016/j.neunet.2018.05.018
   Teramae J.-N., 2012, SCI REP-UK, V2, P1
   Teramae J, 2016, AIP CONF PROC, V1738, DOI 10.1063/1.4951999
   Timme NM, 2016, PLOS COMPUT BIOL, V12, DOI 10.1371/journal.pcbi.1004858
   Traub RD., 1999, FAST OSCILLATIONS CO
   van den Heuvel M, 2008, J NEUROSCI, V28, P10844, DOI 10.1523/JNEUROSCI.2964-08.2008
   van den Heuvel MP, 2013, TRENDS COGN SCI, V17, P683, DOI 10.1016/j.tics.2013.09.012
   Vogels TP, 2005, J NEUROSCI, V25, P10786, DOI 10.1523/JNEUROSCI.3508-05.2005
   Wagatsuma N, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0080788
   Wagatsuma Nobuhiko, 2011, Front Comput Neurosci, V5, P31, DOI 10.3389/fncom.2011.00031
   WALKER P, 1975, PERCEPT PSYCHOPHYS, V18, P467, DOI 10.3758/BF03204122
   Whittington MA, 2000, INT J PSYCHOPHYSIOL, V38, P315, DOI 10.1016/S0167-8760(00)00173-2
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Yang AC, 2013, PROG NEURO-PSYCHOPH, V45, P253, DOI 10.1016/j.pnpbp.2012.09.015
   Zhang JW, 2019, J COMPUT NEUROSCI, V46, P211, DOI 10.1007/s10827-019-00712-w
NR 91
TC 9
Z9 9
U1 3
U2 11
PD AUG
PY 2021
VL 32
IS 8
BP 3525
EP 3537
DI 10.1109/TNNLS.2020.3015208
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Computer Science, Theory & Methods; Engineering,
   Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Iannella, N
   Kindermann, L
AF Iannella, N
   Kindermann, L
TI Finding iterative roots with a spiking neural network
SO INFORMATION PROCESSING LETTERS
DT Article
DE spiking neural network; iterative root; functional equations;
   distributed computing
AB In recent years, both multilayer perceptrons and networks of spiking neurons have been used in applications ranging from detailed models of specific cortical areas to image processing. A more challenging application is to find solutions to functional equations in order to gain insights to underlying phenomena. Finding the roots of real valued monotonically increasing function mappings is the solution to a particular class of functional equation. Furthermore, spiking neural network approaches in solving problems described by functional equations, may be an useful tool to provide important insights to how different regions of the brain may co-ordinate signaling within and between modalities, thus providing a possible basis to construct a theory of brain function. In this letter, we present for the first time a spiking neural network architecture based on integrate-and-fire units and delays, that is capable of calculating the functional or iterative root of nonlinear functions, by solving a particular class of functional equation. (c) 2005 Elsevier B.V. All rights reserved.
C1 RIKEN, Brain Sci Inst, Lab Visual Neurocomp, Wako, Saitama 3510198, Japan.
   RIKEN, Brain Sci Inst, Lab Math Neurosci, Wako, Saitama 3510198, Japan.
RP Iannella, N (corresponding author), RIKEN, Brain Sci Inst, Lab Visual Neurocomp, 2-1 Hirosawa, Wako, Saitama 3510198, Japan.
EM nicolang@brain.riken.go.jp
CR Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   Brunel N, 2000, J COMPUT NEUROSCI, V8, P183, DOI 10.1023/A:1008925309027
   Iannella N, 2004, MATH BIOSCI, V188, P117, DOI 10.1016/j.mbs.2003.10.002
   Iannella N, 2001, NEURAL NETWORKS, V14, P933, DOI 10.1016/S0893-6080(01)00080-6
   Izhikevich EM, 2000, INT J BIFURCAT CHAOS, V10, P1171, DOI 10.1142/S0218127400000840
   Izhikevich EM, 2001, SIAM REV, V43, P315, DOI 10.1137/S0036144500382064
   Kindermann L, 1998, ICONIP'98: THE FIFTH INTERNATIONAL CONFERENCE ON NEURAL INFORMATION PROCESSING JOINTLY WITH JNNS'98: THE 1998 ANNUAL CONFERENCE OF THE JAPANESE NEURAL NETWORK SOCIETY - PROCEEDINGS, VOLS 1-3, P713
   KINDERMANN L, 2000, P 7 INT C NEUR INF P, P565
   KUZMA M, 1990, ITERATIVE FUNCTIONAL
   RICE RE, 1980, AM MATH MON, V87, P252, DOI 10.2307/2321556
NR 11
TC 25
Z9 26
U1 0
U2 4
PD SEP 30
PY 2005
VL 95
IS 6
BP 545
EP 551
DI 10.1016/j.ipl.2005.05.022
WC Computer Science, Information Systems
DA 2023-11-11
ER

PT J
AU Zhang, AG
   Li, XM
   Gao, YM
   Niu, YZ
AF Zhang, Anguo
   Li, Xiumin
   Gao, Yueming
   Niu, Yuzhen
TI Event-Driven Intrinsic Plasticity for Spiking Convolutional Neural
   Networks
SO IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
DT Article
DE IP networks; Neurons; Biological neural networks; Computational
   modeling; Biological system modeling; Encoding; Biomembranes;
   Event-driven intrinsic plasticity (IP); input-driven IP; IP; self-driven
   IP; spiking neural network (SNN)
ID EXCITABILITY; NEURONS; SPARSE
AB The biologically discovered intrinsic plasticity (IP) learning rule, which changes the intrinsic excitability of an individual neuron by adaptively turning the firing threshold, has been shown to be crucial for efficient information processing. However, this learning rule needs extra time for updating operations at each step, causing extra energy consumption and reducing the computational efficiency. The event-driven or spike-based coding strategy of spiking neural networks (SNNs), i.e., neurons will only be active if driven by continuous spiking trains, employs all-or-none pulses (spikes) to transmit information, contributing to sparseness in neuron activations. In this article, we propose two event-driven IP learning rules, namely, input-driven and self-driven IP, based on basic IP learning. Input-driven means that IP updating occurs only when the neuron receives spiking inputs from its presynaptic neurons, whereas self-driven means that IP updating only occurs when the neuron generates a spike. A spiking convolutional neural network (SCNN) is developed based on the ANN2SNN conversion method, i.e., converting a well-trained rate-based artificial neural network to an SNN via directly mapping the connection weights. By comparing the computational performance of SCNNs with different IP rules on the recognition of MNIST, FashionMNIST, Cifar10, and SVHN datasets, we demonstrate that the two event-based IP rules can remarkably reduce IP updating operations, contributing to sparse computations and accelerating the recognition process. This work may give insights into the modeling of brain-inspired SNNs for low-power applications.
C1 [Zhang, Anguo; Gao, Yueming] Fuzhou Univ, Coll Phys & Informat Engn, Fuzhou 350108, Peoples R China.
   [Zhang, Anguo; Gao, Yueming] Key Lab Med Instrumentat & Pharmaceut Technol Fuj, Fuzhou 350116, Peoples R China.
   [Zhang, Anguo] Ruijie Networks Co Ltd, Res Inst Ruijie, Fuzhou 350002, Peoples R China.
   [Li, Xiumin] Chongqing Univ, Coll Automat, Chongqing 400030, Peoples R China.
   [Niu, Yuzhen] Fuzhou Univ, Coll Math & Comp Sci, Fujian Key Lab Network Comp & Intelligent Informa, Fuzhou 350108, Fujian, Peoples R China.
   [Niu, Yuzhen] Minist Educ, Key Lab Spatial Data Min & Informat Sharing, Fuzhou 350108, Fujian, Peoples R China.
RP Li, XM (corresponding author), Chongqing Univ, Coll Automat, Chongqing 400030, Peoples R China.
EM xmli@cqu.edu.cn
CR [Anonymous], ARXIV160902053
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Compte A, 2003, J NEUROPHYSIOL, V89, P2707, DOI 10.1152/jn.00845.2002
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Debanne D, 2019, CURR OPIN NEUROBIOL, V54, P73, DOI 10.1016/j.conb.2018.09.001
   Deng L, 2020, NEURAL NETWORKS, V121, P294, DOI 10.1016/j.neunet.2019.09.005
   Desai NS, 1999, NAT NEUROSCI, V2, P515, DOI 10.1038/9165
   Diehl PU, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON REBOOTING COMPUTING (ICRC)
   Diehl Peter U, 2015, 2015 INT JOINT C NEU, P1, DOI [DOI 10.1109/IJCNN.2015.7280696, 10.1109/IJCNN.2015.7280696]
   Disterhoft JF, 2006, TRENDS NEUROSCI, V29, P587, DOI 10.1016/j.tins.2006.08.005
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   Han B, 2018, IEEE T MULTI-SCALE C, V4, P613, DOI 10.1109/TMSCS.2017.2737625
   Histed MH, 2009, NEURON, V63, P508, DOI 10.1016/j.neuron.2009.07.016
   Holt GR, 1997, NEURAL COMPUT, V9, P1001, DOI 10.1162/neco.1997.9.5.1001
   Jadi MP, 2014, P NATL ACAD SCI USA, V111, P6780, DOI 10.1073/pnas.1405300111
   Johnson AP, 2018, IEEE T CIRCUITS-I, V65, P687, DOI 10.1109/TCSI.2017.2726763
   Kaiser J., 2019, ARXIV190404805
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Li CG, 2013, IEEE T AUTON MENT DE, V5, P62, DOI 10.1109/TAMD.2012.2211101
   Li CG, 2011, IEEE T AUTON MENT DE, V3, P277, DOI 10.1109/TAMD.2011.2159379
   Maass W, 2004, J COMPUT SYST SCI, V69, P593, DOI 10.1016/j jcss.2004.04.001
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   MUKHOPADHYAY AK, 2018, P IEEE SENSORS, P2018, DOI DOI 10.1109/ICSENS.2018.8589757
   Nataraj K, 2010, NEURON, V68, P750, DOI 10.1016/j.neuron.2010.09.033
   Naveros F, 2015, IEEE T NEUR NET LEAR, V26, P1567, DOI 10.1109/TNNLS.2014.2345844
   Neckar A, 2019, P IEEE, V107, P144, DOI 10.1109/JPROC.2018.2881432
   Neftci E, 2014, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00272
   Neftci EO, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00324
   Netzer Y., 2011, NIPS WORKSH DEEP LEA, DOI DOI 10.2118/18761-MS
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   Rueckauer B., 2016, ARXIV161204052
   Rueckauer B, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351295
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Stromatias E, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00350
   Susi G., 2018, ARXIV180100864
   Touboul JD, 2011, J COMPUT NEUROSCI, V31, P485, DOI 10.1007/s10827-011-0327-y
   Triesch J, 2007, NEURAL COMPUT, V19, P885, DOI 10.1162/neco.2007.19.4.885
   Wang H, 2019, BRAIN TOPOGR, V32, P255, DOI 10.1007/s10548-018-0682-3
   Xiao H, 2017, ARXIV PREPRINT ARXIV
   Zhang AG, 2023, IEEE T COGN DEV SYST, V15, P337, DOI 10.1109/TCDS.2020.3041610
   Zhang AG, 2019, NEUROCOMPUTING, V365, P102, DOI 10.1016/j.neucom.2019.07.009
   Zhang ML, 2019, IEEE T NEUR NET LEAR, V30, P123, DOI 10.1109/TNNLS.2018.2833077
   Zhang ML, 2018, IEEE T COGN DEV SYST, V10, P151, DOI 10.1109/TCDS.2017.2651943
   Zhang W, 2003, NAT REV NEUROSCI, V4, P885, DOI 10.1038/nrn1248
NR 46
TC 16
Z9 16
U1 2
U2 17
PD MAY
PY 2022
VL 33
IS 5
BP 1986
EP 1995
DI 10.1109/TNNLS.2021.3084955
EA JUN 2021
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Computer Science, Theory & Methods; Engineering,
   Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Stromatias, E
   Soto, M
   Serrano-Gotarredona, T
   Linares-Barranco, B
AF Stromatias, Evangelos
   Soto, Miguel
   Serrano-Gotarredona, Teresa
   Linares-Barranco, Bernabe
TI An Event-Driven Classifier for Spiking Neural Networks Fed with
   Synthetic or Dynamic Vision Sensor Data
SO FRONTIERS IN NEUROSCIENCE
DT Article
DE spiking neural networks; supervised learning; event driven processing;
   DVS sensors; convolutional neural networks; fully connected neural
   networks; neuromorphic
ID LARGE-SCALE MODEL
AB This paper introduces a novel methodology for training an event-driven classifier within a Spiking Neural Network (SNN) System capable of yielding good classification results when using both synthetic input data and real data captured from Dynamic Vision Sensor (DVS) chips. The proposed supervised method uses the spiking activity provided by an arbitrary topology of prior SNN layers to build histograms and train the classifier in the frame domain using the stochastic gradient descent algorithm. In addition, this approach can cope with leaky integrate-and-fire neuron models within the SNN, a desirable feature for real-world SNN applications, where neural activation must fade away after some time in the absence of inputs. Consequently, this way of building histograms captures the dynamics of spikes immediately before the classifier. We tested our method on the MNIST data set using different synthetic encodings and real DVS sensory data sets such as N-MNIST, MNIST-DVS, and Poker-DVS using the same network topology and feature maps. We demonstrate the effectiveness of our approach by achieving the highest classification accuracy reported on the N-MNIST (97.77%) and Poker-DVS (100%) real DVS data sets to date with a spiking convolutional network. Moreover, by using the proposed method we were able to retrain the output layer of a previously reported spiking neural network and increase its performance by 2%, suggesting that the proposed classifier can be used as the output layer in works where features are extracted using unsupervised spike-based learning methods. In addition, we also analyze SNN performance figures such as total event activity and network latencies, which are relevant for eventual hardware implementations. In summary, the paper aggregates unsupervised-trained SNNs with a supervised-trained SNN classifier, combining and applying them to heterogeneous sets of benchmarks, both synthetic and from real DVS chips.
C1 [Stromatias, Evangelos; Soto, Miguel; Serrano-Gotarredona, Teresa; Linares-Barranco, Bernabe] Univ Seville, CSIC, Inst Microelectron Sevilla CNM, Seville, Spain.
RP Linares-Barranco, B (corresponding author), Univ Seville, CSIC, Inst Microelectron Sevilla CNM, Seville, Spain.
EM bernabe@imse-cnm.csic.es
CR Anand R, 2009, SELF-DEFENSE IN INTERNATIONAL RELATIONS, P1, DOI 10.1057/9780230245747
   [Anonymous], 2012, 2012 INT JOINT C NEU, DOI [DOI 10.1109/IJCNN.2012.6252637, 10.1109/IJCNN.2012.6252637]
   Leñero-Bardallo JA, 2011, IEEE J SOLID-ST CIRC, V46, P1443, DOI 10.1109/JSSC.2011.2118490
   Pérez-Carrasco JA, 2013, IEEE T PATTERN ANAL, V35, P2706, DOI 10.1109/TPAMI.2013.71
   Bichler O, 2012, NEURAL NETWORKS, V32, P339, DOI 10.1016/j.neunet.2012.02.022
   Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16
   Camuñas-Mesa L, 2010, IEEE INT SYMP CIRC S, P249, DOI 10.1109/ISCAS.2010.5537918
   Camuñas-Mesa L, 2012, IEEE J SOLID-ST CIRC, V47, P504, DOI 10.1109/JSSC.2011.2167409
   Camuñas-Mesa L, 2011, IEEE T CIRCUITS-I, V58, P777, DOI 10.1109/TCSI.2010.2078851
   Ciresan D, 2012, PROC CVPR IEEE, P3642, DOI 10.1109/CVPR.2012.6248110
   Cohen GK, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00184
   DAN Y, 1992, SCIENCE, V256, P1570, DOI 10.1126/science.1317971
   Delbruck T., 2013, JAER OPEN SOURCE PRO
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Diehl Peter U, 2015, 2015 INT JOINT C NEU, P1, DOI [DOI 10.1109/IJCNN.2015.7280696, 10.1109/IJCNN.2015.7280696]
   Eliasmith C, 2012, SCIENCE, V338, P1202, DOI 10.1126/science.1225266
   Esser S.K., 2015, ADV NEURAL INFORM PR, P1117
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   Gerstner W., 2002, SPIKING NEURON MODEL
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hunsberger E., 2015, ABS151008829 CORR
   Isaksson A, 2008, PATTERN RECOGN LETT, V29, P1960, DOI 10.1016/j.patrec.2008.06.018
   Izhikevich EM, 2008, P NATL ACAD SCI USA, V105, P3593, DOI 10.1073/pnas.0712231105
   Kheradpisheh S. R., 2016, ABS161101421 CORR
   Lagorce X, 2017, IEEE T PATTERN ANAL, V39, P1346, DOI 10.1109/TPAMI.2016.2574707
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee JM, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00191
   Lichtsteiner Patrick, 2008, IEEE Journal of Solid-State Circuits, V43, P566, DOI 10.1109/JSSC.2007.914337
   Liu Q, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00496
   Liu SC, 2010, IEEE INT SYMP CIRC S, P2027, DOI 10.1109/ISCAS.2010.5537164
   Maass W, 2004, J COMPUT SYST SCI, V69, P593, DOI 10.1016/j jcss.2004.04.001
   Mahowald M., 1994, ANALOG VLSI SYSTEM S
   Masquelier T, 2007, PLOS COMPUT BIOL, V3, P247, DOI 10.1371/journal.pcbi.0030031
   MEAD C, 1990, P IEEE, V78, P1629, DOI 10.1109/5.58356
   Merolla P., 2010, ABS10095473 CORR
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   MIT Technology Review, 2013, 10 BREAKTHR TECHN 20
   Neftci E., 2017, ABS161205596 CORR
   Neftci E, 2014, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00272
   Neftci EO, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00241
   Neil D, 2016, IEEE INT SYMP CIRC S, P2282, DOI 10.1109/ISCAS.2016.7539039
   Neil D, 2014, IEEE T VLSI SYST, V22, P2621, DOI 10.1109/TVLSI.2013.2294916
   Nowotny T, 2014, FRONT ROBOT AI, DOI 10.3389/frobt.2014.00005
   O'Connor P., 2016, ABS160208323 CORR
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   Orchard G, 2015, FRONT NEUROSCI-SWITZ, V9, DOI [10.3389/fnins.2015.00437, 10.3389/fhins.2015.00437]
   Orchard G, 2015, IEEE T PATTERN ANAL, V37, P2028, DOI 10.1109/TPAMI.2015.2392947
   Posch C, 2014, P IEEE, V102, P1470, DOI 10.1109/JPROC.2014.2346153
   Posch C, 2011, IEEE J SOLID-ST CIRC, V46, P259, DOI 10.1109/JSSC.2010.2085952
   Querlioz D, 2013, IEEE T NANOTECHNOL, V12, P288, DOI 10.1109/TNANO.2013.2250995
   Roclin D., 2013, NEUR NETW IJCNN 2013, P1
   Rueckauer B., 2016, ARXIV161204052 CORR
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Serrano-Gotarredona T, 2015, IEEE INT SYMP CIRC S, P2405, DOI 10.1109/ISCAS.2015.7169169
   Serrano-Gotarredona T, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00481
   Serrano-Gotarredona T, 2013, IEEE J SOLID-ST CIRC, V48, P827, DOI 10.1109/JSSC.2012.2230553
   SIEGERT AJF, 1951, PHYS REV, V81, P617, DOI 10.1103/PhysRev.81.617
   Soto M., 2017, SLOW POKER DVS DATA
   Stromatias E., 2013, 2013 INT JOINT C NEU, P1, DOI [DOI 10.1109/IJCNN.2013.6706927, 10.1109/ijcnn.2013.6706927]
   Stromatias E, 2015, IEEE IJCNN
   Stromatias E, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00222
   Tapson JC, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00104
   van Schaik A, 2015, NEUROCOMPUTING, V149, P233, DOI 10.1016/j.neucom.2014.01.071
NR 66
TC 67
Z9 72
U1 1
U2 20
PD JUN 28
PY 2017
VL 11
AR 350
DI 10.3389/fnins.2017.00350
WC Neurosciences
DA 2023-11-11
ER

PT C
AU Ye, C
   Kornijcuk, V
   Kim, J
   Jeong, DS
AF Ye, ChangMin
   Kornijcuk, Vladimir
   Kim, Jeeson
   Jeong, Doo Seok
GP IEEE
TI FPGA implementation of sequence-to-sequence predicting spiking neural
   networks
SO 2020 17TH INTERNATIONAL SOC DESIGN CONFERENCE (ISOCC 2020)
SE International SoC Design Conference
DT Proceedings Paper
CT 17th International SoC Design Conference (ISOCC)
CY OCT 21-24, 2020
CL Yeosu, SOUTH KOREA
DE sequence-predicting spiking neural network; LbAP algorithm; rule-based
   event routing
AB We propose a hardware-efficient method to implement sequence-predicting spiking neural networks (SPSNN) on a field-programmable gate array board. The SPSNN is capable of sequence-to-sequence prediction (associative recall) when fully trained using the learning by backpropagating action potential (LbAP) algorithm. The key to the hardware-efficiency lies in the rule-based event (routing) method in place of conventional lookup-table-based methods which are memory-hungry methods, particularly, when both forward and inverse lookups should be considered.
C1 [Ye, ChangMin; Kornijcuk, Vladimir; Kim, Jeeson; Jeong, Doo Seok] Hanyang Univ, Div Mat Sci & Engn, Seoul, South Korea.
RP Jeong, DS (corresponding author), Hanyang Univ, Div Mat Sci & Engn, Seoul, South Korea.
EM dooseokj@hanyang.ac.kr
CR Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Gerstner W., 2002, SPIKING NEURON MODEL
   Kim D, 2020, IEEE ACCESS, V8, P110523, DOI 10.1109/ACCESS.2020.3001296
   Kornijcuk V, 2019, ADV MATER TECHNOL-US, V4, DOI 10.1002/admt.201800345
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Pedroni BU, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00357
NR 6
TC 0
Z9 0
U1 0
U2 2
PY 2020
BP 322
EP 323
DI 10.1109/ISOCC50952.2020.9332910
WC Computer Science, Hardware & Architecture; Engineering, Electrical &
   Electronic
DA 2023-11-11
ER

PT J
AU Masquelier, T
AF Masquelier, Timothee
TI Back-propagation Now Works in Spiking Neural Networks!
SO ERCIM NEWS
DT Article
AB Back-propagation is THE learning algorithm behind the deep learning revolution. Until recently, it was not possible to use it in spiking neural networks (SNN), due to non-differentiability issues. But these issues can now be circumvented, signalling a new era for SNNs.
C1 [Masquelier, Timothee] Univ Toulouse 3, UMR5549, CNRS, Ctr Rech Cerveau & Cognit, Toulouse, France.
RP Masquelier, T (corresponding author), Univ Toulouse 3, UMR5549, CNRS, Ctr Rech Cerveau & Cognit, Toulouse, France.
EM timothee.masquelier@cnrs.fr
CR Kheradpisheh SR, 2020, INT J NEURAL SYST, V30, DOI 10.1142/S0129065720500276
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Pellegrini T, 2021, IEEE W SP LANG TECH, P97, DOI 10.1109/SLT48900.2021.9383587
   Wozniak S, 2020, NAT MACH INTELL, V2, P325, DOI 10.1038/s42256-020-0187-0
NR 4
TC 0
Z9 0
U1 0
U2 0
PD APR
PY 2021
IS 125
SI SI
BP 11
EP 12
WC Computer Science, Interdisciplinary Applications
DA 2023-11-11
ER

PT C
AU Traub, M
   Legenstein, R
   Otte, S
AF Traub, Manuel
   Legenstein, Robert
   Otte, Sebastian
GP IEEE
TI Many-Joint Robot Arm Control with Recurrent Spiking Neural Networks
SO 2021 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS
   (IROS)
SE IEEE International Conference on Intelligent Robots and Systems
DT Proceedings Paper
CT IEEE/RSJ International Conference on Intelligent Robots and Systems
   (IROS)
CY SEP 27-OCT 01, 2021
CL ELECTR NETWORK
ID KINEMATICS
AB In the paper, we show how scalable, low-cost trunk-like robotic arms can be constructed using only basic 3D-printing equipment and simple electronics. The design is based on uniform, stackable joint modules with three degrees of freedom each. Moreover, we present an approach for controlling these robots with recurrent spiking neural networks. At first, a spiking forward model learns motor-pose correlations from movement observations. After training, intentions can be projected back through unrolled spike trains of the forward model essentially routing the intention-driven motor gradients towards the respective joints, which unfolds goal-direction navigation. We demonstrate that spiking neural networks can thus effectively control trunk-like robotic arms with up to 75 articulated degrees of freedom with near millimeter accuracy.
C1 [Traub, Manuel; Otte, Sebastian] Univ Tubingen, Neurocognit Modeling Grp, Comp Sci Dept, Sand 14, D-72076 Tubingen, Germany.
   [Legenstein, Robert] Graz Univ Technol, Fac Comp Sci & Biomed Engn, Inffeldgasse 16b, A-8010 Graz, Austria.
RP Traub, M (corresponding author), Univ Tubingen, Neurocognit Modeling Grp, Comp Sci Dept, Sand 14, D-72076 Tubingen, Germany.
EM manuel.traub@uni-tuebingen.de; robert.legenstein@igi.tugraz.at;
   sebastian.otte@uni-tuebingen.de
CR [Anonymous], 1997, NEURAL COMPUT
   Bartow A., 2013, P 12 WSEAS INT C SIG, P181
   Bayani S, 2015, IEEE ASME INT C ADV, P1271, DOI 10.1109/AIM.2015.7222713
   Bellec G., 2018, ADV NEURAL INFORM PR, P787
   Bellec G, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-17236-y
   Bing ZS, 2018, FRONT NEUROROBOTICS, V12, DOI 10.3389/fnbot.2018.00035
   Bouganis A., 2010, P 2010 INT JOINT C N, P1, DOI DOI 10.1109/IJCNN.2010.5596525
   Crespi A, 2005, ROBOT AUTON SYST, V50, P163, DOI 10.1016/j.robot.2004.09.015
   Hannan MW, 2003, J ROBOTIC SYST, V20, P45, DOI 10.1002/rob.10070
   Kingma D. P., 2014, ARXIV, DOI DOI 10.48550/ARXIV.1412.6980
   Kintel M., 2009, OPENSCAD PROGRAMMERS
   Maass W, 1998, PULSED NEURAL NETWORKS, P55
   Oja M, 2003, NEURAL COMPUTING SUR, V3, P1
   Otte S, 2018, LECT NOTES COMPUT SC, V11141, P748, DOI 10.1007/978-3-030-01424-7_73
   Otte S, 2017, LECT NOTES COMPUT SC, V10613, P262, DOI 10.1007/978-3-319-68600-4_31
   Otte S, 2016, LECT NOTES COMPUT SC, V9886, P149, DOI 10.1007/978-3-319-44778-0_18
   Park HY, 2020, ADV MATER, V32, DOI 10.1002/adma.202002120
   Reddi S J, 2019, ARXIV190409237
   Rolf M, 2014, IEEE T NEUR NET LEAR, V25, P1147, DOI 10.1109/TNNLS.2013.2287890
   Roy K, 2019, NATURE, V575, P607, DOI 10.1038/s41586-019-1677-2
   Schumann CL, 2019, AIDS BEHAV, V23, P5, DOI 10.1007/s10461-017-1727-4
   Transeth AA, 2009, ROBOTICA, V27, P999, DOI 10.1017/S0263574709005414
NR 22
TC 0
Z9 0
U1 0
U2 0
PY 2021
BP 4918
EP 4925
DI 10.1109/IROS51168.2021.9636001
WC Automation & Control Systems; Computer Science, Artificial Intelligence;
   Engineering, Electrical & Electronic; Robotics
DA 2023-11-11
ER

PT J
AU He, H
   Shang, YJ
   Yang, X
   Di, YZ
   Lin, JJ
   Zhu, YM
   Zheng, WH
   Zhao, JF
   Ji, MY
   Dong, LY
   Deng, N
   Lei, YL
   Chai, ZH
AF He, Hu
   Shang, Yingjie
   Yang, Xu
   Di, Yingze
   Lin, Jiajun
   Zhu, Yimeng
   Zheng, Wenhao
   Zhao, Jinfeng
   Ji, Mengyao
   Dong, Liya
   Deng, Ning
   Lei, Yunlin
   Chai, Zenghao
TI Constructing an Associative Memory System Using Spiking Neural Network
SO FRONTIERS IN NEUROSCIENCE
DT Article
DE spiking neural network; artificial intelligence; associative memory
   system; Hebb's rule; STDP
AB Development of computer science has led to the blooming of artificial intelligence (AI), and neural networks are the core of AI research. Although mainstream neural networks have done well in the fields of image processing and speech recognition, they do not perform well in models aimed at understanding contextual information. In our opinion, the reason for this is that the essence of building a neural network through parameter training is to fit the data to the statistical law through parameter training. Since the neural network built using this approach does not possess memory ability, it cannot reflect the relationship between data with respect to the causality. Biological memory is fundamentally different from the current mainstream digital memory in terms of the storage method. The information stored in digital memory is converted to binary code and written in separate storage units. This physical isolation destroys the correlation of information. Therefore, the information stored in digital memory does not have the recall or association functions of biological memory which can present causality. In this paper, we present the results of our preliminary effort at constructing an associative memory system based on a spiking neural network. We broke the neural network building process into two phases: the Structure Formation Phase and the Parameter Training Phase. The Structure Formation Phase applies a learning method based on Hebb's rule to provoke neurons in the memory layer growing new synapses to connect to neighbor neurons as a response to the specific input spiking sequences fed to the neural network. The aim of this phase is to train the neural network to memorize the specific input spiking sequences. During the Parameter Training Phase, STDP and reinforcement learning are employed to optimize the weight of synapses and thus to find a way to let the neural network recall the memorized specific input spiking sequences. The results show that our memory neural network could memorize different targets and could recall the images it had memorized.
C1 [He, Hu; Shang, Yingjie; Dong, Liya; Deng, Ning] Tsinghua Univ, Inst Microelect, Beijing, Peoples R China.
   [Yang, Xu; Di, Yingze; Lin, Jiajun; Zhu, Yimeng; Zheng, Wenhao; Zhao, Jinfeng; Ji, Mengyao; Lei, Yunlin; Chai, Zenghao] Beijing Inst Technol, Sch Comp Sci & Technol, Beijing, Peoples R China.
RP Yang, X (corresponding author), Beijing Inst Technol, Sch Comp Sci & Technol, Beijing, Peoples R China.
EM yangxu@tsinghua.edu.cn
CR [Anonymous], 1996, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.1996.517075
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bohte S. M., 2000, TECHNICAL REPORT
   Egmont-Petersen M, 2002, PATTERN RECOGN, V35, P2279, DOI 10.1016/S0031-3203(01)00178-9
   Fiesler E., 1994, INT C ART NEUR NETW, P26
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   He H, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0212368
   Hebb D. O., 1988, NEUROCOMPUTING FDN R
   HOPFIELD JJ, 1988, IEEE CIRCUIT DEVIC, V4, P3, DOI 10.1109/101.8118
   Indiveri G., 2003, CIRC SYST 2003 P 200, V4, P4, DOI DOI 10.1109/ISCAS.2003.1206342
   Jennings N.R., 2012, FDN MACHINE LEARNING
   Kasabov N, 2013, NEURAL NETWORKS, V41, P188, DOI 10.1016/j.neunet.2012.11.014
   Kasabov NK, 2017, IEEE T NEUR NET LEAR, V28, P887, DOI 10.1109/TNNLS.2016.2612890
   Kasabov NK, 2014, NEURAL NETWORKS, V52, P62, DOI 10.1016/j.neunet.2014.01.006
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lecun Y., 2010, MNIST DATABASE HANDW
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Perez-Uribe A., 1999, STRUCTURE ADAPTABLE
   Plesser H. E., 2015, NEST NEURAL SIMULATI
   Quinlan PT, 1998, NEURAL NETWORKS, V11, P577, DOI 10.1016/S0893-6080(98)00033-1
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   SMIEJA FJ, 1993, CIRC SYST SIGNAL PR, V12, P331, DOI 10.1007/BF01189880
   Thorpe S, 1998, COMPUTATIONAL NEUROSCIENCE: TRENDS IN RESEARCH, P113
   Wade JJ, 2010, IEEE T NEURAL NETWOR, V21, P1817, DOI 10.1109/TNN.2010.2074212
   Zaknich A, 1998, IEEE T SIGNAL PROCES, V46, P1980, DOI 10.1109/78.700969
NR 25
TC 12
Z9 12
U1 1
U2 10
PD JUL 3
PY 2019
VL 13
AR 650
DI 10.3389/fnins.2019.00650
WC Neurosciences
DA 2023-11-11
ER

PT C
AU Zhao, XM
   Lin, XH
   Zhang, Z
AF Zhao, Xiaoman
   Lin, Xianghong
   Zhang, Zhen
BE Paul, R
TI A Semi-Supervised Multi-Spike Learning Algorithm for Deep Spiking Neural
   Networks
SO 2023 IEEE 13TH ANNUAL COMPUTING AND COMMUNICATION WORKSHOP AND
   CONFERENCE, CCWC
DT Proceedings Paper
CT IEEE 13th Annual Computing and Communication Workshop and Conference
   (CCWC)
CY MAR 08-11, 2023
CL ELECTR NETWORK
DE deep spiking neural network; semi-supervised learning; STDP; broadcast
   alignment; pattern classification
AB The deep spiking neural network (DSNN) model contains a mass of parameters, a high-performance deep model depends on a huge quantity of labeled data for solving specific problems, but collecting these labeled data is time-consuming and costly. Semi-supervised learning methods can overcome these difficulties by the unlabeled and labeled data. This paper proposes a semi-supervised multi- spike learning algorithm for DSNNs, in which the unsupervised learning rule based on spike timing-dependent plasticity is applied to adjust the synaptic weights through the unlabeled data, and the supervised learning rule based on broadcast alignment mechanism is applied to update the network weights through the labeled data. Applying spike train to encode image data, the proposed algorithm is validated on the MNIST digital image benchmark dataset. Compared with supervised learning using the labeled data, experiments indicate that the comparable classification accuracy can be achieved by the proposed semi-supervised method in DSNNs.
C1 [Zhao, Xiaoman; Lin, Xianghong; Zhang, Zhen] Northwest Normal Univ, Coll Comp Sci & Engn, Lanzhou, Peoples R China.
RP Zhao, XM (corresponding author), Northwest Normal Univ, Coll Comp Sci & Engn, Lanzhou, Peoples R China.
EM 2021212120@nwnu.edu.cn; linxh@nwnu.edu.cn; 2020211974@nwnu.edu.cn
CR Barber MJ, 2009, PHYS REV E, V80, DOI 10.1103/PhysRevE.80.026129
   Belkin M, 2006, J MACH LEARN RES, V7, P2399
   Bengio Y, 2016, Arxiv, DOI arXiv:1502.04156
   Bengio Y, 2017, NEURAL COMPUT, V29, P555, DOI 10.1162/NECO_a_00934
   Bohte Sander M., 2004, Natural Computing, V3, P195, DOI 10.1023/B:NACO.0000027755.02868.60
   Fatahi M, 2016, 2016 6TH INTERNATIONAL CONFERENCE ON COMPUTER AND KNOWLEDGE ENGINEERING (ICCKE), P153, DOI 10.1109/ICCKE.2016.7802132
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   GROSSBERG S, 1987, COGNITIVE SCI, V11, P23, DOI 10.1111/j.1551-6708.1987.tb00862.x
   Kang C, 2006, P 19 INT FLORIDA ART
   Lai JL, 2022, INFORM SCIENCES, V609, P465, DOI 10.1016/j.ins.2022.07.102
   Lee C, 2019, IEEE T COGN DEV SYST, V11, P384, DOI 10.1109/TCDS.2018.2833071
   Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508
   Lillicrap TP, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms13276
   Lin X, 2018, SPIKING NEURAL NETWO
   Liu D, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351591
   Merz C J, 1992, INT JOINT C NEURAL N
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   OConnor P., 2016, DEEP SPIKING NETWORK
   Samadi A, 2017, NEURAL COMPUT, V29, P578, DOI 10.1162/NECO_a_00929
   Thiele J C, 2019, U.S. Patent Application, Patent No. [16/196,515, 16196515]
   Wang YX, 2021, IEEE T COGN DEV SYST, V13, P514, DOI 10.1109/TCDS.2020.2971655
   [王桢文 Wang Zhenwen], 2013, [计算机研究与发展, Journal of Computer Research and Development], V50, P2642
   Wu J N, 2013, CHINESE J BIOMEDICAL, V32, P588
   Zhu X., 2002, LEARNING LABELS UNLA, V3175, P237, DOI [10.1007/978-3-540-28649-3_29, DOI 10.1007/978-3-540-28649-3_29]
NR 24
TC 0
Z9 0
U1 1
U2 1
PY 2023
BP 976
EP 982
DI 10.1109/CCWC57344.2023.10099067
WC Computer Science, Artificial Intelligence; Computer Science,
   Interdisciplinary Applications; Telecommunications
DA 2023-11-11
ER

PT J
AU Afifi, A
   Ayatollahi, A
   Raissi, F
AF Afifi, Ahmad
   Ayatollahi, Ahmad
   Raissi, Farshid
TI CMOL implementation of spiking neurons and spike-timing dependent
   plasticity
SO INTERNATIONAL JOURNAL OF CIRCUIT THEORY AND APPLICATIONS
DT Article
DE nanoelectronics; CMOL; spiking neurons; neuromorphic networks; STDP
   learning
ID SYNAPTIC PLASTICITY; STDP; CIRCUITS; ARCHITECTURES; SYNAPSES; NETWORKS;
   MODELS; POWER; CNN
AB Successful implementation of spiking neural networks onto CMOS-Molecular (CMOL) architecture has already been proposed, but the ability of dynamic learning has not yet been addressed. Here, we propose a spiking neural topology with spike-timing-dependent learning ability and provide its basic building blocks that are easily mapped onto CMOL architecture. The learning method modifies state of synaptic switches, using spatially and temporally local information which is available at the synapse when state modification is performed.
   The performance of the proposed topology is analyzed with regards to pre- and post-synaptic spike timing, and simulation results are provided for a synapse with spike-timing-dependent plasticity properties. Furthermore, its performance as spike-timing correlation learning and synchrony detection in a small feed-forward network is demonstrated as a case example. Copyright (C) 2010 John Wiley & Sons, Ltd.
C1 [Afifi, Ahmad; Ayatollahi, Ahmad] Iran Univ Sci & Technol, EE Dept, Tehran, Iran.
   [Raissi, Farshid] KN Toosi Univ Technol, ECE Dept, Tehran, Iran.
RP Afifi, A (corresponding author), Iran Univ Sci & Technol, EE Dept, Tehran, Iran.
EM ah_afifi@iust.ac.ir
CR Afifi A, 2009, IEICE ELECTRON EXPR, V6, P148, DOI 10.1587/elex.6.148
   [Anonymous], 2005, ACM J EMERG TECH COM, DOI DOI 10.1145/1084748.1084750
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Chen A, 2005, INT EL DEVICES MEET, P765
   Chicca E, 2003, IEEE T NEURAL NETWOR, V14, P1297, DOI 10.1109/TNN.2003.816367
   Csurgay AI, 2007, INT J CIRC THEOR APP, V35, P471, DOI 10.1002/cta.444
   Debanne D, 1998, J PHYSIOL-LONDON, V507, P237, DOI 10.1111/j.1469-7793.1998.237bu.x
   Gao CJ, 2007, IEEE T CIRCUITS-I, V54, P2502, DOI 10.1109/TCSI.2007.907830
   Gerstner W, 1998, PULSED NEURAL NETWORKS, P353
   Gerstner W., 2002, SPIKING NEURON MODEL
   Gorchetchnikov A, 2005, NEURAL NETWORKS, V18, P458, DOI 10.1016/j.neunet.2005.06.019
   Grossberg S., 1974, Progress theor Biol, V3, P51
   HEBB DO, 1988, ORG BEHAV, P60
   Hoekstra J, 2007, INT J CIRC THEOR APP, V35, P213, DOI 10.1002/cta.412
   Indiveri G, 2006, IEEE T NEURAL NETWOR, V17, P211, DOI 10.1109/TNN.2005.860850
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, NEURAL COMPUT, V15, P1511, DOI 10.1162/089976603321891783
   Kistler WM, 2000, NEURAL COMPUT, V12, P385, DOI 10.1162/089976600300015844
   LEE JH, 2005, IWANN 2005 SPAIN, P446
   Lee JH, 2007, INT J CIRC THEOR APP, V35, P239, DOI 10.1002/cta.410
   Likharev KK, 2005, LECT NOTES PHYS, V680, P447
   Maass W, 2004, J COMPUT SYST SCI, V69, P593, DOI 10.1016/j jcss.2004.04.001
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   Masoumi M, 2006, NANOTECHNOLOGY, V17, P89, DOI 10.1088/0957-4484/17/1/015
   Mozsáry A, 2007, INT J CIRC THEOR APP, V35, P149, DOI 10.1002/cta.385
   Rák A, 2009, INT J CIRC THEOR APP, V37, P587, DOI 10.1002/cta.569
   Ravinuthula V, 2009, INT J CIRC THEOR APP, V37, P631, DOI 10.1002/cta.488
   Sasaki K, 2006, IEICE T ELECTRON, VE89C, P1637, DOI 10.1093/ietele/e89-c.11.1637
   Saudargiene A, 2004, NEURAL COMPUT, V16, P595, DOI 10.1162/089976604772744929
   Snider GS, 2007, NANOTECHNOLOGY, V18, DOI 10.1088/0957-4484/18/36/365202
   Snider GS, 2008, 2008 IEEE INTERNATIONAL SYMPOSIUM ON NANOSCALE ARCHITECTURES, P85, DOI 10.1109/NANOARCH.2008.4585796
   Snider GS, 2007, NANOTECHNOLOGY, V18, DOI 10.1088/0957-4484/18/3/035204
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Strukov DB, 2005, NANOTECHNOLOGY, V16, P888, DOI 10.1088/0957-4484/16/6/045
   Strukov DB, 2005, NANOTECHNOLOGY, V16, P137, DOI 10.1088/0957-4484/16/1/028
   Türel Ö, 2004, INT J CIRC THEOR APP, V32, P277, DOI 10.1002/cta.282
   Türel Ö, 2003, INT J CIRC THEOR APP, V31, P37, DOI 10.1002/cta.223
   VOGELSTEIN RJ, 2003, P NIPS 03, P15
   Wagner R, 2009, INT J CIRC THEOR APP, V37, P87, DOI 10.1002/cta.498
   ZHANG W, 2008, 8 IEEE C NAN NANO 08, P737
NR 41
TC 9
Z9 10
U1 0
U2 15
PD APR
PY 2011
VL 39
IS 4
BP 357
EP 372
DI 10.1002/cta.638
WC Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Sun, CY
   Chen, QY
   Chen, K
   He, GQ
   Fu, YX
   Li, L
AF Sun, Congyi
   Chen, Qinyu
   Chen, Kai
   He, Guoqiang
   Fu, Yuxiang
   Li, Li
GP IEEE
TI Unsupervised Learning Based on Temporal Coding Using STDP in Spiking
   Neural Networks
SO 2022 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS (ISCAS 22)
SE IEEE International Symposium on Circuits and Systems
DT Proceedings Paper
CT IEEE International Symposium on Circuits and Systems (ISCAS)
CY MAY 28-JUN 01, 2022
CL Austin, TX
DE Temporal coding; Spiking neural network; Spiking-timing dependent
   plasticity
AB Spiking Neural Networks (SNNs) have been recognized as one of the next generation of Neural Networks (NNs), showing a great potential in a variety of applications. SpikingTiming Dependent Plasticity (STDP) underlies the brain's learning mechanisms, and trains SNNs with great energy efficiency. In this paper, we propose a low-cost spike-time based unsupervised learning method. It constructs a SNN with one fully-connected excitatory layer structure without inhibitory layer, and trains the SNN with STDP using a first-spike-based temporal coding scheme where input information is directly encoded into spike times. It only updates the synaptic weights connected to the neuron that first generates a spike in a forward propagation step, which reduces the frequency of the synaptic weight updates significantly. The forward propagation process can be stopped once a neuron fires whether in the training mode or the inference mode, by which many unnecessary computations are just avoided and the latency in the inference mode is reduced. The method was used to train on the classification task on MNIST dataset and achieved an accuracy of 90.4% with 800 excitatory neurons.
C1 [Sun, Congyi; Chen, Kai; He, Guoqiang; Fu, Yuxiang; Li, Li] Nanjing Univ, Sch Elect Sci & Engn, Nanjing, Peoples R China.
   [Chen, Qinyu] Univ Shanghai Sci & Technol, Inst Photon Chips, Shanghai, Peoples R China.
RP Fu, YX; Li, L (corresponding author), Nanjing Univ, Sch Elect Sci & Engn, Nanjing, Peoples R China.
EM yuxiangfu@nju.edu.cn; lili@nju.edu.cn
CR [Anonymous], 2016, ARXIV161101421
   Bear Mark F., 1994, Current Opinion in Neurobiology, V4, P389, DOI 10.1016/0959-4388(94)90101-5
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Ferr<prime>e P., 2018, FRONTIERS COMPUTATIO
   Gerstner W, 1996, NATURE, V383, P76, DOI 10.1038/383076a0
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee C, 2019, IEEE T COGN DEV SYST, V11, P384, DOI 10.1109/TCDS.2018.2833071
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   Mostafa H, 2018, IEEE T NEUR NET LEAR, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Rajendran B, 2013, IEEE T ELECTRON DEV, V60, P246, DOI 10.1109/TED.2012.2227969
   Sjöström PJ, 2001, NEURON, V32, P1149, DOI 10.1016/s0896-6273(01)00542-6
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Wang Q, 2017, NEUROCOMPUTING, V221, P146, DOI 10.1016/j.neucom.2016.09.071
   Zhang W, 2003, NAT REV NEUROSCI, V4, P885, DOI 10.1038/nrn1248
NR 16
TC 0
Z9 0
U1 1
U2 1
PY 2022
BP 2142
EP 2146
DI 10.1109/ISCAS48785.2022.9937812
WC Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Kiselev, M
AF Kiselev, Mikhail
GP IEEE
TI Rate Coding vs. Temporal Coding - Is Optimum Between?
SO 2016 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUL 24-29, 2016
CL Vancouver, CANADA
DE spiking neural network; polychronization; STDP; rate coding; temporal
   coding; chaotic network; network self-organization
ID SPIKING NEURAL-NETWORKS; CODE
AB In this paper, we consider a novel approach to information representation in spiking neural networks. In a certain sense it is a combination of two well-known coding schemes - rate coding and temporal coding. Namely, it is based on asynchronous activity of ensembles of polychronous neuronal groups - groups of neurons firing in determined order with strictly fixed relative temporal delays. We demonstrate how a rate-coded input signal may be converted into this representation form by network structures which are formed as a result of network self-organization process based on STDP-style synaptic plasticity.
C1 [Kiselev, Mikhail] Chuvash State Univ, Dept Math & Informat Technol, Cheboxary, Russia.
RP Kiselev, M (corresponding author), Chuvash State Univ, Dept Math & Informat Technol, Cheboxary, Russia.
EM mkiselev@megaputer.ru
CR Brette R, 2015, FRONT SYST NEUROSCI, V9, DOI 10.3389/fnsys.2015.00151
   Butts DA, 2007, NATURE, V449, P92, DOI [10.1038/nature06105, 10.1038/natureO6105]
   Debanne D, 2013, NAT REV NEUROSCI, V14, P63, DOI 10.1038/nrn3361
   Gardner B, 2015, NEURAL COMPUT, V27, P2548, DOI 10.1162/NECO_a_00790
   Gerstner W., 2002, SPIKING NEURON MODEL
   Guise M, 2014, NEURAL COMPUT, V26, P2052, DOI 10.1162/NECO_a_00620
   Ikegaya Y, 2004, SCIENCE, V304, P559, DOI 10.1126/science.1093173
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Kiselev M. V., 2014, P C NEUR COMP THEOR, P264
   Kiselev M, 2013, LECT NOTES COMPUT SC, V7902, P510, DOI 10.1007/978-3-642-38679-4_51
   Kiselev MV, 2014, COMPUT INTEL NEUROSC, V2014, DOI 10.1155/2014/476580
   Mehta MR, 2002, NATURE, V417, P741, DOI 10.1038/nature00807
   Szatmáry B, 2010, PLOS COMPUT BIOL, V6, DOI 10.1371/journal.pcbi.1000879
NR 13
TC 15
Z9 17
U1 0
U2 6
PY 2016
BP 1355
EP 1359
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Lehmann, HM
   Hille, J
   Grassmann, C
   Issakov, V
AF Lehmann, Hendrik M.
   Hille, Julian
   Grassmann, Cyprian
   Issakov, Vadim
TI Direct Signal Encoding With Analog Resonate-and-Fire Neurons
SO IEEE ACCESS
DT Article
DE Neural networks; Resonant frequency; Encoding; Sensors; Codes;
   Biological neural networks; Oscillators; Neuromorphics; Sensor; spiking
   neural networks; neuromorphic hardware; signal encoding
ID SYSTEMS
AB Sensors are an essential element in a wide range of applications. As the number of sensors increases, so does the amount of data collected with them. This raises the challenge of efficiently processing this data. Spiking Neural Networks (SNNs) represents a promising approach to solve this problem through event-based, parallelized data processing. For SNNs to be genuinely efficient, some fundamental challenges arise, like converting analog signals to spike events. An emerging possibility is the use of Resonate-and-Fire (R&F) neurons, capable of reacting to specific frequency components of input signals. In this work, we present a possible analog implementation for a R&F neuron and show the practical encoding of analog signals into a spiking domain using actual measurements. The coding method allows analog sensor signals to be directly applied to SNNs for efficient data processing. In the future, this approach can potentially enable the direct integration of analog Spiking Neural Networks into sensors.
C1 [Lehmann, Hendrik M.; Issakov, Vadim] Tech Univ Carolo Wilhelmina Braunschweig, Inst CMOS Design, D-38106 Braunschweig, Germany.
   [Lehmann, Hendrik M.; Hille, Julian; Grassmann, Cyprian; Issakov, Vadim] Infineon Technol AG, D-85579 Neubiberg, Germany.
   [Hille, Julian] Tech Univ Munich, Dept Informat, D-80333 Munich, Germany.
RP Lehmann, HM (corresponding author), Tech Univ Carolo Wilhelmina Braunschweig, Inst CMOS Design, D-38106 Braunschweig, Germany.; Lehmann, HM (corresponding author), Infineon Technol AG, D-85579 Neubiberg, Germany.
EM hendrik.lehmann@infineon.com
CR Adrian ED, 1926, J PHYSIOL-LONDON, V61, P151, DOI 10.1113/jphysiol.1926.sp002281
   Auge D., 2020, RESONATE AND FIRE NE
   Auge D, 2021, LECT NOTES COMPUT SC, V12895, P245, DOI 10.1007/978-3-030-86383-8_20
   Auge D, 2021, NEURAL PROCESS LETT, V53, P4693, DOI 10.1007/s11063-021-10562-2
   Azam MA, 2018, J APPL PHYS, V124, DOI 10.1063/1.5042308
   Birkoben T, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-74219-1
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Debat G, 2021, FRONT COMPUT NEUROSC, V15, DOI 10.3389/fncom.2021.658764
   Diniz P. S. R., 2010, DIGIT SIGNAL PROCESS
   Frenkel C, 2019, IEEE T BIOMED CIRC S, V13, P145, DOI 10.1109/TBCAS.2018.2880425
   Furber S, 2016, J NEURAL ENG, V13, DOI 10.1088/1741-2560/13/5/051001
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   GRAY CM, 1989, NATURE, V338, P334, DOI 10.1038/338334a0
   Guglielmi E, 2020, IEEE J SOLID-ST CIRC, V55, P2094, DOI 10.1109/JSSC.2020.2973639
   Hille J., 2022, P INT C NEUR SYST JU, P1
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   Izhikevich EM, 2001, NEURAL NETWORKS, V14, P883, DOI 10.1016/S0893-6080(01)00078-8
   Kasabov N. K., 2019, TIME SPACE SPIKING N, DOI DOI 10.1007/978-3-662-57715-8
   Kornijcuk V, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00212
   Kugele A, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00439
   Lehmann HM, 2022, IEEE T CIRCUITS-I, V69, P4837, DOI 10.1109/TCSI.2022.3204433
   Lehmann HM, 2022, PRIME 2022: 17TH INTERNATIONAL CONFERENCE ON PHD RESEARCH IN MICROELECTRONICS AND ELECTRONICS, P293, DOI 10.1109/PRIME55000.2022.9816777
   Lehmann HM, 2021, IEEE INT CONF MICROW, P280, DOI 10.1109/COMCAS52219.2021.9629011
   Lichtsteiner P, 2008, IEEE J SOLID-ST CIRC, V43, P566, DOI 10.1109/JSSC.2007.914337
   Liu SC, 2010, CURR OPIN NEUROBIOL, V20, P288, DOI 10.1016/j.conb.2010.03.007
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Mayr C, 2019, Arxiv, DOI arXiv:1911.02385
   Nakada K., 2005, INT S NONL THEOR ITS, P82
   Neckar A, 2019, P IEEE, V107, P144, DOI 10.1109/JPROC.2018.2881432
   Orchard G, 2021, IEEE WRK SIG PRO SYS, P254, DOI 10.1109/SiPS52927.2021.00053
   Osborn L., 2017, MYOEL CONTR S U NEW, P188
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Rozenberg MJ, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-47348-5
   Rueckauer B, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351295
   Schaumann R., 2001, DESIGN ANALOG FILTER, V1
   Schemmel J, 2020, Arxiv, DOI arXiv:2003.11996
   Schenk C., 1993, HALBLEITERSCHALTUNGS
   Shayegannia M., 2007, PROC 4 IEEE GCC C, P1
   Srivastava KH, 2017, P NATL ACAD SCI USA, V114, P1171, DOI 10.1073/pnas.1611734114
   Stuijt J, 2021, FRONT NEUROSCI-SWITZ, V15, DOI 10.3389/fnins.2021.664208
   Su KL., 2012, ANALOG FILTERS
   THEUNISSEN F, 1995, J COMPUT NEUROSCI, V2, P149, DOI 10.1007/BF00961885
   Williams A. B., 2014, ANALOG FILTER CIRCUI, V1st Edition
NR 44
TC 0
Z9 0
U1 1
U2 1
PY 2023
VL 11
BP 50052
EP 50063
DI 10.1109/ACCESS.2023.3278098
WC Computer Science, Information Systems; Engineering, Electrical &
   Electronic; Telecommunications
DA 2023-11-11
ER

PT J
AU Gütig, R
AF Guetig, Robert
TI To spike, or when to spike?
SO CURRENT OPINION IN NEUROBIOLOGY
DT Review
ID NEURAL-NETWORKS; LEARNING ALGORITHM; GRADIENT DESCENT; REINFORCEMENT;
   PLASTICITY; DEPENDENCE; NEURONS; SPARSE; CORTEX; RULE
AB Recent experimental reports have suggested that cortical networks can operate in regimes were sensory information is encoded by relatively small populations of spikes and their precise relative timing. Combined with the discovery of spike timing dependent plasticity, these findings have sparked growing interest in the capabilities of neurons to encode and decode spike timing based neural representations. To address these questions, a novel family of methodologically diverse supervised learning algorithms for spiking neuron models has been developed. These models have demonstrated the high capacity of simple neural architectures to operate also beyond the regime of the well established independent rate codes and to utilize theoretical advantages of spike timing as an additional coding dimension.
C1 Max Planck Inst Expt Med, D-37075 Gottingen, Germany.
RP Gütig, R (corresponding author), Max Planck Inst Expt Med, Hermann Rein Str 3, D-37075 Gottingen, Germany.
EM guetig@em.mpg.de
CR Albers C., 2013, ADV NEURAL INFORM PR, V26, P1709
   [Anonymous], 2010, FRONT SYNAPTIC NEURO
   BARBER D, 2003, ADV NEURAL INFORM PR, V15, P149
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Bohte SM, ERROR BACKPROPAGATIO
   Booij O, 2005, INFORM PROCESS LETT, V95, P552, DOI 10.1016/j.ipl.2005.05.023
   Brea J, 2013, J NEUROSCI, V33, P9565, DOI 10.1523/JNEUROSCI.4098-12.2013
   Dayan P, 2001, THEORETICAL NEUROSCI, P34
   Fang H, 2010, NEURAL COMPUT
   Farries MA, 2007, J NEUROPHYSIOL, V98, P3648, DOI 10.1152/jn.00364.2007
   Feldman DE, 2012, NEURON, V75, P556, DOI 10.1016/j.neuron.2012.08.001
   Florian RV, 2007, NEURAL COMPUT, V19, P1468, DOI 10.1162/neco.2007.19.6.1468
   Florian RV, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0040233
   Gerstner W., 2002, SPIKING NEURON MODEL
   Ghosh-Dastidar S, 2009, NEURAL NETW
   Gollisch T, 2008, SCIENCE, V319, P1108, DOI 10.1126/science.1149639
   Gütig R, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0053063
   Gütig R, 2009, PLOS BIOL, V7, DOI 10.1371/journal.pbio.1000141
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Gutig R, 2012, COS SALT LAK CIT US, P58
   Houweling AR, 2008, NATURE, V451, P65, DOI 10.1038/nature06447
   Huber D, 2008, NATURE, V451, P61, DOI 10.1038/nature06445
   Izhikevich EM, 2007, CEREB CORTEX, V17, P2443, DOI 10.1093/cercor/bhl152
   Legenstein R, 2005, NEURAL COMPUT, V17, P2337, DOI 10.1162/0899766054796888
   Legenstein R, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.1000180
   Malenka RC, 1999, SCIENCE, V285, P1870, DOI 10.1126/science.285.5435.1870
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   Memmesheimer RM, 2012, COS SALT LAK CIT US, P113
   Pfister JP, 2006, NEURAL COMPUT, V18, P1318, DOI 10.1162/neco.2006.18.6.1318
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Rosenblatt F, 1962, PRINCIPLES NEURODYNA, P88
   Rubin R, 2010, PHYS REV LETT, V105, DOI 10.1103/PhysRevLett.105.218102
   Seung HS, 2003, NEURON, V40, P1063, DOI 10.1016/S0896-6273(03)00761-X
   Sjöström PJ, 2006, NEURON, V51, P227, DOI 10.1016/j.neuron.2006.06.017
   Sporea I, 2013, NEURAL COMPUT, V25, P473, DOI 10.1162/NECO_a_00396
   Stüttgen MC, 2008, NAT NEUROSCI, V11, P1091, DOI 10.1038/nn.2162
   Urbanczik R, 2009, NEURAL COMPUT, V21, P340, DOI 10.1162/neco.2008.09-07-605
   VanRullen R, 2005, TRENDS NEUROSCI, V28, P1, DOI 10.1016/j.tins.2004.10.010
   Victor JD, 1996, J NEUROPHYSIOL, V76, P1310, DOI 10.1152/jn.1996.76.2.1310
   Wolfe J, 2010, CURR OPIN NEUROBIOL, V20, P306, DOI 10.1016/j.conb.2010.03.006
   Xie XH, 2004, PHYS REV E, V69, DOI 10.1103/PhysRevE.69.041909
   Xu Y, 2013, NEURAL COMPUT, V25, P1472, DOI 10.1162/NECO_a_00450
   Xu Y, 2013, NEURAL NETWORKS, V43, P99, DOI 10.1016/j.neunet.2013.02.003
NR 43
TC 63
Z9 64
U1 0
U2 28
PD APR
PY 2014
VL 25
BP 134
EP 139
DI 10.1016/j.conb.2014.01.004
WC Neurosciences
DA 2023-11-11
ER

PT C
AU Wang, YX
   Howard, N
AF Wang, Yingxu
   Howard, Newton
GP IEEE
TI The Spike Frequency Modulation (SFM) Theory for Neuroinformatics and
   Cognitive Cybernetics
SO 2019 IEEE INTERNATIONAL CONFERENCE ON SYSTEMS, MAN AND CYBERNETICS (SMC)
SE IEEE International Conference on Systems Man and Cybernetics Conference
   Proceedings
DT Proceedings Paper
CT IEEE International Conference on Systems, Man and Cybernetics (SMC)
CY OCT 06-09, 2019
CL Bari, ITALY
DE Neuroinformatics; cognitive cybernetics; neural signaling systems; spike
   frequency modulation; cognitive informatics; mathematical models; neural
   networks; cognitive systems; brain-machine interfaces
ID MODELS
AB One of the fundamental problems in neurology and neuroinformatics is whether the neural signals in human nerves systems are digital or analogue. This paper presents a novel neural signaling theory of Spike Frequency Modulation (SFM), which explains the nature of neural signals and their transformation in the nervous systems of the brain. Mathematical models of the unified signals of neural spikes across the sensory, associate and motor neurons are formally described. The time-divided mechanism for neural signal transmission and the space-divided mechanism for neural semantic representation in human nervous systems are rigorously explained. A set of experimental simulations demonstrates the SFM theory and the cognitive mechanisms of the neural pathways and networks. The SFM theory reveals the neurological and cognitive foundations of both natural and artificial neural networks for brain-inspired systems and engineering applications.
C1 [Wang, Yingxu] Univ Calgary, Schulich Sch Engn, Dept Elect & Comp Engn, Int Inst Cognit Informat & Cognit Comp, Calgary, AB, Canada.
   [Wang, Yingxu] Univ Calgary, Hotchkiss Brain Inst, Calgary, AB, Canada.
   [Howard, Newton] Univ Oxford, Computat Neurosci Lab, Oxford, England.
RP Wang, YX (corresponding author), Univ Calgary, Schulich Sch Engn, Dept Elect & Comp Engn, Int Inst Cognit Informat & Cognit Comp, Calgary, AB, Canada.; Wang, YX (corresponding author), Univ Calgary, Hotchkiss Brain Inst, Calgary, AB, Canada.
EM yingxu@ucalgary.ca; newton.howard@nds.ox.ac
CR Ascoli GA, 2003, NEUROINFORMATICS, V1, P1, DOI 10.1385/NI:1:1:001
   Beltrame F, 1999, IEEE Trans Inf Technol Biomed, V3, P239, DOI 10.1109/4233.788587
   Carter R., 2009, THE HUMAN BRAIN
   Chee-Ruiter CWJ, 1998, COMPUTATIONAL NEUROSCIENCE: TRENDS IN RESEARCH, P591
   Dayan P., 2005, THEORETICAL NEUROSCI
   Jirsa VK, 2004, NEUROINFORMATICS, V2, P183, DOI 10.1385/NI:2:2:183
   Lepora NF, 2013, BIOINSPIR BIOMIM, V8, DOI 10.1088/1748-3182/8/1/013001
   Marieb EN., 2017, HUMAN ANATOMY
   Sternberg R. J., 1998, SEARCH HUMAN MIND
   Wang Y., 2019, IEEE SYSTEM MAN CYBE, V5
   Wang Y., 2003, BRAIN MIND, V4, P151, DOI DOI 10.1023/A:1025401527570
   Wang YX, 2012, J ADV MATH APPL, V1, P206, DOI 10.1166/jama.2012.1015
   Wang YX, 2011, INT J COGN INFORM NA, V5, P75, DOI 10.4018/jcini.2011010105
   Wang YX, 2006, IEEE T SYST MAN CY C, V36, P203, DOI 10.1109/TSMCC.2006.871151
   WIENER N., 1961, CYBERNETICS CONTROL
   Wilson R. A., 2001, MIT ENCY COGNITIVE S
NR 16
TC 1
Z9 1
U1 0
U2 0
PY 2019
BP 2220
EP 2224
WC Computer Science, Cybernetics; Computer Science, Information Systems
DA 2023-11-11
ER

PT J
AU Amirshahi, A
   Hashemi, M
AF Amirshahi, Alireza
   Hashemi, Matin
TI ECG Classification Algorithm Based on STDP and R-STDP Neural Networks
   for Real-Time Monitoring on Ultra Low-Power Personal Wearable Devices
SO IEEE TRANSACTIONS ON BIOMEDICAL CIRCUITS AND SYSTEMS
DT Article
DE Electrocardiography; Neurons; Biological neural networks; Feature
   extraction; Real-time systems; Energy consumption; Monitoring; Cardiac
   monitoring; Electrocardiogram (ECG) classification; embedded real-time
   systems; low power consumption; machine learning; spiking neural network
   (SNN); wearable devices
ID FEATURES; SYSTEM
AB This paper presents a novel ECG classification algorithm for inclusion as part of real-time cardiac monitoring systems in ultra low-power wearable devices. The proposed solution is based on spiking neural networks which are the third generation of neural networks. In specific, we employ spike-timing dependent plasticity (STDP), and reward-modulated STDP (R-STDP), in which the model weights are trained according to the timings of spike signals, and reward or punishment signals. Experiments show that the proposed solution is suitable for real-time operation, achieves comparable accuracy with respect to previous methods, and more importantly, its energy consumption in real-time classification of ECG signals is significantly smaller. In specific, energy consumption is 1.78$\boldsymbol {\mu }$J per beat, which is 2 to 9 orders of magnitude smaller than previous neural network based ECG classification methods.
C1 [Amirshahi, Alireza; Hashemi, Matin] Sharif Univ Technol, Learning & Intelligent Syst Lab, Dept Elect Engn, Tehran 11356, Iran.
RP Hashemi, M (corresponding author), Sharif Univ Technol, Learning & Intelligent Syst Lab, Dept Elect Engn, Tehran 11356, Iran.
EM alireza.amirshahi@ee.sharif.edu; matin@sharif.edu
CR [Anonymous], THESIS
   [Anonymous], 1997, MIT BIH ARRHYTHMIA D
   [Anonymous], INHIBITORY PLASTICIT
   [Anonymous], BRIAN 2 DOCUMENTATIO
   [Anonymous], THESIS
   [Anonymous], 2015, INT J COMPUT VISION, DOI DOI 10.1007/s11263-014-0788-3
   [Anonymous], 2013, DESIGN AND ANAL, DOI DOI 10.1115/PVP2013-97083
   [Anonymous], 1987, AAMI REC PRACT TEST
   [Anonymous], P ACM SIGPLAN NOT
   [Anonymous], 2019, IEEE J BIOMED HEALTH
   [Anonymous], THEORETICAL NEUROSCI
   [Anonymous], IEEE T BIOMED CIRCUI
   [Anonymous], IEEE T BIOMED CIRCUI
   Bayasi N, 2015, IEEE INT SYMP CIRC S, P746, DOI 10.1109/ISCAS.2015.7168741
   Biagetti G, 2016, IEEE T CONSUM ELECTR, V62, P258, DOI 10.1109/TCE.2016.7613192
   Bing ZS, 2018, FRONT NEUROROBOTICS, V12, DOI 10.3389/fnbot.2018.00035
   Bote JM, 2018, IEEE J BIOMED HEALTH, V22, P429, DOI 10.1109/JBHI.2017.2671443
   Chou CY, 2018, IEEE T BIOMED CIRC S, V12, P801, DOI 10.1109/TBCAS.2018.2828031
   Crippa P, 2015, INT J SIMULAT SYST S, V16, P1
   Da He D, 2015, IEEE T BIOMED CIRC S, V9, P370, DOI 10.1109/TBCAS.2014.2346761
   Das A, 2018, NEURAL NETWORKS, V99, P134, DOI 10.1016/j.neunet.2017.12.015
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Foroozannejad MH, 2014, IEEE T COMPUT AID D, V33, P752, DOI 10.1109/TCAD.2014.2299958
   Foroozannejad MH, 2012, ACM T DES AUTOMAT EL, V17, DOI 10.1145/2209291.2209300
   Frémaux N, 2016, FRONT NEURAL CIRCUIT, V9, DOI 10.3389/fncir.2015.00085
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   Hashemi M, 2013, ACM T EMBED COMPUT S, V13, DOI 10.1145/2539036.2539042
   Hoekema R, 2001, IEEE T BIO-MED ENG, V48, P551, DOI 10.1109/10.918594
   Hu YH, 1997, IEEE T BIO-MED ENG, V44, P891, DOI 10.1109/10.623058
   Ince T, 2009, IEEE T BIO-MED ENG, V56, P1415, DOI 10.1109/TBME.2009.2013934
   Indiveri G., 2015, P EL DEV M IEDM 2015, p4.2.1, DOI [DOI 10.1109/IEDM.2015.7409623, 10.1109/iedm.2015.7409623]
   Jain SK, 2017, IEEE T BIOMED CIRC S, V11, P314, DOI 10.1109/TBCAS.2016.2592382
   Kachuee M., 2018, P IEEE INT C HEALTHC, P443
   Kasabov N, 2015, INFORM SCIENCES, V294, P565, DOI 10.1016/j.ins.2014.06.028
   Khayat M, 2017, STEEL COMPOS STRUCT, V23, P1, DOI 10.12989/scs.2017.23.1.001
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Kiranyaz S, 2016, IEEE T BIO-MED ENG, V63, P664, DOI 10.1109/TBME.2015.2468589
   Lee SY, 2015, IEEE J BIOMED HEALTH, V19, P236, DOI 10.1109/JBHI.2014.2310354
   Li PF, 2017, IEEE T BIO-MED ENG, V64, P78, DOI 10.1109/TBME.2016.2539421
   Masquelier T, 2007, PLOS COMPUT BIOL, V3, P247, DOI 10.1371/journal.pcbi.0030031
   Meister M, 1999, NEURON, V22, P435, DOI 10.1016/S0896-6273(00)80700-X
   Moody GA, 2001, IEEE ENG MED BIOL, V20, P45, DOI 10.1109/51.932724
   Moradi S, 2014, IEEE T BIOMED CIRC S, V8, P98, DOI 10.1109/TBCAS.2013.2255873
   Mozafari M, 2018, IEEE T NEUR NET LEAR, V29, P6178, DOI 10.1109/TNNLS.2018.2826721
   Qiao N, 2017, IEEE SOI3DSUB MICRO
   Qiao N, 2016, BIOMED CIRC SYST C, P552, DOI 10.1109/BioCAS.2016.7833854
   Qiao N, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00141
   Rajpurkar P., 2017, ARXIV
   Shyu LY, 2004, IEEE T BIO-MED ENG, V51, P1269, DOI 10.1109/TBME.2004.824131
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Sopic D, 2018, IEEE T BIOMED CIRC S, V12, P982, DOI 10.1109/TBCAS.2018.2848477
   Srinivasa N, 2013, FRONT COMPUT NEUROSC, V7, DOI 10.3389/fncom.2013.00010
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Tang XC, 2018, IEEE T BIOMED CIRC S, V12, P751, DOI 10.1109/TBCAS.2018.2823275
   Teijeiro T, 2018, IEEE J BIOMED HEALTH, V22, P409, DOI 10.1109/JBHI.2016.2631247
   Wang XL, 2014, IEEE J BIOMED HEALTH, V18, P739, DOI 10.1109/JBHI.2013.2286157
   Zhang WB, 2018, 2018 INTERNATIONAL CONFERENCE ON BIG DATA AND ARTIFICIAL INTELLIGENCE (BDAI 2018), P47, DOI 10.1109/BDAI.2018.8546681
NR 57
TC 53
Z9 56
U1 4
U2 32
PD DEC
PY 2019
VL 13
IS 6
BP 1483
EP 1493
DI 10.1109/TBCAS.2019.2948920
WC Engineering, Biomedical; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Kampakis, S
AF Kampakis, Stylianos
TI Investigating the computational power of spiking neurons with
   non-standard behaviors
SO NEURAL NETWORKS
DT Article
DE Computational power; Spiking; Non-standard behavior; Bursting;
   Oscillators; Rebound; Meta-learning; Rational analysis; Izhikevich
   neuron
ID NEURAL-NETWORK; REBOUND SPIKING; COUPLED NEURONS; INFORMATION; BURSTS;
   MODEL; OSCILLATION; UNIT
AB Spiking neural networks have been called the third generation of neural networks. Their main difference with respect to the previous two generations is the use of realistic neuron models. Their computational power has been well studied with respect to threshold gates and sigmoidal neurons. However, biologically realistic models of spiking neurons can produce behaviors that can be computationally relevant, but their power has not been assessed in the same way. This paper studies the computational power of neurons with different behaviors based on the previous analyses conducted by Maass and Schmitt. The studied behaviors are rebound spiking, resonance and bursting. The results of the analysis are presented. A theoretical motivation for this study is presented and a discussion is done on the possible implications of the findings for using networks of spiking neurons for performing computations. (c) 2013 Elsevier Ltd. All rights reserved.
C1 [Kampakis, Stylianos] Univ Edinburgh, Dept Informat, Edinburgh EH8 9YL, Midlothian, Scotland.
RP Kampakis, S (corresponding author), Ano Tzoumagias 1, Thessaloniki, Greece.
EM stylianos.kampakis@gmail.com
CR Abbott LF, 1999, BRAIN RES BULL, V50, P303, DOI 10.1016/S0361-9230(99)00161-6
   ANDERSON JR, 1991, BEHAV BRAIN SCI, V14, P471, DOI 10.1017/S0140525X00070801
   [Anonymous], 1972, PERCEPTRONS INTRO CO
   [Anonymous], 1997, IEEE T EVOLUTIONARY
   Balu R, 2007, J NEUROPHYSIOL, V97, P1959, DOI 10.1152/jn.01115.2006
   Bharat Rao R., 1995, Machine Learning. Proceedings of the Twelfth International Conference on Machine Learning, P471
   Bodyanskiy Y, 2008, PRO BIENN BALT EL C, P213, DOI 10.1109/BEC.2008.4657517
   Bohte S. M., 2001, IEEE T NEURAL NETWOR, V13
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Bouganis A., 2010, P 2010 INT JOINT C N, P1, DOI DOI 10.1109/IJCNN.2010.5596525
   Brazdil P., 2008, COGNITIVE TECHNOLOGI
   Buzsáki G, 2004, SCIENCE, V304, P1926, DOI 10.1126/science.1099745
   Cao JL, 2010, J NEUROSCI, V30, P16453, DOI 10.1523/JNEUROSCI.3177-10.2010
   Cybenko G., 1989, Mathematics of Control, Signals, and Systems, V2, P303, DOI 10.1007/BF02551274
   Dehaene S, 2004, CURR OPIN NEUROBIOL, V14, P218, DOI 10.1016/j.conb.2004.03.008
   Dethier J., 2011, ADV NEURAL INFORM PR
   Enomoto A, 2006, J NEUROSCI, V26, P3412, DOI 10.1523/JNEUROSCI.5274-05.2006
   Eyherabide HG, 2008, FRONT COMPUT NEUROSC, V2, DOI 10.3389/neuro.10.003.2008
   Felix RA, 2011, J NEUROSCI, V31, P12566, DOI 10.1523/JNEUROSCI.2450-11.2011
   Friston KJ, 2010, NAT REV NEUROSCI, V11, P127, DOI 10.1038/nrn2787
   Gerstner W., 2002, SPIKING NEURON MODEL
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   Goaillard JM, 2010, J NEUROSCI, V30, P4687, DOI 10.1523/JNEUROSCI.2998-09.2010
   Hagras H, 2004, IEEE INT CONF ROBOT, P4620, DOI 10.1109/ROBOT.2004.1302446
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   Hutcheon B, 2000, TRENDS NEUROSCI, V23, P216, DOI 10.1016/S0166-2236(00)01547-2
   Iannella N, 2001, NEURAL NETWORKS, V14, P933, DOI 10.1016/S0893-6080(01)00080-6
   Izhikevich E M, 2007, DYNAM SYST, DOI [DOI 10.1017/S0143385704000173, DOI 10.7551/MITPRESS/2526.001.0001]
   Izhikevich EM, 2001, NEURAL NETWORKS, V14, P883, DOI 10.1016/S0893-6080(01)00078-8
   Izhikevich EM, 2003, TRENDS NEUROSCI, V26, P161, DOI 10.1016/S0166-2236(03)00034-1
   Kampakis S., 2011, J SOFT COMPUTING
   Lavin A, 1996, J NEUROPHYSIOL, V75, P1432, DOI 10.1152/jn.1996.75.4.1432
   Li CYT, 2009, SCIENCE, V324, P643, DOI 10.1126/science.1169957
   Lisman JE, 1997, TRENDS NEUROSCI, V20, P38, DOI 10.1016/S0166-2236(96)10070-9
   LLINAS R, 1986, J PHYSIOL-LONDON, V376, P163, DOI 10.1113/jphysiol.1986.sp016147
   Ma WJ, 2006, NAT NEUROSCI, V9, P1432, DOI 10.1038/nn1790
   Maass W, 1996, NEURAL COMPUT, V8, P1, DOI 10.1162/neco.1996.8.1.1
   Maass W, 1997, NEURAL COMPUT, V9, P279, DOI 10.1162/neco.1997.9.2.279
   Maass W., 1995, Advances in Neural Information Processing Systems 7, P183
   Maass W, 1999, INFORM COMPUT, V153, P26, DOI 10.1006/inco.1999.2806
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maass W, 1997, ADV NEUR IN, V9, P211
   Maass W, 1996, ADV NEUR IN, V8, P211
   Maass W, 2001, PULSED NEURAL NETWOR
   Macready, 1995, SFITR9502010
   Marr D., 1982, VISION
   Marsat G, 2010, J COMP PHYSIOL A, V196, P315, DOI 10.1007/s00359-010-0514-8
   Masoller C, 2009, PHILOS T R SOC A, V367, P3255, DOI 10.1098/rsta.2009.0096
   MCCORMICK DA, 1992, J NEUROPHYSIOL, V68, P1384, DOI 10.1152/jn.1992.68.4.1384
   McCulloch Warren S., 1943, BULL MATH BIOPHYS, V5, P115, DOI 10.1007/BF02478259
   Meftah B, 2010, NEURAL PROCESS LETT, V32, P131, DOI 10.1007/s11063-010-9149-6
   Nieder A, 2009, ANNU REV NEUROSCI, V32, P185, DOI 10.1146/annurev.neuro.051508.135550
   Pedroarena C, 1997, P NATL ACAD SCI USA, V94, P724, DOI 10.1073/pnas.94.2.724
   Potjans W, 2009, NEURAL COMPUT, V21, P301, DOI 10.1162/neco.2008.08-07-593
   Rolls E, 2010, NOISY BRAIN
   Salinas E, 2006, NAT NEUROSCI, V9, P1349, DOI 10.1038/nn1106-1349
   Sancristóbal B, 2008, LECT NOTES COMPUT SC, V5164, P695, DOI 10.1007/978-3-540-87559-8_72
   Schmitt M, 1998, ANN MATH ARTIFICIAL, V24, P1
   Su HL, 2001, J NEUROSCI, V21, P4173, DOI 10.1523/JNEUROSCI.21-12-04173.2001
   Supèr H, 2011, J COGNITIVE NEUROSCI, V23, P491, DOI 10.1162/jocn.2010.21512
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   Wolpert D. H., 1996, NEURAL COMPUTATION, V8
NR 63
TC 3
Z9 3
U1 0
U2 19
PD JUL
PY 2013
VL 43
BP 41
EP 54
DI 10.1016/j.neunet.2013.01.011
WC Computer Science, Artificial Intelligence; Neurosciences
DA 2023-11-11
ER

PT J
AU Kim, D
   Chakraborty, B
   She, XY
   Lee, E
   Kang, B
   Mukhopadhyay, S
AF Kim, Daehyun
   Chakraborty, Biswadeep
   She, Xueyuan
   Lee, Edward
   Kang, Beomseok
   Mukhopadhyay, Saibal
TI MONETA: A Processing-In-Memory-Based Hardware Platform for the Hybrid
   Convolutional Spiking Neural Network With Online Learning
SO FRONTIERS IN NEUROSCIENCE
DT Article
DE spiking neural network (SNN); processing-in-memory (PIM); convolutional
   spiking neural network; on-line learning; on-chip learning;
   spike-time-dependent plasticity (STDP); AI accelerator; hybrid network
ID NEURONS
AB We present a processing-in-memory (PIM)-based hardware platform, referred to as MONETA, for on-chip acceleration of inference and learning in hybrid convolutional spiking neural network. MONETAuses 8T static random-access memory (SRAM)-based PIM cores for vector matrix multiplication (VMM) augmented with spike-time-dependent-plasticity (STDP) based weight update. The spiking neural network (SNN)-focused data flow is presented to minimize data movement in MONETAwhile ensuring learning accuracy. MONETAsupports on-line and on-chip training on PIM architecture. The STDP-trained convolutional neural network within SNN (ConvSNN) with the proposed data flow, 4-bit input precision, and 8-bit weight precision shows only 1.63% lower accuracy in CIFAR-10 compared to the STDP accuracy implemented by the software. Further, the proposed architecture is used to accelerate a hybrid SNN architecture that couples off-chip supervised (back propagation through time) and on-chip unsupervised (STDP) training. We also evaluate the hybrid network architecture with the proposed data flow. The accuracy of this hybrid network is 10.84% higher than STDP trained accuracy result and 1.4% higher compared to the backpropagated training-based ConvSNN result with the CIFAR-10 dataset. Physical design of MONETAin 65 nm complementary metal-oxide-semiconductor (CMOS) shows 18.69 tera operation per second (TOPS)/W, 7.25 TOPS/W and 10.41 TOPS/W power efficiencies for the inference mode, learning mode, and hybrid learning mode, respectively.
C1 [Kim, Daehyun; Chakraborty, Biswadeep; She, Xueyuan; Lee, Edward; Kang, Beomseok; Mukhopadhyay, Saibal] Georgia Inst Technol, Dept Elect & Comp Engn, Atlanta, GA 30332 USA.
RP Kim, D (corresponding author), Georgia Inst Technol, Dept Elect & Comp Engn, Atlanta, GA 30332 USA.
EM daehyun.kim@gatech.edu
CR Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Buhler FN, 2017, SYMP VLSI CIRCUITS, pC30, DOI 10.23919/VLSIC.2017.8008536
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Chakraborty B, 2021, IEEE T IMAGE PROCESS, V30, P9014, DOI 10.1109/TIP.2021.3122092
   Chen GK, 2019, IEEE J SOLID-ST CIRC, V54, P992, DOI 10.1109/JSSC.2018.2884901
   Chen YH, 2016, ISSCC DIG TECH PAP I, V59, P262, DOI 10.1109/ISSCC.2016.7418007
   Chi P, 2016, CONF PROC INT SYMP C, P27, DOI 10.1109/ISCA.2016.13
   Chuang PY, 2020, DES AUT CON, DOI 10.1109/dac18072.2020.9218714
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng L, 2020, IEEE J SOLID-ST CIRC, V55, P2228, DOI 10.1109/JSSC.2020.2970709
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Diehl Peter U, 2015, 2015 INT JOINT C NEU, P1, DOI [DOI 10.1109/IJCNN.2015.7280696, 10.1109/IJCNN.2015.7280696]
   Gerstner W., 2002, SPIKING NEURON MODEL
   He K., 2016, P IEEE C COMPUTER VI
   Imani M, 2019, PROCEEDINGS OF THE 2019 46TH INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA '19), P802, DOI 10.1145/3307650.3322237
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Kim D, 2020, IEEE SOLID-ST CIRC L, V3, P278, DOI 10.1109/LSSC.2020.3013448
   Kim S., 2020, AAAI CONF ARTIF INTE, P11270
   Kim S, 2011, 2011 11TH INTERNATIONAL CONFERENCE ON CONTROL, AUTOMATION AND SYSTEMS (ICCAS), P1
   Ledinauskas E., 2020, ARXIV PREPRINT ARXIV, DOI [10.48550/ARXIV.2006.04436, DOI 10.48550/ARXIV.2006.04436]
   Lee C, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00119
   Lee C, 2019, IEEE T COGN DEV SYST, V11, P384, DOI 10.1109/TCDS.2018.2833071
   Lee C, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00435
   Long Y., 2020, P 2017 IEEE FAR E FO, P1, DOI DOI 10.1109/I2MTC43012.2020.9129204
   Long Y, 2019, IEEE J EXPLOR SOLID-, V5, P113, DOI 10.1109/JXCDC.2019.2923745
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Miquel J.R., 2021, ARXIV PREPRINT ARXIV, DOI [10.48550/ARXIV.2106.05624, DOI 10.48550/ARXIV.2106.05624]
   Narayanan S, 2020, ANN I S COM, P349, DOI 10.1109/ISCA45697.2020.00038
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Panda P, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00653
   Park J., 2019, 2019 IEEE INT SOLIDS, P140
   Peng XC, 2021, IEEE T COMPUT AID D, V40, P2306, DOI 10.1109/TCAD.2020.3043731
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Shafiee A, 2016, CONF PROC INT SYMP C, P14, DOI 10.1109/ISCA.2016.12
   She X., 2020, 2020 INT JOINT C NEU, P1
   She XY, 2021, PATTERN RECOGN, V118, DOI 10.1016/j.patcog.2021.108002
   She XY, 2019, DES AUT TEST EUROPE, P450, DOI [10.23919/DATE.2019.8714846, 10.23919/date.2019.8714846]
   Simonyan Karen, 2015, ARXIV, DOI [10.48550/arXiv.1409.1556, DOI 10.48550/ARXIV.1409.1556, 10.48550/ARXIV.1409.1556]
   Singh S, 2020, ANN I S COM, P363, DOI 10.1109/ISCA45697.2020.00039
   Srinivasan G, 2018, ACM J EMERG TECH COM, V14, DOI 10.1145/3266229
   Sze Vivienne, 2020, SYNTHESIS LECT COMPU, DOI DOI 10.2200/S01004ED1V01Y202004CAC050
   Tavanaei A., 2016, PREPRINT
   Wang GR, 2021, FRONT NEUROSCI-SWITZ, V15, DOI 10.3389/fnins.2021.615279
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
NR 47
TC 2
Z9 2
U1 3
U2 16
PD APR 11
PY 2022
VL 16
AR 775457
DI 10.3389/fnins.2022.775457
WC Neurosciences
DA 2023-11-11
ER

PT J
AU Zhang, GX
   Zhang, XH
   Rong, HN
   Paul, P
   Zhu, M
   Neri, F
   Ong, YS
AF Zhang, Gexiang
   Zhang, Xihai
   Rong, Haina
   Paul, Prithwineel
   Zhu, Ming
   Neri, Ferrante
   Ong, Yew-Soon
TI A Layered Spiking Neural System for Classification Problems
SO INTERNATIONAL JOURNAL OF NEURAL SYSTEMS
DT Article
DE Spiking neural networks; spiking neural P systems; layered weighted
   fuzzy spiking neural P systems; supervised learning
ID P SYSTEMS; NETWORK; BACKPROPAGATION; ALGORITHM
AB Biological brains have a natural capacity for resolving certain classification tasks. Studies on biologically plausible spiking neurons, architectures and mechanisms of artificial neural systems that closely match biological observations while giving high classification performance are gaining momentum. Spiking neural P systems (SN P systems) are a class of membrane computing models and third-generation neural networks that are based on the behavior of biological neural cells and have been used in various engineering applications. Furthermore, SN P systems are characterized by a highly flexible structure that enables the design of a machine learning algorithm by mimicking the structure and behavior of biological cells without the over-simplification present in neural networks. Based on this aspect, this paper proposes a novel type of SN P system, namely, layered SN P system (LSN P system), to solve classification problems by supervised learning. The proposed LSN P system consists of a multi-layer network containing multiple weighted fuzzy SN P systems with adaptive weight adjustment rules. The proposed system employs specific ascending dimension techniques and a selection method of output neurons for classification problems. The experimental results obtained using benchmark datasets from the UCI machine learning repository and MNIST dataset demonstrated the feasibility and effectiveness of the proposed LSN P system. More importantly, the proposed LSN P system presents the first SN P system that demonstrates sufficient performance for use in addressing real-world classification problems.
C1 [Zhang, Gexiang] Chengdu Univ Informat Technol, Sch Control Engn, Chengdu 610225, Peoples R China.
   [Zhang, Xihai] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Rong, Haina] Southwest Jiaotong Univ, Sch Elect Engn, Chengdu 610031, Peoples R China.
   [Paul, Prithwineel; Zhu, Ming] Chengdu Univ Informat Technol, Sch Control Engn, Chengdu 610225, Peoples R China.
   [Neri, Ferrante] Univ Surrey, NICE Grp, Dept Comp Sci, Surrey, England.
   [Ong, Yew-Soon] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore, Singapore.
RP Neri, F (corresponding author), Univ Surrey, NICE Grp, Dept Comp Sci, Surrey, England.
EM zhgxdylan@126.com; xihaizhang@tju.edu.cn; ronghaina@126.com;
   prithwineelpaul@gmail.com; zhuming@cuit.edu.cn; f.neri@surrey.ac.uk;
   ASYSOng@ntu.edu.sg
CR Adeli H, 2010, AUTOMATED EEG-BASED DIAGNOSIS OF NEUROLOGICAL DISORDERS: INVENTING THE FUTURE OF NEUROLOGY, P1
   Ahmadlou M, 2010, INTEGR COMPUT-AID E, V17, P197, DOI 10.3233/ICA-2010-0345
   Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   Alam KMR, 2020, NEURAL COMPUT APPL, V32, P8675, DOI 10.1007/s00521-019-04359-7
   [Anonymous], REGULARIZATION NEURA
   [Anonymous], 2017, REAL LIFE APPL MEMBR
   Bohte SM, 2005, INFORM PROCESS LETT, V95, P519, DOI 10.1016/j.ipl.2005.05.018
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Caraffini F, 2019, INFORM SCIENCES, V477, P186, DOI 10.1016/j.ins.2018.10.033
   Chen ZH, 2018, NEURAL COMPUT APPL, V29, P695, DOI 10.1007/s00521-016-2489-z
   Ciresan D, 2012, NEURAL NETWORKS, V32, P333, DOI 10.1016/j.neunet.2012.02.023
   Comsa IM, 2022, IEEE T NEUR NET LEAR, V33, P5939, DOI 10.1109/TNNLS.2021.3071976
   Cui J, 2020, IEEE T VEH TECHNOL, V69, P10494, DOI 10.1109/TVT.2020.3009165
   de la Cruz RTA, 2019, J MEMBRANE COMPUT, V1, P161, DOI 10.1007/s41965-019-00021-2
   Dekking F.M., 2005, MODERN INTRO PROBABI
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Diehl Peter U, 2015, 2015 INT JOINT C NEU, P1, DOI [DOI 10.1109/IJCNN.2015.7280696, 10.1109/IJCNN.2015.7280696]
   Dora S, 2016, NEUROCOMPUTING, V171, P1216, DOI 10.1016/j.neucom.2015.07.086
   Dora S, 2019, IEEE T CYBERNETICS, V49, P989, DOI 10.1109/TCYB.2018.2791282
   Dora S, 2017, INFORM SCIENCES, V414, P19, DOI 10.1016/j.ins.2017.05.050
   Ghosh-Dastidar S, 2007, INTEGR COMPUT-AID E, V14, P187
   Ghosh-Dastidar S, 2009, ADV INTEL SOFT COMPU, V61, P167
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Gollisch T, 2008, SCIENCE, V319, P1108, DOI 10.1126/science.1149639
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554
   Ionescu M, 2006, FUND INFORM, V71, P279
   Jeyasothy A, 2019, Arxiv, DOI arXiv:1904.11367
   Jeyasothy A, 2021, EXPERT SYST APPL, V178, DOI 10.1016/j.eswa.2021.114985
   Khanna R, 2019, IEEE SENS J, V19, P4571, DOI 10.1109/JSEN.2019.2901271
   Kostal L, 2007, EUR J NEUROSCI, V26, P2693, DOI 10.1111/j.1460-9568.2007.05880.x
   Lazo PPL, 2021, J MEMBRANE COMPUT, V3, P149, DOI 10.1007/s41965-021-00072-4
   LeCun Y., MNIST DATABASE HANDW
   Lichman M., UC IRVINE MACHINE LE
   Lotter W, 2020, NAT MACH INTELL, V2, P210, DOI 10.1038/s42256-020-0170-9
   Lv ZQ, 2021, INT J NEURAL SYST, V31, DOI 10.1142/S0129065720500495
   Machingal P, 2020, IEEE IJCNN, DOI 10.1109/ijcnn48605.2020.9207620
   Martín-Vide C, 2003, THEOR COMPUT SCI, V296, P295, DOI 10.1016/S0304-3975(02)00659-X
   Muhammad K, 2021, IEEE T NEUR NET LEAR, V32, P507, DOI 10.1109/TNNLS.2020.2995800
   Nazari S, 2019, NEUROCOMPUTING, V330, P196, DOI 10.1016/j.neucom.2018.10.066
   Pan LQ, 2017, INT J NEURAL SYST, V27, DOI 10.1142/S0129065717500423
   Pan LQ, 2012, NEURAL COMPUT, V24, P805, DOI 10.1162/NECO_a_00238
   Päun G, 2000, J COMPUT SYST SCI, V61, P108, DOI 10.1006/jcss.1999.1693
   Peng H, 2019, IEEE T NEUR NET LEAR, V30, P1672, DOI 10.1109/TNNLS.2018.2872999
   Pereira DR, 2020, NEURAL COMPUT APPL, V32, P6393, DOI 10.1007/s00521-019-04146-4
   Plana LA, 2011, ACM J EMERG TECH COM, V7, DOI 10.1145/2043643.2043647
   Querlioz D, 2013, IEEE T NANOTECHNOL, V12, P288, DOI 10.1109/TNANO.2013.2250995
   Rafiei MH, 2017, IEEE T NEUR NET LEAR, V28, P3074, DOI 10.1109/TNNLS.2017.2682102
   Ridet JL, 1997, TRENDS NEUROSCI, V20, P570, DOI 10.1016/S0166-2236(97)01139-9
   Rigotti M, 2013, NATURE, V497, P585, DOI 10.1038/nature12160
   Rosenblatt F., 1988, PERCEPTION PROBABILI, P89
   Roy K, 2019, NATURE, V575, P607, DOI 10.1038/s41586-019-1677-2
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Salakhutdinov R, 2012, NEURAL COMPUT, V24, P1967, DOI 10.1162/NECO_a_00311
   Song T, 2019, IEEE T NANOBIOSCI, V18, P176, DOI 10.1109/TNB.2019.2896981
   Tavanaei A, 2019, NEUROCOMPUTING, V330, P39, DOI 10.1016/j.neucom.2018.11.014
   Verlan S, 2020, J MEMBRANE COMPUT, V2, P355, DOI 10.1007/s41965-020-00050-2
   Wade JJ, 2010, IEEE T NEURAL NETWOR, V21, P1817, DOI 10.1109/TNN.2010.2074212
   Wang J, 2013, INT J COMPUT MATH, V90, P857, DOI 10.1080/00207160.2012.743653
   Wang J, 2013, IEEE T FUZZY SYST, V21, P209, DOI 10.1109/TFUZZ.2012.2208974
   Wang J, 2010, NEURAL COMPUT, V22, P2615, DOI 10.1162/NECO_a_00022
   Wang RM, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00213
   Wang T, 2015, IEEE T POWER SYST, V30, P1182, DOI 10.1109/TPWRS.2014.2347699
   Wang ZQ, 2000, IEEE T NEURAL NETWOR, V11, P47, DOI 10.1109/72.822509
   Whittington JCR, 2019, TRENDS COGN SCI, V23, P235, DOI 10.1016/j.tics.2018.12.005
   Wu TF, 2021, IEEE T NEUR NET LEAR, V32, P2443, DOI 10.1109/TNNLS.2020.3005538
   Wu TF, 2018, INT J NEURAL SYST, V28, DOI 10.1142/S0129065718500132
   Wu TF, 2018, IEEE T NEUR NET LEAR, V29, P3349, DOI 10.1109/TNNLS.2017.2726119
   Zhang GX, 2021, INT J NEURAL SYST, V31, DOI 10.1142/S0129065720500550
   Zhang GX, 2014, INT J NEURAL SYST, V24, DOI 10.1142/S0129065714400061
   Zhu M, 2021, INT J NEURAL SYST, V31, DOI 10.1142/S0129065720500549
NR 72
TC 32
Z9 32
U1 13
U2 72
PD AUG
PY 2022
VL 32
IS 08
AR 2250023
DI 10.1142/S012906572250023X
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT J
AU Zhong, XY
   Pan, HB
AF Zhong, Xueyan
   Pan, Hongbing
TI A Spike Neural Network Model for Lateral Suppression of
   Spike-Timing-Dependent Plasticity with Adaptive Threshold
SO APPLIED SCIENCES-BASEL
DT Article
DE Spike Neural Network; spike-timing-dependent plasticity; lateral
   inhibition; adaptive threshold; Leaky Integrate-and-Fire; pulse coding
ID STDP
AB Aiming at the practical constraints of high resource occupancy and complex calculations in the existing Spike Neural Network (SNN) image classification model, in order to seek a more lightweight and efficient machine vision solution, this paper proposes an adaptive threshold Spike Neural Network (SNN) model of lateral inhibition of Spike-Timing-Dependent Plasticity (STDP). The conversion from grayscale image to pulse sequence is completed by convolution normalization and first pulse time coding. The network self-classification is realized by combining the classical Spike-Timing-Dependent Plasticity algorithm (STDP) and lateral suppression algorithm. The occurrence of overfitting is effectively suppressed by introducing an adaptive threshold. The experimental results on the MNIST data set show that compared with the traditional SNN classification model, the complexity of the weight update algorithm is reduced from O(n(2)) to O(1), and the accuracy rate can still remain stable at about 96%. The provided model is conducive to the migration of software algorithms to the bottom layer of the hardware platform, and can provide a reference for the realization of edge computing solutions for small intelligent hardware terminals with high efficiency and low power consumption.
C1 [Zhong, Xueyan; Pan, Hongbing] Nanjing Univ, Sch Elect Sci & Engn, Nanjing 210023, Peoples R China.
   [Zhong, Xueyan] Nanjing Vocat Inst Railway Technol, Coll Intelligent Engn, Nanjing 210031, Peoples R China.
RP Zhong, XY (corresponding author), Nanjing Univ, Sch Elect Sci & Engn, Nanjing 210023, Peoples R China.; Zhong, XY (corresponding author), Nanjing Vocat Inst Railway Technol, Coll Intelligent Engn, Nanjing 210031, Peoples R China.
EM zhongxueyan1987@163.com; phb@nju.edu.cn
CR Amirshahi A, 2019, IEEE T BIOMED CIRC S, V13, P1483, DOI 10.1109/TBCAS.2019.2948920
   Andrzejak RG, 2001, PHYS REV E, V64, DOI 10.1103/PhysRevE.64.061907
   Borst A, 1999, NAT NEUROSCI, V2, P947, DOI 10.1038/14731
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Gao ZK, 2019, IEEE T NEUR NET LEAR, V30, P2755, DOI 10.1109/TNNLS.2018.2886414
   Hu SG, 2021, NEURAL COMPUT APPL, V33, P12317, DOI 10.1007/s00521-021-05832-y
   Hwang S, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11052059
   Kasabov NK, 2014, NEURAL NETWORKS, V52, P62, DOI 10.1016/j.neunet.2014.01.006
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Lammie C, 2019, IEEE T CIRCUITS-I, V66, P1558, DOI 10.1109/TCSI.2018.2881753
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee C, 2019, IEEE T COGN DEV SYST, V11, P384, DOI 10.1109/TCDS.2018.2833071
   Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508
   Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012
   Li XM, 2018, PHYSICA A, V491, P716, DOI 10.1016/j.physa.2017.08.053
   Liu SC, 2010, CURR OPIN NEUROBIOL, V20, P288, DOI 10.1016/j.conb.2010.03.007
   Michielli N, 2019, COMPUT BIOL MED, V106, P71, DOI 10.1016/j.compbiomed.2019.01.013
   Mohammadi Y, 2019, IRAN CONF ELECTR ENG, P1765, DOI [10.1109/IranianCEE.2019.8786540, 10.1109/iraniancee.2019.8786540]
   Mozafari M, 2018, IEEE T NEUR NET LEAR, V29, P6178, DOI 10.1109/TNNLS.2018.2826721
   Nuntalid N, 2011, LECT NOTES COMPUT SC, V7062, P451, DOI 10.1007/978-3-642-24955-6_54
   Orhan U, 2011, EXPERT SYST APPL, V38, P13475, DOI 10.1016/j.eswa.2011.04.149
   Petro B, 2020, IEEE T NEUR NET LEAR, V31, P358, DOI 10.1109/TNNLS.2019.2906158
   Sengupta N, 2017, INFORM SCIENCES, V406, P133, DOI 10.1016/j.ins.2017.04.017
   Shi MT, 2020, FRONT COMPUT NEUROSC, V14, DOI 10.3389/fncom.2020.00007
   Wang XW, 2020, NEURAL NETWORKS, V125, P258, DOI 10.1016/j.neunet.2020.02.011
   Xia Y, 2020, J ROBOT NETW ARTIF L, V7, P121, DOI 10.2991/jrnal.k.200528.010
   Xiang SY, 2021, IEEE J SEL TOP QUANT, V27, DOI 10.1109/JSTQE.2020.3005589
   [赵梓铭 Zhao Ziming], 2018, [计算机研究与发展, Journal of Computer Research and Development], V55, P327
   Zheng N, 2018, IEEE T NEUR NET LEAR, V29, P4287, DOI 10.1109/TNNLS.2017.2761335
NR 30
TC 2
Z9 2
U1 5
U2 25
PD JUN
PY 2022
VL 12
IS 12
AR 5980
DI 10.3390/app12125980
WC Chemistry, Multidisciplinary; Engineering, Multidisciplinary; Materials
   Science, Multidisciplinary; Physics, Applied
DA 2023-11-11
ER

PT C
AU Li, HY
   Hao, XY
   Wang, J
   Che, YQ
AF Li, Huiyan
   Hao, Xinyu
   Wang, Jiang
   Che, Yanqiu
BE Li, Z
   Sun, J
TI Automatic Fixed-point Design Method for Spiking Neuron Circuits
SO 2022 41ST CHINESE CONTROL CONFERENCE (CCC)
SE Chinese Control Conference
DT Proceedings Paper
CT 41st Chinese Control Conference (CCC)
CY JUL 25-27, 2022
CL Hefei, PEOPLES R CHINA
DE Fixed-point design; Hardware implementation; Spiking neurons
ID HARDWARE; NETWORKS
AB Improving hardware implementation efficiency of spiking neuron models is the basis of realizing high-performance spiking neural networks and is also beneficial to increasing the achievable network size. To this end, this work proposes an automatic fixed-point design method for designing spiking neuron circuits under any precision requirement to improve the realization efficiency. The performance of the method is evaluated on both phenomenological and biophysical neuron models and also a neural network with online learning. The results show that compared with the reference neuron model, the model designed with the proposed method can save 30% of resource consumption and increase 10% of the working frequency when the firing rate error is required to be 10%. The firing mode learning task can also be completed successfully without performance degradation with a neural network composed of the designed fixed-point neuron models, which demonstrates the effectiveness of the proposed method.
C1 [Li, Huiyan; Che, Yanqiu] Tianjin Univ Technol & Educ, Sch Automat & Elect Engn, Tianjin 300222, Peoples R China.
   [Hao, Xinyu; Wang, Jiang] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
RP Li, HY (corresponding author), Tianjin Univ Technol & Educ, Sch Automat & Elect Engn, Tianjin 300222, Peoples R China.
EM lhy2740@126.com; jiang.wang@tju.edu.cn
CR Farsa EZ, 2019, IEEE T CIRCUITS-II, V66, P1582, DOI 10.1109/TCSII.2019.2890846
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   Guo WZ, 2022, IEEE T NEUR NET LEAR, V33, P3988, DOI 10.1109/TNNLS.2021.3055421
   Heidarpour M, 2016, IEEE T CIRCUITS-I, V63, P1986, DOI 10.1109/TCSI.2016.2598161
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Huang X, 2021, IEEE INT ULTRA SYM, DOI 10.1109/IUS52206.2021.9593901
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Kreiser R, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00551
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maguire LP, 2007, NEUROCOMPUTING, V71, P13, DOI 10.1016/j.neucom.2006.11.029
   Patel K., 2021, ARXIV210608921
   Pérez J, 2018, NEURAL NETWORKS, V104, P15, DOI 10.1016/j.neunet.2018.04.002
   Ponulak F., 2005, RESUME NEW SUPERVISE
   Roy K, 2019, NATURE, V575, P607, DOI 10.1038/s41586-019-1677-2
   Roy S, 2004, DES AUT CON, P484, DOI 10.1145/996566.996701
NR 15
TC 0
Z9 0
U1 0
U2 0
PY 2022
BP 7094
EP 7099
WC Automation & Control Systems
DA 2023-11-11
ER

PT J
AU Huang, XH
   Zheng, ZG
   Hu, G
   Wu, S
   Rasch, MJ
AF Huang, Xuhui
   Zheng, Zhigang
   Hu, Gang
   Wu, Si
   Rasch, Malte J.
TI Different propagation speeds of recalled sequences in plastic spiking
   neural networks
SO NEW JOURNAL OF PHYSICS
DT Article
DE sequential activity recall; spiking neural network; spike-timing
   dependent plasticity
ID TIMING-DEPENDENT PLASTICITY; FEEDFORWARD NETWORKS; SYNCHRONOUS SPIKING;
   STABLE PROPAGATION; HIPPOCAMPAL REPLAY; SYNFIRE CHAINS; SPARSE CODE;
   REVERBERATION; REACTIVATION; NEURONS
AB Neural networks can generate spatiotemporal patterns of spike activity. Sequential activity learning and retrieval have been observed in many brain areas, and e.g. is crucial for coding of episodic memory in the hippocampus or generating temporal patterns during song production in birds. In a recent study, a sequential activity pattern was directly entrained onto the neural activity of the primary visual cortex (V1) of rats and subsequently successfully recalled by a local and transient trigger. It was observed that the speed of activity propagation in coordinates of the retinotopically organized neural tissue was constant during retrieval regardless how the speed of light stimulation sweeping across the visual field during training was varied. It is well known that spike-timing dependent plasticity (STDP) is a potential mechanism for embedding temporal sequences into neural network activity. How training and retrieval speeds relate to each other and how network and learning parameters influence retrieval speeds, however, is not well described. We here theoretically analyze sequential activity learning and retrieval in a recurrent neural network with realistic synaptic short-term dynamics and STDP. Testing multiple STDP rules, we confirm that sequence learning can be achieved by STDP. However, we found that a multiplicative nearest-neighbor (NN) weight update rule generated weight distributions and recall activities that best matched the experiments in V1. Using network simulations and mean-field analysis, we further investigated the learning mechanisms and the influence of network parameters on recall speeds. Our analysis suggests that a multiplicative STDP rule with dominant NN spike interaction might be implemented in V1 since recall speed was almost constant in an NMDA-dominant regime. Interestingly, in an AMPA-dominant regime, neural circuits might exhibit recall speeds that instead follow the change in stimulus speeds. This prediction could be tested in experiments.
C1 [Huang, Xuhui; Zheng, Zhigang; Hu, Gang] Beijing Normal Univ, Dept Phys, Beijing 100875, Peoples R China.
   [Huang, Xuhui] Chinese Acad Sci, Brainnetome Ctr, Inst Automat, Beijing, Peoples R China.
   [Huang, Xuhui] Chinese Acad Sci, Natl Lab Pattern Recognit, Inst Automat, Beijing, Peoples R China.
   [Wu, Si; Rasch, Malte J.] Beijing Normal Univ, State Key Lab Cognit Neurosci & Learning, Beijing 100875, Peoples R China.
   [Wu, Si; Rasch, Malte J.] Beijing Normal Univ, IDG McGovern Inst Brain Res, Beijing 100875, Peoples R China.
   [Wu, Si; Rasch, Malte J.] Beijing Normal Univ, Ctr Collaborat & Innovat Brain & Learning Sci, Beijing 100875, Peoples R China.
RP Huang, XH (corresponding author), Beijing Normal Univ, Dept Phys, Beijing 100875, Peoples R China.
EM wusi@bnu.edu.cn; malte.rasch@bnu.edu.cn
CR Abbott LF, 1996, CEREB CORTEX, V6, P406, DOI 10.1093/cercor/6.3.406
   Aviel Y, 2004, NEUROCOMPUTING, V58, P123, DOI 10.1016/j.neucom.2004.01.032
   Bashan A, 2012, NAT COMMUN, V3, DOI 10.1038/ncomms1705
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Byrnes S, 2011, NEURAL COMPUT, V23, P2567, DOI 10.1162/NECO_a_00184
   Carr MF, 2011, NAT NEUROSCI, V14, P147, DOI 10.1038/nn.2732
   Chance FS, 1998, J NEUROSCI, V18, P4785
   Contreras EJB, 2013, NEURON, V79, P555, DOI 10.1016/j.neuron.2013.06.013
   Dan Y, 2006, PHYSIOL REV, V86, P1033, DOI 10.1152/physrev.00030.2005
   Dave AS, 2000, SCIENCE, V290, P812, DOI 10.1126/science.290.5492.812
   Davidson TJ, 2009, NEURON, V63, P497, DOI 10.1016/j.neuron.2009.07.027
   Diba K, 2007, NAT NEUROSCI, V10, P1241, DOI 10.1038/nn1961
   Diesmann M, 1999, NATURE, V402, P529, DOI 10.1038/990101
   Eagleman SL, 2012, P NATL ACAD SCI USA, V109, P19450, DOI 10.1073/pnas.1212059109
   Euston DR, 2007, SCIENCE, V318, P1147, DOI 10.1126/science.1148979
   Fiete IR, 2010, NEURON, V65, P563, DOI 10.1016/j.neuron.2010.02.003
   Froemke RC, 2002, NATURE, V416, P433, DOI 10.1038/416433a
   Gerstner W., 2002, SPIKING NEURON MODEL
   Gjorgjieva J, 2011, P NATL ACAD SCI USA, V108, P19383, DOI 10.1073/pnas.1105933108
   Hahnloser RHR, 2002, NATURE, V419, P65, DOI 10.1038/nature00974
   Han F, 2008, NEURON, V60, P321, DOI 10.1016/j.neuron.2008.08.026
   Hanuschkin A, 2011, J COMPUT NEUROSCI, V31, P509, DOI 10.1007/s10827-011-0318-z
   Holcman D, 2006, PLOS COMPUT BIOL, V2, P174, DOI 10.1371/journal.pcbi.0020023
   Hosaka R, 2008, NEURAL COMPUT, V20, P415, DOI 10.1162/neco.2007.11-05-043
   Huang XH, 2014, CHINESE PHYS B, V23, DOI 10.1088/1674-1056/23/10/108703
   Ikegaya Y, 2004, SCIENCE, V304, P559, DOI 10.1126/science.1093173
   Ivanov PC, 2014, UNDERST COMPLEX SYST, P203, DOI 10.1007/978-3-319-03518-5_10
   Izhikevich EM, 2004, CEREB CORTEX, V14, P933, DOI 10.1093/cercor/bhh053
   Izhikevich EM, 2003, NEURAL COMPUT, V15, P1511, DOI 10.1162/089976603321891783
   Ji DY, 2007, NAT NEUROSCI, V10, P100, DOI 10.1038/nn1825
   Karlsson MP, 2009, NAT NEUROSCI, V12, P913, DOI 10.1038/nn.2344
   Kistler WM, 2002, NEURAL COMPUT, V14, P987, DOI 10.1162/089976602753633358
   Klampfl S, 2013, J NEUROSCI, V33, P11515, DOI 10.1523/JNEUROSCI.5044-12.2013
   Kumar A, 2008, J NEUROSCI, V28, P5268, DOI 10.1523/JNEUROSCI.2542-07.2008
   Kumar A, 2010, NAT REV NEUROSCI, V11, P615, DOI 10.1038/nrn2886
   Lehn H, 2009, J NEUROSCI, V29, P3475, DOI 10.1523/JNEUROSCI.5370-08.2009
   Li MR, 2006, PHYS REV E, V74, DOI 10.1103/PhysRevE.74.011918
   Litvak V, 2003, J NEUROSCI, V23, P3006
   Long MA, 2010, NATURE, V468, P394, DOI 10.1038/nature09514
   Luczak A, 2007, P NATL ACAD SCI USA, V104, P347, DOI 10.1073/pnas.0605643104
   MARGOLIASH D, 1992, J NEUROSCI, V12, P4309
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   Masuda N, 2007, J COMPUT NEUROSCI, V22, P327, DOI 10.1007/s10827-007-0022-1
   Mongillo G, 2008, SCIENCE, V319, P1543, DOI 10.1126/science.1150769
   Nádasdy Z, 1999, J NEUROSCI, V19, P9497
   O'Neill J, 2008, NAT NEUROSCI, V11, P209, DOI 10.1038/nn2037
   Pastalkova E, 2008, SCIENCE, V321, P1322, DOI 10.1126/science.1159775
   Pfister JP, 2006, J NEUROSCI, V26, P9673, DOI 10.1523/JNEUROSCI.1425-06.2006
   Rubin J, 2001, PHYS REV LETT, V86, P364, DOI 10.1103/PhysRevLett.86.364
   Sjostrom J., 2010, SCHOLARPEDIA 52 REVI, V5, P1362, DOI [10.4249/scholarpedia.1362, 10.4249%2Fscholarpedia.1362, DOI 10.4249/SCHOLARPEDIA.1362]
   Sjöström PJ, 2001, NEURON, V32, P1149, DOI 10.1016/s0896-6273(01)00542-6
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Stoop R, 2013, PHYS REV LETT, V110, DOI 10.1103/PhysRevLett.110.108105
   Suri RE, 2002, BIOL CYBERN, V87, P440, DOI 10.1007/s00422-002-0355-9
   Swadlow H. A., 2012, SCHOLARPEDIA, V7, DOI [10.4249/scholarpedia.1451, DOI 10.4249/SCHOLARPEDIA.1451]
   Takahashi YK, 2009, PHYS REV E, V79, DOI 10.1103/PhysRevE.79.051904
   Tsodyks M, 1998, NEURAL COMPUT, V10, P821, DOI 10.1162/089976698300017502
   Tsodyks MV, 1997, P NATL ACAD SCI USA, V94, P719, DOI 10.1073/pnas.94.2.719
   van Rossum MCW, 2000, J NEUROSCI, V20, P8812
   Varela JA, 1999, J NEUROSCI, V19, P4293
   Varela JA, 1997, J NEUROSCI, V17, P7926
   Wang HX, 2005, NAT NEUROSCI, V8, P187, DOI 10.1038/nn1387
   Wang ST, 2006, PHYS REV LETT, V96, DOI 10.1103/PhysRevLett.96.018103
   Wang XJ, 2002, NEURON, V36, P955, DOI 10.1016/S0896-6273(02)01092-9
   WILSON MA, 1994, SCIENCE, V265, P676, DOI 10.1126/science.8036517
   Xu SJ, 2012, NAT NEUROSCI, V15, P449, DOI 10.1038/nn.3036
   Yuan WJ, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0084644
NR 67
TC 6
Z9 6
U1 1
U2 25
PD MAR 6
PY 2015
VL 17
AR 035006
DI 10.1088/1367-2630/17/3/035006
WC Physics, Multidisciplinary
DA 2023-11-11
ER

PT C
AU Howard, G
   Bull, L
   Lanzi, PL
AF Howard, Gerard
   Bull, Larry
   Lanzi, Pier-Luca
GP IEEE
TI A Spiking Neural Representatin for XCSF
SO 2010 IEEE CONGRESS ON EVOLUTIONARY COMPUTATION (CEC)
SE IEEE Congress on Evolutionary Computation
DT Proceedings Paper
CT 2010 IEEE World Congress on Computational Intelligence
CY JUL 18-23, 2010
CL Barcelona, SPAIN
AB This paper presents a Learning Classifier System (LCS) where each traditional rule is represented by a spiking neural network, a type of network with dynamic internal state. The evolutionary design process exploits parameter self-adaptation and a constructionist approach, providing the system with a flexible knowledge representation. It is shown how this approach allows for the evolution of networks of appropriate complexity to emerge whilst solving a continuous maze environment. Additionally, we extend the system to allow for temporal state decomposition. We evaluate our spiking neural LCS against one that uses Multi Layer Perceptron rules.
C1 [Howard, Gerard; Bull, Larry] Univ West England, Dept Comp Sci, Bristol BS16 1QY, Avon, England.
   [Lanzi, Pier-Luca] Politecn Milan, Dipartimento Elect & Informazione, Milan, Italy.
RP Howard, G (corresponding author), Univ West England, Dept Comp Sci, Bristol BS16 1QY, Avon, England.
EM gerard2.howard@uwc.ac.uk; larry.bull@uwe.ac.uk; pierluca.lanzi@polimi.it
CR Ahluwalia M, 1999, GECCO-99: PROCEEDINGS OF THE GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P11
   [Anonymous], 1986, PARALLEL DISTRIBUTED, DOI DOI 10.7551/MITPRESS/5236.001.0001
   [Anonymous], 2001, EVOLUTIONARY ROBOTIC
   [Anonymous], GEN EV COMP C GECCO
   Boyan J. A., 1995, Advances in Neural Information Processing Systems 7, P369
   Buhmann MD, 2003, C MO AP C M, DOI 10.1017/CBO9780511543241
   Bull L., 2002, Parallel Problem Solving from Nature - PPSN VII. 7th International Conference. Proceedings (Lecture Notes in Computer Science Vol.2439), P558
   Bull L, 2003, IEEE C EVOL COMPUTAT, P991
   Bull L., 2009, INT J PARAL IN PRESS
   Bull L., 2000, AN AN 6 INT C SIM AD
   Bull L., 2002, P 4 ANN C GEN EV COM, P905
   Dam HH, 2008, IEEE T KNOWL DATA EN, V20, P26, DOI 10.1109/TKDE.2007.190671
   Dolan C. P., 1987, Genetic Algorithms and their Applications: Proceedings of the Second International Conference on Genetic Algorithms, P123
   Federici D, 2005, P CEC 2005 IEEE C EV
   Floreano D., 2005, P 8 INT C ART LIF DE
   Floreano D, 2008, EVOL INTELL, V1, P47, DOI 10.1007/s12065-007-0002-4
   Gerstner W., 2002, SPIKING NEURON MODEL
   Giani A, 1995, P 4 ANN C EV PROGR E
   HOLLAND JH, 1976, PROG THEOR BIOL, V4, P263
   Howard D., 2009, GECCO 2009
   Howard D., 2008, GECCO 2008, P1977
   Hurst J., 2002, Parallel Problem Solving from Nature - PPSN VII. 7th International Conference. Proceedings (Lecture Notes in Computer Science Vol.2439), P588
   Hurst J, 2006, ARTIF LIFE, V12, P353, DOI 10.1162/artl.2006.12.3.353
   Indiveri G., 2001, IEEE T NEUR IN PRESS
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Korkin M, 1998, P 2 INT C EV SYST BI
   Lanzi P. L., IEEE C EV COMP CEC 2, P2270
   Lanzi PL, 2005, IEEE C EVOL COMPUTAT, P2032
   Lanzi PL, 1999, GECCO-99: PROCEEDINGS OF THE GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P345
   LIN LJ, 1992, MACH LEARN, V8, P625
   MAHADEVAN S, 1992, ARTIF INTELL, V55, P311, DOI 10.1016/0004-3702(92)90058-6
   O'Hara T, 2005, IEEE C EVOL COMPUTAT, P2046
   OHARA T, 2004, UWELCSG04004
   Port R.F., 1995, MIND MOTION EXPLORAT
   Quartz SR, 1997, BEHAV BRAIN SCI, V20, P537
   Rocha M., EPIA 2003, P24
   Santamaria JC, 1997, ADAPT BEHAV, V6, P163, DOI 10.1177/105971239700600201
   Schlessinger E., 2005, P EUR C ART LIF ECAL
   THAM CK, 1995, ROBOT AUTON SYST, V15, P247, DOI 10.1016/0921-8890(95)00005-Z
   Valenzuela-Rendon M, 1991, P 4 INT C GENETIC AL, P346
   Wilson S. W., 2001, 4 INT WORKSH LEARN C
   Wilson SW, 1995, EVOL COMPUT, V3, P149, DOI 10.1162/evco.1995.3.2.149
   Wilson Stewart W, 2000, LEARNING CLASSIFIER, P209, DOI [DOI 10.1007/3-540-45027-011, 10.1007/3-540-45027-0_11, DOI 10.1007/3-540-45027-0_11]
NR 43
TC 1
Z9 1
U1 0
U2 0
PY 2010
WC Engineering, Electrical & Electronic; Mathematical & Computational
   Biology
DA 2023-11-11
ER

PT C
AU Yue, Y
   Baltes, M
   Abujahar, N
   Sun, T
   Smith, CD
   Bihl, T
   Liu, J
AF Yue, Ye
   Baltes, Marc
   Abujahar, Nidal
   Sun, Tao
   Smith, Charles D.
   Bihl, Trevor
   Liu, Jundong
GP IEEE
TI HYBRID SPIKING NEURAL NETWORKS FINE-TUNING FOR HIPPOCAMPUS SEGMENTATION
SO 2023 IEEE 20TH INTERNATIONAL SYMPOSIUM ON BIOMEDICAL IMAGING, ISBI
SE IEEE International Symposium on Biomedical Imaging
DT Proceedings Paper
CT 20th IEEE International Symposium on Biomedical Imaging (ISBI)
CY APR 18-21, 2023
CL Cartagena, COLOMBIA
DE Spiking neural network; image segmentation; hippocampus; brain; U-Net;
   ANN-SNN conversion
AB Over the past decade, artificial neural networks (ANNs) have made tremendous advances, in part due to the increased availability of annotated data. However, ANNs typically require significant power and memory consumptions to reach their full potential. Spiking neural networks (SNNs) have recently emerged as a low-power alternative to ANNs due to their sparsity nature.
   SNN, however, are not as easy to train as ANNs. In this work, we propose a hybrid SNN training scheme and apply it to segment human hippocampi from magnetic resonance images. Our approach takes ANN-SNN conversion as an initialization step and relies on spike-based backpropagation to fine-tune the network. Compared with the conversion and direct training solutions, our method has advantages in both segmentation accuracy and training efficiency. Experiments demonstrate the effectiveness of our model in achieving the design goals.
C1 [Yue, Ye; Baltes, Marc; Abujahar, Nidal; Sun, Tao; Liu, Jundong] Ohio Univ, Sch Elect Engn & Comp Sci, Athens, OH 45701 USA.
   [Smith, Charles D.] Univ Kentucky, Dept Neurol, Lexington, KY USA.
   [Bihl, Trevor] Wright State Univ, Dept Biomed, Ind & Human Factors Engn, Dayton, OH 45435 USA.
RP Liu, J (corresponding author), Ohio Univ, Sch Elect Engn & Comp Sci, Athens, OH 45701 USA.
EM liuj1@ohio.edu
CR Bellec G, 2018, ADV NEUR IN, V31
   Chen YN, 2017, LECT NOTES COMPUT SC, V10541, P88, DOI 10.1007/978-3-319-67389-9_11
   Chen YN, 2017, I S BIOMED IMAGING, P192, DOI 10.1109/ISBI.2017.7950499
   Coupé P, 2011, NEUROIMAGE, V54, P940, DOI 10.1016/j.neuroimage.2010.09.018
   Davies M, 2021, P IEEE, V109, P911, DOI 10.1109/JPROC.2021.3067593
   Diehl PU, 2015, IEEE IJCNN
   Ho ND, 2021, DES AUT CON, P793, DOI 10.1109/DAC18074.2021.9586266
   Hobbs KH, 2016, I S BIOMED IMAGING, P19, DOI 10.1109/ISBI.2016.7493201
   Hunsberger E, 2015, Arxiv, DOI arXiv:1510.08829
   Kim Y, 2021, Arxiv, DOI arXiv:2110.07742
   Kim Y, 2021, FRONT NEUROSCI-SWITZ, V15, DOI 10.3389/fnins.2021.773954
   Li Yan, 2021, arXiv
   Liu QH, 2020, IEEE T NEUR NET LEAR, V31, P5300, DOI 10.1109/TNNLS.2020.2966058
   Manna Davide Liberato, 2022, NEUROMORPHIC COMPUTI, V1
   Patel K, 2021, Arxiv, DOI arXiv:2106.08921
   Rasmussen D, 2019, NEUROINFORMATICS, V17, P611, DOI 10.1007/s12021-019-09424-z
   Rathi Nitin, 2020, ENABLING DEEP SPIKIN
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Roy K, 2019, NATURE, V575, P607, DOI 10.1038/s41586-019-1677-2
   Rueckauer Bodo, 2016, ARXIV, DOI DOI 10.3389/FNINS.2017.00682
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Shi BB, 2017, PATTERN RECOGN, V63, P487, DOI 10.1016/j.patcog.2016.09.032
   Shrestha Sumit B., 2018, ADV NEURAL INFORM PR, V31, P1
   Song YT, 2015, LECT NOTES COMPUT SC, V9351, P190, DOI 10.1007/978-3-319-24574-4_23
   Tong T, 2013, NEUROIMAGE, V76, P11, DOI 10.1016/j.neuroimage.2013.02.069
   Vicente-Sola Alex, 2022, NEUROMORPHIC COMPUTI, P1
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
NR 27
TC 0
Z9 0
U1 0
U2 0
PY 2023
DI 10.1109/ISBI53787.2023.10230610
WC Computer Science, Artificial Intelligence; Engineering, Biomedical;
   Radiology, Nuclear Medicine & Medical Imaging
DA 2023-11-11
ER

PT J
AU Han, JH
   Li, ZL
   Zheng, WM
   Zhang, YH
AF Han, Jianhui
   Li, Zhaolin
   Zheng, Weimin
   Zhang, Youhui
TI Hardware Implementation of Spiking Neural Networks on FPGA
SO TSINGHUA SCIENCE AND TECHNOLOGY
DT Article
DE Spiking Neural Network (SNN); Field-Programmable Gate Arrays (FPGA);
   digital circuit; low-power; MNIST
AB Inspired by real biological neural models, Spiking Neural Networks (SNNs) process information with discrete spikes and show great potential for building low-power neural network systems. This paper proposes a hardware implementation of SNN based on Field-Programmable Gate Arrays (FPGA). It features a hybrid updating algorithm, which combines the advantages of existing algorithms to simplify hardware design and improve performance. The proposed design supports up to 16384 neurons and 16.8 million synapses but requires minimal hardware resources and archieves a very low power consumption of 0.477W. A test platform is built based on the proposed design using a Xilinx FPGA evaluation board, upon which we deploy a classification task on the MNIST dataset. The evaluation results show an accuracy of 97.06% and a frame rate of 161 frames per second.
C1 [Han, Jianhui] Tsinghua Univ, Inst Microelect, Beijing 100084, Peoples R China.
   [Li, Zhaolin] Tsinghua Univ, Res Inst Informat Technol, Beijing 100084, Peoples R China.
   [Zheng, Weimin; Zhang, Youhui] Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
RP Li, ZL (corresponding author), Tsinghua Univ, Res Inst Informat Technol, Beijing 100084, Peoples R China.; Zhang, YH (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
EM hanjh16@mails.tsinghua.edu.cn; lzl73@mail.tsinghua.edu.cn;
   zwm-dcs@mail.tsinghua.edu.cn; zyh02@tsinghua.edu.cn
CR [Anonymous], XIL ZYNQ 7000 SOC ZC
   [Anonymous], NVIDIA TESL P100 WOR
   [Anonymous], NVIDIA SYST MAN INT
   Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Cheung Kit, 2012, Artificial Neural Networks and Machine Learning - ICANN 2012. Proceedings of the 22nd International Conference on Artificial Neural Networks, P113, DOI 10.1007/978-3-642-33269-2_15
   Ciresan DC, 2010, NEURAL COMPUT, V22, P3207, DOI 10.1162/NECO_a_00052
   Diehl Peter U, 2015, 2015 INT JOINT C NEU, P1, DOI [DOI 10.1109/IJCNN.2015.7280696, 10.1109/IJCNN.2015.7280696]
   Farquhar E, 2006, IEEE INT SYMP CIRC S, P4114, DOI 10.1109/ISCAS.2006.1693534
   Han S., 2015, ARXIV PREPRINT 15100
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Khan MM, 2008, IEEE IJCNN, P2849, DOI 10.1109/IJCNN.2008.4634199
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Liu M, 2009, L N INST COMP SCI SO, V3, P44
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Mohamed AR, 2012, IEEE T AUDIO SPEECH, V20, P14, DOI 10.1109/TASL.2011.2109382
   Moore SW, 2012, ANN IEEE SYM FIELD P, P133, DOI 10.1109/FCCM.2012.32
   Neil D, 2014, IEEE T VLSI SYST, V22, P2621, DOI 10.1109/TVLSI.2013.2294916
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   Paszke A., 2017, NIPS AUTODIFF WORKSH
   Pfeil T, 2016, PHYS REV X, V6, DOI 10.1103/PhysRevX.6.021023
NR 21
TC 47
Z9 47
U1 6
U2 84
PD AUG
PY 2020
VL 25
IS 4
BP 479
EP 486
DI 10.26599/TST.2019.9010019
WC Computer Science, Information Systems; Computer Science, Software
   Engineering; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Enriquez-Gaytan, J
   Gómez-Castañeda, F
   Moreno-Cadenas, JA
   Flores-Nava, LM
AF Enriquez-Gaytan, J.
   Gomez-Castaneda, F.
   Moreno-Cadenas, J. A.
   Flores-Nava, L. M.
GP IEEE
TI Spiking Neural Network Trained by Metaheuristics for Gas Sensing
SO 2017 INTERNATIONAL CONFERENCE ON ELECTRONICS, COMMUNICATIONS AND
   COMPUTERS (CONIELECOMP)
SE International Conference on Electronics Communications and Computers
   CONIELECOMP
DT Proceedings Paper
CT 27th International Conference on Electronics, Communications and
   Computers (CONIELECOMP)
CY FEB 22-24, 2017
CL Univ Americas Pubela, Cholula, MEXICO
HO Univ Americas Pubela
DE Spiking neural networks (SNN-Izhikevich's model); bio-inspired signal
   processing; Metaheuristic algorithm; Artificial Bee Colony algorithm
   (ABC algorithm)
AB In this manuscript, we propose a bio-inspired method of signal processing in a task for the detection of gases (Methane and Iso-Butane) using an array of four gas sensors by Figaro and Spiking Neural Network (SNN-Izhikevich's model), supported by the synaptic current model. Our SNN is trained by the metaheuristic algorithm of Artificial Bee Colony algorithm (ABC algorithm) in a supervised manner. This is demonstrated using Matlab.
C1 [Enriquez-Gaytan, J.; Gomez-Castaneda, F.; Moreno-Cadenas, J. A.; Flores-Nava, L. M.] IPN, CINVESTAV, Dept Elect Engn, Mexico City, DF, Mexico.
RP Enriquez-Gaytan, J (corresponding author), IPN, CINVESTAV, Dept Elect Engn, Mexico City, DF, Mexico.
EM jenriquezg@cinvestav.mx; fgomez@cinvestav.mx; jmoreno@cinvestav.mx;
   lmflores@cinvestav.mx
CR Dayan P., 2001, THEORETICAL NEUROSCI
   Fountas Z., 2011, SPIKING NEURAL NETWO
   Hagan MT., 1996, NEURAL NETWORK DESIG
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2010, PHILOS T R SOC A, V368, P5061, DOI 10.1098/rsta.2010.0130
   Kandel E.R., 2012, PRINCIPLES NEURAL SC
   Karaboga D., 2005, TR06
   Karaboga D, 2007, LECT NOTES COMPUT SC, V4529, P789, DOI 10.1007/978-3-540-72950-1_77
   Karaboga D, 2014, ARTIF INTELL REV, V42, P21, DOI 10.1007/s10462-012-9328-0
   Ortiz I. C. Matadamas, 2014, APLICACION REDES NEU
   Perez-Garcia A. N., 2014, INT C EL ENG COMP SC
   Ramirez L. G. Corona, 2015, SENSORES ACTUADORES
   Sörensen K, 2015, INT T OPER RES, V22, P3, DOI 10.1111/itor.12001
   Vazquez RA, 2015, COMPUT INTEL NEUROSC, V2015, DOI 10.1155/2015/947098
   Wu QX, 2007, LECT NOTES ARTIF INT, V4682, P26
   Yigitbasi Elif Deniz, 2013, International Journal of Information and Electronics Engineering, V3, P634, DOI 10.7763/IJIEE.2013.V3.394
NR 17
TC 0
Z9 0
U1 2
U2 3
PY 2017
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
DA 2023-11-11
ER

PT J
AU Rathi, N
   Roy, K
AF Rathi, Nitin
   Roy, Kaushik
TI DIET-SNN: A Low-Latency Spiking Neural Network With Direct Input
   Encoding and Leakage and Threshold Optimization
SO IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
DT Article
DE Neurons; Training; Encoding; Backpropagation; Task analysis;
   Computational modeling; Biological neural networks; Backpropagation
   through time (BPTT); convolutional neural networks; spiking neural
   networks (SNNs); supervised learning
ID ON-CHIP
AB Bioinspired spiking neural networks (SNNs), operating with asynchronous binary signals (or spikes) distributed over time, can potentially lead to greater computational efficiency on event-driven hardware. The state-of-the-art SNNs suffer from high inference latency, resulting from inefficient input encoding and suboptimal settings of the neuron parameters (firing threshold and membrane leak). We propose DIET-SNN, a low-latency deep spiking network trained with gradient descent to optimize the membrane leak and the firing threshold along with other network parameters (weights). The membrane leak and threshold of each layer are optimized with end-to-end backpropagation to achieve competitive accuracy at reduced latency. The input layer directly processes the analog pixel values of an image without converting it to spike train. The first convolutional layer converts analog inputs into spikes where leaky-integrate-and-fire (LIF) neurons integrate the weighted inputs and generate an output spike when the membrane potential crosses the trained firing threshold. The trained membrane leak selectively attenuates the membrane potential, which increases activation sparsity in the network. The reduced latency combined with high activation sparsity provides massive improvements in computational efficiency. We evaluate DIET-SNN on image classification tasks from CIFAR and ImageNet datasets on VGG and ResNet architectures. We achieve top-1 accuracy of 69% with five timesteps (inference latency) on the ImageNet dataset with 12x less compute energy than an equivalent standard artificial neural network (ANN). In addition, DIET-SNN performs 20-500x faster inference compared to other state-of-the-art SNN models.
C1 [Rathi, Nitin; Roy, Kaushik] Purdue Univ, Sch Elect & Comp Engn, W Lafayette, IN 47907 USA.
RP Rathi, N (corresponding author), Purdue Univ, Sch Elect & Comp Engn, W Lafayette, IN 47907 USA.
EM rathi2@purdue.edu; kaushik@purdue.edu
CR Almomani A, 2019, CLUSTER COMPUT, V22, P419, DOI 10.1007/s10586-018-02891-0
   [Anonymous], 2019, ARXIV190109948
   [Anonymous], 2019, CORR
   Bellec G., 2018, ADV NEURAL INFORM PR, P787
   Bing Han, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13555, DOI 10.1109/CVPR42600.2020.01357
   Bohte S. M., 2000, 8th European Symposium on Artificial Neural Networks. ESANN"2000. Proceedings, P419
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Chakraborty I, 2020, NAT MACH INTELL, V2, P43, DOI 10.1038/s42256-019-0134-0
   Chankyu Lee, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12374), P366, DOI 10.1007/978-3-030-58526-6_22
   Chen Tianqi, 2016, ARXIV160406174
   Chen YH, 2016, CONF PROC INT SYMP C, P367, DOI 10.1109/ISCA.2016.40
   Comsa JM, 2020, INT CONF ACOUST SPEE, P8529, DOI [10.1109/ICASSP40776.2020.9053856, 10.1109/icassp40776.2020.9053856]
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Diehl Peter U, 2015, 2015 INT JOINT C NEU, P1, DOI [DOI 10.1109/IJCNN.2015.7280696, 10.1109/IJCNN.2015.7280696]
   Fang W., 2020, ARXIV200705785
   Frady E. P, 2020, ARXIV200412691
   Freund K, 2019, GOOGLE CLOUD DOUBLES
   Gerstner W., 2002, SPIKING NEURON MODEL
   Han S., 2015, ARXIV151000149
   He YH, 2018, LECT NOTES COMPUT SC, V11211, P815, DOI 10.1007/978-3-030-01234-2_48
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Horowitz M, 2014, ISSCC DIG TECH PAP I, V57, P10, DOI 10.1109/ISSCC.2014.6757323
   Huh D, 2018, ADV NEUR IN, V31
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90
   Kheradpisheh SR, 2020, INT J NEURAL SYST, V30, DOI 10.1142/S0129065720500276
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Li D, 2016, PROCEEDINGS OF 2016 IEEE INTERNATIONAL CONFERENCES ON BIG DATA AND CLOUD COMPUTING (BDCLOUD 2016) SOCIAL COMPUTING AND NETWORKING (SOCIALCOM 2016) SUSTAINABLE COMPUTING AND COMMUNICATIONS (SUSTAINCOM 2016) (BDCLOUD-SOCIALCOM-SUSTAINCOM 2016), P477, DOI 10.1109/BDCloud-SocialCom-SustainCom.2016.76
   Li G., 2019, ARXIV190701167
   Lillicrap TP, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms13276
   Liu, 2020, ARXIV PREPRINT ARXIV
   Lu S, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00535
   MAINEN ZF, 1995, SCIENCE, V268, P1503, DOI 10.1126/science.7770778
   Mostafa H, 2018, IEEE T NEUR NET LEAR, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Painkras E, 2013, IEEE J SOLID-ST CIRC, V48, P1943, DOI 10.1109/JSSC.2013.2259038
   Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32
   Rathi Nitin, 2020, INT C LEARN REPR
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Samadi A, 2017, NEURAL COMPUT, V29, P578, DOI 10.1162/NECO_a_00929
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Shen YM, 2017, ANN IEEE SYM FIELD P, P93, DOI 10.1109/FCCM.2017.47
   Shrestha SB, 2018, ADV NEUR IN, V31
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Taherkhani A, 2018, IEEE T NEUR NET LEAR, V29, P5394, DOI 10.1109/TNNLS.2018.2797801
   Wu YJ, 2019, AAAI CONF ARTIF INTE, P1311
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Yin B, 2020, IEEE INTERNET THINGS, V7, P8748, DOI [10.1109/JIOT.2020.2996562, 10.1145/3407197.3407225]
   Zheng H., 2020, ARXIV201105280
NR 50
TC 42
Z9 42
U1 3
U2 18
PD JUN
PY 2023
VL 34
IS 6
BP 3174
EP 3182
DI 10.1109/TNNLS.2021.3111897
EA SEP 2021
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Computer Science, Theory & Methods; Engineering,
   Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Medini, C
   Vijayan, A
   Zacharia, RM
   Rajagopal, LP
   Nair, B
   Diwakar, S
AF Medini, Chaitanya
   Vijayan, Asha
   Zacharia, Ritu Maria
   Rajagopal, Lekshmi Priya
   Nair, Bipin
   Diwakar, Shyam
BE Mauri, JL
   Thampi, SM
   Wozniak, M
   Marques, O
   Krishnaswamy, D
   Sahni, S
   Callegari, C
   Takagi, H
   Bojkovic, ZS
   Vinod, M
   Prasad, NR
   Calero, JMA
   Rodrigues, J
   Que, XY
   Meghanathan, N
   Sandhu, R
   Au, E
TI Spike Encoding for Pattern Recognition: Comparing Cerebellum Granular
   Layer Encoding and BSA algorithms
SO 2015 INTERNATIONAL CONFERENCE ON ADVANCES IN COMPUTING, COMMUNICATIONS
   AND INFORMATICS (ICACCI)
DT Proceedings Paper
CT International Conference on Advances in Computing, Communications and
   Informatics ICACCI
CY AUG 10-13, 2015
CL SCMS Grp of Inst, Aluva, INDIA
HO SCMS Grp of Inst
DE nature-inspired computing; classification; machine learning; encoding;
   spiking neuron
ID MODEL; INFORMATION
AB Spiking neural encoding models allow classification of real world tasks to suit for brain-machine interfaces in addition to serving as internal models. We developed a new spike encoding model inspired from cerebellum granular layer and tested different classification techniques like SVM, Naive Bayes, MLP for training spiking neural networks to perform pattern recognition tasks on encoded datasets. As a precursor to spiking network-based pattern recognition, in this study, real world datasets were encoded into spike trains. The objective of this study was to encode information from datasets into spiking neuron patterns that were relevant for spiking neural networks and for conventional machine learning algorithms. In this initial study, we present a new approach similar to cerebellum granular layer encoding and compared it with BSA encoding techniques. We have also compared the efficiency of the encoded dataset with different datasets and with standard machine learning algorithms.
C1 [Medini, Chaitanya; Vijayan, Asha; Zacharia, Ritu Maria; Rajagopal, Lekshmi Priya; Nair, Bipin; Diwakar, Shyam] Amrita Univ, Amrita Vishwa Vidyapeetham, Amrita Sch Biotechnol, Clappana PO, Kollam, Kerala, India.
RP Medini, C (corresponding author), Amrita Univ, Amrita Vishwa Vidyapeetham, Amrita Sch Biotechnol, Clappana PO, Kollam, Kerala, India.
EM krishnachaitanya@am.amrita.edu; ashavijayan@am.amrita.edu;
   ritu.mariaz@gmail.com; lekshmipriya310@gmail.com; bipin@amrita.edu;
   shyam@amrita.edu
CR Albus J. S., 1975, J DYN SYST MEAS CONT
   Albus J. S., 1971, THEORY CEREBELLAR FU, V10, P25
   [Anonymous], 2012, MATL SIGN PROC TOOLB
   [Anonymous], 2005, 20 NAT C ART INT 17
   [Anonymous], 2010, THEORY SPIKE TIMING
   Brunel N, 2004, NEURON, V43, P745, DOI 10.1016/j.neuron.2004.08.023
   Carrillo RR, 2008, BIOSYSTEMS, V94, P18, DOI 10.1016/j.biosystems.2008.05.008
   Cerminara NL, 2009, J PHYSIOL-LONDON, V587, P429, DOI 10.1113/jphysiol.2008.163337
   Clopath C, 2012, PLOS COMPUT BIOL, V8, DOI 10.1371/journal.pcbi.1002448
   Diwakar S., 2014, 6 INT C NEUR COMP TH
   Diwakar S, 2009, J NEUROPHYSIOL, V101, P519, DOI 10.1152/jn.90382.2008
   Gao Y, 2003, I IEEE EMBS C NEUR E, P189
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Han J, 2011, DATA MINING CONCEPTS
   Iakymchuk T., 2014, 19 WORLD C INT FED A, P701
   Jin X, 2010, COMPUT SCI ENG, V12, P91, DOI 10.1109/MCSE.2010.112
   Koyama S., 2011, BMC NEUROSCI S1, V12, pP177
   Lichman M., 2013, UCI MACHINE LEARNING
   Lichman M, 2013, P 3 INT C INN COMP T
   MARR D, 1969, J PHYSIOL-LONDON, V202, P437, DOI 10.1113/jphysiol.1969.sp008820
   McKennoch S, 2006, IEEE IJCNN, P3970
   Medini C., 2010, UCI MACHINE LEARNING
   Medini C., 2010, IMPROVED SPIKING NEU
   Medini C., 2013, BRAIN COMPUTER INTER, V2012
   Medini C, 2012, COMPUT INTEL NEUROSC, V2012, DOI 10.1155/2012/359529
   Memmesheimer RM, 2014, NEURON, V82, P925, DOI 10.1016/j.neuron.2014.03.026
   Mitsuo K, 2003, PROG BRAIN RES, V142, P171
   Naud R, 2008, BIOL CYBERN, V99, P335, DOI 10.1007/s00422-008-0264-7
   Nuntalid N., 2011, EVOLVING PROBABILIST, P451
   Nuntalid N, 2011, LECT NOTES COMPUT SC, V7062, P451, DOI 10.1007/978-3-642-24955-6_54
   Purves D., 2004, BIOSYSTEMS
   Rolls ET, 2011, PROG NEUROBIOL, V95, P448, DOI 10.1016/j.pneurobio.2011.08.002
   Schrauwen B, 2003, IEEE IJCNN, P2825
   Schweighofer N, 1998, EUR J NEUROSCI, V10, P95, DOI 10.1046/j.1460-9568.1998.00007.x
   The MathWorks Inc, 2012, TECHNIQUES DESIGNING
   Wolpert DM, 1998, NEURAL NETWORKS, V11, P1317, DOI 10.1016/S0893-6080(98)00066-5
NR 37
TC 4
Z9 4
U1 2
U2 2
PY 2015
BP 1619
EP 1625
WC Computer Science, Theory & Methods; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Villa, AEP
   Asai, Y
   Iglesias, J
   Chibirova, OK
   Cabessa, J
   Dutoit, P
   Shaposhnyk, V
AF Villa, Alessandro E. P.
   Asai, Yoshiyuki
   Iglesias, Javier
   Chibirova, Olga K.
   Cabessa, Jeremie
   Dutoit, Pierre
   Shaposhnyk, Vladyslav
BE Wang, R
   Gu, F
TI Dynamical Systems and Accurate Temporal Information Transmission in
   Neural Networks
SO ADVANCES IN COGNITIVE NEURODYNAMICS (II)
DT Proceedings Paper
CT 2nd International Conference on Cognitive Neurodynamics (ICCN 2009)
CY NOV 15-19, 2009
CL E China Univ Sci & Technol, Hangzhou, PEOPLES R CHINA
HO E China Univ Sci & Technol
ID PATTERNS
AB We simulated the activity of hierarchically organized spiking neural networks characterized by an initial developmental phase featuring cell death followed by spike timing dependent synaptic plasticity in presence of background noise. Upstream networks receiving spatiotemporally organized external inputs projected to downstream networks disconnected from external inputs. The observation of precise firing sequences, formed by recurrent patterns of spikes intervals above chance levels, suggested the build-up of an unsupervised connectivity able to sustain and preserve temporal information processing.
C1 [Villa, Alessandro E. P.] Univ Joseph Fourier, Grenoble Inst Neurosci, Neuroheurist Res Grp, Grenoble, France.
   [Villa, Alessandro E. P.] HUGE, Dept Psychiat, Sleep Res Lab, Geneva, Switzerland.
   [Villa, Alessandro E. P.] Univ Lausanne, Neuroheurist Res Grp, ISI HEC, Lausanne, Switzerland.
RP Villa, AEP (corresponding author), Univ Joseph Fourier, Grenoble Inst Neurosci, Neuroheurist Res Grp, Grenoble, France.
EM Alessandro.Villa@neuroheuristic.org
CR Abeles M., 1991, CORTICONICS NEURAL C
   Asai Y, 2008, NEURAL NETWORKS, V21, P799, DOI 10.1016/j.neunet.2008.06.014
   Asai Y, 2008, J BIOL PHYS, V34, P325, DOI 10.1007/s10867-008-9093-0
   Chechik G, 1999, NEURAL COMPUT, V11, P2061, DOI 10.1162/089976699300016089
   Chibirova O, 2008, LECT NOTES COMPUT SC, V5216, P296
   Iglesias J, 2008, LECT NOTES COMPUT SC, V5164, P646, DOI 10.1007/978-3-540-87559-8_67
   Iglesias J, 2008, INT J NEURAL SYST, V18, P267, DOI 10.1142/S0129065708001580
   Innocenti GM, 2005, NAT REV NEUROSCI, V6, P955, DOI 10.1038/nrn1790
   Roberts PD, 2002, BIOL CYBERN, V87, P392, DOI 10.1007/s00422-002-0361-y
   Segundo JP, 2003, INT J BIFURCAT CHAOS, V13, P2035, DOI 10.1142/S0218127403007886
   SHATZ CJ, 1990, NEURON, V5, P745, DOI 10.1016/0896-6273(90)90333-B
   Tetko IV, 2001, J NEUROSCI METH, V105, P1, DOI 10.1016/S0165-0270(00)00336-8
   Villa AEP, 2000, CONC ADV BRAIN RES, V3, P1
NR 13
TC 0
Z9 0
U1 0
U2 1
PY 2011
BP 61
EP 65
DI 10.1007/978-90-481-9695-1_8
WC Neurosciences
DA 2023-11-11
ER

PT J
AU Fernando, S
   Kumarasinghe, N
AF Fernando, Subha
   Kumarasinghe, Nishantha
TI Modeling honeybee communication using network of spiking neural networks
   to simulate nectar reporting behavior
SO ARTIFICIAL LIFE AND ROBOTICS
DT Article
DE Honeybee foraging; Spiking neurons; Swarm cognition; Swarm intelligence
ID TREMBLE DANCE; BEES; NEURONS
AB The paper presents the findings of the research that attempted to mathematically model the cognitive behavior that could arise due to the interaction between honeybees in a colony during forager recruitment process. The model defines a honeybee as a spiking neural network, and colony as a network of spiking neural networks. The proposed mathematical model has been evaluated by analyzing the cognitive behavior generated by the main network which represents honeybees' interaction as interactions of component networks (i.e. spiking neural networks). Accordingly, behavior of the component network, that represents an unemployed forager in the colony, was examined under different scenarios by setting networks' parameters to simulate ecological situations in the colony. The reporting of different level of quantity of nectar sources by scouts to the colony, an attempt made by a scout to attract more unemployed foragers for foraging, and influence by dancing foragers to attract other unemployed foragers for foraging are those ecological colony states that have been tested in this research. The results of all these cases have supported that the proposed mathematical model can sufficiently simulate the unemployed forager's behavior during recruitment process.
C1 [Fernando, Subha] Univ Moratuwa, Dept Computat Math, Moratuwa, Sri Lanka.
   [Kumarasinghe, Nishantha] Gen Sir John Kotelawala Def Univ, Ctr Behav Neurosci & Computat, Colombo, Sri Lanka.
RP Fernando, S (corresponding author), Univ Moratuwa, Dept Computat Math, Moratuwa, Sri Lanka.
EM subhaf@uom.lk; drkumarasinghe2015@yahoo.com
CR Abbott LF, 2000, NAT NEUROSCI, V3, P1178, DOI 10.1038/81453
   Ahmed H., 2012, 2012585 QUEENS U SCH, VK7L3N6
   [Anonymous], 1974, SOCIAL BEHAV BEES
   Bailis P, 2010, LECT NOTES COMPUTER, V6234
   Blum C, 2005, PHYS LIFE REV, V2, P353, DOI 10.1016/j.plrev.2005.10.001
   Bressan JMA, 2015, FRONT NEUROANAT, V8, DOI 10.3389/fnana.2014.00166
   Fernando S, 2014, INT J COMPUT APPL, V104, P45
   Fernando S, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P1336, DOI 10.1109/IJCNN.2011.6033379
   Gil Mariana, 2010, Commun Integr Biol, V3, P95
   Izhikevich E. M., 2007, DYNAMICAL SYSTEMS NE
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Karaboga D, 2009, ARTIF INTELL REV, V31, P61, DOI 10.1007/s10462-009-9127-4
   Krink T, SWARM INTELLIGENCE I
   Kumarasinghe, 2015, INT J COMPUT APPL, V130, P33
   Myerscough MR, 2003, P ROY SOC B-BIOL SCI, V270, P577, DOI 10.1098/rspb.2002.2293
   Passino KM, 2006, BEHAV ECOL SOCIOBIOL, V59, P427, DOI 10.1007/s00265-005-0067-y
   Perk CG, 2006, J NEUROPHYSIOL, V95, P1147, DOI 10.1152/jn.01220.2004
   Quintavalle A, 2013, BIOSCIENCES MASTER R, P1
   Seeley TD, 2000, J COMP PHYSIOL A, V186, P813, DOI 10.1007/s003590000134
   SEELEY TD, 1992, BEHAV ECOL SOCIOBIOL, V31, P375, DOI 10.1007/BF00170604
   Sinakevitch I, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0014536
   Tautz J, 2003, P NATL ACAD SCI USA, V100, P7343, DOI 10.1073/pnas.1232346100
   Thom C, 2003, J EXP BIOL, V206, P2111, DOI 10.1242/jeb.00398
   Trianni V., 2011, ADV ARTIFICIAL LIFE, P270
   Trianni V, 2011, SWARM INTELL-US, V5, P3, DOI 10.1007/s11721-010-0050-8
   Yahya H, 2007, MIRACLE HONEYBEE
NR 26
TC 0
Z9 0
U1 1
U2 3
PD JUN
PY 2018
VL 23
IS 2
BP 241
EP 248
DI 10.1007/s10015-017-0418-6
WC Robotics
DA 2023-11-11
ER

PT J
AU Liu, X
   Liu, HJ
   Tang, YG
   Gao, Q
AF Liu, Xian
   Liu, Huijun
   Tang, Yinggan
   Gao, Qing
TI Fuzzy PID control of epileptiform spikes in a neural mass model
SO NONLINEAR DYNAMICS
DT Article
DE Neural mass model; Epileptiform spikes; Fuzzy PID control
ID DEEP BRAIN-STIMULATION; MATHEMATICAL-MODEL; DYNAMICS; EPILEPSY; EEG;
   MECHANISMS; RESPONSES; MEG/EEG
AB In this paper, the problem of controlling epileptiform spikes in a neural mass model is addressed. Considering the complication and nonlinearity of the neural mass model, a fuzzy PID controller is designed so that epileptiform spikes are quenched and the output waveform tracks an expected one. The tracking effect is analyzed by numerical simulation for a regular network of coupled neural populations. The effect of important model parameters on the control energy and the effect of the types of controlled populations on the ability to realize the tracking purpose are analyzed for the same network.
C1 [Liu, Xian; Liu, Huijun; Tang, Yinggan; Gao, Qing] Yanshan Univ, Key Lab Ind Comp Control Engn Hebei Prov, Inst Elect Engn, Qinhuangdao 066004, Peoples R China.
RP Liu, X (corresponding author), Yanshan Univ, Key Lab Ind Comp Control Engn Hebei Prov, Inst Elect Engn, Qinhuangdao 066004, Peoples R China.
EM liuxian@ysu.edu.cn
CR [Anonymous], FUZZY CONTROLLER DES
   [Anonymous], 1981, ELECT FIELDS BRAIN
   Benabid AL, 2007, EXPERT REV MED DEVIC, V4, P895, DOI 10.1586/17434440.4.6.895
   Çetin S, 2010, NONLINEAR DYNAM, V61, P465, DOI 10.1007/s11071-010-9662-1
   Cona F, 2011, NEUROIMAGE, V57, P1045, DOI 10.1016/j.neuroimage.2011.05.007
   David O, 2003, NEUROIMAGE, V20, P1743, DOI 10.1016/j.neuroimage.2003.07.015
   David O, 2006, NEUROIMAGE, V31, P1580, DOI 10.1016/j.neuroimage.2006.02.034
   David O, 2006, NEUROIMAGE, V30, P1255, DOI 10.1016/j.neuroimage.2005.10.045
   Ding L, 2010, NONLINEAR DYNAM, V60, P131, DOI 10.1007/s11071-009-9585-x
   Goodfellow M, 2012, NEUROIMAGE, V59, P2644, DOI 10.1016/j.neuroimage.2011.08.060
   Goodfellow M, 2011, NEUROIMAGE, V55, P920, DOI 10.1016/j.neuroimage.2010.12.074
   Guclu R, 2009, NONLINEAR DYNAM, V58, P553, DOI 10.1007/s11071-009-9500-5
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Ivancevic T, 2009, NONLINEAR DYNAM, V56, P23, DOI 10.1007/s11071-008-9376-9
   JANSEN BH, 1993, BIOL CYBERN, V68, P275, DOI 10.1007/BF00224863
   JANSEN BH, 1995, BIOL CYBERN, V73, P357, DOI 10.1007/BF00199471
   Khosravi S, 2012, NONLINEAR DYNAM, V69, P1825, DOI 10.1007/s11071-012-0389-z
   Krauss GL, 2007, ACTA NEUROCHIR SUPPL, V97, P347
   Lin CCK, 2012, J NEURAL ENG, V9, DOI 10.1088/1741-2560/9/2/026026
   Lopes da Silva F H, 1976, Prog Brain Res, V45, P281
   Lytton WW, 2008, NAT REV NEUROSCI, V9, P626, DOI 10.1038/nrn2416
   Passim K M, 2001, FUZZY CONTROL
   Pollo C, 2007, ACTA NEUROCHIR SUPPL, V97, P311
   Ren CE, 2012, NONLINEAR DYNAM, V67, P941, DOI 10.1007/s11071-011-0036-0
   Rubchinsky LL, 2012, NONLINEAR DYNAM, V68, P329, DOI 10.1007/s11071-011-0223-z
   Sen Bhattacharya B, 2011, NEURAL NETWORKS, V24, P631, DOI 10.1016/j.neunet.2011.02.009
   Shirahata T, 2011, ACTA BIOL HUNG, V62, P211, DOI 10.1556/ABiol.62.2011.2.11
   Sunderam S, 2010, EPILEPSY BEHAV, V17, P6, DOI 10.1016/j.yebeh.2009.10.017
   van Albada SJ, 2009, J THEOR BIOL, V257, P664, DOI 10.1016/j.jtbi.2008.12.013
   van Albada SJ, 2009, J THEOR BIOL, V257, P642, DOI 10.1016/j.jtbi.2008.12.018
   Wendling F, 2000, BIOL CYBERN, V83, P367, DOI 10.1007/s004220000160
   Zavaglia M., 2010, COMPUT INTELL NEUROS, V10, P1155
NR 32
TC 23
Z9 24
U1 5
U2 37
PD JAN
PY 2013
VL 71
IS 1-2
BP 13
EP 23
DI 10.1007/s11071-012-0638-1
WC Engineering, Mechanical; Mechanics
DA 2023-11-11
ER

PT C
AU Kulkarni, SR
   Babu, AV
   Rajendran, B
AF Kulkarni, Shruti R.
   Babu, Anakha V.
   Rajendran, Bipin
GP IEEE
TI Spiking Neural Networks - Algorithms, Hardware Implementations and
   Applications
SO 2017 IEEE 60TH INTERNATIONAL MIDWEST SYMPOSIUM ON CIRCUITS AND SYSTEMS
   (MWSCAS)
SE Midwest Symposium on Circuits and Systems Conference Proceedings
DT Proceedings Paper
CT 60th IEEE International Midwest Symposium on Circuits and Systems
   (MWSCAS)
CY AUG 06-09, 2017
CL Tufts Univ, Medford Somerville Campus, Boston, MA
HO Tufts Univ, Medford Somerville Campus
ID NEURONS; DEVICE; CIRCUIT; SYNAPSE; RESUME; MODEL
AB Spiking Neural Networks (SNNs) are the third generation of artificial neural networks that closely mimic the time encoding and information processing aspects of the human brain. It has been postulated that these networks are more efficient for realizing cognitive computing systems compared to second generation networks that are widely used in machine learning algorithms today. In this paper, we review the learning algorithms, hardware demonstrations and potential applications of SNN based learning systems.
C1 [Kulkarni, Shruti R.; Babu, Anakha V.; Rajendran, Bipin] New Jersey Inst Technol, Dept Elect & Comp Engn, Newark, NJ 07102 USA.
RP Kulkarni, SR (corresponding author), New Jersey Inst Technol, Dept Elect & Comp Engn, Newark, NJ 07102 USA.
EM srk68@njit.edu; av442@njit.edu; bipin@njit.edu
CR Abbott LF, 1999, BRAIN RES BULL, V50, P303, DOI 10.1016/S0361-9230(99)00161-6
   Alibart F, 2012, ADV FUNCT MATER, V22, P609, DOI 10.1002/adfm.201101935
   Allred JM, 2016, IEEE IJCNN, P2492, DOI 10.1109/IJCNN.2016.7727509
   Ambrogio S, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00056
   [Anonymous], EL DEV M IEDM 2014 I
   Anwani N., 2015, NEUR NETW IJCNN 2015
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Boahen K, 2005, SCI AM, V292, P56, DOI 10.1038/scientificamerican0505-56
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Booij O, 2005, INFORM PROCESS LETT, V95, P552, DOI 10.1016/j.ipl.2005.05.023
   Bora Ashish, 2014, 2014 International Joint Conference on Neural Networks (IJCNN), P2079, DOI 10.1109/IJCNN.2014.6889892
   Brette R., 2012, SIMULATING SPIKING N
   Burr GW, 2008, IBM J RES DEV, V52, P449, DOI 10.1147/rd.524.0449
   Burr GW, 2017, ADV PHYS-X, V2, P89, DOI 10.1080/23746149.2016.1259585
   Covi E, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00482
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Diehl Peter U, 2015, 2015 INT JOINT C NEU, P1, DOI [DOI 10.1109/IJCNN.2015.7280696, 10.1109/IJCNN.2015.7280696]
   Fidjeland Andreas K, 2010, 2010 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2010.5596678
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   Ge CJ, 2017, INFORM SCIENCES, V399, P30, DOI 10.1016/j.ins.2017.03.006
   Gehlhaar J, 2014, ACM SIGPLAN NOTICES, V49, P317, DOI 10.1145/2541940.2564710
   Hebb D. O., 2005, THE ORGANIZATION OF
   Hunsberger E., 2016, ARXIV161105141
   Hwu T., 2017, IEEE T COGN DEV SYST, VPP, P1, DOI DOI 10.1109/ISCAS.2017.8050981
   Indiveri Giacomo, 2011, Front Neurosci, V5, P118, DOI 10.3389/fnins.2011.00118
   Jackson BL, 2013, ACM J EMERG TECH COM, V9, DOI 10.1145/2463585.2463588
   Jerry Matthew, 2016, 2016 74th Annual Device Research Conference (DRC), P1, DOI 10.1109/DRC.2016.7548503
   Jo SH, 2010, NANO LETT, V10, P1297, DOI 10.1021/nl904092h
   Kasabov NK, 2014, NEURAL NETWORKS, V52, P62, DOI 10.1016/j.neunet.2014.01.006
   Kibong Moon, 2016, 2016 International Symposium on VLSI Technology, Systems and Application (VLSI-TSA), P1, DOI 10.1109/VLSI-TSA.2016.7480499
   Kim S, 2011, 2011 11TH INTERNATIONAL CONFERENCE ON CONTROL, AUTOMATION AND SYSTEMS (ICCAS), P1
   Kraft F. M., 2006, P EPFL LATSIS S 2006, P97
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
   Kulkarni S., 2017, SUPERVISED LEARNING
   Kuzum D, 2012, NANO LETT, V12, P2179, DOI 10.1021/nl201040y
   Lazar A. A., 2005, PROCEEDINGS OF CONFE
   Lee JM, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00191
   Lee WW, 2017, IEEE T NEUR NET LEAR, V28, P849, DOI 10.1109/TNNLS.2015.2509479
   Lin J, 2016, INT EL DEVICES MEET
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Mandal S, 2014, SCI REP-UK, V4, DOI 10.1038/srep05333
   McKennoch S, 2006, IEEE IJCNN, P3970
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Mohemmed A, 2012, INT J NEURAL SYST, V22, DOI 10.1142/S0129065712500128
   Nageswaran JM, 2009, NEURAL NETWORKS, V22, P791, DOI 10.1016/j.neunet.2009.06.028
   Ostwal V., 2015, 2015 International Symposium on VLSI Technology, Systems and Applications (VLSI-TSA), P1, DOI 10.1109/VLSI-TSA.2015.7117569
   Panwar N, 2014, IEEE DEVICE RES CONF, P135, DOI 10.1109/DRC.2014.6872334
   Panzeri S, 2010, TRENDS NEUROSCI, V33, P111, DOI 10.1016/j.tins.2009.12.001
   Patterson D, 1997, IEEE MICRO, V17, P34, DOI 10.1109/40.592312
   Ponulak F., 2006, PROC EPFL LATSIS S D, P119
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Prasad C., 2015, NEUR NETW IJCNN 2015
   Purves D., 2001, NEUROSCIENCE
   Qiao N, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00141
   Rajendran B, 2007, IEEE T ELECTRON DEV, V54, P707, DOI 10.1109/TED.2007.891300
   Rajesh Manjesh B, 2016, 2016 INT C CIRCUIT P, P1, DOI DOI 10.1109/ICCPCT.2016.7530116
   Reid David, 2014, PLoS One, V9, pe103656, DOI 10.1371/journal.pone.0103656
   Rueckauer B., 2016, ARXIV161204052
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707
   Shetty C, 2015, IEEE INT SYMP CIRC S, P1905, DOI 10.1109/ISCAS.2015.7169038
   Suri M, 2011, 2011 IEEE INTERNATIONAL ELECTRON DEVICES MEETING (IEDM)
   Taherkhani A, 2015, IEEE T NEUR NET LEAR, V26, P3137, DOI 10.1109/TNNLS.2015.2404938
   Tandon P., 2016, NEURAL INFORM PROCES
   Tuma T, 2016, NAT NANOTECHNOL, V11, P693, DOI [10.1038/nnano.2016.70, 10.1038/NNANO.2016.70]
   VONNEUMANN J, 1993, IEEE ANN HIST COMPUT, V15, P28
   Wu, 2016, ARXIV160908144
   Yu QF, 2013, PLOS ONE, V8, DOI [10.1371/journal.pone.0059627, 10.1371/journal.pone.0078318]
   Yu SM, 2011, IEEE T ELECTRON DEV, V58, P2729, DOI 10.1109/TED.2011.2147791
   Zhang C., 2015, P 2015 ACM SIGDA INT, P161, DOI 10.1145/2684746.2689060
NR 70
TC 10
Z9 10
U1 0
U2 7
PY 2017
BP 426
EP 431
WC Engineering, Electrical & Electronic; Telecommunications
DA 2023-11-11
ER

PT J
AU Zhu, GL
   Hua, X
   Yu, GJ
   Chai, ZL
AF Zhu, Guoliang
   Hua, Xia
   Yu, Gongjian
   Chai, Zhilei
TI SNN Simulation Performance Prediction: A Nonempirical Method
SO JOURNAL OF CIRCUITS SYSTEMS AND COMPUTERS
DT Article
DE Nest simulator; spiking neural networks; performance model
ID PROCESSOR; NETWORKS; MODELS
AB As a third generation artificial neural network, spiking neuron network is expected to expand the artificial intelligence world. However, as a more detailed simulation of brain, a single run of spiking neural networks (SNNs) simulation can take hours to days. To get a better prediction of SNN simulation performance, existing work requires gathering result of actual runs to conduct accurate modeling. In this paper, we propose a nonempirical SNN simulation performance prediction method, prototyped in a hybrid CPU-FPGA cluster. Experiments show that our method, without actual simulation run, can get comparable accuracy with orders of magnitude less runtime cost.
C1 [Zhu, Guoliang; Hua, Xia; Yu, Gongjian; Chai, Zhilei] Jiangnan Univ, Wuxi 214002, Jiangsu, Peoples R China.
RP Chai, ZL (corresponding author), Jiangnan Univ, Wuxi 214002, Jiangsu, Peoples R China.
EM zlchai@jiangnan.edu.cn
CR Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   Balaji A., ARXIV200909298
   Balaji A, 2019, IEEE COMPUT ARCHIT L, V18, P149, DOI 10.1109/LCA.2019.2951507
   Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Brain, BRAIN SNN SIMULATOR
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Falsafi B., 1997, ACM Transactions on Modeling and Computer Simulation, V7, P104, DOI 10.1145/244804.244808
   Furber SB, 2013, IEEE T COMPUT, V62, P2454, DOI 10.1109/TC.2012.142
   GENESIS, GENESIS SNN SIMULATO
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   Heath T., 2005, P 10 ACM SIGPLAN S P, P186, DOI [DOI 10.1145/1065944.1065969.(AVAILABLE, 10.1145/1065944.1065969]
   Helias M, 2012, FRONT NEUROINFORM, V6, DOI 10.3389/fninf.2012.00026
   igi, CSIM SNN SIMULATOR
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Kim S., ARXIV190306530
   Kunkel S, 2012, FRONT NEUROINFORM, V5, DOI 10.3389/fninf.2011.00035
   Lee D, 2018, CONF PROC INT SYMP C, P275, DOI 10.1109/ISCA.2018.00032
   Liu J., 2009, IEEE CIRC SYST INT C, P1, DOI DOI 10.1109/CAS-ICTD.2009.4960772
   Ma D, 2017, J SYST ARCHITECT, V77, P43, DOI 10.1016/j.sysarc.2017.01.003
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Moore SW, 2012, ANN IEEE SYM FIELD P, P133, DOI 10.1109/FCCM.2012.32
   Moradi S, 2018, IEEE T BIOMED CIRC S, V12, P106, DOI 10.1109/TBCAS.2017.2759700
   Morrison A, 2007, NEURAL COMPUT, V19, P1437, DOI 10.1162/neco.2007.19.6.1437
   Nandakumar SR, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-64878-5
   nba, SNNAP SNN SIMULATOR
   Nest, NEST SNN SIMULATOR
   Neuron, NEURON SNN SIMULATOR
   Pakkenberg B, 2003, EXP GERONTOL, V38, P95, DOI 10.1016/S0531-5565(02)00151-1
   Pei J, 2019, NATURE, V572, P106, DOI 10.1038/s41586-019-1424-8
   Purves D., 2008, COGN NEUROSCI-UK
   Purves D., 2008, NEUROSCIENCE
   Qiao H, 2003, IEEE T SYST MAN CY B, V33, P925, DOI 10.1109/TSMCB.2002.804368
   Qin XL, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON PROGNOSTICS AND HEALTH MANAGEMENT (ICPHM), P1, DOI [10.1109/ATNAC.2017.8215431, 10.1109/ICPHM.2017.7998297]
   Riesenhuber M, 1999, NAT NEUROSCI, V2, P1019, DOI 10.1038/14819
   Rudolph-Lilith M, 2012, NEURAL COMPUT, V24, P1426, DOI 10.1162/NECO_a_00278
   Schenck W., 2014, SC14 C SUP NEW ORL L
   Schuller I.K., 2015, NEUROMORPHIC COMPUTI
   She XY, 2019, DES AUT TEST EUROPE, P450, DOI [10.23919/DATE.2019.8714846, 10.23919/date.2019.8714846]
   Shrestha S. B., ARXIV181008646
   Smith, 2019, KEYNOTE FCRC
   Song SH, 2020, 21ST ACM SIGPLAN/SIGBED CONFERENCE ON LANGUAGES, COMPILERS, AND TOOLS FOR EMBEDDED SYSTEMS (LCTES '20), P38, DOI 10.1145/3372799.3394364
   Tikidji-Hamburyan RA, 2017, FRONT NEUROINFORM, V11, DOI 10.3389/fninf.2017.00046
   van Albada SJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00291
   Wu JC, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351221
   Wu JB, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00836
   Yudanov D., 2010, P INT JOINT C NEUR N, P1, DOI [10.1109/IJCNN.2010.5596334, DOI 10.1109/IJCNN.2010.5596334]
NR 46
TC 0
Z9 0
U1 0
U2 4
PD JUL 15
PY 2022
VL 31
IS 10
AR 2250183
DI 10.1142/S0218126622501833
WC Computer Science, Hardware & Architecture; Engineering, Electrical &
   Electronic
DA 2023-11-11
ER

PT J
AU Han, LX
   Huang, P
   Wang, YJ
   Zhou, Z
   Zhang, YZ
   Liu, XY
   Kang, JF
AF Han, Lixia
   Huang, Peng
   Wang, Yijiao
   Zhou, Zheng
   Zhang, Yizhou
   Liu, Xiaoyan
   Kang, Jinfeng
TI Efficient Discrete Temporal Coding Spike-Driven In-Memory Computing
   Macro for Deep Neural Network Based on Nonvolatile Memory
SO IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS I-REGULAR PAPERS
DT Article
DE Encoding; In-memory computing; Capacitors; Biological neural networks;
   Neurons; Delays; Nonvolatile memory; In-memory computing; deep neural
   network; non-volatile memory; discrete temporal coding; spike-driven
   macro
ID CHIP
AB Nonvolatile memory (NVM) based neural network can directly perform in situ computation in memory to significantly reduce energy consumption resulting from the data movement. However, the energy consumption by the analog-to-digital converter (ADC) restricts the efficiency of the mixed-signal in-memory computing macro. The rate coding spike-driven in-memory computing macro can increase the energy efficiency via eliminating the ADC, but the improvement is limited because substantial energy is consumed for the coding of multiple spikes. In this work, we propose a discrete temporal coding spike-driven in-memory computing macro, including input coding scheme, weight mapping method, and improved leaky integrate-and-fire (LIF) neuron circuit, to perform the efficient forward inference of deep neural networks based on NVM array. We then optimize the designment of the proposed in-memory computing macro to mitigate the neural network accuracy loss due to the nonlinearity of the LIF neuron and voltage drop caused by interconnect resistance. Because the temporal coding scheme reduces spike numbers and the improved-LIF circuit simultaneously integrates two bit-lines current corresponding to positive and negative weight, the proposed macro achieves 46.63TOPS/W energy efficiency and 1.92TOPS throughput for 3bit temporal coding precision.
C1 [Han, Lixia; Huang, Peng; Zhou, Zheng; Zhang, Yizhou; Liu, Xiaoyan; Kang, Jinfeng] Peking Univ, Sch Integrated Circuits, Beijing 100871, Peoples R China.
   [Wang, Yijiao] Beihang Univ, Fert Beijing Inst, Sch Integrated Circuit Sci & Engn, MIIT Key Lab Spintron, Beijing 100191, Peoples R China.
RP Huang, P (corresponding author), Peking Univ, Sch Integrated Circuits, Beijing 100871, Peoples R China.; Wang, YJ (corresponding author), Beihang Univ, Fert Beijing Inst, Sch Integrated Circuit Sci & Engn, MIIT Key Lab Spintron, Beijing 100191, Peoples R China.
EM phwang@pku.edu.cn; yijiaowang@buaa.edu.cn
CR Ambrogio S, 2019, INT EL DEVICES MEET, DOI 10.1109/iedm19573.2019.8993482
   Bo Li, 2015, 2015 Asia-Pacific Microwave Conference (APMC). Proceedings, P1, DOI 10.1109/APMC.2015.7411621
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Cai H, 2022, IEEE T CIRCUITS-I, V69, P1519, DOI 10.1109/TCSI.2022.3140769
   Cai W., 2022, SCI CHINA INFORM SCI, V65, P1
   Canziani A., 2016, ARXIV
   Chen LR, 2017, DES AUT TEST EUROPE, P19, DOI 10.23919/DATE.2017.7926952
   Chen YH, 2016, ISSCC DIG TECH PAP I, V59, P262, DOI 10.1109/ISSCC.2016.7418007
   Cheng-Xin Xue, 2020, 2020 IEEE International Solid- State Circuits Conference - (ISSCC), P244, DOI 10.1109/ISSCC19947.2020.9063078
   Emer, 2017, PROC VLSI SHORT COUR, P1
   Giacomin E, 2019, IEEE T CIRCUITS-I, V66, P643, DOI 10.1109/TCSI.2018.2872455
   Gokmen T, 2019, INT EL DEVICES MEET, DOI 10.1109/iedm19573.2019.8993573
   Guo ZX, 2021, P IEEE, V109, P1398, DOI 10.1109/JPROC.2021.3084997
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Han LX, 2021, INT RELIAB PHY SYM, DOI 10.1109/IRPS46558.2021.9405200
   Han RZ, 2019, IEEE T CIRCUITS-I, V66, P1692, DOI 10.1109/TCSI.2018.2885574
   Hao Y, 2021, SCI CHINA INFORM SCI, V64, DOI 10.1007/s11432-021-3235-7
   Horowitz M, 2014, ISSCC DIG TECH PAP I, V57, P10, DOI 10.1109/ISSCC.2014.6757323
   Huang YX, 2022, IEEE T CIRCUITS-I, V69, P518, DOI 10.1109/TCSI.2021.3124553
   JIANG H, 2018, IEEE INT SYMP CIRC S
   Joshi V, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-16108-9
   Jouppi NP, 2017, 44TH ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA 2017), P1, DOI 10.1145/3079856.3080246
   Kim W, 2019, S VLSI TECH, pT66, DOI [10.23919/VLSIT.2019.8776551, 10.23919/vlsit.2019.8776551]
   Li ZQ, 2020, INT J GEOGR INF SCI, V34, P1378, DOI 10.1080/13658816.2020.1720692
   Liu Q, 2020, ISSCC DIG TECH PAP I, P500, DOI 10.1109/ISSCC19947.2020.9062953
   Luo J, 2019, INT EL DEVICES MEET, DOI 10.1109/iedm19573.2019.8993535
   Mackin C, 2019, ADV ELECTRON MATER, V5, DOI 10.1002/aelm.201900026
   Merrikh-Bayat F, 2018, IEEE T NEUR NET LEAR, V29, P4782, DOI 10.1109/TNNLS.2017.2778940
   Mochida R, 2018, 2018 IEEE SYMPOSIUM ON VLSI TECHNOLOGY, P175, DOI 10.1109/VLSIT.2018.8510676
   Moons B, 2017, ISSCC DIG TECH PAP I, P246, DOI 10.1109/ISSCC.2017.7870353
   Park S, 2020, DES AUT CON, DOI [10.1109/dac18072.2020.9218689, 10.1007/s00779-020-01476-2]
   Si X, 2019, IEEE T CIRCUITS-I, V66, P4172, DOI 10.1109/TCSI.2019.2928043
   Soliman T, 2020, INT EL DEVICES MEET, P29, DOI DOI 10.1109/IEDM13553.2020.9372124
   Tao TM, 2021, IEEE T CIRCUITS-I, V68, P1906, DOI 10.1109/TCSI.2021.3060798
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   Waldrop MM, 2016, NATURE, V530, P144, DOI 10.1038/530144a
   Xiang Y., 2019, IEDM, P38, DOI [10.1109/IEDM19573.2019.8993508, DOI 10.1109/IEDM19573.2019.8993508]
   Xiang YC, 2020, IEEE T ELECTRON DEV, V67, P2329, DOI 10.1109/TED.2020.2987439
   Xue CX, 2021, ISSCC DIG TECH PAP I, V64, P246, DOI 10.1109/ISSCC42613.2021.9365769
   Yan BN, 2019, S VLSI TECH, pT86, DOI [10.23919/vlsit.2019.8776485, 10.23919/VLSIT.2019.8776485]
   Yao P, 2020, NATURE, V577, P641, DOI 10.1038/s41586-020-1942-4
   Yin SH, 2020, IEEE J SOLID-ST CIRC, V55, P1733, DOI 10.1109/JSSC.2019.2963616
   Yu S., 2015, IEDM, P451, DOI 10.1109/IEDM.2015.7409718
   Yu SM, 2018, P IEEE, V106, P260, DOI 10.1109/JPROC.2018.2790840
   Zhang L, 2019, AAAI CONF ARTIF INTE, P1319
   Zhang SH, 2020, DES AUT TEST EUROPE, P1426, DOI 10.23919/DATE48585.2020.9116323
   Zhang XM, 2020, INT EL DEVICES MEET, DOI 10.1109/IEDM13553.2020.9371937
NR 47
TC 1
Z9 1
U1 9
U2 27
PD NOV
PY 2022
VL 69
IS 11
BP 4487
EP 4498
DI 10.1109/TCSI.2022.3194918
EA AUG 2022
WC Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Vaila, R
   Chiasson, J
   Saxena, V
AF Vaila, Ruthvik
   Chiasson, John
   Saxena, Vishal
TI A Deep Unsupervised Feature Learning Spiking Neural Network With
   Binarized Classification Layers for the EMNIST Classification
SO IEEE TRANSACTIONS ON EMERGING TOPICS IN COMPUTATIONAL INTELLIGENCE
DT Article
DE Neurons; Feature extraction; Backpropagation; Biological neural
   networks; Synapses; Encoding; Membrane potentials; STDP; Spiking
   Networks; Surrogate Gradients; EMNIST; Binary Activations; Reduced
   Multiplications
AB End user AI is trained on large server farms with data collected from the users. With ever increasing demand for IoT devices, there is a need for deep learning approaches that can be implemented at the Edge in an energy efficient manner. In this work we approach this using spiking neural networks. The unsupervised learning technique of spike timing dependent plasticity (STDP) and binary activations are used to extract features from spiking input data. Gradient descent (backpropagation) is used only on the output layer to perform training for classification. The accuracies obtained for the balanced EMNIST data set compare favorably with other approaches. The effect of the stochastic gradient descent (SGD) approximations on learning capabilities of our network are also explored.
C1 [Vaila, Ruthvik] Boise State Univ, Dept Elect & Comp Engn, Boise, ID 83706 USA.
   [Chiasson, John] Boise State Univ, ECE, Boise, ID 83706 USA.
   [Saxena, Vishal] Univ Delaware, ECE, Newark, DE 19716 USA.
RP Vaila, R (corresponding author), Boise State Univ, Dept Elect & Comp Engn, Boise, ID 83706 USA.
EM ruthvikvaila@u.boisestate.edu; JohnChiasson@boisestate.edu;
   vsaxena@udel.edu
CR [Anonymous], 2015, ARXIV PREPRINT ARXIV
   Anwani N., 2015, P INT JOINT C NEUR N, P1, DOI DOI 10.1109/IJCNN.2015
   Baldominos A, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9153169
   Bekolay T, 2014, FRONT NEUROINFORM, V7, DOI 10.3389/fninf.2013.00048
   Bichler O., 2019, IEEE IJCNN
   Burges, 2010, MNIST HANDWRITTEN DI, DOI DOI 10.1561/2400000035
   Cohen G, 2017, IEEE IJCNN, P2921, DOI 10.1109/IJCNN.2017.7966217
   Comsa I. M., 2019, ARXIV190713223
   Conradt Jorg, 2009, 2009 IEEE 12th International Conference on Computer Vision Workshops, ICCV Workshops, P780, DOI 10.1109/ICCVW.2009.5457625
   Delorme A, 2001, NEUROCOMPUTING, V38, P539, DOI 10.1016/S0925-2312(01)00403-9
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Frenkel C., 2019, LEARNING FEEDBACK DI
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   Gardner B, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0161335
   GROSSBERG S, 1987, COGNITIVE SCI, V11, P23, DOI 10.1111/j.1551-6708.1987.tb00862.x
   Hines M., 2013, NEURON SIMUL ENV, P1
   Hubara I, 2018, J MACH LEARN RES, V18
   Jaeger Dieter, ENCY COMPUTATIONAL N, DOI [10.1007/978-1-4614-7320-6_795-1, DOI 10.1007/978-1-4614-7320-6_795-1]
   Jiao X, 2018, DES AUT TEST EUROPE, P1223, DOI 10.23919/DATE.2018.8342202
   Jin Yingyezhe, 2018, P 32 INT C NEUR INF
   Kheradpisheh S. R, COMMUNICATION
   Kheradpisheh S. Reza, 2019, ARXIV191009495
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Kheradpisheh SR, 2016, NEUROCOMPUTING, V205, P382, DOI 10.1016/j.neucom.2016.04.029
   Kiselev M, 2016, IEEE IJCNN, P1355, DOI 10.1109/IJCNN.2016.7727355
   Krizhevsky A., 2012, THE CIFAR 10 DATASET
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee C, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00435
   Lee EH, 2017, INT CONF ACOUST SPEE, P5900, DOI 10.1109/ICASSP.2017.7953288
   Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508
   Lillicrap TP, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms13276
   Liu JQ, 2018, 2018 51ST ANNUAL IEEE/ACM INTERNATIONAL SYMPOSIUM ON MICROARCHITECTURE (MICRO), P655, DOI [10.1109/MICR0.2018.00059, 10.1109/MICRO.2018.00059]
   Masquelier T, 2017, THESIS U TOULOUSE 3
   Masquelier T, 2008, PLOS ONE, V3, DOI 10.1371/journal.pone.0001377
   Mostafa Hesham, 2018, IEEE Trans Neural Netw Learn Syst, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Mozafari M, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00625
   Neftci EO, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00324
   Nielsen M. A, 2015, NEURAL NETW DEEP LEA
   Panda P., 2019, SCALABLE EFFICIENT A
   Posch C, 2010, IEEE INT SYMP CIRC S, P1392, DOI 10.1109/ISCAS.2010.5537265
   Rocke A., 2017, PAULISPACE JUN
   Rouhani BD, 2016, I SYMPOS LOW POWER E, P112, DOI 10.1145/2934583.2934599
   Rueckauer B., 2016, ARXIV161204052
   Sakemi Y, 2020, SUPERVISED LEARNING
   Saxena Vishal, 2018, Journal of Low Power Electronics and Applications, V8, DOI 10.3390/jlpea8040034
   Shawon A., 2018, 2018 INT C BANGL, V2018, P1, DOI 10.1109/ICBSLP.2018.8554900
   Tavanaei A, 2018, 2018 INT JOINT C NEU, DOI DOI 10.1109/IJCNN.2018.8489104
   Vaila R., 2019, NICE WORKSHOP SERIES
   Vaila R., 2020, ARXIV200211843
   Vaila R., 2020, ARXIV200211044
   Vaila R., 2019, ARXIV PREPRINT ARXIV
   Vaila R., 2019, P INT C NEUR SYST, P1
   Whittington JCR, 2019, TRENDS COGN SCI, V23, P235, DOI 10.1016/j.tics.2018.12.005
   Wu XY, 2019, IEEE T NANOTECHNOL, V18, P149, DOI 10.1109/TNANO.2018.2871680
   Wu XY, 2015, IEEE T CIRCUITS-II, V62, P1088, DOI 10.1109/TCSII.2015.2456372
   Wu XY, 2015, IEEE J EM SEL TOP C, V5, P254, DOI 10.1109/JETCAS.2015.2433552
NR 56
TC 12
Z9 12
U1 2
U2 21
PD FEB
PY 2022
VL 6
IS 1
BP 124
EP 135
DI 10.1109/TETCI.2020.3035164
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT J
AU Kim, CS
   Kim, T
   Min, KK
   Kim, S
   Park, BG
AF Kim, Chae Soo
   Kim, Taehyung
   Min, Kyung Kyu
   Kim, Sungjun
   Park, Byung-Gook
TI 3D Integrable W/SiN<i><sub>x</sub></i>/<i>n</i>-Si/<i>p</i>-Si 1D1R
   Unipolar Resistive Random Access Memory Synapse for Suppressing Reverse
   Leakage in Spiking Neural Network
SO JOURNAL OF NANOSCIENCE AND NANOTECHNOLOGY
DT Article
DE Resistive Random Access Memory; 1 Diode-1 RRAM; Spiking Neural Network;
   3D Structure
ID DEVICES
AB In this paper, we pose reverse leakage current issue which occurs when resistive random access memory (RRAM) is used as synapse for spiking neural networks (SNNs). To prevent this problem, 1 diode-1 RRAM (1D1R) synapse is suggested and simulated to examine their current rectifying chracteristics, Furthermore, high density of 1 K 3D 1D1R synapse array structure and its process flow are proposed.
C1 [Kim, Chae Soo; Kim, Taehyung; Min, Kyung Kyu; Park, Byung-Gook] Seoul Natl Univ, Dept Elect & Comp Engn, Seoul 08826, South Korea.
   [Kim, Sungjun] Chungbuk Natl Univ, Sch Elect Engn, Cheongju 28644, South Korea.
RP Park, BG (corresponding author), Seoul Natl Univ, Dept Elect & Comp Engn, Seoul 08826, South Korea.
CR Aluguri R, 2016, IEEE J ELECTRON DEVI, V4, P294, DOI 10.1109/JEDS.2016.2594190
   Ambrogio S, 2016, IEEE T ELECTRON DEV, V63, P1508, DOI 10.1109/TED.2016.2526647
   [Anonymous], 2003, P 2003 INT S CIRC SY
   Burr GW, 2014, J VAC SCI TECHNOL B, V32, DOI 10.1116/1.4889999
   Ielmini D, 2018, MICROELECTRON ENG, V190, P44, DOI 10.1016/j.mee.2018.01.009
   Indiveri G, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00073
   Jang J, 2009, 2009 SYMPOSIUM ON VLSI TECHNOLOGY, DIGEST OF TECHNICAL PAPERS, P192
   Kim S, 2017, CURR APPL PHYS, V17, P146, DOI 10.1016/j.cap.2016.11.017
   Kuegeler C, 2009, SOLID STATE ELECTRON, V53, P1287, DOI 10.1016/j.sse.2009.09.034
   Kwon MW, 2017, J NANOSCI NANOTECHNO, V17, P3038, DOI 10.1166/jnn.2017.14025
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Park S, 2012, 2012 IEEE INTERNATIONAL ELECTRON DEVICES MEETING (IEDM)
   Pavlidis NG, 2005, IEEE IJCNN, P2190
   Querlioz D, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P1775, DOI 10.1109/IJCNN.2011.6033439
   Xin JG, 2001, IEEE IJCNN, P1772, DOI 10.1109/IJCNN.2001.938430
NR 15
TC 4
Z9 4
U1 3
U2 26
PD AUG
PY 2020
VL 20
IS 8
BP 4735
EP 4739
DI 10.1166/jnn.2020.17806
WC Chemistry, Multidisciplinary; Nanoscience & Nanotechnology; Materials
   Science, Multidisciplinary; Physics, Applied; Physics, Condensed Matter
DA 2023-11-11
ER

PT C
AU Wang, F
   Severa, WM
   Rothganger, F
AF Wang, Felix
   Severa, William M.
   Rothganger, Fred
GP ACM
TI Acquisition and Representation of Spatio-Temporal Signals in
   Polychronizing Spiking Neural Networks
SO PROCEEDINGS OF THE 2019 7TH ANNUAL NEURO-INSPIRED COMPUTATIONAL ELEMENTS
   WORKSHOP (NICE 2019)
DT Proceedings Paper
CT 7th Annual Neuro-Inspired Computational Elements Workshop (NICE)
CY MAR 26-28, 2019
CL Albany, NY
DE Spiking neural networks; polychronization; signal processing;
   spatio-temporal coding; internal model
ID RECOGNITION
AB The ability of an intelligent agent to process complex signals such as those found in audio or video depends heavily on the nature of the internal representation of the relevant information. This work explores the mechanisms underlying this process by investigating theories inspired by the function of the neocortex. In particular, we focus on the phenomenon of polychronization, which describes the self-organization in a spiking neural network resulting from the interplay between network structure, driven spiking activity, and synaptic plasticity. What emerges are groups of neurons that exhibit reproducible, time-locked patterns of spiking activity. We propose that this representation is well suited to spatio-temporal signal processing, as it naturally resembles patterns found in real-world signals. We explore the computational properties of this approach and demonstrate the ability of a simple polychronizing network to learn different spatio-temporal signals.
C1 [Wang, Felix; Severa, William M.; Rothganger, Fred] Sandia Natl Labs, POB 5800, Albuquerque, NM 87185 USA.
RP Wang, F (corresponding author), Sandia Natl Labs, POB 5800, Albuquerque, NM 87185 USA.
EM felwang@sandia.gov; wmsever@sandia.gov; frothga@sandia.gov
CR [Anonymous], 1993, NAT I STANDARDS TECH, DOI DOI 10.35111/17GK-BN40
   Barlow H, 2001, NETWORK-COMP NEURAL, V12, P241, DOI 10.1088/0954-898X/12/3/301
   Braitenberg V., 1991, ANATOMY CORTEX STAT
   DAVIS SB, 1980, IEEE T ACOUST SPEECH, V28, P357, DOI 10.1109/TASSP.1980.1163420
   Edelman GM., 1987, NEURAL DARWINISM THE
   Guise M, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00009
   HULL JJ, 1994, IEEE T PATTERN ANAL, V16, P550, DOI 10.1109/34.291440
   Izhikevich EM, 2004, CEREB CORTEX, V14, P933, DOI 10.1093/cercor/bhh053
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   LEE KF, 1989, IEEE T ACOUST SPEECH, V37, P1641, DOI 10.1109/29.46546
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   MILLER GA, 1956, PSYCHOL REV, V63, P81, DOI 10.1037/h0043158
   Moore B. C. J., 2012, INTRO PSYCHOL HEARIN
   Morris RGM, 1999, BRAIN RES BULL, V50, P437, DOI 10.1016/S0361-9230(99)00182-3
   Paugam-Moisy H, 2008, NEUROCOMPUTING, V71, P1143, DOI 10.1016/j.neucom.2007.12.027
   Prut Y, 1998, J NEUROPHYSIOL, V79, P2857, DOI 10.1152/jn.1998.79.6.2857
   Rolston JD, 2007, NEUROSCIENCE, V148, P294, DOI 10.1016/j.neuroscience.2007.05.025
   Ross Ashby W., 1970, INT J SYST SCI, V1, P89, DOI [DOI 10.1080/00207727008920220, 10.1080/00207727008920220]
   Szatmáry B, 2010, PLOS COMPUT BIOL, V6, DOI 10.1371/journal.pcbi.1000879
   Tetzlaff C, 2012, BIOL CYBERN, V106, P715, DOI 10.1007/s00422-012-0529-z
   Wang F, 2015, PROCEDIA COMPUT SCI, V61, P322, DOI 10.1016/j.procs.2015.09.149
NR 22
TC 0
Z9 0
U1 0
U2 1
PY 2020
DI 10.1145/3320288.3320291
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Rocke, P
   McGinley, B
   Morgan, F
   Maher, J
AF Rocke, Patrick
   McGinley, Brian
   Morgan, Fearghal
   Maher, John
BE Diniz, PC
   Marques, E
   Bertels, K
   Fernandes, MM
   Cardoso, JMP
TI Reconfigurable hardware evolution platform for a spiking neural network
   robotics controller
SO RECONFIGURABLE COMPUTING: ARCHITECTURES, TOOLS AND APPLICATIONS
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 3rd International Workshop on Applied Reconfigurable Computing
CY MAR 27-29, 2007
CL Mangaratiba, BRAZIL
DE reconfigurable hardware; FPAA; genetic algorithm; spiking neural
   networks; evolutionary computation
AB This paper describes a platform for the hardware evolution of Spiking Neural Network (SNN) based robotics controllers on multiple Field Programmable Analogue Arrays (FPAAs). The SNN robotics controller, evolved using a GA, performs obstacle avoidance and navigation. A robotics simulator is used to evaluate the performance of the evolved hardware SNN. Simulated sonar data is input to FPAA neurons and the SNN returns motor control data to the simulator. Initial results indicate the emergence of effective navigation behavior.
C1 [Rocke, Patrick; McGinley, Brian; Morgan, Fearghal; Maher, John] NUI Galway, Dept Elect Engn, Res Grp, BIRC, Galway, Ireland.
RP Rocke, P (corresponding author), NUI Galway, Dept Elect Engn, Res Grp, BIRC, Galway, Ireland.
EM patrick.rocke@nuigalway.ie; brian.mcginley@nuigalway.ie;
   fearghal.morgan@nuigalway.ie; john.maher@nuigalway.ie
CR Bellis S, 2004, 2004 IEEE INTERNATIONAL CONFERENCE ON FIELD-PROGRAMMABLE TECHNOLOGY, PROCEEDINGS, P449, DOI 10.1109/FPT.2004.1393322
   BERENSON D, UNPUB 2005 NASADOD C
   FLOREANO D, EVOLUTIONARY BITS SP
   FLOREANO D, EVOLVING NEURAL NEUR
   Gerstner W., 2002, SPIKING NEURON MODEL
   Haykin S., 1994, NEURAL NETWORKS COMP
   Holland J. H., 1992, ADAPTATION NATURAL A
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   MAHER J, 2006, IEEE S FIELD PROGR C
   MCGINLEY B, 2005, WORKSH LIF PERC SYST
   NOLFT S, 1994, 4 INT WORKSH ART LIF
   ROCKE P, 2005, 2005 INT C REC COMP, P11
   Terry MA, 2006, LECT NOTES COMPUT SC, V3907, P332
   UPEGUI C, METHODOLGY EVOLVING
   URZELAI J, EVOLUTION ADAPTIVE S
   XINYAO, 1999, P IEEE, V87
NR 16
TC 9
Z9 9
U1 0
U2 3
PY 2007
VL 4419
BP 373
EP +
WC Computer Science, Hardware & Architecture; Computer Science, Theory &
   Methods
DA 2023-11-11
ER

PT J
AU Stöckel, A
   Jenzen, C
   Thies, M
   Rückert, U
AF Stoeckel, Andreas
   Jenzen, Christoph
   Thies, Michael
   Rueckert, Ulrich
TI Binary Associative Memories as a Benchmark for Spiking Neuromorphic
   Hardware
SO FRONTIERS IN COMPUTATIONAL NEUROSCIENCE
DT Article
DE neuromorphic hardware; spiking neural networks; benchmark; associative
   memory
ID NETWORK; MODELS; GENERATION; CELLS
AB Large-scale neuromorphic hardware platforms, specialized computer systems for energy efficient simulation of spiking neural networks, are being developed around the world, for example as part of the European Human Brain Project (HBP). Due to conceptual differences, a universal performance analysis of these systems in terms of runtime, accuracy and energy efficiency is non-trivial, yet indispensable for further hard-and software development. In this paper we describe a scalable benchmark based on a spiking neural network implementation of the binary neural associative memory. We treat neuromorphic hardware and software simulators as black-boxes and execute exactly the same network description across all devices. Experiments on the HBP platforms under varying configurations of the associative memory show that the presented method allows to test the quality of the neuron model implementation, and to explain significant deviations from the expected reference output.
C1 [Stoeckel, Andreas; Jenzen, Christoph; Thies, Michael; Rueckert, Ulrich] Bielefeld Univ, Fac Technol, Cognitron & Sensor Syst, Cluster Excellence Cognit Interact Technol, Bielefeld, Germany.
   [Stoeckel, Andreas] Univ Waterloo, Ctr Theoret Neurosci, Waterloo, ON, Canada.
RP Stöckel, A; Jenzen, C (corresponding author), Bielefeld Univ, Fac Technol, Cognitron & Sensor Syst, Cluster Excellence Cognit Interact Technol, Bielefeld, Germany.
EM astoecke@uwaterloo.ca; cjenzen@techfak.uni-bielefeld.de
CR [Anonymous], FRONT NEUR C NEUR 20
   [Anonymous], 2007, SCHOLARPEDIA, DOI DOI 10.4249/SCH0LARPEDIA.1903
   [Anonymous], THESIS
   [Anonymous], 1991, NEURONAL NETWORKS HI
   [Anonymous], 1961, KYBERNETIK
   [Anonymous], THESIS
   [Anonymous], 2015, 2015 INT JOINT C NEU, DOI DOI 10.1109/IJCNN.2015.7280625
   [Anonymous], 2017, ARXIV170301909
   [Anonymous], THESIS
   [Anonymous], THESIS
   [Anonymous], ASS MEMORY SYSTEM TH
   BHALLA US, 1993, J NEUROPHYSIOL, V69, P1948, DOI 10.1152/jn.1993.69.6.1948
   Bill J, 2010, FRONT COMPUT NEUROSC, V4, DOI 10.3389/fncom.2010.00129
   Braitenberg V, 1998, CORTEX STAT GEOMETRY, V2nd
   Brette R, 2005, J NEUROPHYSIOL, V94, P3637, DOI 10.1152/jn.00686.2005
   BRILLINGER DR, 1988, BIOL CYBERN, V59, P189, DOI 10.1007/BF00318010
   Brüderle D, 2011, BIOL CYBERN, V104, P263, DOI 10.1007/s00422-011-0435-9
   Davison Andrew P, 2008, Front Neuroinform, V2, P11, DOI 10.3389/neuro.11.011.2008
   Diamond A, 2016, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00491
   Ehrlich M, 2010, ARTIFICIAL NEURAL NETWORKS AND INTELLIGENT INFORMATION PROCESSING, P43
   Feng WC, 2007, COMPUTER, V40, P50, DOI 10.1109/MC.2007.445
   Fieres J, 2008, IEEE IJCNN, P969, DOI 10.1109/IJCNN.2008.4633916
   Friedmann S, 2017, IEEE T BIOMED CIRC S, V11, P128, DOI 10.1109/TBCAS.2016.2579164
   Furber S., 2010, 2010 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2010.5596364
   Furber SB, 2013, IEEE T COMPUT, V62, P2454, DOI 10.1109/TC.2012.142
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   Gerstner W, 2018, SCHOLARPEDIA, V3, P1343
   Gewaltig M-O., 2007, SCHOLARPEDIA, V2, DOI [10.4249/scholarpedia.1430, DOI 10.4249/SCHOLARPEDIA.1430]
   Gray CM, 1996, SCIENCE, V274, P109, DOI 10.1126/science.274.5284.109
   Hasler J, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00118
   Hebb D. O., 1949, ORG BEHAV A NEUROPSY
   HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554
   Lansner A, 2009, TRENDS NEUROSCI, V32, P178, DOI 10.1016/j.tins.2008.12.002
   Markram H, 2012, SCI AM, V306, P50, DOI 10.1038/scientificamerican0612-50
   McCulloch Warren S., 1943, BULL MATH BIOPHYS, V5, P115, DOI 10.1007/BF02478259
   Mundy A., 2015, P INT JOINT C NEUR N, P1, DOI DOI 10.1109/IJCNN.2015.7280390
   NELDER JA, 1965, COMPUT J, V7, P308, DOI 10.1093/comjnl/7.4.308
   Painkras E, 2013, IEEE J SOLID-ST CIRC, V48, P1943, DOI 10.1109/JSSC.2013.2259038
   PALM G, 1980, BIOL CYBERN, V36, P19, DOI 10.1007/BF00337019
   Palm G, 2013, NEURAL NETWORKS, V37, P163, DOI 10.1016/j.neunet.2012.08.013
   Petrovici MA, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0108590
   Pfeil T, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00011
   Press W. H., 2007, NUM REC ART SCI COMP
   RUECKERT U, 1991, ARTIFICIAL NEURAL NETWORKS, VOLS 1 AND 2, P1195
   Schemmel J, 2010, IEEE INT SYMP CIRC S, P1947, DOI 10.1109/ISCAS.2010.5536970
   Schmuker M, 2014, P NATL ACAD SCI USA, V111, P2081, DOI 10.1073/pnas.1303053111
   Scholze S, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00117
   Schwenker F, 1996, NEURAL NETWORKS, V9, P445, DOI 10.1016/0893-6080(95)00112-3
   Sharp T., 2013, 2013 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2013.6706988
   Stromatias E., 2013, 2013 INT JOINT C NEU, P1, DOI [DOI 10.1109/IJCNN.2013.6706927, 10.1109/ijcnn.2013.6706927]
   Thanasoulis V, 2014, IEEE INT SYMP CIRC S, P265, DOI 10.1109/ISCAS.2014.6865116
   WILLSHAW DJ, 1969, NATURE, V222, P960, DOI 10.1038/222960a0
   Yavuz E, 2016, SCI REP-UK, V6, DOI 10.1038/srep18854
NR 53
TC 2
Z9 2
U1 0
U2 3
PD AUG 22
PY 2017
VL 11
AR 71
DI 10.3389/fncom.2017.00071
WC Mathematical & Computational Biology; Neurosciences
DA 2023-11-11
ER

PT J
AU Zhang, SQ
   Zhang, ZY
   Zhou, ZH
AF Zhang, Shao-Qun
   Zhang, Zhao-Yu
   Zhou, Zhi-Hua
TI Bifurcation Spiking Neural Network
SO JOURNAL OF MACHINE LEARNING RESEARCH
DT Article
DE Spiking Neural Network; Leaky Integrate-and-Fire model; Control Rates;
   Eigenvalues; Bifurcation Dynamical System
ID MODEL
AB Spiking neural networks (SNNs) have attracted much attention due to their great potential for modeling time-dependent signals. The performance of SNNs depends not only on picking an apposite architecture and searching optimal connection weights as well as conventional deep neural networks, but also on the careful tuning of many hyper-parameters within fundamental spiking neural models. However, so far, there has been less systematic work on analyzing SNNs' dynamical characteristics, especially ones relative to these internal hyper-parameters, which leads to whether SNNs are adequate for modeling actual data relies on fortune. In this work, we provide a theoretical framework for investigating spiking neural models from the perspective of dynamical systems. As a result, we point out that the LIF model with control rate hyper-parameters is a bifurcation dynamical system. This point explains why the performance of SNNs is so sensitive to the setting of control rate hyper-parameters, leading to a recommendation that diverse and adaptive eigenvalues are beneficial to improve the performance of SNNs. Inspired by this insight, we develop the Bifurcation Spiking Neural Network (BSNN) with supervised implementation, and theoretically show that BSNN is an adaptive dynamical system. Experiments validate the effectiveness of BSNN on several benchmark data sets, showing that BSNN achieves superior performance to existing SNNs and is robust to the setting of control rates.
C1 [Zhang, Shao-Qun; Zhang, Zhao-Yu; Zhou, Zhi-Hua] Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing 210023, Peoples R China.
RP Zhang, SQ (corresponding author), Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing 210023, Peoples R China.
EM ZHANGSQ@LAMDA.NJU.EDU.CN; ZHANGZHAOYU@LAMDA.NJU.EDU.CN;
   ZHOUZH@LAMDA.NJU.EDU.CN
CR Anumula J, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00023
   Beck A, 2013, SIAM J OPTIMIZ, V23, P2037, DOI 10.1137/120887679
   Bergstra J, 2012, J MACH LEARN RES, V13, P281
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Burkitt AN, 2006, BIOL CYBERN, V95, P97, DOI 10.1007/s00422-006-0082-8
   Burkitt AN, 2006, BIOL CYBERN, V95, P1, DOI 10.1007/s00422-006-0068-6
   Carlson KD, 2014, FRONT NEUROSCI-SWITZ, V8, DOI 10.3389/fnins.2014.00010
   Chou C.-N., 2019, P 10 INN THEOR COMP
   Cohen G, 2017, IEEE IJCNN, P2921, DOI 10.1109/IJCNN.2017.7966217
   Cohen GK, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00184
   Dumont G, 2017, PLOS COMPUT BIOL, V13, DOI 10.1371/journal.pcbi.1005691
   Eldan R, 2016, C LEARN THEOR, P907
   GERSTNER W, 1995, PHYS REV E, V51, P738, DOI 10.1103/PhysRevE.51.738
   Gerstner W., 2002, SPIKING NEURON MODEL
   Hirsch MW, 2012, DIFF EQUAT+
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Hohn N, 2001, PHYS REV E, V63, DOI 10.1103/PhysRevE.63.031902
   Huh D, 2018, ADV NEUR IN, V31
   Hunsberger E., 2015, ARXIV PREPRINT ARXIV
   Hunter D, 2012, IEEE T IND INFORM, V8, P228, DOI 10.1109/TII.2012.2187914
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Jin Yingyezhe, 2018, P 32 INT C NEUR INF
   Kulkarni SR, 2018, NEURAL NETWORKS, V103, P118, DOI 10.1016/j.neunet.2018.03.019
   Kuznetsov Y A, 2013, ELEMENTS APPL BIFURC
   Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508
   Lobo JL, 2020, NEURAL NETWORKS, V121, P88, DOI 10.1016/j.neunet.2019.09.004
   Lorenzo PR, 2017, PROCEEDINGS OF THE 2017 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE (GECCO'17), P481, DOI 10.1145/3071178.3071208
   Lotfi Rezaabad A., 2020, INT C NEUR SYST, P1, DOI DOI 10.1145/3407197.3407211
   McKennoch S, 2006, IEEE IJCNN, P3970
   Neftci EO, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00324
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   OConnor P., 2016, ARXIV160208323
   OConnor Peter, 2019, 22 INT C ARTIFICIAL, V89
   Onuki A., 2002, PHASE TRANSITION DYN
   Orchard G, 2015, FRONT NEUROSCI-SWITZ, V9, DOI [10.3389/fnins.2015.00437, 10.3389/fhins.2015.00437]
   Pillow JW, 2005, J NEUROSCI, V25, P11003, DOI 10.1523/JNEUROSCI.3305-05.2005
   Quiroga RQ, 2005, NATURE, V435, P1102, DOI 10.1038/nature03687
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Shimokawa T, 1999, PHYS REV E, V59, P3427, DOI 10.1103/PhysRevE.59.3427
   Shrestha SB, 2018, ADV NEUR IN, V31
   Snoek J., 2012, PROC INT C NEURAL IN, V25, P2951
   VanRullen R, 2005, TRENDS NEUROSCI, V28, P1, DOI 10.1016/j.tins.2004.10.010
   Weinberger, 2013, ADV NEURAL INFORM PR, P1538
   Wu J.-H., 2021, ARXIV211106027
   Yu Q, 2014, NEUROCOMPUTING, V138, P3, DOI 10.1016/j.neucom.2013.06.052
   Zhang S.-Q., 2021, ARXIV211106222
   Zhang SQ, 2021, NEURAL COMPUT, V33, P2951, DOI 10.1162/neco_a_01431
   Zhang SQ, 2020, FRONT ARTIF INTEL AP, V325, P1714, DOI 10.3233/FAIA200284
   Zhang WR, 2019, ADV NEUR IN, V32
   Zhou ZH, 2021, SCI CHINA INFORM SCI, V64, DOI 10.1007/s11432-020-2885-6
NR 50
TC 3
Z9 3
U1 1
U2 5
PY 2021
VL 22
BP 1
EP 21
WC Automation & Control Systems; Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT C
AU Allen, JN
   Abdel-Aty-Zohdy, HS
   Ewing, RL
AF Allen, JN
   Abdel-Aty-Zohdy, HS
   Ewing, RL
GP ieee
TI Electronic nose inhibition in a spiking neural network for noise
   cancellation
SO PROCEEDINGS OF THE 2004 IEEE SYMPOSIUM ON COMPUTATIONAL INTELLIGENCE IN
   BIOINFORMATICS AND COMPUTATIONAL BIOLOGY
DT Proceedings Paper
CT IEEE Symposium on Computational Intelligence in Bioinformatics and
   Computational Biology
CY OCT 07-08, 2004
CL La Jolla, CA
AB An olfaction detection spiking neural network that detects binary odor patterns is analyzed and implemented. This paper presents a new method for inhibiting spiking neural networks by modulating a detection threshold. Interference noise from active odors is measured by a single inhibitory neuron. The inhibition neuron changes the detection threshold to create tolerance for a system with multiple odors present. A digital implementation of the inhibition is simulated. Comparative results prove that threshold modulation reduces false-positive detection error in high noise scenarios where fifteen odors are active simultaneously.
C1 Oakland Univ, Microelect Syst Design Lab, Rochester, MN USA.
RP Allen, JN (corresponding author), Oakland Univ, Microelect Syst Design Lab, Rochester, MN USA.
CR ABDELATYZOHDY HS, 1997, P 40 IEEE INT MIDW S
   ALLEN J, 2002, P 45 IEEE INT MIDW S
   BUCK L, 1996, CELL, V100, P611
   Dutta R, 2003, NEURAL NETWORKS, V16, P847, DOI 10.1016/S0893-6080(03)00092-3
   FUSI S, 1999, THESIS JERUSALEM U J
   Kermani BG, 1999, IEEE T BIO-MED ENG, V46, P429, DOI 10.1109/10.752940
   ROCHEL O, 2002, EUROSENSORS
   YOUSSIF R, 2002, P 45 IEEE INT MIDW S
NR 8
TC 5
Z9 5
U1 0
U2 1
PY 2004
BP 129
EP 133
WC Biochemical Research Methods; Computer Science, Artificial Intelligence;
   Computer Science, Interdisciplinary Applications
DA 2023-11-11
ER

PT C
AU Hulea, M
   Caruntu, CF
AF Hulea, Mircea
   Caruntu, Constantin Florin
GP IEEE
TI Spiking neural network for controlling the artificial muscles of a
   humanoid robotic arm
SO 2014 18TH INTERNATIONAL CONFERENCE SYSTEM THEORY, CONTROL AND COMPUTING
   (ICSTCC)
SE International Conference on System Theory Control and Computing
DT Proceedings Paper
CT 18th International Conference on System Theory, Control and Computing
   (ICSTCC)
CY OCT 17-19, 2014
CL Sinaia, ROMANIA
DE human-like robotic arm; spiking neural networks; analogue bio-inspired
   neuron; flexinol actuator wires
ID MOTOR; CIRCUITS; LOCOMOTION; MOVEMENT
AB One of the main functions of the human nervous system is the muscles control. The complexity of this function increases for hand and fingers control because of the high diversity and accuracy of the possible motions. Starting from the control mechanisms of the natural muscle we developed a structure of spiking neurons implemented in PCB hardware that is able to drive the elbow of a human-like robotic arm. In order to increase the biological plausibility of the designed robotic arm driving system, the artificial neural network controls artificial muscles that are implemented with Flexinol actuator wires. From our knowledge the control of the actuator wires using spiking neural networks was not performed previously.
   The results show that the excitatory spiking neurons are able to flex and straighten the artificial elbow by stimulating two antagonistic actuator wires. Moreover, by using inhibitory neurons that modulate the neural excitatory activity the arm mobile segment can be driven to specific positions.
   Our final goal is the development of a biologically plausible neural structure that is able to easily drive the hand and fingers of a human-like robotic arm by controlling the artificial muscles implemented with actuator wires. This will give important clues for future developments of spiking neural networks that drive robotic arms for performing complex tasks.
C1 [Hulea, Mircea] Gheorghe Asachi Tech Univ Iasi, Dept Comp Sci & Engn, Fac Automat Control & Comp Engn, Iasi, Romania.
   [Caruntu, Constantin Florin] Gheorghe Asachi Tech Univ Iasi, Dept Automat Control & Appl Informat, Fac Automat Control & Comp Engn, Iasi, Romania.
RP Hulea, M (corresponding author), Gheorghe Asachi Tech Univ Iasi, Dept Comp Sci & Engn, Fac Automat Control & Comp Engn, Iasi, Romania.
EM mhulea@tuiasi.ro; caruntuc@ac.tuiasi.ro
CR Bouganis A., 2010, P 2010 INT JOINT C N, P1, DOI DOI 10.1109/IJCNN.2010.5596525
   CHEVALIER G, 1990, TRENDS NEUROSCI, V13, P277, DOI 10.1016/0166-2236(90)90109-N
   DeLong MR, 2007, ARCH NEUROL-CHICAGO, V64, P20, DOI 10.1001/archneur.64.1.20
   Fagg AH, 2009, IEEE T NEUR SYS REH, V17, P487, DOI 10.1109/TNSRE.2009.2029313
   Franklin G. F., 1994, FEEDBACK CONTROL DYN, V3rd
   Gamez D, 2012, BIOINSPIR BIOMIM, V7, DOI 10.1088/1748-3182/7/2/025008
   Goulding M, 2009, NAT REV NEUROSCI, V10, P507, DOI 10.1038/nrn2608
   Grillner S, 2003, NAT REV NEUROSCI, V4, P573, DOI 10.1038/nrn1137
   Grillner S., 1990, SELECTION INITIATION
   Grillner S, 2008, BRAIN RES REV, V57, P2, DOI 10.1016/j.brainresrev.2007.06.027
   Guertin PA, 2009, BRAIN RES REV, V62, P45, DOI 10.1016/j.brainresrev.2009.08.002
   Hulea M., 2011, P 15 C SYST THEOR CO, P282
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Joshi P, 2005, NEURAL COMPUT, V17, P1715, DOI 10.1162/0899766054026684
   Leisman G, 2013, REV NEUROSCIENCE, V24, P9, DOI 10.1515/revneuro-2012-0067
   Nakata Y, 2012, IEEE INT CONF ROBOT, P3153, DOI 10.1109/ICRA.2012.6225362
   Roggen D, 2003, 2003 NASA/DOD CONFERENCE ON EVOLVABLE HARDWARE, P189
   Takakusaki K, 2004, NEUROSCI RES, V50, P137, DOI 10.1016/j.neures.2004.06.015
   Teh Y. H., 2008, THESIS AUSTR NATL U
   Wang Xu, COORDINATED HUNTING
   Zehr EP, 2004, NEUROSCIENTIST, V10, P347, DOI 10.1177/1073858404264680
NR 21
TC 9
Z9 10
U1 0
U2 4
PY 2014
BP 163
EP 168
WC Automation & Control Systems; Computer Science, Hardware & Architecture;
   Computer Science, Information Systems
DA 2023-11-11
ER

PT C
AU Zhang, YD
   Zeng, ZG
   Wen, SP
AF Zhang, Yide
   Zeng, Zhigang
   Wen, Shiping
GP IEEE
TI Implementation of Memristive Neural Networks with Spike-rate-dependent
   Plasticity Synapses
SO PROCEEDINGS OF THE 2014 INTERNATIONAL JOINT CONFERENCE ON NEURAL
   NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUL 06-11, 2014
CL Beijing, PEOPLES R CHINA
ID NEURONS; MEMORY; MODEL
AB The property of changing resistance according to applied currents of memristors makes them candidates for emulating synapses in artificial neural networks. In this paper, we introduce a memristive synapse design into neural network circuits. Combined with modified integrate-andfire (I&F) complementary metal-oxide-semiconducter (CMOS) neurons, the memristive neural network shows similarities to its biological counterpart, in respect of biologically realistic, current-controlled spikes and adaptive synaptic plasticity. Then, the spike-rate-dependent plasticity (SRDP) of the synapse, an extended protocol of the Hebbian learning rule, is originally implemented by the circuit. And some advanced neural activities including learning, associative memory and forgetting are realized based on the SRDP rule. These activities are comprehensively validated on a neural network circuit inspired by famous Pavlov's dog-experiment with simulations and quantitative analyses.
C1 [Zhang, Yide; Zeng, Zhigang; Wen, Shiping] Huazhong Univ Sci & Technol, Sch Automat, Wuhan 430074, Peoples R China.
   [Zhang, Yide; Zeng, Zhigang; Wen, Shiping] Educ Minist China, Key Lab Image Proc & Intelligent Control, Wuhan 430074, Peoples R China.
RP Zhang, YD (corresponding author), Huazhong Univ Sci & Technol, Sch Automat, Wuhan 430074, Peoples R China.
EM edwardchang@hust.edu.cn; hustzgzeng@gmail.com; wenshiping226@126.com
CR [Anonymous], NEUROCOMPUTING
   [Anonymous], 1989, ANALOG VLSI NEURAL S
   [Anonymous], BSIMV3 MANUAL
   [Anonymous], 1927, CONDITIONAL REFLEXES
   Bartolozzi C, 2007, NEURAL COMPUT, V19, P2581, DOI 10.1162/neco.2007.19.10.2581
   Bear Mark F., 1994, Current Opinion in Neurobiology, V4, P389, DOI 10.1016/0959-4388(94)90101-5
   Beiu V, 2003, IEEE T NEURAL NETWOR, V14, P1217, DOI 10.1109/TNN.2003.816365
   Benderli S, 2009, ELECTRON LETT, V45, P377, DOI 10.1049/el.2009.3511
   Bichler O, 2013, NEURAL COMPUT, V25, P549, DOI 10.1162/NECO_a_00377
   BIENENSTOCK EL, 1982, J NEUROSCI, V2, P32, DOI 10.1523/jneurosci.02-01-00032.1982
   Biolek Z, 2009, RADIOENGINEERING, V18, P210
   Brader JM, 2007, NEURAL COMPUT, V19, P2881, DOI 10.1162/neco.2007.19.11.2881
   Cantley KD, 2012, IEEE T NEUR NET LEAR, V23, P565, DOI 10.1109/TNNLS.2012.2184801
   Cederström L, 2013, IEEE INT SYMP CIRC S, P2323, DOI 10.1109/ISCAS.2013.6572343
   Chevaleyre V, 2003, NEURON, V38, P461, DOI 10.1016/S0896-6273(03)00235-6
   Chicca E, 2003, IEEE T NEURAL NETWOR, V14, P1297, DOI 10.1109/TNN.2003.816367
   CHUA LO, 1976, P IEEE, V64, P209, DOI 10.1109/PROC.1976.10092
   CHUA LO, 1971, IEEE T CIRCUITS SYST, VCT18, P507, DOI 10.1109/TCT.1971.1083337
   Dayan P., 2001, THEORETICAL NEUROSCI
   Grossberg S, 2012, NEURAL NETWORKS, V27, P1, DOI [10.1016/j.neunet.2011.10.011, 10.1016/j.neunet.2012.09.017]
   Hebb D. O., 1949, ORG BEHAV
   Hopfield JJ, 1990, NETWORK-COMP NEURAL, V1, P27, DOI 10.1088/0954-898X/1/1/003
   Indiveri G, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00073
   Jo SH, 2010, NANO LETT, V10, P1297, DOI 10.1021/nl904092h
   Joglekar YN, 2009, EUR J PHYS, V30, P661, DOI 10.1088/0143-0807/30/4/001
   Kim H, 2012, IEEE T CIRCUITS-I, V59, P148, DOI 10.1109/TCSI.2011.2161360
   Li SZ, 2013, J MATER CHEM C, V1, P5292, DOI 10.1039/c3tc30575a
   Linares-Barranco B., 2009, NAT PRECED
   Masquelier T, 2007, PLOS COMPUT BIOL, V3, P247, DOI 10.1371/journal.pcbi.0030031
   Masquelier T, 2009, NEURAL COMPUT, V21, P1259, DOI 10.1162/neco.2008.06-08-804
   PEREZCARRASCO JA, 2010, P 2010 IEEE INT S CI, P1659
   Pershin YV, 2010, NEURAL NETWORKS, V23, P881, DOI 10.1016/j.neunet.2010.05.001
   Prodromakis T, 2011, IEEE T ELECTRON DEV, V58, P3099, DOI 10.1109/TED.2011.2158004
   Querlioz D, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P1775, DOI 10.1109/IJCNN.2011.6033439
   Rachmuth G, 2011, P NATL ACAD SCI USA, V108, pE1266, DOI 10.1073/pnas.1106161108
   Richardson MJE, 2010, PHYS REV LETT, V105, DOI 10.1103/PhysRevLett.105.178102
   Rubin J, 2001, PHYS REV LETT, V86, P364, DOI 10.1103/PhysRevLett.86.364
   Shin S, 2010, IEEE T COMPUT AID D, V29, P590, DOI 10.1109/TCAD.2010.2042891
   Snider GS, 2007, NANOTECHNOLOGY, V18, DOI 10.1088/0957-4484/18/36/365202
   Snider G, 2011, COMPUTER, V44, P21, DOI 10.1109/MC.2011.48
   Snider G, 2011, NANOTECHNOLOGY, V22, DOI 10.1088/0957-4484/22/1/015201
   Snider GS, 2008, 2008 IEEE INTERNATIONAL SYMPOSIUM ON NANOSCALE ARCHITECTURES, P85, DOI 10.1109/NANOARCH.2008.4585796
   STAFSTROM CE, 1984, J NEUROPHYSIOL, V52, P264, DOI 10.1152/jn.1984.52.2.264
   Strukov DB, 2008, NATURE, V453, P80, DOI 10.1038/nature06932
   Wang ZQ, 2012, ADV FUNCT MATER, V22, P2759, DOI 10.1002/adfm.201103148
   Ziegler M, 2012, ADV FUNCT MATER, V22, P2744, DOI 10.1002/adfm.201200244
NR 46
TC 25
Z9 29
U1 0
U2 11
PY 2014
BP 2226
EP 2233
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic; Telecommunications
DA 2023-11-11
ER

PT C
AU Gardner, B
   Grüning, A
AF Gardner, Brian
   Gruening, Andre
BE Mladenov, V
   KoprinkovaHristova, P
   Palm, G
   Villa, AEP
   Appollini, B
   Kasabov, N
TI Learning Temporally Precise Spiking Patterns through Reward Modulated
   Spike-Timing-Dependent Plasticity
SO ARTIFICIAL NEURAL NETWORKS AND MACHINE LEARNING - ICANN 2013
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 23rd International Conference on Artificial Neural Networks (ICANN)
CY SEP 10-OCT 13, 2013
CL Techn Univ Sofia, Sofia, BULGARIA
HO Techn Univ Sofia
DE Neuronal Plasticity; Stochastic Neuron; Synapses
ID NEURAL-NETWORKS
AB Precise neuronal spike timing plays an important role in many aspects of cognitive processing. Here, we explore how a spiking neural network can learn to generate temporally precise spikes in response to a spatio-temporal pattern, through spike-timing-dependent plasticity modulated by a delayed reward signal. An escape noise neuron is implemented as the readout to incorporate the effect of background noise on spike timing. We compare the performance of two different escape rate functions that drive spiking in the readout neuron: the Arrhenius & Current (A&C) and Exponential (EXP) model. Our results show that the network can learn to reproduce target spike patterns containing between 1 and 10 spikes with 10 ms temporal accuracy. We also demonstrate the superior performance of the A&C model over the EXP model for the parameters we consider, especially when reproducing a large number of target spikes.
C1 [Gardner, Brian; Gruening, Andre] Univ Surrey, Dept Comp, Guildford GU2 7XH, Surrey, England.
RP Gardner, B (corresponding author), Univ Surrey, Dept Comp, Guildford GU2 7XH, Surrey, England.
EM b.gardner@surrey.ac.uk; a.gruning@surrey.ac.u
CR Bohte Sander M., 2004, Natural Computing, V3, P195, DOI 10.1023/B:NACO.0000027755.02868.60
   Chance FS, 2002, NEURON, V35, P773, DOI 10.1016/S0896-6273(02)00820-6
   El-Laithy K., 2011, COMPUTATIONAL INTELL, V4
   Farries MA, 2007, J NEUROPHYSIOL, V98, P3648, DOI 10.1152/jn.00364.2007
   Florian RV, 2007, NEURAL COMPUT, V19, P1468, DOI 10.1162/neco.2007.19.6.1468
   Frémaux N, 2010, J NEUROSCI, V30, P13326, DOI 10.1523/JNEUROSCI.6249-09.2010
   Gerstner W., 2002, SPIKING NEURON MODEL
   Grüning A, 2012, NEURAL PROCESS LETT, V36, P117, DOI 10.1007/s11063-012-9225-1
   Izhikevich EM, 2007, CEREB CORTEX, V17, P2443, DOI 10.1093/cercor/bhl152
   Legenstein R, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.1000180
   Morrison A, 2008, BIOL CYBERN, V98, P459, DOI 10.1007/s00422-008-0233-1
   Pfister JP, 2006, NEURAL COMPUT, V18, P1318, DOI 10.1162/neco.2006.18.6.1318
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Sporea I, 2013, NEURAL COMPUT, V25, P473, DOI 10.1162/NECO_a_00396
   Urbanczik R, 2009, NAT NEUROSCI, V12, P250, DOI 10.1038/nn.2264
   van Rossum MCW, 2001, NEURAL COMPUT, V13, P751, DOI 10.1162/089976601300014321
   van Rossum MCW, 2000, J NEUROSCI, V20, P8812
NR 17
TC 6
Z9 6
U1 0
U2 1
PY 2013
VL 8131
BP 256
EP 263
WC Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Computer Science, Theory & Methods
DA 2023-11-11
ER

PT C
AU Yousefzadeh, A
   Hoseini, S
   Holanda, P
   Leroux, S
   Werner, T
   Serrano-Gotarredona, T
   Barranco, BL
   Dhoedt, B
   Simoens, P
AF Yousefzadeh, Amirreza
   Hoseini, Sahar
   Holanda, Priscila
   Leroux, Sam
   Werner, Thilo
   Serrano-Gotarredona, Teresa
   Barranco, Bernabe Linares
   Dhoedt, Bart
   Simoens, Pieter
GP IEEE
TI Conversion of Synchronous Artificial Neural Network to Asynchronous
   Spiking Neural Network using sigma-delta quantization
SO 2019 IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE CIRCUITS
   AND SYSTEMS (AICAS 2019)
DT Proceedings Paper
CT 1st IEEE International Conference on Artificial Intelligence Circuits
   and Systems (AICAS)
CY MAR 18-20, 2019
CL Hsinchu, TAIWAN
ID PROCESSOR
AB Artificial Neural Networks (ANNs) show great performance in several data analysis tasks including visual and auditory applications. However, direct implementation of these algorithms without considering the sparsity of data requires high processing power, consume vast amounts of energy and suffer from scalability issues. Inspired by biology, one of the methods which can reduce power consumption and allow scalability in the implementation of neural networks is asynchronous processing and communication by means of action potentials, so-called spikes. In this work, we use the well-known sigma-delta quantization method and introduce an easy and straightforward solution to convert an Artificial Neural Network to a Spiking Neural Network which can be implemented asynchronously in a neuromorphic platform. Briefly, we used asynchronous spikes to communicate the quantized output activations of the neurons. Despite the fact that our proposed mechanism is simple and applicable to a wide range of different ANNs, it outperforms the state-of-the-art implementations from the accuracy and energy consumption point of view. All source code for this project is available upon request for the academic purpose(1).
C1 [Yousefzadeh, Amirreza; Holanda, Priscila; Leroux, Sam; Werner, Thilo; Dhoedt, Bart; Simoens, Pieter] Univ Ghent, Imec, IDLab, Ghent, Belgium.
   [Hoseini, Sahar; Serrano-Gotarredona, Teresa; Barranco, Bernabe Linares] CSIC, Inst Microelectron Sevilla, Seville, Spain.
   [Hoseini, Sahar; Serrano-Gotarredona, Teresa; Barranco, Bernabe Linares] Univ Seville, Seville, Spain.
RP Barranco, BL (corresponding author), CSIC, Inst Microelectron Sevilla, Seville, Spain.; Barranco, BL (corresponding author), Univ Seville, Seville, Spain.
EM Bernabe@imse-cnm.csic.es
CR BOSER BE, 1988, IEEE J SOLID-ST CIRC, V23, P1298, DOI 10.1109/4.90025
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Diehl Peter U, 2015, 2015 INT JOINT C NEU, P1, DOI [DOI 10.1109/IJCNN.2015.7280696, 10.1109/IJCNN.2015.7280696]
   Farabet C, 2012, FRONT NEUROSCI-SWITZ, V6, DOI 10.3389/fnins.2012.00032
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   Grady D., VISION THING MAINLY
   Indiveri Giacomo, 2011, Front Neurosci, V5, P118, DOI 10.3389/fnins.2011.00118
   Kheradpisheh S. R., CORR
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee J., CORR
   Leroux S., CORR
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   MINK JW, 1981, AM J PHYSIOL, V241, pR203, DOI 10.1152/ajpregu.1981.241.3.R203
   Moons B, 2017, IEEE J SOLID-ST CIRC, V52, P903, DOI 10.1109/JSSC.2016.2636225
   Mostafa H., 2017, 2017 IEEE INT S CIRC, P1, DOI [10.1109/ISCAS.2017.8050527, DOI 10.1109/ISCAS.2017.8050527]
   Mostafaei H, 2019, IEEE T IND ELECTRON, V66, P5567, DOI 10.1109/TIE.2018.2869345
   Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32
   Rueckauer B, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351295
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Sengupta A., CORR
   Serrano-Gotarredona T, 2013, IEEE J SOLID-ST CIRC, V48, P827, DOI 10.1109/JSSC.2012.2230553
   Stromatias E, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00350
   Van Rullen R, 2001, NEURAL COMPUT, V13, P1255, DOI 10.1162/08997660152002852
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Yousefzadeh A, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351562
NR 25
TC 16
Z9 17
U1 0
U2 1
PY 2019
BP 81
EP 85
DI 10.1109/aicas.2019.8771624
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
DA 2023-11-11
ER

PT J
AU McKinstry, JL
   Edelman, GM
AF McKinstry, Jeffrey L.
   Edelman, Gerald M.
TI Temporal sequence learning in winner-take-all networks of spiking
   neurons demonstrated in a brain-based device
SO FRONTIERS IN NEUROROBOTICS
DT Article
DE neurorobotics; sequence learning; spiking network; winner-take-all;
   motor control and learning/plasticity; spike-timing dependent
   plasticity; sensorimotor control; large-scale spiking neural networks
ID MOTOR; MOVEMENTS; MODEL
AB Animal behavior often involves a temporally ordered sequence of actions learned from experience. Here we describe simulations of interconnected networks of spiking neurons that learn to generate patterns of activity in correct temporal order. The simulation consists of large-scale networks of thousands of excitatory and inhibitory neurons that exhibit short-term synaptic plasticity and spike timing dependent synaptic plasticity. The neural architecture within each area is arranged to evoke vvinner-take-all (WTA) patterns of neural activity that persist for tens of milliseconds. In order to generate and switch between consecutive firing patterns in correct temporal order, a reentrant exchange of signals between these areas was necessary. To demonstrate the capacity of this arrangement, we used the simulation to train a brain based device responding to visual input by autonomously generating temporal sequences of motor actions.
C1 [McKinstry, Jeffrey L.; Edelman, Gerald M.] Inst Neurosci, San Diego, CA 92037 USA.
RP McKinstry, JL (corresponding author), Inst Neurosci, 800 Silverado St,Suite 302, San Diego, CA 92037 USA.
EM mckinstry@nsi.edu
CR [Anonymous], 1988, C PROGRAMMING LANGUA
   [Anonymous], 1978, MINDFUL BRAIN
   Averbeck BB, 2002, P NATL ACAD SCI USA, V99, P13172, DOI 10.1073/pnas.162485599
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Binshtok AM, 2006, J NEUROSCI, V26, P708, DOI 10.1523/JNEUROSCI.4409-05.2006
   Chen YQ, 2013, FRONT COMPUT NEUROSC, V7, DOI 10.3389/fncom.2013.00016
   Chersi F, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0027652
   Dayan P., 2001, THEORETICAL NEUROSCI
   Edelman GM, 2007, SCIENCE, V318, P1103, DOI 10.1126/science.1148677
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2008, P NATL ACAD SCI USA, V105, P3593, DOI 10.1073/pnas.0712231105
   Izhikevich EM, 2010, PHILOS T R SOC A, V368, P5061, DOI 10.1098/rsta.2010.0130
   Jones LM, 2007, P NATL ACAD SCI USA, V104, P18772, DOI 10.1073/pnas.0705546104
   Liu JK, 2009, J NEUROSCI, V29, P13172, DOI 10.1523/JNEUROSCI.2358-09.2009
   McKinstry JL, 2008, NEURAL NETWORKS, V21, P553, DOI 10.1016/j.neunet.2008.01.004
   Myme CIO, 2003, J NEUROPHYSIOL, V90, P771, DOI 10.1152/jn.00070.2003
   Nakajima T, 2009, J NEUROPHYSIOL, V101, P1883, DOI 10.1152/jn.90636.2008
   Rhodes BJ, 2004, HUM MOVEMENT SCI, V23, P699, DOI 10.1016/j.humov.2004.10.008
   Rutishauser U, 2009, NEURAL COMPUT, V21, P478, DOI 10.1162/neco.2008.03-08-734
   Salinas E, 2009, J NEUROSCI, V29, P4369, DOI 10.1523/JNEUROSCI.0164-09.2009
   Seidemann E, 1996, J NEUROSCI, V16, P752
   Tanji J, 2001, ANNU REV NEUROSCI, V24, P631, DOI 10.1146/annurev.neuro.24.1.631
   Verduzco-Flores SO, 2012, J COMPUT NEUROSCI, V32, P403, DOI 10.1007/s10827-011-0360-x
   ZUCKER RS, 1989, ANNU REV NEUROSCI, V12, P13, DOI 10.1146/annurev.ne.12.030189.000305
NR 24
TC 9
Z9 9
U1 0
U2 3
PY 2013
VL 7
AR 10
DI 10.3389/fnbot.2013.00010
WC Computer Science, Artificial Intelligence; Robotics; Neurosciences
DA 2023-11-11
ER

PT C
AU Pu, JR
   Nambiar, VP
   Do, AT
   Goh, WL
AF Pu, Junran
   Nambiar, Vishnu P.
   Anh Tuan Do
   Goh, Wang Ling
GP IEEE
TI Block-Based Spiking Neural Network Hardware with Deme Genetic Algorithm
SO 2019 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS (ISCAS)
SE IEEE International Symposium on Circuits and Systems
DT Proceedings Paper
CT IEEE International Symposium on Circuits and Systems (IEEE ISCAS)
CY MAY 26-29, 2019
CL Sapporo, JAPAN
DE spiking neural network hardware; spiking neuron model; spatial
   architecture; deme genetic algorithm
AB Hardware implementation of spiking neural networks (SNN) has been the focus of many previous works due to its higher execution speed. A block-based SNN architecture with a simple spiking neuron model is proposed in this paper. Compared to traditional spiking neuron models, the proposed model simplifies the equation of the membrane potential for ease of hardware implementation. The block-based SNN architecture also makes the hardware implementation more scalable and simplifies floorplanning. Deme genetic algorithm (GA) was applied for training the SNN model, and a population encoding scheme was used for spike time conversion. Two case studies were carried out to verify the functionality of the proposed model, namely number recognition and Fisher Iris classification. Experimental results showed that the proposed SNN model with deme GA was able to achieve comparable or higher classification accuracy than previous works.
C1 [Pu, Junran; Goh, Wang Ling] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore, Singapore.
   [Nambiar, Vishnu P.; Anh Tuan Do] ASTAR, Inst Microelect IME, IC Design Dept, Singapore, Singapore.
RP Pu, JR (corresponding author), Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore, Singapore.
EM junran001@e.ntu.edu.sg; vishnu_paramasivam@ime.a-star.edu.sg;
   doat@ime.a-star.edu.sg; ewlgoh@ntu.edu.sg
CR [Anonymous], 2015, SPLST
   [Anonymous], 1991, TOUCHSTONE DELTA SYS
   [Anonymous], 2015, 2015 INT JOINT C NEU, DOI DOI 10.1109/IJCNN.2015.7280625
   [Anonymous], 1996, GALIB C LIB GENETIC
   [Anonymous], 1989, GENETIC ALGORITHMS S
   Dheeru D., 2017, UCI MACHINE LEARNING
   Gerstner W., 2002, SPIKING NEURON MODEL
   Ghosh-Dastidar S, 2007, INTEGR COMPUT-AID E, V14, P187
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Jin X, 2008, IEEE IJCNN, P2812, DOI 10.1109/IJCNN.2008.4634194
   Kumar S, 2002, IEEE COMP SOC ANN, P117, DOI 10.1109/ISVLSI.2002.1016885
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Olshausen BA, 2004, CURR OPIN NEUROBIOL, V14, P481, DOI 10.1016/j.conb.2004.07.007
   Xin JG, 2001, IEEE IJCNN, P1772, DOI 10.1109/IJCNN.2001.938430
NR 14
TC 5
Z9 5
U1 0
U2 0
PY 2019
WC Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Cessac, B
AF Cessac, B.
TI A VIEW OF NEURAL NETWORKS AS DYNAMICAL SYSTEMS
SO INTERNATIONAL JOURNAL OF BIFURCATION AND CHAOS
DT Review
DE Neural networks; dynamical systems; synaptic plasticity; linear
   response; chaos
ID LONG-TERM POTENTIATION; MEAN-FIELD THEORY; FIRE NEURONS; RECURRENT
   NETWORKS; SPIKING NEURONS; CHAOS; MODEL; SYNCHRONIZATION; TRANSMISSION;
   BIFURCATION
AB We present some recent investigations resulting from the modeling of neural networks as dynamical systems, and deal with the following questions, adressed in the context of specific models.
   (i) Characterizing the collective dynamics;
   (ii) Statistical analysis of spike trains;
   (iii) Interplay between dynamics and network structure;
   (iv) Effects of synaptic plasticity.
C1 [Cessac, B.] Univ Nice Sophia Antipolis, CNRS, Lab JA Dieudonne, UMR 6621, Nice, France.
   [Cessac, B.] INRIA, EPI NeuroMathComp, F-06902 Sophia Antipolis, France.
RP Cessac, B (corresponding author), Univ Nice Sophia Antipolis, CNRS, Lab JA Dieudonne, UMR 6621, Nice, France.
CR ABBOTT LF, 1993, PHYS REV E, V48, P1483, DOI 10.1103/PhysRevE.48.1483
   ABELES M, 1993, CONCEPT NEUROSCI, V4, P131
   Abeles M., 1982, LOCAL CORTICAL CIRCU, DOI DOI 10.1007/978-3-642-81708-3
   Abeles M., 1991, CORTICONICS NEURAL C
   Adrian ED, 1926, J PHYSIOL-LONDON, V61, P151, DOI 10.1113/jphysiol.1926.sp002281
   Afraimovich VS, 2007, NONLINEARITY, V20, P1761, DOI 10.1088/0951-7715/20/7/011
   AMARI S, 1972, IEEE T SYST MAN CYB, VSMC2, P643, DOI 10.1109/TSMC.1972.4309193
   AMARI SI, 1977, SIAM J APPL MATH, V33, P95, DOI 10.1137/0133008
   [Anonymous], 2001, ION CHANNELS EXCITAB
   [Anonymous], 1990, ZETA FUNCTIONS PERIO
   [Anonymous], PUBLICATIONS MATH IH
   [Anonymous], P 11 EUR S ART NEUR
   [Anonymous], 2006, ARXIVQBIO0611072
   [Anonymous], PHYS REV E
   Arabzadeh E, 2006, J NEUROSCI, V26, P9216, DOI 10.1523/JNEUROSCI.1491-06.2006
   ARTOLA A, 1990, NATURE, V347, P69, DOI 10.1038/347069a0
   Ashwin P, 2005, NONLINEARITY, V18, P2035, DOI 10.1088/0951-7715/18/5/009
   Atay FM, 2006, PHYSICA D, V224, P35, DOI 10.1016/j.physd.2006.09.018
   Barahona M, 2002, PHYS REV LETT, V89, DOI 10.1103/PhysRevLett.89.054101
   Barbieri R, 2004, NEURAL COMPUT, V16, P277, DOI 10.1162/089976604322742038
   BARJAVEL Rene, 1944, VOYAGEUR IMPRUDENT
   BEN AROUS G, 1997, ANN PROBAB, V25, P1367
   BENAROUS G, 1995, PROBAB THEORY REL, V103, P431
   Bi GQ, 2001, ANNU REV NEUROSCI, V24, P139, DOI 10.1146/annurev.neuro.24.1.139
   BIENENSTOCK EL, 1982, J NEUROSCI, V2, P32, DOI 10.1523/jneurosci.02-01-00032.1982
   Blanchard P, 2000, J STAT PHYS, V98, P375, DOI 10.1023/A:1018639308981
   Blanchard P, 2009, UNDERST COMPLEX SYST, P1, DOI 10.1007/978-3-540-87829-2_1
   BLISS TVP, 1973, J PHYSIOL-LONDON, V232, P357, DOI 10.1113/jphysiol.1973.sp010274
   Boccaletti S, 2006, PHYS REP, V424, P175, DOI 10.1016/j.physrep.2005.10.009
   Bowen R, 2008, LECT NOTES MATH, V470, P1, DOI 10.1007/978-3-540-77695-6
   BOWEN R, 1975, EQUILIBRIUM STATES E, V470
   Bressloff PC, 2000, SIAM J APPL MATH, V60, P820, DOI 10.1137/S0036139998339643
   Bressloff PC, 2000, NEURAL COMPUT, V12, P91, DOI 10.1162/089976600300015907
   BRESSLOFF PC, 2003, EPILEPSY DYNAMIC DIS, pCH7
   Brette R, 2005, J NEUROPHYSIOL, V94, P3637, DOI 10.1152/jn.00686.2005
   Brette R, 2007, J COMPUT NEUROSCI, V23, P349, DOI 10.1007/s10827-007-0038-6
   Brunel N, 1998, J THEOR BIOL, V195, P87, DOI 10.1006/jtbi.1998.0782
   Brunel N, 2003, NEURAL COMPUT, V15, P2281, DOI 10.1162/089976603322362365
   Brunel N, 1999, NEURAL COMPUT, V11, P1621, DOI 10.1162/089976699300016179
   Cessac B, 2007, NONLINEARITY, V20, P2883, DOI 10.1088/0951-7715/20/12/007
   Cessac B, 2008, J MATH BIOL, V56, P311, DOI 10.1007/s00285-007-0117-3
   Cessac B, 2006, CHAOS, V16, DOI 10.1063/1.2126813
   Cessac B, 2007, EUR PHYS J-SPEC TOP, V142, P7, DOI 10.1140/epjst/e2007-00003-5
   Cessac B, 2004, PHYS REV E, V70, DOI 10.1103/PhysRevE.70.056111
   CESSAC B, 1994, PHYSICA D, V74, P24, DOI 10.1016/0167-2789(94)90024-8
   CESSAC B, 1995, J PHYS I, V5, P409, DOI 10.1051/jp1:1995135
   CESSAC B, 1994, EUROPHYS LETT, V26, P577, DOI 10.1209/0295-5075/26/8/004
   CESSAC B, 2009, J STAT PHYS IN PRESS
   Cessac B, 2007, PHYSICA D, V225, P13, DOI 10.1016/j.physd.2006.09.034
   Cessac B, 2008, FRONT COMPUT NEUROSC, V2, DOI 10.3389/neuro.10.002.2008
   CHAZOTTES J, 2009, ENCY COMPLE IN PRESS
   Chazottes JR, 1998, J STAT PHYS, V90, P697, DOI 10.1023/A:1023220802597
   Chizhov AV, 2007, PHYS REV E, V75, DOI 10.1103/PhysRevE.75.011924
   Chow CC, 2000, NEURAL COMPUT, V12, P1643, DOI 10.1162/089976600300015295
   Coombes S, 1999, PHYS REV E, V60, P2086, DOI 10.1103/PhysRevE.60.2086
   Coombes S, 1999, PHYS LETT A, V255, P49, DOI 10.1016/S0375-9601(99)00172-3
   Cooper L., 2004, THEORY CORTICAL PLAS
   Cosandier-Rimélé D, 2007, IEEE T BIO-MED ENG, V54, P380, DOI 10.1109/TBME.2006.890489
   COSNARD M, 1985, LECT NOTES MATH, V1163, P23
   COSNARD M, 1993, MATH APPL BIOL MED
   CRISANTI A, 1987, PHYS REV A, V36, P4922, DOI 10.1103/PhysRevA.36.4922
   CRISANTI A, 1988, PHYS REV A, V37, P4865, DOI 10.1103/PhysRevA.37.4865
   CRONIN J, 1987, MATH ASPECTS H HUXLE
   Dauce E, 1998, NEURAL NETWORKS, V11, P521, DOI 10.1016/S0893-6080(97)00131-7
   Dayan P., 2001, THEORETICAL NEUROSCI
   DEALMEIDA JRL, 1978, J PHYS A-MATH GEN, V11, P983, DOI 10.1088/0305-4470/11/5/028
   Delorme A, 2001, NEUROCOMPUTING, V38, P539, DOI 10.1016/S0925-2312(01)00403-9
   Diesmann M, 1999, NATURE, V402, P529, DOI 10.1038/990101
   DUDEK SM, 1993, J NEUROSCI, V13, P2910
   ECKMANN JP, 1985, REV MOD PHYS, V57, P617, DOI 10.1103/RevModPhys.57.617
   Ermentrout B, 1998, REP PROG PHYS, V61, P353, DOI 10.1088/0034-4885/61/4/002
   ERMENTROUT GB, 1984, SIAM J MATH ANAL, V15, P215, DOI 10.1137/0515019
   ERNST U, 1995, PHYS REV LETT, V74, P1570, DOI 10.1103/PhysRevLett.74.1570
   FAUGERAS O, 2008, FRONT NEUROSCI UNPUB
   FITZHUGH R, 1966, J GEN PHYSIOL, V49, P989, DOI 10.1085/jgp.49.5.989
   Fourcaud N, 2002, NEURAL COMPUT, V14, P2057, DOI 10.1162/089976602320264015
   Fourcaud-Trocmé N, 2003, J NEUROSCI, V23, P11628
   GAMBAUDO J, 1988, CHAOS THEORIE EXPERI
   Gao Y, 2008, ENTROPY-SWITZ, V10, P71, DOI 10.3390/entropy-e10020071
   GERSTNER W, 1995, PHYS REV E, V51, P738, DOI 10.1103/PhysRevE.51.738
   Gerstner W, 2002, BIOL CYBERN, V87, P404, DOI 10.1007/s00422-002-0353-y
   Gerstner W., 2002, SPIKING NEURON MODEL
   GIRKO VL, 1985, THEOR PROBAB APPL+, V29, P694, DOI 10.1137/1129095
   Gong PL, 2007, PHYS REV LETT, V98, DOI 10.1103/PhysRevLett.98.048104
   Grammont F, 1999, EXP BRAIN RES, V128, P118, DOI 10.1007/s002210050826
   GRIMBERT F, 2005, RR5597 INRIA
   Grinstein G, 2005, P NATL ACAD SCI USA, V102, P9948, DOI 10.1073/pnas.0504127102
   GUCKENHEIMER J, 1993, B MATH BIOL, V55, P937, DOI 10.1007/BF02460693
   Guckenheimer J, 2002, SIAM J APPL DYN SYST, V1, P105, DOI 10.1137/S1111111101394040
   Guionnet A, 1997, PROBAB THEORY REL, V109, P183, DOI 10.1007/s004400050130
   Gupta, 2003, STATIC DYNAMIC NEURA, DOI 10.1002/0471427950
   Hansel D, 1998, NEURAL COMPUT, V10, P467, DOI 10.1162/089976698300017845
   Hasegawa H, 2005, PHYS REV E, V72, DOI 10.1103/PhysRevE.72.056139
   Hebb D. O., 1949, ORG BEHAV A NEUROPSY
   Hertz J, 1998, THEORETICAL ASPECTS OF NEURAL COMPUTATION, P135
   HIRSCH MW, 1989, NEURAL NETWORKS, V2, P331, DOI 10.1016/0893-6080(89)90018-X
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Hoppenstaedt F, 1997, WEAKLY CONNECTED NEU
   HUNT BR, 1992, B AM MATH SOC, V27, P217, DOI 10.1090/S0273-0979-1992-00328-2
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, NEURAL COMPUT, V15, P1511, DOI 10.1162/089976603321891783
   Jahnke S, 2008, PHYS REV LETT, V100, DOI 10.1103/PhysRevLett.100.048102
   Jalan S, 2006, CHAOS, V16, DOI 10.1063/1.2336415
   JANSEN BH, 1995, BIOL CYBERN, V73, P357, DOI 10.1007/BF00199471
   JAYNES ET, 1957, PHYS REV, V106, P620, DOI 10.1103/PhysRev.106.620
   Johnson DH, 2004, J COMPUT NEUROSCI, V16, P69, DOI 10.1023/B:JCNS.0000004842.04535.7c
   JOHNSON KO, 1980, J NEUROPHYSIOL, V43, P1793, DOI 10.1152/jn.1980.43.6.1793
   JOLIVET R, 2006, INTEGRATE AND FIRE M
   Jost J, 2002, PHYS REV E, V65, DOI 10.1103/PhysRevE.65.016201
   Katok A., 1998, INTRO MODERN THEORY
   Keener J., 1998, INTERD APPL, DOI 10.1007/b98841
   KEENER JP, 1981, SIAM J APPL MATH, V41, P503, DOI 10.1137/0141042
   Keller G., 1998, EQUILIBRIUM STATES E
   KIRKPATRICK S, 1978, PHYS REV B, V17, P4384, DOI 10.1103/PhysRevB.17.4384
   Kirst C, 2009, FRONT NEUROSCI-SWITZ, V3, P2, DOI 10.3389/neuro.01.009.2009
   Kirst C, 2009, PHYS REV LETT, V102, DOI 10.1103/PhysRevLett.102.068101
   KNIGHT BW, 1972, J GEN PHYSIOL, V59, P734, DOI 10.1085/jgp.59.6.734
   Koch Christof, 1999, P1
   Lago-Fernández LF, 2000, PHYS REV LETT, V84, P2758, DOI 10.1103/PhysRevLett.84.2758
   Lapique L., 1907, J PHYSL PATHOL GEN, V9, P620, DOI DOI 10.1007/S00422-007-0189-6
   LEVY WB, 1983, NEUROSCIENCE, V8, P791, DOI 10.1016/0306-4522(83)90010-6
   Litvak V, 2003, J NEUROSCI, V23, P3006
   MACKAY RS, 1986, PHYSICA D, V19, P206, DOI 10.1016/0167-2789(86)90020-5
   MAHON S, 2003, J PHYSL, V1, P947
   Malenka RC, 1999, SCIENCE, V285, P1870, DOI 10.1126/science.285.5435.1870
   MALSBURG CV, 1973, KYBERNETIK, V14, P85, DOI 10.1007/BF00288907
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   Mattia M, 2002, PHYS REV E, V66, DOI 10.1103/PhysRevE.66.051917
   Memmesheimer RM, 2006, PHYSICA D, V224, P182, DOI 10.1016/j.physd.2006.09.037
   Mezard M., 1987, SPIN GLASS THEORY IN, V9
   MILLER KD, 1989, SCIENCE, V245, P605, DOI 10.1126/science.2762813
   MILNOR J, 1985, COMMUN MATH PHYS, V99, P177, DOI 10.1007/BF01212280
   MIROLLO RE, 1990, SIAM J APPL MATH, V50, P1645, DOI 10.1137/0150098
   MOLGEDEY L, 1992, PHYS REV LETT, V69, P3717, DOI 10.1103/PhysRevLett.69.3717
   Moynot O, 2002, PROBAB THEORY REL, V123, P41, DOI 10.1007/s004400100182
   NAGUMO J, 1962, P IRE, V50, P2061, DOI 10.1109/JRPROC.1962.288235
   Nelson M., 1995, H HUXLEY MODEL, P27
   Nemenman I, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.1000025
   Nirenberg S, 2003, P NATL ACAD SCI USA, V100, P7348, DOI 10.1073/pnas.1131895100
   Nishikawa T, 2003, PHYS REV LETT, V91, DOI 10.1103/PhysRevLett.91.014101
   OSBONE L, 2008, ARXIVORG08033837
   PERRINET L, 2001, NEUROCOMPUTING, V38
   Pillow JW, 2008, NATURE, V454, P995, DOI 10.1038/nature07140
   Pillow JW, 2005, J NEUROSCI, V25, P11003, DOI 10.1523/JNEUROSCI.3305-05.2005
   Rao RPN, 2001, NEURAL COMPUT, V13, P2221, DOI 10.1162/089976601750541787
   Renart A, 2004, MATH COMP BIOL SER, P431
   Rieke F., 1996, SPIKES EXPLORING NEU
   ROSTROGONZALEZ H, 2009, COMP NEUR M CNS
   ROTTERDAM A, 1982, B MATH BIOL, V44, P283
   Roudi Y, 2009, PLOS COMPUT BIOL, V5, DOI 10.1371/journal.pcbi.1000380
   Rudolph M, 2006, NEURAL COMPUT, V18, P2146, DOI 10.1162/neco.2006.18.9.2146
   Ruelle D, 2005, COMMUN MATH PHYS, V258, P445, DOI 10.1007/s00220-004-1267-4
   Ruelle D, 1999, J STAT PHYS, V95, P393, DOI 10.1023/A:1004593915069
   RUELLE D, 1971, COMMUN MATH PHYS, V20, P167, DOI 10.1007/BF01646553
   Ruelle D., 1978, THERMODYNAMIC FORMAL
   Schneidman E, 2006, NATURE, V440, P1007, DOI 10.1038/nature04701
   Segev R, 2004, NAT NEUROSCI, V7, P1155, DOI 10.1038/nn1323
   Senn W, 2000, SIAM J APPL MATH, V61, P1143
   Sinai YG., 1972, RUSSIAN MATH SURV, V27, P21, DOI [10.1070/RM1972v027n04ABEH001383, DOI 10.1070/RM1972V027N04ABEH001383]
   SINANOVIC A, 2006, SIGNAL PROCESS UNPUB
   Siri B, 2008, NEURAL COMPUT, V20, P2937, DOI 10.1162/neco.2008.05-07-530
   Siri B, 2007, J PHYSIOL-PARIS, V101, P136, DOI 10.1016/j.jphysparis.2007.10.003
   SOMPOLINSKY H, 1982, PHYS REV B, V25, P6860, DOI 10.1103/PhysRevB.25.6860
   SOMPOLINSKY H, 1988, PHYS REV LETT, V61, P259, DOI 10.1103/PhysRevLett.61.259
   Soula H, 2006, NEURAL COMPUT, V18, P60, DOI 10.1162/089976606774841567
   Soula H, 2007, NEURAL COMPUT, V19, P3262, DOI 10.1162/neco.2007.19.12.3262
   THEUNISSEN F, 1995, J COMPUT NEUROSCI, V2, P149, DOI 10.1007/BF00961885
   Thomson AM, 2007, FRONT NEUROSCI-SWITZ, V1, P19, DOI 10.3389/neuro.01.1.1.002.2007
   Timme M, 2002, PHYS REV LETT, V89, DOI 10.1103/PhysRevLett.89.258701
   TOUBOUL J, 2007, J PHYSL PAR IN PRESS
   Touboul J, 2008, SIAM J APPL MATH, V68, P1045, DOI 10.1137/070687268
   TREVES A, 1993, NETWORK-COMP NEURAL, V4, P259, DOI 10.1088/0954-898X/4/3/002
   VAN VREESWIJK C, 1998, NEURAL COMPUT, V10, P1321
   vanVreeswijk C, 1996, SCIENCE, V274, P1724, DOI 10.1126/science.274.5293.1724
   VANVREESWIJK C, 2004, WHAT IS NEURAL CODE
   VANVREESWIJK C, 1997, COMPUT NEUROSCI, V97
   Xie XH, 2002, NEURAL COMPUT, V14, P2627, DOI 10.1162/089976602760408008
   [No title captured]
NR 178
TC 19
Z9 21
U1 0
U2 20
PD JUN
PY 2010
VL 20
IS 6
BP 1585
EP 1629
DI 10.1142/S0218127410026721
WC Mathematics, Interdisciplinary Applications; Multidisciplinary Sciences
DA 2023-11-11
ER

PT C
AU Ahmadi, A
   Zwolinski, M
AF Ahmadi, Arash
   Zwolinski, Mark
GP IEEE
TI A Modified Izhikevich Model For Circuit Implementation of Spiking Neural
   Networks
SO 2010 FIRST IEEE LATIN AMERICAN SYMPOSIUM ON CIRCUITS AND SYSTEMS
   (LASCAS)
SE IEEE Latin American Symposium on Circuits and Systems
DT Proceedings Paper
CT 1st IEEE Latin American Symposium on Circuits and Systems (LASCAS)
CY FEB 24-26, 2010
CL BAHAMAS
DE Neural networks; Neural network hardware
AB The Izhikevich neuron model reproduces the spiking and bursting behaviour of certain types of cortical neurons. This model has a second order non-linearity that makes it difficult to implement in hardware. We propose a simplified version of the model that has a piecewise-linear relationship. This modification simplifies the hardware implementation but demonstrates similar dynamic behaviour.
C1 [Ahmadi, Arash; Zwolinski, Mark] Univ Southampton, Sch Elect & Comp Sci, Southampton SO17 1BJ, Hants, England.
RP Ahmadi, A (corresponding author), Univ Southampton, Sch Elect & Comp Sci, Southampton SO17 1BJ, Hants, England.
EM aa5@ecs.soton.ac.uk; mzc@ecs.soton.ac.uk
CR [Anonymous], J PHYSL, DOI DOI 10.1007/BF02459568
   Brunel N, 2007, BIOL CYBERN, V97, P337, DOI 10.1007/s00422-007-0190-0
   Cassidy A, 2007, 2007 IEEE BIOMEDICAL CIRCUITS AND SYSTEMS CONFERENCE, P75, DOI 10.1109/BIOCAS.2007.4463312
   Furber S, 2006, P AISB WORKSH GC5 AR, P29
   Gerstner W., 2002, SPIKING NEURON MODEL
   Glackin B, 2005, LECT NOTES COMPUT SC, V3512, P552
   Izhikevich E M, 2007, DYNAM SYST, DOI [DOI 10.1017/S0143385704000173, DOI 10.7551/MITPRESS/2526.001.0001]
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Mokhtar M, 2008, LECT NOTES COMPUT SC, V5216, P362
   Renaud S, 2007, IEEE INT SYMP CIRC S, P3355, DOI 10.1109/ISCAS.2007.378286
   Shayani H., 2008, NASA ESA C AD HARDW, P236
   Shayani H., 2008, 16 EUR S ART NEUR NE, P197
   Wijekoon JHB, 2008, NEURAL NETWORKS, V21, P524, DOI 10.1016/j.neunet.2007.12.037
NR 13
TC 1
Z9 1
U1 0
U2 0
PY 2010
BP 192
EP 195
WC Computer Science, Theory & Methods; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Daffron, C
   Chan, J
   Disney, A
   Bechtel, L
   Wagner, R
   Dean, ME
   Rose, GS
   Plank, JS
   Birdwell, JD
   Schuman, CD
AF Daffron, Christopher
   Chan, Jason
   Disney, Adam
   Bechtel, Luke
   Wagner, Ryan
   Dean, Mark E.
   Rose, Garrett S.
   Plank, James S.
   Birdwell, J. Douglas
   Schuman, Catherine D.
GP IEEE
TI Extensions and Enhancements for the DANNA Neuromorphic Architecture
SO SOUTHEASTCON 2016
SE IEEE SoutheastCon-Proceedings
DT Proceedings Paper
CT SoutheastCon
CY MAR 30-APR 03, 2016
CL Norfolk, VA
ID SPIKING; NETWORK
AB Dynamic Adaptive Neural Network Arrays (DANNAs) are neuromorphic systems that have been developed for hardware implementation. They feature highly adaptive and programmable structural elements, which model artificial neural networks with spiking behavior. In this paper, we highlight the current hardware implementations of DANNA, including their features and functionalities. We conclude with future directions.
C1 [Daffron, Christopher; Chan, Jason; Disney, Adam; Bechtel, Luke; Wagner, Ryan; Dean, Mark E.; Rose, Garrett S.; Plank, James S.; Birdwell, J. Douglas] Univ Tennessee, Dept Elect Engn & Comp Sci, Knoxville, TN 37996 USA.
   [Schuman, Catherine D.] Oak Ridge Natl Lab, Computat Data Analyt, Oak Ridge, TN USA.
RP Daffron, C (corresponding author), Univ Tennessee, Dept Elect Engn & Comp Sci, Knoxville, TN 37996 USA.
CR Anand R, 2009, SELF-DEFENSE IN INTERNATIONAL RELATIONS, P1, DOI 10.1057/9780230245747
   [Anonymous], 2014, P 2014 BIOM SCI ENG
   Bartolozzi C, 2007, NEURAL COMPUT, V19, P2581, DOI 10.1162/neco.2007.19.10.2581
   Cassidy A, 2007, 2007 IEEE BIOMEDICAL CIRCUITS AND SYSTEMS CONFERENCE, P75, DOI 10.1109/BIOCAS.2007.4463312
   Dean Mark E., 2014, Unconventional Computation and Natural Computation. 13th International Conference. Proceedings: LNCS 8553, P129, DOI 10.1007/978-3-319-08123-6_11
   Furber SB, 2013, IEEE T COMPUT, V62, P2454, DOI 10.1109/TC.2012.142
   Glackin B, 2005, LECT NOTES COMPUT SC, V3512, P552
   Hu M, 2012, DES AUT CON, P498
   Jackson BL, 2013, ACM J EMERG TECH COM, V9, DOI 10.1145/2463585.2463588
   Kunkel S, 2012, FRONT NEUROINFORM, V5, DOI 10.3389/fninf.2011.00035
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Renaud S, 2007, IEEE INT SYMP CIRC S, P3355, DOI 10.1109/ISCAS.2007.378286
   Snook J., 2009, US Patent, Patent No. [7,533,071, 7533071]
   Soltiz M, 2013, IEEE T COMPUT, V62, P1597, DOI 10.1109/TC.2013.75
   Vogelstein RJ, 2007, IEEE T NEURAL NETWOR, V18, P253, DOI 10.1109/TNN.2006.883007
NR 15
TC 1
Z9 1
U1 0
U2 0
PY 2016
WC Computer Science, Theory & Methods; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Ekelmans, P
   Kraynyukovas, N
   Tchumatchenko, T
AF Ekelmans, Pierre
   Kraynyukovas, Nataliya
   Tchumatchenko, Tatjana
TI Targeting operational regimes of interest in recurrent neural networks
SO PLOS COMPUTATIONAL BIOLOGY
DT Article
ID IN-VIVO; VISUAL-CORTEX; NEURONS; MOUSE; EXCITATION; INTEGRATION;
   CIRCUITS; COLUMN; MODEL; ORGANIZATION
AB Neural computations emerge from local recurrent neural circuits or computational units such as cortical columns that comprise hundreds to a few thousand neurons. Continuous progress in connectomics, electrophysiology, and calcium imaging require tractable spiking network models that can consistently incorporate new information about the network structure and reproduce the recorded neural activity features. However, for spiking networks, it is challenging to predict which connectivity configurations and neural properties can generate fundamental operational states and specific experimentally reported nonlinear cortical computations. Theoretical descriptions for the computational state of cortical spiking circuits are diverse, including the balanced state where excitatory and inhibitory inputs balance almost perfectly or the inhibition stabilized state (ISN) where the excitatory part of the circuit is unstable. It remains an open question whether these states can co-exist with experimentally reported nonlinear computations and whether they can be recovered in biologically realistic implementations of spiking networks. Here, we show how to identify spiking network connectivity patterns underlying diverse nonlinear computations such as XOR, bistability, inhibitory stabilization, supersaturation, and persistent activity. We establish a mapping between the stabilized supralinear network (SSN) and spiking activity which allows us to pinpoint the location in parameter space where these activity regimes occur. Notably, we find that biologically-sized spiking networks can have irregular asynchronous activity that does not require strong excitation-inhibition balance or large feedforward input and we show that the dynamic firing rate trajectories in spiking networks can be precisely targeted without error-driven training algorithms.
   Author summaryBiological neural networks must be able to execute diverse nonlinear operations on signals in order to perform complex information processing. While nonlinear transformations have been observed experimentally or in specific theoretical models, a comprehensive theory linking the parameters of a network of spiking neurons to its computations is still lacking. We show that spiking networks can be accurately approximated with a mathematically tractable model, the Stabilized Supralinear Network. Using the mapping we derived between these two frameworks, we show that spiking networks have a rich repertoire of nonlinear regimes at their disposal and link the existence of such regimes to precise conditions on parameters. Notably, we show that classical excitatory-inhibitory networks of leaky integrate-and-fire neurons support nonlinear transformations without the need for synaptic plasticity, intricate wiring diagrams or a complex system of different cell types. The capacity of a network to reliably perform such operations has profound functional implications as they can be the basis permitting the execution of complex computations.
C1 [Ekelmans, Pierre; Kraynyukovas, Nataliya; Tchumatchenko, Tatjana] Max Planck Inst Brain Res, Theory Neural Dynam Grp, Frankfurt, Germany.
   [Ekelmans, Pierre] Frankfurt Inst Adv Studies, Frankfurt, Germany.
   [Kraynyukovas, Nataliya; Tchumatchenko, Tatjana] Univ Klinikum Bonn, Inst Expt Epileptol & Cognit Res, Life & Brain Ctr, Bonn, Germany.
   [Tchumatchenko, Tatjana] Johannes Gutenberg Univ Mainz, Inst Physiol Chem, Med Ctr Johannes Gutenberg, Mainz, Germany.
RP Tchumatchenko, T (corresponding author), Max Planck Inst Brain Res, Theory Neural Dynam Grp, Frankfurt, Germany.; Tchumatchenko, T (corresponding author), Univ Klinikum Bonn, Inst Expt Epileptol & Cognit Res, Life & Brain Ctr, Bonn, Germany.; Tchumatchenko, T (corresponding author), Johannes Gutenberg Univ Mainz, Inst Physiol Chem, Med Ctr Johannes Gutenberg, Mainz, Germany.
EM tatjana.tchumatchenko@uni-mainz.de
CR Ahmadian Y, 2021, NEURON, V109, P3373, DOI 10.1016/j.neuron.2021.07.031
   Ahmadian Y, 2013, NEURAL COMPUT, V25, P1994, DOI 10.1162/NECO_a_00472
   Allen Institute for Brain Science, 2019, SYNAPTIC PHYSL COARS
   AMIT DJ, 1991, NETWORK-COMP NEURAL, V2, P259, DOI 10.1088/0954-898X/2/3/003
   Amit DJ, 1997, CEREB CORTEX, V7, P237, DOI 10.1093/cercor/7.3.237
   Atallah BV, 2012, NEURON, V73, P159, DOI 10.1016/j.neuron.2011.12.013
   Baker C, 2020, PLOS COMPUT BIOL, V16, DOI 10.1371/journal.pcbi.1008192
   Barnes SJ, 2015, J NEUROSCI, V35, P9024, DOI 10.1523/JNEUROSCI.4583-14.2015
   Becker S, 2022, PLOS COMPUT BIOL, V18, DOI 10.1371/journal.pcbi.1010543
   Billeh YN, 2020, NEURON, V106, P388, DOI 10.1016/j.neuron.2020.01.040
   Bock DD, 2011, NATURE, V471, P177, DOI 10.1038/nature09802
   Brunel N, 1999, NEURAL COMPUT, V11, P1621, DOI 10.1162/089976699300016179
   Brunel N, 2000, J COMPUT NEUROSCI, V8, P183, DOI 10.1023/A:1008925309027
   Buice MA, 2010, NEURAL COMPUT, V22, P377, DOI 10.1162/neco.2009.02-09-960
   Busse L, 2009, NEURON, V64, P931, DOI 10.1016/j.neuron.2009.11.004
   Carandini M, 2012, NAT REV NEUROSCI, V13, P51, DOI 10.1038/nrn3136
   Cavanagh SE, 2020, FRONT NEURAL CIRCUIT, V14, DOI 10.3389/fncir.2020.615626
   Cavanagh SE, 2016, ELIFE, V5, DOI 10.7554/eLife.18937
   Chen GF, 2019, NAT COMMUN, V10, DOI 10.1038/s41467-019-08550-1
   Cossell L, 2015, NATURE, V518, P399, DOI 10.1038/nature14182
   de Kock CPJ, 2021, COMMUN BIOL, V4, DOI 10.1038/s42003-021-02241-8
   Destexhe A, 2001, NEUROSCIENCE, V107, P13, DOI 10.1016/S0306-4522(01)00344-X
   FUSTER JM, 1971, SCIENCE, V173, P652, DOI 10.1126/science.173.3997.652
   Gentet LJ, 2012, NAT NEUROSCI, V15, P607, DOI 10.1038/nn.3051
   Gerstner W., 2002, SPIKING NEURON MODEL
   Graham JW., 2015, CELL, V163
   Guest JM., 2021, BIORXIV
   Haider B, 2006, J NEUROSCI, V26, P4535, DOI 10.1523/JNEUROSCI.5297-05.2006
   Hengen KB, 2013, NEURON, V80, P335, DOI 10.1016/j.neuron.2013.08.038
   Herculano-Houzel S, 2013, FRONT NEUROANAT, V7, DOI 10.3389/fnana.2013.00035
   Hofer SB, 2011, NAT NEUROSCI, V14, P1045, DOI 10.1038/nn.2876
   Jercog D, 2017, ELIFE, V6, DOI 10.7554/eLife.22425
   KATZ B, 1965, PROC R SOC SER B-BIO, V161, P483, DOI 10.1098/rspb.1965.0016
   Khajeh R, 2022, PLOS COMPUT BIOL, V18, DOI 10.1371/journal.pcbi.1008836
   Khan AG, 2018, NAT NEUROSCI, V21, P851, DOI 10.1038/s41593-018-0143-z
   Ko H, 2011, NATURE, V473, P87, DOI 10.1038/nature09880
   Kraynyukovaa N, 2018, P NATL ACAD SCI USA, V115, P3464, DOI 10.1073/pnas.1700080115
   Kusmierz L, 2020, PHYS REV LETT, V125, DOI 10.1103/PhysRevLett.125.028101
   Lefort S, 2009, NEURON, V61, P301, DOI 10.1016/j.neuron.2008.12.020
   Li YT, 2013, NAT NEUROSCI, V16, P1324, DOI 10.1038/nn.3494
   Lien AD, 2013, NAT NEUROSCI, V16, P1315, DOI 10.1038/nn.3488
   Margrie TW, 2002, PFLUG ARCH EUR J PHY, V444, P491, DOI 10.1007/s00424-002-0831-z
   Markram H, 2015, CELL, V163, P456, DOI 10.1016/j.cell.2015.09.029
   Marshel JH, 2019, SCIENCE, V365, P558, DOI 10.1126/science.aaw5202
   Meyer HS, 2010, CEREB CORTEX, V20, P2277, DOI 10.1093/cercor/bhq067
   Miller KD., 2020, BIORXIV
   Minsky M., 1969, PERCEPTRONS INTRO CO
   Mongillo G, 2012, PHYS REV LETT, V108, DOI 10.1103/PhysRevLett.108.158101
   Montbrió E, 2015, PHYS REV X, V5, DOI 10.1103/PhysRevX.5.021028
   Motta A, 2019, SCIENCE, V366, P1093, DOI 10.1126/science.aay3134
   Murray JD, 2014, NAT NEUROSCI, V17, P1661, DOI 10.1038/nn.3862
   Oberlaender M, 2012, CEREB CORTEX, V22, P2375, DOI 10.1093/cercor/bhr317
   Okun M, 2008, NAT NEUROSCI, V11, P535, DOI 10.1038/nn.2105
   Ozeki H, 2009, NEURON, V62, P578, DOI 10.1016/j.neuron.2009.03.028
   Persi E, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1001078
   Petersen CCH, 2017, NEURON, V95, P1266, DOI 10.1016/j.neuron.2017.06.049
   Pfeffer CK, 2013, NAT NEUROSCI, V16, P1068, DOI 10.1038/nn.3446
   Piscopo DM, 2013, J NEUROSCI, V33, P4642, DOI 10.1523/JNEUROSCI.5187-12.2013
   Poo C, 2011, NEURON, V72, P41, DOI 10.1016/j.neuron.2011.08.015
   Powell DJ, 2021, CURR BIOL, V31, P4831, DOI 10.1016/j.cub.2021.08.042
   Priebe NJ, 2004, NAT NEUROSCI, V7, P1113, DOI 10.1038/nn1310
   Purves D., 2001, NEUROSCIENCE
   Rancz EA, 2011, NAT NEUROSCI, V14, P527, DOI 10.1038/nn.2765
   Renart A, 2010, SCIENCE, V327, P587, DOI 10.1126/science.1179850
   Ricciardi LM., 1977, DIFFUSION PROCESSES
   Ringach D, 2004, COGNITIVE SCI, V28, P147, DOI 10.1016/j.cogsci.2003.11.003
   Rosenbaum R, 2014, PHYS REV X, V4, DOI 10.1103/PhysRevX.4.021039
   Roxin A, 2011, J NEUROSCI, V31, P16217, DOI 10.1523/JNEUROSCI.1677-11.2011
   Rubin DB, 2015, NEURON, V85, P402, DOI 10.1016/j.neuron.2014.12.026
   Rupprecht P, 2018, NEURON, V100, P669, DOI 10.1016/j.neuron.2018.09.013
   Sadeh S, 2021, NAT REV NEUROSCI, V22, P21, DOI 10.1038/s41583-020-00390-z
   Sanzeni A, 2020, PLOS COMPUT BIOL, V16, DOI 10.1371/journal.pcbi.1008165
   Sanzeni A, 2020, ELIFE, V9, DOI 10.7554/eLife.54875
   Schwalger T, 2017, PLOS COMPUT BIOL, V13, DOI 10.1371/journal.pcbi.1005507
   Seeman SC, 2018, ELIFE, V7, DOI 10.7554/eLife.37349
   SOFTKY WR, 1993, J NEUROSCI, V13, P334
   Stobb M, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0037292
   Stringer C, 2019, NATURE, V571, P361, DOI 10.1038/s41586-019-1346-5
   Stringer C, 2019, SCIENCE, V364, P255, DOI 10.1126/science.aav7893
   Tan AYY, 2011, J NEUROSCI, V31, P12339, DOI 10.1523/JNEUROSCI.2039-11.2011
   Tchumatchenko T, 2010, PHYS REV LETT, V104, DOI 10.1103/PhysRevLett.104.058102
   Tsodyks MV, 1997, J NEUROSCI, V17, P4382
   van Vreeswijk C, 1998, NEURAL COMPUT, V10, P1321, DOI 10.1162/089976698300017214
   vanVreeswijk C, 1996, SCIENCE, V274, P1724, DOI 10.1126/science.274.5293.1724
   WILSON HR, 1972, BIOPHYS J, V12, P1, DOI 10.1016/S0006-3495(72)86068-5
   WOOLSEY TA, 1970, BRAIN RES, V17, P205, DOI 10.1016/0006-8993(70)90079-X
   Wu YK, 2021, ELIFE, V10, DOI [10.7554/eLife.71263, 10.7554/eLife.71263.sa0, 10.7554/eLife.71263.sa1, 10.7554/eLife.71263.sa2]
NR 87
TC 0
Z9 0
U1 1
U2 1
PD MAY
PY 2023
VL 19
IS 5
AR e1011097
DI 10.1371/journal.pcbi.1011097
WC Biochemical Research Methods; Mathematical & Computational Biology
DA 2023-11-11
ER

PT C
AU Zajzon, B
   Duarte, R
   Morrison, A
AF Zajzon, Barna
   Duarte, Renato
   Morrison, Abigail
GP IEEE
TI Transferring State Representations in Hierarchical Spiking Neural
   Networks
SO 2018 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUL 08-13, 2018
CL Rio de Janeiro, BRAZIL
DE stimulus representation; state transfer; modularity; spiking neural
   networks
AB Hierarchical modularity is a parsimonious design principle in many complex systems and underlies various key structural and functional aspects of neurobiological systems, whose modules are recurrent networks of spiking neurons. An essential requirement for such systems to adequately function is the ability to transfer information across multiple modules in a reliable and efficient manner. In this work, we study the characteristics of emergent stimulus representations in recurrent, spiking neural networks and the features that allow efficient information transfer among multiple, interacting sub-networks. We find that the specificity of structural mappings between the modules is strictly required for information to propagate to a sufficient depth, in a sequential setup. Conserved topography not only improves computational performance in all scenarios analyzed, but it proves to be more robust against noise and interference effects, results in less variability in the neural responses and increases memory capacity.
C1 [Zajzon, Barna; Duarte, Renato; Morrison, Abigail] Julich Res Ctr, Inst Neurosci & Med INM 6, Inst Adv Simulat IAS 6, Julich, Germany.
   [Zajzon, Barna; Duarte, Renato; Morrison, Abigail] Julich Res Ctr, JARA Inst Brain Struct Funct Relationships JBI 1, Julich, Germany.
   [Zajzon, Barna] Rhein Westfal TH Aachen, Dept Psychiat Psychotherapy & Psychosomat, Aachen, Germany.
   [Morrison, Abigail] Ruhr Univ Bochum, Fac Psychol, Inst Cognit Neurosci, Bochum, Germany.
RP Zajzon, B (corresponding author), Julich Res Ctr, Inst Neurosci & Med INM 6, Inst Adv Simulat IAS 6, Julich, Germany.; Zajzon, B (corresponding author), Julich Res Ctr, JARA Inst Brain Struct Funct Relationships JBI 1, Julich, Germany.; Zajzon, B (corresponding author), Rhein Westfal TH Aachen, Dept Psychiat Psychotherapy & Psychosomat, Aachen, Germany.
EM b.zajzon@fz-juelich.de
CR [Anonymous], 2016, NAT METHODS, DOI DOI 10.1038/nmeth.3707
   [Anonymous], 2017, NEST 2 12 10
   Duarte R., 2017, BIORXIV
   Duarte R, 2017, NEURAL MICROCIRCUIT
   Duarte R, 2017, CURR OPIN NEUROBIOL, V43, P156, DOI 10.1016/j.conb.2017.02.007
   Duarte RCF, 2014, FRONT COMPUT NEUROSC, V8, DOI 10.3389/fncom.2014.00124
   Enel P, 2016, PLOS COMPUT BIOL, V12, DOI 10.1371/journal.pcbi.1004967
   Gallicchio C, 2017, NEUROCOMPUTING, V268, P87, DOI 10.1016/j.neucom.2016.12.089
   Jaeger H., 2002, ADV NEURAL INF PROCE, V15, P609
   Lukosevicius M, 2009, COMPUT SCI REV, V3, P127, DOI 10.1016/j.cosrev.2009.03.005
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Maass W, 2004, J PHYSIOL-PARIS, V98, P315, DOI 10.1016/j.jphysparis.2005.09.020
   Rigotti M, 2013, NATURE, V497, P585, DOI 10.1038/nature12160
   van den Broek D, 2017, BEST SPIKE FILTER KE
   Vogels TP, 2009, NAT NEUROSCI, V12, P483, DOI 10.1038/nn.2276
NR 15
TC 0
Z9 0
U1 0
U2 1
PY 2018
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Song, S
   Jeon, B
   Kim, M
   Kim, JJ
AF Song, Seunghwan
   Jeon, Bosung
   Kim, Munhyeon
   Kim, Jae-Joon
TI Efficient Convolutional Processing of Spiking Neural Network With
   Weight-Sharing Filters
SO IEEE ELECTRON DEVICE LETTERS
DT Article
DE Convolution; Logic gates; Synapses; Tin; Neural networks; Convolutional
   neural networks; Silicon; Charge trap flash (CTF); efficient
   convolutional processing; spiking neural network (SNN)
AB The importance of implementing an efficient convolutional neural network (CNN) is increasing. A weight-sharing spiking CNN inference system (WS-SCNN) employing efficient convolution layers (ECLs) is proposed and modeled to enable the compact convolutional processing of the spiking neural network (SNN) inference. The proposed ECL efficiently maps convolutional features between inputs and filter weights. The ECL does not replicate the synaptic filter array with respect to input sliding, which minimizes the number of synaptic devices required to implement hardware SNNs. A four-bit weight quantization capability of a fabricated charge-trap flash synaptic device is used to verify the accurate multiplication and summation of weights in the ECL. Moreover, a nine-layer WS-SCNN consisting of multiple ECLs is modeled, and the benefits of the WS-SCNN in terms of the area and energy are evaluated. Simulation results show that the WS-SCNN has 5.68 and 103.5 times higher energy and area efficiency than conventional SCNN systems, respectively.
C1 [Song, Seunghwan; Jeon, Bosung; Kim, Munhyeon; Kim, Jae-Joon] Seoul Natl Univ, Dept Elect & Comp Engn, Seoul 08826, South Korea.
   [Song, Seunghwan; Jeon, Bosung; Kim, Munhyeon; Kim, Jae-Joon] Seoul Natl Univ, Interuniv Semicond Res Ctr, Seoul 08826, South Korea.
RP Kim, JJ (corresponding author), Seoul Natl Univ, Dept Elect & Comp Engn, Seoul 08826, South Korea.; Kim, JJ (corresponding author), Seoul Natl Univ, Interuniv Semicond Res Ctr, Seoul 08826, South Korea.
EM kimjaejoon@snu.ac.kr
CR [Anonymous], 2012, FIELD PROGRAMMABLE G
   bsim, BSIM CMG TECHNICAL M
   Choi HS, 2020, IEEE ELECTR DEVICE L, V41, P1653, DOI 10.1109/LED.2020.3025587
   Diehl Peter U, 2015, 2015 INT JOINT C NEU, P1, DOI [DOI 10.1109/IJCNN.2015.7280696, 10.1109/IJCNN.2015.7280696]
   Esser S. K., P NATL ACAD SCI USA, V113
   Ho ND, 2021, DES AUT CON, P793, DOI 10.1109/DAC18074.2021.9586266
   Hwang S, 2022, IEEE ELECTR DEVICE L, V43, P549, DOI 10.1109/LED.2022.3149029
   Hwang S, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-60572-8
   Ielmini D, 2020, ADV INTELL SYST-GER, V2, DOI 10.1002/aisy.202000040
   Kim C, 2017, ISSCC DIG TECH PAP I, P202, DOI 10.1109/ISSCC.2017.7870331
   Kim H, 2018, IEEE ELECTR DEVICE L, V39, P630, DOI 10.1109/LED.2018.2809661
   Kim J, 2018, I SYMPOS LOW POWER E, P13, DOI 10.1145/3218603.3218639
   Kim TH, 2022, IEEE T ELECTRON DEV, V69, P3151, DOI 10.1109/TED.2022.3169112
   Kim Y, 2015, ACM J EMERG TECH COM, V11, DOI 10.1145/2700234
   LeCun Y., 1995, HDB BRAIN THEORY NEU, P276, DOI 10.5555/303568.303704
   Li YS, 2021, ADV INTELL SYST-GER, V3, DOI 10.1002/aisy.202000137
   Malavena G, 2019, IEEE T ELECTRON DEV, V66, P4727, DOI 10.1109/TED.2019.2940602
   Song S, 2022, IEEE ELECTR DEVICE L, V43, P1657, DOI 10.1109/LED.2022.3197239
   Sugizaki T., 2003, 2003 Symposium on VLSI Technology. Digest of Technical Papers (IEEE Cat. No.03CH37407), P27, DOI 10.1109/VLSIT.2003.1221069
   Tiomkin E., IND TEMPERATURE NAND
   Yu SM, 2021, IEEE T CIRCUITS-I, V68, P2753, DOI 10.1109/TCSI.2021.3072200
   Zhong YN, 2022, NAT ELECTRON, V5, P672, DOI 10.1038/s41928-022-00838-3
NR 22
TC 0
Z9 0
U1 5
U2 5
PD JUN
PY 2023
VL 44
IS 6
BP 1007
EP 1010
DI 10.1109/LED.2023.3265065
WC Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Kushawaha, RK
   Kumar, S
   Banerjee, B
   Velmurugan, R
AF Kushawaha, Ravi Kumar
   Kumar, Saurabh
   Banerjee, Biplab
   Velmurugan, Rajbabu
GP IEEE COMP SOC
TI Distilling Spikes: Knowledge Distillation in Spiking Neural Networks
SO 2020 25TH INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION (ICPR)
SE International Conference on Pattern Recognition
DT Proceedings Paper
CT 25th International Conference on Pattern Recognition (ICPR)
CY JAN 10-15, 2021
CL ELECTR NETWORK
AB Spiking Neural Networks (SNN) are energy-efficient computing architectures that exchange spikes for processing information, unlike classical Artificial Neural Networks (ANN). Due to this, SNNs are better suited for real-life deployments. However, similar to ANNs, SNNs also benefit from deeper architectures to obtain improved performance. Furthermore, like the deep ANNs, the memory, compute and power requirements of SNNs also increase with model size, and model compression becomes a necessity. Knowledge distillation is a model compression technique that enables transferring the learning of a large machine learning model to a smaller model with minimal loss in performance. In this paper, we propose techniques for knowledge distillation in spiking neural networks for the task of image classification. We present ways to distill spikes from a larger SNN, also called the teacher network, to a smaller one, also called the student network, while minimally impacting the classification accuracy. We demonstrate the effectiveness of the proposed method with detailed experiments on three standard datasets while proposing novel distillation methodologies and loss functions. We also present a multi-stage knowledge distillation technique for SNNs using an intermediate network to obtain higher performance from the student network. Our approach is expected to open up new avenues for deploying high performing large SNN models on resource-constrained hardware platforms.
C1 [Kushawaha, Ravi Kumar; Kumar, Saurabh; Banerjee, Biplab; Velmurugan, Rajbabu] Indian Inst Technol, Mumbai, Maharashtra, India.
RP Kushawaha, RK (corresponding author), Indian Inst Technol, Mumbai, Maharashtra, India.
EM rkkush2397@gmail.com; saurabhkm@iitb.ac.in; bbanerjee@iitb.ac.in;
   rajbabu@ee.iitb.ac.in
CR Ahn S, 2019, PROC CVPR IEEE, P9155, DOI 10.1109/CVPR.2019.00938
   Ankit A, 2017, DES AUT CON, DOI 10.1145/3061639.3062311
   [Anonymous], 1995, PYTHON REFERENCE MAN
   Bucila C., 2006, P 12 ACM SIGKDD INT, DOI [DOI 10.1145/1150402.1150464, 10.1145/1150402.1150464]
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Chen G., 2017, ADV NEURAL INFORM PR, V30
   Cheng Y., 2017, ARXIV171009282
   Diamond A, 2016, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00491
   Feng M, 2009, PROC IEEE INT CONF S, P105, DOI 10.1109/ICSM.2009.5306329
   Gerstner W., 2002, SPIKING NEURON MODEL
   Harris CR, 2020, NATURE, V585, P357, DOI 10.1038/s41586-020-2649-2
   Hinton G., 2015, ARXIV150302531, DOI DOI 10.4140/TCP.N.2015.249
   Huang MK, 2018, INTERSPEECH, P3703, DOI 10.21437/Interspeech.2018-1589
   Hunter JD, 2007, COMPUT SCI ENG, V9, P90, DOI 10.1109/MCSE.2007.55
   Kempter R, 1999, PHYS REV E, V59, P4498, DOI 10.1103/PhysRevE.59.4498
   Kim S, 2020, AAAI CONF ARTIF INTE, V34, P11270
   Kim Yoon, 2016, ARXIV160607947
   Kumar S., 2019, ARXIV190810559
   Lee JM, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00191
   Lin CK, 2018, ACM SIGPLAN NOTICES, V53, P78, DOI [10.1145/3192366.3192371, 10.1145/3296979.3192371]
   Liu Xiaodong, 2019, ARXIV190409482
   Mirzadeh S.-I., 2019, ARXIV190203393
   Neil D., 2016, P 31 ANN ACM S APPL, P293
   Paszke A., 2019, ADV NEURAL INFORM PR, P8024
   Paugam-Moisy H, 2012, HDB NATURAL COMPUTIN, V1, P1, DOI DOI 10.1007/978-3-540-92910-9_10
   Samaddar R, 2019, GLOBAL GOVERNANCE AND INDIA'S NORTH-EAST: LOGISTICS, INFRASTRUCTURE AND SOCIETY, P1
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Tian Yonglong, 2019, ARXIV191010699
   Virgilio CD, 2020, NEURAL NETWORKS, V122, P130, DOI 10.1016/j.neunet.2019.09.037
   Wu YJ, 2019, AAAI CONF ARTIF INTE, P1311
   Xiao H, 2017, ARXIV PREPRINT ARXIV
   Yin SY, 2017, DES AUT CON, DOI 10.1145/3061639.3062232
NR 32
TC 5
Z9 5
U1 1
U2 11
PY 2021
BP 4536
EP 4543
DI 10.1109/ICPR48806.2021.9412147
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic; Imaging Science & Photographic Technology
DA 2023-11-11
ER

PT J
AU Pérez, J
   Alcázar, M
   Sánchez, I
   Cabrera, JA
   Nybacka, M
   Castillo, JJ
AF Perez, Javier
   Alcazar, Manuel
   Sanchez, Ignacio
   Cabrera, Juan A.
   Nybacka, Mikael
   Castillo, Juan J.
TI On-line learning applied to spiking neural network for antilock braking
   systems
SO NEUROCOMPUTING
DT Article
DE Antilock brake system; Spiking neural network; On-line learning;
   Supervised learning; Vehicle dynamics; Vehicle safety
ID MODEL
AB Computationally replicating the behaviour of the cerebral cortex to perform the control tasks of daily life in a human being is a challenge today. First, it is necessary to know the structure and connections between the elements of the neural network that perform movement control. Next, a mathematical neural model that adequately resembles biological neurons has to be developed. Finally, a suitable learning model that allows adapting neural network response to changing conditions in the environment is also required. Spiking Neural Networks (SNN) are currently the closest approximation to biological neural networks. SNNs make use of temporal spike trains to deal with inputs and outputs, thus allowing a faster and more complex computation. In this paper, a controller based on an SNN is proposed to perform the control of an anti-lock braking system (ABS) in vehicles. To this end, two neural networks are used to regulate the braking force. The first one is devoted to estimating the optimal slip while the second one is in charge of setting the optimal braking pressure. The latter resembles biological reflex arcs to ensure stability during operation. This neural structure is used to control the fast regulation cycles that occur during ABS operation. Furthermore, an algorithm has been developed to train the network while driving. On-line learning is proposed to update the response of the controller. Hence, to cope with real conditions, a control algorithm based on neural networks that learn by making use of neural plasticity, similar to what occurs in biological systems, has been implemented. Neural connections are modulated using Spike-Timing-Dependent Plasticity (STDP) by means of a supervised learning structure using the slip error as input. Road-type detection has been included in the same neural structure. To validate and to evaluate the performance of the proposed algorithm, simulations as well as experiments in a real vehicle were carried out. The algorithm proved to be able to adapt to changes in adhesion conditions rapidly. This way, the capability of spiking neural networks to perform the full control logic of the ABS has been verified.
C1 [Perez, Javier; Alcazar, Manuel; Sanchez, Ignacio; Cabrera, Juan A.; Castillo, Juan J.] Univ Malaga, Dept Mech Engn, Malaga 29071, Spain.
   [Nybacka, Mikael] KTH Royal Inst Technol, Dept Engn Mech, SE-10044 Stockholm, Sweden.
   [Nybacka, Mikael] KTH Royal Inst Technol, Integrated Transport Res Lab, SE-10044 Stockholm, Sweden.
RP Cabrera, JA (corresponding author), Univ Malaga, Dept Mech Engn, Malaga 29071, Spain.
EM jcabrera@uma.es
CR Amirkhani AOL, 2022, IEEE ACCESS, V10, P58736, DOI 10.1109/ACCESS.2022.3179700
   Arriandiaga A, 2020, IEEE T NEUR NET LEAR, V31, P3920, DOI 10.1109/TNNLS.2019.2947380
   Buchanan K.A., 2010, FRONT SYNAPTIC NEURO, V2, P1
   Comsa I.M., 2019, IEEE T NEUR NET LEAR, P1
   DeWolf T, 2020, FRONT NEUROROBOTICS, V14, DOI 10.3389/fnbot.2020.568359
   Doya K, 1999, NEURAL NETWORKS, V12, P961, DOI 10.1016/S0893-6080(99)00046-5
   Feldman A.G., 2020, J NEUROPHYSIOL
   Feldman AG., 2015, REFERENT CONTROL ACT, DOI 10.1007/978-1-4939-2736-4
   Fernández JP, 2021, IEEE T VEH TECHNOL, V70, P1255, DOI 10.1109/TVT.2021.3055142
   Guan XC, 2020, IEEE ACCESS, V8, P17673, DOI 10.1109/ACCESS.2020.2968240
   Haggerty SE, 2018, FRONT SYST NEUROSCI, V12, DOI 10.3389/fnsys.2018.00004
   HENNEMAN E, 1957, SCIENCE, V126, P1345, DOI 10.1126/science.126.3287.1345
   Ivanov V, 2015, IEEE T VEH TECHNOL, V64, P3878, DOI 10.1109/TVT.2014.2361860
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2007, CEREB CORTEX, V17, P2443, DOI 10.1093/cercor/bhl152
   Jain LC, 2002, STUD FUZZ SOFT COMP, V84, P137
   Jing HH, 2011, IEEE T VEH TECHNOL, V60, P1470, DOI 10.1109/TVT.2011.2125806
   Kandel ER, 2001, SCIENCE, V294, P1030, DOI 10.1126/science.1067020
   Kiencke U., 2005, AUTOMOTIVE CONTROL S
   Lagani G, 2021, NEURAL NETWORKS, V143, P719, DOI 10.1016/j.neunet.2021.08.003
   Lin CM, 2003, IEEE T NEURAL NETWOR, V14, P351, DOI 10.1109/TNN.2002.806950
   Liu C, 2021, IEEE ACCESS, V9, P17071, DOI 10.1109/ACCESS.2021.3053280
   Nandakumar SR, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-64878-5
   Oniz Y, 2015, NEUROCOMPUTING, V149, P690, DOI 10.1016/j.neucom.2014.07.061
   Pacejka HB, 2012, TIRE AND VEHICLE DYNAMICS, 3RD EDITION, P1, DOI 10.1016/B978-0-08-097016-5.00001-2
   Fernández JP, 2021, NEUROCOMPUTING, V463, P237, DOI 10.1016/j.neucom.2021.08.005
   Pretagostini F, 2020, IEEE ACCESS, V8, P10951, DOI 10.1109/ACCESS.2020.2965644
   Radac MB, 2018, NEUROCOMPUTING, V275, P317, DOI 10.1016/j.neucom.2017.08.036
   Ranjan JAK, 2020, J SUPERCOMPUT, V76, P6545, DOI 10.1007/s11227-019-02881-y
   Raychaudhuri T, 1995, CONTROL 95, P369
   Reif K., 2014, BRAKES BRAKE CONTROL
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Sabanovi E., 2020, IDENTIFICATION ROAD
   Sassella A, 2022, NONLINEAR ANAL-HYBRI, V46, DOI 10.1016/j.nahs.2022.101220
   Savitski D, 2016, INT J AUTO TECH-KOR, V17, P327, DOI 10.1007/s12239-016-0033-x
   Shulz D.E., 2013, SPIKE TIMING DEPENDE
   Su J, 2021, IEEE ACCESS, V9, P51950, DOI 10.1109/ACCESS.2021.3068159
   Taherkhani A, 2018, IEEE T NEUR NET LEAR, V29, P5394, DOI 10.1109/TNNLS.2018.2797801
   Tan HW, 2019, ADV INTELL SYST-GER, V1, DOI 10.1002/aisy.201900036
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Tavernini D, 2020, IEEE T IND ELECTRON, V67, P3990, DOI 10.1109/TIE.2019.2916387
   Taylor P., 2010, VEH SYST DYN INT J V, P37
   Voutsas K, 2007, IEEE T NEURAL NETWOR, V18, P1785, DOI 10.1109/TNN.2007.899623
   Wang HW, 2021, IEEE ACCESS, V9, P40349, DOI 10.1109/ACCESS.2021.3064960
   Wang XW, 2020, NEURAL NETWORKS, V125, P258, DOI 10.1016/j.neunet.2020.02.011
NR 46
TC 0
Z9 0
U1 1
U2 1
PD NOV 28
PY 2023
VL 559
AR 126784
DI 10.1016/j.neucom.2023.126784
EA SEP 2023
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT J
AU Yang, SM
   Deng, B
   Li, HY
   Liu, C
   Wang, J
   Yu, HT
   Qin, YM
AF Yang, Shuangming
   Deng, Bin
   Li, Huiyan
   Liu, Chen
   Wang, Jiang
   Yu, Haitao
   Qin, Yingmei
TI FPGA implementation of hippocampal spiking network and its real-time
   simulation on dynamical neuromodulation of oscillations
SO NEUROCOMPUTING
DT Article
DE Field programmable gate array (FPGA); Spiking neural network;
   High-performance neurocomputing; Hippocampal oscillations; Real-time
   implementation
ID NEURAL-NETWORK; THETA RHYTHM; BASAL GANGLIA; NAVIGATION; MODELS; MEMORY
AB Neural information is represented and transmitted among single neurons by a series of all-or-none neural codes with certain oscillation dynamics. Real-time implementation of the hippocampal spiking network is a promising avenue to investigate the complexity underlying spatiotemporal information encoding and the emergent coherence that arises with the properly coupling of large number of neurons. This paper presents a real-time scalable hardware platform for implementing hippocampal spiking neural network (HSNN) with 10 K neurons, which introduces a novel network-on-chip architecture for the randomly connected spiking neural networks (SNNs). The effects of endogenous surroundings and neural heterogeneity are taken into consideration in the hardware design, which replicates more relevant biological dynamics in comparison with the state-of-the-art studies. Based on the hardware synthesis and theoretical analysis, it is demonstrated that the proposed implementation is able to mimic hippocampal oscillation modulation dynamics under external stimuli, which is vital for the reasonable design of noninvasive electrotherapeutic strategies. The proposed implementation is meaningful for both the efficient hardware implementation of the randomly connected SNNs and the dynamic investigation of the HSNNs. (C) 2017 Elsevier B.V. All rights reserved.
C1 [Yang, Shuangming; Deng, Bin; Liu, Chen; Wang, Jiang; Yu, Haitao] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Li, Huiyan; Qin, Yingmei] Tianjin Univ Technol & Educ, Sch Automat & Elect Engn, Tianjin 300222, Peoples R China.
RP Yu, HT (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
EM htyu@tju.edu.cn
CR Anastassiou CA, 2010, J NEUROSCI, V30, P1925, DOI 10.1523/JNEUROSCI.3635-09.2010
   [Anonymous], 2010, 2010 IEEE INT S PARA
   Axmacher N, 2010, P NATL ACAD SCI USA, V107, P3228, DOI 10.1073/pnas.0911531107
   Barbieri R, 2000, NEUROCOMPUTING, V32, P629, DOI 10.1016/S0925-2312(00)00225-3
   Berger TW, 2012, IEEE T NEUR SYS REH, V20, P198, DOI 10.1109/TNSRE.2012.2189133
   Bonabi SY, 2014, FRONT NEUROSCI-SWITZ, V8, DOI 10.3389/fnins.2014.00379
   Bray LCJ, 2010, FRONT NEURAL CIRCUIT, V4, DOI 10.3389/fncir.2010.00122
   Bush D, 2010, PLOS COMPUT BIOL, V6, DOI 10.1371/journal.pcbi.1000839
   Buzsáki G, 2005, HIPPOCAMPUS, V15, P827, DOI 10.1002/hipo.20113
   Buzsáki G, 2013, NAT NEUROSCI, V16, P130, DOI 10.1038/nn.3304
   Cassidy A, 2008, 2008 IEEE BIOMEDICAL CIRCUITS AND SYSTEMS CONFERENCE - INTELLIGENT BIOMEDICAL SYSTEMS (BIOCAS), P289, DOI 10.1109/BIOCAS.2008.4696931
   Eichenbaum H, 2000, NAT REV NEUROSCI, V1, P41, DOI 10.1038/35036213
   Hájos N, 2004, J NEUROSCI, V24, P9127, DOI 10.1523/JNEUROSCI.2113-04.2004
   Hartley T, 2014, PHILOS T R SOC B, V369, DOI 10.1098/rstb.2012.0510
   Hasselmo ME, 2001, NEUROCOMPUTING, V38, P633, DOI 10.1016/S0925-2312(01)00411-8
   Himavathi S, 2007, IEEE T NEURAL NETWOR, V18, P880, DOI 10.1109/TNN.2007.891626
   Huhn Z, 2005, HIPPOCAMPUS, V15, P950, DOI 10.1002/hipo.20112
   Igarashi J, 2011, NEURAL NETWORKS, V24, P950, DOI 10.1016/j.neunet.2011.06.008
   Iyer SS, 2005, IBM J RES DEV, V49, P333, DOI 10.1147/rd.492.0333
   Javitt DC, 2008, NAT REV DRUG DISCOV, V7, P68, DOI 10.1038/nrd2463
   Johnson EL, 2015, CURR OPIN NEUROBIOL, V31, P18, DOI 10.1016/j.conb.2014.07.021
   Kodogiannis VS, 2013, INT J NEURAL SYST, V23, DOI 10.1142/S012906571350024X
   Lega B, 2016, CEREB CORTEX, V26, P268, DOI 10.1093/cercor/bhu232
   Lega BC, 2012, HIPPOCAMPUS, V22, P748, DOI 10.1002/hipo.20937
   Maguire LP, 2007, NEUROCOMPUTING, V71, P13, DOI 10.1016/j.neucom.2006.11.029
   Marshall L, 2005, BMC NEUROSCI, V6, DOI 10.1186/1471-2202-6-23
   Mejias JF, 2012, PHYS REV LETT, V108, DOI 10.1103/PhysRevLett.108.228102
   Memmesheimer RM, 2010, P NATL ACAD SCI USA, V107, P11092, DOI 10.1073/pnas.0909615107
   Minkovich K, 2014, IEEE T NEUR NET LEAR, V25, P316, DOI 10.1109/TNNLS.2013.2276056
   Misra J, 2010, NEUROCOMPUTING, V74, P239, DOI 10.1016/j.neucom.2010.03.021
   Mokhtar M, 2008, LECT NOTES COMPUT SC, V5216, P362
   Oren I, 2006, J NEUROSCI, V26, P9923, DOI 10.1523/JNEUROSCI.1580-06.2006
   Palop JJ, 2007, NEURON, V55, P697, DOI 10.1016/j.neuron.2007.07.025
   Palop JJ, 2006, NATURE, V443, P768, DOI 10.1038/nature05289
   Petrovici MA, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0108590
   Pevzner A, 2016, FRONT SYST NEUROSCI, V10, DOI 10.3389/fnsys.2016.00030
   Reato D, 2010, J NEUROSCI, V30, P15067, DOI 10.1523/JNEUROSCI.2059-10.2010
   Recce M, 2000, NEUROCOMPUTING, V32, P225, DOI 10.1016/S0925-2312(00)00168-5
   Taxidis J, 2013, FRONT COMPUT NEUROSC, V7
   Wang RC, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00014
   Wang XJ, 1996, J NEUROSCI, V16, P6402
   Williams JH, 1997, J NEUROPHYSIOL, V78, P2631, DOI 10.1152/jn.1997.78.5.2631
   Xin Y, 2014, MICROELECTRON J, V45, P690, DOI 10.1016/j.mejo.2014.03.018
   Yang SM, 2016, NEUROCOMPUTING, V177, P274, DOI 10.1016/j.neucom.2015.11.026
   Yang SM, 2015, NEURAL NETWORKS, V71, P62, DOI 10.1016/j.neunet.2015.07.017
   Zamanlooy B, 2014, IEEE T VLSI SYST, V22, P39, DOI 10.1109/TVLSI.2012.2232321
NR 46
TC 23
Z9 24
U1 3
U2 28
PD MAR 22
PY 2018
VL 282
BP 262
EP 276
DI 10.1016/j.neucom.2017.12.031
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT J
AU Ding, YQ
   Zuo, L
   Yang, KS
   Chen, ZS
   Hu, J
   Xiahou, T
AF Ding, Yongqi
   Zuo, Lin
   Yang, Kunshan
   Chen, Zhongshu
   Hu, Jian
   Xiahou, Tangfan
TI An improved probabilistic spiking neural network with enhanced
   discriminative ability
SO KNOWLEDGE-BASED SYSTEMS
DT Article
DE Spiking neural network; Probabilistic firing mechanism; Attention
   mechanism; Leaky integrate-and-fire neuron
ID COMPUTATION; POTENTIALS; DEEPER
AB The non-differentiability of the spike activity has been a hindrance to the development of high-performance spiking neural networks (SNNs). Current learning algorithms mainly focus on achieving attractive SNNs based on surrogate gradient or conversion, yet their performance is still limited. The probability-based SNNs use the probabilistic mechanism to smooth out spike activity, showing a promising way for training SNNs. This work optimizes the probabilistic mechanism and proposes the probabilistic firing mechanism (PFM) for spiking neurons. PFM enables differentiable spike activity and can be adapted to a variety of spiking neurons. In addition, to eliminate the negative influence of probabilistic uncertainty, the attention discrimination mechanism (ADM) is proposed, which enables the neurons to respond efficiently by adaptively distinguishing the salient elements of the input current. By fusing PFM, ADM, and Leaky Integrate-and-Fire (LIF) neurons, we constructed the Probabilistic Attention Leaky Integrate-and-Fire (PALIF) neuron and Probabilistic Attention Spiking Neural Network (PASNN). Ablation studies confirm the effectiveness of PFM and ADM, and indicate that PASNN is suitable for low-latency scenarios. Experiments on both static image and neuromorphic datasets, including CIFAR10, CIFAR100, N-MNIST, and CIFAR10-DVS, demonstrate that PASNN achieves competitive performance in terms of accuracy and inference speed.
C1 [Ding, Yongqi; Zuo, Lin; Yang, Kunshan; Hu, Jian] Univ Elect Sci & Technol China, Sch Informat & Software Engn, Chengdu, Sichuan, Peoples R China.
   [Chen, Zhongshu; Xiahou, Tangfan] Univ Elect Sci & Technol China, Sch Mech & Elect Engn, Chengdu, Sichuan, Peoples R China.
   [Zuo, Lin] Xiyuan Ave, Chengdu 611731, Sichuan, Peoples R China.
RP Zuo, L (corresponding author), Xiyuan Ave, Chengdu 611731, Sichuan, Peoples R China.
EM linzuo@uestc.edu.cn
CR Abro WA, 2022, KNOWL-BASED SYST, V242, DOI 10.1016/j.knosys.2022.108318
   Andrew A.M., 2003, KYBERNETES, P32
   Bing Han, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13555, DOI 10.1109/CVPR42600.2020.01357
   Bu T., 2022, P INT C LEARN REPR I
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Cheng X, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1519
   Deng S., 2022, P INT C LEARN REPR I
   Ding Jianhao, 2021, P 30 INT JOINT C ART, P2328, DOI [DOI 10.24963/IJCAI.2021/321, 10.24963/ijcai.2021/321]
   Ding XH, 2021, PROC CVPR IEEE, P13728, DOI 10.1109/CVPR46437.2021.01352
   Fang W, 2021, ADV NEUR IN, V34
   Fang W, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2641, DOI 10.1109/ICCV48922.2021.00266
   Feng L., 2022, P INT JOINT C ART IN, P2471
   Gidon A, 2020, SCIENCE, V367, P83, DOI 10.1126/science.aax6239
   Guo YF, 2023, PATTERN RECOGN, V142, DOI 10.1016/j.patcog.2023.109639
   Guo Y, 2022, LECT NOTES COMPUT SC, V13672, P52, DOI 10.1007/978-3-031-19775-8_4
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/TPAMI.2019.2913372, 10.1109/CVPR.2018.00745]
   Hu RH, 2019, IEEE T NEUR NET LEAR, V30, P1984, DOI 10.1109/TNNLS.2018.2875471
   Jang H, 2022, IEEE T NEUR NET LEAR, V33, P2034, DOI 10.1109/TNNLS.2022.3144296
   Kasabov N, 2010, NEURAL NETWORKS, V23, P16, DOI 10.1016/j.neunet.2009.08.010
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Kim Y, 2021, NEURAL NETWORKS, V144, P686, DOI 10.1016/j.neunet.2021.09.022
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Kugele A, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00439
   Kundu S, 2021, IEEE WINT CONF APPL, P3952, DOI 10.1109/WACV48630.2021.00400
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li HM, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00309
   Li Y., 2021, P INT C MACH LEARN I, V139, P6316
   Liu GS, 2022, IEEE T CYBERNETICS, DOI 10.1109/TCYB.2022.3198259
   Liu QH, 2022, INT CONF ACOUST SPEE, P8922, DOI 10.1109/ICASSP43922.2022.9746865
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Magee JC, 2000, NAT REV NEUROSCI, V1, P181, DOI 10.1038/35044552
   Orchard G, 2015, FRONT NEUROSCI-SWITZ, V9, DOI [10.3389/fnins.2015.00437, 10.3389/fhins.2015.00437]
   Paszke A, 2019, ADV NEUR IN, V32
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Selvaraju RR, 2017, IEEE I CONF COMP VIS, P618, DOI 10.1109/ICCV.2017.74
   SPRUSTON N, 1994, TRENDS NEUROSCI, V17, P161, DOI 10.1016/0166-2236(94)90094-9
   Sun Y., 2023, ARXIV
   Sutskever I, 2014, ADV NEUR IN, V27
   Vaswani A, 2017, ADV NEUR IN, V30
   Wang C.Y., 2022, ARXIV
   Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813
   Wang Y., 2022, P INT JOINT C ART IN, P2501
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Wu ZZ, 2022, IEEE T NEUR NET LEAR, V33, P6249, DOI 10.1109/TNNLS.2021.3073016
   Xie X., 2023, IEEE T NEUR NET LEAR, P1
   Xie XR, 2017, IEEE T NEUR NET LEAR, V28, P1411, DOI 10.1109/TNNLS.2016.2541339
   Yan ZL, 2021, AAAI CONF ARTIF INTE, V35, P10577
   Yao M, 2023, IEEE T PATTERN ANAL, V45, P9393, DOI 10.1109/TPAMI.2023.3241201
   Yao M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10201, DOI 10.1109/ICCV48922.2021.01006
   Yu C., 2022, ARXIV
   Yu Q, 2022, IEEE T NEUR NET LEAR, V33, P1714, DOI 10.1109/TNNLS.2020.3043415
   Zhan QG, 2022, IEEE T CYBERNETICS, V52, P13323, DOI 10.1109/TCYB.2021.3079097
   Zhang ML, 2022, IEEE T NEUR NET LEAR, V33, P1947, DOI 10.1109/TNNLS.2021.3110991
   Zhao DC, 2022, NEURAL NETWORKS, V154, P68, DOI 10.1016/j.neunet.2022.06.036
   Zheng HL, 2021, AAAI CONF ARTIF INTE, V35, P11062
   Zhou Z., 2023, P INT C LEARN REPR I
   Zhu R., 2022, ARXIV
   Zuo L, 2022, RELIAB ENG SYST SAFE, V225, DOI 10.1016/j.ress.2022.108561
   Zuo L, 2020, NEUROCOMPUTING, V408, P1, DOI 10.1016/j.neucom.2020.01.109
NR 62
TC 0
Z9 0
U1 0
U2 0
PD NOV 25
PY 2023
VL 280
AR 111024
DI 10.1016/j.knosys.2023.111024
EA SEP 2023
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT C
AU Cardoso, MC
   Silva, M
   Vellasco, MMBR
   Cataldo, E
AF Cardoso, Marcelo C.
   Silva, Marco
   Vellasco, Marley M. B. R.
   Cataldo, Edson
BE Roy, A
   Angelov, P
   Alimi, A
   Venayagamoorthy, K
   Trafalis, T
TI Quantum-Inspired Features and Parameter Optimization of Spiking Neural
   Networks for a Case Study from Atmospheric
SO INNS CONFERENCE ON BIG DATA 2015 PROGRAM
SE Procedia Computer Science
DT Proceedings Paper
CT INNS Conference on Big Data
CY AUG 08-10, 2015
CL San Francisco, CA
DE Atmospheric Discharges; Spiking Neural Network; Clustering; Evolutionary
   Algorithms
AB Identified cluster of atmospheric discharges, sufficiently near from transmissions line, could be an important alarm to support real time decisions. Lightning are important events that affect the electrical power system operation, which are often responsible for transmission lines outages, and can trigger a sequence of events that lead to system collapse. The Brazilian lightning network detection monitors nearly 18 million events monthly and all this data must be processed and analyzed. This paper uses a hybrid model named the Quantum binary-real evolving Spiking Neural Network (QbrSNN) for clustering problem, where the features and parameters of a spiking neural network (SNN) are optimized using the Quantum-Inspired Evolutionary Algorithm with representation Binary-Real (QIEA-BR). The proposed model is applied to atmospheric discharges data, with a significantly higher clustering accuracy than traditional techniques.
C1 [Cardoso, Marcelo C.; Silva, Marco; Vellasco, Marley M. B. R.] Pontif Catholic Univ Rio de Janeiro PUC Rio, Rio De Janeiro, Brazil.
   [Cataldo, Edson] Univ Fed Fluminense, Rio De Janeiro, Brazil.
RP Cardoso, MC (corresponding author), Pontif Catholic Univ Rio de Janeiro PUC Rio, Rio De Janeiro, Brazil.
EM mcascardo1@gmail.com; mabs21@ele.puc-rio.br; marley@ele.puc-rio.br;
   ecataldo@im.uff.br
CR Bohte S. M., 2003, THESIS I PROGRAMMING
   da Cruz A. V. A., 2006, QUANTUM INSPIRED EVO, P2630
   de Pinho Anderson Guimaraes, 2009, Proceedings of the 2009 World Congress on Nature & Biologically Inspired Computing (NaBIC 2009), P445, DOI 10.1109/NABIC.2009.5393327
   Everitt B.S., 2001, CLUSTER ANAL, V4th
   Hamed H. N. A., 2011, NUMERICAL ANAL SCI C, P133
   Hamed HNA, 2009, 2009 INTERNATIONAL CONFERENCE OF SOFT COMPUTING AND PATTERN RECOGNITION, P695, DOI 10.1109/SoCPaR.2009.139
   Han KH, 2002, IEEE T EVOLUT COMPUT, V6, P580, DOI 10.1109/TEVC.2002.804320
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   Jusevicius M. A. R., 2007, 9 INT S LIGHTN PROT
   Kaufman L., 2009, FINDING GROUPS DATA, V344
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Natschlager T, 1998, NETWORK-COMP NEURAL, V9, P319, DOI 10.1088/0954-898X/9/3/003
   Silva M, 2014, IEEE IJCNN, P2391, DOI 10.1109/IJCNN.2014.6889566
   Tan PN., 2013, INTRO DATA MINING, P487
   de Lima GRT, 2013, COMPUT GEOSCI-UK, V57, P158, DOI 10.1016/j.cageo.2013.04.016
NR 15
TC 3
Z9 3
U1 0
U2 3
PY 2015
VL 53
BP 74
EP 81
DI 10.1016/j.procs.2015.07.281
WC Computer Science, Theory & Methods
DA 2023-11-11
ER

PT C
AU Menescal, DD
   de Castro, LN
AF Menescal, Diego Duarte
   de Castro, Leandro Nunes
BE Omatu, S
   Mehmood, R
   Sitek, P
   Cicerone, S
   Rodriguez, S
TI On the Generation of Desired Outputs for Spike Neural Networks (SNN)
SO 19TH INTERNATIONAL SYMPOSIUM ON DISTRIBUTED COMPUTING AND ARTIFICIAL
   INTELLIGENCE
SE Lecture Notes in Networks and Systems
DT Proceedings Paper
CT 19th International Symposium on Distributed Computing and Artificial
   Intelligence
CY JUL 13-15, 2022
CL L'Aquila, ITALY
DE Spike neural network; Desired output; Spike generation
AB In supervised learning algorithms, it is necessary to define an error function for the parameter adjustment process to take place. This function generation requires the input feature vectors and their respective desired outputs. In the context of neural networks, the network outputs are compared with the desired outputs so as to compute the error function. For standard networks, such as Perceptron, Adaline and others, the desired outputs are basically a class label or output value that will be directly used to calculate the network error. In the case of bioinspired networks, such as those using Leaky Integrate-and-Fire (LIF) neurons, their output are electrical impulses (spikes). In such cases, the electrical impulse has a built-in temporal dependence that does not occur for Perceptron neurons, thus representing a challenge to calculate the desired output values (spikes) for Spike Neural Networks (SNN). The purpose of this paper is to define an analytical solution to build the desired spikes for each category in a classification problem for a SNN. The computational challenge encountered to represent the dynamics of spike generation in bioinspired neurons will also be discussed, which has a direct impact on the objective of the proposed solution.
C1 [Menescal, Diego Duarte; de Castro, Leandro Nunes] Univ Prebiteriana Mackenzie, Comp & Informat Fac, Nat Comp & Machine Learning Lab LCoN, BR-01302907 Sao Paulo, SP, Brazil.
RP Menescal, DD (corresponding author), Univ Prebiteriana Mackenzie, Comp & Informat Fac, Nat Comp & Machine Learning Lab LCoN, BR-01302907 Sao Paulo, SP, Brazil.
EM dmenescal@outlook.com
CR Dayan P., 2005, THEORETICAL NEUROSCI
   de Carvalho ACPLF, 2009, STUD COMPUT INTELL, V205, P177
   Kaur P., 2021, BIOINSPIRED NEUROCOM, V903
   Kinaneva D., 2021, 3 INT C HUMAN COMPUT, P1
   Kreuz T, 2013, J NEUROPHYSIOL, V109, P1457, DOI 10.1152/jn.00873.2012
   MAASS W., 1999, PULSED NEURAL NETWOR
   Miller P., 2018, INTRO COURSE COMPUTA
   Python Programming Language Homepage, US
   Velichko A, 2020, IEEE T CIRCUITS-II, V67, P3477, DOI 10.1109/TCSII.2020.2997117
   Wang XW, 2020, NEURAL NETWORKS, V125, P258, DOI 10.1016/j.neunet.2020.02.011
NR 10
TC 0
Z9 0
U1 1
U2 1
PY 2023
VL 583
BP 100
EP 110
DI 10.1007/978-3-031-20859-1_11
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods
DA 2023-11-11
ER

PT J
AU Im, J
   Kim, J
   Yoo, HN
   Baek, JW
   Kwon, D
   Oh, S
   Kim, J
   Hwang, J
   Park, BG
   Lee, JH
AF Im, Jiseong
   Kim, Jaehyeon
   Yoo, Ho-Nam
   Baek, Jong-Won
   Kwon, Dongseok
   Oh, Seongbin
   Kim, Jangsaeng
   Hwang, Joon
   Park, Byung-Gook
   Lee, Jong-Ho
TI On-Chip Trainable Spiking Neural Networks Using Time-To-First-Spike
   Encoding
SO IEEE ACCESS
DT Article
DE Neurons; Firing; Hardware; Training; System-on-chip; Flash memories;
   Power demand; Spiking neural networks (SNNs); time-to-first-spike
   (TTFS); on-chip training; synaptic devices; NAND flash
ID MODEL
AB Artificial Neural Networks (ANNs) have shown remarkable performance in various fields. However, ANN relies on the von-Neumann architecture, which consumes a lot of power. Hardware-based spiking neural networks (SNNs) inspired by a human brain have become an alternative with significantly low power consumption. In this paper, we propose on-chip trainable SNNs using a time-to-first-spike (TTFS) method. We modify the learning rules of conventional SNNs using TTFS to be suitable for on-chip learning. Vertical NAND flash memory cells fabricated by a device manufacturer are used as synaptic devices. The entire learning process considering the hardware implementation is also demonstrated. The performance of the proposed network is evaluated through the MNIST classification in system-level simulation using Python. The proposed SNNs show an accuracy of 96% for a network size of 784 - 400 - 10. We also investigate the effect of non-ideal cell characteristics (such as pulse-to-pulse and device-to-device variations) on inference accuracy. Our networks demonstrate excellent immunity for various device variations compared with the networks using off-chip training.
C1 [Kwon, Dongseok] Seoul Natl Univ, Dept Elect & Comp Engn, SNU, Seoul 08826, South Korea.
   [Lee, Jong-Ho] Seoul Natl Univ, Dept Elect & Comp Engn, Seoul 08826, South Korea.
   Seoul Natl Univ, ISRC, Seoul 08826, South Korea.
RP Lee, JH (corresponding author), Seoul Natl Univ, Dept Elect & Comp Engn, Seoul 08826, South Korea.
EM jhl@snu.ac.kr
CR Abbott LF, 1999, BRAIN RES BULL, V50, P303, DOI 10.1016/S0361-9230(99)00161-6
   Akashdeep, 2017, EXPERT SYST APPL, V88, P249, DOI 10.1016/j.eswa.2017.07.005
   Ambrosi J., 2018, P IEEE INT C REB COM, P1, DOI [10.1109/icrc.2018.8638612, DOI 10.1109/ICRC.2018.8638612]
   [Anonymous], 2008, IEEE IEDM
   Arozi M, 2020, SYMMETRY-BASEL, V12, DOI 10.3390/sym12040541
   Burkitt AN, 2006, BIOL CYBERN, V95, P1, DOI 10.1007/s00422-006-0068-6
   Chen PY, 2015, ICCAD-IEEE ACM INT, P194, DOI 10.1109/ICCAD.2015.7372570
   Dai YT, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-017-02527-8
   Dodge S, 2019, ACM T APPL PERCEPT, V16, DOI 10.1145/3306241
   Drakaki M., 2005, INT C TECHN AUT ICTA, P322
   Duchi J, 2009, J MACH LEARN RES, V10, P2899
   Franco S., 2002, DESIGN OPERATIONAL A
   Hasan R, 2017, MICROELECTRON J, V66, P31, DOI 10.1016/j.mejo.2017.05.005
   Kheradpisheh SR, 2020, INT J NEURAL SYST, V30, DOI 10.1142/S0129065720500276
   Kim H, 2017, NANOTECHNOLOGY, V28, DOI 10.1088/1361-6528/aa86f8
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
   Kwon D, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00423
   Kwon D, 2019, IEEE T ELECTRON DEV, V66, P395, DOI 10.1109/TED.2018.2879821
   LANG JH, 1985, IEEE T COMPUT, V34, P475, DOI 10.1109/TC.1985.1676588
   Lin CH, 1998, XI BRAZILIAN SYMPOSIUM ON INTEGRATED CIRCUIT DESIGN, PROCEEDINGS, P195, DOI 10.1109/SBCCI.1998.715440
   Liu KF, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00835
   Mostafa Hesham, 2018, IEEE Trans Neural Netw Learn Syst, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Oh S., 2020, ARXIV200605033
   Petrenko S, 2018, BIG DATA TECHNOLOGIE, P115
   Querlioz D, 2012, IEEE INT SYMP NANO, P203
   Roy K, 2019, NATURE, V575, P607, DOI 10.1038/s41586-019-1677-2
   Taherkhani A, 2020, NEURAL NETWORKS, V122, P253, DOI 10.1016/j.neunet.2019.09.036
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Yu SM, 2018, P IEEE, V106, P260, DOI 10.1109/JPROC.2018.2790840
   Yu SM, 2012, 2012 IEEE INTERNATIONAL ELECTRON DEVICES MEETING (IEDM)
NR 30
TC 0
Z9 0
U1 4
U2 11
PY 2022
VL 10
BP 31263
EP 31272
DI 10.1109/ACCESS.2022.3160271
WC Computer Science, Information Systems; Engineering, Electrical &
   Electronic; Telecommunications
DA 2023-11-11
ER

PT C
AU McDaid, L
   Harkin, J
   Hall, S
   Dowrick, T
   Chen, Y
   Marsland, J
AF McDaid, L.
   Harkin, J.
   Hall, S.
   Dowrick, T.
   Chen, Y.
   Marsland, J.
BE Dimitrov, DP
   Mladenov, V
   Jordanova, S
   Mastorakis, N
TI EMBRACE: Emulating Biologically-Inspired architectures on Hardware
SO PROCEEDINGS OF THE 9TH WSEAS INTERNATIONAL CONFERENCE ON NEURAL NETWORKS
   (NN' 08): ADVANCED TOPICS ON NEURAL NETWORKS
SE Artificial Intelligence Series-WSEAS
DT Proceedings Paper
CT 9th WSEAS International Conference on Neural Networks (NN 08)
CY MAY 02-04, 2008
CL Tech Univ Sofia, Sofia, BULGARIA
HO Tech Univ Sofia
DE synapse; Network-on-Chip; Spiking Neural Networks; reconfigurable;
   neuron; mixed-mode
ID INTERCONNECT; SYNAPSES; NEURONS; DESIGN
AB This paper highlights and discusses the current challenges in the implementation of large scale Spiking Neural Networks (SNNs) in hardware. A mixed-mode approach to realising scalable SNNs on a reconfigurable hardware platform is presented. The approach uses compact low power analogue spiking neuron cells, with a weight storage capability, interconnected using Network on Chip (NoC) routers. Results presented show that this route to hardware implementation is promising.
C1 [McDaid, L.; Harkin, J.] Univ Ulster, Intelligent Syst Res Ctr, Coleraine BT52 1SA, Londonderry, North Ireland.
   [Hall, S.; Dowrick, T.; Chen, Y.; Marsland, J.] Univ Liverpool, Dept Elect Engn Elect, Liverpool L69 3BX, Merseyside, England.
RP McDaid, L (corresponding author), Univ Ulster, Intelligent Syst Res Ctr, Coleraine BT52 1SA, Londonderry, North Ireland.
EM lj.mcdaid@ulster.ac.uk
CR BAINBRIDGE WJ, 2004, P DATE 04 PAR FEB, V3, P274
   Bartolozzi C, 2006, NEUROCOMPUTING, V69, P1971, DOI 10.1016/j.neucom.2005.06.022
   Chang MCF, 2001, P IEEE, V89, P456, DOI 10.1109/5.920578
   Chen GQ, 2005, IEEE INT SYMP CIRC S, P2514
   CHEN Y, 3 INT S NEUR NETW IS
   CHEN Y, WCCI IN PRESS
   Chicca E, 2003, IEEE T NEURAL NETWOR, V14, P1297, DOI 10.1109/TNN.2003.816367
   CHICCA E, 2004, ISCAS 04 MAY, V5, P357
   Davis JA, 2001, P IEEE, V89, P305, DOI 10.1109/5.915376
   DeHon A, 2004, IEEE T VLSI SYST, V12, P1038, DOI 10.1109/TVLSI.2004.827562
   Diorio C, 1996, IEEE T ELECTRON DEV, V43, P1972, DOI 10.1109/16.543035
   Farquhar E, 2005, IEEE T CIRCUITS-I, V52, P477, DOI 10.1109/TCSI.2004.842871
   FLOYD BA, 2002, IEEE J SOLID STATE C, V37
   GOOSSENS KGW, 2008, INT S NETW CHIP APR
   Huang DW, 2003, IEEE J SEL TOP QUANT, V9, P614, DOI 10.1109/JSTQE.2003.812506
   Indiveri G, 2006, IEEE T NEURAL NETWOR, V17, P211, DOI 10.1109/TNN.2005.860850
   Indiveri G, 2003, PROCEEDINGS OF THE 2003 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL IV, P820
   Joyner JW, 2004, IEEE T VLSI SYST, V12, P367, DOI 10.1109/TVLSI.2004.825835
   KANAZAWA Y, 2003, SICE ANN C FUK AUG 4
   LINARES-BARRANCO B, 1991, IEEE J SOLID-ST CIRC, V26, P956, DOI 10.1109/4.92015
   Maguire LP, 2007, NEUROCOMPUTING, V71, P13, DOI 10.1016/j.neucom.2006.11.029
   MEINDL JD, 2001, INT EL DEV M
   Murray A F, 1997, Int J Neural Syst, V8, P559, DOI 10.1142/S0129065797000525
   Naeemi A, 2005, IEEE ELECTR DEVICE L, V26, P544, DOI 10.1109/LED.2005.852744
   NOUSIAS I, 2006, NASA ESA ADAPTIVE HA, P420
   Patel GN, 1997, ELECTRON LETT, V33, P997, DOI 10.1049/el:19970686
   PEARSON MJ, 2007, IEEE T NEURAL NETS, V18
   ROSATO JJ, FUTURE FAB INT, V8
   SCHEMMEL J, 2004, IEEE INT P NEUR NETW, V3, P1711
   Simoni MF, 1999, IEEE T CIRCUITS-II, V46, P967, DOI 10.1109/82.775396
   SMITH KC, 1988, COMPUTER, V21, P17, DOI 10.1109/2.48
   Tsodyks MV, 1997, P NATL ACAD SCI USA, V94, P719, DOI 10.1073/pnas.94.2.719
   Tuffy F, 2007, NEUROCOMPUTING, V71, P30, DOI 10.1016/j.neucom.2006.11.027
   WANG P, 2004, IEEE T VERY LARGE SC, V12
   WEIXIONG Z, 2002, SOLID STATE ELECT, V22, P268
   Wolf W, 2005, DES AUT TEST EUROPE, P86, DOI 10.1109/DATE.2005.217
   YOUNG JL, 2004, IEEE INT S CIRC SYST, V4, P744
   Zhu JH, 2003, LECT NOTES COMPUT SC, V2778, P1062
NR 38
TC 1
Z9 1
U1 0
U2 1
PY 2008
BP 167
EP +
WC Computer Science, Artificial Intelligence; Computer Science,
   Interdisciplinary Applications
DA 2023-11-11
ER

PT J
AU Sun, Z
   Cutsuridis, V
   Caiafa, CF
   Solé-Casals, J
AF Sun, Zhe
   Cutsuridis, Vassilis
   Caiafa, Cesar F.
   Sole-Casals, Jordi
TI Brain Simulation and Spiking Neural Networks
SO COGNITIVE COMPUTATION
DT Editorial Material
C1 [Sun, Zhe] Juntendo Univ, Fac Hlth Data Sci, Urayasu, Chiba 2790013, Japan.
   [Sun, Zhe] Ctr Adv Photon Image Proc Res Team, RIKEN, Wako, Saitama 3510198, Japan.
   [Cutsuridis, Vassilis] Univ Lincoln, Sch Comp Sci, Lincoln LN6 7TS, England.
   [Caiafa, Cesar F.] CONICET CCT La Plata CIC PBA UNLP, Inst Argentino Radioastron, Casilla Correo N5, RA-1894 Villa Elisa, BA, Argentina.
   [Sole-Casals, Jordi] Univ Vic, Cent Univ Catalonia, Data & Signal Proc Res Grp, Vic 08500, Catalonia, Spain.
   [Sole-Casals, Jordi] Univ Cambridge, Dept Psychiat, Cambridge CB2 0SZ, England.
RP Sun, Z (corresponding author), Juntendo Univ, Fac Hlth Data Sci, Urayasu, Chiba 2790013, Japan.; Sun, Z (corresponding author), Ctr Adv Photon Image Proc Res Team, RIKEN, Wako, Saitama 3510198, Japan.
EM z.sun.kc@juntendo.ac.jp; vcutsuridis@lincoln.ac.uk; ccaiafa@fi.uba.ar;
   jordi.sole@uvic.cat
CR Cakan C, 2023, COGN COMPUT, V15, P1132, DOI 10.1007/s12559-021-09931-9
   Chen H, 2022, CERAM INT, V48, P20400, DOI [10.1016/j.ceramint.2022.03.325, 10.1109/TAAI57707.2022.00010]
   Crook-Rumsey M, 2023, COGN COMPUT, V15, P1273, DOI 10.1007/s12559-022-10075-7
   Kobayashi T, 2022, CHIN J COMMUN, V15, P378, DOI 10.1080/17544750.2021.1987283
   Kopsick JD, 2023, COGN COMPUT, V15, P1190, DOI 10.1007/s12559-021-09954-2
   Luboeinski J, 2023, COGN COMPUT, V15, P1211, DOI 10.1007/s12559-022-10021-7
   Salustri M, 2023, COGN COMPUT, V15, P1231, DOI 10.1007/s12559-022-10034-2
   Shaw R, 2023, COGN COMPUT, V15, P1243, DOI 10.1007/s12559-022-10023-5
   Xue X, 2022, COGN COMPUT, P1
   Yang YZ, 2023, POLYM BULL, V80, P1817, DOI [10.1007/s00289-022-04158-6, 10.1109/ICSICT55466.2022.9963420]
   Zhang J, 2023, COGN COMPUT, V15, P1106, DOI 10.1007/s12559-021-09981-z
NR 11
TC 0
Z9 0
U1 2
U2 2
PD JUL
PY 2023
VL 15
IS 4
BP 1103
EP 1105
DI 10.1007/s12559-023-10156-1
EA JUN 2023
WC Computer Science, Artificial Intelligence; Neurosciences
DA 2023-11-11
ER

PT C
AU Xie, XR
   Qu, H
   Liu, GS
   Liu, LS
AF Xie, Xiurui
   Qu, Hong
   Liu, Guisong
   Liu, Lingshuang
BE Loo, CK
   Yap, KS
   Wong, KW
   Teoh, A
   Huang, K
TI Recognizing Human Actions by Using the Evolving Remote Supervised Method
   of Spiking Neural Networks
SO NEURAL INFORMATION PROCESSING (ICONIP 2014), PT I
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 21st International Conference on Neural Information Processing (ICONIP)
CY NOV 03-06, 2014
CL Kuching, MALAYSIA
DE Human actions recognition; Spiking neural networks; Particle swarm
   optimization; Remote supervised learning method
AB This paper proposes a novel approach based on spiking neural networks to recognize human actions in videos. In our method, a star skeleton detector is designed to extract spatial features of input videos, and a classifier using evolving ReSuMe algorithm is proposed, with scale and shift invariance, to recognize input patterns. In learning algorithm, the remote supervised learning method(ReSuMe) is improved by the particle swarm optimization(PSO) algorithm. Experimental results on KTH and Weizmann dataset prove that our method achieves a significant improvement in performance compared with traditional ReSuMe and other method based on neural networks.
C1 [Xie, Xiurui; Qu, Hong; Liu, Guisong; Liu, Lingshuang] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Sichuan, Peoples R China.
RP Xie, XR (corresponding author), Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Sichuan, Peoples R China.
EM hongqu@uestc.edu.cn
CR [Anonymous], 2006, P 4 ACM INT WORKSHOP, DOI DOI 10.1145/1178782.1178808
   Escobar MJ, 2009, INT J COMPUT VISION, V82, P284, DOI 10.1007/s11263-008-0201-1
   Gerstner W, 2002, SPIKING NERUAL MODEL
   Han J, 2012, MOR KAUF D, P1
   Meng Y, 2011, IEEE T NEURAL NETWOR, V22, P1952, DOI 10.1109/TNN.2011.2171044
   Petridis V, 2009, MED C CONTR AUTOMAT, P406, DOI 10.1109/MED.2009.5164575
   Poli Riccardo, 2008, Journal of Artificial Evolution & Applications, DOI [10.1155/2008/685175, 10.1155/2008/761459]
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Schüldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462
   Viola P, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P734
NR 10
TC 1
Z9 1
U1 0
U2 4
PY 2014
VL 8834
BP 366
EP 373
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods
DA 2023-11-11
ER

PT C
AU Yudanov, D
   Reznik, L
AF Yudanov, Dmitri
   Reznik, Leon
GP IEEE
TI Scalable Multi-Precision Simulation of Spiking Neural Networks on GPU
   with OpenCL
SO 2012 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUN 10-15, 2012
CL Brisbane, AUSTRALIA
DE spiking neural network simulation; high precision; GPU implementation;
   OpenCL
ID NEURONS
AB Biologically-realistic multi-precision spiking neural network (SNN) simulation is designed and implemented on a new GPU device Radeon (TM) HD 7970 using OpenCL framework. The implementation aims to investigate the role of time precision in simulated SNNs. Simulation methods and GPU platforms are reviewed. Simulation model and process are presented and analyzed. The GPU model is capable of simulating a SNN with up to two million neurons. GPU and CPU results are directly verified and found to match exactly.
C1 [Yudanov, Dmitri] AMD, Austin, TX 78741 USA.
   [Reznik, Leon] Rochester Inst Technol, Dept Comp Sci, Rochester, NY 14623 USA.
RP Yudanov, D (corresponding author), AMD, Austin, TX 78741 USA.
EM dxy7370@amd.com; lr@cs.rit.edu
CR [Anonymous], 2010, SOLVING ORDINARY DIF
   [Anonymous], 2011, HETEROGENEOUS COMPUT
   [Anonymous], 2011, CUDA EXAMPLE INTRO G
   [Anonymous], CEREBRAL PLASTICITY
   [Anonymous], BMC NEUROSCI
   [Anonymous], 2010, ALGORITHMS THEORY CO
   [Anonymous], HETEROGENEOUS COMPUT
   Ariav G, 2003, J NEUROSCI, V23, P7750
   Arrabales Raul, 2009, 2009 IEEE Symposium on Computational Intelligence and Games (CIG), P217, DOI 10.1109/CIG.2009.5286473
   Azouz R, 1999, J NEUROSCI, V19, P2209
   Brette R, 2007, J COMPUT NEUROSCI, V23, P349, DOI 10.1007/s10827-007-0038-6
   BULIRSCH R, 1966, NUMER MATH, V8, P1, DOI 10.1007/BF02165234
   Daga M., 2011, Proceedings of the 2011 Symposium on Application Accelerators in High-Performance Computing (SAAHPC 2011), P141, DOI 10.1109/SAAHPC.2011.29
   Fountas Z, 2011, IEEE CONF COMPU INTE, P350, DOI 10.1109/CIG.2011.6032027
   George AA, 2011, J NEUROSCI, V31, P14721, DOI 10.1523/JNEUROSCI.1424-11.2011
   Grinblat G L, 2011, SIMULATION, V88, P299
   Hanuschkin A, 2010, FRONTIERS NEUROINFOR, V4
   Hoffmann J, 2010, LECT NOTES COMPUT SC, V6352, P184, DOI 10.1007/978-3-642-15819-3_23
   Houston M, 2011, FUS DEV SUMM AMD GRA
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Jeffrey A, 2008, HDB MATH FORMULAS IN
   Kayser C, 2010, P NATL ACAD SCI USA, V107, P16976, DOI 10.1073/pnas.1012656107
   Kofman E, 2001, SIMUL-T SOC MOD SIM, V18, P123
   KONISHI M, 1993, SCI AM, V268, P66, DOI 10.1038/scientificamerican0493-66
   Lee S H, 40202010 STS U VIRG
   Lesica NA, 2008, J NEUROSCI, V28, P5412, DOI 10.1523/JNEUROSCI.0073-08.2008
   MAINEN ZF, 1995, SCIENCE, V268, P1503, DOI 10.1126/science.7770778
   Martinez G. L, 2011, THESIS
   Merrill DG, 2010, PACT 2010: PROCEEDINGS OF THE NINETEENTH INTERNATIONAL CONFERENCE ON PARALLEL ARCHITECTURES AND COMPILATION TECHNIQUES, P545, DOI 10.1145/1854273.1854344
   Pallipuran V K, 2011, J SUPERCOMPUT, P1
   Parker G. E., 1996, Neural, Parallel & Scientific Computations, V4, P97
   Poggio T, 2010, CNS GPU BASED FRAMEW
   Richert M., 2011, FRONTIERS NEUROINFOR, V5
   Schoppa NE, 2006, NEURON, V49, P271, DOI 10.1016/j.neuron.2005.11.038
   Sharpee TO, 2011, CURR OPIN NEUROBIOL, V21, P761, DOI 10.1016/j.conb.2011.05.027
   Stewart R D, 2011, J COMPUTATIONAL NEUR, V30, P1
   Stewart RD, 2009, J COMPUT NEUROSCI, V27, P115, DOI 10.1007/s10827-008-0131-5
   Thibeault C. M., 2011, Proceedings of the ISCA 3rd International Conference on Bioinformatics and Computational Biology, P146
   Yudanov D., 2010, P INT JOINT C NEUR N, P1, DOI [10.1109/IJCNN.2010.5596334, DOI 10.1109/IJCNN.2010.5596334]
   Zheng G, 2009, J COMPUT NEUROSCI, V26, P409, DOI 10.1007/s10827-008-0119-1
NR 40
TC 0
Z9 0
U1 0
U2 2
PY 2012
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT J
AU Arriandiaga, A
   Portillo, E
   Espinosa-Ramos, JI
   Kasabov, NK
AF Arriandiaga, Ander
   Portillo, Eva
   Espinosa-Ramos, Josafath Israel
   Kasabov, Nikola K.
TI Pulsewidth Modulation-Based Algorithm for Spike Phase Encoding and
   Decoding of Time-Dependent Analog Data
SO IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
DT Article
DE Encoding; Neurons; Pulse width modulation; Decoding; Signal processing
   algorithms; Heuristic algorithms; Data models; Analog data; data
   compression; spike encoding; spike series decoding; spiking neural
   networks (SNNs); streaming data
ID NEURAL-NETWORK METHODOLOGY; LOSSLESS COMPRESSION; EEG DATA;
   CLASSIFICATION; COMMUNICATION; INFORMATION; PATTERNS; NEURONS; CODES
AB This article proposes a new spike encoding and decoding algorithm for analog data. The algorithm uses the pulsewidth modulation principles to achieve a high reconstruction accuracy of the signal, along with a high level of data compression. Two benchmark data sets are used to illustrate the method: stock index time series and human voice data. Applications of the method for spiking neural network (SNN) modeling and neuromorphic implementations are discussed. The proposed method would allow the development of new applications of SNNs as regression techniques for predictive time-series modeling.
C1 [Arriandiaga, Ander; Portillo, Eva] Univ Basque Country, Dept Automat Control & Syst Engn, Fac Engn, Bilbao 48080, Spain.
   [Espinosa-Ramos, Josafath Israel; Kasabov, Nikola K.] Auckland Univ Technol, Res Inst, Knowledge Engn & Discovery, Auckland 1010, New Zealand.
RP Arriandiaga, A (corresponding author), Univ Basque Country, Dept Automat Control & Syst Engn, Fac Engn, Bilbao 48080, Spain.
EM ander.arriandiaga@ehu.eus
CR Al-Bahadili H, 2008, COMPUT MATH APPL, V56, P143, DOI 10.1016/j.camwa.2007.11.043
   Andrzejak RG, 2001, PHYS REV E, V64, DOI 10.1103/PhysRevE.64.061907
   [Anonymous], [No title captured]
   [Anonymous], 1994, 754 ANSIIEEE
   [Anonymous], 2017, ARXIV170301909
   [Anonymous], 2017, P 2017 IEEE BIOM CIR
   Bohte Sander M., 2004, Natural Computing, V3, P195, DOI 10.1023/B:NACO.0000027755.02868.60
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Bose P, 2016, IEEE T GEOSCI REMOTE, V54, P6563, DOI 10.1109/TGRS.2016.2586602
   Burtscher M, 2007, IEEE DATA COMPR CONF, P293
   Capecci E, 2015, NEURAL NETWORKS, V68, P62, DOI 10.1016/j.neunet.2015.03.009
   Cattani A., 2015, ARXIV150403954
   Diehl PU, 2015, IEEE IJCNN
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Doborjeh M. G., 2017, 2020 INT
   Doborjeh MG, 2016, IEEE T BIO-MED ENG, V63, P1830, DOI 10.1109/TBME.2015.2503400
   Eliasmith C., 2016, ARXIV161105141
   Engelson V., 2000, Proceedings DCC 2000. Data Compression Conference, DOI 10.1109/DCC.2000.838221
   Esser SK, 2016, P NATL ACAD SCI USA, V113, P11441, DOI 10.1073/pnas.1604850113
   Finelli LA, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.1000062
   Fowler J.E., 1994, P 1994 S VOLUME VISU, P43
   Gao T, 2016, EVOL SYST-GER, V7, P277, DOI 10.1007/s12530-016-9144-x
   Gerstner W., 2002, SPIKING NEURON MODEL, V1st, P11
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   Gruning A., 2014, 22 EUR S ART NEUR NE
   Gütig R, 2009, PLOS BIOL, V7, DOI 10.1371/journal.pbio.1000141
   Guyonneau R, 2004, J PHYSIOL-PARIS, V98, P487, DOI 10.1016/j.jphysparis.2005.09.004
   Havenith MN, 2011, J NEUROSCI, V31, P8570, DOI 10.1523/JNEUROSCI.2817-10.2011
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Hough M., 2009, P INT C ROB ART LIF, P1
   Huang J, 2011, IEEE T CIRCUITS-I, V58, P1178, DOI 10.1109/TCSI.2010.2094350
   Isenburg M, 2005, COMPUT AIDED DESIGN, V37, P869, DOI 10.1016/j.cad.2004.09.015
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Jamali M, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms13229
   Kasabov N, 2017, IEEE T COGN DEV SYST, V9, P293, DOI 10.1109/TCDS.2016.2636291
   Kasabov N, 2016, NEURAL NETWORKS, V78, P1, DOI 10.1016/j.neunet.2015.09.011
   Kasabov N, 2015, INFORM SCIENCES, V294, P565, DOI 10.1016/j.ins.2014.06.028
   Kasabov N, 2014, NEUROCOMPUTING, V134, P269, DOI 10.1016/j.neucom.2013.09.049
   Kasabov NK, 2017, IEEE T NEUR NET LEAR, V28, P887, DOI 10.1109/TNNLS.2016.2612890
   Kominek J., 2003, CMU ARTIC DATABASES
   Lazzaro J., 1995, Proceedings. Sixteenth Conference on Advanced Research in VLSI, P158, DOI 10.1109/ARVLSI.1995.515618
   Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508
   Lindstrom P, 2006, IEEE T VIS COMPUT GR, V12, P1245, DOI 10.1109/TVCG.2006.143
   Lindstrom P, 2014, IEEE T VIS COMPUT GR, V20, P2674, DOI 10.1109/TVCG.2014.2346458
   Lopes-dos-Santos V, 2015, J NEUROPHYSIOL, V113, P1015, DOI 10.1152/jn.00380.2014
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Mehrotra K., 1997, ELEMENTS ARTIFICIAL
   Meng XY, 2016, AUTOMATICA, V70, P173, DOI 10.1016/j.automatica.2016.03.012
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Montemurro MA, 2007, J NEUROPHYSIOL, V98, P1871, DOI 10.1152/jn.00593.2007
   Nadasdy Z, 2009, FRONT SYST NEUROSCI, V3, DOI 10.3389/neuro.06.006.2009
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   Panzeri S, 2010, TRENDS NEUROSCI, V33, P111, DOI 10.1016/j.tins.2009.12.001
   Perrinet L., 2002, 10th European Symposium on Artificial Neural Networks. ESANN'2002. Proceedings, P313
   Ratanaworabhan P, 2006, IEEE DATA COMPR CONF, P133
   Reid D., 2015, 11 INT C INT COMP TH
   Reid D, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0103656
   Rumbell T, 2014, IEEE T NEUR NET LEAR, V25, P894, DOI 10.1109/TNNLS.2013.2283140
   Schrauwen B, 2003, IEEE IJCNN, P2825
   Shannon CE, 1998, P IEEE, V86, P447, DOI 10.1109/JPROC.1998.659497
   Shin JH, 2010, IEEE T NEURAL NETWOR, V21, P1697, DOI 10.1109/TNN.2010.2050600
   Shrestha SB, 2017, NEURAL NETWORKS, V86, P54, DOI 10.1016/j.neunet.2016.10.011
   Storn R, 1997, J GLOBAL OPTIM, V11, P341, DOI 10.1023/A:1008202821328
   Sun J, 2012, ADV IND CONTROL, P25, DOI 10.1007/978-1-4471-2885-4_2
   Tu EM, 2017, IEEE T NEUR NET LEAR, V28, P1305, DOI 10.1109/TNNLS.2016.2536742
   Walter F, 2016, NEURAL PROCESS LETT, V44, P103, DOI 10.1007/s11063-015-9478-6
   Wang ZZ, 2016, NEUROCOMPUTING, V173, P1203, DOI 10.1016/j.neucom.2015.08.078
   Yu Q, 2016, IEEE T NEUR NET LEAR, V27, P621, DOI 10.1109/TNNLS.2015.2416771
NR 69
TC 2
Z9 2
U1 3
U2 20
PD OCT
PY 2020
VL 31
IS 10
BP 3920
EP 3931
DI 10.1109/TNNLS.2019.2947380
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Computer Science, Theory & Methods; Engineering,
   Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Milo, V
   Pedretti, G
   Carboni, R
   Calderoni, A
   Ramaswamy, N
   Ambrogio, S
   Ielmini, D
AF Milo, V.
   Pedretti, G.
   Carboni, R.
   Calderoni, A.
   Ramaswamy, N.
   Ambrogio, S.
   Ielmini, D.
GP IEEE
TI Demonstration of hybrid CMOS/RRAM neural networks with spike
   time/rate-dependent plasticity
SO 2016 IEEE INTERNATIONAL ELECTRON DEVICES MEETING (IEDM)
SE IEEE International Electron Devices Meeting
DT Proceedings Paper
CT 62nd Annual IEEE International Electron Devices Meeting (IEDM)
CY DEC 03-07, 2016
CL San Francisco, CA
ID SYNAPSES
AB Neural networks with resistive-switching memory (RRAM) synapses can mimic learning and recognition in the human brain, thus overcoming the major limitations of von Neumann computing architectures. While most researchers aim at supervised learning of a pre-determined set of patterns, unsupervised learning of patterns might be attractive for brain-inspired robot/drone navigation. Here we demonstrate neural networks with CMOS/RRAM synapses capable of unsupervised learning by spike-time dependent plasticity (STDP) and spike-rate dependent plasticity (SRDP). First, STDP learning in a RRAM synaptic network is demonstrated. Then we present a 4-transistor/1-resistor synapse capable of SRDP, finally demonstrating SRDP learning, update, and recognition of patterns at the level of neural network.
C1 [Milo, V.; Pedretti, G.; Carboni, R.; Ambrogio, S.; Ielmini, D.] Politecn Milan, DEIB, Piazza L da Vinci 32, I-20133 Milan, Italy.
   [Milo, V.; Pedretti, G.; Carboni, R.; Ambrogio, S.; Ielmini, D.] IU NET, Piazza L da Vinci 32, I-20133 Milan, Italy.
   [Calderoni, A.; Ramaswamy, N.] Micron Technol, Boise, ID USA.
RP Milo, V (corresponding author), Politecn Milan, DEIB, Piazza L da Vinci 32, I-20133 Milan, Italy.; Milo, V (corresponding author), IU NET, Piazza L da Vinci 32, I-20133 Milan, Italy.
EM valerio.milo@polimi.it
CR Ambrogio S., 2016, S VLSI, P196
   Ambrogio S, 2016, IEEE T ELECTRON DEV, V63, P1508, DOI 10.1109/TED.2016.2526647
   Balatti S, 2015, IEEE T ELECTRON DEV, V62, P3365, DOI 10.1109/TED.2015.2463104
   Burr GW, 2015, IEEE T ELECTRON DEV, V62, P3498, DOI 10.1109/TED.2015.2439635
   Garbin D, 2015, IEEE T ELECTRON DEV, V62, P2494, DOI 10.1109/TED.2015.2440102
   Kuzum D, 2012, NANO LETT, V12, P2179, DOI 10.1021/nl201040y
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Prezioso M, 2016, SCI REP-UK, V6, DOI 10.1038/srep21331
   Rachmuth G, 2011, P NATL ACAD SCI USA, V108, pE1266, DOI 10.1073/pnas.1106161108
   Sangkil Kim, 2015, 2015 IEEE MTT-S International Microwave Symposium (IMS2015), P1, DOI 10.1109/MWSYM.2015.7166723
   Wang ZQ, 2015, FRONT NEUROSCI-SWITZ, V8, DOI 10.3389/fnins.2014.00438
NR 11
TC 46
Z9 46
U1 0
U2 12
PY 2016
AR 16.8
WC Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Wang, XQ
   Hou, ZG
   Lv, F
   Tan, M
   Wang, YJ
AF Wang, Xiuqing
   Hou, Zeng-Guang
   Lv, Feng
   Tan, Min
   Wang, Yongji
BE Huang, T
   Zeng, Z
   Li, C
   Leung, CS
TI A Target-Reaching Controller for Mobile Robots Using Spiking Neural
   Networks
SO NEURAL INFORMATION PROCESSING, ICONIP 2012, PT IV
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 19th International Conference on Neural Information Processing (ICONIP)
CY NOV 11-15, 2012
CL Doha, QATAR
DE Mobile robot; spiking neural networks; navigation controller; target
   reaching; obstacle-avoidance; wall-following
AB Autonomous navigation plays important role in mobile robots. In this paper, a navigation controller based on spiking neural networks (SNNs) for mobile robots is presented. The proposed target-reaching navigation controller, in which the reactive architecture is used, is composed of three sub-controllers: the obstacle-avoidance controller and the wall-following controller using spiking neural networks (SNNs), and the goal-reaching controller. The experimental results show that the navigation controller can control the mobile robot to reach the target successfully while avoiding the obstacle and following the wall to get rid of the deadlock caused by local minimum. The proposed navigation controller does not require accurate mathematical models of the environment, and is suitable to unknown and unstructured environment.
C1 [Wang, Xiuqing; Lv, Feng] Hebei Normal Univ, Vocat & Tech Inst, Shijiazhuang 050031, Hebei, Peoples R China.
   [Hou, Zeng-Guang; Tan, Min] Chinese Acad Sci, Inst Automat, State Key Lab Management & Control Complex Syst, Beijing 100090, Peoples R China.
   [Wang, Yongji] Chinese Acad Sci, Inst Software, State Key Lab Comp Sci, Beijing 100190, Peoples R China.
RP Wang, XQ (corresponding author), Hebei Normal Univ, Vocat & Tech Inst, Shijiazhuang 050031, Hebei, Peoples R China.
EM xiuqingwang2004@yahoo.com.cn; zengguang.hou@ia.ac.cn;
   lvfeng@mail.tsinghua.edu.cn; min.tan@ia.ac.cn; ywang@itechs.iscas.ac.cn
CR Alamdari ARSA, 2005, PROCEEDINGS OF WORLD ACADEMY OF SCIENCE, ENGINEERING AND TECHNOLOGY, VOL 6, P49
   [Anonymous], 1993, NEURAL NETWORK PERCE
   Arkin R. C., 1990, Robotics and Autonomous Systems, V6, P105, DOI 10.1016/S0921-8890(05)80031-4
   Floreano D, 2005, ARTIF LIFE, V11, P121, DOI 10.1162/1064546053278900
   Floreano D., 2001, LNCS, P38
   Hagras H, 2004, IEEE INT CONF ROBOT, P4620, DOI 10.1109/ROBOT.2004.1302446
   Kasabov Nikola, 2012, Advances in Computational Intelligence. IEEE World Congress on Computational Intelligence (WCCI 2012). Plenary/Invited Lectures, P234, DOI 10.1007/978-3-642-30687-7_12
   Kasabov N, 2010, NEURAL NETWORKS, V23, P16, DOI 10.1016/j.neunet.2009.08.010
   Kasabov Nikola, 2009, Natural Computing, V8, P199, DOI 10.1007/s11047-008-9066-z
   MAASS W., 1999, PULSED NEURAL NETWOR
   Mohareri O, 2012, NEUROCOMPUTING, V88, P54, DOI 10.1016/j.neucom.2011.06.035
   Na YK, 2003, AUTON ROBOT, V15, P193, DOI 10.1023/A:1025597227189
   Rossomando FG, 2011, CONTROL ENG PRACT, V19, P215, DOI 10.1016/j.conengprac.2010.11.011
   Schliebs S, 2009, NEURAL NETWORKS, V22, P623, DOI 10.1016/j.neunet.2009.06.038
   Soula H., 2005, P AM ASS ART INT AAA, P1
   Vreeken Jilles, 2002, UUCS2003008
   Wang X-L, 2007, THESIS
   Wang XQ, 2008, NEUROCOMPUTING, V71, P655, DOI 10.1016/j.neucom.2007.08.025
   Wang XQ, 2009, 2009 INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND COMPUTATIONAL INTELLIGENCE, VOL I, PROCEEDINGS, P194, DOI 10.1109/AICI.2009.448
   Wang XQ, 2008, ICNC 2008: FOURTH INTERNATIONAL CONFERENCE ON NATURAL COMPUTATION, VOL 4, PROCEEDINGS, P125, DOI 10.1109/ICNC.2008.718
   Wysoski SG, 2008, NEUROCOMPUTING, V71, P2563, DOI 10.1016/j.neucom.2007.12.038
   Xiuqing Wang, 2011, 2011 10th IEEE International Conference on Cognitive Informatics & Cognitive Computing (ICCI-CC 2011), P348, DOI 10.1109/COGINF.2011.6016164
   Ye J, 2008, NEUROCOMPUTING, V71, P1561, DOI 10.1016/j.neucom.2007.04.014
NR 23
TC 3
Z9 3
U1 0
U2 2
PY 2012
VL 7666
BP 652
EP 659
WC Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Computer Science, Theory & Methods
DA 2023-11-11
ER

PT J
AU Di Paolo, EA
AF Di Paolo, EA
TI Spike-timing dependent plasticity for evolved robots
SO ADAPTIVE BEHAVIOR
DT Article
DE evolutionary robotics; spiking neural networks; spike-timing dependent
   plasticity; activity-dependent synaptic scaling; neural noise;
   robustness
ID EXPERIENCE
AB Plastic spiking neural networks are synthesized for phototactic robots using evolutionary techniques. Synaptic plasticity asymmetrically depends on the precise relative timing between presynaptic and postsynaptic spikes at the millisecond range and on longer-term activity-dependent regulatory scaling. Comparative studies have been carried out for different kinds of plastic neural networks with low and high levels of neural noise. In all cases, the evolved controllers are highly robust against internal synaptic decay and other perturbations. The importance of the precise timing of spikes is demonstrated by randomizing the spike trains. In the low neural noise scenario, weight values undergo rhythmic changes at the mesoscale due to bursting, but during periods of high activity they are finely regulated at the microscale by synchronous or entrained firing. Spike train randomization results in loss of performance in this case. In contrast, in the high neural noise scenario, robots are robust to loss of information in the timing of the spike trains, demonstrating the counterintuitive results that plasticity, which is dependent on precise spike timing, can work even in its absence, provided the behavioral strategies make use of robust longer-term invariants of sensorimotor interaction. A comparison with a rate-based model of synaptic plasticity shows that under similarly noisy conditions, asymmetric spike-timing dependent plasticity achieves better performance by means of efficient reduction in weight variance over time. Performance also presents negative sensitivity to reduced levels of noise, showing that random firing has a functional value.
C1 Univ Sussex, Sch Cognit & Comp Sci, Brighton BN1 9QH, E Sussex, England.
RP Di Paolo, EA (corresponding author), Univ Sussex, Sch Cognit & Comp Sci, Brighton BN1 9QH, E Sussex, England.
EM ezequiel@cogs.susx.ac.uk
CR Abbott LF, 2000, NAT NEUROSCI, V3, P1178, DOI 10.1038/81453
   Abbott LF, 1996, CEREB CORTEX, V6, P406, DOI 10.1093/cercor/6.3.406
   Beer R. D., 1996, From Animals to Animats 4. Proceedings of the Fourth International Conference on Simulation of Adaptive Behavior, P421
   Beer R.D., 1990, INTELLIGENCE ADAPTIV
   Beer Randall D., 1992, Adaptive Behavior, V1, P91, DOI 10.1177/105971239200100105
   Bi GQ, 2001, ANNU REV NEUROSCI, V24, P139, DOI 10.1146/annurev.neuro.24.1.139
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Brody CD, 1999, NEURAL COMPUT, V11, P1537, DOI 10.1162/089976699300016133
   CHECHIK G, 2002, UNPUB SPIKE TIMING D
   DIPAOLO EA, 1990, ANIMALS ANIMATS, V6
   Floreano D, 2000, NEURAL NETWORKS, V13, P431, DOI 10.1016/S0893-6080(00)00032-0
   FLOREANO D, 2001, EVOLUTIONARY ROBOTIC, V4
   Gerstner W, 1997, P NATL ACAD SCI USA, V94, P12740, DOI 10.1073/pnas.94.24.12740
   HARVEY I, 1994, ANIMALS ANIMATS, V3, P292
   Hopfield JJ, 2001, P NATL ACAD SCI USA, V98, P1282, DOI 10.1073/pnas.031567098
   Horn D, 1998, NEURAL COMPUT, V10, P1, DOI 10.1162/089976698300017863
   Husbands P, 1998, CONNECT SCI, V10, P185, DOI 10.1080/095400998116404
   Jakobi N, 1997, ADAPT BEHAV, V6, P325, DOI 10.1177/105971239700600205
   Kempter R, 1999, PHYS REV E, V59, P4498, DOI 10.1103/PhysRevE.59.4498
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   Mehta MR, 2002, NATURE, V417, P741, DOI 10.1038/nature00807
   Mehta MR, 2000, NEURON, V25, P707, DOI 10.1016/S0896-6273(00)81072-7
   Mehta MR, 1997, P NATL ACAD SCI USA, V94, P8918, DOI 10.1073/pnas.94.16.8918
   Rao RPN, 2001, NEURAL COMPUT, V13, P2221, DOI 10.1162/089976601750541787
   Rubin J, 2001, PHYS REV LETT, V86, P364, DOI 10.1103/PhysRevLett.86.364
   Ruppin E, 2002, NAT REV NEUROSCI, V3, P132, DOI 10.1038/nrn729
   Slocum AC, 2000, FROM ANIM ANIMAT, P430
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Stopfer M, 1997, NATURE, V390, P70, DOI 10.1038/36335
   SUTTON RS, 1988, MACH LEARN, V3, P220
   Tuckwell H.C., 1988, INTRO THEORETICAL NE, V2
   Turrigiano GG, 1999, TRENDS NEUROSCI, V22, P221, DOI 10.1016/S0166-2236(98)01341-1
   Turrigiano GG, 1998, NATURE, V391, P892, DOI 10.1038/36103
   van Rossum MCW, 2000, J NEUROSCI, V20, P8812
   Yao HS, 2001, NEURON, V32, P315, DOI 10.1016/S0896-6273(01)00460-3
NR 37
TC 39
Z9 40
U1 1
U2 10
PY 2002
VL 10
IS 3-4
BP 243
EP 263
DI 10.1177/1059712302010003006
WC Computer Science, Artificial Intelligence; Psychology, Experimental;
   Social Sciences, Interdisciplinary
DA 2023-11-11
ER

PT J
AU Oniz, Y
   Kaynak, O
AF Oniz, Yesim
   Kaynak, Okyay
TI Control of a direct drive robot using fuzzy spiking neural networks with
   variable structure systems-based learning algorithm
SO NEUROCOMPUTING
DT Article
DE Direct drive robot; Spiking neural networks; Variable structure systems
   based learning algorithm; Robot trajectory control
ID TRACKING CONTROL; NONLINEAR-SYSTEMS; CLASSIFICATION; PREDICTION
AB In this work, a sliding mode theory based supervised training algorithm that implements fuzzy reasoning on a spiking neural network has been developed and tested on the trajectory control problem of a two-degrees-of-freedom direct drive robotic manipulator. To describe the generation of a new spike train from the incoming spike trains Spike Response Model has been utilized and the Lyapunov stability method has been adopted in the derivation of the update rules for the neurocontroller parameters. The results of the real-time experiments indicate that stable online tuning and fast learning speed are the prominent characteristics of the proposed algorithm.(C) 2014 Elsevier B.V. All rights reserved.
C1 [Oniz, Yesim; Kaynak, Okyay] Bogazici Univ, Dept Elect & Elect Engn, Istanbul, Turkey.
   [Kaynak, Okyay] Harbin Inst Technol, Harbin, Peoples R China.
RP Oniz, Y (corresponding author), Bogazici Univ, Dept Elect & Elect Engn, Istanbul, Turkey.
EM yesim.oniz@boun.edu.tr; okyay.kaynak@boun.edu.tr
CR Abiyev R., 2012, P 2012 IEEE ASME INT, P1030
   Ahmed S, 2012, EVOL SYST, V3, P179, DOI 10.1007/s12530-012-9053-6
   [Anonymous], 1991, APPL NONLINEAR CONTR
   Ardalani-Farsa M, 2010, NEUROCOMPUTING, V73, P2540, DOI 10.1016/j.neucom.2010.06.004
   Batllori R, 2011, PROCEDIA COMPUT SCI, V6, DOI 10.1016/j.procs.2011.08.060
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Chen CW, 2013, J VIB CONTROL, V19, P1333, DOI 10.1177/1077546312442232
   Gandhi T, 2011, NEUROCOMPUTING, V74, P3051, DOI 10.1016/j.neucom.2011.04.029
   Gerstner W., 2002, SPIKING NEURON MODEL
   Ghosh-Dastidar S, 2007, INTEGR COMPUT-AID E, V14, P187
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   *INT MOT INC, 1992, DIR DRIV MAN RES DEV
   Johansson RS, 2004, NAT NEUROSCI, V7, P170, DOI 10.1038/nn1177
   Juang CF, 2002, IEEE T FUZZY SYST, V10, P155, DOI 10.1109/91.995118
   Kasabov N, 2014, NEUROCOMPUTING, V134, P269, DOI 10.1016/j.neucom.2013.09.049
   Kayacan E, 2012, IEEE T IND ELECTRON, V59, P3510, DOI 10.1109/TIE.2011.2182017
   Li ZK, 2013, IEEE T AUTOMAT CONTR, V58, P518, DOI 10.1109/TAC.2012.2208295
   LIN CT, 1995, FUZZY SET SYST, V70, P183, DOI 10.1016/0165-0114(94)00216-T
   Lin D, 2010, FUZZY SET SYST, V161, P2066, DOI 10.1016/j.fss.2010.03.006
   Maass W, 1997, NEURAL COMPUT, V9, P279, DOI 10.1162/neco.1997.9.2.279
   McCulloch W. S., 1943, B MATH BIOPHYS, V5, P115, DOI [10.1007/BF02478259, DOI 10.1007/BF02478259]
   McFall KS, 2013, J FRANKLIN I, V350, P300, DOI 10.1016/j.jfranklin.2012.11.003
   Mohareri O, 2012, NEUROCOMPUTING, V88, P54, DOI 10.1016/j.neucom.2011.06.035
   Oniz Y, 2009, IEEE T SYST MAN CY B, V39, P551, DOI 10.1109/TSMCB.2008.2007966
   Pan YP, 2013, NEUROCOMPUTING, V99, P15, DOI 10.1016/j.neucom.2012.05.011
   Parma GG, 1998, ELECTRON LETT, V34, P97, DOI 10.1049/el:19980062
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Sadati N, 2008, NEUROCOMPUTING, V71, P2702, DOI 10.1016/j.neucom.2007.06.019
   Shin JH, 2010, IEEE T NEURAL NETWOR, V21, P1697, DOI 10.1109/TNN.2010.2050600
   SLOTINE JJ, 1983, INT J CONTROL, V38, P465, DOI 10.1080/00207178308933088
   Topalov AV, 2011, NEUROCOMPUTING, V74, P1883, DOI 10.1016/j.neucom.2010.07.035
   Topalov AV, 2004, J PROCESS CONTR, V14, P581, DOI 10.1016/j.jprocont.2003.10.005
   Topalov AV, 2001, IEEE T SYST MAN CY B, V31, P445, DOI 10.1109/3477.931542
   Utkin V. I., 1999, SLIDING MODE CONTROL, V9
   VanRullen R, 2005, TRENDS NEUROSCI, V28, P1, DOI 10.1016/j.tins.2004.10.010
   Vreeken J., 2002, UUCS2003008
   Wang XQ, 2008, NEUROCOMPUTING, V71, P655, DOI 10.1016/j.neucom.2007.08.025
   Wang XQ, 2014, NEUROCOMPUTING, V134, P230, DOI 10.1016/j.neucom.2013.07.055
   Wehr M, 1996, NATURE, V384, P162, DOI 10.1038/384162a0
   Werbos P., 1974, THESIS HARVARD U
   Wu QX, 2013, NEUROCOMPUTING, V116, P3, DOI 10.1016/j.neucom.2012.01.046
   Yin S., 2014, DATA BASED TECHNIQUE
   Yin S, 2014, IEEE T IND ELECTRON, V61, P6418, DOI 10.1109/TIE.2014.2301773
   Yu SH, 2004, FUZZY SET SYST, V148, P469, DOI 10.1016/j.fss.2003.12.004
   Zak S.H., 2003, SYSTEMS CONTROL, V388
   Zhang H, 2013, INT J CONTROL, V86, P1824, DOI 10.1080/00207179.2013.797107
   Zhang H, 2013, IEEE T IND INFORM, V9, P337, DOI 10.1109/TII.2012.2225434
NR 47
TC 20
Z9 23
U1 4
U2 55
PD FEB 3
PY 2015
VL 149
BP 690
EP 699
DI 10.1016/j.neucom.2014.07.061
PN B
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT J
AU Adineh-vand, A
   Karimi, G
   Khazaei, M
AF Adineh-vand, Ayoub
   Karimi, Gholamreza
   Khazaei, Mozafar
TI Digital Implementation of a Spiking Convolutional Neural Network for
   Tumor Detection
SO INFORMACIJE MIDEM-JOURNAL OF MICROELECTRONICS ELECTRONIC COMPONENTS AND
   MATERIALS
DT Article
DE Brain tissue; MRI images; Spiking Neural Network; Digital
   Implementation; STDP
ID MODEL; FPGA
AB The structural variation of the brain tissue creates challenges for detection of tumors in MRI images. In this paper, an architecture for spiking convolutional neural networks (SCNNs) is implemented in an embedded system and their potential is evaluated in terms of hardware utilization and power consumption in complex applications such as tumor detection. Accordingly, the structure of the proposed SCNN is implemented on a field-programmable gate array (FPGA) using fixed point arithmetic. To evaluate the speed, accuracy and flexibility of the proposed SCNN, lzhikevich neuron model is used with the spike-timing-dependent plasticity (STDP) learning rule. The suggested neural network is explored for digital implementation possibility and costs. Results of the hardware synthesis and digital implementation are presented on an FPGA.
C1 [Adineh-vand, Ayoub; Karimi, Gholamreza] Razi Univ, Fac Engn, Elect Engn Dept, Kermanshah, Iran.
   [Khazaei, Mozafar] Kermanshah Univ Med Sci, Reprod Res Ctr, Kermanshah, Iran.
RP Karimi, G (corresponding author), Razi Univ, Fac Engn, Elect Engn Dept, Kermanshah, Iran.
EM ghkarimi@razi.ac.ir
CR Azghadi M.R., 2012, 2012 INT JOINT C NEU, P1, DOI [10.1109/IJCNN.2012.6252820, DOI 10.1109/IJCNN.2012.6252820]
   Farsa E. Zaman, 2019, IEEE T CIRCUITS SY 2
   Farsa EZ, 2015, J COMPUT ELECTRON, V14, P707, DOI 10.1007/s10825-015-0709-x
   FITZHUGH R, 1961, BIOPHYS J, V1, P445, DOI 10.1016/S0006-3495(61)86902-6
   Gilan A. A., 2019, IEEE T CIRCUITS SY 2
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Indiveri G, 2006, IEEE T NEURAL NETWOR, V17, P211, DOI 10.1109/TNN.2005.860850
   Guerra-Hernandez EI, 2017, IEEE ACCESS, V5, P8301, DOI 10.1109/ACCESS.2017.2696985
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Jiménez-Fernández A, 2017, IEEE T NEUR NET LEAR, V28, P804, DOI 10.1109/TNNLS.2016.2583223
   Karimi G, 2018, INT J CIRC THEOR APP, V46, P965, DOI 10.1002/cta.2457
   Markram H, 2011, FRONT NEURAL CIRCUIT, V5, DOI 10.3389/fncir.2011.00006
   Motamedi M, 2016, ASIA S PACIF DES AUT, P575, DOI 10.1109/ASPDAC.2016.7428073
   NAGUMO J, 1962, P IRE, V50, P2061, DOI 10.1109/JRPROC.1962.288235
   Naka T, 2019, IEEE T CIRCUITS-II, V66, P1247, DOI 10.1109/TCSII.2018.2876974
   Paugam-Moisy H, 2012, HDB NATURAL COMPUTIN, V1, P1, DOI DOI 10.1007/978-3-540-92910-9_10
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Ponulak F, 2011, ACTA NEUROBIOL EXP, V71, P409
   Pullini A., 2017, IEEE T CIRCUITS SYST
   Putnam A, 2014, CONF PROC INT SYMP C, P13, DOI 10.1109/ISCA.2014.6853195
   Rahman A, 2016, DES AUT TEST EUROPE, P1393
   ROSE RM, 1989, PROC R SOC SER B-BIO, V237, P267, DOI 10.1098/rspb.1989.0049
   Tapiador-Morales R, 2019, IEEE T BIOMED CIRC S, V13, P159, DOI 10.1109/TBCAS.2018.2880012
   Yang SM, 2016, NEUROCOMPUTING, V177, P274, DOI 10.1016/j.neucom.2015.11.026
   Zhang C., 2015, P 2015 ACM SIGDA INT, P161, DOI 10.1145/2684746.2689060
NR 26
TC 4
Z9 4
U1 0
U2 4
PD DEC
PY 2019
VL 49
IS 4
BP 193
EP 201
DI 10.33180/InfMIDEM2019.401
WC Engineering, Electrical & Electronic; Materials Science,
   Multidisciplinary
DA 2023-11-11
ER

PT C
AU Kulkarni, SR
   Alexiades, JM
   Rajendran, B
AF Kulkarni, Shruti R.
   Alexiades, John M.
   Rajendran, Bipin
GP IEEE
TI Learning and Real-time Classification of Hand-written Digits With
   Spiking Neural Networks
SO 2017 24TH IEEE INTERNATIONAL CONFERENCE ON ELECTRONICS, CIRCUITS AND
   SYSTEMS (ICECS)
SE IEEE International Conference on Electronics Circuits and Systems
DT Proceedings Paper
CT 24th IEEE International Conference on Electronics, Circuits and Systems
   (ICECS)
CY DEC 05-08, 2017
CL Batumi, GEORGIA
DE Spiking neural networks; classification; supervised learning; GPU based
   acceleration; real-time processing
AB We describe a novel spiking neural network (SNN) for automated, real-time handwritten digit classification and its implementation on a GP-GPU platform. Information processing within the network, from feature extraction to classification is implemented by mimicking the basic aspects of neuronal spike initiation and propagation in the brain. The feature extraction layer of the SNN uses fixed synaptic weight maps to extract the key features of the image and the classifier layer uses the recently developed NormAD approximate gradient descent based supervised learning algorithm for spiking neural networks to adjust the synaptic weights. On the standard MNIST database images of handwritten digits, our network achieves an accuracy of 99.80% on the training set and 98.06% on the test set, with nearly 7x fewer parameters compared to the state-of-the-art spiking networks. We further use this network in a GPU based user-interface system demonstrating real-time SNN simulation to infer digits written by different users. On a test set of 500 such images, this real-time platform achieves an accuracy exceeding 97% while making a prediction within an SNN emulation time of less than 100 ms.
C1 [Kulkarni, Shruti R.; Alexiades, John M.; Rajendran, Bipin] New Jersey Inst Technol, Dept Elect & Comp Engn, Newark, NJ 07102 USA.
RP Kulkarni, SR (corresponding author), New Jersey Inst Technol, Dept Elect & Comp Engn, Newark, NJ 07102 USA.
EM srk68@njit.edu; jma59@njit.edu; bipin@njit.edu
CR [Anonymous], 2016, ARXIV161105141
   [Anonymous], OPTIMIZING PARALLEL
   Anwani N., 2015, P INT JOINT C NEUR N, P1, DOI DOI 10.1109/IJCNN.2015
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Coates A., 2013, 30 INT C MACH LEARN
   Culjak I., 2012, 2012 35th International Convention on Information and Communication Technology, Electronics and Microelectronics, P1725
   Diehl Peter U, 2015, 2015 INT JOINT C NEU, P1, DOI [DOI 10.1109/IJCNN.2015.7280696, 10.1109/IJCNN.2015.7280696]
   Fidjeland Andreas K, 2010, 2010 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2010.5596678
   Krichmar JL, 2015, ACM J EMERG TECH COM, V11, DOI 10.1145/2629509
   Lee JM, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00191
   Lee WW, 2017, IEEE T NEUR NET LEAR, V28, P849, DOI 10.1109/TNNLS.2015.2509479
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Mohemmed A, 2012, INT J NEURAL SYST, V22, DOI 10.1142/S0129065712500128
   Nageswaran JM, 2009, NEURAL NETWORKS, V22, P791, DOI 10.1016/j.neunet.2009.06.028
   Naveros F, 2017, FRONT NEUROINFORM, V11, DOI 10.3389/fninf.2017.00007
   Olshausen BA, 2003, J COGNITIVE NEUROSCI, V15, P154, DOI 10.1162/089892903321107891
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Rueckauer B., 2016, ARXIV161204052
   Wan L., 2013, P 30 INT C MACH LEAR, V28, P1058
   Yavuz E, 2016, SCI REP-UK, V6, DOI 10.1038/srep18854
   Yudanov D, 2010, IEEE IJCNN
NR 21
TC 5
Z9 5
U1 1
U2 2
PY 2017
BP 128
EP 131
WC Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT S
AU Eriksson, J
   Torres, O
   Mitchell, A
   Tucker, G
   Lindsay, K
   Halliday, D
   Rosenberg, J
   Moreno, JM
   Villa, AEP
AF Eriksson, J
   Torres, O
   Mitchell, A
   Tucker, G
   Lindsay, K
   Halliday, D
   Rosenberg, J
   Moreno, JM
   Villa, AEP
BE Tyrrell, AM
   Haddow, PC
   Torresen, J
TI Spiking neural networks for reconfigurable POEtic tissue
SO EVOLVABLE SYSTEMS: FROM BIOLOGY TO HARDWARE, PROCEEDINGS
SE Lecture Notes in Computer Science
DT Article; Proceedings Paper
CT 5th International Conference on Evolvable Systems
CY MAR 17-20, 2003
CL TRONDHEIM, NORWAY
ID SYNAPTIC PLASTICITY; SIMULATION; MEMORY; LONG
AB Vertebrate and most invertebrate organisms interact with their environment through processes of adaptation and learning. Such processes are generally controlled by complex networks of nerve cells, or neurons, and their interactions. Neurons are characterized by all-or-none discharges - the spikes and the time series corresponding to the sequences of the discharges - the spike trains - carry most of the information used for intercellular communication. This paper describes biologically inspired spiking neural network models suitable for digital hardware implementation. We consider bio-realism, hardware friendliness, and performance as factors which influence the ability of these models to integrate into a flexible computational substrate inspired by evolutionary, developmental and learning aspects of living organisms. Both software and hardware simulations have been used to assess and compare the different models to determine the most suitable spiking neural network model.
C1 Univ York, York YO10 5DD, N Yorkshire, England.
   Univ Glasgow, Glasgow, Lanark, Scotland.
   Tech Univ Catalunya, Barcelona, Spain.
   Univ Lausanne, Lab Neuroheurist, Lausanne, Switzerland.
   Univ Grenoble 1, Grenoble, France.
RP Eriksson, J (corresponding author), Univ York, York YO10 5DD, N Yorkshire, England.
EM acmll@ohm.york.ac.uk
CR [Anonymous], 1998, PULSED NEURAL NETWOR
   [Anonymous], 1998, SYNAPTIC ORG BRAIN
   Bohte S, 2000, P EUR S ART NEUR NET, P419
   CHRISTODOULOU C, 1992, P INT JOINT C NEUR N, V3, P165
   Del Giudice P, 2001, NEUROCOMPUTING, V38, P1175, DOI 10.1016/S0925-2312(01)00557-4
   Fusi S, 2000, NEURAL COMPUT, V12, P2227, DOI 10.1162/089976600300014917
   Fusi S, 2001, NEUROCOMPUTING, V38, P1223, DOI 10.1016/S0925-2312(01)00571-9
   HARTMANN G, 1997, P 6 INT C MICR NEUR, P130
   Hill SL, 1997, NETWORK-COMP NEURAL, V8, P165, DOI 10.1088/0954-898X/8/2/004
   Jahnke A., 1996, Proceedings of the Fifth International Conference on Microelectronics for Neural Networks and Fuzzy Systems. MicroNeuro'96, P232, DOI 10.1109/MNNFS.1996.493796
   Mattia M, 2000, NEURAL COMPUT, V12, P2305, DOI 10.1162/089976600300014953
   MAYA S, 2000, LNCS, V1896, P270
   ROBERTS PD, IN PRESS BIOL CYBERN
   ROGGEN D, UNPUB 5 INT C EV SYS
   TEMPESTI G, UNPUB 5 INT C EV SYS
   TYRRELL AM, 5 INT C EV SYST ICES
   Villa AEP, 2000, CONC ADV BRAIN RES, V3, P1
NR 17
TC 11
Z9 12
U1 0
U2 1
PY 2003
VL 2606
BP 165
EP 173
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Computer Science, Theory & Methods
DA 2023-11-11
ER

PT C
AU Kulkarni, SR
   Baghini, MS
AF Kulkarni, Shruti R.
   Baghini, Maryam Shojaei
BE Wang, H
   Yuen, SY
   Wang, L
   Shao, L
   Wang, X
TI Spiking Neural Network based ASIC for Character Recognition
SO 2013 NINTH INTERNATIONAL CONFERENCE ON NATURAL COMPUTATION (ICNC)
DT Proceedings Paper
CT 9th International Conference on Natural Computation (ICNC)
CY JUL 23-25, 2013
CL Shenyang, PEOPLES R CHINA
DE Spiking Neural networks; character recognition; ASIC design
AB Spiking neural networks are the recent models of artificial neural networks. These networks use biologically similar neuron models as their basic computation units. This paper presents and compares a custom spiking neural network (SNN) with a conventional nearest neighbour classifier for hand written character recognition. The classifiers are designed and simulated in 90nm CMOS technology. The two algorithms are compared in terms of their success rates and their hardware requirements (based on the area and power estimates). The classification performance of the SNN is also compared with that of second generation feedforward neural network, with the same set of images. The robustness of SNN is demonstrated in this work by its ability to classify the 30 out of 32 noisy characters images presented as compared to the nearest neighbour algorithm, which correctly classified only 20 of them. The feedforward neural network using backpropagation algorithm was able to correctly identify 29 out of 32 noisy images in MATLAB. In terms of hardware, the ASIC realizing the nearest neighbour classifier dissipates power of 1.2mW and an area of 380 m x 380 m, while the SNN dissipates 16.7mW power and an area of 1mm x 1mm. The higher area and power requirements for the SNN stem from its inherent parallel architecture. Earlier works have focused on realization of a single spiking neuron and its variants while this work brings about the application using networks of these neurons and their suitability for custom realization.
C1 [Kulkarni, Shruti R.; Baghini, Maryam Shojaei] Indian Inst Technol, Dept Elect Engn, Bombay, Maharashtra, India.
RP Kulkarni, SR (corresponding author), Indian Inst Technol, Dept Elect Engn, Bombay, Maharashtra, India.
EM shruti123k@yahoo.co.in; mshojaei@ee.iitb.ac.in
CR Abbott LF, 1999, BRAIN RES BULL, V50, P303, DOI 10.1016/S0361-9230(99)00161-6
   Bhuiyan MA, 2009, CIMSVP 2009: IEEE SYMPOSIUM ON COMPUTATIONAL INTELLIGENCE FOR MULTIMEDIA SIGNAL AND VISION PROCESSING, P29
   Blumenstein M, 2003, PROC INT CONF DOC, P137
   Gaurav D. D., 2012, CORR
   Gerstner W, 2002, BIOL CYBERN, V87, P404, DOI 10.1007/s00422-002-0353-y
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Gupta A, 2007, IEEE IJCNN, P53, DOI 10.1109/IJCNN.2007.4370930
   Hong L, 2010, J APPL SCI, V10, P1841
   Jolivet R, 2003, LECT NOTES COMPUT SC, V2714, P846
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Mamtha H.R., 2011, P INT C SOFTW COMP A, V9
   Nagare A. P., 2011, INT J COMPUTER APPL, V25, P36
   PANCHEV C, 2002, P INT C ART NEUR NET, P896
   Paugam-moisy H., 2010, COMPUTING SPIKING NE
   Singh B, 2011, J PATTERN RECOGNIT R, V6, P269, DOI 10.13176/11.302
   Suruchi G. D., 2012, INT J ENG INNOVATIVE, V1
   Vamvakas G, 2010, PATTERN RECOGN, V43, P2807, DOI 10.1016/j.patcog.2010.02.018
NR 17
TC 0
Z9 0
U1 0
U2 4
PY 2013
BP 194
EP 199
WC Computer Science, Artificial Intelligence; Mathematical & Computational
   Biology
DA 2023-11-11
ER

PT J
AU Zhang, ML
   Qu, H
   Xie, XR
   Kurths, J
AF Zhang, Malu
   Qu, Hong
   Xie, Xiurui
   Kurths, Juergen
TI Supervised learning in spiking, neural networks with noise-threshold
SO NEUROCOMPUTING
DT Article
DE Spiking neurons; Noise-threshold; Supervised learning; Spiking neural
   networks (SNNs); Anti-noise capability
ID PRECISION; NEURONS; INFORMATION; SIGNALS; TRAINS; MODEL
AB With a similar capability of processing spikes as biological neural systems, networks of spiking neurons are expected to achieve a performance similar to that of living brains. Despite the achievement of spiking neuron based applications, most of them assume noise-free condition for learning and testing. This assumption, though fairly general, ignores the fact that noise widely exists in spiking neural networks (SNNs) and the neural response can be significantly disturbed by noise. Therefore, how to deal with noise is an important issue in the applications of SNNs. Here, by analyzing strategies employed to make spiking neurons robust to noise, also inspired by biological neurons, we propose a strategy that train spiking neurons with a dynamic firing threshold named noise-threshold. The noise-threshold can be applied by the existing supervised learning methods to improve the noise tolerance of them. Experimental results show that, with a combination of noise-threshold, the anti-noise capability of the existing supervised learning methods improves significantly, and the trained neuron can precisely and reliably reproduce target sequences of spikes even under highly noisy conditions. More importantly, the SNNs-based computational model equipped with a noise-threshold is more robust and can achieve a good performance even with different types of noise. Therefore, the noise-threshold is significant to practical applications and theoretical researches of SNNs.
C1 [Zhang, Malu; Qu, Hong; Xie, Xiurui] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 610054, Peoples R China.
   [Kurths, Juergen] Potsdam Inst Climate Impact Res PIK, D-14473 Potsdam, Germany.
   [Kurths, Juergen] Humboldt Univ, Dept Phys, D-12489 Berlin, Germany.
   [Kurths, Juergen] Univ Aberdeen, Inst Complex Syst & Math Biol, Aberdeen AB24 3UE, Scotland.
RP Qu, H (corresponding author), Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 610054, Peoples R China.
EM hongqu@uestc.edu.cn
CR Albers C., ARXIV14076525
   [Anonymous], PLOS ONE
   Azouz R, 2000, P NATL ACAD SCI USA, V97, P8110, DOI 10.1073/pnas.130200797
   Bair W, 1996, NEURAL COMPUT, V8, P1185, DOI 10.1162/neco.1996.8.6.1185
   Berry MJ, 1998, ADV NEUR IN, V10, P110
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Borst A, 1999, NAT NEUROSCI, V2, P947, DOI 10.1038/14731
   Brunel N, 1999, NEURAL COMPUT, V11, P1621, DOI 10.1162/089976699300016179
   Chicca E., 2000, P WORLD C NEUR
   Dora S, 2016, NEUROCOMPUTING, V171, P1216, DOI 10.1016/j.neucom.2015.07.086
   Florian R.V., 2013, PLOS ONE, V7
   Fusi S, 2000, IEEE IJCNN, P121, DOI 10.1109/IJCNN.2000.861291
   Gerstner W., 2002, SPIKING NEURON MODEL
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Gollisch T, 2008, SCIENCE, V319, P1108, DOI 10.1126/science.1149639
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Henze DA, 2001, NEUROSCIENCE, V105, P121, DOI 10.1016/S0306-4522(01)00167-1
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Hu J, 2013, NEURAL COMPUT, V25, P450, DOI 10.1162/NECO_a_00395
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Kempter R, 1999, ADV NEUR IN, V11, P125
   Kerr D, 2015, NEUROCOMPUTING, V158, P268, DOI 10.1016/j.neucom.2015.01.011
   Maass W, 1997, NEURAL COMPUT, V9, P279, DOI 10.1162/neco.1997.9.2.279
   Maass W., 2006, NEURAL NETWORKS, V10, P1659
   Maass W., NOISY SPIKING NEURON
   MAINEN ZF, 1995, SCIENCE, V268, P1503, DOI 10.1126/science.7770778
   Manwani A, 1999, NEURAL COMPUT, V11, P1797, DOI 10.1162/089976699300015972
   Memmesheimer RM, 2014, NEURON, V82, P925, DOI 10.1016/j.neuron.2014.03.026
   Mohemmed A, 2012, INT J NEURAL SYST, V22, DOI 10.1142/S0129065712500128
   Montemurro MA, 2007, J NEUROPHYSIOL, V98, P1871, DOI 10.1152/jn.00593.2007
   Nadasdy Z, 2009, FRONT SYST NEUROSCI, V3, DOI 10.3389/neuro.06.006.2009
   Ponulak F., 2006, THESIS
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Qu H, 2015, NEUROCOMPUTING, V151, P310, DOI 10.1016/j.neucom.2014.09.034
   Qu H, 2009, IEEE T NEURAL NETWOR, V20, P1724, DOI 10.1109/TNN.2009.2029858
   Reinagel P, 2000, J NEUROSCI, V20, P5392, DOI 10.1523/JNEUROSCI.20-14-05392.2000
   Rieke F., SPIKES EXPLORING NEU
   Rostro-Gonzalez H, 2015, NEUROCOMPUTING, V170, P47, DOI 10.1016/j.neucom.2015.03.090
   Rostro-Gonzalez H., 2015, NEUROCOMPUTING
   Schneidman E, 1998, NEURAL COMPUT, V10, P1679, DOI 10.1162/089976698300017089
   Schneidman E., THESIS
   Schreiber S, 2003, NEUROCOMPUTING, V52-4, P925, DOI 10.1016/S0925-2312(02)00838-X
   Shmiel T, 2005, P NATL ACAD SCI USA, V102, P18655, DOI 10.1073/pnas.0509346102
   Taherkhani A., 2015, IEEE T NEURAL NETW L
   Taherkhani A., 2014, P ESANN
   Taherkhani A., 2015, INT JOINT C NEUR NET, P1
   Uzzell VJ, 2004, J NEUROPHYSIOL, V92, P780, DOI 10.1152/jn.01171.2003
   van Rossum MCW, 2003, J NEUROPHYSIOL, V89, P2406, DOI 10.1152/jn.01106.2002
   vanVreeswijk C, 1996, SCIENCE, V274, P1724, DOI 10.1126/science.274.5293.1724
   Nguyen VA, 2012, IEEE T NEUR NET LEAR, V23, P971, DOI 10.1109/TNNLS.2012.2191419
   Wang JL, 2014, NEUROCOMPUTING, V144, P526, DOI 10.1016/j.neucom.2014.04.017
   Wang WW, 2012, IEEE T NEUR NET LEAR, V23, P1574, DOI 10.1109/TNNLS.2012.2208477
   Wang ZZ, 2016, NEUROCOMPUTING, V173, P1203, DOI 10.1016/j.neucom.2015.08.078
   Wei S, 2011, NEUROCOMPUTING, V74, P1485, DOI 10.1016/j.neucom.2011.01.005
   Xu Y, 2013, NEURAL COMPUT, V25, P1472, DOI 10.1162/NECO_a_00450
   Yu Q., 2015, IEEE T NEURAL NETW L
   Yu Q, 2013, IEEE T NEUR NET LEAR, V24, P1539, DOI 10.1109/TNNLS.2013.2245677
   Zhang M., 2015, P 18 AS PAC S INT EV, V1
NR 58
TC 50
Z9 50
U1 1
U2 35
PD JAN 5
PY 2017
VL 219
BP 333
EP 349
DI 10.1016/j.neucom.2016.09.044
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT C
AU Tao, XL
   Michel, HE
AF Tao, XL
   Michel, HE
BE Saito, TT
TI Novel artificial neural networks for remote-sensing data classification
SO Optics and Photonics in Global Homeland Security
SE PROCEEDINGS OF THE SOCIETY OF PHOTO-OPTICAL INSTRUMENTATION ENGINEERS
   (SPIE)
DT Proceedings Paper
CT Conference on Optics and Photonics in Global Homeland Security
CY MAR 29-APR 01, 2005
CL Orlando, FL
DE normalized radial basis function (NRBF); spiking neural network;
   remote-sensing data classification
ID SPIKING NEURONS
AB This paper discusses two novel artificial neural network architectures applied to multi-class classification problems of remote-sensing data. These approaches are 1) a spiking neural-network model for the partitioning of data into clusters, and 2) a neuron model based on complex-valued weights (CVN). In the former model, the learning process is based on the Spike Timing-Dependent Plasticity rule under the Hebbian Learning framework. With temporally encoded inputs, the synaptic efficiencies of the delays between the pre- and post-synaptic spikes can store the information of different data clusters. With the encoding method using Gaussian receptive fields, the model was applied to the remote-sensing data. The result showed that it could provide more useful information than using traditional clustering method such as K-means. The CVN model has proved to be more powerful than traditional neuron models in solving the XOR problem and image processing problems. This paper discusses an implementation of the complex-valued neuron in NRBF neural networks to improve the NRBF structure. The complex-valued weights are used in the supervised learning part of an NRBF neural network. This classifier was tested with satellite multi-spectral image data and results show that this neural network model is more accurate and powerful than the conventional NRBF model.
C1 Univ Massachusetts, Dept Elect & Comp Engn, Dartmouth, MA 02747 USA.
RP Tao, XL (corresponding author), Univ Massachusetts, Dept Elect & Comp Engn, 285 Old Westport Rd, Dartmouth, MA 02747 USA.
CR [Anonymous], 2001, RADIAL BASIS FUNCTIO
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   Cha I, 1996, IEEE T IMAGE PROCESS, V5, P964, DOI 10.1109/83.503912
   Gerstner W, 2002, BIOL CYBERN, V87, P404, DOI 10.1007/s00422-002-0353-y
   Hebb D. O., 1949, ORG BEHAV
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   MOODY J, 1988, P 1988 CONN MOD SUM
   Moody J, 1989, NEURAL COMPUT, V1, P281, DOI 10.1162/neco.1989.1.2.281
   Natschlager T, 1998, NETWORK-COMP NEURAL, V9, P319, DOI 10.1088/0954-898X/9/3/003
   ROBERT A, 1997, REMOTE SENSING MODEL
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   TAO X, 2003, P SPIE
   TAO X, 2004, P IC AI 04 JUN 21 24
NR 14
TC 2
Z9 2
U1 0
U2 3
PY 2005
VL 5781
BP 127
EP 138
DI 10.1117/12.609117
WC Optics; Imaging Science & Photographic Technology
DA 2023-11-11
ER

PT J
AU Siddique, N
   Widrow, B
   Maguire, L
AF Siddique, Nazmul
   Widrow, Bernard
   Maguire, Liam
TI Special Issue: Spiking Neural Networks INTRODUCTION
SO INTERNATIONAL JOURNAL OF NEURAL SYSTEMS
DT Editorial Material
CR Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Goudar V, 2014, NAT NEUROSCI, V17, P487, DOI 10.1038/nn.3679
   Halassa MM, 2010, ANNU REV PHYSIOL, V72, P335, DOI 10.1146/annurev-physiol-021909-135843
   Henneberger C, 2010, NATURE, V463, P232, DOI 10.1038/nature08673
   Kumar A, 2010, NAT REV NEUROSCI, V11, P615, DOI 10.1038/nrn2886
   Owen SF, 2013, NATURE, V500, P458, DOI 10.1038/nature12330
NR 6
TC 0
Z9 0
U1 0
U2 13
PD AUG
PY 2014
VL 24
IS 5
SI SI
AR 1403002
DI 10.1142/S0129065714030026
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT C
AU Zou, CL
   Cui, XX
   Kuang, YS
   Wang, Y
   Wang, XA
AF Zou, Chenglong
   Cui, Xiaoxin
   Kuang, Yisong
   Wang, Yuan
   Wang, Xinan
GP IEEE
TI A Hybrid Spiking Recurrent Neural Network on Hardware for Efficient
   Emotion Recognition
SO 2022 IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE CIRCUITS
   AND SYSTEMS (AICAS 2022): INTELLIGENT TECHNOLOGY IN THE POST-PANDEMIC
   ERA
DT Proceedings Paper
CT IEEE International Conference on Artificial Intelligence Circuits and
   Systems (AICAS) - Intelligent Technology in the Post-Pandemic Era
CY JUN 13-15, 2022
CL Incheon, SOUTH KOREA
DE AI; RNN; SNN; emotion recognition
AB In recent years, neuromorphic engineering based on spiking neural networks (SNNs) for real-time and low-power artificial intelligence (AI) tasks has attracted a lot of interest. However, most of the previous implementations on hardware of these algorithms concentrate on traditional feedforward fully-connected/convolutional neural network (CNNs) architectures which are used for vision image processing. Their applications in temporal text tasks using recurrent neural networks (RNNs) is less discussed. In this paper, we point out main difficulties of RNNs implementation on conventional neuromorphic systems and propose a hardware-oriented spiking RNN architecture for emotion recognition, which absorbs the external dynamics of traditional RNN and internal dynamics of SNN. Experimental results on two emotion recognition datasets show our spiking RNNs achieve comparable accuracies with other deep learning models and efficient run-time performance.
C1 [Zou, Chenglong; Cui, Xiaoxin; Kuang, Yisong; Wang, Yuan] Peking Univ, Sch Integrated Circuits, Key Lab Microelect Devices & Circuits, Beijing 100871, Peoples R China.
   [Zou, Chenglong; Wang, Xinan] Peking Univ, Sch ECE, Key Lab Integrated Microsyst, Shenzhen Grad Sch, Shenzhen 518055, Peoples R China.
RP Cui, XX; Wang, Y (corresponding author), Peking Univ, Sch Integrated Circuits, Key Lab Microelect Devices & Circuits, Beijing 100871, Peoples R China.
EM cuixx@pku.edu.cn; wangyuan@pku.edu.cn
CR Amir A, 2017, PROC CVPR IEEE, P7388, DOI 10.1109/CVPR.2017.781
   BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181
   Bengio Y, 2013, Arxiv, DOI arXiv:1305.2982
   Bouvier M, 2019, ACM J EMERG TECH COM, V15, DOI 10.1145/3304103
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Deng L, 2020, NEURAL NETWORKS, V121, P294, DOI 10.1016/j.neunet.2019.09.005
   Diehl PU, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON REBOOTING COMPUTING (ICRC)
   Diehl PU, 2015, IEEE IJCNN
   Esser SK, 2016, P NATL ACAD SCI USA, V113, P11441, DOI 10.1073/pnas.1604850113
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Kuang YS, 2021, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS51556.2021.9401195
   Levy O, 2014, ADV NEUR IN, V27
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Yousefzadeh A, 2019, 2019 IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE CIRCUITS AND SYSTEMS (AICAS 2019), P81, DOI [10.1109/AICAS.2019.8771624, 10.1109/aicas.2019.8771624]
   Zhang Ye, 2016, Arxiv, DOI [arXiv:1510.03820, DOI 10.48550/ARXIV.1510.03820]
   Zou C., 2020, IEEE INT SYMP CIRC S, P1, DOI DOI 10.1109/iscas45731.2020.9180918
   Zou CL, 2021, FRONT NEUROSCI-SWITZ, V15, DOI 10.3389/fnins.2021.694170
NR 18
TC 0
Z9 0
U1 0
U2 6
PY 2022
BP 332
EP 335
DI 10.1109/AICAS54282.2022.9869950
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Morie, T
AF Morie, Takashi
GP IEEE
TI CMOS Circuits and Nanodevices for Spike Based Neural Computing
SO 2015 IEEE INTERNATIONAL MEETING FOR FUTURE OF ELECTRON DEVICES, KANSAI
   (IMFEDK)
DT Proceedings Paper
CT IEEE International Meeting for Future of Electron Devices, Kansai
   (IMFEDK)
CY JUN 04-05, 2015
CL Kyoto, JAPAN
DE spiking neuron; pulse-coupled oscillator; coupled Markov random field
   model; multiply-and-accumulation calculation; CMOS circuit; nanodisk
   array
AB This paper describes hardware implementation of two integrate-and-fire type neuron models for spike based computing: pulse-coupled phase oscillator networks and spiking neural networks. A coupled Markov random field model for image region segmentation can be implemented using a pulse-coupled phase oscillator network. Multiply-and-accumulation calculation can be performed using rise timing of responses in an integrate-and-fire type spiking neuron model. Both oscillator and neuron models can be implemented by CMOS circuits consisting of capacitors with current sources or resistors. For constructing large-scale networks, nanodisk array structures are used for realizing high resistance.
C1 [Morie, Takashi] Kyushu Inst Technol, Grad Sch Life Sci & Syst Engn, Kitakyushu, Fukuoka 8080196, Japan.
RP Morie, T (corresponding author), Kyushu Inst Technol, Grad Sch Life Sci & Syst Engn, Kitakyushu, Fukuoka 8080196, Japan.
EM morie@brain.kyutech.ac.jp
CR Huang CH, 2009, JPN J APPL PHYS, V48, DOI 10.1143/JJAP.48.04C187
   Igarashi M, 2010, APPL PHYS EXPRESS, V3, DOI 10.1143/APEX.3.085202
   MAASS W., 1999, PULSED NEURAL NETWOR
   Matsuzaka K., 2012, NONLINEAR THEORY ITS, V3, P180
   Matsuzaka K, 2015, ELECTRON LETT, V51, P46, DOI 10.1049/el.2014.2105
   Morie T, 2014, ASIA S PACIF DES AUT, P185, DOI 10.1109/ASPDAC.2014.6742887
NR 6
TC 1
Z9 1
U1 0
U2 0
PY 2015
WC Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Liu, Q
   Furber, S
AF Liu, Qian
   Furber, Steve
BE Hirose, A
   Ozawa, S
   Doya, K
   Ikeda, K
   Lee, M
   Liu, D
TI Noisy Softplus: A Biology Inspired Activation Function
SO NEURAL INFORMATION PROCESSING, ICONIP 2016, PT IV
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 23rd International Conference on Neural Information Processing (ICONIP)
CY OCT 16-21, 2016
CL Kyoto, JAPAN
DE Noisy softplus; Biologically-inspired; Spiking neural network;
   Activation function; LIF neurons
AB The Spiking Neural Network (SNN) has not achieved the recognition/classification performance of its non-spiking competitor, the Artificial Neural Network(ANN), particularly when used in deep neural networks. The mapping of a well-trained ANN to an SNN is a hot topic in this field, especially using spiking neurons with biological characteristics. This paper proposes a new biologically-inspired activation function, Noisy Softplus, which is well-matched to the response function of LIF (Leaky Integrate-and-Fire) neurons. A convolutional network (ConvNet) was trained on the MNIST database with Noisy Softplus units and converted to an SNN while maintaining a close classification accuracy. This result demonstrates the equivalent recognition capability of the more biologically-realistic SNNs and bring biological features to the activation units in ANNs.
C1 [Liu, Qian; Furber, Steve] Univ Manchester, Adv Processor Technol Grp, Manchester M13 9PL, Lancs, England.
RP Liu, Q (corresponding author), Univ Manchester, Adv Processor Technol Grp, Manchester M13 9PL, Lancs, England.
EM qian.liu-3@manchester.ac.uk; steve.furber@manchester.ac.uk
CR Buesing L, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1002211
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Davison Andrew P, 2008, Front Neuroinform, V2, P11, DOI 10.3389/neuro.11.011.2008
   Diehl P.U., 2016, CONVERSION ARTIFICIA
   Diehl PU, 2015, IEEE IJCNN
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   Hunsberger E., 2015, SPIKING DEEP NETWORK
   Jug F, 2012, SWISS SOC NEUROSCI, V1
   Liu Q., 2016, FRONT NEURO IN PRESS
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Neftci E, 2014, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00272
   OConnor P, 2016, DEEP SPIKING NETWORK
   Stromatias E., 2015, INT JOINT C NEUR NET
   Xavier Glorot, 2011, P 14 INT C ARTIFICIA, P315, DOI DOI 10.1002/ECS2.1832
NR 15
TC 21
Z9 24
U1 0
U2 12
PY 2016
VL 9950
BP 405
EP 412
DI 10.1007/978-3-319-46681-1_49
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods
DA 2023-11-11
ER

PT J
AU Yu, HT
   Guo, XM
   Wang, J
   Deng, B
   Wei, XL
AF Yu, Haitao
   Guo, Xinmeng
   Wang, Jiang
   Deng, Bin
   Wei, Xile
TI Vibrational resonance in adaptive small-world neuronal networks with
   spike-timing-dependent plasticity
SO PHYSICA A-STATISTICAL MECHANICS AND ITS APPLICATIONS
DT Article
DE Vibrational resonance; Spike-timing-dependent plasticity; Neural
   network; Small-world network
ID STOCHASTIC RESONANCE; EXCITABLE SYSTEMS; COHERENCE; NOISE;
   SYNCHRONIZATION; CONNECTIVITY; ENHANCEMENT; MECHANISMS; MEDIA
AB The phenomenon of vibrational resonance is investigated in adaptive Newman-Watts small-world neuronal networks, where the strength of synaptic connections between neurons is modulated based on spike-timing-dependent plasticity. Numerical results demonstrate that there exists appropriate amplitude of high-frequency driving which is able to optimize the neural ensemble response to the weak low-frequency periodic signal. The effect of networked vibrational resonance can be significantly affected by spike-timing-dependent plasticity. It is shown that spike-timing-dependent plasticity with dominant depression can always improve the efficiency of vibrational resonance, and a small adjusting rate can promote the transmission of weak external signal in small-world neuronal networks. In addition, the network topology plays an important role in the vibrational resonance in spike-timing-dependent plasticity-induced neural systems, where the system response to the subthreshold signal is maximized by an optimal network structure. Furthermore, it is demonstrated that the introduction of inhibitory synapses can considerably weaken the phenomenon of vibrational resonance in the hybrid small-world neuronal networks with spike-timing-dependent plasticity. (C) 2015 Elsevier B.V. All rights reserved.
C1 [Yu, Haitao; Guo, Xinmeng; Wang, Jiang; Deng, Bin; Wei, Xile] Tianjin Univ, Sch Elect Engn & Automat, Tianjin 300072, Peoples R China.
RP Wang, J (corresponding author), Tianjin Univ, Sch Elect Engn & Automat, Tianjin 300072, Peoples R China.
EM eejwang@tju.edu.cn
CR Bayati M, 2012, PHYS REV E, V86, DOI 10.1103/PhysRevE.86.011925
   BENZI R, 1981, J PHYS A-MATH GEN, V14, pL453, DOI 10.1088/0305-4470/14/11/006
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Bordet M, 2012, ELECTRON LETT, V48, P903, DOI 10.1049/el.2012.1343
   Chizhevsky VN, 2008, PHYS REV E, V77, DOI 10.1103/PhysRevE.77.051126
   Deng B, 2010, CHAOS, V20, DOI 10.1063/1.3324700
   Feldman DE, 2005, SCIENCE, V310, P810, DOI 10.1126/science.1115807
   Fino E, 2009, NEUROSCIENCE, V160, P744, DOI 10.1016/j.neuroscience.2009.03.015
   Gammaitoni L, 1998, REV MOD PHYS, V70, P223, DOI 10.1103/RevModPhys.70.223
   Gitterman M, 2001, J PHYS A-MATH GEN, V34, pL355, DOI 10.1088/0305-4470/34/24/101
   Haenggi P., 1996, Nonlinear physics of complex systems. Current status and future trends, P294
   Hu DL, 2014, COMPUT BIOL MED, V45, P80, DOI 10.1016/j.compbiomed.2013.11.022
   Karmarkar UR, 2002, BIOL CYBERN, V87, P373, DOI 10.1007/s00422-002-0351-0
   Kleberg FI, 2014, FRONT COMPUT NEUROSC, V8, DOI 10.3389/fncom.2014.00053
   Landa PS, 2000, J PHYS A-MATH GEN, V33, pL433, DOI 10.1088/0305-4470/33/45/103
   Lee S, 2009, PLOS COMPUT BIOL, V5, DOI 10.1371/journal.pcbi.1000602
   Li XM, 2009, CHAOS, V19, DOI 10.1063/1.3076394
   Lindner B, 2004, PHYS REP, V392, P321, DOI 10.1016/j.physrep.2003.10.015
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   Nevian T, 2006, J NEUROSCI, V26, P11001, DOI 10.1523/JNEUROSCI.1749-06.2006
   Newman MEJ, 1999, PHYS LETT A, V263, P341, DOI 10.1016/S0375-9601(99)00757-4
   Nishiyama M, 2000, NATURE, V408, P584, DOI 10.1038/35046067
   Ozer M, 2009, PHYS LETT A, V373, P964, DOI 10.1016/j.physleta.2009.01.034
   Perc M, 2005, NEW J PHYS, V7, DOI 10.1088/1367-2630/7/1/252
   Perc M, 2005, PHYS REV E, V71, DOI 10.1103/PhysRevE.71.026229
   Perc M, 2007, PHYS REV E, V76, DOI 10.1103/PhysRevE.76.066203
   Perc M, 2007, CHAOS SOLITON FRACT, V31, P280, DOI 10.1016/j.chaos.2005.10.018
   Rajasekar S, 2011, CHAOS, V21, DOI 10.1063/1.3610213
   Ren QS, 2012, PHYS REV E, V86, DOI 10.1103/PhysRevE.86.056103
   Ren QS, 2012, PHYS REV E, V85, DOI 10.1103/PhysRevE.85.022901
   Shin CW, 2006, PHYS REV E, V74, DOI 10.1103/PhysRevE.74.045101
   Song S, 2001, NEURON, V32, P339, DOI 10.1016/S0896-6273(01)00451-2
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Sun XJ, 2008, CHAOS, V18, DOI 10.1063/1.2900402
   Tzounopoulos T, 2007, NEURON, V54, P291, DOI 10.1016/j.neuron.2007.03.026
   Ullner E, 2003, PHYS LETT A, V312, P348, DOI 10.1016/S0375-9601(03)00681-9
   Uzuntarla M, 2015, COMMUN NONLINEAR SCI, V22, P367, DOI 10.1016/j.cnsns.2014.08.040
   van Rossum MCW, 2000, J NEUROSCI, V20, P8812
   Wang CJ, 2014, INT J MOD PHYS B, V28, DOI 10.1142/S0217979214501033
   Wang QY, 2010, INT J MOD PHYS B, V24, P1201, DOI 10.1142/S0217979210055317
   Wang QY, 2008, PHYS LETT A, V372, P5681, DOI 10.1016/j.physleta.2008.07.005
   Wang QY, 2012, CHAOS, V22, DOI 10.1063/1.4767719
   Wang QY, 2012, COMMUN NONLINEAR SCI, V17, P3979, DOI 10.1016/j.cnsns.2012.02.019
   Wang QY, 2009, CHAOS, V19, DOI 10.1063/1.3133126
   WIESENFELD K, 1995, NATURE, V373, P33, DOI 10.1038/373033a0
   Wu D, 2011, PHYS REV E, V84, DOI 10.1103/PhysRevE.84.021102
   Xue M, 2013, EUR PHYS J B, V86, DOI 10.1140/epjb/e2013-30782-3
   Yang JH, 2010, CHAOS, V20, DOI 10.1063/1.3481343
   Yang LJ, 2012, PHYS REV E, V86, DOI 10.1103/PhysRevE.86.016209
   Yu HT, 2015, PHYSICA A, V419, P307, DOI 10.1016/j.physa.2014.10.031
   Yu HT, 2012, CHAOS, V22, DOI 10.1063/1.4729462
   Zhigulin VP, 2003, PHYS REV E, V67, DOI 10.1103/PhysRevE.67.021901
NR 52
TC 12
Z9 12
U1 2
U2 41
PD OCT 15
PY 2015
VL 436
BP 170
EP 179
DI 10.1016/j.physa.2015.05.037
WC Physics, Multidisciplinary
DA 2023-11-11
ER

PT J
AU Delbruck, T
   van Schaik, A
   Hasler, J
AF Delbruck, Tobi
   van Schaik, Andre
   Hasler, Jennifer
TI Research topic: neuromorphic engineering systems and applications. A
   snapshot of neuromorphic systems engineering
SO FRONTIERS IN NEUROSCIENCE
DT Editorial Material
DE neuromorphic engineering; neural networks; event-based; spiking neural
   networks; dynamic vision sensor; floating gate; neural simulation;
   synaptic plasticity
C1 [Delbruck, Tobi] Univ Zurich, Inst Neuroinformat, Zurich, Switzerland.
   [Delbruck, Tobi] ETH, Zurich, Switzerland.
   [van Schaik, Andre] Univ Western Sydney, MARCS Inst, Sydney, NSW, Australia.
   [Hasler, Jennifer] Georgia Inst Technol, Sch Elect & Comp Engn, Atlanta, GA 30332 USA.
RP van Schaik, A (corresponding author), Univ Western Sydney, MARCS Inst, Sydney, NSW, Australia.
EM a.vanschaik@uws.edu.au
CR Ambroise M, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00215
   Brandli C, 2014, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00275
   Camuñas-Mesa LA, 2014, FRONT NEUROSCI-SWITZ, V8, DOI 10.3389/fnins.2014.00048
   Carlson KD, 2014, FRONT NEUROSCI-SWITZ, V8, DOI 10.3389/fnins.2014.00010
   Clady X, 2014, FRONT NEUROSCI-SWITZ, V8, DOI 10.3389/fnins.2014.00009
   Coath M, 2014, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00278
   Delbruck T, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00223
   Gupta P, 2014, FRONT NEUROSCI-SWITZ, V8, DOI 10.3389/fnins.2014.00054
   Marr B, 2014, FRONT NEUROSCI-SWITZ, V8, DOI 10.3389/fnins.2014.00086
   Neftci E, 2014, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00272
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   Rea F, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00234
   Sandamirskaya Y, 2014, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00276
   Wang RCM, 2014, FRONT NEUROSCI-SWITZ, V8, DOI 10.3389/fnins.2014.00051
NR 14
TC 3
Z9 3
U1 1
U2 26
PD DEC 19
PY 2014
VL 8
AR 424
DI 10.3389/fnins.2014.00424
WC Neurosciences
DA 2023-11-11
ER

PT C
AU Saleh, AY
   Hamed, HNB
   Shamsuddin, SM
   Ibrahim, AO
AF Saleh, Abdulrazak Yahya
   Hamed, Haza Nuzly Bin Abdull
   Shamsuddin, Siti Mariyam
   Ibrahim, Ashraf Osman
BE Saeed, F
   Gazem, N
   Patnaik, S
   Balaid, ASS
   Mohammed, F
TI A New Hybrid K-Means Evolving Spiking Neural Network Model Based on
   Differential Evolution
SO RECENT TRENDS IN INFORMATION AND COMMUNICATION TECHNOLOGY
SE Lecture Notes on Data Engineering and Communications Technologies
DT Proceedings Paper
CT 2nd International Conference of Reliable Information and Communication
   Technology (IRICT)
CY APR 23-24, 2017
CL Johor, MALAYSIA
DE Clustering; K-means; Differential evolution; Spiking neural network;
   Evolving spiking neural networks; K-DESNN
ID ALGORITHM; NEURONS; OPTIMIZATION
AB Clustering is one of the essential unsupervised learning techniques in Data Mining. In this paper, a new hybrid (K-DESNN) approach to combine differential evolution and K-means evolving spiking neural network model (K-means ESNN) for clustering problems has been proposed. The proposed model examines that ESNN improves by using K-DESNN model. This approach improves the flexibility of the ESNN algorithm in producing better solutions which is utilized to conquer the K-means disadvantages. Various UCI machine learning data sets have been utilized for evaluating the performance of this model. The results have shown that K-DESNN is much better than the original K-means ESNN in the number of pre-synaptic neurons measure and clustering accuracy performance.
C1 [Saleh, Abdulrazak Yahya] Univ Malaysia Sarawak, FSKPM Fac, Kota Samarahan 94300, Sarawak, Malaysia.
   [Hamed, Haza Nuzly Bin Abdull] UTM, Fac Comp, Soft Comp Res Grp, Skudai 81310, Johor, Malaysia.
   [Shamsuddin, Siti Mariyam] UTM, UTM Big Data Ctr, Skudai 81310, Johor, Malaysia.
   [Ibrahim, Ashraf Osman] Alzaiem Alazhari Univ, Fac Comp & Technol, Khartoum, Sudan.
   [Ibrahim, Ashraf Osman] Nile Coll, Khartoum 11111, Sudan.
RP Saleh, AY (corresponding author), Univ Malaysia Sarawak, FSKPM Fac, Kota Samarahan 94300, Sarawak, Malaysia.
EM Abdulrazakalhababi@gmail.com; haza@utm.my; sitimariyams@gmail.com;
   ashrafosman2@gmail.com
CR Abbass HA, 2001, IEEE C EVOL COMPUTAT, P207, DOI 10.1109/CEC.2001.934391
   Alsabti K., 1997, EFFICIENT K MEANS CL, V43, P5
   [Anonymous], INT J ARTIF INTELL
   [Anonymous], P 2 INT WORKSH WEB D
   Belatreche A, 2006, NEW MATH NAT COMPUT, V2, P237, DOI 10.1142/S179300570600049X
   Berkhin P, 2006, GROUPING MULTIDIMENSIONAL DATA: RECENT ADVANCES IN CLUSTERING, P25
   Bock Hans-Hermann, 2007, SELECTED CONTRIBUTIO, P161, DOI [DOI 10.1093/humrep/dew218, DOI 10.1007/978-3-540-73560-1_15]
   Bohte SM, 2005, INFORM PROCESS LETT, V95, P519, DOI 10.1016/j.ipl.2005.05.018
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   Brette R, 2007, J COMPUT NEUROSCI, V23, P349, DOI 10.1007/s10827-007-0038-6
   Das S, 2011, IEEE T EVOLUT COMPUT, V15, P4, DOI 10.1109/TEVC.2010.2059031
   Fayyad U, 1996, AI MAG, V17, P37
   Firouzi B., 2008, WORLD ACAD SCI ENG T, V36, P605
   Godara S., 2016, INDIAN J SCI TECHNOL, V9
   Gong WY, 2011, SOFT COMPUT, V15, P645, DOI 10.1007/s00500-010-0591-1
   Hamed HNA, 2015, LECT NOTES COMPUT SC, V9377, P382, DOI 10.1007/978-3-319-25393-0_42
   Hamed HNA, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P2653, DOI 10.1109/IJCNN.2011.6033565
   Hamed HNA, 2009, LECT NOTES COMPUT SC, V5864, P611, DOI 10.1007/978-3-642-10684-2_68
   He ZY, 2005, LECT NOTES ARTIF INT, V3801, P157
   Huang ZX, 1998, DATA MIN KNOWL DISC, V2, P283, DOI 10.1023/A:1009769707641
   Huang ZX, 1999, IEEE T FUZZY SYST, V7, P446, DOI 10.1109/91.784206
   Ilonen J, 2003, NEURAL PROCESS LETT, V17, P93, DOI 10.1023/A:1022995128597
   Jain AK, 2010, PATTERN RECOGN LETT, V31, P651, DOI 10.1016/j.patrec.2009.09.011
   Kasabov Nikola, 2012, Advances in Computational Intelligence. IEEE World Congress on Computational Intelligence (WCCI 2012). Plenary/Invited Lectures, P234, DOI 10.1007/978-3-642-30687-7_12
   Kasabov N, 2006, 2006 INTERNATIONAL SYMPOSIUM ON EVOLVING FUZZY SYSTEMS, PROCEEDINGS, P8, DOI 10.1109/ISEFS.2006.251185
   Kasabov N, 2014, NEUROCOMPUTING, V134, P269, DOI 10.1016/j.neucom.2013.09.049
   Kasabov N, 2013, NEURAL NETWORKS, V41, P188, DOI 10.1016/j.neunet.2012.11.014
   Kasabov NK, 2014, NEURAL NETWORKS, V52, P62, DOI 10.1016/j.neunet.2014.01.006
   Kotsiantis S., 2004, WSEAS T INF SCI APPL, V1, P73
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   MacQueen J., 1967, P 5 BERK S MATH STAT, V14, P281
   Mandloi M., 2014, SURVEY CLUSTERING AL
   Michlovsky Z., 2009, STRING PATTERN RECOG, P611
   Nazeer KAA, 2010, ELECT ENG COMPUTING, P433
   Noman N, 2008, IEEE T EVOLUT COMPUT, V12, P107, DOI 10.1109/TEVC.2007.895272
   Patel VR, 2011, COMM COM INF SC, V250, P307
   Pham DT, 2005, P I MECH ENG C-J MEC, V219, P103, DOI 10.1243/095440605X8298
   Saleh A.Y., 2014, INT J ADV SOFT COMPU, V6
   Saleh A.Y., 2014, INT C REC TRENDS INF, P13
   Schliebs S, 2013, EVOL SYST-GER, V4, P87, DOI 10.1007/s12530-013-9074-9
   Schliebs S, 2009, LECT NOTES COMPUT SC, V5506, P1229, DOI 10.1007/978-3-642-02490-0_149
   Schliebs S, 2009, NEURAL NETWORKS, V22, P623, DOI 10.1016/j.neunet.2009.06.038
   Singh A., 2013, INT J COMPUT APPL, V67, P13, DOI DOI 10.5120/11430-6785
   Soltic S, 2008, IEEE IJCNN, P2091, DOI 10.1109/IJCNN.2008.4634085
   Storn R, 1997, J GLOBAL OPTIM, V11, P341, DOI 10.1023/A:1008202821328
   Sun Y, 2002, PATTERN RECOGN LETT, V23, P875, DOI 10.1016/S0167-8655(01)00163-5
   Thakare Y, 2015, INT J COMPUT APPL, V110
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   Thorpe S., 1997, CAN HUMAN VISUAL SYS
   Wu J., 2012, ADV K MEANS CLUSTERI, DOI 10.1007/978-3-642-29807-3
   Wu XD, 2008, KNOWL INF SYST, V14, P1, DOI 10.1007/s10115-007-0114-2
   Wysoski SG, 2006, LECT NOTES COMPUT SC, V4179, P1133
   Wysoski SG, 2006, LECT NOTES COMPUT SC, V4131, P61
   Zaki Mohammed J., 2014, DATA MINING ANAL FUN, DOI DOI 10.1017/CBO9780511810114
NR 55
TC 0
Z9 0
U1 0
U2 3
PY 2018
VL 5
BP 571
EP 583
DI 10.1007/978-3-319-59427-9_60
WC Automation & Control Systems; Computer Science, Artificial Intelligence;
   Computer Science, Information Systems; Computer Science, Software
   Engineering; Telecommunications
DA 2023-11-11
ER

PT C
AU Nguyen, DA
   Tran, XT
   Dang, KN
   Iacopi, F
AF Duy-Anh Nguyen
   Xuan-Tu Tran
   Dang, Khanh N.
   Iacopi, Francesca
BE Tran, XT
   Bui, DH
TI A lightweight Max-Pooling method and architecture for Deep Spiking
   Convolutional Neural Networks
SO APCCAS 2020: PROCEEDINGS OF THE 2020 IEEE ASIA PACIFIC CONFERENCE ON
   CIRCUITS AND SYSTEMS (APCCAS 2020)
DT Proceedings Paper
CT 16th IEEE Asia Pacific Conference on Circuits and Systems (IEEE APCCAS)
   / IEEE Asia Pacific Conference on Postgraduate Research in
   Microelectronics and Electronics (PrimeAsia)
CY DEC 08-10, 2020
CL Halong, VIETNAM
DE Deep Convolutional Spiking Neural Networks; ANN-to-SNN conversion;
   Spiking Max Pooling
AB The training of Deep Spiking Neural Networks (DSNNs) is facing many challenges due to the non-differentiable nature of spikes. The conversion of a traditional Deep Neural Networks (DNNs) to its DSNNs counterpart is currently one of the prominent solutions, as it leverages many state-of-the-art pretrained models and training techniques. However, the conversion of max-pooling layer is a non-trivia task. The state-of-the-art conversion methods either replace the max-pooling layer with other pooling mechanisms or use a max-pooling method based on the cumulative number of output spikes. This incurs both memory storage overhead and increases computational complexity, as one inference in DSNNs requires many timesteps, and the number of output spikes after each layer needs to be accumulated. In this paper1, we propose a novel max-pooling mechanism that is not based on the number of output spikes but is based on the membrane potential of the spiking neurons. Simulation results show that our approach still preserves classification accuracies on MNIST and CIFAR10 dataset. Hardware implementation results show that our proposed hardware block is lightweight with an area cost of 15.3kGEs, at a maximum frequency of 300 MHz.
C1 [Duy-Anh Nguyen; Xuan-Tu Tran; Dang, Khanh N.] Vietnam Natl Univ, SISLAB, Univ Engn & Technol, Hanoi, Vietnam.
   [Xuan-Tu Tran] Univ Technol Sydney, Ultimo, Australia.
   [Duy-Anh Nguyen] UTS VNU Joint Technol & Innovat Res Ctr JTIRC, Hanoi, Vietnam.
RP Tran, XT (corresponding author), Vietnam Natl Univ, SISLAB, Univ Engn & Technol, Hanoi, Vietnam.; Tran, XT (corresponding author), Univ Technol Sydney, Ultimo, Australia.
EM tutx@vnu.edu.vn
CR Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Diehl PU, 2015, IEEE IJCNN
   Guo SH, 2020, IEEE EMBED SYST LETT, V12, P21, DOI 10.1109/LES.2019.2919244
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
NR 5
TC 2
Z9 2
U1 0
U2 0
PY 2020
BP 209
EP 212
DI 10.1109/apccas50809.2020.9301703
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Computer Science, Theory & Methods; Engineering,
   Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Wang, H
   Li, YF
   Zhang, Y
AF Wang, Huan
   Li, Yan-Fu
   Zhang, Ying
TI Bioinspired spiking spatiotemporal attention framework for lithium-ion
   batteries state-of-health estimation
SO RENEWABLE & SUSTAINABLE ENERGY REVIEWS
DT Article
DE Lithium-ion batteries; Prognostic and health management; Capacity
   prediction; Spiking neural network
ID NETWORK
AB State-of-health (SOH) estimation of batteries is crucial for ensuring the safety of energy storage systems. Prediction models based on external information (current, voltage, etc.) and artificial neural networks (ANN) are effective solutions. However, external information easily interferes, and the ANN-based model has data dependence, high energy consumption, and insufficient cognitive ability. This motivates us to utilize precise battery physical and chemical degradation information and brain-inspired spiking neural networks (SNNs) for accurate SOH estimation. Therefore, this study proposes a bioinspired spiking spatiotemporal attention neural network (SSA-Net) framework for battery health state monitoring by utilizing full-life-cycle electrochemical impedance spectroscopy (EIS). SSA-Net perfectly models brain neurons' information transmission mechanism and neuron dynamics, thereby endowing it with efficient spatiotemporal feature processing capabilities and low power consumption. Based on the designed spiking residual architecture, SSA-Net constructs a deep spiking information encoding framework achieving high gradient transfer efficiency. More importantly, this study proposes a novel SNN-based spiking spatiotemporal attention module, which realizes the enhancement of useful spiking features and discards worthless information through an adaptive spiking feature selection mechanism. Experimental results show that SSA-Net effectively extracts electrochemical features associated with battery degradation, facilitating precise modeling of the nonlinear relationship between EIS data and SOH and achieving competitive performance.
C1 [Wang, Huan; Li, Yan-Fu; Zhang, Ying] Tsinghua Univ, Dept Ind Engn, Beijing 100084, Peoples R China.
RP Li, YF (corresponding author), Tsinghua Univ, Dept Ind Engn, Beijing 100084, Peoples R China.
EM liyanfu@tsinghua.edu.cn
CR Bian C, 2020, ENERGY, V191, DOI 10.1016/j.energy.2019.116538
   Cheng G, 2021, ENERGY, V232, DOI 10.1016/j.energy.2021.121022
   Christensen PA, 2021, RENEW SUST ENERG REV, V148, DOI 10.1016/j.rser.2021.111240
   Ding P, 2021, RENEW SUST ENERG REV, V148, DOI 10.1016/j.rser.2021.111287
   Fang W., 2021, ADV NEURAL INFORM PR, V34, P21056, DOI DOI 10.48550/ARXIV.2102.04159
   Fang WAC, SPIKINGJELLY
   Fang W, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2641, DOI 10.1109/ICCV48922.2021.00266
   Feng XN, 2019, IEEE T VEH TECHNOL, V68, P8583, DOI 10.1109/TVT.2019.2927120
   Gerstner W., 2002, SPIKING NEURON MODEL, DOI DOI 10.1017/CBO9780511815706
   Han XB, 2015, J POWER SOURCES, V278, P802, DOI 10.1016/j.jpowsour.2014.12.101
   Horowitz M, 2014, ISSCC DIG TECH PAP I, V57, P10, DOI 10.1109/ISSCC.2014.6757323
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/TPAMI.2019.2913372, 10.1109/CVPR.2018.00745]
   Jones PK, 2022, NAT COMMUN, V13, DOI 10.1038/s41467-022-32422-w
   Khumprom P, 2019, ENERGIES, V12, DOI 10.3390/en12040660
   Li PH, 2022, RENEW SUST ENERG REV, V156, DOI 10.1016/j.rser.2021.111843
   Li PH, 2020, J POWER SOURCES, V459, DOI 10.1016/j.jpowsour.2020.228069
   Li Y, 2019, RENEW SUST ENERG REV, V113, DOI 10.1016/j.rser.2019.109254
   Lu JH, 2023, NAT COMMUN, V14, DOI 10.1038/s41467-023-38458-w
   Lu JH, 2022, ENERGY STORAGE MATER, V50, P139, DOI 10.1016/j.ensm.2022.05.007
   Ma J, 2015, SCI CHINA TECHNOL SC, V58, P2038, DOI 10.1007/s11431-015-5961-6
   Meng HX, 2023, RELIAB ENG SYST SAFE, V236, DOI 10.1016/j.ress.2023.109288
   Meng HX, 2019, RENEW SUST ENERG REV, V116, DOI 10.1016/j.rser.2019.109405
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Nejad S, 2016, J POWER SOURCES, V316, P183, DOI 10.1016/j.jpowsour.2016.03.042
   Pan HH, 2018, ENERGY, V160, P466, DOI 10.1016/j.energy.2018.06.220
   Pei J, 2019, NATURE, V572, P106, DOI 10.1038/s41586-019-1424-8
   Rauf H, 2022, RENEW SUST ENERG REV, V156, DOI 10.1016/j.rser.2021.111903
   Shen S, 2019, J ENERGY STORAGE, V25, DOI 10.1016/j.est.2019.100817
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Tian JP, 2022, ENERGY STORAGE MATER, V51, P372, DOI 10.1016/j.ensm.2022.06.053
   Tian JP, 2022, ENERGY STORAGE MATER, V50, P718, DOI 10.1016/j.ensm.2022.06.007
   Tian JP, 2020, IEEE T POWER ELECTR, V35, P10363, DOI 10.1109/TPEL.2020.2978493
   Ungurean L, 2020, INT J ENERG RES, V44, P6767, DOI 10.1002/er.5413
   Wang H, ARXIV
   Wang H, 2023, RELIAB ENG SYST SAFE, V233, DOI 10.1016/j.ress.2023.109102
   Wang H, 2022, KNOWL-BASED SYST, V239, DOI 10.1016/j.knosys.2021.107978
   Wang H, 2022, IEEE T NEUR NET LEAR, V33, P4757, DOI 10.1109/TNNLS.2021.3060494
   Yao JC, 2023, ENERGY, V271, DOI 10.1016/j.energy.2023.127033
   Zhang CL, 2022, FRONT ENERGY RES, V10, DOI 10.3389/fenrg.2022.1013800
   Zhang CL, 2022, IEEE T VEH TECHNOL, V71, P2601, DOI 10.1109/TVT.2021.3138959
   Zhang Y, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-16779-4
   Zhang Y, 2022, RENEW SUST ENERG REV, V161, DOI 10.1016/j.rser.2022.112282
   Zhao SS, 2022, J ENERGY STORAGE, V52, DOI 10.1016/j.est.2022.104901
   Zhou ZK, 2020, J CLEAN PROD, V267, DOI 10.1016/j.jclepro.2020.121882
   Zraibi B, 2021, IEEE T VEH TECHNOL, V70, P4252, DOI 10.1109/TVT.2021.3071622
NR 45
TC 0
Z9 0
U1 0
U2 0
PD DEC
PY 2023
VL 188
AR 113728
DI 10.1016/j.rser.2023.113728
WC Green & Sustainable Science & Technology; Energy & Fuels
DA 2023-11-11
ER

PT C
AU Alnajjar, F
   Murase, K
AF Alnajjar, F
   Murase, K
BE Murase, K
   Sekiyama, K
   Kubota, N
   Naniwa, T
   Sitte, J
TI Self-organization of spiking neural network generating autonomous
   behavior in a miniature mobile robot
SO PROCEEDINGS OF THE 3RD INTERNATIONAL SYMPOSIUM ON AUTONOMOUS MINIROBOTS
   FOR RESEARCH AND EDUTAINMENT (AMIRE 2005)
DT Proceedings Paper
CT 3rd International Symposium on Autonomous Minirobots for Research and
   Edutainment
CY SEP 20-22, 2005
CL Fukui, JAPAN
AB Purpose of this study is to develop self-organization algorithm of spiking neural network applicable to autonomous robots. We first formulated a spiking neural network model whose inputs and outputs were analog. We then implemented it into a miniature mobile robot Khepera. In order to see whether or not a solution(s) for the given task exists with the spiking neural network, the robot was evolved with the genetic algorithm (GA) in an environment. The robot acquired the obstacle-avoidance and navigation task successfully, exhibiting the presence of the solution. Then, a self-organization algorithm based on the use-dependent synaptic potentiation and depotentiation was formulated and implemented into the robot. In the environment, the robot gradually organized the network and the obstacle avoidance behavior was formed. The time needed for the training was much less than with genetic evolution, approximately one fifth (1/5).
C1 Univ Fukui, Dept Human & Artificial Intelligence Syst, Fukui 9108507, Japan.
RP Alnajjar, F (corresponding author), Univ Fukui, Dept Human & Artificial Intelligence Syst, Fukui 9108507, Japan.
EM fady@synapse.his.fukui-u.ac.jp
CR Floreano D., 2001, LNCS, P38
   Gerstner W., 2002, SPIKING NEURON MODEL
   Islam M, 2002, IEICE T INF SYST, VE85D, P1118
   MAASS W, 1996, AUSTR C NEUR NETW
   Ruf B, 1998, IEEE T NEURAL NETWOR, V9, P575, DOI 10.1109/72.668899
   Sala D. M., 1998, Australian Journal of Intelligent Information Processing Systems, V5, P161
NR 6
TC 1
Z9 1
U1 0
U2 0
PY 2006
BP 255
EP +
DI 10.1007/3-540-29344-2_38
WC Robotics
DA 2023-11-11
ER

PT J
AU Bing, ZS
   Baumann, I
   Jiang, ZY
   Huang, K
   Cai, CX
   Knoll, A
AF Bing, Zhenshan
   Baumann, Ivan
   Jiang, Zhuangyi
   Huang, Kai
   Cai, Caixia
   Knoll, Alois
TI Supervised Learning in SNN via Reward-Modulated Spike-Timing-Dependent
   Plasticity for a Target Reaching Vehicle
SO FRONTIERS IN NEUROROBOTICS
DT Article
DE spiking neural network; R-STDP; supervised learning; end-to-end control;
   autonomous locomotion
ID NEURAL-NETWORK; MOBILE ROBOT; NAVIGATION; MODEL; STDP
AB Spiking neural networks (SNNs) offer many advantages over traditional artificial neural networks (ANNs) such as biological plausibility, fast information processing, and energy efficiency. Although SNNs have been used to solve a variety of control tasks using the Spike-Timing-Dependent Plasticity (STDP) learning rule, existing solutions usually involve hard-coded network architectures solving specific tasks rather than solving different kinds of tasks generally. This results in neglecting one of the biggest advantages of ANNs, i.e., being general-purpose and easy-to-use due to their simple network architecture, which usually consists of an input layer, one or multiple hidden layers and an output layer. This paper addresses the problem by introducing an end-to-end learning approach of spiking neural networks constructed with one hidden layer and reward-modulated Spike-Timing-Dependent Plasticity (R-STDP) synapses in an all-to-all fashion. We use the supervised reward-modulated Spike-Timing-Dependent-Plasticity learning rule to train two different SNN-based sub-controllers to replicate a desired obstacle avoiding and goal approaching behavior, provided by pre-generated datasets. Together they make up a target-reaching controller, which is used to control a simulated mobile robot to reach a target area while avoiding obstacles in its path. We demonstrate the performance and effectiveness of our trained SNNs to achieve target reaching tasks in different unknown scenarios.
C1 [Bing, Zhenshan; Baumann, Ivan; Jiang, Zhuangyi; Cai, Caixia; Knoll, Alois] Tech Univ Munich, Chair Robot Artificial Intelligence & Embedded Sy, Dept Informat, Munich, Germany.
   [Huang, Kai] Sun Yat Sen Univ, Dept Data & Comp Sci, Guangzhou, Guangdong, Peoples R China.
   [Huang, Kai] Peng Cheng Lab, Shenzhen, Peoples R China.
RP Cai, CX (corresponding author), Tech Univ Munich, Chair Robot Artificial Intelligence & Embedded Sy, Dept Informat, Munich, Germany.
EM ccxtum@gmail.com
CR Alnajjar F, 2008, IEEE IJCNN, P2207, DOI 10.1109/IJCNN.2008.4634103
   Ambrosano A., 2016, RETINA COLOR OPPONEN
   [Anonymous], 2014, SURVEY PUBLIC OPINIO, DOI DOI 10.1109/ICCVE.2014.45
   [Anonymous], 2012, ADV ARTIF NEURAL SYS, DOI DOI 10.1155/2012/713581
   [Anonymous], 2016, INT C EV BAS CONTR C
   [Anonymous], 1992, P THEOR PRAX FUZZ LO
   [Anonymous], 2000, THE BRAIN EXPLAINED
   Beyeler M, 2015, NEURAL NETWORKS, V72, P75, DOI 10.1016/j.neunet.2015.09.005
   Bicho E, 1998, IEEE IND ELEC, P1176, DOI 10.1109/IECON.1998.724266
   Bing ZS, 2018, FRONT NEUROROBOTICS, V12, DOI 10.3389/fnbot.2018.00035
   Bingsheng Zhang, 2018, 2018 IEEE International Conference on Internet of Things (iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom) and IEEE Smart Data (SmartData). Proceedings
   Blum H., 2017, ROB SCI SYST 2017 C
   Bouganis A., 2010, P 2010 INT JOINT C N, P1, DOI DOI 10.1109/IJCNN.2010.5596525
   BROOKS RA, 1986, IEEE T ROBOTIC AUTOM, V2, P14, DOI 10.1109/JRA.1986.1087032
   Carrillo RR, 2008, BIOSYSTEMS, V94, P18, DOI 10.1016/j.biosystems.2008.05.008
   Cassidy AS, 2014, INT CONF HIGH PERFOR, P27, DOI 10.1109/SC.2014.8
   Clawson TS, 2016, IEEE DECIS CONTR P, P3381, DOI 10.1109/CDC.2016.7798778
   Cyr A, 2012, ADAPT BEHAV, V20, P257, DOI 10.1177/1059712312442231
   DeSouza GN, 2002, IEEE T PATTERN ANAL, V24, P237, DOI 10.1109/34.982903
   Echeveste R, 2015, NEURAL COMPUT, V27, P672, DOI 10.1162/NECO_a_00707
   Esser S.K., 2015, ADV NEURAL INFORM PR, P1117
   Foderaro G, 2010, IEEE DECIS CONTR P, P911, DOI 10.1109/CDC.2010.5717260
   Frémaux N, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003024
   Friudenberg P, 2018, IEEE ACCESS, V6, P16948, DOI 10.1109/ACCESS.2018.2802468
   Gerstner W., 2002, SPIKING NEURON MODEL
   Helgadóttir LI, 2013, I IEEE EMBS C NEUR E, P891, DOI 10.1109/NER.2013.6696078
   Huang WH, 2006, ROBOT AUTON SYST, V54, P288, DOI 10.1016/j.robot.2005.11.004
   Indiveri GC, 1999, IEEE T CIRCUITS-II, V46, P1337, DOI 10.1109/82.803473
   Izhikevich EM, 2007, CEREB CORTEX, V17, P2443, DOI 10.1093/cercor/bhl152
   Kaiser J, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON SIMULATION, MODELING, AND PROGRAMMING FOR AUTONOMOUS ROBOTS (SIMPAR), P127, DOI 10.1109/SIMPAR.2016.7862386
   Kruse T, 2013, ROBOT AUTON SYST, V61, P1726, DOI 10.1016/j.robot.2013.05.007
   Lewis M. A., 2000, Proceedings 2000 ICRA. Millennium Conference. IEEE International Conference on Robotics and Automation. Symposia Proceedings (Cat. No.00CH37065), P494, DOI 10.1109/ROBOT.2000.844103
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Mazumder P, 2016, INTEGRATION, V54, P109, DOI 10.1016/j.vlsi.2016.01.002
   Milde MB, 2017, FRONT NEUROROBOTICS, V11, DOI 10.3389/fnbot.2017.00028
   Neftci EO, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00324
   Nichols E, 2013, IEEE T CYBERNETICS, V43, P115, DOI 10.1109/TSMCB.2012.2200674
   Ponulak F, 2011, ACTA NEUROBIOL EXP, V71, P409
   Rohmer E, 2013, IEEE INT C INT ROBOT, P1321, DOI 10.1109/IROS.2013.6696520
   Rothman JS, 2014, PROG MOL BIOL TRANSL, V123, P305, DOI 10.1016/B978-0-12-397897-4.00004-8
   Ru D, 2014, 2014 IEEE 14TH INTERNATIONAL CONFERENCE ON NANOTECHNOLOGY (IEEE-NANO), P873, DOI 10.1109/NANO.2014.6968000
   Shim MS, 2017, IEEE IJCNN, P3098, DOI 10.1109/IJCNN.2017.7966242
   Spuler M., 2015, P INT JOINT C NEURAL, V2015, DOI [10.1109/IJCNN.2015.7280521, DOI 10.1109/IJCNN.2015.7280521]
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   Vasilaki E, 2009, PLOS COMPUT BIOL, V5, DOI 10.1371/journal.pcbi.1000586
   Wang XQ, 2008, NEUROCOMPUTING, V71, P655, DOI 10.1016/j.neucom.2007.08.025
   Wang XQ, 2014, NEUROCOMPUTING, V134, P230, DOI 10.1016/j.neucom.2013.07.055
   Wang XQ, 2009, 2009 INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND COMPUTATIONAL INTELLIGENCE, VOL I, PROCEEDINGS, P194, DOI 10.1109/AICI.2009.448
   Wysoski SG, 2010, NEURAL NETWORKS, V23, P819, DOI 10.1016/j.neunet.2010.04.009
   Zhang X, 2013, IEEE DECIS CONTR P, P6798, DOI 10.1109/CDC.2013.6760966
   2017, IEEE IJCNN, P2243, DOI DOI 10.1109/IJCNN.2017.7966127
NR 51
TC 20
Z9 20
U1 2
U2 13
PD MAY 3
PY 2019
VL 13
AR 18
DI 10.3389/fnbot.2019.00018
WC Computer Science, Artificial Intelligence; Robotics; Neurosciences
DA 2023-11-11
ER

PT J
AU Yang, B
   Qin, L
   Peng, H
   Guo, CG
   Luo, XH
   Wang, J
AF Yang, Bo
   Qin, Lang
   Peng, Hong
   Guo, Chenggang
   Luo, Xiaohui
   Wang, Jun
TI SDDC-Net: A U-shaped deep spiking neural P convolutional network for
   retinal vessel segmentation
SO DIGITAL SIGNAL PROCESSING
DT Article
DE Deep learning; Retinal vessels; Spiking neural P system; Convolutional
   neural network; Dilated convolution
ID BLOOD-VESSELS; MATCHED-FILTER; MODEL; SYSTEMS
AB As an essential step in the early diagnosis of retinopathy, the blood vessels morphological attributes assist specialists to obtain pathological information efficiently. Most existing deep learning methods are based on U-shaped convolutional neural networks for the segmentation of blood vessels and have made substantial progress. However, the variance between vessel images remains challenging for segmentation algorithms, as demonstrated by poor cross-validation performance between different datasets. In this paper, a novel U-shaped deep convolutional network is proposed for retinal vessel segmentation, namely spiking neural P-type Dual-channel dilated convolutional network (SDDC-Net). We redesign the classical U-shaped convolution network based on the spiking neural P system computational mechanism for the first time. Distinct from the conventional convolutional neural network, SDDC-Net integrates the spiking neural P system convolutional neurons into the classic encoder-decoder architecture. We employ dilated convolution into an encoder, which improves both capabilities of perceiving more contexts and perceptual sensitivity of thin blood. We evaluate this model on three public datasets (DRIVE, STARE, CHASE_DB1), which indicates the more sensitive detection of thin vessels compared to most existing methods, showing higher sensitivity and F1 metrics. Compared to the baseline U-Net, our sensitivity, accuracy, and F1 score metrics on DRIVE dataset surpass by 10.66%, 1.73%, and 1.47% respectively. We also evaluate the effectiveness of spiking neural P system and dilated convolution in the ablation experiments, which demonstrates that accuracy increases with few drops in specificity. The cross-validation experiments show that our model has not only effective segmentation ability but also excellent generalization ability.(c) 2023 Elsevier Inc. All rights reserved.
C1 [Yang, Bo; Qin, Lang; Peng, Hong; Guo, Chenggang; Luo, Xiaohui] Xihua Univ, Sch Comp & Software Engn, Chengdu 610039, Peoples R China.
   [Wang, Jun] Xihua Univ, Sch Elect Engn & Elect Informat, Chengdu 610039, Peoples R China.
RP Guo, CG (corresponding author), Xihua Univ, Sch Comp & Software Engn, Chengdu 610039, Peoples R China.
EM chenggang.guo90@hotmail.com
CR Al-Diri B, 2009, IEEE T MED IMAGING, V28, P1488, DOI 10.1109/TMI.2009.2017941
   Al-Rawi M, 2007, COMPUT BIOL MED, V37, P262, DOI 10.1016/j.compbiomed.2006.03.003
   Azzopardi G, 2015, MED IMAGE ANAL, V19, P46, DOI 10.1016/j.media.2014.08.002
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Cai SJ, 2020, QUANT IMAG MED SURG, V10, P1275, DOI 10.21037/qims-19-1090
   Cai YL, 2022, INFORM SCIENCES, V587, P473, DOI 10.1016/j.ins.2021.12.058
   Campochiaro PA, 2021, PROG RETIN EYE RES, V83, DOI 10.1016/j.preteyeres.2020.100921
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Deng XY, 2022, BIOMED SIGNAL PROCES, V73, DOI 10.1016/j.bspc.2021.103467
   Donahue, 2014, PROC CVPR IEEE, P580, DOI [10.1109/CVPR.2014.8, DOI 10.1109/CVPR.2014.81]
   Fraz MM, 2012, COMPUT METH PROG BIO, V108, P600, DOI 10.1016/j.cmpb.2011.08.009
   Gegundez-Arias ME, 2021, COMPUT METH PROG BIO, V205, DOI 10.1016/j.cmpb.2021.106081
   Orlando JI, 2017, IEEE T BIO-MED ENG, V64, P16, DOI 10.1109/TBME.2016.2535311
   Ionescu M, 2006, FUND INFORM, V71, P279
   Jin QG, 2019, KNOWL-BASED SYST, V178, P149, DOI 10.1016/j.knosys.2019.04.025
   Khan TM, 2022, BIOMED SIGNAL PROCES, V71, DOI 10.1016/j.bspc.2021.103169
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li B, 2021, SIGNAL PROCESS, V178, DOI 10.1016/j.sigpro.2020.107793
   Li B, 2021, INT J NEURAL SYST, V31, DOI 10.1142/S0129065720500501
   Li B, 2020, KNOWL-BASED SYST, V196, DOI 10.1016/j.knosys.2020.105794
   Li LZ, 2020, IEEE WINT CONF APPL, P3645, DOI 10.1109/WACV45572.2020.9093621
   Li QL, 2016, IEEE T MED IMAGING, V35, P109, DOI 10.1109/TMI.2015.2457891
   Liskowski P, 2016, IEEE T MED IMAGING, V35, P2369, DOI 10.1109/TMI.2016.2546227
   Liu Q, 2023, IEEE T NEUR NET LEAR, V34, P6227, DOI 10.1109/TNNLS.2021.3134792
   Liu Q, 2022, KNOWL-BASED SYST, V235, DOI 10.1016/j.knosys.2021.107656
   Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965
   Long LF, 2022, KNOWL-BASED SYST, V253, DOI 10.1016/j.knosys.2022.109568
   Long LF, 2022, NEURAL NETWORKS, V152, P300, DOI 10.1016/j.neunet.2022.04.030
   Long LF, 2022, INT J NEURAL SYST, V32, DOI 10.1142/S0129065722500204
   Martinez-Perez ME, 2007, MED IMAGE ANAL, V11, P47, DOI 10.1016/j.media.2006.11.004
   Pan LQ, 2009, INT J COMPUT COMMUN, V4, P273, DOI 10.15837/ijccc.2009.3.2435
   Paum G, 2007, J UNIVERS COMPUT SCI, V13, P1707
   Paun Gh, 2010, OXFORD HDB MEMBRANE
   Peng H, 2021, COMPUT VIS IMAGE UND, V210, DOI 10.1016/j.cviu.2021.103228
   Peng H, 2020, NEURAL NETWORKS, V127, P110, DOI 10.1016/j.neunet.2020.04.014
   Peng H, 2020, INT J NEURAL SYST, V30, DOI 10.1142/S0129065720500082
   Peng H, 2020, KNOWL-BASED SYST, V188, DOI 10.1016/j.knosys.2019.105064
   Peng H, 2019, KNOWL-BASED SYST, V163, P875, DOI 10.1016/j.knosys.2018.10.016
   Peng H, 2019, IEEE T NEUR NET LEAR, V30, P1672, DOI 10.1109/TNNLS.2018.2872999
   Peng H, 2017, NEURAL NETWORKS, V95, P66, DOI 10.1016/j.neunet.2017.08.003
   Rangayyan RM, 2007, CAN CON EL COMP EN, P717
   Ronneberger O., 2015, P MED IM COMP COMP A, P234, DOI DOI 10.1007/978-3-319-24574-4_28
   Roychowdhury S, 2015, IEEE T BIO-MED ENG, V62, P1738, DOI 10.1109/TBME.2015.2403295
   Salazar-Gonzalez A, 2014, IEEE J BIOMED HEALTH, V18, P1874, DOI 10.1109/JBHI.2014.2302749
   Soares JVB, 2006, IEEE T MED IMAGING, V25, P1214, DOI 10.1109/TMI.2006.879967
   Staal J, 2004, IEEE T MED IMAGING, V23, P501, DOI 10.1109/TMI.2004.825627
   Ting DSW, 2019, PROG RETIN EYE RES, V72, DOI 10.1016/j.preteyeres.2019.04.003
   Wang DY, 2020, IEEE J BIOMED HEALTH, V24, P3384, DOI 10.1109/JBHI.2020.3002985
   Wang PQ, 2018, IEEE WINT CONF APPL, P1451, DOI 10.1109/WACV.2018.00163
   Wang X, 2022, NEUROCOMPUTING, V486, P135, DOI 10.1016/j.neucom.2021.11.017
   Wu YC, 2019, LECT NOTES COMPUT SC, V11764, P264, DOI 10.1007/978-3-030-32239-7_30
   Xian RH, 2023, INT J NEURAL SYST, V33, DOI 10.1142/S0129065722500605
   Yan ZQ, 2018, IEEE T BIO-MED ENG, V65, P1912, DOI 10.1109/TBME.2018.2828137
   You XG, 2011, PATTERN RECOGN, V44, P2314, DOI 10.1016/j.patcog.2011.01.007
   Alom MZ, 2018, Arxiv, DOI [arXiv:1802.06955, DOI 10.48550/ARXIV.1802.06955]
   Zeng ZT, 2019, IEEE ACCESS, V7, P21420, DOI 10.1109/ACCESS.2019.2896920
   Zhang B, 2010, COMPUT BIOL MED, V40, P438, DOI 10.1016/j.compbiomed.2010.02.008
   Zhang J, 2014, COMPUT MED IMAG GRAP, V38, P517, DOI 10.1016/j.compmedimag.2014.05.010
   Zhang Y, 2022, BIOMED SIGNAL PROCES, V73, DOI 10.1016/j.bspc.2021.103472
   Zhang Y, 2022, EXPERT SYST APPL, V195, DOI 10.1016/j.eswa.2022.116526
   Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660
   Zhao SW, 2022, J MEMBRANE COMPUT, V4, P87, DOI 10.1007/s41965-022-00094-6
   Zhao YT, 2015, IEEE T MED IMAGING, V34, P1797, DOI 10.1109/TMI.2015.2409024
NR 64
TC 4
Z9 4
U1 16
U2 17
PD MAY
PY 2023
VL 136
AR 104002
DI 10.1016/j.dsp.2023.104002
EA MAR 2023
WC Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Yang, SM
   Wang, J
   Deng, B
   Li, HY
   Che, YQ
AF Yang, Shuangming
   Wang, Jiang
   Deng, Bin
   Li, Huiyan
   Che, Yanqiu
GP IEEE
TI Digital Implementation of the Retinal Spiking Neural Network under Light
   Stimulation
SO 2019 9TH INTERNATIONAL IEEE/EMBS CONFERENCE ON NEURAL ENGINEERING (NER)
SE International IEEE EMBS Conference on Neural Engineering
DT Proceedings Paper
CT 9th IEEE/EMBS International Conference on Neural Engineering (NER)
CY MAR 20-23, 2019
CL San Francisco, CA
ID EFFICIENT FPGA IMPLEMENTATION; AMACRINE CELLS; GANGLION; ARCHITECTURE
AB The visual system is one of the most important pathways of obtaining information for human being and other animals. The retina is responsible for initial processing of visual information and transmitting signals to the second processing system by using the spiking activity patterns. This paper implements a retinal spiking neural network based on field-programmable gate array (FPGA), and uses different scopes of light stimulation to stimulate the digital retinal network and induce different spiking activities. The retina neural network contains 96 neurons, which uses Hodgkin-Huxley type neuron model to build neural network using three-layer feedforward neural network structure. The neural network is implemented using Cyclone IV EP4CE115 FPGA, and uses OV7620 camera to obtain external signals. The state machine control the input information of the retina system, and the firing patterns are finally displayed on oscilloscope device. Experimental results show that the proposed digital retinal network can generate the dual-peak response of the retinal ganglion cells. This work is meaningful for the design of the retina prostheses and is helpful for the investigation of the underlying mechanisms of the retinal activities.
C1 [Li, Huiyan] Tianjin Univ Technol & Educ, Sch Automat & Elect Engn, Tianjin 300222, Peoples R China.
   [Yang, Shuangming; Wang, Jiang; Deng, Bin] Tianjin Univ, Sch Elect & Informat Engn, Tianjin, Peoples R China.
   [Che, Yanqiu] Penn State Univ, Coll Med, Dept Neurosurg, Hershey, PA 17033 USA.
   [Che, Yanqiu] Penn State Univ, Ctr Neural Engn, Hershey, PA 17033 USA.
RP Li, HY (corresponding author), Tianjin Univ Technol & Educ, Sch Automat & Elect Engn, Tianjin 300222, Peoples R China.
EM yangshuangming@tju.edu.cn; jiangwang@tju.edu.cn; dengbin@tju.edu.cn;
   lhy2740@126.com; yche@pennstatehealth.psu.edu
CR Breuninger T, 2011, J NEUROSCI, V31, P6504, DOI 10.1523/JNEUROSCI.0616-11.2011
   Chan YC, 2008, VISION RES, V48, P2466, DOI 10.1016/j.visres.2008.08.010
   Choi TYW, 2005, IEEE T CIRCUITS-I, V52, P1049, DOI 10.1109/TCSI.2005.849136
   DAWSON WW, 1973, SCIENCE, V181, P747, DOI 10.1126/science.181.4101.747
   EHINGER B, 1969, Z ZELLFORSCH MIK ANA, V97, P285, DOI 10.1007/BF00344763
   He QH, 2011, EUR J NEUROSCI, V33, P36, DOI 10.1111/j.1460-9568.2010.07484.x
   Helmstaedter M, 2013, NATURE, V500, P168, DOI 10.1038/nature12346
   MacNeil MA, 1998, NEURON, V20, P971, DOI 10.1016/S0896-6273(00)80478-X
   Mustafi D, 2009, PROG RETIN EYE RES, V28, P289, DOI 10.1016/j.preteyeres.2009.05.003
   Pang JJ, 2007, VISION RES, V47, P384, DOI 10.1016/j.visres.2006.09.021
   Puller C, 2011, J COMP NEUROL, V519, P759, DOI 10.1002/cne.22546
   Roehlecke C, 2011, MOL VIS, V17, P876
   Snellman J, 2008, PROG RETIN EYE RES, V27, P450, DOI 10.1016/j.preteyeres.2008.03.003
   STERLING P, 1988, J NEUROSCI, V8, P623
   Volder J. E., 1959, IRE T ELECT COMPUT, VEC-8, P330, DOI [10.1109/TEC.1959.5222693, DOI 10.1109/TEC.1959.5222693]
   Wang L, 2011, NEURAL REGEN RES, V6, P1254, DOI 10.3969/j.issn.1673-5374.2011.16.010
   WASSLE H, 1991, PHYSIOL REV, V71, P447, DOI 10.1152/physrev.1991.71.2.447
   Yang S., 2018, IEEE T CYBERNETICS
   Yang S., SCI REPORTS, V7, P40152
   Yang SM, 2018, NEUROCOMPUTING, V314, P394, DOI 10.1016/j.neucom.2018.07.006
   Yang SM, 2018, NEUROCOMPUTING, V282, P262, DOI 10.1016/j.neucom.2017.12.031
   Yang SM, 2018, PHYSICA A, V494, P484, DOI 10.1016/j.physa.2017.11.155
   Yang SM, 2017, NEURAL NETWORKS, V94, P220, DOI 10.1016/j.neunet.2017.07.012
   Yang SM, 2016, NEUROCOMPUTING, V177, P274, DOI 10.1016/j.neucom.2015.11.026
   Yang SM, 2015, NEURAL NETWORKS, V71, P62, DOI 10.1016/j.neunet.2015.07.017
   Zamanlooy B, 2014, IEEE T VLSI SYST, V22, P39, DOI 10.1109/TVLSI.2012.2232321
NR 26
TC 4
Z9 4
U1 0
U2 1
PY 2019
BP 542
EP 545
DI 10.1109/ner.2019.8716932
WC Engineering, Biomedical; Neurosciences
DA 2023-11-11
ER

PT C
AU Thiruvarudchelvan, V
   Bossomaier, T
AF Thiruvarudchelvan, Vaenthan
   Bossomaier, Terry
GP IEEE
TI Towards Realtime Stance Classification by Spiking Neural Network
SO 2012 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUN 10-15, 2012
CL Brisbane, AUSTRALIA
ID EVENT-DRIVEN SIMULATION; NEURONS
AB Spiking neural networks are a popular area of current research in both artificial intelligence and neuroscience. Unlike second generation networks like the multilayer perceptron (MLP), they simulate rather than emulate neuronal interactions. Spiking networks have been shown to be theoretically more powerful than earlier generation networks, and have repeatedly been suggested as ideal for realtime problems due to their time-basis. Because of their sparse nature, real neural networks are also extremely power-efficient, a pressing concern in computing today. This raises the possibility of applying sparse spiking networks for power-saving. To investigate these ideas, we wish to apply a spiking network to realtime data classification. As a first step, we use a feedforward network with the SpikeProp algorithm to classify offline skeleton data derived from a depth camera. Classifier networks were successfully trained, but we found SpikeProp considerably more complex to apply than backpropagation. There is considerable potential for optimization and power efficiency, and we hope to compare the performance of our system with more established learning techniques in a realtime setting.
C1 [Thiruvarudchelvan, Vaenthan; Bossomaier, Terry] Charles Sturt Univ, Ctr Res Complex Syst, Bathurst, NSW 2795, Australia.
RP Thiruvarudchelvan, V (corresponding author), Charles Sturt Univ, Ctr Res Complex Syst, Bathurst, NSW 2795, Australia.
EM vthiru@csu.edu.au; tbossomaier@csu.edu.au
CR Albers S, 2010, COMMUN ACM, V53, P86, DOI 10.1145/1735223.1735245
   Ananthanarayanan R., 2009, SC 09 P C HIGH PERF
   [Anonymous], 1986, PARALLEL DISTRIBUTED, DOI DOI 10.7551/MITPRESS/5236.001.0001
   [Anonymous], 2011, IISU DEV GUID VER 2
   [Anonymous], ADV NEURAL INFORM PR
   [Anonymous], 2010 INT JOINT C NEU
   BIALEK W, 1991, SCIENCE, V252, P1854, DOI 10.1126/science.2063199
   Boahen K, 2005, SCI AM, V292, P56, DOI 10.1038/scientificamerican0505-56
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Carroll J., 2011, PERFORMANCE IN PRESS
   Carter J, 2010, COMPUTER, V43, P76, DOI 10.1109/MC.2010.198
   Delorme A, 2003, NETWORK-COMP NEURAL, V14, P613, DOI 10.1088/0954-898X/14/4/301
   Donofrio D, 2009, COMPUTER, V42, P62, DOI 10.1109/MC.2009.353
   Fidjeland Andreas K, 2010, 2010 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2010.5596678
   Gerstner W., 2002, SPIKING NEURON MODEL
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Jin X, 2010, COMPUT SCI ENG, V12, P91, DOI 10.1109/MCSE.2010.112
   Kim NS, 2003, COMPUTER, V36, P68, DOI 10.1109/MC.2003.1250885
   Laughlin SB, 2003, SCIENCE, V301, P1870, DOI 10.1126/science.1089662
   Laughlin SB, 1998, NAT NEUROSCI, V1, P36, DOI 10.1038/236
   Lobb CJ, 2005, Workshop on Principles of Advanced and Distributed Simulation, Proceedings, P16, DOI 10.1109/PADS.2005.18
   Maass W, 1999, INFORM COMPUT, V153, P26, DOI 10.1006/inco.1999.2806
   Mattia M, 2000, NEURAL COMPUT, V12, P2305, DOI 10.1162/089976600300014953
   McKennoch S, 2006, IEEE IJCNN, P3970
   MOORE S, 2002, THESIS U BATH
   Mouraud A., 2006, P PDCN06 PAR DISTR C
   Ros E, 2006, NEURAL COMPUT, V18, P2959, DOI 10.1162/neco.2006.18.12.2959
   Schrauwen B., 2004, NEUR NETW 2004 P 200, V1, p[xlvii, 3302]
   Swanson S, 2011, IEEE COMMUN MAG, V49, P112, DOI 10.1109/MCOM.2011.5741155
   Takase Haruhiko, 2009, Proceedings 2009 International Joint Conference on Neural Networks (IJCNN 2009 - Atlanta), P3062, DOI 10.1109/IJCNN.2009.5178756
   Wakamatsu T, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P951, DOI 10.1109/IJCNN.2011.6033325
   Werbos P., 1974, REGRESSION NEW TOOLS
NR 33
TC 0
Z9 0
U1 0
U2 1
PY 2012
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT J
AU Song, T
   Pan, LQ
   Wu, TF
   Zheng, P
   Wong, MLD
   Rodriguez-Paton, A
AF Song, Tao
   Pan, Linqiang
   Wu, Tingfang
   Zheng, Pan
   Wong, M. L. Dennis
   Rodriguez-Paton, Alfonso
TI Spiking Neural P Systems With Learning Functions
SO IEEE TRANSACTIONS ON NANOBIOSCIENCE
DT Article
DE Bio-inspired computing; membrane computing; spiking neural P system;
   learning; letter classification
ID SYNAPSES WORKING; NETWORKS; RECOGNITION; NEURONS; RULES; MODEL;
   CLASSIFICATION
AB Spiking neural P systems (SN P systems) are a class of distributed and parallel neural-like computing models, inspired from the way neurons communicate by means of spikes. In this paper, a new variant of the systems, called SN P systems with learning functions, is introduced. Such systems can dynamically strengthen and weaken connections among neurons during the computation. A class of specific SN P systems with simple Hebbian learning function is constructed to recognize English letters. The experimental results show that the SN P systems achieve average accuracy rate 98.76% in the test case without noise. In the test cases with low, medium, and high noises, the SN P systems outperform back propagation neural networks and probabilistic neural networks. Moreover, comparing with spiking neural networks, SN P systems perform a little better in recognizing letters with noise. The result of this paper is promising in terms of the fact that it is the first attempt to use SN P systems in pattern recognition after many theoretical advancements of SN P systems, and SN P systems exhibit the feasibility for tackling pattern recognition problems.
C1 [Song, Tao; Pan, Linqiang; Wu, Tingfang] Huazhong Univ Sci & Technol, Sch Automat, Educ Minist China, Key Lab Image Proc & Intelligent Control, Wuhan 430074, Hubei, Peoples R China.
   [Song, Tao] China Univ Petr, Coll Comp & Commun Engn, Qingdao 266580, Shandong, Peoples R China.
   [Zheng, Pan] Univ Canterbury, Dept Accounting & Informat Syst, Christchurch 8041, New Zealand.
   [Wong, M. L. Dennis] Heriot Watt Univ Malaysia, Sch Engn & Phys Sci, Kuala Lumpur 62200, Malaysia.
   [Rodriguez-Paton, Alfonso] Univ Politecn Madrid, Dept Inteligencia Artificial, Campus Montegancedo, E-28660 Madrid, Spain.
RP Pan, LQ (corresponding author), Huazhong Univ Sci & Technol, Sch Automat, Educ Minist China, Key Lab Image Proc & Intelligent Control, Wuhan 430074, Hubei, Peoples R China.
EM lqpan@mail.hust.edu.cn
CR [Anonymous], INT J FOUND COMPUT S
   [Anonymous], BIOSYSTEMS
   [Anonymous], NEUROCOMPUTING
   [Anonymous], P IEEE INT C MACH VI
   [Anonymous], FUNDAM INFORM
   [Anonymous], 2008, WORLD APPL SCI J
   [Anonymous], 2016, IEEE T NEURAL NETWOR
   [Anonymous], COMPUT KNOWL TECHNOL
   [Anonymous], 2004, OXFORD HDB COMPUTATI
   [Anonymous], 2010, SPIKING NEURAL P SYS
   [Anonymous], P 5 BRAINST WEEK MEM
   [Anonymous], NEURAL NETW
   Buonomano DV, 1999, NEURAL COMPUT, V11, P103, DOI 10.1162/089976699300016836
   Burges CJC, 1998, DATA MIN KNOWL DISC, V2, P121, DOI 10.1023/A:1009715923555
   Carpenter G., 1991, PATTERN RECOGNITION
   CARPENTER GA, 1989, NEURAL NETWORKS, V2, P243, DOI 10.1016/0893-6080(89)90035-X
   Cavaliere M, 2009, THEOR COMPUT SCI, V410, P2352, DOI 10.1016/j.tcs.2009.02.031
   Deco G, 1998, NETWORK-COMP NEURAL, V9, P303, DOI 10.1088/0954-898X/9/3/002
   Eurich CW, 2000, NEURAL COMPUT, V12, P1519, DOI 10.1162/089976600300015240
   Ghosh-Dastidar S, 2007, INTEGR COMPUT-AID E, V14, P187
   Gong MG, 2015, IEEE T NEUR NET LEAR, V26, P3263, DOI 10.1109/TNNLS.2015.2469673
   Goodman DFM, 2009, FRONT NEUROSCI-SWITZ, V3, P192, DOI 10.3389/neuro.01.026.2009
   Gupta A, 2007, IEEE IJCNN, P53, DOI 10.1109/IJCNN.2007.4370930
   Hagan MT., 1996, NEURAL NETWORK DESIG
   Haibo He, 2009, IEEE Transactions on Knowledge and Data Engineering, V21, P1263, DOI 10.1109/TKDE.2008.239
   Hopcroft J. E., 1979, INTRO AUTOMATA THEOR
   Ibarra OH, 2009, THEOR COMPUT SCI, V410, P2982, DOI 10.1016/j.tcs.2009.03.004
   Ionescu M, 2006, FUND INFORM, V71, P279
   Ishdorj TO, 2010, THEOR COMPUT SCI, V411, P2345, DOI 10.1016/j.tcs.2010.01.019
   Kang M, 2008, INFORM SCIENCES, V178, P3802, DOI 10.1016/j.ins.2008.05.011
   Kasabov NK, 2014, NEURAL NETWORKS, V52, P62, DOI 10.1016/j.neunet.2014.01.006
   LeCun Y, 2015, NATURE, V521, p7553 436 444, DOI [10.1038/nature14539, DOI 10.1038/NATURE14539]
   Leporati Alberto, 2009, Natural Computing, V8, P681, DOI 10.1007/s11047-008-9091-y
   LI JH, 1989, IEEE T CIRCUITS SYST, V36, P1405, DOI 10.1109/31.41297
   Liu CL, 2003, PATTERN RECOGN, V36, P2271, DOI 10.1016/S0031-3203(03)00085-2
   Liu YT, 2016, IEEE T NEUR NET LEAR, V27, P347, DOI 10.1109/TNNLS.2015.2496330
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maass W, 2001, PULSED NEURAL NETWOR
   MOLLER MF, 1993, NEURAL NETWORKS, V6, P525, DOI 10.1016/S0893-6080(05)80056-5
   Ni Z, 2013, IEEE T NEUR NET LEAR, V24, P913, DOI 10.1109/TNNLS.2013.2247627
   Pan LQ, 2011, SCI CHINA INFORM SCI, V54, P1596, DOI 10.1007/s11432-011-4303-y
   Pan LQ, 2010, LECT NOTES COMPUT SC, V5957, P436
   Pan LQ, 2009, INT J COMPUT COMMUN, V4, P273, DOI 10.15837/ijccc.2009.3.2435
   Panchev C, 2006, LECT NOTES COMPUT SC, V4131, P750
   Paum G, 2007, J UNIVERS COMPUT SCI, V13, P1707
   Päun G, 2000, J COMPUT SYST SCI, V61, P108, DOI 10.1006/jcss.1999.1693
   Paun Gh, 2010, OXFORD HDB MEMBRANE
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Roy S, 2016, IEEE T NEUR NET LEAR, V27, P1572, DOI 10.1109/TNNLS.2015.2447011
   SIEGELMANN HT, 1995, J COMPUT SYST SCI, V50, P132, DOI 10.1006/jcss.1995.1013
   Sipser M., 2013, INTRO THEORY COMPUTA
   Song T., 2018, NEURAL PROCESS LETT, P1, DOI 0.1007/s11063-018-9947-9
   Song T, 2016, IEEE T NANOBIOSCI, V15, P666, DOI 10.1109/TNB.2016.2598879
   Song T, 2016, INFORM SCIENCES, V372, P380, DOI 10.1016/j.ins.2016.08.055
   Song T, 2015, IEEE T NANOBIOSCI, V14, P465, DOI 10.1109/TNB.2015.2402311
   Song T, 2015, IEEE T NANOBIOSCI, V14, P38, DOI 10.1109/TNB.2014.2367506
   Song T, 2014, THEOR COMPUT SCI, V529, P82, DOI 10.1016/j.tcs.2014.01.001
   Song T, 2013, IEEE T NANOBIOSCI, V12, P255, DOI 10.1109/TNB.2013.2271278
   Song T, 2013, INFORM SCIENCES, V219, P197, DOI 10.1016/j.ins.2012.07.023
   Stimberg M, 2014, FRONT NEUROINFORM, V8, DOI 10.3389/fninf.2014.00006
   Wang J, 2010, NEURAL COMPUT, V22, P2615, DOI 10.1162/NECO_a_00022
   Wang T, 2015, IEEE T POWER SYST, V30, P1182, DOI 10.1109/TPWRS.2014.2347699
   Wasserman P D, 1993, ADV METHODS NEURAL C
   Zeng XX, 2012, IEEE T NANOBIOSCI, V11, P366, DOI 10.1109/TNB.2012.2211034
   Zhang GX, 2014, INT J NEURAL SYST, V24, DOI 10.1142/S0129065714400061
   Zhang HG, 2014, IEEE T NEUR NET LEAR, V25, P1229, DOI 10.1109/TNNLS.2014.2317880
   Zhong XN, 2015, NEUROCOMPUTING, V149, P116, DOI 10.1016/j.neucom.2014.01.060
   Zhou B., 2014, ADV NEURAL INFORM PR, V1, P1, DOI DOI 10.1162/153244303322533223
NR 68
TC 83
Z9 83
U1 4
U2 46
PD APR
PY 2019
VL 18
IS 2
BP 176
EP 190
DI 10.1109/TNB.2019.2896981
WC Biochemical Research Methods; Nanoscience & Nanotechnology
DA 2023-11-11
ER

PT J
AU Jin, X
   Luján, M
   Plana, LA
   Davies, S
   Temple, S
   Furber, SB
AF Jin, Xin
   Lujan, Mikel
   Plana, Luis A.
   Davies, Sergio
   Temple, Steve
   Furber, Steve B.
TI MODELING SPIKING NEURAL NETWORKS ON SPINNAKER
SO COMPUTING IN SCIENCE & ENGINEERING
DT Editorial Material
C1 [Furber, Steve B.] Univ Manchester, Sch Comp Sci, Adv Processor Technol Res Grp, Manchester M13 9PL, Lancs, England.
EM jinxa@cs.man.ac.uk; mikel.lujan@manchester.ac.uk;
   luis.plana@manchester.ac.uk; daviess@cs.man.ac.uk; temples@cs.man.ac.uk;
   steve.furber@manchester.ac.uk
CR HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Izhikevich EM, 2004, CEREB CORTEX, V14, P933, DOI 10.1093/cercor/bhh053
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   JIN X, 2008, P 2008 INT JOINT C N, P2812
   Khan MM, 2008, IEEE IJCNN, P2849, DOI 10.1109/IJCNN.2008.4634199
   Markram H, 2006, NAT REV NEUROSCI, V7, P153, DOI 10.1038/nrn1848
   Merolla PA, 2007, IEEE T CIRCUITS-I, V54, P301, DOI 10.1109/TCSI.2006.887474
   Plana LA, 2007, IEEE DES TEST COMPUT, V24, P454, DOI 10.1109/MDT.2007.149
   Sporns O, 2000, CEREB CORTEX, V10, P127, DOI 10.1093/cercor/10.2.127
NR 9
TC 63
Z9 71
U1 2
U2 15
PD SEP-OCT
PY 2010
VL 12
IS 5
BP 91
EP 97
DI 10.1109/MCSE.2010.112
WC Computer Science, Interdisciplinary Applications
DA 2023-11-11
ER

PT J
AU Meftah, B
   Lezoray, O
   Benyettou, A
AF Meftah, B.
   Lezoray, O.
   Benyettou, A.
TI Segmentation and Edge Detection Based on Spiking Neural Network Model
SO NEURAL PROCESSING LETTERS
DT Article
DE Spiking neuron networks; Segmentation; Edge detection; Hebbian learning
ID IMAGE SEGMENTATION; NEURONS
AB The process of segmenting images is one of the most critical ones in automatic image analysis whose goal can be regarded as to find what objects are present in images. Artificial neural networks have been well developed so far. First two generations of neural networks have a lot of successful applications. Spiking neuron networks (SNNs) are often referred to as the third generation of neural networks which have potential to solve problems related to biological stimuli. They derive their strength and interest from an accurate modeling of synaptic interactions between neurons, taking into account the time of spike emission. SNNs overcome the computational power of neural networks made of threshold or sigmoidal units. Based on dynamic event-driven processing, they open up new horizons for developing models with an exponential capacity of memorizing and a strong ability to fast adaptation. Moreover, SNNs add a new dimension, the temporal axis, to the representation capacity and the processing abilities of neural networks. In this paper, we present how SNN can be applied with efficacy in image segmentation and edge detection. Results obtained confirm the validity of the approach.
C1 [Meftah, B.] Univ Mascara, Equipe EDTEC, Mascara, Algeria.
   [Lezoray, O.] Univ Caen Basse Normandie, GREYC UMR CNRS 6072, F-14050 Caen, France.
   [Benyettou, A.] Univ Mohamed Boudiaf USTO, Lab Signal Image & Parole SIMPA, Oran, Algeria.
RP Meftah, B (corresponding author), Univ Mascara, Equipe EDTEC, Mascara, Algeria.
EM meftahb@yahoo.fr
CR [Anonymous], 2000, HDB MEDICAL IMAGING, DOI 10.1117/3.831079.ch3
   Averbeck BB, 2006, NAT REV NEUROSCI, V7, P358, DOI 10.1038/nrn1888
   Bohte Sander M., 2004, Natural Computing, V3, P195, DOI 10.1023/B:NACO.0000027755.02868.60
   Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   Buhmann JM, 2005, NEURAL COMPUT, V17, P1010, DOI 10.1162/0899766053491913
   Butts DA, 2007, NATURE, V449, P92, DOI [10.1038/nature06105, 10.1038/natureO6105]
   Chicurel M, 2002, SCIENCE, V295, P606, DOI 10.1126/science.295.5555.606
   Dayan P., 2001, THEORETICAL NEUROSCI
   Deng YN, 2001, IEEE T PATTERN ANAL, V23, P800, DOI 10.1109/34.946985
   Freixenet J, 2002, LECT NOTES COMPUT SC, V2352, P408, DOI 10.1007/3-540-47977-5_27
   Gerstner W, 2002, BIOL CYBERN, V87, P404, DOI 10.1007/s00422-002-0353-y
   Gerstner W., 2002, SPIKING NEURON MODEL
   Ghosh-Dastidar S, 2009, ADV INTEL SOFT COMPU, V61, P167
   GIRAU B, 2006, EUR S ART NEUR NETW, P173
   Gupta A, 2009, IEEE IJCNN, P1189
   Leibold C, 2001, NEURAL NETWORKS, V14, P805, DOI 10.1016/S0893-6080(01)00081-8
   Liew AWC, 2005, IEEE T FUZZY SYST, V13, P444, DOI 10.1109/TFUZZ.2004.841748
   Ma Z, 2010, COMPUT METHOD BIOMEC, V13, P235, DOI 10.1080/10255840903131878
   MAASS W, 2001, RELEVANCE NEURAL NET
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   MEFTAH B, 2008, IEEE WORLD C COMP IN, P682
   Melkemi KE, 2006, PATTERN RECOGN LETT, V27, P1230, DOI 10.1016/j.patrec.2005.07.021
   Meurie C, 2005, INT J ROBOT AUTOM, V20, P63, DOI 10.2316/Journal.206.2005.2.206-2780
   Natschlager T, 1998, NETWORK-COMP NEURAL, V9, P319, DOI 10.1088/0954-898X/9/3/003
   OSTER M, 2004, P 11 IEEE INT C EL C, V11, P203
   Paugam-Moisy H., 2009, HDB NATURAL COMPUTIN, P40
   Rowcliffe P, 2002, LECT NOTES COMPUT SC, V2415, P69
   Senthilkumaran N., 2009, International Journal of Recent Trends in Engineering, V1, P250
   Simoes AD, 2008, LECT NOTES ARTIF INT, V5249, P227, DOI 10.1007/978-3-540-88190-2_28
   Stein RB, 2005, NAT REV NEUROSCI, V6, P389, DOI 10.1038/nrn1668
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   Wu QX, 2008, NEUROCOMPUTING, V71, P2055, DOI 10.1016/j.neucom.2007.10.020
NR 32
TC 50
Z9 58
U1 3
U2 23
PD OCT
PY 2010
VL 32
IS 2
BP 131
EP 146
DI 10.1007/s11063-010-9149-6
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT J
AU Renaud, S
   Tomas, J
   Lewis, N
   Bornat, Y
   Daouzli, A
   Rudolph, M
   Destexhe, A
   Saïghi, S
AF Renaud, S.
   Tomas, J.
   Lewis, N.
   Bornat, Y.
   Daouzli, A.
   Rudolph, M.
   Destexhe, A.
   Saighi, S.
TI PAX: A mixed hardware/software simulation platform for spiking neural
   networks
SO NEURAL NETWORKS
DT Article
DE Spiking neural networks; Integrated circuits; Hardware simulation;
   Conductance-based neuron models; Spike-Timing-Dependent Plasticity
ID NEURONS; MODEL; SYNAPSES; SYSTEM
AB Many hardware-based solutions now exist for the simulation of bio-like neural networks. Less conventional than software-based systems, these types of simulators generally combine digital and analog forms of computation. In this paper we present a mixed hardware-software platform, specifically designed for the simulation of spiking neural networks, using conductance-based models of neurons and synaptic connections with dynamic adaptation rules (Spike-Timing-Dependent Plasticity). The neurons and networks are configurable, and are computed in 'biological real time' by which we mean that the difference between simulated time and simulation time is guaranteed lower than 50 mu s. After presenting the issues and context involved in the design and use of hardware-based spiking neural networks, we describe the analog neuromimetic integrated circuits which form the core of the platform. We then explain the organization and computation principles of the modules within the platform, and present experimental results which validate the system. Designed as a tool for computational neuroscience, the platform is exploited Ill collaborative research projects together with neurobiology and computer science partners. (C) 2010 Elsevier Ltd. All rights reserved.
C1 [Renaud, S.; Tomas, J.; Lewis, N.; Bornat, Y.; Daouzli, A.; Saighi, S.] Univ Bordeaux, IMS, ENSEIRB, CNRS,UMR5218, F-33405 Talence, France.
   [Rudolph, M.; Destexhe, A.] CNRS, UNIC, UPR2191, F-91198 Gif Sur Yvette, France.
RP Renaud, S (corresponding author), Univ Bordeaux, IMS, ENSEIRB, CNRS,UMR5218, 351 Cours Liberat, F-33405 Talence, France.
EM sylvie.renaud@ims-bordeaux.fr
CR Akay M., 2007, HDB NEURAL ENG 2
   Badoual M, 2006, INT J NEURAL SYST, V16, P79, DOI 10.1142/S0129065706000524
   Binczak S, 2006, NEURAL NETWORKS, V19, P684, DOI 10.1016/j.neunet.2005.07.011
   BORNAT Y, 2005, P 20 C DES CIRC INT
   Brette R, 2005, J NEUROPHYSIOL, V94, P3637, DOI 10.1152/jn.00686.2005
   Brette R, 2007, J COMPUT NEUROSCI, V23, P349, DOI 10.1007/s10827-007-0038-6
   CONNORS BW, 1990, TRENDS NEUROSCI, V13, P99, DOI 10.1016/0166-2236(90)90185-D
   Destexhe A, 2001, NEUROSCIENCE, V107, P13, DOI 10.1016/S0306-4522(01)00344-X
   DESTEXHE A, 1994, NEURAL COMPUT, V6, P14, DOI 10.1162/neco.1994.6.1.14
   Farquhar E, 2005, IEEE T CIRCUITS-I, V52, P477, DOI 10.1109/TCSI.2004.842871
   Fieres J, 2006, IEEE IJCNN, P21
   FITZHUGH R, 1961, BIOPHYS J, V1, P445, DOI 10.1016/S0006-3495(61)86902-6
   Froemke RC, 2002, NATURE, V416, P433, DOI 10.1038/416433a
   Gerstner W., 2002, SPIKING NEURON MODEL
   Glackin B, 2005, LECT NOTES COMPUT SC, V3512, P552
   Graas EL, 2004, NEUROINFORMATICS, V2, P417, DOI 10.1385/NI:2:4:417
   Gupta A, 2000, SCIENCE, V287, P273, DOI 10.1126/science.287.5451.273
   Hasler P, 2007, IEEE INT SYMP CIRC S, P3359, DOI 10.1109/ISCAS.2007.378287
   Hines ML, 1997, NEURAL COMPUT, V9, P1179, DOI 10.1162/neco.1997.9.6.1179
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Indiveri G, 2006, IEEE T NEURAL NETWOR, V17, P211, DOI 10.1109/TNN.2005.860850
   Indiveri G, 2007, IEEE INT SYMP CIRC S, P3371, DOI 10.1109/ISCAS.2007.378290
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Johansson C, 2007, NEURAL NETWORKS, V20, P48, DOI 10.1016/j.neunet.2006.05.029
   Jung R, 2001, IEEE T NEUR SYS REH, V9, P319, DOI 10.1109/7333.948461
   Le Masson G, 2002, NATURE, V417, P854, DOI 10.1038/nature00825
   Liu SC, 2004, IEEE T NEURAL NETWOR, V15, P1305, DOI 10.1109/TNN.2004.832725
   MAHOWALD M, 1991, NATURE, V354, P515, DOI 10.1038/354515a0
   McCormick DA, 1997, ANNU REV NEUROSCI, V20, P185, DOI 10.1146/annurev.neuro.20.1.185
   Migliore M, 2006, J COMPUT NEUROSCI, V21, P119, DOI 10.1007/s10827-006-7949-5
   MORRIS C, 1981, BIOPHYS J, V35, P193, DOI 10.1016/S0006-3495(81)84782-0
   RENAUD S, 1999, IEEE T BIOMEDICAL EN, V46, P638
   Renaud S, 2007, IEEE INT SYMP CIRC S, P3355, DOI 10.1109/ISCAS.2007.378286
   Renaud-Le Masson S, 2004, INFORM SCIENCES, V161, P57, DOI 10.1016/j.ins.2003.03.007
   Schemmel J, 2007, IEEE INT SYMP CIRC S, P3367, DOI 10.1109/ISCAS.2007.378289
   Sorensen M, 2004, J NEUROSCI, V24, P5427, DOI 10.1523/JNEUROSCI.4449-03.2004
   Vogelstein RJ, 2004, 2004 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL 5, PROCEEDINGS, P385
   Zou Q, 2006, NETWORK-COMP NEURAL, V17, P211, DOI 10.1080/09548980600711124
NR 38
TC 15
Z9 17
U1 0
U2 5
PD SEP
PY 2010
VL 23
IS 7
BP 905
EP 916
DI 10.1016/j.neunet.2010.02.006
WC Computer Science, Artificial Intelligence; Neurosciences
DA 2023-11-11
ER

PT J
AU Yan, YL
   Chu, HM
   Jin, Y
   Huan, YX
   Zou, Z
   Zheng, LR
AF Yan, Yulong
   Chu, Haoming
   Jin, Yi
   Huan, Yuxiang
   Zou, Zhuo
   Zheng, Lirong
TI Backpropagation With Sparsity Regularization for Spiking Neural Network
   Learning
SO FRONTIERS IN NEUROSCIENCE
DT Article
DE spiking neural network; backpropagation; sparsity regularization;
   spiking sparsity; synaptic sparsity
ID INTELLIGENCE; PERFORMANCE
AB The spiking neural network (SNN) is a possible pathway for low-power and energy-efficient processing and computing exploiting spiking-driven and sparsity features of biological systems. This article proposes a sparsity-driven SNN learning algorithm, namely backpropagation with sparsity regularization (BPSR), aiming to achieve improved spiking and synaptic sparsity. Backpropagation incorporating spiking regularization is utilized to minimize the spiking firing rate with guaranteed accuracy. Backpropagation realizes the temporal information capture and extends to the spiking recurrent layer to support brain-like structure learning. The rewiring mechanism with synaptic regularization is suggested to further mitigate the redundancy of the network structure. Rewiring based on weight and gradient regulates the pruning and growth of synapses. Experimental results demonstrate that the network learned by BPSR has synaptic sparsity and is highly similar to the biological system. It not only balances the accuracy and firing rate, but also facilitates SNN learning by suppressing the information redundancy. We evaluate the proposed BPSR on the visual dataset MNIST, N-MNIST, and CIFAR10, and further test it on the sensor dataset MIT-BIH and gas sensor. Results bespeak that our algorithm achieves comparable or superior accuracy compared to related works, with sparse spikes and synapses.
C1 [Yan, Yulong; Chu, Haoming; Jin, Yi; Huan, Yuxiang; Zou, Zhuo; Zheng, Lirong] Fudan Univ, Sch Informat Sci & Technol, Shanghai, Peoples R China.
RP Zou, Z; Zheng, LR (corresponding author), Fudan Univ, Sch Informat Sci & Technol, Shanghai, Peoples R China.
EM zhuo@fudan.edu.cn; lrzheng@fudan.edu.cn
CR Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   Allred J.M., 2020, PREPRINT
   Amirshahi A, 2019, IEEE T BIOMED CIRC S, V13, P1483, DOI 10.1109/TBCAS.2019.2948920
   [Anonymous], 2019, P AAAI C ARTIFICIAL
   Bartol TM, 2015, ELIFE, V4, DOI 10.7554/eLife.10778
   Bauer FC, 2019, IEEE T BIOMED CIRC S, V13, P1575, DOI 10.1109/TBCAS.2019.2953001
   Bialas M, 2022, IEEE T NEUR NET LEAR, V33, P5215, DOI 10.1109/TNNLS.2021.3069683
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Chen HT, 2020, PROC CVPR IEEE, P1465, DOI 10.1109/CVPR42600.2020.00154
   Cheng ZQ, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P1897, DOI 10.1145/3343031.3350898
   Cho SG, 2019, IEEE CUST INTEGR CIR, DOI 10.1109/CICC.2019.8780116
   Comsa IM, 2022, IEEE T NEUR NET LEAR, V33, P5939, DOI 10.1109/TNNLS.2021.3071976
   Cook SJ, 2019, NATURE, V571, P63, DOI 10.1038/s41586-019-1352-7
   Corradi F, 2019, IEEE IJCNN
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Dempsey WP, 2022, P NATL ACAD SCI USA, V119, DOI 10.1073/pnas.2107661119
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Diehl Peter U, 2015, 2015 INT JOINT C NEU, P1, DOI [DOI 10.1109/IJCNN.2015.7280696, 10.1109/IJCNN.2015.7280696]
   Ding C, 2021, 2021 IEEE 3RD INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE CIRCUITS AND SYSTEMS (AICAS), DOI [10.1109/ICAICE54393.2021.00008, 10.1109/AICAS51828.2021.9458445]
   Finnerty A., 2017, REDUCE POWER COST CO
   Frenkel C., 2021, ARXIV PREPRINT ARXIV
   Guo WZ, 2020, ELECTRONICS-SWITZ, V9, DOI 10.3390/electronics9071059
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hubara I., 2016, ADV NEURAL INFORM PR, Vvol 29
   Imam N, 2020, NAT MACH INTELL, V2, P181, DOI 10.1038/s42256-020-0159-4
   Ioffe S., 2015, ICML, DOI DOI 10.1007/S13398-014-0173-7.2
   Jin Y., 2018, ADV NEURAL INF PROCE, V31, P1
   Kaiser J, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00424
   Kim S, 2020, AAAI CONF ARTIF INTE, V34, P11270
   Kingma D. P., 2014, ARXIV, DOI DOI 10.48550/ARXIV.1412.6980
   Kolagasioglu E., 2018, ENERGY EFFICIENT FEA
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Liang MX, 2021, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS51556.2021.9401607
   Lillicrap TP, 2020, NAT REV NEUROSCI, V21, P335, DOI 10.1038/s41583-020-0277-3
   Loshchilov Ilya, 2017, ARXIV171105101
   Luo LQ, 2021, SCIENCE, V373, P1103, DOI 10.1126/science.abg7285
   Marisa T, 2017, IEEE T BIOMED CIRC S, V11, P267, DOI 10.1109/TBCAS.2016.2619858
   Milo R, 2004, SCIENCE, V303, P1538, DOI 10.1126/science.1089167
   Moody GA, 2001, IEEE ENG MED BIOL, V20, P45, DOI 10.1109/51.932724
   Mozafari M, 2018, IEEE T NEUR NET LEAR, V29, P6178, DOI 10.1109/TNNLS.2018.2826721
   Nguyen T.N.N., 2021, PREPRINT
   Orchard G, 2015, FRONT NEUROSCI-SWITZ, V9, DOI [10.3389/fnins.2015.00437, 10.3389/fhins.2015.00437]
   Paszke A, 2019, ADV NEUR IN, V32
   Pei J, 2019, NATURE, V572, P106, DOI 10.1038/s41586-019-1424-8
   Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32
   Rathi N, 2019, IEEE T COMPUT AID D, V38, P668, DOI 10.1109/TCAD.2018.2819366
   Roy K, 2019, NATURE, V575, P607, DOI 10.1038/s41586-019-1677-2
   Rueckauer B, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351295
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Shi Y, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00405
   Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270
   Stöckl C, 2021, NAT MACH INTELL, V3, DOI 10.1038/s42256-021-00311-4
   Sun J.(, 2015, IEEE I CONF COMP VIS, P1026, DOI DOI 10.1109/ICCV.2015.123
   Tang H, 2020, NEUROCOMPUTING, V407, P300, DOI 10.1016/j.neucom.2020.05.031
   Tang P.T.P., 2017, ARXIV PREPRINT ARXIV
   Thorpe S, 1998, COMPUTATIONAL NEUROSCIENCE: TRENDS IN RESEARCH, P113
   Vaila R., 2019, P INT C NEUR SYST, P1
   Vergara A, 2013, SENSOR ACTUAT B-CHEM, V185, P462, DOI 10.1016/j.snb.2013.05.027
   Wu YC, 2020, ELECTRON LETT, V56, P1230, DOI 10.1049/el.2020.2224
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Yan YL, 2021, 2021 IEEE 3RD INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE CIRCUITS AND SYSTEMS (AICAS), DOI 10.1109/AICAS51828.2021.9458461
   Yan ZL, 2021, BIOMED SIGNAL PROCES, V63, DOI 10.1016/j.bspc.2020.102170
   Zhang W., 2019, P ADV NEUR INF PROC, V32, P1
NR 64
TC 4
Z9 4
U1 5
U2 20
PD APR 14
PY 2022
VL 16
AR 760298
DI 10.3389/fnins.2022.760298
WC Neurosciences
DA 2023-11-11
ER

PT J
AU Sporea, I
   Grüning, A
AF Sporea, Ioana
   Gruening, Andre
TI Supervised Learning in Multilayer Spiking Neural Networks
SO NEURAL COMPUTATION
DT Article
ID ERROR-BACKPROPAGATION; CLASSIFICATION; REINFORCEMENT; PROPAGATION;
   NEURONS
AB We introduce a supervised learning algorithm for multilayer spiking neural networks. The algorithm overcomes a limitation of existing learning algorithms: it can be applied to neurons firing multiple spikes in artificial neural networks with hidden layers. It can also, in principle, be used with any linearizable neuron model and allows different coding schemes of spike train patterns. The algorithm is applied successfully to classic linearly nonseparable benchmarks such as the XOR problem and the Iris data set, as well as to more complex classification and mapping problems. The algorithm has been successfully tested in the presence of noise, requires smaller networks than reservoir computing, and results in faster convergence than existing algorithms for similar tasks such as SpikeProp.
C1 [Sporea, Ioana; Gruening, Andre] Univ Surrey, Dept Comp, Guildford GU2 7XH, Surrey, England.
RP Sporea, I (corresponding author), Univ Surrey, Dept Comp, Guildford GU2 7XH, Surrey, England.
EM i.nica@surrey.ac.uk; a.gruning@surrey.ac.uk
CR [Anonymous], 1986, PARALLEL DISTRIBUTED, DOI DOI 10.7551/MITPRESS/5236.001.0001
   [Anonymous], P 15 PRORISC WORKSH
   [Anonymous], 2001, HDB BIOL PHYS
   Bohte S., 2010, ADV NEURAL INFORM PR, V23, P253
   Bohte SM, 2011, LECT NOTES COMPUT SC, V6791, P60, DOI 10.1007/978-3-642-21735-7_8
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Booij O, 2005, INFORM PROCESS LETT, V95, P552, DOI 10.1016/j.ipl.2005.05.023
   deCharms RC, 1996, NATURE, V381, P610, DOI 10.1038/381610a0
   Elias J. G., 2002, PULSED NEURAL NETWOR
   Fisher RA, 1936, ANN EUGENIC, V7, P179, DOI 10.1111/j.1469-1809.1936.tb02137.x
   Fitzsimonds RM, 1997, NATURE, V388, P439, DOI 10.1038/41267
   Gerstner W., 2002, SPIKING NEURON MODEL
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   Glackin C, 2011, NEURAL NETWORKS, V24, P247, DOI 10.1016/j.neunet.2010.11.008
   Grüning A, 2007, NEURAL COMPUT, V19, P3108, DOI 10.1162/neco.2007.19.11.3108
   Grüning A, 2012, NEURAL PROCESS LETT, V36, P117, DOI 10.1007/s11063-012-9225-1
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Harris KD, 2008, TRENDS NEUROSCI, V31, P130, DOI 10.1016/j.tins.2007.12.002
   Hebb D. O., 1949, ORG BEHAV
   Johansson RS, 2004, NAT NEUROSCI, V7, P170, DOI 10.1038/nn1177
   KNUDSEN EI, 1994, J NEUROSCI, V14, P3985
   Knudsen EI, 2002, NATURE, V417, P322, DOI 10.1038/417322a
   Legenstein R, 2005, NEURAL COMPUT, V17, P2337, DOI 10.1162/0899766054796888
   Maass W, 1997, NEURAL COMPUT, V9, P279, DOI 10.1162/neco.1997.9.2.279
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Masaru F, 2008, IEEE IJCNN, P840, DOI 10.1109/IJCNN.2008.4633895
   Mauk MD, 2004, ANNU REV NEUROSCI, V27, P307, DOI 10.1146/annurev.neuro.27.070203.144247
   McKennoch S, 2009, NEURAL COMPUT, V21, P9, DOI 10.1162/neco.2008.09-07-610
   Neuenschwander S, 1996, NATURE, V379, P728, DOI 10.1038/379728a0
   Ponulak F., 2006, RESUME PROOF CONVERG
   Ponulak F, 2008, INT J APPL MATH COMP, V18, P117, DOI 10.2478/v10006-008-0011-1
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Roelfsema P. R., 2005, NEURAL COMPUT, V17, P1
   Rojas R., 1996, NEURAL NETWORKS, P149, DOI 10.1007/978-3-642-61068-4_7
   Rostro-Gonzalez H., 2010, REVERSE ENG SPIKING
   Ruf B, 1997, NEURAL PROCESS LETT, V5, P9, DOI 10.1023/A:1009697008681
   Shepherd JD, 2006, NEURON, V52, P475, DOI 10.1016/j.neuron.2006.08.034
   Sporea I, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P1090, DOI 10.1109/IJCNN.2011.6033344
   Takase Haruhiko, 2009, Proceedings 2009 International Joint Conference on Neural Networks (IJCNN 2009 - Atlanta), P3062, DOI 10.1109/IJCNN.2009.5178756
   Tao HZW, 2000, J NEUROSCI, V20, P3233, DOI 10.1523/JNEUROSCI.20-09-03233.2000
   THORPE SJ, 1989, CONNECTIONISM IN PERSPECTIVE, P63
   Tino P, 2005, LECT NOTES COMPUT SC, V3611, P666
   Urbanczik R, 2009, NAT NEUROSCI, V12, P250, DOI 10.1038/nn.2264
   van Rossum MCW, 2001, NEURAL COMPUT, V13, P751, DOI 10.1162/089976601300014321
   Wade JJ, 2010, IEEE T NEURAL NETWOR, V21, P1817, DOI 10.1109/TNN.2010.2074212
   WATT AJ, 2010, FRONTIERS SYNAPTIC N, V2
   Wehr M, 1996, NATURE, V384, P162, DOI 10.1038/384162a0
   Xin JG, 2001, IEEE IJCNN, P1772, DOI 10.1109/IJCNN.2001.938430
NR 48
TC 94
Z9 100
U1 0
U2 46
PD FEB
PY 2013
VL 25
IS 2
BP 473
EP 509
DI 10.1162/NECO_a_00396
WC Computer Science, Artificial Intelligence; Neurosciences
DA 2023-11-11
ER

PT C
AU Yusuf, ZM
   Hamed, HNA
   Yusuf, LM
   Isa, MA
AF Yusuf, Zulhairi Mi
   Hamed, Haza Nuzly Abdull
   Yusuf, Lizawati Mi
   Isa, Mohd Adham
GP IEEE
TI Evolving Spiking Neural Network (ESNN) and Harmony Search Algorithm
   (HSA) for parameter optimization
SO PROCEEDINGS OF THE 2017 6TH INTERNATIONAL CONFERENCE ON ELECTRICAL
   ENGINEERING AND INFORMATICS (ICEEI'17)
SE International Conference on Electrical Engineering and Informatics
DT Proceedings Paper
CT 6th International Conference on Electrical Engineering and Informatics
   (ICEEI) - Sustainable Society Through Digital Innovation
CY NOV 25-27, 2017
CL Langkawi, MALAYSIA
DE Evolving Spiking Neural Network (ESNN); Harmony Search Algorithm;
   Parameter Optimization; Modulation Factor; Proportion Factor; Similarity
   Factors
AB Spiking Neural Network (SNN) acts as a part of the third generation of Artificial Neural Networks (ANNs). Evolving Spiking Neural Network (ESNN) is one of the most broadly utilized among in SNN models in numerous current research works. During the classification process, ESNN model acts as a classifier and three parameters are used in this article. However, the parameters are needed to set manually before the classification process. To solve the stated problems, ESNN required an optimizer that able to optimize the three parameters such as similarity value, modulation factor and proportion factor. The best estimations of parameters are adaptively chosen by Harmony Search Algorithm (HSA) to abstain from choosing appropriate values for specific issues through the trial-and-error approach. Therefore, this article proposed the integration of ESNN as a classifier and HSA as an optimizer for parameter optimization. The experimental results give favorable accuracy rates via the hybrid of ESNN and HSA.
C1 [Yusuf, Zulhairi Mi; Hamed, Haza Nuzly Abdull; Yusuf, Lizawati Mi; Isa, Mohd Adham] Univ Teknol Malaysia, Fac Comp, Johor Baharu 81310, Johor, Malaysia.
RP Yusuf, ZM (corresponding author), Univ Teknol Malaysia, Fac Comp, Johor Baharu 81310, Johor, Malaysia.
EM zulhairi.miyusuf@gmail.com; haza@utm.my; lizawati@utm.my;
   mohadham@utm.my
CR Abdull Hamed HN., 2012, NOVEL INTEGRATED MET
   Ehrlich H. C., LNCS, V4128, P1148
   Gao X., 2016, RAKENTEIDEN MEKANIIK, V49, P119
   Geem ZW, 2001, SIMULATION, V76, P60, DOI 10.1177/003754970107600201
   Gerstner W., 2002, SPIKING NEURON MODEL
   Hamed HNA, 2009, 2009 INTERNATIONAL CONFERENCE OF SOFT COMPUTING AND PATTERN RECOGNITION, P695, DOI 10.1109/SoCPaR.2009.139
   Kasabov N., 2007, EVOLVING CONNECTIONI
   KEEL (Knowledge Extraction based on Evolutionary Learning), 2005, IR PIM IND DIAB HEAR
   MAASS W, 1996, AUSTR C NEUR NETW
   Saleh A.Y., 2014, INT J ADV SOFT COMPU, V6
   Schliebs S, 2009, LECT NOTES COMPUT SC, V5506, P1229, DOI 10.1007/978-3-642-02490-0_149
   Theodossiou N., 2011, P 3 INT CEMEPE SECOT
   Thorpe S, 1998, COMPUTATIONAL NEUROSCIENCE: TRENDS IN RESEARCH, P113
   Wysoski SG, 2010, NEURAL NETWORKS, V23, P819, DOI 10.1016/j.neunet.2010.04.009
NR 14
TC 0
Z9 0
U1 0
U2 0
PY 2017
WC Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Antiqueira, L
   Zhao, L
AF Antiqueira, Lucas
   Zhao, Liang
BE Engelbrecht, A
   Filho, CJAB
   Neto, FBD
TI Structural relationships between spiking neural networks and functional
   samples
SO 2013 1ST BRICS COUNTRIES CONGRESS ON COMPUTATIONAL INTELLIGENCE AND 11TH
   BRAZILIAN CONGRESS ON COMPUTATIONAL INTELLIGENCE (BRICS-CCI & CBIC)
DT Proceedings Paper
CT 1st BRICS Countries Congress on Computational Intelligence / 11th
   Brazilian Congress on Computational Intelligence (BRICS-CCI and CBIC)
CY SEP 08-11, 2013
CL Recife, BRAZIL
DE computer simulation; modeling; network theory (graphs); spatial networks
ID COMPLEX NETWORKS; SCALE-FREE; BRAIN; RECORDINGS; EMERGENCE; TOPOLOGY;
   EEG
AB Models of spiking neural networks have a great potential to become a crucial tool in the development of complex network theory. Of particular interest, these models can be used to better understand the important class of brain functional networks, which are frequently studied in the context of computational network analysis. A fundamental question is whether functional connectivity sampling via surface multichannel recordings is able to reproduce the main connectivity features of the underlying spatial neural network. In this work we address this problem through computational modeling using the integrate-and-fire spiking neuron model, which enabled us to relate neural connectivity and the respective mesoscopic dynamics. Functional samples were then compared to an idealized spatial neural network model in terms of established topological network measurements. Results show that some measurements (e.g., betweenness centrality) are able to fairly approximate functional and spatial networks. Therefore, under specific circumstances of sampling size and simulation approach, it is possible to say that functional networks are able to reproduce connectivity features of the underlying neural network.
C1 [Antiqueira, Lucas; Zhao, Liang] Univ Sao Paulo, ICMC, Inst Math & Comp Sci, BR-13560970 Sao Carlos, SP, Brazil.
RP Antiqueira, L (corresponding author), Univ Sao Paulo, ICMC, Inst Math & Comp Sci, POB 668, BR-13560970 Sao Carlos, SP, Brazil.
EM lantiq@icmc.usp.br; zhao@icmc.usp.br
CR Achlioptas D, 2009, J ACM, V56, DOI 10.1145/1538902.1538905
   Antiqueira L, 2010, NEUROIMAGE, V53, P439, DOI 10.1016/j.neuroimage.2010.06.018
   Antiqueira L, 2009, NEW J PHYS, V11, DOI 10.1088/1367-2630/11/1/013058
   Arenas A, 2008, PHYS REP, V469, P93, DOI 10.1016/j.physrep.2008.09.002
   Azevedo FAC, 2009, J COMP NEUROL, V513, P532, DOI 10.1002/cne.21974
   Barabási AL, 1999, SCIENCE, V286, P509, DOI 10.1126/science.286.5439.509
   Boas PRV, 2010, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2010/03/P03009
   Bullmore ET, 2012, NAT REV NEUROSCI, V13, P336, DOI 10.1038/nrn3214
   Buzsáki G, 2012, NAT REV NEUROSCI, V13, P407, DOI 10.1038/nrn3241
   Clauset A, 2005, PHYS REV LETT, V94, DOI 10.1103/PhysRevLett.94.018701
   Costa LD, 2007, ADV PHYS, V56, P167, DOI 10.1080/00018730601170527
   Costa LD, 2011, ADV PHYS, V60, P329, DOI 10.1080/00018732.2011.572452
   Dall'Asta L, 2006, THEOR COMPUT SCI, V355, P6, DOI 10.1016/j.tcs.2005.12.009
   Deco G, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.1000092
   Erdos P., 1959, PUBL MATH-DEBRECEN, V6, P290
   FREEMAN LC, 1977, SOCIOMETRY, V40, P35, DOI 10.2307/3033543
   Gerhard F, 2011, FRONT COMPUT NEUROSC, V5, P1, DOI [10.3389/fncom.2011.00023, 10.3389/fncom.2011.00004]
   GILBERT EN, 1959, ANN MATH STAT, V30, P1141, DOI 10.1214/aoms/1177706098
   Han JDJ, 2005, NAT BIOTECHNOL, V23, P839, DOI 10.1038/nbt1116
   Joudaki Amir, 2012, PLoS One, V7, pe35673, DOI 10.1371/journal.pone.0035673
   Kaiser M, 2004, PHYS REV E, V69, DOI 10.1103/PhysRevE.69.036103
   Kaiser M, 2011, NEUROIMAGE, V57, P892, DOI 10.1016/j.neuroimage.2011.05.025
   Koschützki D, 2005, LECT NOTES COMPUT SC, V3418, P16
   Kramer MA, 2011, J NEUROSCI, V31, P15757, DOI 10.1523/JNEUROSCI.2287-11.2011
   Latapy M, 2008, IEEE INFOCOM SER, P2333
   Latora V, 2001, PHYS REV LETT, V87, DOI 10.1103/PhysRevLett.87.198701
   Lee SH, 2006, PHYS REV E, V73, DOI 10.1103/PhysRevE.73.016102
   Newman M. E. J., 2018, NETWORKS INTRO, V2nd, DOI [10.1093/acprof:oso/9780199206650.001.0001, DOI 10.1093/ACPROF:OSO/9780199206650.001.0001]
   Newman MEJ, 2002, PHYS REV LETT, V89, DOI 10.1103/PhysRevLett.89.208701
   Nunez P. L., 2006, ELECT FIELDS BRAIN N, V2nd Edn
   Olejniczak P, 2006, J CLIN NEUROPHYSIOL, V23, P186, DOI 10.1097/01.wnp.0000220079.61973.6c
   Palva S, 2012, TRENDS COGN SCI, V16, P219, DOI 10.1016/j.tics.2012.02.004
   Pereda E, 2005, PROG NEUROBIOL, V77, P1, DOI 10.1016/j.pneurobio.2005.10.003
   Roth A, 2009, COMPUT NEUROSCI-MIT, P139
   Rubinov M, 2011, NEUROIMAGE, V56, P2068, DOI 10.1016/j.neuroimage.2011.03.069
   Shkarayev MS, 2009, EPL-EUROPHYS LETT, V88, DOI 10.1209/0295-5075/88/50001
   Stam CJ, 2010, INT J PSYCHOPHYSIOL, V77, P186, DOI 10.1016/j.ijpsycho.2010.06.024
   Stumpf MPH, 2005, P NATL ACAD SCI USA, V102, P4221, DOI 10.1073/pnas.0501179102
   Trappenberg TP., 2002, FUNDAMENTALS COMPUTA
   Watts DJ, 1998, NATURE, V393, P440, DOI 10.1038/30918
   WAXMAN BM, 1988, IEEE J SEL AREA COMM, V6, P1617, DOI 10.1109/49.12889
NR 41
TC 3
Z9 3
U1 0
U2 3
PY 2013
BP 46
EP 54
DI 10.1109/BRICS-CCI-CBIC.2013.19
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT J
AU Siddique, NH
   McDaid, LJ
   Kasabov, N
   Widrow, B
AF Siddique, N. H.
   McDaid, L. J.
   Kasabov, N.
   Widrow, B.
TI Special Issue: Spiking Neural Networks INTRODUCTION
SO INTERNATIONAL JOURNAL OF NEURAL SYSTEMS
DT Editorial Material
C1 [Siddique, N. H.; McDaid, L. J.] Univ Ulster, Coleraine BT52 1SA, Londonderry, North Ireland.
   [Kasabov, N.] Auckland Univ Technol, Auckland, New Zealand.
   [Widrow, B.] Stanford Univ, Stanford, CA 94305 USA.
RP Siddique, NH (corresponding author), Univ Ulster, Coleraine BT52 1SA, Londonderry, North Ireland.
CR Christodoulou C, 2002, NEURAL NETWORKS, V15, P891, DOI 10.1016/S0893-6080(02)00034-5
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Halassa MM, 2010, ANNU REV PHYSIOL, V72, P335, DOI 10.1146/annurev-physiol-021909-135843
   Henneberger C, 2010, NATURE, V463, P232, DOI 10.1038/nature08673
   Iglesias J, 2008, INT J NEURAL SYST, V18, P267, DOI 10.1142/S0129065708001580
   Kasinski A., 2006, International Journal of Applied Mathematics and Computer Science, V16, P101
   Perea G, 2009, TRENDS NEUROSCI, V32, P421, DOI 10.1016/j.tins.2009.05.001
   Rossello JL, 2009, INT J NEURAL SYST, V19, P465, DOI 10.1142/S0129065709002166
NR 9
TC 1
Z9 1
U1 0
U2 8
PD DEC
PY 2010
VL 20
IS 6
BP V
EP VII
DI 10.1142/S0129065710002590
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT C
AU Liu, QH
   Xing, D
   Feng, L
   Tang, HJ
   Pan, G
AF Liu, Qianhui
   Xing, Dong
   Feng, Lang
   Tang, Huajin
   Pan, Gang
GP IEEE
TI EVENT-BASED MULTIMODAL SPIKING NEURAL NETWORK WITH ATTENTION MECHANISM
SO 2022 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)
SE International Conference on Acoustics Speech and Signal Processing
   ICASSP
DT Proceedings Paper
CT 47th IEEE International Conference on Acoustics, Speech and Signal
   Processing (ICASSP)
CY MAY 22-27, 2022
CL Singapore, SINGAPORE
DE spiking neural networks; multimodal learning; dynamic vision sensors;
   dynamic audio sensors
AB Human brain can effectively integrate visual and auditory information. Dynamic Vision Sensor (DVS) and Dynamic Audio Sensor (DAS) are event-based sensors imitating the mechanism of human retina and cochlea. Since the sensors record the visual and auditory input as asynchronous discrete events, they are inherently suitable to cooperate with the spiking neural network (SNN). Existing works of SNNs for processing events mainly focus on unimodality, however, audiovisual multimodal SNNs are still limited. In this paper, we propose an end-to-end event-based multimodal spiking neural network. The network consists of visual and auditory uni-modal subnetworks and a novel attention-based cross-modal subnetwork for fusion. The attention mechanism measures the significance of each modality and allocates the weights to two modalities. We evaluate our proposed multimodal network on an event-based audiovisual joint dataset (MNIST-DVS and N-TIDIGITS datasets). Experimental results show the performance improvement of this multimodal network and the effectiveness of our proposed attention mechanism.
C1 [Liu, Qianhui; Xing, Dong; Feng, Lang; Tang, Huajin; Pan, Gang] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou, Peoples R China.
   [Liu, Qianhui] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore, Singapore.
   [Tang, Huajin; Pan, Gang] Zhejiang Lab, Hangzhou, Peoples R China.
RP Pan, G (corresponding author), Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou, Peoples R China.; Pan, G (corresponding author), Zhejiang Lab, Hangzhou, Peoples R China.
EM qianhuiliu@zju.edu.cn; dongxing@zju.edu.cn; langfeng@zju.edu.cn;
   htang@zju.edu.cn; gpan@zju.edu.cn
CR Anumula J, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00023
   Chen Zehao, 2021, P IEEECVF C COMPUTER, P14760
   Gu PJ, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1366
   Jin Yingyezhe, 2018, P 32 INT C NEUR INF
   Kawashima R, 1999, NEUROIMAGE, V10, P209, DOI 10.1006/nimg.1999.0452
   Li Xiaoya, 2019, ISCAS, P1
   Lichtsteiner P, 2008, IEEE J SOLID-ST CIRC, V43, P566, DOI 10.1109/JSSC.2007.914337
   Liu QH, 2020, AAAI CONF ARTIF INTE, V34, P1308
   Liu QH, 2020, IEEE T NEUR NET LEAR, V31, P5300, DOI 10.1109/TNNLS.2020.2966058
   Liu Qianhui, 2021, IJCAI, P1743
   Liu SC, 2014, IEEE T BIOMED CIRC S, V8, P453, DOI 10.1109/TBCAS.2013.2281834
   Orchard G, 2015, IEEE T PATTERN ANAL, V37, P2028, DOI 10.1109/TPAMI.2015.2392947
   Rathi N, 2021, IEEE TETCI, V5, P143, DOI 10.1109/TETCI.2018.2872014
   Roy K, 2019, NATURE, V575, P607, DOI 10.1038/s41586-019-1677-2
   Shrestha Sumit Bam, 2018, ADV NEURAL INFORM PR, P1412
   Wang HD, 2021, 2021 THE 6TH INTERNATIONAL CONFERENCE ON INTEGRATED CIRCUITS AND MICROSYSTEMS (ICICM 2021), P430, DOI 10.1109/ICICM54364.2021.9660297
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Xiao R, 2020, IEEE T NEUR NET LEAR, V31, P3649, DOI 10.1109/TNNLS.2019.2945630
   Zhang ML, 2020, IEEE J-STSP, V14, P592, DOI 10.1109/JSTSP.2020.2983547
   Zhang ML, 2019, IEEE T NEUR NET LEAR, V30, P123, DOI 10.1109/TNNLS.2018.2833077
   Zhang Malu, 2021, IEEE T NEURAL NETWOR
   Zhang WR, 2019, ADV NEUR IN, V32
NR 22
TC 2
Z9 2
U1 0
U2 8
PY 2022
BP 8922
EP 8926
DI 10.1109/ICASSP43922.2022.9746865
WC Acoustics; Computer Science, Artificial Intelligence; Engineering,
   Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Bousoulas, P
   Tsioustas, C
   Hadfield, J
   Aslanidis, V
   Limberopoulos, S
   Tsoukalas, D
AF Bousoulas, P.
   Tsioustas, C.
   Hadfield, J.
   Aslanidis, V
   Limberopoulos, S.
   Tsoukalas, D.
TI Low Power Stochastic Neurons From SiO<sub>2</sub>-Based Bilayer
   Conductive Bridge Memristors for Probabilistic Spiking Neural Network
   Applications-Part II: Modeling
SO IEEE TRANSACTIONS ON ELECTRON DEVICES
DT Article
DE Biological neuron; numerical modeling; spiking neural networks;
   stochastic firing
ID NANOPARTICLES; SIZE; TEMPERATURE; SYNAPSES; FILMS
AB A deep understanding of the underlying resistive switching mechanism for the implementation of volatile memristive properties is regarded as of great importance for enhancing their performance. Along these lines, a 2-D dynamical model is introduced to interpret the whole memristive pattern within the bilayer configuration, as well as the crucial of the dense layer of the Pt nanoparticles (NPs) on the local thermal distribution. Moreover, the probabilistic leaky-integrate-and-fire (LIF) neuron properties were simulated by considering a simple RC circuit in order to perform Bayesian extrapolation within a spiking neural network. A classification application is consequently demonstrated by using the liver tumor dataset. The advantageous capabilities of the stochastic-based spike neural networks (SNNs) are highlighted in striking contrast with the conventional artificial neural networks (ANNs), as well as the deterministic-based SNNs, in terms of prediction accuracy and power consumption.
C1 [Bousoulas, P.; Tsioustas, C.; Hadfield, J.; Aslanidis, V; Limberopoulos, S.; Tsoukalas, D.] Natl Tech Univ Athens, Sch Appl Math & Phys Sci, Athens 15780, Greece.
RP Bousoulas, P (corresponding author), Natl Tech Univ Athens, Sch Appl Math & Phys Sci, Athens 15780, Greece.
EM panbous@mail.ntua.gr
CR [Anonymous], 2011, INT J DATABASE MANAG, DOI DOI 10.5121/IJDMS.2011.3207
   Asoro MA, 2009, MICROSC MICROANAL, V15, P706, DOI 10.1017/S1431927609097013
   Avramescu ML, 2017, ENVIRON SCI POLLUT R, V24, P1553, DOI 10.1007/s11356-016-7932-2
   Azouz R, 2000, P NATL ACAD SCI USA, V97, P8110, DOI 10.1073/pnas.130200797
   Begoli E, 2019, NAT MACH INTELL, V1, P20, DOI 10.1038/s42256-018-0004-1
   Bousoulas P, 2021, APPL PHYS LETT, V118, DOI 10.1063/5.0044647
   Bousoulas P, 2016, J APPL PHYS, V120, DOI 10.1063/1.4964872
   Bousoulas P., LOW POWER STOCHAST 1
   Bousoulas P, 2020, NANOTECHNOLOGY, V31, DOI 10.1088/1361-6528/aba3a1
   Cai FX, 2019, NAT ELECTRON, V2, P290, DOI 10.1038/s41928-019-0270-x
   Cannon RC, 2010, PLOS COMPUT BIOL, V6, DOI 10.1371/journal.pcbi.1000886
   Carneiro JO, 2019, COATINGS, V9, DOI 10.3390/coatings9080468
   Chang SH, 2008, APPL PHYS LETT, V92, DOI 10.1063/1.2924304
   Choi S, 2017, NANO LETT, V17, P3113, DOI 10.1021/acs.nanolett.7b00552
   CORDERO GG, 2016, SEMICOND SCI TECH, V31
   Ham D, 2021, NAT ELECTRON, V4, P635, DOI 10.1038/s41928-021-00646-1
   Huang HM, 2020, ADV INTELL SYST-GER, V2, DOI 10.1002/aisy.202000149
   Isacoff EY, 2013, NEURON, V80, P658, DOI 10.1016/j.neuron.2013.10.040
   Lacy F, 2009, IEEE SENS J, V9, P1111, DOI 10.1109/JSEN.2009.2026514
   Li HT, 2017, IEEE T CIRCUITS-I, V64, P2263, DOI 10.1109/TCSI.2017.2709812
   Li JX, 2020, MATER HORIZ, V7, P71, DOI 10.1039/c9mh01206k
   Luo WH, 2008, J PHYS CHEM C, V112, P2359, DOI 10.1021/jp0770155
   Morrison A, 2008, BIOL CYBERN, V98, P459, DOI 10.1007/s00422-008-0233-1
   Nanda KK, 2002, PHYS REV A, V66, DOI 10.1103/PhysRevA.66.013208
   Naundorf B, 2006, NATURE, V440, P1060, DOI 10.1038/nature04610
   Nessler B, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003037
   Papakonstantinopoulos C, 2021, ACS APPL ELECTRON MA, V3, P2729, DOI 10.1021/acsaelm.1c00302
   Pouget A, 2013, NAT NEUROSCI, V16, P1170, DOI 10.1038/nn.3495
   RAVINDRA NM, 1986, J APPL PHYS, V60, P1139, DOI 10.1063/1.337358
   Sakellaropoulos D, 2020, IEEE ELECTR DEVICE L, V41, P1013, DOI 10.1109/LED.2020.2997565
   Serb A, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms12611
   Sheny DS, 2013, SPECTROCHIM ACTA A, V114, P267, DOI 10.1016/j.saa.2013.05.028
   Sun HT, 2014, ADV FUNCT MATER, V24, P5679, DOI 10.1002/adfm.201401304
   Yang YC, 2012, NAT COMMUN, V3, DOI 10.1038/ncomms1737
   Zhao Y, 2018, IEEE T ELECTRON DEV, V65, P4290, DOI 10.1109/TED.2018.2865225
NR 35
TC 5
Z9 5
U1 1
U2 12
PD MAY
PY 2022
VL 69
IS 5
BP 2368
EP 2376
DI 10.1109/TED.2022.3160140
EA MAR 2022
WC Engineering, Electrical & Electronic; Physics, Applied
DA 2023-11-11
ER

PT J
AU Chakraborty, A
   Panda, S
   Chakrabarti, S
AF Chakraborty, Ayan
   Panda, Sashmita
   Chakrabarti, Saswat
TI Action Potential Parameters and Spiking Behavior of Cortical Neurons: A
   Statistical Analysis for Designing Spiking Neural Networks
SO IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS
DT Article
DE Action potential (AP); energy per spike; hypothesis testing; information
   gain; interspike interval (ISI); k-means clustering; Kullback-Leibler
   (KL) divergence; spiking frequency
ID FIRING FREQUENCY
AB Cortical neurons exhibit several spiking dynamics both in in-vivo and in-vitro experiments. Neural spikes or action potentials (APs) are also observed in various shapes and forms. Statistical correlation between AP parameters and associated spiking behavior of a neuron is discussed in this article. Three fundamental parameters: 1) width; 2) height; and 3) energy of an AP along with spiking frequency and interspike interval (ISI) are extracted for 91 human cortical neurons selected from Allen Institute for Brain Science (AIBS) database. It has been shown that neurons firing narrow, short, and low-energy APs have higher spiking frequency compared to the neurons with wide and taller APs. For a rise in excitation, it has been presented that information gain for neurons firing wider spikes is less compared to information gain for neurons firing narrow spikes. It has been shown that neurons with low spiking frequency and high spiking frequency dissipate energy of similar order for total spiking activity for similar excitation. Implications of the statistical inferences drawn are explained for a computational model of a spiking neuron. The effect of changing AP width on the overall dynamics of a spiking neural network is also highlighted. The key findings of this study will be useful for designing spiking neural networks for various cognitive applications.
C1 [Chakraborty, Ayan; Panda, Sashmita; Chakrabarti, Saswat] Indian Inst Technol Kharagpur, GS Sanyal Sch Telecommun, Kharagpur 721302, India.
   [Chakrabarti, Saswat] Indian Inst Technol Kharagpur, Fac Biotechnol & Biosci, Kharagpur 721302, India.
RP Chakraborty, A (corresponding author), Indian Inst Technol Kharagpur, GS Sanyal Sch Telecommun, Kharagpur 721302, India.
EM chakraborty.ayan.1991@gmail.com; sashmita@iitkgp.ac.in;
   saswat@ece.iitkgp.ac.in
CR Adonias GL, 2020, FRONT COMPUT NEUROSC, V14, DOI 10.3389/fncom.2020.556628
   Adonias GL, 2020, IEEE T NANOBIOSCI, V19, P224, DOI 10.1109/TNB.2020.2975942
   Aghababaiyan K, 2018, IEEE T NANOBIOSCI, V17, P78, DOI 10.1109/TNB.2018.2800899
   [Anonymous], 2016, ALL CELL TYP DAT
   Baranauskas G, 2003, NAT NEUROSCI, V6, P258, DOI 10.1038/nn1019
   Bean BP, 2007, NAT REV NEUROSCI, V8, P451, DOI 10.1038/nrn2148
   Berger T, 2010, IEEE T INFORM THEORY, V56, P852, DOI 10.1109/TIT.2009.2037089
   Boddum K, 2017, NEUROPHARMACOLOGY, V118, P102, DOI 10.1016/j.neuropharm.2017.02.024
   Burman I., 2021, P IEEE NAT C COMM NC, P1
   Chakraborty A, 2021, I IEEE EMBS C NEUR E, P77, DOI 10.1109/NER49283.2021.9441251
   Chakraborty A, 2021, I IEEE EMBS C NEUR E, P734, DOI 10.1109/NER49283.2021.9441230
   Chistiakova M, 2019, J NEUROSCI, V39, P6865, DOI 10.1523/JNEUROSCI.3039-18.2019
   Cook ND, 2008, NEUROSCIENCE, V153, P556, DOI 10.1016/j.neuroscience.2008.02.042
   Dayan P., 2001, THEORETICAL NEUROSCI
   Druckmann S, 2013, CEREB CORTEX, V23, P2994, DOI 10.1093/cercor/bhs290
   Ganguly C, 2020, IEEE T NEUR SYS REH, V28, P772, DOI 10.1109/TNSRE.2020.2975203
   Ghavami Siavash, 2018, IEEE Transactions on Molecular, Biological, and Multi-Scale Communications, V4, P221, DOI 10.1109/TMBMC.2019.2937291
   Goldental A, 2014, FRONT COMPUT NEUROSC, V8, DOI 10.3389/fncom.2014.00052
   Gouwens NW, 2019, NAT NEUROSCI, V22, P1182, DOI 10.1038/s41593-019-0417-0
   Hernáth F, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-49197-8
   Hore A., 2021, P IEEE ADV COMM TECH, P1
   Ikeda K, 2005, NEURAL COMPUT, V17, P2719, DOI 10.1162/089976605774320593
   Lin XH, 2023, IEEE T COGN DEV SYST, V15, P16, DOI 10.1109/TCDS.2021.3140115
   Mehta MR, 2002, NATURE, V417, P741, DOI 10.1038/nature00807
   Nowak LG, 2003, J NEUROPHYSIOL, V89, P1541, DOI 10.1152/jn.00580.2002
   Ramezani H, 2018, IEEE T NANOBIOSCI, V17, P260, DOI 10.1109/TNB.2018.2838056
   Sun SH, 2021, J PHYSIOL-LONDON, V599, P2211, DOI 10.1113/JP280844
   Tateno T, 2004, J NEUROPHYSIOL, V92, P2283, DOI 10.1152/jn.00109.2004
   Veletic M, 2019, P IEEE, V107, P1425, DOI 10.1109/JPROC.2019.2915199
   Wijesinghe P, 2018, IEEE TETCI, V2, P345, DOI 10.1109/TETCI.2018.2829924
   Wu JB, 2023, IEEE T NEUR NET LEAR, V34, P446, DOI 10.1109/TNNLS.2021.3095724
NR 31
TC 0
Z9 0
U1 2
U2 2
PD JUN
PY 2023
VL 15
IS 2
BP 808
EP 818
DI 10.1109/TCDS.2022.3185028
WC Computer Science, Artificial Intelligence; Robotics; Neurosciences
DA 2023-11-11
ER

PT J
AU Gelen, AG
   Atasoy, A
AF Gelen, Aykut Gorkem
   Atasoy, Ayten
TI SPAYK: An environment for spiking neural network simulation
SO TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES
DT Article
DE Spiking neural network; STDP based learning; supervised classification;
   unsupervised pattern recognition
AB In research areas such as mobile robotics and computer vision, energy and computational efficiency have become critical. This has greatly increased interest in high-efficiency neuromorphic hardware and spiking neural networks. Because neuromorphic hardware is not yet widely available, spiking neural network studies are conducted by simulations. There are numerous simulators available today, each designed for a specific purpose. In this paper, a novel and open -source package (SPAYK) for simulating spiking neural networks is presented. SPAYK has been proposed to speed up spiking neural network research. In the majority of simulators, networks are expressed with differential equations and require advanced neuroscience knowledge since such simulators are generally designed for brain and neuroscience research. SPAYK, on the other hand, is specifically designed as a framework to easily design spiking neural networks for practical problems. SPAYK is an easy-to-use Python package. There are three fundamental classes in the core: the model class for creating neuron groups, the organization class for simulating tissues, and the learning class for synaptic plasticity. While developing and testing the SPAYK environment, various experiments were carried out. This study includes three of these experiments. In the first experiment, we investigated the behavior of a group of Izhikevich neurons for visual stimuli. Also, a single Izhikevich neuron has been trained to respond to a particular label in a supervised manner with synaptic plasticity. In the second experiment, a well-known experiment was repeated to validate SPAYK. In this experiment, a neuron trained by synaptic plasticity can recognize repetitive patterns in a spike train. In the third experiment, a similar neuron was simulated with stimuli with multiple labels adapted from the MNIST dataset. It has been shown that the neuron can classify a particular label by synaptic plasticity. All these experiments and the SPAYK environment are presented as open-source tools.
C1 [Gelen, Aykut Gorkem] Erzincan Binali Yildirim Univ, Dept Elect & Elect Engn, Erzincan, Turkiye.
   [Atasoy, Ayten] Karadeniz Tech Univ, Dept Elect & Elect Engn, Trabzon, Turkiye.
RP Gelen, AG (corresponding author), Erzincan Binali Yildirim Univ, Dept Elect & Elect Engn, Erzincan, Turkiye.
EM aykut.gelen@erzincan.edu.tr
CR Bekolay T, 2014, FRONT NEUROINFORM, V7, DOI 10.3389/fninf.2013.00048
   Cao ZQ, 2015, NEURAL COMPUT APPL, V26, P1839, DOI 10.1007/s00521-015-1848-5
   Cheng X, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1519
   Choudhary T, 2018, INT CONF INTEL INFOR, P1
   Eliasmith C, 2002, NEURAL ENG COMPUTATI
   Fang WAC, SPIKINGJELLY
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   Gerstner W, 2018, SCHOLARPEDIA, V3, P1343
   Gewaltig M-O., 2007, SCHOLARPEDIA, V2, DOI [10.4249/scholarpedia.1430, DOI 10.4249/SCHOLARPEDIA.1430]
   Goodman Dan, 2008, Front Neuroinform, V2, P5, DOI 10.3389/neuro.11.005.2008
   HEBB D. O., 1949
   Hines ML, 2001, NEUROSCIENTIST, V7, P123, DOI 10.1177/107385840100700207
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Kim S, 2020, AAAI CONF ARTIF INTE, V34, P11270
   Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508
   Li XM, 2020, NEURAL PLAST, V2020, DOI 10.1155/2020/8851351
   Liu JX, 2018, PROCEEDINGS OF 2018 10TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND COMPUTING (ICMLC 2018), P230, DOI 10.1145/3195106.3195115
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Masquelier T, 2008, PLOS ONE, V3, DOI 10.1371/journal.pone.0001377
   Mozafari M, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00625
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Skocik MJ, 2014, IEEE T NEUR NET LEAR, V25, P1474, DOI 10.1109/TNNLS.2013.2294016
   Tikidji-Hamburyan RA, 2017, FRONT NEUROINFORM, V11, DOI 10.3389/fninf.2017.00046
   Vitay J, 2015, FRONT NEUROINFORM, V9, DOI 10.3389/fninf.2015.00019
   Wang XW, 2020, NEURAL NETWORKS, V125, P258, DOI 10.1016/j.neunet.2020.02.011
   Wilson MA, 1989, GENESIS SYSTEM SIMUL, P485
   Wu JB, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00836
   Yan ZL, 2021, BIOMED SIGNAL PROCES, V63, DOI 10.1016/j.bspc.2020.102170
NR 29
TC 0
Z9 0
U1 5
U2 6
PY 2023
VL 31
IS 2
BP 462
EP 480
DI 10.55730/1300-0632.3995
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
DA 2023-11-11
ER

PT C
AU Hori, S
   Zapata, M
   Madrenas, J
   Morie, T
   Tamukoh, H
AF Hori, Sansei
   Zapata, Mireya
   Madrenas, Jordi
   Morie, Takashi
   Tamukoh, Hakaru
BE Lintas, A
   Rovetta, S
   Verschure, PFMJ
   Villa, AEP
TI An Implementation of a Spiking Neural Network Using Digital Spiking
   Silicon Neuron Model on a SIMD Processor
SO ARTIFICIAL NEURAL NETWORKS AND MACHINE LEARNING - ICANN 2017, PT I
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 26th International Conference on Artificial Neural Networks (ICANN)
CY SEP 11-14, 2017
CL Alghero, ITALY
DE SNN; DSSN; SIMD processor; FPGA
C1 [Hori, Sansei; Morie, Takashi; Tamukoh, Hakaru] Kyushu Inst Technol, Grad Sch Life Sci & Syst Engn, Kitakyushu, Fukuoka, Japan.
   [Zapata, Mireya; Madrenas, Jordi] Univ Politecn Cataluna, Dept Elect Engn, Barcelona, Spain.
RP Hori, S (corresponding author), Kyushu Inst Technol, Grad Sch Life Sci & Syst Engn, Kitakyushu, Fukuoka, Japan.
EM hori-sansei@edu.brain.kyutech.ac.jp; mireya.zapata@upc.edu;
   jordi.madrenas@upc.edu; morie@brain.kyutech.ac.jp;
   tamukoh@brain.kyutech.ac.jp
CR Li J, 2012, FRONT NEUROSCI-SWITZ, V6, DOI 10.3389/fnins.2012.00183
   Zapata M, 2016, LECT NOTES COMPUT SC, V9886, P365, DOI 10.1007/978-3-319-44778-0_43
NR 2
TC 1
Z9 1
U1 0
U2 0
PY 2017
VL 10613
BP 437
EP 438
PN I
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods
DA 2023-11-11
ER

PT C
AU Meftah, B
   Benyettou, A
   Lezoray, O
   QingXiang, W
AF Meftah, B.
   Benyettou, A.
   Lezoray, O.
   QingXiang, W.
GP IEEE
TI Image Clustering with Spiking Neuron Network
SO 2008 IEEE INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS, VOLS 1-8
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks
CY JUN 01-08, 2008
CL Hong Kong, PEOPLES R CHINA
AB The process of segmenting images is one of the most critical ones in automatic image analysis whose goal can be regarded as to find what objects are presented in images. Artificial neural networks have been well developed. First two generations of neural networks have a lot of successful applications. Spiking Neuron Networks (SNNs) are often referred to as the 3(rd) generation of neural networks which have potential to solve problems related to biological stimuli. They derive their strength and interest from an accurate modeling of synaptic interactions between neurons, taking into account the time of spike emission. SNNs overcome the computational power of neural networks made of threshold or sigmoidal units. Moreover, SNNs add a new dimension, the temporal axis, to the representation capacity and the processing abilities of neural networks. In this paper, we present how SNN can be applied with efficacy in image segmentation.
C1 [Meftah, B.] Ctr Univ Mustapha Stambouli, Equipe EDTEC LRSBG, Mascara, Algeria.
   [Benyettou, A.] Univ Mohamed Boudiaf, Lab Signal Image & Parole SIMPA, Oran, Algeria.
   [Lezoray, O.] Univ Caen, CNRS, GREYC, UMR 6072, F-14050 Caen, France.
   [QingXiang, W.] Univ Ulster Magee Derry, Sch Comp & Intelligent Syst, Londonderry BT48 7JL, North Ireland.
RP Meftah, B (corresponding author), Ctr Univ Mustapha Stambouli, Equipe EDTEC LRSBG, Mascara, Algeria.
CR [Anonymous], 1988, CAMBRIDGE STUD MATH
   Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   BRAGA TBL, 2000, ARTIFICIAL NEURAL NE
   Dayan P., 2004, THEORETICAL NEUROSCI
   Gerstner W., 2002, SPIKING NEURON MODEL
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Maass W, 2001, THEOR COMPUT SCI, V261, P157, DOI 10.1016/S0304-3975(00)00137-7
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Meurie C, 2005, INT J ROBOT AUTOM, V20, P63, DOI 10.2316/Journal.206.2005.2.206-2780
   OSTER M, 2004, P 11 IEEE INT C EL C, V11, P203
   Paugam-Moisy H, 2006, SPIKING NEURON NETWO
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   Zhang YJ, 2001, ISSPA 2001: SIXTH INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND ITS APPLICATIONS, VOLS 1 AND 2, PROCEEDINGS, P148, DOI 10.1109/ISSPA.2001.949797
NR 13
TC 5
Z9 6
U1 0
U2 3
PY 2008
BP 681
EP 685
DI 10.1109/IJCNN.2008.4633868
WC Computer Science, Artificial Intelligence; Computer Science,
   Cybernetics; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Kuroe, Y
   Iima, H
   Maeda, Y
AF Kuroe, Yasuaki
   Iima, Hitoshi
   Maeda, Yutaka
BE Merelo, JJ
   Garibaldi, J
   Barranco, AL
   Madani, K
   Warwick, K
TI Learning Method of Recurrent Spiking Neural Networks to Realize Various
   Firing Patterns using Particle Swarm Optimization
SO IJCCI: PROCEEDINGS OF THE 11TH INTERNATIONAL JOINT CONFERENCE ON
   COMPUTATIONAL INTELLIGENCE
DT Proceedings Paper
CT 11th International Joint Conference on Computational Intelligence
   (IJCCI)
CY SEP 17-19, 2019
CL Vienna, AUSTRIA
DE Spiking Neural Network; Learning Method; Particle Swarm Optimization;
   Burst Firing; Periodic Firing
AB Recently it has been reported that artificial spiking neural networks (SNNs) are computationally more powerful than the conventional neural networks. In biological neural networks of living organisms, various firing patterns of nerve cells have been observed, typical examples of which are burst firings and periodic firings. In this paper we propose a learning method which can realize various firing patterns for recurrent SNNs (RSSNs). We have already proposed learning methods of RSNNs in which the learning problem is formulated such that the number of spikes emitted by a neuron and their firing instants coincide with given desired ones. In this paper, in addition to that, we consider several desired properties of a target RSNN and proposes cost functions for realizing them. Since the proposed cost functions are not differentiable with respect to the learning parameters, we propose a learning method based on the particle swarm optimization.
C1 [Kuroe, Yasuaki; Maeda, Yutaka] Kansai Univ, Fac Engn Sci, Suita, Osaka, Japan.
   [Kuroe, Yasuaki; Iima, Hitoshi] Kyoto Inst Technol, Fac Informat & Human Sci, Kyoto, Japan.
RP Kuroe, Y (corresponding author), Kansai Univ, Fac Engn Sci, Suita, Osaka, Japan.; Kuroe, Y (corresponding author), Kyoto Inst Technol, Fac Informat & Human Sci, Kyoto, Japan.
CR [Anonymous], 2010, DYNAMICAL SYSTEMS NE
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   GERSTNER W, 1993, BIOL CYBERN, V69, P503, DOI 10.1007/BF01185422
   GERSTNER W, 1993, ADV NEURAL INFORM PR, V6, P363
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Kennedy J., 1995, SWARM INTELL-US, V4, P1942
   Kuroe Y., 2010, P INT JOINT C NEUR N, P2561
   Kuroe Y., 1992, ISCIE CONTROL INFORM, V36, P634
   Lee JM, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00191
   Maass W, 1997, NEURAL COMPUT, V9, P279, DOI 10.1162/neco.1997.9.2.279
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Mass W, 1996, COMPUTATIONAL COMPLE, V7
   MASS W, 1998, PULSED NEURAL NETS
   Rumelhart D.E., 1986, PARALLEL DISTRIBUTED, V1, DOI DOI 10.7551/MITPRESS/5236.001.0001
   Rumelhart DE., 1988, PARALLEL DISTRIBUTED
   Selvaratnam K., 2000, Transactions of the Institute of Systems, Control and Information Engineers, V13, P95, DOI 10.5687/iscie.13.3_95
NR 16
TC 0
Z9 0
U1 0
U2 0
PY 2019
BP 479
EP 486
DI 10.5220/0008164704790486
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT J
AU Pande, S
   Morgan, F
   Smit, G
   Bruintjes, T
   Rutgers, J
   McGinley, B
   Cawley, S
   Harkin, J
   McDaid, L
AF Pande, Sandeep
   Morgan, Fearghal
   Smit, Gerard
   Bruintjes, Tom
   Rutgers, Jochem
   McGinley, Brian
   Cawley, Seamus
   Harkin, Jim
   McDaid, Liam
TI Fixed latency on-chip interconnect for hardware spiking neural network
   architectures
SO PARALLEL COMPUTING
DT Article
DE Network on Chip (NoC); Spiking Neural Networks (SNN); Synaptic
   connectivity; Latency jitter
ID NEURONS; PLATFORM; DESIGN
AB Information in a Spiking Neural Network (SNN) is encoded as the relative timing between spikes. Distortion in spike timings can impact the accuracy of SNN operation by modifying the precise firing time of neurons within the SNN. Maintaining the integrity of spike timings is crucial for reliable operation of SNN applications. A packet switched Network on Chip (NoC) infrastructure offers scalable connectivity for spike communication in hardware SNN architectures. However, shared resources in NoC architectures can result in unwanted variation in spike packet transfer latency. This packet latency jitter distorts the timing information conveyed on the synaptic connections in the SNN, resulting in unreliable application behaviour.
   This paper presents a SystemC simulation based analysis of the synaptic information distortion in NoC based hardware SNNs. The paper proposes a fixed spike transfer latency ring topology interconnect for spike communication between neural tiles, using a novel time-stamped spike broadcast flow control scheme. The proposed architectural technique is evaluated using spike rates employed in previously reported mesh topology NoC based hardware SNN applications, which exhibited spike latency jitter over NoC paths. Results indicate that the proposed interconnect offers fixed spike transfer latency and eliminates the associated information distortion.
   The paper presents the micro-architecture of the proposed ring router. The FPGA validated ring interconnect architecture has been synthesised using 65 nm low-power CMOS technology. Silicon area comparisons for various ring sizes are presented. Scalability of the proposed architecture has been addressed by employing a hierarchical NoC architecture. (C) 2013 Elsevier B.V. All rights reserved.
C1 [Pande, Sandeep; Morgan, Fearghal; McGinley, Brian; Cawley, Seamus] Natl Univ Ireland, Galway, Ireland.
   [Smit, Gerard; Bruintjes, Tom; Rutgers, Jochem] Univ Twente, NL-7500 AE Enschede, Netherlands.
RP Pande, S (corresponding author), Natl Univ Ireland, Galway, Ireland.
EM sandeep.pande@nuigalway.ie; fearghal.morgan@nuigalway.ie;
   g.j.m.smit@utwente.nl; t.m.bruintje-s@utwente.nl;
   j.h.rutgers@utwente.nl; brian.mcginley@nuigalway.ie;
   s.cawley6@nuigalway.ie; jg.harkin@ulster.ac.uk; lj.mcdaid@ulster.ac.uk
CR [Anonymous], ACM COMPUTING SURVEY
   [Anonymous], 1999, NEURAL NETWORKS COMP
   Auda G, 1999, Int J Neural Syst, V9, P129, DOI 10.1142/S0129065799000125
   Benini L, 2001, ISSS'01: 14TH INTERNATIONAL SYMPOSIUM ON SYSTEM SYNTHESIS, P33, DOI 10.1109/ISSS.2001.957909
   Benini L, 2002, COMPUTER, V35, P70, DOI 10.1109/2.976921
   Bohte SM, 2005, INFORM PROCESS LETT, V95, P519, DOI 10.1016/j.ipl.2005.05.018
   Bolotin E, 2004, J SYST ARCHITECT, V50, P105, DOI 10.1016/j.sysarc.2003.07.004
   Cawley S, 2011, GENET PROGRAM EVOL M, V12, P257, DOI 10.1007/s10710-011-9130-9
   Dall'Osso M, 2003, PR IEEE COMP DESIGN, P536, DOI 10.1109/ICCD.2003.1240952
   Dally W., 2003, PRINCIPLES PRACTICES
   Ehrlich M, 2007, P INT C SENS CIRC IN
   Emery R, 2009, 2009 3RD ACM/IEEE INTERNATIONAL SYMPOSIUM ON NETWORKS-ON-CHIP, P144, DOI 10.1109/NOCS.2009.5071462
   Furber S, 2009, NINTH INTERNATIONAL CONFERENCE ON APPLICATION OF CONCURRENCY TO SYSTEM DESIGN, PROCEEDINGS, P3, DOI 10.1109/ACSD.2009.17
   Gerstner W, 1997, P NATL ACAD SCI USA, V94, P12740, DOI 10.1073/pnas.94.24.12740
   Gerstner W., 2002, SPIKING NEURON MODEL
   Glackin B, 2005, LECT NOTES COMPUTER, V3512, P1
   Goossens K, 2005, IEEE DES TEST COMPUT, V22, P414, DOI 10.1109/MDT.2005.99
   HAPPEL BLM, 1994, NEURAL NETWORKS, V7, P985, DOI 10.1016/S0893-6080(05)80155-8
   Harkin J, 2009, INT J RECONFIGURABLE, V2009, DOI 10.1155/2009/908740
   Kepa K, 2009, I C FIELD PROG LOGIC, P403, DOI 10.1109/FPL.2009.5272250
   Kim John, 2009, Proceedings of the 2009 2nd International Workshop on Network on Chip Architectures (NoCArc 2009), P5, DOI 10.1145/1645213.1645217
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maguire LP, 2007, NEUROCOMPUTING, V71, P13, DOI 10.1016/j.neucom.2006.11.029
   Pande Sandeep, 2010, Proceedings 2010 International Symposium on System-on-Chip - SOC, P139, DOI 10.1109/ISSOC.2010.5625566
   Pande S., 2013, NEURAL PROCESS LETT, V1370-4621, P1
   Pearson MJ, 2007, IEEE T NEURAL NETWOR, V18, P1472, DOI 10.1109/TNN.2007.891203
   Rieke F., 1999, SPIKES EXPLORING NEU
   Ronco E, 1995, CSC95026 U GLASG, V1, P1
   Ros E, 2006, IEEE T NEURAL NETWOR, V17, P1050, DOI 10.1109/TNN.2006.875980
   Salminen E, 2007, DSD 2007: 10TH EUROMICRO CONFERENCE ON DIGITAL SYSTEM DESIGN ARCHITECTURES, METHODS AND TOOLS, PROCEEDINGS, P503, DOI 10.1109/DSD.2007.4341515
   Schemmel J, 2008, IEEE IJCNN, P431, DOI 10.1109/IJCNN.2008.4633828
   Strogatz SH, 2001, NATURE, V410, P268, DOI 10.1038/35065725
   Theocharides T, 2004, IEEE INT SOC CONF, P191, DOI 10.1109/SOCC.2004.1362404
   Thorpe S, 1996, NATURE, V381, P520, DOI 10.1038/381520a0
   Upegui A, 2005, MICROPROCESS MICROSY, V29, P211, DOI 10.1016/j.micpro.2004.08.012
   Vainbrand D, 2011, MICROPROCESS MICROSY, V35, P152, DOI 10.1016/j.micpro.2010.08.005
   Vogelstein RJ, 2007, IEEE T NEURAL NETWOR, V18, P253, DOI 10.1109/TNN.2006.883007
   Watts DJ, 1998, NATURE, V393, P440, DOI 10.1038/30918
   Wiklund D., 2003, Proceedings International Parallel and Distributed Processing Symposium, DOI 10.1109/IPDPS.2003.1213180
NR 39
TC 24
Z9 24
U1 2
U2 18
PD SEP
PY 2013
VL 39
IS 9
SI SI
BP 357
EP 371
DI 10.1016/j.parco.2013.04.010
WC Computer Science, Theory & Methods
DA 2023-11-11
ER

PT C
AU Zhan, GG
   Song, ZT
   Fang, T
   Zhang, Y
   Le, S
   Zhang, XZ
   Wang, SY
   Lin, YF
   Jia, J
   Zhang, LH
   Kang, XY
AF Zhan, Gege
   Song, Zuoting
   Fang, Tao
   Zhang, Yuan
   Le, Song
   Zhang, Xueze
   Wang, Shouyan
   Lin, Yifang
   Jia, Jie
   Zhang, Lihua
   Kang, Xiaoyang
GP IEEE
TI Applications of Spiking Neural Network in Brain Computer Interface
SO 2021 9TH IEEE INTERNATIONAL WINTER CONFERENCE ON BRAIN-COMPUTER
   INTERFACE (BCI)
SE International Winter Workshop on Brain-Computer Interface
DT Proceedings Paper
CT 9th IEEE International Winter Conference on Brain-Computer Interface
   (BCI)
CY FEB 22-24, 2021
CL Korea Univ Inst Artificial Intelligence, ELECTR NETWORK
HO Korea Univ Inst Artificial Intelligence
DE Spiking neural network (SNN); BCI; EEG; Brain Disease; Motor Imagery
ID EEG DATA; CLASSIFICATION; RECOGNITION; METHODOLOGY; NEUCUBE; PATTERN;
   MODELS
AB Spiking neural network (SNN) is regarded as the third generation of the artificial neural network, which takes biologically plausible spiking neurons as the basic computing unit. Due to its ability to capture the rich dynamics of biological neurons and to represent and integrate different information dimensions, such as time, frequency and phase, SNN provides a powerful tool for modeling complex information processing in the brain. EEG can help us better understand brain activity and structure, and shows great potential in implementing Brain-Computer Interface (BCI). In this review, we mainly summarize the application of the SNN model in EEG signal processing. These applications are grouped into four categories, each of which is further explored using examples from previous studies.
C1 [Zhan, Gege; Song, Zuoting; Fang, Tao; Zhang, Yuan; Le, Song; Zhang, Xueze; Zhang, Lihua; Kang, Xiaoyang] Fudan Univ, Acad Engn & Technol,Minist Educ, Engn Res Ctr AI & Robot,MOE Frontiers Ctr Brain S, Shanghai Engn Res Ctr AI & Robot,Inst AI & Robot, Shanghai, Peoples R China.
   [Wang, Shouyan] Fudan Univ, Inst Sci & Technol Brain Inspired Intelligence, Shanghai, Peoples R China.
   [Lin, Yifang; Jia, Jie] Fudan Univ, Huashan Hosp, Dept Rehabil Med, Shanghai, Peoples R China.
   [Zhang, Lihua; Kang, Xiaoyang] Ji Hua Lab, Foshan, Guangdong, Peoples R China.
   [Kang, Xiaoyang] Res Ctr Intelligent Sensing, Zhejiang Lab, Hangzhou 311100, Peoples R China.
RP Kang, XY (corresponding author), Fudan Univ, Acad Engn & Technol,Minist Educ, Engn Res Ctr AI & Robot,MOE Frontiers Ctr Brain S, Shanghai Engn Res Ctr AI & Robot,Inst AI & Robot, Shanghai, Peoples R China.; Kang, XY (corresponding author), Ji Hua Lab, Foshan, Guangdong, Peoples R China.; Kang, XY (corresponding author), Res Ctr Intelligent Sensing, Zhejiang Lab, Hangzhou 311100, Peoples R China.
EM xiaoyang_kang@fudan.edu.cn
CR Behrenbeck J, 2019, J NEURAL ENG, V16, DOI 10.1088/1741-2552/aafabc
   Beyeler M, 2013, NEURAL NETWORKS, V48, P109, DOI 10.1016/j.neunet.2013.07.012
   Capecci E, 2016, IEEE IJCNN, P1360, DOI 10.1109/IJCNN.2016.7727356
   Capecci E, 2015, NEURAL NETWORKS, V68, P62, DOI 10.1016/j.neunet.2015.03.009
   Carino-Escobar R, 2016, LECT NOTES COMPUT SC, V9713, P245, DOI 10.1007/978-3-319-41009-8_26
   Doborjeh MG, 2018, EVOL SYST-GER, V9, P195, DOI 10.1007/s12530-017-9178-8
   Doborjeh MG, 2016, IEEE IJCNN, P1373, DOI 10.1109/IJCNN.2016.7727358
   Doborjeh MG, 2016, IEEE T BIO-MED ENG, V63, P1830, DOI 10.1109/TBME.2015.2503400
   Doborjeh ZG, 2018, COGN COMPUT, V10, P35, DOI 10.1007/s12559-017-9517-x
   Gerstner W., 2002, SPIKING NEURON MODEL
   Ghosh-Dastidar S, 2007, INTEGR COMPUT-AID E, V14, P187
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Goel P, 2006, LECT NOTES ARTIF INT, V4251, P825
   Goel P, 2008, INT J KNOWL-BASED IN, V12, P295, DOI 10.3233/KES-2008-12404
   Gütig R, 2014, CURR OPIN NEUROBIOL, V25, P134, DOI 10.1016/j.conb.2014.01.004
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Kasabov N. K., 2019, SPRINGER SERIES BIO, P169
   Kasabov N, 2015, INFORM SCIENCES, V294, P565, DOI 10.1016/j.ins.2014.06.028
   Kasabov NK, 2014, NEURAL NETWORKS, V52, P62, DOI 10.1016/j.neunet.2014.01.006
   Kawano H, 2016, LECT NOTES COMPUT SC, V9950, P221, DOI 10.1007/978-3-319-46681-1_27
   Kulkarni SR, 2018, NEURAL NETWORKS, V103, P118, DOI 10.1016/j.neunet.2018.03.019
   Lin Xiang-hong, 2009, Acta Electronica Sinica, V37, P1270
   Luo Y., 2020, ENVIRON SCI POLLUT R, VPP, P1, DOI DOI 10.1007/s11356-020-08864-4
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Mashford BS, 2017, IBM J RES DEV, V61, DOI 10.1147/JRD.2017.2663978
   Niranjani AN, 2017, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON INTELLIGENT SUSTAINABLE SYSTEMS (ICISS 2017), P901, DOI 10.1109/ISS1.2017.8389309
   Nuntalid N, 2011, LECT NOTES COMPUT SC, V7062, P451, DOI 10.1007/978-3-642-24955-6_54
   Tan C, 2020, NEURAL PROCESS LETT, V52, P1675, DOI 10.1007/s11063-020-10322-8
   Virgilio GCD, 2019, LECT NOTES COMPUT SC, V11524, P14, DOI 10.1007/978-3-030-21077-9_2
   Virgilio CD, 2020, NEURAL NETWORKS, V122, P130, DOI 10.1016/j.neunet.2019.09.037
   Wang HT, 2020, IEEE ACCESS, V8, P86850, DOI 10.1109/ACCESS.2020.2992631
   Wang XW, 2020, NEURAL NETWORKS, V125, P258, DOI 10.1016/j.neunet.2020.02.011
NR 32
TC 3
Z9 3
U1 3
U2 16
PY 2021
BP 288
EP 293
DI 10.1109/BCI51272.2021.9385361
WC Computer Science, Cybernetics; Neuroimaging
DA 2023-11-11
ER

PT S
AU Hosaka, R
   Ikeguchi, T
   Aihara, K
AF Hosaka, Ryosuke
   Ikeguchi, Tohru
   Aihara, Kazuyuki
BE King, I
   Wang, J
   Chan, L
   Wang, DL
TI Self-organizing rhythmic patterns with spatio-temporal spikes in class I
   and class II neural networks
SO NEURAL INFORMATION PROCESSING, PT 1, PROCEEDINGS
SE LECTURE NOTES IN COMPUTER SCIENCE
DT Article; Proceedings Paper
CT 13th International Conference on Neural Informational Processing
CY OCT 03-06, 2006
CL Hong Kong, PEOPLES R CHINA
ID NEURONS; OSCILLATIONS
AB Regularly spiking neurons are classified into two categories, Class I and Class II, by their firing properties for constant inputs. To investigate how the firing properties of single neurons affect to ensemble rhythmic activities in neural networks, we constructed different types of neural networks whose excitatory neurons are the Class I neurons or the Class II neurons. The networks were driven by random inputs and developed with STDP learning. As a result, the Class I and the Class II neural networks generate different types of rhythmic activities: the Class I neural network generates slow rhythmic activities, and the Class II neural network generates fast rhythmic activities.
C1 Saitama Univ, Grad Sch Sci & Engn, Sakura Ku, Urawa, Saitama 3388570, Japan.
   JST, ERATO, Aihara Complex Modelling Project, Shibuya Ku, Tokyo 1510064, Japan.
   Univ Tokyo, Inst Ind Sci, Meguro Ku, Tokyo 1538505, Japan.
RP Hosaka, R (corresponding author), Saitama Univ, Grad Sch Sci & Engn, Sakura Ku, 255 Shimo Ohkubo, Urawa, Saitama 3388570, Japan.
EM hosaka@nls.ics.saitama-u.ac.jp
CR Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Buzsáki G, 2004, SCIENCE, V304, P1926, DOI 10.1126/science.1099745
   CONNORS BW, 1990, TRENDS NEUROSCI, V13, P99, DOI 10.1016/0166-2236(90)90185-D
   Gibson JR, 1999, NATURE, V402, P75, DOI 10.1038/47035
   Gray CM, 1996, SCIENCE, V274, P109, DOI 10.1126/science.274.5284.109
   HODGKIN AL, 1948, J PHYSIOL-LONDON, V107, P165, DOI 10.1113/jphysiol.1948.sp004260
   HOSAKA R, 2006, UNPUB NEURAL COMPUTA
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   MIYAKAWA H, 2003, BIOPHYSICS NEURONS
   Nicolelis MAL, 2001, ADV NEURAL POPULATIO
   OKEEFE J, 1993, HIPPOCAMPUS, V3, P317, DOI 10.1002/hipo.450030307
   Rinzel J., 1989, METHODS NEURONAL MOD, P251
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
NR 14
TC 0
Z9 0
U1 0
U2 1
PY 2006
VL 4232
BP 39
EP 48
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods
DA 2023-11-11
ER

PT C
AU Ostrau, C
   Homburg, J
   Klarhorst, C
   Thies, M
   Rückert, U
AF Ostrau, Christoph
   Homburg, Jonas
   Klarhorst, Christian
   Thies, Michael
   Rueckert, Ulrich
BE Farkas, I
   Masulli, P
   Wermter, S
TI Benchmarking Deep Spiking Neural Networks on Neuromorphic Hardware
SO ARTIFICIAL NEURAL NETWORKS AND MACHINE LEARNING, ICANN 2020, PT II
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 29th International Conference on Artificial Neural Networks (ICANN)
CY SEP 15-18, 2020
CL Bratislava, SLOVAKIA
DE Spiking neural networks; Neural architecture search; Benchmark
ID ARCHITECTURE
AB With more and more event-based neuromorphic hardware systems being developed at universities and in industry, there is a growing need for assessing their performance with domain specific measures. In this work, we use the methodology of converting pre-trained non-spiking to spiking neural networks to evaluate the performance loss and measure the energy-per-inference for three neuromorphic hardware systems (BrainScaleS, Spikey, SpiNNaker) and common simulation frameworks for CPU (NEST) and CPU/GPU (GeNN). For analog hardware we further apply a re-training technique known as hardware-in-the-loop training to cope with device mismatch. This analysis is performed for five different networks, including three networks that have been found by an automated optimization with a neural architecture search framework. We demonstrate that the conversion loss is usually below one percent for digital implementations, and moderately higher for analog systems with the benefit of much lower energy-per-inference costs.
C1 [Ostrau, Christoph; Homburg, Jonas; Klarhorst, Christian; Thies, Michael; Rueckert, Ulrich] Bielefeld Univ, Tech Fac, Bielefeld, Germany.
RP Ostrau, C (corresponding author), Bielefeld Univ, Tech Fac, Bielefeld, Germany.
EM costrau@techfak.uni-bielefeld.de
CR Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Davies M, 2019, NAT MACH INTELL, V1, P386, DOI 10.1038/s42256-019-0097-1
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Davison Andrew P, 2008, Front Neuroinform, V2, P11, DOI 10.3389/neuro.11.011.2008
   Diehl Peter U, 2015, 2015 INT JOINT C NEU, P1, DOI [DOI 10.1109/IJCNN.2015.7280696, 10.1109/IJCNN.2015.7280696]
   Furber SB, 2013, IEEE T COMPUT, V62, P2454, DOI 10.1109/TC.2012.142
   Gewaltig M-O., 2007, SCHOLARPEDIA, V2, DOI [10.4249/scholarpedia.1430, DOI 10.4249/SCHOLARPEDIA.1430]
   Goltz J., 2019, FAST DEEP NEUROMORPH, DOI [10.1145/3381755.3381770, DOI 10.1145/3381755.3381770]
   Homburg JD, 2019, LECT NOTES COMPUT SC, V11507, P735, DOI 10.1007/978-3-030-20518-8_61
   Jordan J., 2019, NEST 2 18 0, DOI DOI 10.5281/ZENODO.2605422
   Knight JC, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00941
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Moradi S, 2018, IEEE T BIOMED CIRC S, V12, P106, DOI 10.1109/TBCAS.2017.2759700
   Neckar A, 2019, P IEEE, V107, P144, DOI 10.1109/JPROC.2018.2881432
   Ostrau C., 2020, NEURO INSPIRED COMPU, P1, DOI [10.1145/3381755.3381772, DOI 10.1145/3381755.3381772]
   Ostrau C., 2019, C P 2019 INT C HIGH, DOI [10.1109/HPCS48598.2019.9188207, DOI 10.1109/HPCS48598.2019.9188207]
   Petrovici MA, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0108590
   Pfeil T, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00011
   Rhodes O, 2020, PHILOS T R SOC A, V378, DOI 10.1098/rsta.2019.0160
   Schemmel J, 2010, IEEE INT SYMP CIRC S, P1947, DOI 10.1109/ISCAS.2010.5536970
   Schmitt S, 2017, IEEE IJCNN, P2227, DOI 10.1109/IJCNN.2017.7966125
   Stöckel A, 2017, FRONT COMPUT NEUROSC, V11, DOI 10.3389/fncom.2017.00071
   van Albada SJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00291
   Yavuz E, 2016, SCI REP-UK, V6, DOI 10.1038/srep18854
NR 25
TC 3
Z9 3
U1 1
U2 3
PY 2020
VL 12397
BP 610
EP 621
DI 10.1007/978-3-030-61616-8_49
WC Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Computer Science, Theory & Methods
DA 2023-11-11
ER

PT J
AU Fukai, T
AF Fukai, T
TI Oscillations for rapid selection of neural activities based on spike
   timing
SO NEUROREPORT
DT Article
DE oscillatory neural activities; spike-timing code; recurrent inhibition;
   competitive behaviour; integrate-and-fire neurones; computer simulations
ID INTEGRATION; NEURONS; CORTEX; MODEL
AB In cortical information processing, neuronal inputs are transformed into sequences of action potentials. However, the neural codes used for embedding information in the spike trains remain unclear. Here a neural network consisting of recurrent inhibitory connections is shown to achieve selective activation and inactivation of neurones very efficiently according to spike timing rather than firing rates, when they are stimulated by periodic spike trains. Oscillatory neural activities serve as an accurate clock for the spike-timing code utilized in the rapid selection of neural activities. These results suggest that differences in spike timing of <1 ms can be of functional significance in certain neural information processing.
C1 RIKEN,FRONTIER RES PROGRAM,LAB NEURAL MODELING,WAKO,SAITAMA 35191,JAPAN.
RP Fukai, T (corresponding author), TOKAI UNIV,DEPT ELECTR,KITAKANAME 1117,HIRATSUKA,KANAGAWA 25912,JAPAN.
CR ABELES M, 1993, J NEUROPHYSIOL, V70, P1629, DOI 10.1152/jn.1993.70.4.1629
   [Anonymous], 1988, INTRO THEORETIC NEUR
   BLAND BH, 1993, PROG NEUROBIOL, V41, P157, DOI 10.1016/0301-0082(93)90007-F
   COHEN MA, 1983, IEEE T SYST MAN CYB, V13, P815, DOI 10.1109/TSMC.1983.6313075
   COULTRIP R, 1992, NEURAL NETWORKS, V5, P47, DOI 10.1016/S0893-6080(05)80006-1
   DEVRIES SH, 1993, CELL, V72, P139, DOI 10.1016/S0092-8674(05)80033-9
   ENGEL AK, 1992, TRENDS NEUROSCI, V15, P218, DOI 10.1016/0166-2236(92)90039-B
   ERWIN E, 1995, NEURAL COMPUT, V7, P425, DOI 10.1162/neco.1995.7.3.425
   FUKAI T, 1995, TOKAI U RIKEN PREPRI
   Gray C M, 1994, J Comput Neurosci, V1, P11, DOI 10.1007/BF00962716
   Hikosaka O, 1991, Curr Opin Neurobiol, V1, P638, DOI 10.1016/S0959-4388(05)80042-X
   HOPFIELD JJ, 1984, P NATL ACAD SCI-BIOL, V81, P3088, DOI 10.1073/pnas.81.10.3088
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   KASKI S, 1994, NEURAL NETWORKS, V7, P973, DOI 10.1016/S0893-6080(05)80154-6
   MAINEN ZF, 1995, SCIENCE, V268, P1503, DOI 10.1126/science.7770778
   NICOLELIS MAL, 1995, SCIENCE, V268, P1353, DOI 10.1126/science.7761855
   OKEEFE J, 1993, HIPPOCAMPUS, V3, P317, DOI 10.1002/hipo.450030307
   SKAGGS WE, 1995, U ARIZONA PREPRINT
   SOFTKY WR, 1993, J NEUROSCI, V13, P334
   Tamori Y., 1993, Society for Neuroscience Abstracts, V19, P547
   TANAKA S, 1990, NEURAL NETWORKS, V3, P625, DOI 10.1016/0893-6080(90)90053-N
   von der Malsburg C, 1973, Kybernetik, V14, P85
   WILSON HR, 1972, BIOPHYS J, V12, P1, DOI 10.1016/S0006-3495(72)86068-5
NR 23
TC 4
Z9 4
U1 0
U2 1
PD DEC 29
PY 1995
VL 7
IS 1
BP 273
EP 277
DI 10.1097/00001756-199512000-00065
WC Neurosciences
DA 2023-11-11
ER

PT J
AU Capecci, E
   Lobo, JL
   Lana, I
   Espinosa-Ramos, JI
   Kasabov, N
AF Capecci, Elisa
   Lobo, Jesus L.
   Lana, Ibai
   Espinosa-Ramos, Josafath I.
   Kasabov, Nikola
TI Modelling gene interaction networks from time-series gene expression
   data using evolving spiking neural networks
SO EVOLVING SYSTEMS
DT Article
DE Artificial intelligence; Evolving spiking neural networks;
   Transcriptome; Gene expression; Microarray; Data analysis; Gene
   interaction networks; Nickel allergy; Allergic contact dermatitis
ID SPATIOTEMPORAL DATA; CLASSIFICATION; BRAIN; NEUCUBE; ARRAY; SET
AB The genetic mechanisms responsible for the differentiation, metabolism, morphology and function of a cell in both normal and abnormal conditions can be uncovered by the analysis of transcriptomes. Mining big data such as the information encoded in nucleic acids, proteins, and metabolites has challenged researchers for several years now. Even though bioinformatics and system biology techniques have improved greatly and many improvements have been done in these fields of research, most of the processes that influence gene interaction over time are still unknown. In this study, we apply state-of-the art spiking neural network techniques to model, analyse and extract information about the regulatory processes of gene expression over time. A case study of microarray profiling in human skin during elicitation of eczema is used to examine the temporal association of genes involved in the inflammatory response, by means of a gene interaction network. Spiking neural network techniques are able to learn the interaction between genes using information encoded from the time-series gene expression data as spikes. The temporal interaction is learned, and the patterns of activity extracted and analysed with a gene interaction network. Results demonstrated that useful knowledge can be extracted from the data by using spiking neural network, unlocking some of the possible mechanisms involved in the regulatory process of gene expression.
C1 [Capecci, Elisa; Espinosa-Ramos, Josafath I.; Kasabov, Nikola] Auckland Univ Technol, Knowledge Engn & Discovery Res Inst, AUT Tower,Cnr Rutland & Wakefield St, Auckland, New Zealand.
   [Lobo, Jesus L.; Lana, Ibai] Tecnalia Res & Innovat, P Tecnol Bizkaia, Ed 700, Derio 48160, Spain.
RP Capecci, E (corresponding author), Auckland Univ Technol, Knowledge Engn & Discovery Res Inst, AUT Tower,Cnr Rutland & Wakefield St, Auckland, New Zealand.
EM elisa.capecci@gmail.com; jesus.lopez@tecnalia.com;
   iba.lana@tecnalia.com; vjier1979@gmail.com; nkasabov@aut.ac.nz
CR Angelov P, 2013, INFORM SCIENCES, V222, P163, DOI 10.1016/j.ins.2012.08.006
   [Anonymous], EVOLVING SYSTEMS
   [Anonymous], 2010, EVOLVING INTELLIGENT
   [Anonymous], 1996, SPECTRAL GRAPH THEOR
   [Anonymous], 2005, P 1 INT WORKSH GEN F
   [Anonymous], 2018, STAT MACH LEARN TOOL
   [Anonymous], REV MANAGERIAL SCI, DOI DOI 10.1080/08927014.2017.1351958
   [Anonymous], 2002, MICROARRAYS INTEGRAT
   [Anonymous], 2004, STAT MICROARRAYS
   Argyriou Andreas, 2006, NIPS, P41
   Barrett T, 2013, NUCLEIC ACIDS RES, V41, pD991, DOI 10.1093/nar/gks1193
   Beer DG, 2002, NAT MED, V8, P816, DOI 10.1038/nm733
   Beleites C, 2013, ANAL CHIM ACTA, V760, P25, DOI 10.1016/j.aca.2012.11.007
   Bradley P. S., 1998, Machine Learning. Proceedings of the Fifteenth International Conference (ICML'98), P82
   Capano V, 2015, SCI REP-UK, V5, DOI 10.1038/srep09895
   Capecci E, 2015, NEURAL NETWORKS, V68, P62, DOI 10.1016/j.neunet.2015.03.009
   Causton HC, 2009, MICROARRAY GENE EXPR
   DeVries T, 2017, INT C LEARN REPR
   Dobbin KK, 2008, CLIN CANCER RES, V14, P108, DOI 10.1158/1078-0432.CCR-07-0443
   Dobbin KK, 2007, BIOSTATISTICS, V8, P101, DOI 10.1093/biostatistics/kxj036
   Edgar R, 2002, NUCLEIC ACIDS RES, V30, P207, DOI 10.1093/nar/30.1.207
   Espinosa-Ramos JI, 2019, IEEE T COGN DEV SYST, V11, P63, DOI 10.1109/TCDS.2017.2776863
   Ezkurdia I, 2014, HUM MOL GENET, V23, P5866, DOI 10.1093/hmg/ddu309
   Feuk L, 2006, NAT REV GENET, V7, P85, DOI 10.1038/nrg1767
   Fürnkranz J, 2002, J MACH LEARN RES, V2, P721, DOI 10.1162/153244302320884605
   GERSTNER W, 1995, PHYS REV E, V51, P738, DOI 10.1103/PhysRevE.51.738
   Gerstner W, 2001, PLAUSIBLE NEURAL NET
   Gerstner W, 2012, SCIENCE, V338, P60, DOI 10.1126/science.1227356
   Ghosh-Dastidar S, 2007, INTEGR COMPUT-AID E, V14, P187
   He X, 2006, P ADV NEUR INF PROC, P507
   Hebb D. O., 1949, ORG BEHAV A NEUROPSY
   Huang SJ, 2018, CANCER GENOM PROTEOM, V15, P41, DOI 10.21873/cgp.20063
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Kasabov Nikola, 2012, Artificial Neural Networks in Pattern Recognition. Proceedings of the 5th INNS IAPR TC 3 GIRPR Workshop, ANNPR 2012, P225, DOI 10.1007/978-3-642-33212-8_21
   Kasabov N, 2016, NEURAL NETWORKS, V78, P1, DOI 10.1016/j.neunet.2015.09.011
   Kasabov N, 2015, INFORM SCIENCES, V294, P565, DOI 10.1016/j.ins.2014.06.028
   Kasabov N, 2013, NEURAL NETWORKS, V41, P188, DOI 10.1016/j.neunet.2012.11.014
   Kasabov NK, 2014, NEURAL NETWORKS, V52, P62, DOI 10.1016/j.neunet.2014.01.006
   Kelly JG, 2010, ANAL BIOANAL CHEM, V398, P2191, DOI 10.1007/s00216-010-4179-5
   Koefoed L, 2018, 2018 INT JOINT C NEU, P1, DOI [10.1109/IJCNN.2018.8489634, DOI 10.1109/IJCNN.2018.8489634]
   Kumar C, 2009, FEBS LETT, V583, P1703, DOI 10.1016/j.febslet.2009.03.035
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Marks S, 2017, EVOL SYST-GER, V8, P193, DOI 10.1007/s12530-016-9170-8
   McLachlan G, 2005, WILEY SERIES PROBABI
   McLachlan G. J., 2005, ANAL MICROARRAY GENE
   MOLLER MF, 1993, NEURAL NETWORKS, V6, P525, DOI 10.1016/S0893-6080(05)80056-5
   Mortazavi A, 2008, NAT METHODS, V5, P621, DOI 10.1038/nmeth.1226
   Mukherjee S, 2003, J COMPUT BIOL, V10, P119, DOI 10.1089/106652703321825928
   Nuntalid N, 2011, LECT NOTES COMPUT SC, V7062, P451, DOI 10.1007/978-3-642-24955-6_54
   Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7
   Panda S, 2003, TRENDS CELL BIOL, V13, P151, DOI 10.1016/S0962-8924(03)00006-0
   Pedersen MB, 2007, J INVEST DERMATOL, V127, P2585, DOI 10.1038/sj.jid.5700902
   Pertea M, 2010, GENOME BIOL, V11, DOI 10.1186/gb-2010-11-5-206
   Radovic M, 2017, BMC BIOINFORMATICS, V18, DOI 10.1186/s12859-016-1423-9
   Roffo G, 2018, FEATURE SELECTION LI
   Roffo G, 2017, IEEE I CONF COMP VIS, P1407, DOI 10.1109/ICCV.2017.156
   Roffo G, 2015, IEEE I CONF COMP VIS, P4202, DOI 10.1109/ICCV.2015.478
   Schrauwen B, 2003, IEEE IJCNN, P2825
   Sebastiani P, 2003, STAT SCI, V18, P33, DOI 10.1214/ss/1056397486
   Shen EH, 2012, TRENDS NEUROSCI, V35, P711, DOI 10.1016/j.tins.2012.09.005
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Sun XL, 2010, BMC BIOINFORMATICS, V11, DOI 10.1186/1471-2105-11-607
   Tapia JE, 2013, IEEE T INF FOREN SEC, V8, P488, DOI 10.1109/TIFS.2013.2242063
   Thorpe S, 1998, COMPUTATIONAL NEUROSCIENCE: TRENDS IN RESEARCH, P113
   Tomasev N, 2015, STUD COMPUT INTELL, V584, P231, DOI 10.1007/978-3-662-45620-0_11
   Trevisan J, 2014, J BIOPHOTONICS, V7, P254, DOI 10.1002/jbio.201300190
   Trevisan J, 2010, ANALYST, V135, P3266, DOI 10.1039/c0an00586j
   Tu EM, 2017, IEEE T NEUR NET LEAR, V28, P1305, DOI 10.1109/TNNLS.2016.2536742
   Tu EM, 2014, IEEE IJCNN, P638, DOI 10.1109/IJCNN.2014.6889717
   Tu EM, 2014, NEUROCOMPUTING, V143, P109, DOI 10.1016/j.neucom.2014.05.067
   Verleysen M, 2005, LECT NOTES COMPUT SC, V3512, P758
   Wang XW, 2008, BMC SYST BIOL, V2, DOI 10.1186/1752-0509-2-58
   Yixiong Chen, 2013, Neural Information Processing. 20th International Conference, ICONIP 2013. Proceedings: LNCS 8228, P70, DOI 10.1007/978-3-642-42051-1_10
   Zhou F, 2012, PROC CVPR IEEE, P127, DOI 10.1109/CVPR.2012.6247667
NR 74
TC 0
Z9 0
U1 1
U2 10
PD DEC
PY 2020
VL 11
IS 4
BP 599
EP 613
DI 10.1007/s12530-019-09269-6
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT J
AU Rosenbaum, R
   Tchumatchenko, T
   Moreno-Bote, R
AF Rosenbaum, Robert
   Tchumatchenko, Tatjana
   Moreno-Bote, Ruben
TI Correlated neuronal activity and its relationship to coding, dynamics
   and network architecture
SO FRONTIERS IN COMPUTATIONAL NEUROSCIENCE
DT Editorial Material
DE neuronal correlations; neural synchrony; neural coding; spike train
   analysis; neuronal networks; noise correlation
C1 [Rosenbaum, Robert] Univ Notre Dame, Dept Appl & Computat Math & Stat, Notre Dame, IN 46556 USA.
   [Rosenbaum, Robert] Ctr Neural Basis Cognit, Pittsburgh, PA USA.
   [Tchumatchenko, Tatjana] Max Planck Inst Brain Res, Dept Theory Neural Dynam, Frankfurt, Germany.
   [Moreno-Bote, Ruben] Parc Sanitari St Joan de Deu, Res Unit, Barcelona, Spain.
   [Moreno-Bote, Ruben] Univ Barcelona, Barcelona, Spain.
   [Moreno-Bote, Ruben] Ctr Invest Biomed Red Salud Mental CIBERSAM, Barcelona, Spain.
RP Rosenbaum, R (corresponding author), Univ Notre Dame, Dept Appl & Computat Math & Stat, Notre Dame, IN 46556 USA.
EM robertr@pitt.edu
CR Abeles M., 1982, LOCAL CORTICAL CIRCU, DOI DOI 10.1007/978-3-642-81708-3
   Alvarado-Rojas C, 2013, FRONT COMPUT NEUROSC, V7, DOI 10.3389/fncom.2013.00140
   Barreiro AK, 2014, FRONT COMPUT NEUROSC, V8, DOI 10.3389/fncom.2014.00010
   Bird AD, 2014, FRONT COMPUT NEUROSC, V8, DOI 10.3389/fncom.2014.00002
   Bolhasani E, 2013, FRONT COMPUT NEUROSC, V7, DOI 10.3389/fncom.2013.00108
   Cohen MR, 2011, NAT NEUROSCI, V14, P811, DOI 10.1038/nn.2842
   de la Rocha J, 2007, NATURE, V448, P802, DOI 10.1038/nature06028
   Dipoppa M, 2013, FRONT COMPUT NEUROSC, V7, DOI 10.3389/fncom.2013.00139
   Engel AK, 2001, TRENDS COGN SCI, V5, P16, DOI 10.1016/S1364-6613(00)01568-0
   Finger H, 2014, FRONT COMPUT NEUROSC, V7, DOI 10.3389/fncom.2013.00195
   FUNAHASHI S, 1989, J NEUROPHYSIOL, V61, P331, DOI 10.1152/jn.1989.61.2.331
   Grytskyy D, 2013, FRONT COMPUT NEUROSC, V7, DOI 10.3389/fncom.2013.00131
   Helias M, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003428
   Jahnke S, 2013, FRONT COMPUT NEUROSC, V7, DOI 10.3389/fncom.2013.00153
   Kilpatrick ZP, 2013, FRONT COMPUT NEUROSC, V7, DOI 10.3389/fncom.2013.00082
   Moreno R, 2002, PHYS REV LETT, V89, DOI 10.1103/PhysRevLett.89.288101
   Moreno-Bote R, 2006, PHYS REV LETT, V96, DOI 10.1103/PhysRevLett.96.028101
   Ostojic S, 2009, J NEUROSCI, V29, P10234, DOI 10.1523/JNEUROSCI.1275-09.2009
   Renart A, 2010, SCIENCE, V327, P587, DOI 10.1126/science.1179850
   Rosenbaum R, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00058
   Rosenbaum RJ, 2010, FRONT COMPUT NEUROSC, V4, DOI 10.3389/fncom.2010.00009
   Salinas E, 2001, NAT REV NEUROSCI, V2, P539, DOI 10.1038/35086012
   Schwalger T, 2013, FRONT COMPUT NEUROSC, V7, DOI 10.3389/fncom.2013.00164
   SINGER W, 1995, ANNU REV NEUROSCI, V18, P555, DOI 10.1146/annurev.neuro.18.1.555
   Tchumatchenko T, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1002239
   Tchumatchenko T, 2010, PHYS REV LETT, V104, DOI 10.1103/PhysRevLett.104.058102
   Torre E, 2013, FRONT COMPUT NEUROSC, V7, DOI 10.3389/fncom.2013.00132
   Wimmer K, 2014, NAT NEUROSCI, V17, P431, DOI 10.1038/nn.3645
   Zanin M, 2013, FRONT COMPUT NEUROSC, V7, DOI 10.3389/fncom.2013.00125
   Zhou PC, 2013, FRONT COMPUT NEUROSC, V7, DOI 10.3389/fncom.2013.00113
NR 30
TC 8
Z9 8
U1 1
U2 17
PD AUG 27
PY 2014
VL 8
AR 102
DI 10.3389/fncom.2014.00102
WC Mathematical & Computational Biology; Neurosciences
DA 2023-11-11
ER

PT J
AU Zenke, F
   Vogels, TP
AF Zenke, Friedemann
   Vogels, Tim P.
TI The Remarkable Robustness of Surrogate Gradient Learning for Instilling
   Complex Function in Spiking Neural Networks
SO NEURAL COMPUTATION
DT Article
ID MODELS; INTELLIGENCE; DYNAMICS; POWER
AB Brains process information in spiking neural networks. Their intricate connections shape the diverse functions these networks perform. Yet how network connectivity relates to function is poorly understood, and the functional capabilities of models of spiking networks are still rudimentary. The lack of both theoretical insight and practical algorithms to find the necessary connectivity poses a major impediment to both studying information processing in the brain and building efficient neuromorphic hardware systems. The training algorithms that solve this problem for artificial neural networks typically rely on gradient descent. But doing so in spiking networks has remained challenging due to the nondifferentiable nonlinearity of spikes. To avoid this issue, one can employ surrogate gradients to discover the required connectivity. However, the choice of a surrogate is not unique, raising the question of how its implementation influences the effectiveness of the method. Here, we use numerical simulations to systematically study how essential design parameters of surrogate gradients affect learning performance on a range of classification problems. We show that surrogate gradient learning is robust to different shapes of underlying surrogate derivatives, but the choice of the derivative's scale can substantially affect learning performance. When we combine surrogate gradients with suitable activity regularization techniques, spiking networks perform robust information processing at the sparse activity limit. Our study provides a systematic account of the remarkable robustness of surrogate gradient learning and serves as a practical guide to model functional spiking neural networks.
C1 [Zenke, Friedemann; Vogels, Tim P.] Univ Oxford, Ctr Neural Circuits & Behav, Oxford OX1 3SR, England.
   [Zenke, Friedemann] Friedrich Miescher Inst Biomed Res, CH-4058 Basel, Switzerland.
   [Vogels, Tim P.] IST Austria, A-3400 Klosterneuburg, Austria.
RP Zenke, F (corresponding author), Univ Oxford, Ctr Neural Circuits & Behav, Oxford OX1 3SR, England.
EM friedemann.zenke@fmi.ch; tim.vogels@ist.ac.at
CR Amir A, 2017, PROC CVPR IEEE, P7388, DOI 10.1109/CVPR.2017.781
   [Anonymous], 2019, BIOL INSPIRED ALTERN
   Barrett DGT, 2019, CURR OPIN NEUROBIOL, V55, P55, DOI 10.1016/j.conb.2019.01.007
   Bellec G., 2018, ADV NEURAL INFORM PR, P795
   Boahen K, 2017, COMPUT SCI ENG, V19, P14, DOI 10.1109/MCSE.2017.33
   Bohte SM, 2011, LECT NOTES COMPUT SC, V6791, P60, DOI 10.1007/978-3-642-21735-7_8
   Burges, 2010, MNIST HANDWRITTEN DI, DOI DOI 10.1561/2400000035
   Cramer B., 2020, ARXIV200607239
   Cramer B, 2022, IEEE T NEUR NET LEAR, V33, P2744, DOI 10.1109/TNNLS.2020.3044364
   CRICK F, 1989, NATURE, V337, P129, DOI 10.1038/337129a0
   Cueva C. J., 2019, BIORXIV, DOI [10.1101/504936, DOI 10.1101/504936]
   Esser SK, 2016, P NATL ACAD SCI USA, V113, P11441, DOI 10.1073/pnas.1604850113
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   Gütig R, 2016, SCIENCE, V351, DOI 10.1126/science.aab4113
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Hochreiter S, 1998, INT J UNCERTAIN FUZZ, V6, P107, DOI 10.1142/S0218488598000094
   Huh Dongsung, 2018, ADV NEURAL INFORM PR, P1440
   Hunsberger Eric, 2015, ARXIV151008829
   Kaiming He, 2015, 2015 IEEE International Conference on Computer Vision (ICCV). Proceedings, P1026, DOI 10.1109/ICCV.2015.123
   Kingma D. P., 2014, ARXIV, DOI DOI 10.48550/ARXIV.1412.6980
   Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508
   Maheswaranathan N, 2019, DEEP LEARNING MODELS, DOI 10.1101/340943
   Mante V, 2013, NATURE, V503, P78, DOI 10.1038/nature12742
   McClure P, 2016, FRONT COMPUT NEUROSC, V10, DOI 10.3389/fncom.2016.00131
   McIntosh LT, 2016, ADV NEUR IN, V29
   Michaels J. A., 2019, NEURAL NETWORK MODEL, DOI DOI 10.1101/742189
   Mishkin D., 2016, IEEE T NEUR NET LEAR, V29, P3227, DOI 10.1109/TNNLS. 2017.2726060
   Murray JM, 2019, ELIFE, V8, DOI 10.7554/eLife.43299
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Neftci EO, 2018, ISCIENCE, V5, P52, DOI 10.1016/j.isci.2018.06.010
   Orchard G, 2015, FRONT NEUROSCI-SWITZ, V9, DOI [10.3389/fnins.2015.00437, 10.3389/fhins.2015.00437]
   Paszke A, 2019, ADV NEUR IN, V32
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Pospisil DA, 2018, ELIFE, V7, DOI 10.7554/eLife.38242
   Richards BA, 2019, NAT NEUROSCI, V22, P1761, DOI 10.1038/s41593-019-0520-2
   Roy K, 2019, NATURE, V575, P607, DOI 10.1038/s41586-019-1677-2
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707
   Schemmel J, 2010, IEEE INT SYMP CIRC S, P1947, DOI 10.1109/ISCAS.2010.5536970
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Shrestha S. B., 2018, ADV NEURAL INFORM PR
   Sterling P., 2017, PRINCIPLES NEURAL DE
   Stroud JP, 2018, NAT NEUROSCI, V21, P1774, DOI 10.1038/s41593-018-0276-0
   Sussillo D, 2013, NEURAL COMPUT, V25, P626, DOI 10.1162/NECO_a_00409
   Tanaka Hidenori, 2019, Adv Neural Inf Process Syst, V32, P8537
   Vogels TP, 2005, J NEUROSCI, V25, P10786, DOI 10.1523/JNEUROSCI.3508-05.2005
   Wang J, 2018, NAT NEUROSCI, V21, P102, DOI 10.1038/s41593-017-0028-6
   Warden P., 2018, ARXIV
   Williams RJ, 1989, NEURAL COMPUT, V1, P270, DOI 10.1162/neco.1989.1.2.270
   Williamson RC, 2019, CURR OPIN NEUROBIOL, V55, P40, DOI 10.1016/j.conb.2018.12.009
   Wozniak S, 2020, NAT MACH INTELL, V2, P325, DOI 10.1038/s42256-020-0187-0
   Xuan Huang, 2021, 2021 IEEE Intl Conf on Dependable, Autonomic and Secure Computing, Intl Conf on Pervasive Intelligence and Computing, Intl Conf on Cloud and Big Data Computing, Intl Conf on Cyber Science and Technology Congress (DASC/PiCom/CBDCom/CyberSciTech), P867, DOI 10.1109/DASC-PICom-CBDCom-CyberSciTech52372.2021.00144
   Yamins DLK, 2016, NAT NEUROSCI, V19, P356, DOI 10.1038/nn.4244
   Yamins DLK, 2014, P NATL ACAD SCI USA, V111, P8619, DOI 10.1073/pnas.1403112111
   Zenke F, 2018, NEURAL COMPUT, V30, P1514, DOI 10.1162/neco_a_01086
   Zimmer Romain, 2019, ARXIV191110124
NR 55
TC 56
Z9 56
U1 3
U2 12
PD MAR
PY 2021
VL 33
IS 4
BP 899
EP 925
DI 10.1162/neco_a_01367
WC Computer Science, Artificial Intelligence; Neurosciences
DA 2023-11-11
ER

PT C
AU Kato, H
   Kimura, T
   Ikeguchi, T
AF Kato, Hideyuki
   Kimura, Takayuki
   Ikeguchi, Tohru
BE In, V
   Longhini, P
   Palacious, A
TI Self-Organized Neural Network Structure Depending on the STDP Learning
   Rules
SO APPLICATIONS OF NONLINEAR DYNAMICS-MODEL AND DESIGN OF COMPLEX SYSTEMS
SE Understanding Complex Systems Springer Complexity
DT Proceedings Paper
CT International Conference on Applied Nonlinear Dynamics
CY SEP 24-27, 2007
CL Poipu Beach, HI
ID NEURONS; EPSPS
AB Complex systems are widely studied in various fields of science. A neural network is one of the typical examples of the complex systems. Recent studies in neuroscience reported that the neural networks are dynamically self-organized by the spike-timing dependent synaptic plasticity (STDP). Although the neural networks change their structure using the STDP dynamically, neural networks are often analyzed in a static state. Thus, in this paper, we analyze neural network structure from a dynamical point of view. Then, we show that the self-organized neural network to which the STDP learning rule is applied generates the small-world effect and randomness of the inter-spike intervals (ISIs) in the self-organized neural network increases as the small-world effect becomes higher.
C1 [Kato, Hideyuki] Saitama Univ, Sakura Ku, 255 Shimo Okubo, Saitama 3388570, Japan.
RP Kato, H (corresponding author), Saitama Univ, Sakura Ku, 255 Shimo Okubo, Saitama 3388570, Japan.
EM kato@nls.ics.saitama-u.ac.jp
CR Abbott LF, 2000, NAT NEUROSCI, V3, P1178, DOI 10.1038/81453
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   GUTIG SRR, 2003, J NEUROSCI, V23, P3687
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   SOFTKY WR, 1993, J NEUROSCI, V13, P334
   Watts DJ, 1998, NATURE, V393, P440, DOI 10.1038/30918
NR 7
TC 2
Z9 2
U1 0
U2 1
PY 2009
BP 413
EP +
DI 10.1007/978-3-540-85632-0_36
WC Computer Science, Hardware & Architecture; Computer Science,
   Interdisciplinary Applications; Engineering, Electrical & Electronic;
   Physics, Applied; Telecommunications
DA 2023-11-11
ER

PT C
AU Meng, QY
   Xiao, MQ
   Yan, S
   Wang, Y
   Lin, ZC
   Luo, ZQ
AF Meng, Qingyan
   Xiao, Mingqing
   Yan, Shen
   Wang, Yisen
   Lin, Zhouchen
   Luo, Zhi-Quan
GP IEEE COMP SOC
TI Training High-Performance Low-Latency Spiking Neural Networks by
   Differentiation on Spike Representation
SO 2022 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION
   (CVPR)
SE IEEE Conference on Computer Vision and Pattern Recognition
DT Proceedings Paper
CT IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CY JUN 18-24, 2022
CL New Orleans, LA
AB Spiking Neural Network (SNN) is a promising energy-efficient AI model when implemented on neuromorphic hardware. However, it is a challenge to efficiently train SNNs due to their non-differentiability. Most existing methods either suffer from high latency (i.e., long simulation time steps), or cannot achieve as high performance as Artificial Neural Networks (ANNs). In this paper, we propose the Differentiation on Spike Representation (DSR) method, which could achieve high performance that is competitive to ANNs yet with low latency. First, we encode the spike trains into spike representation using (weighted) firing rate coding. Based on the spike representation, we systematically derive that the spiking dynamics with common neural models can be represented as some sub-differentiable mapping. With this viewpoint, our proposed DSR method trains SNNs through gradients of the mapping and avoids the common non-differentiability problem in SNN training. Then we analyze the error when representing the specific mapping with the forward computation of the SNN. To reduce such error, we propose to train the spike threshold in each layer, and to introduce a new hyperparameter for the neural models. With these components, the DSR method can achieve state-of-the-art SNN performance with low latency on both static and neuromorphic datasets, including CIFAR-10, CIFAR-100, ImageNet, and DVS-CIFAR10.
C1 [Meng, Qingyan; Luo, Zhi-Quan] Chinese Univ Hong Kong, Shenzhen, Peoples R China.
   [Meng, Qingyan; Luo, Zhi-Quan] Shenzhen Res Inst Big Data, Shenzhen, Peoples R China.
   [Xiao, Mingqing; Wang, Yisen; Lin, Zhouchen] Peking Univ, Sch Artificial Intelligence, Key Lab Machine Percept MoE, Beijing, Peoples R China.
   [Yan, Shen] Peking Univ, Ctr Data Sci, Beijing, Peoples R China.
   [Wang, Yisen; Lin, Zhouchen] Peking Univ, Inst Artificial Intelligence, Beijing, Peoples R China.
   [Lin, Zhouchen] Peng Cheng Lab, Shenzhen, Guangdong, Peoples R China.
RP Lin, ZC (corresponding author), Peking Univ, Sch Artificial Intelligence, Key Lab Machine Percept MoE, Beijing, Peoples R China.; Lin, ZC (corresponding author), Peking Univ, Inst Artificial Intelligence, Beijing, Peoples R China.; Lin, ZC (corresponding author), Peng Cheng Lab, Shenzhen, Guangdong, Peoples R China.
EM qingyanmeng@link.cuhk.edu.cn; mingqing_xiao@pku.edu.cn;
   yanshen@pku.edu.cn; yisen.wang@pku.edu.cn; zlin@pku.edu.cn;
   luozq@cuhk.edu.cn
CR [Anonymous], 2018, NEURIPS
   [Anonymous], 2000, ESANN
   Bellec G., 2018, NEURIPS
   Burkitt AN, 2006, BIOL CYBERN, V95, P1, DOI 10.1007/s00422-006-0068-6
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Caporale N, 2008, ANNU REV NEUROSCI, V31, P25, DOI 10.1146/annurev.neuro.31.060407.125639
   Choi J., 2018, ARXIV180506085, P1, DOI DOI 10.23919/PANPACIFIC.2018.8319019
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Deng L, 2020, NEURAL NETWORKS, V121, P294, DOI 10.1016/j.neunet.2019.09.005
   Deng S., 2021, ICLR
   Diehl Peter U, 2015, 2015 INT JOINT C NEU, P1, DOI [DOI 10.1109/IJCNN.2015.7280696, 10.1109/IJCNN.2015.7280696]
   Ding Jianhao, 2021, IJCAI
   Esser Steve K, 2015, NEURIPS
   Esser SK, 2016, P NATL ACAD SCI USA, V113, P11441, DOI 10.1073/pnas.1604850113
   Fang Wei, 2021, ICCV
   Han Bing, 2020, CVPR
   Han Bing, 2020, ECCV
   He K., 2016, P IEEE C COMPUTER VI
   HEBB D. O., 1949
   Huh Dongsung, 2018, NEURIPS
   Hunsberger Eric, 2015, ARXIV151008829
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Kim S., 2020, AAAI
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Lee C, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00119
   Legenstein R, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.1000180
   Li H, 2017, PROT CONTR MOD POW, V2, DOI 10.1186/s41601-017-0040-6
   Li Yuhang, 2021, ICML
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Mostafa H, 2018, IEEE T NEUR NET LEAR, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Panzeri S, 2001, NEURAL COMPUT, V13, P1311, DOI 10.1162/08997660152002870
   Pei J, 2019, NATURE, V572, P106, DOI 10.1038/s41586-019-1424-8
   Rathi Nitin, 2020, INT C LEARN REPR
   Rathi Nitin, 2020, ARXIV200803658
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Stöckl C, 2021, NAT MACH INTELL, V3, DOI 10.1038/s42256-021-00311-4
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Wang LL, 2020, PHYTOPATHOL RES, V2, DOI 10.1186/s42483-020-00048-9
   Wang Y., 2018, IEEE T NEUR NET LEAR
   Wei Fangyun, 2021, NEURIPS, V3, P6
   Wu Hao, 2021, AAAI
   WU JB, 2021, TNNLS, DOI DOI 10.1109/TNNLS.2021.3095724
   Wu YF, 2018, J NANOFLUIDS, V7, P1, DOI 10.1166/jon.2018.1437
   Wu Yu, 2019, AAAI
   Wunderlich TC, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-91786-z
   Xiao M., 2021, NEURIPS
   Yan Zhanglu, 2021, AAAI
   Yang Yi-Rui, 2021, ICML
   Zheng H., 2021, AAAI
   Zhou Shibo, 2021, AAAI
NR 54
TC 10
Z9 10
U1 1
U2 5
PY 2022
BP 12434
EP 12443
DI 10.1109/CVPR52688.2022.01212
WC Computer Science, Artificial Intelligence; Imaging Science &
   Photographic Technology
DA 2023-11-11
ER

PT C
AU Ko, CW
   Lin, YD
   Chung, HW
   Jan, GJ
AF Ko, CW
   Lin, YD
   Chung, HW
   Jan, GJ
BE Chang, HK
   Zhang, YT
TI An EEG spike detection algorithm using artificial neural network with
   multi-channel correlation
SO PROCEEDINGS OF THE 20TH ANNUAL INTERNATIONAL CONFERENCE OF THE IEEE
   ENGINEERING IN MEDICINE AND BIOLOGY SOCIETY, VOL 20, PTS 1-6: BIOMEDICAL
   ENGINEERING TOWARDS THE YEAR 2000 AND BEYOND
SE PROCEEDINGS OF ANNUAL INTERNATIONAL CONFERENCE OF THE IEEE ENGINEERING
   IN MEDICINE AND BIOLOGY SOCIETY
DT Proceedings Paper
CT 20th Annual International Conference of the
   IEEE-Engineering-in-Medicine-and-Biology-Society
CY OCT 29-NOV 01, 1998
CL HONG KONG, PEOPLES R CHINA
DE spike detection; electroencephalography; radial basis function;
   artificial neural network; incidence matrix
AB An automatic spike detection algorithm for classification of multi-channel electroencephalographic (EEG) signals based on artificial neural network is presented. Radial basis function (RBF) neural network was chosen for single channel recognition, with model optimization using receiver operating characteristics analysis. Waveform simplification was employed for high noise immunity. Feature extraction with as few as three parameters was used as preparation for the inputs to the neural network. Identification of multi-channel geometric correlation was performed to further lower the false-positive rate by using an incidence matrix. Threshold value for spike classification was chosen for simultaneous maximization of detection sensitivity and selectivity. Evaluation with visual analysis in this preliminary study showed a 83% sensitivity using 16-channel continuous EEG records of four patients, while a high false positive rate was found, which was believed to arise from the extensive and exhaustive visual analysis process. The computation time required for spike detection was significantly less than that needed for online display of the signals on the monitor. We believe that the algorithm proposed in this study is robust and that the simple structure of RBF neural network yields high potential for real-time implementation.
C1 Natl Taiwan Univ, Dept Elect Engn, Taipei 10764, Taiwan.
RP Ko, CW (corresponding author), Natl Taiwan Univ, Dept Elect Engn, Rm 238,1,Sect 4,Roosevelt Rd, Taipei 10764, Taiwan.
EM r85165@cctwin.ee.ntu.edu.tw
CR BODENSTEIN G, 1977, P IEEE, V65, P642, DOI 10.1109/PROC.1977.10543
   Gabor AJ, 1996, ELECTROEN CLIN NEURO, V99, P257, DOI 10.1016/0013-4694(96)96001-0
   GOTMAN J, 1992, ELECTROEN CLIN NEURO, V83, P12, DOI 10.1016/0013-4694(92)90127-4
   GOTMAN J, 1976, ELECTROEN CLIN NEURO, V41, P513, DOI 10.1016/0013-4694(76)90063-8
   GOTMAN J, 1996, TREATMENT EPILEPSY P, P280
   KOOI KA, 1966, NEUROLOGY, V16, P59, DOI 10.1212/WNL.16.1.59
   PAURI F, 1992, ELECTROEN CLIN NEURO, V82, P1, DOI 10.1016/0013-4694(92)90175-H
   WEBBER WRS, 1994, ELECTROEN CLIN NEURO, V91, P194, DOI 10.1016/0013-4694(94)90069-8
   WEBBER WRS, 1993, ELECTROEN CLIN NEURO, V87, P364, DOI 10.1016/0013-4694(93)90149-P
NR 9
TC 18
Z9 19
U1 0
U2 1
PY 1998
VL 20
BP 2070
EP 2073
PN 1-6
WC Cardiac & Cardiovascular Systems; Engineering, Biomedical; Medical
   Informatics; Medical Laboratory Technology; Clinical Neurology;
   Radiology, Nuclear Medicine & Medical Imaging
DA 2023-11-11
ER

PT C
AU Wang, F
AF Wang, Felix
BE Dagli, CH
TI Simulation Tool for Asynchronous Cortical Streams (STACS): Interfacing
   with Spiking Neural Networks
SO COMPLEX ADAPTIVE SYSTEMS, 2015
SE Procedia Computer Science
DT Proceedings Paper
CT Conference on Engineering Cyber Physical Systems - Machine Learning,
   Data Analytics and Smart Systems Architecting
CY NOV 02-04, 2015
CL San Jose, CA
DE simulation; spiking; neural network; asynchronous communication;
   closed-loop; embodied cognition; biological models
ID NEURONS
AB We present a Simulation Tool for Asynchronous Cortical Streams (STACS) for studying spiking neural networks exhibiting adaptation in a closed-loop system. The goal is to develop a more complete understanding of the emergent behaviors at the network level, and attention is given to methods of analysis at this scale. In particular, STACS facilitates the development of network level metrics of spiking activity. At the same time, emphasis is placed on biologically faithful models of spiking and plasticity with respect to the underlying neural substrate. The essential component, however, is the ability of the neural system in interfacing with the environment. This is because behaviors such as learning and adaptation are inherently closed-loop processes that involve the interaction between an intelligent agent and its environment, here, embodied cognition. To this end, STACS utilizes a portable communication protocol, YARP, for interfacing and interacting with a wide range of external devices, both sensory and motor, as well as the ability to create user-defined methods. In doing so, we may capture and respond to real world input to a neural network, simulating experimentation of live cortical cultures such as on multielectrode arrays. (C) 2015 The Authors. Published by Elsevier B.V.
C1 [Wang, Felix] Univ Illinois, Beckman Inst, 405 N Mathews Ave, Urbana, IL 61801 USA.
RP Wang, F (corresponding author), Univ Illinois, Beckman Inst, 405 N Mathews Ave, Urbana, IL 61801 USA.
EM fywang2@illinois.edu
CR Brette R, 2007, J COMPUT NEUROSCI, V23, P349, DOI 10.1007/s10827-007-0038-6
   BROWN R, 1988, COMMUN ACM, V31, P1220, DOI 10.1145/63039.63045
   Clopath C, 2010, FRONTIERS SYNAPTIC N, V2, P1
   Engel AK, 2001, NAT REV NEUROSCI, V2, P704, DOI 10.1038/35094565
   Gewaltig M-O., 2007, SCHOLARPEDIA, V2, DOI [10.4249/scholarpedia.1430, DOI 10.4249/SCHOLARPEDIA.1430]
   Gleeson P, 2010, PLOS COMPUTATIONAL B
   Hellwig B, 2000, BIOL CYBERN, V82, P111, DOI 10.1007/PL00007964
   Hines ML, 1997, NEURON BOOK
   Izhikevich EM, 2005, IEEE T NEURAL NETWOR, V94, P3637
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   KALE LV, 1993, SIGPLAN NOTICES, V28, P91, DOI 10.1145/167962.165874
   Karypis G, 1999, SIAM REV, V41, P278, DOI 10.1137/S0036144598334138
   Metta G., 2006, International Journal of Advanced Robotic Systems, V3, P43
   Minkovich K, 2014, IEEE T NEURAL NETWOR, V25
   Oksendal B. K., 2002, STOCHASTIC DIFFERENT
   Plesser HE, 2007, LECT NOTES COMPUT SC, V4641, P672
   Vainbrand D, 2011, MICROPROCESS MICROSY, V35, P152, DOI 10.1016/j.micpro.2010.08.005
   WIENER N., 1961, CYBERNETICS CONTROL
NR 18
TC 2
Z9 2
U1 0
U2 2
PY 2015
VL 61
BP 322
EP 327
DI 10.1016/j.procs.2015.09.149
WC Computer Science, Information Systems; Computer Science, Theory &
   Methods
DA 2023-11-11
ER

PT J
AU Okreghe, CO
   Zamani, M
   Demosthenous, A
AF Okreghe, Christian O.
   Zamani, Majid
   Demosthenous, Andreas
TI A Deep Neural Network-Based Spike Sorting With Improved Channel
   Selection and Artefact Removal
SO IEEE ACCESS
DT Article
DE Sorting; Recording; Feature extraction; Classification algorithms;
   Convolutional neural networks; Deep learning; Electroencephalography;
   Artefact removal; channel selection; convolutional neural network (CNN);
   deep learning; deep spike detection (DSD); extracellular recordings;
   real-time sorting; spike sorting
ID NEURONS
AB In order to implement highly efficient brain-machine interface (BMI) systems, high-channel count sensing is often used to record extracellular action potentials. However, the extracellular recordings are typically severely contaminated by artefacts and various noise sources, rendering the separation of multi-unit neural recordings an immensely challenging task. Removing artefact and noise from neural events can improve the spike sorting performance and classification accuracy. This paper presents a deep learning technique called deep spike detection (DSD) with a strong learning ability of high-dimensional vectors for neural channel selection and artefacts removal from the selected neural channel. The proposed method significantly improves spike detection compared to the conventional methods by sequentially diminishing the noise level and discarding the active artefacts in the recording channels. The simulated and experimental results show that there is considerably better performance when the extracellular raw recordings are cleaned prior to assigning individual spikes to the neurons that generated them. The DSD achieves an overall classification accuracy of 91.53% and outperformes Wave_clus by 3.38% on the simulated dataset with various noise levels and artefacts.
C1 [Okreghe, Christian O.; Zamani, Majid; Demosthenous, Andreas] UCL, Dept Elect & Elect Engn, London WC1E 7JE, England.
RP Okreghe, CO (corresponding author), UCL, Dept Elect & Elect Engn, London WC1E 7JE, England.
EM christian.okreghe.16@ucl.ac.uk
CR Aflalo T, 2015, SCIENCE, V348, P906, DOI 10.1126/science.aaa5417
   Arsene C, 2020, Arxiv, DOI arXiv:1908.10417
   Averbeck BB, 2006, NAT REV NEUROSCI, V7, P358, DOI 10.1038/nrn1888
   Baldazzi G, 2020, J NEURAL ENG, V17, DOI 10.1088/1741-2552/abc741
   Buneo Christopher A, 2016, CRCNS, V1.0, DOI 10.6080/K0CZ353K
   Carlson D, 2019, CURR OPIN NEUROBIOL, V55, P90, DOI 10.1016/j.conb.2019.02.007
   Chaudhary U, 2016, NAT REV NEUROL, V12, P513, DOI 10.1038/nrneurol.2016.113
   Christie BP, 2015, J NEURAL ENG, V12, DOI 10.1088/1741-2560/12/1/016009
   Chung JE, 2017, NEURON, V95, P1381, DOI 10.1016/j.neuron.2017.08.030
   Collinger JL, 2013, LANCET, V381, P557, DOI 10.1016/S0140-6736(12)61816-9
   Dehnen G, 2021, BRAIN SCI, V11, DOI 10.3390/brainsci11060761
   Einevoll GT, 2012, CURR OPIN NEUROBIOL, V22, P11, DOI 10.1016/j.conb.2011.10.001
   Fabietti Marcos, 2021, Brain Inform, V8, P14, DOI 10.1186/s40708-021-00135-3
   Golub MD, 2015, ELIFE, V4, DOI [10.7554/elife.10015, 10.7554/eLife.10015]
   Issar D, 2020, J NEUROPHYSIOL, V123, P1472, DOI 10.1152/jn.00641.2019
   Kechris C, 2021, IEEE ENG MED BIO, P890, DOI 10.1109/EMBC46164.2021.9630585
   Klaes C, 2015, J NEUROSCI, V35, P15466, DOI 10.1523/JNEUROSCI.2747-15.2015
   Lawlor PN, 2018, J COMPUT NEUROSCI, V45, P173, DOI 10.1007/s10827-018-0696-6
   Lecoq J, 2021, NAT METHODS, V18, P1401, DOI 10.1038/s41592-021-01285-2
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Li ZH, 2020, BRAIN SCI, V10, DOI 10.3390/brainsci10110835
   Lopes F, 2021, IEEE ACCESS, V9, P149955, DOI 10.1109/ACCESS.2021.3125728
   Mirzaei S, 2020, IEEE ENG MED BIO, P894, DOI 10.1109/EMBC44109.2020.9176515
   Mzurikwao D, 2019, 2019 IEEE SECOND INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND KNOWLEDGE ENGINEERING (AIKE), P195, DOI 10.1109/AIKE.2019.00042
   Leite NMN, 2018, IEEE INT C BIOINFORM, P2605, DOI 10.1109/BIBM.2018.8621080
   Pachitariu M., 2016, BIORXIV, DOI DOI 10.1101/061481
   Park IY, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10010301
   Pedreira C, 2012, J NEUROSCI METH, V211, P58, DOI 10.1016/j.jneumeth.2012.07.010
   Perich Matthew G, 2018, CRCNS, V1.0, DOI 10.6080/K0FT8J72
   Quiroga R. Q., 2009, WAVE CLUS UNSUPERVIS
   Quiroga RQ, 2004, NEURAL COMPUT, V16, P1661, DOI 10.1162/089976604774201631
   Rácz M, 2020, J NEURAL ENG, V17, DOI 10.1088/1741-2552/ab4896
   Saif-ur-Rehman M, 2021, J NEURAL ENG, V18, DOI 10.1088/1741-2552/abc8d4
   Saif-ur-Rehman M, 2019, J NEURAL ENG, V16, DOI 10.1088/1741-2552/ab1e63
   Schafer RJ, 2011, SCIENCE, V332, P1568, DOI 10.1126/science.1199892
   Schirrmeister RT, 2017, HUM BRAIN MAPP, V38, P5391, DOI 10.1002/hbm.23730
   Seong C., 2021, IEEE T BIOMED CIRC S, V15, P1441
   Shi Y, 2013, J NEUROPHYSIOL, V109, P2097, DOI 10.1152/jn.00223.2012
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Steinmetz NA, 2021, SCIENCE, V372, P258, DOI 10.1126/science.abf4588
   Sun WT, 2020, NEUROCOMPUTING, V404, P108, DOI 10.1016/j.neucom.2020.04.029
   Wouters J, 2021, J NEURAL ENG, V18, DOI 10.1088/1741-2552/ac0f4a
   Yang BH, 2018, BIOMED SIGNAL PROCES, V43, P148, DOI 10.1016/j.bspc.2018.02.021
   Yang K., 2017, PROC INT C CLOUD TEC, V17
   Yi JH, 2022, IEEE OPEN J CIRCUITS, V3, P168, DOI 10.1109/OJCAS.2022.3184302
   Zamani Majid, 2022, Annu Int Conf IEEE Eng Med Biol Soc, V2022, P4884, DOI 10.1109/EMBC48229.2022.9871487
   Zamani M, 2020, IEEE T BIOMED CIRC S, V14, P221, DOI 10.1109/TBCAS.2020.2969910
   Zamani M, 2018, IEEE T BIOMED CIRC S, V12, P665, DOI 10.1109/TBCAS.2018.2825421
   Zamani M, 2014, IEEE T NEUR SYS REH, V22, P716, DOI 10.1109/TNSRE.2014.2309678
NR 49
TC 1
Z9 1
U1 4
U2 5
PY 2023
VL 11
BP 15131
EP 15143
DI 10.1109/ACCESS.2023.3242643
WC Computer Science, Information Systems; Engineering, Electrical &
   Electronic; Telecommunications
DA 2023-11-11
ER

PT J
AU Ju, PH
AF Ju, Ping-Hua
TI HOT HEAVY RAIL STEEL SURFACE ONLINE FAULTS DETECTING BASED ON FUZZY
   SPIKING NEURAL NETWORK
SO METALURGIA INTERNATIONAL
DT Article
DE hot-rolled heavy rail; faults detecting; feature extraction; neural
   network
AB Aiming at slow hot-rolled heavy rail surface faults detecting and low precision currently, this paper research a suit of system which is based on the fuzzy spiking neural network. In the system several high speed linear CCD cameras are used to collect pictures of hot-rolled heavy rail. According to the geometric characteristics of the heavy rail and its defect characteristics of high-frequency region, some image processing technologies are applied in workstation. The system is optically proposes too dark and sun regional overlapping fusion method and image correlation between pixel lines' algorithm to get sum face faults and fuzzy spiking neural network to make classification for the characteristics of low SVM training algorithm., Using fuzzy spiking neural network for classification on detection of hot heavy rail surface, greatly improve the speed and accuracy of online testing, and the detection correction rate is over than 80 percent.
C1 Chongqing Univ, Natl Key Lab Mech Transmiss, Chongqing 400044, Peoples R China.
RP Ju, PH (corresponding author), Chongqing Univ, Natl Key Lab Mech Transmiss, Chongqing 400044, Peoples R China.
CR Bassiou N, 2007, COMPUT VIS IMAGE UND, V107, P108, DOI 10.1016/j.cviu.2006.11.012
   Bodyanskiy Ye., 2008 INT BIENN BALT
   Glackin C, 2011, NEURAL NETWORKS, V24, P247, DOI 10.1016/j.neunet.2010.11.008
   Guan Xin, 2009, Railway Computer Application, V18, P27
   [韩思奇 Han Siqi], 2002, [系统工程与电子技术, System engineering & electronic technology], V24, P91
   Qi J.H., 2011, J UNIV SCI TECHNOL B, V33, P12
   TAO Gong-ming, 2010, STEEL ROLLING, V27, P32
   Wang Ai-ping, 2012, Journal of System Simulation, V24, P81
   WANG FENG-CHAO, 2010, OPTICS J, V30, P713
   ZHANG XUE-WU, 2011, OPTICS J, V31, P1
NR 10
TC 0
Z9 0
U1 0
U2 8
PY 2012
VL 17
IS 12
BP 48
EP 52
WC Metallurgy & Metallurgical Engineering
DA 2023-11-11
ER

PT J
AU Bertens, P
   Lee, SW
AF Bertens, Paul
   Lee, Seong-Whan
TI Network of evolvable neural units can learn synaptic learning rules and
   spiking dynamics
SO NATURE MACHINE INTELLIGENCE
DT Article
ID T-MAZE; EVOLUTION; NEURONS; NEUROTRANSMITTER; SYNCHRONIZATION;
   NEUROEVOLUTION; PLASTICITY
AB Although deep neural networks have seen great success in recent years through various changes in overall architectures and optimization strategies, their fundamental underlying design remains largely unchanged. Computational neuroscience may provide more biologically realistic models of neural processing mechanisms, but they are still high-level abstractions of empirical behaviour. Here we propose an evolvable neural unit (ENU) that can evolve individual somatic and synaptic compartment models of neurons in a scalable manner. We demonstrate that ENUs can evolve to mimic integrate-and-fire neurons and synaptic spike-timing-dependent plasticity. Furthermore, by constructing a network where an ENU takes the place of each synapse and neuron, we evolve an agent capable of learning to solve a T-maze environment task. This network independently discovers spiking dynamics and reinforcement-type learning rules, opening up a new path towards biologically inspired artificial intelligence.
   Bertens and Lee propose an evolvable neural unit, a recurrent neural network-based module that can evolve individual somatic and synaptic compartment models of neurons. By constructing networks of these evolvable neural units, they can evolve agents that learn synaptic update rules and the spiking dynamics of neurons.
C1 [Bertens, Paul; Lee, Seong-Whan] Korea Univ, Dept Brain & Cognit Engn, Seoul, South Korea.
   [Lee, Seong-Whan] Korea Univ, Dept Artificial Intelligence, Seoul, South Korea.
RP Lee, SW (corresponding author), Korea Univ, Dept Brain & Cognit Engn, Seoul, South Korea.; Lee, SW (corresponding author), Korea Univ, Dept Artificial Intelligence, Seoul, South Korea.
EM sw.lee@korea.ac.kr
CR Abbott LF, 2000, NAT NEUROSCI, V3, P1178, DOI 10.1038/81453
   Abbott LF, 1999, BRAIN RES BULL, V50, P303, DOI 10.1016/S0361-9230(99)00161-6
   [Anonymous], 2010, FRONT SYNAPTIC NEURO
   [Anonymous], 2008, ARTIFICIAL LIFE
   Arnold DV, 2002, IEEE T EVOLUT COMPUT, V6, P30, DOI [10.1109/4235.985690, 10.1023/A:1015059928466]
   Back T., 1991, P 4 INT C GEN ALG MO, V2
   Back Thomas, 1996, EVOLUTIONARY ALGORIT
   Bellec G., 2018, ADV NEURAL INFORM PR, P787
   BENGIO A, 1992, MONUMENT HIST, P2
   Blynel J, 2003, LECT NOTES COMPUT SC, V2611, P593
   Börgers C, 2003, NEURAL COMPUT, V15, P509, DOI 10.1162/089976603321192059
   Buhry L, 2011, NEURAL COMPUT, V23, P2599, DOI 10.1162/NECO_a_00170
   Carlson D, 2013, 2013 9TH IEEE INTERNATIONAL CONFERENCE ON DISTRIBUTED COMPUTING IN SENSOR SYSTEMS (IEEE DCOSS 2013), P1, DOI 10.1109/DCOSS.2013.73
   Carlson KD, 2014, FRONT NEUROSCI-SWITZ, V8, DOI 10.3389/fnins.2014.00010
   CATTERALL WA, 1995, ANNU REV BIOCHEM, V64, P493, DOI 10.1146/annurev.biochem.64.1.493
   Cho K., 2014, P C EMP METH NAT LAN, P1724
   Collingridge GL, 2004, NAT REV NEUROSCI, V5, P952, DOI 10.1038/nrn1556
   Deacon RMJ, 2006, NAT PROTOC, V1, P7, DOI 10.1038/nprot.2006.2
   Di Paolo EA, 2003, PHILOS T R SOC A, V361, P2299, DOI 10.1098/rsta.2003.1256
   Doya K, 2002, NEURAL NETWORKS, V15, P495, DOI 10.1016/S0893-6080(02)00044-8
   Fell J, 2011, NAT REV NEUROSCI, V12, P105, DOI 10.1038/nrn2979
   Flagel SB, 2011, NATURE, V469, P53, DOI 10.1038/nature09588
   Floreano D, 2006, INT J INTELL SYST, V21, P1005, DOI 10.1002/int.20173
   Frémaux N, 2016, FRONT NEURAL CIRCUIT, V9, DOI 10.3389/fncir.2015.00085
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Hassabis D, 2017, NEURON, V95, P245, DOI 10.1016/j.neuron.2017.06.011
   Hausknecht M, 2014, IEEE T COMP INTEL AI, V6, P355, DOI 10.1109/TCIAIG.2013.2294713
   Hebb D. O., 1949, ORG BEHAV
   Hilfiker S, 1999, PHILOS T ROY SOC B, V354, P269, DOI 10.1098/rstb.1999.0378
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Hollenbeck PJ, 2005, J CELL SCI, V118, P5411, DOI [10.1242/jcs.02745, 10.1242/jcs.053850]
   HORNIK K, 1991, NEURAL NETWORKS, V4, P251, DOI 10.1016/0893-6080(91)90009-T
   Igel C, 2003, IEEE C EVOL COMPUTAT, P2588
   Jain AK, 1996, COMPUTER, V29, P31, DOI 10.1109/2.485891
   Kepecs A, 2002, J NEUROSCI, V22, P9053
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lehman J, 2018, GECCO'18: PROCEEDINGS OF THE 2018 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P450, DOI 10.1145/3205455.3205474
   Levitan I, 2015, NEURON CELL MOL BIOL
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   MCCORMICK DA, 1989, J NEUROPHYSIOL, V62, P1018, DOI 10.1152/jn.1989.62.5.1018
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Mountcastle VB, 1997, BRAIN, V120, P701, DOI 10.1093/brain/120.4.701
   Mouret Jean-Baptiste, 2014, GROWING ADAPTIVE MAC, P251, DOI DOI 10.1007/978-3-642-55337-0_9
   Nedergaard M, 2002, NAT REV NEUROSCI, V3, P748, DOI 10.1038/nrn916
   Oliphant TE., 2006, A GUIDE TO NUMPY, V1
   Paszke A., 2017, OPEN REV
   Risi S, 2010, LECT NOTES ARTIF INT, V6226, P533, DOI 10.1007/978-3-642-15193-4_50
   Rounds E. L., 2016, INT C PAR PROBL SOLV, P537
   Sacramento J, 2018, ADV NEUR IN, V31
   Salimans T., 2017, ABS170303864 CORR
   Soltoggio A, 2007, IEEE C EVOL COMPUTAT, P2471, DOI 10.1109/CEC.2007.4424781
   Spruston N, 2008, NAT REV NEUROSCI, V9, P206, DOI 10.1038/nrn2286
   Stanley KO, 2019, NAT MACH INTELL, V1, P24, DOI 10.1038/s42256-018-0006-z
   Sutskever I., 2013, INT C MACH LEARN, P1139, DOI DOI 10.1007/S00287-015-0911-Z
   Vale RD, 2003, CELL, V112, P467, DOI 10.1016/S0092-8674(03)00111-9
   Venkadesh S, 2018, FRONT NEUROINFORM, V12, DOI 10.3389/fninf.2018.00008
   Wierstra D, 2014, J MACH LEARN RES, V15, P949
NR 57
TC 5
Z9 5
U1 1
U2 16
PD DEC
PY 2020
VL 2
IS 12
DI 10.1038/s42256-020-00267-x
WC Computer Science, Artificial Intelligence; Computer Science,
   Interdisciplinary Applications
DA 2023-11-11
ER

PT C
AU Zuo, L
   Ma, LY
   Xiao, YQ
   Zhang, ML
   Qu, H
AF Zuo, Lin
   Ma, Linyao
   Xiao, Yanqing
   Zhang, Malu
   Qu, Hong
BE Liu, D
   Xie, S
   Li, Y
   Zhao, D
   ElAlfy, ESM
TI A Dynamic Region Generation Algorithm for Image Segmentation Based on
   Spiking Neural Network
SO NEURAL INFORMATION PROCESSING (ICONIP 2017), PT III
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 24th International Conference on Neural Information Processing (ICONIP)
CY NOV 14-18, 2017
CL Guangzhou, PEOPLES R CHINA
DE Spiking neural network; Image segmentation; Pattern recognition
ID RULE
AB We propose a dynamic region generation algorithm for image segmentation based on spiking neural network inspired by human visual cortex that shows the tremendous capacity of processing image. The network structure generated by the proposed algorithm is automatically and dynamically. An image can be decomposed into several different shape and size of regions that look like superpixels. Merging these regions based on the color space similarity can extract contour. Dynamic network architecture brings stronger computing power. Dynamic generation method leads to more flexible network. Experimental results on BCDS300 dataset confirm that our approach achieves satisfactory segmentation results for different images compared with SLIC.
C1 [Zuo, Lin; Ma, Linyao; Xiao, Yanqing; Zhang, Malu; Qu, Hong] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 610054, Peoples R China.
RP Qu, H (corresponding author), Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 610054, Peoples R China.
EM hongqu@uestc.edu.com
CR Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120
   Afifi A, 2009, 2009 EUROPEAN CONFERENCE ON CIRCUIT THEORY AND DESIGN, VOLS 1 AND 2, P563, DOI 10.1109/ECCTD.2009.5275035
   Alpert S, 2012, IEEE T PATTERN ANAL, V34, P315, DOI 10.1109/TPAMI.2011.130
   Ang C. H., 2011, PROC INT C FIELD PRO, P1, DOI [10.1109/FPT.2011.6132701, DOI 10.1109/FPT.2011.6132701]
   De Berredo R.C., 2005, THESIS
   Gerstner W., 2002, SPIKING NEURON MODEL
   Ghosh-Dastidar S, 2007, INTEGR COMPUT-AID E, V14, P187
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Hosoya T, 2005, NATURE, V436, P71, DOI 10.1038/nature03689
   Kerr D, 2015, NEUROCOMPUTING, V158, P268, DOI 10.1016/j.neucom.2015.01.011
   Lin XH, 2014, LECT NOTES COMPUT SC, V8588, P248, DOI 10.1007/978-3-319-09333-8_27
   Masquelier T, 2009, NEURAL COMPUT, V21, P1259, DOI 10.1162/neco.2008.06-08-804
   Meng Y, 2011, IEEE T NEURAL NETWOR, V22, P1952, DOI 10.1109/TNN.2011.2171044
   Qu H, 2015, NEUROCOMPUTING, V151, P310, DOI 10.1016/j.neucom.2014.09.034
   Qu H, 2009, IEEE T NEURAL NETWOR, V20, P1724, DOI 10.1109/TNN.2009.2029858
   Sun Q.Y., 2015, FRUIT IMAGE SEGMENTA
   van de Sande KEA, 2011, IEEE I CONF COMP VIS, P1879, DOI 10.1109/ICCV.2011.6126456
   Wolters A, 2003, J NEUROPHYSIOL, V89, P2339, DOI 10.1152/jn.00900.2002
   Wu QX, 2013, NEUROCOMPUTING, V116, P3, DOI 10.1016/j.neucom.2012.01.046
   Xie XR, 2017, IEEE T NEUR NET LEAR, V28, P1411, DOI 10.1109/TNNLS.2016.2541339
   Zhang M., 2017, IEEE T COGN DEV SYST
NR 21
TC 0
Z9 0
U1 0
U2 1
PY 2017
VL 10636
BP 816
EP 824
DI 10.1007/978-3-319-70090-8_83
PN III
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods
DA 2023-11-11
ER

PT C
AU Yousefzadeh, A
   Masquelier, T
   Serrano-Gotarredona, T
   Linares-Barranco, B
AF Yousefzadeh, A.
   Masquelier, T.
   Serrano-Gotarredona, T.
   Linares-Barranco, B.
GP IEEE
TI Hardware Implementation of Convolutional STDP for On-line Visual Feature
   Learning
SO 2017 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS (ISCAS)
SE IEEE International Symposium on Circuits and Systems
DT Proceedings Paper
CT IEEE International Symposium on Circuits and Systems (ISCAS)
CY MAY 28-31, 2017
CL Baltimore, MD
DE Neuromorphic Systems; Spike Time Dependent Plasticity (STDP); Spiking
   Neural Networks; Hardware Implementation of Neural Systems; Learning
   Systems
AB We present a highly hardware friendly STDP (Spike Timing Dependent Plasticity) learning rule for training Spiking Convolutional Cores in Unsupervised mode and training Fully Connected Classifiers in Supervised Mode. Examples are given for a 2-layer Spiking Neural System which learns in real time features from visual scenes obtained with spiking DVS (Dynamic Vision Sensor) Cameras.
C1 [Yousefzadeh, A.; Serrano-Gotarredona, T.; Linares-Barranco, B.] CSIC, Inst Microelect Sevilla, Seville, Spain.
   [Yousefzadeh, A.; Serrano-Gotarredona, T.; Linares-Barranco, B.] Univ Seville, Seville, Spain.
   [Masquelier, T.] Univ Toulouse, CNRS, CERCO UMR 5549, F-31300 Toulouse, France.
RP Yousefzadeh, A (corresponding author), CSIC, Inst Microelect Sevilla, Seville, Spain.; Yousefzadeh, A (corresponding author), Univ Seville, Seville, Spain.
EM reza@imse-cnm.csic.es; bernabe@imse-cnm.csic.es
CR Pérez-Carrasco JA, 2013, IEEE T PATTERN ANAL, V35, P2706, DOI 10.1109/TPAMI.2013.71
   Bichler O, 2012, NEURAL NETWORKS, V32, P339, DOI 10.1016/j.neunet.2012.02.022
   Corradi F., JAER
   Iakymchuk T, 2014, IEEE INT SYMP CIRC S, P1556, DOI 10.1109/ISCAS.2014.6865445
   Markran H., 1997, SCIENCE, V275
   Masquelier T, 2016, SCI REP-UK, V6, DOI 10.1038/srep24086
   Masquelier Timothee, 2007, PLOS COMPUT BIOL
   Oster M., 2009, NEURAL COMPUT
   Serrano-Gotarredona Rafael, 2009, IEEE T NEURAL NETWOR
   Serrano-Gotarredona T., 2013, IEEE J SOLID STATE C, V48
   Yousefzadeh A., 2016, REAL TIME VIDEOS
NR 11
TC 0
Z9 0
U1 0
U2 1
PY 2017
BP 2323
EP 2326
WC Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Hong, CF
   Wang, J
   Che, YQ
AF Hong, Chaofei
   Wang, Jiang
   Che, Yanqiu
GP IEEE
TI Information Transmission through Temporal Structure in Synchronous
   spikes
SO 2019 9TH INTERNATIONAL IEEE/EMBS CONFERENCE ON NEURAL ENGINEERING (NER)
SE International IEEE EMBS Conference on Neural Engineering
DT Proceedings Paper
CT 9th IEEE/EMBS International Conference on Neural Engineering (NER)
CY MAR 20-23, 2019
CL San Francisco, CA
ID SPIKING; PROPAGATION; NETWORKS
AB Neuronal gamma-band synchronization is a common phenomenon found in cortical networks, which is considered as a potential mechanism for communication among brain areas. How neural assemblies transit information within the narrow time window of each gamma cycle is still an open question. Previous modeling studies have demonstrated that precise spike timing can robustly carry information with the propagation of strongly synchronized spikes. Here we show that the temporal structure of loosely synchronized spikes within each gamma cycle can also effectively carry information in the noisy cortical networks. The relative spiking phase of the synchronous spikes are significantly more consistent under the same stimulus compared to those in random stimuli. Moreover, there is an optimal conduction delay distribution for the network to maximize the information transmission. Our work suggests that the loosely synchronized spikes in the gamma cycles may provide a fundamental mechanism for neural communication using temporal codes.
C1 [Hong, Chaofei; Wang, Jiang] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Che, Yanqiu] Penn State Coll Med, Dept Neurosurg, Hershey, PA 17033 USA.
RP Hong, CF (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
EM hongchf@tju.edu.cn; jiangwang@tju.edu.cn; yche@pennstatehealth.psu.edu
CR Andersen P., 2007, HIPPOCAMPUS BOOK
   Bruno RM, 2006, SCIENCE, V312, P1622, DOI 10.1126/science.1124593
   Burkitt AN, 2006, BIOL CYBERN, V95, P1, DOI 10.1007/s00422-006-0068-6
   Dayan P., 2000, J CHEM INF MODEL
   Diesmann M, 1999, NATURE, V402, P529, DOI 10.1038/990101
   Fries P, 2007, TRENDS NEUROSCI, V30, P309, DOI 10.1016/j.tins.2007.05.005
   Fries P, 2015, NEURON, V88, P220, DOI 10.1016/j.neuron.2015.09.034
   Goodman Dan, 2008, Front Neuroinform, V2, P5, DOI 10.3389/neuro.11.005.2008
   Kremkow J, 2010, J NEUROSCI, V30, P15760, DOI 10.1523/JNEUROSCI.3874-10.2010
   Kumar A, 2008, J NEUROSCI, V28, P5268, DOI 10.1523/JNEUROSCI.2542-07.2008
   Kumar A, 2010, NAT REV NEUROSCI, V11, P615, DOI 10.1038/nrn2886
   Wu G. K., 2008, NEURON
NR 12
TC 0
Z9 0
U1 0
U2 3
PY 2019
BP 1118
EP 1121
DI 10.1109/ner.2019.8717154
WC Engineering, Biomedical; Neurosciences
DA 2023-11-11
ER

PT C
AU Xiao, R
   Yan, R
   Tang, HJ
   Tan, KC
AF Xiao, Rong
   Yan, Rui
   Tang, Huajin
   Tan, Kay Chen
BE Sun, F
   Liu, H
   Hu, D
TI A Spiking Neural Network Model for Sound Recognition
SO COGNITIVE SYSTEMS AND SIGNAL PROCESSING, ICCSIP 2016
SE Communications in Computer and Information Science
DT Proceedings Paper
CT 3rd International Conference on Cognitive Systems and Information
   Processing (ICCSIP)
CY NOV 19-23, 2016
CL Beijing, PEOPLES R CHINA
DE Temporal coding; Temporal learning; Time-frequency information; Spiking
   neural network; Sound recognition
ID OSCILLATIONS; CODE; CLASSIFICATION
AB This paper presents a spiking neural network (SNN) model of leaky integrate-and-fire (LIF) neurons for sound recognition, which provides a way to simulate the brain processes. Neural coding and learning by processing external stimulus and recognizing different patterns are important parts in SNN model. Based on features extracted from the time-frequency representation of sound, we present a time-frequency encoding method which can retain the adequate information of original sound and generate spikes from represented features. The generated spikes are further used to train the SNN model with plausible supervised synaptic learning rule to efficiently perform various classification tasks. By testing the encoding and learning methods in RWCP database, experiments demonstrate that the proposed SNN model can achieve the robust performance for sound recognition across a variety of noise conditions.
C1 [Xiao, Rong; Yan, Rui; Tang, Huajin] Sichuan Univ, Coll Comp Sci, Neuromorph Comp Res Ctr, Chengdu, Sichuan, Peoples R China.
   [Tan, Kay Chen] City Univ Hong Kong, Dept Comp Sci, Kowloon Tong, Hong Kong, Peoples R China.
RP Tang, HJ (corresponding author), Sichuan Univ, Coll Comp Sci, Neuromorph Comp Res Ctr, Chengdu, Sichuan, Peoples R China.
EM rxiao@stu.scu.edu.cn; ryan@scu.edu.cn; htang@scu.edu.cn;
   kaytan@cityu.edu.hk
CR Benchenane K, 2010, NEURON, V66, P921, DOI 10.1016/j.neuron.2010.05.013
   Bohte Sander M., 2004, Natural Computing, V3, P195, DOI 10.1023/B:NACO.0000027755.02868.60
   Brody CD, 2003, NEURON, V37, P843, DOI 10.1016/S0896-6273(03)00120-X
   Cowling M, 2003, PATTERN RECOGN LETT, V24, P2895, DOI 10.1016/S0167-8655(03)00147-8
   Dennis J, 2013, INT CONF ACOUST SPEE, P803, DOI 10.1109/ICASSP.2013.6637759
   Gerstner W., 2002, SPIKING NEURON MODEL
   Goldhor R. S., 1993, ICASSP-93. 1993 IEEE International Conference on Acoustics, Speech, and Signal Processing (Cat. No.92CH3252-4), P149, DOI 10.1109/ICASSP.1993.319077
   Gütig R, 2014, CURR OPIN NEUROBIOL, V25, P134, DOI 10.1016/j.conb.2014.01.004
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Heil P, 1997, J NEUROPHYSIOL, V77, P2616, DOI 10.1152/jn.1997.77.5.2616
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Hu J, 2016, IEEE COMPUT INTELL M, V11, P56, DOI 10.1109/MCI.2016.2532268
   Hu J, 2013, NEURAL COMPUT, V25, P450, DOI 10.1162/NECO_a_00395
   Indiveri G, 2015, P IEEE, V103, P1379, DOI 10.1109/JPROC.2015.2444094
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Le Mouel C, 2014, J COMPUT NEUROSCI, V37, P333, DOI 10.1007/s10827-014-0505-9
   Leutgeb S, 2005, CURR OPIN NEUROBIOL, V15, P738, DOI 10.1016/j.conb.2005.10.002
   Liu L., 1999, THESIS
   Masquelier T, 2009, NEURAL COMPUT, V21, P1259, DOI 10.1162/neco.2008.06-08-804
   Mehta MR, 2002, NATURE, V417, P741, DOI 10.1038/nature00807
   Meister M, 1999, NEURON, V22, P435, DOI 10.1016/S0896-6273(00)80700-X
   Natarajan R, 2008, NEURAL COMPUT, V20, P2325, DOI 10.1162/neco.2008.01-07-436
   O'Shaughnessy D, 2008, PATTERN RECOGN, V41, P2965, DOI 10.1016/j.patcog.2008.05.008
   Orchard G, 2015, IEEE T PATTERN ANAL, V37, P2028, DOI 10.1109/TPAMI.2015.2392947
   Panzeri S, 2010, TRENDS NEUROSCI, V33, P111, DOI 10.1016/j.tins.2009.12.001
   Perez-Orive J, 2002, SCIENCE, V297, P359, DOI 10.1126/science.1070502
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Tiesinga P, 2008, NAT REV NEUROSCI, V9, P97, DOI 10.1038/nrn2315
   Valero X, 2012, IEEE T MULTIMEDIA, V14, P1684, DOI 10.1109/TMM.2012.2199972
   VanRullen R, 2005, TRENDS NEUROSCI, V28, P1, DOI 10.1016/j.tins.2004.10.010
   Nguyen VA, 2012, IEEE T NEUR NET LEAR, V23, P971, DOI 10.1109/TNNLS.2012.2191419
   WOODARD JP, 1992, IEEE T SIGNAL PROCES, V40, P1833, DOI 10.1109/78.143457
   Yu Q, 2016, IEEE T NEUR NET LEAR, V27, P621, DOI 10.1109/TNNLS.2015.2416771
   Yu QF, 2013, PLOS ONE, V8, DOI [10.1371/journal.pone.0059627, 10.1371/journal.pone.0078318]
   Zhao B., 2015, IEEE T NEURAL NETWOR, V26, P24
NR 35
TC 7
Z9 7
U1 1
U2 7
PY 2017
VL 710
BP 584
EP 594
DI 10.1007/978-981-10-5230-9_57
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods
DA 2023-11-11
ER

PT J
AU Rácz, M
   Liber, C
   Németh, E
   Fiáth, R
   Rokai, J
   Harmati, I
   Ulbert, I
   Márton, G
AF Racz, Melinda
   Liber, Csaba
   Nemeth, Erik
   Fiath, Richard
   Rokai, Janos
   Harmati, Istvan
   Ulbert, Istvan
   Marton, Gergely
TI Spike detection and sorting with deep learning
SO JOURNAL OF NEURAL ENGINEERING
DT Article
DE spike detection; spike sorting; spike prediction; LSTM; recurrent neural
   network; convolutional neural network
ID DISCHARGE; SHIFT
AB Objective. The extraction and identification of single-unit activities in intracortically recorded electric signals have a key role in basic neuroscience, but also in applied fields, like in the development of high-accuracy brain-computer interfaces. The purpose of this paper is to present our current results on the detection, classification and prediction of neural activities based on multichannel action potential recordings. Approach. Throughout our investigations, a deep learning approach utilizing convolutional neural networks and a combination of recurrent and convolutional neural networks was applied, with the latter used in case of spike detection and the former used for cases of sorting and predicting spiking activities. Main results. In our experience, the algorithms applied prove to be useful in accomplishing the tasks mentioned above: our detector could reach an average recall of 69%, while we achieved an average accuracy of 89% in classifying activities produced by more than 20 distinct neurons. Significance. Our findings support the concept of creating real-time, high-accuracy action potential based BCIs in the future, providing a flexible and robust algorithmic background for further development.
C1 [Racz, Melinda; Liber, Csaba; Harmati, Istvan] Budapest Univ Technol & Econ, Dept Elect Engn & Informat, Budapest, Hungary.
   [Racz, Melinda; Nemeth, Erik; Fiath, Richard; Rokai, Janos; Ulbert, Istvan; Marton, Gergely] Pazmany Peter Catholic Univ, Fac Informat Technol & Bion, Budapest, Hungary.
   [Fiath, Richard; Ulbert, Istvan; Marton, Gergely] Hungarian Acad Sci, Inst Cognit Neurosci & Psychol, Res Ctr Nat Sci, Budapest, Hungary.
   [Marton, Gergely] Obuda Univ, Doctoral Sch Mat Sci & Technol, Budapest, Hungary.
   [Rokai, Janos] Semmelweis Univ, Karoly Racz Sch PhD Studies, Budapest, Hungary.
RP Rácz, M (corresponding author), Budapest Univ Technol & Econ, Dept Elect Engn & Informat, Budapest, Hungary.; Rácz, M (corresponding author), Pazmany Peter Catholic Univ, Fac Informat Technol & Bion, Budapest, Hungary.
EM racz.melinda@ppke.hu
CR ABELES M, 1977, P IEEE, V65, P762, DOI 10.1109/PROC.1977.10559
   [Anonymous], 1997, NEURAL COMPUT
   Arora A, 2018, J NEURAL ENG, V15, DOI 10.1088/1741-2552/aae131
   Biffi E., 2008, 4th IET International Conference on Advances in Medical, Signal and Information Processing, MEDSIP 2008, DOI 10.1049/cp:20080434
   Chah E, 2011, J NEURAL ENG, V8, DOI 10.1088/1741-2560/8/1/016006
   Chaure FJ, 2018, J NEUROPHYSIOL, V120, P1859, DOI 10.1152/jn.00339.2018
   Chen Y, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.82
   Choi JH, 2006, IEEE T BIO-MED ENG, V53, P738, DOI 10.1109/TBME.2006.870239
   Craik A, 2019, J NEURAL ENG, V16, DOI 10.1088/1741-2552/ab0ab5
   Fee MS, 1996, J NEUROSCI METH, V69, P175, DOI 10.1016/S0165-0270(96)00050-7
   Fiáth R, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-018-36816-z
   Fiáth R, 2018, BIOSENS BIOELECTRON, V106, P86, DOI 10.1016/j.bios.2018.01.060
   FUKUSHIMA K, 1980, BIOL CYBERN, V36, P193, DOI 10.1007/BF00344251
   GEORGOPOULOS AP, 1986, SCIENCE, V233, P1416, DOI 10.1126/science.3749885
   GEORGOPOULOS AP, 1983, EXP BRAIN RES, V49, P327
   Hochberg LR, 2012, NATURE, V485, P372, DOI 10.1038/nature11076
   Houston B, 2019, J NEURAL ENG, V16, DOI 10.1088/1741-2552/aae67f
   Hulata E, 2002, J NEUROSCI METH, V117, P1, DOI 10.1016/S0165-0270(02)00032-8
   Kim KH, 2000, IEEE T BIO-MED ENG, V47, P1406, DOI 10.1109/10.871415
   Lewicki MS, 1998, NETWORK-COMP NEURAL, V9, pR53, DOI 10.1088/0954-898X/9/4/001
   Li J, 2019, J NEURAL ENG, V16, DOI 10.1088/1741-2552/aaeaae
   Márton G, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0145307
   Oweiss KG, 2002, NEUROCOMPUTING, V44, P1133, DOI 10.1016/S0925-2312(02)00436-8
   Pachitariu M., 2016, NIPS P, V29, P4448
   Quiroga RQ, 2004, NEURAL COMPUT, V16, P1661, DOI 10.1162/089976604774201631
   Rey HG, 2015, BRAIN RES BULL, V119, P106, DOI 10.1016/j.brainresbull.2015.04.007
   Saif-ur-Rehman M, 2019, J NEURAL ENG, V16, DOI 10.1088/1741-2552/ab1e63
   Schirrmeister RT, 2017, HUM BRAIN MAPP, V38, P5391, DOI 10.1002/hbm.23730
   SCHWARTZ AB, 1988, J NEUROSCI, V8, P2913
   Schwartz AB, 2006, NEURON, V52, P205, DOI 10.1016/j.neuron.2006.09.019
   Stevenson IH, 2011, NAT NEUROSCI, V14, P139, DOI 10.1038/nn.2731
   Takahashi S, 2003, NEUROSCI RES, V46, P265, DOI 10.1016/S0168-0102(03)00103-2
   Vargas-Irwin C, 2007, J NEUROSCI METH, V164, P1, DOI 10.1016/j.jneumeth.2007.03.025
   Vogelstein R J, 2005, C P IEEE ENG MED BIO, V1, P546, DOI [10.1109/IEMBS.2004.1403215, DOI 10.1109/IEMBS.2004.1403215]
   Waldert S, 2009, J PHYSIOL-PARIS, V103, P244, DOI 10.1016/j.jphysparis.2009.08.007
   Wang GL, 2006, IEEE T BIO-MED ENG, V53, P1195, DOI 10.1109/TBME.2006.873397
   WERBOS PJ, 1988, NEURAL NETWORKS, V1, P339, DOI 10.1016/0893-6080(88)90007-X
   Wodlinger B, 2015, J NEURAL ENG, V12, DOI 10.1088/1741-2560/12/1/016011
   Wood F, 2004, P ANN INT IEEE EMBS, V26, P4009
   Xu HJ, 2018, J NEURAL ENG, V15, DOI 10.1088/1741-2552/aa9451
   Yang K, 2017, J PHYS CONF SER, V910, DOI 10.1088/1742-6596/910/1/012062
NR 41
TC 33
Z9 33
U1 3
U2 34
PD FEB
PY 2020
VL 17
IS 1
AR 016038
DI 10.1088/1741-2552/ab4896
WC Engineering, Biomedical; Neurosciences
DA 2023-11-11
ER

PT J
AU Ullah, S
   Henke, M
   Narisetti, N
   Panzarova, K
   Trtilek, M
   Hejatko, J
   Gladilin, E
AF Ullah, Sajid
   Henke, Michael
   Narisetti, Narendra
   Panzarova, Klara
   Trtilek, Martin
   Hejatko, Jan
   Gladilin, Evgeny
TI Towards Automated Analysis of Grain Spikes in Greenhouse Images Using
   Neural Network Approaches: A Comparative Investigation of Six Methods
SO SENSORS
DT Article
DE high-throughput plant image analysis; spike detection; spike
   segmentation; deep learning; automated plant phenotyping
ID CLASSIFICATION
AB Automated analysis of small and optically variable plant organs, such as grain spikes, is highly demanded in quantitative plant science and breeding. Previous works primarily focused on the detection of prominently visible spikes emerging on the top of the grain plants growing in field conditions. However, accurate and automated analysis of all fully and partially visible spikes in greenhouse images renders a more challenging task, which was rarely addressed in the past. A particular difficulty for image analysis is represented by leaf-covered, occluded but also matured spikes of bushy crop cultivars that can hardly be differentiated from the remaining plant biomass. To address the challenge of automated analysis of arbitrary spike phenotypes in different grain crops and optical setups, here, we performed a comparative investigation of six neural network methods for pattern detection and segmentation in RGB images, including five deep and one shallow neural network. Our experimental results demonstrate that advanced deep learning methods show superior performance, achieving over 90% accuracy by detection and segmentation of spikes in wheat, barley and rye images. However, spike detection in new crop phenotypes can be performed more accurately than segmentation. Furthermore, the detection and segmentation of matured, partially visible and occluded spikes, for which phenotypes substantially deviate from the training set of regular spikes, still represent a challenge to neural network models trained on a limited set of a few hundreds of manually labeled ground truth images. Limitations and further potential improvements of the presented algorithmic frameworks for spike image analysis are discussed. Besides theoretical and experimental investigations, we provide a GUI-based tool (SpikeApp), which shows the application of pre-trained neural networks to fully automate spike detection, segmentation and phenotyping in images of greenhouse-grown plants.
C1 [Ullah, Sajid; Henke, Michael; Hejatko, Jan] Masaryk Univ, CEITEC Cent European Inst Technol, Plant Sci Core Facil, Brno 60200, Czech Republic.
   [Narisetti, Narendra; Gladilin, Evgeny] Leibniz Inst Plant Genet & Crop Plant Res IPK, D-06466 Gatersleben, Germany.
   [Panzarova, Klara; Trtilek, Martin] Spol Sro, PSI Photon Syst Instruments, Drasov 66424, Czech Republic.
RP Ullah, S (corresponding author), Masaryk Univ, CEITEC Cent European Inst Technol, Plant Sci Core Facil, Brno 60200, Czech Republic.; Gladilin, E (corresponding author), Leibniz Inst Plant Genet & Crop Plant Res IPK, D-06466 Gatersleben, Germany.
EM sajid.ullah@ceitec.muni.cz; mhenke@uni-goettingen.de;
   narisetti@ipk-gatersleben.de; panzarova@psi.cz; martin@psi.cz;
   hejatko@sci.muni.cz; gladilin@ipk-gatersleben.de
CR Alharbi N, 2018, PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION APPLICATIONS AND METHODS (ICPRAM 2018), P346, DOI 10.5220/0006580403460355
   [Anonymous], 1990, WAVELETS, DOI [10.1007/978-3-642-75988-8_28, DOI 10.1007/978-3-642-75988-8_28]
   Azad Reza, 2020, Computer Vision - ECCV 2020 Workshops. Proceedings. Lecture Notes in Computer Science (LNCS 12535), P251, DOI 10.1007/978-3-030-66415-2_16
   Bi Kun, 2010, Transactions of the Chinese Society of Agricultural Engineering, V26, P212
   Bochkovskiy A., 2020, PREPRINT
   Brox T., 2015, P INT C MED IM COMP, P234, DOI 10.1007/978-3-319-24574-4-28
   Chang T, 1993, IEEE T IMAGE PROCESS, V2, P429, DOI 10.1109/83.242353
   Chen L.-C., 2014, ARXIV
   Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49
   Christodoulou CI, 2003, IEEE T MED IMAGING, V22, P902, DOI 10.1109/TMI.2003.815066
   Costa C, 2019, FRONT PLANT SCI, V9, DOI 10.3389/fpls.2018.01933
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Everingham M., PASCAL VISUAL OBJECT
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Grillo O, 2017, COMPUT ELECTRON AGR, V141, P223, DOI 10.1016/j.compag.2017.07.024
   Guo ZF, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-31977-3
   Hasan MM, 2018, PLANT METHODS, V14, DOI 10.1186/s13007-018-0366-8
   He K., 2015, ARXIV
   Huang J, 2017, PROC CVPR IEEE, P3296, DOI 10.1109/CVPR.2017.351
   King DB, 2015, ACS SYM SER, V1214, P1
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
   Li QY, 2017, PLANT METHODS, V13, DOI 10.1186/s13007-017-0231-1
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Minervini M, 2015, IEEE SIGNAL PROC MAG, V32, P126, DOI 10.1109/MSP.2015.2405111
   Misra T, 2020, PLANT METHODS, V16, DOI 10.1186/s13007-020-00582-9
   Narisetti N, 2020, FRONT PLANT SCI, V11, DOI 10.3389/fpls.2020.00666
   Pieruschka R, 2019, PLANT PHENOMICS, V2019, DOI 10.34133/2019/7507131
   Pound M. P., 2017, P IEEE CVF INT C COM
   Redmon J., 2018, TECH REPORT, P1
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Salehi SSM, 2017, LECT NOTES COMPUT SC, V10541, P379, DOI 10.1007/978-3-319-67389-9_44
   Tan CW, 2020, FRONT PLANT SCI, V11, DOI 10.3389/fpls.2020.00259
   Tzutalin, 2015, LAB FREE SOFTW MIT L
   Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5
NR 34
TC 1
Z9 1
U1 2
U2 17
PD NOV
PY 2021
VL 21
IS 22
AR 7441
DI 10.3390/s21227441
WC Chemistry, Analytical; Engineering, Electrical & Electronic; Instruments
   & Instrumentation
DA 2023-11-11
ER

PT J
AU Contoyiannis, YF
   Kosmidis, EK
   Diakonos, FK
   Kampitakis, M
   Potirakis, SM
AF Contoyiannis, Yiannis F.
   Kosmidis, Efstratios K.
   Diakonos, Fotios K.
   Kampitakis, Myron
   Potirakis, Stelios M.
TI A hybrid artificial neural network for the generation of critical
   fluctuations and inter-spike intervals
SO CHAOS SOLITONS & FRACTALS
DT Article
DE Criticality; Intermittency; Kink-antikink solitons; Artificial neural
   network; Membrane potential of biological neurons; Spikes
AB The recently introduced [Contoyiannis et al., 2021] hybrid artificial neural network can simulate the dynamics of membrane potential fluctuations of real neurons based on fundamental principles of Physics. Here, we propose a temporal description of the membrane potential fluctuations, which resembles the soliton solutions in phi(4) field theory. Within this framework, kink-antikink dynamics are associated with spike generation. Furthermore, we show that the simulation can also reproduce the distribution of inter-spike intervals of biological neurons in their critical state [Kosmidis et al., 2018]. A proposal for the intermittency origin of these fluctuations is discussed.
C1 [Contoyiannis, Yiannis F.; Potirakis, Stelios M.] Univ West Attica, Dept Elect & Elect Engn, Ancient Olive Grove Campus, GR-12241 Egaleo, Greece.
   [Contoyiannis, Yiannis F.; Diakonos, Fotios K.] Univ Athens, Dept Phys, Athens, Greece.
   [Kosmidis, Efstratios K.] Aristotle Univ Thessaloniki, Dept Med, Lab Physiol, Thessaloniki, Greece.
   [Kampitakis, Myron] Hellen Elect Distribut Network Operator SA, Network Major Installat Dept, 72 Athinon Ave, GR-18547 Faliro, Greece.
   [Potirakis, Stelios M.] Natl Observ Athens, Inst Astron Astrophys Space Applicat & Remote Sens, Metaxa and Vasileos Pavlou, GR-15236 Athens, Greece.
RP Potirakis, SM (corresponding author), Univ West Attica, Dept Elect & Elect Engn, Ancient Olive Grove Campus, GR-12241 Egaleo, Greece.
EM yiaconto@uniwa.gr; kosmidef@auth.gr; fdiakono@phys.uoa.gr;
   m.kampitakis@deddie.gr; spoti@uniwa.gr
CR Amit D. J., 1989, MODELING BRAIN FUNCT
   [Anonymous], 1985, QUANTUM FIELD THEORY, DOI DOI 10.1017/CBO9780511813900
   [Anonymous], 1993, QUANTUM FIELD THEORY
   [Anonymous], 2007, NEUROQUANTOLOGY, DOI DOI 10.14704/NQ.2007.5.3.137
   Avramiea AE, 2022, J NEUROSCI, V42, P2221, DOI 10.1523/JNEUROSCI.1095-21.2022
   Bezaire MJ, 2013, HIPPOCAMPUS, V23, P751, DOI 10.1002/hipo.22141
   Bornholdt S, 2000, PHYS REV LETT, V84, P6114, DOI 10.1103/PhysRevLett.84.6114
   Brochini L, 2016, SCI REP-UK, V6, DOI 10.1038/srep35831
   Contoyiannis YF, 2021, PHYSICA A, V577, DOI 10.1016/j.physa.2021.126073
   Contoyiannis YF, 2007, PHYS REV E, V76, DOI 10.1103/PhysRevE.76.031138
   Contoyiannis YF, 2002, PHYS REV LETT, V89, DOI 10.1103/PhysRevLett.89.035701
   Dahmen D, 2019, P NATL ACAD SCI USA, V116, P13051, DOI 10.1073/pnas.1818972116
   DEWILDE P, 1997, NEURAL NETWORK MODEL
   Eftaxias K, 2018, COMPLEXITY OF SEISMIC TIME SERIES: MEASUREMENT AND APPLICATION, P437, DOI 10.1016/B978-0-12-813138-1.00013-4
   Friedman N, 2012, PHYS REV LETT, V108, DOI 10.1103/PhysRevLett.108.208102
   Georgiev D, 2004, BIOMED REV, V15, P67, DOI DOI 10.14748/BMR.V15.103
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Ivancevic VG, 2013, ARXIV13050613V1 Q BI
   Kinouchi O, 2006, NAT PHYS, V2, P348, DOI 10.1038/nphys289
   Kosmidis EK, 2018, EUR J NEUROSCI, V48, P2343, DOI 10.1111/ejn.14117
   Landau LD, 1969, STAT PHYS 2, V9
   Landmann S., SELF ORG CRITICALITY
   Levina A, 2007, NAT PHYS, V3, P857, DOI 10.1038/nphys758
   Lovecchio E, 2012, FRONT PHYSIOL, V3, DOI 10.3389/fphys.2012.00098
   METROPOLIS N, 1953, J CHEM PHYS, V21, P1087, DOI 10.1063/1.1699114
   Naundorf B, 2006, NATURE, V440, P1060, DOI 10.1038/nature04610
   Potirakis SM, 2019, PHYSICA A, V528, DOI 10.1016/j.physa.2019.121360
   Rubin R, 2017, P NATL ACAD SCI USA, V114, pE9366, DOI 10.1073/pnas.1705841114
   Schuster H.G., 1989, DETERMINIST CHAOS IN
   Tetzlaff C, 2010, PLOS COMPUT BIOL, V6, DOI 10.1371/journal.pcbi.1001013
   Tkacik G, 2015, P NATL ACAD SCI USA, V112, P11508, DOI 10.1073/pnas.1514188112
   Villarrubia G, 2018, NEUROCOMPUTING, V272, P10, DOI 10.1016/j.neucom.2017.04.075
   Werner G, 2007, BIOSYSTEMS, V90, P496, DOI 10.1016/j.biosystems.2006.12.001
   Wilting J, 2019, CURR OPIN NEUROBIOL, V58, P105, DOI 10.1016/j.conb.2019.08.002
NR 34
TC 1
Z9 1
U1 0
U2 3
PD JUN
PY 2022
VL 159
AR 112115
DI 10.1016/j.chaos.2022.112115
EA APR 2022
WC Mathematics, Interdisciplinary Applications; Physics, Multidisciplinary;
   Physics, Mathematical
DA 2023-11-11
ER

PT C
AU Cherdo, Y
   Miramond, B
   Pegatoquet, A
AF Cherdo, Yann
   Miramond, Benoit
   Pegatoquet, Alain
GP IEEE
TI Time series prediction and anomaly detection with recurrent spiking
   neural networks
SO 2023 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS, IJCNN
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUN 18-23, 2023
CL Broadbeach, AUSTRALIA
DE Spiking Neural Networks; CNN; LSTM; RNN; Unsupervised Anomaly detection;
   time-series prediction; Surrogate Gradient Descent
AB In the recent years, Spiking Neural Networks have gain much attention from the research community. They can now be trained using the powerful gradient descent and have drifted from the neuroscience to the Machine Learning community. An abundant literature shows that they can perform well on classical Artificial Intelligence tasks such as image or signal classification while consuming less energy than state-of-the-art models like Convolutional Neural Networks. Yet, there is very little work about their performance on unsupervised anomaly detection and time-series prediction. Indeed, the processing of such temporal data requires different encoding and decoding mechanisms and rises questions about their capacity to model a dynamical signal with long term temporal dependencies. In this paper, we propose for the first time a Sparse Recurrent Spiking Neural Network with specific encoding and decoding mechanisms to successfully predict time-series and do Unsupervised Anomaly Detection. We also provide a framework to describe in detail our model computational costs and fairly compare them with state-of-the-art models. Despite improvable performances, we show that our model perform well on these tasks and open a door for further studies of such applications for Spiking Neural Networks.
C1 [Cherdo, Yann] Renault Software, LEAT, CNRS UMR 7248, Biot, France.
   [Miramond, Benoit; Pegatoquet, Alain] Univ Cote Azur, LEAT, CNRS UMR 7248, Biot, France.
RP Cherdo, Y (corresponding author), Renault Software, LEAT, CNRS UMR 7248, Biot, France.
EM yann.cherdo@univ-cotedazur.fr; benoit.miramond@univ-cotedazur.fr;
   alain.pegatoquet@univ-cotedazur.fr
CR Ahmad S., 2019, ARXIV190311257
   Ahmad S., 2015, ARXIV150307469
   Ahmad S, 2017, NEUROCOMPUTING, V262, P134, DOI 10.1016/j.neucom.2017.04.070
   [Anonymous], 2020, METEONET OPEN REFERE
   [Anonymous], 2015, P EUR S ANN
   Auge D, 2021, NEURAL PROCESS LETT, V53, P4693, DOI 10.1007/s11063-021-10562-2
   Bellec G., 2018, ADV NEURAL INFORM PR, P787
   BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181
   Bontemps L, 2016, LECT NOTES COMPUT SC, V10018, P141, DOI 10.1007/978-3-319-48057-2_9
   Calvin W. H., 1998, CEREBRAL CODE THINKI
   Chauhan S., 2015, DSAA
   Cherdo Y., 2020, ICASSP
   Cho K, 2014, ARXIV, DOI [10.3115/v1/w14-4012, DOI 10.3115/V1/W14-4012]
   Cordone L., 2022, IJCNN
   Cordone L, 2021, IEEE IJCNN, DOI 10.1109/IJCNN52387.2021.9533514
   Dampfhoffer M., 2022, INT C ART NEUR NETW
   Datta G., 2022, ARXIV221012613
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Den`eve S., 2016, NATURE NEUROSCIENCE
   Dennler N., 2021, C ART INT CIRC SYST
   Eshraghian J. K., 2021, ARXIV210912894
   Filonov P., 2016, ARXIV161206676
   Gerstner W., 2002, SPIKING NEURON MODEL, DOI DOI 10.1017/CBO9780511815706
   Gerz F., 2022, IJCNN IEEE
   GUHA S, 2016, ICML, V48
   Hagenaars J., 2021, ADV NEURAL INFORM PR
   Hawkins J., 2004, INTELLIGENCE
   Hochreiter S., 1997, LONG SHORT TERM MEMO, V9, P1735
   Izhikevich E. M., 2003, IEEE T NEURAL NETWOR
   Kejariwal A., 2015, TWITTER ANOMALYDETEC
   L<spacing diaeresis>angkvist M., 2014, PATTERN RECOGNITION
   Laptev N., 2015, ACM SIGKDD
   Lavin A., 2015, IEEE
   Lemaire E., 2022, ARXIV221013107
   Li C., 2022, FRONTIERS NEUROSCIEN
   Liu Q., 2022, KNOWLEDGE BASED SYST
   Long L., 2022, KNOWLEDGE BASED SYST
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maciag P. S., 2021, NEURAL NETWORKS
   Munir M., 2019, IEEE ACCESS
   Neftci Emre O, 2019, IEEE SIGNAL PROCESSI
   Pavlidis N., 2005, P IEEE IJCNN
   Pytorch, US
   Rao A, 2022, NAT MACH INTELL, V4, P467, DOI 10.1038/s42256-022-00480-w
   Stanway A., 2013, ETSY SKYLINE ONLINE
   Thorpe S, 1998, COMPUTATIONAL NEUROSCIENCE: TRENDS IN RESEARCH, P113
   Wang W., 2020, ARXIV200109220
   Wei D., 2021, APPL ENERGY
   Yahoo! webscope research, S5 LAB AN DET DAT VE
   Yan Z., 2021, BIOMEDICAL SIGNAL PR
   Yin B., 2020, INT C NEUR SYST
   Yin B., 2021, NATURE MACHINE INTEL
NR 52
TC 0
Z9 0
U1 1
U2 1
PY 2023
DI 10.1109/IJCNN54540.2023.10191614
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Meftah, B
   Lezoray, O
   Lecluse, M
   Benyettou, A
AF Meftah, Boudjelal
   Lezoray, Olivier
   Lecluse, Michel
   Benyettou, Abdelkader
BE Diamantaras, K
   Duch, W
   Iliadis, LS
TI Cell Microscopic Segmentation with Spiking Neuron Networks
SO ARTIFICIAL NEURAL NETWORKS-ICANN 2010, PT I
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 20th International Conference on Artificial Neural Networks
CY SEP 15-18, 2010
CL Thessaloniki, GREECE
DE Cell microscopic images; Hebbian learning; Segmentation; Spiking Neuron
   Networks
ID NUCLEI
AB Spiking Neuron Networks (SNNs) overcome the computational power of neural networks made of thresholds or sigmoidal units. Indeed, SNNs add a new dimension, the temporal axis, to the representation capacity and the processing abilities of neural networks. In this paper, we present how SNN can be applied with efficacy for cell microscopic image segmentation. Results obtained confirm the validity of the approach. The strategy is performed on cytological color images. Quantitative measures are used to evaluate the resulting segmentations.
C1 [Meftah, Boudjelal] Univ Mascara, Equipe EDTEC, Mascara, Algeria.
   [Meftah, Boudjelal; Lezoray, Olivier] Univ Caen Basse Normandie, CNRS 6072, UMR, GREYC, F-14050 Caen, France.
   [Lecluse, Michel] Ctr Hospitalier Public Cotentin, Serv anatom cytolog pathologiques, F-50130 Octeville, France.
   [Benyettou, Abdelkader] Univ Mohamed Boudiaf, Lab Signal Image Parole, Oran, Algeria.
RP Meftah, B (corresponding author), Univ Mascara, Equipe EDTEC, Mascara, Algeria.
CR Adiga PSU, 2001, PATTERN RECOGN, V34, P1449, DOI 10.1016/S0031-3203(00)00076-5
   Anoraganingrum D., 1999, Proceedings 10th International Conference on Image Analysis and Processing, P1043, DOI 10.1109/ICIAP.1999.797734
   Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   Borsotti M, 1998, PATTERN RECOGN LETT, V19, P741, DOI 10.1016/S0167-8655(98)00052-X
   DEBERREDO RC, 2005, THESIS U MINAS GERAI
   Di Ruberto C, 2000, INT C PATT RECOG, P397, DOI 10.1109/ICPR.2000.903568
   GERSTNER W, 1995, PHYS REV E, V51, P738, DOI 10.1103/PhysRevE.51.738
   Gerstner W., 2002, SPIKING NEURON MODEL
   Glory E, 2005, LECT NOTES COMPUT SC, V3708, P227
   Gupta A, 2009, IEEE IJCNN, P1189
   Karlsson A, 2003, LECT NOTES COMPUT SC, V2749, P595
   Knesel EA, 1996, ACTA CYTOL, V40, P60
   Lezoray O, 2002, IEEE T IMAGE PROCESS, V11, P783, DOI 10.1109/TIP.2002.800889
   Lin G, 2003, CYTOM PART A, V56A, P23, DOI 10.1002/cyto.a.10079
   LUDEMIR BTB, 2000, ARTIFICIAL NEURAL NE
   Maass W, 1997, NEURAL COMPUT, V9, P279, DOI 10.1162/neco.1997.9.2.279
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   MAASS W, 2001, RELEVANCE NEURAL NET
   Meurie C, 2005, INT J ROBOT AUTOM, V20, P63, DOI 10.2316/Journal.206.2005.2.206-2780
   Mouroutis T, 1998, BIOIMAGING, V6, P79, DOI 10.1002/1361-6374(199806)6:2<79::AID-BIO3>3.0.CO;2-#
   MURASHOV D, 2004, P 7 INT C PATT REC I, V3, P814
   Natschlager T, 1998, NETWORK-COMP NEURAL, V9, P319, DOI 10.1088/0954-898X/9/3/003
   OSTER M, 2004, P 11 IEEE INT C EL C, V11, P203
   PAPANICOLAOU ON, 1942, SCIENCE, V95, P432
   Patten SF, 1996, ACTA CYTOL, V40, P45
   PAUGAMMOISY H, 2009, HDB NATURAL IN PRESS
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   WU BJ, 2001, INTRO NEURAL DYNAMIC
   Wu HS, 2000, J MICROSC-OXFORD, V197, P296, DOI 10.1046/j.1365-2818.2000.00653.x
NR 29
TC 3
Z9 4
U1 0
U2 1
PY 2010
VL 6352
BP 117
EP +
PN I
WC Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Computer Science, Theory & Methods
DA 2023-11-11
ER

PT C
AU Cai, RT
   Wu, QX
   Wang, P
   Sun, HH
   Wang, ZC
AF Cai, Rongtai
   Wu, Qingxiang
   Wang, Ping
   Sun, Honghai
   Wang, Zichen
BE Zhang, Y
   Zhou, ZH
   Zhang, C
   Li, Y
TI Moving Target Detection and Classification Using Spiking Neural Networks
SO INTELLIGENT SCIENCE AND INTELLIGENT DATA ENGINEERING, ISCIDE 2011
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 2nd Sino-Foreign-Interchange Workshop on Intelligence Science and
   Intelligent Data Engineering (IScIDE)
CY OCT 23-25, 2011
CL Xian, PEOPLES R CHINA
DE Spiking Neural Network; Hebb learning rule; object detection and
   classification; visual surveillance
AB We proposed a spiking neural network (SNN) to detect moving target in video streams and classify them into real categorization in this paper. The proposed SNN uses spike trains to encoding information such as the gray value of pixels or feature parameters of the target, detects moving target by simulating the visual cortex for motion detection in biological system with axonal delays and classify them into different categorizations according to their distance to categorization's centers found by Hebb learning rule. The experimental results show that the proposed SNN is promising in intelligence computation and applicable in general visual surveillance system.
C1 [Cai, Rongtai; Wu, Qingxiang; Wang, Ping] Fujian Normal Univ, Sch Phys Opt Elect & Informat, Fuzhou 350108, Fujian, Peoples R China.
   [Sun, Honghai; Wang, Zichen] Chinese Acad Sci, Changchun Inst Opt, Fine Mech & Phys, Changchun 130033, Peoples R China.
RP Cai, RT (corresponding author), Fujian Normal Univ, Sch Phys Opt Elect & Informat, Fuzhou 350108, Fujian, Peoples R China.
EM gjrtcai@163.com
CR Bothe S.M., 2003, THESIS
   Caunce A., 2010, P BRIT MACH VIS C
   Dayan P., 2001, THEORETICAL NEUROSCI
   Gerstner W., 2002, SPIKING NEURON MODEL
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Kohn M, 2010, CONSTELLATIONS, V17, P572, DOI 10.1111/j.1467-8675.2010.00615.x
   Lalonde J F, 2011, THESIS
   Lin JW, 2002, TRENDS NEUROSCI, V25, P449, DOI 10.1016/S0166-2236(02)02212-9
   Lipton A. J., 1998, 4 IEEE WORKSH APPL C
   MAASS W., 1999, PULSED NEURAL NETWOR
   Natschlager T., 1999, THESIS
   Thorpe SJ, 2000, ISCAS 2000: IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS - PROCEEDINGS, VOL IV, P405, DOI 10.1109/ISCAS.2000.858774
   Wu QX, 2008, LECT NOTES COMPUT SC, V5227, P76
NR 13
TC 2
Z9 2
U1 0
U2 5
PY 2012
VL 7202
BP 210
EP 217
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods
DA 2023-11-11
ER

PT J
AU Stanojevic, A
   Cherubini, G
   Wozniak, S
   Eleftheriou, E
AF Stanojevic, Ana
   Cherubini, Giovanni
   Wozniak, Stanislaw
   Eleftheriou, Evangelos
TI Time-encoded multiplication-free spiking neural networks: application to
   data classification tasks
SO NEURAL COMPUTING & APPLICATIONS
DT Article
DE Spiking neural network; Temporal coding; Low-complexity data
   classification; Multiplication-free inference; Backpropagation through
   time
ID STORAGE
AB Spiking neural networks (SNNs) are mimicking computationally powerful biologically inspired models in which neurons communicate through sequences of spikes, regarded here as sparse binary sequences of zeros and ones. In neuroscience it is conjectured that time encoding, where the information is carried by the temporal position of spikes, is playing a crucial role at least in some parts of the brain where estimation of the spiking rate with a large latency cannot take place. Motivated by the efficiency of temporal coding, compared with the widely used rate coding, the goal of this paper is to develop and train an energy-efficient time-coded deep spiking neural network system. To ensure that the similarity among input stimuli is translated into a correlation of the spike sequences, we introduce correlative temporal encoding and extended correlative temporal encoding techniques to map analog input information into input spike patterns. Importantly, we propose an implementation where all multiplications in the system are replaced with at most a few additions. As a more efficient alternative to both rate-coded SNNs and artificial neural networks, such system represents a preferable solution for the implementation of neuromorphic hardware. We consider data classification tasks where input spike patterns are presented to a feed-forward architecture with leaky-integrate-and-fire neurons. The SNN is trained by backpropagation through time with the objective to match sequences of output spikes with those of specifically designed target spike patterns, each corresponding to exactly one class. During inference the target spike pattern with the smallest van Rossum distance from the output spike pattern determines the class. Extensive simulations indicate that the proposed system achieves a classification accuracy at par with that of state-of-the-art machine learning models.
C1 [Stanojevic, Ana; Cherubini, Giovanni; Wozniak, Stanislaw; Eleftheriou, Evangelos] IBM Res Zurich, Ruschlikon, Switzerland.
   [Stanojevic, Ana] Ecole Polytech Fed Lausanne EPFL, Lausanne, Switzerland.
   [Eleftheriou, Evangelos] Axelera AI, Zurich, Switzerland.
RP Stanojevic, A (corresponding author), IBM Res Zurich, Ruschlikon, Switzerland.; Stanojevic, A (corresponding author), Ecole Polytech Fed Lausanne EPFL, Lausanne, Switzerland.
EM ans@zurich.ibm.com
CR Bellec G, 2018, ADV NEUR IN, V31
   Bellec G, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-17236-y
   Bohnstingl T, 2022, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2022.3153985
   Bohte SM, 2011, LECT NOTES COMPUT SC, V6791, P60, DOI 10.1007/978-3-642-21735-7_8
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Boybat I, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-04933-y
   Bulat A, 2019, ARXIV
   Cao ZQ, 2015, NEURAL COMPUT APPL, V26, P1839, DOI 10.1007/s00521-015-1848-5
   Cherubini G, 2016, COMPUTER, V49, P43, DOI 10.1109/MC.2016.117
   Comsa IM, 2022, IEEE T NEUR NET LEAR, V33, P5939, DOI 10.1109/TNNLS.2021.3071976
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411
   Courbariaux M., 2016, ARXIV
   Dasgupta S, 2017, SCIENCE, V358, P793, DOI 10.1126/science.aam9868
   Devlin J., 2018, PREPRINT
   Eichler K, 2017, NATURE, V548, P175, DOI 10.1038/nature23455
   Fabre-Thorpe M, 1998, NEUROREPORT, V9, P303, DOI 10.1097/00001756-199801260-00023
   Farsa EZ, 2019, IEEE T CIRCUITS-II, V66, P1582, DOI 10.1109/TCSII.2019.2890846
   Furui S, 2012, IEEE SIGNAL PROC MAG, V29, P16, DOI 10.1109/MSP.2012.2209906
   Garain A, 2021, NEURAL COMPUT APPL, V33, P12591, DOI 10.1007/s00521-021-05910-1
   Gardner B, 2015, ARXIV
   Gardner B, 2015, NEURAL COMPUT, V27, P2548, DOI 10.1162/NECO_a_00790
   Geifman YG, 2018, US
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   Ghosh-Dastidar S, 2009, ADV INTEL SOFT COMPU, V61, P167
   Gollisch T, 2008, SCIENCE, V319, P1108, DOI 10.1126/science.1149639
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Greene D., 2006, PROC 23 INT C MACHIN, P377
   Horowitz M, 2014, ISSCC DIG TECH PAP I, V57, P10, DOI 10.1109/ISSCC.2014.6757323
   Hu SG, 2021, NEURAL COMPUT APPL, V33, P12317, DOI 10.1007/s00521-021-05832-y
   HUBEL DH, 1959, J PHYSIOL-LONDON, V148, P574, DOI 10.1113/jphysiol.1959.sp006308
   Huh D., 2017, ARXIV
   Hunsberger E., 2015, ARXIV
   Jankowski Mikolaj, 2020, IEEE INT WORK SIGN P, P1
   Jimenez-Romero C, 2017, NEURAL COMPUT APPL, V28, pS755, DOI 10.1007/s00521-016-2398-1
   Johansson RS, 2004, NAT NEUROSCI, V7, P170, DOI 10.1038/nn1177
   Kingma D. P., 2014, ARXIV, DOI DOI 10.48550/ARXIV.1412.6980
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Kubke MF, 2002, J NEUROSCI, V22, P7671
   Lin M., 2020, ADV NEURAL INFORM PR, V33, P7474
   Lin Mingbao, 2021, ARXIV
   Liu JX, 2021, NEURAL COMPUT APPL, V33, P11753, DOI 10.1007/s00521-021-05817-x
   Liu ZC, 2018, LECT NOTES COMPUT SC, V11219, P747, DOI 10.1007/978-3-030-01267-0_44
   Luo YH, 2022, NEURAL COMPUT APPL, V34, P9967, DOI 10.1007/s00521-022-06984-1
   Manning C, 1999, FDN STAT NATURAL LAN
   Marimuthu, 2010, ARXIV
   Mostafa H, 2018, IEEE T NEUR NET LEAR, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Ponulak F, 2011, ACTA NEUROBIOL EXP, V71, P409
   Qaiser S., 2018, INT J COMPUTER APPL, V181, P25, DOI [10.5120/ijca2018917395, DOI 10.5120/IJCA2018917395]
   Ranjan JAK, 2020, J SUPERCOMPUT, V76, P6545, DOI 10.1007/s11227-019-02881-y
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Ryali C. K., 2020, PROC 37 INT C MACH L, P8295
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Sebastian A, 2018, J APPL PHYS, V124, DOI 10.1063/1.5042413
   She XY, 2019, DES AUT TEST EUROPE, P450, DOI [10.23919/DATE.2019.8714846, 10.23919/date.2019.8714846]
   SIMONS G, 1971, ANN MATH STAT, V42, P1735, DOI 10.1214/aoms/1177693172
   Sjöström PJ, 2008, PHYSIOL REV, V88, P769, DOI 10.1152/physrev.00016.2007
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Stanojevic A, 2020, 27193 NBER, P1, DOI DOI 10.1353/ECA.2020.0000
   Strubell Emma, 2019, ARXIV
   Sze V, 2019, NEURIPS, V138
   Togaçar M, 2021, NEURAL COMPUT APPL, V33, P6147, DOI 10.1007/s00521-020-05388-3
   van Rossum MCW, 2001, NEURAL COMPUT, V13, P751, DOI 10.1162/089976601300014321
   Venkatesan V, 2018, IEEE INT CONGR BIG, P232, DOI 10.1109/BigDataCongress.2018.00040
   Volobuev, 2011, NAT SCI, V3, P53
   Wang Y, 2021, P IEEECVF INT C COMP, P5360
   Wozniak S, 2020, NAT MACH INTELL, V2, P325, DOI 10.1038/s42256-020-0187-0
   Xiao H, 2017, ARXIV PREPRINT ARXIV
   Xu ZH, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P5178, DOI 10.1109/ICCV48922.2021.00515
   Zambrano D, 2017, ARXIV
   Zechun Liu, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12359), P143, DOI 10.1007/978-3-030-58568-6_9
   Zenke F, 2018, NEURAL COMPUT, V30, P1514, DOI 10.1162/neco_a_01086
   Zhang AG, 2019, NEUROCOMPUTING, V365, P102, DOI 10.1016/j.neucom.2019.07.009
   Zhao DC, 2020, FRONT COMPUT NEUROSC, V14, DOI 10.3389/fncom.2020.576841
NR 74
TC 0
Z9 0
U1 5
U2 23
PD MAR
PY 2023
VL 35
IS 9
SI SI
BP 7017
EP 7033
DI 10.1007/s00521-022-07910-1
EA DEC 2022
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT J
AU Kim, DW
   Yi, WS
   Choi, JY
   Ashiba, K
   Baek, JU
   Jun, HS
   Kim, JJ
   Park, JG
AF Kim, Dong Won
   Yi, Woo Seok
   Choi, Jin Young
   Ashiba, Kei
   Baek, Jong Ung
   Jun, Han Sol
   Kim, Jae Joon
   Park, Jea Gun
TI Double MgO-Based Perpendicular Magnetic Tunnel Junction for Artificial
   Neuron
SO FRONTIERS IN NEUROSCIENCE
DT Article
DE neuromorphic; MRAM; spiking neuron; spiking neural network; artificial
   neuron
ID SPIKING; NETWORKS; MODEL
AB A perpendicular spin transfer torque (p-STT)-based neuron was developed for a spiking neural network (SNN). It demonstrated the integration behavior of a typical neuron in an SNN; in particular, the integration behavior corresponding to magnetic resistance change gradually increased with the input spike number. This behavior occurred when the spin electron directions between double Co2Fe6B2 free and pinned layers in the p-STT-based neuron were switched from parallel to antiparallel states. In addition, a neuron circuit for integrate-and-fire operation was proposed. Finally, pattern-recognition simulation was performed for a single-layer SNN.
C1 [Kim, Dong Won; Baek, Jong Ung; Jun, Han Sol; Park, Jea Gun] Hanyang Univ, Dept Nanoscale Semicond Engn, Seoul, South Korea.
   [Yi, Woo Seok; Kim, Jae Joon] Pohang Univ Sci & Technol, Dept Creat IT Engn, Pohang, South Korea.
   [Choi, Jin Young] Hanyang Univ, MRAM Ctr, Dept Elect & Comp Engn, Seoul, South Korea.
   [Ashiba, Kei; Park, Jea Gun] SUMCO Corp, Wafer Engn Dept, Imari, Japan.
RP Park, JG (corresponding author), Hanyang Univ, Dept Nanoscale Semicond Engn, Seoul, South Korea.; Park, JG (corresponding author), SUMCO Corp, Wafer Engn Dept, Imari, Japan.
EM parkjgl@hanyang.ac.kr
CR [Anonymous], TECH DIG INT ELECT D
   [Anonymous], PROC CVPR IEEE
   Burr G. W., 2014, IEDM, DOI 10.1109/IEDM.2014.7047135
   Du ZD, 2015, PROCEEDINGS OF THE 48TH ANNUAL IEEE/ACM INTERNATIONAL SYMPOSIUM ON MICROARCHITECTURE (MICRO-48), P494, DOI 10.1145/2830772.2830789
   Dutta S, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-07418-y
   Gentet LJ, 2000, BIOPHYS J, V79, P314, DOI 10.1016/S0006-3495(00)76293-X
   Grollier J, 2016, P IEEE, V104, P2024, DOI 10.1109/JPROC.2016.2597152
   Hansen M, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00091
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/TPAMI.2019.2913372, 10.1109/CVPR.2018.00745]
   Indiveri G, 2013, NANOTECHNOLOGY, V24, DOI 10.1088/0957-4484/24/38/384010
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Johnson AP, 2018, IEEE T CIRCUITS-I, V65, P687, DOI 10.1109/TCSI.2017.2726763
   Kondo K, 2018, J PHYS D APPL PHYS, V51, DOI 10.1088/1361-6463/aad592
   Kurenkov A, 2019, ADV MATER, V31, DOI 10.1002/adma.201900636
   Lee DY, 2016, SCI REP-UK, V6, DOI 10.1038/srep38125
   Lee DY, 2016, NANOSCALE RES LETT, V11, DOI 10.1186/s11671-016-1637-9
   Lee JM, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00191
   Lee SE, 2016, NPG ASIA MATER, V8, DOI 10.1038/am.2016.162
   Liyanagedera CM, 2017, PHYS REV APPL, V8, DOI 10.1103/PhysRevApplied.8.064017
   MacLaren JM, 2001, J APPL PHYS, V89, P6895, DOI 10.1063/1.1357839
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Mizrahi A, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-03963-w
   Monroe D, 2014, COMMUN ACM, V57, P13, DOI 10.1145/2601069
   Querlioz D, 2015, P IEEE, V103, P1398, DOI 10.1109/JPROC.2015.2437616
   Sengupta A, 2016, SCI REP-UK, V6, DOI 10.1038/srep30039
   Sharmin S, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-11732-w
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Sourikopoulos I, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00123
   Srinivasan G, 2017, DES AUT TEST EUROPE, P530, DOI 10.23919/DATE.2017.7927045
   Suzuki M, 2016, ACTA MATER, V106, P155, DOI 10.1016/j.actamat.2016.01.011
   Szegedy C., 2015, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2015.7298594
   Torrejon J, 2017, NATURE, V547, P428, DOI 10.1038/nature23011
   Tuma T, 2016, NAT NANOTECHNOL, V11, P693, DOI [10.1038/nnano.2016.70, 10.1038/NNANO.2016.70]
   Victora RH, 2003, IEEE T MAGN, V39, P710, DOI 10.1109/TMAG.2003.808998
   Zahari F, 2015, AIMS MATER SCI, V2, P203, DOI 10.3934/matersci.2015.3.203
   Zhang DM, 2016, IEEE INT SYMP NANO, P173, DOI 10.1145/2950067.2950105
   Ziegler M, 2015, IEEE T BIOMED CIRC S, V9, P197, DOI 10.1109/TBCAS.2015.2410811
NR 38
TC 12
Z9 12
U1 0
U2 9
PD APR 30
PY 2020
VL 14
AR 309
DI 10.3389/fnins.2020.00309
WC Neurosciences
DA 2023-11-11
ER

PT C
AU Yao, HY
   Huang, HP
   Huang, YC
   Lo, CC
AF Yao, Huang-Yu
   Huang, Hsuan-Pei
   Huang, Yu-Chi
   Lo, Chung-Chuan
GP IEEE
TI Flyintel - a Platform for Robot Navigation based on a Brain-Inspired
   Spiking Neural Network
SO 2019 IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE CIRCUITS
   AND SYSTEMS (AICAS 2019)
DT Proceedings Paper
CT 1st IEEE International Conference on Artificial Intelligence Circuits
   and Systems (AICAS)
CY MAR 18-20, 2019
CL Hsinchu, TAIWAN
DE spiking neural network; navigation; robotics; Drosophila; central
   complex
AB Spiking neural networks (SNN) are regarded by many as the "third generation network" that will solve computation problems in a more biologically realistic way. In our project, we design a robotic platform controlled by a user-defined SNN in order to develop a next generation artificial intelligence robot with high flexibility. This paper describes the preliminary progress of the project. We first implement a basic simple decision network and the robot is able to perform a basic but vital foraging and risk-avoiding task. Next, we implement the neural network of the fruit fly central complex in order to endow the robot with spatial orientation memory, a crucial function underlying the ability of spatial navigation.
C1 [Yao, Huang-Yu; Huang, Hsuan-Pei; Huang, Yu-Chi; Lo, Chung-Chuan] Natl Tsing Hua Univ, Inst Syst Neurosci, Hsinchu 30013, Taiwan.
RP Yao, HY (corresponding author), Natl Tsing Hua Univ, Inst Syst Neurosci, Hsinchu 30013, Taiwan.
EM huangyu_yao@lolab-nthu.org; peihuang@lolab-nthu.org; hyc@lolab-nthu.org;
   cclo@mx.nthu.edu.tw
CR Fernandes E, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL TECHNOLOGY (ICIT), P3320, DOI 10.1109/ICIT.2015.7125590
   Huang YC, 2019, FRONT NEUROINFORM, V12, DOI 10.3389/fninf.2018.00099
   Kreiser R, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351509
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Meng ZH, 2017, IEEE/SICE I S SYS IN, P651, DOI 10.1109/SII.2017.8279295
   Moser EI, 2008, ANNU REV NEUROSCI, V31, P69, DOI 10.1146/annurev.neuro.31.061307.090723
   Su TS, 2017, NAT COMMUN, V8, DOI 10.1038/s41467-017-00191-6
   Tang G., 2018, ICONS 18
NR 8
TC 1
Z9 2
U1 0
U2 6
PY 2019
BP 219
EP 220
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
DA 2023-11-11
ER

PT J
AU Song, S
   Kim, M
   Jeon, B
   Ryu, D
   Kim, S
   Lee, K
   Lee, JH
   Kim, JJ
   Shim, W
   Kwon, D
   Park, BG
AF Song, Seunghwan
   Kim, Munhyeon
   Jeon, Bosung
   Ryu, Donghyun
   Kim, Sihyun
   Lee, Kitae
   Lee, Jong-Ho
   Kim, Jae-Joon
   Shim, Wonbo
   Kwon, Daewoong
   Park, Byung-Gook
TI Spiking Neural Network With Weight-Sharing Synaptic Array for
   Multi-input Processing
SO IEEE ELECTRON DEVICE LETTERS
DT Article
DE FeFETs; Neurons; Parallel processing; Logic gates; Integrated circuit
   modeling; Biological neural networks; Transistors; Spiking neural
   networks (SNNs); multi-input processing SNN system; ferroelectric FETs
ID FLASH
AB A multi-input processing spiking neural network inference system (MSS) is proposed to enhance the parallel processing capabilities of the spiking neural network (SNN) inference relative to the conventional SNN inference. Processing multiple input samples using MSS with shared synaptic arrays for compute-in-memory drastically reduces the number of synaptic arrays assigned for parallel processing, thereby minimizing the effort required by parallel processing networks. A shared weight array is evaluated using the 4-bit quantization capabilities of a manufactured ferroelectric field-effect-transistor as a synaptic device. A batch action was implemented as a multi-input on a 3-layer fully connected network to verify the MSS. The benefits of MSS in energy consumption and area throughput are rigorously investigated and estimated based on the number of multi-input processing. It is proven that the simultaneously processing of multiple input samples using the proposed MSS boosts the energy and area efficiency by up to 9.12 and 242 times, respectively.
C1 [Song, Seunghwan; Kim, Munhyeon; Jeon, Bosung; Ryu, Donghyun; Kim, Sihyun; Lee, Kitae; Lee, Jong-Ho; Kim, Jae-Joon; Park, Byung-Gook] Seoul Natl Univ, Dept Elect & Comp Engn, Seoul 08826, South Korea.
   [Song, Seunghwan; Kim, Munhyeon; Jeon, Bosung; Ryu, Donghyun; Kim, Sihyun; Lee, Kitae; Lee, Jong-Ho; Kim, Jae-Joon; Park, Byung-Gook] Seoul Natl Univ, Interuniv Semicond Res Ctr, Seoul 08826, South Korea.
   [Shim, Wonbo] Seoul Natl Univ Sci & Technol, Dept Elect & Informat Engn, Seoul 01811, South Korea.
   [Kwon, Daewoong] Inha Univ, Dept Elect & Comp Engn, Incheon 22212, South Korea.
   [Kwon, Daewoong] Inha Univ, 3D Convergence Ctr, Incheon 22212, South Korea.
RP Park, BG (corresponding author), Seoul Natl Univ, Dept Elect & Comp Engn, Seoul 08826, South Korea.; Park, BG (corresponding author), Seoul Natl Univ, Interuniv Semicond Res Ctr, Seoul 08826, South Korea.; Shim, W (corresponding author), Seoul Natl Univ Sci & Technol, Dept Elect & Informat Engn, Seoul 01811, South Korea.; Kwon, D (corresponding author), Inha Univ, Dept Elect & Comp Engn, Incheon 22212, South Korea.; Kwon, D (corresponding author), Inha Univ, 3D Convergence Ctr, Incheon 22212, South Korea.
EM wbshim@seoultech.ac.kr; dw79kwon@inha.ac.kr; bgpark@snu.ac.kr
CR [Anonymous], 2017, INT EL DEVICES MEET
   [Anonymous], BSIM CMG TECHNICAL M
   Bengio Y., 2007, LARGE SCALE KERNEL M, DOI DOI 10.7551/MITPRESS/7496.003.0016
   Cruz-Albrecht JM, 2012, IEEE T BIOMED CIRC S, V6, P246, DOI 10.1109/TBCAS.2011.2174152
   Diehl PU, 2015, IEEE IJCNN
   Dutta S, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00634
   Hadidi R, 2018, IEEE ROBOT AUTOM LET, V3, P3709, DOI 10.1109/LRA.2018.2856261
   Han JH, 2020, TSINGHUA SCI TECHNOL, V25, P479, DOI 10.26599/TST.2019.9010019
   Huang YP, 2019, Arxiv, DOI arXiv:1811.06965
   Hwang S, 2018, IEEE ELECTR DEVICE L, V39, P1441, DOI 10.1109/LED.2018.2853635
   Kim CH, 2018, IEEE T ELECTRON DEV, V65, P1774, DOI 10.1109/TED.2018.2817266
   Kim H, 2018, IEEE ELECTR DEVICE L, V39, P630, DOI 10.1109/LED.2018.2809661
   Kim TH, 2020, IEEE T NANOTECHNOL, V19, P475, DOI 10.1109/TNANO.2020.2996814
   LeCun Y., 1995, HDB BRAIN THEORY NEU, P276, DOI 10.5555/303568.303704
   Lee J, 2019, IEEE ELECTR DEVICE L, V40, P1358, DOI 10.1109/LED.2019.2928335
   Mulaosmanovic H, 2017, S VLSI TECH, pT176, DOI 10.23919/VLSIT.2017.7998165
   Ni K, 2018, INT EL DEVICES MEET
   Ni K, 2018, IEEE ELECTR DEVICE L, V39, P1656, DOI 10.1109/LED.2018.2872347
   Painkras E, 2013, IEEE J SOLID-ST CIRC, V48, P1943, DOI 10.1109/JSSC.2013.2259038
   Regev A, 2020, 2020 2ND IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE CIRCUITS AND SYSTEMS (AICAS 2020), P145, DOI [10.1109/aicas48895.2020.9073840, 10.1109/AICAS48895.2020.9073840]
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Wu J, 2017, NATL KEY LAB NOVEL S, V5, P23, DOI DOI 10.1007/978-3-642-28661-2-5
NR 22
TC 2
Z9 2
U1 4
U2 23
PD OCT
PY 2022
VL 43
IS 10
BP 1657
EP 1660
DI 10.1109/LED.2022.3197239
WC Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Lin, J
   Yuan, JS
AF Lin, Jie
   Yuan, Jiann-Shiun
TI Analysis and Simulation of Capacitor-Less ReRAM-Based Stochastic Neurons
   for the in-Memory Spiking Neural Network
SO IEEE TRANSACTIONS ON BIOMEDICAL CIRCUITS AND SYSTEMS
DT Article; Proceedings Paper
CT IEEE Biomedical Circuits and Systems Conference (BioCAS)
CY OCT 19-21, 2017
CL Torino, ITALY
DE Neuromorphic; resistive random-access memory (ReRAM); spiking neural
   network; stochastic neuron; unsupervised learning
ID DESIGN; ENERGY; NOISE; MODEL
AB The stochastic neuron is a key for event-based probabilistic neural networks. We propose a stochastic neuron using a metal-oxide resistive random-access memory (ReRAM). The ReRAM's conducting filament with built-in stochasticity is used to mimic the neuron's membrane capacitor, which temporally integrates input spikes. A capacitor-less neuron circuit is designed, laid out, and simulated. The output spiking train of the neuron obeys the Poisson distribution. Using the 65-nm CMOS technology node, the area of the neuron is 14 x 5 mu m(2), which is one ninth the size of a 1-pF capacitor. The average power consumption of the neuron is 1.289 mu W. We introduce the neural array-A modified one-transistor-one-ReRAM (1T1R) crossbar that integrates the ReRAM neurons with ReRAM synapses to form a compact and energy efficient in-memory spiking neural network. A spiking deep belief network (DBN) with a noisy rectified linear unit (NReLU) is trained and mapped to the spiking DBN using the proposed ReRAM neurons. Simulation results show that the ReRAM neuron-based DBN is able to recognize the handwritten digits with 94.7% accuracy and is robust against theReRAMprocess variation effect.
C1 [Lin, Jie; Yuan, Jiann-Shiun] Univ Cent Florida, Dept Elect & Comp Engn, Orlando, FL 32826 USA.
RP Yuan, JS (corresponding author), Univ Cent Florida, Dept Elect & Comp Engn, Orlando, FL 32826 USA.
EM ljie@knights.ucf.edu; yuanj@mail.ucf.edu
CR ABBOTT LF, 1993, PHYS REV E, V48, P1483, DOI 10.1103/PhysRevE.48.1483
   Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   Al-Shedivat M, 2015, IEEE J EM SEL TOP C, V5, P242, DOI 10.1109/JETCAS.2015.2435512
   Ankit A, 2017, DES AUT CON, DOI 10.1145/3061639.3062311
   [Anonymous], P EUR WORKSH CMOS VA, DOI DOI 10.1109/VARI.2014.6957074
   [Anonymous], 2015, INT J COMPUT VISION, DOI DOI 10.1007/s11263-014-0788-3
   [Anonymous], P 2011 INT EL DEV M
   [Anonymous], 2003, BSIM4 3 0 MOSFET MOD
   [Anonymous], P WORLD C NEUR
   [Anonymous], 2016, 2016 IEE INT C REB C, DOI [DOI 10.1109/ICRC.2016.7738691, 10.1109/ICRC.2016.7738691]
   [Anonymous], ADV NEURAL INFORM PR, DOI DOI 10.1103/PHYSREVAPPLIED.9.044036
   [Anonymous], P 2016 IEEE INT EL D
   Chechik G, 1999, NEURAL COMPUT, V11, P2061, DOI 10.1162/089976699300016089
   Cui JW, 2016, IEEE INT SYMP CIRC S, P121, DOI 10.1109/ISCAS.2016.7527185
   Diehl Peter U, 2015, 2015 INT JOINT C NEU, P1, DOI [DOI 10.1109/IJCNN.2015.7280696, 10.1109/IJCNN.2015.7280696]
   Dong XY, 2012, IEEE T COMPUT AID D, V31, P994, DOI 10.1109/TCAD.2012.2185930
   Fang Z, 2010, IEEE ELECTR DEVICE L, V31, P476, DOI 10.1109/LED.2010.2041893
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Hu M, 2016, DES AUT CON, DOI 10.1145/2897937.2898010
   Indiveri G, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00073
   Jiang ZZ, 2014, INT CONF SIM SEMI PR, P41, DOI 10.1109/SISPAD.2014.6931558
   Jo SH, 2009, NANO LETT, V9, P496, DOI 10.1021/nl803669s
   Jolivet R, 2006, J COMPUT NEUROSCI, V21, P35, DOI 10.1007/s10827-006-7074-5
   Kwon MW, 2017, J NANOSCI NANOTECHNO, V17, P3038, DOI 10.1166/jnn.2017.14025
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee MJ, 2011, NAT MATER, V10, P625, DOI [10.1038/nmat3070, 10.1038/NMAT3070]
   Li S., 2016, PROC 53ND ACM EDAC I, P1
   Lin J, 2017, J LOW POWER ELECTRON, V13, P497, DOI 10.1166/jolpe.2017.1503
   Lin J, 2016, 2016 13TH IEEE INTERNATIONAL CONFERENCE ON SOLID-STATE AND INTEGRATED CIRCUIT TECHNOLOGY (ICSICT), P713, DOI 10.1109/ICSICT.2016.7999020
   Lin J, 2016, J LOW POWER ELECTRON, V12, P218, DOI 10.1166/jolpe.2016.1445
   Maass W, 2014, P IEEE, V102, P860, DOI 10.1109/JPROC.2014.2310593
   McPherson J, 2003, APPL PHYS LETT, V82, P2121, DOI 10.1063/1.1565180
   Moreno-Bote R, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003522
   Nair V, 2010, INT C MACH LEARN HAI, V27, P807
   Naous R, 2016, IEEE T NANOTECHNOL, V15, P15, DOI 10.1109/TNANO.2015.2493960
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   Palma G, 2013, PROCEEDINGS OF THE 2013 IEEE/ACM INTERNATIONAL SYMPOSIUM ON NANOSCALE ARCHITECTURES (NANOARCH), P95, DOI 10.1109/NanoArch.2013.6623051
   Posch C, 2011, IEEE J SOLID-ST CIRC, V46, P259, DOI 10.1109/JSSC.2010.2085952
   Roy K, 2003, P IEEE, V91, P305, DOI 10.1109/JPROC.2002.808156
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Tuma T, 2016, NAT NANOTECHNOL, V11, P693, DOI [10.1038/nnano.2016.70, 10.1038/NNANO.2016.70]
   Wong HSP, 2012, P IEEE, V100, P1951, DOI 10.1109/JPROC.2012.2190369
   Yu SM, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00186
   Zhao CY, 2016, IEEE T MULTI-SCALE C, V2, P265, DOI 10.1109/TMSCS.2016.2607164
NR 44
TC 16
Z9 19
U1 0
U2 11
PD OCT
PY 2018
VL 12
IS 5
BP 1004
EP 1017
DI 10.1109/TBCAS.2018.2843286
WC Engineering, Biomedical; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Pellegrini, T
   Zimmer, R
   Masquelier, T
AF Pellegrini, Thomas
   Zimmer, Romain
   Masquelier, Timothee
GP IEEE
TI LOW-ACTIVITY SUPERVISED CONVOLUTIONAL SPIKING NEURAL NETWORKS APPLIED TO
   SPEECH COMMANDS RECOGNITION
SO 2021 IEEE SPOKEN LANGUAGE TECHNOLOGY WORKSHOP (SLT)
SE IEEE Workshop on Spoken Language Technology
DT Proceedings Paper
CT IEEE Spoken Language Technology Workshop (SLT)
CY JAN 19-22, 2021
CL ELECTR NETWORK
DE Spiking neural networks; surrogate gradient; speech command recognition
AB Deep Neural Networks (DNNs) are the current state-of-the-art models in many speech related tasks. There is a growing interest, though, for more biologically realistic, hardware friendly and energy efficient models, named Spiking Neural Networks (SNNs). Recently, it has been shown that SNNs can be trained efficiently, in a supervised manner, using backpropagation with a surrogate gradient trick. In this work, we report speech command (SC) recognition experiments using supervised SNNs. We explored the Leaky-Integrate-Fire (LIF) neuron model for this task, and show that a model comprised of stacked dilated convolution spiking layers can reach an error rate very close to standard DNNs on the Google SC v1 dataset: 5.5%, while keeping a very sparse spiking activity, below 5%, thank to a new regularization term. We also show that modeling the leakage of the neuron membrane potential is useful, since the LIF model outperformed its non-leaky model counterpart significantly.
C1 [Pellegrini, Thomas; Zimmer, Romain] Univ Toulouse, IRIT, Toulouse, France.
   [Zimmer, Romain; Masquelier, Timothee] Univ Toulouse 3, CNRS, CERCO UMR 5549, Toulouse, France.
RP Pellegrini, T (corresponding author), Univ Toulouse, IRIT, Toulouse, France.
CR Adrian ED, 1926, J PHYSIOL-LONDON, V61, P151, DOI 10.1113/jphysiol.1926.sp002281
   Bellec G., 2018, ADV NEURAL INFORM PR, P787
   de Andrade Douglas Coimbra, 2018, NEURAL ATTENTION MOD
   Gerstner W., 2002, SPIKING NEURON MODEL
   Gerstner W, 2009, SCIENCE, V326, P379, DOI 10.1126/science.1181936
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Kobayashi R, 2009, FRONT COMPUT NEUROSC, V3, DOI 10.3389/neuro.10.009.2009
   Liu L., 2019, ARXIV PREPRINT ARXIV
   Maass W, 1999, INFORM COMPUT, V153, P26, DOI 10.1006/inco.1999.2806
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Olshausen BA, 2003, J COGNITIVE NEUROSCI, V15, P154, DOI 10.1162/089892903321107891
   Santos, 2015, LIBROSA V0 4 0
   STEIN RB, 1967, BIOPHYS J, V7, P37, DOI 10.1016/S0006-3495(67)86574-3
   Tavanaei A, 2017, LECT NOTES COMPUT SC, V10639, P899, DOI 10.1007/978-3-319-70136-3_95
   Warden Pete, 2018, SPEECH COMMANDS DATA
   Wu JC, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351221
   Wu JB, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00199
   Zhang ML, 2019, AAAI CONF ARTIF INTE, P1327
   Zhang Y, 2015, IEEE T NEUR NET LEAR, V26, P2635, DOI 10.1109/TNNLS.2015.2388544
   Zimmer Romain, 2019, ARXIV PREPRINT ARXIV
NR 20
TC 17
Z9 17
U1 0
U2 8
PY 2021
BP 97
EP 103
DI 10.1109/SLT48900.2021.9383587
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
DA 2023-11-11
ER

PT C
AU Zhang, ZM
   Wu, QX
   Wang, X
   Sun, QY
AF Zhang, Zhenmin
   Wu, Qingxiang
   Wang, Xuan
   Sun, Qiyan
BE Xiao, Z
   Tong, Z
   Li, K
   Wang, X
   Li, K
TI Training Spiking Neural Networks With the Improved Grey-Level
   Co-occurrence Matrix Algorithm for Texture Analysis
SO 2015 11TH INTERNATIONAL CONFERENCE ON NATURAL COMPUTATION (ICNC)
DT Proceedings Paper
CT 11th International Conference on Natural Computation (ICNC) / 12th
   International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)
CY AUG 15-17, 2015
CL Zhangjiajie, PEOPLES R CHINA
DE spiking neural network; GLCM algorithm; Feature Extraction; Texture
   Classification
AB Texture refers to the tactile impression, such as rough, silky, bumpy, and other texture terms. The Grey-Level Co-occurrence Matrix (GLCM) algorithm is widely used in visual images for texture feature extraction, image structure characterization analysis and texture classification. The GLCM can not only give the statistics of pixel gray values occur in an image, but also give multiple characteristics of the images. Since the primate brain, which is constructed with spiking neurons, has excellent performance in terms of image feature extraction, the improved GLCM algorithm is used to train a spiking neural network and also to simulate the brain's ability about extract key information and utilize these extracted feature information to classify different texture image. Experimental results in this article show that this combination of the GLCM and spiking neural network can effectively extract image features, and the texture classification results is also to achieve satisfactory effect.
C1 [Zhang, Zhenmin; Wu, Qingxiang; Wang, Xuan; Sun, Qiyan] Fujian Normal Univ, Coll Photon & Elect Engn, Fuzhou, Peoples R China.
RP Wu, QX (corresponding author), Fujian Normal Univ, Coll Photon & Elect Engn, Fuzhou, Peoples R China.
CR [Anonymous], SYSTEMS MAN CYBERNET
   Beck MW, 2014, ECOL INDIC, V45, P195, DOI 10.1016/j.ecolind.2014.04.002
   Benco M, 2007, RADIOENGINEERING, V16, P64
   Benco M, 2014, INT J ADV ROBOT SYST, V11, DOI 10.5772/58692
   Bergounioux M, 2014, ADV IMAG ELECT PHYS, V181, P35, DOI 10.1016/B978-0-12-800091-5.00002-1
   Brodatz P., 1966, TEXTURE PHOTOGRAPHIC
   Brownstone RM, 2015, NEURON, V86, P9, DOI 10.1016/j.neuron.2015.03.056
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Kosik KS, 2013, NATURE, V503, P31, DOI 10.1038/503031a
   Kuebler E.S., 2013, IEEE IJCNN
   Mallat B. S., 2007, MALLAT WAVELET TOUR, V31, P85
   Masland RH, 2001, NAT NEUROSCI, V4, P877, DOI 10.1038/nn0901-877
   Mirzapour F, 2013, IRAN CONF ELECTR ENG
   Nazemi A, 2015, NEUROCOMPUTING, V152, P369, DOI 10.1016/j.neucom.2014.10.054
   Ojala T., 2002, OUTEX NEW FRAMEWORK, V1, P706
   Sadtler PT, 2014, NATURE, V512, P423, DOI 10.1038/nature13665
   Saroja GAS, 2013, 2013 IEEE CONFERENCE ON INFORMATION AND COMMUNICATION TECHNOLOGIES (ICT 2013), P1319
   Schultz SK, 2001, AM J PSYCHIAT, V158, P662, DOI 10.1176/appi.ajp.158.4.662
   Tamura H., 1978, TEXTURAL FEATURES CO, V8, P473
   Van de Wouwer G, 1999, IEEE T IMAGE PROCESS, V8, P592, DOI 10.1109/83.753747
   Wu Q.X., 2006, KNOWLEDGE REPRESENTA, V4, P2801
   Wu QX, 2013, NEUROCOMPUTING, V116, P3, DOI 10.1016/j.neucom.2012.01.046
   Wu QX, 2009, LECT NOTES ARTIF INT, V5755, P21
   Xue MS, 2014, NATURE, V511, P596, DOI 10.1038/nature13321
   Zhang J, 2008, HPCC 2008: 10TH IEEE INTERNATIONAL CONFERENCE ON HIGH PERFORMANCE COMPUTING AND COMMUNICATIONS, PROCEEDINGS, P782, DOI 10.1109/HPCC.2008.55
NR 25
TC 0
Z9 0
U1 1
U2 3
PY 2015
BP 1069
EP 1074
WC Computer Science, Theory & Methods
DA 2023-11-11
ER

PT C
AU Taeckens, E
   Dong, R
   Shah, S
AF Taeckens, Elijah
   Dong, Ryan
   Shah, Sahil
GP IEEE
TI A Biologically Plausible Spiking Neural Network for Decoding Kinematics
   in the Hippocampus and Premotor Cortex
SO 2023 11TH INTERNATIONAL IEEE/EMBS CONFERENCE ON NEURAL ENGINEERING, NER
SE International IEEE EMBS Conference on Neural Engineering
DT Proceedings Paper
CT 11th International IEEE EMBS Conference on Neural Engineering (IEEE/EMBS
   NER)
CY APR 24-27, 2023
CL IEEE Engn Med & Biol Soc, Baltimore, MD
HO IEEE Engn Med & Biol Soc
AB This work presents a spiking neural network for predicting kinematics from neural data towards accurate and energy-efficient brain machine interface. A brain machine interface is a technological system that interprets neural signals to allow motor impaired patients to control prosthetic devices. Spiking neural networks have the potential to improve brain machine interface technology due to their low power cost and close similarity to biological neural structures. The SNN in this study uses the leaky integrate-and-fire model to simulate the behavior of neurons, and learns using a local learning method that uses surrogate gradient to learn the parameters of the network. The network implements a novel continuous time output encoding scheme that allows for regression-based learning. The SNN is trained and tested offline on neural and kinematic data recorded from the premotor cortex of a primate and the hippocampus of a rat. The model is evaluated by finding the correlation between the predicted kinematic data and true kinematic data, and achieves peak Pearson Correlation Coefficients of 0.77 for the premotor cortex recordings and 0.80 for the hippocampus recordings. The accuracy of the model is benchmarked against a Kalman filter decoder and a LSTM network, as well as a spiking neural network trained with backpropagation to compare the effects of local learning.
C1 [Taeckens, Elijah; Dong, Ryan; Shah, Sahil] Univ Maryland, Dept Elect & Comp Engn, College Pk, MD 20742 USA.
RP Shah, S (corresponding author), Univ Maryland, Dept Elect & Comp Engn, College Pk, MD 20742 USA.
EM sshah389@umd.edu
CR Arthur JV, 2011, IEEE T CIRCUITS-I, V58, P1034, DOI 10.1109/TCSI.2010.2089556
   Boi F, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00563
   Chowdhury S. N., 2022, 20 IEEE INTERREGIONA
   Dethier J, 2013, J NEURAL ENG, V10, DOI 10.1088/1741-2560/10/3/036008
   Gehrig M, 2020, IEEE INT CONF ROBOT, P4195, DOI [10.1109/icra40945.2020.9197133, 10.1109/ICRA40945.2020.9197133]
   Glaser Joshua I, 2020, eNeuro, V7, DOI 10.1523/ENEURO.0506-19.2020
   Indiveri G, 2003, PROCEEDINGS OF THE 2003 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL IV, P820
   Jackson A, 2006, IEEE T NEUR SYS REH, V14, P187, DOI 10.1109/TNSRE.2006.875547
   Kaiser J, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00424
   Kim SP, 2008, J NEURAL ENG, V5, P455, DOI 10.1088/1741-2560/5/4/010
   Lawlor PN, 2018, J COMPUT NEUROSCI, V45, P173, DOI 10.1007/s10827-018-0696-6
   Liao JW, 2022, 2022 IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE CIRCUITS AND SYSTEMS (AICAS 2022): INTELLIGENT TECHNOLOGY IN THE POST-PANDEMIC ERA, P134, DOI 10.1109/AICAS54282.2022.9869846
   MEAD C, 1990, P IEEE, V78, P1629, DOI 10.1109/5.58356
   Mizuseki K, 2009, MULTIUNIT RECORDINGS
   Mizuseki K, 2009, NEURON, V64, P267, DOI 10.1016/j.neuron.2009.08.037
   Musallam S, 2004, SCIENCE, V305, P258, DOI 10.1126/science.1097938
   N. S. C. I. S. Center, 2022, TRAUMATIC SPINAL COR
   Neftci EO, 2019, Arxiv, DOI arXiv:1901.09948
   Perich M. G, 2018, EXTRACELLULAR NEURAL
   Shah S, 2019, I IEEE EMBS C NEUR E, P1138, DOI [10.1109/ner.2019.8717137, 10.1109/NER.2019.8717137]
   Shaikh S., 2022, HDB BIOCHIPS, P869
   Snoek J., 2012, PROC INT C NEURAL IN, V25, P2951
   Stuijt J, 2021, BRAIN EVENT DRIVEN F
   Sussillo D, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms13749
NR 24
TC 0
Z9 0
U1 0
U2 0
PY 2023
DI 10.1109/NER52421.2023.10123745
WC Computer Science, Artificial Intelligence; Engineering, Biomedical;
   Neurosciences
DA 2023-11-11
ER

PT J
AU Naveros, F
   Garrido, JA
   Carrillo, RR
   Ros, E
   Luque, NR
AF Naveros, Francisco
   Garrido, Jesus A.
   Carrillo, Richard R.
   Ros, Eduardo
   Luque, Niceto R.
TI Event- and Time-Driven Techniques Using Parallel CPU-GPU Co-processing
   for Spiking Neural Networks (vol 11, 7, 2017)
SO FRONTIERS IN NEUROINFORMATICS
DT Correction
DE event- and time-driven techniques; CPU; GPU; look-up table; spiking
   neural models; bi-fixed-step integration methods
C1 [Naveros, Francisco; Garrido, Jesus A.; Carrillo, Richard R.; Ros, Eduardo] Univ Granada, Res Ctr Informat & Commun Technol, Dept Comp Architecture & Technol, Granada, Spain.
   [Luque, Niceto R.] Vis Inst, Aging Vis & Act Lab, Paris, France.
   [Luque, Niceto R.] Pierre & Marie Curie Univ, CNRS, INSERM, Paris, France.
RP Ros, E (corresponding author), Univ Granada, Res Ctr Informat & Commun Technol, Dept Comp Architecture & Technol, Granada, Spain.; Luque, NR (corresponding author), Vis Inst, Aging Vis & Act Lab, Paris, France.; Luque, NR (corresponding author), Pierre & Marie Curie Univ, CNRS, INSERM, Paris, France.
EM eros@ugr.es; niceto.luque@inserm.fr
CR Naveros F, 2017, FRONT NEUROINFORM, V11, DOI 10.3389/fninf.2017.00007
NR 1
TC 5
Z9 5
U1 1
U2 9
PD MAY 4
PY 2018
VL 12
AR 24
DI 10.3389/fninf.2018.00024
WC Mathematical & Computational Biology; Neurosciences
DA 2023-11-11
ER

PT C
AU Adam, K
AF Adam, Karen
GP IEEE
TI A TIME ENCODING APPROACH TO TRAINING SPIKING NEURAL NETWORKS
SO 2022 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING (ICASSP)
SE International Conference on Acoustics Speech and Signal Processing
   ICASSP
DT Proceedings Paper
CT 47th IEEE International Conference on Acoustics, Speech and Signal
   Processing (ICASSP)
CY MAY 22-27, 2022
CL Singapore, SINGAPORE
DE Time encoding; spiking neural networks; learning
ID BAND-LIMITED SIGNALS; FINITE-RATE; RECONSTRUCTION; LOIHI
AB While Spiking Neural Networks (SNNs) have been gaining in popularity, it seems that the algorithms used to train them are not powerful enough to solve the same tasks as those tackled by classical Artificial Neural Networks (ANNs).
   In this paper, we provide an extra tool to help us understand and train SNNs by using theory from the field of time encoding. Time encoding machines (TEMs) can be used to model integrate-and-fire neurons and have well-understood reconstruction properties.
   We will see how one can take inspiration from the field of TEMs to interpret the spike times of SNNs as constraints on the SNNs' weight matrices. More specifically, we study how to train one-layer SNNs by solving a set of linear constraints, and how to train two-layer SNNs by leveraging the all-or-none and asynchronous properties of the spikes emitted by SNNs. These properties of spikes result in an alternative to backpropagation which is not possible in the case of simultaneous and graded activations as in classical ANNs.
C1 [Adam, Karen] Ecole Polytech Fed Lausanne EPFL, Sch Comp & Commun Sci, Lausanne, Switzerland.
RP Adam, K (corresponding author), Ecole Polytech Fed Lausanne EPFL, Sch Comp & Commun Sci, Lausanne, Switzerland.
CR Adam K, 2020, INT CONF ACOUST SPEE, P9264, DOI [10.1109/icassp40776.2020.9053294, 10.1109/ICASSP40776.2020.9053294]
   Adam K, 2020, IEEE T SIGNAL PROCES, V68, P1105, DOI 10.1109/TSP.2020.2967182
   Adam Karen, 2021, ARXIV210414511
   Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   Alexandru R, 2020, IEEE T SIGNAL PROCES, V68, P747, DOI 10.1109/TSP.2019.2961301
   Boerlin M, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003258
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Comsa I.M., 2020, ICASSP 2020 2020 IEE, DOI DOI 10.1109/ICASSP40776.2020.9053856
   Cordone Lo<spacing diaeresis>ic, 2021, ARXIV210412579
   Davies M, 2021, P IEEE, V109, P911, DOI 10.1109/JPROC.2021.3067593
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Gontier D, 2014, APPL COMPUT HARMON A, V36, P63, DOI 10.1016/j.acha.2013.02.002
   Hilton M, 2021, 2021 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP 2021), P5474, DOI 10.1109/ICASSP39728.2021.9414759
   Kamath Abijith Jagannath, 2021, ARXIV210703344
   Lazar AA, 2004, NEUROCOMPUTING, V58, P53, DOI 10.1016/j.neucom.2004.01.022
   Lazar AA, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL VI, PROCEEDINGS, P709
   Lazar AA, 2008, NEURAL COMPUT, V20, P2715, DOI 10.1162/neco.2008.06-07-559
   Lazar AA, 2010, IEEE T INFORM THEORY, V56, P821, DOI 10.1109/TIT.2009.2037040
   Ma CX, 2021, IEEE IJCNN, DOI 10.1109/IJCNN52387.2021.9534390
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Pacholska Michalina, 2020, ARXIV200104933
   Pfister JP, 2006, J NEUROSCI, V26, P9673, DOI 10.1523/JNEUROSCI.1425-06.2006
   Rudresh S, 2020, INT CONF ACOUST SPEE, P5585, DOI [10.1109/ICASSP40776.2020.9053120, 10.1109/icassp40776.2020.9053120]
   Thao NT, 2021, IEEE T SIGNAL PROCES, V69, P341, DOI 10.1109/TSP.2020.3043809
   VanRullen R, 2005, TRENDS NEUROSCI, V28, P1, DOI 10.1016/j.tins.2004.10.010
   Vetterli M, 2002, IEEE T SIGNAL PROCES, V50, P1417, DOI 10.1109/TSP.2002.1003065
   Wunderlich TC, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-91786-z
NR 27
TC 0
Z9 0
U1 1
U2 2
PY 2022
BP 5957
EP 5961
DI 10.1109/ICASSP43922.2022.9746319
WC Acoustics; Computer Science, Artificial Intelligence; Engineering,
   Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Dellaferrera, G
   Martinelli, F
   Cernak, M
AF Dellaferrera, Giorgia
   Martinelli, Flavio
   Cernak, Milos
GP IEEE
TI A BIN ENCODING TRAINING OF A SPIKING NEURAL NETWORK BASED VOICE ACTIVITY
   DETECTION
SO 2020 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL
   PROCESSING
SE International Conference on Acoustics Speech and Signal Processing
   ICASSP
DT Proceedings Paper
CT IEEE International Conference on Acoustics, Speech, and Signal
   Processing (ICASSP)
CY MAY 04-08, 2020
CL Barcelona, SPAIN
DE spiking neural networks; voice activity detection; bin encoding;
   supervised learning
AB Advances of deep learning for Artificial Neural Networks (ANNs) have led to significant improvements in the performance of digital signal processing systems implemented on digital chips. Although recent progress in low-power chips is remarkable, neuromorphic chips that run Spiking Neural Networks (SNNs) based applications offer an even lower power consumption, as a consequence of the ensuing sparse spike-based coding scheme. In this work, we develop a SNN-based Voice Activity Detection (VAD) system that belongs to the building blocks of any audio and speech processing system. We propose to use the bin encoding, a novel method to convert log mel filterbank bins of single-time frames into spike patterns. We integrate the proposed scheme in a bilayer spiking architecture which was evaluated on the QUT-NOISE-TIMIT corpus. Our approach shows that SNNs enable an ultra low-power implementation of a VAD classifier that consumes only 3.8 mu W, while achieving state-of-the-art performance. The code is freely available on Code Ocean [1].
C1 [Dellaferrera, Giorgia; Martinelli, Flavio; Cernak, Milos] Ecole Polytech Fed Lausanne EPFL, Lausanne, Switzerland.
   [Dellaferrera, Giorgia; Martinelli, Flavio] Logitech Europe SA, Lausanne, Switzerland.
RP Dellaferrera, G (corresponding author), Ecole Polytech Fed Lausanne EPFL, Lausanne, Switzerland.; Dellaferrera, G (corresponding author), Logitech Europe SA, Lausanne, Switzerland.
CR Benyassine A, 1997, IEEE COMMUN MAG, V35, P64, DOI 10.1109/35.620527
   Blouw Peter, 2018, ABS181201739 CORR
   CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411
   Davidson M, 2018, INT J OCCUP ENV HEAL, V24, P75, DOI 10.1080/10773525.2018.1517234
   Dean D., 2010, INTERSPEECH
   Dellaferrera G., 2020, BIN ENCODING SNN VAD
   Dong MX, 2018, FRONT MOL NEUROSCI, V11, DOI 10.3389/fnmol.2018.00257
   Ghaemmaghami H., 2015, INTERSPEECH 2015
   Ghaemmaghami H, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 3 AND 4, P3118
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Hazan H., 2018, ABS180709374 CORR
   Hunsberger E., 2016, TRAINING SPIKING DEE, V11
   Li J., 2004, P ICASSP
   Meoni G, 2018, 2018 14TH CONFERENCE ON PHD RESEARCH IN MICROELECTRONICS AND ELECTRONICS (PRIME 2018), P41, DOI 10.1109/PRIME.2018.8430328
   Price M, 2018, IEEE J SOLID-ST CIRC, V53, P66, DOI 10.1109/JSSC.2017.2752838
   Ramírez J, 2004, SPEECH COMMUN, V42, P271, DOI 10.1016/j.specom.2003.10.002
   Silva DA, 2017, TELECOMMUN INF TECH, P37, DOI 10.1007/978-3-319-53753-5_4
   Sohn J, 1999, IEEE SIGNAL PROC LET, V6, P1, DOI 10.1109/97.736233
   Tavanaei A., 2017, ABS170603170 CORR
   Tavanaei A., 2016, ABS160600802 CORR
   Verstraeten D., 2005, ISOLATED WORD RECOGN, V01, P435
   Verstraeten D, 2006, IEEE IJCNN, P1050
   Wu JB, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00836
   Yang MH, 2018, ISSCC DIG TECH PAP I, P346, DOI 10.1109/ISSCC.2018.8310326
   Yu Q., 2019, ABS190201094 CORR
NR 25
TC 6
Z9 6
U1 1
U2 3
PY 2020
BP 3207
EP 3211
DI 10.1109/icassp40776.2020.9054761
WC Acoustics; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Bennett, A
   White, A
AF Bennett, Adam
   White, Anthony
TI Synfire circuits: Constraint programming technique for combining
   functional groupings of spiking neurons
SO BIOLOGICALLY INSPIRED COGNITIVE ARCHITECTURES
DT Article
DE Artificial intelligence; Spiking neural network; Constraint programming;
   Modular neural network
ID CHAINS
AB Existing training techniques for spiking neuronal networks tend to be monolithic in nature and scale poorly to larger networks. This paper presents a technique for combining multiple functional neural groupings into a more complex composite network. This is accomplished by ensuring that four axioms hold true for the composite network. The axioms were designed to ensure that incoming signals arrive simultaneously to any component groupings. A number of experiments were conducted in which an algorithm implementing the axioms was used to combine component groupings into more complex networks; these experiments show the practical utility of the technique and reinforce by demonstration the correctness of the axioms.
C1 [Bennett, Adam; White, Anthony] Carleton Univ, 1125 Colonel By Dr, Ottawa, ON K1S 5B6, Canada.
RP Bennett, A (corresponding author), Carleton Univ, 1125 Colonel By Dr, Ottawa, ON K1S 5B6, Canada.
EM adamebennett@cmail.carleton.ca; arpwhite@scs.carleton.ca
CR Bennett A., 2018, THESIS
   BLODGETT HC, 1947, J EXP PSYCHOL, V37, P412, DOI 10.1037/h0059305
   Brooks R. A., 1990, Robotics and Autonomous Systems, V6, P3, DOI 10.1016/S0921-8890(05)80025-9
   Ferreira C, 2002, SOFT COMPUTING AND INDUSTRY, P635
   Gerstein GL, 2012, J NEUROSCI METH, V206, P54, DOI 10.1016/j.jneumeth.2012.02.003
   Hayon G, 2005, J COMPUT NEUROSCI, V18, P41, DOI 10.1007/s10827-005-5479-1
   Jeanson F., 2013, THESIS
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Prudhornme C., 2017, 6241 TASC LS2N CNRS
   Ranhel J., 2011, Proceedings 2011 IEEE Symposium on Foundations of Computational Intelligence (FOCI 2011), P66, DOI 10.1109/FOCI.2011.5949465
   Ranhel J, 2012, IEEE T NEUR NET LEAR, V23, P916, DOI 10.1109/TNNLS.2012.2190421
   Wernick W, 1942, T AM MATH SOC, V51, P117, DOI 10.2307/1989982
NR 12
TC 0
Z9 0
U1 0
U2 0
PD AUG
PY 2018
VL 25
BP 66
EP 71
DI 10.1016/j.bica.2018.07.008
WC Computer Science, Artificial Intelligence; Neurosciences
DA 2023-11-11
ER

PT J
AU Woodward, A
   Froese, T
   Ikegami, T
AF Woodward, Alexander
   Froese, Tom
   Ikegami, Takashi
TI Neural coordination can be enhanced by occasional interruption of normal
   firing patterns: A self-optimizing spiking neural network model
SO NEURAL NETWORKS
DT Article
DE Self-optimization; Hopfield network; Spiking neurons; Global neural
   coordination; Psychedelics; Altered states of consciousness
ID OPTIMIZATION; NEURONS; PLASTICITY; MEMORY; TIME
AB The state space of a conventional Hopfield network typically exhibits many different attractors of which only a small subset satisfies constraints between neurons in a globally optimal fashion. It has recently been demonstrated that combining Hebbian learning with occasional alterations of normal neural states avoids this problem by means of self-organized enlargement of the best basins of attraction. However, so far it is not clear to what extent this process of self-optimization is also operative in real brains. Here we demonstrate that it can be transferred to more biologically plausible neural networks by implementing a self-optimizing spiking neural network model. In addition, by using this spiking neural network to emulate a Hopfield network with Hebbian learning, we attempt to make a connection between rate-based and temporal coding based neural systems. Although further work is required to make this model more realistic, it already suggests that the efficacy of the self-optimizing process is independent from the simplifying assumptions of a conventional Hopfield network. We also discuss natural and cultural processes that could be responsible for occasional alteration of neural firing patterns in actual brains. (C) 2014 Elsevier Ltd. All rights reserved.
C1 [Woodward, Alexander; Ikegami, Takashi] Univ Tokyo, Grad Sch Arts & Sci, Tokyo 1538902, Japan.
   [Froese, Tom] Univ Nacl Autonoma Mexico, Inst Invest Matemat Aplicadas & Sistemas, Dept Ciencias Computac, Mexico City 04510, DF, Mexico.
   [Froese, Tom] Univ Nacl Autonoma Mexico, Ctr Ciencias Complejidad, Mexico City 04510, DF, Mexico.
RP Woodward, A (corresponding author), Univ Tokyo, Grad Sch Arts & Sci, Tokyo 1538902, Japan.
EM alex.w.nz@gmail.com
CR [Anonymous], 1969, RITUAL PROCESS STRUC
   [Anonymous], 1908, RITES PASSAGE
   Bailey CH, 2000, NAT REV NEUROSCI, V1, P11, DOI 10.1038/35036191
   BEER RD, 1995, ADAPT BEHAV, V3, P469, DOI 10.1177/105971239500300405
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Carhart-Harris RL, 2014, FRONT HUM NEUROSCI, V8, DOI 10.3389/fnhum.2014.00020
   Davies AP, 2011, ARTIF LIFE, V17, P167, DOI 10.1162/artl_a_00030
   Froese T, 2013, ADAPT BEHAV, V21, P199, DOI 10.1177/1059712313483145
   Gerstner W., 2002, SPIKING NEURON MODEL
   HOPFIELD JJ, 1985, BIOL CYBERN, V52, P141
   HOPFIELD JJ, 1984, P NATL ACAD SCI-BIOL, V81, P3088, DOI 10.1073/pnas.81.10.3088
   HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554
   Huang YY, 2004, P NATL ACAD SCI USA, V101, P859, DOI 10.1073/pnas.2237201100
   Koch C, 2000, NAT NEUROSCI, V3, P1171, DOI 10.1038/81444
   Kryzhanovsky B, 2008, LECT NOTES ARTIF INT, V5097, P89, DOI 10.1007/978-3-540-69731-2_10
   Kupferschmidt K, 2014, SCIENCE, V345, P18, DOI 10.1126/science.345.6192.18
   Maass W, 1997, NETWORK-COMP NEURAL, V8, P355, DOI 10.1088/0954-898X/8/4/002
   Maass W, 1997, NEURAL COMPUT, V9, P279, DOI 10.1162/neco.1997.9.2.279
   Muthukumaraswamy S. D., 2013, J NEUROSCIENCE, V33
   Nozawa H, 1992, CHAOS, V2, P377, DOI 10.1063/1.165880
   Rojas R., 1996, NEURAL NETWORKS, P149, DOI 10.1007/978-3-642-61068-4_7
   Tanaka H, 2009, IEICE T FUND ELECTR, VE92A, P1690, DOI 10.1587/transfun.E92.A.1690
   Tino P, 2005, LECT NOTES COMPUT SC, V3611, P666
   Varela F. J., 1999, J CONSCIOUSNESS STUD, V6, P2
   Watson RA, 2011, ADAPT BEHAV, V19, P227, DOI 10.1177/1059712311412797
   Watson RA, 2011, ARTIF LIFE, V17, P147, DOI 10.1162/artl_a_00029
   Watson RA, 2011, COMPLEXITY, V16, P17, DOI 10.1002/cplx.20346
   Wilson S, 2012, BRIT J PSYCHIAT, V200, P273, DOI 10.1192/bjp.bp.111.104091
NR 28
TC 15
Z9 15
U1 0
U2 19
PD FEB
PY 2015
VL 62
SI SI
BP 39
EP 46
DI 10.1016/j.neunet.2014.08.011
WC Computer Science, Artificial Intelligence; Neurosciences
DA 2023-11-11
ER

PT J
AU Wang, Y
   Deng, YL
   Cao, LH
   Zhang, JH
   Yang, L
AF Wang, Ye
   Deng, Yaling
   Cao, Lihong
   Zhang, Jiahong
   Yang, Lei
TI Retrospective memory integration accompanies reconfiguration of neural
   cell assemblies
SO HIPPOCAMPUS
DT Article
DE associative inference; associative memory; cell assembly; recurrent
   spiking neural network; retrospective memory integration
ID MEDIAL TEMPORAL-LOBE; HIPPOCAMPAL SYSTEM; MODEL; INFORMATION;
   CONSOLIDATION; MECHANISMS; SUPPORTS; NEURONS; SPIKES; REPLAY
AB Memory is a dynamic process that is based on and can be altered by experiences. Integrating memories of multiple experiences (memory integration) is the basis of flexible and complex decision-making. However, the mechanism of memory integration in neural networks of the brain remains poorly understood. In this study, we built a recurrent spiking network model and investigated the neural mechanism of memory integration before a decision is made (retrospective memory integration) at the neural circuit level. Our simulations suggest that retrospective memory integration accompanies reconfiguration of neural cell assemblies. Additionally, partially blocking neural network plasticity leads to failure of memory integration. These findings can potentially guide the experimental investigation of the neural mechanism of retrospective memory integration and serve as the basis for developing new artificial intelligence algorithms.
C1 [Wang, Ye; Deng, Yaling; Cao, Lihong; Zhang, Jiahong] Commun Univ China, State Key Lab Media Convergence & Commun, Beijing 100024, Peoples R China.
   [Wang, Ye; Deng, Yaling; Cao, Lihong] Commun Univ China, Neurosci & Intelligent Media Inst, Beijing, Peoples R China.
   [Yang, Lei] Pacific Northwest Res Inst, 720 Broadway, Seattle, WA 98122 USA.
RP Wang, Y (corresponding author), Commun Univ China, State Key Lab Media Convergence & Commun, Beijing 100024, Peoples R China.
EM yewang@cuc.edu.cn
CR Barron HC, 2020, CELL, V183, P228, DOI 10.1016/j.cell.2020.08.035
   Biderman N, 2020, TRENDS COGN SCI, V24, P542, DOI 10.1016/j.tics.2020.04.004
   Bunsey M, 1996, NATURE, V379, P255, DOI 10.1038/379255a0
   Buzsáki G, 2010, NEURON, V68, P362, DOI 10.1016/j.neuron.2010.09.023
   Cai DJ, 2016, NATURE, V534, P115, DOI 10.1038/nature17955
   Carr MF, 2011, NAT NEUROSCI, V14, P147, DOI 10.1038/nn.2732
   Cichon J, 2015, NATURE, V520, P180, DOI 10.1038/nature14251
   Collin SHP, 2015, NAT NEUROSCI, V18, P1562, DOI 10.1038/nn.4138
   De Falco E, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms13408
   DeVito LM, 2010, HIPPOCAMPUS, V20, P208, DOI 10.1002/hipo.20610
   Diekelmann S, 2010, NAT REV NEUROSCI, V11, P114, DOI 10.1038/nrn2762
   Eichenbaum H, 2000, NAT REV NEUROSCI, V1, P41, DOI 10.1038/35036213
   Ellenbogen JM, 2007, P NATL ACAD SCI USA, V104, P7723, DOI 10.1073/pnas.0700094104
   Gershman SJ, 2014, J EXP PSYCHOL GEN, V143, P182, DOI 10.1037/a0030844
   Hasselmo ME, 1997, BEHAV BRAIN RES, V89, P1, DOI 10.1016/S0166-4328(97)00048-X
   Hebb D. O., 1949, ORG BEHAV A NEUROPSY
   Howard MW, 2005, PSYCHOL REV, V112, P75, DOI 10.1037/0033-295X.112.1.75
   Howard MW, 2002, J MEM LANG, V46, P85, DOI 10.1006/jmla.2001.2798
   Ison MJ, 2015, NEURON, V87, P220, DOI 10.1016/j.neuron.2015.06.016
   Joo HR, 2018, NAT REV NEUROSCI, V19, P744, DOI 10.1038/s41583-018-0077-1
   Josselyn SA, 2020, SCIENCE, V367, P39, DOI 10.1126/science.aaw4325
   Josselyn SA, 2015, NAT REV NEUROSCI, V16, P521, DOI 10.1038/nrn4000
   Karlsson MP, 2009, NAT NEUROSCI, V12, P913, DOI 10.1038/nn.2344
   Kastellakis G, 2016, CELL REP, V17, P1491, DOI 10.1016/j.celrep.2016.10.015
   Koster R, 2018, NEURON, V99, P1342, DOI 10.1016/j.neuron.2018.08.009
   Kumaran D, 2016, TRENDS COGN SCI, V20, P512, DOI 10.1016/j.tics.2016.05.004
   Kumaran D, 2012, PSYCHOL REV, V119, P573, DOI 10.1037/a0028681
   Litwin-Kumar A, 2014, NAT COMMUN, V5, DOI 10.1038/ncomms6319
   MCNAUGHTON BL, 1987, TRENDS NEUROSCI, V10, P408, DOI 10.1016/0166-2236(87)90011-7
   Moscovitch M, 2016, ANNU REV PSYCHOL, V67, P105, DOI 10.1146/annurev-psych-113011-143733
   Nicola W, 2019, NAT NEUROSCI, V22, P1168, DOI 10.1038/s41593-019-0415-2
   Nicolás B, 2021, NEUROIMAGE, V226, DOI 10.1016/j.neuroimage.2020.117558
   Pfister JP, 2006, J NEUROSCI, V26, P9673, DOI 10.1523/JNEUROSCI.1425-06.2006
   Poirazi P, 2020, NAT REV NEUROSCI, V21, P303, DOI 10.1038/s41583-020-0301-7
   Pokorny C, 2020, CEREB CORTEX, V30, P952, DOI 10.1093/cercor/bhz140
   Preston AR, 2004, HIPPOCAMPUS, V14, P148, DOI 10.1002/hipo.20009
   Quiroga RQ, 2019, CELL, V179, P1015, DOI 10.1016/j.cell.2019.10.016
   Rolls ET, 2013, FRONT SYST NEUROSCI, V7, DOI [10.3389/fnsys.2013.00074, 10.3389/fncel.2013.00098]
   Schlichting ML, 2015, CURR OPIN BEHAV SCI, V1, P1, DOI 10.1016/j.cobeha.2014.07.005
   Schlichting ML, 2014, HIPPOCAMPUS, V24, P1248, DOI 10.1002/hipo.22310
   Shohamy D, 2008, NEURON, V60, P378, DOI 10.1016/j.neuron.2008.09.023
   Shohamy D, 2015, CURR OPIN BEHAV SCI, V5, P85, DOI 10.1016/j.cobeha.2015.08.010
   Tompary A, 2017, NEURON, V96, P228, DOI 10.1016/j.neuron.2017.09.005
   TREVES A, 1994, HIPPOCAMPUS, V4, P374, DOI 10.1002/hipo.450040319
   van Kesteren MTR, 2012, TRENDS NEUROSCI, V35, P211, DOI 10.1016/j.tins.2012.02.001
   Wang Y, 2019, COMPUT INTEL NEUROSC, V2019, DOI 10.1155/2019/2367075
   Wong FS, 2019, ELIFE, V8, DOI 10.7554/eLife.47085
   Xie WZ, 2020, NAT HUM BEHAV, V4, P937, DOI 10.1038/s41562-020-0901-2
   Zeithamova D, 2012, NEURON, V75, P168, DOI 10.1016/j.neuron.2012.05.010
   Zeithamova D, 2012, FRONT HUM NEUROSCI, V6, DOI 10.3389/fnhum.2012.00070
   Zeithamova D, 2010, J NEUROSCI, V30, P14676, DOI 10.1523/JNEUROSCI.3250-10.2010
   Zenke F, 2015, NAT COMMUN, V6, DOI 10.1038/ncomms7922
NR 52
TC 1
Z9 1
U1 3
U2 13
PD MAR
PY 2022
VL 32
IS 3
BP 179
EP 192
DI 10.1002/hipo.23399
EA DEC 2021
WC Neurosciences
DA 2023-11-11
ER

PT C
AU Lu, B
   Wen, K
   Wang, C
AF Lu, Bo
   Wen, Kai
   Wang, Chuan
GP IEEE
TI Quantum Computation Using Coherent Ising Machines Based on Spiking
   Neural Networks
SO 2022 14TH INTERNATIONAL CONFERENCE ON WIRELESS COMMUNICATIONS AND SIGNAL
   PROCESSING, WCSP
SE International Conference on Wireless Communications and Signal
   Processing
DT Proceedings Paper
CT 14th IEEE International Conference on Wireless Communications and Signal
   Processing (WCSP)
CY NOV 01-03, 2022
CL Nanjing, PEOPLES R CHINA
DE Coherent Ising machines; spiking neural networks; combinatorial
   optimization; quantum computation
ID OPTIMIZATION; SIMULATION; NEURONS
AB Coherent Ising machines (CIMs) are a promising approach for quantum computation. It exhibits excellent performance in solving the combinatorial optimization problems, however, the existence of amplitude heterogeneity limits the applications of CIMs. Here in this work, we present a system of the spiking neural network composed of antisymmetrically coupled degenerate optical parametric oscillator pulses and dissipative pulses. It works based on the principles of CIM during solving the combinatorial optimization problems. Moreover, a nonlinear transfer function is chosen to mitigate the amplitude inhomogeneities and destabilize the resulting local minima according to the dynamical behavior of spiking neurons.
C1 [Lu, Bo; Wang, Chuan] Beijing Normal Univ Beijing, Sch Artificial Intelligence, Beijing, Peoples R China.
   [Wen, Kai] Beijing QBoson Quantum Technol Co Ltd, Beijing, Peoples R China.
RP Lu, B (corresponding author), Beijing Normal Univ Beijing, Sch Artificial Intelligence, Beijing, Peoples R China.
EM lubo@mail.bnu.edu.cn; wenk@boseq.com; wangchuan@bnu.edu.cn
CR Arora S, 2009, COMPUTATIONAL COMPLEXITY: A MODERN APPROACH, P1, DOI 10.1017/CBO9780511804090
   BARAHONA F, 1988, OPER RES, V36, P493, DOI 10.1287/opre.36.3.493
   Böhm F, 2021, COMMUN PHYS-UK, V4, DOI 10.1038/s42005-021-00655-8
   Böhm F, 2019, NAT COMMUN, V10, DOI 10.1038/s41467-019-11484-3
   Böhm F, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-07328-1
   Brette R, 2007, J COMPUT NEUROSCI, V23, P349, DOI 10.1007/s10827-007-0038-6
   Hamerly R, 2019, SCI ADV, V5, DOI 10.1126/sciadv.aau0823
   Inagaki T, 2021, NAT COMMUN, V12, DOI 10.1038/s41467-021-22576-4
   Inagaki T, 2016, SCIENCE, V354, P603, DOI 10.1126/science.aah4243
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Kadowaki T, 1998, PHYS REV E, V58, P5355, DOI 10.1103/PhysRevE.58.5355
   Kitchen DB, 2004, NAT REV DRUG DISCOV, V3, P935, DOI 10.1038/nrd1549
   Leleu T, 2019, PHYS REV LETT, V122, DOI 10.1103/PhysRevLett.122.040607
   Leleu T, 2017, PHYS REV E, V95, DOI 10.1103/PhysRevE.95.022118
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   McMahon PL, 2016, SCIENCE, V354, P614, DOI 10.1126/science.aah5178
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Okawachi Y, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-17919-6
   Pierangeli D, 2019, PHYS REV LETT, V122, DOI 10.1103/PhysRevLett.122.213902
   Saccone M, 2022, NAT PHYS, V18, P517, DOI 10.1038/s41567-022-01538-7
   Soler-Dominguez A, 2017, ACM COMPUT SURV, V50, DOI 10.1145/3054133
   Wang Z, 2013, PHYS REV A, V88, DOI 10.1103/PhysRevA.88.063853
   Yu Q., 2012, 2012 INT JOINT C NEU, P1
NR 23
TC 0
Z9 0
U1 3
U2 3
PY 2022
BP 813
EP 817
DI 10.1109/WCSP55476.2022.10039429
WC Engineering, Electrical & Electronic; Telecommunications
DA 2023-11-11
ER

PT J
AU Wang, J
   Li, TF
   Sun, C
   Yan, RQ
   Chen, XF
AF Wang, Jun
   Li, Tianfu
   Sun, Chuang
   Yan, Ruqiang
   Chen, Xuefeng
TI Improved spiking neural network for intershaft bearing fault diagnosis
SO JOURNAL OF MANUFACTURING SYSTEMS
DT Article
DE Spiking neural network; Intershaft bearing; Fault diagnosis; Data
   encoding; Surrogate gradient
ID MACHINERY; ATTENTION; NEURONS
AB The intershaft bearing is located between the high and low-pressure rotors of the aero-engine, where the working environment is harsh, the load variation range is large, and the lubrication and heat dissipation are poor. The fault of the intershaft bearing is sudden and will cause the engine to hold the shaft and break the shaft, which is more harmful than the ordinary bearings. In recent years, bearing intelligent fault diagnosis methods based on deep learning have been widely applied. However, most of the existing methods are mainly proposed based on second-generation neural networks. Spiking neural network (SNN), also known as the third-generation neural network, mimics the dynamics of the biological brain and is more powerful for processing time-series information. As we know, vibration data is a typical time-series data, SNN would have stronger feature extraction potential for it. In this paper, we propose an improved spiking neural network (ISNN) for intershaft bearing fault diagnosis. Specifically, we propose an encoding method to encode raw data into spike sequences and demonstrate that the encoding method is accurate and efficient. Then we derive the gradient relation in the ISNN and mathematically prove the replacement of invalid gradients in it. Furthermore, we compensate for the loss of information in forward propagation and simplify the process of backpropagation by constructing suitable spiking neurons. Finally, we test the ISNN on the fault dataset of intershaft bearings, and the results show that the proposed ISNN outperforms previous SNNs and typical second-generation neural networks.
C1 [Wang, Jun; Li, Tianfu; Sun, Chuang; Yan, Ruqiang; Chen, Xuefeng] Xi An Jiao Tong Univ, Dept Mech Engn, Xian 710049, Shaanxi, Peoples R China.
   [Li, Tianfu] Ecole Polytech Fed Lausanne, Lab Intelligent Maintenance & Operat Syst, CH-1015 Lausanne, Switzerland.
RP Sun, C (corresponding author), Xi An Jiao Tong Univ, Dept Mech Engn, Xian 710049, Shaanxi, Peoples R China.
EM ch.sun@xjtu.edu.cn
CR An ZH, 2020, ISA T, V100, P155, DOI 10.1016/j.isatra.2019.11.010
   Andrew A.M., 2003, KYBERNETES, V32
   [Anonymous], 1994, WAVELETS
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Cao HR, 2022, J MANUF SYST, V62, P186, DOI 10.1016/j.jmsy.2021.11.016
   Deng L, 2020, NEURAL NETWORKS, V121, P294, DOI 10.1016/j.neunet.2019.09.005
   Ge Y, 2022, J MANUF SYST, V63, P177, DOI 10.1016/j.jmsy.2022.03.009
   GERSTNER W, 1995, PHYS REV E, V51, P738, DOI 10.1103/PhysRevE.51.738
   He WH, 2020, NEURAL NETWORKS, V132, P108, DOI 10.1016/j.neunet.2020.08.001
   He Y, 2020, IEEE ACCESS, V8, P203058, DOI 10.1109/ACCESS.2020.3034305
   Heil P, 2004, CURR OPIN NEUROBIOL, V14, P461, DOI 10.1016/j.conb.2004.07.002
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Hu RH, 2019, IEEE T NEUR NET LEAR, V30, P1984, DOI 10.1109/TNNLS.2018.2875471
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Jalayer M, 2021, COMPUT IND, V125, DOI 10.1016/j.compind.2020.103378
   Jiang ZN, 2017, APPL SCI-BASEL, V7, DOI 10.3390/app7090937
   Jin DZ, 2002, PHYS REV LETT, V89, DOI 10.1103/PhysRevLett.89.208102
   Kun Feng, 2021, 2021 GLOBAL RELIABIL, P1
   Lazar AA, 2004, NEUROCOMPUTING, V58, P53, DOI 10.1016/j.neucom.2004.01.022
   Li TF, 2022, IEEE T SYST MAN CY-S, V52, P2302, DOI 10.1109/TSMC.2020.3048950
   Li TF, 2021, IEEE T IND ELECTRON, V68, P12739, DOI 10.1109/TIE.2020.3040669
   MOUNTCASTLE VB, 1957, J NEUROPHYSIOL, V20, P408, DOI 10.1152/jn.1957.20.4.408
   Mu XK, 2021, J MANUF SYST, V61, P112, DOI 10.1016/j.jmsy.2021.08.010
   Muruganatham B, 2013, MECH SYST SIGNAL PR, V35, P150, DOI 10.1016/j.ymssp.2012.08.019
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Nik Dennler, 2021, 2021 IEEE 3 INT C AR, P1
   Pan J, 2018, IEEE T IND ELECTRON, V65, P4973, DOI 10.1109/TIE.2017.2767540
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Senanayake DA, 2021, IEEE T NEUR NET LEAR, V32, P4588, DOI 10.1109/TNNLS.2020.3023941
   Slepova LO, 2018, PROCEEDINGS OF THE 2018 IEEE CONFERENCE OF RUSSIAN YOUNG RESEARCHERS IN ELECTRICAL AND ELECTRONIC ENGINEERING (EICONRUS), P992, DOI 10.1109/EIConRus.2018.8317256
   Sun C, 2018, IEEE T IND INFORM, V14, P3261, DOI 10.1109/TII.2018.2819674
   Taherkhani A, 2018, IEEE T NEUR NET LEAR, V29, P5394, DOI 10.1109/TNNLS.2018.2797801
   Tang JX, 2016, IEEE T NEUR NET LEAR, V27, P809, DOI 10.1109/TNNLS.2015.2424995
   Thorpe S, 1996, NATURE, V381, P520, DOI 10.1038/381520a0
   Verstraete D, 2017, SHOCK VIB, V2017, DOI 10.1155/2017/5067651
   Wang H, 2022, IEEE T NEUR NET LEAR, V33, P4757, DOI 10.1109/TNNLS.2021.3060494
   Wang XW, 2020, NEURAL NETWORKS, V125, P258, DOI 10.1016/j.neunet.2020.02.011
   Wen L, 2020, NEURAL COMPUT APPL, V32, P6111, DOI 10.1007/s00521-019-04097-w
   Wu YJ, 2019, AAAI CONF ARTIF INTE, P1311
   Xie ZL, 2022, J MANUF SYST, V62, P301, DOI 10.1016/j.jmsy.2021.12.003
   Xu Z, 2021, J MANUF SYST, V58, P132, DOI 10.1016/j.jmsy.2020.08.002
   Yan Y, 2014, NAT NEUROSCI, V17, P1380, DOI 10.1038/nn.3805
   Ye Z, 2021, J MANUF SYST, V59, P467, DOI 10.1016/j.jmsy.2021.03.022
   Yu XL, 2022, IEEE T IND INFORM, V18, P185, DOI 10.1109/TII.2021.3070324
   Zhang DC, 2020, IEEE T INSTRUM MEAS, V69, P2996, DOI 10.1109/TIM.2019.2929669
   Zhang JL, 2019, IEEE ASIAN SOLID STA, P213, DOI [10.1109/a-sscc47793.2019.9056903, 10.1109/A-SSCC47793.2019.9056903]
   Zhang KY, 2020, J MANUF SYST, V55, P273, DOI 10.1016/j.jmsy.2020.04.016
   Zhang TL, 2022, IEEE T NEUR NET LEAR, V33, P7621, DOI 10.1109/TNNLS.2021.3085966
   Zhang YH, 2021, MEASUREMENT, V171, DOI 10.1016/j.measurement.2020.108774
   Zhao B, 2021, J MANUF SYST, V59, P565, DOI 10.1016/j.jmsy.2021.03.024
   Zuo L, 2022, RELIAB ENG SYST SAFE, V225, DOI 10.1016/j.ress.2022.108561
   Zuo L, 2021, J MANUF SYST, V61, P714, DOI 10.1016/j.jmsy.2020.07.003
NR 53
TC 5
Z9 5
U1 23
U2 36
PD OCT
PY 2022
VL 65
BP 208
EP 219
DI 10.1016/j.jmsy.2022.09.003
EA SEP 2022
WC Engineering, Industrial; Engineering, Manufacturing; Operations Research
   & Management Science
DA 2023-11-11
ER

PT J
AU Booij, O
   Nguyen, HT
AF Booij, O
   Nguyen, HT
TI A gradient descent rule for spiking neurons emitting multiple spikes
SO INFORMATION PROCESSING LETTERS
DT Article
DE spiking neural networks; temporal pattern recognition;
   error-backpropagation; parallel processing
ID NETWORKS
AB A supervised learning rule for Spiking Neural Networks (SNNs) is presented that can cope with neurons that spike multiple times. The rule is developed by extending the existing SpikeProp algorithm which could only be used for one spike per neuron. The problem caused by the discontinuity in the spike process is counteracted with a simple but effective rule, which makes the learning process more efficient. Our learning rule is successfully tested on a classification task of Poisson spike trains. We also applied the algorithm on a temporal version of the XOR problem and show that it is possible to learn this classical problem using only one spiking neuron making use of a hair-trigger situation. (c) 2005 Elsevier B.V. All rights reserved.
C1 Univ Amsterdam, Fac Sci, NL-1098 SJ Amsterdam, Netherlands.
RP Booij, O (corresponding author), Univ Amsterdam, Fac Sci, Kruislaan 403, NL-1098 SJ Amsterdam, Netherlands.
EM obooij@science.uva.nl; hieu@science.uva.nl
CR [Anonymous], 1986, PARALLEL DISTRIBUTED
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Gerstner W., 2002, SPIKING NEURON MODEL
   Heeger D., 2000, POISSON MODEL SPIKE
   Hopfield JJ, 2001, P NATL ACAD SCI USA, V98, P1282, DOI 10.1073/pnas.031567098
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   Maass W, 1997, ADV NEUR IN, V9, P211
   MAASS W., 1999, PULSED NEURAL NETWOR
   MOORE S, 2002, THESIS U BATH
   Natschlager T, 1998, NETWORK-COMP NEURAL, V9, P319, DOI 10.1088/0954-898X/9/3/003
   NATSCHLAGER T, 2004, ADV NEURAL INFORMATI, V16
   Ruf B, 1997, LECT NOTES COMPUT SC, V1240, P380, DOI 10.1007/BFb0032496
   Schrauwen B, 2004, IEEE IJCNN, P471, DOI 10.1109/IJCNN.2004.1379954
   XIN J, 2001, P IEEE INT JOINT C N
   [No title captured]
NR 15
TC 113
Z9 133
U1 0
U2 10
PD SEP 30
PY 2005
VL 95
IS 6
BP 552
EP 558
DI 10.1016/j.ipl.2005.05.023
WC Computer Science, Information Systems
DA 2023-11-11
ER

PT C
AU Poulsen, TM
   Moore, RK
AF Poulsen, Thomas M.
   Moore, Roger K.
GP IEEE
TI Sound localization through evolutionary learning applied to spiking
   neural networks
SO 2007 IEEE SYMPOSIUM ON FOUNDATIONS OF COMPUTATIONAL INTELLIGENCE, VOLS 1
   AND 2
DT Proceedings Paper
CT IEEE Symposium on Foundations of Computational Intelligence
CY APR 01-05, 2007
CL Honolulu, HI
ID NEURONS
AB A biologically based learning framework is established to study neural modeling with respect to sound source localization. This involves a 2-dimensional environment wherein agents must locate sound sources that are periodically resituated whilst emitting pulses at regular intervals. Agents employ a spiking neural model that controls movement on the basis of binaural inputs, and evolutionary learning (EL) is applied to evolve neural connectivity and weights. It is demonstrated that agents are successfully able to locate sound sources and that the simulative framework can be extended to address questions pertaining to the evolution of spiking neural networks.
C1 [Poulsen, Thomas M.] Univ Sheffield, Speech & Hearing Grp, Dept Comp Sci, Sheffield S10 2TN, S Yorkshire, England.
   [Moore, Roger K.] Univ Sheffield, Dept Comp Sci, Speech & Hearing Grp, Sheffield S10 2TN, S Yorkshire, England.
RP Poulsen, TM (corresponding author), Univ Sheffield, Speech & Hearing Grp, Dept Comp Sci, Sheffield S10 2TN, S Yorkshire, England.
CR Beer Randall D., 1992, Adaptive Behavior, V1, P91, DOI 10.1177/105971239200100105
   BEER RD, 1996, P 4 INT C SIM AD BEH, P421
   Di Paolo EA, 2000, ADAPT BEHAV, V8, P27, DOI 10.1177/105971230000800103
   FLOREANO D, 2001, EVOLUTIONARY ROBOTIC, V4
   Gabbiani F, 1996, NATURE, V384, P564, DOI 10.1038/384564a0
   Gerstner W., 2002, SPIKING NEURON MODEL
   Kistler WM, 1997, NEURAL COMPUT, V9, P1015, DOI 10.1162/neco.1997.9.5.1015
   Machens CK, 2003, NAT NEUROSCI, V6, P341, DOI 10.1038/nn1036
   Rokem A, 2006, J NEUROPHYSIOL, V95, P2541, DOI 10.1152/jn.00891.2005
   VANSTEVENINCK RD, 1988, PROC R SOC SER B-BIO, V234, P379
   WERNER GM, 1991, P 2 INT C ART LIF, P659
   Wright BD, 2002, ADV NEUR IN, V14, P309
NR 12
TC 4
Z9 4
U1 0
U2 0
PY 2007
BP 350
EP +
DI 10.1109/FOCI.2007.371495
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT C
AU Ross, M
   Berberian, N
   Cyr, A
   Thériault, F
   Chartier, S
AF Ross, Matt
   Berberian, Nareg
   Cyr, Andre
   Theriault, Frederic
   Chartier, Sylvain
BE Lintas, A
   Rovetta, S
   Verschure, PFMJ
   Villa, AEP
TI Learning Distance-Behavioural Preferences Using a Single Sensor in a
   Spiking Neural Network
SO ARTIFICIAL NEURAL NETWORKS AND MACHINE LEARNING - ICANN 2017, PT I
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 26th International Conference on Artificial Neural Networks (ICANN)
CY SEP 11-14, 2017
CL Alghero, ITALY
DE Spiking neural networks; Spike timing dependent plasticity; Robotic
   simulation; Sensory calibration
ID NEURONS
AB Actions from autonomous agents demand adaptive rules rather than being hard coded. Contrary to using multiple pre-calibrated sensors, utilizing a single non-calibrated sensor in combination with neural elements could provide flexibility through learning, to effectively cope with changing environments. The objective of this study was to design an adaptive system with the potential capability of learning behavioural preferences in relation to distinct distances from a wall using only a single ultrasonic sensor. Using spike-timing dependent plasticity (STDP) as a learning mechanism in a spiking neural network (SNN), the agent displayed the correct behaviour and was successful in learning the desired behavioural preference at a medium distance. However, the agent treated far and close distances as ambiguous inputs from the sensory environment, despite the presentation of reinforcement cues during learning.
C1 [Ross, Matt; Berberian, Nareg; Cyr, Andre; Chartier, Sylvain] Univ Ottawa, Sch Psychol, 136 Jean Jacques Lussier, Ottawa, ON K1N 6N5, Canada.
   [Theriault, Frederic] Cegep Vieux Montreal, Comp Sci, 255 Ontario St E, Montreal, PQ H2X 1X6, Canada.
RP Ross, M (corresponding author), Univ Ottawa, Sch Psychol, 136 Jean Jacques Lussier, Ottawa, ON K1N 6N5, Canada.
EM mross094@uottawa.ca; nberb062@uottawa.ca
CR [Anonymous], 2010, SPIKING NEURAL NETWO
   [Anonymous], 2011, NEURAL NETWORKS LEAR
   Pérez-Carrasco JA, 2010, IEEE T NEURAL NETWOR, V21, P609, DOI 10.1109/TNN.2009.2039943
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Bryan Kolb I. W., 2014, INTRO BRAIN BEHAV, VFourth
   Caporale N, 2008, ANNU REV NEUROSCI, V31, P25, DOI 10.1146/annurev.neuro.31.060407.125639
   Cyr A, 2015, J ROBOT, V2015, DOI 10.1155/2015/643869
   Cyr A, 2014, FRONT NEUROROBOTICS, V8, P1, DOI 10.3389/fnbot.2014.00021
   Cyr A, 2009, NEURAL COMPUT APPL, V18, P431, DOI 10.1007/s00521-009-0254-2
   Humphries MD, 2007, PHILOS T R SOC B, V362, P1627, DOI 10.1098/rstb.2007.2057
   Iakymchuk T, 2015, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-015-0059-4
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Ross M., 2017, LEARNING DISTANCE BE
   Theriault F., 2017, LEARNING DISTANCE BE
   Wenstrup JJ, 2011, NEUROSCI BIOBEHAV R, V35, P2073, DOI 10.1016/j.neubiorev.2010.12.015
NR 15
TC 0
Z9 0
U1 0
U2 1
PY 2017
VL 10613
BP 110
EP 118
DI 10.1007/978-3-319-68600-4_14
PN I
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods
DA 2023-11-11
ER

PT J
AU Kim, D
   She, XY
   Rahman, NM
   Chekuri, VCK
   Mukhopadhyay, S
AF Kim, Daehyun
   She, Xueyuan
   Rahman, Nael Mizanur
   Chekuri, Venkata Chaitanya Krishna
   Mukhopadhyay, Saibal
TI Processing-In-Memory-Based On-Chip Learning With Spike-Time-Dependent
   Plasticity in 65-nm CMOS
SO IEEE SOLID-STATE CIRCUITS LETTERS
DT Article
DE Accelerator; on-chip learning; spike-timing-dependent plasticity (STDP);
   spiking neural network (SNN)
AB A processing-in-memory (PIM)-based accelerator is presented in 65-nm CMOS for on-chip learning in spiking neural network using timing-based stochastic spike-timing-dependent plasticity (STDP). The design uses mixed-signal processing in the 8T-SRAM array for spike accumulation and all-digital computation for neuron dynamics and synaptic weight updates. The 0.39-mm(2) and 14.83-mW test chip demonstrates 100K images/second learning rate and 148.3 nJ/image learning energy.
C1 [Kim, Daehyun; She, Xueyuan; Rahman, Nael Mizanur; Chekuri, Venkata Chaitanya Krishna; Mukhopadhyay, Saibal] Georgia Inst Technol, Sch Elect & Comp Engn, Atlanta, GA 30332 USA.
RP Kim, D (corresponding author), Georgia Inst Technol, Sch Elect & Comp Engn, Atlanta, GA 30332 USA.
EM daehyun.kim@gatech.edu
CR Buhler FN, 2017, SYMP VLSI CIRCUITS, pC30, DOI 10.23919/VLSIC.2017.8008536
   Jung Kuk Kim, 2015, 2015 Symposium on VLSI Circuits (VLSI Circuits), pC50, DOI 10.1109/VLSIC.2015.7231323
   Kim S, 2011, 2011 11TH INTERNATIONAL CONFERENCE ON CONTROL, AUTOMATION AND SYSTEMS (ICCAS), P1
   Park J, 2019, ISSCC DIG TECH PAP I, V62, P140, DOI 10.1109/ISSCC.2019.8662398
   She XY, 2019, DES AUT TEST EUROPE, P450, DOI [10.23919/DATE.2019.8714846, 10.23919/date.2019.8714846]
NR 5
TC 8
Z9 8
U1 0
U2 3
PY 2020
VL 3
BP 278
EP 281
DI 10.1109/LSSC.2020.3013448
WC Computer Science, Hardware & Architecture; Engineering, Electrical &
   Electronic
DA 2023-11-11
ER

PT J
AU Chen, QY
   He, GQ
   Wang, XY
   Xu, J
   Shen, SR
   Chen, H
   Fu, YX
   Li, L
AF Chen, Qinyu
   He, Guoqiang
   Wang, Xinyuan
   Xu, Jin
   Shen, Sirui
   Chen, Hui
   Fu, Yuxiang
   Li, Li
TI A 67.5 μJ/Prediction Accelerator for Spiking Neural Networks in Image
   Segmentation
SO IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS II-EXPRESS BRIEFS
DT Article
DE Image segmentation; spiking neural network; VLSI
ID PROCESSOR
AB Spiking Neural Networks (SNNs) is promising to enable low power and high performance edge computing hardware design and have recently attracted attentions of researchers. Compared to Artificial Neural Networks (ANNs), SNNs, which present more realistic brain-inspired computing models, are developed as an alternative to ANNs. However, the temporal primitive of SNNs causes irregular and repeated data accesses, leading to high latency and extra power consumption. In this work, we propose an efficient architecture for SNNs by exploiting event-based characteristics. A reconfigurable spiking neuron processing unit is proposed to support a variety of spike-layers. Furthermore, to reduce the cycles needed per frame, an efficient dataflow with fast-filtering mechanism is introduced to leverage the sparsity of discrete spikes. The results show that this design achieves 67.5 mu J/image prediction energy with a throughput of 2.2K FPS. The core size is 0.89 mm(2) under 28-nm technology, with 90.98% computing hardware utilization and a competitive accuracy 97.10% on a driving dataset.
C1 [Chen, Qinyu; He, Guoqiang; Wang, Xinyuan; Xu, Jin; Shen, Sirui; Chen, Hui; Fu, Yuxiang; Li, Li] Nanjing Univ, Sch Elect & Engn, Nanjing 210000, Peoples R China.
RP Fu, YX; Li, L (corresponding author), Nanjing Univ, Sch Elect & Engn, Nanjing 210000, Peoples R China.
EM cqy@smail.nju.edu.cn; yuxiangfu@nju.edu.cn; lili@nju.edu.cn
CR Benini L., 2015, P 25 EDITION GREAT L, P199, DOI DOI 10.1145/2742060.2743766
   Chen GK, 2019, IEEE J SOLID-ST CIRC, V54, P992, DOI 10.1109/JSSC.2018.2884901
   Chen Q., 2021, PROC IEEE INT S CIRC, P1, DOI DOI 10.1038/NMETH.1318
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Farsa EZ, 2019, IEEE T CIRCUITS-II, V66, P1582, DOI 10.1109/TCSII.2019.2890846
   Gao C, 2018, PROCEEDINGS OF THE 2018 ACM/SIGDA INTERNATIONAL SYMPOSIUM ON FIELD-PROGRAMMABLE GATE ARRAYS (FPGA'18), P21, DOI 10.1145/3174243.3174261
   Kaiser J, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00424
   Narayanan S, 2020, ANN I S COM, P349, DOI 10.1109/ISCA45697.2020.00038
   Park J, 2020, IEEE J SOLID-ST CIRC, V55, P108, DOI 10.1109/JSSC.2019.2942367
   Roy A, 2017, I SYMPOS LOW POWER E
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Whatmough PN, 2017, ISSCC DIG TECH PAP I, P242, DOI 10.1109/ISSCC.2017.7870351
NR 12
TC 7
Z9 7
U1 1
U2 11
PD FEB
PY 2022
VL 69
IS 2
BP 574
EP 578
DI 10.1109/TCSII.2021.3098633
WC Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Park, S
   Kim, S
   Na, B
   Yoon, S
AF Park, Seongsik
   Kim, Seijoon
   Na, Byunggook
   Yoon, Sungroh
GP IEEE
TI T2FSNN: Deep Spiking Neural Networks with Time-to-first-spike Coding
SO PROCEEDINGS OF THE 2020 57TH ACM/EDAC/IEEE DESIGN AUTOMATION CONFERENCE
   (DAC)
SE Design Automation Conference DAC
DT Proceedings Paper
CT 57th ACM/IEEE Design Automation Conference (DAC)
CY JUL 20-24, 2020
CL ELECTR NETWORK
DE Biological neural networks; Neuromorphics; Supervised learning; Image
   classification
AB Spiking neural networks (SNNs) have gained considerable interest due to their energy-efficient characteristics, yet lack of a scalable training algorithm has restricted their applicability in practical machine learning problems. The deep neural network-to-SNN conversion approach has been widely studied to broaden the applicability of SNNs. Most previous studies, however, have not fully utilized spatio-temporal aspects of SNNs, which has led to inefficiency in terms of number of spikes and inference latency. In this paper, we present T2FSNN, which introduces the concept of time-to-first-spike coding into deep SNNs using the kernel-based dynamic threshold and dendrite to overcome the aforementioned drawback. In addition, we propose gradient-based optimization and early firing methods to further increase the efficiency of the T2FSNN. According to our results, the proposed methods can reduce inference latency and number of spikes to 22% and less than 1%, compared to those of burst coding, which is the state-of-the-art result on the CIFAR-100.
C1 [Yoon, Sungroh] Seoul Natl Univ, Dept Elect & Comp Engn, ASRI, INMC, Seoul 08826, South Korea.
   Seoul Natl Univ, Inst Engn Res, Seoul 08826, South Korea.
RP Yoon, S (corresponding author), Seoul Natl Univ, Dept Elect & Comp Engn, ASRI, INMC, Seoul 08826, South Korea.
EM sryoon@snu.ac.kr
CR Adrian ED, 1926, J PHYSIOL-LONDON, V61, P49, DOI 10.1113/jphysiol.1926.sp002273
   [Anonymous], 2015, NIPS
   Butts DA, 2007, NATURE, V449, P92, DOI [10.1038/nature06105, 10.1038/natureO6105]
   Diehl PU, 2015, IEEE IJCNN
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   Jin Y., 2018, NIPS
   Kim J, 2018, NEUROCOMPUTING, V311, P373, DOI 10.1016/j.neucom.2018.05.087
   Kim S., 2020, AAAI
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Montemurro MA, 2008, CURR BIOL, V18, P375, DOI 10.1016/j.cub.2008.02.023
   Park S., 2018, AAAI
   Park S, 2019, PERSPECT CONTEMP KOR, P1, DOI 10.1007/s12083-019-00780-w
   Rueckauer B, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351295
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Tan MX, 2019, PR MACH LEARN RES, V97
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   Wu Yu, 2019, AAAI
   Zhang L, 2019, AAAI
NR 19
TC 72
Z9 78
U1 6
U2 51
PY 2020
DI 10.1109/dac18072.2020.9218689
WC Computer Science, Software Engineering; Computer Science, Theory &
   Methods
DA 2023-11-11
ER

PT J
AU Mirsadeghi, M
   Shalchian, M
   Kheradpisheh, SR
   Masquelier, T
AF Mirsadeghi, Maryam
   Shalchian, Majid
   Kheradpisheh, Saeed Reza
   Masquelier, Timothee
TI Spike time displacement-based error backpropagation in convolutional
   spiking neural networks
SO NEURAL COMPUTING & APPLICATIONS
DT Article
DE StiDi-BP algorithm; Convolutional spiking neural networks; Real-valued
   weights; Binary weights
AB In this paper, we introduce a supervised learning algorithm, which avoids backward recursive gradient computation, for training deep convolutional spiking neural networks (SNNs) with single-spike-based temporal coding. The algorithm employs a linear approximation to compute the derivative of the spike latency with respect to the membrane potential, and it uses spiking neurons with piecewise linear postsynaptic potential to reduce the computational cost and the complexity of neural processing. To evaluate the performance of the proposed algorithm in deep architectures, we employ it in convolutional SNNs for the image classification task. For two popular benchmarks of MNIST and Fashion-MNIST datasets, the network reaches accuracies of, respectively, 99.2 and 92.8%. The trade-off between memory storage capacity and computational cost with accuracy is analyzed by applying two sets of weights: real-valued weights that are updated in the backward pass and their signs, binary weights, that are employed in the feedforward process. We evaluate the binary CSNN on two datasets of MNIST and Fashion-MNIST and obtain acceptable performance with a negligible accuracy drop with respect to real-valued weights (about 0.6 and 0.8% drops, respectively).
C1 [Mirsadeghi, Maryam; Shalchian, Majid] Amirkabir Univ Technol, Dept Elect Engn, Tehran, Iran.
   [Kheradpisheh, Saeed Reza] Shahid Beheshti Univ, Fac Math Sci, Dept Comp & Data Sci, Tehran, Iran.
   [Masquelier, Timothee] Univ Toulouse 3, CerCo UMR 5549, CNRS, Toulouse, France.
RP Shalchian, M (corresponding author), Amirkabir Univ Technol, Dept Elect Engn, Tehran, Iran.
EM shalchian@aut.ac.ir
CR Bohte SM, 2011, LECT NOTES COMPUT SC, V6791, P60, DOI 10.1007/978-3-642-21735-7_8
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Cho J, 2021, ELECTRONICS-SWITZ, V10, DOI 10.3390/electronics10030230
   Comsa IM., 2021, IEEE T NEUR NET LEAR, V35, P1
   Deng Shikuang, 2021, INT C LEARN REPR
   Esser S, 2015, BACKPROPAGATION ENER, P28
   Esser SK, 2016, P NATL ACAD SCI USA, V113, P11441, DOI 10.1073/pnas.1604850113
   Fang W., 2021, P IEEE CVF INT C COM, P2661, DOI 10.1109/ICCV48922.2021.00266
   Han X., 2017, ARXIV
   Huh D, 2018, ADV NEUR IN, V31
   Jolliffe IT, 2016, PHILOS T R SOC A, V374, DOI 10.1098/rsta.2015.0202
   Kheradpisheh SR, 2022, IEEE ACCESS, V10, P70769, DOI 10.1109/ACCESS.2022.3187033
   Kheradpisheh SR, 2022, NEURAL PROCESS LETT, V54, P1255, DOI 10.1007/s11063-021-10680-x
   Kheradpisheh SR, 2020, INT J NEURAL SYST, V30, DOI 10.1142/S0129065720500276
   Kheradpisheh SR, 2016, NEUROCOMPUTING, V205, P382, DOI 10.1016/j.neucom.2016.04.029
   Kundu S, 2021, IEEE WINT CONF APPL, P3952, DOI 10.1109/WACV48630.2021.00400
   Laydevant J., 2021, P IEEE CVF C COMP VI, P4640
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee C, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00119
   Lee JM, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00191
   Liu SY, 2015, PROCEEDINGS 3RD IAPR ASIAN CONFERENCE ON PATTERN RECOGNITION ACPR 2015, P730, DOI 10.1109/ACPR.2015.7486599
   Masquelier T, 2018, FRONT COMPUT NEUROSC, V12, DOI 10.3389/fncom.2018.00074
   Mirsadeghi M, 2021, NEUROCOMPUTING, V427, P131, DOI 10.1016/j.neucom.2020.11.052
   Mostafa H, 2018, IEEE T NEUR NET LEAR, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Mozafari M, 2018, IEEE T NEUR NET LEAR, V29, P6178, DOI 10.1109/TNNLS.2018.2826721
   Muramatsu N, 2021, ARXIV
   Neftci EO, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00324
   Nomura O, 2022, IEEE T CIRCUITS-II, V69, P3640, DOI 10.1109/TCSII.2022.3184313
   Rathi N., 2020, ARXIV
   Roweis S, 2002, ADV NEUR IN
   Rueckauer B, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351295
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Shrestha Sumit Bam, 2018, ADV NEURAL INFORM PR, P1412
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Szegedy C., 2015, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2015.7298594
   Wang YX, 2021, IEEE T COGN DEV SYST, V13, P514, DOI 10.1109/TCDS.2020.2971655
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Zenke F, 2018, NEURAL COMPUT, V30, P1514, DOI 10.1162/neco_a_01086
   Zhang ML, 2022, IEEE T NEUR NET LEAR, V33, P1947, DOI 10.1109/TNNLS.2021.3110991
   Zhang W, 2020, 34 C NEURAL INFORM P
   Zhou SB, 2021, AAAI CONF ARTIF INTE, V35, P11143
NR 42
TC 0
Z9 0
U1 6
U2 6
PD JUL
PY 2023
VL 35
IS 21
BP 15891
EP 15906
DI 10.1007/s00521-023-08567-0
EA APR 2023
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT C
AU Wang, RC
   Jin, C
   McEwan, A
   van Schaik, A
AF Wang, Runchun
   Jin, Craig
   McEwan, Alistair
   van Schaik, Andre
GP IEEE
TI A programmable axonal propagation delay circuit for time-delay spiking
   neural networks
SO 2011 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS (ISCAS)
SE IEEE International Symposium on Circuits and Systems
DT Proceedings Paper
CT IEEE International Symposium on Circuits and Systems (ISCAS)
CY MAY 15-18, 2011
CL Rio de Janeiro, BRAZIL
AB We present an implementation of a programmable axonal propagation delay circuit which uses one first-order log-domain low-pass filter. Delays may be programmed in the 5-50ms range. It is designed to be a building block for time-delay spiking neural networks. It consists of a leaky-integrate-and-fire core, a spike generator circuit, and a delay adaptation circuit.
C1 [Wang, Runchun; Jin, Craig; McEwan, Alistair; van Schaik, Andre] Univ Sydney, Sch Elect & Informat Engn, Sydney, NSW 2006, Australia.
RP Wang, RC (corresponding author), Univ Sydney, Sch Elect & Informat Engn, Sydney, NSW 2006, Australia.
EM rwan6647@uni.sydney.edu.au
CR Eurich CW, 2000, NEUROCOMPUTING, V32, P741, DOI 10.1016/S0925-2312(00)00239-3
   Gerstner W, 1996, NATURE, V383, P76, DOI 10.1038/383076a0
   Horio Y., 1998, IEEE INT C EL CIRC S, P301
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Lazarro J., 1992, IEEE INT S CIRC SYST, P2220
   MINCH BA, 1995, ADV NEURAL INFORMATI, V7, P739
   Python D., 2001, IEEE J SOLID STATE C, V36
   STANFORD LR, 1987, SCIENCE, V238, P358, DOI 10.1126/science.3659918
   Stevens B, 1998, J NEUROSCI, V18, P9303
NR 9
TC 12
Z9 17
U1 0
U2 1
PY 2011
BP 869
EP 872
WC Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Zhang, T
   Xiang, SY
   Liu, WZ
   Han, YA
   Guo, XX
   Hao, Y
AF Zhang, Tao
   Xiang, Shuiying
   Liu, Wenzhuo
   Han, Yanan
   Guo, Xingxing
   Hao, Yue
TI Hybrid Spiking Fully Convolutional Neural Network for Semantic
   Segmentation
SO ELECTRONICS
DT Article
DE spiking convolutional neural network; semantic segmentation; surrogate
   gradient; supervised training
ID INTELLIGENCE; PROCESSOR
AB The spiking neural network (SNN) exhibits distinct advantages in terms of low power consumption due to its event-driven nature. However, it is limited to simple computer vision tasks because the direct training of SNNs is challenging. In this study, we propose a hybrid architecture called the spiking fully convolutional neural network (SFCNN) to expand the application of SNNs in the field of semantic segmentation. To train the SNN, we employ the surrogate gradient method along with backpropagation. The accuracy of mean intersection over union (mIoU) for the VOC2012 dataset is higher than that of existing spiking FCNs by almost 30%. The accuracy of mIoU can reach 39.6%. Moreover, the proposed hybrid SFCNN achieved excellent segmentation performance for other datasets such as COCO2017, DRIVE, and Cityscapes. Our hybrid SFCNN is a valuable and interesting contribution to extending the functionality of SNNs, especially for power-constrained applications.
C1 [Zhang, Tao; Xiang, Shuiying; Liu, Wenzhuo; Han, Yanan; Guo, Xingxing] Xidian Univ, State Key Lab Integrated Serv Networks, Xian 710071, Peoples R China.
   [Xiang, Shuiying; Hao, Yue] Xidian Univ, State Key Discipline Lab Wide Bandgap Semicond Tec, Xian 710071, Peoples R China.
RP Xiang, SY (corresponding author), Xidian Univ, State Key Lab Integrated Serv Networks, Xian 710071, Peoples R China.; Xiang, SY (corresponding author), Xidian Univ, State Key Discipline Lab Wide Bandgap Semicond Tec, Xian 710071, Peoples R China.
EM 21011210532@stu.xidian.edu.cn; syxiang@xidian.edu.cn;
   22011211053@stu.xidian.edu.cn; yhao@xidian.edu.cn; xxguo@xidian.edu.cn;
   ynanhan@stu.xidian.edu.cn
CR Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   Al-Rawi M, 2007, COMPUT BIOL MED, V37, P262, DOI 10.1016/j.compbiomed.2006.03.003
   Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615
   Bu T, 2023, Arxiv, DOI arXiv:2303.04347
   Chaurasia A, 2017, 2017 IEEE VISUAL COMMUNICATIONS AND IMAGE PROCESSING (VCIP)
   Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Ding JH, 2021, Arxiv, DOI arXiv:2105.11654
   Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4
   Ferré P, 2018, FRONT COMPUT NEUROSC, V12, DOI 10.3389/fncom.2018.00024
   Geng QC, 2021, IEEE T IMAGE PROCESS, V30, P2436, DOI 10.1109/TIP.2020.3046921
   github, US
   Horowitz M, 2014, ISSCC DIG TECH PAP I, V57, P10, DOI 10.1109/ISSCC.2014.6757323
   Jégou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57
   Huynh PK, 2022, Arxiv, DOI arXiv:2202.08897
   Kim S, 2020, AAAI CONF ARTIF INTE, V34, P11270
   Kim Y, 2022, NEUROMORPH COMPUT EN, V2, DOI 10.1088/2634-4386/ac9b86
   Le NQK, 2022, J CHEM INF MODEL, DOI 10.1021/acs.jcim.2c01034
   Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508
   Li SX, 2021, IEEE T CIRCUITS-I, V68, P1543, DOI 10.1109/TCSI.2021.3052885
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Long J., 2015, ARXIV14114038, DOI DOI 10.1109/CVPR.2015.7298965
   Lu Y, 2019, LECT NOTES COMPUT SC, V11554, P97, DOI 10.1007/978-3-030-22796-8_11
   Midya R, 2019, ADV ELECTRON MATER, V5, DOI 10.1002/aelm.201900060
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Park S, 2020, DES AUT CON, DOI [10.1109/dac18072.2020.9218689, 10.1007/s00779-020-01476-2]
   Paszke A, 2019, ADV NEUR IN, V32
   Rathi N, 2020, Arxiv, DOI arXiv:2008.03658
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Roy K, 2019, NATURE, V575, P607, DOI 10.1038/s41586-019-1677-2
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Safa A, 2021, SOFTW IMPACTS, V10, DOI 10.1016/j.simpa.2021.100131
   Shastri BJ, 2021, NAT PHOTONICS, V15, P102, DOI 10.1038/s41566-020-00754-y
   Sze V, 2017, P IEEE, V105, P2295, DOI 10.1109/JPROC.2017.2761740
   Xiang SY, 2021, IEEE J SEL TOP QUANT, V27, DOI 10.1109/JSTQE.2020.3005589
   Xiang SY, 2019, IEEE J SEL TOP QUANT, V25, DOI 10.1109/JSTQE.2019.2911565
   Yuan QT, 2023, BRIEF BIOINFORM, V24, DOI 10.1093/bib/bbac630
   Zenke F, 2021, NEURAL COMPUT, V33, P899, DOI 10.1162/neco_a_01367
NR 38
TC 0
Z9 0
U1 3
U2 3
PD SEP
PY 2023
VL 12
IS 17
AR 3565
DI 10.3390/electronics12173565
WC Computer Science, Information Systems; Engineering, Electrical &
   Electronic; Physics, Applied
DA 2023-11-11
ER

PT C
AU Maliavko, AA
   Gavrilov, AV
AF Maliavko, Aleksandr A.
   Gavrilov, Andrey V.
GP IEEE
TI Towards Development of Self-Learning and Self-Modification Spiking
   Neural Network as Model of Brain
SO 2016 13TH INTERNATIONAL SCIENTIFIC-TECHNICAL CONFERENCE ON ACTUAL
   PROBLEMS OF ELECTRONIC INSTRUMENT ENGINEERING (APEIE), VOL 2
SE International Conference on Actual Problems of Electronic Instrument
   Engineering
DT Proceedings Paper
CT 13th International Scientific-Technical Conference on Actual Problems of
   Electronics Instrument Engineering (APEIE)
CY OCT 03-06, 2016
CL Novosibirsk, RUSSIA
DE Artificial intelligence; Neural networks; Brain; Machine learning
AB In this paper novel architecture of artificial intelligence based on spike neural networks is proposed. This architecture includes some perception, some action modules and control module supervising learning of others. Control module is based on generation and learning to generate basic emotions: positive and negative. These emotions are supervising learning and behavior of whole system.
C1 [Maliavko, Aleksandr A.; Gavrilov, Andrey V.] Novosibirsk State Tech Univ, Novosibirsk, Russia.
RP Maliavko, AA (corresponding author), Novosibirsk State Tech Univ, Novosibirsk, Russia.
CR Anokhin Pyotr K., 1974, BIOL NEUROPHYSIOLOGY
   [Anonymous], INTELLIGENCE
   [Anonymous], 2000, HYBRID NEURAL SYSTEM
   [Anonymous], DESIGNING SOLUTIONS
   [Anonymous], P INT C INT INF ENG
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Gavrilov AV, 2003, KORUS 2003: 7TH KOREA-RUSSIA INTERNATIONAL SYMPOSIUM ON SCIENCE AND TECHNOLOGY, VOL 2, PROCEEDINGS, P143
   Smith LS, 2010, ADV EXP MED BIOL, V657, P167, DOI 10.1007/978-0-387-79100-5_9
   Starzyk JA, 2006, IEEE T NEURAL NETWOR, V17, P1460, DOI 10.1109/TNN.2006.883008
NR 9
TC 2
Z9 3
U1 0
U2 4
PY 2016
BP 461
EP 463
WC Engineering, Electrical & Electronic; Instruments & Instrumentation
DA 2023-11-11
ER

PT C
AU Capecci, E
   Espinosa-Ramos, JI
   Mammone, N
   Kasabov, N
   Duun-Henriksen, J
   Kjaer, TW
   Campolo, M
   La Foresta, F
   Morabito, FC
AF Capecci, Elisa
   Espinosa-Ramos, Josafath I.
   Mammone, Nadia
   Kasabov, Nikola
   Duun-Henriksen, Jonas
   Kjaer, Troels Wesenberg
   Campolo, Maurizio
   La Foresta, Fabio
   Morabito, Francesco C.
GP IEEE
TI Modelling Absence Epilepsy Seizure Data in the NeuCube Evolving Spiking
   Neural Network Architecture
SO 2015 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUL 12-17, 2015
CL Killarney, IRELAND
DE Spiking Neural Networks; EEG; NeuCube; Epilepsy; Childhood Absence
   Seizures
AB Epilepsy is the most diffuse brain disorder that can affect people's lives even on its early stage. In this paper, we used for the first time the spiking neural networks (SNN) framework called NeuCube for the analysis of electroencephalography (EEG) data recorded from a person affected by Absence Epileptic (AE), using permutation entropy (PE) features. Our results demonstrated that the methodology constitutes a valuable tool for the analysis and understanding of functional changes in the brain in term of its spiking activity and connectivity. Future applications of the model aim at personalised modelling of epileptic data for the analysis and the event prediction.
C1 [Capecci, Elisa; Kasabov, Nikola] Auckland Univ Technol, Knowledge Engn & Discovery Res Inst, AUT Tower,Level 7,Cnr Rutland & Wakefield St, Auckland 1010, New Zealand.
   [Espinosa-Ramos, Josafath I.] Natl Polytech Inst, Ctr Res Comp, Mexico City 07738, DF, Mexico.
   [Mammone, Nadia] IRCCS Ctr Neurolesi Bonino Pulejo, Messina, Italy.
   [Duun-Henriksen, Jonas] HypoSafe AS, DK-2800 Lyngby, Denmark.
   [Kjaer, Troels Wesenberg] Roskilde Univ Hosp, Dept Neurol, Neurophysiol Ctr, DK-4000 Roskilde, Denmark.
   [Campolo, Maurizio; La Foresta, Fabio; Morabito, Francesco C.] Mediterranean Univ Reggio Calabria, DICEAM Dept, I-89060 Reggio Di Calabria, Italy.
RP Capecci, E (corresponding author), Auckland Univ Technol, Knowledge Engn & Discovery Res Inst, AUT Tower,Level 7,Cnr Rutland & Wakefield St, Auckland 1010, New Zealand.
EM ecapecci@aut.ac.nz; vjier@prodigy.net.mx; nadiamammone@tiscali.it
CR [Anonymous], 2011, IEEE INT S MEDICAL M
   [Anonymous], 2007, RETRIEVED
   Benuskova L., 2007, TOP BIOMED ENG
   Capecci E., 2014, SMART INNOVATION SYS
   Capecci E., 2015, NEURAL NETW IN PRESS
   Chan V, 2007, IEEE T CIRCUITS-I, V54, P48, DOI 10.1109/TCSI.2006.887979
   Crunelli V, 2002, NAT REV NEUROSCI, V3, P371, DOI 10.1038/nrn811
   Dhoble K., 2012, P INT JOINT C NEUR N, P1, DOI 10.1109/IJCNN.2012.6252439
   Doborjeh MG, 2014, 2014 IEEE SYMPOSIUM ON EVOLVING AND AUTONOMOUS LEARNING SYSTEMS (EALS), P73, DOI 10.1109/EALS.2014.7009506
   Duun-Henriksen J, 2012, PEDIATR NEUROL, V46, P287, DOI 10.1016/j.pediatrneurol.2012.02.018
   Duun-Henriksen J, 2012, CLIN NEUROPHYSIOL, V123, P84, DOI 10.1016/j.clinph.2011.06.001
   Espinosa-Ramos J. I., 2015, NEURAL NETWORK UNPUB
   Gerstner W, 2001, PLAUSIBLE NEURAL NET
   Kasabov Nikola, 2012, Artificial Neural Networks in Pattern Recognition. Proceedings of the 5th INNS IAPR TC 3 GIRPR Workshop, ANNPR 2012, P225, DOI 10.1007/978-3-642-33212-8_21
   Kasabov N., 2014, INFORM SCI
   Kasabov N., 2014, NEUROCOMPUT IN PRESS
   Kasabov N, 2013, NEURAL NETWORKS, V41, P188, DOI 10.1016/j.neunet.2012.11.014
   Kasabov N, 2010, NEURAL NETWORKS, V23, P16, DOI 10.1016/j.neunet.2009.08.010
   Kasabov NK, 2014, NEURAL NETWORKS, V52, P62, DOI 10.1016/j.neunet.2014.01.006
   Koessler L, 2009, NEUROIMAGE, V46, P64, DOI 10.1016/j.neuroimage.2009.02.006
   Lancaster JL, 2000, HUM BRAIN MAPP, V10, P120, DOI 10.1002/1097-0193(200007)10:3<120::AID-HBM30>3.0.CO;2-8
   Mammone N, 2012, INT J NEURAL SYST, V22, DOI 10.1142/S0129065712500244
   Manning JPA, 2003, TRENDS PHARMACOL SCI, V24, P542, DOI 10.1016/j.tips.2003.08.006
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Talairach J., 1988, COPLANAR STEREOTAXIS
   Thorpe S, 1998, COMPUTATIONAL NEUROSCIENCE: TRENDS IN RESEARCH, P113
   Tu EM, 2014, IEEE IJCNN, P638, DOI 10.1109/IJCNN.2014.6889717
   Yixiong Chen, 2013, Neural Information Processing. 20th International Conference, ICONIP 2013. Proceedings: LNCS 8228, P70, DOI 10.1007/978-3-642-42051-1_10
NR 28
TC 0
Z9 0
U1 1
U2 1
PY 2015
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Lobo, JL
   Lana, I
   Del Ser, J
   Bilbao, MN
   Kasabov, N
AF Lobo, Jesus L.
   Lana, Ibai
   Del Ser, Javier
   Bilbao, Miren Nekane
   Kasabov, Nikola
TI Evolving Spiking Neural Networks for online learning over drifting data
   streams
SO NEURAL NETWORKS
DT Review
DE Spiking Neural Networks; Data reduction; Online learning; Concept drift
ID SELF-GENERATING PROTOTYPES; PRINCIPLES
AB Nowadays huge volumes of data are produced in the form of fast streams, which are further affected by non-stationary phenomena. The resulting lack of stationarity in the distribution of the produced data calls for efficient and scalable algorithms for online analysis capable of adapting to such changes (concept drift). The online learning field has lately turned its focus on this challenging scenario, by designing incremental learning algorithms that avoid becoming obsolete after a concept drift occurs. Despite the noted activity in the literature, a need for new efficient and scalable algorithms that adapt to the drift still prevails as a research topic deserving further effort. Surprisingly, Spiking Neural Networks, one of the major exponents of the third generation of artificial neural networks, have not been thoroughly studied as an online learning approach, even though they are naturally suited to easily and quickly adapting to changing environments. This work covers this research gap by adapting Spiking Neural Networks to meet the processing requirements that online learning scenarios impose. In particular the work focuses on limiting the size of the neuron repository and making the most of this limited size by resorting to data reduction techniques. Experiments with synthetic and real data sets are discussed, leading to the empirically validated assertion that, by virtue of a tailored exploitation of the neuron repository, Spiking Neural Networks adapt better to drifts, obtaining higher accuracy scores than naive versions of Spiking Neural Networks for online learning environments. (C) 2018 Elsevier Ltd. All rights reserved.
C1 [Lobo, Jesus L.; Lana, Ibai; Del Ser, Javier] TECNALIA, Div ICT, Parque Tecnol Bizkaia, Derio 48160, Spain.
   [Del Ser, Javier; Bilbao, Miren Nekane] Univ Basque Country UPV EHU, Bilbao 48013, Spain.
   [Del Ser, Javier] BCAM, Bilbao 48009, Spain.
   [Kasabov, Nikola] Auckland Univ Technol, KEDRI, Auckland 1010, New Zealand.
RP Lobo, JL (corresponding author), TECNALIA, Div ICT, Parque Tecnol Bizkaia, Derio 48160, Spain.
EM jesus.lopez@tecnalia.com
CR Aggarwal Charu C, 2006, P 32 INT C VERY LARG, P607
   Alippi C, 2008, IEEE T NEURAL NETWOR, V19, P2053, DOI 10.1109/TNN.2008.2003998
   Alippi Cesare, 2014, INTELLIGENCE EMBEDDE
   Alnajjar F, 2008, IEEE IJCNN, P2207, DOI 10.1109/IJCNN.2008.4634103
   [Anonymous], 2018, EVOL SYST-GER, DOI DOI 10.1007/s12530-016-9168-2
   [Anonymous], 2007, MACHINE LEARNING DAT
   Baena-Garcia M., 2006, 4 INT WORKSH KNOWL D, V6, P77, DOI DOI 10.1007/978-3-642-23857-4_12
   Barddal JP, 2017, J SYST SOFTWARE, V127, P278, DOI 10.1016/j.jss.2016.07.005
   Belatreche A, 2007, SOFT COMPUT, V11, P239, DOI [10.1007/s00500-006-0065-7, 10.1007/S00500-006-0065-7]
   Bifet A, 2006, LECT NOTES ARTIF INT, V4265, P29
   Bifet A, 2009, LECT NOTES ARTIF INT, V5828, P23, DOI 10.1007/978-3-642-05224-8_4
   Bifet A, 2007, PROCEEDINGS OF THE SEVENTH SIAM INTERNATIONAL CONFERENCE ON DATA MINING, P443
   Bohte S. M., 2000, 8th European Symposium on Artificial Neural Networks. ESANN"2000. Proceedings, P419
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324
   Cohen E., 2003, P 22 ACM SIGMOD SIGA, P223, DOI DOI 10.1145/773153.773175
   Dawid AP, 1999, BERNOULLI, V5, P125, DOI 10.2307/3318616
   Derrac J., 2010, MEMET COMPUT, V2, P183, DOI DOI 10.1007/S12293-010-0048-1
   Ditzler G, 2015, IEEE COMPUT INTELL M, V10, P12, DOI 10.1109/MCI.2015.2471196
   Domingos P, 2003, J COMPUT GRAPH STAT, V12, P945, DOI 10.1198/1061860032544
   Domingos P., 2000, Proceedings. KDD-2000. Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P71, DOI 10.1145/347090.347107
   Elwell R, 2011, IEEE T NEURAL NETWOR, V22, P1517, DOI 10.1109/TNN.2011.2160459
   Escalante HJ, 2016, APPL SOFT COMPUT, V40, P569, DOI 10.1016/j.asoc.2015.12.015
   Fayed HA, 2007, PATTERN RECOGN, V40, P1498, DOI 10.1016/j.patcog.2006.10.018
   Gama J, 2004, LECT NOTES ARTIF INT, V3171, P286
   Gama J, 2014, ACM COMPUT SURV, V46, DOI 10.1145/2523813
   García S, 2012, IEEE T PATTERN ANAL, V34, P417, DOI 10.1109/TPAMI.2011.142
   Gerstner W., 2002, SPIKING NEURON MODEL
   Gomes HM, 2017, MACH LEARN, V106, P1469, DOI 10.1007/s10994-017-5642-8
   Gomes HM, 2017, ACM COMPUT SURV, V50, DOI 10.1145/3054925
   Gonçalves PM, 2013, PATTERN RECOGN LETT, V34, P1018, DOI 10.1016/j.patrec.2013.02.005
   GROSSBERG S, 1988, NEURAL NETWORKS, V1, P17, DOI 10.1016/0893-6080(88)90021-4
   Harries Michael, 1999, SPLICE 2 COMP EVALUA
   HART PE, 1968, IEEE T INFORM THEORY, V14, P515, DOI 10.1109/TIT.1968.1054155
   Hu WW, 2016, IEEE T CYBERNETICS, V46, P2719, DOI 10.1109/TCYB.2015.2487318
   Kasabov N., 2007, EVOLVING CONNECTIONI
   Kasabov N, 2016, NEURAL NETWORKS, V78, P1, DOI 10.1016/j.neunet.2015.09.011
   Kasabov N, 2013, NEURAL NETWORKS, V41, P188, DOI 10.1016/j.neunet.2012.11.014
   Kasabov NK, 2014, NEURAL NETWORKS, V52, P62, DOI 10.1016/j.neunet.2014.01.006
   Khamassi I., 2017, CEUR WORKSHOP P, V1958
   Khamassi I, 2015, COGN COMPUT, V7, P772, DOI 10.1007/s12559-015-9341-0
   Klinkenberg R., 2004, Intelligent Data Analysis, V8, P281
   Krawczyk B, 2017, INFORM FUSION, V37, P132, DOI 10.1016/j.inffus.2017.02.004
   Li J, 2015, INT J SENS NETW, V17, P163, DOI 10.1504/IJSNET.2015.068179
   Lobo J. L., 2017, APPL SOFT COMPUTING
   Meena L, 2015, LECT NOTES COMPUT SC, V9489, P671, DOI 10.1007/978-3-319-26532-2_74
   Minku LL, 2012, IEEE T KNOWL DATA EN, V24, P619, DOI 10.1109/TKDE.2011.58
   Minku LL, 2010, IEEE T KNOWL DATA EN, V22, P730, DOI 10.1109/TKDE.2009.156
   Ng W, 2008, LECT NOTES COMPUT SC, V4947, P204, DOI 10.1007/978-3-540-78568-2_17
   Oliveira DVR, 2012, PROC INT C TOOLS ART, P904, DOI 10.1109/ICTAI.2012.126
   Ponulak F., 2005, RESUME NEW SUPERVISE, V42
   Ponulak F, 2008, INT J APPL MATH COMP, V18, P117, DOI 10.2478/v10006-008-0011-1
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Ruiqin Chang, 2011, Journal of Computers, V6, P1493, DOI 10.4304/jcp.6.7.1493-1500
   Schliebs S, 2013, EVOL SYST-GER, V4, P87, DOI 10.1007/s12530-013-9074-9
   Soltic S, 2008, IEEE IJCNN, P2091, DOI 10.1109/IJCNN.2008.4634085
   Soltic S, 2010, INT J NEURAL SYST, V20, P437, DOI 10.1142/S012906571000253X
   Thorpe S, 1998, COMPUTATIONAL NEUROSCIENCE: TRENDS IN RESEARCH, P113
   Thorpe SJ, 1997, ADV NEUR IN, V9, P901
   TOMEK I, 1976, IEEE T SYST MAN CYB, V6, P769, DOI 10.1109/tsmc.1976.4309452
   TOMEK I, 1976, IEEE T SYST MAN CYB, V6, P448
   Triguero I, 2012, IEEE T SYST MAN CY C, V42, P86, DOI 10.1109/TSMCC.2010.2103939
   Triguero I, 2010, IEEE T NEURAL NETWOR, V21, P1984, DOI 10.1109/TNN.2010.2087415
   VITTER JS, 1985, ACM T MATH SOFTWARE, V11, P37, DOI 10.1145/3147.3165
   Wang JL, 2017, IEEE T NEUR NET LEAR, V28, P30, DOI 10.1109/TNNLS.2015.2501322
   Wang JL, 2014, NEUROCOMPUTING, V144, P526, DOI 10.1016/j.neucom.2014.04.017
   Wang S, 2018, IEEE T NEUR NET LEAR, V29, P4802, DOI 10.1109/TNNLS.2017.2771290
   Webb GI, 2016, DATA MIN KNOWL DISC, V30, P964, DOI 10.1007/s10618-015-0448-4
   WILSON DL, 1972, IEEE T SYST MAN CYB, VSMC2, P408, DOI 10.1109/TSMC.1972.4309137
   Wysoski SG, 2006, LECT NOTES COMPUT SC, V4179, P1133
   Wysoski SG, 2010, NEURAL NETWORKS, V23, P819, DOI 10.1016/j.neunet.2010.04.009
   Zhou ZH, 2014, IEEE COMPUT INTELL M, V9, P62, DOI 10.1109/MCI.2014.2350953
   Zliobaite I, 2016, STUD BIG DATA, V16, P91, DOI 10.1007/978-3-319-26989-4_4
   Zliobaite Indre, 2010, ABS10104784 CORR
NR 74
TC 45
Z9 47
U1 0
U2 47
PD DEC
PY 2018
VL 108
BP 1
EP 19
DI 10.1016/j.neunet.2018.07.014
WC Computer Science, Artificial Intelligence; Neurosciences
DA 2023-11-11
ER

PT C
AU Kuang, ZB
   Wang, J
   Yang, SM
   Yi, GS
   Deng, B
   Wei, XL
AF Kuang, Zaibo
   Wang, Jiang
   Yang, Shuangming
   Yi, Guosheng
   Deng, Bin
   Wei, Xile
GP IEEE
TI Digital Implementation of the Spiking Neural Network and Its Digit
   Recognition
SO PROCEEDINGS OF THE 2019 31ST CHINESE CONTROL AND DECISION CONFERENCE
   (CCDC 2019)
SE Chinese Control and Decision Conference
DT Proceedings Paper
CT 31st Chinese Control And Decision Conference (CCDC)
CY JUN 03-05, 2019
CL Nanchang, PEOPLES R CHINA
DE Spiking neural network; Real-time implementation; FPGA; STDP; Digit
   recognition
ID EFFICIENT FPGA IMPLEMENTATION
AB Motivated by biological principles of neural systems, spiking neural network (SNN) shows a tremendous potential in solving pattern recognition and cognitive tasks in recent years. In this study, a biologically inspired SNN composed of three layers is implemented on a reconfigurable FPGA with high computational efficiency and low hardware cost. The proposed SNN is consists of spiking neurons simulated by leaky-integrate-and-fire neuron model. In addition, spiking-time-dependent-plasticity based on event-driven is utilized to train the constructed network. The real-time hardware realization of the proposed SNN demonstrates powerful and efficient learning scheme. Results on different datasets shows that the proposed SNN implementation has the merit of capability of coping with pattern recognition tasks. Furthermore, the proposed implementation with remarkable performance could be applied and embed in bio-inspired neuromorphic platform such as robots for recognition tasks and on-line applications.
C1 [Kuang, Zaibo; Wang, Jiang; Yang, Shuangming; Yi, Guosheng; Deng, Bin; Wei, Xile] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
RP Wang, J (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
EM jiangwang@tju.edu.cn
CR Berger TW, 2012, IEEE T NEUR SYS REH, V20, P198, DOI 10.1109/TNSRE.2012.2189133
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Lee SW, 2015, PLOS BIOL, V13, DOI 10.1371/journal.pbio.1002137
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   O'reilly R.C., 2000, COMPUTATIONAL EXPLOR
   Rumelhart David E., 1986, PARALLEL DISTRIBUTED, V1, DOI 10.1016/b978-1-4832-1446-7.50035-2
   Tal D, 2014, NEURAL COMPUT, V9, P305
   Xin Y, 2014, MICROELECTRON J, V45, P690, DOI 10.1016/j.mejo.2014.03.018
   Yang S., 2018, IEEE T CYBERNETICS
   Yang SM, 2018, NEUROCOMPUTING, V314, P394, DOI 10.1016/j.neucom.2018.07.006
   Yang SM, 2018, NEUROCOMPUTING, V282, P262, DOI 10.1016/j.neucom.2017.12.031
   Yang SM, 2018, PHYSICA A, V494, P484, DOI 10.1016/j.physa.2017.11.155
   Yang SM, 2017, NEURAL NETWORKS, V94, P220, DOI 10.1016/j.neunet.2017.07.012
   Yang SM, 2017, SCI REP-UK, V7, DOI 10.1038/srep40152
   Yang SM, 2016, NEUROCOMPUTING, V177, P274, DOI 10.1016/j.neucom.2015.11.026
   Yang SM, 2015, NEURAL NETWORKS, V71, P62, DOI 10.1016/j.neunet.2015.07.017
NR 17
TC 3
Z9 3
U1 3
U2 11
PY 2019
BP 3621
EP 3625
DI 10.1109/ccdc.2019.8832952
WC Automation & Control Systems; Operations Research & Management Science
DA 2023-11-11
ER

PT J
AU Wu, J
   Furber, S
AF Wu, Jian
   Furber, Steve
TI A Multicast Routing Scheme for a Universal Spiking Neural Network
   Architecture
SO COMPUTER JOURNAL
DT Article
DE neural network; routing; event-driven; multicast; fault-tolerance
ID SIMULATION
AB A multicast routing infrastructure is proposed as a core feature of SpiNNaker, a massively parallel computer for the real-time simulation of large-scale spiking neural networks. The infrastructure is implemented using a communications router, based on an event-driven routing scheme, on each multicore processing node in the system. The design considerations emphasize the difference between the requirements of neural network communications and those of conventional computer networks and on-chip networks. The focus of the design is on neural modelling flexibility, power-efficiency, fault-tolerance and the communication throughput of the router.
C1 [Wu, Jian; Furber, Steve] Univ Manchester, Sch Comp Sci, Adv Processor Technol Grp, Manchester M13 9PL, Lancs, England.
RP Wu, J (corresponding author), Univ Manchester, Sch Comp Sci, Adv Processor Technol Grp, Oxford Rd, Manchester M13 9PL, Lancs, England.
EM wuj@cs.man.ac.uk
CR Agis R, 2007, INT J ELECTRON, V94, P469, DOI 10.1080/00207210701308625
   [Anonymous], P 10 SIAM C PAR PROC
   Bainbridge J, 2002, IEEE MICRO, V22, P16, DOI 10.1109/MM.2002.1044296
   Boahen KA, 2000, IEEE T CIRCUITS-II, V47, P416, DOI 10.1109/82.842110
   Boucheny C, 2005, LECT NOTES COMPUT SC, V3512, P136
   Cong B, 1997, PROCEEDINGS OF THE IEEE 1997 AEROSPACE AND ELECTRONICS CONFERENCE - NAECON 1997, VOLS 1 AND 2, P231, DOI 10.1109/NAECON.1997.618084
   Dally W., 2003, PRINCIPLES PRACTICES
   Danese G, 2002, IEEE MICRO, V22, P20, DOI 10.1109/MM.2002.1013301
   Furber S, 2006, P AISB WORKSH GC5 AR, P29
   Furber S, 2008, STUD COMPUT INTELL, V115, P763, DOI 10.1098/rsif.2006.0177
   GELENBE E, 1993, NEURAL COMPUT, V5, P154, DOI 10.1162/neco.1993.5.1.154
   Gelenbe E, 1989, NEURAL COMPUT, V1, P502, DOI 10.1162/neco.1989.1.4.502
   Gerstner W., 2002, SPIKING NEURON MODEL
   HAMMERSTROM D, 1990, P INT JOINT C NEUR N, P537
   Hu JC, 2004, DES AUT CON, P260
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   JAHNKE A, 1997, LECT NOTES COMPUTER, V1327, P1187
   JIN X, 2008, P IJCNN 08 HONG KONG, P2813
   Kumar A, 2007, PR IEEE COMP DESIGN, P63, DOI 10.1109/ICCD.2007.4601881
   Plana Luis A., 2008, 2008 2nd ACM/IEEE International Symposium on Networks-on-Chip (NOCS '08), P215, DOI 10.1109/NOCS.2008.4492744
   Plana LA, 2007, IEEE DES TEST COMPUT, V24, P454, DOI 10.1109/MDT.2007.149
   Plesser HE, 2007, LECT NOTES COMPUT SC, V4641, P672
   RAJEEV D, 1998, IEEE T PARALL DISTR, V9, P1004
   Ramacher U., 1995, Proceedings 9th International Parallel Processing Symposium (Cat. No.95TH8052), P774, DOI 10.1109/IPPS.1995.395862
   Rumelhart DE., 1988, PARALLEL DISTRIBUTED
   Sparso J, 2001, PRINCIPLES OF ASYNCHRONOUS CIRCUIT DESIGN: A SYSTEMS PERSPECTIVE, P3
   SUN Q, 2007, P 2007 INT C PAR PRO, P21
   TUCKER LW, 1988, COMPUTER, V21, P26, DOI 10.1109/2.74
   Tutsch D, 2001, LECT NOTES COMPUT SC, V2093, P478
   Wu J., 2006, P 18 UK AS FOR, V2, P16
   STRUCTURE TYPICAL NE
NR 31
TC 14
Z9 14
U1 4
U2 8
PD MAR
PY 2010
VL 53
IS 3
BP 280
EP 288
DI 10.1093/comjnl/bxp024
WC Computer Science, Hardware & Architecture; Computer Science, Information
   Systems; Computer Science, Software Engineering; Computer Science,
   Theory & Methods
DA 2023-11-11
ER

PT C
AU Hsieh, HY
   Li, PY
   Yang, CH
   Tang, KT
AF Hsieh, Hung-Yi
   Li, Pin-Yi
   Yang, Cheng-Han
   Tang, Kea-Tiong
GP IEEE
TI A High Learning Capability Probabilistic Spiking Neural Network Chip
SO 2018 INTERNATIONAL SYMPOSIUM ON VLSI DESIGN, AUTOMATION AND TEST
   (VLSI-DAT)
SE International Symposium on VLSI Design Automation and Test
DT Proceedings Paper
CT International Symposium on VLSI Design, Automation and Test (VLSI-DAT)
CY APR 16-19, 2018
CL Hsinchu, TAIWAN
DE Online learning; Probabilistic Spiking Neural Network;
   Switehed-capacitor; Learning capability
ID IMPLEMENTATION; HARDWARE
AB This paper presents an analog implementation of probabilistic spiking neural network for portable or biomedical applications which require learning or classification. Online learning adjusts weights by spike based computation. The weight is saved in the long-term synaptic memory. The circuit primarily uses the switched-capacitor structures and was fabricated using 0.18 mu m CMOS technology. This chip consumes less than 10 mu W under a 1V supply and the core area of the chip occupies 0.43mm(2). The chip can learn 80 random patterns with the area under curve of 0.8. The result indicates the chip is appropriate for portable or biomedical applications.
C1 [Hsieh, Hung-Yi; Li, Pin-Yi; Yang, Cheng-Han; Tang, Kea-Tiong] Natl Tsing Hua Univ, Dept Elect Engn, Neuromorph & Biomed Engn Lab, Hsinchu 30013, Taiwan.
RP Hsieh, HY (corresponding author), Natl Tsing Hua Univ, Dept Elect Engn, Neuromorph & Biomed Engn Lab, Hsinchu 30013, Taiwan.
EM hyhsieh@larc.ee.nthu.edu.tw; pinyili@foxmial.com; jason950374@gmail.com;
   kttang@ee.nthu.edu.tw
CR Cauwenberghs G, 1999, IEEE T CIRCUITS-II, V46, P240, DOI 10.1109/82.754858
   Fawcett T, 2006, PATTERN RECOGN LETT, V27, P861, DOI 10.1016/j.patrec.2005.10.010
   Hsieh HY, 2013, IEEE T NEUR NET LEAR, V24, P2063, DOI 10.1109/TNNLS.2013.2271644
   Hsieh HY, 2012, IEEE T NEUR NET LEAR, V23, P1065, DOI 10.1109/TNNLS.2012.2195329
   Kasabov N, 2010, NEURAL NETWORKS, V23, P16, DOI 10.1016/j.neunet.2009.08.010
   Misra J, 2010, NEUROCOMPUTING, V74, P239, DOI 10.1016/j.neucom.2010.03.021
   Mitra S, 2009, IEEE T BIOMED CIRC S, V3, P32, DOI 10.1109/TBCAS.2008.2005781
   Sun Q, 2011, IEEE T NEURAL NETWOR, V22, P858, DOI 10.1109/TNN.2011.2125986
   Sun YW, 2012, COMPUT BIOL MED, V42, P751, DOI 10.1016/j.compbiomed.2012.04.007
   Zhang R, 2012, IEEE T NEUR NET LEAR, V23, P330, DOI 10.1109/TNNLS.2011.2178315
NR 10
TC 2
Z9 2
U1 0
U2 0
PY 2018
WC Automation & Control Systems; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Eshraghian, JK
   Wang, XX
   Lu, WD
AF Eshraghian, Jason K.
   Wang, Xinxin
   Lu, Wei D.
TI Memristor-Based Binarized Spiking Neural Networks
SO IEEE NANOTECHNOLOGY MAGAZINE
DT Article
ID MEMORY
C1 [Eshraghian, Jason K.; Wang, Xinxin; Lu, Wei D.] Univ Michigan, Dept Elect Engn & Comp Sci, Ann Arbor, MI 48109 USA.
RP Eshraghian, JK (corresponding author), Univ Michigan, Dept Elect Engn & Comp Sci, Ann Arbor, MI 48109 USA.
EM jeshraghian@gmail.com; xinxinw@umich.edu; wluee@eecs.umich.edu
CR Amodei D., 2019, AI AND COMPUTE
   [Anonymous], 2013, RELIABILITY MAINTAIN, DOI DOI 10.1109/IEDM.2013.6724674
   Azghadi MR, 2020, ADV INTELL SYST-GER, V2, DOI 10.1002/aisy.201900189
   Bakir MS, 2008, IEEE CUST INTEGR CIR, P663, DOI 10.1109/CICC.2008.4672173
   Banbury Colby R., 2020, ARXIV200304821
   Bengio Yoshua, 2013, ABS13083432 CORR
   Bergstra J., 2011, ADV NEURAL INFORM PR, V24, DOI 10.5555/2986459.2986743
   BLAKEMOR.C, 1970, NATURE, V228, P37, DOI 10.1038/228037a0
   Brown T., 2020, PROC ADV NEUR INF PR, P1877
   Cai FX, 2019, NAT ELECTRON, V2, P290, DOI 10.1038/s41928-019-0270-x
   Chanthbouala A, 2012, NAT MATER, V11, P860, DOI [10.1038/nmat3415, 10.1038/NMAT3415]
   Chen B, 2015, 2015 IEEE INTERNATIONAL ELECTRON DEVICES MEETING (IEDM)
   Chen YC, 2021, ELEC COMP C, P1645, DOI 10.1109/ECTC32696.2021.00261
   Choi BJ, 2005, J APPL PHYS, V98, DOI 10.1063/1.2001146
   Correll JM, 2020, IEEE J EXPLOR SOLID-, V6, P36, DOI 10.1109/JXCDC.2020.2992228
   Davies M., 2017, ARXIV170505475
   Dhar P, 2020, NAT MACH INTELL, V2, P423, DOI 10.1038/s42256-020-0219-9
   Dong S, 2018, PROCEEDINGS OF THE 2018 ACM/SPEC INTERNATIONAL CONFERENCE ON PERFORMANCE ENGINEERING (ICPE '18), P96, DOI 10.1145/3184407.3184423
   Ebong I, 2010, INT C MICROELECTRON, P292, DOI 10.1109/ICM.2010.5696142
   Eshraghian J. K., 2021, ARXIV210912894
   Eshraghian JK, 2021, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS51556.2021.9401672
   Eshraghian JK, 2019, 2019 IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE CIRCUITS AND SYSTEMS (AICAS 2019), P267, DOI [10.1109/aicas.2019.8771550, 10.1109/AICAS.2019.8771550]
   Eshraghian JK, 2018, IEEE T VLSI SYST, V26, P2816, DOI 10.1109/TVLSI.2018.2829918
   Esser Steven K, 2019, ARXIV190208153
   Fang W., 2021, P IEEE CVF INT C COM, P2661, DOI 10.1109/ICCV48922.2021.00266
   Frenkel, ARXIV210601288, V2021
   Gou JP, 2021, INT J COMPUT VISION, V129, P1789, DOI 10.1007/s11263-021-01453-z
   Gupta S, 2015, PR MACH LEARN RES, V37, P1737
   Hady FT, 2017, P IEEE, V105, P1822, DOI 10.1109/JPROC.2017.2731776
   Han S., 2015, ARXIV151000149
   Hu M, 2016, DES AUT CON, DOI 10.1145/2897937.2898010
   Hubara I, 2016, ADV NEUR IN, V29
   Hunsberger Eric, 2015, ARXIV151008829
   Ielmini D, 2018, NAT ELECTRON, V1, P333, DOI 10.1038/s41928-018-0092-2
   Jamieson K, 2016, JMLR WORKSH CONF PRO, V51, P240
   Jouppi NP, 2017, 44TH ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA 2017), P1, DOI 10.1145/3079856.3080246
   Kang SM, 2021, IEEE T CIRCUITS-I, V68, P4837, DOI 10.1109/TCSI.2021.3126555
   King DB, 2015, ACS SYM SER, V1214, P1
   Knag, P 2016 IEEE S VLSI C, P1, DOI [10.1109/VLSIC.2016.7573526, DOI 10.1109/VLSIC.2016.7573526]
   Knag P, 2015, IEEE J SOLID-ST CIRC, V50, P1070, DOI 10.1109/JSSC.2014.2386892
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
   Lapique L., 1907, J PHYSL PATHOL GEN, V9, P620, DOI DOI 10.1007/S00422-007-0189-6
   Lask, 2020, ARXIV201203837
   Li YY, 2020, ADV MATER, V32, DOI 10.1002/adma.202003984
   Li ZY, 2021, IEEE J SOLID-ST CIRC, V56, P1105, DOI 10.1109/JSSC.2020.3045369
   Lin CY, 2020, APPL MATER TODAY, V21, DOI 10.1016/j.apmt.2020.100848
   Lin CY, 2020, SMALL, V16, DOI 10.1002/smll.202003964
   Loshchilov I., 2017, 5 INT C LEARNING REP, P1
   Menzel S, 2012, J APPL PHYS, V111, DOI 10.1063/1.3673239
   Naveros F, 2015, IEEE T NEUR NET LEAR, V26, P1567, DOI 10.1109/TNNLS.2014.2345844
   Perez-Nieves N, 2021, NAT COMMUN, V12, DOI 10.1038/s41467-021-26022-3
   Polino A., 2018, ARXIV180205668
   Rich D., 2020, NANOCHIPS 2030, P127, DOI DOI 10.1007/978-3-030-18338-7_9
   Richards BA, 2019, NAT NEUROSCI, V22, P1761, DOI 10.1038/s41593-019-0520-2
   Sebastian A, 2020, NAT NANOTECHNOL, V15, P529, DOI 10.1038/s41565-020-0655-z
   Shafiee A, 2016, CONF PROC INT SYMP C, P14, DOI 10.1109/ISCA.2016.12
   Shim WB, 2020, INT RELIAB PHY SYM, DOI 10.1109/irps45951.2020.9129252
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Strukov DB, 2008, NATURE, V453, P80, DOI 10.1038/nature06932
   Thompson N. C., 2020, ARXIV200705558
   Uhlich S., ARXIV190511452
   Umuroglu Y, 2017, FPGA'17: PROCEEDINGS OF THE 2017 ACM/SIGDA INTERNATIONAL SYMPOSIUM ON FIELD-PROGRAMMABLE GATE ARRAYS, P65, DOI 10.1145/3020078.3021744
   Valov I, 2011, NANOTECHNOLOGY, V22, DOI 10.1088/0957-4484/22/25/254003
   WAN WE, 2020, ISSCC DIG TECH PAP I, P498, DOI DOI 10.1109/ISSCC19947.2020.9062979
   Wang QW, 2019, INT EL DEVICES MEET, DOI 10.1109/iedm19573.2019.8993641
   Wang XX, 2022, IEEE T CIRCUITS-II, V69, P559, DOI 10.1109/TCSII.2021.3097035
   Wang XX, 2020, IEEE T CIRCUITS-I, V67, P4224, DOI 10.1109/TCSI.2020.3000468
   WERBOS PJ, 1990, P IEEE, V78, P1550, DOI 10.1109/5.58337
   Wolf Lasse F, 2020, PROC ICML WORKSHOP C
   Yao P, 2020, NATURE, V577, P641, DOI 10.1038/s41586-020-1942-4
   Zenke F, 2021, NEURAL COMPUT, V33, P899, DOI 10.1162/neco_a_01367
   Zhang JL, 2017, FPGA'17: PROCEEDINGS OF THE 2017 ACM/SIGDA INTERNATIONAL SYMPOSIUM ON FIELD-PROGRAMMABLE GATE ARRAYS, P25, DOI 10.1145/3020078.3021698
   Zhang T., 2019, ARXIV191004540
   Zhu JY, 2018, DES AUT TEST EUROPE, P241, DOI 10.23919/DATE.2018.8342010
   Zidan MA, 2018, NAT ELECTRON, V1, P22, DOI 10.1038/s41928-017-0006-8
   Zidan MA, 2013, MICROELECTRON J, V44, P176, DOI 10.1016/j.mejo.2012.10.001
NR 76
TC 25
Z9 25
U1 0
U2 19
PD APR
PY 2022
VL 16
IS 2
BP 14
EP 23
DI 10.1109/MNANO.2022.3141443
EA JAN 2022
WC Nanoscience & Nanotechnology
DA 2023-11-11
ER

PT C
AU Daddinounou, S
   Vatajelu, EI
AF Daddinounou, Salah
   Vatajelu, Elena Ioana
BE Kubatova, H
   Steininger, A
   Jenihhin, M
   Garbolino, T
   Fiser, P
   Belohoubek, J
   Borecky, J
TI Synaptic Control for Hardware Implementation of Spike Timing Dependent
   Plasticity
SO 2022 25TH INTERNATIONAL SYMPOSIUM ON DESIGN AND DIAGNOSTICS OF
   ELECTRONIC CIRCUITS AND SYSTEMS (DDECS)
SE IEEE International Symposium on Design and Diagnostics of Electronic
   Circuits & Systems
DT Proceedings Paper
CT 25th International Symposium on Design and Diagnostics of Electronic
   Circuits and Systems (DDECS)
CY APR 06-08, 2022
CL Prague, CZECH REPUBLIC
DE Spiking Neural Networks (SNN); unsupervised learning; Magnetic Tunnel
   Junction (MTJ); neuromorphics.
ID NETWORK
AB Spiking neural networks (SNN) are biologically plausible networks. Compared to formal neural networks, they come with huge benefits related to their asynchronous processing and massively parallel architecture. Recent developments in neuromorphics aim to implement these SNNs in hardware to fully exploit their potential in terms of low energy consumption. In this paper, the plasticity of a multi-state conductance synapse in SNN is shown. The synapse is a compound of multiple Magnetic Tunnel Junction (MTJ) devices connected in parallel. The network performs learning by potentiation and depression of the synapses. In this paper we show how these two mechanisms can be obtained in hardware-implemented SNNs. We present a methodology to achieve the Spike Timing Dependent Plasticity (STDP) learning rule in hardware by carefully engineering the post- and pre-synaptic signals. We demonstrate synaptic plasticity as a function of the relative spiking time of input and output neurons only.
C1 [Daddinounou, Salah; Vatajelu, Elena Ioana] Univ Grenoble Alpes, CNRS, Grenoble INP, TIMA, F-38000 Grenoble, France.
RP Daddinounou, S (corresponding author), Univ Grenoble Alpes, CNRS, Grenoble INP, TIMA, F-38000 Grenoble, France.
EM salah.daddinounou@univ-grenoble-alpes.fr; elena-ioana.vatajelu@cnrs.fr
CR Andreeva NV, 2020, BIONANOSCIENCE, V10, P824, DOI 10.1007/s12668-020-00778-2
   Ballard Z, 2021, NAT MACH INTELL, V3, P556, DOI 10.1038/s42256-021-00360-9
   Courbariaux M, 2016, Arxiv, DOI [arXiv:1602.02830, DOI 10.48550/ARXIV.1602.02830]
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Furber SB, 2013, IEEE T COMPUT, V62, P2454, DOI 10.1109/TC.2012.142
   Kok M., 2020, THESIS U TWENTE
   LECUN Y, 1989, P ADV NEUR INF PROC, V2, P1
   Maranhao G, 2021, IET CIRC DEVICE SYST, V15, P237, DOI 10.1049/cds2.12018
   Markovic D, 2020, NAT REV PHYS, V2, P499, DOI 10.1038/s42254-020-0208-2
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Querlioz D, 2013, IEEE T NANOTECHNOL, V12, P288, DOI 10.1109/TNANO.2013.2250995
   Strong SP, 1998, PHYS REV LETT, V80, P197, DOI 10.1103/PhysRevLett.80.197
   Zhang DM, 2016, IEEE T BIOMED CIRC S, V10, P828, DOI 10.1109/TBCAS.2016.2533798
   Zhang Y, 2015, IEEE T ELECTRON DEV, V62, P2048, DOI 10.1109/TED.2015.2414721
NR 15
TC 2
Z9 2
U1 2
U2 6
PY 2022
BP 106
EP 111
WC Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Gupta, STP
   Linares-Serrano, P
   Sen Bhattacharya, B
   Serrano-Gotarredona, T
AF Gupta, Shriya T. P.
   Linares-Serrano, Pablo
   Sen Bhattacharya, Basabdatta
   Serrano-Gotarredona, Teresa
GP IEEE
TI Foveal-pit inspired filtering of DVS spike response
SO 2021 55TH ANNUAL CONFERENCE ON INFORMATION SCIENCES AND SYSTEMS (CISS)
DT Proceedings Paper
CT 55th Annual Conference on Information Sciences and Systems (CISS)
CY MAR 24-26, 2021
CL ELECTR NETWORK
DE dynamic vision sensor; neural filtering; spiking neural network;
   classification; difference of gaussian; convolution; foveal-pit
AB In this paper, we present results of processing Dynamic Vision Sensor (DVS) recordings of visual patterns with a retinal model based on foveal-pit inspired Difference of Gaussian (DoG) filters. A DVS sensor was stimulated with varying number of vertical white and black bars of different spatial frequencies moving horizontally at a constant velocity. The output spikes generated by the DVS sensor were applied as input to a set of DoG filters inspired by the receptive field structure of the primate visual pathway. In particular, these filters mimic the receptive fields of the midget and parasol ganglion cells (spiking neurons of the retina) that sub-serve the photo-receptors of the fovealpit. The features extracted with the foveal-pit model are used for further classification using a spiking convolutional neural network trained with a backpropagation variant adapted for spiking neural networks.
C1 [Gupta, Shriya T. P.] Microsoft Corp Pvt Ltd, Microsoft Res & Dev Ctr, Bangalore, Karnataka, India.
   [Linares-Serrano, Pablo; Serrano-Gotarredona, Teresa] CSIC, CNM, IMSE, Inst Microelect Sevilla, Seville, Spain.
   [Linares-Serrano, Pablo; Serrano-Gotarredona, Teresa] Univ Seville, Seville, Spain.
   [Sen Bhattacharya, Basabdatta] BITS Pilani Goa Campus, Dept Comp Sci, Pilani, Goa, India.
RP Gupta, STP (corresponding author), Microsoft Corp Pvt Ltd, Microsoft Res & Dev Ctr, Bangalore, Karnataka, India.
EM shriyatp99@gmail.com; pablolinareserrano@gmail.com;
   basabdattab@goa.bits-pilani.ac.in; terese@imse-cnm.csic.es
CR Bekolay T, 2014, FRONT NEUROINFORM, V7, DOI 10.3389/fninf.2013.00048
   Bengio Y., 2007, LARGE SCALE KERNEL M, V34, P1
   Camuñas-Mesa L, 2012, IEEE J SOLID-ST CIRC, V47, P504, DOI 10.1109/JSSC.2011.2167409
   Diehl PU, 2015, IEEE IJCNN
   Gerstner W., 2002, SPIKING NEURON MODEL
   Gupta S, 2020, SYMP VLSI CIRCUITS, DOI 10.1109/vlsicircuits18222.2020.9162772
   Hunsberger E., 2016, ARXIV PREPRINT ARXIV
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y, 2015, NATURE, V521, p7553 436 444, DOI [10.1038/nature14539, DOI 10.1038/NATURE14539]
   Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012
   Mostafa Hesham, 2018, IEEE Trans Neural Netw Learn Syst, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Neftci EO, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00324
   Orchard G, 2015, FRONT NEUROSCI-SWITZ, V9, DOI [10.3389/fnins.2015.00437, 10.3389/fhins.2015.00437]
   Sen Bhattacharya B, 2010, IEEE T NEURAL NETWOR, V21, P1087, DOI 10.1109/TNN.2010.2048339
   Sen-Bhattacharya B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00454
   Serrano-Gotarredona T, 2013, IEEE J SOLID-ST CIRC, V48, P827, DOI 10.1109/JSSC.2012.2230553
   Stromatias E, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00350
   Van Rullen R, 2001, NEURAL COMPUT, V13, P1255, DOI 10.1162/08997660152002852
   Virtanen P, 2020, NAT METHODS, V17, P261, DOI 10.1038/s41592-019-0686-2
   Young A. R., 2019, IEEE ACCESS, V7
NR 21
TC 0
Z9 0
U1 0
U2 0
PY 2021
DI 10.1109/CISS50987.2021.9400245
WC Computer Science, Information Systems; Telecommunications
DA 2023-11-11
ER

PT J
AU Pyle, R
   Rosenbaum, R
AF Pyle, Ryan
   Rosenbaum, Robert
TI Spatiotemporal Dynamics and Reliable Computations in Recurrent Spiking
   Neural Networks
SO PHYSICAL REVIEW LETTERS
DT Article
ID CORTICAL CIRCUITS; MODEL; SYSTEMS; NEURONS; CORTEX; CONNECTIVITY;
   MOVEMENTS; PATTERNS; MEMORY; WAVES
AB Randomly connected networks of excitatory and inhibitory spiking neurons provide a parsimonious model of neural variability, but are notoriously unreliable for performing computations. We show that this difficulty is overcome by incorporating the well-documented dependence of connection probability on distance. Spatially extended spiking networks exhibit symmetry-breaking bifurcations and generate spatiotemporal patterns that can be trained to perform dynamical computations under a reservoir computing framework.
C1 [Pyle, Ryan; Rosenbaum, Robert] Univ Notre Dame, Dept Appl & Computat Math & Stat, Notre Dame, IN 46556 USA.
   [Rosenbaum, Robert] Univ Notre Dame, Interdisciplinary Ctr Network Sci & Applicat, Notre Dame, IN 46556 USA.
RP Pyle, R (corresponding author), Univ Notre Dame, Dept Appl & Computat Math & Stat, Notre Dame, IN 46556 USA.
CR Abbott LF, 2016, NAT NEUROSCI, V19, P350, DOI 10.1038/nn.4241
   Amit DJ, 1997, CEREB CORTEX, V7, P237, DOI 10.1093/cercor/7.3.237
   [Anonymous], 2010, DYNAMICAL SYSTEMS NE
   [Anonymous], 1989, METHODS SOLUTION APP
   Bressloff PC, 2012, J PHYS A-MATH THEOR, V45, DOI 10.1088/1751-8113/45/3/033001
   Brunel N, 2000, J COMPUT NEUROSCI, V8, P183, DOI 10.1023/A:1008925309027
   Churchland MM, 2012, NATURE, V487, P51, DOI 10.1038/nature11129
   Coombes S, 2005, BIOL CYBERN, V93, P91, DOI 10.1007/s00422-005-0574-y
   Ercsey-Ravasz M, 2013, NEURON, V80, P184, DOI 10.1016/j.neuron.2013.07.036
   Ermentrout B, 1998, REP PROG PHYS, V61, P353, DOI 10.1088/0034-4885/61/4/002
   Gerstner W., 2002, SPIKING NEURON MODEL
   Hennequin G, 2014, NEURON, V82, P1394, DOI 10.1016/j.neuron.2014.04.045
   Kapitula T, 2004, DISCRETE CONT DYN-A, V10, P857, DOI 10.3934/dcds.2004.10.857
   Keane A, 2015, J NEUROSCI, V35, P1591, DOI 10.1523/JNEUROSCI.1669-14.2015
   Knoblauch K., 2014, MICRO MESO MACROCONN, P45
   Lajoie G, 2013, PHYS REV E, V87, DOI 10.1103/PhysRevE.87.052901
   Ledoux E, 2011, FRONT COMPUT NEUROSC, V5, DOI 10.3389/fncom.2011.00025
   Levy RB, 2012, J NEUROSCI, V32, P5609, DOI 10.1523/JNEUROSCI.5158-11.2012
   Lim S, 2013, NAT NEUROSCI, V16, P1306, DOI 10.1038/nn.3492
   Lindner B, 2004, PHYS REP, V392, P321, DOI 10.1016/j.physrep.2003.10.015
   Lund JS, 2003, CEREB CORTEX, V13, P15, DOI 10.1093/cercor/13.1.15
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Monteforte M, 2012, PHYS REV X, V2, DOI 10.1103/PhysRevX.2.041007
   Ostojic S, 2014, NAT NEUROSCI, V17, P594, DOI 10.1038/nn.3658
   Pyle R, 2016, PHYS REV E, V93, DOI 10.1103/PhysRevE.93.040302
   Renart A, 2010, SCIENCE, V327, P587, DOI 10.1126/science.1179850
   Ricard MR, 2009, J NONLINEAR SCI, V19, P467, DOI 10.1007/s00332-009-9041-6
   Richardson MJE, 2007, PHYS REV E, V76, DOI 10.1103/PhysRevE.76.021919
   Richardson MJE, 2009, PHYS REV E, V80, DOI 10.1103/PhysRevE.80.021928
   Rosenbaum R, 2016, FRONT COMPUT NEUROSC, V10, DOI 10.3389/fncom.2016.00039
   Rosenbaum R, 2014, PHYS REV X, V4, DOI 10.1103/PhysRevX.4.021039
   Roxin A, 2005, PHYS REV LETT, V94, DOI 10.1103/PhysRevLett.94.238103
   Sadeh S, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0127547
   Sadeh S, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0114237
   Shadlen Michael N., 1994, Current Opinion in Neurobiology, V4, P569, DOI 10.1016/0959-4388(94)90059-0
   Shenoy KV, 2013, ANNU REV NEUROSCI, V36, P337, DOI 10.1146/annurev-neuro-062111-150509
   SOFTKY WR, 1993, J NEUROSCI, V13, P334
   SOMPOLINSKY H, 1988, PHYS REV LETT, V61, P259, DOI 10.1103/PhysRevLett.61.259
   Sussillo D, 2009, NEURON, V63, P544, DOI 10.1016/j.neuron.2009.07.018
   van Vreeswijk C, 1998, NEURAL COMPUT, V10, P1321, DOI 10.1162/089976698300017214
   vanVreeswijk C, 1996, SCIENCE, V274, P1724, DOI 10.1126/science.274.5293.1724
   Vogels TP, 2011, SCIENCE, V334, P1569, DOI 10.1126/science.1211095
NR 42
TC 31
Z9 31
U1 1
U2 34
PD JAN 6
PY 2017
VL 118
IS 1
AR 018103
DI 10.1103/PhysRevLett.118.018103
WC Physics, Multidisciplinary
DA 2023-11-11
ER

PT J
AU BERGSTRO.RM
AF BERGSTRO.RM
TI UNIT SPIKE ACTIVITY IN COELENTERAN NEURAL NETWORK
SO NATURWISSENSCHAFTEN
DT Note
CR HORRIDGE GA, 1954, J EXP BIOL, V31, P594
   Lentz TL., 1968, PRIMITIVE NERVOUS SY
   ROBSON EA, 1969, J EXP BIOL, V50, P151
NR 3
TC 3
Z9 3
U1 0
U2 1
PY 1971
VL 58
IS 3
BP 153
EP &
DI 10.1007/BF00593116
WC Multidisciplinary Sciences
DA 2023-11-11
ER

PT J
AU Neftci, EO
   Mostafa, H
   Zenke, F
AF Neftci, Emre O.
   Mostafa, Hesham
   Zenke, Friedemann
TI Surrogate Gradient Learning in Spiking Neural Networks: Bringing the
   Power of Gradient-based optimization to spiking neural networks
SO IEEE SIGNAL PROCESSING MAGAZINE
DT Article
DE Neural networks; Fault tolerance; Energy efficiency; Biological system
   modeling
ID ALGORITHM
AB Spiking neural networks (SNNs) are nature's versatile solution to fault-tolerant, energy-efficient signal processing. To translate these benefits into hardware, a growing number of neuromorphic spiking NN processors have attempted to emulate biological NNs. These developments have created an imminent need for methods and tools that enable such systems to solve real-world signal processing problems. Like conventional NNs, SNNs can be trained on real, domain-specific data; however, their training requires the overcoming of a number of challenges linked to their binary and dynamical nature. This article elucidates step-by-step the problems typically encountered when training SNNs and guides the reader through the key concepts of synaptic plasticity and data-driven learning in the spiking setting. Accordingly, it gives an overview of existing approaches and provides an introduction to surrogate gradient (SG) methods, specifically, as a particularly flexible and efficient method to overcome the aforementioned challenges.
C1 [Neftci, Emre O.] Univ Calif Irvine, Dept Cognit Sci & Comp Sci, Irvine, CA 92697 USA.
   [Mostafa, Hesham] Intels Artificial Intelligence Prod Grp, Off CTO, Santa Clara, CA USA.
   [Zenke, Friedemann] Friedrich Miescher Inst Biomed Res, Basel, Switzerland.
RP Neftci, EO (corresponding author), Univ Calif Irvine, Dept Cognit Sci & Comp Sci, Irvine, CA 92697 USA.
EM eneftci@uci.edu; hesham.mostafa@intel.com; friedemann.zenke@fmi.ch
CR Abbott LF, 2016, NAT NEUROSCI, V19, P350, DOI 10.1038/nn.4241
   ACKLEY DH, 1985, COGNITIVE SCI, V9, P147
   [Anonymous], 2016, BINARIZED NEURAL NET
   [Anonymous], 2016, DEEP LEARNING
   Anwani N., 2015, P INT JOINT C NEUR N, P1, DOI DOI 10.1109/IJCNN.2015
   Baldi P, 2016, NEURAL NETWORKS, V83, P51, DOI 10.1016/j.neunet.2016.07.006
   Bellec G., 2018, ADV NEURAL INFORM PR, P795
   Bellec G., 2019, BIOL INSPIRED ALTERN
   Bengio Yoshua, 2013, ABS13083432 CORR
   Boahen K, 2017, COMPUT SCI ENG, V19, P14, DOI 10.1109/MCSE.2017.33
   Bohte SM, 2011, LECT NOTES COMPUT SC, V6791, P60, DOI 10.1007/978-3-642-21735-7_8
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Brea J, 2013, J NEUROSCI, V33, P9565, DOI 10.1523/JNEUROSCI.4098-12.2013
   Esser SK, 2016, P NATL ACAD SCI USA, V113, P11441, DOI 10.1073/pnas.1604850113
   Gardner B, 2015, NEURAL COMPUT, V27, P2548, DOI 10.1162/NECO_a_00790
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   Gilra A, 2017, ELIFE, V6, DOI 10.7554/eLife.28295
   Guerguiev J, 2017, ELIFE, V6, DOI 10.7554/eLife.22901
   Gütig R, 2014, CURR OPIN NEUROBIOL, V25, P134, DOI 10.1016/j.conb.2014.01.004
   Huh Dongsung, 2018, ADV NEURAL INFORM PR, P1440
   Hunsberger E., 2015, SPIKING DEEP NETWORK
   Kaiser J., 2018, SYNAPTIC PLASTICITY
   Lee JM, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00191
   Lillicrap TP, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms13276
   Memmesheimer RM, 2014, NEURON, V82, P925, DOI 10.1016/j.neuron.2014.03.026
   Mostafa H, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00608
   Mostafa Hesham, 2018, IEEE Trans Neural Netw Learn Syst, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Mostafa H, 2018, NEURAL COMPUT, V30, P1542, DOI 10.1162/neco_a_01080
   Neftci EO, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00324
   Nicola W, 2017, NAT COMMUN, V8, DOI 10.1038/s41467-017-01827-3
   Nokland A., 2016, ADV NEURAL INFORM PR
   OConnor P., 2017, TEMPORALLY EFFICIENT
   Pfister JP, 2006, NEURAL COMPUT, V18, P1318, DOI 10.1162/neco.2006.18.6.1318
   Rezende DJ, 2014, FRONT COMPUT NEUROSC, V8, DOI 10.3389/fncom.2014.00038
   Shrestha S. B., 2018, ADV NEURAL INFORM PR
   Shrestha S. B., 2018, P 32 INT C NEUR INF, P1419, DOI [10.5555/3326943.3327073, DOI 10.5555/3326943.3327073]
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Williams RJ, 1989, NEURAL COMPUT, V1, P270, DOI 10.1162/neco.1989.1.2.270
   Wozniak S., 2018, DEEP NETWORKS INCORP
   Zenke F, 2018, NEURAL COMPUT, V30, P1514, DOI 10.1162/neco_a_01086
NR 40
TC 375
Z9 381
U1 9
U2 64
PD NOV
PY 2019
VL 36
IS 6
BP 51
EP 63
DI 10.1109/MSP.2019.2931595
WC Engineering, Electrical & Electronic
HC Y
HP N
DA 2023-11-11
ER

PT C
AU Hu, ZH
   Wang, T
   Hu, XL
AF Hu, Zhanhao
   Wang, Tao
   Hu, Xiaolin
BE Liu, D
   Xie, S
   Li, Y
   Zhao, D
   ElAlfy, ESM
TI An STDP-Based Supervised Learning Algorithm for Spiking Neural Networks
SO NEURAL INFORMATION PROCESSING (ICONIP 2017), PT II
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 24th International Conference on Neural Information Processing (ICONIP)
CY NOV 14-18, 2017
CL Guangzhou, PEOPLES R CHINA
DE STDP; SNN; Supervised learning
AB Compared with rate-based artificial neural networks, Spiking Neural Networks (SNN) provide a more biological plausible model for the brain. But how they perform supervised learning remains elusive. Inspired by recent works of Bengio et al., we propose a supervised learning algorithm based on Spike-Timing Dependent Plasticity (STDP) for a hierarchical SNN consisting of Leaky Integrate-and-fire (LIF) neurons. A time window is designed for the presynaptic neuron and only the spikes in this window take part in the STDP updating process. The model is trained on the MNIST dataset. The classification accuracy approach that of a Multilayer Perceptron (MLP) with similar architecture trained by the standard back-propagation algorithm.
C1 [Hu, Zhanhao] Tsinghua Univ, Dept Phys, Beijing, Peoples R China.
   [Wang, Tao] Huawei Technol, Beijing, Peoples R China.
   [Hu, Xiaolin] Tsinghua Univ, Ctr Brain Inspired Comp Res CBICR, Tsinghua Natl Lab Informat Sci & Technol TNList, Dept Comp Sci & Technol, Beijing, Peoples R China.
RP Hu, XL (corresponding author), Tsinghua Univ, Ctr Brain Inspired Comp Res CBICR, Tsinghua Natl Lab Informat Sci & Technol TNList, Dept Comp Sci & Technol, Beijing, Peoples R China.
EM xlhu@tsinghua.edu.cn
CR [Anonymous], 2016, ARXIV161101421
   [Anonymous], 2015, ARXIV150905936
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Booij O, 2005, INFORM PROCESS LETT, V95, P552, DOI 10.1016/j.ipl.2005.05.023
   Clopath C, 2010, NAT NEUROSCI, V13, P344, DOI 10.1038/nn.2479
   Dayan P., 2001, THEORETICAL NEUROSCI
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Querlioz D, 2013, IEEE T NANOTECHNOL, V12, P288, DOI 10.1109/TNANO.2013.2250995
   Scellier B, 2017, FRONT COMPUT NEUROSC, V11, DOI 10.3389/fncom.2017.00024
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Xie XH, 2000, ADV NEUR IN, V12, P199
   Xie XR, 2017, IEEE T NEUR NET LEAR, V28, P1411, DOI 10.1109/TNNLS.2016.2541339
NR 17
TC 5
Z9 5
U1 1
U2 3
PY 2017
VL 10635
BP 92
EP 100
DI 10.1007/978-3-319-70096-0_10
PN II
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods
DA 2023-11-11
ER

PT C
AU Zhang, Z
   Wu, QX
   Wang, X
   Sun, QY
AF Zhang, Zhenmin
   Wu, Qingxiang
   Wang, Xuan
   Sun, Qiyan
BE Xiao, Z
   Tong, Z
   Li, K
   Wang, X
   Li, K
TI Training Spiking Neural Networks With the Improved Grey-Level
   Co-occurrence Matrix Algorithm for Texture Analysis
SO 2015 11TH INTERNATIONAL CONFERENCE ON NATURAL COMPUTATION (ICNC)
DT Proceedings Paper
CT 11th International Conference on Natural Computation (ICNC) / 12th
   International Conference on Fuzzy Systems and Knowledge Discovery (FSKD)
CY AUG 15-17, 2015
CL Zhangjiajie, PEOPLES R CHINA
DE spiking neural network; GLCM algorithm; Feature Extraction; Texture
   Classification
AB Texture refers to the tactile impression, such as rough, silky, bumpy, and other texture terms. The Grey-Level Co-occurrence Matrix (GLCM) algorithm is widely used in visual images for texture feature extraction, image structure characterization analysis and texture classification. The GLCM can not only give the statistics of pixel gray values occur in an image, but also give multiple characteristics of the images. Since the primate brain, which is constructed with spiking neurons, has excellent performance in terms of image feature extraction, the improved GLCM algorithm is used to train a spiking neural network and also to simulate the brain's ability about extract key information and utilize these extracted feature information to classify different texture image. Experimental results in this article show that this combination of the GLCM and spiking neural network can effectively extract image features, and the texture classification results is also to achieve satisfactory effect.
C1 [Zhang, Zhenmin; Wu, Qingxiang; Wang, Xuan; Sun, Qiyan] Fujian Normal Univ, Coll Photon & Elect Engn, Fuzhou, Peoples R China.
RP Wu, QX (corresponding author), Fujian Normal Univ, Coll Photon & Elect Engn, Fuzhou, Peoples R China.
CR [Anonymous], SYSTEMS MAN CYBERNET
   Beck MW, 2014, ECOL INDIC, V45, P195, DOI 10.1016/j.ecolind.2014.04.002
   Benco M., 2014, INT J ADV ROBOTIC SY, V11
   Benco M, 2007, RADIOENGINEERING, V16, P64
   Bergounioux M, 2014, ADV IMAG ELECT PHYS, V181, P35, DOI 10.1016/B978-0-12-800091-5.00002-1
   Brodatz P., 1966, TEXTURE PHOTOGRAPHIC
   Brownstone RM, 2015, NEURON, V86, P9, DOI 10.1016/j.neuron.2015.03.056
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Kosik KS, 2013, NATURE, V503, P31, DOI 10.1038/503031a
   Kuebler E.S., 2013, IEEE IJCNN
   Mallat B. S., 2007, MALLAT WAVELET TOUR, V31, P85
   Masland RH, 2001, NAT NEUROSCI, V4, P877, DOI 10.1038/nn0901-877
   Mirzapour F, 2013, IRAN CONF ELECTR ENG
   Nazemi A, 2015, NEUROCOMPUTING, V152, P369, DOI 10.1016/j.neucom.2014.10.054
   Ojala T., 2002, OUTEX NEW FRAMEWORK, V1, P706
   Sadtler PT, 2014, NATURE, V512, P423, DOI 10.1038/nature13665
   Saroja GAS, 2013, 2013 IEEE CONFERENCE ON INFORMATION AND COMMUNICATION TECHNOLOGIES (ICT 2013), P1319
   Schultz SK, 2001, AM J PSYCHIAT, V158, P662, DOI 10.1176/appi.ajp.158.4.662
   Tamura H., 1978, TEXTURAL FEATURES CO, V8, P473
   Van de Wouwer G, 1999, IEEE T IMAGE PROCESS, V8, P592, DOI 10.1109/83.753747
   Wu Q.X., 2006, KNOWLEDGE REPRESENTA, V4, P2801
   Wu QX, 2013, NEUROCOMPUTING, V116, P3, DOI 10.1016/j.neucom.2012.01.046
   Wu QX, 2009, LECT NOTES ARTIF INT, V5755, P21
   Xue MS, 2014, NATURE, V511, P596, DOI 10.1038/nature13321
   Zhang J, 2008, HPCC 2008: 10TH IEEE INTERNATIONAL CONFERENCE ON HIGH PERFORMANCE COMPUTING AND COMMUNICATIONS, PROCEEDINGS, P782, DOI 10.1109/HPCC.2008.55
NR 25
TC 0
Z9 0
U1 1
U2 1
PY 2015
BP 825
EP 830
WC Computer Science, Theory & Methods
DA 2023-11-11
ER

PT C
AU Valova, I
   Gueorguieva, N
   Georgiev, G
AF Valova, I
   Gueorguieva, N
   Georgiev, G
GP IEEE
TI Modeling weakly connected networks of neural oscillators with spiking
   neurons
SO INTERNATIONAL CONFERENCE ON SYSTEMS, MAN AND CYBERNETICS, VOL 1-4,
   PROCEEDINGS
SE IEEE International Conference on Systems Man and Cybernetics Conference
   Proceedings
DT Proceedings Paper
CT IEEE International Conference on Systems, Man and Cybernetics
CY OCT 10-12, 2005
CL Waikoloa, HI
DE spiking neurons; neural oscillations; biologically inspired neuron
   models
ID OLFACTORY-BULB; DENDRITES
AB The goal of this research is to investigate the relationships between synaptic organizations (anatomy) of the neural networks and the dynamical properties (function) of weakly connected networks of neural oscillators. It is shown how certain parameters of the spiking neuron model can be used to represent these dynamics. The two proposed modesl are based on the two main cell types in the olfactory bulb, the mitral and granule cells. The dynamics that have been simulated include the reciprocal and lateral inhibition of mitral cells by granule cells, as well as the saturation of mitral cells. The simulations show how certain spike inputs to mitral cells correspond to cortex recognition and discrimination in the olfactory bulb.
C1 Univ Massachusetts Dartmouth, N Dartmouth, MA 02747 USA.
RP Valova, I (corresponding author), Univ Massachusetts Dartmouth, 285 Old Westport Rd, N Dartmouth, MA 02747 USA.
EM ivalova@umassd.edu; natachag@csi.cuny.edu; georgiev@uwosh.edu
CR Abbott LF, 2000, NAT NEUROSCI, V3, P1178, DOI 10.1038/81453
   Christodoulou C, 2002, NEURAL NETWORKS, V15, P891, DOI 10.1016/S0893-6080(02)00034-5
   Dayan P., 2001, THEORETICAL NEUROSCI
   GERSTNER W, POPULATION DYNAMICS, V12, P43
   Gerstner W., 2002, SPIKING NEURON MODEL
   Horn D, 2000, ADV NEUR IN, V12, P129
   Kay LM, 1999, NAT NEUROSCI, V2, P1003, DOI 10.1038/14801
   Kempter R, 1999, PHYS REV E, V59, P4498, DOI 10.1103/PhysRevE.59.4498
   Margrie TW, 2001, P NATL ACAD SCI USA, V98, P319, DOI 10.1073/pnas.011523098
   MORI K, 1987, PROG NEUROBIOL, V29, P275, DOI 10.1016/0301-0082(87)90024-4
   Rieke F., 1999, SPIKES EXPLORING NEU
   RUF B, 1998, THESIS TECHNICAL U G
   Schoppa NE, 1999, NAT NEUROSCI, V2, P1106, DOI 10.1038/16033
   Shen GYY, 1999, J NEUROPHYSIOL, V82, P3006, DOI 10.1152/jn.1999.82.6.3006
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   VALOVA I, 2004, P ART NEUR NETW ENG, P3
NR 16
TC 0
Z9 0
U1 0
U2 1
PY 2005
BP 810
EP 815
WC Computer Science, Artificial Intelligence; Computer Science, Cybernetics
DA 2023-11-11
ER

PT C
AU Nguyen, VT
   Trinh, QK
   Zhang, RY
   Nakashima, Y
AF Nguyen, Van-Tinh
   Quang-Kien Trinh
   Zhang, Renyuan
   Nakashima, Yasuhiko
GP IEEE
TI XNOR-BSNN: In-Memory Computing Model for Deep Binarized Spiking Neural
   Network
SO 2021 INTERNATIONAL CONFERENCE ON HIGH PERFORMANCE BIG DATA AND
   INTELLIGENT SYSTEMS (HPBD&IS)
DT Proceedings Paper
CT International Conference on High Performance Big Data and Intelligent
   Systems (HPBD and IS)
CY DEC 05-07, 2021
CL Macau, PEOPLES R CHINA
DE In-memory computing; Binary Spiking Neural Network; residual connection
AB This paper proposes a residual binarized spiking neural network (B-SNN) model suited for inmemory computing (IMC) implementation. While in most of the prior arts, due to the nature of spike represented unipolar format, the B-SNN were implemented using either complex or non-regular logic that is not suited for IMC and/or makes the network inflexible. In this work, we present a B-SNN model that permits the direct adoption of a unipolar format spike on the XNOR array, i.e., allows fully exploiting IMC's potential benefit based on the highly regular and simple array structure. Also, instead of indirectly taking the B-SNN model from the trained BNN, we propose a residual model for deep B-SNN networks. The system simulation shows that our trained network achieves reasonably good accuracy (59.11%) on CIFAR100 with very low inference latency (only 8 time-steps BSNN).
C1 [Nguyen, Van-Tinh; Zhang, Renyuan; Nakashima, Yasuhiko] NARA Inst Sci & Technol, Sch Informat Sci, Ikoma, Japan.
   [Quang-Kien Trinh] Le Quy Don Tech Univ, Dept Microelect & Microproc, Hanoi, Vietnam.
RP Nguyen, VT (corresponding author), NARA Inst Sci & Technol, Sch Informat Sci, Ikoma, Japan.
EM nguyen.van_tinh.np3@is.naist.jp; kien.trinh@lqdtu.edu.vn;
   rzhang@is.naist.jp; nakashim@is.naist.jp
CR Abu Lebdeh M, 2017, IEEE T CIRCUITS-I, V64, P2427, DOI 10.1109/TCSI.2017.2706299
   Chuang PY, 2020, DES AUT CON, DOI 10.1109/dac18072.2020.9218714
   Deng L., 2019, ARXIV PREPRINT ARXIV
   Esser SK, 2016, P NATL ACAD SCI USA, V113, P11441, DOI 10.1073/pnas.1604850113
   Guo WZ, 2021, FRONT NEUROSCI-SWITZ, V15, DOI [10.3389/fnins.2021.638474, 10.1007/s11704-020-9230-x]
   Hubara I, 2018, J MACH LEARN RES, V18
   Kheradpisheh S., 2020, ARXIV PREPRINT ARXIV
   Kim Y, 2020, ARXIV PREPRINT ARXIV
   Krizhevsky A., 2009, LEARNING MULTIPLE LA, DOI DOI 10.1145/3233231
   Lu S, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00535
   Pham T., 2021, 2021 IEEE INT S CIRC
   Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32
   Sebastian A, 2020, NAT NANOTECHNOL, V15, P529, DOI 10.1038/s41565-020-0655-z
   She XY, 2019, IEEE IJCNN
   Srinivasan G, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00189
   Tinh N. V., 2021, IEEE ACCESS, V9
   Wang YX, 2021, IEEE T COGN DEV SYST, V13, P514, DOI 10.1109/TCDS.2020.2971655
   Wu YJ, 2019, AAAI CONF ARTIF INTE, P1311
NR 18
TC 0
Z9 0
U1 3
U2 4
PY 2021
BP 17
EP 21
DI 10.1109/HPBDIS53214.2021.9658467
WC Computer Science, Artificial Intelligence; Computer Science,
   Cybernetics; Computer Science, Theory & Methods
DA 2023-11-11
ER

PT J
AU Guo, YF
   Huang, XH
   Ma, Z
AF Guo, Yufei
   Huang, Xuhui
   Ma, Zhe
TI Direct learning-based deep spiking neural networks: a review
SO FRONTIERS IN NEUROSCIENCE
DT Review
DE spiking neural network; brain-inspired computation; direct learning;
   deep neural network; energy efficiency; spatial-temporal processing
ID GRADIENT DESCENT; ALGORITHM
AB The spiking neural network (SNN), as a promising brain-inspired computational model with binary spike information transmission mechanism, rich spatially-temporal dynamics, and event-driven characteristics, has received extensive attention. However, its intricately discontinuous spike mechanism brings difficulty to the optimization of the deep SNN. Since the surrogate gradient method can greatly mitigate the optimization difficulty and shows great potential in directly training deep SNNs, a variety of direct learning-based deep SNN works have been proposed and achieved satisfying progress in recent years. In this paper, we present a comprehensive survey of these direct learning-based deep SNN works, mainly categorized into accuracy improvement methods, efficiency improvement methods, and temporal dynamics utilization methods. In addition, we also divide these categorizations into finer granularities further to better organize and introduce them. Finally, the challenges and trends that may be faced in future research are prospected.
C1 [Guo, Yufei; Huang, Xuhui; Ma, Zhe] Intelligent Sci & Technol Acad CASIC, Beijing, Peoples R China.
   [Guo, Yufei; Huang, Xuhui; Ma, Zhe] Sci Res Lab Aerosp Intelligent Syst & Technol, Beijing, Peoples R China.
RP Ma, Z (corresponding author), Intelligent Sci & Technol Acad CASIC, Beijing, Peoples R China.; Ma, Z (corresponding author), Sci Res Lab Aerosp Intelligent Syst & Technol, Beijing, Peoples R China.
EM mazhe_thu@163.com
CR Barchid S, 2023, Arxiv, DOI arXiv:2304.10211
   Bellec G, 2018, ADV NEUR IN, V31
   Bing Han, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P388, DOI 10.1007/978-3-030-58607-2_23
   Biswas S, 2022, Arxiv, DOI arXiv:2211.10754
   Bittar A, 2022, Arxiv, DOI arXiv:2212.01187
   Bittar A, 2022, FRONT NEUROSCI-SWITZ, V16, DOI 10.3389/fnins.2022.865897
   Bohte SM, 2011, LECT NOTES COMPUT SC, V6791, P60, DOI 10.1007/978-3-642-21735-7_8
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Booij O, 2005, INFORM PROCESS LETT, V95, P552, DOI 10.1016/j.ipl.2005.05.023
   Bu T, 2023, Arxiv, DOI arXiv:2303.04347
   Bu T, 2022, AAAI CONF ARTIF INTE, P11
   Chen Y., 2021, ARXIV PREPRINT ARXIV
   Chen Y, 2022, INT CONF ACOUST SPEE, P8927, DOI 10.1109/ICASSP43922.2022.9746774
   Cheng X, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1519
   Chowdhury SS, 2021, IEEE IJCNN, DOI 10.1109/IJCNN52387.2021.9534111
   Chowdhury SS, 2022, LECT NOTES COMPUT SC, V13671, P709, DOI 10.1007/978-3-031-20083-0_42
   Cordone L., 2022, ARXIV
   Deng S., 2022, ARXIV
   Ding Jianchuan, 2022, ADV NEURAL INFORM PR
   Duan Chaoteng, 2022, ADV NEURAL INFORM PR
   Dupeyroux J, 2021, IEEE INT CONF ROBOT, P96, DOI 10.1109/ICRA48506.2021.9560937
   Fang W., 2021, ADV NEURAL INFORM PR, V34, P21056, DOI DOI 10.48550/ARXIV.2102.04159
   Fang W, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2641, DOI 10.1109/ICCV48922.2021.00266
   Feng L., 2022, P INT JOINT C ART IN, P2471
   Gao Y, 2023, IEEE T PATTERN ANAL, V45, P7764, DOI 10.1109/TPAMI.2022.3224051
   Guo Y., 2022, ADV NEURAL INFORM PR
   Guo YF, 2023, PATTERN RECOGN, V142, DOI 10.1016/j.patcog.2023.109639
   Guo YF, 2022, LECT NOTES COMPUT SC, V13671, P36, DOI 10.1007/978-3-031-20083-0_3
   Guo Y, 2022, LECT NOTES COMPUT SC, V13672, P52, DOI 10.1007/978-3-031-19775-8_4
   Guo YF, 2022, PROC CVPR IEEE, P326, DOI 10.1109/CVPR52688.2022.00042
   Hagenaars J., 2021, ADV NEURAL INF PROCE, V34, P7167
   Han B, 2023, Arxiv, DOI arXiv:2211.12219
   Han CS, 2022, 37TH ANNUAL ACM SYMPOSIUM ON APPLIED COMPUTING, P1048, DOI 10.1145/3477314.3507085
   Hao YZ, 2020, NEURAL NETWORKS, V121, P387, DOI 10.1016/j.neunet.2019.09.007
   Hong CF, 2020, IEEE T NEUR NET LEAR, V31, P1285, DOI 10.1109/TNNLS.2019.2919662
   Hu YF, 2023, Arxiv, DOI arXiv:2112.08954
   Ikegawa S, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22082876
   Kim S, 2020, AAAI CONF ARTIF INTE, V34, P11270
   Kim Y, 2022, LECT NOTES COMPUT SC, V13684, P36, DOI 10.1007/978-3-031-20053-3_3
   Kim Y, 2022, LECT NOTES COMPUT SC, V13672, P102, DOI 10.1007/978-3-031-19775-8_7
   Kim Y, 2021, FRONT NEUROSCI-SWITZ, V15, DOI 10.3389/fnins.2021.773954
   Kosta AK, 2022, Arxiv, DOI arXiv:2209.11741
   Kushawaha RK, 2021, INT C PATT RECOG, P4536, DOI 10.1109/ICPR48806.2021.9412147
   Leng L., 2022, ADV NEURAL INFORM PR
   Li Wenshuo, 2022, P IEEECVF C COMPUTER, P783, DOI [10.1109/CVPR52688.2022.00086, DOI 10.1109/CVPR52688.2022.00086]
   Li Yan, 2021, arXiv
   Li Y., 2022, ARXIV
   Li Y., 2022, ARXIV PREPRINT ARXIV
   Li Y., 2021, P INT C MACH LEARN I, V139, P6316
   Liu FX, 2022, AAAI CONF ARTIF INTE, P1692
   Lobov SA, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00088
   Luo XL, 2022, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2022.3164930
   Luo YH, 2021, Arxiv, DOI arXiv:2003.07584
   Meng Q., 2022, P IEEE CVF C COMP VI, DOI [10.1109/CVPR52688.2022.01212, DOI 10.1109/CVPR52688.2022.01212]
   Na BYG, 2022, Arxiv, DOI arXiv:2201.12738
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Nomura O, 2022, IEEE T CIRCUITS-II, V69, P3640, DOI 10.1109/TCSII.2022.3184313
   Parameshwara CM, 2021, IEEE INT C INT ROBOT, P3414, DOI 10.1109/IROS51168.2021.9636506
   Patel K, 2021, Arxiv, DOI arXiv:2106.08921
   Pellegrini T, 2021, IEEE W SP LANG TECH, P97, DOI 10.1109/SLT48900.2021.9383587
   Ponghiran W, 2022, AAAI CONF ARTIF INTE, P8001
   Ponulak F, 2011, ACTA NEUROBIOL EXP, V71, P409
   Rancon U, 2021, Arxiv, DOI arXiv:2109.13751
   Rathi N, 2020, Arxiv, DOI arXiv:2008.03658
   Roy K, 2019, NATURE, V575, P607, DOI 10.1038/s41586-019-1677-2
   Sadovsky E, 2023, 2023 33RD INTERNATIONAL CONFERENCE RADIOELEKTRONIKA, RADIOELEKTRONIKA, DOI 10.1109/RADIOELEKTRONIKA57919.2023.10109082
   She Xueyuan, 2021, INT C LEARN REPR
   Shen G., 2023, PREPRINT, DOI [10.48550/arXiv.2301.12356, DOI 10.48550/ARXIV.2301.12356]
   Stagsted RK, 2020, ROBOTICS: SCIENCE AND SYSTEMS XVI
   Takuya S, 2021, PROC IEEE COOL CHIPS, DOI 10.1109/COOLCHIPS52128.2021.9410323
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Tavanaei A, 2017, LECT NOTES COMPUT SC, V10639, P899, DOI 10.1007/978-3-319-70136-3_95
   Tavanaei A, 2017, NEUROCOMPUTING, V240, P191, DOI 10.1016/j.neucom.2017.01.088
   Viale A, 2022, IEEE INT C INT ROBOT, P79, DOI 10.1109/IROS47612.2022.9981034
   Wang S., 2022, ADV NEURAL INFORM PR
   Wang XW, 2020, NEURAL NETWORKS, V125, P258, DOI 10.1016/j.neunet.2020.02.011
   Wang XT, 2023, Arxiv, DOI arXiv:2303.11127
   Wang Y., 2022, INT JOINT C ART INT, DOI [10.24963/ijcai.2022/347, DOI 10.24963/IJCAI.2022/347]
   Wu J, 2019, DEEP SPIKING NEURAL
   Wu JC, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351221
   Wu JB, 2022, IEEE T PATTERN ANAL, V44, P7824, DOI 10.1109/TPAMI.2021.3114196
   Wu JB, 2023, IEEE T NEUR NET LEAR, V34, P446, DOI 10.1109/TNNLS.2021.3095724
   Wu JB, 2019, INTERSPEECH, P3667
   Wu JB, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00199
   Wu JB, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00836
   Wu YJ, 2019, AAAI CONF ARTIF INTE, P1311
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Xu Q, 2023, Arxiv, DOI arXiv:2304.09500
   Xu Q, 2023, Arxiv, DOI arXiv:2304.05627
   Xu Y, 2013, NEURAL NETWORKS, V43, P99, DOI 10.1016/j.neunet.2013.02.003
   Yamazaki K, 2022, BRAIN SCI, V12, DOI 10.3390/brainsci12070863
   Yang Q., 2022, ARXIV PREPRINT ARXIV
   Yao M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10201, DOI 10.1109/ICCV48922.2021.01006
   Yao X., 2022, 36 C NEUR INF PROC S
   Yin B, 2020, IEEE INTERNET THINGS, V7, P8748, DOI [10.1109/JIOT.2020.2996562, 10.1145/3407197.3407225]
   Yin BJ, 2021, NAT MACH INTELL, V3, P905, DOI 10.1038/s42256-021-00397-w
   Yu L, 2023, IEEE T PATTERN ANAL, V45, P8660, DOI 10.1109/TPAMI.2022.3227448
   Yu Q, 2022, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2022.3165527
   Yu Q, 2022, IEEE T NEUR NET LEAR, V33, P1134, DOI 10.1109/TNNLS.2020.3040969
   Zambrano D, 2016, Arxiv, DOI arXiv:1609.02053
   Zenke F, 2018, NEURAL COMPUT, V30, P1514, DOI 10.1162/neco_a_01086
   Zhang DZ, 2022, Arxiv, DOI arXiv:2204.07050
   Zhang JQ, 2022, PROC CVPR IEEE, P8791, DOI 10.1109/CVPR52688.2022.00860
   Zhang ML, 2022, IEEE T NEUR NET LEAR, V33, P1947, DOI 10.1109/TNNLS.2021.3110991
   Zhang ML, 2020, NEUROCOMPUTING, V409, P103, DOI 10.1016/j.neucom.2020.03.079
   Zhang ML, 2020, IEEE J-STSP, V14, P592, DOI 10.1109/JSTSP.2020.2983547
   Zhang ML, 2019, AAAI CONF ARTIF INTE, P1327
   Zhang W, 2020, ADV NEURAL INFORM PR, V33, P12022, DOI DOI 10.48550/ARXIV.2002.10085
   Zheng H., 2021, P AAAI, DOI [10.1609/aaai.v35i12.17320, DOI 10.1609/AAAI.V35I12.17320]
   Zhou S., 2021, P AAAI C ART INT, DOI [10.1609/aaai.v35i12.17329, DOI 10.1609/AAAI.V35I12.17329]
   Zhou SB, 2020, IEEE ACCESS, V8, P76903, DOI 10.1109/ACCESS.2020.2990416
   Zhu L, 2022, PROC CVPR IEEE, P3584, DOI 10.1109/CVPR52688.2022.00358
   Zhu R.-J., 2022, ARXIV
   Zhu Yaoyu, 2022, 36 C NEURAL INFORM P
   Zimmer R, 2019, Arxiv, DOI arXiv:1911.10124
   Zou SH, 2023, Arxiv, DOI arXiv:2303.09681
NR 116
TC 1
Z9 1
U1 18
U2 18
PD JUN 16
PY 2023
VL 17
AR 1209795
DI 10.3389/fnins.2023.1209795
WC Neurosciences
DA 2023-11-11
ER

PT C
AU Chaturvedi, S
   Kurshid, AA
AF Chaturvedi, S.
   Kurshid, A. A.
BE Bahrami, M
TI ASIC Implementation for Improved Character Recognition and
   Classification using SNN Model
SO PROCEEDINGS OF THE 2015 INTERNATIONAL CONFERENCE ON SOFT COMPUTING AND
   SOFTWARE ENGINEERING (SCSE'15)
SE Procedia Computer Science
DT Proceedings Paper
CT International Conference on Soft Computing and Software Engineering
   (SCSE)
CY MAR 05-06, 2015
CL Univ California, Berkeley, CA
HO Univ California
DE Artificial Neural Network (ANN); Spiking Neural Network (SNN);
   Leaky-Integrate & Fire Neuron Model; Izhikev ch Model; Pattern
   Classification& Recognition; Application Specific Integrated Circuits
   (ASIC)
AB The third generation of spiking neural networks raises the level of biological realism by using individual spikes. So instead of using rate coding, these neurons use pulse coding mechanisms where neurons receive and do send out individual pulses, allowing multiplexing of information.This work depicts how Spiking neural network model is used for character recognition and classification. Here, we adapt to the technique of using ASIC for large scale simulations of the Izhikevich model and use RTL Clock gating approach for reducing the dynamic power. Here the focus is on how power consumption and system cost can be reduced for large production run. The full custom biologically plausible spiking neural network model is implemented on ASIC with 90 nm Process. The Izhikevich spiking neuron model is best suited for large scale cortical simulations due to its accuracy, efficiency, power and simulation time. The classification efficiency of SNN based on MATLAB simulations is demonstrated in this work by its ability to classify the 27 characters correctly out of 30 noisy character images presented. The ASIC realizing the English character classification and recognition dissipates power of 2.8 mW and an area of 120312 lam'. This work brings about the application of using networks of these spiking neurons for character recognition and their suitability for custom realization with reduced power consumption. (C) 2015 The Authors. Published by Elsevier B.V.
C1 [Chaturvedi, S.] GHRCE, Elect, Nagpur, MS, India.
   [Kurshid, A. A.] RCOEM, Elect, Nagpur, MS, India.
RP Chaturvedi, S (corresponding author), GHRCE, Elect, Nagpur, MS, India.
EM soni2569@gmail.com
CR Chaturvedi Soni, 2014, CIIT INT J DIGITAL I, V06
   Chaturvedi Soni, 2014 INT C WIR COMM
   Chaturvedi Soni, 2013, INT J EMERGING TREND, V1
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Gupta A., 2007, IEEE NEUR NETW C ORL
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Kulkarni Shruti R, 2013, 9 INT C NAT COMP ICN
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Meftah B., 2012, SCI, V427, P525
   Sharma Abhishek, 2013, INT J ENG TRENDS TEC, V4
   Vazquez Roberto A., 2010, 7 INT C EL ENG COMP
NR 11
TC 3
Z9 3
U1 0
U2 1
PY 2015
VL 62
BP 151
EP 158
DI 10.1016/j.procs.2015.08.428
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
DA 2023-11-11
ER

PT J
AU Pang, LL
   Liu, JX
   Harkin, J
   Martin, G
   McElholm, M
   Javed, A
   McDaid, L
AF Pang, Lili
   Liu, Junxiu
   Harkin, Jim
   Martin, George
   McElholm, Malachy
   Javed, Aqib
   McDaid, Liam
TI Case Study-Spiking Neural Network Hardware System for Structural Health
   Monitoring
SO SENSORS
DT Article
DE structural health monitoring; damage state classification; spiking
   neural networks; feature extraction; artificial neural networks
ID MACHINE; CLASSIFICATION; CAPACITY; MODEL
AB This case study provides feasibility analysis of adapting Spiking Neural Networks (SNN) based Structural Health Monitoring (SHM) system to explore low-cost solution for inspection of structural health of damaged buildings which survived after natural disaster that is, earthquakes or similar activities. Various techniques are used to detect the structural health status of a building for performance benchmarking, including different feature extraction methods and classification techniques (e.g., SNN, K-means and artificial neural network etc.). The SNN is utilized to process the sensory data generated from full-scale seven-story reinforced concrete building to verify the classification performances. Results show that the proposed SNN hardware has high classification accuracy, reliability, longevity and low hardware area overhead.
C1 [Pang, Lili] Nanjing Inst Technol, Sch Innovat & Entrepreneurship, Ind Ctr, Nanjing 211167, Peoples R China.
   [Liu, Junxiu; Harkin, Jim; Martin, George; McElholm, Malachy; Javed, Aqib; McDaid, Liam] Ulster Univ, Sch Comp Engn & Intelligent Syst, Derry BT48 7JL, North Ireland.
RP Pang, LL (corresponding author), Nanjing Inst Technol, Sch Innovat & Entrepreneurship, Ind Ctr, Nanjing 211167, Peoples R China.; Liu, JX (corresponding author), Ulster Univ, Sch Comp Engn & Intelligent Syst, Derry BT48 7JL, North Ireland.
EM panglili@njit.edu.cn; j.liu1@ulster.ac.uk; jg.harkin@ulster.ac.uk;
   gs.martin@ulster.ac.uk; m.mcelholm@ulster.ac.uk; javed-a@ulster.ac.uk;
   lj.mcdaid@ulster.ac.uk
CR Abdo MAB., 2014, STRUCTURAL HLTH MONI
   Amezquita-Sanchez JP, 2018, SCI IRAN, V25, P2913, DOI 10.24200/sci.2018.21136
   Amezquita-Sanchez JP, 2015, SCI IRAN, V22, P1931
   Azam SE, 2019, STRUCT CONTROL HLTH, V26, DOI 10.1002/stc.2288
   Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Bouzenad A, 2019, INVENTIONS-BASEL, V4, DOI 10.3390/inventions4010017
   Bull LA, 2019, MECH SYST SIGNAL PR, V134, DOI 10.1016/j.ymssp.2019.106294
   Chen T.W., 2009, P ICASSP IEEE INT C
   de Oliveira MA, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18010152
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   González AF, 2019, J PHARM PHARMACOGN R, V7, P12
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Harkin J, 2009, INT J RECONFIGURABLE, V2009, DOI 10.1155/2009/908740
   Hernandez E, 2018, EARTHQ ENG STRUCT D, V47, P2561, DOI 10.1002/eqe.3099
   Higgins I, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0180174
   Hsu TY, 2018, SENSORS-BASEL, V18, DOI 10.3390/s18051437
   Jang S, 2010, SMART STRUCT SYST, V6, P439, DOI 10.12989/sss.2010.6.5_6.439
   Javed A, 2020, IEEE INT SYMP CIRC S
   Karayannis CG, 2016, CONSTR BUILD MATER, V105, P227, DOI 10.1016/j.conbuildmat.2015.12.019
   Kasabov N, 2016, NEURAL NETWORKS, V78, P1, DOI 10.1016/j.neunet.2015.09.011
   Kasabov N, 2013, NEURAL NETWORKS, V41, P188, DOI 10.1016/j.neunet.2012.11.014
   Kasabov NK, 2017, IEEE T NEUR NET LEAR, V28, P887, DOI 10.1109/TNNLS.2016.2612890
   Lee JM, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00191
   Liu J., 2015, P IEEE INT S CIRC SY
   Liu J., 2017, LECT NOTES ARTIFICIA
   Liu JX, 2019, IEEE T NEUR NET LEAR, V30, P865, DOI 10.1109/TNNLS.2018.2854291
   Medhi M, 2019, J NONDESTRUCT EVAL, V38, DOI 10.1007/s10921-019-0601-x
   Mesquita E, 2018, ENG STRUCT, V161, P108, DOI 10.1016/j.engstruct.2018.02.013
   Moaveni B, 2011, J STRUCT ENG-ASCE, V137, P705, DOI 10.1061/(ASCE)ST.1943-541X.0000300
   Moaveni B, 2010, STRUCT SAF, V32, P347, DOI 10.1016/j.strusafe.2010.03.006
   Naeem M, 2015, IEEE T NEUR NET LEAR, V26, P2370, DOI 10.1109/TNNLS.2014.2382334
   Notley S., 2018, ARXIV180502294
   Oh BK, 2017, APPL SOFT COMPUT, V58, P576, DOI 10.1016/j.asoc.2017.05.029
   Park SW, 2015, MEASUREMENT, V59, P352, DOI 10.1016/j.measurement.2014.09.063
   Roy K, 2019, NATURE, V575, P607, DOI 10.1038/s41586-019-1677-2
   Sun YW, 2012, COMPUT BIOL MED, V42, P751, DOI 10.1016/j.compbiomed.2012.04.007
   Wang JF, 2017, J PERFORM CONSTR FAC, V31, DOI 10.1061/(ASCE)CF.1943-5509.0000952
   Wang L., 2015, LECT NOTES ARTIFICIA
   Worden K, 2007, PHILOS T R SOC A, V365, P515, DOI 10.1098/rsta.2006.1938
   Zhang Y.Z., 2019, ZIDONGHUA XUEBAO, DOI [10.16383/j.aas.c180685, DOI 10.16383/J.AAS.C180685]
NR 40
TC 10
Z9 10
U1 4
U2 14
PD SEP
PY 2020
VL 20
IS 18
AR 5126
DI 10.3390/s20185126
WC Chemistry, Analytical; Engineering, Electrical & Electronic; Instruments
   & Instrumentation
DA 2023-11-11
ER

PT J
AU Lee, IH
   Cho, UI
AF Lee, IH
   Cho, UI
TI Realization of spiking by an excitable chemical system
SO PHYSICAL CHEMISTRY CHEMICAL PHYSICS
DT Article
ID NEURAL-NETWORK MODELS; PATTERN-RECOGNITION; REACTOR NETWORKS; TEMPORAL
   CODE; NEURONS; IMPLEMENTATION; OSCILLATIONS; INFORMATION; PROPAGATION;
   BEHAVIOR
AB It is theoretically demonstrated that an excitable chemical system can function as a spiking neuron and a chemical spiking neuron network can be constructed. The Oregonator is used as a model for a chemical spiking neuron. The chemical spiking neuron modeled by the Oregonator is similar in behavior to the spiking neuron model of Maass even though the Oregonator has a different excitation mechanism from the spiking neuron model. In the spiking neuron network, information is encoded and processed by using the timing of the spikes in spiking neurons. It is shown that chemical spiking neuron networks can be constructed by the unidirectional selective coupling of the spiking chemical neurons. The chemical spiking neuron network can process information encoded temporally like the spiking neuron network with a restriction in the range of the weight of couplings between the chemical neurons.
C1 Yonsei Univ, Dept Chem, Seoul 120749, South Korea.
RP Cho, UI (corresponding author), Yonsei Univ, Dept Chem, Seoul 120749, South Korea.
CR Buonomano DV, 1999, NEURAL COMPUT, V11, P103, DOI 10.1162/089976699300016836
   Dechert G, 1996, J PHYS CHEM-US, V100, P19043, DOI 10.1021/jp9616066
   Deco G, 1997, PHYS REV LETT, V79, P4697, DOI 10.1103/PhysRevLett.79.4697
   DEKEPPER P, 1990, J PHYS CHEM-US, V94, P6525, DOI 10.1021/j100380a004
   Diesmann M, 1999, NATURE, V402, P529, DOI 10.1038/990101
   DOLNIK M, 1989, J PHYS CHEM-US, V93, P2764, DOI 10.1021/j100344a015
   DOLNIK M, 1992, J PHYS CHEM-US, V96, P3218, DOI 10.1021/j100187a009
   Fetz EE, 1997, SCIENCE, V278, P1901, DOI 10.1126/science.278.5345.1901
   FIELD RJ, 1974, J CHEM PHYS, V60, P1877, DOI 10.1063/1.1681288
   Fields R.J, 1974, FARADAY CHEM SOC, V9, P21, DOI [10.1039/fs9740900021, DOI 10.1039/FS9740900021]
   FINKEOVA J, 1990, J PHYS CHEM-US, V94, P4110, DOI 10.1021/j100373a042
   Furukawa S, 2000, J NEUROSCI, V20, P1216, DOI 10.1523/JNEUROSCI.20-03-01216.2000
   GERSTNER W, 1995, PHYS REV E, V51, P738, DOI 10.1103/PhysRevE.51.738
   HJELMFELT A, 1995, PHYSICA D, V84, P180, DOI 10.1016/0167-2789(95)00014-U
   HJELMFELT A, 1993, J PHYS CHEM-US, V97, P7988, DOI 10.1021/j100132a030
   HJELMFELT A, 1993, SCIENCE, V260, P335, DOI 10.1126/science.260.5106.335
   HJELMFELT A, 1991, P NATL ACAD SCI USA, V88, P10983, DOI 10.1073/pnas.88.24.10983
   HJELMFELT A, 1992, P NATL ACAD SCI USA, V89, P383, DOI 10.1073/pnas.89.1.383
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Hohmann W, 1999, J PHYS CHEM A, V103, P7606, DOI 10.1021/jp991480n
   Hohmann W, 1998, J PHYS CHEM A, V102, P3103, DOI 10.1021/jp980377f
   Hohmann W, 1996, J CHEM SOC FARADAY T, V92, P2873, DOI 10.1039/ft9969202873
   Hohmann W, 1997, J PHYS CHEM A, V101, P7364, DOI 10.1021/jp971939i
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   LAPLANTE JP, 1995, J PHYS CHEM-US, V99, P10063, DOI 10.1021/j100025a001
   LEBENDER D, 1994, J PHYS CHEM-US, V98, P7533, DOI 10.1021/j100082a023
   Maass W, 1997, NEURAL COMPUT, V9, P279, DOI 10.1162/neco.1997.9.2.279
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   MAINEN ZF, 1995, SCIENCE, V268, P1503, DOI 10.1126/science.7770778
   MCCLURKIN JW, 1991, SCIENCE, V253, P675, DOI 10.1126/science.1908118
   MIDDLEBROOKS JC, 1994, SCIENCE, V264, P842, DOI 10.1126/science.8171339
   OKAMOTO M, 1988, BIOL CYBERN, V58, P295, DOI 10.1007/BF00363938
   PACAULT A, 1976, ACCOUNTS CHEM RES, V9, P438, DOI 10.1021/ar50108a003
   Parodi O, 1996, BIOL CYBERN, V74, P497, DOI 10.1007/BF00209421
   Pavlasek J, 1997, BIOL CYBERN, V77, P359, DOI 10.1007/s004220050396
   Prut Y, 1998, J NEUROPHYSIOL, V79, P2857, DOI 10.1152/jn.1998.79.6.2857
   Reich DS, 2000, J NEUROSCI, V20, P1964
   Riehle A, 1997, SCIENCE, V278, P1950, DOI 10.1126/science.278.5345.1950
   RUOFF P, 1986, J CHEM PHYS, V84, P1413, DOI 10.1063/1.450484
   RUOFF P, 1982, CHEM PHYS LETT, V90, P76, DOI 10.1016/0009-2614(82)83328-9
   RUOFF P, 1983, NATURWISSENSCHAFTEN, V70, P306, DOI 10.1007/BF00404839
   Steinbock O, 1996, J PHYS CHEM-US, V100, P18970, DOI 10.1021/jp961209v
   Thorpe SJ, 1997, ADV NEUR IN, V9, P901
   Watanabe M, 1998, BIOL CYBERN, V78, P87, DOI 10.1007/s004220050416
NR 44
TC 2
Z9 2
U1 0
U2 3
PY 2001
VL 3
IS 1
BP 94
EP 98
DI 10.1039/b006784i
WC Chemistry, Physical; Physics, Atomic, Molecular & Chemical
DA 2023-11-11
ER

PT J
AU Büchel, J
   Lenz, G
   Hu, YL
   Sheik, S
   Sorbaro, M
AF Buechel, Julian
   Lenz, Gregor
   Hu, Yalun
   Sheik, Sadique
   Sorbaro, Martino
TI Adversarial attacks on spiking convolutional neural networks for
   event-based vision
SO FRONTIERS IN NEUROSCIENCE
DT Article
DE spiking convolutional neural networks; adversarial examples;
   neuromorphic engineering; robust AI; dynamic vision sensors
ID MEMORY
AB Event-based dynamic vision sensors provide very sparse output in the form of spikes, which makes them suitable for low-power applications. Convolutional spiking neural networks model such event-based data and develop their full energy-saving potential when deployed on asynchronous neuromorphic hardware. Event-based vision being a nascent field, the sensitivity of spiking neural networks to potentially malicious adversarial attacks has received little attention so far. We show how white-box adversarial attack algorithms can be adapted to the discrete and sparse nature of event-based visual data, and demonstrate smaller perturbation magnitudes at higher success rates than the current state-of-the-art algorithms. For the first time, we also verify the effectiveness of these perturbations directly on neuromorphic hardware. Finally, we discuss the properties of the resulting perturbations, the effect of adversarial training as a defense strategy, and future directions.
C1 [Buechel, Julian] IBM Res, Zurich, Switzerland.
   [Lenz, Gregor; Sheik, Sadique] SynSense, Zurich, Switzerland.
   [Hu, Yalun] SynSense, Chengdu, Peoples R China.
   [Sorbaro, Martino] Swiss Fed Inst Technol, AI Ctr, Zurich, Switzerland.
   [Sorbaro, Martino] Univ Zurich, Inst Neuroinformat, ETH, Zurich, Switzerland.
RP Büchel, J (corresponding author), IBM Res, Zurich, Switzerland.; Sorbaro, M (corresponding author), Swiss Fed Inst Technol, AI Ctr, Zurich, Switzerland.; Sorbaro, M (corresponding author), Univ Zurich, Inst Neuroinformat, ETH, Zurich, Switzerland.
EM msorbaro@ethz.ch
CR Akhtar N, 2018, IEEE ACCESS, V6, P14410, DOI 10.1109/ACCESS.2018.2807385
   Amir A, 2017, PROC CVPR IEEE, P7388, DOI 10.1109/CVPR.2017.781
   Bagheri A., 2018, 2018 IEEE 19 INT WOR
   Balkanski E, 2020, Arxiv, DOI arXiv:2010.11782
   Bengio Y, 2013, Arxiv, DOI arXiv:1308.3432
   Biggio B, 2018, PATTERN RECOGN, V84, P317, DOI 10.1016/j.patcog.2018.07.023
   Brown TB, 2018, Arxiv, DOI arXiv:1712.09665
   Cherupally SK, 2022, SEMICOND SCI TECH, V37, DOI 10.1088/1361-6641/ac461f
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Esser SK, 2016, P NATL ACAD SCI USA, V113, P11441, DOI 10.1073/pnas.1604850113
   Eykholt K, 2018, PROC CVPR IEEE, P1625, DOI 10.1109/CVPR.2018.00175
   Furber SB, 2013, IEEE T COMPUT, V62, P2454, DOI 10.1109/TC.2012.142
   Giraud C, 2004, INT FED INFO PROC, V153, P159
   Khaddam-Aljameh R., 2021, 2021 Symposium on VLSI Technology
   Kim Y, 2014, CONF PROC INT SYMP C, P361, DOI 10.1109/ISCA.2014.6853210
   Liang L, 2023, IEEE T NEUR NET LEAR, V34, P2569, DOI 10.1109/TNNLS.2021.3106961
   Liu Q., 2019, P IEEECVF C COMPUTER
   Marcovecchio A, 2022, INT J CLIMATOL, V42, P81, DOI 10.1002/joc.7233
   Modas A, 2019, PROC CVPR IEEE, P9079, DOI 10.1109/CVPR.2019.00930
   Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282
   Moradi S, 2018, IEEE T BIOMED CIRC S, V12, P106, DOI 10.1109/TBCAS.2017.2759700
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Orchard G, 2015, FRONT NEUROSCI-SWITZ, V9, DOI [10.3389/fnins.2015.00437, 10.3389/fhins.2015.00437]
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Sharmin Saima, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12374), P399, DOI 10.1007/978-3-030-58526-6_24
   Sorbaro M, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00662
   Stutz D, 2021, Arxiv, DOI arXiv:2006.13977
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Zhang HY, 2019, PR MACH LEARN RES, V97
NR 29
TC 1
Z9 1
U1 6
U2 8
PD DEC 22
PY 2022
VL 16
AR 1068193
DI 10.3389/fnins.2022.1068193
WC Neurosciences
DA 2023-11-11
ER

PT J
AU Kino, H
   Fukushima, T
   Tanaka, T
AF Kino, Hisashi
   Fukushima, Takafumi
   Tanaka, Tetsu
TI Generation of STDP With Non-Volatile Tunnel-FET Memory for Large-Scale
   and Low-Power Spiking Neural Networks
SO IEEE JOURNAL OF THE ELECTRON DEVICES SOCIETY
DT Article
DE Nonvolatile memory; Logic gates; Synapses; Field effect transistors;
   Programming; MONOS devices; Delays; Spiking neural network; tunnel FET;
   MONOS; spike-timing-dependent plasticity; synaptic device
ID TIMING-DEPENDENT PLASTICITY; FLOATING-GATE
AB Spiking neural networks (SNNs) have attracted considerable attention as next-generation neural networks. As SNNs consist of devices that have spike-timing-dependent plasticity (STDP) characteristics, STDP is one of the critical characteristics we need to consider to implement an SNN. In this study, we generated the STDP of a biological synapse with non-volatile tunnel-field-effect-transistor (tunnel FET) memory that has a charge-storage layer and a tunnel FET structure. Tunnel FET is a promising structure to reduce the operation voltage owing to its steep sub-threshold slope. Therefore, the nonvolatile tunnel-FET memory we propose enables the implementation of low-operation-voltage SNNs. This article reports the I - V, programming, and both symmetric and asymmetric STDP characteristics of a non-volatile tunnel-FET memory with p-channel-MOS-like operation.
C1 [Kino, Hisashi] Tohoku Univ, Frontier Res Inst Interdisciplinary Sci, Sendai, Miyagi 9808579, Japan.
   [Fukushima, Takafumi; Tanaka, Tetsu] Tohoku Univ, Grad Sch Engn, Dept Mech Syst Engn, Sendai, Miyagi 9808579, Japan.
   [Tanaka, Tetsu] Tohoku Univ, Grad Sch Biomed Engn, Dept Biomed Engn, Sendai, Miyagi 9808579, Japan.
RP Kino, H (corresponding author), Tohoku Univ, Frontier Res Inst Interdisciplinary Sci, Sendai, Miyagi 9808579, Japan.
EM kino@lbc.mech.tohoku.ac.jp
CR Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Du Y, 2019, IEEE T COMPUT AID D, V38, P1811, DOI 10.1109/TCAD.2018.2859237
   Gopalakrishnan R, 2014, IEEE IJCNN, P4296, DOI 10.1109/IJCNN.2014.6889631
   Izhikevich EM, 2008, P NATL ACAD SCI USA, V105, P3593, DOI 10.1073/pnas.0712231105
   Jo SH, 2010, NANO LETT, V10, P1297, DOI 10.1021/nl904092h
   Kang DH, 2015, NEUROCOMPUTING, V155, P153, DOI 10.1016/j.neucom.2014.12.036
   Kato K, 2020, APPL PHYS EXPRESS, V13, DOI 10.35848/1882-0786/ab9875
   Kim CH, 2018, IEEE T ELECTRON DEV, V65, P1774, DOI 10.1109/TED.2018.2817266
   Kim H, 2016, IEEE ELECTR DEVICE L, V37, P249, DOI 10.1109/LED.2016.2521863
   Kim M, 2015, IEEE T ELECTRON DEV, V62, P9, DOI 10.1109/TED.2014.2371038
   Kino H., 2017, INT C SOL STAT DEV M, P791
   Kino H., 2019, INT C SOLID STATE DE, P673
   Kino H, 2020, 2020 IEEE ELECTRON DEVICES TECHNOLOGY AND MANUFACTURING CONFERENCE (EDTM 2020), DOI 10.1109/edtm47692.2020.9118027
   Kino H, 2020, JPN J APPL PHYS, V59, DOI 10.35848/1347-4065/ab6867
   Kino H, 2019, IEEE J ELECTRON DEVI, V7, P1225, DOI 10.1109/JEDS.2019.2936180
   Kino H, 2018, JPN J APPL PHYS, V57, DOI 10.7567/JJAP.57.04FE07
   Kurenkov A, 2019, ADV MATER, V31, DOI 10.1002/adma.201900636
   Matsuura K, 2018, IEEE J ELECTRON DEVI, V6, P1246, DOI 10.1109/JEDS.2018.2883133
   Ohno T, 2011, NAT MATER, V10, P591, DOI [10.1038/NMAT3054, 10.1038/nmat3054]
   Pankaala Mikko, 2009, Proceedings 2009 International Joint Conference on Neural Networks (IJCNN 2009 - Atlanta), P2409, DOI 10.1109/IJCNN.2009.5178879
   Park J, 2017, IEEE T ELECTRON DEV, V64, P2438, DOI 10.1109/TED.2017.2685519
   Pei YL, 2011, IEEE T NANOTECHNOL, V10, P528, DOI 10.1109/TNANO.2010.2050331
   Sangkil Kim, 2015, 2015 IEEE MTT-S International Microwave Symposium (IMS2015), P1, DOI 10.1109/MWSYM.2015.7166723
   She XY, 2019, IEEE IJCNN
   Smith AW, 2014, NEUROCOMPUTING, V124, P210, DOI 10.1016/j.neucom.2013.07.007
   Tan HW, 2019, ADV INTELL SYST-GER, V1, DOI 10.1002/aisy.201900036
   Wang W, 2019, FARADAY DISCUSS, V213, P453, DOI 10.1039/c8fd00097b
NR 27
TC 2
Z9 2
U1 4
U2 14
PY 2020
VL 8
BP 1266
EP 1271
DI 10.1109/JEDS.2020.3025336
WC Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Belatreche, A
   Maguire, LP
   McGinnity, M
AF Belatreche, Ammar
   Maguire, Liam P.
   McGinnity, Martin
TI Advances in design and application of spiking neural networks
SO SOFT COMPUTING
DT Article; Proceedings Paper
CT 6th International FLINS Conference on Applied Artificial Intelligence
CY SEP 01-03, 2004
CL Blankenberge, BELGIUM
DE spiking neurons; spike response model; integrate-and-fire model; dynamic
   synapse; evolutionary strategy; temporal coding; supervised learning
AB This paper presents new findings in the design and application of biologically plausible neural networks based on spiking neuron models, which represent a more plausible model of real biological neurons where time is considered as an important feature for information encoding and processing in the brain. The design approach consists of an evolutionary strategy based supervised training algorithm, newly developed by the authors, and the use of different biologically plausible neuronal models. A dynamic synapse (DS) based neuron model, a biologically more detailed model, and the spike response model (SRM) are investigated in order to demonstrate the efficacy of the proposed approach and to further our understanding of the computing capabilities of the nervous system. Unlike the conventional synapse, represented as a static entity with a fixed weight, employed in conventional and SRM-based neural networks, a DS is weightless and its strength changes upon the arrival of incoming input spikes. Therefore its efficacy depends on the temporal structure of the impinging spike trains. In the proposed approach, the training of the network free parameters is achieved using an evolutionary strategy where, instead of binary encoding, real values are used to encode the static and DS parameters which underlie the learning process. The results show that spiking neural networks based on both types of synapse are capable of learning non-linearly separable data by means of spatio-temporal encoding. Furthermore, a comparison of the obtained performance with classical neural networks (multi-layer perceptrons) is presented.
C1 Univ Ulster, Fac Engn, Sch Comp & Intelligent Syst, Intelligent Syst Engn Lab, Derry BT48 7JL, North Ireland.
RP Belatreche, A (corresponding author), Univ Ulster, Fac Engn, Sch Comp & Intelligent Syst, Intelligent Syst Engn Lab, Magee Campus,Northland Rd, Derry BT48 7JL, North Ireland.
EM a.belatreche@ulster.ac.uk
CR Bäck T, 1993, EVOL COMPUT, V1, P1, DOI 10.1162/evco.1993.1.1.1
   Belatreche A, 2004, APPLIED COMPUTATIONAL INTELLIGENCE, P205, DOI 10.1142/9789812702661_0040
   Belatreche A, 2003, PROCEEDINGS OF THE 7TH JOINT CONFERENCE ON INFORMATION SCIENCES, P1524
   Beyer HG, 1995, EVOL COMPUT, V3, P311, DOI 10.1162/evco.1995.3.3.311
   Bohte S. M., 2000, 8th European Symposium on Artificial Neural Networks. ESANN"2000. Proceedings, P419
   Dibazar AA, 2003, IEEE IJCNN, P3146
   Fisher RA, 1936, ANN EUGENIC, V7, P179, DOI 10.1111/j.1469-1809.1936.tb02137.x
   Fuhrmann G, 2002, J NEUROPHYSIOL, V87, P140, DOI 10.1152/jn.00258.2001
   Gautrais J, 1998, BIOSYSTEMS, V48, P57, DOI 10.1016/S0303-2647(98)00050-1
   George S, 2003, IEEE IJCNN, P666
   GERSTNER W, 1995, PHYS REV E, V51, P738, DOI 10.1103/PhysRevE.51.738
   Gerstner W., 2002, SPIKING NEURON MODEL
   Goldberg D.E, 1989, GENETIC ALGORITHMS S, V27, P27
   Heittmann A, 2004, 2004 47TH MIDWEST SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL II, CONFERENCE PROCEEDINGS, P373
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Holland J. H., 1992, ADAPTATION NATURAL A
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   MAASS W., 1999, PULSED NEURAL NETWOR
   Mangasarian OL., 1990, CANC DIAGNOSIS VIA L
   Markram H, 1996, NATURE, V382, P807, DOI 10.1038/382807a0
   Pantic L, 2003, NETWORK-COMP NEURAL, V14, P17, DOI 10.1088/0954-898X/14/1/302
   Rao RPN, 2001, NEURAL COMPUT, V13, P2221, DOI 10.1162/089976601750541787
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Thorpe S, 1996, NATURE, V381, P520, DOI 10.1038/381520a0
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   Tsodyks M, 1998, NEURAL COMPUT, V10, P821, DOI 10.1162/089976698300017502
   WOLDBERG WW, 1990, P NATL ACAD SCI USA, V87, P9193
   Yao X, 1999, IEEE T EVOLUT COMPUT, V3, P82, DOI 10.1109/4235.771163
   Zador AM, 1997, NEURON, V19, P1, DOI 10.1016/S0896-6273(00)80341-4
NR 30
TC 60
Z9 65
U1 0
U2 17
PD FEB
PY 2007
VL 11
IS 3
BP 239
EP 248
DI 10.1007/s00500-006-0065-7
WC Computer Science, Artificial Intelligence; Computer Science,
   Interdisciplinary Applications
DA 2023-11-11
ER

PT J
AU Johnson, MG
   Chartier, S
AF Johnson, Melissa G.
   Chartier, Sylvain
TI Spike neural models Part I: The Hodgkin-Huxley model
SO QUANTITATIVE METHODS FOR PSYCHOLOGY
DT Article
DE Spiking neural networks; neural models; Hodkgin-Huxley model
AB Artificial neural networks, or ANNs, have grown a lot since their inception back in the 1940s. But no matter the changes, one of the most important components of neural networks is still the node, which represents the neuron. Within spiking neural networks, the node is especially important because it contains the functions and properties of neurons that are necessary for their network. One important aspect of neurons is the ionic flow which produces action potentials, or spikes. Forces of diffusion and electrostatic pressure work together with the physical properties of the cell to move ions around changing the cell membrane potential which ultimately produces the action potential. This tutorial reviews the Hodkgin-Huxley model and shows how it simulates the ionic flow of the giant squid axon via four differential equations. The model is implemented in Matlab using Euler's Method to approximate the differential equations. By using Euler's method, an extra parameter is created, the time step. This new parameter needs to be carefully considered or the results of the node may be impaired.
C1 [Johnson, Melissa G.; Chartier, Sylvain] Univ Ottawa, Ottawa, ON, Canada.
RP Johnson, MG (corresponding author), Univ Ottawa, Ottawa, ON, Canada.
EM mjohn140@uottawa.ca
CR ABBOTT LF, 1990, LECT NOTES PHYS, V368, P5
   Carlson N. R, 2016, PHYSL BEHAV
   DasGupta B., 1994, MATH RES, V79, P641
   Denker J., 1987, Complex Systems, V1, P877
   Dreiseitl S, 2002, J BIOMED INFORM, V35, P352, DOI 10.1016/S1532-0464(03)00034-0
   DuBois ML, 2010, CELL BIO RES PROG, P1
   Gerstner W., 2002, SPIKING NEURON MODEL
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V116, P424, DOI 10.1113/jphysiol.1952.sp004716
   HODGKIN AL, 1952, PROC R SOC SER B-BIO, V140, P177, DOI 10.1098/rspb.1952.0054
   HODGKIN AL, 1952, COLD SPRING HARB SYM, V17, P43, DOI 10.1101/SQB.1952.017.01.007
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V116, P449, DOI 10.1113/jphysiol.1952.sp004717
   HODGKIN AL, 1955, J PHYSIOL-LONDON, V128, P28, DOI 10.1113/jphysiol.1955.sp005290
   HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554
   Kuebler E. S., 2014, BMC NEUROSCI, V15, P1, DOI [DOI 10.1186/1471-2202-15-S1-P26][PMCID, 10.1186/1471-2202-15-S1-P26, DOI 10.1186/1471-2202-15-S1-P26]
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   McCulloch W. S., 1943, B MATH BIOPHYS, V5, P115, DOI [10.1007/BF02478259, DOI 10.1007/BF02478259]
   ROSENBLATT F, 1958, PSYCHOL REV, V65, P386, DOI 10.1037/h0042519
   Shen Y, 2013, J BIOMOL NMR, V56, P227, DOI 10.1007/s10858-013-9741-y
   Sutskever I., 2014, ADV NEURAL INFORM PR, P3104, DOI DOI 10.5555/2969033.2969173
   Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220
   Zamani M., 2010, PROC INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2010.5596806
NR 23
TC 3
Z9 4
U1 0
U2 3
PY 2017
VL 13
IS 2
BP 105
EP 119
DI 10.20982/tqmp.13.2.p105
WC Social Sciences, Interdisciplinary
DA 2023-11-11
ER

PT C
AU Gibson, T
   Henderson, JA
   Wiles, J
AF Gibson, Tingting (Amy)
   Henderson, James A.
   Wiles, Janet
GP IEEE
TI Predicting Temporal Sequences Using an Event-based Spiking Neural
   Network Incorporating Learnable Delays
SO PROCEEDINGS OF THE 2014 INTERNATIONAL JOINT CONFERENCE ON NEURAL
   NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUL 06-11, 2014
CL Beijing, PEOPLES R CHINA
DE spiking neural networks; transmission delays; delay learning;
   spike-delay-variance learning; dynamic vision sensor
ID TIME-SERIES; DRIVEN SIMULATION; NEURONS; MULTISTEP; SPIKENET; MODELS
AB This paper presents a novel paradigm for a spiking neural network to forecast temporal sequences. The key to the approach is a new model of a spiking neuron that can make multi-step predictions, using learnable temporal delays at both dendrites and axons. This model is able to learn the temporal structure of space-time events, adaptable to multiple scales, with the neurons able to function asynchronously to predict future events in a video sequence. This approach contrasts with conventional neural network approaches that use fixed time steps and iterative prediction. Simulations were conducted to compare the new model to a conventional iterative paradigm on motion sequences from a frame-free event-driven Dynamic Vision Sensor (DVS128, 16k pixels), showing that the new approach consistently has a low prediction error while the iterative paradigm is affected by propagated errors.
C1 [Gibson, Tingting (Amy); Henderson, James A.; Wiles, Janet] Univ Queensland, Sch ITEE, Brisbane, Qld 4072, Australia.
RP Wiles, J (corresponding author), Univ Queensland, Sch ITEE, Brisbane, Qld 4072, Australia.
EM j.wiles@uq.edu.au
CR [Anonymous], ADV NEURAL INFORM PR
   [Anonymous], 1988, SUPERVISED LEARNING
   [Anonymous], 2002, 9EMES RENCONTRES INT
   Barbounis TG, 2006, IEEE T ENERGY CONVER, V21, P273, DOI 10.1109/TEC.2005.847954
   Berthouze L, 2006, NEURAL PROCESS LETT, V23, P27, DOI 10.1007/s11063-005-2838-x
   Boucheny C, 2005, LECT NOTES COMPUT SC, V3512, P136
   Brader JM, 2007, NEURAL COMPUT, V19, P2881, DOI 10.1162/neco.2007.19.11.2881
   Brandli C, 2014, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00275
   Burgsteiner H, 2007, APPL INTELL, V26, P99, DOI 10.1007/s10489-006-0007-1
   Campolo M, 2003, HYDROLOG SCI J, V48, P381, DOI 10.1623/hysj.48.3.381.45286
   Chandy K. M., 2006, GARTN APPL INT WEB S
   Chang FJ, 2007, HYDROLOG SCI J, V52, P114, DOI 10.1623/hysj.52.1.114
   Cheng CT, 2008, J HYDROL, V361, P118, DOI 10.1016/j.jhydrol.2008.07.040
   Debanne D, 2004, NAT REV NEUROSCI, V5, P304, DOI 10.1038/nrn1397
   Delorme A, 2003, NETWORK-COMP NEURAL, V14, P613, DOI 10.1088/0954-898X/14/4/301
   Drazen D, 2011, EXP FLUIDS, V51, P1465, DOI 10.1007/s00348-011-1207-y
   ELMAN JL, 1990, COGNITIVE SCI, V14, P179, DOI 10.1207/s15516709cog1402_1
   Eun Yeong Ahn, 2011, Proceedings 2011 IEEE Symposium on Computational Intelligence for Multimedia, Signal and Vision Processing (CIMSIVP 2011), P52, DOI 10.1109/CIMSIVP.2011.5949251
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Gibson T., 2014, P 2014 INT IN PRESS
   Guerra FA, 2008, CHAOS SOLITON FRACT, V35, P967, DOI 10.1016/j.chaos.2006.05.077
   Hill T, 1996, MANAGE SCI, V42, P1082, DOI 10.1287/mnsc.42.7.1082
   Kayacan E, 2010, EXPERT SYST APPL, V37, P1784, DOI 10.1016/j.eswa.2009.07.064
   Koeth F, 2013, BIOL INSPIR COGN ARC, V6, P8, DOI 10.1016/j.bica.2013.05.010
   Lichtsteiner P, 2008, IEEE J SOLID-ST CIRC, V43, P566, DOI 10.1109/JSSC.2007.914337
   Liu SC, 2010, IEEE INT SYMP CIRC S, P2027, DOI 10.1109/ISCAS.2010.5537164
   Mattia M, 2000, NEURAL COMPUT, V12, P2305, DOI 10.1162/089976600300014953
   Menezes JMP, 2008, NEUROCOMPUTING, V71, P3335, DOI 10.1016/j.neucom.2008.01.030
   Mirzaee H, 2009, CHAOS SOLITON FRACT, V41, P1975, DOI 10.1016/j.chaos.2008.08.016
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   Pal SK, 2008, STUD COMPUT INTELL, V103, P23
   Palmer A, 2006, TOURISM MANAGE, V27, P781, DOI 10.1016/j.tourman.2005.05.006
   Panchev C, 2005, LECT NOTES ARTIF INT, V3575, P182
   Pao HT, 2007, ENERG CONVERS MANAGE, V48, P907, DOI 10.1016/j.enconman.2006.08.016
   Reutimann J, 2003, NEURAL COMPUT, V15, P811, DOI 10.1162/08997660360581912
   Serrano-Gotarredona T., MNIST DVS DATABASE
   Sterne P, 2012, NEURAL COMPUT, V24, P2053, DOI 10.1162/NECO_a_00306
   Stratton P, 2010, NEUROIMAGE, V52, P1070, DOI 10.1016/j.neuroimage.2010.01.027
   Sutskever I, 2011, ICML
   Teräsvirta T, 2005, INT J FORECASTING, V21, P755, DOI 10.1016/j.ijforecast.2005.04.010
   Thorpe SJ, 2004, NEUROCOMPUTING, V58, P857, DOI 10.1016/j.neucom.2004.01.138
   Tolnai S, 2009, J NEUROPHYSIOL, V102, P1206, DOI 10.1152/jn.00275.2009
   WAIBEL A, 1989, IEEE T ACOUST SPEECH, V37, P328, DOI 10.1109/29.21701
   Wright P. W., 2012, NEUR NETW IJCNN 2012, P1
   Zhang GP, 2001, COMPUT OPER RES, V28, P381, DOI 10.1016/S0305-0548(99)00123-9
NR 45
TC 5
Z9 5
U1 0
U2 1
PY 2014
BP 3213
EP 3220
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic; Telecommunications
DA 2023-11-11
ER

PT C
AU Mohemmed, A
   Schliebs, S
   Matsuda, S
   Kasabov, N
AF Mohemmed, Ammar
   Schliebs, Stefan
   Matsuda, Satoshi
   Kasabov, Nikola
BE Iliadis, L
   Jayne, C
TI Method for Training a Spiking Neuron to Associate Input-Output Spike
   Trains
SO ENGINEERING APPLICATIONS OF NEURAL NETWORKS, PT I
SE IFIP Advances in Information and Communication Technology
DT Proceedings Paper
CT 12th INNS EANN-SIG International Conference (EANN 2011)/7th IFIP 12 5
   International Conference (AIAI 2011)
CY SEP 15-18, 2011
CL Corfu, GREECE
DE Spiking Neural Networks; Supervised Learning; Spatio-temporal patterns
AB We propose a novel supervised learning rule allowing the training of a precise input-output behavior to a spiking neuron. A single neuron can be trained to associate (map) different output spike trains to different multiple input spike trains. Spike trains are transformed into continuous functions through appropriate kernels and then Delta rule is applied. The main advantage of the method is its algorithmic simplicity promoting its straightforward application to building spiking neural networks (SNN) for engineering problems. We experimentally demonstrate on a synthetic benchmark problem the suitability of the method for spatio-temporal classification. The obtained results show promising efficiency and precision of the proposed method.
C1 [Mohemmed, Ammar; Schliebs, Stefan; Matsuda, Satoshi; Kasabov, Nikola] Knowledge Engn Discovery Res Inst, Auckland 1010, New Zealand.
RP Mohemmed, A (corresponding author), Knowledge Engn Discovery Res Inst, 350 Queen St, Auckland 1010, New Zealand.
EM amohemme@aut.ac.nz; sschlieb@aut.ac.nz; matsuda.satoshi@nihon-u.ac.jp;
   nkasabov@aut.ac.nz
CR Bell CC, 1997, NATURE, V387, P278, DOI 10.1038/387278a0
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Bohte S. M., 2000, 8th European Symposium on Artificial Neural Networks. ESANN"2000. Proceedings, P419
   Florian R. V., 2010, CHRONOTRON NEURON LE
   Gerstner W., 2002, SPIKING NEURON MODEL
   Gewaltig M-O., 2007, SCHOLARPEDIA, V2, DOI [10.4249/scholarpedia.1430, DOI 10.4249/SCHOLARPEDIA.1430]
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Kasinski A., 2006, International Journal of Applied Mathematics and Computer Science, V16, P101
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Nordlie E, 2009, PLOS COMPUT BIOL, V5, DOI 10.1371/journal.pcbi.1000456
   Ponulak F., 2005, RESUME NEW SUPERVISE
   Ponulak F, 2008, INT J APPL MATH COMP, V18, P117, DOI 10.2478/v10006-008-0011-1
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   van Rossum MCW, 2001, NEURAL COMPUT, V13, P751, DOI 10.1162/089976601300014321
   Victor JD, 1997, NETWORK-COMP NEURAL, V8, P127, DOI 10.1088/0954-898X/8/2/003
NR 15
TC 14
Z9 14
U1 2
U2 2
PY 2011
VL 363
BP 219
EP 228
WC Computer Science, Artificial Intelligence; Computer Science,
   Interdisciplinary Applications
DA 2023-11-11
ER

PT C
AU Rafe, AW
   Garcia, JA
   Raffe, WL
AF Rafe, Andrew W.
   Garcia, Jaime A.
   Raffe, William L.
GP IEEE
TI Exploration Of Encoding And Decoding Methods For Spiking Neural Networks
   On The Cart Pole And Lunar Lander Problems Using Evolutionary Training
SO 2021 IEEE CONGRESS ON EVOLUTIONARY COMPUTATION (CEC 2021)
SE IEEE Congress on Evolutionary Computation
DT Proceedings Paper
CT IEEE Congress on Evolutionary Computation (IEEE CEC)
CY JUN 28-JUL 01, 2021
CL ELECTR NETWORK
DE genetic algorithm; spiking neurons; spiking neural network; spike train;
   reinforcement learning
ID MODEL
AB Spiking Neural Networks are increasingly drawing interest due to their potential for large efficiency gains when used with neuromorphic computers. However, when attempting to replicate the successes of Artificial Neural Networks, challenges are faced due to their vastly different architectures and therefore differing methods for training and optimisation. There has been minimal analysis of the differences between encoding and decoding methods and the effect of state space exposure periods on the performance of these networks. The core contribution of this paper is the detailed analysis of decoding methods, state exposure periods, and a learned input encoding method of an evolved Spiking Neural Network within the Reinforcement Learning context. This is demonstrated using the Cart Pole and Lunar Lander Reinforcement Learning problems. The paper discovers a negative correlation between the generation to reach the goal and the state space exposure period over all decoding methods tested. The state exposure period is also found to influence the number of random actions taken due to the decoding methods being unable to select an action. This paper explores the differences in temporal and rate-based decoding as well as identifying benefits in resetting networks to their default states between episode steps. Additionally, the novel input encoder, is effective at pre-processing state information using the same evolutionary algorithm as the rest of the network.
C1 [Rafe, Andrew W.; Garcia, Jaime A.; Raffe, William L.] Univ Technol Sydney, Sch Comp Sci, Sydney, NSW, Australia.
RP Rafe, AW (corresponding author), Univ Technol Sydney, Sch Comp Sci, Sydney, NSW, Australia.
EM andrew.w.rafe@student.uts.edu.au; jaime.garcia@uts.edu.au;
   william.raffe@uts.edu.au
CR Bing ZS, 2018, IEEE INT CONF ROBOT, P4725
   Brockman Greg, 2016, arXiv
   Burkitt AN, 2006, BIOL CYBERN, V95, P1, DOI 10.1007/s00422-006-0068-6
   Hebb D. O., 1949, ORG BEHAV A NEUROPSY
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508
   Li M, 2017, FRONT CELL NEUROSCI, V11, DOI 10.3389/fncel.2017.00236
   Markowska-Kaczmar U, 2015, SOFT COMPUT, V19, P3465, DOI 10.1007/s00500-014-1515-2
   VanRullen R, 2005, TRENDS NEUROSCI, V28, P1, DOI 10.1016/j.tins.2004.10.010
   Wiklendt L, 2009, NEURAL COMPUT APPL, V18, P369, DOI 10.1007/s00521-008-0187-1
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Yee E., 2011, Proceedings of the 2011 11th International Conference on Hybrid Intelligent Systems (HIS 2011), P411, DOI 10.1109/HIS.2011.6122141
   Zhang L., 2018, IEEE IJCNN, P1
NR 15
TC 1
Z9 1
U1 0
U2 2
PY 2021
BP 498
EP 505
DI 10.1109/CEC45853.2021.9504921
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods; Engineering, Electrical & Electronic; Mathematical &
   Computational Biology; Operations Research & Management Science
DA 2023-11-11
ER

PT C
AU Ratnasingam, S
   McGinnity, TM
AF Ratnasingam, Sivalogeswaran
   McGinnity, T. M.
GP IEEE
TI A Spiking Neural Network for Tactile Form Based Object Recognition
SO 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUL 31-AUG 05, 2011
CL San Jose, CA
DE Tactile object recognition; robotic object recognition; tactile form
   perception
ID NEURONS; TOUCH
AB This paper proposes a biologically plausible system for object recognition based on tactile form perception. A spiking neural network, an encoding scheme for converting the input values into spike trains, a method for converting the output spike pattern into reliable features for object recognition and a training approach for the spiking neural network are proposed. Three separate spiking neural networks are used in this recognition system. Three features, based on the output firing pattern of the three networks, are projected onto a three dimensional space. Each class of objects forms a cluster in the three-dimensional feature space. During the training the firing threshold of the hidden layer is modified in such a way that the cluster formed by an object is small and does not overlap with neighbouring clusters. The system has been tested with a number of objects for recognition based on shape. In addition, the system has also been tested for the ability to recognise objects of the same shape but different size. The results show the proposed system gives good performance in recognising objects based on tactile form perception.
C1 [Ratnasingam, Sivalogeswaran; McGinnity, T. M.] Univ Ulster, Sch Comp & Intelligent Syst, Intelligent Syst Res Ctr, Coleraine BT52 1SA, Londonderry, North Ireland.
RP Ratnasingam, S (corresponding author), Univ Ulster, Sch Comp & Intelligent Syst, Intelligent Syst Res Ctr, Magee Campus, Coleraine BT52 1SA, Londonderry, North Ireland.
EM s.ratnasingam@ulster.ac.uk; tm.mcginnity@ulster.ac.uk
CR Arbib MA, 2000, NEURAL NETWORKS, V13, P975, DOI 10.1016/S0893-6080(00)00070-8
   Azouz R, 2000, P NATL ACAD SCI USA, V97, P8110, DOI 10.1073/pnas.130200797
   BIEDERMAN I, 1987, PSYCHOL REV, V94, P115, DOI 10.1037/0033-295X.94.2.115
   BURKE D, 1988, J PHYSIOL-LONDON, V402, P347, DOI 10.1113/jphysiol.1988.sp017208
   Calvert GA., 2004, HDB MULTISENSORY PRO
   CHAVEZNORIEGA LE, 1990, EXP BRAIN RES, V79, P633
   Dario P, 2000, 2000 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2000), VOLS 1-3, PROCEEDINGS, P1, DOI 10.1109/IROS.2000.894573
   DeLaurentis K. J., 2000, DEV SHAPE MEMORY ALL
   Desai NS, 1999, NAT NEUROSCI, V2, P515, DOI 10.1038/9165
   Heidemann G, 2004, IEEE INT CONF ROBOT, P813, DOI 10.1109/ROBOT.2004.1307249
   Ittyerah M, 2007, BRIT J PSYCHOL, V98, P589, DOI 10.1348/000712606X171531
   James TW, 2007, CAN J EXP PSYCHOL, V61, P219, DOI 10.1037/cjep2007023
   Johnsson M, 2005, LECT NOTES COMPUT SC, V3561, P386
   Johnsson M., 2007, AUTONOMOUS ROBOTIC S, P239
   Johnsson M., 2006, P 9 SCAND C ART INT, P127
   Johnsson M, 2007, ROBOT AUTON SYST, V55, P720, DOI 10.1016/j.robot.2007.05.003
   Johnsson M, 2010, J ROBOT, V2010, DOI 10.1155/2010/860790
   KLATZKY RL, 1987, J EXP PSYCHOL GEN, V116, P356
   LEDERMAN SJ, 1990, COGNITIVE PSYCHOL, V22, P421, DOI 10.1016/0010-0285(90)90009-S
   Natale L., 2006, 6 INT C EP ROB PAR F, P20
   Okamura AM, 1997, IEEE INT CONF ROBOT, P2485, DOI 10.1109/ROBOT.1997.619334
   Petriu E. M., 2003, IEEE T INSTRUMENTATI, V53, P1425
   Ratnasingam S., 2011, IEEE S SERI IN PRESS
   Wade J., 2010, THESIS U ULSTER
NR 24
TC 3
Z9 3
U1 0
U2 3
PY 2011
BP 880
EP 885
WC Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Computer Science, Theory & Methods; Engineering, Electrical &
   Electronic
DA 2023-11-11
ER

PT C
AU Yudanov, D
   Shaaban, M
   Melton, R
   Reznik, L
AF Yudanov, Dmitri
   Shaaban, Muhammad
   Melton, Roy
   Reznik, Leon
GP IEEE
TI GPU-Based Simulation of Spiking Neural Networks with Real-Time
   Performance & High Accuracy
SO 2010 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS IJCNN 2010
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT World Congress on Computational Intelligence (WCCI 2010)
CY 2010
CL Barcelona, SPAIN
DE GPU; CUDA; STDP; spiking neural network; high accuracy; parallel
   computing; shared memory
ID NEURONS; MODEL
AB A novel GPU-based simulation of spiking neural networks is implemented as a hybrid system using Parker-Sochacki numerical integration method with adaptive order. Full single-precision floating-point accuracy for all model variables is achieved. The implementation is validated with exact matching of all neuron potential traces from GPU-based simulation versus those of a reference CPU-based simulation. A network of 4096 Izhikevich neurons simulated on an NVIDIA GTX260 device achieves real-time performance with a speedup of 9 compared to simulation executed on Opteron 285, 2.6-GHz device.
C1 [Yudanov, Dmitri; Shaaban, Muhammad; Melton, Roy; Reznik, Leon] Rochester Inst Technol, Dept Comp Engn, Rochester, NY 14623 USA.
RP Yudanov, D (corresponding author), Rochester Inst Technol, Dept Comp Engn, Rochester, NY 14623 USA.
EM dxy7370@gmail.com; meseec@rit.edu; Roy.Melton@mail.rit.edu;
   lr@cs.rit.edu
CR [Anonymous], 2008, EFFICIENT SPARSE MAT
   [Anonymous], 2010, CUDA DATA PARALLEL P
   Brette R, 2007, J COMPUT NEUROSCI, V23, P349, DOI 10.1007/s10827-007-0038-6
   Coddington EA, 1955, THEORY ORDINARY DIFF
   Fidjeland AK, 2009, IEEE INT CONF ASAP, P137, DOI 10.1109/ASAP.2009.24
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2008, P NATL ACAD SCI USA, V105, P3593, DOI 10.1073/pnas.0712231105
   Lapique L., 1907, J PHYSL PATHOL GEN, V9, P620, DOI DOI 10.1007/S00422-007-0189-6
   Nageswaran J., 2009, NEURAL NETWORKS  JUL
   Nageswaran Jayram Moorkanikara, 2009, 2009 IEEE International Symposium on Circuits and Systems - ISCAS 2009, P1917, DOI 10.1109/ISCAS.2009.5118157
   Parker G. E., 1996, Neural, Parallel & Scientific Computations, V4, P97
   Picard E., 1922, TRAITE ANAL, V3
   Stewart RD, 2009, J COMPUT NEUROSCI, V27, P115, DOI 10.1007/s10827-008-0131-5
   2008, NVIDIA CUDA PROGRAMM
   2009, NVIDIA CUDA C PROGRA
NR 16
TC 1
Z9 1
U1 0
U2 0
PY 2010
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Computer Science, Theory & Methods; Engineering,
   Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Wang, Y
   Duan, SK
   Chen, F
AF Wang, Yuan
   Duan, Shukai
   Chen, Feng
TI Efficient asynchronous federated neuromorphic learning of spiking neural
   networks
SO NEUROCOMPUTING
DT Article
DE Asynchronous federated learning; Spiking Neural Network; Average spike
   rate; Model stalenss
ID POWER
AB Spiking Neural Networks (SNNs) can be trained on resource-constrained devices at low computational costs. There has been little attention to training them on a large-scale distributed system like federated learning. Federated Learning (FL) can be exploited to perform collaborative training for higher accuracy, involving multiple resource-constrained devices. In this paper, we introduce SNNs into asynchronous federated learning (AFL), which adapts to the statistical heterogeneity of users and complex communication environments. A novel fusion weight based on information age and average spike rate is designed, which aims to reduce the impact of model staleness. Numerical experiments validate SNNs on federated learning with MNIST, FashionMNIST, CIFAR10 and SVHN benchmarks, achieving better accuracy and desirable convergence under Non-IID settings.
C1 [Wang, Yuan; Duan, Shukai; Chen, Feng] Southwest Univ, Coll Artificial Intelligence, Chongqing 400715, Peoples R China.
RP Chen, F (corresponding author), Southwest Univ, Coll Artificial Intelligence, Chongqing 400715, Peoples R China.
EM wangyuan_stu@163.com; duansk@swu.edu.cn; fengchen.uestc@gmail.com
CR Bing Han, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13555, DOI 10.1109/CVPR42600.2020.01357
   Chen M., 2019, INT C LEARN RE UNPUB
   Chen RZ, 2018, IEEE IJCNN, P404
   Diehl PU, 2015, IEEE IJCNN
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Gu B, 2022, IEEE T NEUR NET LEAR, V33, P6103, DOI 10.1109/TNNLS.2021.3072238
   Jang H, 2019, IEEE SIGNAL PROC MAG, V36, P64, DOI 10.1109/MSP.2019.2935234
   Kairouz P, 2021, FOUND TRENDS MACH LE, V14, P1, DOI 10.1561/2200000083
   Karimireddy SP, 2020, PR MACH LEARN RES, V119
   Kheradpisheh SR, 2022, NEURAL PROCESS LETT, V54, P1255, DOI 10.1007/s11063-021-10680-x
   Kim S, 2020, AAAI CONF ARTIF INTE, V34, P11270
   Kim Y, 2021, FRONT NEUROSCI-SWITZ, V15, DOI 10.3389/fnins.2021.773954
   Lee C, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00119
   Lee J, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00143
   Li QB, 2021, PROC CVPR IEEE, P10708, DOI 10.1109/CVPR46437.2021.01057
   Li Tian, 2020, P MACH LEARN SYST, DOI DOI 10.48550/ARXIV.1812.06127
   Lu YL, 2020, IEEE T IND INFORM, V16, P2134, DOI 10.1109/TII.2019.2942179
   McMahan HB, 2017, PR MACH LEARN RES, V54, P1273
   Mostafa H, 2018, IEEE T NEUR NET LEAR, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Nguyen J, 2022, Arxiv, DOI arXiv:2106.06639
   Rathi N, 2023, IEEE T NEUR NET LEAR, V34, P3174, DOI 10.1109/TNNLS.2021.3111897
   Roy K, 2019, NATURE, V575, P607, DOI 10.1038/s41586-019-1677-2
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Shi C, 2021, IEEE T CIRCUITS-II, V68, P1581, DOI 10.1109/TCSII.2021.3063784
   Skatchkovsky N, 2020, INT CONF ACOUST SPEE, P8524, DOI [10.1109/icassp40776.2020.9053861, 10.1109/ICASSP40776.2020.9053861]
   Tavanaei A, 2019, NEUROCOMPUTING, V330, P39, DOI 10.1016/j.neucom.2018.11.014
   Venkatesha Y, 2021, IEEE T SIGNAL PROCES, V69, P6183, DOI 10.1109/TSP.2021.3121632
   Wang HB, 2022, IEEE T BIOMED CIRC S, V16, P636, DOI 10.1109/TBCAS.2022.3189240
   Wang ZY, 2022, IEEE T WIREL COMMUN, V21, P6961, DOI 10.1109/TWC.2022.3153495
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Xie C, 2020, Arxiv, DOI arXiv:1903.03934
   Xie K, 2022, IEEE T VEH TECHNOL, V71, P9980, DOI 10.1109/TVT.2022.3178808
   Yang Q, 2019, ACM T INTEL SYST TEC, V10, DOI 10.1145/3298981
   Yu H, 2019, AAAI CONF ARTIF INTE, P5693
   Zhao B, 2015, IEEE T NEUR NET LEAR, V26, P1963, DOI 10.1109/TNNLS.2014.2362542
   Zheng HL, 2020, Arxiv, DOI arXiv:2011.05280
   Zhou YH, 2022, IEEE T PARALL DISTR, V33, P192, DOI 10.1109/TPDS.2021.3090331
NR 39
TC 0
Z9 0
U1 1
U2 1
PD NOV 7
PY 2023
VL 557
AR 126686
DI 10.1016/j.neucom.2023.126686
EA AUG 2023
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT C
AU Jeanson, F
   White, A
AF Jeanson, Francis
   White, Anthony
BE Soule, T
TI Evolving Axonal Delay Neural Networks for Robot Control
SO PROCEEDINGS OF THE FOURTEENTH INTERNATIONAL CONFERENCE ON GENETIC AND
   EVOLUTIONARY COMPUTATION CONFERENCE
DT Proceedings Paper
CT 14th International Conference on Genetic and Evolutionary Computation
   Conference (GECCO)
CY JUL 07-11, 2012
CL Philadelphia, PA
DE Neural Coding; Spiking Neural Network; Axonal Delays; Coincidence
   Detection; Neural Adaptation; Embodied Cognition
ID VISUAL-CORTEX; CAT; OSCILLATIONS; DYNAMICS; NEURONS
AB This paper investigates the dynamical and control properties of a discrete spiking neural network model with axonal delays. After examining contemporary work on spike timing as a mechanism for neural coding, we introduce a simple axonal delay network model which, via coincidence detection, demonstrates the presence of biologically observed regimes such as sustained firing and the emergence of synchrony. We establish delay criteria allowing for the classification of three distinct regimes including global synchrony, complex firing, and dissipation. We then proceed to test this model in a robot light seeking task. Results show that evolving network delays is sufficient for solving the task. We conclude by hypothesizing that global synchronous firing is more suited to reactive behaviours while complex firing patterns may serve as an organizing mechanism for more indirect processing.
C1 [Jeanson, Francis; White, Anthony] Carleton Univ, Ottawa, ON K1S 5B6, Canada.
RP Jeanson, F (corresponding author), Carleton Univ, 1125 Colonel Dr, Ottawa, ON K1S 5B6, Canada.
EM fjeanson@connect.carleton.ca; arpwhite@scs.carleton.ca
CR Abeles M., 1991, CORTICONICS NEURAL C
   BOUYER JJ, 1981, ELECTROEN CLIN NEURO, V51, P244, DOI 10.1016/0013-4694(81)90138-3
   Braitenberg V., 1984, VEHICLES EXPT SYNTHE
   BROOKS RA, 1991, ARTIF INTELL, V47, P139, DOI 10.1016/0004-3702(91)90053-M
   Brunel N, 2000, J COMPUT NEUROSCI, V8, P183, DOI 10.1023/A:1008925309027
   Buszaki G., 2006, RHYTHMS BRAIN
   ECKHORN R, 1988, BIOL CYBERN, V60, P121, DOI 10.1007/BF00202899
   Edelman GM., 1987, NEURAL DARWINISM THE
   Engel AK, 2001, TRENDS COGN SCI, V5, P16, DOI 10.1016/S1364-6613(00)01568-0
   Fujii H, 1996, NEURAL NETWORKS, V9, P1303, DOI 10.1016/S0893-6080(96)00054-8
   Izhikevich EM, 2004, CEREB CORTEX, V14, P933, DOI 10.1093/cercor/bhh053
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Konig P, 1996, TRENDS NEUROSCI, V19, P130, DOI 10.1016/S0166-2236(96)80019-1
   Kriener B, 2008, NEURAL COMPUT, V20, P2185, DOI 10.1162/neco.2008.02-07-474
   Michel O., 1995, KHEPERA SIMULATOR PA
   MURTHY VN, 1992, P NATL ACAD SCI USA, V89, P5670, DOI 10.1073/pnas.89.12.5670
   NOLFI S, 1996, P WORKSH EV COMP MAC
   Perkel D. H., 1968, Bulletin Neurosci Res Progr, V6, P221
   PETERS A, 1993, CEREB CORTEX, V3, P49, DOI 10.1093/cercor/3.1.49
   Ren M, 2007, SCIENCE, V316, P758, DOI 10.1126/science.1135468
   Scheier C., 1995, ADV ARTIFICIAL LIFE, P862
   SPERLING G, 1960, PSYCHOL MONOGR, V74, P1, DOI 10.1037/h0093759
   Stevens CF, 1999, NEURON, V22, P139, DOI 10.1016/S0896-6273(00)80685-6
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   THORPE SJ, 1989, CONNECTIONISM IN PERSPECTIVE, P63
   von der Marlsburg C., 1981, CORRELATION THEORY B, P2
   Ward LM, 2003, TRENDS COGN SCI, V7, P553, DOI 10.1016/j.tics.2003.10.012
NR 28
TC 5
Z9 5
U1 2
U2 4
PY 2012
BP 121
EP 128
DI 10.1145/2330163.2330181
WC Computer Science, Theory & Methods; Mathematical & Computational Biology
DA 2023-11-11
ER

PT C
AU Xue, FZ
   Zhang, Y
   Zhou, HJ
   Li, XM
AF Xue, Fangzheng
   Zhang, Yang
   Zhou, Hongjun
   Li, Xiumin
BE DelgadoGarcia, JM
   Pan, X
   SanchezCampusano, R
   Wang, R
TI Effects of Temporal Integration on Computational Performance of Spiking
   Neural Network
SO ADVANCES IN COGNITIVE NEURODYNAMICS (VI)
SE Advances in Cognitive Neurodynamics
DT Proceedings Paper
CT 6th International Conference on Cognitive Neurodynamics (ICCN)
CY AUG 01-05, 2017
CL Carmona, SPAIN
DE Spiking neural network; Time constant; Time delay; Computational
   performance
ID TIME
AB In spiking neural networks (SNN), information is considered to be encoded mainly in the temporal patterns of their firing activity. Temporal integration of information plays a crucial role in a variety of cognitive processes, such as sensory discrimination, decision-making, or interval timing. However, it is rarely considered in traditional computational SNN models. In this paper, we investigate the influence of temporal integration on the computational performance of liquid state machine (LSM) from two aspects: the synaptic decay constant and time delay from presynaptic neurons to the output neurons. LSM is a biologically spiking neural network model for real-time computing on time-varying inputs, where the high dimensionality of dynamical spikes is transformed into smoothly changing states through synaptic integration into the readout neuron. Our experimental results show that increasing the decay constant of synapses from SNN to the output neuron can remarkably improve the computational performance due to the enhancement of temporal integration. Moreover, transmission delays have an even larger impact on the richness of dynamical states, which in turn significantly increase the computational accuracy of SNN. These results may have important implications for the modeling of spiking neural networks with excellent computational performance.
C1 [Xue, Fangzheng; Zhang, Yang; Li, Xiumin] Chongqing Univ, Coll Automat, Chongqing, Peoples R China.
   [Zhou, Hongjun] Chongqing Univ, Sch Econ & Business Adm, Chongqing, Peoples R China.
RP Li, XM (corresponding author), Chongqing Univ, Coll Automat, Chongqing, Peoples R China.
EM xmli@cqu.edu.cn
CR Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Joshi P, 2004, LECT NOTES COMPUT SC, V3141, P258
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Maass W, 2001, THEOR COMPUT SCI, V261, P157, DOI 10.1016/S0304-3975(00)00137-7
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maass W., 1997, Proceedings of the Tenth Annual Conference on Computational Learning Theory, P54, DOI 10.1145/267460.267477
   Maass W., 2002, DBLP, P213
   Maass W., 1999, COMPLEXITY LEARNING
   Maass W, 2007, PLOS COMPUT BIOL, V3, P15, DOI 10.1371/journal.pcbi.0020165
   Schmitt M., 1998, COMPUTING BOOLEAN FU
   Schrauwen B, 2008, NEURAL NETWORKS, V21, P511, DOI 10.1016/j.neunet.2007.12.009
   Verstraeten D, 2005, INFORM PROCESS LETT, V95, P521, DOI 10.1016/j.ipl.2005.05.019
NR 12
TC 0
Z9 0
U1 0
U2 1
PY 2018
BP 127
EP 133
DI 10.1007/978-981-10-8854-4_16
WC Neurosciences; Neuroimaging
DA 2023-11-11
ER

PT C
AU Venceslai, V
   Marchisio, A
   Alouani, I
   Martina, M
   Shafique, M
AF Venceslai, Valerio
   Marchisio, Alberto
   Alouani, Ihsen
   Martina, Maurizio
   Shafique, Muhammad
GP IEEE
TI NeuroAttack: Undermining Spiking Neural Networks Security through
   Externally Triggered Bit-Flips
SO 2020 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN) held as part
   of the IEEE World Congress on Computational Intelligence (IEEE WCCI)
CY JUL 19-24, 2020
CL ELECTR NETWORK
DE Machine Learning; Spiking Neural Networks; Reliability; Adversarial
   Attacks; Fault-Injection Attacks; Deep Neural Networks; DNN; SNN;
   Security; Resilience; Cross-Layer
AB Due to their proven efficiency, machine-learning systems are deployed in a wide range of complex real-life problems. More specifically, Spiking Neural Networks (SNNs) emerged as a promising solution to the accuracy, resourceutilization, and energy-efficiency challenges in machine-learning systems. While these systems are going mainstream, they have inherent security and reliability issues. In this paper, we propose NeuroAttack, a cross-layer attack that threatens the SNNs integrity by exploiting low-level reliability issues through a high-level attack. Particularly, we trigger a fault-injection based sneaky hardware backdoor through a carefully crafted adversarial input noise. Our results on Deep Neural Networks (DNNs) and SNNs show a serious integrity threat to state-of-the art machine-learning techniques.
C1 [Venceslai, Valerio; Marchisio, Alberto; Shafique, Muhammad] Tech Univ Wien, Vienna, Austria.
   [Venceslai, Valerio; Martina, Maurizio] Politecn Torino, Turin, Italy.
   [Alouani, Ihsen] Univ Polytech Hauts De France, Valenciennes, France.
RP Venceslai, V (corresponding author), Tech Univ Wien, Vienna, Austria.; Venceslai, V (corresponding author), Politecn Torino, Turin, Italy.
EM s254591@studenti.polito.it; alberto.marchisio@tuwien.ac.at;
   ihsen.alouani@uphf.fr; maurizio.martina@polito.it;
   muhammad.shafiquel@tuwien.ac.at
CR Abbassi I. H., 2018, DATE
   [Anonymous], 2017, CORR
   [Anonymous], 2019, CORR
   Beeman D, 2013, ENCY COMPUTATIONAL N
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Bouvier M., 2019, SPIKING NEURAL NETWO
   Cardaliaguet P., 1992, NEURAL NETWORKS
   Clements J., 2018, CORR
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Gibson G. J., 1989, ICASSP
   Goodfellow I. J., 2014, INT C LEARNING REPRE, DOI DOI 10.1109/CVPR.2016.90
   Han S., 2016, INT C LEARN REPR ICL
   Hanif M. A., 2018, DATE
   Hanif M. A., 2019, PHILOS T ROYAL SOC A
   Hanif MA, 2020, IEEE INT ON LINE, DOI 10.1109/iolts50870.2020.9159734
   Hanif MA, 2018, J LOW POWER ELECTRON, V14, P520, DOI 10.1166/jolpe.2018.1575
   Hanif MA, 2018, IEEE INT ON LINE, P257, DOI 10.1109/IOLTS.2018.8474192
   Hazan H, 2018, FRONT NEUROINFORM, V12, DOI 10.3389/fninf.2018.00089
   Hoang L.-H., 2020, DATE
   Izhikevich E. M., 2003, IEEE T NEURAL NETWOR
   Kim Y, 2014, CONF PROC INT SYMP C, P361, DOI 10.1109/ISCA.2014.6853210
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee C., 2018, FRONTIERS NEUROSCIEN
   Lee J. Y., 2016, CORR, Vabs/1603.03827
   Liu YN, 2017, ICCAD-IEEE ACM INT, P131, DOI 10.1109/ICCAD.2017.8203770
   Liu YQ, 2018, 25TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2018), DOI 10.14722/ndss.2018.23291
   Marchisio Alberto, 2019, 2019 IEEE Computer Society Annual Symposium on VLSI (ISVLSI), P553, DOI 10.1109/ISVLSI.2019.00105
   Marchisio A., 2020, IJCNN
   Marchisio A., 2018, IJCNN
   Marchisio A., 2020, DATE
   Marchisio A., 2020, DAC
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Neftci E. O., 2019, SIGNAL PROCESSING MA
   Neggaz M. A., 2018, ICCD
   Neggaz M. A., 2019, IEEE DESIGN TEST
   Rakin A. S., 2019, CORR
   Reagen B, 2018, DES AUT CON, DOI 10.1145/3195970.3195997
   Reagen B, 2016, CONF PROC INT SYMP C, P267, DOI 10.1109/ISCA.2016.32
   Rodriguez J., 2019, FDT
   Rueckauer B., FRONTIERS NEUROSCIEN
   Vaila R., 2019, CORR
   Wang ZZ, 2014, INT J NEURAL SYST, V24, DOI 10.1142/S0129065714400048
   Zhang J. J., 2019, DAC
NR 44
TC 13
Z9 13
U1 0
U2 3
PY 2020
DI 10.1109/ijcnn48605.2020.9207351
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture
DA 2023-11-11
ER

PT J
AU Gupta, S
   Singal, G
   Garg, D
   Jagannathan, S
AF Gupta, Surbhi
   Singal, Gaurav
   Garg, Deepak
   Jagannathan, Sarangapani
TI QC_SANE: Robust Control in DRL Using Quantile Critic With Spiking Actor
   and Normalized Ensemble
SO IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
DT Article
DE Artificial neural networks; Neurons; Uncertainty; Task analysis;
   Robustness; Statistics; Sociology; Actor critic; deep reinforcement
   learning (DRL); ensemble; reinforcement learning (RL); robust control;
   spiking neural network (SNN)
AB Recently introduced deep reinforcement learning (DRL) techniques in discrete-time have resulted in significant advances in online games, robotics, and so on. Inspired from recent developments, we have proposed an approach referred to as Quantile Critic with Spiking Actor and Normalized Ensemble (QC_SANE) for continuous control problems, which uses quantile loss to train critic and a spiking neural network (NN) to train an ensemble of actors. The NN does an internal normalization using a scaled exponential linear unit (SELU) activation function and ensures robustness. The empirical study on multijoint dynamics with contact (MuJoCo)-based environments shows improved training and test results than the state-of-the-art approach: population coded spiking actor network (PopSAN).
C1 [Gupta, Surbhi; Garg, Deepak] Bennett Univ, Dept Comp Sci Engn, Greater Noida 201310, Uttar Pradesh, India.
   [Singal, Gaurav] Netaji Subhas Univ Technol, Dept Comp Sci Engn, New Delhi 110078, India.
   [Jagannathan, Sarangapani] Missouri Univ Sci & Technol, Dept Elect & Comp Engn, Rolla, MO 65409 USA.
RP Singal, G (corresponding author), Netaji Subhas Univ Technol, Dept Comp Sci Engn, New Delhi 110078, India.
EM gsurbhi1993@gmail.com; gaurav.singal@nsut.ac.in;
   deepakgarg108@gmail.com; sarangap@mst.edu
CR Comsa JM, 2020, INT CONF ACOUST SPEE, P8529, DOI [10.1109/ICASSP40776.2020.9053856, 10.1109/icassp40776.2020.9053856]
   Garg, 2019, ARXIV190904121
   Gupta S, 2021, ARCH COMPUT METHOD E, V28, P4715, DOI 10.1007/s11831-021-09552-3
   Haarnoja T, 2018, PR MACH LEARN RES, V80
   Hans A., 2010, 2010 Ninth International Conference on Machine Learning and Applications (ICMLA 2010), P401, DOI 10.1109/ICMLA.2010.66
   Hessel M, 2018, AAAI CONF ARTIF INTE, P3215
   Hill A., 2018, STABLE BASELINES
   Kaul, 2020, ARXIV200105209
   Klambauer G, 2017, ADV NEURAL INFORM PR, P971, DOI DOI 10.5555/3294771.3294864
   KUZNETSOV A, 2020, INT C MACHINE LEARNI, P5556
   Leahy, 2020, ARXIV201009042
   Lillicrap TP., 2015, ARXIV, DOI DOI 10.1016/S1098-3015(10)67722-4
   Lopez-Paz D, 2018, ARXIV181100908
   Ma XY, 2020, VEH TECHNOL CONFE, DOI 10.1109/VTC2020-Fall49728.2020.9348512
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Michmizos, 2020, ARXIV201009635
   Mnih V., 2013, ARXIV, DOI DOI 10.1038/NATURE14236
   Ota K, 2020, P INT C MACH LEARN, P7424
   Patel D, 2019, NEURAL NETWORKS, V120, P108, DOI 10.1016/j.neunet.2019.08.009
   Rathi Nitin, 2020, INT C LEARN REPR
   Schneider J., 2020, ARXIV201109588
   Silver D, 2014, PR MACH LEARN RES, V32
   Takeuchi, 2006, NONPARAMETRIC QUANTI
   Tieck JCV, 2021, IEEE ROBOT AUTOM LET, V6, P2894, DOI 10.1109/LRA.2020.3034067
   van Hasselt H, 2016, AAAI CONF ARTIF INTE, P2094
   Wang ZY, 2016, PR MACH LEARN RES, V48
   Wattenhofer, 2019, ARXIV190611941
   Zhang ST, 2019, AAAI CONF ARTIF INTE, P5789
   Zhou, 2017, ARXIV171208987
NR 29
TC 0
Z9 0
U1 5
U2 12
PD SEP
PY 2023
VL 34
IS 9
BP 6656
EP 6662
DI 10.1109/TNNLS.2021.3129525
EA DEC 2021
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Computer Science, Theory & Methods; Engineering,
   Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Kim, J
   Kim, CH
   Woo, SY
   Kang, WM
   Seo, YT
   Lee, S
   Oh, S
   Bae, JH
   Park, BG
   Lee, JH
AF Kim, Jangsaeng
   Kim, Chul-Heung
   Woo, Sung Yun
   Kang, Won-Mook
   Seo, Young-Tak
   Lee, Soochang
   Oh, Seongbin
   Bae, Jong-Ho
   Park, Byung-Gook
   Lee, Jong-Ho
TI Initial synaptic weight distribution for fast learning speed and high
   recognition rate in STDP-based spiking neural network
SO SOLID-STATE ELECTRONICS
DT Article
DE Initial synaptic weight distribution; Spike-timing-dependent plasticity
   (STDP); Homeostasis functionality; NOR flash memory; Spiking neural
   networks
ID MEMORY
AB We analyze that the initial synaptic weight distribution affects the performance, such as the learning speed, recognition rate and the power consumption in the spiking neural networks (SNNs) based on spike-timing-dependent plasticity (STDP) learning rule. A thin-film transistor (TFT)-type NOR flash memory is used as a synaptic device. In this fully connected two-layer neuromorphic system using the proposed pulse scheme, the results with and without the homeostasis functionality were analyzed separately. In addition, power consumption of the network in various initial synaptic weight distributions, and recognition rate that varies with the number of output neurons are also investigated. In pattern recognition for 28 x 28 MNIST handwritten patterns, higher performance is achieved in various aspects when the initial synaptic weights are distributed near the maximum value.
C1 [Kim, Jangsaeng; Kim, Chul-Heung; Woo, Sung Yun; Kang, Won-Mook; Seo, Young-Tak; Lee, Soochang; Oh, Seongbin; Park, Byung-Gook; Lee, Jong-Ho] Seoul Natl Univ, Dept Elect & Comp Engn, Seoul 151742, South Korea.
   [Kim, Jangsaeng; Kim, Chul-Heung; Woo, Sung Yun; Kang, Won-Mook; Seo, Young-Tak; Lee, Soochang; Oh, Seongbin; Park, Byung-Gook; Lee, Jong-Ho] Seoul Natl Univ, ISRC, Seoul 151742, South Korea.
   [Bae, Jong-Ho] Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA.
RP Lee, JH (corresponding author), Seoul Natl Univ, Dept Elect & Comp Engn, Seoul 151742, South Korea.; Lee, JH (corresponding author), Seoul Natl Univ, ISRC, Seoul 151742, South Korea.
EM jhl@snu.ac.kr
CR [Anonymous], 2010, 2010 INT JOINT C NEU
   [Anonymous], 2012, IEDM
   Burr GW, 2015, IEEE T ELECTRON DEV, V62, P3498, DOI 10.1109/TED.2015.2439635
   Fernandez-Redondo M., 2000, Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium, P543, DOI 10.1109/IJCNN.2000.860828
   Fernandez-Redondo M., 2001, 9th European Symposium on Artificial Neural Networks. ESANN'2001. Proceedings, P119
   Indiveri G, 2015, P IEEE, V103, P1379, DOI 10.1109/JPROC.2015.2444094
   Kang WM, 2019, IEEE IJCNN
   Kim CH, 2018, IEEE T ELECTRON DEV, V65, P1774, DOI 10.1109/TED.2018.2817266
   LeCun Y, 2015, NATURE, V521, p7553 436 444, DOI [10.1038/nature14539, DOI 10.1038/NATURE14539]
   Lee S, 2019, J NANOSCI NANOTECHNO, V19, P6050, DOI 10.1166/jnn.2019.17025
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Querlioz D, 2013, IEEE T NANOTECHNOL, V12, P288, DOI 10.1109/TNANO.2013.2250995
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Sidler S, 2017, LECT NOTES COMPUT SC, V10613, P281, DOI 10.1007/978-3-319-68600-4_33
   Suri M, 2011, 2011 IEEE INTERNATIONAL ELECTRON DEVICES MEETING (IEDM)
   Yam JYF, 2000, NEUROCOMPUTING, V30, P219, DOI 10.1016/S0925-2312(99)00127-7
   Zamarreño-Ramos C, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00026
NR 17
TC 3
Z9 3
U1 2
U2 37
PD MAR
PY 2020
VL 165
AR 107742
DI 10.1016/j.sse.2019.107742
WC Engineering, Electrical & Electronic; Physics, Applied; Physics,
   Condensed Matter
DA 2023-11-11
ER

PT J
AU Bohte, SM
   Kok, JN
AF Bohte, SM
   Kok, JN
TI Applications of spiking neural networks
SO INFORMATION PROCESSING LETTERS
DT Editorial Material
DE distributed computing
C1 CWI, NL-1098 SJ Amsterdam, Netherlands.
   Leiden Univ, LIACS, NL-2300 RA Leiden, Netherlands.
RP Bohte, SM (corresponding author), CWI, Kruislaan 413, NL-1098 SJ Amsterdam, Netherlands.
EM s.m.bohte@cwi.nl; joost@liacs.nl
CR [Anonymous], 2005, ADV NEURAL INF PROCE
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Deneve S., 2005, ADV NEURAL INFORM PR, V17, P353
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Zemel R, 2005, ADV NEURAL INFORM PR, V17, P1609
NR 5
TC 16
Z9 16
U1 0
U2 6
PD SEP 30
PY 2005
VL 95
IS 6
BP 519
EP 520
DI 10.1016/j.ipl.2005.05.018
WC Computer Science, Information Systems
DA 2023-11-11
ER

PT J
AU Jeon, M
   Kang, T
   Lee, JJ
   Lee, W
AF Jeon, Mingi
   Kang, Taewook
   Lee, Jae-Jin
   Lee, Woojoo
TI A Study on the Low-Power Operation of the Spike Neural Network Using the
   Sensory Adaptation Method
SO MATHEMATICS
DT Article
DE artificial neural networks; spiking neural networks; neuromorphic;
   frequency adaptation
ID MODEL; PROCESSOR; LOIHI
AB Motivated by the idea that there should be a close relationship between biological significance and low power driving of spike neural networks (SNNs), this paper aims to focus on spike-frequency adaptation, which deviates significantly from existing biological meaningfulness, and develop a new spike-frequency adaptation with more biological characteristics. As a result, this paper proposes the sensory adaptation method that reflects the mechanisms of the human sensory organs, and studies network architectures and neuron models for the proposed method. Next, this paper introduces a dedicated SNN simulator that can selectively apply the conventional spike-frequency adaptation and the proposed method, and provides the results of functional verification and effectiveness evaluation of the proposed method. Through intensive simulation, this paper reveals that the proposed method can produce a level of training and testing performance similar to the conventional method while significantly reducing the number of spikes to 32.66% and 45.63%, respectively. Furthermore, this paper contributes to SNN research by showing an example based on in-depth analysis that embedding biological meaning in SNNs may be closely related to the low-power driving characteristics of SNNs.
C1 [Jeon, Mingi; Lee, Woojoo] Chung Ang Univ, Sch Elect & Elect Engn, 84 Heukseok Ro, Seoul 06974, South Korea.
   [Kang, Taewook; Lee, Jae-Jin] Elect & Telecommun Res Inst, 218 Gajeong Ro, Daejeon 34129, South Korea.
RP Lee, W (corresponding author), Chung Ang Univ, Sch Elect & Elect Engn, 84 Heukseok Ro, Seoul 06974, South Korea.
EM space@cau.ac.kr
CR Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   Barbier T, 2021, IEEE COMPUT SOC CONF, P1377, DOI 10.1109/CVPRW53098.2021.00152
   Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Davies M., 2019, P NEURO INSPIRED COM
   Davies M, 2021, P IEEE, V109, P911, DOI 10.1109/JPROC.2021.3067593
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Doborjeh M, 2021, NEURAL NETWORKS, V144, P522, DOI 10.1016/j.neunet.2021.09.013
   Guo SS, 2019, PR GR LAK SYMP VLSI, P63, DOI 10.1145/3299874.3317966
   Ha GE, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms13791
   HAN K, 2019, INT C  INDOOR POSIT, pNIL79, DOI DOI 10.1109/ipin.2019.8911751
   Han K, 2021, IEEE INTERNET THINGS, V8, P4642, DOI 10.1109/JIOT.2020.3027479
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Kang T, 2022, IEEE T INSTRUM MEAS, V71, DOI 10.1109/TIM.2022.3187719
   Kang T, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2021.3082273
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Khodaverdian Zeinab, 2021, 2021 7th International Conference on Web Research (ICWR), P191, DOI 10.1109/ICWR51868.2021.9443133
   Khodaverdian Z, 2021, ARXIV, DOI DOI 10.48550/ARXIV.2106.12178
   Kulkarni SR, 2017, IEEE I C ELECT CIRC, P128, DOI 10.1109/ICECS.2017.8292015
   Lee W, 2019, IEEE T COMPUT AID D, V38, P1758, DOI 10.1109/TCAD.2018.2859240
   Li SX, 2021, IEEE T CIRCUITS-I, V68, P1543, DOI 10.1109/TCSI.2021.3052885
   Liu DQ, 2017, NEUROCOMPUTING, V249, P212, DOI 10.1016/j.neucom.2017.04.003
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   MORRIS C, 1981, BIOPHYS J, V35, P193, DOI 10.1016/S0006-3495(81)84782-0
   Morrison A, 2007, NEURAL COMPUT, V19, P1437, DOI 10.1162/neco.2007.19.6.1437
   Nessler B, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003037
   Peron SP, 2009, BIOL CYBERN, V100, P505, DOI 10.1007/s00422-009-0304-y
   Peykani P., 2021, BIG DATA COMPUT VISI, V1, P170
   Pfister JP, 2006, J NEUROSCI, V26, P9673, DOI 10.1523/JNEUROSCI.1425-06.2006
   Querlioz D, 2013, IEEE T NANOTECHNOL, V12, P288, DOI 10.1109/TNANO.2013.2250995
   Shukla R., 2021, BIG DATA COMPUT VISI, V1, P1
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
NR 35
TC 0
Z9 0
U1 0
U2 1
PD NOV
PY 2022
VL 10
IS 22
AR 4191
DI 10.3390/math10224191
WC Mathematics
DA 2023-11-11
ER

PT S
AU Florian, RV
AF Florian, Razvan V.
BE Nolfi, S
   Baldassarre, G
   Calabretta, R
   Hallam, JCT
   Marocco, D
   Meyer, JA
   Miglino, O
   Parisi, D
TI Spiking neural controllers for pushing objects around
SO FROM ANIMALS TO ANIMATS 9, PROCEEDINGS
SE Lecture Notes in Computer Science
DT Article; Proceedings Paper
CT 9th International Conference on Simulation of Adaptive Behavior
CY SEP 25-29, 2006
CL Italian Natl Res Council, Rome, ITALY
HO Italian Natl Res Council
ID TIMING-DEPENDENT PLASTICITY; NEURONS
AB We evolve spiking neural networks that implement a seek-push-release drive for a simple simulated agent interacting with objects. The evolved agents display minimally-cognitive behavior, by switching as a function of context between the three sub-behaviors and by being able to discriminate relative object size. The neural controllers have either static synapses or synapses featuring spike-timing-dependent plasticity (STDP). Both types of networks are able to solve the task with similar efficacy, but networks with plastic synapses evolved faster. In the evolved networks, plasticity plays a minor role during the interaction with the environment and is used mostly to tune synapses when networks start to function.
C1 Ctr Cognit & Neural Studies, Cluj Napoca 400504, Romania.
   Univ Babes Bolyai, Inst Interdisciplinary Expt Res, Cluj Napoca 400271, Romania.
RP Florian, RV (corresponding author), Ctr Cognit & Neural Studies, Str Saturn 24, Cluj Napoca 400504, Romania.
EM florian@coneural.org
CR [Anonymous], P AAAI SPRING S DEV
   BEER R, 1996, P 4 INT C SIMULATION, P721
   Bi GQ, 2002, BIOL CYBERN, V87, P319, DOI 10.1007/s00422-002-0349-7
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   DAMPER R, 1998, P 1998 IEEE INT S CI
   Damper RI, 2003, LECT NOTES COMPUT SC, V2611, P616
   Damper RI, 2000, ROBOT AUTON SYST, V31, P247, DOI 10.1016/S0921-8890(99)00122-0
   Dan Y, 2004, NEURON, V44, P23, DOI 10.1016/j.neuron.2004.09.007
   DasGupta B, 1996, NEURAL COMPUT, V8, P805, DOI 10.1162/neco.1996.8.4.805
   Di Paolo EA, 2003, PHILOS T R SOC A, V361, P2299, DOI 10.1098/rsta.2003.1256
   Di Paolo EA, 2002, ADAPT BEHAV, V10, P243, DOI 10.1177/1059712302010003006
   DIPAOLO EA, 2002, EPSRCBBSRC INT WORKS
   Federici D, 2005, NEURAL NETWORKS, V18, P746, DOI 10.1016/j.neunet.2005.06.006
   Federici D, 2005, P CEC 2005 IEEE C EV
   FLOREANO D, 2001, EVOLUTIONARY ROBOTIC, V4
   FLOREANO D, 2002, P 3 INT S HUM ART IN
   Floreano D, 2002, P 8 INT C ART LIF
   Florian RV, 2005, Seventh International Symposium on Symbolic and Numeric Algorithms for Scientific Computing, Proceedings, P299
   FLORIAN RV, 2003, 0301 CTR COGN NEUR S
   FLORIAN RV, 2003, CONEURAL0302 CTR COG
   FLORIAN RV, 2003, CONEURAL0303 CTR COG
   FRENCH RLB, 2001, P GEN EV COMP C GECC, P1099
   FRENCH RLB, 2002, P 7 INT C SIM AD BEH, P335
   Gerstner W., 2002, SPIKING NEURON MODEL
   HAGRAS HAK, 2004, P 2004 IEEE INT C RO
   Katada Y, 2004, LECT NOTES COMPUT SC, V3242, P952
   Katada Y, 2003, 2003 IEEE INTERNATIONAL SYMPOSIUM ON COMPUTATIONAL INTELLIGENCE IN ROBOTICS AND AUTOMATION, VOLS I-III, PROCEEDINGS, P318
   MAAS W, 1999, PULSED NEURAL NETWOR
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   MURESAN RC, 2004, INT C INT ENG SYST S
   Pfeifer R., 1999, UNDERSTANDING INTELL
   ROGGEN D, 2003, 2003 NASA DOD C EVOL, P199
   Ruppin E, 2002, NAT REV NEUROSCI, V3, P132, DOI 10.1038/nrn729
   SAGGIE K, 2003, LECT NOTES COMPUTER, V2801
   SAGGIEWEXLER K, 2005, ARTIF LIFE, V12, P1
   Schnitger G., 1994, THEORETICAL ADV NEUR, P127
   Slocum AC, 2000, FROM ANIM ANIMAT, P430
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   SOULA H, 2004, LAST MINUTE RESULTS
   SOULA H, 2003, P INT C ART NEUR NET
   TURNEY P, 1996, 13 INT C MACH LEARN, P135
   Van Leeuwen M, 2003, EVOLVING VISION BASE
NR 43
TC 8
Z9 8
U1 0
U2 4
PY 2006
VL 4095
BP 570
EP 581
WC Computer Science, Artificial Intelligence; Robotics
DA 2023-11-11
ER

PT J
AU Li, M
   Ruan, HB
   Qi, Y
   Guo, TT
   Wang, P
   Pan, G
AF Li, Ming
   Ruan, Haibo
   Qi, Yu
   Guo, Tiantian
   Wang, Ping
   Pan, Gang
TI Odor Recognition with a Spiking Neural Network for Bioelectronic Nose
SO SENSORS
DT Article
DE odor recognition; bioelectronic nose; spiking neural network
ID DISCRIMINATION; MODEL; MACHINE; NEURONS
AB Electronic noses recognize odors using sensor arrays, and usually face difficulties for odor complicacy, while animals have their own biological sensory capabilities for various types of odors. By implanting electrodes into the olfactory bulb of mammalian animals, odors may be recognized by decoding the recorded neural signals, in order to construct a bioelectronic nose. This paper proposes a spiking neural network (SNN)-based odor recognition method from spike trains recorded by the implanted electrode array. The proposed SNN-based approach exploits rich timing information well in precise time points of spikes. To alleviate the overfitting problem, we design a new SNN learning method with a voltage-based regulation strategy. Experiments are carried out using spike train signals recorded from the main olfactory bulb in rats. Results show that our SNN-based approach achieves the state-of-the-art performance, compared with other methods. With the proposed voltage regulation strategy, it achieves about 15% improvement compared with a classical SNN model.
C1 [Li, Ming; Ruan, Haibo; Qi, Yu; Pan, Gang] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310027, Zhejiang, Peoples R China.
   [Guo, Tiantian; Wang, Ping] Zhejiang Univ, Dept Biomed Engn, Hangzhou 310027, Zhejiang, Peoples R China.
   [Pan, Gang] Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310027, Zhejiang, Peoples R China.
RP Qi, Y; Pan, G (corresponding author), Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310027, Zhejiang, Peoples R China.; Pan, G (corresponding author), Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310027, Zhejiang, Peoples R China.
EM lming@zju.edu.cn; hbruan@zju.edu.cn; qiyu@zju.edu.cn;
   21315047@zju.edu.cn; cnpwang@zju.edu.cn; gpan@zju.edu.cn
CR Adrian ED, 1926, J PHYSIOL-LONDON, V61, P49, DOI 10.1113/jphysiol.1926.sp002273
   Bi GQ, 2001, ANNU REV NEUROSCI, V24, P139, DOI 10.1146/annurev.neuro.24.1.139
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Breer H, 2003, ANAL BIOANAL CHEM, V377, P427, DOI 10.1007/s00216-003-2113-9
   Burkitt AN, 2006, BIOL CYBERN, V95, P1, DOI 10.1007/s00422-006-0068-6
   Di Pietrantonio F, 2015, BIOSENS BIOELECTRON, V67, P516, DOI 10.1016/j.bios.2014.09.027
   Dong Q, 2013, BIOSENS BIOELECTRON, V49, P263, DOI 10.1016/j.bios.2013.05.035
   GARDNER JW, 1994, SENSOR ACTUAT B-CHEM, V18, P211
   Guo TT, 2016, SENSOR ACTUAT B-CHEM, V225, P34, DOI 10.1016/j.snb.2015.11.010
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Indiveri G, 2006, IEEE T NEURAL NETWOR, V17, P211, DOI 10.1109/TNN.2005.860850
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Lee SH, 2010, BIOTECHNOL BIOPROC E, V15, P22, DOI [10.1007/s12257-009-3077-1, 10.1007/S12257-009-3077-1]
   Ma X, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00044
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Olshausen BA, 2003, J COGNITIVE NEUROSCI, V15, P154, DOI 10.1162/089892903321107891
   Pan G, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00555
   PERSAUD K, 1982, NATURE, V299, P352, DOI 10.1038/299352a0
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Qi Y, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1597
   Serruya MD, 2002, NATURE, V416, P141, DOI 10.1038/416141a
   Sun XY, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0167497
   Wasilewski T, 2018, SENSOR ACTUAT B-CHEM, V257, P511, DOI 10.1016/j.snb.2017.10.086
   Wasilewski T, 2017, BIOSENS BIOELECTRON, V87, P480, DOI 10.1016/j.bios.2016.08.080
   Wu JB, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00836
   Wu ZH, 2016, IEEE INTELL SYST, V31, P44, DOI 10.1109/MIS.2016.105
   Wu ZH, 2013, IEEE INTELL SYST, V28, P28, DOI 10.1109/MIS.2013.137
   You KJ, 2011, IEEE T BIO-MED ENG, V58, P1208, DOI 10.1109/TBME.2010.2103312
   Yu Q, 2016, IEEE T NEUR NET LEAR, V27, P621, DOI 10.1109/TNNLS.2015.2416771
   Yu Q, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0078318
   Yu YP, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0147754
   Zhang SM, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-018-36885-0
NR 35
TC 8
Z9 8
U1 3
U2 37
PD MAR 1
PY 2019
VL 19
IS 5
AR 993
DI 10.3390/s19050993
WC Chemistry, Analytical; Engineering, Electrical & Electronic; Instruments
   & Instrumentation
DA 2023-11-11
ER

PT J
AU Guo, L
   Shi, HY
   Chen, YG
   Yu, HL
AF Guo, Lei
   Shi, Hongyi
   Chen, Yunge
   Yu, Hongli
TI Anti-interference ability of deep spiking neural network
SO JOURNAL OF INTEGRATIVE NEUROSCIENCE
DT Article
DE Deep spiking neural network; synaptic plasticity; anti-interference;
   electric field; firing rate; correlation
ID MODEL
AB Organisms have the advantages of self-adaptive mechanisms and an anti-interference ability. To investigate the anti-interference ability of a deep spiking neural network that simulates a biological neural system, the correlation between membrane potential and firing rate is interpreted as an anti-interference index so as to investigate the anti-interference ability of a deep spiking neural network under the regulation of synaptic plasticity in the presence of different amplitudes of an electric field. When the relative variation rate of firing rate is less than 10% or the correlation between the membrane potential is greater than half, the influence of electric field on neural network is relatively small. Otherwise, the influence is relatively large. Simulation results show that: based on the regulation of synaptic plasticity, within a certain electric field interference range, the relative rate of variation of cell firing rates is small compared with non-interference, while correlation between the membrane potential in each layer is large when compared to non-interference.
C1 [Guo, Lei] Hebei Univ Technol, Coll Elect Engn, Dept Biomed Engn, Tianjin 300130, Peoples R China.
   [Shi, Hongyi] Hebei Univ Technol, Sch Elect Engn, State Key Lab Reliable & Intelligence Elect Equip, Tianjin 300130, Peoples R China.
   [Chen, Yunge] Hebei Univ Technol, Sch Elect Engn, Key Lab Electromagnet Field & Elect Apparat Relia, Tianjin 300130, Peoples R China.
RP Guo, L (corresponding author), Hebei Univ Technol, Coll Elect Engn, Dept Biomed Engn, Tianjin 300130, Peoples R China.
EM 2004008@hebut.edu.cn
CR Bédard C, 2006, PHYS REV E, V73, DOI 10.1103/PhysRevE.73.051911
   Chang Xiao-long, 2014, Journal of Shanghai Jiaotong University, V48, P1485
   [陈云芝 Chen Yunzhi], 2014, [中国医学物理学杂志, Chinese Journal of Medical Physics], V31, P4820
   Chen YZ, 2015, J BIOMEDICAL ENG, V39, P697
   Guo L, 2018, J INTEGR NEUROSCI, V17, P1
   Izhikevich EM, 2004, CEREB CORTEX, V14, P933, DOI 10.1093/cercor/bhh053
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   [巨政权 Ju Zhengquan], 2012, [微电子学与计算机, Microelectronics & Computer], V29, P89
   [蔺想红 Lin Xianghong], 2015, [电子学报, Acta Electronica Sinica], V43, P577
   Litwin-Kumar A, 2014, NAT COMMUN, V5, DOI 10.1038/ncomms6319
   Nobukawa S, 2015, J ARTIF INTELL SOFT, V5, P109, DOI 10.1515/jaiscr-2015-0023
   Radman T, 2009, BRAIN STIMUL, V2, P215, DOI 10.1016/j.brs.2009.03.007
   Rich S, 2015, BMC NEUROSCI, V16, pP303
   Wang ML, 2015, ACTA PHYS SINICA, V64, P416
   Wei Y, 2014, J NEUROSCI, V34, P15804, DOI 10.1523/JNEUROSCI.3929-12.2014
   Yang Tianpeng, 2013, Journal of Beijing University of Aeronautics and Astronautics, V39, P697
   Yang YP, 2013, J BEIJING U AERONAUT, V39, P705
   Yu Kai, 2014, Application Research of Computers, V31, P70, DOI 10.3969/j.issn.1001-3695.2014.01.016
   [于凯 Yu Kai], 2013, [天津大学学报. 自然科学与工程技术版, Journal of Tianjin University], V46, P726
   Yuan L, 2014, CHINESE ENG SCI, V16, P76
   [张昭昭 Zhang Zhaozhao], 2014, [信息与控制, Information and Control], V43, P181
NR 21
TC 0
Z9 0
U1 1
U2 13
PY 2018
VL 17
IS 4
BP 307
EP 311
DI 10.31083/j.jin.2018.04.0407
WC Neurosciences
DA 2023-11-11
ER

PT C
AU Garrido, JA
   Carrillo, RR
   Luque, NR
   Ros, E
AF Garrido, Jesus A.
   Carrillo, Richard R.
   Luque, Niceto R.
   Ros, Eduardo
BE Cabestany, J
   Rojas, I
   Joya, G
TI Event and Time Driven Hybrid Simulation of Spiking Neural Networks
SO Advances in Computational Intelligence, IWANN 2011, Pt I
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 11th International Work-Conference on Artificial Neural Networks (IWANN)
CY JUN 08-10, 2011
CL Torremolinos, SPAIN
DE spiking neural networks; simulation; time-driven; event-driven
ID NEURONS; INTEGRATE; DYNAMICS; MODELS
AB Emerging research areas in neuroscience are requiring simulation of large and detailed spiking neural networks. Although event-driven methods have been recently proposed to simulate these networks, they still present some drawbacks. To obtain the advantages of an event-driven simulation method and a traditional time-driven method, we present a hybrid method. This method efficiently simulates neural networks composed of several neural models: highly active neurons or neurons defined by very-complex model are simulated using a time-driven method whereas other neurons are simulated using an event-driven method based in lookup tables. To perform a comparative study of this hybrid method in terms of speed and accuracy, a model of the cerebellar granular layer has been simulated. The performance results showed that a hybrid simulation can provide considerable advantages when the network is composed of neurons with different characteristics.
C1 [Garrido, Jesus A.; Luque, Niceto R.; Ros, Eduardo] Univ Granada, Dept Comp Architecture & Technol, CITIC, ETSI Informat & Telecomunicac, E-18071 Granada, Spain.
   [Carrillo, Richard R.] Univ Almeria, Dept Comp Architecture & Elect, Almeria, Spain.
RP Garrido, JA (corresponding author), Univ Granada, Dept Comp Architecture & Technol, CITIC, ETSI Informat & Telecomunicac, E-18071 Granada, Spain.
EM jgarrido@atc.ugr.es; rcarrillo@atc.ugr.es; nluque@atc.ugr.es;
   eros@atc.ugr.es
CR [Anonymous], ADV NEURAL INFORM PR
   [Anonymous], 1998, BOOK GENESIS EXPLORI, DOI DOI 10.1007/978-1-4612-1634-63
   Brette R, 2006, NEURAL COMPUT, V18, P2004, DOI 10.1162/neco.2006.18.8.2004
   Coenen OJMD, 2001, AUTON ROBOT, V11, P291, DOI 10.1023/A:1012403510221
   D'Haene M, 2009, NEURAL COMPUT, V21, P1068, DOI 10.1162/neco.2008.02-08-707
   Delorme A, 1999, NEUROCOMPUTING, V26-7, P989, DOI 10.1016/S0925-2312(99)00095-8
   Gerstner W., 2002, SPIKING NEURON MODEL
   Hines ML, 1997, NEURAL COMPUT, V9, P1179, DOI 10.1162/neco.1997.9.6.1179
   KANDEL E., 1981, PRINCIPLES NEUROSCIE
   Makino T, 2003, NEURAL COMPUT APPL, V11, P210, DOI 10.1007/s00521-003-0358-z
   O'reilly R.C., 2000, COMPUTATIONAL EXPLOR
   Reutimann J, 2003, NEURAL COMPUT, V15, P811, DOI 10.1162/08997660360581912
   Ros E, 2006, NEURAL COMPUT, V18, P2959, DOI 10.1162/neco.2006.18.12.2959
NR 13
TC 10
Z9 10
U1 1
U2 5
PY 2011
VL 6691
BP 554
EP 561
WC Computer Science, Artificial Intelligence; Computer Science,
   Interdisciplinary Applications; Computer Science, Theory & Methods
DA 2023-11-11
ER

PT J
AU Shrestha, A
   Ahmed, K
   Wang, YZ
   Widemann, DP
   Moody, AT
   Van Essen, BC
   Qiu, QR
AF Shrestha, Amar
   Ahmed, Khadeer
   Wang, Yanzhi
   Widemann, David P.
   Moody, Adam T.
   Van Essen, Brian C.
   Qiu, Qinru
TI Modular Spiking Neural Circuits for Mapping Long Short-Term Memory on a
   Neurosynaptic Processor
SO IEEE JOURNAL ON EMERGING AND SELECTED TOPICS IN CIRCUITS AND SYSTEMS
DT Article; Proceedings Paper
CT IEEE/ACM 36th International Conference on Computer-Aided Design (ICCAD)
CY NOV 13-16, 2017
CL Irvine, CA
DE Spiking neural networks; recurrent neural networks; long short-term
   memory; neuromorphic hardware
ID NETWORKS
AB Due to the distributed and asynchronous nature of neural computation through low-energy spikes, brain-inspired hardware systems offer high energy efficiency and massive parallelism. One such platform is the IBM TrueNorth neurosynaptic system. Recently, TrueNorth compatible representation learning algorithms have emerged, achieving close to the state-of-the-art performance in various data sets. However, its application in temporal sequence processing models, such as recurrent neural networks (RNNs), is still only at the proof of concept level. There is an inherent difficulty in capturing temporal dynamics of an RNN using spiking neurons, which is only exasperated by the hardware constraints in connectivity and synaptic weight resolution. This paper presents a design flow that overcomes these difficulties and maps a special case of recurrent networks called long short-term memory (LSTM) onto a spike-based platform. The framework utilizes various approximation techniques, such as activation discretization, weight quantization, and scaling and rounding, spiking neural circuits that implement the complex gating mechanisms, and a store-and-release technique to enable neuron synchronization and faithful storage. While the presented techniques can be applied to map LSTM to any spiking neural network (SNN) simulator/emulator, here we choose the TrueNorth chip as the target platform by adhering to its hardware constraints. Three LSTM applications, parity check, extended Reber grammar, and question classification, are evaluated. The tradeoffs among accuracy, performance, and energy tradeoffs achieved on TrueNorth are demonstrated. This is compared with the performance on an SNN platform without hardware constraints, which represents the upper bound of the achievable accuracy.
C1 [Shrestha, Amar; Ahmed, Khadeer; Wang, Yanzhi; Qiu, Qinru] Syracuse Univ, Dept Elect Engn & Comp Sci, Syracuse, NY 13224 USA.
   [Ahmed, Khadeer] Synopsys, Mountain View, CA 94043 USA.
   [Widemann, David P.; Moody, Adam T.; Van Essen, Brian C.] Lawrence Livermore Natl Lab, Livermore, CA 94551 USA.
RP Shrestha, A (corresponding author), Syracuse Univ, Dept Elect Engn & Comp Sci, Syracuse, NY 13224 USA.
EM amshrest@syr.edu; kahmed@syr.edu; ywang393@syr.edu; widemann1@llnl.gov;
   moody20@llnl.gov; vanessen1@llnl.gov; qiqiu@syr.edu
CR Abadi M., 2015, TENSORFLOW LARGE SCA
   Ahmed K, 2016, IEEE IJCNN, P4286, DOI 10.1109/IJCNN.2016.7727759
   Amir A, 2013, IEEE IJCNN
   [Anonymous], 2016, BINARIZED NEURAL NET
   [Anonymous], 2015, DRAW RECURRENT NEURA
   [Anonymous], EFFECTIVE QUANTIZATI
   [Anonymous], 2015, BENCHMARKING LSTM NE
   [Anonymous], RECURRENT NEURAL NET
   [Anonymous], 2016, QUANTIZED NEURAL NET
   [Anonymous], 2014, P 2014 C EMP METH NA, DOI DOI 10.3115/V1/D14-1162
   [Anonymous], 2016, 2016 IEE INT C REB C, DOI [DOI 10.1109/ICRC.2016.7738691, 10.1109/ICRC.2016.7738691]
   [Anonymous], 2016, TERNARY WEIGHT NETWO
   [Anonymous], 2015, IMPROVING PERFORMANC
   BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181
   Bengio Yoshua, 2013, ABS13083432 CORR
   Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Cassidy AS, 2014, INT CONF HIGH PERFOR, P27, DOI 10.1109/SC.2014.8
   Chen QW, 2017, DES AUT TEST EUROPE, P205, DOI 10.23919/DATE.2017.7926983
   Chung J., 2014, NIPS 2014 WORKSHOP D, P1
   Diehl PU, 2016, IEEE IJCNN, P4278, DOI 10.1109/IJCNN.2016.7727758
   Diehl Peter U, 2015, 2015 INT JOINT C NEU, P1, DOI [DOI 10.1109/IJCNN.2015.7280696, 10.1109/IJCNN.2015.7280696]
   Esser SK, 2016, P NATL ACAD SCI USA, V113, P11441, DOI 10.1073/pnas.1604850113
   Gers FA, 2003, J MACH LEARN RES, V3, P115, DOI 10.1162/153244303768966139
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Hinton G., 2012, RMSPROP DIVIDE GRADI
   Khan MM, 2008, IEEE IJCNN, P2849, DOI 10.1109/IJCNN.2008.4634199
   Laudani A, 2015, COMPUT INTEL NEUROSC, V2015, DOI 10.1155/2015/818243
   Li X., 2002, P 19 INT C COMP LING, DOI DOI 10.3115/1072228.1072378
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Preissl R, 2012, INT CONF HIGH PERFOR
   Shrestha A, 2017, ICCAD-IEEE ACM INT, P631, DOI 10.1109/ICCAD.2017.8203836
   Sutskever I., 2014, ADV NEUR IN, P3104
   Venugopalan S, 2015, IEEE I CONF COMP VIS, P4534, DOI 10.1109/ICCV.2015.515
   Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935
   Zhou SC, 2017, J COMPUT SCI TECH-CH, V32, P667, DOI 10.1007/s11390-017-1750-y
NR 35
TC 5
Z9 5
U1 1
U2 16
PD DEC
PY 2018
VL 8
IS 4
BP 782
EP 795
DI 10.1109/JETCAS.2018.2856117
WC Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Evans, BD
   Stringer, SM
AF Evans, Benjamin D.
   Stringer, Simon M.
TI Transformation-invariant visual representations in self-organizing
   spiking neural networks
SO FRONTIERS IN COMPUTATIONAL NEUROSCIENCE
DT Article
DE transformation-invariant visual object recognition; integrate and fire;
   spiking neural net; continuous transformation learning; trace learning;
   inferior temporal cortex
ID TIMING-DEPENDENT PLASTICITY; OBJECT RECOGNITION; NEURONAL RESPONSES;
   SINGLE NEURONS; CORTEX; SYSTEM; SYNCHRONIZATION; SENSITIVITY;
   STATISTICS; FEATURES
AB The ventral visual pathway achieves object and face recognition by building transformation-invariant representations from elementary visual features. In previous computer simulation studies with rate-coded neural networks, the development of transformation-invariant representations has been demonstrated using either of two biologically plausible learning mechanisms, Trace learning and Continuous Transformation (CT) learning. However, it has not previously been investigated how transformation-invariant representations may be learned in a more biologically accurate spiking neural network. A key issue is how the synaptic connection strengths in such a spiking network might self-organize through Spike-Time Dependent Plasticity (STDP) where the change in synaptic strength is dependent on the relative times of the spikes emitted by the presynaptic and postsynaptic neurons rather than simply correlated activity driving changes in synaptic efficacy. Here we present simulations with conductance-based integrate-and-fire (IF) neurons using a STDP learning rule to address these gaps in our understanding. It is demonstrated that with the appropriate selection of model parameters and training regime, the spiking network model can utilize either Trace-like or CT-like learning mechanisms to achieve transform-invariant representations.
C1 [Evans, Benjamin D.; Stringer, Simon M.] Univ Oxford, Dept Expt Psychol, Ctr Theoret Neurosci & Artificial Intelligence, Oxford OX1 3UD, England.
RP Evans, BD (corresponding author), Univ Oxford, Dept Expt Psychol, Ctr Theoret Neurosci & Artificial Intelligence, S Parks Rd, Oxford OX1 3UD, England.
EM benjamin.evans@psy.ox.ac.uk
CR Abbott LF, 2000, NAT NEUROSCI, V3, P1178, DOI 10.1038/81453
   Amit DJ, 1997, NETWORK-COMP NEURAL, V8, P373, DOI 10.1088/0954-898X/8/4/003
   [Anonymous], 2005, INFORM THEORY INFERE
   Bell AJ, 1997, VISION RES, V37, P3327, DOI 10.1016/S0042-6989(97)00121-1
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   BIALEK W, 1991, SCIENCE, V252, P1854, DOI 10.1126/science.2063199
   Booth MCA, 1998, CEREB CORTEX, V8, P510, DOI 10.1093/cercor/8.6.510
   Dan Y, 2006, PHYSIOL REV, V86, P1033, DOI 10.1152/physrev.00030.2005
   David SV, 2004, J NEUROSCI, V24, P6991, DOI 10.1523/JNEUROSCI.1422-04.2004
   DESIMONE R, 1991, J COGNITIVE NEUROSCI, V3, P1, DOI 10.1162/jocn.1991.3.1.1
   Diesmann M, 1999, NATURE, V402, P529, DOI 10.1038/990101
   Elliffe MCM, 2002, BIOL CYBERN, V86, P59, DOI 10.1007/s004220100284
   Felsen G, 2005, PLOS BIOL, V3, P1819, DOI 10.1371/journal.pbio.0030342
   Felsen G, 2005, NAT NEUROSCI, V8, P1643, DOI 10.1038/nn1608
   FERSTER D, 1995, SCIENCE, V270, P756, DOI 10.1126/science.270.5237.756
   FOLDIAK P, 1993, COMPUTATION AND NEURAL SYSTEMS, P55
   Földiák P, 1991, NEURAL COMPUT, V3, P194, DOI 10.1162/neco.1991.3.2.194
   Fries P, 2002, J NEUROSCI, V22, P3739
   FUKUSHIMA K, 1988, NEURAL NETWORKS, V1, P119, DOI 10.1016/0893-6080(88)90014-7
   Gerstner W., 2006, SPIKING NEURON MODEL
   Hebb D. O., 1949, ORG BEHAV A NEUROPSY
   HUBEL DH, 1968, J PHYSIOL-LONDON, V195, P215, DOI 10.1113/jphysiol.1968.sp008455
   ITO M, 1995, J NEUROPHYSIOL, V73, P218, DOI 10.1152/jn.1995.73.1.218
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Kreiter AK, 1996, J NEUROSCI, V16, P2381
   KUWABARA N, 1993, J NEUROPHYSIOL, V69, P1713, DOI 10.1152/jn.1993.69.5.1713
   MAASS W., 1999, PULSED NEURAL NETWOR
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   Masquelier T, 2009, J NEUROSCI, V29, P13484, DOI 10.1523/JNEUROSCI.2207-09.2009
   MCCORMICK DA, 1985, J NEUROPHYSIOL, V54, P782, DOI 10.1152/jn.1985.54.4.782
   McCulloch Warren S., 1943, BULL MATH BIOPHYS, V5, P115, DOI 10.1007/BF02478259
   Mel BW, 1997, NEURAL COMPUT, V9, P777, DOI 10.1162/neco.1997.9.4.777
   Michler F, 2009, J NEUROPHYSIOL, V102, P953, DOI 10.1152/jn.90651.2008
   Op De Beeck H, 2000, J COMP NEUROL, V426, P505, DOI 10.1002/1096-9861(20001030)426:4<505::AID-CNE1>3.0.CO;2-M
   Perrinet L, 2001, NEUROCOMPUTING, V38, P817, DOI 10.1016/S0925-2312(01)00460-X
   Perrinet L., 2003, THESIS U P SABATIER
   Quiroga RQ, 2005, NATURE, V435, P1102, DOI 10.1038/nature03687
   Riesenhuber M, 1999, NAT NEUROSCI, V2, P1019, DOI 10.1038/14819
   Rolls E. T., 1998, NEURAL NETWORKS BRAI
   Rolls ET, 1997, EXP BRAIN RES, V114, P149, DOI 10.1007/PL00005615
   Rolls ET, 2000, NEURAL COMPUT, V12, P2547, DOI 10.1162/089976600300014845
   Síma J, 2003, NEURAL COMPUT, V15, P2727, DOI 10.1162/089976603322518731
   Simoncelli EP, 2003, CURR OPIN NEUROBIOL, V13, P144, DOI 10.1016/S0959-4388(03)00047-3
   Stringer SM, 2006, BIOL CYBERN, V94, P128, DOI 10.1007/s00422-005-0030-z
   Stringer SM, 2000, NEURAL NETWORKS, V13, P305, DOI 10.1016/S0893-6080(00)00017-4
   Tanaka K, 1996, NEURAL NETWORKS, V9, P1459, DOI 10.1016/S0893-6080(96)00045-7
   TANAKA K, 1991, J NEUROPHYSIOL, V66, P170, DOI 10.1152/jn.1991.66.1.170
   Thorpe S, 1996, NATURE, V381, P520, DOI 10.1038/381520a0
   Thorpe SJ, 2000, ISCAS 2000: IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS - PROCEEDINGS, VOL IV, P405, DOI 10.1109/ISCAS.2000.858774
   TOVEE MJ, 1994, J NEUROPHYSIOL, V72, P1049, DOI 10.1152/jn.1994.72.3.1049
   Troyer TW, 1998, J NEUROSCI, V18, P5908
   van Hateren JH, 1998, P ROY SOC B-BIOL SCI, V265, P359, DOI 10.1098/rspb.1998.0303
   van Rossum MCW, 2000, J NEUROSCI, V20, P8812
   Van Rullen R, 2001, NEURAL COMPUT, V13, P1255, DOI 10.1162/08997660152002852
   Wallis G, 1997, PROG NEUROBIOL, V51, P167, DOI 10.1016/S0301-0082(96)00054-8
NR 55
TC 18
Z9 18
U1 0
U2 11
PD JUL 25
PY 2012
VL 6
AR 46
DI 10.3389/fncom.2012.00046
WC Mathematical & Computational Biology; Neurosciences
DA 2023-11-11
ER

PT C
AU Slepova, LO
   Skripnik, TN
AF Slepova, Liudmila O.
   Skripnik, Tatiana N.
GP IEEE
TI Neurotechnologies in Signal Processing for Implementation of Interaction
   Between Electronic Sensors and the Nervous System
SO PROCEEDINGS OF THE 2019 IEEE CONFERENCE OF RUSSIAN YOUNG RESEARCHERS IN
   ELECTRICAL AND ELECTRONIC ENGINEERING (EICONRUS)
SE IEEE NW Russia Young Researchers in Electrical and Electronic
   Engineering Conference
DT Proceedings Paper
CT IEEE Conference of Russian Young Researchers in Electrical and
   Electronic Engineering (EIConRus)
CY JAN 28-31, 2019
CL St Petersburg Electrotechn Univ, RUSSIA
HO St Petersburg Electrotechn Univ
DE action potential; biological model; odor classification; spiking neural
   network; pattern recognition; STDP; neural prostheses
AB Over the years of research and development in the field of neurotechnologies, a great variety of artificial neural networks has been created, to some extent simulating the functionality of biological neural systems. The neuron of the spike neural network is considered. The priority areas of use of this type of neural networks are: spatial navigation and environmental analysis; analysis of smells and dynamic graphic and sound information; neuroprosthetics and neural interfaces. There are a huge number of simplified neural models, but most of them still require large resources. The paper discusses the main advantages and disadvantages of the considered neural networks.
C1 [Slepova, Liudmila O.] ITMO Univ, Fac Control Syst & Robot, St Petersburg, Russia.
   [Skripnik, Tatiana N.] St Petersburg State Marine Tech Univ, Dept Marine Elect, St Petersburg, Russia.
RP Skripnik, TN (corresponding author), St Petersburg State Marine Tech Univ, Dept Marine Elect, St Petersburg, Russia.
EM marine_electronics@corp.smtu.ru
CR Dang B, 2018, AIP CONF PROC, V2034, DOI 10.1063/1.5067350
   Ivanov AV, 2018, PROCEEDINGS OF THE 2018 IEEE CONFERENCE OF RUSSIAN YOUNG RESEARCHERS IN ELECTRICAL AND ELECTRONIC ENGINEERING (EICONRUS), P882, DOI 10.1109/EIConRus.2018.8317229
   Kotlyarevskaya MV, 2018, PROCEEDINGS OF THE 2018 IEEE CONFERENCE OF RUSSIAN YOUNG RESEARCHERS IN ELECTRICAL AND ELECTRONIC ENGINEERING (EICONRUS), P894, DOI 10.1109/EIConRus.2018.8317232
   Popov AV, 2018, PROCEEDINGS OF THE 2018 IEEE CONFERENCE OF RUSSIAN YOUNG RESEARCHERS IN ELECTRICAL AND ELECTRONIC ENGINEERING (EICONRUS), P962, DOI 10.1109/EIConRus.2018.8317249
   Slepova LO, 2018, PROCEEDINGS OF THE 2018 IEEE CONFERENCE OF RUSSIAN YOUNG RESEARCHERS IN ELECTRICAL AND ELECTRONIC ENGINEERING (EICONRUS), P992, DOI 10.1109/EIConRus.2018.8317256
   Zhilenkov Anton, 2016, Vibroengineering Procedia. 22nd International Conference on Vibroengineering, P17
   Zhilenkov AA, 2018, PROCEEDINGS OF THE 2018 IEEE CONFERENCE OF RUSSIAN YOUNG RESEARCHERS IN ELECTRICAL AND ELECTRONIC ENGINEERING (EICONRUS), P1040, DOI 10.1109/EIConRus.2018.8317267
   Zhilenkov AA, 2017, IEEE NW RUSS YOUNG, P1100, DOI 10.1109/EIConRus.2017.7910747
NR 8
TC 0
Z9 0
U1 0
U2 1
PY 2019
BP 1237
EP 1239
DI 10.1109/eiconrus.2019.8657125
WC Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Kulkarni, SR
   Alexiades, JM
   Rajendran, B
AF Kulkarni, Shruti R.
   Alexiades, John M.
   Rajendran, Bipin
GP IEEE
TI Live Demonstration: Image Classification Using Bio-inspired Spiking
   Neural Networks
SO 2018 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS (ISCAS)
SE IEEE International Symposium on Circuits and Systems
DT Proceedings Paper
CT IEEE International Symposium on Circuits and Systems (ISCAS)
CY MAY 27-30, 2018
CL Florence, ITALY
AB We present a live demonstration of an image classification system using bio-inspired Spiking Neural Networks. Our network is three-layered and is trained with the images from the MNIST database, achieving an accuracy of 98:06%. Synapses connecting the output layer neurons obey the spike based weight-adaptation rule using the supervised learning algorithm called NormAD. This network, implemented on a graphical processing unit (GPU), is used to classify digits drawn by users on a touch-screen interface in real-time. The spike propagation maps generated and displayed by the platform reveal key insights about information processing mechanisms of the brain.
C1 [Kulkarni, Shruti R.; Alexiades, John M.; Rajendran, Bipin] New Jersey Inst Technol, Dept Elect & Comp Engn, Newark, NJ 07102 USA.
RP Rajendran, B (corresponding author), New Jersey Inst Technol, Dept Elect & Comp Engn, Newark, NJ 07102 USA.
EM bipin@njit.edu
CR Kulkarni SR, 2017, IEEE I C ELECT CIRC, P128, DOI 10.1109/ICECS.2017.8292015
NR 1
TC 0
Z9 0
U1 0
U2 1
PY 2018
DI 10.1109/ISCAS.2018.8351810
WC Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Camuñas-Mesa, LA
   Linares-Barranco, B
   Serrano-Gotarredona, T
AF Camunas-Mesa, Luis A.
   Linares-Barranco, Bernabe
   Serrano-Gotarredona, Teresa
GP IEEE
TI Implementation of a tunable spiking neuron for STDP with memristors in
   FDSOI 28nm
SO 2020 2ND IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE
   CIRCUITS AND SYSTEMS (AICAS 2020)
DT Proceedings Paper
CT 2nd IEEE International Conference on Artificial Intelligence Circuits
   and Systems (AICAS)
CY AUG 31-SEP 04, 2020
CL ELECTR NETWORK
DE Spiking Neural Networks; Event-driven processing; STDP; Memristors
ID ARCHITECTURE
AB Hybrid memristor-CMOS techniques have been recently proposed to build large-scale neural networks with learning capabilities. The intrinsic characteristics of memristors make them specially suited to implement synaptic connections between layers of spiking neurons, undergoing STDP learning (Spike-Timing-Dependent Plasticity) mechanisms when processing spikes with particular shapes. In a previous work, we proposed a tunable spiking neuron circuit which can generate spikes with controllable shape. In this work, the spike generator circuit has been implemented in FDSOI 28nm technology, and it has demonstrated its capability to produce spikes with pulse widths in the range between 8 mu s and 100ms.
C1 [Camunas-Mesa, Luis A.; Linares-Barranco, Bernabe; Serrano-Gotarredona, Teresa] CSIC, Inst Microelect Sevilla IMSE CNM, Seville, Spain.
   [Camunas-Mesa, Luis A.; Linares-Barranco, Bernabe; Serrano-Gotarredona, Teresa] Univ Seville, Seville, Spain.
RP Camuñas-Mesa, LA (corresponding author), CSIC, Inst Microelect Sevilla IMSE CNM, Seville, Spain.; Camuñas-Mesa, LA (corresponding author), Univ Seville, Seville, Spain.
EM camunas@imse-cnm.csic.es; bernabe@imse-cnm.csic.es;
   terese@imse-cnm.csic.es
CR Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Bouvier M, 2019, ACM J EMERG TECH COM, V15, DOI 10.1145/3304103
   Camuñas-Mesa LA, 2019, MATERIALS, V12, DOI 10.3390/ma12172745
   CHUA LO, 1971, IEEE T CIRCUITS SYST, VCT18, P507, DOI 10.1109/TCT.1971.1083337
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Frenkel C, 2019, IEEE T BIOMED CIRC S, V13, P145, DOI 10.1109/TBCAS.2018.2880425
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   GERSTNER W, 1993, BIOL CYBERN, V69, P503, DOI 10.1007/BF01185422
   Indiveri G, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00073
   Linares-Barranco B., 2009, NAT PREC, DOI DOI 10.1038/NPRE.2009.3010.1
   Masquelier T, 2008, PLOS ONE, V3, DOI 10.1371/journal.pone.0001377
   Masquelier T, 2009, NEURAL COMPUT, V21, P1259, DOI 10.1162/neco.2008.06-08-804
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Moradi S, 2018, IEEE T BIOMED CIRC S, V12, P106, DOI 10.1109/TBCAS.2017.2759700
   Neckar A, 2019, P IEEE, V107, P144, DOI 10.1109/JPROC.2018.2881432
   Schemmel J, 2010, IEEE INT SYMP CIRC S, P1947, DOI 10.1109/ISCAS.2010.5536970
   Serrano-Gotarredona T, 2012, IEEE I C ELECT CIRC, P949, DOI 10.1109/ICECS.2012.6463504
   Shen JC, 2016, SCI CHINA INFORM SCI, V59, DOI 10.1007/s11432-015-5511-7
   Strukov DB, 2008, NATURE, V453, P80, DOI 10.1038/nature06932
   Zamarreño-Ramos C, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00026
NR 21
TC 3
Z9 3
U1 1
U2 2
PY 2020
BP 94
EP 98
DI 10.1109/aicas48895.2020.9073994
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Wang, RCM
   Hamilton, TJ
   Tapson, JC
   van Schaik, A
AF Wang, Runchun M.
   Hamilton, Tara J.
   Tapson, Jonathan C.
   van Schaik, Andre
TI A mixed-signal implementation of a polychronous spiking neural network
   with delay adaptation
SO FRONTIERS IN NEUROSCIENCE
DT Article
DE mixed-signal implementation; polychronous spiking neural network; analog
   implementation; multiplexed neuron array; neuromorphic engineering
ID COMPUTATION; MODEL; CMOS
AB We present a mixed-signal implementation of a re-configurable polychronous spiking neural network capable of storing and recalling spatio-temporal patterns. The proposed neural network contains one neuron array and one axon array. Spike Timing Dependent Delay Plasticity is used to fine-tune delays and add dynamics to the network. In our mixed-signal implementation, the neurons and axons have been implemented as both analog and digital circuits. The system thus consists of one FPGA, containing the digital neuron array and the digital axon array, and one analog IC containing the analog neuron array and the analog axon array. The system can be easily configured to use different combinations of each. We present and discuss the experimental results of all combinations of the analog and digital axon arrays and the analog and digital neuron arrays. The test results show that the proposed neural network is capable of successfully recalling more than 85% of stored patterns using both analog and digital circuits.
C1 [Wang, Runchun M.; Hamilton, Tara J.; Tapson, Jonathan C.; van Schaik, Andre] Univ Western Sydney, MARCS Inst, Sydney, NSW, Australia.
RP Wang, RCM (corresponding author), Univ Western Sydney, MARCS Inst, Locked Bag 1797, Penrith, NSW 2751, Australia.
EM mark.wang@uws.edu.au
CR [Anonymous], 2012, IEEE INT JOINT C NEU, DOI [DOI 10.1109/IJCNN.2012.6252636, DOI 10.5167/UZH-75361]
   Arthur JV, 2004, IEEE IJCNN, P1699
   Basu A, 2010, IEEE T BIOMED CIRC S, V4, P311, DOI 10.1109/TBCAS.2010.2055157
   Boahen KA, 2000, IEEE T CIRCUITS-II, V47, P416, DOI 10.1109/82.842110
   Brink S, 2013, IEEE T BIOMED CIRC S, V7, P71, DOI 10.1109/TBCAS.2012.2197858
   Cauwenberghs G, 1996, ISCAS 96: 1996 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS - CIRCUITS AND SYSTEMS CONNECTING THE WORLD, VOL 3, P334, DOI 10.1109/ISCAS.1996.541601
   Dowrick T, 2013, NEUROCOMPUTING, V108, P79, DOI 10.1016/j.neucom.2012.12.004
   Gao CJ, 2007, IEEE T CIRCUITS-I, V54, P2502, DOI 10.1109/TCSI.2007.907830
   Gerstner W, 1996, NATURE, V383, P76, DOI 10.1038/383076a0
   Goldberg DH, 2001, NEURAL NETWORKS, V14, P781, DOI 10.1016/S0893-6080(01)00057-0
   Harkin J, 2009, INT J RECONFIGURABLE, V2009, DOI 10.1155/2009/908740
   Harkin J, 2008, I C FIELD PROG LOGIC, P482
   Hasler J, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00118
   HORIO Y, 1990, 1990 IEEE INTERNATIONAL SYMP ON CIRCUITS AND SYSTEMS, VOLS 1-4, P2986, DOI 10.1109/ISCAS.1990.112638
   Hussain S., NEUROCOMPUT IN PRESS, P1
   Hussain S, 2012, 2012 IEEE ASIA PACIFIC CONFERENCE ON CIRCUITS AND SYSTEMS (APCCAS), P304, DOI 10.1109/APCCAS.2012.6419032
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Johansson C, 2007, NEURAL NETWORKS, V20, P48, DOI 10.1016/j.neunet.2006.05.029
   Levy WB, 1996, NEURAL COMPUT, V8, P531, DOI 10.1162/neco.1996.8.3.531
   Liu S-C, 2002, ANALOG VLSI CIRCUITS
   Masuda N, 2003, NEURAL COMPUT, V15, P103, DOI 10.1162/089976603321043711
   Mihalas S, 2009, NEURAL COMPUT, V21, P704, DOI 10.1162/neco.2008.12-07-680
   Minkovich K, 2012, IEEE T NEUR NET LEAR, V23, P889, DOI 10.1109/TNNLS.2012.2191795
   Mirhassani M, 2007, MICROELECTRON ENG, V84, P300, DOI 10.1016/j.mee.2006.02.014
   Python D, 2001, IEEE J SOLID-ST CIRC, V36, P1067, DOI 10.1109/4.933462
   Saighi S., 2010, 2010 INT JOINT C NEU, P1, DOI [10.1109/IJCNN.2010.5596979, DOI 10.1109/IJCNN.2010.5596979]
   Schemmel J, 2008, IEEE IJCNN, P431, DOI 10.1109/IJCNN.2008.4633828
   Scholze S, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00117
   Sheik Sadique, 2013, Biomimetic and Biohybrid Systems. Second International Conference, Living Machines 2013. Proceedings. LNCS 8064, P262, DOI 10.1007/978-3-642-39802-5_23
   Van Rullen R, 2001, NEURAL COMPUT, V13, P1255, DOI 10.1162/08997660152002852
   Vogelstein RJ, 2007, IEEE T NEURAL NETWOR, V18, P253, DOI 10.1109/TNN.2006.883007
   Wang R., 2013, 2013 IEEE INT S CIRC, P2
   Wang R., 2011, 2011 7 INT C INT SEN, P97, DOI [10.1109/ISSNIP.2011.6146572, DOI 10.1109/ISSNIP.2011.6146572]
   Wang RC, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00014
   Weste N. H. E., 2005, CMOS VLSI DESIGN CIR, V3rd
   Yu T, 2010, IEEE INT SYMP CIRC S, P2558, DOI 10.1109/ISCAS.2010.5537114
   Zaveri MS, 2011, NEURAL NETWORKS, V24, P291, DOI 10.1016/j.neunet.2010.12.003
NR 38
TC 21
Z9 21
U1 1
U2 1
PD MAR 18
PY 2014
VL 8
DI 10.3389/fnins.2014.00051
WC Neurosciences
DA 2023-11-11
ER

PT C
AU Sporea, I
   Grüning, A
AF Sporea, Ioana
   Gruening, Andre
GP IEEE
TI Reference time in SpikeProp
SO 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUL 31-AUG 05, 2011
CL San Jose, CA
ID SPIKING NEURONS; NETWORKS
AB Although some studies have been done on the learning algorithm for spiking neural networks SpikeProp, little has been mentioned about the required input bias neuron that sets the reference time start. This paper examines the importance of the reference time in neural networks based on temporal encoding. The findings refute previous assumptions about the reference start time.
C1 [Sporea, Ioana; Gruening, Andre] Univ Surrey, Dept Comp, FEPS, Guildford GU2 7XH, Surrey, England.
RP Sporea, I (corresponding author), Univ Surrey, Dept Comp, FEPS, Guildford GU2 7XH, Surrey, England.
EM i.nica@surrey.ac.uk; a.gruning@surrey.ac.uk
CR [Anonymous], P 15 PRORISC WORKSH
   [Anonymous], 2001, HDB BIOL PHYS
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Booij O, 2005, INFORM PROCESS LETT, V95, P552, DOI 10.1016/j.ipl.2005.05.023
   Fujita M., 2008, P IEEE INT JOINT C N, P840
   Maass W, 1997, NEURAL COMPUT, V9, P279, DOI 10.1162/neco.1997.9.2.279
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maass W, 1997, ADV NEUR IN, V9, P211
   McKennoch S, 2006, IEEE IJCNN, P3970
   Minsky M., 1969, PERCEPTRONS INTRO CO
   MOORE S, 2002, THESIS U BATH
   Rojas R., 1996, NEURAL NETWORKS, P149, DOI 10.1007/978-3-642-61068-4_7
   Takase Haruhiko, 2009, Proceedings 2009 International Joint Conference on Neural Networks (IJCNN 2009 - Atlanta), P3062, DOI 10.1109/IJCNN.2009.5178756
   THORPE SJ, 1989, CONNECTIONISM IN PERSPECTIVE, P63
   Xin JG, 2001, IEEE IJCNN, P1772, DOI 10.1109/IJCNN.2001.938430
NR 15
TC 2
Z9 2
U1 0
U2 0
PY 2011
BP 1090
EP 1092
WC Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Computer Science, Theory & Methods; Engineering, Electrical &
   Electronic
DA 2023-11-11
ER

PT J
AU Fidjeland, AK
   Gamez, D
   Shanahan, MP
   Lazdins, E
AF Fidjeland, Andreas K.
   Gamez, David
   Shanahan, Murray P.
   Lazdins, Edgars
TI Three Tools for the Real-Time Simulation of Embodied Spiking Neural
   Networks Using GPUs
SO NEUROINFORMATICS
DT Article
DE Simulation; 3D visualization; Spike encoding; Robotics; Spiking neural
   networks; iCub; GPU
ID 2-DIMENSIONAL LIMB MOVEMENTS; MUSCLE-SPINDLE FEEDBACK; NEURONS; BRAIN;
   INFORMATION; HUMANS; MODEL; SKIN; ARCHITECTURE; AFFERENTS
AB This paper presents a toolbox of solutions that enable the user to construct biologically-inspired spiking neural networks with tens of thousands of neurons and millions of connections that can be simulated in real time, visualized in 3D and connected to robots and other devices. NeMo is a high performance simulator that works with a variety of neural and oscillator models and performs parallel simulations on either GPUs or multi-core processors. SpikeStream is a visualization and analysis environment that works with NeMo and can construct networks, store them in a database and visualize their activity in 3D. The iSpike library provides biologically-inspired conversion between real data and spike representations to support work with robots, such as the iCub. Each of the tools described in this paper can be used independently with other software, and they also work well together.
C1 [Fidjeland, Andreas K.; Gamez, David; Shanahan, Murray P.; Lazdins, Edgars] Univ London Imperial Coll Sci Technol & Med, London SW7 2AZ, England.
RP Fidjeland, AK (corresponding author), Univ London Imperial Coll Sci Technol & Med, 180 Queens Gate, London SW7 2AZ, England.
EM andreas.fidjeland@imperial.ac.uk
CR Aleksander I., 2005, WORLD MY MIND MY MIN
   Anand R, 2009, SELF-DEFENSE IN INTERNATIONAL RELATIONS, P1, DOI 10.1057/9780230245747
   Andreou A., 1994, MIDW S CIRC SYST, P97
   [Anonymous], 2011, FRONT NEUROINFORM
   Bergenheim M, 2000, EXP BRAIN RES, V134, P301, DOI 10.1007/s002210000471
   Bernardet U, 2010, NEUROINFORMATICS, V8, P113, DOI 10.1007/s12021-010-9069-7
   Bernhard F, 2006, LECT NOTES COMPUT SC, V3994, P236
   Bhowmik D., 2012, P INT JOINT C NEUR N
   Binzegger T, 2004, J NEUROSCI, V24, P8441, DOI 10.1523/JNEUROSCI.1400-04.2004
   Bolduc M, 1998, COMPUT VIS IMAGE UND, V69, P170, DOI 10.1006/cviu.1997.0560
   Bouganis A., 2010, P IEEE INT JOINT C N, P4104
   Bower J. M., 2003, HDB BRAIN THEORY NEU, P475
   Brette R, 2007, J COMPUT NEUROSCI, V23, P349, DOI 10.1007/s10827-007-0038-6
   Buchmann T., 2011, THESIS IMPERIAL COLL
   Cannata Giorgio, 2008, 2008 IEEE International Conference on Multisensor Fusion and Integration for Intelligent Systems (MFI 2008), P434, DOI 10.1109/MFI.2008.4648033
   Carnevale T., 2006, NEURON BOOK, DOI DOI 10.1017/CBO9780511541612
   Clark A., 2008, SUPERSIZING MIND EMB
   Collins DF, 1996, J PHYSIOL-LONDON, V496, P857, DOI 10.1113/jphysiol.1996.sp021733
   Collins DF, 2005, J NEUROPHYSIOL, V94, P1699, DOI 10.1152/jn.00191.2005
   Davison A.P., 2008, FRONTIERS NEUROINFOR, V2
   Djurfeldt M, 2008, IBM J RES DEV, V52, P31, DOI 10.1147/rd.521.0031
   EDIN BB, 1995, J PHYSIOL-LONDON, V487, P243, DOI 10.1113/jphysiol.1995.sp020875
   ENROTHCUGELL C, 1966, J PHYSIOL-LONDON, V187, P517, DOI 10.1113/jphysiol.1966.sp008107
   FERRELL WR, 1987, J PHYSIOL-LONDON, V386, P63, DOI 10.1113/jphysiol.1987.sp016522
   Fidjeland A.K., 2011, NEMO MANUAL
   Fidjeland AK, 2010, P IEEE WORLD C COMP, P536
   Fidjeland AK, 2009, IEEE INT CONF ASAP, P137, DOI 10.1109/ASAP.2009.24
   Fitzpatrick P, 2008, ROBOT AUTON SYST, V56, P29, DOI 10.1016/j.robot.2007.09.014
   Fountas Z, 2011, IEEE CONF COMPU INTE, P350, DOI 10.1109/CIG.2011.6032027
   Gamez D, 2012, BIOINSPIR BIOMIM, V7, DOI 10.1088/1748-3182/7/2/025008
   Gamez D., 2011, ISPIKE MANUAL
   Gamez D, 2006, P IEEE 5 CHAPT C ADV, P85
   Gamez D., 2011, SPIKESTREAM MANUAL
   Gamez D, 2007, LECT NOTES COMPUT SC, V4668, P360
   Gamez D, 2011, CONSCIOUS COGN, V20, P1403, DOI 10.1016/j.concog.2011.05.016
   Gamez D, 2010, CONSCIOUS COGN, V19, P294, DOI 10.1016/j.concog.2009.11.001
   GEORGOPOULOS AP, 1986, SCIENCE, V233, P1416, DOI 10.1126/science.3749885
   Gewaltig M-O., 2007, SCHOLARPEDIA, V2, DOI [10.4249/scholarpedia.1430, DOI 10.4249/SCHOLARPEDIA.1430]
   Goodman DFM, 2010, NEUROINFORMATICS, V8, P183, DOI 10.1007/s12021-010-9082-x
   Goodman DFM, 2009, FRONT NEUROSCI-SWITZ, V3, P192, DOI 10.3389/neuro.01.026.2009
   Grill-Spector K, 2004, ANNU REV NEUROSCI, V27, P649, DOI 10.1146/annurev.neuro.27.070203.144220
   Grillner S, 2005, TRENDS NEUROSCI, V28, P364, DOI 10.1016/j.tins.2005.05.004
   Hagmann P, 2008, PLOS BIOL, V6, P1479, DOI 10.1371/journal.pbio.0060159
   Hammarlund P, 1998, J COMPUT NEUROSCI, V5, P443, DOI 10.1023/A:1008893429695
   Han B., 2010, P INT JOINT C NEUR N, P3050
   Hellwig B, 2000, BIOL CYBERN, V82, P111, DOI 10.1007/PL00007964
   Hoshi T, 2006, IEEE INT CONF ROBOT, P3463, DOI 10.1109/ROBOT.2006.1642231
   Ijspeert AJ, 2007, SCIENCE, V315, P1416, DOI 10.1126/science.1138353
   Indiveri G, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00073
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2008, P NATL ACAD SCI USA, V105, P3593, DOI 10.1073/pnas.0712231105
   Jones KE, 2001, J PHYSIOL-LONDON, V536, P635, DOI 10.1111/j.1469-7793.2001.0635c.xd
   JURGENS R, 1981, BIOL CYBERN, V39, P87, DOI 10.1007/BF00336734
   Kit Cheung, 2009, Proceedings of the 2009 International Conference on Field-Programmable Technology (FPT 2009), P247, DOI 10.1109/FPT.2009.5377667
   Krichmar JL, 2005, P NATL ACAD SCI USA, V102, P2111, DOI 10.1073/pnas.0409792102
   Kuramoto Y., 2003, CHEM OSCILLATIONS WA
   Linares-Barranco A, 2007, IEEE INT SYMP CIRC S, P1192, DOI 10.1109/ISCAS.2007.378265
   Liu JD, 2006, INT J AUTOM COMPUT, V3, P336, DOI 10.1007/s11633-006-0336-x
   Lyon R. F., 1982, Proceedings of ICASSP 82. IEEE International Conference on Acoustics, Speech and Signal Processing, P1282
   MACEFIELD G, 1990, J PHYSIOL-LONDON, V429, P113, DOI 10.1113/jphysiol.1990.sp018247
   Maguire LP, 2007, NEUROCOMPUTING, V71, P13, DOI 10.1016/j.neucom.2006.11.029
   Markram H, 2006, NAT REV NEUROSCI, V7, P153, DOI 10.1038/nrn1848
   Marques H., 2010, INT C HUM ROB, P391, DOI DOI 10.1109/ICHR.2010.5686344
   Masquelier T, 2007, PLOS COMPUT BIOL, V3, P247, DOI 10.1371/journal.pcbi.0030031
   Metta G., 2008, P WORKSH PERF METR I
   Meuth Ryan J., 2007, 22nd IEEE International Symposium on Intelligent Control, ISIC 2007. Part of IEEE Multi-conference on Systems and Control, P524, DOI 10.1109/ISIC.2007.4450940
   Morrison A, 2008, BIOL CYBERN, V98, P459, DOI 10.1007/s00422-008-0233-1
   Nageswaran JM, 2009, NEURAL NETWORKS, V22, P791, DOI 10.1016/j.neunet.2009.06.028
   Noë A, 2004, J CONSCIOUSNESS STUD, V11, P3
   Nowotny T., 2011, BMC NEUROSCI, V12, P239
   Rast A, 2011, NEURAL NETWORKS, V24, P961, DOI 10.1016/j.neunet.2011.06.014
   Ribot-Ciscar E, 2003, EXP BRAIN RES, V149, P512, DOI 10.1007/s00221-003-1384-x
   ROBINSON DA, 1964, J PHYSIOL-LONDON, V174, P245, DOI 10.1113/jphysiol.1964.sp007485
   Roll JP, 2000, EXP BRAIN RES, V134, P311, DOI 10.1007/s002210000472
   Roll JP, 2004, EXP BRAIN RES, V157, P359, DOI 10.1007/s00221-004-1853-x
   Rossant C, 2011, FRONT NEUROSCI-SWITZ, V5, DOI [10.3389/fnins.2011.00009, 10.3389/fninf.2011.00009]
   Schemmel J, 2010, IEEE INT SYMP CIRC S, P1947, DOI 10.1109/ISCAS.2010.5536970
   SCHWARTZ EL, 1980, VISION RES, V20, P645, DOI 10.1016/0042-6989(80)90090-5
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Sporns O., 2007, SCHOLARPEDIA, V2, P4695, DOI [DOI 10.4249/SCHOLARPEDIA.4695, 10.4249/scholarpedia.4695]
   Thomas D.B., 2009, P IEEE S FIELD PROGR
   Tiesel JP, 2009, IEEE IJCNN, P754
   Tononi G, 2008, BIOL BULL-US, V215, P216, DOI 10.2307/25470707
   Vogels TP, 2005, J NEUROSCI, V25, P10786, DOI 10.1523/JNEUROSCI.3508-05.2005
   Yudanov D., 2010, P INT JOINT C NEUR N
NR 85
TC 4
Z9 4
U1 0
U2 8
PD JUL
PY 2013
VL 11
IS 3
BP 267
EP 290
DI 10.1007/s12021-012-9174-x
WC Computer Science, Interdisciplinary Applications; Neurosciences
DA 2023-11-11
ER

PT J
AU Gholami, M
   Farsa, EZ
   Karimi, G
AF Gholami, Morteza
   Farsa, Edris Zaman
   Karimi, Gholamreza
TI Reconfigurable field-programmable gate array-based on-chip learning
   neuromorphic digital implementation for nonlinear function approximation
SO INTERNATIONAL JOURNAL OF CIRCUIT THEORY AND APPLICATIONS
DT Article
DE digital hardware; field-programmable gate array (FPGA); hardware
   implementation; neuromorphic; on-chip learning; spiking neural networks
   (SNNs)
ID SPIKING NEURAL-NETWORKS; LOW-ERROR; HARDWARE; MODEL; PROCESSOR; NEURONS;
   DESIGN
AB Hardware implementations of spiking neural networks, which are known as neuromorphic architectures, provide an explicit understanding of brain performance. As a result, biological features of the brain may well inspire the next generation of computers and electronic systems used in such areas as signal processing, image processing, function approximation, and pattern recognition. Approximating nonlinear functions has many uses in computer science and applied mathematics. The sigmoid is the most universal activation function in neural networks by which the relationship between biological and artificial neurons is defined. It is a suitable option for predicting the probability of anything from 0 to 1 as output. In this paper, a spiking neural network using Izhikevich neurons and a gradient descent learning algorithm are propounded to approximate the sigmoid and other nonlinear functions. The flexibility of the spiking network is demonstrated by showing the average relative errors in the approximation process. A time- and cost-efficient digital neuromorphic implementation on the base of on-chip learning method for approximating the sigmoid function is also discussed. The paper reports the results of the hardware synthesis and the spiking network's physical implementation on a field-programmable gate array. The maximum frequency and throughput of the implemented network were 83.209 MHz and 9.86 Mb/s, respectively.
C1 [Gholami, Morteza; Karimi, Gholamreza] Razi Univ, Elect Engn Dept, Fac Engn, Kermanshah, Iran.
   [Farsa, Edris Zaman] Islamic Azad Univ, Dept Comp Engn, Sanandaj Branch, Sanandaj, Iran.
RP Karimi, G (corresponding author), Razi Univ, Elect Engn Dept, Fac Engn, Kermanshah, Iran.
EM ghkarimi@razi.ac.ir
CR Abdoli B, 2020, INT J CIRC THEOR APP, V48, P2141, DOI 10.1002/cta.2877
   Amin HH, 2005, LECT NOTES COMPUT SC, V3644, P621
   Amiri M, 2019, INT J CIRC THEOR APP, V47, P483, DOI 10.1002/cta.2596
   Armato A, 2011, MICROPROCESS MICROSY, V35, P557, DOI 10.1016/j.micpro.2011.05.007
   Asgari H, 2020, IEEE T CIRCUITS-II, V67, P2697, DOI 10.1109/TCSII.2020.2968588
   Asgari H, 2020, INT J CIRC THEOR APP, V48, P724, DOI 10.1002/cta.2753
   Azghadi MR, 2017, IEEE T BIOMED CIRC S, V11, P434, DOI 10.1109/TBCAS.2016.2618351
   Bajger M, 2008, J SIGNAL PROCESS SYS, V52, P137, DOI 10.1007/s11265-007-0140-z
   Basu A, 2010, IEEE T BIOMED CIRC S, V4, P311, DOI 10.1109/TBCAS.2010.2055157
   Bonabi SY, 2014, FRONT NEUROSCI-SWITZ, V8, DOI 10.3389/fnins.2014.00379
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   DeBole MV, 2019, COMPUTER, V52, P20, DOI 10.1109/MC.2019.2903009
   Farsa EZ, 2019, IEEE T CIRCUITS-II, V66, P1582, DOI 10.1109/TCSII.2019.2890846
   Farsa EZ, 2015, J COMPUT ELECTRON, V14, P707, DOI 10.1007/s10825-015-0709-x
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   Heidarpur M, 2019, IEEE T CIRCUITS-I, V66, P2651, DOI 10.1109/TCSI.2019.2899356
   Iannella N, 2001, NEURAL NETWORKS, V14, P933, DOI 10.1016/S0893-6080(01)00080-6
   Indiveri G, 2006, IEEE T NEURAL NETWOR, V17, P211, DOI 10.1109/TNN.2005.860850
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Karimi G, 2018, INT J CIRC THEOR APP, V46, P965, DOI 10.1002/cta.2457
   Kugele A, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00439
   Lammie C, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351532
   Ma D, 2017, J SYST ARCHITECT, V77, P43, DOI 10.1016/j.sysarc.2017.01.003
   Maguire LP, 2007, NEUROCOMPUTING, V71, P13, DOI 10.1016/j.neucom.2006.11.029
   Nawrocki RA, 2016, IEEE T ELECTRON DEV, V63, P3819, DOI 10.1109/TED.2016.2598413
   Neil D, 2014, IEEE T VLSI SYST, V22, P2621, DOI 10.1109/TVLSI.2013.2294916
   Pani D, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00090
   Ponulak F, 2011, ACTA NEUROBIOL EXP, V71, P409
   Rajendran B, 2019, IEEE SIGNAL PROC MAG, V36, P97, DOI 10.1109/MSP.2019.2933719
   Schumann CL, 2019, AIDS BEHAV, V23, P5, DOI 10.1007/s10461-017-1727-4
   Serrano-Gotarredona T, 2013, IEEE CIRC SYST MAG, V13, P74, DOI 10.1109/MCAS.2013.2256271
   Young AR, 2019, IEEE ACCESS, V7, P135606, DOI 10.1109/ACCESS.2019.2941772
NR 32
TC 8
Z9 8
U1 0
U2 12
PD AUG
PY 2021
VL 49
IS 8
BP 2425
EP 2435
DI 10.1002/cta.3075
EA JUN 2021
WC Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Volanis, G
   Antonopoulos, A
   Makris, Y
   Hatzopoulos, AA
AF Volanis, Georgios
   Antonopoulos, Angelos
   Makris, Yiorgos
   Hatzopoulos, Alkis A.
TI Toward Silicon-Based Cognitive Neuromorphic ICs-A Survey
SO IEEE DESIGN & TEST
DT Article
DE Neurons; Integrated circuit modeling; Computers; Silicon; Neuromorphics;
   Biological neural networks
ID SPIKING NEURONS; NEURAL-NETWORKS; ON-CHIP; INTEGRATION; SYSTEM; ARRAY
C1 [Volanis, Georgios; Makris, Yiorgos] Univ Texas Dallas, Elect Engn, Richardson, TX 75080 USA.
   [Antonopoulos, Angelos] Univ Texas Dallas, Richardson, TX 75080 USA.
   [Hatzopoulos, Alkis A.] Aristotle Univ Thessaloniki, Dept Elect & Comp Engn, Elect Lab, GR-54006 Thessaloniki, Greece.
RP Volanis, G (corresponding author), Univ Texas Dallas, Dept Elect Engn, Richardson, TX 75080 USA.
EM gxv130830@utdallas.edu
CR Alvado L, 2004, NEUROCOMPUTING, V58, P109, DOI 10.1016/j.neucom.2004.01.030
   Ambrogio S, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00056
   [Anonymous], 1989, ANALOG VLSI NEURAL S
   [Anonymous], 2001, INT PROC WORLD C NEU
   [Anonymous], 1950, J CLIN PSYCHOL, V6, P307
   Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Brader JM, 2007, NEURAL COMPUT, V19, P2881, DOI 10.1162/neco.2007.19.11.2881
   Chan V, 2007, IEEE T CIRCUITS-I, V54, P48, DOI 10.1109/TCSI.2006.887979
   Chicca E, 2014, P IEEE, V102, P1367, DOI 10.1109/JPROC.2014.2313954
   Cruz-Albrecht JM, 2013, NANOTECHNOLOGY, V24, DOI 10.1088/0957-4484/24/38/384011
   Diehl PU, 2015, IEEE IJCNN
   Fong XY, 2016, IEEE T COMPUT AID D, V35, P1, DOI 10.1109/TCAD.2015.2481793
   Häfliger P, 2007, IEEE T NEURAL NETWOR, V18, P551, DOI 10.1109/TNN.2006.884676
   Hasler J, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00118
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Indiveri G, 2006, IEEE T NEURAL NETWOR, V17, P211, DOI 10.1109/TNN.2005.860850
   Indiveri G, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00118
   Indiveri G, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00073
   Indiveri G, 2009, COGN COMPUT, V1, P119, DOI 10.1007/s12559-008-9003-6
   Lichtsteiner P, 2008, IEEE J SOLID-ST CIRC, V43, P566, DOI 10.1109/JSSC.2007.914337
   MAHOWALD M, 1991, NATURE, V354, P515, DOI 10.1038/354515a0
   Maliuk D, 2015, IEEE T NEUR NET LEAR, V26, P1721, DOI 10.1109/TNNLS.2014.2354406
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Mitra S, 2009, IEEE T BIOMED CIRC S, V3, P32, DOI 10.1109/TBCAS.2008.2005781
   Mostafa H, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00357
   Painkras E, 2013, IEEE J SOLID-ST CIRC, V48, P1943, DOI 10.1109/JSSC.2013.2259038
   Poon CS, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00108
   Qiao N, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00141
   Rigotti M, 2010, NEUROIMAGE, V52, P833, DOI 10.1016/j.neuroimage.2010.01.047
   Schemmel J, 2008, IEEE IJCNN, P431, DOI 10.1109/IJCNN.2008.4633828
   Serrano-Gotarredona R, 2009, IEEE T NEURAL NETWOR, V20, P1417, DOI 10.1109/TNN.2009.2023653
   Thomas A, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00241
   van Schaik A, 2001, NEURAL NETWORKS, V14, P617, DOI 10.1016/S0893-6080(01)00067-3
   Vogelstein RJ, 2007, IEEE T NEURAL NETWOR, V18, P253, DOI 10.1109/TNN.2006.883007
   Walter F, 2015, NEURAL NETWORKS, V72, P152, DOI 10.1016/j.neunet.2015.07.004
   Wang RM, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00180
NR 36
TC 7
Z9 7
U1 0
U2 15
PD MAY-JUN
PY 2016
VL 33
IS 3
BP 91
EP 102
DI 10.1109/MDAT.2016.2545159
WC Computer Science, Hardware & Architecture; Engineering, Electrical &
   Electronic
DA 2023-11-11
ER

PT J
AU Guo, SH
   Wang, L
   Chen, BZ
   Dou, Q
AF Guo, Shasha
   Wang, Lei
   Chen, Baozi
   Dou, Qiang
TI An Overhead-Free Max-Pooling Method for SNN
SO IEEE EMBEDDED SYSTEMS LETTERS
DT Article
DE Neurons; Microsoft Windows; Mathematical model; Training; Computational
   modeling; Biological neural networks; Task analysis; Approximate; max
   pooling; overhead; spiking neural network (SNN)
AB Spiking neural networks (SNNs) have been shown to be accurate, fast, and efficient in classical machine vision tasks, such as object recognition or detection. It is typical to convert a pretrained deep neural network into an SNN since training SNN is not easy. The max-pooling (MP) function is widely adopted in most state-of-the-art deep neural networks. To maintain the accuracy of the SNN obtained through conversion, this function is an important element to be implemented. However, it is difficult due to the dynamic characteristics of spikes. As far as we know, existing solutions adopt additional technologies except the spiking neuron model to implement MP or approximate MP, which introduce overhead of memory storage and computation. In this letter, we propose a novel method that utilizes only the spiking neuron model to approximate MP. Our method does not incur any overhead. We validate our method with three datasets and six networks including three oxford visual geometry group-like networks. And the experimental results show that the performance (accuracy and convergence rate) of our method is as good as or even better than that of the existing method.
C1 [Guo, Shasha; Wang, Lei; Chen, Baozi; Dou, Qiang] Natl Univ Def Technol, Coll Comp Sci & Technol, Changsha 410073, Peoples R China.
RP Guo, SH (corresponding author), Natl Univ Def Technol, Coll Comp Sci & Technol, Changsha 410073, Peoples R China.
EM guoshasha13@nudt.edu.cn; arrowya@gmail.com
CR [Anonymous], 2011, P NIPS WORKSHOP DEEP
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Chen RS, 2019, POSTGRAD MED, V131, P73, DOI 10.1080/00325481.2019.1552824
   Diehl PU, 2015, IEEE IJCNN
   Esser SK, 2016, P NATL ACAD SCI USA, V113, P11441, DOI 10.1073/pnas.1604850113
   Hu J, 2013, NEURAL COMPUT, V25, P450, DOI 10.1162/NECO_a_00395
   Hunsberger Eric, 2015, ARXIV151008829
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
   Krizhevsky Alex, 2012, LEARNING MULTIPLE LA
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li CY, 2017, OMICS, V21, P749, DOI 10.1089/omi.2017.0120
   Lin ZT, 2017, ELECTRON LETT, V53, P1347, DOI 10.1049/el.2017.2219
   Masquelier T, 2007, PLOS COMPUT BIOL, V3, P247, DOI 10.1371/journal.pcbi.0030031
   Orchard G, 2015, IEEE T PATTERN ANAL, V37, P2028, DOI 10.1109/TPAMI.2015.2392947
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Yu AJ, 2002, NEURAL COMPUT, V14, P2857, DOI 10.1162/089976602760805313
   Zisserman, 2014, CORR
NR 18
TC 6
Z9 6
U1 0
U2 12
PD MAR
PY 2020
VL 12
IS 1
BP 21
EP 24
DI 10.1109/LES.2019.2919244
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Du, YW
   Jin, J
   Wang, Q
   Fan, JY
AF Du, Yuwei
   Jin, Jing
   Wang, Qiang
   Fan, Jianyin
GP IEEE
TI EMG-Based Continuous Motion Decoding of Upper Limb with Spiking Neural
   Network
SO 2022 IEEE INTERNATIONAL INSTRUMENTATION AND MEASUREMENT TECHNOLOGY
   CONFERENCE (I2MTC 2022)
SE IEEE Instrumentation and Measurement Technology Conference
DT Proceedings Paper
CT IEEE International Instrumentation and Measurement Technology Conference
   (I2MTC)
CY MAY 16-19, 2022
CL Ottawa, CANADA
DE electromyography; continuous motion; spiking neural network
AB Surface electromyography (EMG), generated during muscle activities of human beings, allows intuitive control for human-robot interaction to happen. Decoding human movement intention from EMG accurately and instantaneously is one of the most important parts of the whole control task. Spiking neural network (SNN) with spiking neurons is more computationally powerful than networks with non-spiking neurons and contains temporal information (time-dependency). Compared with discrete motion classification task, motion regression is more meaningful and helpful for the underlying applications including assisting human beings' activities of daily living (ADLs). We proposed a novel method deploying SNN in human motion regression task. An SNN is built to decode elbow joint angle from preprocessed surface EMG signals and achieved satisfying accuracy compared with long short-term memory. According to the experiment results, SNN is competent to decode motion information from surface EMG.
C1 [Du, Yuwei; Jin, Jing; Wang, Qiang; Fan, Jianyin] Harbin Inst Technol, Dept Control Sci & Engn, Harbin, Peoples R China.
RP Du, YW (corresponding author), Harbin Inst Technol, Dept Control Sci & Engn, Harbin, Peoples R China.
EM hitduyw@163.com; injinghit@hit.edu.cn; wanggiang@hit.edu.cn;
   fanjianyin_q@foxmail.com
CR Artemiadis PK, 2007, 2007 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-9, P495
   Artemiadis PK, 2010, IEEE T INF TECHNOL B, V14, P582, DOI 10.1109/TITB.2010.2040832
   Shrestha SB, 2018, Arxiv, DOI arXiv:1810.08646
   Bao TZ, 2021, IEEE T INSTRUM MEAS, V70, DOI 10.1109/TIM.2020.3036654
   Donati E, 2019, IEEE T BIOMED CIRC S, V13, P795, DOI 10.1109/TBCAS.2019.2925454
   Gehrig M, 2020, IEEE INT CONF ROBOT, P4195, DOI [10.1109/icra40945.2020.9197133, 10.1109/ICRA40945.2020.9197133]
   Jiang N, 2014, IEEE T NEUR SYS REH, V22, P549, DOI 10.1109/TNSRE.2013.2287383
   Khan SM, 2020, IEEE REV BIOMED ENG, V13, P248, DOI 10.1109/RBME.2019.2950897
   Ma JX, 2015, IEEE T HUM-MACH SYST, V45, P74, DOI 10.1109/THMS.2014.2358634
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Pizzolato S, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0186132
   Wen RS, 2020, IEEE ROBOT AUTOM LET, V5, P2762, DOI 10.1109/LRA.2020.2974439
   Zhuang Y, 2019, IEEE T IND INFORM, V15, P1211, DOI 10.1109/TII.2018.2875729
NR 13
TC 0
Z9 0
U1 4
U2 7
PY 2022
DI 10.1109/I2MTC48687.2022.9806710
WC Instruments & Instrumentation
DA 2023-11-11
ER

PT J
AU Han, RX
   Wang, J
   Miao, R
   Deng, B
   Qin, YM
   Yu, HT
   Wei, XL
AF Han, Ruixue
   Wang, Jiang
   Miao, Rui
   Deng, Bin
   Qin, Yingmei
   Yu, Haitao
   Wei, Xile
TI Propagation of Collective Temporal Regularity in Noisy Hierarchical
   Networks
SO IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
DT Article
DE Feedforward neural network; information entropy; Izhikevich model;
   spike-timing-dependent plasticity (STDP); spiking regularity; synaptic
   dynamics; topology-dependent coherence resonance (CR)
ID TIMING-DEPENDENT PLASTICITY; HIGH-CONDUCTANCE STATE; SPIKING ACTIVITY;
   SYNAPTIC PLASTICITY; SYNCHRONOUS SPIKING; SIGNAL PROPAGATION;
   EXCITABLE-MEMBRANES; COHERENCE RESONANCE; GABAERGIC SYNAPSES; NEURONAL
   NETWORKS
AB Neuronal communication between different brain areas is achieved in terms of spikes. Consequently, spike-time regularity is closely related to many cognitive tasks and timing precision of neural information processing. A recent experiment on primate parietal cortex reports that spike-time regularity increases consistently from primary sensory to higher cortical regions. This observation conflicts with the influential view that spikes in the neocortex are fundamentally irregular. To uncover the underlying network mechanism, we construct a multilayered feedforward neural information transmission pathway and investigate how spike-time regularity evolves across subsequent layers. Numerical results reveal that despite the obviously irregular spiking patterns in previous several layers, neurons in downstream layers can generate rather regular spikes, which depends on the network topology. In particular, we find that collective temporal regularity in deeper layers exhibits resonance-like behavior with respect to both synaptic connection probability and synaptic weight, i.e., the optimal topology parameter maximizes the spike-timing regularity. Furthermore, it is demonstrated that synaptic properties, including inhibition, synaptic transient dynamics, and plasticity, have significant impacts on spike-timing regularity propagation. The emergence of the increasingly regular spiking (RS) patterns in higher parietal regions can, thus, be viewed as a natural consequence of spiking activity propagation between different brain areas. Finally, we validate an important function served by increased RS: promoting reliable propagation of spike-rate signals across downstream layers.
C1 [Han, Ruixue; Wang, Jiang; Miao, Rui; Deng, Bin; Yu, Haitao; Wei, Xile] Tianjin Univ, Sch Elect & Automat Engn, Tianjin 300072, Peoples R China.
   [Qin, Yingmei] Tianjin Univ Technol & Educ Tianjin, Sch Automat & Elect Engn, Tianjin 300222, Peoples R China.
RP Wei, XL (corresponding author), Tianjin Univ, Sch Elect & Automat Engn, Tianjin 300072, Peoples R China.
EM ruixuehantju@sina.com; jiangwang@tju.edu.cn; 361812194@qq.com;
   dengbin@tju.edu.cn; eeymqin@tju.edu.cn; htyu@tju.edu.cn;
   xilewei@tju.edu.cn
CR August DA, 1999, J COMPUT NEUROSCI, V6, P71, DOI 10.1023/A:1008861001091
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Braitenberg V., 1991, J ANAT, V179, P203
   Brunel N, 2000, J COMPUT NEUROSCI, V8, P183, DOI 10.1023/A:1008925309027
   Bullmore ET, 2009, NAT REV NEUROSCI, V10, P186, DOI 10.1038/nrn2575
   Caporale N, 2008, ANNU REV NEUROSCI, V31, P25, DOI 10.1146/annurev.neuro.31.060407.125639
   Carron R, 2013, FRONT SYST NEUROSCI, V7, DOI 10.3389/fnsys.2013.00112
   Câteau H, 2001, NEURAL NETWORKS, V14, P675, DOI 10.1016/S0893-6080(01)00065-X
   CONNORS BW, 1990, TRENDS NEUROSCI, V13, P99, DOI 10.1016/0166-2236(90)90185-D
   Debanne D, 1999, J NEUROSCI, V19, P10664, DOI 10.1523/JNEUROSCI.19-24-10664.1999
   Debanne D, 1996, P NATL ACAD SCI USA, V93, P11225, DOI 10.1073/pnas.93.20.11225
   Destexhe A, 2003, NAT REV NEUROSCI, V4, P739, DOI 10.1038/nrn1198
   Destexhe A, 1994, J Comput Neurosci, V1, P195, DOI 10.1007/BF00961734
   Destexhe A, 2009, J COMPUT NEUROSCI, V27, P493, DOI 10.1007/s10827-009-0164-4
   Diesmann M, 1999, NATURE, V402, P529, DOI 10.1038/990101
   Dorval AD, 2008, J NEUROSCI METH, V173, P129, DOI 10.1016/j.jneumeth.2008.05.013
   Feinerman O, 2005, J NEUROPHYSIOL, V94, P3406, DOI 10.1152/jn.00264.2005
   Felleman DJ, 1991, CEREB CORTEX, V1, P1, DOI 10.1093/cercor/1.1.1
   Fetz E., 1991, NORMAL ALTERED STATE
   Gerstner W, 2000, NEURAL COMPUT, V12, P43, DOI 10.1162/089976600300015899
   Gilson M, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0025339
   Gjorgjieva J, 2011, P NATL ACAD SCI USA, V108, P19383, DOI 10.1073/pnas.1105933108
   Gong YB, 2009, J CHEM PHYS, V130, DOI 10.1063/1.3125512
   GRAY CM, 1989, NATURE, V338, P334, DOI 10.1038/338334a0
   Guo DQ, 2009, PHYS REV E, V79, DOI 10.1103/PhysRevE.79.051921
   Gütig R, 2003, J NEUROSCI, V23, P3697
   Haas JS, 2006, J NEUROPHYSIOL, V96, P3305, DOI 10.1152/jn.00551.2006
   Hahn G, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003811
   HESTRIN S, 1990, NEURON, V5, P247, DOI 10.1016/0896-6273(90)90162-9
   Hiratani N, 2015, PLOS COMPUT BIOL, V11, DOI 10.1371/journal.pcbi.1004227
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Jahnke S, 2013, FRONT COMPUT NEUROSC, V7, DOI 10.3389/fncom.2013.00153
   Jiruska P, 2013, J PHYSIOL-LONDON, V591, P787, DOI 10.1113/jphysiol.2012.239590
   Karbowski J, 2002, PHYS REV E, V65, DOI 10.1103/PhysRevE.65.031902
   Kistler WM, 2000, NEURAL COMPUT, V12, P385, DOI 10.1162/089976600300015844
   Kleberg FI, 2014, FRONT COMPUT NEUROSC, V8, DOI 10.3389/fncom.2014.00053
   Kodangattil JN, 2013, J PHYSIOL-LONDON, V591, P4699, DOI 10.1113/jphysiol.2013.257873
   Kremkow J, 2010, J NEUROSCI, V30, P15760, DOI 10.1523/JNEUROSCI.3874-10.2010
   Kuhn A, 2004, J NEUROSCI, V24, P2345, DOI 10.1523/JNEUROSCI.3349-03.2004
   Kumar A, 2008, J NEUROSCI, V28, P5268, DOI 10.1523/JNEUROSCI.2542-07.2008
   Kumar A, 2008, NEURAL COMPUT, V20, P1, DOI 10.1162/neco.2008.20.1.1
   Kumar A, 2011, FRONT SYST NEUROSCI, V5, DOI 10.3389/fnsys.2011.00086
   Kumar A, 2010, NAT REV NEUROSCI, V11, P615, DOI 10.1038/nrn2886
   Lee AK, 2002, NEURON, V36, P1183, DOI 10.1016/S0896-6273(02)01096-6
   Lee S, 2009, PLOS COMPUT BIOL, V5, DOI 10.1371/journal.pcbi.1000602
   Li MR, 2006, PHYS REV E, V74, DOI 10.1103/PhysRevE.74.011918
   Li QS, 2008, PHYS REV E, V77, DOI 10.1103/PhysRevE.77.036117
   Litvak V, 2003, J NEUROSCI, V23, P3006
   Luz Y, 2012, PLOS COMPUT BIOL, V8, DOI 10.1371/journal.pcbi.1002334
   Maffei A, 2011, NEURAL PLAST, V2011, DOI 10.1155/2011/254724
   Magee JC, 1997, SCIENCE, V275, P209, DOI 10.1126/science.275.5297.209
   Maimon G, 2009, NEURON, V62, P426, DOI 10.1016/j.neuron.2009.03.021
   Maquet P, 2001, SCIENCE, V294, P1048, DOI 10.1126/science.1062856
   MCCORMICK DA, 1985, J NEUROPHYSIOL, V54, P782, DOI 10.1152/jn.1985.54.4.782
   Mehring C, 2003, BIOL CYBERN, V88, P395, DOI 10.1007/s00422-002-0384-4
   Nádasdy Z, 1999, J NEUROSCI, V19, P9497
   Ozer M, 2008, PHYS LETT A, V372, P6498, DOI 10.1016/j.physleta.2008.09.007
   Ozer M, 2009, EPL-EUROPHYS LETT, V86, DOI 10.1209/0295-5075/86/40008
   PERKEL DH, 1967, BIOPHYS J, V7, P391, DOI 10.1016/S0006-3495(67)86596-2
   Pikovsky A, 2002, PHYS REV LETT, V88, DOI 10.1103/PhysRevLett.88.050601
   Pikovsky AS, 1997, PHYS REV LETT, V78, P775, DOI 10.1103/PhysRevLett.78.775
   RAMAN IM, 1992, NEURON, V9, P173, DOI 10.1016/0896-6273(92)90232-3
   Renart A, 2010, SCIENCE, V327, P587, DOI 10.1126/science.1179850
   Reyes AD, 2003, NAT NEUROSCI, V6, P593, DOI 10.1038/nn1056
   Richardson MJE, 2004, PHYS REV E, V69, DOI 10.1103/PhysRevE.69.051918
   Riehle A, 1997, SCIENCE, V278, P1950, DOI 10.1126/science.278.5345.1950
   Rieke F., 1999, SPIKES EXPLORING NEU
   Rubin J, 2001, PHYS REV LETT, V86, P364, DOI 10.1103/PhysRevLett.86.364
   Salinas E, 2000, J NEUROSCI, V20, P6193, DOI 10.1523/JNEUROSCI.20-16-06193.2000
   Scannell JW, 1999, CEREB CORTEX, V9, P277, DOI 10.1093/cercor/9.3.277
   Schmid G, 2004, PHYS BIOL, V1, P61, DOI 10.1088/1478-3967/1/2/002
   Schmid G, 2004, PHYSICA A, V344, P665, DOI 10.1016/j.physa.2004.06.049
   Shadlen MN, 1998, J NEUROSCI, V18, P3870
   SHANNON CE, 1948, BELL SYST TECH J, V27, P379, DOI 10.1002/j.1538-7305.1948.tb01338.x
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Steele PM, 1999, J NEUROPHYSIOL, V81, P1559, DOI 10.1152/jn.1999.81.4.1559
   Stickgold R, 2000, NAT NEUROSCI, V3, P1237, DOI 10.1038/81756
   Sun XJ, 2014, SCI CHINA TECHNOL SC, V57, P879, DOI 10.1007/s11431-014-5529-x
   Svirskis G, 2000, BIOPHYS J, V79, P629, DOI 10.1016/S0006-3495(00)76321-1
   Tallon-Baudry C, 2009, FRONT BIOSCI-LANDMRK, V14, P321, DOI 10.2741/3246
   Thorpe SJ, 2001, SCIENCE, V291, P260, DOI 10.1126/science.1058249
   TSODYKS MV, 1995, NETWORK-COMP NEURAL, V6, P111, DOI 10.1088/0954-898X/6/2/001
   VAADIA E, 1995, NATURE, V373, P515, DOI 10.1038/373515a0
   van Rossum MCW, 2002, J NEUROSCI, V22, P1956, DOI 10.1523/JNEUROSCI.22-05-01956.2002
   van Rossum MCW, 2000, J NEUROSCI, V20, P8812
   van Vreeswijk C, 1998, NEURAL COMPUT, V10, P1321, DOI 10.1162/089976698300017214
   vanVreeswijk C, 1996, SCIENCE, V274, P1724, DOI 10.1126/science.274.5293.1724
   Varela F, 2001, NAT REV NEUROSCI, V2, P229, DOI 10.1038/35067550
   Vogels TP, 2013, FRONT NEURAL CIRCUIT, V7, DOI 10.3389/fncir.2013.00119
   Vogels TP, 2011, SCIENCE, V334, P1569, DOI 10.1126/science.1211095
   Vogels TP, 2009, NAT NEUROSCI, V12, P483, DOI 10.1038/nn.2276
   Vogels TP, 2005, J NEUROSCI, V25, P10786, DOI 10.1523/JNEUROSCI.3508-05.2005
   Wang MS, 2004, CHEMPHYSCHEM, V5, P1602, DOI 10.1002/cphc.200400255
   Wang ST, 2006, PHYS REV LETT, V96, DOI 10.1103/PhysRevLett.96.018103
   Wang YQ, 2000, PHYS REV E, V61, P740, DOI 10.1103/PhysRevE.61.740
   Womelsdorf T, 2006, NATURE, V439, P733, DOI 10.1038/nature04258
   Woodin MA, 2003, NEURON, V39, P807, DOI 10.1016/S0896-6273(03)00507-5
   Xu SJ, 2012, NAT NEUROSCI, V15, P449, DOI 10.1038/nn.3036
   Yilmaz E, 2013, PHYS LETT A, V377, P1301, DOI 10.1016/j.physleta.2013.03.007
NR 100
TC 5
Z9 5
U1 2
U2 24
PD JAN
PY 2017
VL 28
IS 1
BP 191
EP 205
DI 10.1109/TNNLS.2015.2502993
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Computer Science, Theory & Methods; Engineering,
   Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Hart, CB
AF Hart, Corey B.
BE Dagli, CH
TI Variable Time Delays and Representational Capacity in Sparsely Connected
   Populations of Spiking Neurons
SO COMPLEX ADAPTIVE SYSTEMS: EMERGING TECHNOLOGIES FOR EVOLVING SYSTEMS:
   SOCIO-TECHNICAL, CYBER AND BIG DATA
SE Procedia Computer Science
DT Proceedings Paper
CT Complex Adaptive Systems Conference
CY NOV 13-15, 2013
CL Baltimore, MD
DE Artificial neural networks; spiking neurons; neural assembly computing
ID COMPUTATION; NETWORKS
AB Successive generations of artificial neural networks have leveraged their multiplicity of connections and weights for significant improvements in information processing capability and memory capacity. The most recent generation of artificial neural networks, third generation networks, consist of spiking neuron models that attempt to mimic the complex dynamic features exhibited by real biological neurons in the hopes of improvements in computational and representational capacities. While the theoretical capabilities of these networks are impressive, understanding the nature and extent of their computational advantages, and the appropriate network architectures and algorithms necessary for their successful exploitation, have lagged far behind the theory. With this in mind, we herein explore the representational capacity of two related forms of neural networks: synfire chains, and polychronic networks. We find that the computational capacity of such cellular assembly based networks increases with the size of between-neural-pool time delays and that for relatively small changes in time delay, linear increases in network representational capacities are obtained. (C) 2013 The Authors. Published by Elsevier B.V.
C1 Lockheed Martin IS&GS, Adv Technol & Innovat, King Of Prussia, PA 19406 USA.
RP Hart, CB (corresponding author), Lockheed Martin IS&GS, Adv Technol & Innovat, 230 Mall Blvd, King Of Prussia, PA 19406 USA.
EM corey.hart@lmco.com
CR Abeles M., 1982, LOCAL CORTICAL CIRCU, DOI DOI 10.1007/978-3-642-81708-3
   GROSSBERG S, 1973, STUD APPL MATH, V52, P213
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Minsky Marvin, 1969, PERCEPTRONS
   Paugham-Moisy S., 2006, SPIKING NEURAL NETWO
   Rahnel J, 2012, IEEE T NEURAL NETWOR, V23, P916
   Rosenblatt F., 1970, 854601 CORN AER LAB
NR 9
TC 0
Z9 0
U1 0
U2 0
PY 2013
VL 20
BP 22
EP 26
DI 10.1016/j.procs.2013.09.233
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods
DA 2023-11-11
ER

PT J
AU Schæfer, M
   Schoenauer, T
   Wolff, C
   Hartmann, G
   Klar, H
   Rückert, U
AF Schæfer, M
   Schoenauer, T
   Wolff, C
   Hartmann, G
   Klar, H
   Rückert, U
TI Simulation of spiking neural networks -: architectures and
   implementations
SO NEUROCOMPUTING
DT Article; Proceedings Paper
CT 8th European Symposium on Artificial Neural Networks (ESANN)
CY APR 26-28, 2001
CL BRUGGE, BELGIUM
DE spiking neural networks; PCNN simulation; accelerators; vision networks
ID VISUAL-CORTEX; ORIENTATION COLUMNS; PRINCIPLES; EMERGENCE; CELLS
AB The fast simulation of large networks of spiking neurons is a major task for the examination of biology-inspired vision systems. Networks of this type label features by synchronization of spikes and there is strong demand to simulate these effects in real world environments. As the calculations for one model neuron are complex, the digital simulation of large networks is not efficient using existing simulation systems. Consequently, it is necessary to develop special simulation techniques. This article introduces a wide range of concepts for the different parts of digital simulator systems for large vision networks and presents accelerators based on these foundations. (C) 2002 Elsevier Science B.V. All rights reserved.
C1 Univ Paderborn, Heinz Nixdorf Inst, D-33102 Paderborn, Germany.
   Tech Univ Berlin, Inst Microelect & Solid State Elect, D-10587 Berlin, Germany.
RP Wolff, C (corresponding author), Univ Paderborn, Heinz Nixdorf Inst, D-33102 Paderborn, Germany.
CR [Anonymous], 1998, PULSED NEURAL NETWOR
   BIENENSTOCK EL, 1982, J NEUROSCI, V2, P32, DOI 10.1523/jneurosci.02-01-00032.1982
   Eckhorn R, 1990, NEURAL COMPUT, V2, P293, DOI 10.1162/neco.1990.2.3.293
   ECKHORN R, 1989, P ICNN, V1, P723
   FRANK G, 1995, P INT C NEUR NETW IC, V4, P2014
   FRANK G, 1997, HNI VERLAGSSCHRIFTEN, V26
   FRENCH AS, 1970, IEEE T BIO-MED ENG, VBM17, P248, DOI 10.1109/TBME.1970.4502739
   Gerstner W, 1996, NATURE, V383, P76, DOI 10.1038/383076a0
   GRAY CM, 1989, P NATL ACAD SCI USA, V86, P1698, DOI 10.1073/pnas.86.5.1698
   HARTMANN G, 1990, PARALLEL PROCESSING IN NEURAL SYSTEMS AND COMPUTERS, P361
   HARTMANN G, 1997, SPIKE128K ACCELERATO, P130
   HEEMSKERK JNH, 1995, THESIS LEIDEN U RIJK
   HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837
   HUBEL DH, 1982, NATURE, V299, P515, DOI 10.1038/299515a0
   JAHNKE A, 1997, INT C ART NEUR NETW, P1187
   JAHNKE A, 1996, MICRONEURO 96, P232
   Kempter R, 1999, PHYS REV E, V59, P4498, DOI 10.1103/PhysRevE.59.4498
   KEMPTER R, 1997, NATURWISSENSCHATLICH, V17
   LINSKER R, 1986, P NATL ACAD SCI USA, V83, P8779, DOI 10.1073/pnas.83.22.8779
   LINSKER R, 1986, P NATL ACAD SCI USA, V83, P7508, DOI 10.1073/pnas.83.19.7508
   LINSKER R, 1986, P NATL ACAD SCI USA, V83, P8390, DOI 10.1073/pnas.83.21.8390
   MOHRAZ K, 1997, P IMACS WORLD C 97 B, V6, P523
   NIERBUR E, 1994, ADV NEURAL INFORMATI, V6, P904
   Preis R, 1997, ADVANCES IN COMPUTATIONAL MECHANICS WITH PARALLEL AND DISTRIBUTED PROCESSING, P63, DOI 10.4203/ccp.45.3.1
   ROTH U, 1997, MICRONEURO 97, P31
   ROTH U, 1995, IWANN 95, P720
   SCHFER M, 1999, MICRONEURO 99, P316
   SCHOENAUER T, 1998, VIDYNN 98
   SCHOENAUER T, 1998, ICCIN 98 INT C COMP, P17
   SCHOENAUER T, 2000, INT JOINT C NEUR NET
   SINGER W, 1995, SCIENCE, V270, P758, DOI 10.1126/science.270.5237.758
   WEITZEL L, 1997, CAIP 97
   WOLFF C, 1999, DSP DTSCH 99, P267
   WOLFF C, 1999, MICRONEURO 99, P324
NR 34
TC 26
Z9 32
U1 1
U2 7
PD OCT
PY 2002
VL 48
BP 647
EP 679
AR PII S0925-2312(01)00633-6
DI 10.1016/S0925-2312(01)00633-6
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT C
AU Galán-Prado, F
   Rosselló, JL
AF Galan-Prado, Fabio
   Rossello, Josep L.
BE Rojas, I
   Joya, G
   Catala, A
TI Smart Hardware Implementation of Spiking Neural Networks
SO ADVANCES IN COMPUTATIONAL INTELLIGENCE, IWANN 2017, PT I
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 14th International Work-Conference on Artificial Neural Networks (IWANN)
CY JUN 14-16, 2017
CL Cadiz, SPAIN
DE Neuromorphic hardware; Spiking Neural Network; FPGA
ID NEURONS
AB During last years a lot of attention have been focused to the hardware implementation of Artificial Neural Networks (ANN) to efficiently exploit the inherent parallelism associated to these systems. From the different types of ANN, the Spiking Neural Networks (SNN) arise as a promising bio-inspired model that is able to emulate the expected neural behavior with a high confidence. Many works are centered in using analog circuitry to reproduce SNN with a high degree of precision, while minimizing the area and the energy costs. Nevertheless, the reliability and flexibility of these systems is lower if compared with digital implementations. In this paper we present a new, low-cost bio-inspired digital neural model for SNN along with an auxiliary Computer Aided Design (CAD) tool for the efficient implementation of high-volume SNN.
C1 [Galan-Prado, Fabio; Rossello, Josep L.] Univ Illes Balears, Phys Dept, Elect Engn Grp, Palma De Mallorca, Spain.
RP Galán-Prado, F (corresponding author), Univ Illes Balears, Phys Dept, Elect Engn Grp, Palma De Mallorca, Spain.
EM fabio.galan@uib.es; j.rossello@uib.es
CR Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   Cassidy A, 2007, 2007 IEEE BIOMEDICAL CIRCUITS AND SYSTEMS CONFERENCE, P75, DOI 10.1109/BIOCAS.2007.4463312
   Cassidy AS, 2013, NEURAL NETWORKS, V45, P4, DOI 10.1016/j.neunet.2013.05.011
   Kaulmann Tim, 2007, 7th International Conference on Hybrid Intelligent Systems, HIS 2007, P302
   London M, 2010, NATURE, V466, P123, DOI 10.1038/nature09086
   Misra J, 2010, NEUROCOMPUTING, V74, P239, DOI 10.1016/j.neucom.2010.03.021
   Morris-Reich A, 2017, PAL CRIT ST ANTISEM, P1, DOI 10.1007/978-3-319-49953-6_1
   Natschlager T, 1998, NETWORK-COMP NEURAL, V9, P319, DOI 10.1088/0954-898X/9/3/003
   Omondi AR, 2006, FPGA IMPLEMENTATIONS OF NEURAL NETWORKS, P1, DOI 10.1007/0-387-28487-7_1
   Rosselló JL, 2016, INT J NEURAL SYST, V26, DOI 10.1142/S0129065715500367
   Rosselló JL, 2014, INT J NEURAL SYST, V24, DOI 10.1142/S0129065714300034
   Schrauwen B, 2008, NEURAL NETWORKS, V21, P511, DOI 10.1016/j.neunet.2007.12.009
   Segev I., 1998, A BRADFORD BOOK, V2
   Soltic S, 2010, INT J NEURAL SYST, V20, P437, DOI 10.1142/S012906571000253X
   Steinmetz PN, 2000, J COMPUT NEUROSCI, V9, P133, DOI 10.1023/A:1008967807741
   Wysoski SG, 2008, NEUROCOMPUTING, V71, P2563, DOI 10.1016/j.neucom.2007.12.038
NR 16
TC 2
Z9 2
U1 1
U2 7
PY 2017
VL 10305
BP 560
EP 568
DI 10.1007/978-3-319-59153-7_48
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT J
AU Liu, JK
   She, ZS
AF Liu, Jian K.
   She, Zhen-Su
TI A Spike-Timing Pattern Based Neural Network Model for the Study of
   Memory Dynamics
SO PLOS ONE
DT Article
AB It is well accepted that the brain's computation relies on spatiotemporal activity of neural networks. In particular, there is growing evidence of the importance of continuously and precisely timed spiking activity. Therefore, it is important to characterize memory states in terms of spike-timing patterns that give both reliable memory of firing activities and precise memory of firing timings. The relationship between memory states and spike-timing patterns has been studied empirically with large-scale recording of neuron population in recent years. Here, by using a recurrent neural network model with dynamics at two time scales, we construct a dynamical memory network model which embeds both fast neural and synaptic variation and slow learning dynamics. A state vector is proposed to describe memory states in terms of spike-timing patterns of neural population, and a distance measure of state vector is defined to study several important phenomena of memory dynamics: partial memory recall, learning efficiency, learning with correlated stimuli. We show that the distance measure can capture the timing difference of memory states. In addition, we examine the influence of network topology on learning ability, and show that local connections can increase the network's ability to embed more memory states. Together theses results suggest that the proposed system based on spike-timing patterns gives a productive model for the study of detailed learning and memory dynamics.
RP Liu, JK (corresponding author), Univ Calif Los Angeles, Dept Math, Los Angeles, CA 90024 USA.
EM liujk@ucla.edu; she@pku.edu.cn
CR BIALEK W, 1991, SCIENCE, V252, P1854, DOI 10.1126/science.2063199
   Börgers C, 2005, P NATL ACAD SCI USA, V102, P7002, DOI 10.1073/pnas.0502366102
   Borst A, 1999, NAT NEUROSCI, V2, P947, DOI 10.1038/14731
   Brunel N, 2000, J COMPUT NEUROSCI, V8, P183, DOI 10.1023/A:1008925309027
   Buonomano DV, 2009, NAT REV NEUROSCI, V10, P113, DOI 10.1038/nrn2558
   BUONOMANO DV, 1995, SCIENCE, V267, P1028, DOI 10.1126/science.7863330
   Buonomano DV, 2005, J NEUROPHYSIOL, V94, P2275, DOI 10.1152/jn.01250.2004
   Buonomano DV, 2000, J NEUROSCI, V20, P1129, DOI 10.1523/JNEUROSCI.20-03-01129.2000
   Buzsáki G, 2007, NATURE, V446, P267, DOI 10.1038/446267a
   Dan Y, 2006, PHYSIOL REV, V86, P1033, DOI 10.1152/physrev.00030.2005
   de Almeida L, 2007, LEARN MEMORY, V14, P795, DOI 10.1101/lm.730207
   DESTEXHE A, 1994, NEURAL COMPUT, V6, P14, DOI 10.1162/neco.1994.6.1.14
   EDELMAN GM, 1993, NEURON, V10, P115, DOI 10.1016/0896-6273(93)90304-A
   Fröhlich F, 2008, J NEUROSCI, V28, P1709, DOI 10.1523/JNEUROSCI.4263-07.2008
   Fusi S, 2005, NEURON, V45, P599, DOI 10.1016/j.neuron.2005.02.001
   GOLOMB D, 1994, J NEUROPHYSIOL, V72, P1109, DOI 10.1152/jn.1994.72.3.1109
   Gupta A, 2000, SCIENCE, V287, P273, DOI 10.1126/science.287.5451.273
   Hahnloser RHR, 2002, NATURE, V419, P65, DOI 10.1038/nature00974
   Harris KD, 2005, NAT REV NEUROSCI, V6, P399, DOI 10.1038/nrn1669
   Harris KD, 2003, NATURE, V424, P552, DOI 10.1038/nature01834
   Izhikevich EM, 2004, CEREB CORTEX, V14, P933, DOI 10.1093/cercor/bhh053
   Izhikevich EM, 2003, TRENDS NEUROSCI, V26, P161, DOI 10.1016/S0166-2236(03)00034-1
   Izhikevich EM, 2007, CEREB CORTEX, V17, P2443, DOI 10.1093/cercor/bhl152
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Jacobs A. L., 2009, P NATL ACAD SCI US
   LIU JK, 2009, FRONT SYST NEUROSCI
   Markram H, 1998, P NATL ACAD SCI USA, V95, P5323, DOI 10.1073/pnas.95.9.5323
   Mauk MD, 2004, ANNU REV NEUROSCI, V27, P307, DOI 10.1146/annurev.neuro.27.070203.144247
   Mehring C, 2003, BIOL CYBERN, V88, P395, DOI 10.1007/s00422-002-0384-4
   Morrison A, 2008, BIOL CYBERN, V98, P459, DOI 10.1007/s00422-008-0233-1
   Muniak MA, 2007, J NEUROSCI, V27, P11687, DOI 10.1523/JNEUROSCI.1486-07.2007
   Nelson SB, 2008, NEURON, V60, P477, DOI 10.1016/j.neuron.2008.10.020
   Pastalkova E, 2008, SCIENCE, V321, P1322, DOI 10.1126/science.1159775
   Rabinovich MI, 2006, REV MOD PHYS, V78, P1213, DOI 10.1103/RevModPhys.78.1213
   Salinas E, 2001, NAT REV NEUROSCI, V2, P539, DOI 10.1038/35086012
   Siri B, 2008, NEURAL COMPUT, V20, P2937, DOI 10.1162/neco.2008.05-07-530
   Siri B, 2007, J PHYSIOL-PARIS, V101, P136, DOI 10.1016/j.jphysparis.2007.10.003
   Song S, 2005, PLOS BIOL, V3, P507, DOI 10.1371/journal.pbio.0030068
   THEUNISSEN F, 1995, J COMPUT NEUROSCI, V2, P149, DOI 10.1007/BF00961885
   Tiesinga P, 2008, NAT REV NEUROSCI, V9, P97, DOI 10.1038/nrn2315
   Vogels TP, 2005, ANNU REV NEUROSCI, V28, P357, DOI 10.1146/annurev.neuro.28.061604.135637
NR 41
TC 6
Z9 6
U1 0
U2 8
PD JUL 24
PY 2009
VL 4
IS 7
AR e6247
DI 10.1371/journal.pone.0006247
WC Multidisciplinary Sciences
DA 2023-11-11
ER

PT C
AU Shrestha, A
   Ahmed, K
   Wang, YZ
   Qiu, QR
AF Shrestha, Amar
   Ahmed, Khadeer
   Wang, Yanzhi
   Qiu, Qinru
GP IEEE
TI Stable Spike-Timing Dependent Plasticity Rule for Multilayer
   Unsupervised and Supervised Learning
SO 2017 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY MAY 14-19, 2017
CL Anchorage, AK
DE spiking neural network; STDP; digit recognition; unsupervised learning;
   supervised learning; quantized STDP
ID SYNAPTIC PLASTICITY; NETWORKS; MODEL
AB Spike-Timing Dependent Plasticity (STDP), the canonical learning rule for spiking neural networks (SNN), is gaining tremendous interest because of its simplicity, efficiency and biological plausibility. However, to date, multilayer feed-forward networks of spiking neurons are either only partially trained using STDP or pre-trained using traditional deep neural networks which are converted to deep spiking neural networks or a two-layer network where STDP learnt features are manually labelled. In this work, we present a low-cost, simplified, yet stable STDP rule for layer-wise unsupervised and supervised training of a multilayer feed-forward SNN. We propose to approximate Bayesian neuron using Stochastic Integrate and Fire (SIF) neuron model and introduce a supervised learning approach using teacher neurons to train the classification layer with one neuron per class. A SNN is trained for classification of handwritten digits with multiple layers of spiking neurons, including both the feature extraction and classification layer, using the proposed STDP rule. Our method achieves comparable to better accuracy on MNIST dataset than manually labelled two layer networks for the same sized hidden layer. We also analyze the parameter space to provide rationales for parameter fine-tuning and provide additional methods to improve noise resilience and input intensity variations. We further propose a Quantized 2-Power Shift (Q2PS) STDP rule, which reduces the implementation cost of digital hardware while achieves comparable performance.
C1 [Shrestha, Amar; Ahmed, Khadeer; Wang, Yanzhi; Qiu, Qinru] Syracuse Univ, Dept Elect Engn & Comp Sci, Syracuse, NY 13244 USA.
RP Shrestha, A (corresponding author), Syracuse Univ, Dept Elect Engn & Comp Sci, Syracuse, NY 13244 USA.
EM amshrest@syr.ed; khahmed@syr.ed; ywang393@syr.ed; qiqiu@syr.ed
CR Abbott LF, 2000, NAT NEUROSCI, V3, P1178, DOI 10.1038/81453
   Afshar S, 2015, IEEE T BIOMED CIRC S, V9, P188, DOI 10.1109/TBCAS.2015.2416391
   Ahmed K., 2016, NEUR NETW IJCNN 2016
   Ahmed K., 2016, VLSI ISVLSI 2016 IEE
   [Anonymous], 2013, EVENT DRIVEN CONTRAS
   [Anonymous], 2015, NEUROMORPHIC ENG SYS
   [Anonymous], 2009, P C HIGH PERFORMANCE
   Burbank KS, 2012, PLOS COMPUT BIOL, V8, DOI 10.1371/journal.pcbi.1002393
   Castellani GC, 2005, LEARN MEMORY, V12, P423, DOI 10.1101/lm.80705
   Diehl PU, 2015, IEEE IJCNN
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Gerstner W., 2002, SPIKING NEURON MODEL
   Gilson M, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0025339
   Javed F, 2010, AM J CLIN NUTR, V91, P907, DOI 10.3945/ajcn.2009.28512
   Kasabov N, 2013, NEURAL NETWORKS, V41, P188, DOI 10.1016/j.neunet.2012.11.014
   Kempter R, 1999, PHYS REV E, V59, P4498, DOI 10.1103/PhysRevE.59.4498
   Kepecs J. T. A., 2002, ADV NEUR INF PROC SY
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lesica NA, 2007, NEURON, V55, P479, DOI 10.1016/j.neuron.2007.07.013
   Levy N, 2001, NEURAL NETWORKS, V14, P815, DOI 10.1016/S0893-6080(01)00044-2
   Lisman J., 2010, SPIKE TIMING DEPENDE, V26, P53
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Masquelier T, 2007, PLOS COMPUT BIOL, V3, P247, DOI 10.1371/journal.pcbi.0030031
   Matias FS, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0140504
   Nessler B, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003037
   Querlioz D., 2011, NEUR NETW IJCNN 2011
   Sengupta S., 2015, ARXIV150307490
   Shouval HZ, 2002, P NATL ACAD SCI USA, V99, P10831, DOI 10.1073/pnas.152343099
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Teramae J, 2014, P IEEE, V102, P500, DOI 10.1109/JPROC.2014.2306254
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   van Rossum MCW, 2000, J NEUROSCI, V20, P8812
NR 32
TC 27
Z9 27
U1 0
U2 10
PY 2017
BP 1999
EP 2006
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Sun, HQ
   Qi, Y
   Wang, YM
AF Sun, Huaqin
   Qi, Yu
   Wang, Yueming
BE Ying, X
TI Delving into Temporal-Spectral Connections in Spike-LFP Decoding by
   Transformer Networks
SO HUMAN BRAIN AND ARTIFICIAL INTELLIGENCE, HBAI 2022
SE Communications in Computer and Information Science
DT Proceedings Paper
CT International Workshop on Human Brain and Artificial Intelligence (HBAI)
CY JUL 23, 2022
CL Vienna, AUSTRIA
DE Brain-computer interfaces; Spike-LFP fusion; Neural decoding
AB Invasive brain-computer interfaces (iBCIs) have demonstrated great potential in neural function restoration by decoding intention from brain signals for external device control. Spike trains and local field potentials (LFPs) are two typical intracortical neural signals with good complementarity from time and frequency domains. However, existing studies mostly focused on a single type of signal, and the interaction between the two signals has not been well studied. This study proposes a temporal-spectral transformer network (TSNet) to model the temporal (with spikes), spectral (with LFPs), and mutual (with both signals) connections in spike-LFPs towards robust neural decoding. Experiments with clinical neural signals demonstrate that the attention-based connection model enables the dynamic temporal-spectral compensation in spike and LFP signals, which improves the robustness against temporal shifts and noises in neural decoding.
C1 [Sun, Huaqin; Wang, Yueming] Zhejiang Univ, Qiushi Acad Adv Studies, Hangzhou, Peoples R China.
   [Qi, Yu] Zhejiang Univ, Sch Med, Affiliated Mental Hlth Ctr, Hangzhou, Peoples R China.
   [Qi, Yu] Zhejiang Univ, Sch Med, Hangzhou Peoples Hosp 7, Hangzhou, Peoples R China.
   [Qi, Yu] Zhejiang Univ, Sch Med, MOE Frontier Sci Ctr Brain Sci & Brain Machine In, Hangzhou, Peoples R China.
RP Wang, YM (corresponding author), Zhejiang Univ, Qiushi Acad Adv Studies, Hangzhou, Peoples R China.
EM sunhuaqin@zju.edu.cn; qiyu@zju.edu.cn; ymingwang@zju.edu.cn
NR 0
TC 1
Z9 1
U1 1
U2 1
PY 2023
VL 1692
BP 15
EP 29
DI 10.1007/978-981-19-8222-4_2
WC Computer Science, Artificial Intelligence; Computer Science,
   Interdisciplinary Applications; Neurosciences
DA 2023-11-11
ER

PT C
AU Liu, Y
   Cheng, L
AF Liu, Yang
   Cheng, Long
GP IEEE
TI Spiking-Neural-Network Based Fugl-Meyer Hand Gesture Recognition For
   Wearable Hand Rehabilitation Robot
SO 2018 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUL 08-13, 2018
CL Rio de Janeiro, BRAZIL
DE spiking neural networks; surface electromyography; Fugl-Meyer
   assessment; hand gesture recogniton; SpikeProp
ID CLASSIFICATION; STROKE
AB Hand rehabilitation robot can assist the patients in completing rehabilitation exercises. Usually these rehabilitation exercises are designed according to Fugl-Meyer Assessment(FMA). Surface electromyography(sEMG) signal is the most commonly used physiological signal to identify the patient's movement intention. However, recognizing the hand gesture based on the sEMG signal is still a challenging problem due to the low amplitude and non-stationary characteristics of the sEMG signal. In this paper, eight standard hand movements in FMA are selected for the active exercises by hand rehabilitation robots. A total of 15 volunteers' sEMG signals are collected in the course of the experiment. Four time domain features, integral EMG(IEGM), root mean square(RMS), zero crossings(ZC) and energy percentage(EP), are used to identify hand gestures. A feedforward spiking neural network receives the above time domain feature data, and combines the population coding with the Spikeprop learning algorithm to realize the accurate recognition of hand gestures. The experimental results show that: (1) the spiking neural network can achieve a satisfactory classification accuracy by using only 15 neurons; (2) the classification accuracy using all four features are highest with an accuracy of 96.5%; (3) under the same number of neurons, the classification accuracy of the spiking neural network is higher than that of the multilayer perceptron, radial basis function network and support vector machine. This demonstrates the fact that spiking neural networks can achieve a satisfactory classification accuracy with a smaller network size.
C1 [Liu, Yang; Cheng, Long] Chinese Acad Sci, Inst Automat, State Key Lab Management & Control Complex Syst, Beijing 100190, Peoples R China.
   [Liu, Yang; Cheng, Long] Univ Chinese Acad Sci, Beijing, Peoples R China.
RP Cheng, L (corresponding author), Chinese Acad Sci, Inst Automat, State Key Lab Management & Control Complex Syst, Beijing 100190, Peoples R China.; Cheng, L (corresponding author), Univ Chinese Acad Sci, Beijing, Peoples R China.
EM long.cheng@ia.ac.cn
CR [Anonymous], 2012, HDB NATURAL COMPUTIN, DOI 10.1007/978-3-540-92910-9_22
   Atzori M, 2016, FRONT NEUROROBOTICS, V10, DOI 10.3389/fnbot.2016.00009
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Cao ZQ, 2015, NEURAL COMPUT APPL, V26, P1839, DOI 10.1007/s00521-015-1848-5
   Chen M, 2017, IEEE ANN INT CONF CY, P1472, DOI 10.1109/CYBER.2017.8446436
   Chen Y., 2013, P 20 INT C NEUR INF, V8228, P70
   Dobkin BH, 2005, NEW ENGL J MED, V352, P1677, DOI 10.1056/NEJMcp043511
   Gladstone DJ, 2002, NEUROREHAB NEURAL RE, V16, P232, DOI 10.1177/154596802401105171
   González-Izal M, 2012, J ELECTROMYOGR KINES, V22, P501, DOI 10.1016/j.jelekin.2012.02.019
   Guo SX, 2015, SENSORS-BASEL, V15, P9022, DOI 10.3390/s150409022
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Khadivi A, 2005, INT CONF ACOUST SPEE, P385
   Kleine BU, 2001, J APPL PHYSIOL, V91, P1588, DOI 10.1152/jappl.2001.91.4.1588
   Liu SY, 2017, IEEE IND ELEC, P5701, DOI 10.1109/IECON.2017.8216989
   Lobov S, 2015, SENSORS-BASEL, V15, P27894, DOI 10.3390/s151127894
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Mesa I, 2014, EXPERT SYST APPL, V41, P5190, DOI 10.1016/j.eswa.2014.03.014
   Naik GR, 2010, IEEE T INF TECHNOL B, V14, P301, DOI 10.1109/TITB.2009.2037752
   Ostwald SK, 2008, J NEUROSCI NURS, V40, P173, DOI 10.1097/01376517-200806000-00008
   Panzeri S, 2015, TRENDS COGN SCI, V19, P162, DOI 10.1016/j.tics.2015.01.002
   Peng L, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND BIOMIMETICS (ROBIO), P1486, DOI 10.1109/ROBIO.2016.7866537
   Rong Y, 2013, NEUROPHYSIOLOGY+, V45, P39, DOI 10.1007/s11062-013-9335-z
   Shrestha SB, 2017, NEURAL NETWORKS, V96, P33, DOI 10.1016/j.neunet.2017.08.010
   Tu EM, 2017, IEEE T NEUR NET LEAR, V28, P1305, DOI 10.1109/TNNLS.2016.2536742
   Wang WQ, 2016, IEEE T SYST MAN CY-S, V46, P980, DOI 10.1109/TSMC.2016.2531653
   Xie XR, 2017, NEUROCOMPUTING, V241, P152, DOI 10.1016/j.neucom.2017.01.086
   Zhang F, 2016, IEEE T HUM-MACH SYST, V46, P761, DOI 10.1109/THMS.2016.2562510
   Zhu XZ, 2008, FBIE: 2008 INTERNATIONAL SEMINAR ON FUTURE BIOMEDICAL INFORMATION ENGINEERING, PROCEEDINGS, P160, DOI 10.1109/FBIE.2008.38
NR 28
TC 0
Z9 0
U1 0
U2 4
PY 2018
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Glackin, C
   McDaid, L
   Maguire, L
   Sayers, H
AF Glackin, Cornelius
   McDaid, Liam
   Maguire, Liam
   Sayers, Heather
BE Kurkova, V
   Neruda, R
   Koutnik, J
TI Implementing fuzzy reasoning on a spiking neural network
SO ARTIFICIAL NEURAL NETWORKS - ICANN 2008, PT II
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 18th International Conference on Artificial Neural Networks (ICANN 2008)
CY SEP 03-06, 2008
CL Prague, CZECH REPUBLIC
DE spiking neuron model; dynamic synapse; supervised learning; receptive
   field; fuzzy reasoning
AB This paper presents a supervised training algorithm that implements fuzzy reasoning on a. spiking neural network. Neuron selectivity is facilitated using receptive fields that enable individual neurons to be responsive to certain spike train frequencies. The receptive fields behave in a similar manner as fuzzy membership functions. The network is supervised but, learning only occurs locally as in the biological case. The connectivity of the hidden and output layers is representative of a fuzzy rule base. The advantages and disadvantages of the network topology for the IRIS classification task are demonstrated and directions of current and future work are discussed.
C1 [Glackin, Cornelius; McDaid, Liam; Maguire, Liam; Sayers, Heather] Univ Ulster, Fac Engn, Sch Comp & Intelligent Syst, Coleraine BT48 7JL, Londonderry, North Ireland.
RP Glackin, C (corresponding author), Univ Ulster, Fac Engn, Sch Comp & Intelligent Syst, Magee Campus, Coleraine BT48 7JL, Londonderry, North Ireland.
EM glackin-c1@ulster.ac.uk; lj.mcdaid@ulster.ac.uk;
   lp.maguire@ulster.ac.uk; hm.sayers@ulster.ac.uk
CR Abbott LF, 2000, NAT NEUROSCI, V3, P1178, DOI 10.1038/81453
   Abdelbar AM, 2006, NEURAL COMPUT APPL, V15, P1, DOI 10.1007/s00521-005-0001-2
   BARLOW HB, 1953, J PHYSIOL-LONDON, V119, P69, DOI 10.1113/jphysiol.1953.sp004829
   Belatreche A, 2003, P IEEE CYB INT CHALL, P39
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   CARNELL A, 2005, 13 EUR S ART NEUR NE
   Dunn J. C., 1973, Journal of Cybernetics, V3, P32, DOI 10.1080/01969727308546046
   Fisher RA, 1936, ANN EUGENIC, V7, P179, DOI 10.1111/j.1469-1809.1936.tb02137.x
   KASINSKI A, 2005, COMP SUPERVISED LEAR
   MAASS W, 1996, EL C COMP COMPL ECCC, V3
   Natschläger T, 2001, NETWORK-COMP NEURAL, V12, P75, DOI 10.1088/0954-898X/12/1/305
   Pfister JP, 2003, LECT NOTES COMPUT SC, V2714, P92
   Ruf B, 1997, NEURAL PROCESS LETT, V5, P9, DOI 10.1023/A:1009697008681
   Sougné J, 2000, PERSP NEURAL COMP, P23
   Tsodyks M, 1998, NEURAL COMPUT, V10, P821, DOI 10.1162/089976698300017502
   ZADEH LA, 1965, INFORM CONTROL, V8, P338, DOI 10.1016/S0019-9958(65)90241-X
NR 16
TC 8
Z9 8
U1 0
U2 0
PY 2008
VL 5164
BP 258
EP 267
WC Computer Science, Theory & Methods
DA 2023-11-11
ER

PT C
AU Xie, JL
   Zhao, QJ
   Zhao, JY
AF Xie, Jinli
   Zhao, Qinjun
   Zhao, Jianyu
BE Cong, F
   Leung, A
   Wei, Q
TI Burst and Correlated Firing in Spiking Neural Network with Global
   Inhibitory Feedback
SO ADVANCES IN NEURAL NETWORKS, PT I
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 14th International Symposium on Neural Networks (ISNN)
CY JUN 21-26, 2017
CL JAPAN
DE Burst; Correlation; Inhibitory feedback; Leaky integrate-and-fire neuron
ID PRIMARY VISUAL-CORTEX; NEURONAL CORRELATION; INFORMATION; COMPUTATION
AB Burst and correlated firing activities are observed experimentally in a variety of brain areas, which transmit and communicate information predominantly through spikes. The firing mode of spiking neurons relies on specific network characteristics. The inhibitory feedback is thought to be crucial to the burst firing. However, the effects of inhibitory feedback, and in particular the resulting bursting, on neural correlations need further studies. In order to understand how inhibitory feedback circuit modulates correlations and burst, we carry out numerical simulations of spiking neural network with global inhibitory feedback. Owing to the feedback inhibition, the neurons fire correlated action potentials of a long time scale and exhibit bursting fire pattern. We also found that, with constant output firing rate, the burst firing enhanced network correlations. These results suggest that in the spiking neural network with globally inhibitory feedback the shifts in the feedback strength can induce changes in burst probability, and then effect the correlated firing activities.
C1 [Xie, Jinli; Zhao, Qinjun; Zhao, Jianyu] Univ Jinan, Sch Elect Engn, Jinan 250022, Shandong, Peoples R China.
RP Xie, JL (corresponding author), Univ Jinan, Sch Elect Engn, Jinan 250022, Shandong, Peoples R China.
EM cse_xiejl@ujn.edu.cn; cse_zhaoqj@ujn.edu.cn; cse_zjy@ujn.edu.cn
CR Averbeck BB, 2006, NAT REV NEUROSCI, V7, P358, DOI 10.1038/nrn1888
   Chan HK, 2016, FRONT COMPUT NEUROSC, V10, DOI 10.3389/fncom.2016.00042
   de la Rocha J, 2007, NATURE, V448, P802, DOI 10.1038/nature06028
   Dipoppa M, 2013, FRONT COMPUT NEUROSC, V7, DOI 10.3389/fncom.2013.00139
   Gerkin RC, 2013, P NATL ACAD SCI USA, V110, P17083, DOI 10.1073/pnas.1303830110
   Kepecs A, 2003, NETWORK-COMP NEURAL, V14, P103, DOI 10.1088/0954-898X/14/1/306
   Kim SY, 2015, COGN NEURODYNAMICS, V9, P411, DOI 10.1007/s11571-015-9334-4
   Kohn A, 2005, J NEUROSCI, V25, P3661, DOI 10.1523/JNEUROSCI.5106-04.2005
   Krahe R, 2004, NAT REV NEUROSCI, V5, P13, DOI 10.1038/nrn1296
   Miller KJ, 2007, J NEUROSCI, V27, P2424, DOI 10.1523/JNEUROSCI.3886-06.2007
   Okun M, 2008, NAT NEUROSCI, V11, P535, DOI 10.1038/nn.2105
   Sah N, 2013, EUR J NEUROSCI, V38, P2542, DOI 10.1111/ejn.12262
   Sherman SM, 2002, PHILOS T R SOC B, V357, P1695, DOI 10.1098/rstb.2002.1161
   Smith MA, 2008, J NEUROSCI, V28, P12591, DOI 10.1523/JNEUROSCI.2929-08.2008
   Stephen C, NAT COMMUN
   Xie JL, 2012, NEUROCOMPUTING, V83, P146, DOI 10.1016/j.neucom.2011.12.004
   Xie JL, 2013, COGN NEURODYNAMICS, V7, P325, DOI 10.1007/s11571-013-9241-5
NR 17
TC 0
Z9 0
U1 0
U2 0
PY 2017
VL 10261
BP 529
EP 535
DI 10.1007/978-3-319-59072-1_62
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods
DA 2023-11-11
ER

PT C
AU Chakraborty, B
   Mukhopadhyay, S
AF Chakraborty, Biswadeep
   Mukhopadhyay, Saibal
GP IEEE
TI Brain-Inspired Spiking Neural Network for Online Unsupervised Time
   Series Prediction
SO 2023 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS, IJCNN
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUN 18-23, 2023
CL Broadbeach, AUSTRALIA
DE spiking neural network; recurrent; STDP; Wasserstein distance;
   persistent homologies; online time series prediction
ID OPTIMIZATION
AB Energy and data-efficient online time series prediction for predicting evolving dynamical systems are critical in several fields, especially edge AI applications that need to update continuously based on streaming data. However, current Deep Neural Network (DNN)-based supervised online learning models require a large amount of training data and cannot quickly adapt when the underlying system changes. Moreover, these models require continuous retraining with incoming data making them highly inefficient. We present a novel Continuous Learning-based Unsupervised Recurrent Spiking Neural Network Model (CLURSNN), trained with spike timing dependent plasticity (STDP) to solve these issues. CLURSNN makes online predictions by reconstructing the underlying dynamical system using Random Delay Embedding by measuring the membrane potential of neurons in the recurrent layer of the recurrent spiking neural network (RSNN) with the highest betweenness centrality. We also use topological data analysis to propose a novel methodology using the Wasserstein Distance between the persistent homologies of the predicted and observed time series as a loss function. We show that the proposed online time series prediction methodology outperforms state-of-the-art DNN models when predicting an evolving Lorenz63 dynamical system.
C1 [Chakraborty, Biswadeep; Mukhopadhyay, Saibal] Georgia Inst Technol, Atlanta, GA 30332 USA.
RP Chakraborty, B (corresponding author), Georgia Inst Technol, Atlanta, GA 30332 USA.
EM biswadeep@gatech.edu; saibal.mukhopadhyay@ece.gatech.edu
CR Aljundi R, 2019, PROC CVPR IEEE, P11246, DOI 10.1109/CVPR.2019.01151
   Anava O., 2013, C LEARN THEOR, P172
   Bhatnagar Aadyot, 2021, ARXIV210909265
   Bubenik P., 2020, TOPOLOGICAL DATA ANA, P97, DOI DOI 10.1007/978-3-030-43408-3_4
   Chakraborty B, 2021, FRONT NEUROSCI-SWITZ, V15, DOI 10.3389/fnins.2021.695357
   Chakraborty B, 2021, IEEE T IMAGE PROCESS, V30, P9014, DOI 10.1109/TIP.2021.3122092
   Chakraborty Biswadeep, 2023, 11 INT C LEARN UNPUB
   Chakraborty Biswadeep, 2022, ARXIV221104297
   Chang ZQ, 2021, IEEE INTERNET THINGS, V8, P13849, DOI 10.1109/JIOT.2021.3088875
   Cui LZ, 2018, INT J MACH LEARN CYB, V9, P1399, DOI 10.1007/s13042-018-0834-5
   FREEMAN LC, 1977, SOCIOMETRY, V40, P35, DOI 10.2307/3033543
   Gama J, 2014, ACM COMPUT SURV, V46, DOI 10.1145/2523813
   Gidea M, 2018, PHYSICA A, V491, P820, DOI 10.1016/j.physa.2017.09.028
   Laptev N, 2015, S5 LABELED ANOMALY D
   Leite D, 2020, EVOL SYST-GER, V11, P181, DOI 10.1007/s12530-020-09334-5
   Li SY, 2019, ADV NEUR IN, V32
   Liu CH, 2016, AAAI CONF ARTIF INTE, P1867
   LORENZ EN, 1963, J ATMOS SCI, V20, P130, DOI 10.1175/1520-0469(1963)020<0130:DNF>2.0.CO;2
   Ma HF, 2018, P NATL ACAD SCI USA, V115, pE9994, DOI 10.1073/pnas.1802987115
   Mahyar I, 2018, PHYSICA A, V497, P166, DOI 10.1016/j.physa.2017.12.145
   Margin DA, 2022, INT BLACK SEA CONF, P208, DOI [10.1109/BLACKSEACOM54372.2022.9858322, 10.1109/BlackSeaCom54372.2022.9858322]
   Petro B, 2020, IEEE T NEUR NET LEAR, V31, P358, DOI 10.1109/TNNLS.2019.2906158
   Petropoulos F, 2022, INT J FORECASTING, V38, P705, DOI 10.1016/j.ijforecast.2021.11.001
   Reid David, 2014, PLoS One, V9, pe103656, DOI 10.1371/journal.pone.0103656
   Pool RR, 2011, NEURAL COMPUT, V23, P1768, DOI 10.1162/NECO_a_00140
   Sahoo Doyen, 2017, ARXIV171103705
   She Xueyuan, 2021, INT C LEARN REPR
   Williams RJ, 1989, NEURAL COMPUT, V1, P270, DOI 10.1162/neco.1989.1.2.270
   Yoo S, 2022, IEEE INT MEM WORKSH, P17, DOI 10.1109/IMW52921.2022.9779247
   Yue ZH, 2022, AAAI CONF ARTIF INTE, P8980
NR 30
TC 0
Z9 0
U1 2
U2 2
PY 2023
DI 10.1109/IJCNN54540.2023.10191645
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Jeong, H
   Shi, LP
AF Jeong, Hongsik
   Shi, Luping
TI Memristor devices for neural networks
SO JOURNAL OF PHYSICS D-APPLIED PHYSICS
DT Review
DE memristor; artificial neural network; spiking neural network
ID RESISTIVE-SWITCHING MEMORY; PHASE-CHANGE MEMORY; SPIKING NEURONS;
   STATISTICAL FLUCTUATIONS; SYNAPTIC PLASTICITY; FACE RECOGNITION; AMDAHLS
   LAW; RRAM; DESIGN; ALGORITHM
AB Neural network technologies have taken center stage owing to their powerful computing capability for supporting deep learning in artificial intelligence. However, conventional synaptic devices such as SRAM and DRAM are not satisfactory solutions for neural networks. Recently, several types of memristor devices have become popular alternatives because of their outstanding characteristics such as scalability, high performance, and non-volatility. To understand the characteristics of memristors, a comparison among memristors has been made, considering both maturity and performance. Magneto-resistance random access memory, phase-change random access memory, and resistive random access memory among the proposed memristors are good candidates as synaptic devices for weight storage and matrixvector multiplication required in artificial neural networks (ANNs). Moreover, these devices play key roles as synaptic devices in research for bio-plausible spiking neural networks (SNNs) because their distinctive switching properties are well matched for emulating synaptic and neuron functions of biological neural networks. In this paper we review motivation, advantage, technology, and applications of memristor devices for neural networks from practical approaches of ANNs to futuristic research of SNNs, considering the current status of memristor technology.
C1 [Jeong, Hongsik] Tsinghua Univ, Dept EE, Beijing 100084, Peoples R China.
   [Jeong, Hongsik] Tsinghua Univ, CBICR, Beijing 100084, Peoples R China.
   [Shi, Luping] Tsinghua Univ, Beijing Innovat Ctr Future Chip, Ctr Brain Inspired Comp Res, Beijing 100084, Peoples R China.
   [Shi, Luping] Tsinghua Univ, Precis Instrument Dept, Beijing 100084, Peoples R China.
RP Shi, LP (corresponding author), Tsinghua Univ, Beijing Innovat Ctr Future Chip, Ctr Brain Inspired Comp Res, Beijing 100084, Peoples R China.; Shi, LP (corresponding author), Tsinghua Univ, Precis Instrument Dept, Beijing 100084, Peoples R China.
EM lpshi@mail.tsinghua.edu.cn
CR Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   Alibart F, 2010, ADV FUNCT MATER, V20, P330, DOI 10.1002/adfm.200901335
   Ambrogio S, 2016, IEEE T ELECTRON DEV, V63, P1508, DOI 10.1109/TED.2016.2526647
   Ambrogio S, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00056
   Ambrogio S, 2014, IEEE T ELECTRON DEV, V61, P2920, DOI 10.1109/TED.2014.2330202
   Ambrogio S, 2014, IEEE T ELECTRON DEV, V61, P2912, DOI 10.1109/TED.2014.2330200
   Ando K, 2018, IEEE J SOLID-ST CIRC, V53, P983, DOI 10.1109/JSSC.2017.2778702
   Andrew A.M., 2003, KYBERNETES, V32
   Andri R, 2018, IEEE T COMPUT AID D, V37, P48, DOI 10.1109/TCAD.2017.2682138
   [Anonymous], 2017, ICCAD-IEEE ACM INT
   [Anonymous], S VLSI TECHN
   [Anonymous], PROC IEEE 26TH INT W
   [Anonymous], IEDM DIGEST OF TECHN
   [Anonymous], IEDM DIGEST OF TECHN
   [Anonymous], 2013, IEEE T PATTERN ANAL, DOI DOI 10.1109/TPAMI.2012.59
   [Anonymous], P IEEE 8 INT MEM WOR
   [Anonymous], 2015, IEEE INT SOLID STATE
   [Anonymous], IEDM DIGEST OF TECHN
   [Anonymous], NANOTECHNOLOGY
   [Anonymous], 2010, INT J COMPUTER SCI C
   [Anonymous], 2014, SCI REP
   [Anonymous], P 31 AAAI C ART INT
   [Anonymous], ISSCC
   [Anonymous], 2016, NAT COMMUN
   [Anonymous], 2015, P IEEE INT EL DEV M
   [Anonymous], P IEEE ACM INT S NAN
   [Anonymous], FRONTIERS NEUROSCI
   [Anonymous], IEDM DIGEST OF TECHN
   [Anonymous], 2012, P 1 SO AFRICAN SOLAR
   [Anonymous], IEDM DIGEST OF TECHN
   [Anonymous], P IEEE ACM INT S NAN
   [Anonymous], P SARC INT C
   [Anonymous], IEDM DIGEST OF TECHN
   [Anonymous], IEDM DIGEST OF TECHN
   [Anonymous], 2012 IEEE INT EL DEV
   [Anonymous], SCIENCE
   [Anonymous], PROC ADVANCES IN NEU
   [Anonymous], INT S COMP ARCH
   [Anonymous], P INT JOINT C NEUR N
   [Anonymous], 2013, PROC S VLSI TECH
   [Anonymous], P INT SOC DES C ISOC
   [Anonymous], P INT JOINT C NEUR N
   [Anonymous], P IEEE INT C REB COM
   [Anonymous], ADV FUNCT MAT
   [Anonymous], P MRS
   [Anonymous], 2016, P INT C EMERGING TEC
   [Anonymous], IEDM DIGEST OF TECHN
   [Anonymous], FRONTIERS NEUROSCI
   [Anonymous], ARXIV160907061CSNE
   [Anonymous], ARXIV 1504 00300V2
   [Anonymous], 2013, INT J COMPUTER SCI I
   [Anonymous], IEDM
   [Anonymous], NANOTECHNOLOGY
   [Anonymous], 2015, ACCOUNTING MANAGEMEN
   [Anonymous], IEEE T ELECT DEVICES
   [Anonymous], 2015, INT J COMPUT APPL
   [Anonymous], P IEEE MAGN C INTERM
   [Anonymous], ARXIV170902260V1
   [Anonymous], SCIENCE
   [Anonymous], 2015, INT J MATH SCI COMPU, DOI DOI 10.5815/IJMSC.2015.03.02
   [Anonymous], P IEEE INT JOINT C N
   Apalkov D, 2013, ACM J EMERG TECH COM, V9, DOI 10.1145/2463585.2463589
   Babacan Y, 2016, NEUROCOMPUTING, V203, P86, DOI 10.1016/j.neucom.2016.03.060
   Bagheri-Soulla AA, 2017, MICROELECTRON J, V64, P9, DOI 10.1016/j.mejo.2017.03.014
   Bajo J, 2018, NEUROCOMPUTING, V272, P1, DOI 10.1016/j.neucom.2017.06.022
   Barbounis TG, 2006, NEUROCOMPUTING, V69, P466, DOI 10.1016/j.neucom.2005.02.003
   Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Bessonov AA, 2015, NAT MATER, V14, P199, DOI [10.1038/NMAT4135, 10.1038/nmat4135]
   Bhatti S, 2017, MATER TODAY, V20, P530, DOI 10.1016/j.mattod.2017.07.007
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Bichler O, 2012, IEEE T ELECTRON DEV, V59, P2206, DOI 10.1109/TED.2012.2197951
   Bill J, 2014, FRONT NEUROSCI-SWITZ, V8, DOI 10.3389/fnins.2014.00412
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Borisenko KB, 2011, ACTA MATER, V59, P4335, DOI 10.1016/j.actamat.2011.03.057
   Borst A, 1999, NAT NEUROSCI, V2, P947, DOI 10.1038/14731
   Brette R, 2006, NEURAL COMPUT, V18, P2004, DOI 10.1162/neco.2006.18.8.2004
   Brette R, 2007, J COMPUT NEUROSCI, V23, P349, DOI 10.1007/s10827-007-0038-6
   Brette R, 2012, NETWORK-COMP NEURAL, V23, P167, DOI 10.3109/0954898X.2012.730170
   Brunel N, 2014, CURR OPIN NEUROBIOL, V25, P149, DOI 10.1016/j.conb.2014.01.005
   Burr GW, 2017, ADV PHYS-X, V2, P89, DOI 10.1080/23746149.2016.1259585
   Burr GW, 2015, IEEE T ELECTRON DEV, V62, P3498, DOI 10.1109/TED.2015.2439635
   Cai Y, 2013, 2013 IEEE 31ST INTERNATIONAL CONFERENCE ON COMPUTER DESIGN (ICCD), P123, DOI 10.1109/ICCD.2013.6657034
   Carrillo S, 2013, IEEE T PARALL DISTR, V24, P2451, DOI 10.1109/TPDS.2012.289
   Cassidy AS, 2012, IEEE T COMPUT, V61, P1110, DOI 10.1109/TC.2011.169
   Chang YJ, 2013, J APPL PHYS, V114, DOI 10.1063/1.4829915
   Chanthbouala A, 2012, NAT MATER, V11, P860, DOI [10.1038/nmat3415, 10.1038/NMAT3415]
   Chen WH, 2018, ISSCC DIG TECH PAP I, P494, DOI 10.1109/ISSCC.2018.8310400
   Chen YJ, 2014, INT SYMP MICROARCH, P609, DOI 10.1109/MICRO.2014.58
   Chua L, 2011, APPL PHYS A-MATER, V102, P765, DOI 10.1007/s00339-011-6264-9
   CHUA LO, 1976, P IEEE, V64, P209, DOI 10.1109/PROC.1976.10092
   CHUA LO, 1971, IEEE T CIRCUITS SYST, VCT18, P507, DOI 10.1109/TCT.1971.1083337
   Chung SW, 2016, INT EL DEVICES MEET
   Couet S, 2016, IEEE MAGN LETT, V7, DOI 10.1109/LMAG.2016.2545638
   Courbariaux M., 2016, C NEUR INF PROC SYST
   Covi E, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00482
   Deng L, 2013, INT CONF ACOUST SPEE, P8599, DOI 10.1109/ICASSP.2013.6639344
   Dias FM, 2004, ENG APPL ARTIF INTEL, V17, P945, DOI 10.1016/j.engappai.2004.08.011
   Dieny B, 2017, REV MOD PHYS, V89, DOI 10.1103/RevModPhys.89.025008
   Du ZD, 2015, PROCEEDINGS OF THE 48TH ANNUAL IEEE/ACM INTERNATIONAL SYMPOSIUM ON MICROARCHITECTURE (MICRO-48), P494, DOI 10.1145/2830772.2830789
   Edwards AH, 2015, P IEEE, V103, P1004, DOI 10.1109/JPROC.2015.2441752
   Farabet Clement, 2011, COMP VIS PATT REC WO
   Feng LW, 2010, APPL PHYS LETT, V96, DOI 10.1063/1.3428777
   Fischetti M, 2011, SCI AM, V305, P104, DOI 10.1038/scientificamerican1111-104
   Floreano D, 1998, NEURAL NETWORKS, V11, P1461, DOI 10.1016/S0893-6080(98)00082-3
   Fumarola A, 2018, IEEE J ELECTRON DEVI, V6, P169, DOI 10.1109/JEDS.2017.2782184
   FUNAHASHI K, 1993, NEURAL NETWORKS, V6, P801, DOI 10.1016/S0893-6080(05)80125-X
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   Furber SB, 2013, IEEE T COMPUT, V62, P2454, DOI 10.1109/TC.2012.142
   Gao J, 2016, IEEE T CYBERNETICS, V46, P2323, DOI 10.1109/TCYB.2015.2475376
   Garcia V, 2009, NATURE, V460, P81, DOI 10.1038/nature08128
   GODFREY MD, 1993, IEEE ANN HIST COMPUT, V15, P11, DOI 10.1109/85.194088
   Goodman Dan, 2008, Front Neuroinform, V2, P5, DOI 10.3389/neuro.11.005.2008
   Grezes C, 2016, APPL PHYS LETT, V108, DOI 10.1063/1.4939446
   Gupta S, 2011, INT J ADV COMPUT SC, V2, P50
   HAPPEL BLM, 1994, NEURAL NETWORKS, V7, P985, DOI 10.1016/S0893-6080(05)80155-8
   Hardy NF, 2018, NEURAL COMPUT, V30, P378, DOI [10.1162/NECO_a_01041, 10.1162/neco_a_01041]
   He W, 2016, IEEE T SYST MAN CY-S, V46, P759, DOI 10.1109/TSMC.2015.2466194
   Hebb DO, 1950, J CLIN PSYCHOL, V6, P307
   Hemanth DJ, 2014, NEUROCOMPUTING, V130, P98, DOI 10.1016/j.neucom.2011.12.066
   Hill MD, 2008, COMPUTER, V41, P33, DOI 10.1109/MC.2008.209
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8
   Horowitz M, 2014, ISSCC DIG TECH PAP I, V57, P10, DOI 10.1109/ISSCC.2014.6757323
   Hosomi M., 2005, IEDM
   Hu M, 2018, ADV MATER, V30, DOI 10.1002/adma.201705914
   Hu SG, 2014, NANOSCI NANOTECH LET, V6, P729, DOI 10.1166/nnl.2014.1888
   Hu XF, 2015, NEUROCOMPUTING, V162, P150, DOI 10.1016/j.neucom.2015.03.057
   Hu ZQ, 2013, APPL PHYS LETT, V102, DOI 10.1063/1.4795145
   Ielmini D, 2018, MICROELECTRON ENG, V190, P44, DOI 10.1016/j.mee.2018.01.009
   Ielmini D, 2010, APPL PHYS LETT, V96, DOI 10.1063/1.3304167
   Ikeda S, 2010, NAT MATER, V9, P721, DOI [10.1038/NMAT2804, 10.1038/nmat2804]
   Ikeda S, 2007, IEEE T ELECTRON DEV, V54, P991, DOI 10.1109/TED.2007.894617
   Indiveri G., 2015, P EL DEV M IEDM 2015, p4.2.1, DOI [DOI 10.1109/IEDM.2015.7409623, 10.1109/iedm.2015.7409623]
   Indiveri G, 2015, P IEEE, V103, P1379, DOI 10.1109/JPROC.2015.2444094
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Jain AK, 1996, COMPUTER, V29, P31, DOI 10.1109/2.485891
   Jo SH, 2010, NANO LETT, V10, P1297, DOI 10.1021/nl904092h
   Jouppi NP, 2018, IEEE MICRO, V38, P10, DOI 10.1109/MM.2018.032271057
   Juefei-Xu F, 2017, PROC CVPR IEEE, P4284, DOI 10.1109/CVPR.2017.456
   Jung MC, 2007, APPL PHYS LETT, V91, DOI 10.1063/1.2773959
   Kaastra I, 1996, NEUROCOMPUTING, V10, P215, DOI 10.1016/0925-2312(95)00039-9
   Kang DH, 2015, NEUROCOMPUTING, V155, P153, DOI 10.1016/j.neucom.2014.12.036
   Kang D, 2016, ISSCC DIG TECH PAP I, V59, P130, DOI 10.1109/ISSCC.2016.7417941
   Kasabov N, 2013, NEURAL NETWORKS, V41, P188, DOI 10.1016/j.neunet.2012.11.014
   Kasabov NK, 2014, NEURAL NETWORKS, V52, P62, DOI 10.1016/j.neunet.2014.01.006
   Kawahara A., 2012, 2012 IEEE International Solid-State Circuits Conference (ISSCC), P432, DOI 10.1109/ISSCC.2012.6177078
   Kim D, 2016, IEEE T COMPUT, V65, P1720, DOI 10.1109/TC.2015.2462819
   Kim J. P., 2011, 2011 Symposium on VLSI Circuits. Digest of Technical Papers, P296
   Kim KH, 2010, APPL PHYS LETT, V96, DOI 10.1063/1.3294625
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kuderer M, 2015, IEEE INT CONF ROBOT, P2641, DOI 10.1109/ICRA.2015.7139555
   KUMAR K., 2012, INT J INF TECHNOL CO, V6, P57, DOI [10.5815/ijitcs.2012.06.08, DOI 10.5815/IJITCS.2012.06.08]
   Kuzum D, 2013, NANOTECHNOLOGY, V24, DOI 10.1088/0957-4484/24/38/382001
   Kuzum D, 2012, NANO LETT, V12, P2179, DOI 10.1021/nl201040y
   Lashkare S, 2017, IEEE ELECTR DEVICE L, V38, P1212, DOI 10.1109/LED.2017.2723503
   Lawrence S, 1997, IEEE T NEURAL NETWOR, V8, P98, DOI 10.1109/72.554195
   Lee HY, 2010, INT EL DEVICES MEET
   Lee HS, 2013, IEEE T CONSUM ELECTR, V59, P107, DOI 10.1109/TCE.2013.6490248
   Lee J, 2011, IEEE ELECTR DEVICE L, V32, P1113, DOI 10.1109/LED.2011.2157075
   Lee KJ, 2008, IEEE J SOLID-ST CIRC, V43, P150, DOI 10.1109/JSSC.2007.908001
   Lee S, 2015, NAT COMMUN, V6, DOI 10.1038/ncomms9407
   Levine DS, 2007, PHYS LIFE REV, V4, P37, DOI 10.1016/j.plrev.2006.10.001
   Li KS, 2014, S VLSI TECH
   Liu JC, 2015, IEEE T ELECTRON DEV, V62, P2510, DOI 10.1109/TED.2015.2444663
   Liu Q, 2010, ACS NANO, V4, P6162, DOI 10.1021/nn1017582
   Liu TY, 2014, IEEE J SOLID-ST CIRC, V49, P140, DOI [10.1109/JSSC.2013.2280296, 10.1109/ISSCC.2013.6487703]
   Lu WZ, 2003, NEUROCOMPUTING, V51, P387, DOI 10.1016/S0925-2312(02)00623-9
   Lukosevicius M, 2009, COMPUT SCI REV, V3, P127, DOI 10.1016/j.cosrev.2009.03.005
   Luo T, 2017, IEEE T COMPUT, V66, P73, DOI 10.1109/TC.2016.2574353
   Maass W, 1997, NETWORK-COMP NEURAL, V8, P355, DOI 10.1088/0954-898X/8/4/002
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Majumdar A, 2012, ACM T ARCHIT CODE OP, V9, DOI 10.1145/2133382.2133388
   Malik P, 2013, IBM J RES DEV, V57, DOI 10.1147/JRD.2013.2241359
   Matsugu M, 2003, NEURAL NETWORKS, V16, P555, DOI 10.1016/S0893-6080(03)00115-1
   McCarthy J, 2006, AI MAG, V27, P12
   Meena JS, 2014, NANOSCALE RES LETT, V9, DOI 10.1186/1556-276X-9-526
   Merced-Grafals EJ, 2016, NANOTECHNOLOGY, V27, DOI 10.1088/0957-4484/27/36/365202
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Mikolov T, 2011, INT CONF ACOUST SPEE, P5528
   Misra J, 2010, NEUROCOMPUTING, V74, P239, DOI 10.1016/j.neucom.2010.03.021
   MOORE GE, 1995, P SOC PHOTO-OPT INS, V2438, P2, DOI 10.1117/12.209244
   Nageswaran JM, 2009, NEURAL NETWORKS, V22, P791, DOI 10.1016/j.neunet.2009.06.028
   Naous R, 2016, AIP ADV, V6, DOI 10.1063/1.4967352
   Nayak A, 2012, ADV FUNCT MATER, V22, P3606, DOI 10.1002/adfm.201200640
   Neil D, 2014, IEEE T VLSI SYST, V22, P2621, DOI 10.1109/TVLSI.2013.2294916
   Nguyen HN, 2015, NEUROCOMPUTING, V151, P996, DOI 10.1016/j.neucom.2014.03.085
   Ni LB, 2017, IEEE J EXPLOR SOLID-, V3, P37, DOI 10.1109/JXCDC.2017.2697910
   Noble JA, 2006, IEEE T MED IMAGING, V25, P987, DOI 10.1109/TMI.2006.877092
   Ohno T, 2011, NAT MATER, V10, P591, DOI [10.1038/NMAT3054, 10.1038/nmat3054]
   Olshausen BA, 2004, CURR OPIN NEUROBIOL, V14, P481, DOI 10.1016/j.conb.2004.07.007
   Pantazi A, 2016, NANOTECHNOLOGY, V27, DOI 10.1088/0957-4484/27/35/355205
   Park S, 2017, ECS J SOLID STATE SC, V6, pN148, DOI 10.1149/2.0151709jss
   Pedretti G, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-05480-0
   Pelácz JI, 2014, INT J COMPUT INT SYS, V7, P344, DOI 10.1080/18756891.2014.889498
   Pellizzer F, 2004, 2004 SYMPOSIUM ON VLSI TECHNOLOGY, DIGEST OF TECHNICAL PAPERS, P18
   Pérez E, 2017, IEEE J ELECTRON DEVI, V5, P64, DOI 10.1109/JEDS.2016.2618425
   Ponulak F, 2011, ACTA NEUROBIOL EXP, V71, P409
   Prezioso M, 2015, NATURE, V521, P61, DOI 10.1038/nature14441
   Prusa Joseph D., 2017, Journal of Big Data, V4, DOI 10.1186/s40537-017-0065-8
   Qiao N, 2016, BIOMED CIRC SYST C, P552, DOI 10.1109/BioCAS.2016.7833854
   Qiao N, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00141
   Querlioz D, 2013, IEEE T NANOTECHNOL, V12, P288, DOI 10.1109/TNANO.2013.2250995
   Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32
   Reed S., 2013, COGNITION THEORIES A, V9th
   Reese MG, 2001, COMPUT CHEM, V26, P51, DOI 10.1016/S0097-8485(01)00099-7
   Russo U, 2009, IEEE T ELECTRON DEV, V56, P1040, DOI 10.1109/TED.2009.2016019
   Ryoo KC, 2011, JPN J APPL PHYS, V50, DOI 10.1143/JJAP.50.04DD15
   Sagara T, 2014, NEUROCOMPUTING, V142, P201, DOI 10.1016/j.neucom.2014.04.048
   Sakamoto T, 2003, APPL PHYS LETT, V82, P3032, DOI 10.1063/1.1572964
   Schemmel J, 2007, IEEE INT SYMP CIRC S, P3367, DOI 10.1109/ISCAS.2007.378289
   Schemmel J, 2010, IEEE INT SYMP CIRC S, P1947, DOI 10.1109/ISCAS.2010.5536970
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Sengupta A, 2015, APPL PHYS LETT, V106, DOI 10.1063/1.4914111
   Seo K, 2011, NANOTECHNOLOGY, V22, DOI 10.1088/0957-4484/22/25/254023
   Sheridan PM, 2017, NAT NANOTECHNOL, V12, P784, DOI [10.1038/NNANO.2017.83, 10.1038/nnano.2017.83]
   Shimeng Yu, 2016, IEEE Solid-State Circuits Magazine, V8, P43, DOI 10.1109/MSSC.2016.2546199
   Shin D, 2017, ISSCC DIG TECH PAP I, P240, DOI 10.1109/ISSCC.2017.7870350
   Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Song S, 2001, NEURON, V32, P339, DOI 10.1016/S0896-6273(01)00451-2
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Strukov DB, 2008, NATURE, V453, P80, DOI 10.1038/nature06932
   Sun ZL, 2014, NEUROCOMPUTING, V128, P153, DOI 10.1016/j.neucom.2012.11.057
   Szatmáry B, 2010, PLOS COMPUT BIOL, V6, DOI 10.1371/journal.pcbi.1000879
   Tuma T, 2016, IEEE ELECTR DEVICE L, V37, P1238, DOI 10.1109/LED.2016.2591181
   Tuma T, 2016, NAT NANOTECHNOL, V11, P693, DOI [10.1038/nnano.2016.70, 10.1038/NNANO.2016.70]
   Turan M, 2018, NEUROCOMPUTING, V275, P1861, DOI 10.1016/j.neucom.2017.10.014
   Turing AM, 1937, P LOND MATH SOC, V42, P230, DOI 10.1112/plms/s2-42.1.230
   Valov I, 2011, NANOTECHNOLOGY, V22, DOI 10.1088/0957-4484/22/25/254003
   Vincent AF, 2015, IEEE T BIOMED CIRC S, V9, P166, DOI 10.1109/TBCAS.2015.2414423
   Vongehr S, 2015, SCI REP-UK, V5, DOI 10.1038/srep11657
   Wang M, 2010, S VLSI TECH, P89, DOI 10.1109/VLSIT.2010.5556182
   Wang T, 2016, IEEE T NEUR NET LEAR, V27, P416, DOI 10.1109/TNNLS.2015.2411671
   Wang X, 2016, SCI REP-UK, V6, DOI 10.1038/srep27624
   Wang YY, 2015, PROCEEDINGS OF THE 2015 SYMPOSIUM ON PIEZOELECTRICITY, ACOUSTIC WAVES AND DEVICE APPLICATIONS, P189, DOI 10.1109/SPAWDA.2015.7364469
   Wang ZR, 2017, NAT MATER, V16, P101, DOI [10.1038/nmat4756, 10.1038/NMAT4756]
   Werner T, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00474
   WIDROW B, 1990, P IEEE, V78, P1415, DOI 10.1109/5.58323
   Wu QT, 2018, NANOSCALE, V10, P5875, DOI 10.1039/c8nr00222c
   Wu YT, 2012, PROCEEDINGS OF THE ASME INTERNATIONAL MECHANICAL ENGINEERING CONGRESS AND EXPOSITION, 2011, VOL 4, PTS A AND B, P1
   Wulfkuhle JD, 2003, NAT REV CANCER, V3, P267, DOI 10.1038/nrc1043
   Wuttig M, 2007, NAT MATER, V6, P824, DOI 10.1038/nmat2009
   Xia LX, 2016, DES AUT TEST EUROPE, P469
   Xia LX, 2016, J COMPUT SCI TECH-CH, V31, P3, DOI 10.1007/s11390-016-1608-8
   Xiong F, 2013, NANO LETT, V13, P464, DOI 10.1021/nl3038097
   Yang JJS, 2013, NAT NANOTECHNOL, V8, P13, DOI [10.1038/nnano.2012.240, 10.1038/NNANO.2012.240]
   Yang R, 2012, ACS NANO, V6, P9515, DOI 10.1021/nn302510e
   Yang SC, 2017, MECH SYST SIGNAL PR, V87, P81, DOI 10.1016/j.ymssp.2016.04.015
   Yoon SM, 2011, APPL PHYS A-MATER, V102, P983, DOI 10.1007/s00339-011-6280-9
   Yu SM, 2018, P IEEE, V106, P260, DOI 10.1109/JPROC.2018.2790840
   Yu SM, 2013, ADV MATER, V25, P1774, DOI 10.1002/adma.201203680
   Yu SM, 2011, IEEE T ELECTRON DEV, V58, P2729, DOI 10.1109/TED.2011.2147791
   Zbontar J, 2016, J MACH LEARN RES, V17
   Zhang JJ, 2015, IEEE INTELL SYST, V30, P16, DOI 10.1109/MIS.2015.69
   Zhang Y, 2018, IEEE T CIRCUITS-I, V65, P677, DOI 10.1109/TCSI.2017.2729787
   Zhang YN, 2005, IEEE T NEURAL NETWOR, V16, P1477, DOI 10.1109/TNN.2005.857946
   Zhao H, 2012, J PHYS D APPL PHYS, V45, DOI 10.1088/0022-3727/45/2/025001
   Zhao W, 2003, ACM COMPUT SURV, V35, P399, DOI 10.1145/954339.954342
   Zheng N, 2018, IEEE T NANOTECHNOL, V17, P520, DOI 10.1109/TNANO.2018.2821131
   Zhong SM, 2012, IEEE T NEUR NET LEAR, V23, P480, DOI 10.1109/TNNLS.2011.2177860
   Zhou H, 2017, J ARTIF INTELL RES, V58, P703, DOI 10.1613/jair.5259
   Zhou XL, 2013, APPL PHYS LETT, V103, DOI 10.1063/1.4818662
NR 263
TC 71
Z9 73
U1 21
U2 340
PD JAN 9
PY 2019
VL 52
IS 2
AR 023003
DI 10.1088/1361-6463/aae223
WC Physics, Applied
DA 2023-11-11
ER

PT J
AU Karakida, R
   Igarashi, Y
   Nagata, K
   Okada, M
AF Karakida, Ryo
   Igarashi, Yasuhiko
   Nagata, Kenji
   Okada, Masato
TI Inter-Layer Correlation in a Feed-Forward Network with Intra-Layer
   Common Noise
SO JOURNAL OF THE PHYSICAL SOCIETY OF JAPAN
DT Article
DE feed-forward neural network; common noise; synchronous firing; spike
   correlation
ID HIGHER-ORDER INTERACTIONS; PRIMARY VISUAL-CORTEX; NEURAL POPULATION;
   CORTICAL ACTIVITY; DISCHARGE
AB Neural networks generate correlated neural activities. In a multi-layer network, experimental studies have shown that spike correlations appear within a layer and between different layers. It is input common among neurons in each layer that realizes such correlated activities. Theoretical studies have demonstrated that common input given to neurons within a layer, which we call "intra-layer common noise'', generates spike correlation within the layer, which is "intralayer correlation'', in a feed-forward network. However, it has not been studied whether the common noise can generate spike correlation between different layers, which is "inter-layer correlation''. In this study, we constructed a theory of inter-layer correlation and calculated the theoretical values of the inter-layer correlation in a multi-layer feedforward network with intra-layer common noise. Our theory revealed that the common noise generates the inter-layer correlation, which coincided with results of simulation.
C1 [Karakida, Ryo; Igarashi, Yasuhiko; Nagata, Kenji; Okada, Masato] Univ Tokyo, Grad Sch Frontier Sci, Kashiwa, Chiba 2778561, Japan.
   [Igarashi, Yasuhiko] Japan Soc Promot Sci, Chiyoda Ku, Tokyo 1028472, Japan.
   [Okada, Masato] RIKEN, Brain Sci Inst, Wako, Saitama 3510198, Japan.
RP Karakida, R (corresponding author), Univ Tokyo, Grad Sch Frontier Sci, Kashiwa, Chiba 2778561, Japan.
EM okada@k.u-tokyo.ac.jp
CR ABELES M, 1993, J NEUROPHYSIOL, V70, P1629, DOI 10.1152/jn.1993.70.4.1629
   Amari S, 2003, NEURAL COMPUT, V15, P127, DOI 10.1162/089976603321043720
   Ganmor E, 2011, P NATL ACAD SCI USA, V108, P9679, DOI 10.1073/pnas.1019641108
   GEORGOPOULOS AP, 1986, SCIENCE, V233, P1416, DOI 10.1126/science.3749885
   Hamaguchi K, 2005, NEURAL COMPUT, V17, P2034, DOI 10.1162/0899766054322937
   Hansen BJ, 2012, NEURON, V76, P590, DOI 10.1016/j.neuron.2012.08.029
   Ikegaya Y, 2004, SCIENCE, V304, P559, DOI 10.1126/science.1093173
   Kohn A, 2005, J NEUROSCI, V25, P3661, DOI 10.1523/JNEUROSCI.5106-04.2005
   Macke JH, 2011, PHYS REV LETT, V106, DOI 10.1103/PhysRevLett.106.208102
   Montani F, 2009, PHILOS T R SOC A, V367, P3297, DOI 10.1098/rsta.2009.0082
   Ohiorhenuan IE, 2010, NATURE, V466, P617, DOI 10.1038/nature09178
   Rosenbaum RJ, 2010, FRONT COMPUT NEUROSC, V4, DOI 10.3389/fncom.2010.00009
   Schneidman E, 2006, NATURE, V440, P1007, DOI 10.1038/nature04701
   Shimazaki H, 2012, PLOS COMPUT BIOL, V8, DOI 10.1371/journal.pcbi.1002385
   Tchumatchenko T, 2010, PHYS REV LETT, V104, DOI 10.1103/PhysRevLett.104.058102
   Yamana M, 2005, J PHYS SOC JPN, V74, P2260, DOI 10.1143/JPSJ.74.2260
   Yu JN, 2010, NEURON, V68, P1187, DOI 10.1016/j.neuron.2010.11.027
   Yu S, 2011, J NEUROSCI, V31, P17514, DOI 10.1523/JNEUROSCI.3127-11.2011
   ZOHARY E, 1994, NATURE, V370, P140, DOI 10.1038/370140a0
NR 19
TC 0
Z9 0
U1 0
U2 9
PD JUN
PY 2013
VL 82
IS 6
DI 10.7566/JPSJ.82.064007
WC Physics, Multidisciplinary
DA 2023-11-11
ER

PT C
AU Cardarilli, GC
   Cristini, A
   Di Nunzio, L
   Re, M
   Salerno, M
   Susi, G
AF Cardarilli, Gian Carlo
   Cristini, Alessandro
   Di Nunzio, Luca
   Re, Marco
   Salerno, Mario
   Susi, Gianluca
BE Matthews, MB
TI Spiking Neural Networks based on LIF with Latency: Simulation and
   Synchronization Effects
SO 2013 ASILOMAR CONFERENCE ON SIGNALS, SYSTEMS AND COMPUTERS
SE Conference Record of the Asilomar Conference on Signals Systems and
   Computers
DT Proceedings Paper
CT 47th Asilomar Conference on Signals, Systems, and Computers
CY NOV 03-06, 2013
CL Pacific Grove, CA
DE LIF with Latency Model; Spiking Neural Networks; Synchronization
   Effects; Jitter; intelligent DSP applications
ID EVENT-DRIVEN SIMULATION; ANTEROVENTRAL COCHLEAR NUCLEUS; SYNCHRONOUS
   SPIKING; SYNAPTIC INPUT; TEMPORAL INFORMATION; STABLE PROPAGATION;
   ELECTRIC FISH; FIRING RATES; NEURONS; MODEL
AB In this paper, a work on spiking neural networks based on a model of a kind of Leaky Integrate-and-Fire (LIF) neuron with latency is presented. Efficient simulations are carried out through an ad hoc event-driven approach, highlighting some particular effects of synchrony in a simple feedforward network topology. These results are consistent with literature results and, thanks to the implementation of the biologically plausible latency effect in the model, new results have emerged from the simulations. The authors plan to apply these results in the near future to applications in which this kind of neural networks and Digital Signal Processing (DSP) applications can be merged to obtain powerful nonlinear DSP techniques. In the plan of the authors is also the definition of a hardware prototype of the network based on analog/digital techniques.
C1 [Cardarilli, Gian Carlo; Cristini, Alessandro; Di Nunzio, Luca; Re, Marco; Salerno, Mario; Susi, Gianluca] Univ Roma Tor Vergata, Dept Elect Engn, I-00133 Rome, Italy.
RP Cardarilli, GC (corresponding author), Univ Roma Tor Vergata, Dept Elect Engn, Via Politecn 1, I-00133 Rome, Italy.
EM cardarilli@ieee.org; alessandro.cristini84@gmail.com;
   di.nunzio@ing.uniroma2.it; re@ieee.org; salerno@eln.uniroma2.it;
   gianluca.susi@uniroma2.it
CR Aertsen A, 1996, J PHYSIOLOGY-PARIS, V90, P243, DOI 10.1016/S0928-4257(97)81432-5
   [Anonymous], 1968, NEUROSCI RES PROGR B
   Belatreche A, 2007, SOFT COMPUT, V11, P239, DOI [10.1007/s00500-006-0065-7, 10.1007/S00500-006-0065-7]
   Brette R, 2007, J COMPUT NEUROSCI, V23, P349, DOI 10.1007/s10827-007-0038-6
   Burkitt AN, 2006, BIOL CYBERN, V95, P97, DOI 10.1007/s00422-006-0082-8
   Burkitt AN, 2006, BIOL CYBERN, V95, P1, DOI 10.1007/s00422-006-0068-6
   Burkitt AN, 1999, NEURAL COMPUT, V11, P871, DOI 10.1162/089976699300016485
   CARR CE, 1993, ANNU REV NEUROSCI, V16, P223, DOI 10.1146/annurev.ne.16.030193.001255
   CARR CE, 1986, J NEUROSCI, V6, P107
   Câteau H, 2001, NEURAL NETWORKS, V14, P675, DOI 10.1016/S0893-6080(01)00065-X
   Citri A, 2008, NEUROPSYCHOPHARMACOL, V33, P18, DOI 10.1038/sj.npp.1301559
   D'Haene M, 2009, NEURAL COMPUT, V21, P1068, DOI 10.1162/neco.2008.02-08-707
   Diesmann M, 1999, NATURE, V402, P529, DOI 10.1038/990101
   Edelman GM., 1987, NEURAL DARWINISM THE
   FitzHugh R., 1955, B MATH BIOPHYS, V17, P257, DOI DOI 10.1007/BF02477753
   Guo YX, 1997, J NEUROSCI, V17, P1761
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   JORIS PX, 1994, J NEUROPHYSIOL, V71, P1037, DOI 10.1152/jn.1994.71.3.1037
   JORIS PX, 1994, J NEUROPHYSIOL, V71, P1022, DOI 10.1152/jn.1994.71.3.1022
   Kistler WM, 2002, NEURAL COMPUT, V14, P987, DOI 10.1162/089976602753633358
   Kumar A, 2008, J NEUROSCI, V28, P5268, DOI 10.1523/JNEUROSCI.2542-07.2008
   Kumar A, 2010, NAT REV NEUROSCI, V11, P615, DOI 10.1038/nrn2886
   Litvak V, 2003, J NEUROSCI, V23, P3006
   Maass W, 2002, PHYS NEUR N, P373
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Marsalek PR, 1997, P NATL ACAD SCI USA, V94, P735, DOI 10.1073/pnas.94.2.735
   Mattia M, 2000, NEURAL COMPUT, V12, P2305, DOI 10.1162/089976600300014953
   Paolini AG, 2001, HEARING RES, V159, P101, DOI 10.1016/S0378-5955(01)00327-6
   Pearson MJ, 2007, IEEE T NEURAL NETWOR, V18, P1472, DOI 10.1109/TNN.2007.891203
   Ponulak F, 2011, ACTA NEUROBIOL EXP, V71, P409
   Ros E, 2006, NEURAL COMPUT, V18, P2959, DOI 10.1162/neco.2006.18.12.2959
   Rosselló JL, 2012, INT J NEURAL SYST, V22, DOI 10.1142/S0129065712500141
   Salerno M., 2013, ACEEE INT J INFORM T, V3
   Salerno M, 2011, BIOINFORMATICS 2011, P116
   van Rossum MCW, 2002, J NEUROSCI, V22, P1956, DOI 10.1523/JNEUROSCI.22-05-01956.2002
   Vogels TP, 2005, J NEUROSCI, V25, P10786, DOI 10.1523/JNEUROSCI.3508-05.2005
NR 37
TC 5
Z9 5
U1 0
U2 2
PY 2013
BP 1838
EP 1842
WC Computer Science, Information Systems; Engineering, Electrical &
   Electronic; Telecommunications
DA 2023-11-11
ER

PT C
AU Ambrose, JD
   Foshie, AZ
   Dean, ME
   Plank, JS
   Rose, GS
   Mitchell, JP
   Schuman, CD
   Bruer, G
AF Ambrose, Jonathan D.
   Foshie, Adam Z.
   Dean, Mark E.
   Plank, James S.
   Rose, Garrett S.
   Mitchell, J. Parker
   Schuman, Catherine D.
   Bruer, Grant
GP IEEE
TI GRANT: Ground-Roaming Autonomous Neuromorphic Targeter
SO 2020 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN) held as part
   of the IEEE World Congress on Computational Intelligence (IEEE WCCI)
CY JUL 19-24, 2020
CL ELECTR NETWORK
DE Autonomous Robots; Field-programmable Gate Array; Neural Network
   Hardware; Neuromorphic; Recurrent Spiking Neural Networks; Robot Control
AB In this work we describe the design, implementation, and testing of the first neuromorphic robot capable of obstacle avoidance, grid coverage, and targeting controlled by the second generation Dynamic Adaptive Neural Network Array (DANNA2) digital spiking neuromorphic processor. The simplicity of the DANNA2 processor along with the TENNLab hardware/software co-design framework allows for compact spiking networks that can run efficiently on a small, resource-constrained, platform such as a Xilinx Artix-7 field-programmable gate array. Additionally, we present the dynamic reconfigurability of DANNA2 arrays as a method of realizing complex, multi-objective tasks on hardware that is restricted to relatively small networks.
C1 [Ambrose, Jonathan D.; Foshie, Adam Z.; Dean, Mark E.; Plank, James S.; Rose, Garrett S.] Univ Tennessee, Dept Elect Engn & Comp Sci, Knoxville, TN 37996 USA.
   [Mitchell, J. Parker; Schuman, Catherine D.] Oak Ridge Natl Lab, POB 2009, Oak Ridge, TN 37830 USA.
   [Bruer, Grant] Georgia Inst Technol, Sch Computat Sci & Engn, Atlanta, GA 30332 USA.
RP Ambrose, JD (corresponding author), Univ Tennessee, Dept Elect Engn & Comp Sci, Knoxville, TN 37996 USA.
EM jambros2@vols.utk.edu; afoshie@vols.utk.edu; markdean@utk.edu;
   jplank@utk.edu; garose@utk.edu; mitchelljp1@ornl.gov;
   schumancd@ornl.gov; gbruer@gatech.edu
CR Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   [Anonymous], 2016, INT JOINT C NEUR NET
   Bekey G.A., 2012, NEURAL NETWORKS ROBO, V202
   Bobrowicz S., 2018, ARTY Z7 20 BASE LINU
   Bojarski Mariusz, 2016, arXiv
   Cabessa J, 2014, INT J NEURAL SYST, V24, DOI 10.1142/S0129065714500294
   Fischl K. D., 2017, P 51 ANN C INF SCI S, P1, DOI DOI 10.1109/CISS.2017.7926179
   Huang T., 2019, RPLIDAR SDK
   Hwu T, 2017, IEEE IJCNN, P635, DOI 10.1109/IJCNN.2017.7965912
   Larson S., 2017, QUADRATURE DECODER V
   LeGrand R., 2019, PIXY2
   Liu WB, 2017, NEUROCOMPUTING, V234, P11, DOI 10.1016/j.neucom.2016.12.038
   Lohmann N., 2019, JSON MODERN C
   Merolla P., 2011, IEEE CUST INT CIRC C, P1, DOI DOI 10.1109/CICC.2011.6055294
   Milde MB, 2017, FRONT NEUROROBOTICS, V11, DOI 10.3389/fnbot.2017.00028
   Mitchell J., THESIS
   Mitchell JP, 2017, 2017 IEEE 5TH INTERNATIONAL SYMPOSIUM ON ROBOTICS AND INTELLIGENT SENSORS (IRIS), P136, DOI 10.1109/IRIS.2017.8250111
   Orsucci M., 2018, PETALINUX ARTY Z7 20
   Perez B., 2018, XILINX AXIDMA
   Plank J. S., 2017, IEEE INT C REB COMP
   Plank J S., 2018, IEEE LETT COMPUT SOC, V1, P17, DOI [DOI 10.1109/LOCS.2018.2885976, 10.1109/LOCS.2018.2885976]
   Reynolds J. J.M., 2019, IEEE IJCNN, P1, DOI [10.1109/IJCNN.2019.8852472, DOI 10.1109/ijcnn.2019.8852472]
   Schuman C. D., 2017, CORR
   Severa W, 2019, NAT MACH INTELL, V1, P86, DOI 10.1038/s42256-018-0015-y
   Shrestha Sumit Bam, 2018, ADV NEURAL INFORM PR, P1412
   Vreeken J, 2003, SPIKING NEURAL NETWO
   Winternitz, 2018, INT C NEUR COMP SYST, V164, P1, DOI [10.1145/3229884.3229894, DOI 10.1145/3229884.3229894]
NR 27
TC 2
Z9 2
U1 0
U2 0
PY 2020
DI 10.1109/ijcnn48605.2020.9207276
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture
DA 2023-11-11
ER

PT J
AU Wang, Y
   Wang, T
   Liu, LY
AF Wang, Yi
   Wang, Tao
   Liu, Liyuan
TI A fault segment location method for distribution networks based on
   spiking neural P systems and Bayesian estimation
SO PROTECTION AND CONTROL OF MODERN POWER SYSTEMS
DT Article
DE Distribution network; Fault location; Spiking neural P system; Bayesian
   estimation; Contradiction principle
ID DIAGNOSIS; IDENTIFICATION; SCHEME
AB With the increasing scale of distribution networks and the mass access of distributed generation, traditional centralized fault location methods can no longer meet the performance requirements of speed and high accuracy. Therefore, this paper proposes a fault segment location method based on spiking neural P systems and Bayesian estimation for distribution networks with distributed generation. First, the distribution network system topology is decoupled into single-branch networks. A spiking neural P system with excitatory and inhibitory synapses is then proposed to model the suspected faulty segment, and its matrix reasoning algorithm is executed to obtain a preliminary set of location results. Finally, the Bayesian estimation and contradiction principle are applied to verify and correct the initial results to obtain the final location results. Simulation results based on the IEEE 33-node system validate the feasibility and effectiveness of the proposed method.
C1 [Wang, Yi; Wang, Tao; Liu, Liyuan] Xihua Univ, Sch Elect Engn & Elect Informat, Chengdu 610039, Peoples R China.
   [Wang, Tao] Xihua Univ, Minist Educ, Key Lab Fluid & Power Machinery, Chengdu 610039, Peoples R China.
RP Wang, T (corresponding author), Xihua Univ, Sch Elect Engn & Elect Informat, Chengdu 610039, Peoples R China.; Wang, T (corresponding author), Xihua Univ, Minist Educ, Key Lab Fluid & Power Machinery, Chengdu 610039, Peoples R China.
EM wangatao2005@163.com
CR Al-shaher MA, 2003, ELECTR POW SYST RES, V64, P87, DOI 10.1016/S0378-7796(02)00174-8
   Brahma SM, 2011, IEEE T POWER DELIVER, V26, P1545, DOI 10.1109/TPWRD.2011.2106146
   Cai YX, 2011, IEEE T POWER SYST, V26, P794, DOI 10.1109/TPWRS.2010.2055899
   Chen R, 2019, IEEE ACCESS, V7, P181972, DOI 10.1109/ACCESS.2019.2959427
   Chen WH, 2000, IEEE T POWER DELIVER, V15, P710, DOI 10.1109/61.853009
   Cheng L, 2022, PROT CONTR MOD POW, V7, DOI 10.1186/s41601-022-00265-8
   Chien CF, 2002, IEEE T POWER DELIVER, V17, P785, DOI 10.1109/TPWRD.2002.1022804
   Choi MS, 2004, IEEE T POWER DELIVER, V19, P35, DOI 10.1109/TPWRD.2003.820433
   Deng Y, 2022, INT J COMPUT COMMUN, V17, DOI 10.15837/ijccc.2022.1.4542
   [郭利爽 Guo Lishuang], 2020, [电力系统保护与控制, Power System Protection and Control], V48, P76
   Jamali S, 2020, PROT CONTR MOD POW, V5, DOI 10.1186/s41601-020-00162-y
   Jia K, 2020, IEEE T SMART GRID, V11, P312, DOI 10.1109/TSG.2019.2921301
   Jia K, 2013, IEEE T POWER DELIVER, V28, P594, DOI 10.1109/TPWRD.2013.2238560
   Jia K, 2013, IEEE T POWER DELIVER, V28, P38, DOI 10.1109/TPWRD.2012.2215346
   Jiang ZY, 2018, IEEE SYST J, V12, P2566, DOI 10.1109/JSYST.2017.2682185
   [焦彦军 Jiao Yanjun], 2014, [电力系统保护与控制, Power System Protection and Control], V42, P43
   Kiaei I, 2020, IEEE T SMART GRID, V11, P74, DOI 10.1109/TSG.2019.2917506
   Kumar AN, 2020, INT J INTERACT MULTI, V6, P41, DOI 10.9781/ijimai.2019.06.002
   [李明阳 Li Mingyang], 2021, [电力系统及其自动化学报, Proceedings of the CSU-EPSA], V33, P79
   [李钰洋 Li Yuyang], 2021, [电网技术, Power System Technology], V45, P3917
   Ma DY, 2013, ENG APPL ARTIF INTEL, V26, P937, DOI 10.1016/j.engappai.2012.03.017
   Päun G, 2000, J COMPUT SYST SCI, V61, P108, DOI 10.1006/jcss.1999.1693
   Peng H, 2018, IEEE T SMART GRID, V9, P4777, DOI 10.1109/TSG.2017.2670602
   Peng L, 2018, J INTELL FUZZY SYST, V35, P223, DOI 10.3233/JIFS-169582
   Ren FH, 2013, IEEE T POWER SYST, V28, P1442, DOI 10.1109/TPWRS.2012.2223490
   Santos WC, 2017, IEEE T POWER DELIVER, V32, P23, DOI 10.1109/TPWRD.2016.2548942
   Shen SF, 2017, IEEE T POWER DELIVER, V32, P411, DOI 10.1109/TPWRD.2015.2506155
   Sun Z, 2021, INT J PARALLEL EMERG, V36, P11, DOI 10.1080/17445760.2019.1682145
   Tu M, 2014, CHINESE J ELECTRON, V23, P87
   Wang J, 2019, ENG APPL ARTIF INTEL, V82, P102, DOI 10.1016/j.engappai.2019.03.014
   Wang T, 2022, INFORM SCIENCES, V596, P520, DOI 10.1016/j.ins.2022.03.013
   Wang T, 2020, ENG APPL ARTIF INTEL, V92, DOI 10.1016/j.engappai.2020.103680
   Wang T, 2020, INT J ELEC POWER, V119, DOI 10.1016/j.ijepes.2020.105961
   Xiao Y, 2020, PROT CONTR MOD POW, V5, DOI 10.1186/s41601-020-00156-w
   Zhang H., 2004, POWER SYSTEM TECHNOL, V15, P88
   Zhang XH, 2021, INT J UNCONV COMPUT, V16, P239
NR 36
TC 0
Z9 0
U1 15
U2 15
PD DEC
PY 2023
VL 8
IS 1
AR 47
DI 10.1186/s41601-023-00321-x
WC Energy & Fuels; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Marcello, S
   Shunra, Y
   Ruggero, M
AF Marcello, Salustri
   Shunra, Yoshida
   Ruggero, Micheletto
TI Neural and axonal heterogeneity improves information transmission
SO PHYSICA A-STATISTICAL MECHANICS AND ITS APPLICATIONS
DT Article
DE Heterogeneity; Neural; Noise; Delay; Spike
AB The complexity of the brain lies in an intricate, not homogeneous structure; however, it is not clear how important is the heterogeneity at neural and structural levels and how it may play an important constituent in the brain's functionality. Here we show the role of cellular and axonal delay heterogeneity in the brain's performance. We simulated, at different scales, the spiking activity on a toroidal network realized in multiple dimensions with varying degrees of heterogeneity on a network of Izhikevich neuron models. We found that increasing the heterogeneity and network dimension improved the robustness and propagation speed of the spiking activity. Our results demonstrate that the behavior of the spiking activity depends on both the cellular neural and axonal delay heterogeneity. We developed a simple theoretical framework compatible with the results of the simulations, putting forward a novel method to strategically analyze any similar networks. & COPY; 2023 Elsevier B.V. All rights reserved.
C1 [Marcello, Salustri; Shunra, Yoshida; Ruggero, Micheletto] Yokohama City Univ, Grad Sch Nanobiosci, Cognit Informat Lab, 22-2 Seto,Kanazawa Ku, Yokohama 2360027, Japan.
RP Ruggero, M (corresponding author), Yokohama City Univ, Grad Sch Nanobiosci, Cognit Informat Lab, 22-2 Seto,Kanazawa Ku, Yokohama 2360027, Japan.
EM ruggero@yokohama-cu.ac.jp
CR Asl MM, 2017, SCI REP-UK, V7, DOI 10.1038/srep39682
   Budrikis Z, 2021, NAT REV PHYS, V3, P771, DOI 10.1038/s42254-021-00401-7
   Chang W, 2008, J SYST ENG ELECTRON, V19, P694, DOI 10.1016/S1004-4132(08)60141-3
   Chaturvedi S., 2014, CIIT INT J DIGIT IMA, V6
   Coli M, 2000, ISCAS 2000: IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS - PROCEEDINGS, VOL IV, P137, DOI 10.1109/ISCAS.2000.858707
   Eppler Jochen Martin, 2008, Front Neuroinform, V2, P12, DOI 10.3389/neuro.11.012.2008
   Faisal AA, 2008, NAT REV NEUROSCI, V9, P292, DOI 10.1038/nrn2258
   Gutkin B, 2003, J PHYSIOLOGY-PARIS, V97, P209, DOI 10.1016/j.jphysparis.2003.09.005
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Koch C, 1999, SCIENCE, V284, P96, DOI 10.1126/science.284.5411.96
   Nobukawa S, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-017-18783-z
   Olshausen BA, 2003, J COGNITIVE NEUROSCI, V15, P154, DOI 10.1162/089892903321107891
   Perez-Nieves N, 2021, NAT COMMUN, V12, DOI 10.1038/s41467-021-26022-3
   Salustri M, 2023, COGN COMPUT, V15, P1231, DOI 10.1007/s12559-022-10034-2
   Sato M., 1985, B INFORM CYBERN, V21, P97, DOI [10.5109/13371, DOI 10.5109/13371]
   Sonal B., 2013, INT J RECONFIGURABLE, V2, DOI [10.11591/ijres.v2i2.3434, DOI 10.11591/IJRES.V2I2.3434]
   Stacey WC, 2000, J NEUROPHYSIOL, V83, P1394, DOI 10.1152/jn.2000.83.3.1394
   Suter TACS, 2019, SCIENCE, V365, P881, DOI 10.1126/science.aaw8231
   Suwanda R., 2020, Journal of Physics: Conference Series, V1566, DOI 10.1088/1742-6596/1566/1/012058
   Tewari SG, 2016, FRONT INTEGR NEUROSC, V10, DOI 10.3389/fnint.2016.00003
   Tozzi A, 2016, COGN NEURODYNAMICS, V10, P189, DOI 10.1007/s11571-016-9379-z
   Vázquez-Rodríguez B, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-13400-5
   von Neumann J., 1956, AUTOMATA STUDIES, V34, P43, DOI [DOI 10.1515/9781400882618-003, 10.1515/9781400882618-003]
   Zaitsev DA, 2017, THEOR COMPUT SCI, V666, P21, DOI 10.1016/j.tcs.2016.11.002
NR 24
TC 0
Z9 0
U1 0
U2 0
PD MAY 15
PY 2023
VL 618
AR 128627
DI 10.1016/j.physa.2023.128627
EA MAR 2023
WC Physics, Multidisciplinary
DA 2023-11-11
ER

PT J
AU Zhang, ZM
   Wu, QX
   Zhuo, ZQ
   Wang, XW
   Huang, LP
AF Zhang, Zhenmin
   Wu, Qingxiang
   Zhuo, Zhiqiang
   Wang, Xiaowei
   Huang, Liuping
TI Wavelet transform and texture recognition based on spiking neural
   network for visual images
SO NEUROCOMPUTING
DT Article
DE Spiking neural networks; Human visual system; Fast wavelet transform;
   Image reconstruction; Texture classification
ID CLASSIFICATION; SEGMENTATION
AB The functionalities of spiking neurons can be applied to deal with biological stimuli and explain complicated intelligent behaviors of the brain. The wavelet transforms are widely used in image feature extraction and image compression. Based on the principles from the visual system and wavelet theory, spiking neural networks with the ON/OFF neuron pathways inspired from the human visual system are proposed to perform the fast wavelet transform and the reconstruction for visual images. By this way we try to simulate how the human brain uses the volition-controlled method to extract useful image information. Furthermore, we decompose each texture sample with the established networks and calculate the normalized energy of the obtained sub-images at different scales. These energy values are used as features for texture classification. The simulation results show that the spiking neural network can extract the main information of images so that the images can be accurately classified using the information. (C) 2014 Elsevier B.V. All rights reserved.
C1 [Zhang, Zhenmin; Wu, Qingxiang; Zhuo, Zhiqiang; Wang, Xiaowei; Huang, Liuping] Fujian Normal Univ, Coll Photon & Elect Engn, Fujian Prov Key Lab Photon Technol, Fuzhou 350007, Peoples R China.
RP Wu, QX (corresponding author), Fujian Normal Univ, Coll Photon & Elect Engn, Fujian Prov Key Lab Photon Technol, Fuzhou 350007, Peoples R China.
EM qxwu@fjnu.edu.cn
CR Akbarizadeh G, 2012, IEEE T GEOSCI REMOTE, V50, P4358, DOI 10.1109/TGRS.2012.2194787
   [Anonymous], 1999, WAVELET TOUR SIGNAL
   [Anonymous], THESIS U HEIDELBERG
   BOUMAN C, 1991, IEEE T PATTERN ANAL, V13, P99, DOI 10.1109/34.67641
   BOVIK AC, 1990, IEEE T PATTERN ANAL, V12, P55, DOI 10.1109/34.41384
   Brodatz P., 1966, TEXTURE PHOTOGRAPHIC
   Chang T, 1993, IEEE T IMAGE PROCESS, V2, P429, DOI 10.1109/83.242353
   Chui CK., 1992, INTRO WAVELETS, DOI 10.2307/2153134
   COHEN L, 1989, P IEEE, V77, P941, DOI 10.1109/5.30749
   Daubechies I., 1992, CBMS NSF REGIONAL C, V61
   Demb JB, 2007, NEURON, V55, P179, DOI 10.1016/j.neuron.2007.07.001
   Gai S, 2013, AEU-INT J ELECTRON C, V67, P233, DOI 10.1016/j.aeue.2012.08.004
   HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Hu S, 2014, BIO-MED MATER ENG, V24, P129, DOI 10.3233/BME-130793
   Jessell T. M, 1981, PRINCIPLES NEURAL SC
   Johnston J.D., 1980, P IEEE INT C AC SPEE
   Liu C.L., 2010, TUTORIAL WAVELET TRA
   MALLAT SG, 1989, IEEE T PATTERN ANAL, V11, P674, DOI 10.1109/34.192463
   Masland RH, 2001, NAT NEUROSCI, V4, P877, DOI 10.1038/nn0901-877
   Nelson R, 2003, VISUAL NEUROSCIENCES, P260
   Niebur E., 1998, THE ATTENTIVE
   Randen T, 1999, IEEE T PATTERN ANAL, V21, P291, DOI 10.1109/34.761261
   RANDEN T, 1994, OPT ENG, V33, P2617, DOI 10.1117/12.177115
   REED TR, 1990, IEEE T PATTERN ANAL, V12, P1, DOI 10.1109/34.41379
   Taylor WR, 2003, TRENDS NEUROSCI, V26, P379, DOI 10.1016/S0166-2236(03)00167-X
   Van de Wouwer G, 1999, IEEE T IMAGE PROCESS, V8, P592, DOI 10.1109/83.753747
   Wang ZZ, 2008, IEEE T IMAGE PROCESS, V17, P1421, DOI 10.1109/TIP.2008.926150
   Wu QX, 2006, NEUROCOMPUTING, V69, P1912, DOI 10.1016/j.neucom.2005.11.023
   Wu QX, 2007, LECT NOTES ARTIF INT, V4682, P26
   Wu QX, 2007, STUD COMPUT INTELL, V35, P171
   Wu QX, 2013, NEUROCOMPUTING, V116, P3, DOI 10.1016/j.neucom.2012.01.046
   Wu QX, 2009, PROCEEDINGS OF THE 2009 2ND INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING, VOLS 1-9, P2168
   Wu QX, 2009, LECT NOTES ARTIF INT, V5755, P21
   Wu QX, 2008, NEURAL NETWORKS, V21, P1318, DOI 10.1016/j.neunet.2008.05.014
   Zaid MS, 2013, IEEE INT ADV COMPUT, P1149
   Zhang JG, 2002, PATTERN RECOGN, V35, P735, DOI 10.1016/S0031-3203(01)00074-7
NR 37
TC 8
Z9 9
U1 1
U2 31
PD MAR 3
PY 2015
VL 151
BP 985
EP 995
DI 10.1016/j.neucom.2014.03.086
PN 3
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT J
AU Anumula, J
   Neil, D
   Delbruck, T
   Liu, SC
AF Anumula, Jithendar
   Neil, Daniel
   Delbruck, Tobi
   Liu, Shih-Chii
TI Feature Representations for Neuromorphic Audio Spike Streams
SO FRONTIERS IN NEUROSCIENCE
DT Article
DE dynamic audio sensor; spike feature generation; exponential kernels;
   recurrent neural network; audio word classification
ID EVENT; NETWORKS; SENSOR; LONG; TIME
AB Event-driven neuromorphic spiking sensors such as the silicon retina and the silicon cochlea encode the external sensory stimuli as asynchronous streams of spikes across different channels or pixels. Combining state-of-art deep neural networks with the asynchronous outputs of these sensors has produced encouraging results on some datasets but remains challenging. While the lack of effective spiking networks to process the spike streams is one reason, the other reason is that the pre-processing methods required to convert the spike streams to frame-based features needed for the deep networks still require further investigation. This work investigates the effectiveness of synchronous and asynchronous frame-based features generated using spike count and constant event binning in combination with the use of a recurrent neural network for solving a classification task using N-TIDIGITS18 dataset. This spike-based dataset consists of recordings from the Dynamic Audio Sensor, a spiking silicon cochlea sensor, in response to the TIDIGITS audio dataset. We also propose a new pre-processing method which applies an exponential kernel on the output cochlea spikes so that the interspike timing information is better preserved. The results from the N-TIDIGITS18 dataset show that the exponential features perform better than the spike count features, with over 91% accuracy on the digit classification task. This accuracy corresponds to an improvement of at least 2.5% over the use of spike count features, establishing a new state of the art for this dataset.
C1 [Anumula, Jithendar] Univ Zurich, Inst Neuroinformat, Zurich, Switzerland.
   ETH, Zurich, Switzerland.
   [Neil, Daniel] BenevolentAI, New York, NY USA.
RP Anumula, J (corresponding author), Univ Zurich, Inst Neuroinformat, Zurich, Switzerland.
EM anumula@ini.uzh.ch
CR Abadi M., 2016, P OSDI 16 12 USENIX, V16, P265, DOI DOI 10.1038/NN.3331
   Abdollahi M, 2011, BIOMED CIRC SYST C, P269, DOI 10.1109/BioCAS.2011.6107779
   Amir A., 2017, P IEEE C COMP VIS PA, P7243, DOI DOI 10.1109/CVPR.2017.781
   Pérez-Carrasco JA, 2013, IEEE T PATTERN ANAL, V35, P2706, DOI 10.1109/TPAMI.2013.71
   Anumula J., 2017, P IEEE INT S CIRC SY, DOI [10.1109/ISCAS.2017.8050394, DOI 10.1109/ISCAS.2017.8050394]
   Barranco F, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00049
   BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181
   Berner Raphael, 2013, 2013 Symposium on VLSI Circuits, pC186
   Brette R, 2005, J NEUROPHYSIOL, V94, P3637, DOI 10.1152/jn.00686.2005
   Chakrabartty S, 2010, IEEE INT SYMP CIRC S, P513, DOI 10.1109/ISCAS.2010.5537578
   Chan V, 2007, IEEE T CIRCUITS-I, V54, P48, DOI 10.1109/TCSI.2006.887979
   Cho K., 2014, P C EMP METH NAT LAN, P1724
   Chung J., 2014, NIPS 2014 WORKSHOP D, P1
   Cohen GK, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00184
   Delbruck Tobi, 2008, P INT S SECURE LIFE, P21
   Diehl Peter U, 2015, 2015 INT JOINT C NEU, P1, DOI [DOI 10.1109/IJCNN.2015.7280696, 10.1109/IJCNN.2015.7280696]
   Esser SK, 2016, P NATL ACAD SCI USA, V113, P11441, DOI 10.1073/pnas.1604850113
   Farabet C, 2012, FRONT NEUROSCI-SWITZ, V6, DOI 10.3389/fnins.2012.00032
   Gers FA, 2000, IEEE IJCNN, P189, DOI 10.1109/IJCNN.2000.861302
   Graves, 2013, ARXIV13080850
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Graves A., 2006, P 23 INT C MACHINE L, V148, P369, DOI DOI 10.1145/1143844.1143891
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   Klambauer G., 2017, P 31 INT C NEUR INF, P972
   Lagorce X, 2017, IEEE T PATTERN ANAL, V39, P1346, DOI 10.1109/TPAMI.2016.2574707
   Lagorce X, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00046
   Leonard R. G., 1993, TIDIGITS LDC93S10
   LEVENSHT.VI, 1965, DOKL AKAD NAUK SSSR+, V163, P845
   Li CH, 2012, IEEE INT SYMP CIRC S, P1159
   Lichtsteiner Patrick, 2008, IEEE Journal of Solid-State Circuits, V43, P566, DOI 10.1109/JSSC.2007.914337
   Liu SC, 2014, IEEE T BIOMED CIRC S, V8, P453, DOI 10.1109/TBCAS.2013.2281834
   Liu SC, 2010, IEEE INT SYMP CIRC S, P505, DOI 10.1109/ISCAS.2010.5537588
   Lungu I., 2017, P IEEE INT S CIRC SY, DOI [10.1109/ISCAS.2017.8050403, DOI 10.1109/ISCAS.2017.8050403]
   Moeys DP, 2016, 2016 2ND INTERNATIONAL CONFERENCE ON EVENT-BASED CONTROL, COMMUNICATION, AND SIGNAL PROCESSING (EBCCSP), DOI 10.1109/EBCCSP.2016.7605233
   Neil D, 2016, IEEE INT SYMP CIRC S, P2282, DOI 10.1109/ISCAS.2016.7539039
   Neil Daniel, 2016, P 30 INT C NEUR INF, P3882, DOI DOI 10.48550/ARXIV.1610.09513
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   Orchard G, 2015, FRONT NEUROSCI-SWITZ, V9, DOI [10.3389/fnins.2015.00437, 10.3389/fhins.2015.00437]
   Posch C, 2014, P IEEE, V102, P1470, DOI 10.1109/JPROC.2014.2346153
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Serrano-Gotarredona T, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00481
   Stromatias E, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00222
   Szucs A, 1998, J NEUROSCI METH, V81, P159, DOI 10.1016/S0165-0270(98)00033-8
   Tapson J., 2013, ARXIV13047118
   Yang MH, 2016, ISSCC DIG TECH PAP I, V59, P388
   Yang MH, 2015, IEEE J SOLID-ST CIRC, V50, P2149, DOI 10.1109/JSSC.2015.2425886
   Zai AT, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00347
   Zhao B, 2015, IEEE T NEUR NET LEAR, V26, P1963, DOI 10.1109/TNNLS.2014.2362542
NR 50
TC 52
Z9 53
U1 0
U2 9
PD FEB 9
PY 2018
VL 12
AR 23
DI 10.3389/fnins.2018.00023
WC Neurosciences
DA 2023-11-11
ER

PT J
AU Amin, HH
   Fujii, RH
AF Amin, HH
   Fujii, RH
TI \Spiking neural network inter-spike time based decoding scheme
SO IEICE TRANSACTIONS ON INFORMATION AND SYSTEMS
DT Article
DE spike train; spatio-temporal; decoding scheme; inter-spike interval
ID COMPUTATIONAL POWER; NEURONS
AB Information transmission among biological neurons is carried out by a complex series of spike signals. The input inter-spike arrival times at a neuron are believed to carry information which the neurons utilize to carry out a task. In this paper, a new scheme which utilizes the input inter-spike intervals (ISI) for decoding an input spike train is proposed. A spike train consists of a sequence on input spikes with various interspike times. This decoding scheme can also be used for neurons which have multiple synaptic inputs but for which each synapse receives a single spike within one input time window. The ISI decoding neural network requires only a few neurons. Example applications show the usefulness of the decoding scheme.
C1 Aizu Univ, Dept Comp Engn, Aizu Wakamatsu 9658580, Japan.
RP Amin, HH (corresponding author), Aizu Univ, Dept Comp Engn, Aizu Wakamatsu 9658580, Japan.
EM d8042201@u-aizu.ac.jp; fujii@u-aizu.ac.jp
CR ABELES M, 1993, J NEUROPHYSIOL, V70, P1629, DOI 10.1152/jn.1993.70.4.1629
   Agmon-Snir H, 1998, NATURE, V393, P268, DOI 10.1038/30505
   AMIN HH, 2003, 3D FORUM, V17, P191
   [Anonymous], 2003, ADV NEURAL INFORM PR
   [Anonymous], BINAURAL SPATIAL HEA
   Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   Gerstner W., 2002, SPIKING NEURON MODEL
   Hopfield IJ, 2000, P NATL ACAD SCI USA, V97, P13919, DOI 10.1073/pnas.250483697
   Hopfield JJ, 2001, P NATL ACAD SCI USA, V98, P1282, DOI 10.1073/pnas.031567098
   Maass W, 1996, NEURAL COMPUT, V8, P1, DOI 10.1162/neco.1996.8.1.1
   Maass W, 1997, ADV NEUR IN, V9, P211
   MAASS W., 1999, PULSED NEURAL NETWOR
   MAASS W, 2003, COMPUTATIONAL NEUROS, pCH18
   Natschlager T, 1998, NETWORK-COMP NEURAL, V9, P319, DOI 10.1088/0954-898X/9/3/003
   Perrinet L, 2001, NEUROCOMPUTING, V38, P817, DOI 10.1016/S0925-2312(01)00460-X
   Reed MC, 2002, J COMPUT NEUROSCI, V13, P35, DOI 10.1023/A:1019692310817
   Ruf B, 1998, IEEE T NEURAL NETWOR, V9, P575, DOI 10.1109/72.668899
   Simon JZ, 1999, NEUROCOMPUTING, V26-7, P263, DOI 10.1016/S0925-2312(99)00020-X
NR 18
TC 6
Z9 6
U1 0
U2 4
PD AUG
PY 2005
VL E88D
IS 8
BP 1893
EP 1902
DI 10.1093/ietisy/e88-d.8.1893
WC Computer Science, Information Systems; Computer Science, Software
   Engineering
DA 2023-11-11
ER

PT J
AU Kim, H
   Hwang, S
   Park, J
   Yun, S
   Lee, JH
   Park, BG
AF Kim, Hyungjin
   Hwang, Sungmin
   Park, Jungjin
   Yun, Sangdoo
   Lee, Jong-Ho
   Park, Byung-Gook
TI Spiking Neural Network Using Synaptic Transistors and Neuron Circuits
   for Pattern Recognition With Noisy Images
SO IEEE ELECTRON DEVICE LETTERS
DT Article
DE Denoising autoencoder (DAE); synaptic device; neuron circuit;
   neuromorphic system; spiking neural network (SNN); pattern recognition
AB We demonstrate the hardware implementation of spiking neural network (SNN) with synaptic transistors and neuron circuits. The method of conversion from software fully-connected network (FCN) to hardware SNN with little degradation is discussed. The degradation of classification accuracy is analyzed in terms of device variation and noisy images. In addition, the accuracy degradation is significantly improved by stacking denoising autoencoder (DAE) layer. FCN-SNN conversion with very little performance drop is demonstrated using weight normalization, and SNN with DAE layer shows a great tolerance to input image noise.
C1 [Kim, Hyungjin] Univ Calif Santa Barbara, Dept Elect & Comp Engn, Santa Barbara, CA 93106 USA.
   [Hwang, Sungmin; Park, Jungjin; Yun, Sangdoo; Lee, Jong-Ho; Park, Byung-Gook] Seoul Natl Univ, Dept Elect & Comp Engn, Seoul 08826, South Korea.
   [Hwang, Sungmin; Park, Jungjin; Yun, Sangdoo; Lee, Jong-Ho; Park, Byung-Gook] Seoul Natl Univ, Interuniv Semicond Res Ctr, Seoul 08826, South Korea.
RP Kim, H (corresponding author), Univ Calif Santa Barbara, Dept Elect & Comp Engn, Santa Barbara, CA 93106 USA.; Park, BG (corresponding author), Seoul Natl Univ, Dept Elect & Comp Engn, Seoul 08826, South Korea.; Park, BG (corresponding author), Seoul Natl Univ, Interuniv Semicond Res Ctr, Seoul 08826, South Korea.
EM hyungjin@ece.ucsb.edu; bgpark@snu.ac.kr
CR Jeong DS, 2016, ADV ELECTRON MATER, V2, DOI 10.1002/aelm.201600090
   MEAD C, 1990, P IEEE, V78, P1629, DOI 10.1109/5.58356
   Prezioso M, 2015, NATURE, V521, P61, DOI 10.1038/nature14441
   Wan X, 2016, IEEE ELECTR DEVICE L, V37, P299, DOI 10.1109/LED.2016.2517080
   Wang DT, 2016, IEEE ELECTR DEVICE L, V37, P878, DOI 10.1109/LED.2016.2570279
   Wang IT, 2016, NANOTECHNOLOGY, V27, DOI 10.1088/0957-4484/27/36/365204
NR 6
TC 53
Z9 53
U1 2
U2 58
PD APR
PY 2018
VL 39
IS 4
BP 630
EP 633
DI 10.1109/LED.2018.2809661
WC Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Wang, X
   Song, T
   Gong, FM
   Zheng, P
AF Wang, Xun
   Song, Tao
   Gong, Faming
   Zheng, Pan
TI On the Computational Power of Spiking Neural P Systems with
   Self-Organization
SO SCIENTIFIC REPORTS
DT Article
ID ANTI-SPIKES; NETWORK; RECOGNITION; SYNAPSES; MACHINES; STRATEGY;
   WEIGHTS; RULES
AB Neural-like computing models are versatile computing mechanisms in the field of artificial intelligence. Spiking neural P systems (SN P systems for short) are one of the recently developed spiking neural network models inspired by the way neurons communicate. The communications among neurons are essentially achieved by spikes, i.e. short electrical pulses. In terms of motivation, SN P systems fall into the third generation of neural network models. In this study, a novel variant of SN P systems, namely SN P systems with self-organization, is introduced, and the computational power of the system is investigated and evaluated. It is proved that SN P systems with self-organization are capable of computing and accept the family of sets of Turing computable natural numbers. Moreover, with 87 neurons the system can compute any Turing computable recursive function, thus achieves Turing universality. These results demonstrate promising initiatives to solve an open problem arisen by Gh Paun.
C1 [Wang, Xun; Song, Tao; Gong, Faming] China Univ Petr, Coll Comp & Commun Engn, Qingdao 266580, Shandong, Peoples R China.
   [Song, Tao; Zheng, Pan] Swinburne Univ Technol, Fac Engn Comp & Sci, Sarawak Campus, Kuching 93350, Malaysia.
RP Song, T (corresponding author), China Univ Petr, Coll Comp & Commun Engn, Qingdao 266580, Shandong, Peoples R China.; Song, T (corresponding author), Swinburne Univ Technol, Fac Engn Comp & Sci, Sarawak Campus, Kuching 93350, Malaysia.
EM tsong@upc.edu.cn
CR Adl A., 2010, ARXIV10120326
   Burges CJC, 1998, DATA MIN KNOWL DISC, V2, P121, DOI 10.1023/A:1009715923555
   Buzsáki G, 2010, NEURON, V68, P362, DOI 10.1016/j.neuron.2010.09.023
   Cabarle FGC, 2015, NEURAL COMPUT APPL, V26, P1905, DOI 10.1007/s00521-015-1857-4
   Cavaliere M, 2009, THEOR COMPUT SCI, V410, P2352, DOI 10.1016/j.tcs.2009.02.031
   Chen HM, 2007, FUND INFORM, V75, P141
   Chen X, 2016, THEOR COMPUT SCI, V623, P146, DOI 10.1016/j.tcs.2015.12.006
   FRITZKE B, 1994, NEURAL NETWORKS, V7, P1441, DOI 10.1016/0893-6080(94)90091-4
   Gerstner W., 2002, SPIKING NEURON MODEL
   Gheorghe M, 2013, INT J FOUND COMPUT S, V24, P547, DOI 10.1142/S0129054113500202
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Hagan MT., 1996, NEURAL NETWORK DESIG
   Ibarra OH, 2009, THEOR COMPUT SCI, V410, P2982, DOI 10.1016/j.tcs.2009.03.004
   Ionescu M., 2007, P 5 BRAINST WEEK MEM
   Ionescu M, 2006, FUND INFORM, V71, P279
   Korec I, 1996, THEOR COMPUT SCI, V168, P267, DOI 10.1016/S0304-3975(96)00080-1
   Lin C, 2014, NEUROCOMPUTING, V123, P424, DOI 10.1016/j.neucom.2013.08.004
   Liu B, 2015, NUCLEIC ACIDS RES, V43, pW65, DOI 10.1093/nar/gkv458
   Liu B, 2015, BIOINFORMATICS, V31, P1307, DOI 10.1093/bioinformatics/btu820
   Liu CL, 2003, PATTERN RECOGN, V36, P2271, DOI 10.1016/S0031-3203(03)00085-2
   Liu XR, 2015, IEEE T NANOBIOSCI, V14, P617, DOI 10.1109/TNB.2015.2438257
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maass W, 2001, PULSED NEURAL NETWOR
   MINSKY M, 1967, COMPUTATION FINITE I
   Neary T., 2010, P 11 INT C MEMBR COM
   Pan LQ, 2012, NEURAL COMPUT, V24, P805, DOI 10.1162/NECO_a_00238
   Pan LQ, 2010, LECT NOTES COMPUT SC, V5957, P436
   Pan LQ, 2010, THEOR COMPUT SCI, V411, P906, DOI 10.1016/j.tcs.2009.11.010
   Pan LQ, 2009, INT J COMPUT COMMUN, V4, P273, DOI 10.15837/ijccc.2009.3.2435
   Paum G, 2007, J UNIVERS COMPUT SCI, V13, P1707
   Paun A., 2012, MEMBRANE COMPUTING, V7184, P333
   Paun A, 2007, BIOSYSTEMS, V90, P48, DOI 10.1016/j.biosystems.2006.06.006
   Paun Gh., 2002, MEMBRANE COMPUTING I
   Paun Gh, 2010, OXFORD HDB MEMBRANE
   Peng H, 2013, INFORM SCIENCES, V235, P106, DOI 10.1016/j.ins.2012.07.015
   Rozenberg G., 1997, HDB FORMAL LANGUAGES, V3
   SIEGELMANN HT, 1995, J COMPUT SYST SCI, V50, P132, DOI 10.1006/jcss.1995.1013
   Song L, 2014, BMC BIOINFORMATICS, V15, DOI 10.1186/1471-2105-15-298
   Song T, 2015, IEEE T NANOBIOSCI, V14, P465, DOI 10.1109/TNB.2015.2402311
   Song T, 2015, IEEE T NANOBIOSCI, V14, P38, DOI 10.1109/TNB.2014.2367506
   Song T, 2015, NEUROCOMPUTING, V151, P1439, DOI 10.1016/j.neucom.2014.10.044
   Song T, 2014, NEURAL COMPUT APPL, V24, P1833, DOI 10.1007/s00521-013-1397-8
   Song T, 2013, IEEE T NANOBIOSCI, V12, P255, DOI 10.1109/TNB.2013.2271278
   Song T, 2013, J COMPUT THEOR NANOS, V10, P999, DOI 10.1166/jctn.2013.2799
   Song T, 2013, INFORM SCIENCES, V219, P197, DOI 10.1016/j.ins.2012.07.023
   Tetzlaff C., 2013, BMC NEUROSCI, V14, pP415
   Ultsch A., 1993, SELF ORG NEURAL NETW
   Wang J, 2013, IEEE T FUZZY SYST, V21, P209, DOI 10.1109/TFUZZ.2012.2208974
   Wang J, 2010, NEURAL COMPUT, V22, P2615, DOI 10.1162/NECO_a_00022
   Wang T, 2015, IEEE T POWER SYST, V30, P1182, DOI 10.1109/TPWRS.2014.2347699
   Wu YF, 2011, J EXP THEOR ARTIF IN, V23, P63, DOI 10.1080/0952813X.2010.506288
   Zeng JC, 2016, CURR BIOINFORM, V11, P4, DOI 10.2174/1574893611666151119221435
   Zeng XX, 2017, IEEE ACM T COMPUT BI, V14, P687, DOI 10.1109/TCBB.2016.2520947
   Zeng XX, 2016, BRIEF BIOINFORM, V17, P193, DOI 10.1093/bib/bbv033
   Zeng XX, 2014, INFORM SCIENCES, V278, P423, DOI 10.1016/j.ins.2014.03.062
   Zeng XX, 2014, NEURAL COMPUT, V26, P1340, DOI 10.1162/NECO_a_00605
   Zeng XX, 2012, IEEE T NANOBIOSCI, V11, P366, DOI 10.1109/TNB.2012.2211034
   Zeng XX, 2009, FUND INFORM, V97, P275, DOI 10.3233/FI-2009-200
   Zhang GX, 2014, INT J NEURAL SYST, V24, DOI 10.1142/S0129065714400061
   Zhang X., 2015, IEEE T EVOLUT COMPU, V16, P35
   Zhang XY, 2015, IEEE T EVOLUT COMPUT, V19, P201, DOI 10.1109/TEVC.2014.2308305
   Zhang XY, 2008, FUND INFORM, V87, P117
   Zou Q, 2016, BRIEF FUNCT GENOMICS, V15, P55, DOI 10.1093/bfgp/elv024
   Zou Q, 2015, BIOINFORMATICS, V31, P2475, DOI 10.1093/bioinformatics/btv177
NR 64
TC 42
Z9 42
U1 2
U2 54
PD JUN 10
PY 2016
VL 6
AR 27624
DI 10.1038/srep27624
WC Multidisciplinary Sciences
DA 2023-11-11
ER

PT C
AU Zakharov, AV
AF Zakharov, Andrey V.
GP IEEE
TI Information Capacity of a Neural Network with Redundant Connections
   Between Neurons
SO 2017 10TH INTERNATIONAL SYMPOSIUM ON COMPUTATIONAL INTELLIGENCE AND
   DESIGN (ISCID), VOL. 1
SE International Symposium on Computational Intelligence and Design
DT Proceedings Paper
CT 10th International Symposium on Computational Intelligence and Design
   (ISCID)
CY DEC 09-10, 2017
CL Hangzhou, PEOPLES R CHINA
DE recurrent neural network; redundant connections; delay lines; memory;
   temporal sequences; cue pattern; polychronization
ID SHORT-TERM-MEMORY; SERIAL ORDER; MODEL; COMPUTATION; LOOP
AB In this work the model of a spiking recurrent neural network where any pair of neurons can form several connection lines (axons) with different spike propagation times is studied. Through simulation modeling, it has been shown that a neural network with redundant connections between neurons in the form of delay lines provides storage and playback of a significant number of independent temporal sequences of neural pulses. It has been suggested that multiple synaptic inputs from a single neuron in a natural neural network provide some of the information-processing properties of the network.
C1 [Zakharov, Andrey V.] KSMU, Dept Physiol, KFU, Neurobiol Lab, Kazan, Russia.
RP Zakharov, AV (corresponding author), KSMU, Dept Physiol, KFU, Neurobiol Lab, Kazan, Russia.
EM AnVZaharov@kpfu.ru
CR [Anonymous], 2001, FIELD GUIDE DYNAMICA
   [Anonymous], 1987, ADV PSYCHOL, DOI [10.1016/S0166-4115(08)61766-5, DOI 10.1016/S0166-4115(08)61766-5]
   Botvinick M, 2007, J NEUROSCI, V27, P8636, DOI 10.1523/JNEUROSCI.2110-07.2007
   Botvinick MM, 2006, PSYCHOL REV, V113, P201, DOI 10.1037/0033-295X.113.2.201
   Burgess N, 1999, PSYCHOL REV, V106, P551, DOI 10.1037/0033-295X.106.3.551
   BURGESS N, 1992, J MEM LANG, V31, P429, DOI 10.1016/0749-596X(92)90022-P
   Feldmeyer D, 1999, J PHYSIOL-LONDON, V521, P169, DOI 10.1111/j.1469-7793.1999.00169.x
   Gripon V, 2011, IEEE T NEURAL NETWOR, V22, P1087, DOI 10.1109/TNN.2011.2146789
   GROSSBERG S, 1973, STUD APPL MATH, V52, P213
   Henson RNA, 1998, COGNITIVE PSYCHOL, V36, P73, DOI 10.1006/cogp.1998.0685
   Houghton G., 1990, Current Research in Natural Language Generation, P287
   Hussain S, 2014, NEUROCOMPUTING, V138, P14, DOI 10.1016/j.neucom.2013.09.052
   Hussain S, 2012, 2012 IEEE ASIA PACIFIC CONFERENCE ON CIRCUITS AND SYSTEMS (APCCAS), P304, DOI 10.1109/APCCAS.2012.6419032
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Jaeger H., 2002, GMD REP, V152, P60
   Katz LC, 1996, SCIENCE, V274, P1133, DOI 10.1126/science.274.5290.1133
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Sylvester J., 2010, P 10 INT C COGN MOD, P241
   Szatmáry B, 2010, PLOS COMPUT BIOL, V6, DOI 10.1371/journal.pcbi.1000879
   Tsukerman V. D., 2006, MAT BIOL BIOINFORMAT, V1, P108
   Tully PJ, 2016, PLOS COMPUT BIOL, V12, DOI 10.1371/journal.pcbi.1004954
NR 21
TC 0
Z9 0
U1 0
U2 1
PY 2017
BP 16
EP 19
DI 10.1109/ISCID.2017.19
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
DA 2023-11-11
ER

PT J
AU Hamian, M
   Faez, K
   Nazari, S
   Sabeti, M
AF Hamian, Melika
   Faez, Karim
   Nazari, Soheila
   Sabeti, Malihe
TI A novel learning approach in deep spiking neural networks with
   multi-objective optimization algorithms for automatic digit speech
   recognition
SO JOURNAL OF SUPERCOMPUTING
DT Article
DE Spiking neural networks (SNNs); Artificial neural networks (ANNs);
   Gradient-based optimization (GBO); Wild horse optimizer (WHO); Digit
   speech recognition
AB Here, a new layered spiking neural network (SNN) learning framework is proposed using optimization algorithms for rapid and efficient pattern recognition and classification. In connection with the problem of learning deep SNN layers and with the help of different algorithms of gradient-based optimization and wild horse optimization, the two main parameters of spike neurons (threshold voltage and input weights) for different layers are calculated. SNN has been utilized to model several prominent datasets under machine learning systems, including IRIS and Trip datasets and digital speech recognition systems under MATLAB. Then, their performance is compared and evaluated in different scenarios with other deep learning methods such as artificial neural network and adaptive network-based fuzzy inference system. The results indicated an upsurge in identification and estimation accuracy. Integrating the algorithmic power of deep SNNs with adequate neuromorphic hardware creates an opportunity for speech recognition applications running locally on mobile and other applications.
C1 [Hamian, Melika; Faez, Karim; Nazari, Soheila; Sabeti, Malihe] Islamic Azad Univ, Dept Comp Engn, North Tehran Branch, Tehran, Iran.
   [Faez, Karim] Amirkabir Univ Technol, Dept Elect Engn, Tehran, Iran.
   [Nazari, Soheila] Shahid Beheshti Univ, Fac Elect Engn, Tehran, Iran.
RP Faez, K (corresponding author), Islamic Azad Univ, Dept Comp Engn, North Tehran Branch, Tehran, Iran.; Faez, K (corresponding author), Amirkabir Univ Technol, Dept Elect Engn, Tehran, Iran.
EM Hamian.melika@gmail.com; kfaez@aut.ac.ir; so_nazari@sbu.ac.ir;
   sabeti@shirazu.ac.ir
CR Ahmadianfar I, 2020, INFORM SCIENCES, V540, P131, DOI 10.1016/j.ins.2020.06.037
   Aizawa K, 2004, ADV MULTIMEDIA INFOR
   Amin HH, 2005, SPIKING NEURAL NETWO
   Bethi Y, 2021, ARXIV
   Chen C, 2022, J SUPERCOMPUT, V78, P19020, DOI 10.1007/s11227-022-04629-7
   Dr KS., 2021, GLOBAL TRANSITIONS P, V2, P513, DOI [10.1016/j.gltp.2021.08.013, DOI 10.1016/J.GLTP.2021.08.013]
   Eappen G, 2022, APPL SOFT COMPUT, V114, DOI 10.1016/j.asoc.2021.108072
   Ellis D, 2010, SPOKEN DIGITS ENDPOI
   Fitzgerald Jamie M., 2021, 2021 Conference on Lasers and Electro-Optics Europe & European Quantum Electronics Conference (CLEO/Europe-EQEC), DOI 10.1109/CLEO/Europe-EQEC52157.2021.9542576
   Hanchate D. B., 2010, 2010 2nd International Conference on Computer Engineering and Technology (ICCET), P88, DOI 10.1109/ICCET.2010.5486314
   Izhikevich EM, 2007, DYNAMICAL SYSTEMS NE, DOI 10.7551/mitpress/2526.001.0001
   Kim Y, 2021, NEURAL NETWORKS, V144, P686, DOI 10.1016/j.neunet.2021.09.022
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Liu SL, 2023, IEEE T AUTOM SCI ENG, DOI 10.1109/TASE.2023.3269509
   Lobo JL, 2020, NEURAL NETWORKS, V121, P88, DOI 10.1016/j.neunet.2019.09.004
   Lyashenko V, 2021, 2010 2 INT C COMP EN
   MCCULLOCH WS, 1990, B MATH BIOL, V52, P99, DOI 10.1016/S0092-8240(05)80006-0
   Mostafa H, 2018, IEEE T NEUR NET LEAR, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Naruei I, 2022, ENG COMPUT-GERMANY, V38, P3025, DOI 10.1007/s00366-021-01438-z
   Naseri MM, 2020, 6 IRANIAN C SIGNAL P, P1
   Paugam-Moisy H, 2012, HDB NATURAL COMPUTIN, V1, P1, DOI DOI 10.1007/978-3-540-92910-9_10
   Pitti A, 2020, NEURAL NETWORKS, V121, P242, DOI 10.1016/j.neunet.2019.09.023
   Putra RVW, 2020, IEEE T COMPUT AID D, V39, P3601, DOI 10.1109/TCAD.2020.3013049
   ROSENBLATT F, 1958, PSYCHOL REV, V65, P386, DOI 10.1037/h0042519
   Rueckauer B., 2017, FRONT NEUROSCI, V49, P101338
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sayarkin KS, 2018, PROCEEDINGS OF THE 2018 IEEE CONFERENCE OF RUSSIAN YOUNG RESEARCHERS IN ELECTRICAL AND ELECTRONIC ENGINEERING (EICONRUS), P979, DOI 10.1109/EIConRus.2018.8317253
   Sengupta A., 2019, FRONT NEUROSCI, V32, P7987
   Si ZY, 2021, APPL ENERG, V302, DOI 10.1016/j.apenergy.2021.117514
   Tavanaei A., 2019, NEURAL NETWORKS, V56, P89
   Trik M, 2023, INTEGRATION, V89, P12, DOI 10.1016/j.vlsi.2022.11.004
   Vreeken J, 2003, SPIKING NEURAL NETWO
   Woodward A, 2015, NEURAL NETWORKS, V62, P39, DOI 10.1016/j.neunet.2014.08.011
   Wu JB, 2022, IEEE T PATTERN ANAL, V44, P7824, DOI 10.1109/TPAMI.2021.3114196
   Wu JB, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00199
   Zhang HY, 2022, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2022.3183991
NR 36
TC 0
Z9 0
U1 2
U2 2
PD DEC
PY 2023
VL 79
IS 18
BP 20263
EP 20288
DI 10.1007/s11227-023-05420-y
EA JUN 2023
WC Computer Science, Hardware & Architecture; Computer Science, Theory &
   Methods; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Kahng, DS
   Nam, Y
   Lee, D
AF Kahng, Dong-Soo
   Nam, Yoonkey
   Lee, Doheon
BA Yang, JY
BF Yang, JY
BE Yang, MQ
   Zhu, MM
   Zhang, Y
   Arabnia, HR
   Deng, Y
   Bourbakis, N
TI Stochastic simulation model for patterned neural multi-electrode arrays
SO PROCEEDINGS OF THE 7TH IEEE INTERNATIONAL SYMPOSIUM ON BIOINFORMATICS
   AND BIOENGINEERING, VOLS I AND II
DT Proceedings Paper
CT 7th IEEE International Conference on Bioinformatics and Bioengineering
CY OCT 14-17, 2007
CL Boston, MA
DE multi-electrode array; neural network; simulator; spike train;
   stochastic model
ID CORTICAL-NEURONS; MICROELECTRODE ARRAYS; NETWORKS; DYNAMICS; GENERATION;
   CULTURES; SPIKING
AB A multi-electrode array (MEA) is a micro-fabricated cell culture dish with embedded microelectrodes at the bottom of the dish. Recently MEAs with different cell-adhesive patterns are actively used to analyze behaviors of in vitro neural systems. It is difficult to confirm the underlying anatomical synaptic connections of the in vitro neural networks based on neural recordings from MEAs. Meanwhile, computational modeling and simulation cannot only facilitate various in silico combinatorial stimulus-response analysis but also provide connection-aware analysis capability. Here we propose a simulation approach of encompassing the whole MEA experiments including cell seeding, axonal growth, network morphology and the recordings of the electrodes of MEAs. Cell densities and the geometry of network morphology could be varied systematically and multiple spike trains from the simulated networks were obtained.
C1 [Kahng, Dong-Soo; Nam, Yoonkey; Lee, Doheon] Korea Adv Inst Sci & Technol, Dept Bio & Brain Engn, Taejon 305701, South Korea.
RP Lee, D (corresponding author), Korea Adv Inst Sci & Technol, Dept Bio & Brain Engn, Taejon 305701, South Korea.
CR AERTSEN A, 1994, PHYSICA D, V75, P103, DOI 10.1016/0167-2789(94)90278-X
   [Anonymous], 1998, BOOK GENESIS EXPLORI, DOI DOI 10.1007/978-1-4612-1634-63
   [Anonymous], METHODS NEURAL ENSEM
   Brown EN, 2004, NAT NEUROSCI, V7, P456, DOI 10.1038/nn1228
   Chao ZC, 2005, NEUROINFORMATICS, V3, P263, DOI 10.1385/NI:3:3:263
   CHEN H, 2004, THESIS U ILLINOIS UR
   Diesmann M, 2001, NEUROCOMPUTING, V38, P565, DOI 10.1016/S0925-2312(01)00409-X
   EASON G, 1955, PHILOS TR R SOC S-A, V247, P529, DOI 10.1098/rsta.1955.0005
   Eytan D, 2003, J NEUROSCI, V23, P9349
   Hines ML, 1997, NEURAL COMPUT, V9, P1179, DOI 10.1162/neco.1997.9.6.1179
   JIMBO Y, 1993, IEEE T BIO-MED ENG, V40, P804, DOI 10.1109/10.238465
   Jimbo Y, 2000, BIOL CYBERN, V83, P1, DOI 10.1007/PL00007970
   JUNG S, 2007, 3 INT IEEE EMBS C 20, P688
   Kamioka H, 1996, NEUROSCI LETT, V206, P109, DOI 10.1016/S0304-3940(96)12448-4
   MAEDA E, 1995, J NEUROSCI, V15, P6834
   Morin FO, 2005, J BIOSCI BIOENG, V100, P131, DOI 10.1263/jbb.100.131
   Novellino A, 2003, NEUROCOMPUTING, V52-4, P661, DOI [10.1016/S0925-2312(02)00861-5, 10.1016/S0925-23 12(02)00861-5]
   Persi E, 2004, NEUROCOMPUTING, V58, P179, DOI 10.1016/j.neucom.2004.01.040
   Reutimann J, 2003, NEURAL COMPUT, V15, P811, DOI 10.1162/08997660360581912
   Segev R, 2000, NEURAL NETWORKS, V13, P185, DOI 10.1016/S0893-6080(99)00084-2
   Segev R, 2004, PHYS REV LETT, V92, DOI 10.1103/PhysRevLett.92.118102
   Segev R, 2003, PHYS REV LETT, V90, DOI 10.1103/PhysRevLett.90.168101
   Segev R, 2002, PHYS REV LETT, V88, DOI 10.1103/PhysRevLett.88.118102
   Segev R, 2001, PHYSICA A, V302, P64, DOI 10.1016/S0378-4371(01)00441-1
   Segev R, 2001, PHYS REV E, V64, DOI 10.1103/PhysRevE.64.011920
   Streit J, 2001, EUR J NEUROSCI, V14, P191, DOI 10.1046/j.0953-816x.2001.01636.x
   Taketani M., 2006, ADV NETWORK ELECTROP
   Wagenaar DA, 2006, J NEGAT RESULTS BIOM, V5, DOI 10.1186/1477-5751-5-16
   *XNBC, 2007, XNBC VERS 9 7
NR 29
TC 1
Z9 1
U1 0
U2 3
PY 2007
BP 736
EP 740
WC Engineering, Biomedical; Mathematical & Computational Biology
DA 2023-11-11
ER

PT B
AU Jerry, M
   Tsai, WY
   Xie, BH
   Lie, XQ
   Narayanan, V
   Raychowdhury, A
   Datta, S
AF Jerry, Matthew
   Tsai, Wei-yu
   Xie, Baihua
   Lie, Xueqing
   Narayanan, Vijay
   Raychowdhury, Arijit
   Datta, Suman
GP IEEE
TI Phase Transition Oxide Neuron for Spiking Neural Networks
SO 2016 74TH ANNUAL DEVICE RESEARCH CONFERENCE (DRC)
DT Meeting Abstract
CT 74th Annual Device Research Conference (DRC)
CY JUN 19-22, 2016
CL University of Delaware, Newark, DE
HO University of Delaware
C1 [Jerry, Matthew; Datta, Suman] Univ Notre Dame, 275 Fitzpatrick Hall, Notre Dame, IN 46556 USA.
   [Tsai, Wei-yu; Xie, Baihua; Lie, Xueqing; Narayanan, Vijay] Penn State Univ, 342 Informat Sci & Tech Bldg, University Pk, PA 16802 USA.
   [Raychowdhury, Arijit] Georgia Inst Technol, North Ave, Atlanta, GA 30332 USA.
EM sdatta@nd.edu
CR [Anonymous], 2016, IEEE T COMPUT
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   LADD LA, 1969, SOLID STATE COMMUN, V7, P425, DOI 10.1016/0038-1098(69)90888-6
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Shukla N, 2014, SCI REP-UK, V4, DOI 10.1038/srep04964
NR 5
TC 0
Z9 0
U1 1
U2 3
PY 2016
WC Engineering, Electrical & Electronic; Instruments & Instrumentation;
   Materials Science, Multidisciplinary
DA 2023-11-11
ER

PT C
AU Gi, S
   Yeo, I
   Lee, BG
AF Gi, Sanggyun
   Yeo, Injune
   Lee, Byung-geun
GP IEEE
TI Implementation of STDP Learning for Non-volatile Memory-based Spiking
   Neural Network using Comparator Metastability
SO 2019 IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE CIRCUITS
   AND SYSTEMS (AICAS 2019)
DT Proceedings Paper
CT 1st IEEE International Conference on Artificial Intelligence Circuits
   and Systems (AICAS)
CY MAR 18-20, 2019
CL Hsinchu, TAIWAN
DE spike-timing dependent plasticity; spiking neural network;
   metastability; dynamic comparator; non-volatile memory
ID SYNAPSE
AB This paper presents a circuit for spike-timing dependent plasticity (STDP) learning of a non-volatile memory (NVM) based spiking neural network (SNN). Unlike conventional hardware for implementation of STDP learning, the proposed circuit does not require additional memory, amplifiers, or an STDP spike generator. Instead, the circuit utilizes the comparison time information of the dynamic comparator to implement a non-linear transfer curve of STDP learning. The circuit includes a dynamic comparator, NVM device, and some digital circuitry to write the conductance of NVM according to the STDP learning rule. Finally, the conductance response model and designed circuit for the STDP learning are used to compare the simulation results of STDP with mathematical STDP. Applications of the proposed circuit are in the design of NVM-based SNN hardware or other bio-inspired hardware systems.
C1 [Gi, Sanggyun; Yeo, Injune; Lee, Byung-geun] Gwangju Inst Sci & Technol, Sch Elect Engn & Comp Sci, Gwangju, South Korea.
RP Gi, S (corresponding author), Gwangju Inst Sci & Technol, Sch Elect Engn & Comp Sci, Gwangju, South Korea.
EM y2ksk2@gist.ac.kr; injuneyeo@gist.ac.kr; bglee@gist.ac.kr
CR Bichler O, 2012, IEEE T ELECTRON DEV, V59, P2206, DOI 10.1109/TED.2012.2197951
   Cruz-Albrecht JM, 2012, IEEE T BIOMED CIRC S, V6, P246, DOI 10.1109/TBCAS.2011.2174152
   Garbin D, 2015, IEEE T ELECTRON DEV, V62, P2494, DOI 10.1109/TED.2015.2440102
   Indiveri G, 2006, IEEE T NEURAL NETWOR, V17, P211, DOI 10.1109/TNN.2005.860850
   Narayanan S, 2017, IEEE IJCNN, P2451, DOI 10.1109/IJCNN.2017.7966154
   Park H, 2012, 2012 IEEE INTERNATIONAL CONFERENCE ON CONSUMER ELECTRONICS (ICCE), P100, DOI 10.1109/ICCE.2012.6161759
   Pedroni BU, 2016, BIOMED CIRC SYST C, P580, DOI 10.1109/BioCAS.2016.7833861
   Sangkil Kim, 2015, 2015 IEEE MTT-S International Microwave Symposium (IMS2015), P1, DOI 10.1109/MWSYM.2015.7166723
   Serrano-Gotarredona T, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00002
   Suri M, 2013, IEEE T ELECTRON DEV, V60, P2402, DOI 10.1109/TED.2013.2263000
   Vincent AF, 2015, IEEE T BIOMED CIRC S, V9, P166, DOI 10.1109/TBCAS.2015.2414423
   Yu SM, 2011, IEEE T ELECTRON DEV, V58, P2729, DOI 10.1109/TED.2011.2147791
   Zamarreño-Ramos C, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00026
NR 13
TC 0
Z9 0
U1 0
U2 5
PY 2019
BP 239
EP 243
DI 10.1109/aicas.2019.8771602
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
DA 2023-11-11
ER

PT C
AU Arnold, E
   Böcherer, G
   Müller, E
   Spilger, P
   Schemmel, J
   Calabrò, S
   Kuschnerov, M
AF Arnold, Elias
   Boecherer, Georg
   Mueller, Eric
   Spilger, Philipp
   Schemmel, Johannes
   Calabro, Stefano
   Kuschnerov, Maxim
GP IEEE
TI Spiking Neural Network Equalization on Neuromorphic Hardware for IM/DD
   Optical Communication
SO 2022 EUROPEAN CONFERENCE ON OPTICAL COMMUNICATION (ECOC)
DT Proceedings Paper
CT European Conference on Optical Communication (ECOC)
CY SEP 18-22, 2022
CL ELECTR NETWORK
AB A spiking neural network (SNN) nonlinear equalizer model is implemented on the mixedsignal neuromorphic hardware system BrainScaleS-2 and evaluated for an IM/DD link. The BER 2e-3 is achieved with a hardware penalty less than 1 dB, outperforming numeric linear equalization. (C) 2022 The Author(s)
C1 [Arnold, Elias; Mueller, Eric; Spilger, Philipp; Schemmel, Johannes] Heidelberg Univ, Kirchhoff Inst Phys, Elect Vis S, Heidelberg, Germany.
   [Boecherer, Georg; Calabro, Stefano; Kuschnerov, Maxim] Huawei Technol Duesseldorf GmbH, Munich Res Ctr, Munich, Germany.
RP Arnold, E (corresponding author), Heidelberg Univ, Kirchhoff Inst Phys, Elect Vis S, Heidelberg, Germany.
EM elias.arnold@kip.uni-heidelberg.de; georg.bocherer@huawei.com
NR 0
TC 1
Z9 1
U1 0
U2 0
PY 2022
WC Optics; Telecommunications
DA 2023-11-11
ER

PT C
AU Yang, Q
   Liu, Q
   Li, HZ
AF Yang, Qu
   Liu, Qi
   Li, Haizhou
GP Int Speech Commun Assoc
TI Deep Residual Spiking Neural Network for Keyword Spotting in
   Low-Resource Settings
SO INTERSPEECH 2022
SE Interspeech
DT Proceedings Paper
CT Interspeech Conference
CY SEP 18-22, 2022
CL Incheon, SOUTH KOREA
DE Keyword spotting system; Portable devices; Deep spiking neural networks;
   Power efficiency
AB We propose a practical solution for the implementation of keyword spotting (KWS) system on portable devices, that features all three properties required for battery-powered portable scenarios: low power usage, small footprint, and high accuracy. In particular, we study an end-to-end KWS system with deep residual Spiking Neural Network (SNN), perform experiments on Google Speech Commands Dataset, and compare with both state-of-the-art ANN and SNN models. First, the proposed solution outperforms its ANN counterpart and other SNN in terms of energy efficiency. Second, it requires a smaller footprint (86.5K) than other ANN and SNN (210K) models. Third, in terms of classification accuracy, it outperforms the existing power-efficient SNN benchmark by 4% to 17%. The proposed solution is an example of the unparalleled performance of spiking neural network in real-world applications.
C1 [Yang, Qu; Liu, Qi; Li, Haizhou] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore, Singapore.
   [Liu, Qi] South China Univ Technol, Sch Future Technol, Guangzhou, Peoples R China.
   [Li, Haizhou] Chinese Univ Hong Kong, Shenzhen, Peoples R China.
   [Li, Haizhou] Kriston AI, Xiamen, Peoples R China.
RP Yang, Q (corresponding author), Natl Univ Singapore, Dept Elect & Comp Engn, Singapore, Singapore.
EM quyang@u.nus.edu.sg; drliuqi@scut.edu.cn; haizhouli@cuhk.edu.cn
CR Baevski A., 2020, ADV NEURAL INFORM PR
   Bellec G., 2018, ADV NEURAL INFORM PR, P787
   Chen Guoguo, 2014, INT C ACOUST SPEECH, P4087, DOI [DOI 10.1109/ICASSP.2014.6854370, 10.1109/ICASSP.2014.6854370]
   Coucke A, 2019, INT CONF ACOUST SPEE, P6351, DOI 10.1109/ICASSP.2019.8683474
   de Andrade DC, 2018, NEURAL ATTENTION MOD
   Fernández-Marqués J, 2018, PROCEEDINGS OF THE 2018 INTERNATIONAL WORKSHOP ON EMBEDDED AND MOBILE DEEP LEARNING (EMDL '18), P13, DOI 10.1145/3212725.3212731
   Jansson P., 2018, SINGLE WORD SPEECH R
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90
   Lee Chankyu, 2020, FRONTIERS NEUROSCIEN
   Lengerich C., 2016, ARXIV161109405
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Myer S., 2018, ARXIV180704353
   Pan Z., 2020, ARXIV200703274
   Panchapagesan S, 2016, INTERSPEECH, P760, DOI 10.21437/Interspeech.2016-1485
   Pellegrini T., 2020, LOW ACTIVITY SUPERVI
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Sigtia S, 2018, INTERSPEECH, P2092
   Tan K. C., 2020, ARXIV200701204
   Tang R, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P5484, DOI 10.1109/ICASSP.2018.8462688
   Tavanaei A, 2017, LECT NOTES COMPUT SC, V10639, P899, DOI 10.1007/978-3-319-70136-3_95
   Wang Y., 2018, IEEE T NEUR NET LEAR
   Warden P., 2018, ARXIV
   Wu J., 2021, IEEE T NEURAL NETWOR
   Wu JC, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351221
   Wu JB, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00199
   Xu CL, 2020, IEEE-ACM T AUDIO SPE, V28, P1370, DOI 10.1109/TASLP.2020.2987429
   Yilmaz E, 2020, INTERSPEECH, P2557, DOI 10.21437/Interspeech.2020-1230
   Zhang ML, 2019, AAAI CONF ARTIF INTE, P1327
   Zhang Y, 2015, IEEE T NEUR NET LEAR, V26, P2635, DOI 10.1109/TNNLS.2015.2388544
   Zheng H., 2020, ARXIV201105280
NR 32
TC 1
Z9 1
U1 1
U2 1
PY 2022
BP 3023
EP 3027
DI 10.21437/Interspeech.2022-107
WC Acoustics; Audiology & Speech-Language Pathology; Computer Science,
   Artificial Intelligence; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Pavlásek, J
   Jenca, J
AF Pavlásek, J
   Jenca, J
TI Temporal coding and recognition of uncued temporal patterns in neuronal
   spike trains:: Biologically plausible network of coincidence detectors
   and coordinated time delays
SO BIOLOGIA
DT Article
DE neuronal network; spike train; temporal pattern; pattern recognition;
   encoding; decoding
ID NEURAL-NETWORK; MODEL; BINDING; CODE
AB Much of the work on sensory systems assumes that grouping of spikes by time can carry significant information about a stimulus. The fundamental problem of general perception is how can a neural network identify a specific temporal pattern within the stream of pulsatile input activity. A computational model of a neuronal network is described that recognizes a temporal pattern (a group of spikes in a narrow time window) in continuous and uncued spike trains. The devised network performs real-time recognition both in a single neuron employing a temporal code and within the spiking activity of co-activated neurons in responding pathways. The temporal resolution of the spike timing and recognition of a temporal pattern is possible to accuracy within 1 ms limit. Special attention in simulation experiments has been devoted to the aspects of real time processing under condition of background spiking noise. Operation of the network is based upon biologically plausible filtering mechanisms and population neurodynamics.
C1 Slovak Acad Sci, Inst Normal & Pathol Physiol, Dept Neurophysiol, SK-83334 Bratislava, Slovakia.
EM unpfpavl@savba.sk
CR [Anonymous], NEURAL REPRESENTATIO
   [Anonymous], INFORMATION PROCESSI, DOI DOI 10.1007/978-3-662-25549-0_11
   Barlow H B, 1972, Perception, V1, P371, DOI 10.1068/p010371
   Bi GQ, 1999, NATURE, V401, P792, DOI 10.1038/44573
   BIALEK W, 1991, SCIENCE, V252, P1854, DOI 10.1126/science.2063199
   BROOKS VB, 1969, INFORMATION PROCESSI, P231
   BUONOMANO DV, 1995, SCIENCE, V267, P1028, DOI 10.1126/science.7863330
   Buonomano DV, 1999, NEURAL COMPUT, V11, P103, DOI 10.1162/089976699300016836
   BURGESS PR, 1973, HDB SENSORY PHYSIOLO, V2, P29
   CASSEDAY JH, 1995, NEURAL REPRESENTATIO, P25
   CERVERO F, 1980, BRAIN, V103, P717, DOI 10.1093/brain/103.4.717
   COVEY E, 1995, NEURAL REPRESENTATIO
   CRICK F, 1990, COLD SPRING HARB SYM, V55, P953
   Edelman GM., 1987, NEURAL DARWINISM THE
   ENGEL AK, 1992, TRENDS NEUROSCI, V15, P218, DOI 10.1016/0166-2236(92)90039-B
   GRANGER R, 1995, NEURAL REPRESENTATIO, P183
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   Iggo Ainsley, 1974, PERIPHERAL NERVOUS S, P347, DOI 10.1007/978-1-4615-8699-9_14
   JANCO J, 1994, COMPUT ARTIF INTELL, V13, P603
   Koch Christof, 1999, P1
   Konig P, 1996, TRENDS NEUROSCI, V19, P130, DOI 10.1016/S0166-2236(96)80019-1
   KONISHI M, 1990, COLD SH Q B, V55, P575
   Longuet-Higgins H.C., 1969, INFORM PROCESSING NE, P37, DOI [10.1007/978-3-662-25549-0_3, DOI 10.1007/978-3-662-25549-0_3]
   MCCLURKIN JW, 1991, SCIENCE, V253, P675, DOI 10.1126/science.1908118
   MOUNTCASTLE VB, 1997, NEUROSCIENCES, P393
   NOWAK LG, 1967, CEREB CORTEX, V12, P205
   Pavlasek J, 1997, BIOL CYBERN, V77, P359, DOI 10.1007/s004220050396
   Pavlásek J, 1998, GEN PHYSIOL BIOPHYS, V17, P323
   Pavlasek J, 1996, NEURAL NETWORKS, V9, P1131, DOI 10.1016/0893-6080(96)00039-1
   Pavlásek J, 1999, GEN PHYSIOL BIOPHYS, V18, P249
   PAVLASEK J, 1994, RETICULAR FORMATION
   PORT RF, 1995, NEURAL REPRESENTATIO, P77
   REDMAN S, 1983, J PHYSIOL-LONDON, V343, P117, DOI 10.1113/jphysiol.1983.sp014884
   RICHMOND BJ, 1987, J NEUROPHYSIOL, V57, P132, DOI 10.1152/jn.1987.57.1.132
   Rose G. J., 1995, NEURAL REPRESENTATIO, P1
   Segev I, 1998, NATURE, V393, P207, DOI 10.1038/30340
   SINGER W, 1993, ANNU REV PHYSIOL, V55, P349, DOI 10.1146/annurev.physiol.55.1.349
   Singer W, 1999, CURR OPIN NEUROBIOL, V9, P189, DOI 10.1016/S0959-4388(99)80026-9
   THEUNISSEN F, 1995, J COMPUT NEUROSCI, V2, P149, DOI 10.1007/BF00961885
   Thorpe SJ, 1997, ADV NEUR IN, V9, P901
   VONDERMALSBURG C, 1995, CURR OPIN NEUROBIOL, V5, P520, DOI 10.1016/0959-4388(95)80014-X
   Waite PME, 1998, TRENDS NEUROSCI, V21, P265, DOI 10.1016/S0166-2236(97)01217-4
NR 42
TC 2
Z9 2
U1 0
U2 0
PD DEC
PY 2001
VL 56
IS 6
BP 591
EP 604
WC Biology
DA 2023-11-11
ER

PT S
AU Buscicchio, CA
   Górecki, P
   Caponetti, L
AF Buscicchio, Cosimo A.
   Gorecki, Przemyslaw
   Caponetti, Laura
BE Esposito, F
   Ras, ZW
   Malerba, D
   Semeraro, G
TI Speech emotion recognition using spiking neural networks
SO FOUNDATIONS OF INTELLIGENT SYSTEMS, PROCEEDINGS
SE Lecture Notes in Artificial Intelligence
DT Article; Proceedings Paper
CT 16th International Symposium on Methodologies for Intelligent Systems
CY SEP 27-29, 2006
CL Bari, ITALY
AB Human social communication depends largely on exchanges of non-verbal signals, including non-lexical expression of emotions in speech. In this work, we propose a biologically plausible methodology for the problem of emotion recognition, based on the extraction of vowel information from an input speech signal and on the classification of extracted information by a spiking neural network. Initially, a speech signal is segmented into vowel parts which are represented with a set of salient features, related to the Mel-frequency cesptrum. Different emotion classes are then recognized by a spiking neural network and classified into five different emotion classes.
C1 Univ Bari, Dipartimento Informat, I-70126 Bari, Italy.
RP Buscicchio, CA (corresponding author), Univ Bari, Dipartimento Informat, Via E Orabona 4, I-70126 Bari, Italy.
EM buscicchio@di.uniba.it; przemyslaw@di.uniba.it; laura@di.uniba.it
CR [Anonymous], PULSED NEURAL NETWOR
   Brandt A. V., 1983, Proceedings of ICASSP 83. IEEE International Conference on Acoustics, Speech and Signal Processing, P1017
   Chiu C. C., 1994, 1994 International Computer Symposium Conference Proceedings, P83
   Dellaert F, 1996, ICSLP 96 - FOURTH INTERNATIONAL CONFERENCE ON SPOKEN LANGUAGE PROCESSING, PROCEEDINGS, VOLS 1-4, P1970, DOI 10.1109/ICSLP.1996.608022
   FERSTER D, 1995, SCIENCE, V270, P756, DOI 10.1126/science.270.5237.756
   Hopfield JJ, 2001, P NATL ACAD SCI USA, V98, P1282, DOI 10.1073/pnas.031567098
   HORN D, 1999, PULSED NEURAL NETWOR
   Kwon O-W, 2003, EUROPEAN C SPEECH CO, P125
   MAASS W, 2001, HDB BRAIN THEORY NEU
   MCCAULEY L, 1998, P WECC 98 WORKSH EMB
   PETRUSHIN VA, P 1999 C ART NEUR NE
   RABINER J, 1993, FUNDAMENTALS SPEECH
   Reeves B., 1996, MEDIA EQUATION PEOPL, DOI DOI 10.1007/S42452-020-2192-7
   SAGISAKA Y, 1997, COMPUTING PROSODY
   Steeneken HJM, 1999, INT CONF ACOUST SPEE, P2079, DOI 10.1109/ICASSP.1999.758342
   YACOUB S, 2003, P EUR GEN
NR 16
TC 6
Z9 6
U1 0
U2 3
PY 2006
VL 4203
BP 38
EP 46
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT J
AU Jirsa, VK
AF Jirsa, Viktor K.
TI Dispersion and time delay effects in synchronized spike-burst networks
SO COGNITIVE NEURODYNAMICS
DT Article
DE Synchronization; Neural network; Spiking; Bursting; Dispersion; Time
   delay
ID DOPAMINE NEURONS; FIRING PATTERNS; OSCILLATIONS; CORTEX; MODEL;
   INTEGRATION; DYNAMICS; BEHAVIOR; STATES
AB We study spike-burst neural activity and investigate its transitions to synchronized states under electrical coupling. Our reported results include the following: (1) Synchronization of spike-burst activity is a multi-time scale phenomenon and burst synchrony is easier to achieve than spike synchrony. (2) Synchrony of networks with time-delayed connections can be achieved at lower coupling strengths than within the same network with instantaneous couplings. (3) The introduction of parameter dispersion into the network destroys the existence of synchrony in the strict sense, but the network dynamics in major regimes of the parameter space can still be effectively captured by a mean field approach if the couplings are excitatory. Our results on synchronization of spiking networks are general of nature and will aid in the development of minimal models of neuronal populations. The latter are the building blocks of large scale brain networks relevant for cognitive processing.
C1 [Jirsa, Viktor K.] CNRS, Lab Mouvement & Percept, Theoret Neurosci Grp, UMR6152, F-13288 Marseille, France.
   [Jirsa, Viktor K.] Florida Atlantic Univ, Dept Phys, Ctr Complex Syst & Brain Sci, Boca Raton, FL 33431 USA.
RP Jirsa, VK (corresponding author), CNRS, Lab Mouvement & Percept, Theoret Neurosci Grp, UMR6152, F-13288 Marseille, France.
EM jirsa@ccs.fau.edu
CR ABELES M, 1993, J NEUROPHYSIOL, V70, P1629, DOI 10.1152/jn.1993.70.4.1629
   Amzica F, 1998, ELECTROEN CLIN NEURO, V107, P69, DOI 10.1016/S0013-4694(98)00051-0
   Aradi I, 2002, J PHYSIOL-LONDON, V538, P227, DOI 10.1113/jphysiol.2001.013054
   Assisi CG, 2005, PHYS REV LETT, V94, DOI 10.1103/PhysRevLett.94.018106
   Avoli M., 1990, GEN EPILEPSY NEUROBI
   Breakspear M, 2007, HDB BRAIN CONNECTIVI
   BRESSLER SL, 1990, TRENDS NEUROSCI, V13, P161, DOI 10.1016/0166-2236(90)90039-D
   Bressler SL, 2001, TRENDS COGN SCI, V5, P26, DOI 10.1016/S1364-6613(00)01564-3
   Bressler SL, 2006, INT J PSYCHOPHYSIOL, V60, P139, DOI 10.1016/j.ijpsycho.2005.12.008
   BUHMANN J, 1989, PHYS REV A, V40, P4145, DOI 10.1103/PhysRevA.40.4145
   Bullmore ET, 1996, NEUROIMAGE, V4, P16, DOI 10.1006/nimg.1996.0026
   Crick F., 1990, Seminars in the Neurosciences, V2, P263
   DESMEDT JE, 1994, NEUROSCI LETT, V168, P126, DOI 10.1016/0304-3940(94)90432-4
   Dhamala M, 2004, PHYS REV LETT, V92, DOI 10.1103/PhysRevLett.92.028101
   FREEMAN AS, 1985, LIFE SCI, V36, P1983, DOI 10.1016/0024-3205(85)90448-5
   GRACE AA, 1984, J NEUROSCI, V4, P2877
   HINDMARSH JL, 1982, NATURE, V296, P162, DOI 10.1038/296162a0
   HINDMARSH JL, 1984, PROC R SOC SER B-BIO, V221, P87, DOI 10.1098/rspb.1984.0024
   HOSFORD DA, 1992, SCIENCE, V257, P398, DOI 10.1126/science.1321503
   Izhikevich EM, 2000, INT J BIFURCAT CHAOS, V10, P1171, DOI 10.1142/S0218127400000840
   Jirsa V., 2007, HDB BRAIN CONNECTIVI
   Jirsa VK, 2004, NEUROINFORMATICS, V2, P183, DOI 10.1385/NI:2:2:183
   Koch Christof, 1999, P1
   Koob G. F., 1987, BRAIN REWARD SYSTEMS, P35
   LJUNGBERG T, 1992, J NEUROPHYSIOL, V67, P145, DOI 10.1152/jn.1992.67.1.145
   MacDonald N., 1989, BIOL DELAY SYSTEMS L, DOI 10.1007/BF00049132
   MATTHEWS PC, 1990, PHYS REV LETT, V65, P1701, DOI 10.1103/PhysRevLett.65.1701
   MCCORMICK DA, 1990, NEUROSCIENCE, V39, P103, DOI 10.1016/0306-4522(90)90225-S
   McIntosh AR, 2000, NEURAL NETWORKS, V13, P861, DOI 10.1016/S0893-6080(00)00059-9
   MESULAM MM, 1990, ANN NEUROL, V28, P597, DOI 10.1002/ana.410280502
   Miltner WHR, 1999, NATURE, V397, P434, DOI 10.1038/17126
   MILTON J, 2007, HDB BRAIN CONNECTIVI
   Monte S. D., 2003, PHYS REV LETT, V90
   Mountcastle V. B., 1998, PERCEPTUAL NEUROSCIE
   Pecora LM, 1998, PHYS REV LETT, V80, P2109, DOI 10.1103/PhysRevLett.80.2109
   PECORA LM, 1990, PHYS REV LETT, V64, P821, DOI 10.1103/PhysRevLett.64.821
   RINZEL J, 1987, J MATH BIOL, V25, P653, DOI 10.1007/BF00275501
   Roelfsema PR, 1997, NATURE, V385, P157, DOI 10.1038/385157a0
   Rosenblum MG, 1996, PHYS REV LETT, V76, P1804, DOI 10.1103/PhysRevLett.76.1804
   Rubin J, 2000, J MATH BIOL, V41, P513, DOI 10.1007/s002850000065
   Schultz W, 1998, J NEUROPHYSIOL, V80, P1, DOI 10.1152/jn.1998.80.1.1
   SHERMAN SM, 1986, EXP BRAIN RES, V63, P1
   SINGER W, 1995, ANNU REV NEUROSCI, V18, P555, DOI 10.1146/annurev.neuro.18.1.555
   SNEAD OC, 1995, ANN NEUROL, V37, P146, DOI 10.1002/ana.410370204
   SPORNS O, 1989, P NATL ACAD SCI USA, V86, P7265, DOI 10.1073/pnas.86.18.7265
   STERIADE M, 1988, PHYSIOL REV, V68, P649, DOI 10.1152/physrev.1988.68.3.649
   STERIADE M, 1993, SCIENCE, V262, P679, DOI 10.1126/science.8235588
   STROGATZ SH, 1993, PHYS REV E, V47, P220, DOI 10.1103/PhysRevE.47.220
   Tallon-Baudry C, 2001, J NEUROSCI, V21, part. no., DOI 10.1523/JNEUROSCI.21-20-j0008.2001
   Treisman A, 1996, CURR OPIN NEUROBIOL, V6, P171, DOI 10.1016/S0959-4388(96)80070-5
   von Stein A, 1999, CEREB CORTEX, V9, P137, DOI 10.1093/cercor/9.2.137
   WEINBERGER DR, 1987, ARCH GEN PSYCHIAT, V44, P660, DOI 10.1001/archpsyc.1987.01800190080012
NR 52
TC 43
Z9 43
U1 0
U2 3
PD MAR
PY 2008
VL 2
IS 1
BP 29
EP 38
DI 10.1007/s11571-007-9030-0
WC Neurosciences
DA 2023-11-11
ER

PT J
AU Hong, DG
   Kim, SY
   Lim, W
AF Hong, Duk-Geun
   Kim, Sang-Yoon
   Lim, Woochang
TI Effect of Sparse Random Connectivity on the Stochastic Spiking Coherence
   of Inhibitory Subthreshold Neurons
SO JOURNAL OF THE KOREAN PHYSICAL SOCIETY
DT Article
DE Sparse random connectivity; Stochastic spiking coherence
ID EXCITATORY NEURAL-NETWORKS; OSCILLATIONS; DYNAMICS; MODEL; RHYTHMS;
   SYNCHRONIZATION; DISCHARGES; PRINCIPLES; FREQUENCY
AB We study the effect of network structure on the stochastic spiking coherence (i.e., collective coherence emerging via cooperation of noise-induced neural spikings) in an inhibitory population of subthreshold neurons (which cannot fire spontaneously without noise). Previously, stochastic spiking coherence was found to occur for the case of global coupling. However, "sparseness" of a real neural network is well known. Hence, we investigate the effect of sparse random connectivity on the stochastic spiking coherence by varying the average number of synaptic inputs per neuron M-syn. From our numerical results, stochastic spiking coherence seems to emerge if M-syn is larger than a threshold M*(syn) whose dependence on the network size N seems to be quite weak. This stochastic spiking coherence may be well visualized in a raster plot of neural spikes. For a coherent case, partially-occupied "stripes" (composed of spikes and indicating collective coherence) appear. As M-syn is decreased from N - 1 (globally-coupled case), the average occupation degree of spikes increases very slowly. On the other hand, the average pacing degree between spikes (representing the precision of spike timing) decreases slowly, but near M*(syn) its decrease becomes very rapid. This decrease in the pacing degree can also be well seen through merging of multiple peaks in the interspike interval histograms. Due to the effect of the pacing degree, the degree of stochastic spiking coherence decreases abruptly near the threshold M*(syn).
C1 [Hong, Duk-Geun; Kim, Sang-Yoon] Kangwon Natl Univ, Dept Phys, Chunchon 200701, South Korea.
   [Lim, Woochang] Daegu Natl Univ Educ, Dept Sci Educ, Taegu 705115, South Korea.
RP Hong, DG (corresponding author), Kangwon Natl Univ, Dept Phys, Chunchon 200701, South Korea.
EM woochanglim@dnue.ac.kr
CR BARKAI E, 1990, PHYS REV A, V41, P590, DOI 10.1103/PhysRevA.41.590
   Börgers C, 2005, NEURAL COMPUT, V17, P557, DOI 10.1162/0899766053019908
   Brunel N, 2003, J NEUROPHYSIOL, V90, P415, DOI 10.1152/jn.01095.2002
   Brunel N, 1999, NEURAL COMPUT, V11, P1621, DOI 10.1162/089976699300016179
   Brunel N, 2000, J COMPUT NEUROSCI, V8, P183, DOI 10.1023/A:1008925309027
   Brunel N, 2008, CHAOS, V18, DOI 10.1063/1.2779858
   Buzsáki G, 2004, TRENDS NEUROSCI, V27, P186, DOI 10.1016/j.tins.2004.02.007
   Buzsaki G., 2006, RHYTHMS BRAIN, DOI 10.1093/acprof:oso/9780195301069.001.0001
   Geisler C, 2005, J NEUROPHYSIOL, V94, P4344, DOI 10.1152/jn.00510.2004
   Golomb D, 2000, NEURAL COMPUT, V12, P1095, DOI 10.1162/089976600300015529
   GOLOMB D, 1994, PHYSICA D, V72, P259, DOI 10.1016/0167-2789(94)90214-3
   Gray C M, 1994, J Comput Neurosci, V1, P11, DOI 10.1007/BF00962716
   Grosse P, 2002, CLIN NEUROPHYSIOL, V113, P1523, DOI 10.1016/S1388-2457(02)00223-7
   Hansel D, 2003, NEURAL COMPUT, V15, P1, DOI 10.1162/089976603321043685
   HANSEL D, 1995, NEURAL COMPUT, V7, P307, DOI 10.1162/neco.1995.7.2.307
   Hauschildt B, 2006, PHYS REV E, V74, DOI 10.1103/PhysRevE.74.051906
   HODGKIN AL, 1948, J PHYSIOL-LONDON, V107, P165, DOI 10.1113/jphysiol.1948.sp004260
   Izhikevich EM, 2000, INT J BIFURCAT CHAOS, V10, P1171, DOI 10.1142/S0218127400000840
   KURAMOTO Y, 1991, PHYSICA D, V50, P15, DOI 10.1016/0167-2789(91)90075-K
   Lim W, 2011, J COMPUT NEUROSCI, V31, P667, DOI 10.1007/s10827-011-0330-3
   Lim W, 2009, INT J MOD PHYS B, V23, P703, DOI 10.1142/S0217979209049991
   Manrubia S C., 2004, EMERGENCE DYNAMICAL
   Miguel M. S., 2000, INSTABILITIES NONEQU, P35
   MORRIS C, 1981, BIOPHYS J, V35, P193, DOI 10.1016/S0006-3495(81)84782-0
   Rinzel J., 1998, METHODS NEURONAL MOD, P251
   Tass P, 1998, PHYS REV LETT, V81, P3291, DOI 10.1103/PhysRevLett.81.3291
   Tiesinga PHE, 2001, HIPPOCAMPUS, V11, P251, DOI 10.1002/hipo.1041
   Tsumoto K, 2006, NEUROCOMPUTING, V69, P293, DOI 10.1016/j.neucom.2005.03.006
   Van Vreeswijk C, 1994, J Comput Neurosci, V1, P313
   Wang XJ, 2010, PHYSIOL REV, V90, P1195, DOI 10.1152/physrev.00035.2008
   WANG XJ, 1992, NEURAL COMPUT, V4, P84, DOI 10.1162/neco.1992.4.1.84
   WANG XJ, 1995, P NATL ACAD SCI USA, V92, P5577, DOI 10.1073/pnas.92.12.5577
   Wang XJ, 1996, J NEUROSCI, V16, P6402
   WANG XJ, 2003, ENCY COGNITIVE SCI, P272
   Wang YQ, 2000, PHYS REV E, V61, P740, DOI 10.1103/PhysRevE.61.740
   White JA, 1998, J COMPUT NEUROSCI, V5, P5, DOI 10.1023/A:1008841325921
   Whittington MA, 2000, INT J PSYCHOPHYSIOL, V38, P315, DOI 10.1016/S0167-8760(00)00173-2
NR 37
TC 11
Z9 11
U1 0
U2 1
PD OCT
PY 2011
VL 59
IS 4
BP 2840
EP 2846
DI 10.3938/jkps.59.2840
WC Physics, Multidisciplinary
DA 2023-11-11
ER

PT C
AU Böcherer, G
   Strasser, F
   Arnold, E
   Lin, YX
   Schemmel, J
   Calabrò, S
   Kuschnerov, M
AF Boecherer, Georg
   Strasser, Florian
   Arnold, Elias
   Lin, Youxi
   Schemmel, Johannes
   Calabro, Stefano
   Kuschnerov, Maxim
GP IEEE
TI Spiking Neural Network Linear Equalization: Experimental Demonstration
   of 2km 100Gb/s IM/DD PAM4 Optical Transmission
SO 2023 OPTICAL FIBER COMMUNICATIONS CONFERENCE AND EXHIBITION, OFC
DT Proceedings Paper
CT Optical Fiber Communications Conference and Exhibition (OFC)
CY MAR 05-09, 2023
CL San Diego, CA
AB A linear feed-forward equalizer is implemented by a potentially low -power spiking neural network. For a 100Gb/s PAM -4 IM/DD optical 2km transmission, no performance penalty compared to a digital implementation is observed. (c) 2022 The Author(s)
C1 [Boecherer, Georg; Strasser, Florian; Lin, Youxi; Calabro, Stefano; Kuschnerov, Maxim] Huawei Technol Duesseldorf GmbH, Munich Res Ctr, D-80992 Munich, Germany.
   [Arnold, Elias; Schemmel, Johannes] Heidelberg Univ, Elect Vis Grp, Kirchhoff Inst Phys, D-69120 Heidelberg, Germany.
RP Böcherer, G (corresponding author), Huawei Technol Duesseldorf GmbH, Munich Res Ctr, D-80992 Munich, Germany.
CR Arnold E., 2022, OPTICA ADV PHOTONICS
   Arnold E, 2022, 2022 EUROPEAN CONFERENCE ON OPTICAL COMMUNICATION (ECOC)
   Bansbach E.-M., 2022, ARXIV
   Burr GW, 2017, ADV PHYS-X, V2, P89, DOI 10.1080/23746149.2016.1259585
   Caruso G., 2022, EUROPEAN C OPTICAL C
   Eldebiky A, 2022, 2022 EUROPEAN CONFERENCE ON OPTICAL COMMUNICATION (ECOC)
   GARDNER FM, 1986, IEEE T COMMUN, V34, P423, DOI 10.1109/TCOM.1986.1096561
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   Li S, 2021, IEEE PHOTONIC TECH L, V33, P978, DOI 10.1109/LPT.2021.3087323
   Muller E., 2022, FRONT NEUROSCI-SWITZ, V16
   Pehle C, 2022, FRONT NEUROSCI-SWITZ, V16, DOI 10.3389/fnins.2022.795876
   Pehle Christian-Gernot, 2021, Zenodo
   Ranzini SM, 2021, J LIGHTWAVE TECHNOL, V39, P2460, DOI 10.1109/JLT.2021.3049473
   Shastri BJ, 2021, NAT PHOTONICS, V15, P102, DOI 10.1038/s41566-020-00754-y
   Sozos K, 2021, 2021 EUROPEAN CONFERENCE ON OPTICAL COMMUNICATION (ECOC), DOI 10.1109/ECOC52684.2021.9606123
NR 15
TC 0
Z9 0
U1 0
U2 0
PY 2023
WC Engineering, Electrical & Electronic; Optics; Telecommunications
DA 2023-11-11
ER

PT J
AU de Abreu, RS
   Silva, I
   Nunes, YT
   Moioli, RC
   Guedes, LA
AF de Abreu, Rute Souza
   Silva, Ivanovitch
   Nunes, Yuri Thomas
   Moioli, Renan C.
   Guedes, Luiz Affonso
TI Advancing Fault Prediction: A Comparative Study between LSTM and Spiking
   Neural Networks
SO PROCESSES
DT Article
DE spiking neural networks (SNNs); system fault prediction; generalized
   stochastic Petri net (GSPN); industrial processes; LSTM networks
ID INTERNET
AB Predicting system faults is critical to improving productivity, reducing costs, and enforcing safety in industrial processes. Yet, traditional methodologies frequently falter due to the intricate nature of the task. This research presents a novel use of spiking neural networks (SNNs) in anticipating faults in syntactical time series, utilizing the generalized stochastic Petri net (GSPN) model. The inherent ability of SNNs to process both time and space aspects of data positions them as a prime instrument for this endeavor. A comparative evaluation with long short-term memory (LSTM) networks suggests that SNNs offer comparable robustness and performance.
C1 [de Abreu, Rute Souza; Silva, Ivanovitch; Nunes, Yuri Thomas; Guedes, Luiz Affonso] Univ Fed Rio Grande do Norte, Technol Ctr, Postgrad Program Elect & Comp Engn, BR-59078970 Natal, Brazil.
   [Moioli, Renan C.] Univ Fed Rio Grande do Norte, Digital Metropolis Inst, Bioinformat Multidisciplinary Environm, BR-59078970 Natal, Brazil.
RP Silva, I (corresponding author), Univ Fed Rio Grande do Norte, Technol Ctr, Postgrad Program Elect & Comp Engn, BR-59078970 Natal, Brazil.
EM ivanovitch.silva@ufrn.br
CR Arsalan M, 2023, APPL INTELL, V53, P15147, DOI 10.1007/s10489-022-04258-w
   Avizienis A, 2004, IEEE T DEPEND SECURE, V1, P11, DOI 10.1109/TDSC.2004.2
   Bause F., 2013, STOCHASTIC PETRI NET
   Bekolay T, 2014, FRONT NEUROINFORM, V7, DOI 10.3389/fninf.2013.00048
   Ben Jabeur S, 2021, TECHNOL FORECAST SOC, V166, DOI 10.1016/j.techfore.2021.120658
   Biewald L., 2022, EXPT TRACKING WEIGHT
   Bolourani S, 2021, J MED INTERNET RES, V23, DOI 10.2196/24246
   Burelo K, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-85827-w
   Debat G, 2021, FRONT COMPUT NEUROSC, V15, DOI 10.3389/fncom.2021.658764
   Dennler N, 2021, 2021 IEEE 3RD INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE CIRCUITS AND SYSTEMS (AICAS), DOI 10.1109/AICAS51828.2021.9458403
   Eliasmith C., 2003, NEURAL ENG COMPUTATI
   de Barros IE, 2021, COMPUT IND, V125, DOI 10.1016/j.compind.2020.103381
   Gehrig M, 2020, Arxiv, DOI arXiv:2003.02790
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   Hunsberger E, 2016, Arxiv, DOI [arXiv:1611.05141, 10.13140/RG.2.2.10967.06566]
   Kandel E., 1995, ESSENTIALS NEURAL SC, VVolume 6, P425
   Kim Y, 2021, NEURAL NETWORKS, V144, P686, DOI 10.1016/j.neunet.2021.09.022
   Kingma D. P., 2014, ARXIV, DOI DOI 10.48550/ARXIV.1412.6980
   Li JY, 2020, FRONT BIOENG BIOTECH, V8, DOI 10.3389/fbioe.2020.00766
   Lin X., 2020, NEURAL INFORM PROCES, P660
   Lobo JL, 2018, NEURAL NETWORKS, V108, P1, DOI 10.1016/j.neunet.2018.07.014
   Luo YL, 2020, IEEE ACCESS, V8, P46007, DOI 10.1109/ACCESS.2020.2978163
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Macedo D, 2014, IEEE INT C NETW SENS, P417, DOI 10.1109/ICNSC.2014.6819662
   Maciag PS, 2021, NEURAL NETWORKS, V139, P118, DOI 10.1016/j.neunet.2021.02.017
   Maciag PS, 2019, ENVIRON MODELL SOFTW, V118, P262, DOI 10.1016/j.envsoft.2019.04.012
   Nyquist H., 1928, T AIEE, V47, P617, DOI [DOI 10.1109/T-AIEE.1928.5055024, 10.1109/t-aiee.1928.5055024]
   Petri C.A., 1962, THESIS U BONN BONN
   Rasmussen D, 2019, NEUROINFORMATICS, V17, P611, DOI 10.1007/s12021-019-09424-z
   Safa A, 2023, IEEE T NEUR NET LEAR, V34, P2869, DOI 10.1109/TNNLS.2021.3109958
   Sahner R., 1997, IEEE T RELIAB, V46, P441, DOI [10.1109/TR.1997.664017, DOI 10.1109/TR.1997.664017]
   Sanders W.H, 2005, MOBIUS USER MANUAL
   Shrivastava S, 2020, COGENT ECON FINANC, V8, DOI 10.1080/23322039.2020.1729569
   Sopeña JMG, 2022, ENERGIES, V15, DOI 10.3390/en15197256
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Togaçar M, 2021, CHAOS SOLITON FRACT, V144, DOI 10.1016/j.chaos.2021.110714
   Vanarse A, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19081841
   Viola J, 2021, INFORM SCIENCES, V542, P195, DOI 10.1016/j.ins.2020.06.060
   Virgilio CD, 2020, NEURAL NETWORKS, V122, P130, DOI 10.1016/j.neunet.2019.09.037
   Wu JB, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00199
   Zuo L, 2021, J MANUF SYST, V61, P714, DOI 10.1016/j.jmsy.2020.07.003
NR 42
TC 0
Z9 0
U1 2
U2 2
PD SEP
PY 2023
VL 11
IS 9
AR 2772
DI 10.3390/pr11092772
WC Engineering, Chemical
DA 2023-11-11
ER

PT C
AU Gamez, D
AF Gamez, David
BE MarquesDeSa, J
   Alexandre, LA
   Duch, W
   Mandic, DP
TI SpikeStream: A fast and flexible simulator of spiking neural networks
SO ARTIFICIAL NEURAL NETWORKS - ICANN 2007, PT 1, PROCEEDINGS
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 17th International Conference on Artificial Neural Networks (ICANN 2007)
CY SEP 09-13, 2007
CL Oporto, PORTUGAL
DE spiking neural networks; SpikeStream; event-based simulation;
   synchronous simulation; biologically inspired robotics
AB SpikeStream is a new simulator of biologically structured spiking neural networks that can be used to edit, display and simulate up to 100,000 neurons. This simulator uses a combination of event-based and synchronous simulation and stores most of its information in databases, which makes it easy to run simulations across an arbitrary number of machines. A comprehensive graphical interface is included and SpikeStream can send and receive spikes to and from real and virtual robots across a network, The architecture is highly modular, and so other researchers can use its graphical editing facilities to set up their own simulation networks or apply genetic algorithms to the SpikeStream databases. SpikeStream is available for free download under the terms of the GPL.
C1 Univ Essex, Dept Comp Sci, Colchester CO4 3SQ, Essex, England.
RP Gamez, D (corresponding author), Univ Essex, Dept Comp Sci, Colchester CO4 3SQ, Essex, England.
EM daogam@essex.ac.uk
CR BRADER JM, 2006, IN PRESS NEURAL COMP
   BRETTE R, IN PRESS J COMP NEUR
   Dehaene S, 2005, PLOS BIOL, V3, P910, DOI 10.1371/journal.pbio.0030141
   Delorme A, 2003, NETWORK-COMP NEURAL, V14, P613, DOI 10.1088/0954-898X/14/4/301
   DIESMANN M, 2001, FORSCHUNG WISSENSCHA
   Gamez D, 2006, P IEEE 5 CHAPT C ADV, P85
   Gerstner W., 2002, SPIKING NEURON MODEL
   Krichmar JL, 2005, NEUROINFORMATICS, V3, P197, DOI 10.1385/NI:3:3:197
   MARIAN I, 2003, THESIS U COLL DUBLIN
   Shanahan M, 2006, CONSCIOUS COGN, V15, P433, DOI 10.1016/j.concog.2005.11.005
NR 10
TC 7
Z9 7
U1 0
U2 1
PY 2007
VL 4668
BP 360
EP 369
PN I
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods
DA 2023-11-11
ER

PT C
AU Vianello, E
   Werner, T
   Grossi, A
   Nowak, E
   De Salvo, B
   Perniola, L
   Bichler, O
   Yvert, B
AF Vianello, Elisa
   Werner, Thilo
   Grossi, Alessandro
   Nowak, Etienne
   De Salvo, Barbara
   Perniola, Luca
   Bichler, Olivier
   Yvert, Blaise
GP ACM
TI Bio-inspired Programming of Resistive Memory Devices for Implementing
   Spiking Neural Networks
SO PROCEEDINGS OF THE GREAT LAKES SYMPOSIUM ON VLSI 2017 (GLSVLSI' 17)
DT Proceedings Paper
CT Great Lakes Symposium on VLSI (GLSVLSI)
CY MAY 10-12, 2017
CL Banff, CANADA
DE RRAM; Artificial synapses; Unsupervised learning; Long-term plasticity;
   Short-term plasticity; Spiking Neural Networks
AB In this work, we will focus on the role that non-volatile resistive memory technologies (RRAM) can play for modeling key features of biological synapses. We will present an architecture and a reading/programming strategy to emulate both Short and Long Term Plasticity (STP, LTP) rules using non-volatile OxRAM arrays. A visual-pattern extraction application is discussed using spiking neural networks. We demonstrated that Long-Term plasticity allows the neural networks to learn patterns and the Short Term plasticity allows to improve accuracy (reduction of the false positive events generated by white noise in the input data) in presence of significant background noise in the input data.
C1 [Vianello, Elisa; Werner, Thilo; Grossi, Alessandro; Nowak, Etienne; De Salvo, Barbara; Perniola, Luca] CEA, LETI, Minatec Campus, Grenoble, France.
   [Bichler, Olivier] CEA, LIST, Saclay, France.
   [Yvert, Blaise] Clinatec, INSERM, UA01, Grenoble, France.
RP Vianello, E (corresponding author), CEA, LETI, Minatec Campus, Grenoble, France.
EM elisa.vianello@cea.fr
CR Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Bichler O., 2011, PROC OF IJCNN, P859
   Bichler O, 2012, NEURAL NETWORKS, V32, P339, DOI 10.1016/j.neunet.2012.02.022
   Bill J, 2014, FRONT NEUROSCI-SWITZ, V8, DOI 10.3389/fnins.2014.00412
   Burr G.W., 2014, IEDM
   Garbin D, 2015, IEEE INT SYMP NANO, P193, DOI 10.1109/NANOARCH.2015.7180611
   Garbin D, 2015, IEEE T ELECTRON DEV, V62, P2494, DOI 10.1109/TED.2015.2440102
   Grossi A, 2016, INT EL DEVICES MEET
   Lichtsteiner P, 2008, IEEE J SOLID-ST CIRC, V43, P566, DOI 10.1109/JSSC.2007.914337
   Milo V, 2016, INT EL DEVICES MEET
   Padilla A, 2015, IEEE T ELECTRON DEV, V62, P963, DOI 10.1109/TED.2015.2389832
   Querlioz D, 2015, P IEEE, V103, P1398, DOI 10.1109/JPROC.2015.2437616
   Roclin D., 2014, NANOARCH
   Sills S, 2014, S VLSI TECH
   Suri M., 2011, IEDM, V4
   Suri M, 2013, IEEE T ELECTRON DEV, V60, P2402, DOI 10.1109/TED.2013.2263000
   Tsodyks M. V., 1997, P NATL ACAD SCI, V94
   Vianello E., 2014, IEDM
   Wei Z., 2015, IEDM
   Werner T, 2016, INT EL DEVICES MEET
NR 20
TC 2
Z9 2
U1 0
U2 2
PY 2017
BP 393
EP 398
DI 10.1145/3060403.3066871
WC Computer Science, Hardware & Architecture; Computer Science, Information
   Systems; Computer Science, Theory & Methods; Engineering, Electrical &
   Electronic
DA 2023-11-11
ER

PT J
AU Wardak, A
   Gong, PL
AF Wardak, Asem
   Gong, Pulin
TI Fractional diffusion theory of balanced heterogeneous neural networks
SO PHYSICAL REVIEW RESEARCH
DT Article
ID FOKKER-PLANCK EQUATIONS; ANOMALOUS DIFFUSION; LEVY FLIGHTS; DYNAMICS;
   NEURONS; CONNECTIVITY; VARIABILITY; CORTEX; STATES; BRAIN
AB Interactions of large numbers of spiking neurons give rise to complex neural dynamics with fluctuations occurring at multiple scales. Understanding the dynamical mechanisms underlying such complex neural dynamics is a long-standing topic of interest in neuroscience, statistical physics and nonlinear dynamics. Conventionally, fluctuating neural dynamics are formulated as balanced, uncorrelated excitatory and inhibitory inputs with Gaussian properties. Yet heterogeneous, non-Gaussian properties have been widely observed in both neural connections and neural dynamics. Based on balanced neural networks with heterogeneous, non-Gaussian features, our analysis reveals that synaptic inputs possess power-law fluctuations in the limit of large network size, leading to a remarkable relation between complex neural dynamics and the fractional diffusion formalisms of nonequilibrium physical systems. We derive a fractional Fokker-Planck equation with analytically tractable boundary conditions for the network, uniquely accounting for the leapovers caused by the fluctuations of spiking activity. This body of formalisms represents a fractional diffusion theory of heterogeneous neural networks and results in an exact description of the network activity states. In particular, the fractional diffusion theory identifies a dynamical state at which the neural response is maximized as a function of structural connectivity. This state is then implemented in a balanced spiking neural network and displays rich, nonlinear response properties, providing a unified account of a variety of experimental findings on neural dynamics at the individual neuron and network levels, including fluctuations of membrane potentials and population firing rates. Our theory and its network implementations provide a framework for investigating complex neural dynamics emerging from large networks of spiking neurons and their functional roles in neural processing.
C1 [Wardak, Asem; Gong, Pulin] Univ Sydney, Sch Phys, Sydney, NSW 2006, Australia.
RP Gong, PL (corresponding author), Univ Sydney, Sch Phys, Sydney, NSW 2006, Australia.
EM pulin.gong@sydney.edu.au
CR AMIT DJ, 1991, NETWORK-COMP NEURAL, V2, P259, DOI 10.1088/0954-898X/2/3/003
   Amit DJ, 1997, CEREB CORTEX, V7, P237, DOI 10.1093/cercor/7.3.237
   Amit DJ, 1997, NETWORK-COMP NEURAL, V8, P373, DOI 10.1088/0954-898X/8/4/003
   [Anonymous], 1973, ADV APPL PROBAB
   Barbieri F, 2014, J NEUROSCI, V34, P14589, DOI 10.1523/JNEUROSCI.5365-13.2014
   Bertoin J., 2012, LEVY PROCESSES
   BINGHAM NH, 1973, Z WAHRSCHEINLICHKEIT, V26, P273, DOI 10.1007/BF00534892
   Brunel N, 1999, NEURAL COMPUT, V11, P1621, DOI 10.1162/089976699300016179
   Brunel N, 2000, J COMPUT NEUROSCI, V8, P183, DOI 10.1023/A:1008925309027
   Buzsáki G, 2014, NAT REV NEUROSCI, V15, P264, DOI 10.1038/nrn3687
   Chechkin AV, 2003, J PHYS A-MATH GEN, V36, pL537, DOI 10.1088/0305-4470/36/41/L01
   Churchland MM, 2010, NAT NEUROSCI, V13, P369, DOI 10.1038/nn.2501
   Cossell L, 2015, NATURE, V518, P399, DOI 10.1038/nature14182
   Das A, 2019, PHYS REV X, V9, DOI 10.1103/PhysRevX.9.021062
   Deco G, 2012, PLOS COMPUT BIOL, V8, DOI 10.1371/journal.pcbi.1002395
   di Santo S, 2018, P NATL ACAD SCI USA, V115, pE1356, DOI 10.1073/pnas.1712989115
   di Volo M., ARXIV200505596QBIONC
   Dorogovtsev SN, 2008, REV MOD PHYS, V80, P1275, DOI 10.1103/RevModPhys.80.1275
   Evans MR, 2020, J PHYS A-MATH THEOR, V53, DOI 10.1088/1751-8121/ab7cfe
   Gewaltig M-O., 2007, SCHOLARPEDIA, V2, DOI [10.4249/scholarpedia.1430, DOI 10.4249/SCHOLARPEDIA.1430]
   Ghashghaie S, 1996, NATURE, V381, P767, DOI 10.1038/381767a0
   Goris RLT, 2014, NAT NEUROSCI, V17, P858, DOI 10.1038/nn.3711
   Gu YF, 2019, PLOS COMPUT BIOL, V15, DOI 10.1371/journal.pcbi.1006902
   He BYJ, 2014, TRENDS COGN SCI, V18, P480, DOI 10.1016/j.tics.2014.04.003
   Hromádka T, 2013, J NEUROPHYSIOL, V109, P1989, DOI 10.1152/jn.00600.2012
   Jercog D, 2017, ELIFE, V6, DOI 10.7554/eLife.22425
   Jespersen S, 1999, PHYS REV E, V59, P2736, DOI 10.1103/PhysRevE.59.2736
   Kadmon J, 2015, PHYS REV X, V5, DOI 10.1103/PhysRevX.5.041030
   Keane A, 2015, J NEUROSCI, V35, P1591, DOI 10.1523/JNEUROSCI.1669-14.2015
   Klafter I. M. S. J., 2011, 1 STEPRANDOM WALKS
   Klafter J, 2005, PHYS WORLD, V18, P29
   Koren T, 2007, PHYS REV LETT, V99, DOI 10.1103/PhysRevLett.99.160602
   Kusmierz L, 2020, PHYS REV LETT, V125, DOI 10.1103/PhysRevLett.125.028101
   La Nave G, 2019, REV MOD PHYS, V91, DOI 10.1103/RevModPhys.91.021003
   Landau ID, 2016, NEURON, V92, P1106, DOI 10.1016/j.neuron.2016.10.027
   Litwin-Kumar A, 2012, NAT NEUROSCI, V15, P1498, DOI 10.1038/nn.3220
   MANTEGNA RN, 1995, NATURE, V376, P46, DOI 10.1038/376046a0
   Mazzoni A, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.1000239
   Mazzucato L, 2015, J NEUROSCI, V35, P8214, DOI 10.1523/JNEUROSCI.4819-14.2015
   Metzler R, 1999, EUROPHYS LETT, V46, P431, DOI 10.1209/epl/i1999-00279-7
   Metzler R, 2002, CHEM PHYS, V284, P67, DOI 10.1016/S0301-0104(02)00537-2
   Metzler R, 2000, PHYS REP, V339, P1, DOI 10.1016/S0370-1573(00)00070-3
   Miller KJ, 2009, PLOS COMPUT BIOL, V5, DOI 10.1371/journal.pcbi.1000609
   Munn B, 2020, J PHYSIOL-LONDON, V598, P1551, DOI 10.1113/JP278935
   Muñoz MA, 2018, REV MOD PHYS, V90, DOI 10.1103/RevModPhys.90.031001
   Petersen PC, 2016, ELIFE, V5, DOI 10.7554/eLife.18805
   Renart A, 2010, SCIENCE, V327, P587, DOI 10.1126/science.1179850
   Richardson MJE, 2007, PHYS REV E, V76, DOI 10.1103/PhysRevE.76.021919
   Richardson MJE, 2005, NEURAL COMPUT, V17, P923, DOI 10.1162/0899766053429444
   Rosenbaum R, 2017, NAT NEUROSCI, V20, P107, DOI 10.1038/nn.4433
   Rosenbaum R, 2014, PHYS REV X, V4, DOI 10.1103/PhysRevX.4.021039
   Roxin A, 2011, J NEUROSCI, V31, P16217, DOI 10.1523/JNEUROSCI.1677-11.2011
   Scheffer LK, 2020, ELIFE, V9, DOI 10.7554/eLife.57443
   Schertzer D, 2001, J MATH PHYS, V42, P200, DOI 10.1063/1.1318734
   Schmeltzer C, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0121794
   Shadlen MN, 1998, J NEUROSCI, V18, P3870
   Shih CT, 2020, NEUROINFORMATICS, V18, P267, DOI 10.1007/s12021-019-09443-w
   SHLESINGER MF, 1987, PHYS REV LETT, V58, P1100, DOI 10.1103/PhysRevLett.58.1100
   SHLESINGER MF, 1993, NATURE, V363, P31, DOI 10.1038/363031a0
   SOFTKY WR, 1993, J NEUROSCI, V13, P334
   Song S, 2005, PLOS BIOL, V3, P507, DOI 10.1371/journal.pbio.0030068
   STANLEY HE, 1988, NATURE, V335, P405, DOI 10.1038/335405a0
   Tan AYY, 2014, NATURE, V509, P226, DOI 10.1038/nature13159
   Teich MC, 1997, J OPT SOC AM A, V14, P529, DOI 10.1364/JOSAA.14.000529
   Uchaikin V., 1999, CHANCE STABILITY STA, DOI DOI 10.1515/9783110935974
   vanVreeswijk C, 1996, SCIENCE, V274, P1724, DOI 10.1126/science.274.5293.1724
   Vegué M, 2019, PHYS REV E, V100, DOI 10.1103/PhysRevE.100.022208
   W. R. Inc, 2019, MATH VERS 12 0
   Wardak A, 2020, J PHYS A-MATH THEOR, V53, DOI 10.1088/1751-8121/ab8b37
   Wieland S, 2015, PHYS REV E, V92, DOI 10.1103/PhysRevE.92.040901
   Zaburdaev V, 2015, REV MOD PHYS, V87, P483, DOI 10.1103/RevModPhys.87.483
NR 71
TC 8
Z9 8
U1 4
U2 9
PD JAN 27
PY 2021
VL 3
IS 1
AR 013083
DI 10.1103/PhysRevResearch.3.013083
WC Physics, Multidisciplinary
DA 2023-11-11
ER

PT C
AU Xia, Y
   Uenohara, S
   Aihara, K
   Levi, T
AF Xia, Yang
   Uenohara, Seiji
   Aihara, Kazuyuki
   Levi, Timothee
BE Sugisaka, M
   Jia, Y
   Ito, T
   Lee, JJ
TI Real-time implementation of ReSuMe learning in Spiking Neural Network
SO ICAROB 2019: PROCEEDINGS OF THE 2019 INTERNATIONAL CONFERENCE ON
   ARTIFICIAL LIFE AND ROBOTICS
DT Proceedings Paper
CT International Conference on Artificial Life and Robotics (ICAROB)
CY JAN 10-13, 2019
CL JAPAN
DE Spiking neural network; ReSuMe; LIF; FPGA
AB Neuromorphic systems are designed by mimicking or being inspired by the nervous system, which realizes robust, autonomous, and power-efficient information processing by highly parallel architecture. Supervised learning was proposed as a successful concept of information processing in neural network. Recently, there has been an increasing body of evidence that instruction-based learning is also exploited by the brain. ReSuMe is a proposed algorithm by Ponulak and Kasinski in 2010. It proposes a supervised learning for biologically plausible neurons that reproduce template signals (instructions) or patterns encoded in precisely timed sequences of spikes. Here, we present a real-time ReSuMe learning implementation on FPGA using Leaky Integrate-and-fire (LIF) Spiking Neural Network (SNN). FPGA allows real-time implementation and embedded system. We show that this implementation can make successful the learning on a specific pattern.
C1 [Xia, Yang] Univ Tokyo, Grad Sch Engn, Tokyo, Japan.
   [Xia, Yang; Uenohara, Seiji; Aihara, Kazuyuki; Levi, Timothee] 4-7-1 Komaba,Meguro Ku, Tokyo 1538505, Japan.
   [Uenohara, Seiji; Aihara, Kazuyuki; Levi, Timothee] Univ Tokyo, IIS, Tokyo, Japan.
RP Xia, Y (corresponding author), Univ Tokyo, Grad Sch Engn, Tokyo, Japan.; Xia, Y (corresponding author), 4-7-1 Komaba,Meguro Ku, Tokyo 1538505, Japan.
EM xiayang@sat.t.u-tokyo.ac.jp; uenohara@sat.t.u-tokyo.ac.jp;
   aihara@sat.t.u-tokyo.ac.jp; levi@sat.t.u-tokyo.ac.jp
CR Ambroise M, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00215
   Gardner B, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0161335
   Gerstner W., 2002, SPIKING NEURON MODEL
   Gewaltig M-O., 2007, SCHOLARPEDIA, V2, DOI [10.4249/scholarpedia.1430, DOI 10.4249/SCHOLARPEDIA.1430]
   Ghosh-Dastidar S., 2009, J NEURAL NETWORKS
   Goodman DFM, 2009, FRONT NEUROSCI-SWITZ, V3, P192, DOI 10.3389/neuro.01.026.2009
   Grassia F, 2018, ARTIF LIFE ROBOT, V23, P200, DOI 10.1007/s10015-017-0421-y
   Grassia F, 2016, J Physiol Paris, V110, P409, DOI 10.1016/j.jphysparis.2017.02.002
   Grassia F, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00134
   Hines ML, 2001, NEUROSCIENTIST, V7, P123, DOI 10.1177/107385840100700207
   Kasinski A, 2005, LECT NOTES COMPUT SC, V3696, P145, DOI 10.1007/11550822_24
   Kechiche L, 2018, INT J RECONFIGURABLE, V2018, DOI 10.1155/2018/2843582
   Kraft M., 2011, P 3 INT IFAC WORKSH
   Maass W, 2004, MATH COMP BIOL SER, P575
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Natarajan A, 2018, IEEE T BIOMED CIRC S, V12, P918, DOI 10.1109/TBCAS.2018.2837055
   Qiao N, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00141
   Schemmel J., 2017, P INT JOINT C NEUR N
   Wang R., 2013, FRONTIERS NEUROSCIEN, V7, P1, DOI DOI 10.3389/FNINS.201
NR 19
TC 0
Z9 0
U1 0
U2 3
PY 2019
BP 82
EP 86
WC Computer Science, Artificial Intelligence; Engineering, Mechanical;
   Robotics
DA 2023-11-11
ER

PT C
AU Wang, Q
   Li, P
AF Wang, Qian
   Li, Peng
GP IEEE
TI D-LSM: Deep Liquid State Machine with Unsupervised Recurrent Reservoir
   Tuning
SO 2016 23RD INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION (ICPR)
SE International Conference on Pattern Recognition
DT Proceedings Paper
CT 23rd International Conference on Pattern Recognition (ICPR)
CY DEC 04-08, 2016
CL Mexican Assoc Comp Vis Robot & Neural Comp, Cancun, MEXICO
HO Mexican Assoc Comp Vis Robot & Neural Comp
ID SPEECH RECOGNITION
AB The Liquid State Machine (LSM) is a biologically plausible model of computation for recurrent spiking neural networks, which offers promising solutions to real-world applications in both software and hardware based systems. At the same time, deep feedforward rate-based neural networks such as convolutional neural networks (CNNs) have achieved great success in many computer vision related applications. However, a systematic exploration of deep recurrent spiking neural networks is lacking. We propose a new model of Deep Liquid State Machine (D-LSM), which simultaneously explores the powers of recurrent spiking networks and deep architectures. D-LSM consists of multiple basic LSM processing and pooling stages. Recurrent reservoir networks across different LSM stages act as nonlinear filters capable of extracting spatio-temporal features of increasingly higher levels from the input. We propose to train the D-LSM practically by adopting unsupervised training (e.g. through STDP) for recurrent reservoirs and spike-based supervised rules for the final readout stage. The perspective of realizing D-LSM based hardware processors is also presented.
C1 [Wang, Qian; Li, Peng] Texas A&M Univ, Dept Elect & Comp Engn, College Stn, TX 77840 USA.
RP Wang, Q (corresponding author), Texas A&M Univ, Dept Elect & Comp Engn, College Stn, TX 77840 USA.
CR [Anonymous], 2016, NAT METHODS, DOI DOI 10.1038/nmeth.3707
   [Anonymous], DEEP BIG SIMPLE NEUR
   [Anonymous], 2002, ADV NEURAL INFORM PR
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Bi GQ, 2001, ANNU REV NEUROSCI, V24, P139, DOI 10.1146/annurev.neuro.24.1.139
   Ciresan Dan, 2012, COMP VIS PATT REC CV
   Diehl Peter U., 2015, NEUR NETW IJCNN 2015
   Gaines B. R., 1969, ADV INFORM SYSTEMS S, V2, P37
   Ghani A, 2008, LECT NOTES COMPUT SC, V5163, P513, DOI 10.1007/978-3-540-87536-9_53
   HUBEL DH, 1968, J PHYSIOL-LONDON, V195, P215, DOI 10.1113/jphysiol.1968.sp008455
   Krizhevsky A, 2012, ADV NEURAL INFORM PR
   LeCun Y., P IEEE, V86
   Maass W., 2011, COMPUTABILITY CONTEX
   Maass Wolfgang, 2002, NEURAL COMPUTATI NOV
   Morrison A, 2008, BIOL CYBERN, V98, P459, DOI 10.1007/s00422-008-0233-1
   Nikolic D, 2009, PLOS BIOL, V7, DOI 10.1371/journal.pbio.1000260
   Schrauwen B, 2008, NEURAL NETWORKS, V21, P511, DOI 10.1016/j.neunet.2007.12.009
   Schurmann F., ADV NEURAL INFORM PR, V17
   Verstraeten D, 2005, INFORM PROCESS LETT, V95, P521, DOI 10.1016/j.ipl.2005.05.019
   Wang Q, 2015, J NANOMATER, V2015, DOI 10.1155/2015/408634
   Zhang Y, 2015, NEURAL NETWORKS LEAR, V26
NR 21
TC 13
Z9 13
U1 0
U2 3
PY 2016
BP 2652
EP 2657
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT C
AU Allen, JN
   Abdel-Aty-Zohdy, HS
   Ewing, RL
AF Allen, Jacob N.
   Abdel-Aty-Zohdy, Hoda S.
   Ewing, Robert L.
BE Acevedo, GOD
   Garcia, RP
   Cedeno, MJ
TI A compact correlation filter for on-chip learning in a spiking neural
   network
SO IEEE MWSCAS'06: PROCEEDINGS OF THE 2006 49TH MIDWEST SYMPOSIUM ON
   CIRCUITS AND SYSTEMS,
SE Midwest Symposium on Circuits and Systems Conference Proceedings
DT Proceedings Paper
CT 49th IEEE International Midwest Symposium on Circuits and Systems
CY AUG 06-09, 2006
CL San Juan, PR
AB A Hebbian learning algorithm based on proportion sampling is presented that can be used to implement on-chip learning for a binary spiking neural network. A correlation filter estimates when statistical independence has been obtained between subsequent samples. Simulation shows that the correlation filter reduces falsely learned connections in environments were inputs are randomly activated an average of 83% of the total time. A correlation filter for 255 binary samples is implemented using 21 gates and a surface area of .0008cm(2) for a .5 mu fabrication process. Compared to traditional neural networks, the spiking neural network learned an odor in a single epoch resulting in only a 7% error, while classical learning algorithms required multiple epochs and typically resulted in 30% error.
C1 [Allen, Jacob N.; Abdel-Aty-Zohdy, Hoda S.] Oakland Univ, Microelect Syst Design Lab, Dept Elect & Comp Engn, Rochester, MI 48309 USA.
   [Ewing, Robert L.] Wright Patterson Air Force Base, Air Force Res Labs, Informat Directorate, Ohira 45433, Japan.
RP Allen, JN (corresponding author), Oakland Univ, Microelect Syst Design Lab, Dept Elect & Comp Engn, Rochester, MI 48309 USA.
CR ALLEN J, 2005, THESIS OAKLAND U ROC
   ALLEN J, 2004, P 2005 IEEE S COMP I
   BUCK L, 1996, CELL, P611
   *MENT GRAPH, 2006, DES ASICS ADK DES KI
NR 4
TC 2
Z9 2
U1 0
U2 0
PY 2006
BP 733
EP +
WC Computer Science, Theory & Methods; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Johnson, MG
   Chartier, S
AF Johnson, Melissa G.
   Chartier, Sylvain
TI Spike Neural Models Part II: Abstract Neural Models
SO QUANTITATIVE METHODS FOR PSYCHOLOGY
DT Article
DE Neural models; spiking neural networks; leaky integrate and fire;
   Izhikevich's model
ID EXPLANATION; FREQUENCY; HODGKIN
AB Neurons are complex cells that require a lot of time and resources to model completely. In spiking neural networks (SNN) though, not all that complexity is required. Therefore simple, abstract models are often used. These models save time, use less computer resources, and are easier to understand. This tutorial presents two such models: Izhikevich's model, which is biologically realistic in the resulting spike trains but not in the parameters, and the Leaky Integrate and Fire (LIF) model which is not biologically realistic but does quickly and easily integrate input to produce spikes. Izhikevich's model is based on Hodgkin-Huxley's model but simplified such that it uses only two differentiation equations and four parameters to produce various realistic spike patterns. LIF is based on a standard electrical circuit and contains one equation. Either of these two models, or any of the many other models in literature can be used in a SNN. Choosing a neural model is an important task that depends on the goal of the research and the resources available. Once a model is chosen, network decisions such as connectivity, delay, and sparseness, need to be made. Understanding neural models and how they are incorporated into the network is the first step in creating a SNN.
C1 [Johnson, Melissa G.; Chartier, Sylvain] Univ Ottawa, Ottawa, ON, Canada.
RP Johnson, MG (corresponding author), Univ Ottawa, Sch Psychol, 75 Laurier Ave E, Ottawa, ON K1N 6N5, Canada.
EM mjohn140@uottawa.ca
CR Ainsworth M, 2012, NEURON, V75, P572, DOI 10.1016/j.neuron.2012.08.004
   Borst A, 1999, NAT NEUROSCI, V2, P947, DOI 10.1038/14731
   Brette R, 2015, FRONT SYST NEUROSCI, V9, DOI 10.3389/fnsys.2015.00151
   Chirimuuta M, 2014, SYNTHESE, V191, P127, DOI 10.1007/s11229-013-0369-y
   Fields RD, 2014, NEUROSCIENTIST, V20, P426, DOI 10.1177/1073858413504465
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   Gerstner W, 2009, SCIENCE, V326, P379, DOI 10.1126/science.1181936
   Gray CM, 1996, SCIENCE, V274, P109, DOI 10.1126/science.274.5284.109
   GUCKENHEIMER J, 1993, B MATH BIOL, V55, P937, DOI 10.1007/BF02460693
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Johnson MG, 2017, QUANT METH PSYCHOL, V13, P105, DOI 10.20982/tqmp.13.2.p105
   Kaplan DM, 2011, SYNTHESE, V183, P339, DOI 10.1007/s11229-011-9970-0
   Kuznetsov Y A, 2013, ELEMENTS APPL BIFURC
   Levy A, 2013, PHILOS SCI, V80, P241, DOI 10.1086/670300
   Liu YH, 2001, J COMPUT NEUROSCI, V10, P25, DOI 10.1023/A:1008916026143
   Maass W, 1996, NEURAL COMPUT, V8, P1, DOI 10.1162/neco.1996.8.1.1
   Markram Henry, 2011, Front Synaptic Neurosci, V3, P4, DOI 10.3389/fnsyn.2011.00004
   Mihalas S, 2009, NEURAL COMPUT, V21, P704, DOI 10.1162/neco.2008.12-07-680
   St-Hilaire M, 2004, J COMPUT NEUROSCI, V16, P299, DOI 10.1023/B:JCNS.0000025690.02886.93
   Vogels TP, 2005, ANNU REV NEUROSCI, V28, P357, DOI 10.1146/annurev.neuro.28.061604.135637
NR 21
TC 1
Z9 2
U1 0
U2 1
PY 2018
VL 14
IS 1
BP 1
EP 16
DI 10.20982/tqmp.14.1.p001
WC Social Sciences, Interdisciplinary
DA 2023-11-11
ER

PT C
AU Sen Bhattacharya, B
   Serrano-Gotarredona, T
AF Sen Bhattacharya, Basabdatta
   Serrano-Gotarredona, Teresa
GP IEEE
TI On- and Off-centre Pathways in a Retino-Geniculate Spiking Neural
   Network on SpiNNaker
SO 2021 10TH INTERNATIONAL IEEE/EMBS CONFERENCE ON NEURAL ENGINEERING (NER)
SE International IEEE EMBS Conference on Neural Engineering
DT Proceedings Paper
CT 10th International IEEE-EMBS Conference on Neural Engineering (NER)
CY MAY 04-06, 2021
CL Prague, ELECTR NETWORK
ID INHIBITORY CIRCUITS; THALAMUS
AB We present a neural circuit inspired by the brain visual pathway simulated with neuromorphic hardware. The main recipient of the visual information from retinal ganglion cells is the dorsal part of the Lateral Geniculate Nucleus (LGN). Previously, we have implemented a Spiking Neural Network of the LGN on SpiNNaker receiving inputs from an electronic retina (e-retina) chip comprising a Dynamic Vision Sensor. Here, we incorporate a retinotopic structure in the existing LGN network emulating the on- and off-centre receptive fields of the retinal spiking neurons, which are well simulated by the e-retina output. We have parameterised the model to mimic the 'push' (excitation) and 'pull' (inhibition) dynamics observed in the LGN due to the centre-surround organisation of its receptive fields and synaptic connectivities. The model presented here lay the groundwork for research on building a biologically plausible spiking neural network of visual cognition.
C1 [Sen Bhattacharya, Basabdatta] BITS Pilani, Fac Comp Sci & Informat Syst, Goa Campus, Pilani 403726, Goa, India.
   [Serrano-Gotarredona, Teresa] Inst Microelect, Seville 41092, Spain.
RP Sen Bhattacharya, B (corresponding author), BITS Pilani, Fac Comp Sci & Informat Syst, Goa Campus, Pilani 403726, Goa, India.
EM basabdattab@goa.bits-pilani.ac.in; terese@imse-cnm.csic.es
CR Alonso JM, 2006, PROG BRAIN RES, V154, P3, DOI 10.1016/S0079-6123(06)54001-4
   Alonso J.-M., 2009, SCHOLARPEDIA, V4, P5393, DOI [10.4249/scholarpedia.5393, DOI 10.4249/SCHOLARPEDIA.5393]
   Bhattacharya B., 2016, FRONT NEUROSCI-SWITZ
   Hirsch JA, 2015, ANNU REV NEUROSCI, V38, P309, DOI 10.1146/annurev-neuro-071013-014229
   HUBEL DH, 1961, J PHYSIOL-LONDON, V155, P385, DOI 10.1113/jphysiol.1961.sp006635
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2007, CEREB CORTEX, V17, P2443, DOI 10.1093/cercor/bhl152
   Martinez LM, 2014, NEURON, V81, P943, DOI 10.1016/j.neuron.2013.12.014
   Serrano-Gotarredona T, 2013, IEEE J SOLID-ST CIRC, V48, P827, DOI 10.1109/JSSC.2012.2230553
   Usrey WM, 2015, ANNU REV VIS SCI, V1, P351, DOI 10.1146/annurev-vision-082114-035920
   Van Rullen R, 2001, NEURAL COMPUT, V13, P1255, DOI 10.1162/08997660152002852
   Wang X, 2007, NEURON, V55, P465, DOI 10.1016/j.neuron.2007.06.039
   Wang X, 2011, CURR OPIN NEUROBIOL, V21, P726, DOI 10.1016/j.conb.2011.06.004
   Wang X, 2011, NAT NEUROSCI, V14, P224, DOI 10.1038/nn.2707
NR 14
TC 1
Z9 1
U1 0
U2 1
PY 2021
BP 461
EP 464
DI 10.1109/NER49283.2021.9441462
WC Computer Science, Theory & Methods; Engineering, Biomedical;
   Neurosciences
DA 2023-11-11
ER

PT J
AU Roldan, JB
   Maldonado, D
   Aguilera-Pedregosa, C
   Moreno, E
   Aguirre, F
   Romero-Zaliz, R
   García-Vico, AM
   Shen, YQ
   Lanza, M
AF Roldan, Juan B.
   Maldonado, David
   Aguilera-Pedregosa, Cristina
   Moreno, Enrique
   Aguirre, Fernando
   Romero-Zaliz, Rocio
   Garcia-Vico, Angel M.
   Shen, Yaqing
   Lanza, Mario
TI Spiking neural networks based on two-dimensional materials
SO NPJ 2D MATERIALS AND APPLICATIONS
DT Article
ID MEMRISTOR; MEMORY
AB The development of artificial neural networks using memristors is gaining a lot of interest among technological companies because it can reduce the computing time and energy consumption. There is still no memristor, made of any material, capable to provide the ideal figures-of-merit required for the implementation of artificial neural networks, meaning that more research is required. Here we present the use of multilayer hexagonal boron nitride based memristors to implement spiking neural networks for image classification. Our study indicates that the recognition accuracy of the network is high, and that can be resilient to device variability if the number of neurons employed is large enough. There are very few studies that present the use of a two-dimensional material for the implementation of synapses of different features; in our case, in addition to a study of the synaptic characteristics of our memristive devices, we deal with complete spiking neural network training and inference processes.
C1 [Roldan, Juan B.; Maldonado, David; Aguilera-Pedregosa, Cristina] Univ Granada, Fac Ciencias, Dept Elect & Tecnol Comp, Avd Fuentenueva S-N, Granada 18071, Spain.
   [Moreno, Enrique] Czech Tech Univ, Fac Elect Engn, Dept Electromagnet Field, CTU, Prague 16627 6, Dejvice, Czech Republic.
   [Aguirre, Fernando; Lanza, Mario] King Abdullah Univ Sci & Technol KAUST, Phys Sci & Engn Div, Thuwal 239556900, Saudi Arabia.
   [Romero-Zaliz, Rocio; Garcia-Vico, Angel M.] Univ Granada, Andalusian Res Inst Data Sci & Computat Intellige, Granada 18071, Spain.
   [Shen, Yaqing] Soochow Univ, Collaborat Innovat Ctr Suzhou Nano Sci & Technol, Inst Funct Nano & Soft Mat FUNSOM, Suzhou 215123, Peoples R China.
RP Roldan, JB (corresponding author), Univ Granada, Fac Ciencias, Dept Elect & Tecnol Comp, Avd Fuentenueva S-N, Granada 18071, Spain.; Lanza, M (corresponding author), King Abdullah Univ Sci & Technol KAUST, Phys Sci & Engn Div, Thuwal 239556900, Saudi Arabia.
EM jroldan@ugr.es; mario.lanza@kaust.edu.sa
CR Alibart F, 2013, NAT COMMUN, V4, DOI 10.1038/ncomms3072
   Ambrogio S, 2018, NATURE, V558, P60, DOI 10.1038/s41586-018-0180-5
   Burges, 2010, MNIST HANDWRITTEN DI, DOI DOI 10.1561/2400000035
   Chen SC, 2020, NAT ELECTRON, V3, P638, DOI 10.1038/s41928-020-00473-w
   Dev D, 2020, IEEE ELECTR DEVICE L, V41, P936, DOI 10.1109/LED.2020.2988247
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Hazan H, 2018, FRONT NEUROINFORM, V12, DOI 10.3389/fninf.2018.00089
   Ielmini D, 2016, RESISTIVE SWITCHING: FROM FUNDAMENTALS OF NANOIONIC REDOX PROCESSES TO MEMRISTIVE DEVICE APPLICATIONS, P1
   Kim KM, 2015, ADV ELECTRON MATER, V1, DOI 10.1002/aelm.201500095
   Kim T, 2021, FRONT COMPUT NEUROSC, V15, DOI 10.3389/fncom.2021.646125
   Lanza M, 2022, SCIENCE, V376, P1066, DOI 10.1126/science.abj9979
   Li Y, 2014, SCI REP-UK, V4, DOI 10.1038/srep04906
   Maestro-Izquierdo M, 2019, MICROELECTRON ENG, V215, DOI 10.1016/j.mee.2019.111014
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Pan CB, 2017, ADV FUNCT MATER, V27, DOI 10.1002/adfm.201604811
   Paszke A., 2017, NEURIPS
   Prezioso M, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-07757-y
   Prezioso M, 2016, SCI REP-UK, V6, DOI 10.1038/srep21331
   Prezioso M, 2015, NATURE, V521, P61, DOI 10.1038/nature14441
   Sebastian A, 2020, NAT NANOTECHNOL, V15, P529, DOI 10.1038/s41565-020-0655-z
   Seo S, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-07572-5
   Shen YQ, 2021, ADV MATER, V33, DOI 10.1002/adma.202103656
   Shi YY, 2018, NAT ELECTRON, V1, P458, DOI 10.1038/s41928-018-0118-9
   Tang JS, 2019, ADV MATER, V31, DOI 10.1002/adma.201902761
   Tsur E. E., 2021, NEUROMORPHIC ENG SCI, DOI 10.1201/9781003143499
   Wang CY, 2020, ADV ELECTRON MATER, V6, DOI 10.1002/aelm.201901107
   Wang JY, 2021, ACS NANO, V15, P15123, DOI 10.1021/acsnano.1c05565
   Yu SM, 2021, IEEE CIRC SYST MAG, V21, P31, DOI 10.1109/MCAS.2021.3092533
   Yu SM, 2011, IEEE T ELECTRON DEV, V58, P2729, DOI 10.1109/TED.2011.2147791
   Zhao ZY, 2020, IEEE T CIRCUITS-II, V67, P931, DOI 10.1109/TCSII.2020.2980054
   Zheng N., 2019, LEARNING ENERGY EFFI
   Zheng WW, 2022, ADV MATER, V34, DOI 10.1002/adma.202104138
   Zhu KC, 2021, NAT ELECTRON, V4, P775, DOI 10.1038/s41928-021-00672-z
   Zidan MA, 2018, NAT ELECTRON, V1, P22, DOI 10.1038/s41928-017-0006-8
NR 34
TC 8
Z9 8
U1 9
U2 30
PD SEP 9
PY 2022
VL 6
IS 1
AR 63
DI 10.1038/s41699-022-00341-5
WC Nanoscience & Nanotechnology; Materials Science, Multidisciplinary;
   Physics, Applied
DA 2023-11-11
ER

PT J
AU Chatterjee, D
   Kottantharayil, A
AF Chatterjee, Dibyendu
   Kottantharayil, Anil
TI A CMOS Compatible Bulk FinFET-Based Ultra Low Energy Leaky Integrate and
   Fire Neuron for Spiking Neural Networks
SO IEEE ELECTRON DEVICE LETTERS
DT Article
DE Bulk FinFET; impact ionization; leaky integrate and fire neuron; spiking
   neural network (SNN)
ID MOSFETS
AB The fundamental building block of an artificial spiking neural network (SNN) is an element which can effectively mimic a biological neuron. There are several electronic and spintronic devices which have been demonstrated as a neuron. But the main concern here is the energy consumption and large area of those artificial neurons. In this letter, we propose and demonstrate a highly scalable and CMOS compatible bulk FinFET with an n(+) buried layer for ultra low energy artificial neuron using well calibrated TCAD simulations. The proposed device shows the signature spiking frequency versus input voltage curve of a biological neuron. The energy per spike of the integrate block of the proposed leaky integrate and fire (LIF) neuron is 6.3 fJ/spike which is the minimum reported till date.
C1 [Chatterjee, Dibyendu; Kottantharayil, Anil] Indian Inst Technol, Dept Elect Engn, Mumbai 400076, Maharashtra, India.
RP Chatterjee, D (corresponding author), Indian Inst Technol, Dept Elect Engn, Mumbai 400076, Maharashtra, India.
EM dibyendu@ee.iitb.ac.in
CR Abbott LF, 1999, BRAIN RES BULL, V50, P303, DOI 10.1016/S0361-9230(99)00161-6
   [Anonymous], 2013, SENTAURUS DEVICE USE
   [Anonymous], 1985, VOLUNTARY MOVEMENT
   Arthur JV, 2011, IEEE T CIRCUITS-I, V58, P1034, DOI 10.1109/TCSI.2010.2089556
   CHEN CED, 1988, IEEE ELECTR DEVICE L, V9, P636, DOI 10.1109/55.20420
   Dutta S, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-07418-y
   Emery R, 2009, 2009 3RD ACM/IEEE INTERNATIONAL SYMPOSIUM ON NETWORKS-ON-CHIP, P144, DOI 10.1109/NOCS.2009.5071462
   Han JW, 2008, IEEE ELECTR DEVICE L, V29, P632, DOI 10.1109/LED.2008.922142
   Haykin S., 2001, NEURAL NETWORKS COMP
   Indiveri G, 2006, IEEE T NEURAL NETWOR, V17, P211, DOI 10.1109/TNN.2005.860850
   Jaiswal A, 2017, IEEE T ELECTRON DEV, V64, P1818, DOI 10.1109/TED.2017.2671353
   Joubert A., 2012, P 2012 INT JOINT C N, P1, DOI [10.1109/ijcnn.2012.6252600, DOI 10.1109/IJCNN.2012.6252600, 10.1109/IJCNN.2012.6252600]
   KATO K, 1985, IEEE T ELECTRON DEV, V32, P458, DOI 10.1109/T-ED.1985.21963
   Lashkare S, 2018, IEEE ELECTR DEVICE L, V39, P484, DOI 10.1109/LED.2018.2805822
   Lee YJ, 2004, 2004 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL 4, PROCEEDINGS, P744
   Loubet N, 2017, S VLSI TECH, pT230, DOI 10.23919/VLSIT.2017.7998183
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   MAHOWALD M, 1991, NATURE, V354, P515, DOI 10.1038/354515a0
   MATLOUBIAN M, 1990, IEEE T ELECTRON DEV, V37, P1985, DOI 10.1109/16.57160
   Merolla P., 2011, IEEE CUST INT CIRC C, P1, DOI DOI 10.1109/CICC.2011.6055294
   Rajendran B, 2013, IEEE T ELECTRON DEV, V60, P246, DOI 10.1109/TED.2012.2227969
   Tuma T, 2016, NAT NANOTECHNOL, V11, P693, DOI [10.1038/nnano.2016.70, 10.1038/NNANO.2016.70]
   Wijekoon JHB, 2008, NEURAL NETWORKS, V21, P524, DOI 10.1016/j.neunet.2007.12.037
NR 23
TC 35
Z9 36
U1 0
U2 24
PD AUG
PY 2019
VL 40
IS 8
BP 1301
EP 1304
DI 10.1109/LED.2019.2924259
WC Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Dray, J
   Capecci, E
   Kasabov, N
AF Dray, Jack
   Capecci, Elisa
   Kasabov, Nikola
BE Cheng, L
   Leung, ACS
   Ozawa, S
TI Spiking Neural Networks for Cancer Gene Expression Time Series Modelling
   and Analysis
SO NEURAL INFORMATION PROCESSING (ICONIP 2018), PT I
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 25th International Conference on Neural Information Processing (ICONIP)
CY DEC 13-16, 2018
CL Siem Reap, CAMBODIA
DE Spiking Neural Networks; Gene expression; Bioinformatics
ID FEATURE-SELECTION
AB Gene expression can be used for profiling of cancer cell state and classification of disease. Some cancer variants have been attributed to one or few significant gene expression features. This paper investigates the combination of novel features selection methods - Minimum-Redundancy, Maximum-Relevance - and artificial neural networks - the spiking neural network NeuCube architecture - for genomic data classification and analysis. A NeuCube model performs not only a better classification than other machine learning methods, but most importantly contributes to the feature extraction and marker discovery along with providing gene interaction network analysis for selected genes. Results demonstrated that the methodology proposed could contribute to bioinformatics data analysis for the treatment of disease by discovery of new biomarkers from gene expression data.
C1 [Dray, Jack; Capecci, Elisa; Kasabov, Nikola] Auckland Univ Technol AUT, Knowledge Engn & Discovery Res Inst KEDRI, AUT Tower,Level 7,Cnr Rutland & Wakefield St, Auckland 1010, New Zealand.
RP Dray, J (corresponding author), Auckland Univ Technol AUT, Knowledge Engn & Discovery Res Inst KEDRI, AUT Tower,Level 7,Cnr Rutland & Wakefield St, Auckland 1010, New Zealand.
EM xrj3564@aut.uni.ac.nz; elisa.capecci@gmail.com; nkasabov@aut.ac.nz
CR Berkofsky-Fessler W, 2009, MOL CANCER THER, V8, P2517, DOI 10.1158/1535-7163.MCT-09-0083
   Ding C, 2003, PROCEEDINGS OF THE 2003 IEEE BIOINFORMATICS CONFERENCE, P523, DOI 10.1109/CSB.2003.1227396
   Fürnkranz J, 2002, J MACH LEARN RES, V2, P721, DOI 10.1162/153244302320884605
   Huang SJ, 2018, CANCER GENOM PROTEOM, V15, P41, DOI 10.21873/cgp.20063
   Kasabov N., 2018, SPRINGER SERIES BIOA, DOI 10.1007/978-3-662-57715-8
   Kasabov N, 2016, NEURAL NETWORKS, V78, P1, DOI 10.1016/j.neunet.2015.09.011
   Kasabov N, 2013, NEURAL NETWORKS, V41, P188, DOI 10.1016/j.neunet.2012.11.014
   Kasabov NK, 2014, NEURAL NETWORKS, V52, P62, DOI 10.1016/j.neunet.2014.01.006
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Medler D. A., 1998, Neural Computing Surveys, V1
   Peng HC, 2005, IEEE T PATTERN ANAL, V27, P1226, DOI 10.1109/TPAMI.2005.159
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Tarca AL, 2006, AM J OBSTET GYNECOL, V195, P373, DOI 10.1016/j.ajog.2006.07.001
   The MathWorks Inc, STAT MACHINE LEARNIN
   Tu EM, 2017, IEEE T NEUR NET LEAR, V28, P1305, DOI 10.1109/TNNLS.2016.2536742
   Walter, 2002, MOL BIOL CELL
NR 16
TC 0
Z9 0
U1 1
U2 2
PY 2018
VL 11301
BP 625
EP 634
DI 10.1007/978-3-030-04167-0_57
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods
DA 2023-11-11
ER

PT C
AU Marchisio, A
   Nanfa, G
   Khalid, F
   Hanif, MA
   Martina, M
   Shafique, M
AF Marchisio, Alberto
   Nanfa, Giorgio
   Khalid, Faiq
   Hanif, Muhammad Abdullah
   Martina, Maurizio
   Shafique, Muhammad
GP IEEE
TI Is Spiking Secure? A Comparative Study on the Security Vulnerabilities
   of Spiking and Deep Neural Networks
SO 2020 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN) held as part
   of the IEEE World Congress on Computational Intelligence (IEEE WCCI)
CY JUL 19-24, 2020
CL ELECTR NETWORK
DE Machine Learning; Neural Networks; Spiking Neural Networks; Security;
   Adversarial Examples; Attack; Vulnerability; Resilience; SNN; DNN; Deep
   Neural Network
ID TRENDS
AB Spiking Neural Networks (SNNs) claim to present many advantages in terms of biological plausibility and energy efficiency compared to standard Deep Neural Networks (DNNs). Recent works have shown that DNNs are vulnerable to adversarial attacks, i.e., small perturbations added to the input data can lead to targeted or random misclassifications. In this paper, we aim at investigating the key research question: "Are SNNs secure?" Towards this, we perform a comparative study of the security vulnerabilities in SNNs and DNNs w.r.t. the adversarial noise. Afterwards, we propose a novel black-box attack methodology, i.e., without the knowledge of the internal structure of the SNN, which employs a greedy heuristic to automatically generate imperceptible and robust adversarial examples (i.e., attack images) for the given SNN. We perform an in-depth evaluation for a Spiking Deep Belief Network (SDBN) and a DNN having the same number of layers and neurons (to obtain a fair comparison), in order to study the efficiency of our methodology and to understand the differences between SNNs and DNNs w.r.t. the adversarial examples. Our work opens new avenues of research towards the robustness of the SNNs, considering their similarities to the human brain's functionality.
C1 [Marchisio, Alberto; Nanfa, Giorgio; Khalid, Faiq; Hanif, Muhammad Abdullah; Shafique, Muhammad] Tech Univ Wien, Vienna, Austria.
   [Nanfa, Giorgio; Martina, Maurizio] Politecn Torino, Turin, Italy.
RP Marchisio, A (corresponding author), Tech Univ Wien, Vienna, Austria.
EM alberto.marchisio@tuwien.ac.at; giorgio.nanfa@studenti.polito.it;
   faiq.khalid@tuwien.ac.at; muhammad.hanif@tuwien.ac.at;
   maurizio.martina@polito.it; muhammad.shafique@tuwien.ac.at
CR Bagheri A., 2018, SPAWC
   Bengio Y, 2006, ADV NEURAL INFORM PR, P19, DOI DOI 10.7551/MITPRESS/7503.003.0024
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Fatahi M., 2016, CORR
   Gerstner W., 2002, SPIKING NEURON MODEL, DOI DOI 10.1017/CBO9780511815706
   Goh H., 2010, NIPS 2010 WORKSH DEE
   Goodfellow I. J., 2014, INT C LEARNING REPRE, DOI DOI 10.1109/CVPR.2016.90
   HEIBERG KE, 2013, BMC NEUROSCI, V14, DOI DOI 10.1186/1471-2474-14-243
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   HOLMSTROM L, 1992, IEEE T NEURAL NETWOR, V3, P24, DOI 10.1109/72.105415
   Kurakin A., 2017, ICLR WORKSH TRACK P
   Lisitsa D, 2017, IEEE NW RUSS YOUNG, P926, DOI 10.1109/EIConRus.2017.7910708
   Luo B., 2018, AAAI
   Maas W., 1997, T SOC COMPUT SIMUL I
   Madry A., 2018, INT C LEARNING REPRE
   Marchisio Alberto, 2019, 2019 IEEE Computer Society Annual Symposium on VLSI (ISVLSI), P553, DOI 10.1109/ISVLSI.2019.00105
   Marchisio A., 2019, ABS190109878
   Marchisio A., 2019, ABS190201147
   Merino E. R., 2018, CORR
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   MOLLER MF, 1993, NEURAL NETWORKS, V6, P525, DOI 10.1016/S0893-6080(05)80056-5
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   Papernot N, 2017, PROCEEDINGS OF THE 2017 ACM ASIA CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY (ASIA CCS'17), P506, DOI 10.1145/3052973.3053009
   Ponulak F, 2011, ACTA NEUROBIOL EXP, V71, P409
   Shafagatova A, 2019, LECT NOTES COMPUT SC, V11675, P386, DOI 10.1007/978-3-030-26619-6_25
   Shafique M, 2020, IEEE DES TEST, V37, P30, DOI 10.1109/MDAT.2020.2971217
   SIEGERT AJF, 1951, PHYS REV, V81, P617, DOI 10.1103/PhysRev.81.617
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Tavanaei Amirhossein, 2019, NEURAL NETWORKS
   Tieleman T., 2009, P 26 ANN INT C MACH, P1033, DOI DOI 10.1145/1553374.1553506
   Vreeken J., 2003, SPIKING NEURAL NETWO
   Zhang J., 2018, CORR
   Zhang J. J., 2019, P 56 ANN DES AUT C
NR 34
TC 9
Z9 9
U1 0
U2 18
PY 2020
DI 10.1109/ijcnn48605.2020.9207297
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture
DA 2023-11-11
ER

PT J
AU Xu, HB
   Cao, KX
   Chen, HG
   Abudusalamu, A
   Wu, W
   Xue, YX
AF Xu, Hubo
   Cao, Kexin
   Chen, Hongguang
   Abudusalamu, Awuti
   Wu, Wei
   Xue, Yanxue
TI Emotional brain network decoded by biological spiking neural network
SO FRONTIERS IN NEUROSCIENCE
DT Article
DE emotion; affective computing; brain network; neural oscillation;
   neuroregulation; self-backpropagation; spiking neural network;
   brain-computer interface
ID EEG DATA; RECOGNITION
AB IntroductionEmotional disorders are essential manifestations of many neurological and psychiatric diseases. Nowadays, researchers try to explore bi-directional brain-computer interface techniques to help the patients. However, the related functional brain areas and biological markers are still unclear, and the dynamic connection mechanism is also unknown. MethodsTo find effective regions related to different emotion recognition and intervention, our research focuses on finding emotional EEG brain networks using spiking neural network algorithm with binary coding. We collected EEG data while human participants watched emotional videos (fear, sadness, happiness, and neutrality), and analyzed the dynamic connections between the electrodes and the biological rhythms of different emotions. ResultsThe analysis has shown that the local high-activation brain network of fear and sadness is mainly in the parietal lobe area. The local high-level brain network of happiness is in the prefrontal-temporal lobe-central area. Furthermore, the & alpha; frequency band could effectively represent negative emotions, while the & alpha; frequency band could be used as a biological marker of happiness. The decoding accuracy of the three emotions reached 86.36%, 95.18%, and 89.09%, respectively, fully reflecting the excellent emotional decoding performance of the spiking neural network with self- backpropagation. DiscussionThe introduction of the self-backpropagation mechanism effectively improves the performance of the spiking neural network model. Different emotions exhibit distinct EEG networks and neuro-oscillatory-based biological markers. These emotional brain networks and biological markers may provide important hints for brain-computer interface technique exploration to help related brain disease recovery.
C1 [Xu, Hubo; Cao, Kexin; Abudusalamu, Awuti; Xue, Yanxue] Peking Univ, Natl Inst Drug Dependence, Beijing, Peoples R China.
   [Xu, Hubo; Cao, Kexin; Abudusalamu, Awuti; Xue, Yanxue] Peking Univ, Beijing Key Lab Drug Dependence, Beijing, Peoples R China.
   [Xu, Hubo; Cao, Kexin; Abudusalamu, Awuti] Peking Univ, Sch Basic Med Sci, Dept Pharmacol, Beijing, Peoples R China.
   [Chen, Hongguang] Peking Univ, Peking Univ Hosp 6, Inst Mental Hlth, Peking Univ Hosp 6,NHC Key Lab Mental Hlth, Beijing, Peoples R China.
   [Wu, Wei] Chinese Acad Sci, Inst Automat, State Key Lab Multimodal Artificial Intelligence S, Beijing, Peoples R China.
   [Xue, Yanxue] Chinese Inst Brain Res, Beijing, Peoples R China.
   [Xue, Yanxue] Peking Univ, Key Lab Neurosci, Minist Educ, Natl Hlth Commiss, Beijing, Peoples R China.
RP Xue, YX (corresponding author), Peking Univ, Natl Inst Drug Dependence, Beijing, Peoples R China.; Xue, YX (corresponding author), Peking Univ, Beijing Key Lab Drug Dependence, Beijing, Peoples R China.; Wu, W (corresponding author), Chinese Acad Sci, Inst Automat, State Key Lab Multimodal Artificial Intelligence S, Beijing, Peoples R China.; Xue, YX (corresponding author), Chinese Inst Brain Res, Beijing, Peoples R China.; Xue, YX (corresponding author), Peking Univ, Key Lab Neurosci, Minist Educ, Natl Hlth Commiss, Beijing, Peoples R China.
EM wei.wu@ia.ac.cn; yanxuexue@bjmu.edu.cn
CR Aiolli F, 2015, NEUROCOMPUTING, V169, P215, DOI 10.1016/j.neucom.2014.11.078
   Alex M, 2020, IEEE ACCESS, V8, P191080, DOI 10.1109/ACCESS.2020.3032380
   Alhalaseh R, 2020, COMPUTERS, V9, DOI 10.3390/computers9040095
   Barnett L, 2014, J NEUROSCI METH, V223, P50, DOI 10.1016/j.jneumeth.2013.10.018
   Basar E, 2012, INT J PSYCHOPHYSIOL, V86, P1, DOI 10.1016/j.ijpsycho.2012.07.002
   Behrenbeck J, 2019, J NEURAL ENG, V16, DOI 10.1088/1741-2552/aafabc
   Bullmore ET, 2009, NAT REV NEUROSCI, V10, P186, DOI 10.1038/nrn2575
   Desikan RS, 2006, NEUROIMAGE, V31, P968, DOI 10.1016/j.neuroimage.2006.01.021
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Fitzsimonds RM, 1997, NATURE, V388, P439, DOI 10.1038/41267
   GEWEKE J, 1982, J AM STAT ASSOC, V77, P304, DOI 10.2307/2287238
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Hasanzadeh F, 2021, APPL SOFT COMPUT, V101, DOI 10.1016/j.asoc.2020.107028
   Jenke R, 2014, IEEE T AFFECT COMPUT, V5, P327, DOI 10.1109/TAFFC.2014.2339834
   Kasabov NK, 2014, NEURAL NETWORKS, V52, P62, DOI 10.1016/j.neunet.2014.01.006
   Kohn N, 2014, NEUROIMAGE, V87, P345, DOI 10.1016/j.neuroimage.2013.11.001
   Kucyi A, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-019-14166-2
   Kumar N, 2016, PROCEDIA COMPUT SCI, V84, P70, DOI 10.1016/j.procs.2016.04.068
   Liu C., 2022, SCI CHINA LIFE SCI, V11, P3, DOI 10.1007./s11427-022-2206-3
   Liu HJ, 2022, NEURAL NETWORKS, V145, P308, DOI 10.1016/j.neunet.2021.10.023
   Liu YS, 2021, FUTURE GENER COMP SY, V119, P1, DOI 10.1016/j.future.2021.01.010
   Mague SD, 2022, NEURON, V110, P1728, DOI 10.1016/j.neuron.2022.02.016
   Malezieux M., 2023, ANNU REV NEUROSCI, DOI 10.1146./annurev-neuro-111020-103314
   Mousa MH, 2021, PLOS ONE, V16, DOI 10.1371/journal.pone.0255832
   Öniz A, 2009, INT J PSYCHOPHYSIOL, V71, P235, DOI 10.1016/j.ijpsycho.2008.10.003
   Poo MM, 2016, NEURON, V92, P591, DOI 10.1016/j.neuron.2016.10.050
   Scellier B, 2017, FRONT COMPUT NEUROSC, V11, DOI 10.3389/fncom.2017.00024
   Schrouff J, 2016, J NEUROSCI METH, V261, P19, DOI 10.1016/j.jneumeth.2015.11.028
   Seeber M, 2019, NAT COMMUN, V10, DOI 10.1038/s41467-019-08725-w
   Talbot A, 2020, Arxiv, DOI [arXiv:2004.05209, DOI 10.48550/ARXIV.2004.05209]
   Tan C, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20185328
   Tsodyks M, 1998, NEURAL COMPUT, V10, P821, DOI 10.1162/089976698300017502
   Wang XW, 2014, NEUROCOMPUTING, V129, P94, DOI 10.1016/j.neucom.2013.06.046
   Watts DJ, 1998, NATURE, V393, P440, DOI 10.1038/30918
   Zhang TL, 2021, SCI ADV, V7, DOI 10.1126/sciadv.abh0146
   Zhang YQ, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.622759
   Zucker RS, 2002, ANNU REV PHYSIOL, V64, P355, DOI 10.1146/annurev.physiol.64.092501.114547
NR 37
TC 0
Z9 0
U1 10
U2 10
PD JUL 11
PY 2023
VL 17
AR 1200701
DI 10.3389/fnins.2023.1200701
WC Neurosciences
DA 2023-11-11
ER

PT C
AU Lisovskaya, A
   Berkhova, EM
AF Lisovskaya, Angelina
   Berkhova, Evdokiia M.
GP IEEE
TI Processing, Hardware Design and Prospects of Use of Myoelectric
   Prostheses
SO PROCEEDINGS OF THE 2019 IEEE CONFERENCE OF RUSSIAN YOUNG RESEARCHERS IN
   ELECTRICAL AND ELECTRONIC ENGINEERING (EICONRUS)
SE IEEE NW Russia Young Researchers in Electrical and Electronic
   Engineering Conference
DT Proceedings Paper
CT IEEE Conference of Russian Young Researchers in Electrical and
   Electronic Engineering (EIConRus)
CY JAN 28-31, 2019
CL St Petersburg Electrotechn Univ, RUSSIA
HO St Petersburg Electrotechn Univ
DE action potential; biological model; spiking neural network; pattern
   recognition; neural prostheses
AB The problems of the development of modern myoelectric prostheses are considered. The main features of various models of spike neurons and neural networks for signal processing in prosthetic systems are described. Problems of their hardware implementation are considered. The main models showing behavior similar to a biological cell are given.
C1 [Lisovskaya, Angelina; Berkhova, Evdokiia M.] St Petersburg State Marine Tech Univ, Dept Marine Elect, St Petersburg, Russia.
RP Berkhova, EM (corresponding author), St Petersburg State Marine Tech Univ, Dept Marine Elect, St Petersburg, Russia.
EM marine_electronics@corp.smtu.ru
CR Dang B, 2018, AIP CONF PROC, V2034, DOI 10.1063/1.5067350
   Kuznetsova NI, 2018, PROCEEDINGS OF THE 2018 IEEE CONFERENCE OF RUSSIAN YOUNG RESEARCHERS IN ELECTRICAL AND ELECTRONIC ENGINEERING (EICONRUS), P912, DOI 10.1109/EIConRus.2018.8317237
   Slepova LO, 2018, PROCEEDINGS OF THE 2018 IEEE CONFERENCE OF RUSSIAN YOUNG RESEARCHERS IN ELECTRICAL AND ELECTRONIC ENGINEERING (EICONRUS), P987, DOI 10.1109/EIConRus.2018.8317255
   Zhilenkov Anton, 2016, Vibroengineering Procedia. 22nd International Conference on Vibroengineering, P17
   Zhilenkov AA, 2017, IOP C SER EARTH ENV, V87, DOI 10.1088/1755-1315/87/8/082059
NR 5
TC 0
Z9 0
U1 0
U2 2
PY 2019
BP 1187
EP 1189
DI 10.1109/eiconrus.2019.8657273
WC Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Howard, D
   Elfes, A
AF Howard, David
   Elfes, Alberto
BE Sayama, H
   Rieffel, J
   Risi, S
   Doursat, R
   Lipson, H
TI Evolving Spiking Networks for Turbulence-Tolerant Quadrotor Control
SO ALIFE 2014: THE FOURTEENTH INTERNATIONAL CONFERENCE ON THE SYNTHESIS AND
   SIMULATION OF LIVING SYSTEMS
DT Proceedings Paper
CT 14th International Conference on the Synthesis and Simulation of Living
   Systems (ALife)
CY JUL 30-AUG 02, 2014
CL New York, NY
ID EVOLUTION
AB We investigate the automatic development of robust quadrotor neurocontrollers based on spiking neural networks. A self-adaptive evolutionary algorithm is used to generate high-utility topology/weight combinations in the networks, and a simple synaptic plasticity mechanism provides some degree of in-trial adaptation. Incremental evolution gradually increases the severity of environmental conditions that the agent can successfully handle. Results compare the spiking networks to tuned Proportional/Integral/Derivative controllers and feedforward neural networks for waypoint-holding experiments in varied atmospheric conditions. It is shown that the spiking controllers are able to maintain a closer distance to the waypoint than the comparative controllers, and more effectively deal with more challenging environmental conditions.
C1 [Howard, David; Elfes, Alberto] CSIRO, Autonomous Syst Program, 1 Technol Court Pullenvale, Pullenvale, Qld 4069, Australia.
RP Howard, D (corresponding author), CSIRO, Autonomous Syst Program, 1 Technol Court Pullenvale, Pullenvale, Qld 4069, Australia.
EM david.howard@csiro.au
CR [Anonymous], INT SERIES ADV INTEL
   [Anonymous], 2000, EVOLUTIONARYROBOTICS
   [Anonymous], LECT NOTES COMPUTER
   [Anonymous], 1992, GENETIC PROGRAMMING
   Burgsteiner Harald, 2005, P 9 INT C ENG APPL N, P129
   Buzsaki G., 2006, RHYTHMS BRAIN, DOI 10.1093/acprof:oso/9780195301069.001.0001
   Clune J, 2009, IEEE C EVOL COMPUTAT, P2764, DOI 10.1109/CEC.2009.4983289
   De Nardi R, 2013, RN, V13
   De Nardi R, 2006, IEEE C EVOL COMPUTAT, P1784
   Dracopoulos Dimitris C., 2012, Genetic Programming. Proceedings of the 15th European Conference, EuroGP 2012, P25, DOI 10.1007/978-3-642-29139-5_3
   Floreano D., 2003, P 8 INT C ART LIF, P335
   Florian R. V., 2005, 7 INT S SYMB NUM ALG, P8
   Gerstner W., 2002, SPIKING NEURON MODEL
   Hagras H, 2002, INFORM SCIENCES, V145, P1, DOI 10.1016/S0020-0255(02)00221-9
   Hebb D. O., 1949, ORG BEHAV
   Helgadóttir LI, 2013, I IEEE EMBS C NEUR E, P891, DOI 10.1109/NER.2013.6696078
   Hoffmann F., 1998, 3 ON LIN WORLD C SOF, P201
   Holland J. H., 1975, ADAPTATION NATURAL A
   Howard G., 2010, P IEEE C EV COMP JUL, P1
   Howard G, 2014, EVOL COMPUT, V22, P79, DOI 10.1162/EVCO_a_00103
   Koppejan R., 2009, P 11 ANN C GENETIC E, P145
   LIEPMANN HW, 1952, J AERONAUT SCI, V19, P793, DOI 10.2514/8.2491
   Maass W, 1999, NEURAL COMPUT, V11, P903, DOI 10.1162/089976699300016494
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Minorsky N., 1922, J AM SOC NAVAL ENG, V34, P280, DOI [DOI 10.1111/J.1559-3584.1922.TB04958.X, 10.1111/j.1559-3584.1922.tb04958.x]
   Phillips C, 1996, ENG APPL ARTIF INTEL, V9, P175, DOI 10.1016/0952-1976(95)00008-9
   Rechenberg I., 1973, EVOLUTIONSSTRATEGIE
   Rocha M, 2003, LECT NOTES ARTIF INT, V2902, P24
   Rocke P, 2007, LECT NOTES COMPUT SC, V4419, P373
   Rumelhart DE., 1988, PARALLEL DISTRIBUTED
   Saggie-Wexler K, 2006, ARTIF LIFE, V12, P1, DOI 10.1162/106454606775186428
   Shepherd III J., 2010, P 12 ANN C GENETIC E, P1131, DOI DOI 10.1145/18304831830693
   Wiles J., 2010, 17 INT C NEUR INF PR
NR 33
TC 11
Z9 11
U1 0
U2 0
PY 2014
BP 431
EP 438
DI 10.7551/978-0-262-32621-6-ch071
WC Evolutionary Biology; Mathematical & Computational Biology
DA 2023-11-11
ER

PT C
AU Franovic, I
   Miljkovic, V
AF Franovic, Igor
   Miljkovic, Vladimir
BE Reljin, B
   Stankovic, S
TI Percolation approach to formation of synfire chains in two dimensional
   neural networks
SO NEUREL 2006: EIGHT SEMINAR ON NEURAL NETWORK APPLICATIONS IN ELECTRICAL
   ENGINEERING, PROCEEDINGS
DT Proceedings Paper
CT 8th Seminar on Neural Network Applications in Electrical Engineering
CY SEP 26-27, 2006
CL Univ Begrade, Fac Elect Engn, Belgrade, SERBIA
HO Univ Begrade, Fac Elect Engn
DE clustering phenomena; neural networks; neural population dynamics;
   synchronization; synfire chains
ID SPIKING NEURONS; DISTRIBUTED SYNCHRONY; PROPAGATION; POPULATIONS; MODEL
AB We consider the propagation of spike packets in two dimensional networks consisting of locally coupled neural pools. The dynamic attractors of this model, synfire chains, appear for some values of network parameters. The synfire chain formation exhibits behavior, which may be discribed with the percolation phase transition. Using finite-size scaling method, we obtained the critical probabilities and the critical parameter ratio beta/v for different sets of refractoriness and synaptic weights, connecting neighbouring neural pools.
C1 [Miljkovic, Vladimir] Univ Belgrade, Fac Phys, Belgrade, Serbia.
   [Franovic, Igor] Serbian Min Sci & Enviromental Protect, Belgrade, Serbia.
RP Franovic, I (corresponding author), Serbian Min Sci & Enviromental Protect, Belgrade, Serbia.
EM miljko@ff.bg.ac.yu
CR Baltes H, 2001, SENSOR ACTUAT A-PHYS, V92, P1, DOI 10.1016/S0924-4247(01)00532-5
   BIENENSTOCK E, 1995, NETWORK-COMP NEURAL, V6, P179, DOI 10.1088/0954-898X/6/2/004
   Gewaltig MO, 2001, NEURAL NETWORKS, V14, P657, DOI 10.1016/S0893-6080(01)00070-3
   Ginelli F, 2003, PHYS REV E, V67, DOI 10.1103/PhysRevE.67.046217
   Haldeman C, 2005, PHYS REV LETT, V94, DOI 10.1103/PhysRevLett.94.058101
   Horn D, 2000, NEUROCOMPUTING, V32, P409, DOI 10.1016/S0925-2312(00)00193-4
   Kistler WM, 2002, NEURAL COMPUT, V14, P987, DOI 10.1162/089976602753633358
   Kitano K, 2002, NEUROCOMPUTING, V44, P473, DOI 10.1016/S0925-2312(02)00404-6
   Kozma R, 2005, BIOL CYBERN, V92, P367, DOI 10.1007/s00422-005-0565-z
   Levy N, 2001, NEURAL NETWORKS, V14, P815, DOI 10.1016/S0893-6080(01)00044-2
   Marcq P, 1997, PHYS REV E, V55, P2606, DOI 10.1103/PhysRevE.55.2606
   ZANETTE DH, 1998, PHYS REV E, V58
NR 12
TC 0
Z9 0
U1 0
U2 1
PY 2006
BP 69
EP +
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
DA 2023-11-11
ER

PT C
AU Hauptvogel, M
   Madrenas, J
   Moreno, JM
AF Hauptvogel, Michael
   Madrenas, Jordi
   Manuel Moreno, J.
GP IEEE
TI SpiNDeK: An Integrated Design Tool for the Multiprocessor Emulation of
   Complex Bioinspired Spiking Neural Networks
SO 2009 IEEE CONGRESS ON EVOLUTIONARY COMPUTATION, VOLS 1-5
SE IEEE Congress on Evolutionary Computation
DT Proceedings Paper
CT IEEE Congress on Evolutionary Computation
CY MAY 18-21, 2009
CL Trondheim, NORWAY
ID NEURONS; PLATFORM; ARRAY
AB SpiNDeK (Spiking Neural Network Design Kit) is an integrated design tool intended to support the development of emulation of complex bioinspired neural networks. In this work, the most relevant aspects of the tool are reported, regarding the generation of connections as well as synapse and neuron parameters of spiking neural networks as well as the automated code generation and simulation, ready to be executed by an ad-hoc parallel architecture. The tool is fully functional and has demonstrated its usefulness.
C1 [Hauptvogel, Michael; Madrenas, Jordi; Manuel Moreno, J.] Tech Univ Catalunya, Dept Elect Engn, Barcelona, Catalunya, Spain.
RP Hauptvogel, M (corresponding author), Tech Univ Catalunya, Dept Elect Engn, Barcelona, Catalunya, Spain.
EM madrenas@eel.upc.edu; moreno@eel.upc.edu
CR [Anonymous], 1991, THESIS CALTECH
   Boahen KA, 2000, IEEE T CIRCUITS-II, V47, P416, DOI 10.1109/82.842110
   GAIARSA BAY, 2002, TRENDS NEUROSCIENCES, V25
   Iglesias J, 2005, BIOSYSTEMS, V79, P11, DOI 10.1016/j.biosystems.2004.09.016
   Indiveri G, 2006, IEEE T NEURAL NETWOR, V17, P211, DOI 10.1109/TNN.2005.860850
   Jahnke A., 1996, Proceedings of the Fifth International Conference on Microelectronics for Neural Networks and Fuzzy Systems. MicroNeuro'96, P232, DOI 10.1109/MNNFS.1996.493796
   MAASS W, HDB BRAIN THEORY NEU
   Maguire LP, 2007, NEUROCOMPUTING, V71, P13, DOI 10.1016/j.neucom.2006.11.029
   MEILANDER W, 2003, P INT PAR DISTR PAR
   Moreno JM, 2005, LECT NOTES COMPUT SC, V3637, P177, DOI 10.1007/11549703_17
   MORENO JM, 2009, IEEE C EV COMP
   Roggen D, 2003, 2003 NASA/DOD CONFERENCE ON EVOLVABLE HARDWARE, P189
   Ros E, 2006, IEEE T NEURAL NETWOR, V17, P1050, DOI 10.1109/TNN.2006.875980
   Sanchez E, 2007, NASA/ESA CONFERENCE ON ADAPTIVE HARDWARE AND SYSTEMS, PROCEEDINGS, P587
   SONG RMN, 2005, PLOS BIOL, V3
   Talla D, 2003, IEEE T COMPUT, V52, P1015, DOI 10.1109/TC.2003.1223637
   UPEGUI A, 2007, 2 NASA ESA C AD HARD
   VANRULLEN SJT, 2002, SURFING SPIKE WAVE V
   Vogelstein RJ, 2007, IEEE T NEURAL NETWOR, V18, P253, DOI 10.1109/TNN.2006.883007
   VOLGELSTEIN J, 2007, NEURAL COMPUT, V19, P2281
   Vreeken J, 2003, SPIKING NEURAL NETWO
NR 21
TC 2
Z9 3
U1 0
U2 0
PY 2009
BP 142
EP 149
DI 10.1109/CEC.2009.4982941
WC Computer Science, Artificial Intelligence; Computer Science,
   Interdisciplinary Applications; Engineering, Multidisciplinary
DA 2023-11-11
ER

PT C
AU Huang, XY
   Jones, E
   Zhang, SR
   Xie, SY
   Furber, S
   Goulermas, Y
   Marsden, E
   Baistow, I
   Mitra, S
   Hamilton, A
AF Huang, Xiaoyu
   Jones, Edward
   Zhang, Siru
   Xie, Shouyu
   Furber, Steve
   Goulermas, Yannis
   Marsden, Edward
   Baistow, Ian
   Mitra, Srinjoy
   Hamilton, Alister
GP IEEE
TI Spiking Neural Network Based Low-Power Radioisotope Identification using
   FPGA
SO 2020 27TH IEEE INTERNATIONAL CONFERENCE ON ELECTRONICS, CIRCUITS AND
   SYSTEMS (ICECS)
SE IEEE International Conference on Electronics Circuits and Systems
DT Proceedings Paper
CT 27th IEEE International Conference on Electronics, Circuits and Systems
   (IEEE ICECS)
CY NOV 23-25, 2020
CL ELECTR NETWORK
DE event-based signal processing; low power; radioisotope identification;
   spiking neural networks; FPGA; SpiNNaker
AB this paper presents detailed methodology of a Spiking Neural Network (SNN) based low-power design for radioisotope identification. A low power cost of 72 mW has been achieved on FPGA with the inference accuracy of 100% at 10 cm test distance and 97% at 25 cm. The design verification and chip validation methods are presented. It also discusses SNN simulation on SpiNNaker for rapid prototyping and various considerations specific to the application such as test distance, integration time and SNN hyperparameter selections.
C1 [Huang, Xiaoyu; Xie, Shouyu; Mitra, Srinjoy; Hamilton, Alister] Univ Edinburgh, Edinburgh, Midlothian, Scotland.
   [Jones, Edward; Furber, Steve] Univ Manchester, Manchester, Lancs, England.
   [Zhang, Siru; Goulermas, Yannis] Univ Liverpool, Liverpool, Merseyside, England.
   [Marsden, Edward; Baistow, Ian] Kromek Grp Plc, Sedgefield, England.
RP Huang, XY (corresponding author), Univ Edinburgh, Edinburgh, Midlothian, Scotland.
EM xiaoyu.huang@ed.ac.uk; alister.hamilton@ed.ac.uk
CR Davison Andrew P, 2008, Front Neuroinform, V2, P11, DOI 10.3389/neuro.11.011.2008
   Furber S., 2020, SPINNAKER SPIKING NE, DOI 10.1561/9781680836523
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   Gouveia LC, 2011, IEEE T CIRCUITS-I, V58, P791, DOI 10.1109/TCSI.2010.2089552
   Huang X., 2020, ARXIV200705686
   Ju XP, 2020, NEURAL COMPUT, V32, P182, DOI 10.1162/neco_a_01245
   Kamuda M, 2017, IEEE T NUCL SCI, V64, P1858, DOI 10.1109/TNS.2017.2693152
   LAZZARO J, 1993, IEEE T NEURAL NETWOR, V4, P523, DOI 10.1109/72.217193
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
NR 9
TC 1
Z9 1
U1 0
U2 0
PY 2020
DI 10.1109/icecs49266.2020.9294873
WC Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Li, JX
   Li, DJ
   Jiang, RH
   Xiao, R
   Tang, HJ
   Tan, KC
AF Li, Jiaxin
   Li, Dengju
   Jiang, Runhao
   Xiao, Rong
   Tang, Huajin
   Tan, Kay Chen
TI Vision-Action Semantic Associative Learning Based on Spiking Neural
   Networks for Cognitive Robot
SO IEEE COMPUTATIONAL INTELLIGENCE MAGAZINE
DT Article
DE Associative memory; Shape; Semantics; Neural networks; Encoding;
   Cognitive robotics; Convergence
AB Establishing cognitive environments help cognitive robots understand human actions, languages and observed objects. In this paper, a cognitive robotic model based on a novel spiking bidirectional associative memory (BAM) method is presented to establish a cognitive environment. The spiking BAM network uses a supervised spike-based learning rule to learn the relationship between the semantic information of vision and action, where a new coding scheme is proposed to encode and decode vision and action semantic information. Vision semantic information is obtained by the deeply salient shape detection (DSSD) method. Action semantic information is obtained by a spiking neural network (SNN) with the proposed coding method. Experimental results demonstrate that the spiking BAM achieves a good convergence ability and relatively high recall accuracy. Specially, for the cognitive development of the robot, this system can be used to improve the robot's ability to infer human intentions in its natural interactions with humans.
C1 [Li, Jiaxin; Li, Dengju; Xiao, Rong] Sichuan Univ, Chengdu, Peoples R China.
   [Jiang, Runhao; Tang, Huajin] Zhejiang Univ, Hangzhou, Peoples R China.
   [Tan, Kay Chen] Hong Kong Polytech Univ, Hong Kong, Peoples R China.
RP Tang, HJ (corresponding author), Zhejiang Univ, Hangzhou, Peoples R China.
EM htang@zju.edu.cn
CR Alexandre F., 2021, ERCIM NEWS, V125
   Asada M, 2009, IEEE T AUTON MENT DE, V1, P12, DOI 10.1109/TAMD.2009.2021702
   Chang H, 2008, NEUROCOMPUTING, V72, P278, DOI 10.1016/j.neucom.2008.01.010
   Cyr Andre, 2018, Artificial General Intelligence. 11th International Conference, AGI 2018. Proceedings: Lecture Notes in Artificial Intelligence (LNAI 10999), P32, DOI 10.1007/978-3-319-97676-1_4
   Devereux BJ, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-28865-1
   Douglas D. H., 1973, CLASSICS CARTOGRAPHY, V10, P112, DOI [DOI 10.1002/9780470669488.CH2, 10.3138/FM57-6770-U75U-7727, DOI 10.3138/FM57-6770-U75U-7727]
   Gerstner W., 2002, SPIKING NEURON MODEL
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Hampo M., 2020, INT C NEUROMORPHIC S, P1
   Hiolle A, 2012, ACM T INTERACT INTEL, V2, DOI 10.1145/2133366.2133369
   Hou QB, 2017, PROC CVPR IEEE, P5300, DOI 10.1109/CVPR.2017.563
   Johnson M., 2021, THESIS U OTTAWA OTTA
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Luo MR, 2001, COLOR RES APPL, V26, P340, DOI 10.1002/col.1049
   Masuyama N, 2018, NEUROCOMPUTING, V272, P213, DOI 10.1016/j.neucom.2017.06.069
   Masuyama N, 2017, NEURAL COMPUT APPL, V28, P737, DOI 10.1007/s00521-015-2102-x
   Niu D, 2018, LECT NOTES COMPUT SC, V11301, P582, DOI 10.1007/978-3-030-04167-0_53
   Rajavenkatanarayanan A, 2020, ACMIEEE INT CONF HUM, P398, DOI 10.1145/3371382.3378384
   SANDERS RE, 1988, LANG SOC, V17, P604, DOI 10.1017/S004740450001318X
   Sperber D., 1999, ENCY COGN SCI, P111
   Tang HJ, 2017, NEURAL NETWORKS, V87, P27, DOI 10.1016/j.neunet.2016.08.015
   Tikhanoff V, 2011, IEEE T AUTON MENT DE, V3, P17, DOI 10.1109/TAMD.2010.2100390
   van Rossum MCW, 2001, NEURAL COMPUT, V13, P751, DOI 10.1162/089976601300014321
   Yi W., 2014, ADV MECH ENG, V6
   Yorita A, 2011, IEEE T AUTON MENT DE, V3, P64, DOI 10.1109/TAMD.2011.2105868
   Yu Q, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0078318
   Zamani M., 2010, PROC INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2010.5596806
NR 27
TC 2
Z9 2
U1 4
U2 9
PD NOV 1
PY 2022
VL 17
IS 4
BP 27
EP 38
DI 10.1109/MCI.2022.3199623
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT S
AU Amin, HH
   Fujii, RH
AF Amin, HH
   Fujii, RH
BE Huang, DS
   Zhang, XP
   Huang, GB
TI Sound classification and function approximation using spiking neural
   networks
SO ADVANCES IN INTELLIGENT COMPUTING, PT 1, PROCEEDINGS
SE Lecture Notes in Computer Science
DT Article; Proceedings Paper
CT Conference on Aristophanes Upstairs and Downstairs - Peace, Birds and
   Frogs in Ancient and Modern Performance
CY SEP 16-18, 2004
CL Magdalen Coll, Oxford, ENGLAND
HO Magdalen Coll
ID NEURONS
AB The capabilities and robustness of a new spiking neural network (SNN) learning algorithm are demonstrated with sound classification and function approximation applications. The proposed SNN learning algorithm and the radial basis function (RBF) learning method for function approximation are compared. The complexity of the learning algorithm is analyzed.
C1 Univ Aizu, Fukushima, Japan.
RP Amin, HH (corresponding author), Univ Aizu, Fukushima, Japan.
EM d8042201@u-aizu.ac.jp; fujii@u-aizu.ac.jp
CR ABELES M, 1993, J NEUROPHYSIOL, V70, P1629, DOI 10.1152/jn.1993.70.4.1629
   BHTE SM, 2000, P EUR S ART NEUR NET, P419
   Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   Gerstner W., 2002, SPIKING NEURON MODEL
   Haykin S., 2004, NEURAL NETWORKS COMP, V2, P41
   HESHAM H, 2004, P 12 EUR S ART NEUR, P355
   HESHAM H, 2004, P 2004 INT JOINT C N, P477
   Hopfield IJ, 2000, P NATL ACAD SCI USA, V97, P13919, DOI 10.1073/pnas.250483697
   Hopfield JJ, 2001, P NATL ACAD SCI USA, V98, P1282, DOI 10.1073/pnas.031567098
   MAASS W., 1999, PULSED NEURAL NETWOR
   Ruf B, 1998, IEEE T NEURAL NETWOR, V9, P575, DOI 10.1109/72.668899
   RUF B, 1997, THESIS TU GRAZ AUSTR
NR 12
TC 3
Z9 3
U1 0
U2 0
PY 2005
VL 3644
BP 621
EP 630
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods
DA 2023-11-11
ER

PT J
AU Kim, K
   Tudor, A
   Chen, CL
   Lee, D
   Shen, AM
   Chen, Y
AF Kim, Kyunghyun
   Tudor, Andrew
   Chen, Chia-Ling
   Lee, Dongwon
   Shen, Alex M.
   Chen, Yong
TI Bioinspired neuromorphic module based on carbon nanotube/C60/polymer
   composite
SO JOURNAL OF COMPOSITE MATERIALS
DT Article
DE Carbon nanotube polymer composite; neuromorphic module; C60; CMOS Soma
ID NEURAL-NETWORKS; SYNAPSE; NEURONS; DEVICE; BRAIN
AB It is extremely challenging to imitate neural networks with their high-speed parallel signal processing, low power consumption, and intelligent learning capability. In this work, we report a spike neuromorphic module composed of synapstors made from carbon nanotube/C60/polyimide composite and CMOS Somas made from complementary metal-oxide semiconductor electronic circuits. The synapstor emulates a biological synapse with spike signal processing, plasticity, and memory; the CMOS Soma emulates a Soma in a biological neuron with analog parallel signal processing and spike generation. Spikes, short potential pulses, and input to the synapstors trigger postsynaptic currents and generate output spikes from the CMOS Somas in a parallel manner with low power consumption. The module can be modified dynamically on the basis of the synapstor plasticity. Spike neuromorphic modules could potentially be scaled up to emulate biologic neural networks and their functions.
C1 [Kim, Kyunghyun; Tudor, Andrew; Chen, Chia-Ling; Lee, Dongwon; Shen, Alex M.; Chen, Yong] Univ Calif Los Angeles, Dept Mech & Aerosp Engn, Calif NanoSyst Inst, Los Angeles, CA USA.
RP Kim, K (corresponding author), Dept Mech & Aerosp Engn, Engn 4 44-116C,420 Westwood Plaza, Los Angeles, CA 90095 USA.
EM kh12kim@ucla.edu
CR Alibart F, 2012, NANOTECHNOLOGY, V23, DOI 10.1088/0957-4484/23/7/075201
   Alibart F, 2012, ADV FUNCT MATER, V22, P609, DOI 10.1002/adfm.201101935
   Anand R, 2009, SELF-DEFENSE IN INTERNATIONAL RELATIONS, P1, DOI 10.1057/9780230245747
   Andrzejak RG, 2001, PHYS REV E, V64, DOI 10.1103/PhysRevE.64.061907
   BACKUS J, 1978, COMMUN ACM, V21, P613, DOI 10.1145/359576.359579
   Bichler O, 2012, IEEE T ELECTRON DEV, V59, P2206, DOI 10.1109/TED.2012.2197951
   Bohr Mark, 2009, 2009 IEEE International Solid-State Circuits Conference (ISSCC 2009), P23, DOI 10.1109/ISSCC.2009.4977293
   Brette R, 2007, J COMPUT NEUROSCI, V23, P349, DOI 10.1007/s10827-007-0038-6
   Chen YJ, 2008, IEEE IJCNN, P1615, DOI 10.1109/IJCNN.2008.4634013
   Freeman WJ, 2008, NEURAL NETWORKS, V21, P257, DOI 10.1016/j.neunet.2007.12.011
   Hasegawa T, 2010, ADV MATER, V22, P1831, DOI 10.1002/adma.200903680
   Indiveri G, 2003, PROCEEDINGS OF THE 2003 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL IV, P820
   Indiveri G, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00073
   Jain AK, 1996, COMPUTER, V29, P31, DOI 10.1109/2.485891
   Jo SH, 2010, NANO LETT, V10, P1297, DOI 10.1021/nl904092h
   Kim KH, 2012, NANO LETT, V12, P389, DOI 10.1021/nl203687n
   Kim K, 2013, ADV MATER, V25, P1693, DOI 10.1002/adma.201203116
   Kuzum D, 2012, NANO LETT, V12, P2179, DOI 10.1021/nl201040y
   Laughlin SB, 1998, NAT NEUROSCI, V1, P36, DOI 10.1038/236
   MEAD C, 1990, P IEEE, V78, P1629, DOI 10.1109/5.58356
   Nayak A, 2012, ADV FUNCT MATER, V22, P3606, DOI 10.1002/adfm.201200640
   Pickett MD, 2013, NAT MATER, V12, P114, DOI [10.1038/NMAT3510, 10.1038/nmat3510]
   Poon CS, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00108
   Qu MN, 2010, IEEE ELECTR DEVICE L, V31, P1098, DOI 10.1109/LED.2010.2061833
   Ramakrishnan S, 2011, IEEE T BIOMED CIRC S, V5, P244, DOI 10.1109/TBCAS.2011.2109000
   Seo K, 2011, NANOTECHNOLOGY, V22, DOI 10.1088/0957-4484/22/25/254023
   Serrano-Gotarredona T, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00002
   Shen AM, 2013, ACS NANO, V7, P6117, DOI 10.1021/nn401946s
   Snider GS, 2008, 2008 IEEE INTERNATIONAL SYMPOSIUM ON NANOSCALE ARCHITECTURES, P85, DOI 10.1109/NANOARCH.2008.4585796
   Versace M, 2010, IEEE SPECTRUM, V47, P30, DOI 10.1109/MSPEC.2010.5644776
   Yu SM, 2013, ADV MATER, V25, P1774, DOI 10.1002/adma.201203680
NR 31
TC 2
Z9 2
U1 3
U2 50
PD JUN
PY 2015
VL 49
IS 15
SI SI
BP 1809
EP 1822
DI 10.1177/0021998315573559
WC Materials Science, Composites
DA 2023-11-11
ER

PT C
AU Che, JM
   Cao, J
   Feng, S
   Chen, J
   Wang, Y
AF Che, Jinming
   Cao, Jian
   Feng, Shuo
   Chen, Jue
   Wang, Yuan
GP IEEE
TI Linear Leakage: Better Robustness for Spiking Neural Network
SO 2023 25TH INTERNATIONAL CONFERENCE ON ADVANCED COMMUNICATION TECHNOLOGY,
   ICACT
SE International Conference on Advanced Communication Technology
DT Proceedings Paper
CT 25th International Conference on Advanced Communication Technology
   (ICACT) - New Cyber Security Risks for Enterprise Amidst COVID-19
   Pandemic
CY FEB 19-22, 2023
CL Pyeongchang, SOUTH KOREA
DE Spiking neural networks; Spatio-temporal backpropagation;
   Leaky-integrate-fire neuron; Robustness; Noise attacks
ID SYSTEMS
AB As the third-generation neural networks, Spiking Neural Networks (SNNs) have the potential to replace ANNs in noisy input scenarios due to the advantages of low power consumption and high robustness. In this paper, we propose a method that uses linear leakage on Leaky-integrate-fire (LIF) neurons of direct training of SNNs. First, this paper provides an in-depth analysis of the process of integrating and firing spikes on LIF neurons based on the linear leakage method, and finally demonstrates the inverse relationship between the leakage parameters tau(m). and spiking rate (1/T) in linear leakage. Second, this paper designs experiments on the application of the linear leakage method to three different network models based on MNIST and Cifar10 datasets. The experimental results show that the linear leakage method in this paper has stronger robustness compared to the no-leakage and exponential leakage approaches. This finding provides a new idea with practical significance for the further study of LIF models of SNNs.
C1 [Che, Jinming; Cao, Jian; Feng, Shuo; Chen, Jue; Wang, Yuan] Peking Univ, Sch Software Microelect, Beijing, Peoples R China.
RP Che, JM (corresponding author), Peking Univ, Sch Software Microelect, Beijing, Peoples R China.
EM chejinming@stu.pku.edu.cn; caojian@ss.pku.edu.cn;
   fengshuo@stu.pku.edu.cn; chenjue@stu.pku.edu.cn; wangyuan@pku.edu.cn
CR Pérez-Carrasco JA, 2013, IEEE T PATTERN ANAL, V35, P2706, DOI 10.1109/TPAMI.2013.71
   Chowdhury SS, 2021, NEUROCOMPUTING, V464, P83, DOI 10.1016/j.neucom.2021.07.091
   Dayan P., 2001, THEORETICAL NEUROSCI
   Diehl PU, 2015, IEEE IJCNN
   El-Allami R, 2021, PROCEEDINGS OF THE 2021 DESIGN, AUTOMATION & TEST IN EUROPE CONFERENCE & EXHIBITION (DATE 2021), P774, DOI 10.23919/DATE51398.2021.9473981
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Kuang YS, 2021, IEEE T CIRCUITS-II, V68, P2655, DOI 10.1109/TCSII.2021.3052172
   Neggaz MA, 2020, IEEE DES TEST, V37, P76, DOI 10.1109/MDAT.2019.2952336
   Rueckauer B., 2016, ARXIV161204052
   Shafique M, 2020, IEEE DES TEST, V37, P30, DOI 10.1109/MDAT.2020.2971217
   Sharmin Saima, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12374), P399, DOI 10.1007/978-3-030-58526-6_24
   Sharmin S, 2019, COMPREHENSIVE ANAL A, DOI [10.1109/IJCNN.2019.8851732, DOI 10.1109/IJCNN.2019.8851732]
   Srinivasan G, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00189
   Wu YJ, 2019, AAAI CONF ARTIF INTE, P1311
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Yang J, 2015, NONLINEAR DYNAM, V82, P723, DOI 10.1007/s11071-015-2190-2
   Zhang J., 2019, BUILDING ROBUST MACH, DOI [10.1145/3316781.3323472, DOI 10.1145/3316781.3323472]
   Zhang ML, 2017, NEUROCOMPUTING, V219, P333, DOI 10.1016/j.neucom.2016.09.044
   Zheng H, 2020, GOING DEEPER DIRECTL
NR 19
TC 0
Z9 0
U1 0
U2 0
PY 2023
BP 417
EP 423
WC Computer Science, Theory & Methods; Engineering, Electrical &
   Electronic; Telecommunications
DA 2023-11-11
ER

PT J
AU Crook, N
   Goh, WJ
   Hawarat, M
AF Crook, Nigel
   Goh, Wee Jin
   Hawarat, Mohammad
TI Pattern recall in networks of chaotic neurons
SO BIOSYSTEMS
DT Article; Proceedings Paper
CT 6th International Workshop on Information Processing in Cells and
   Tissues
CY AUG 30-SEP 01, 2005
CL St Williams Coll, York, ENGLAND
HO St Williams Coll
DE chaos; spiking neural network
ID HIDDEN MARKOV-MODELS; NEURAL-NETWORKS; OLFACTORY SYSTEM; RECOGNITION;
   ATTRACTORS; DYNAMICS
AB This research investigates the potential utility of chaotic dynamics in neural information processing. A novel chaotic spiking neural network model is presented which is composed of non-linear dynamic state (NDS) neurons. The activity of each NDS neuron is driven by a set of non-linear equations coupled with a threshold based spike output mechanism. If time-delayed self-connections are enabled then the network stabilises to a periodic pattern of activation. Previous publications of this work have demonstrated that the chaotic dynamics which drive the network activity ensure that an extremely large number of such periodic patterns can be generated by this network. This paper presents a major extension to this model which enables the network to recall a pattern of activity from a selection of previously stabilised patterns. (c) 2006 Elsevier Ireland Ltd. All rights reserved.
C1 Oxford Brookes Univ, Dept Comp, Oxford OX3 0BP, England.
RP Crook, N (corresponding author), Oxford Brookes Univ, Dept Comp, Wheatley Campus, Oxford OX3 0BP, England.
EM ntcrook@brookes.ac.uk
CR AIHARA K, 1990, PHYS LETT A, V144, P333, DOI 10.1016/0375-9601(90)90136-C
   Albers DJ, 1998, INT J BIFURCAT CHAOS, V8, P1463, DOI 10.1142/S0218127498001121
   Andrade FR, 1996, J CARDIOVASC ELECTR, V7, P2, DOI 10.1111/j.1540-8167.1996.tb00454.x
   Babloyantz A, 1996, INT J NEURAL SYST, V7, P461, DOI 10.1142/S0129065796000440
   Chen SS, 2004, DISCRETE CONT DYN-A, V10, P805
   Crook N, 2003, INFORM SCIENCES, V150, P59, DOI 10.1016/S0020-0255(02)00370-5
   Crook N, 2002, CYBERNET SYST, V33, P341, DOI 10.1080/01969720290040632
   CROOK N, 2005, P 13 EUR S ART NEUR, P37
   CROOK NT, 2004, P 12 EUR S ART NEUR, P151
   El-Yacoubi A, 1999, IEEE T PATTERN ANAL, V21, P752, DOI 10.1109/34.784288
   Freeman W. J., 1994, TEMPORAL CODING BRAI, P13, DOI [10.1007/978-3-642-85148-3_2, DOI 10.1007/978-3-642-85148-3_2]
   FREEMAN WJ, 1987, BIOL CYBERN, V56, P139, DOI 10.1007/BF00317988
   FREEMAN WJ, 1985, ELECTROEN CLIN NEURO, V61, pS155, DOI 10.1016/0013-4694(85)90601-7
   Hoshino O, 1997, NEURAL NETWORKS, V10, P1375, DOI 10.1016/S0893-6080(97)00022-1
   Komendantov AO, 1996, J THEOR BIOL, V183, P219, DOI 10.1006/jtbi.1996.0215
   LIU Y, 1996, PHYS REV E, V53, P4502
   Pasemann F, 1998, NETWORK-COMP NEURAL, V9, P549, DOI 10.1088/0954-898X/9/4/009
   RABINER LR, 1985, IEEE T ACOUST SPEECH, V33, P561, DOI 10.1109/TASSP.1985.1164586
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   ROSSLER OE, 1976, PHYS LETT A, V57, P397, DOI 10.1016/0375-9601(76)90101-8
   YAO Y, 1990, NEURAL NETWORKS, V3, P153, DOI 10.1016/0893-6080(90)90086-Z
NR 21
TC 16
Z9 17
U1 0
U2 3
PD FEB
PY 2007
VL 87
IS 2-3
BP 267
EP 274
DI 10.1016/j.biosystems.2006.09.022
WC Biology; Mathematical & Computational Biology
DA 2023-11-11
ER

PT J
AU Wang, XQ
   Hou, ZG
   Lv, F
   Tan, M
   Wang, YJ
AF Wang, Xiuqing
   Hou, Zeng-Guang
   Lv, Feng
   Tan, Min
   Wang, Yongji
TI Mobile robots' modular navigation controller using spiking neural
   networks
SO NEUROCOMPUTING
DT Article
DE Mobile robot; Spiking neural networks; Modular navigation controller;
   Target-approaching; Obstacle-avoidance; Wall-following
ID TRACKING CONTROL; NEURONS; SYSTEMS
AB Autonomous navigation plays an important role in mobile robots. Artificial neural networks (ANNs) have been successfully used in nonlinear systems whose models are difficult to build. However, the third generation neural networks - Spiking neural networks (SNNs) - contain features that are more attractive than those of traditional neural networks (NNs). Because SNNs convey both temporal and spatial information, they are more suitable for mobile robots' controller design. In this paper, a modular navigation controller based on promising spiking neural networks for mobile robots is presented. The proposed behavior-based target-approaching navigation controller, in which the reactive architecture is used, is composed of three sub-controllers: the obstacle-avoidance SNN controller, the wall-following SNN controller and the goal-approaching controller. The proposed modular navigation controller does not require accurate mathematical models of the environment, and is suitable to unknown and unstructured environments. Simulation results show that the proposed transition conditions for sub-controllers are feasible. The navigation controller can control the mobile robot to reach a target successfully while avoiding obstacles and following the wall to get rid of the deadlock caused by local minimum. (C) 2014 Elsevier B.V. All rights reserved.
C1 [Wang, Xiuqing; Lv, Feng] Hebei Normal Univ, Shijiazhuang 050031, Peoples R China.
   [Hou, Zeng-Guang; Tan, Min] Chinese Acad Sci, Inst Automat, State Key Lab Management & Control Complex Syst, Beijing 100190, Peoples R China.
   [Wang, Yongji] Chinese Acad Sci, Inst Software, Lab Internet Technol, State Key Lab Comp Sci, Beijing 100190, Peoples R China.
RP Hou, ZG (corresponding author), Chinese Acad Sci, Inst Automat, State Key Lab Management & Control Complex Syst, Beijing 100190, Peoples R China.
EM zengguang.hou@ia.ac.cn
CR Alamdari ARSA, 2005, PROCEEDINGS OF WORLD ACADEMY OF SCIENCE, ENGINEERING AND TECHNOLOGY, VOL 6, P49
   Alnajjar F, 2009, ADAPT BEHAV, V17, P179, DOI 10.1177/1059712309105814
   [Anonymous], 1993, NEURAL NETWORK PERCE
   Arena P, 2009, IEEE T NEURAL NETWOR, V20, P202, DOI 10.1109/TNN.2008.2005134
   Arkin R. C., 1990, Robotics and Autonomous Systems, V6, P105, DOI 10.1016/S0921-8890(05)80031-4
   BARTO AG, 1983, IEEE T SYST MAN CYB, V13, P834, DOI 10.1109/TSMC.1983.6313077
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   Cheng L, 2010, IEEE T NEURAL NETWOR, V21, P1351, DOI 10.1109/TNN.2010.2050601
   Cheng L, 2009, AUTOMATICA, V45, P2312, DOI 10.1016/j.automatica.2009.06.007
   Cyr A, 2012, ADAPT BEHAV, V20, P257, DOI 10.1177/1059712312442231
   Floreano D, 2005, ARTIF LIFE, V11, P121, DOI 10.1162/1064546053278900
   Floreano D., 2001, LNCS, P38
   Gamez D, 2012, BIOINSPIR BIOMIM, V7, DOI 10.1088/1748-3182/7/2/025008
   Gamez D, 2010, CONSCIOUS COGN, V19, P294, DOI 10.1016/j.concog.2009.11.001
   Hagras H, 2004, IEEE INT CONF ROBOT, P4620, DOI 10.1109/ROBOT.2004.1302446
   Hou ZG, 2010, IEEE T SYST MAN CY B, V40, P1075, DOI 10.1109/TSMCB.2009.2034073
   J Li, P IFAC WORLD C 200 1, V16, P1270
   Kasabov Nikola, 2012, Artificial Neural Networks in Pattern Recognition. Proceedings of the 5th INNS IAPR TC 3 GIRPR Workshop, ANNPR 2012, P225, DOI 10.1007/978-3-642-33212-8_21
   Kasabov Nikola, 2012, Advances in Computational Intelligence. IEEE World Congress on Computational Intelligence (WCCI 2012). Plenary/Invited Lectures, P234, DOI 10.1007/978-3-642-30687-7_12
   Kasabov N, 2010, NEURAL NETWORKS, V23, P16, DOI 10.1016/j.neunet.2009.08.010
   Kasabov Nikola, 2009, Natural Computing, V8, P199, DOI 10.1007/s11047-008-9066-z
   Kempter R, 1999, PHYS REV E, V59, P4498, DOI 10.1103/PhysRevE.59.4498
   KUBOTA N, 2006, P 2006 IEEE WORLD C, P530
   Li J.G., 2005, THESIS CHINESE ACAD
   Li JN, 2005, LECT NOTES COMPUT SC, V3496, P640
   LOW KH, 2002, P IEEE INT C ROB AUT, P3869
   Luque NR, 2011, INT J NEURAL SYST, V21, P385, DOI 10.1142/S0129065711002900
   Luque N.R., IEEE T NEURAL NETW, V22
   MAASS W., 1999, PULSED NEURAL NETWOR
   Mohareri O, 2012, NEUROCOMPUTING, V88, P54, DOI 10.1016/j.neucom.2011.06.035
   Na YK, 2003, AUTON ROBOT, V15, P193, DOI 10.1023/A:1025597227189
   Natschlager T, 1998, NETWORK-COMP NEURAL, V9, P319, DOI 10.1088/0954-898X/9/3/003
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Luque NR, 2011, IEEE T SYST MAN CY B, V41, P1299, DOI 10.1109/TSMCB.2011.2138693
   Rossomando FG, 2011, CONTROL ENG PRACT, V19, P215, DOI 10.1016/j.conengprac.2010.11.011
   RUF B, 1998, THESIS TU GRAZ AUSTR
   Schliebs S, 2009, NEURAL NETWORKS, V22, P623, DOI 10.1016/j.neunet.2009.06.038
   Soula H., 2005, P AM ASS ART INT AAA, P1
   Vreeken Jilles, 2002, UUCS2003008
   Wang X-L, 2007, THESIS
   Wang XQ, 2008, NEUROCOMPUTING, V71, P655, DOI 10.1016/j.neucom.2007.08.025
   Wang XQ, 2012, LECT NOTES COMPUT SC, V7666, P652, DOI 10.1007/978-3-642-34478-7_79
   Wang XQ, 2009, 2009 INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND COMPUTATIONAL INTELLIGENCE, VOL I, PROCEEDINGS, P194, DOI 10.1109/AICI.2009.448
   Wang XQ, 2008, ICNC 2008: FOURTH INTERNATIONAL CONFERENCE ON NATURAL COMPUTATION, VOL 4, PROCEEDINGS, P125, DOI 10.1109/ICNC.2008.718
   Wang YJ, 2005, J COMPUT SCI TECH-CH, V20, P774, DOI 10.1007/s11390-005-0774-x
   Wysoski SG, 2008, NEUROCOMPUTING, V71, P2563, DOI 10.1016/j.neucom.2007.12.038
   Xiuqing Wang, 2011, 2011 10th IEEE International Conference on Cognitive Informatics & Cognitive Computing (ICCI-CC 2011), P348, DOI 10.1109/COGINF.2011.6016164
   Xu Wang, 2010, 2010 IEEE Conference on Robotics, Automation and Mechatronics (RAM 2010), P447, DOI 10.1109/RAMECH.2010.5513153
   Ye J, 2008, NEUROCOMPUTING, V71, P3373, DOI 10.1016/j.neucom.2007.11.005
   Ye J, 2008, NEUROCOMPUTING, V71, P1561, DOI 10.1016/j.neucom.2007.04.014
NR 51
TC 24
Z9 27
U1 1
U2 36
PD JUN 25
PY 2014
VL 134
SI SI
BP 230
EP 238
DI 10.1016/j.neucom.2013.07.055
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT C
AU Kasabov, N
   Dhoble, K
   Nuntalid, N
   Mohemmed, A
AF Kasabov, Nikola
   Dhoble, Kshitij
   Nuntalid, Nuttapod
   Mohemmed, Ammar
BE Lu, BL
   Zhang, LQ
   Kwok, J
TI Evolving Probabilistic Spiking Neural Networks for Spatio-temporal
   Pattern Recognition: A Preliminary Study on Moving Object Recognition
SO NEURAL INFORMATION PROCESSING, PT III
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 18th International Conference on Neural Information Processing (ICONIP
   2011)
CY NOV 13-17, 2011
CL Shanghai, PEOPLES R CHINA
DE Spatio-Temporal Patterns; Spiking Neural Network; Reservoir Computing;
   Liquid State Machine
ID SPEECH RECOGNITION; MODELS
AB This paper proposes a novel architecture for continuous spatio-temporal data modeling and pattern recognition utilizing evolving probabilistic spiking neural network 'reservoirs' (epSNNr). The paper demonstrates on a simple experimental data for moving object recognition that: (1) The epSNNr approach is more accurate and flexible than using standard SNN; (2) The use of probabilistic neuronal models is superior in several aspects when compared with the traditional deterministic SNN models, including a better performance on noisy data.
C1 [Kasabov, Nikola; Dhoble, Kshitij; Nuntalid, Nuttapod; Mohemmed, Ammar] Auckland Univ Technol, Knowledge Engn & Discovery Res Inst, Auckland 1010, New Zealand.
RP Kasabov, N (corresponding author), Auckland Univ Technol, Knowledge Engn & Discovery Res Inst, Private Bag 92006, Auckland 1010, New Zealand.
EM nkasabov@aut.ac.nz; kdhoble@aut.ac.nz; nnuntali@aut.ac.nz;
   amohemme@aut.ac.nz
CR Arel I, 2010, IEEE COMPUT INTELL M, V5, P13, DOI 10.1109/MCI.2010.938364
   Bellas F, 2010, IEEE T AUTON MENT DE, V2, P340, DOI 10.1109/TAMD.2010.2086453
   Gerstner W., 2002, SPIKING NEURON MODEL
   Goodman DFM, 2009, FRONT NEUROSCI-SWITZ, V3, P192, DOI 10.3389/neuro.01.026.2009
   Hamed H. N. A., 2010, AUSTR J INTELLIGENT, V11
   INDIVERI G, 2010, P IEEE INT S CIRC SY, P1951
   Indiveri G, 2009, COGN COMPUT, V1, P119, DOI 10.1007/s12559-008-9003-6
   Kasabov N., 2011, IEEE T AUTONOMOUS ME
   Kasabov N., 2007, EVOLVING CONNECTIONI
   Kasabov N, 2010, NEURAL NETWORKS, V23, P16, DOI 10.1016/j.neunet.2009.08.010
   Legenstein R, 2005, NEURAL COMPUT, V17, P2337, DOI 10.1162/0899766054796888
   Maass W, 2002, NEURAL NETWORKS, V15, P155, DOI 10.1016/S0893-6080(01)00144-7
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Maass W., 1999, PULSED NEURAL NETWOR, P157
   Natschläger T, 2002, THEOR COMPUT SCI, V287, P251, DOI 10.1016/S0304-3975(02)00099-3
   Norton D, 2009, IEEE IJCNN, P544
   Poppe R, 2010, IMAGE VISION COMPUT, V28, P976, DOI 10.1016/j.imavis.2009.11.014
   RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626
   Rutishauser U., 2011, NEURAL COMPUT, P1
   Schliebs S, 2010, LECT NOTES COMPUT SC, V6443, P163, DOI 10.1007/978-3-642-17537-4_21
   Schliebs S, 2010, INT J NEURAL SYST, V20, P481, DOI 10.1142/S0129065710002565
   Trentin E, 2001, NEUROCOMPUTING, V37, P91, DOI 10.1016/S0925-2312(00)00308-8
   Turaga P, 2008, IEEE T CIRC SYST VID, V18, P1473, DOI 10.1109/TCSVT.2008.2005594
   Verstraeten D, 2007, NEURAL NETWORKS, V20, P391, DOI 10.1016/j.neunet.2007.04.003
   Wysoski SG, 2010, NEURAL NETWORKS, V23, P819, DOI 10.1016/j.neunet.2010.04.009
NR 25
TC 4
Z9 4
U1 0
U2 3
PY 2011
VL 7064
BP 230
EP 239
PN III
WC Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Computer Science, Theory & Methods
DA 2023-11-11
ER

PT J
AU Xiang, SY
   Ren, ZX
   Zhang, YH
   Song, ZW
   Guo, XX
   Han, GQ
   Hao, Y
AF Xiang, Shuiying
   Ren, Zhenxing
   Zhang, Yahui
   Song, Ziwei
   Guo, Xingxing
   Han, Genquan
   Hao, Yue
TI Training a Multi-Layer Photonic Spiking Neural Network With Modified
   Supervised Learning Algorithm Based on Photonic STDP
SO IEEE JOURNAL OF SELECTED TOPICS IN QUANTUM ELECTRONICS
DT Article
DE Photonic spiking neural network; vertical-cavity surface-emitting
   lasers; multi-layer spiking neural network; spike timing dependent
   plasticity; supervised learning
ID TIMING-DEPENDENT PLASTICITY; NEUROMORPHIC PHOTONICS; INHIBITORY
   DYNAMICS; NEURONS; CLASSIFICATION; IMPLEMENTATION; INTELLIGENCE; VCSELS;
   GAME; GO
AB We propose a framework for hardware architecture and learning algorithm co-design of multi-layer photonic spiking neural network (SNN). The vertical-cavity surface-emitting laser with an embedded saturable absorber (VCSEL-SA) which contains two polarization-resolved modes is employed as a spiking neuron. The connection between two identical polarization modes is considered as the excitatory synapse, whereas the connection between two orthogonal polarization modes is regarded as the inhibitory synapse. The physical model of the photonic spiking neuron is derived based on the combination of spin-flip model and Yamada model. The photonic spike timing dependent plasticity (STDP) is applied to design a hardware-friendly biologically plausible supervised learning algorithm for a multi-layer photonic SNN. Thanks to the polarization mode competition effect in the VCSEL-SA, the proposed neuromorphic network is capable of solving the classical XOR problem. The effect of physical parameters of photonic neuron on the training convergence is also considered. We further extend the multi-layer photonic SNN to realize other logic learning tasks. To the best of our knowledge, such a modified supervised learning algorithm dedicated for a multi-layer photonic SNN has not yet been reported, which is interesting for spiking learning of neuromorphic photonics.
C1 [Xiang, Shuiying; Ren, Zhenxing; Zhang, Yahui; Song, Ziwei; Guo, Xingxing] Xidian Univ, State Key Lab Integrated Serv Networks, Xian 710071, Peoples R China.
   [Xiang, Shuiying; Han, Genquan; Hao, Yue] Xidian Univ, Sch Microelect, State Key Discipline Lab Wide Bandgap Semicond Te, Xian 710071, Peoples R China.
RP Xiang, SY (corresponding author), Xidian Univ, State Key Lab Integrated Serv Networks, Xian 710071, Peoples R China.
EM jxxsy@126.com; 584401206@qq.com; 18332551054@163.com; 1064971297@qq.com;
   1966461830@qq.com; gqhan@xidian.edu.cn; yhao@xidian.edu.cn
CR Abbott LF, 2000, NAT NEUROSCI, V3, P1178, DOI 10.1038/81453
   Bi GQ, 2001, ANNU REV NEUROSCI, V24, P139, DOI 10.1146/annurev.neuro.24.1.139
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Boybat I, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-04933-y
   Chakraborty I, 2019, PHYS REV APPL, V11, DOI 10.1103/PhysRevApplied.11.014063
   Cheng ZG, 2017, SCI ADV, V3, DOI 10.1126/sciadv.1700160
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   de Lima TF, 2019, J LIGHTWAVE TECHNOL, V37, P1515, DOI 10.1109/JLT.2019.2903474
   Deng T, 2018, IEEE ACCESS, V6, P67951, DOI 10.1109/ACCESS.2018.2878940
   Deng T, 2017, IEEE J SEL TOP QUANT, V23, DOI 10.1109/JSTQE.2017.2685140
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Feldmann J, 2019, NATURE, V569, P208, DOI 10.1038/s41586-019-1157-8
   Fok MP, 2020, J LIGHTWAVE TECHNOL, V38, P5318, DOI 10.1109/JLT.2020.2993292
   Fok MP, 2013, OPT LETT, V38, P419, DOI 10.1364/OL.38.000419
   Gautrais J, 1998, BIOSYSTEMS, V48, P57, DOI 10.1016/S0303-2647(98)00050-1
   George JK, 2019, OPT EXPRESS, V27, P5181, DOI 10.1364/OE.27.005181
   Gollisch T, 2008, SCIENCE, V319, P1108, DOI 10.1126/science.1149639
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Guyonneau R, 2005, NEURAL COMPUT, V17, P859, DOI 10.1162/0899766053429390
   Hejda M., 2020, OFC
   Hurtado A, 2012, APPL PHYS LETT, V100, DOI 10.1063/1.3692726
   Hurtado A, 2010, OPT EXPRESS, V18, P25170, DOI 10.1364/OE.18.025170
   Jordan MI, 2015, SCIENCE, V349, P255, DOI 10.1126/science.aaa8415
   Li C, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-04484-2
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   MAINEN ZF, 1995, SCIENCE, V268, P1503, DOI 10.1126/science.7770778
   Masquelier T, 2008, PLOS ONE, V3, DOI 10.1371/journal.pone.0001377
   MEAD C, 1990, P IEEE, V78, P1629, DOI 10.1109/5.58356
   Mohemmed A, 2012, INT J NEURAL SYST, V22, DOI 10.1142/S0129065712500128
   Nahmias MA, 2013, IEEE J SEL TOP QUANT, V19, DOI 10.1109/JSTQE.2013.2257700
   Nawrocki RA, 2016, IEEE T ELECTRON DEV, V63, P3819, DOI 10.1109/TED.2016.2598413
   Nguyen THO, 2017, CLIN TRANSL IMMUNOL, V6, DOI 10.1038/cti.2017.4
   Nishitani Y, 2015, IEEE T NEUR NET LEAR, V26, P2999, DOI 10.1109/TNNLS.2015.2399491
   Pammi VA, 2020, IEEE J SEL TOP QUANT, V26, DOI 10.1109/JSTQE.2019.2929187
   Pei J, 2019, NATURE, V572, P106, DOI 10.1038/s41586-019-1424-8
   Peng HT, 2020, IEEE J SEL TOP QUANT, V26, DOI 10.1109/JSTQE.2019.2927582
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Prucnal PR, 2016, ADV OPT PHOTONICS, V8, P228, DOI 10.1364/AOP.8.000228
   Ren QS, 2015, OPT EXPRESS, V23, P25247, DOI 10.1364/OE.23.025247
   Ríos C, 2019, SCI ADV, V5, DOI 10.1126/sciadv.aau5759
   Roberts A, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-57849-3
   Robertson J, 2020, IEEE J SEL TOP QUANT, V26, DOI 10.1109/JSTQE.2019.2931215
   Robertson J, 2017, OPT LETT, V42, P1560, DOI 10.1364/OL.42.001560
   Roy K, 2019, NATURE, V575, P607, DOI 10.1038/s41586-019-1677-2
   SANMIGUEL M, 1995, PHYS REV A, V52, P1728, DOI 10.1103/PhysRevA.52.1728
   Schuman C.D., 2017, SURVEY NEUROMORPHIC, Vabs/1705.06963
   Scirè A, 2002, OPT LETT, V27, P391, DOI 10.1364/OL.27.000391
   Selmi F, 2014, PHYS REV LETT, V112, DOI 10.1103/PhysRevLett.112.183902
   Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Song ZW, 2020, IEEE J SEL TOP QUANT, V26, DOI 10.1109/JSTQE.2020.2975564
   Sporea I, 2013, NEURAL COMPUT, V25, P473, DOI 10.1162/NECO_a_00396
   Taherkhani A, 2020, NEURAL NETWORKS, V122, P253, DOI 10.1016/j.neunet.2019.09.036
   Tait AN, 2019, PHYS REV APPL, V11, DOI 10.1103/PhysRevApplied.11.064043
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Toole R, 2016, J LIGHTWAVE TECHNOL, V34, P470, DOI 10.1109/JLT.2015.2475275
   Toole R, 2015, OPT EXPRESS, V23, P16133, DOI 10.1364/OE.23.016133
   Wang ZR, 2018, NAT ELECTRON, V1, P137, DOI 10.1038/s41928-018-0023-2
   Xia QF, 2019, NAT MATER, V18, P309, DOI 10.1038/s41563-019-0291-x
   Xiang S., 2020, OFC
   Xiang SY, 2020, OPT LETT, V45, P1104, DOI 10.1364/OL.383942
   Xiang SY, 2019, IEEE J SEL TOP QUANT, V25, DOI 10.1109/JSTQE.2019.2911565
   Xiang SY, 2018, IEEE J QUANTUM ELECT, V54, DOI 10.1109/JQE.2018.2879484
   Xiang SY, 2018, J LIGHTWAVE TECHNOL, V36, P4227, DOI 10.1109/JLT.2018.2818195
   YAMADA M, 1993, IEEE J QUANTUM ELECT, V29, P1330, DOI 10.1109/3.236146
   Yu Q, 2016, IEEE T NEUR NET LEAR, V27, P621, DOI 10.1109/TNNLS.2015.2416771
   Zhang Y, 2019, OPT LETT, V44, P1548, DOI 10.1364/OL.44.001548
   Zhang YH, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-34537-x
   Zheng QH, 2016, RIV PUBL SER INNOV, V9, P1
NR 71
TC 22
Z9 23
U1 7
U2 249
PD MAR-APR
PY 2021
VL 27
IS 2
AR 7500109
DI 10.1109/JSTQE.2020.3005589
WC Engineering, Electrical & Electronic; Quantum Science & Technology;
   Optics; Physics, Applied
DA 2023-11-11
ER

PT C
AU Enríquez-Gaytán, J
   Gómez-Castañeda, F
   Moreno-Cadenas, JA
   Flores-Nava, LM
AF Enriquez-Gaytan, J.
   Gomez-Castaneda, F.
   Moreno-Cadenas, J. A.
   Flores-Nava, L. M.
GP IEEE
TI Experimental Spiking Neural Network: Solving the XOR Paradigm with
   Metaheuristics
SO 2018 15TH INTERNATIONAL CONFERENCE ON ELECTRICAL ENGINEERING, COMPUTING
   SCIENCE AND AUTOMATIC CONTROL (CCE)
DT Proceedings Paper
CT 15th International Conference on Electrical Engineering, Computing
   Science and Automatic Control (CCE)
CY SEP 05-07, 2018
CL Mexico City, MEXICO
DE Spiking neural network; Efficient artificial bee colony algorithm;
   Synapic conductance
AB This work presents a supervised training strategy applied to a biorealistic Spiking Neural Network (SNN) with feedforward 2-2-1 architecture. This network uses Izhikevich neurons with regular-spiking behavior. The input layer, which has 2 nodes, generates temporal pulse trains that pass through synaptic conductances. These conductances transform voltages into currents. The receiving currents by 2 hidden-neurons also generate voltage pulses into synaptic conductances towards the output neuron. Each synaptic conductance has 2-parallel Alpha functions, whose weighting factors are found by the Efficient Artificial Bee Colony Algorithm (EABC Algorithm). This is a variant of the Artificial Bee Colony Algorithm (ABC Algorithm). The efficacy of the EABC algorithm in this SNN is shown solving the XOR paradigm.
C1 [Enriquez-Gaytan, J.; Gomez-Castaneda, F.; Moreno-Cadenas, J. A.; Flores-Nava, L. M.] IPN, CINVESTAV, Elect Engn Dept, Mexico City, DF, Mexico.
RP Enríquez-Gaytán, J (corresponding author), IPN, CINVESTAV, Elect Engn Dept, Mexico City, DF, Mexico.
EM jenriquezg@cinvestav.mx; fgomez@cinvestav.mx; jmoreno@cinvestav.mx;
   lmflores@cinvestav.mx
CR [Anonymous], 2010, DYNAMICAL SYSTEMS NE
   Belatreche A, 2007, SOFT COMPUT, V11, P239, DOI [10.1007/s00500-006-0065-7, 10.1007/S00500-006-0065-7]
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Dayan P., 2005, THEORETICAL NEUROSCI
   Patel Subhash, 2017, WSEAS Transactions on Circuits and Systems, V16, P108
   Rowcliffe P, 2008, IEEE T NEURAL NETWOR, V19, P1626, DOI 10.1109/TNN.2008.2000999
   Siarry P, 2016, METAHEURISTICS
   Xie XR, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0150329
NR 8
TC 1
Z9 1
U1 0
U2 0
PY 2018
WC Automation & Control Systems; Computer Science, Theory & Methods;
   Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT C
AU de Oliveira-Neto, JR
   Belfort, FD
   Cavalcanti-Neto, R
   Ranhel, J
AF de Oliveira-Neto, Jose Rodrigues
   Belfort, Felipe Duque
   Cavalcanti-Neto, Rafael
   Ranhel, Joao
GP IEEE
TI Magnitude Comparison in Analog Spiking Neural Assemblies
SO PROCEEDINGS OF THE 2014 INTERNATIONAL JOINT CONFERENCE ON NEURAL
   NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUL 06-11, 2014
CL Beijing, PEOPLES R CHINA
ID OSCILLATIONS; NETWORKS; MODEL; CELL
AB Spiking neural systems can represent external stimuli and internal states by means of sets of neurons firing together, the so-called cell assemblies. Neural assembly computing (NAC) is an approach that investigates how spiking neural assemblies represent things and states of the world, how interaction among assemblies results in information processing, computation and behavior. Mainly, NAC deals with digital assemblies in which all-or-none cell members are firing. The notion of analog assemblies is introduced, describing sets of neurons that represent something proportionally to their driving stimuli. Interactions among digital and analog cell assemblies create a rich computational environment. In this paper a spiking neural network that compares the magnitude of two analog assemblies is presented.
C1 [de Oliveira-Neto, Jose Rodrigues; Belfort, Felipe Duque; Cavalcanti-Neto, Rafael; Ranhel, Joao] Univ Fed Pernambuco, UFPE DES, Ave Arquitetura SN,4th Floor, Recife, PE, Brazil.
RP de Oliveira-Neto, JR (corresponding author), Univ Fed Pernambuco, UFPE DES, Ave Arquitetura SN,4th Floor, Recife, PE, Brazil.
EM jranhel@ieee.org
CR Abeles M., 2009, SCHOLARPEDIA, V4, P1441, DOI [DOI 10.4249/SCH0LARPEDIA.1441.REVISI0N, 10.4249/scholarpedia.1441]
   [Anonymous], NEURAL NETWORKS CYBE
   [Anonymous], 2002, ORG BEHAV NEUROPSYCH
   BIENENSTOCK E, 1995, NETWORK-COMP NEURAL, V6, P179, DOI 10.1088/0954-898X/6/2/004
   Brette R, 2012, PLOS COMPUT BIOL, V8, DOI 10.1371/journal.pcbi.1002561
   Butts DA, 2007, NATURE, V449, P92, DOI [10.1038/nature06105, 10.1038/natureO6105]
   Buzsáki G, 2004, SCIENCE, V304, P1926, DOI 10.1126/science.1099745
   Buzsáki G, 2010, NEURON, V68, P362, DOI 10.1016/j.neuron.2010.09.023
   Engel AK, 2001, NAT REV NEUROSCI, V2, P704, DOI 10.1038/35094565
   Foldiak P., 2008, SCHOLARPEDIA, V3, P2984, DOI DOI 10.4249/SCHOLARPEDIA.2984
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Gross CG, 2002, NEUROSCIENTIST, V8, P512, DOI 10.1177/107385802237175
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Kandel ER., 2021, PRINCIPLES NEURAL SC, V6
   Konig P, 1996, TRENDS NEUROSCI, V19, P130, DOI 10.1016/S0166-2236(96)80019-1
   Paugam-Moisy H., 2010, HDB NATURAL COMPUTIN, V1, P1
   Ranhel J., 2013, P 1 BRICS COUNTR C B, V1, P1
   Ranhel J, 2012, IEEE T NEUR NET LEAR, V23, P916, DOI 10.1109/TNNLS.2012.2190421
   Rojas R., 1996, NEURAL NETWORKS, P149, DOI 10.1007/978-3-642-61068-4_7
   VanRullen R, 2005, TRENDS NEUROSCI, V28, P1, DOI 10.1016/j.tins.2004.10.010
   Wickens JR, 1997, BIOL CYBERN, V77, P351, DOI 10.1007/s004220050395
NR 22
TC 0
Z9 0
U1 0
U2 0
PY 2014
BP 3186
EP 3191
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic; Telecommunications
DA 2023-11-11
ER

PT C
AU Matsumoto, T
   Takase, H
   Kawanaka, H
   Tsuruoka, S
AF Matsumoto, Takashi
   Takase, Haruhiko
   Kawanaka, Hiroharu
   Tsuruoka, Shinji
GP IEEE
TI A Learning Method for SpikeProp without Redundant Spikes -Automatic
   Adjusting Delay of Connections-
SO 2015 IEEE 8TH INTERNATIONAL WORKSHOP ON COMPUTATIONAL INTELLIGENCE AND
   APPLICATIONS (IWCIA) PROCEEDINGS
DT Proceedings Paper
CT 8th IEEE International Workshop on Computational Intelligence and
   Applications (IWCIA)
CY NOV 06-07, 2015
CL Hiroshima, JAPAN
DE Spiking Neural Network; SpikeProp; Time Series Signal Processing
ID SPIKING NEURONS
AB SpikeProp, which is proposed by Booij, is a kind of spiking neural networks. It can learn the timing of output spikes, but cannot adjust the number of output spikes. Our research group has discussed the problem and proposed a learning method that can adjust both timing and number of spikes. However, its learning performance depends on the initial network structure (the number of hidden units, delay, the number of sub-connections, and so on). In this article, we discuss the problem, especially the dependency to delay. We proposed the method that removes sub-connections that have unnecessary delay during training. By the proposed method, we successed training more than 87% regardless of the number of initial delays.
C1 [Matsumoto, Takashi; Takase, Haruhiko; Kawanaka, Hiroharu] Mie Univ, Grad Sch Engn, Tsu, Mie, Japan.
   [Tsuruoka, Shinji] Mie Univ, Tsu, Mie, Japan.
RP Matsumoto, T (corresponding author), Mie Univ, Grad Sch Engn, Tsu, Mie, Japan.
CR [Anonymous], 1998, PULSED NEURAL NETWOR
   [Anonymous], 1986, CMUCS86126
   BENJAMIN S, 2004, P INT JOINT C NEUR N, P471
   Bohte Sander M., 2004, Natural Computing, V3, P195, DOI 10.1023/B:NACO.0000027755.02868.60
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Booij O, 2005, INFORM PROCESS LETT, V95, P552, DOI 10.1016/j.ipl.2005.05.023
   Matsumoto T., 2014, P 6 INT WORKSH REG I, P59
   Ponulak F., 2006, INT J APPL MATH COMP, V6, P101
NR 8
TC 2
Z9 2
U1 0
U2 1
PY 2015
BP 15
EP 19
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT C
AU Dabbous, A
   Ibrahim, A
   Valle, M
AF Dabbous, Ali
   Ibrahim, Ali
   Valle, Maurizio
BE Valle, M
   Lehmhus, D
   Gianoglio, C
   Ragusa, E
   Seminara, L
   Bosse, S
   Ibrahim, A
   Thoben, KD
TI Feed-Forward SNN for Touch Modality Prediction
SO ADVANCES IN SYSTEM-INTEGRATED INTELLIGENCE, SYSINT 2022
SE Lecture Notes in Networks and Systems
DT Proceedings Paper
CT 6th International Conference on System-Integrated Intelligence (SysInt)
CY SEP 07-09, 2022
CL Genova, ITALY
DE Tactile sensing system; Spiking neural network; Neuromorphic hardware;
   Brian2 simulator; Touch modality
ID SPIKE
AB Recently, Spiking Neural Networks (SNNs) have been considered as alternatives to the common deep neural networks (DNNs) when the energy efficiency has been targeted. The SNNs adopt an event-driven information processing approach in which the computational expenses are reduced considerably compared to DNNs without affecting the system performance. This paper presents an efficient framework based on SNNs for touch modality classification. The proposed work outperforms similar state of the art solutions by achieving a classification accuracy of 99.97% with decreased complexity and increased number of classes.
C1 [Dabbous, Ali; Ibrahim, Ali; Valle, Maurizio] Univ Genoa, Dept Elect Elect Telecommun Engn & Naval Architec, Genoa, Italy.
   [Ibrahim, Ali] Lebanese Int Univ LIU, Dept Elect & Elect Engn, Beirut 1105, Lebanon.
RP Ibrahim, A; Valle, M (corresponding author), Univ Genoa, Dept Elect Elect Telecommun Engn & Naval Architec, Genoa, Italy.; Ibrahim, A (corresponding author), Lebanese Int Univ LIU, Dept Elect & Elect Engn, Beirut 1105, Lebanon.
EM AliDabbous@edu.unige.it; ali.ibrahim@liu.edu.lb; maurizio.valle@unige.it
CR Bologna LL, 2011, J PHYSIOL-PARIS, V105, P25, DOI 10.1016/j.jphysparis.2011.08.002
   Dabbous A, 2021, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS51556.2021.9401749
   Dabbous A., 2021 28 IEEE INT C E, P1
   Dahiya RS, 2010, IEEE T ROBOT, V26, P1, DOI 10.1109/TRO.2009.2033627
   Decherchi S, 2011, IEEE T ROBOT, V27, P635, DOI 10.1109/TRO.2011.2130030
   Gianoglio C., 2021, 2021 28 IEEE INT C E, P1
   Gianoglio C, 2022, IEEE SENS J, V22, P659, DOI 10.1109/JSEN.2021.3129323
   Gütig R, 2014, CURR OPIN NEUROBIOL, V25, P134, DOI 10.1016/j.conb.2014.01.004
   Gütig R, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0053063
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Iwata H, 2005, IEEE T IND ELECTRON, V52, P1468, DOI 10.1109/TIE.2005.858739
   Kim J, 2020, IEEE SYS MAN CYBERN, P178, DOI [10.1109/SMC42975.2020.9283337, 10.1109/smc42975.2020.9283337]
   Legenstein R, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.1000180
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Nessler B., 2009, ADV NEURAL INFORM PR, P1357
   Rice KL, 2009, 2009 INTERNATIONAL CONFERENCE ON RECONFIGURABLE COMPUTING AND FPGAS, P451, DOI 10.1109/ReConFig.2009.77
   Schrauwen B, 2008, NEURAL NETWORKS, V21, P511, DOI 10.1016/j.neunet.2007.12.009
   Stiehl W. D., 2004, 2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566), P3015
   Stimberg M, 2019, ELIFE, V8, DOI 10.7554/eLife.47314
   Sun J., 2017, ROBOTIC SENSE TOUCH
   Talbot M, 2008, 2008 WORLD AUTOMATION CONGRESS PROCEEDINGS, VOLS 1-3, P433
   VanRullen R, 2005, TRENDS NEUROSCI, V28, P1, DOI 10.1016/j.tins.2004.10.010
NR 22
TC 0
Z9 0
U1 0
U2 3
PY 2023
VL 546
BP 215
EP 222
DI 10.1007/978-3-031-16281-7_21
WC Computer Science, Artificial Intelligence; Engineering,
   Multidisciplinary
DA 2023-11-11
ER

PT J
AU Jiang, WW
   Li, J
   Liu, HB
   Qian, XC
   Ge, Y
   Wang, LD
   Duan, SK
AF Jiang, Wenwu
   Li, Jie
   Liu, Hongbo
   Qian, Xicong
   Ge, Yuan
   Wang, Lidan
   Duan, Shukai
TI Memristor-based multi-synaptic spiking neuron circuit for spiking neural
   network
SO CHINESE PHYSICS B
DT Article
DE memristor; multi-synaptic circuit; spiking neuron; spiking neural
   network (SNN)
ID CMOS; MODEL
AB Spiking neural networks (SNNs) are widely used in many fields because they work closer to biological neurons. However, due to its computational complexity, many SNNs implementations are limited to computer programs. First, this paper proposes a multi-synaptic circuit (MSC) based on memristor, which realizes the multi-synapse connection between neurons and the multi-delay transmission of pulse signals. The synapse circuit participates in the calculation of the network while transmitting the pulse signal, and completes the complex calculations on the software with hardware. Secondly, a new spiking neuron circuit based on the leaky integrate-and-fire (LIF) model is designed in this paper. The amplitude and width of the pulse emitted by the spiking neuron circuit can be adjusted as required. The combination of spiking neuron circuit and MSC forms the multi-synaptic spiking neuron (MSSN). The MSSN was simulated in PSPICE and the expected result was obtained, which verified the feasibility of the circuit. Finally, a small SNN was designed based on the mathematical model of MSSN. After the SNN is trained and optimized, it obtains a good accuracy in the classification of the IRIS-dataset, which verifies the practicability of the design in the network.
C1 [Jiang, Wenwu; Li, Jie; Liu, Hongbo; Qian, Xicong; Ge, Yuan; Wang, Lidan; Duan, Shukai] Southwest Univ, Coll Artificial Intelligence, Chongqing 400715, Peoples R China.
   [Wang, Lidan; Duan, Shukai] Natl & Local Joint Engn Lab Intelligent Transmiss, Chongqing 400715, Peoples R China.
   [Wang, Lidan; Duan, Shukai] Brain Inspired Comp & Intelligent Control Chongqi, Chongqing 400715, Peoples R China.
   [Wang, Lidan; Duan, Shukai] Chongqing Brain Sci Collaborat Innovat Ctr, Chongqing 400715, Peoples R China.
RP Duan, SK (corresponding author), Southwest Univ, Coll Artificial Intelligence, Chongqing 400715, Peoples R China.; Duan, SK (corresponding author), Natl & Local Joint Engn Lab Intelligent Transmiss, Chongqing 400715, Peoples R China.; Duan, SK (corresponding author), Brain Inspired Comp & Intelligent Control Chongqi, Chongqing 400715, Peoples R China.; Duan, SK (corresponding author), Chongqing Brain Sci Collaborat Innovat Ctr, Chongqing 400715, Peoples R China.
EM duansk@swu.edu.cn
CR Afifi A, 2010, IEICE T FUND ELECTR, VE93A, P1670, DOI 10.1587/transfun.E93.A.1670
   Aleksander I, 2004, NATURE, V432, P18, DOI 10.1038/432018a
   Babacan Y, 2016, NEUROCOMPUTING, V203, P86, DOI 10.1016/j.neucom.2016.03.060
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Brette R, 2005, J NEUROPHYSIOL, V94, P3637, DOI 10.1152/jn.00686.2005
   Cai FX, 2019, NAT ELECTRON, V2, P290, DOI 10.1038/s41928-019-0270-x
   Chen YR, 2009, 2009 IEEE/ACM INTERNATIONAL SYMPOSIUM ON NANOSCALE ARCHITECTURES, P7, DOI 10.1109/NANOARCH.2009.5226363
   CHUA LO, 1971, IEEE T CIRCUITS SYST, VCT18, P507, DOI 10.1109/TCT.1971.1083337
   Cruz-Albrecht JM, 2012, IEEE T BIOMED CIRC S, V6, P246, DOI 10.1109/TBCAS.2011.2174152
   Fauth M, 2015, PLOS COMPUT BIOL, V11, DOI 10.1371/journal.pcbi.1004031
   Hajiabadi Z, 2021, J COMPUT ELECTRON, V20, P1625, DOI 10.1007/s10825-021-01719-2
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Howard G, 2012, IEEE T EVOLUT COMPUT, V16, P711, DOI 10.1109/TEVC.2011.2170199
   Hu M., 2018, ADV MATER, V30, DOI [10.1002/adma.v30.9, DOI 10.1002/ADMA.V30.9]
   Hu M, 2017, IEEE T COMPUT AID D, V36, P1353, DOI 10.1109/TCAD.2016.2618866
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Jiang WW, 2019, NAT ELECTRON, V2, P376, DOI 10.1038/s41928-019-0307-1
   Jo SH, 2010, NANO LETT, V10, P1297, DOI 10.1021/nl904092h
   Kim H, 2012, P IEEE, V100, P2061, DOI 10.1109/JPROC.2011.2166749
   Kim K, 2020, NPG ASIA MATER, V12, DOI 10.1038/s41427-020-00261-0
   Kim T, 2018, ELECTRON LETT, V54, P1022, DOI 10.1049/el.2018.5047
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Li C, 2019, NAT MACH INTELL, V1, P49, DOI 10.1038/s42256-018-0001-4
   Liu HJ, 2020, CHINESE PHYS B, V29, DOI 10.1088/1674-1056/ab65b5
   Liu YD., 2014, ACTA PHYS SIN, V63, DOI [10.7498/aps, DOI 10.7498/APS]
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Natschlager T, 1998, NETWORK-COMP NEURAL, V9, P319, DOI 10.1088/0954-898X/9/3/003
   Ohtani T, 2008, IEICE T FUND ELECTR, VE91A, P891, DOI 10.1093/ietfec/e91-a.3.891
   Roy K, 2019, NATURE, V575, P607, DOI 10.1038/s41586-019-1677-2
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Serb A, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms12611
   Shin HC, 2016, IEEE T MED IMAGING, V35, P1285, DOI 10.1109/TMI.2016.2528162
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Strukov DB, 2008, NATURE, V453, P80, DOI 10.1038/nature06932
   Woo S, 2020, IEEE T ELECTRON DEV, V67, P2995, DOI 10.1109/TED.2020.2995785
   Wu X., 2015, 2015 IEEE International Magnetics Conference (INTERMAG), DOI 10.1109/INTMAG.2015.7156718
   Yao P, 2020, NATURE, V577, P641, DOI 10.1038/s41586-020-1942-4
   Yao P, 2017, NAT COMMUN, V8, DOI 10.1038/ncomms15199
   Yong T., 2010, SYNAPSE, V41, P258, DOI [10.1002/(ISSN)1098-2396, DOI 10.1002/(ISSN)1098-2396]
   Zayer F, 2019, AEU-INT J ELECTRON C, V100, P56, DOI 10.1016/j.aeue.2019.01.003
   Zhao L, 2018, NEUROCOMPUTING, V314, P207, DOI 10.1016/j.neucom.2018.06.062
NR 41
TC 5
Z9 6
U1 14
U2 60
PD MAR 1
PY 2022
VL 31
IS 4
AR 040702
DI 10.1088/1674-1056/ac380b
WC Physics, Multidisciplinary
DA 2023-11-11
ER

PT J
AU Memmesheimer, RM
   Timme, M
AF Memmesheimer, Raoul-Martin
   Timme, Marc
TI Non-Additive Coupling Enables Propagation of Synchronous Spiking
   Activity in Purely Random Networks
SO PLOS COMPUTATIONAL BIOLOGY
DT Article
ID FIRE NEURON MODEL; COINCIDENCE DETECTION; DENDRITIC SPIKES; BASAL
   DENDRITES; SYNCHRONIZATION; PATTERNS; INPUT; DYNAMICS; COMPUTATION;
   STABILITY
AB Despite the current debate about the computational role of experimentally observed precise spike patterns it is still theoretically unclear under which conditions and how they may emerge in neural circuits. Here, we study spiking neural networks with non-additive dendritic interactions that were recently uncovered in single-neuron experiments. We show that supra-additive dendritic interactions enable the persistent propagation of synchronous activity already in purely random networks without superimposed structures and explain the mechanism underlying it. This study adds a novel perspective on the dynamics of networks with nonlinear interactions in general and presents a new viable mechanism for the occurrence of patterns of precisely timed spikes in recurrent networks.
C1 [Memmesheimer, Raoul-Martin] Radboud Univ Nijmegen, Dept Neuroinformat, NL-6525 ED Nijmegen, Netherlands.
   [Timme, Marc] Max Planck Inst Dynam & Self Org, Network Dynam Grp, Gottingen, Germany.
   [Timme, Marc] Bernstein Ctr Computat Neurosci, Gottingen, Germany.
RP Memmesheimer, RM (corresponding author), Radboud Univ Nijmegen, Dept Neuroinformat, NL-6525 ED Nijmegen, Netherlands.
EM r.memmesheimer@science.ru.nl
CR ABELES M, 1993, J NEUROPHYSIOL, V70, P1629, DOI 10.1152/jn.1993.70.4.1629
   Abeles M., 1982, LOCAL CORTICAL CIRCU, DOI DOI 10.1007/978-3-642-81708-3
   Amit DJ, 1997, CEREB CORTEX, V7, P237, DOI 10.1093/cercor/7.3.237
   [Anonymous], 1999, DENDRITES
   Ariav G, 2003, J NEUROSCI, V23, P7750
   Aviel Y, 2005, NEURAL COMPUT, V17, P691, DOI 10.1162/0899766053019962
   Bienenstock E, 1996, BRAIN THEORY BIOL BA
   Brunel N, 2000, J COMPUT NEUROSCI, V8, P183, DOI 10.1023/A:1008925309027
   Burkitt AN, 2006, BIOL CYBERN, V95, P97, DOI 10.1007/s00422-006-0082-8
   Burkitt AN, 2006, BIOL CYBERN, V95, P1, DOI 10.1007/s00422-006-0068-6
   Cash S, 1999, NEURON, V22, P383, DOI 10.1016/S0896-6273(00)81098-3
   Dayan P., 2001, THEORETICAL NEUROSCI
   Diesmann M, 1999, NATURE, V402, P529, DOI 10.1038/990101
   ERNST U, 1995, PHYS REV LETT, V74, P1570, DOI 10.1103/PhysRevLett.74.1570
   Feinerman O, 2006, J NEUROSCI, V26, P4526, DOI 10.1523/JNEUROSCI.4692-05.2006
   Gansel K, 2005, SOC NEUR ABSTR
   Gasparini S, 2006, J NEUROSCI, V26, P2088, DOI 10.1523/JNEUROSCI.4428-05.2006
   Gasparini S, 2004, J NEUROSCI, V24, P11046, DOI 10.1523/JNEUROSCI.2520-04.2004
   Gewaltig MO, 2001, NEURAL NETWORKS, V14, P657, DOI 10.1016/S0893-6080(01)00070-3
   Häusser M, 2000, SCIENCE, V290, P739, DOI 10.1126/science.290.5492.739
   Hayon G, 2003, J COMPUT NEUROSCI, V18, P41
   Hebb D. O., 1949, ORG BEHAV
   Helias M, 2010, PLOS COMPUT BIOL, V6, DOI 10.1371/journal.pcbi.1000929
   HERRMANN M, 1995, NETWORK-COMP NEURAL, V6, P403, DOI 10.1088/0954-898X/6/3/006
   Holmgren C, 2003, J PHYSIOL-LONDON, V551, P139, DOI 10.1113/jphysiol.2003.044784
   Ikegaya Y, 2004, SCIENCE, V304, P559, DOI 10.1126/science.1093173
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Jahnke S, 2008, PHYS REV LETT, V100, DOI 10.1103/PhysRevLett.100.048102
   Jin DZ, 2002, PHYS REV LETT, V89, DOI 10.1103/PhysRevLett.89.208102
   Johansson RS, 2004, NAT NEUROSCI, V7, P170, DOI 10.1038/nn1177
   Jolivet R, 2006, ADV NETWORK ELECTROP
   Katz Y, 2007, PLOS COMPUT BIOL, V3, P2432, DOI 10.1371/journal.pcbi.0030234
   Kumar A, 2008, J NEUROSCI, V28, P5268, DOI 10.1523/JNEUROSCI.2542-07.2008
   LESTIENNE R, 1987, BRAIN RES, V437, P214, DOI 10.1016/0006-8993(87)91638-6
   London M, 2005, ANNU REV NEUROSCI, V28, P503, DOI 10.1146/annurev.neuro.28.061604.135703
   Long MA, 2010, NATURE, V468, P394, DOI 10.1038/nature09514
   Mehring C, 2003, BIOL CYBERN, V88, P395, DOI 10.1007/s00422-002-0384-4
   MEL BW, 1992, ADV NEUR IN, V4, P35
   Memmesheimer R, 2008, THESIS GEORG AUGUST
   Memmesheimer RM, 2006, PHYS REV LETT, V97, DOI 10.1103/PhysRevLett.97.188101
   Memmesheimer RM, 2006, PHYSICA D, V224, P182, DOI 10.1016/j.physd.2006.09.037
   Memmesheimer RM, 2010, P NATL ACAD SCI USA, V107, P11092, DOI 10.1073/pnas.0909615107
   Milo R, 2002, SCIENCE, V298, P824, DOI 10.1126/science.298.5594.824
   MIROLLO RE, 1990, SIAM J APPL MATH, V50, P1645, DOI 10.1137/0150098
   Mokeichev A, 2007, NEURON, V53, P413, DOI 10.1016/j.neuron.2007.01.017
   Morita K, 2008, J NEUROSCI, V28, P7699, DOI 10.1523/JNEUROSCI.0059-08.2008
   Nevian T, 2007, NAT NEUROSCI, V10, P206, DOI 10.1038/nn1826
   Nordlie E, 2009, PLOS COMPUT BIOL, V5, DOI 10.1371/journal.pcbi.1000456
   Nowak LG, 1997, CEREB CORTEX, V7, P487, DOI 10.1093/cercor/7.6.487
   Oram MW, 1999, J NEUROPHYSIOL, V81, P3021, DOI 10.1152/jn.1999.81.6.3021
   Pipa G, 2007, NEUROCOMPUTING, V70, P2064, DOI 10.1016/j.neucom.2006.10.142
   Poirazi P, 2003, NEURON, V37, P989, DOI 10.1016/S0896-6273(03)00149-1
   Polsky A, 2004, NAT NEUROSCI, V7, P621, DOI 10.1038/nn1253
   Poznanski Roman R, 2002, J Integr Neurosci, V1, P69, DOI 10.1142/S0219635202000050
   Prut Y, 1998, J NEUROPHYSIOL, V79, P2857, DOI 10.1152/jn.1998.79.6.2857
   Rauch A, 2003, J NEUROPHYSIOL, V90, P1598, DOI 10.1152/jn.00293.2003
   Rhodes PA, 2008, NEURAL COMPUT, V20, P2000, DOI 10.1162/neco.2008.04-07-511
   Riehle A, 1997, SCIENCE, V278, P1950, DOI 10.1126/science.278.5345.1950
   Roxin A, 2005, PHYS REV LETT, V94, DOI 10.1103/PhysRevLett.94.238103
   Singer W, 2004, FUNCTIONAL NEUROIMAG
   SOFTKY W, 1994, NEUROSCIENCE, V58, P13, DOI 10.1016/0306-4522(94)90154-6
   Song S, 2005, PLOS BIOL, V3, P507, DOI 10.1371/journal.pbio.0030068
   Sporns O, 2004, PLOS BIOL, V2, P1910, DOI 10.1371/journal.pbio.0020369
   Spruston N, 2002, DENDRITES
   Tetzlaff T, 2004, NEUROCOMPUTING, V58, P117, DOI 10.1016/j.neucom.2004.01.031
   Timme M, 2002, PHYS REV LETT, V89, DOI 10.1103/PhysRevLett.89.258701
   Timme M, 2002, THESIS GEORG AUGUST
   TRAUB RD, 1982, SCIENCE, V216, P745, DOI 10.1126/science.7079735
   Urban NN, 1998, P NATL ACAD SCI USA, V95, P11450, DOI 10.1073/pnas.95.19.11450
   van Rossum MCW, 2002, J NEUROSCI, V22, P1956, DOI 10.1523/JNEUROSCI.22-05-01956.2002
   vanVreeswijk C, 1996, SCIENCE, V274, P1724, DOI 10.1126/science.274.5293.1724
   Vogels TP, 2005, J NEUROSCI, V25, P10786, DOI 10.1523/JNEUROSCI.3508-05.2005
   Wang QY, 2011, PLOS ONE, V6, DOI [10.1371/journal.pone.0015851, 10.1371/journal.pone.0021006]
   Wang QY, 2009, PHYS REV E, V80, DOI 10.1103/PhysRevE.80.026206
   Wang XJ, 1999, J NEUROSCI, V19, P9587
NR 75
TC 14
Z9 14
U1 0
U2 12
PD APR
PY 2012
VL 8
IS 4
AR e1002384
DI 10.1371/journal.pcbi.1002384
WC Biochemical Research Methods; Mathematical & Computational Biology
DA 2023-11-11
ER

PT J
AU Liu, SQ
   Fan, T
   Lu, QS
AF Liu, SQ
   Fan, T
   Lu, QS
TI The spike order of the winnerless competition (WLC) model and its
   application to the inhibition neural system
SO INTERNATIONAL JOURNAL OF NONLINEAR SCIENCES AND NUMERICAL SIMULATION
DT Article
DE neural system; spike; inhibition
ID NETWORKS
AB The paper studies the relationship between the electric spike order of the winnerless competition (WLC) model and the inhibition direction of the inhibitory neural system. The numerical results show the spike order of the neuron is opposite to the inhibition direction of the network. This spike order property connects the WLC model with any inhibitory structure together. With the help of WLC model this paper analyzes the spatial patterns of olfactory cortex and explains the relationship of the external stimulus to the spatial patterns. The pattern displayed here can be seen in the experiment of rabbit olfactory cortex.
C1 S China Univ Technol, Dept Appl Math, Guangzhou 510640, Peoples R China.
   BeiHang Univ Aeronaut & Astronaut, Dept Appl Math, Beijing 100083, Peoples R China.
RP Liu, SQ (corresponding author), S China Univ Technol, Dept Appl Math, Guangzhou 510640, Peoples R China.
EM mashqliu@scut.edu.cn; qishaolu@hotmail.com
CR Carleton A, 2002, J PHYSIOLOGY-PARIS, V96, P115, DOI 10.1016/S0928-4257(01)00087-0
   FREEMAN W, 2000, J NEURODYNAMICS EXPL
   Freeman W. J., 1992, International Journal of Bifurcation and Chaos in Applied Sciences and Engineering, V2, P451, DOI 10.1142/S0218127492000653
   GUO AK, 2000, COMPUTATIONAL NEUROS
   HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554
   LIU SQ, 2004, ACTA BIOPHYSICA SINI, V20, P21
   Liu ZG, 2004, INT J NONLIN SCI NUM, V5, P355
   Ma SQ, 2005, INT J NONLIN SCI NUM, V6, P13
   Marder E, 1996, PHYSIOL REV, V76, P687, DOI 10.1152/physrev.1996.76.3.687
   Mori K, 1999, SCIENCE, V286, P711, DOI 10.1126/science.286.5440.711
   NICHOLLS JG, 2002, NEURON BRAIN
   Rabinovich M, 2001, PHYS REV LETT, V87, DOI 10.1103/PhysRevLett.87.068102
   SEJNOWSKI TJ, 1995, NATURE, V376, P21, DOI 10.1038/376021a0
   So P, 1998, BIOPHYS J, V74, P2776, DOI 10.1016/S0006-3495(98)77985-8
   Wang HX, 2005, INT J NONLIN SCI NUM, V6, P7
   Yang ZQ, 2005, INT J NONLIN SCI NUM, V6, P1
NR 16
TC 14
Z9 14
U1 0
U2 3
PY 2005
VL 6
IS 2
BP 133
EP 138
WC Engineering, Multidisciplinary; Mathematics, Applied; Mechanics;
   Physics, Mathematical
DA 2023-11-11
ER

PT J
AU Izhikevich, EM
AF Izhikevich, Eugene M.
TI Polychronization: Computation with spikes
SO NEURAL COMPUTATION
DT Article
ID NEURONAL AVALANCHES; CONDUCTION-VELOCITY; IMPULSE CONDUCTION; NEURAL
   ASSEMBLIES; ACTIVITY PATTERNS; EFFERENT NEURONS; CORTICAL-NEURONS;
   NETWORKS; MODEL; CONNECTIVITY
AB We present a minimal spiking network that can polychronize, that is, exhibit reproducible time-locked but not synchronous firing patterns with millisecond precision, as in synfire braids. The network consists of cortical spiking neurons with axonal conduction delays and spike-timing-dependent plasticity (STDP); a ready-to-use MATLAB code is included. It exhibits sleeplike oscillations, gamma (40 Hz) rhythms, conversion of firing rates to spike timings, and other interesting regimes. Due to the interplay between the delays and STDP, the spiking neurons spontaneously self-organize into groups and generate patterns of stereotypical polychronous activity. To our surprise, the number of coexisting polychronous groups far exceeds the number of neurons in the network, resulting in an unprecedented memory capacity of the system. We speculate on the significance of polychrony to the theory of neuronal group selection (TNGS, neural Darwinism), cognitive neural computations, binding and gamma rhythm, mechanisms of attention, and consciousness as "attention to memories.".
C1 Inst Neurosci, San Diego, CA 92121 USA.
RP Izhikevich, EM (corresponding author), Inst Neurosci, 10640 John Jay Hopkins Dr, San Diego, CA 92121 USA.
EM Eugene.Izhikevich@nsi.edu
CR Abeles M., 1991, CORTICONICS NEURAL C
   ABELES M, 2002, HDB BRAIN THEORY NEU, P1143
   Amit DJ, 1997, CEREB CORTEX, V7, P237, DOI 10.1093/cercor/7.3.237
   Baker SN, 2000, J NEUROPHYSIOL, V84, P1770, DOI 10.1152/jn.2000.84.4.1770
   Beggs JM, 2004, J NEUROSCI, V24, P5216, DOI 10.1523/JNEUROSCI.0540-04.2004
   Beggs JM, 2003, J NEUROSCI, V23, P11167
   Bellen A, 2003, NUMERICAL METHODS DE
   BIENENSTOCK E, 1995, NETWORK-COMP NEURAL, V6, P179, DOI 10.1088/0954-898X/6/2/004
   Braitenberg V., 1991, ANATOMY CORTEX STAT
   BRYANT HL, 1976, J PHYSIOL-LONDON, V260, P279, DOI 10.1113/jphysiol.1976.sp011516
   Buzsaki G., 1994, TEMPORAL CODING BRAI
   Chang EY, 2000, J NEUROPHYSIOL, V84, P1136, DOI 10.1152/jn.2000.84.3.1136
   CHANGEUX JP, 1976, COGNITION, V33, P25
   Desai NS, 2002, NAT NEUROSCI, V5, P783, DOI 10.1038/nn878
   Diesmann M, 1999, NATURE, V402, P529, DOI 10.1038/990101
   Edelman G. M, 2004, WIDER SKY PHENOMENAL
   EDELMAN GM, 1993, NEURON, V10, P115, DOI 10.1016/0896-6273(93)90304-A
   Edelman GM, 2001, P NATL ACAD SCI USA, V98, P13763, DOI 10.1073/pnas.231499798
   Edelman GM., 1987, NEURAL DARWINISM THE
   Eurich CW, 1999, PHYS REV LETT, V82, P1594, DOI 10.1103/PhysRevLett.82.1594
   FERSTER D, 1983, J PHYSIOL-LONDON, V342, P181, DOI 10.1113/jphysiol.1983.sp014846
   Foss J, 2000, J NEUROPHYSIOL, V84, P975, DOI 10.1152/jn.2000.84.2.975
   Gerstner W, 1996, NATURE, V383, P76, DOI 10.1038/383076a0
   GROSSBERG S, 1988, NEURAL NETWORKS, V1, P17, DOI 10.1016/0893-6080(88)90021-4
   HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   HOPPENSTEADT FC, 1997, WEEKLY CONNECTED NEU
   Huning H, 1998, NEURAL COMPUT, V10, P555, DOI 10.1162/089976698300017665
   Ikegaya Y, 2004, SCIENCE, V304, P559, DOI 10.1126/science.1093173
   Izhikevich E M, 2007, DYNAM SYST, DOI [DOI 10.1017/S0143385704000173, DOI 10.7551/MITPRESS/2526.001.0001]
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Krichmar JL, 2002, CEREB CORTEX, V12, P818, DOI 10.1093/cercor/12.8.818
   Lindsey BG, 1997, J NEUROPHYSIOL, V78, P1714, DOI 10.1152/jn.1997.78.3.1714
   Litvak V, 2003, J NEUROSCI, V23, P3006
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   MAINEN ZF, 1995, SCIENCE, V268, P1503, DOI 10.1126/science.7770778
   Mao BQ, 2001, NEURON, V32, P883, DOI 10.1016/S0896-6273(01)00518-9
   Mazurek ME, 2002, NAT NEUROSCI, V5, P463, DOI 10.1038/nn836
   Miller R, 1996, BIOL CYBERN, V75, P263, DOI 10.1007/s004220050293
   Miller R, 1996, BIOL CYBERN, V75, P253, DOI 10.1007/s004220050292
   Oram MW, 1999, J NEUROPHYSIOL, V81, P3021, DOI 10.1152/jn.1999.81.6.3021
   Prut Y, 1998, J NEUROPHYSIOL, V79, P2857, DOI 10.1152/jn.1998.79.6.2857
   Reinagel P, 2002, J NEUROSCI, V22, P6837
   Riehle A, 1997, SCIENCE, V278, P1950, DOI 10.1126/science.278.5345.1950
   Salami M, 2003, P NATL ACAD SCI USA, V100, P6174, DOI 10.1073/pnas.0937380100
   Senn W, 2002, NEURAL COMPUT, V14, P583, DOI 10.1162/089976602317250915
   Seth A.K., 2004, ANIMALS ANIMATS
   Seth AK, 2004, CEREB CORTEX, V14, P1185, DOI 10.1093/cercor/bhh079
   Shadlen Michael N., 1994, Current Opinion in Neurobiology, V4, P569, DOI 10.1016/0959-4388(94)90059-0
   Shadlen MN, 1999, NEURON, V24, P67, DOI 10.1016/S0896-6273(00)80822-3
   Shadlen MN, 1998, J NEUROSCI, V18, P3870
   SINGER W, 1995, ANNU REV NEUROSCI, V18, P555, DOI 10.1146/annurev.neuro.18.1.555
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Stewart I, 2003, SIAM J APPL DYN SYST, V2, P609, DOI 10.1137/S1111111103419896
   STREHLER BL, 1986, P NATL ACAD SCI USA, V83, P9812, DOI 10.1073/pnas.83.24.9812
   SWADLOW HA, 1988, J NEUROPHYSIOL, V59, P1162, DOI 10.1152/jn.1988.59.4.1162
   SWADLOW HA, 1980, ANNU REV BIOPHYS BIO, V9, P143, DOI 10.1146/annurev.bb.09.060180.001043
   SWADLOW HA, 1975, P NATL ACAD SCI USA, V72, P5156, DOI 10.1073/pnas.72.12.5156
   SWADLOW HA, 1994, J NEUROPHYSIOL, V71, P437, DOI 10.1152/jn.1994.71.2.437
   SWADLOW HA, 1992, J NEUROPHYSIOL, V68, P605, DOI 10.1152/jn.1992.68.2.605
   SWADLOW HA, 1974, EXP NEUROL, V43, P445, DOI 10.1016/0014-4886(74)90183-6
   SWADLOW HA, 1985, J NEUROPHYSIOL, V54, P1346, DOI 10.1152/jn.1985.54.5.1346
   Tetko IV, 2001, J NEUROSCI METH, V105, P15, DOI 10.1016/S0165-0270(00)00337-X
   Turrigiano GG, 1998, NATURE, V391, P892, DOI 10.1038/36103
   vanSteveninck RRD, 1997, SCIENCE, V275, P1805, DOI 10.1126/science.275.5307.1805
   vanVreeswijk C, 1996, SCIENCE, V274, P1724, DOI 10.1126/science.274.5293.1724
   Villa AEP, 1999, P NATL ACAD SCI USA, V96, P1106, DOI 10.1073/pnas.96.3.1106
   Volman V, 2005, PHYS BIOL, V2, P98, DOI 10.1088/1478-3975/2/2/003
   Whittington MA, 2000, INT J PSYCHOPHYSIOL, V38, P315, DOI 10.1016/S0167-8760(00)00173-2
   Wiener J., 1992, ORDINARY DELAY DIFFE
NR 71
TC 697
Z9 796
U1 19
U2 104
PD FEB
PY 2006
VL 18
IS 2
BP 245
EP 282
DI 10.1162/089976606775093882
WC Computer Science, Artificial Intelligence; Neurosciences
DA 2023-11-11
ER

PT J
AU Gangopadhyay, A
   Mehta, D
   Chakrabartty, S
AF Gangopadhyay, Ahana
   Mehta, Darshit
   Chakrabartty, Shantanu
TI A Spiking Neuron and Population Model Based on the Growth Transform
   Dynamical System
SO FRONTIERS IN NEUROSCIENCE
DT Article
DE spiking neuron model; growth transforms; energy-minimization; dynamical
   system; network model; neural dynamics; associative memory; adaptation
ID NETWORKS; OPTIMIZATION; GENERATION
AB In neuromorphic engineering, neural populations are generally modeled in a bottom-up manner, where individual neuron models are connected through synapses to form large-scale spiking networks. Alternatively, a top-down approach treats the process of spike generation and neural representation of excitation in the context of minimizing some measure of network energy. However, these approaches usually define the energy functional in terms of some statistical measure of spiking activity (ex. firing rates), which does not allow independent control and optimization of neurodynamical parameters. In this paper, we introduce a new spiking neuron and population model where the dynamical and spiking responses of neurons can be derived directly from a network objective or energy functional of continuous-valued neural variables like the membrane potential. The key advantage of the model is that it allows for independent control over three neuro-dynamical properties: (a) control over the steady-state population dynamics that encodes the minimum of an exact network energy functional; (b) control over the shape of the action potentials generated by individual neurons in the network without affecting the network minimum; and (c) control over spiking statistics and transient population dynamics without affecting the network minimum or the shape of action potentials. At the core of the proposed model are different variants of Growth Transform dynamical systems that produce stable and interpretable population dynamics, irrespective of the network size and the type of neuronal connectivity (inhibitory or excitatory). In this paper, we present several examples where the proposed model has been configured to produce different types of single-neuron dynamics as well as population dynamics. In one such example, the network is shown to adapt such that it encodes the steady-state solution using a reduced number of spikes upon convergence to the optimal solution. In this paper, we use this network to construct a spiking associative memory that uses fewer spikes compared to conventional architectures, while maintaining high recall accuracy at high memory loads.
C1 [Gangopadhyay, Ahana; Chakrabartty, Shantanu] Washington Univ, Dept Elect & Syst Engn, St Louis, MO 63110 USA.
   [Mehta, Darshit] Washington Univ, Dept Biomed Engn, St Louis, MO 63110 USA.
RP Chakrabartty, S (corresponding author), Washington Univ, Dept Elect & Syst Engn, St Louis, MO 63110 USA.
EM shantanu@wustl.edu
CR Abbott LF, 1999, BRAIN RES BULL, V50, P303, DOI 10.1016/S0361-9230(99)00161-6
   Adrian ED, 1926, J PHYSIOL-LONDON, V61, P151, DOI 10.1113/jphysiol.1926.sp002281
   AGMON A, 1989, NEUROSCI LETT, V99, P137, DOI 10.1016/0304-3940(89)90278-4
   [Anonymous], 2013, 2013 INT JOINT C NEU
   [Anonymous], 2006, PREDICTING STRUCTURE
   BAUM LE, 1968, PAC J MATH, V27, P211, DOI 10.2140/pjm.1968.27.211
   Bialek, 1999, SPIKES EXPLORING NEU, V7
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Brumberg JC, 2000, J NEUROSCI, V20, P4829
   Cessac B, 2011, J MATH BIOL, V62, P863, DOI 10.1007/s00285-010-0358-4
   Chakrabartty S, 2007, J MACH LEARN RES, V8, P813
   Chatterjee O, 2018, IEEE T NEUR NET LEAR, V29, P6052, DOI 10.1109/TNNLS.2018.2817367
   Clarke LE, 2013, NAT REV NEUROSCI, V14, P311, DOI 10.1038/nrn3484
   CONNORS BW, 1990, TRENDS NEUROSCI, V13, P99, DOI 10.1016/0166-2236(90)90185-D
   Cutsuridis V, 2010, HIPPOCAMPUS, V20, P423, DOI 10.1002/hipo.20661
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Fields RD, 2014, NEUROSCIENTIST, V20, P426, DOI 10.1177/1073858413504465
   FITZHUGH R, 1961, BIOPHYS J, V1, P445, DOI 10.1016/S0006-3495(61)86902-6
   Friedrich RW, 2001, SCIENCE, V291, P889, DOI 10.1126/science.291.5505.889
   Galán RF, 2004, NEURAL COMPUT, V16, P999, DOI 10.1162/089976604773135078
   Gangopadhyay A., 2019, SPIKING NEURON POPUL, DOI [10.1101/523944, DOI 10.1101/523944]
   Gangopadhyay A, 2018, IEEE T NEUR NET LEAR, V29, P2379, DOI 10.1109/TNNLS.2017.2695171
   Gangopadhyay A, 2018, IEEE T NEUR NET LEAR, V29, P1961, DOI 10.1109/TNNLS.2017.2690434
   GARDNER E, 1988, J PHYS A-MATH GEN, V21, P271, DOI 10.1088/0305-4470/21/1/031
   Gerstner W., 2002, SPIKING NEURON MODEL, DOI [DOI 10.1017/CBO9780511815706, 10.1017/cbo9780511815706]
   Gibson JR, 1999, NATURE, V402, P75, DOI 10.1038/47035
   Gore A, 2010, IEEE T CIRCUITS-I, V57, P604, DOI 10.1109/TCSI.2009.2025002
   Gray CM, 1996, SCIENCE, V274, P109, DOI 10.1126/science.274.5284.109
   Hasselmo M., 2002, HDB BRAIN THEORY NEU
   Hinton G.E., 1986, PARALLEL DISTRIBUTED, V1, P282, DOI DOI 10.1234/12345678
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554
   Izhikevich E.M., 2007, DYNAMICAL SYSTEMS NE, DOI [10.7551/mitpress/2526.001.0001, DOI 10.7551/MITPRESS/2526.001.0001]
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Jonke Z, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00118
   Kuhn HW, 1951, P 2 BERK S MATH STAT, P481, DOI DOI 10.1007/BF01582292
   Lansner A, 2009, TRENDS NEUROSCI, V32, P178, DOI 10.1016/j.tins.2008.12.002
   Lee JM, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00191
   Mar DJ, 1999, P NATL ACAD SCI USA, V96, P10450, DOI 10.1073/pnas.96.18.10450
   MCCORMICK DA, 1985, J NEUROPHYSIOL, V54, P782, DOI 10.1152/jn.1985.54.4.782
   McCormick DA, 2014, CURR OPIN NEUROBIOL, V29, pIV, DOI 10.1016/j.conb.2014.10.010
   Mehta D., 2019, GROWTH TRANSFORM NEU
   Nakano T, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0115620
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   Palm G, 2013, NEURAL NETWORKS, V37, P163, DOI 10.1016/j.neunet.2012.08.013
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Shrestha S. B., 2018, P 32 INT C NEUR INF, P1419, DOI [10.5555/3326943.3327073, DOI 10.5555/3326943.3327073]
   Síma J, 2003, NEURAL COMPUT, V15, P693, DOI 10.1162/089976603321192130
   Soula H, 2006, NEURAL COMPUT, V18, P60, DOI 10.1162/089976606774841567
   Stopfer M, 2003, NEURON, V39, P991, DOI 10.1016/j.neuron.2003.08.011
   THORPE SJ, 1990, PARALLEL PROCESSING IN NEURAL SYSTEMS AND COMPUTERS, P91
   Traub RD., 1991, NEURONAL NETWORKS HI, DOI [10.1017/CBO9780511895401, DOI 10.1017/CBO9780511895401]
   Wright SH, 2004, ADV PHYSIOL EDUC, V28, P139, DOI 10.1152/advan.00029.2004
   Xin JG, 2001, IEEE IJCNN, P1772, DOI 10.1109/IJCNN.2001.938430
   Zenke F, 2018, NEURAL COMPUT, V30, P1514, DOI 10.1162/neco_a_01086
NR 56
TC 3
Z9 3
U1 0
U2 7
PD MAY 12
PY 2020
VL 14
AR 425
DI 10.3389/fnins.2020.00425
WC Neurosciences
DA 2023-11-11
ER

PT J
AU Pês, BD
   Guimaraes, JG
   Oroski, E
   Bonfim, MJD
AF Pes, Beatriz dos Santos
   Guimaraes, Janaina Goncalves
   Oroski, Elder
   do Couto Bonfim, Marlio Jose
TI A Spiking Neural Network implemented with Single-Electron Transistors
   and NoCs
SO NANO COMMUNICATION NETWORKS
DT Article
DE SNNs; NoC; Spiking neurons; ANNs; Nanoelectronics
ID CIRCUIT SIMULATION; INTERCONNECT; OSCILLATORS; DESIGN; MODEL
AB This work proposes a Spiking Neural Network, SNN, based on a nanoelectronic spiking neuron - as building block - and a 2D-mesh network-on-chip, NoC - as interconnect architecture. The SNN obtained from the NoC is an alternative for high density architectures, providing reconfigurability, high scalability and low power consumption. A look-up table router was used to connect all units. Further on, the eXclusive-OR, XOR, benchmark problem was used to validate the functionality of the nanoelectronic SNN. (C) 2018 Elsevier B.V. All rights reserved.
C1 [Pes, Beatriz dos Santos] Fed Inst Parana IFPR, Campus Campo Largo, BR-83601190 Campo Largo, PR, Brazil.
   [Guimaraes, Janaina Goncalves] Univ Santa Catarina UFSC, Dept Engn, Campus Blumenau, BR-89036256 Blumenau, SC, Brazil.
   [Oroski, Elder] Univ Parana UTFPR, Dept Elect Engn, Campus Curitiba, BR-80230901 Curitiba, PR, Brazil.
   [do Couto Bonfim, Marlio Jose] Univ Parana UFPR, Dept Elect Engn, Campus Politecn, BR-81530900 Curitiba, Parana, Brazil.
RP Pês, BD (corresponding author), Fed Inst Parana IFPR, Campus Campo Largo, BR-83601190 Campo Largo, PR, Brazil.
EM beatriz.santos@ifpr.edu.br; janaina.guimaraes@ufsc.br;
   oroski@utfpr.edu.br; marliob@eletrica.ufpr.br
CR [Anonymous], 2001, COMP MICROE
   [Anonymous], 2004, CIRCUIT DESIGN VHDL
   Benini L, 2002, COMPUTER, V35, P70, DOI 10.1109/2.976921
   Bohr MT, 2002, IEEE T NANOTECHNOL, V1, P56, DOI 10.1109/TNANO.2002.1005426
   Burkitt AN, 2006, BIOL CYBERN, V95, P1, DOI 10.1007/s00422-006-0068-6
   Carrillo S, 2012, NEURAL NETWORKS, V33, P42, DOI 10.1016/j.neunet.2012.04.004
   Cawley S, 2011, GENET PROGRAM EVOL M, V12, P257, DOI 10.1007/s10710-011-9130-9
   Nogueira CPDM, 2012, J COMPUT THEOR NANOS, V9, P974, DOI 10.1166/jctn.2012.2127
   Dally WJ, 2001, DES AUT CON, P684, DOI 10.1109/DAC.2001.935594
   DeHon A, 2004, IEEE T VLSI SYST, V12, P1038, DOI 10.1109/TVLSI.2004.827562
   Elalfi A., 2013, IJCSI INT J COMPUTER, V10, P83
   Fidjeland AK, 2009, IEEE INT CONF ASAP, P137, DOI 10.1109/ASAP.2009.24
   Furber S, 2006, P AISB WORKSH GC5 AR, P29
   Furber SB, 2013, IEEE T COMPUT, V62, P2454, DOI 10.1109/TC.2012.142
   Gerstner W., 2002, SPIKING NEURON MODEL
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Goser KF, 1997, P IEEE, V85, P558, DOI 10.1109/5.573741
   Guerreiro AMG, 2007, BIOL CYBERN, V97, P211, DOI 10.1007/s00422-007-0169-x
   Guerrier P., 2000, Proceedings Design, Automation and Test in Europe Conference and Exhibition 2000 (Cat. No. PR00537), P250, DOI 10.1109/DATE.2000.840047
   Guimaraes JG, 2013, J COMPUT THEOR NANOS, V10, P2563, DOI 10.1166/jctn.2013.3248
   Guimaraes JG, 2011, J COMPUT THEOR NANOS, V8, P1831, DOI 10.1166/jctn.2011.1890
   Hadley P., 2002, P 29 INT S LAUS 7 10, P125
   Hanson George W, 2008, FUNDAMENTALS NANOELE
   Harkin J, 2009, INT J RECONFIGURABLE, V2009, DOI 10.1155/2009/908740
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Ji Y, 2016, J COMPUT SCI TECH-CH, V31, P50, DOI 10.1007/s11390-016-1611-0
   Kamimura T, 2015, ACSR ADV COMPUT, V18, P577
   Kamimura T., 2015, AUTOMATION CONTROL I, V3, P63, DOI DOI 10.11648/J.ACIS.20150305.11
   Kanazawa Y, 2004, INT J ROBOT AUTOM, V19, P206, DOI 10.2316/Journal.206.2004.4.206-2716
   Kim KJ, 2012, APPL INTELL, V36, P887, DOI 10.1007/s10489-011-0303-2
   Kim S, 2011, 2011 11TH INTERNATIONAL CONFERENCE ON CONTROL, AUTOMATION AND SYSTEMS (ICCAS), P1
   Knag P, 2015, IEEE J SOLID-ST CIRC, V50, P1070, DOI 10.1109/JSSC.2014.2386892
   Kumar S, 2002, IEEE COMP SOC ANN, P117, DOI 10.1109/ISVLSI.2002.1016885
   Kuzum D, 2012, IEEE T ELECTRON DEV, V59, P3489, DOI 10.1109/TED.2012.2217146
   Li H, 2013, SPRINGER SER MATER S, V187, P1, DOI 10.1007/978-1-4614-8169-0
   Lientschnig G, 2003, JPN J APPL PHYS 1, V42, P6467, DOI 10.1143/JJAP.42.6467
   Liu B., 2010, NANO COMMUN NETW, P232, DOI DOI 10.1016/J.NANCOM.2010.09.003
   Liu WP, 2011, PHYSCS PROC, V22, P170, DOI 10.1016/j.phpro.2011.11.027
   Lundstrom M., P 2002 INT S LOW POW, P172
   Maguire LP, 2007, NEUROCOMPUTING, V71, P13, DOI 10.1016/j.neucom.2006.11.029
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Ono Y, 2005, J APPL PHYS, V97, DOI 10.1063/1.1843271
   Oya T, 2006, CHAOS SOLITON FRACT, V27, P887, DOI 10.1016/j.chaos.2005.04.059
   Pal S., 2013, IJETAE, V3, P615
   Parekh R, 2014, MICROELECTRON J, V45, P1087, DOI 10.1016/j.mejo.2014.05.020
   Park H, 2000, NATURE, V407, P57, DOI 10.1038/35024031
   Pashkin YA, 2000, APPL PHYS LETT, V76, P2256, DOI 10.1063/1.126313
   Paul D. J., 2002, ENCY PHYS SCI TECHNO, V10, P285
   Pês BD, 2017, J COMPUT ELECTRON, V16, P98, DOI 10.1007/s10825-016-0928-9
   Rietman EA, 2003, ROBOT AUTON SYST, V45, P249, DOI 10.1016/j.robot.2003.08.002
   Saneei Mohsen, 2006, 2006 International Conference on Microelectronics, P36, DOI 10.1109/ICM.2006.373261
   Schemmel J, 2010, IEEE INT SYMP CIRC S, P1947, DOI 10.1109/ISCAS.2010.5536970
   Schemmel J, 2008, IEEE IJCNN, P431, DOI 10.1109/IJCNN.2008.4633828
   Sengupta A, 2016, PHYS REV APPL, V6, DOI 10.1103/PhysRevApplied.6.064003
   Smirnov EA, 2014, AASRI PROC, V6, P89, DOI 10.1016/j.aasri.2014.05.013
   Wasshuber C, 2002, PROCEEDING OF THE 2002 3RD INTERNATIONAL SYMPOSIUM ON QUALITY ELECTRONIC DESIGN, P502, DOI 10.1109/ISQED.2002.996795
   Yin D, 2011, ACM SIGCOMM COMP COM, V41, P125, DOI 10.1145/1925861.1925882
   Zhang FM, 2005, MICROELECTRON J, V36, P741, DOI 10.1016/j.mejo.2005.01.003
NR 59
TC 4
Z9 4
U1 1
U2 5
PD SEP
PY 2018
VL 17
BP 21
EP 29
DI 10.1016/j.nancom.2018.06.001
WC Engineering, Electrical & Electronic; Nanoscience & Nanotechnology;
   Telecommunications
DA 2023-11-11
ER

PT J
AU Liu, F
   Tao, WT
   Yang, J
   Wu, W
   Wang, J
AF Liu, Fang
   Tao, Wentao
   Yang, Jie
   Wu, Wei
   Wang, Jian
TI STNet: A novel spiking neural network combining its own time signal with
   the spatial signal of an artificial neural network
SO FRONTIERS IN NEUROSCIENCE
DT Article
DE hybrid network; spiking neural network; artificial neural network;
   spatio-temporal information; STNet
ID CLASSIFICATION
AB IntroductionThis article proposes a novel hybrid network that combines the temporal signal of a spiking neural network (SNN) with the spatial signal of an artificial neural network (ANN), namely the Spatio-Temporal Combined Network (STNet). MethodsInspired by the way the visual cortex in the human brain processes visual information, two versions of STNet are designed: a concatenated one (C-STNet) and a parallel one (P-STNet). In the C-STNet, the ANN, simulating the primary visual cortex, extracts the simple spatial information of objects first, and then the obtained spatial information is encoded as spiking time signals for transmission to the rear SNN which simulates the extrastriate visual cortex to process and classify the spikes. With the view that information from the primary visual cortex reaches the extrastriate visual cortex via ventral and dorsal streams, in P-STNet, the parallel combination of the ANN and the SNN is employed to extract the original spatio-temporal information from samples, and the extracted information is transferred to a posterior SNN for classification. ResultsThe experimental results of the two STNets obtained on six small and two large benchmark datasets were compared with eight commonly used approaches, demonstrating that the two STNets can achieve improved performance in terms of accuracy, generalization, stability, and convergence. DiscussionThese prove that the idea of combining ANN and SNN is feasible and can greatly improve the performance of SNN.
C1 [Liu, Fang; Tao, Wentao; Yang, Jie; Wu, Wei] Dalian Univ Technol, Sch Math Sci, Dalian, Peoples R China.
   [Liu, Fang; Tao, Wentao; Yang, Jie; Wu, Wei] Key Lab Computat Math & Data Intelligence Liaoning, Dalian, Peoples R China.
   [Wang, Jian] China Univ Petr East China, Coll Sci, Qingdao, Peoples R China.
RP Yang, J (corresponding author), Dalian Univ Technol, Sch Math Sci, Dalian, Peoples R China.; Yang, J (corresponding author), Key Lab Computat Math & Data Intelligence Liaoning, Dalian, Peoples R China.
EM yangjiee@dlut.edu.cn
CR Abiodun OI, 2019, IEEE ACCESS, V7, P158820, DOI 10.1109/ACCESS.2019.2945545
   Amin HH, 2017, INT CONF UBIQ FUTUR, P1
   Arora T., 2019, SYNAPTIC WEIGHT UPDA
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Cheng X, 2020, Arxiv, DOI arXiv:2010.03140
   Davidson S, 2021, FRONT NEUROSCI-SWITZ, V15, DOI 10.3389/fnins.2021.651141
   Dora S, 2016, NEUROCOMPUTING, V171, P1216, DOI 10.1016/j.neucom.2015.07.086
   Dora S, 2019, IEEE T CYBERNETICS, V49, P989, DOI 10.1109/TCYB.2018.2791282
   Nguyen DA, 2021, J LOW POWER ELECT AP, V11, DOI 10.3390/jlpea11020023
   Fu Q, 2022, ENTROPY-SWITZ, V24, DOI 10.3390/e24111543
   Hao YZ, 2020, NEURAL NETWORKS, V121, P387, DOI 10.1016/j.neunet.2019.09.007
   He YL, 2019, ADV MATER, V31, DOI 10.1002/adma.201900903
   Hoffer E, 2018, Arxiv, DOI [arXiv:1705.08741, DOI 10.48550/ARXIV.1705.08741]
   Huang YC, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00509
   Joukal M., 2017, HOMONYMOUS VISUAL FI, P1
   Kang TY, 2017, BMC BIOINFORMATICS, V18, DOI 10.1186/s12859-017-1984-2
   Kasabov NK, 2014, NEURAL NETWORKS, V52, P62, DOI 10.1016/j.neunet.2014.01.006
   Kheradpisheh SR, 2020, INT J NEURAL SYST, V30, DOI 10.1142/S0129065720500276
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Li YL, 2010, INT CONF SIGN PROCES, P211, DOI 10.1109/ICOSP.2010.5655138
   López-Vázquez G, 2019, COMPUT INTEL NEUROSC, V2019, DOI 10.1155/2019/4182639
   Muramatsu N., 2021, ARXIV PREPRINT
   Nobukawa S, 2019, J ARTIF INTELL SOFT, V9, P283, DOI 10.2478/jaiscr-2019-0009
   Pan Z., 2019, INT JOINT C NEUR NET, P1, DOI [10.1109/IJCNN.2019.8851858, DOI 10.2112/SI94-001.1, DOI 10.1109/IJCNN.2019.8851858]
   Rafi T.H., 2021, BRIEF REVIEW SPIKING, V2021, DOI [10.20944/preprints202104.0202.v1, DOI 10.20944/PREPRINTS202104.0202.V1]
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Seo K.-K., 2013, IEEE T IND ELECTRON, V6, P127, DOI [10.14257/ijca.2013.6.5.12, DOI 10.14257/IJCA.2013.6.5.12]
   Stewart KM, 2022, NEUROMORPH COMPUT EN, V2, DOI 10.1088/2634-4386/ac8828
   Taherkhani A, 2020, NEURAL NETWORKS, V122, P253, DOI 10.1016/j.neunet.2019.09.036
   Wade JJ, 2010, IEEE T NEURAL NETWOR, V21, P1817, DOI 10.1109/TNN.2010.2074212
   Wang KN, 2020, APPL MATH COMPUT, V377, DOI 10.1016/j.amc.2020.125186
   Wang W, 2018, SCI ADV, V4, DOI 10.1126/sciadv.aat4752
   Xie XD, 2020, NEURAL COMPUT APPL, V32, P13441, DOI 10.1007/s00521-020-04752-7
   Xu Q, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1646
   Zhang ML, 2022, IEEE T NEUR NET LEAR, V33, P1947, DOI 10.1109/TNNLS.2021.3110991
   Zhao JH, 2022, IEEE T NEUR NET LEAR, V33, P4096, DOI 10.1109/TNNLS.2021.3055825
NR 36
TC 0
Z9 0
U1 9
U2 9
PD APR 18
PY 2023
VL 17
AR 1151949
DI 10.3389/fnins.2023.1151949
WC Neurosciences
DA 2023-11-11
ER

PT C
AU Zhang, JR
   Wang, JJ
   Yan, JW
   Wang, CM
   Pu, SL
AF Zhang Jingren
   Wang Jingjing
   Yan Jingwei
   Wang Chunmao
   Pu Shiliang
GP IEEE
TI Deep Spiking Neural Network for High-Accuracy and Energy-Efficient Face
   Action Unit Recognition
SO 2021 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUL 18-22, 2021
CL ELECTR NETWORK
DE spiking neural networks; face action unit recognition; channel-wise
   normalization; graph convolution network
ID COMMUNICATION
AB In recent years, spiking neural networks (SNNs) have received significant attention as the third-generation of networks due to their event-driven and low-powered nature. However, their applications have been limited to relatively simple tasks such as image classification, since it is difficult to train SNNs and converting deep artificial neural networks (ANNs) into SNNs directly usually causes large accuracy degradation. In this paper, we employ an SNN to solve a more challenging multi-label classification task and propose the first spiking-based network for face action unit (AU) recognition. Specifically, a relation extracting module based on graph convolution network (GCN) is proposed to leverage AU regional features. Channel-wise normalization methods for residual blocks of the Resnet backbone and GCN blocks are proposed for ANN-to-SNN conversion to keep the high performance. Experiments on the BP4D dataset show that our proposed model achieves high-accuracy performance, and converges 3 times faster than previous methods.
C1 [Zhang Jingren; Wang Jingjing; Yan Jingwei; Wang Chunmao; Pu Shiliang] Hangzhou Hikvis Digital Technol CO LTD, Hikvis Res Inst, Hangzhou, Peoples R China.
RP Zhang, JR (corresponding author), Hangzhou Hikvis Digital Technol CO LTD, Hikvis Res Inst, Hangzhou, Peoples R China.
EM zhangjingren@hikvision.com; wangjingjing9@hikvision.com;
   yanjingwei@hikvision.com; wangchunmao@hikvision.com;
   pushiliang.hri@hikvision.com
CR Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Diehl Peter U, 2015, 2015 INT JOINT C NEU, P1, DOI [DOI 10.1109/IJCNN.2015.7280696, 10.1109/IJCNN.2015.7280696]
   Gautrais J, 1998, BIOSYSTEMS, V48, P57, DOI 10.1016/S0303-2647(98)00050-1
   Horowitz M, 2014, ISSCC DIG TECH PAP I, V57, P10, DOI 10.1109/ISSCC.2014.6757323
   Izhikevich EM, 2003, TRENDS NEUROSCI, V26, P161, DOI 10.1016/S0166-2236(03)00034-1
   KIM S, 2020, 2020 AAAI C ART INT, P127
   Lee JM, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00191
   Li GB, 2019, AAAI CONF ARTIF INTE, P8594
   Li W, 2018, IEEE T PATTERN ANAL, V40, P2583, DOI 10.1109/TPAMI.2018.2791608
   Li W, 2017, PROC CVPR IEEE, P6766, DOI 10.1109/CVPR.2017.716
   Lichtsteiner Patrick, 2008, IEEE Journal of Solid-State Circuits, V43, P566, DOI 10.1109/JSSC.2007.914337
   Liu LL, 2019, 2019 IEEE/WIC/ACM INTERNATIONAL CONFERENCE ON WEB INTELLIGENCE WORKSHOPS (WI 2019 COMPANION), P171, DOI 10.1145/3358695.3360943
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Masquelier T, 2007, PLOS COMPUT BIOL, V3, P247, DOI 10.1371/journal.pcbi.0030031
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Mostafa H., 2017, 2017 IEEE INT S CIRC, P1, DOI [10.1109/ISCAS.2017.8050527, DOI 10.1109/ISCAS.2017.8050527]
   PARK S, 2020, 2019 ACMIEEE DES AUT, V67, P1775, DOI DOI 10.1109/TBME.2019.2947089
   Qiao N, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00141
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Shao ZW, 2018, LECT NOTES COMPUT SC, V11217, P725, DOI 10.1007/978-3-030-01261-8_43
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   THORPE SJ, 1990, PARALLEL PROCESSING IN NEURAL SYSTEMS AND COMPUTERS, P91
   Zhang X, 2014, IMAGE VISION COMPUT, V32, P692, DOI 10.1016/j.imavis.2014.06.002
   Zhao KL, 2016, PROC CVPR IEEE, P3391, DOI 10.1109/CVPR.2016.369
NR 26
TC 0
Z9 0
U1 1
U2 3
PY 2021
DI 10.1109/IJCNN52387.2021.9533451
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Lehmann, HM
   Hille, J
   Grassmann, C
   Issakov, V
AF Lehmann, Hendrik M.
   Hille, Julian
   Grassmann, Cyprian
   Issakov, Vadim
GP IEEE
TI Spiking Neural Networks based Rate-Coded Logic Gates for Automotive
   Applications in BiCMOS
SO 2021 IEEE INTERNATIONAL CONFERENCE ON MICROWAVES, ANTENNAS,
   COMMUNICATIONS AND ELECTRONIC SYSTEMS (COMCAS)
SE IEEE International Conference on Microwaves Communications Antennas and
   Electronic Systems
DT Proceedings Paper
CT IEEE International Conference on Microwaves, Antennas, Communications
   and Electronic Systems (COMCAS)
CY NOV 01-03, 2021
CL Tel Aviv, ISRAEL
AB Spiking Neural Networks (SNNs) represent the third generation of artificial neural networks. In this work, we evaluate the core element of SNN, the neuron circuit equivalent, in terms of temperature robustness for automotive applications. Thanks to the operating point stabilization, the proposed circuit-level neuron implementation achieves a broad frequency tuning range up to 42MHz and operates over a wide temperature range from -40 degrees C to 125 degrees C. At the maximum spiking frequency of 42MHz, the circuit consumes a DC power of only 300nW. We use the proposed neuron circuit to realize two fundamental logic gates, AND and OR, by means of analog rate-encoded spiking neural networks. To the best of the authors' knowledge, these are the first reported SNN-based logic gates measured over the automotive temperature range. We showcase the suitability of SNN circuit implementation for automotive applications. The circuits are realized in a 130nm BiCMOS.
C1 [Lehmann, Hendrik M.; Issakov, Vadim] TU Braunschweig, Inst CMOS Design, Braunschweig, Germany.
   [Hille, Julian] Tech Univ Munich, Chair Robot AI & Real Time Syst, Munich, Germany.
   [Lehmann, Hendrik M.; Hille, Julian; Grassmann, Cyprian; Issakov, Vadim] Infineon Technol AG, Neubiberg, Germany.
RP Lehmann, HM (corresponding author), TU Braunschweig, Inst CMOS Design, Braunschweig, Germany.; Lehmann, HM (corresponding author), Infineon Technol AG, Neubiberg, Germany.
EM hendrik.lehmann@infineon.com
CR Arsanjani MA, 2022, INT J MANAG SCI ENG, V17, P259, DOI 10.1080/17509653.2021.2009053
   Automotive Electronics Council Component Technical Committee, AECQ100012
   Chevalier P, 2018, 2018 IEEE BICMOS AND COMPOUND SEMICONDUCTOR INTEGRATED CIRCUITS AND TECHNOLOGY SYMPOSIUM (BCICTS), P64, DOI 10.1109/BCICTS.2018.8550963
   Chicca E, 2014, P IEEE, V102, P1367, DOI 10.1109/JPROC.2014.2313954
   Enriquez-Gaytan J., 2018, 2018 15 INT C ELECT, P1
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Issakov V, 2019, 2019 IEEE BICMOS AND COMPOUND SEMICONDUCTOR INTEGRATED CIRCUITS AND TECHNOLOGY SYMPOSIUM (BCICTS 2019), DOI 10.1109/bcicts45179.2019.8972781
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Lammert V, 2020, GER MICROW CONF, P96
   Reljan-Delaney M, 2017, 2017 COMPUTING CONFERENCE, P701, DOI 10.1109/SAI.2017.8252173
   Tahtirvanci Aykut, 2018, 26 SIGNAL PROCESSING, P1, DOI [10.1109/SIU.2018.8404470, DOI 10.1109/SIU.2018.8404470]
   Vreeken J., 2003, UUCS2003008
   Wijekoon JHB, 2008, NEURAL NETWORKS, V21, P524, DOI 10.1016/j.neunet.2007.12.037
   Wu B., 2018, P IEEE INT C CONS EL, P1
NR 14
TC 2
Z9 2
U1 0
U2 3
PY 2021
BP 280
EP 285
DI 10.1109/COMCAS52219.2021.9629011
WC Computer Science, Information Systems; Engineering, Electrical &
   Electronic; Telecommunications
DA 2023-11-11
ER

PT J
AU Maguire, L
AF Maguire, Liam
TI Does Soft Computing Classify Research in Spiking Neural Networks?
SO INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS
DT Article
DE Spiking neural networks; classification
ID SYNAPTIC PLASTICITY; NEURONS; SIMULATION; MODEL; REINFORCEMENT; FPGA;
   IMPLEMENTATIONS; SYNAPSES; PLATFORM; COMPLEX
AB The last fifty years has witnessed considerable activity in research that develops computational approaches inspired by nature. There are a number of umbrella terms used by researchers to classify their contributions. This can cause problems in disseminating and sharing results and potentially restricts research due to a lack of knowledge of the varied contributions. This paper reviews research in spiking neural networks and attempts to determine if the term Soft Computing can be used to classify contributions in this area.
C1 Univ Ulster, Sch Comp & Intelligent Syst, Intelligent Syst Res Ctr, Derry BT48 7JL, North Ireland.
RP Maguire, L (corresponding author), Univ Ulster, Sch Comp & Intelligent Syst, Intelligent Syst Res Ctr, Derry BT48 7JL, North Ireland.
EM lp.maguire@ulster.ac.uk
CR Abbott LF, 2000, NAT NEUROSCI, V3, P1178, DOI 10.1038/81453
   Adibi P, 2005, NEUROCOMPUTING, V64, P335, DOI 10.1016/j.neucom.2004.10.111
   Aisa B, 2008, NEURAL NETWORKS, V21, P1146, DOI 10.1016/j.neunet.2008.06.016
   ALLEN C, 1994, P NATL ACAD SCI USA, V91, P10380, DOI 10.1073/pnas.91.22.10380
   Alnajjar F, 2006, 2006 IEEE CONFERENCE ON ROBOTICS, AUTOMATION AND MECHATRONICS, VOLS 1 AND 2, P205
   Alnajjar F, 2008, IEEE IJCNN, P2207, DOI 10.1109/IJCNN.2008.4634103
   AMIN HH, 2005, PROCEEDINGS 1, P456
   AMIN HH, 2005, INT C INT COMP ICIC, P621
   [Anonymous], 2005, IEEE IJCNN
   [Anonymous], 2004, FPGA BASED SYSTEM DE
   BARBER D, 2003, ADV NEURAL INFORM PR, V15, P149
   Belatreche A, 2007, SOFT COMPUT, V11, P239, DOI [10.1007/s00500-006-0065-7, 10.1007/S00500-006-0065-7]
   BERNHARD F, 2006, INT C COMP SCI ICCS, P236
   Bi GQ, 1999, NATURE, V401, P792, DOI 10.1038/44573
   *BNN, 2004, BIOL NEUR NETW TOOLB
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   Booij O, 2005, INFORM PROCESS LETT, V95, P552, DOI 10.1016/j.ipl.2005.05.023
   Booth V, 1995, J COMPUT NEUROSCI, V2, P299, DOI 10.1007/BF00961442
   BOWER J, 2008, BOOK GENESIS EXPLORI
   Braitenberg V, 1998, CORTEX STAT GEOMETRY, V2nd
   Butera RJ, 1999, J NEUROPHYSIOL, V82, P382, DOI 10.1152/jn.1999.82.1.382
   Cassidy A, 2007, 2007 IEEE BIOMEDICAL CIRCUITS AND SYSTEMS CONFERENCE, P75, DOI 10.1109/BIOCAS.2007.4463312
   CHARLTON MP, 1982, J PHYSIOL-LONDON, V323, P173, DOI 10.1113/jphysiol.1982.sp014067
   Chicca E, 2003, IEEE T NEURAL NETWOR, V14, P1297, DOI 10.1109/TNN.2003.816367
   de Queiroz MS, 2006, NEUROCOMPUTING, V70, P14, DOI 10.1016/j.neucom.2006.07.002
   Delorme A, 1999, NEUROCOMPUTING, V26-7, P989, DOI 10.1016/S0925-2312(99)00095-8
   Djurfeldt M, 2008, IBM J RES DEV, V52, P31, DOI 10.1147/rd.521.0031
   Florian RV, 2007, NEURAL COMPUT, V19, P1468, DOI 10.1162/neco.2007.19.6.1468
   FLORIAN RV, 2005, P 7 INT S SYMB NUM A
   Furber S, 2006, P AISB WORKSH GC5 AR, P29
   Gerstner W, 2002, BIOL CYBERN, V87, P404, DOI 10.1007/s00422-002-0353-y
   GLACKIN C, 2008, ART NEUR NETW 18 INT
   Gokhale M, 2005, RECONFIGURABLE COMPU
   Gonzalez-Nalda P, 2008, NEUROCOMPUTING, V71, P721, DOI 10.1016/j.neucom.2007.07.032
   Graas EL, 2004, NEUROINFORMATICS, V2, P417, DOI 10.1385/NI:2:4:417
   Guerrero-Rivera R, 2006, NEURAL COMPUT, V18, P2651, DOI 10.1162/neco.2006.18.11.2651
   HEBB D. O., 1949
   Hines ML, 1997, NEURAL COMPUT, V9, P1179, DOI 10.1162/neco.1997.9.6.1179
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Honghoon Jang, 2008, 2008 Digital Image Computing: Techniques and Applications, P155, DOI 10.1109/DICTA.2008.82
   Indiveri G, 2006, IEEE T NEURAL NETWOR, V17, P211, DOI 10.1109/TNN.2005.860850
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Jahnke A., 1996, Proceedings of the Fifth International Conference on Microelectronics for Neural Networks and Fuzzy Systems. MicroNeuro'96, P232, DOI 10.1109/MNNFS.1996.493796
   Johnston SP, 2006, 2006 3RD INTERNATIONAL IEEE CONFERENCE INTELLIGENT SYSTEMS, VOLS 1 AND 2, P621
   Kasinski A, 2005, LECT NOTES COMPUT SC, V3696, P145, DOI 10.1007/11550822_24
   Kistler WM, 2002, BIOL CYBERN, V87, P416, DOI 10.1007/s00422-002-0359-5
   LAHABAR S, 2008, P NAT C COMP VIS PAT
   Legenstein R, 2005, NEURAL COMPUT, V17, P2337, DOI 10.1162/0899766054796888
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maguire LP, 2007, NEUROCOMPUTING, V71, P13, DOI 10.1016/j.neucom.2006.11.029
   Mak TS, 2005, I IEEE EMBS C NEUR E, P144
   Mallik U, 2005, IEEE INT SYMP CIRC S, P1919, DOI 10.1109/ISCAS.2005.1464988
   Markram H, 2006, NAT REV NEUROSCI, V7, P153, DOI 10.1038/nrn1848
   *MATHW, MATL US GUID
   Maya S., 2000, Field-Programmable Logic and Applications. Roadmap to Reconfigurable Computing. 10th International Conference, FPL 2000. Proceedings (Lecture Notes in Computer Science Vol.1896), P270
   McKennoch Sam, 2007, 2006 International Joint Conference on Neural Networks, P3970
   MEAD C, 1990, P IEEE, V78, P1629, DOI 10.1109/5.58356
   MOORE S, 2002, THESIS U BATH
   Morrison A, 2005, NEURAL COMPUT, V17, P1776, DOI 10.1162/0899766054026648
   *NEURODIMENSION, NEUR SOFTW SIM TOOL
   Pavlidis NG, 2005, IEEE IJCNN, P2190
   PEARSON M, 2005, P ICANN 05 WARS SEPT
   Pearson MJ, 2007, IEEE T NEURAL NETWOR, V18, P1472, DOI 10.1109/TNN.2007.891203
   PETRON E, 1999, LINUX J
   Pfister JP, 2003, LECT NOTES COMPUT SC, V2714, P92
   Renaud-Le Masson S, 2004, INFORM SCIENCES, V161, P57, DOI 10.1016/j.ins.2003.03.007
   Roggen D, 2003, 2003 NASA/DOD CONFERENCE ON EVOLVABLE HARDWARE, P189
   Ros E, 2006, IEEE T NEURAL NETWOR, V17, P1050, DOI 10.1109/TNN.2006.875980
   Rossmann M., 1996, Proceedings of the Fifth International Conference on Microelectronics for Neural Networks and Fuzzy Systems. MicroNeuro'96, P322, DOI 10.1109/MNNFS.1996.493810
   Ruf B, 1997, NEURAL PROCESS LETT, V5, P9, DOI 10.1023/A:1009697008681
   Schemell J, 2004, IEEE IJCNN, P1711
   Schemmel J, 2007, IEEE INT SYMP CIRC S, P3367, DOI 10.1109/ISCAS.2007.378289
   Schoenauer T, 2000, IEEE IJCNN, P490, DOI 10.1109/IJCNN.2000.860819
   SCHOENAUER T, 1998, NEURAL NETWORKS, V3728, P87
   SCHRAUWEN B, 2006, 2006 IEEE INT JOINT
   Schrauwen B, 2008, NEURAL NETWORKS, V21, P511, DOI 10.1016/j.neunet.2007.12.009
   Senn W, 2002, NEURAL COMPUT, V14, P583, DOI 10.1162/089976602317250915
   Seung HS, 2003, NEURON, V40, P1063, DOI 10.1016/S0896-6273(03)00761-X
   Silva Sergio M., 2007, 2006 International Joint Conference on Neural Networks, P3978
   Soltic S, 2008, IEEE IJCNN, P2091, DOI 10.1109/IJCNN.2008.4634085
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   SOUGNE JP, 2001, DEV EVOLUTION, P23
   STRAIN RJ, 2006, INT JOINT C NEUR NET, P3409
   THOMSON AM, 1994, TRENDS NEUROSCI, V17, P119, DOI 10.1016/0166-2236(94)90121-X
   Tino P, 2005, LECT NOTES COMPUT SC, V3611, P666
   Upegui A, 2005, MICROPROCESS MICROSY, V29, P211, DOI 10.1016/j.micpro.2004.08.012
   UPEGUI A, 2003, P INT C COMP INT ICI
   VOGELSTEIN R, 2005, ADV NEURAL INFORM PR, V17
   Vogelstein RJ, 2007, IEEE T NEURAL NETWOR, V18, P253, DOI 10.1109/TNN.2006.883007
   Wade JJ, 2008, IEEE IJCNN, P2648, DOI 10.1109/IJCNN.2008.4634169
   Weinstein RK, 2007, IEEE T NEUR SYS REH, V15, P83, DOI 10.1109/TNSRE.2007.891379
   Wu QX, 2006, NEUROCOMPUTING, V69, P1912, DOI 10.1016/j.neucom.2005.11.023
   Wysoski SG, 2006, LECT NOTES COMPUT SC, V4131, P61
   Xie XH, 2004, PHYS REV E, V69, DOI 10.1103/PhysRevE.69.041909
   Xin JG, 2001, IEEE IJCNN, P1772, DOI 10.1109/IJCNN.2001.938430
   ZIRPE M, 2007, THESIS U NEVADA
   Zou Q, 2006, NEUROCOMPUTING, V69, P1137, DOI 10.1016/j.neucom.2005.12.061
NR 98
TC 0
Z9 0
U1 0
U2 5
PD JUN
PY 2010
VL 3
IS 2
SI SI
BP 176
EP 189
WC Computer Science, Artificial Intelligence; Computer Science,
   Interdisciplinary Applications
DA 2023-11-11
ER

PT C
AU Wu, NC
   Chen, TH
   Huang, CT
AF Wu, Nai-Chun
   Chen, Tsu-Hsiang
   Huang, Chih-Tsun
GP IEEE
TI Hardware-aware Model Architecture for Ternary Spiking Neural Networks
SO 2023 INTERNATIONAL VLSI SYMPOSIUM ON TECHNOLOGY, SYSTEMS AND
   APPLICATIONS, VLSI-TSA/VLSI-DAT
DT Proceedings Paper
CT International VLSI Symposium on Technology, Systems and Applications
   (VLSI-TSA/VLSI-DAT)
CY APR 17-20, 2023
CL Hsinchu, TAIWAN
AB This paper presents a hardware-aware model architecture for ternary spiking neural networks. Under realistic hardware constraints, an effective training flow is proposed with ternary model weights and quantized voltage thresholds. In addition to the enhanced output decoding of the last-50%-spike to prevent the warm-up issue, our network also incorporates a hybrid architecture by attaching two simple fully-connected artificial neural layers as the output stage. Transforming from the asymmetrical ResNeXt architecture, the proposed SNN achieves a top-1 accuracy of 90% on the CIFAR-10 dataset, with a 5.9% improvement compared to the baseline model.
C1 [Wu, Nai-Chun; Chen, Tsu-Hsiang; Huang, Chih-Tsun] Natl Tsing Hua Univ, Dept Comp Sci, Hsinchu, Taiwan.
RP Wu, NC (corresponding author), Natl Tsing Hua Univ, Dept Comp Sci, Hsinchu, Taiwan.
CR Chuang PY, 2020, DES AUT CON, DOI 10.1109/dac18072.2020.9218714
   Deng L, 2020, NEURAL NETWORKS, V121, P294, DOI 10.1016/j.neunet.2019.09.005
   Deng S., 2022, INT C LEARNING REPRE
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Kugele A., 2021, DAGM GERM C PATT REC, P297, DOI [10.1007/978-3-030-92659-5_19, DOI 10.1007/978-3-030-92659-5_19]
   Li FF, 2016, Arxiv, DOI arXiv:1605.04711
   Tan P.- Y., 2020, ARXIV
   Tan PY, 2021, PROCEEDINGS OF THE 2021 DESIGN, AUTOMATION & TEST IN EUROPE CONFERENCE & EXHIBITION (DATE 2021), P575, DOI 10.23919/DATE51398.2021.9474151
   Wan W., 2020, S VLSI TECH, P1, DOI 10.1109/VLSITECHNOLOGY18217.2020
   Wu YJ, 2019, AAAI CONF ARTIF INTE, P1311
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634
   Yan BN, 2019, S VLSI TECH, pT86, DOI [10.23919/vlsit.2019.8776485, 10.23919/VLSIT.2019.8776485]
   Yoon JH, 2022, IEEE J SOLID-ST CIRC, V57, P845, DOI 10.1109/JSSC.2022.3141370
   Zheng HL, 2021, AAAI CONF ARTIF INTE, V35, P11062
NR 15
TC 0
Z9 0
U1 0
U2 0
PY 2023
DI 10.1109/VLSI-TSA/VLSI-DAT57221.2023.10134319
WC Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Zhong, Y
   Cui, XX
   Kuang, YS
   Liu, KF
   Wang, Y
   Huang, R
AF Zhong, Yi
   Cui, Xiaoxin
   Kuang, Yisong
   Liu, Kefei
   Wang, Yuan
   Huang, Ru
GP IEEE
TI A Spike-event-based Neuromorphic Processor with Enhanced On-chip STDP
   Learning in 28nm CMOS
SO 2021 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS (ISCAS)
SE IEEE International Symposium on Circuits and Systems
DT Proceedings Paper
CT IEEE International Symposium on Circuits and Systems (IEEE ISCAS)
CY MAY 22-28, 2021
CL Daegu, SOUTH KOREA
DE spiking neural network; neuromorphic processor; spike-timing-dependent
   plasticity (STDP); online learning
AB Event-based spiking neural network (SNN) has displayed a promising prospect to realize real-time, efficient and intelligent hardware platforms. Whereas great efforts are still being appealed to explore the possibility of introducing online learning abilities to neuromorphic systems. In this paper, a 28-nm CMOS neuromorphic processor is presented, fulfilling online learning by adopting counter and lookup table (LUT) based spike-timing-dependent plasticity (STDP) rule. Designed to work at high-precision scenarios, the presented processor integrates up to 1024 neurons and 256K signed 9-bit synapses. It also ensures chip array interconnection to fit large neural networks. Moreover, by utilizing the sparse property of spike events to minimize activity rate, the typical power consumption is further reduced to 3.348mW for training MNIST dataset.
C1 [Zhong, Yi; Cui, Xiaoxin; Kuang, Yisong; Liu, Kefei; Wang, Yuan; Huang, Ru] Peking Univ, Inst Microelect, Key Lab Microelect Devices & Circuits, Beijing 100871, Peoples R China.
RP Cui, XX (corresponding author), Peking Univ, Inst Microelect, Key Lab Microelect Devices & Circuits, Beijing 100871, Peoples R China.
EM cuixx@pku.edu.cn
CR Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   Chen GK, 2019, IEEE J SOLID-ST CIRC, V54, P992, DOI 10.1109/JSSC.2018.2884901
   Chistiakova M, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00089
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Frenkel C, 2019, IEEE T BIOMED CIRC S, V13, P999, DOI 10.1109/TBCAS.2019.2928793
   Frenkel C, 2019, IEEE T BIOMED CIRC S, V13, P145, DOI 10.1109/TBCAS.2018.2880425
   Furber SB, 2013, IEEE T COMPUT, V62, P2454, DOI 10.1109/TC.2012.142
   Huayaney FLM, 2016, IEEE INT SYMP CIRC S, P373, DOI 10.1109/ISCAS.2016.7527248
   Iakymchuk T, 2015, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-015-0059-4
   Kasabov N. K., 2019, SPRINGER SERIES BIO, P127
   Kim S, 2011, 2011 11TH INTERNATIONAL CONFERENCE ON CONTROL, AUTOMATION AND SYSTEMS (ICCAS), P1
   Mayr C, 2016, IEEE T BIOMED CIRC S, V10, P243, DOI 10.1109/TBCAS.2014.2379294
   Moradi S, 2018, IEEE T BIOMED CIRC S, V12, P106, DOI 10.1109/TBCAS.2017.2759700
   Qiao N, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00141
   Schemmel J, 2010, IEEE INT SYMP CIRC S, P1947, DOI 10.1109/ISCAS.2010.5536970
   Sim J., 2019 34 INT TECHN C, P1
NR 16
TC 4
Z9 4
U1 0
U2 4
PY 2021
DI 10.1109/ISCAS51556.2021.9401194
WC Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Nobukawa, S
   Nishimura, H
   Yamanishi, T
AF Nobukawa, Sou
   Nishimura, Haruhiko
   Yamanishi, Teruya
BE Wang, Y
   Kwong, S
   Feldman, J
   Howard, N
   Sheu, P
   Widrow, B
TI Emergent Patterns and Spontaneous Activity in Spiking Neural Networks
   with Dual Complex Network Structure
SO PROCEEDINGS OF 2018 IEEE 17TH INTERNATIONAL CONFERENCE ON COGNITIVE
   INFORMATICS & COGNITIVE COMPUTING (ICCI*CC 2018)
DT Proceedings Paper
CT 17th IEEE International Conference on Cognitive Informatics and
   Cognitive Computing (ICCI*CC)
CY JUL 16-18, 2018
CL Univ Calif, Berkeley, CA
HO Univ Calif
ID SIGNAL; CONNECTIVITY; DYNAMICS; BRAIN
AB In the cerebral cortex, the distribution of excitatory post-synaptic potential exhibits log-normal distribution. Recently, it has been reported that this distribution generates a spontaneous activity. Moreover, this distribution may have useful effect in enhancing abilities of associative memory recall and can induce burst spiking to play a crucial role in memory consolidation. The weak synaptic networks in this log-normal distribution exhibit random network characteristics, while the strong synaptic networks have small-world characteristics. The concern with the functionality of fluctuation of neural activity and duality of synaptic connectivity has been brought to public attention. Therefore, in this study, to determine the relationship between the complexity of spontaneous activity and duality of synaptic connectivity, we introduced a spiking neural network with the duality of synaptic connectivity. Subsequently, we conducted multi-scale entropy analysis for spontaneous activity and clustering analysis of emergent spiking pattern. The results revealed that in case wherein strong synaptic connections exhibit intermediate characteristic of small world network, specific spiking patterns arise among the spatio-temporal irregular spiking activity. Additionally, multi-scale entropy profile of the spiking activity exhibits a unimodal maximum peak at a slow temporal scale corresponding to the profile of the actual brain activity.
C1 [Nobukawa, Sou] Chiba Inst Technol, Dept Comp Sci, 2-17-1 Tsudanuma, Narashino, Chiba 2750016, Japan.
   [Nishimura, Haruhiko] Univ Hyogo, Grad Sch Appl Informat, 7-1-28 Chuo Ku, Kobe, Hyogo 6508588, Japan.
   [Yamanishi, Teruya] Fukui Univ Technol, Dept Management Informat Sci, 3-6-1 Gakuen, Fukui, Fukui 9108505, Japan.
RP Nobukawa, S (corresponding author), Chiba Inst Technol, Dept Comp Sci, 2-17-1 Tsudanuma, Narashino, Chiba 2750016, Japan.
EM nobukawa@cs.it-chiba.ac.jp; haru@ai.u-hyogo.ac.jp;
   yamanisi@fukui-ut.ac.jp
CR Allen EA, 2014, CEREB CORTEX, V24, P663, DOI 10.1093/cercor/bhs352
   Betzel RF, 2016, NEUROIMAGE, V127, P287, DOI 10.1016/j.neuroimage.2015.12.001
   Betzel RF, 2012, FRONT COMPUT NEUROSC, V6, DOI 10.3389/fncom.2012.00074
   Calhoun VD, 2014, NEURON, V84, P262, DOI 10.1016/j.neuron.2014.10.015
   Costa M, 2002, PHYS REV LETT, V89, DOI 10.1103/PhysRevLett.89.068102
   Denève S, 2017, NEURON, V94, P969, DOI 10.1016/j.neuron.2017.05.016
   Destexhe A, 2009, J COMPUT NEUROSCI, V27, P493, DOI 10.1007/s10827-009-0164-4
   Fiser J, 2004, NATURE, V431, P573, DOI 10.1038/nature02907
   Garrett DD, 2011, J NEUROSCI, V31, P4496, DOI 10.1523/JNEUROSCI.5641-10.2011
   Garrett DD, 2010, J NEUROSCI, V30, P4914, DOI 10.1523/JNEUROSCI.5166-09.2010
   Guo DQ, 2010, IEEE T NEURAL NETWOR, V21, P895, DOI 10.1109/TNN.2010.2044419
   Hiratani N, 2013, FRONT COMPUT NEUROSC, V6, DOI 10.3389/fncom.2012.00102
   Hromadka T, 2008, PLOS BIOL, V6, P124, DOI 10.1371/journal.pbio.0060016
   Jonke Z, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00118
   Koren V, 2017, PLOS COMPUT BIOL, V13, DOI 10.1371/journal.pcbi.1005355
   Lefort S, 2009, NEURON, V61, P301, DOI 10.1016/j.neuron.2008.12.020
   McCormick DA, 1999, SCIENCE, V285, P541, DOI 10.1126/science.285.5427.541
   McDonnell MD, 2011, NAT REV NEUROSCI, V12, P415, DOI 10.1038/nrn3061
   McIntosh AR, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.1000106
   Omura Y, 2015, J NEUROSCI, V35, P14585, DOI 10.1523/JNEUROSCI.4944-14.2015
   Sakata S, 2009, NEURON, V64, P404, DOI 10.1016/j.neuron.2009.09.020
   SOFTKY WR, 1993, J NEUROSCI, V13, P334
   Song S, 2005, PLOS BIOL, V3, P507, DOI 10.1371/journal.pbio.0030068
   Takahashi T, 2013, PROG NEURO-PSYCHOPH, V45, P258, DOI 10.1016/j.pnpbp.2012.05.001
   Teramae J, 2012, SCI REP-UK, V2, DOI 10.1038/srep00485
   Vogels TP, 2005, J NEUROSCI, V25, P10786, DOI 10.1523/JNEUROSCI.3508-05.2005
   Watanabe Y, 2016, ASIA JT CONF INF SEC, P115, DOI 10.1109/AsiaJCIS.2016.26
   Watts DJ, 1998, NATURE, V393, P440, DOI 10.1038/30918
   Yang AC, 2013, PROG NEURO-PSYCHOPH, V45, P253, DOI 10.1016/j.pnpbp.2012.09.015
   Zalesky A, 2014, P NATL ACAD SCI USA, V111, P10341, DOI 10.1073/pnas.1400181111
NR 30
TC 2
Z9 2
U1 0
U2 0
PY 2018
BP 159
EP 165
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods
DA 2023-11-11
ER

PT J
AU Eskandari, E
   Ahmadi, A
   Gomar, S
AF Eskandari, Elahe
   Ahmadi, Arash
   Gomar, Shaghayegh
TI Effect of spike-timing-dependent plasticity on neural assembly computing
SO NEUROCOMPUTING
DT Article
DE Izhikevich model; Neural assembly computing (NAC); Spike
   timing-dependent plasticity (STDP); Spiking neural network (SNN)
ID SYNCHRONY
AB Spiking neural network (SNNs) are practical and realistic neural models, which have attracted considerable attention and many valuable physical realizations have been implemented. Neural assembly computing (NAC) is a new approach to SNNs, which is known to be a promising mechanism for explaining large-scale neural behavior, and it has been used to examine and explore the computational activities of neural cell assemblies. To obtain algorithms based on NAC, neural coalitions are considered to be responsible for patterns, memorizing them, and controlling their hierarchical relatives. In addition, spike timing-dependent plasticity (STDP) can be employed as a synaptic plasticity learning rule to modify the synaptic weights in neural networks, thereby allowing the convergence of neural activities to a spatiotemporal neuron-network pattern. Thus, applying STDP to NAC can be a powerful tool in neuroscience computing. Investigations in this area are also useful for understanding biological systems. In this study, we investigated the effect of applying STDP to NAC. Our simulation results showed that applying STDP rule increased the average number of firings for each event in neural assemblies by adjusting the weights of the connections. Moreover, the firing of neural assemblies resulted in a sequence of events in a closed loop. We determined correlations to measure the similarity between the patterns in each assembly, which showed that STDP made the network fire with a more distinctive and similar pattern. In addition, after memorizing a pattern, the frequency of events increased and the firing patterns became faster. Therefore, STDP can improve and accelerate the overall NAC process. Thus, our simulation results demonstrate that STDP can help NAC to obtain a more distinctive pattern in the network output. (C) 2016 Elsevier B.V. All rights reserved.
C1 [Eskandari, Elahe; Ahmadi, Arash] Razi Univ, Dept Elect Engn, Kermanshah 6714967346, Iran.
   [Gomar, Shaghayegh] Univ Windsor, Elect & Comp Dept, Windsor, ON N9B 3P4, Canada.
RP Ahmadi, A (corresponding author), Razi Univ, Dept Elect Engn, Kermanshah 6714967346, Iran.
EM eeskandari90@gmail.com; aahmadi@razi.ac.ir; gomar@uwindsor.ca
CR Abbott LF, 2000, NAT NEUROSCI, V3, P1178, DOI 10.1038/81453
   [Anonymous], 1992, ARTIFICIAL NEURAL NE
   [Anonymous], THESIS
   [Anonymous], 2005, NEURAL NETWORKS METH, DOI DOI 10.1007/3-540-28847-3
   Baladron J, 2015, NEURAL NETWORKS, V67, P1, DOI 10.1016/j.neunet.2015.03.002
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Buzsáki G, 2010, NEURON, V68, P362, DOI 10.1016/j.neuron.2010.09.023
   Caporale N, 2008, ANNU REV NEUROSCI, V31, P25, DOI 10.1146/annurev.neuro.31.060407.125639
   Creed R. S., 1932, REFLEX ACTIVITY SPIN
   de Oliveira-Neto Rodrigues, 2014, P IEEE INT JOINT C N
   Desai NS, 2002, NAT NEUROSCI, V5, P783, DOI 10.1038/nn878
   Gerstner W., 2002, SPIKING NEURON MODEL
   Gomar S, 2014, IEEE T CIRCUITS-I, V61, P1206, DOI 10.1109/TCSI.2013.2286030
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Kempter R, 1999, PHYS REV E, V59, P4498, DOI 10.1103/PhysRevE.59.4498
   Laurent G, 2002, NAT REV NEUROSCI, V3, P884, DOI 10.1038/nrn964
   Paugam-Moisy H, 2012, HDB NATURAL COMPUTIN, V1, P1, DOI DOI 10.1007/978-3-540-92910-9_10
   Ranhel J, 2012, IEEE T NEUR NET LEAR, V23, P916, DOI 10.1109/TNNLS.2012.2190421
   Ranhel Joao, 2013, P 2013 BRICS C IEEE
   Singer W, 1999, CURR OPIN NEUROBIOL, V9, P189, DOI 10.1016/S0959-4388(99)80026-9
   Singer W, 1999, NEURON, V24, P49, DOI 10.1016/S0896-6273(00)80821-1
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Uhlhaas PJ, 2009, FRONT INTEGR NEUROSC, V3, DOI 10.3389/neuro.07.017.2009
   Varela F, 2001, NAT REV NEUROSCI, V2, P229, DOI 10.1038/35067550
   Vreeken Jilles, 2002, UUCS20032008
   Wang XJ, 2001, TRENDS NEUROSCI, V24, P455, DOI 10.1016/S0166-2236(00)01868-3
   Wittenberg GM, 2006, J NEUROSCI, V26, P6610, DOI 10.1523/JNEUROSCI.5388-05.2006
NR 28
TC 7
Z9 10
U1 0
U2 10
PD MAY 26
PY 2016
VL 191
BP 107
EP 116
DI 10.1016/j.neucom.2016.01.003
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT C
AU Garrido, JA
   Ros, E
AF Garrido, Jesus A.
   Ros, Eduardo
BE Lintas, A
   Rovetta, S
   Verschure, PFMJ
   Villa, AEP
TI Sparse Pattern Representation in a Realistic Recurrent Spiking Neural
   Network
SO ARTIFICIAL NEURAL NETWORKS AND MACHINE LEARNING - ICANN 2017, PT I
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 26th International Conference on Artificial Neural Networks (ICANN)
CY SEP 11-14, 2017
CL Alghero, ITALY
DE Pattern representation; Recurrent networks; Oscillations; Cerebellum
C1 [Garrido, Jesus A.; Ros, Eduardo] Univ Granada, CITIC UGR, Dept Comp Architecture & Technol, Granada, Spain.
RP Garrido, JA; Ros, E (corresponding author), Univ Granada, CITIC UGR, Dept Comp Architecture & Technol, Granada, Spain.
EM jesusgarrido@ugr.es; eros@ugr.es
CR Asai Y, 2012, BRAIN RES, V1434, P17, DOI 10.1016/j.brainres.2011.10.012
   Cayco-Gajic A., 2017, BIORXIV
   Garrido JA, 2016, INT J NEURAL SYST, V26, DOI 10.1142/S0129065716500209
NR 3
TC 0
Z9 0
U1 0
U2 0
PY 2017
VL 10613
BP 434
EP 435
PN I
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods
DA 2023-11-11
ER

PT J
AU Nikonov, DE
   Young, IA
AF Nikonov, Dmitri E.
   Young, Ian A.
TI Benchmarking Delay and Energy of Neural Inference Circuits
SO IEEE JOURNAL ON EXPLORATORY SOLID-STATE COMPUTATIONAL DEVICES AND
   CIRCUITS
DT Article
DE Benchmarking; beyond-CMOS; CNN; neural network; neuromorphic; power;
   spiking; spintronic; throughput
ID HARDWARE; NETWORKS; DEVICES
AB Neural network circuits and architectures are currently under active research for applications to artificial intelligence and machine learning. Their physical performance metrics (area, time, and energy) are estimated. Various types of neural networks (artificial, cellular, spiking, and oscillator) are implemented with multiple CMOS and beyond-CMOS (spintronic, ferroelectric, and resistive memory) devices. A consistent and transparent methodology is proposed and used to benchmark this comprehensive set of options across several application cases. Promising architecture/device combinations are identified.
C1 [Nikonov, Dmitri E.; Young, Ian A.] Intel Corp, Components Res, Hillsboro, OR 97124 USA.
RP Nikonov, DE (corresponding author), Intel Corp, Components Res, Hillsboro, OR 97124 USA.
EM dmitri.e.nikonov@intel.com
CR [Anonymous], FRONTIERS NEUROSCI
   [Anonymous], BENCHMARKING DEVICES
   [Anonymous], ADV NEURAL INFORM PR
   [Anonymous], 2016, P IEEE INT S CIRC SY, DOI DOI 10.48550/ARXIV.1605.07678
   [Anonymous], P S VLSI TECHN KYOT
   [Anonymous], SUPPLEMENTARY MAT TH
   Blouw P., 2018, ARXIV181201739
   Canziani A., 2017, PROC IEEE INT S CIRC, P1
   Chatterjee B, 2019, IEEE T VLSI SYST, V27, P1365, DOI 10.1109/TVLSI.2019.2896611
   Chen P.-Y., 2017, IEEE INT EL DEV M IE
   Chen PY, 2018, IEEE T COMPUT AID D, V37, P3067, DOI 10.1109/TCAD.2018.2789723
   Du ZD, 2015, PROCEEDINGS OF THE 48TH ANNUAL IEEE/ACM INTERNATIONAL SYMPOSIUM ON MICROARCHITECTURE (MICRO-48), P494, DOI 10.1145/2830772.2830789
   Furber S, 2016, J NEURAL ENG, V13, DOI 10.1088/1741-2560/13/5/051001
   Haensch W, 2019, P IEEE, V107, P108, DOI 10.1109/JPROC.2018.2871057
   Hasler J, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00118
   Indiveri G, 2015, P IEEE, V103, P1379, DOI 10.1109/JPROC.2015.2444094
   Jouppi NP, 2017, 44TH ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA 2017), P1, DOI 10.1145/3079856.3080246
   LECUN Y, 1989, IEEE COMMUN MAG, V27, P41, DOI 10.1109/35.41400
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y, 2015, NATURE, V521, p7553 436 444, DOI [10.1038/nature14539, DOI 10.1038/NATURE14539]
   Marinella MJ, 2018, IEEE J EM SEL TOP C, V8, P86, DOI 10.1109/JETCAS.2018.2796379
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Misra J, 2010, NEUROCOMPUTING, V74, P239, DOI 10.1016/j.neucom.2010.03.021
   Moore GE, 1998, P IEEE, V86, P82, DOI 10.1109/JPROC.1998.658762
   Nikonov DE, 2015, IEEE J EXPLOR SOLID-, V1, P3, DOI 10.1109/JXCDC.2015.2418033
   Nikonov DE, 2013, P IEEE, V101, P2498, DOI 10.1109/JPROC.2013.2252317
   Pan CY, 2016, IEEE J EXPLOR SOLID-, V2, P36, DOI 10.1109/JXCDC.2016.2633251
   Schuman CD., 2017, ARXIV
   Sengupta A, 2017, IEEE IJCNN, P4557, DOI 10.1109/IJCNN.2017.7966434
   Sengupta A, 2016, PHYS REV APPL, V6, DOI 10.1103/PhysRevApplied.6.064003
   Sharp T, 2012, J NEUROSCI METH, V210, P110, DOI 10.1016/j.jneumeth.2012.03.001
   Srinivasan G, 2016, SCI REP-UK, V6, DOI 10.1038/srep29545
   Sze V, 2017, P IEEE, V105, P2295, DOI 10.1109/JPROC.2017.2761740
   Thakur CS, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00891
   Yang TJ, 2017, CONF REC ASILOMAR C, P1916, DOI 10.1109/ACSSC.2017.8335698
   Yu SM, 2018, P IEEE, V106, P260, DOI 10.1109/JPROC.2018.2790840
   Zaveri MS, 2011, NEURAL NETWORKS, V24, P291, DOI 10.1016/j.neunet.2010.12.003
NR 37
TC 10
Z9 10
U1 1
U2 4
PD DEC
PY 2019
VL 5
IS 2
BP 75
EP 84
DI 10.1109/JXCDC.2019.2956112
PN 1
WC Computer Science, Hardware & Architecture
DA 2023-11-11
ER

PT C
AU Mesquida, T
   Valentian, A
   Bol, D
   Beigne, E
AF Mesquida, Thomas
   Valentian, Alexandre
   Bol, David
   Beigne, Edith
GP IEEE
TI Impact of the AER-Induced Timing Distortion on Spiking Neural Networks
   Implementing DSP
SO 2016 12TH CONFERENCE ON PH.D. RESEARCH IN MICROELECTRONICS AND
   ELECTRONICS (PRIME)
DT Proceedings Paper
CT 12th Conference on Ph.D. Research in Microelectronics and Electronics
   (PRIME)
CY JUN 27-30, 2016
CL Lisbon, PORTUGAL
AB Spiking Neural Networks are considered to be the latest generation of artificial neural networks. They rely on principles inspired from brain operation and use coding strategies based on relative timings and/or rates to transport information. Implementing large hardware networks requires the use of Address Event Representation (AER) which affects the inter-spike timings. This paper focuses on various mathematical AER models and their influence on spike rate coding in Signal Processing applications implemented with SNN. For the most basic M/M/1 queue model, there is no effect on the results given by the considered benchmark multiplier operator, whereas for more realistic M/D/1 and M/G/1 models, the relative error comparing to Poisson process is respectively below 3% and 3.8% for realistic operations.
C1 [Mesquida, Thomas; Valentian, Alexandre; Beigne, Edith] Univ Grenoble Alpes, CEA, LETI, MINATEC Campus, F-38054 Grenoble, France.
   [Bol, David] Catholic Univ Louvain, ICTEAM Inst, Louvain La Neuve, Belgium.
RP Mesquida, T (corresponding author), Univ Grenoble Alpes, CEA, LETI, MINATEC Campus, F-38054 Grenoble, France.
EM thomas.mesquida@cea.fr; alexandre.valentian@cea.fr;
   david.bol@uclouvain.be; edith.beigne@cea.fr
CR [Anonymous], 2013, P INT S COMPUTER ARC, DOI [DOI 10.1145/2508148.2485923), DOI 10.1145/2485922.2485923]
   Boahen KA, 2000, IEEE T CIRCUITS-II, V47, P416, DOI 10.1109/82.842110
   Gross D., 2011, WILEY SERIES PROBABI
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Maass W., 1996, NEURAL INFORM PROCES, P211
   SRINIVASAN MV, 1976, BIOL CYBERN, V21, P227, DOI 10.1007/BF00344168
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   Zamarreño-Ramos C, 2013, IEEE T BIOMED CIRC S, V7, P82, DOI 10.1109/TBCAS.2012.2195725
NR 8
TC 1
Z9 1
U1 0
U2 0
PY 2016
WC Computer Science, Hardware & Architecture; Engineering, Electrical &
   Electronic
DA 2023-11-11
ER

PT C
AU Van Vaerenbergh, T
   Fiers, M
   Dambre, J
   Bienstman, P
AF Van Vaerenbergh, Thomas
   Fiers, Martin
   Dambre, Joni
   Bienstman, Peter
GP IEEE
TI An optical delayline based on excitable microrings
SO 2014 IEEE PHOTONICS CONFERENCE (IPC)
SE IEEE Photonics Conference
DT Proceedings Paper
CT 27th IEEE Photonics Conference (IPC)
CY OCT 12-16, 2014
CL San Diego, CA
AB We present in simulation a photonic neural circuit achieving a 200 ns spike delay, based on excitability in microrings. This type of delayline paves the way towards fully integrated optical spiking neural networks.
C1 [Van Vaerenbergh, Thomas; Fiers, Martin; Bienstman, Peter] Univ Ghent, IMEC, Dept Informat Technol, Photon Res Grp, B-9000 Ghent, Belgium.
   [Van Vaerenbergh, Thomas; Fiers, Martin; Bienstman, Peter] Univ Ghent, Ctr Nano & Biophoton NB Photon, B-9000 Ghent, Belgium.
   [Dambre, Joni] Univ Ghent, IMEC, Elect & Informat Syst, Comp Syst Lab, B-9000 Ghent, Belgium.
RP Van Vaerenbergh, T (corresponding author), Univ Ghent, IMEC, Dept Informat Technol, Photon Res Grp, B-9000 Ghent, Belgium.
EM thomas.vanvaerenbergh@intec.ugent.be
CR Brunstein M, 2012, PHYS REV A, V85, DOI 10.1103/PhysRevA.85.031803
   Coomans W, 2011, PHYS REV E, V84, DOI 10.1103/PhysRevE.84.036209
   Fiers M, 2012, J OPT SOC AM B, V29, P896, DOI 10.1364/JOSAB.29.000896
   Melloni A, 2010, IEEE PHOTONICS J, V2, P181, DOI 10.1109/JPHOT.2010.2044989
   Nahmias MA, 2013, IEEE J SEL TOP QUANT, V19, DOI 10.1109/JSTQE.2013.2257700
   Van Vaerenbergh T, 2012, PHYS REV A, V86, DOI 10.1103/PhysRevA.86.063808
   Van Vaerenbergh T, 2012, OPT EXPRESS, V20, P20292, DOI 10.1364/OE.20.020292
   Xia FN, 2007, NAT PHOTONICS, V1, P65, DOI 10.1038/nphoton.2006.42
   Xiao Z, 2013, OPT EXPRESS, V21, P21285, DOI 10.1364/OE.21.021285
   Yacomotti AM, 2006, PHYS REV LETT, V97, DOI 10.1103/PhysRevLett.97.143904
NR 10
TC 1
Z9 1
U1 0
U2 12
PY 2014
BP 118
EP 119
WC Engineering, Electrical & Electronic; Optics
DA 2023-11-11
ER

PT J
AU Afshar, S
   Cohen, GK
   Wang, RM
   Van Schaik, A
   Tapson, J
   Lehmann, T
   Hamilton, TJ
AF Afshar, Saeed
   Cohen, Gregory K.
   Wang, Runchun M.
   Van Schaik, Andre
   Tapson, Jonathan
   Lehmann, Torsten
   Hamilton, Tara J.
TI The ripple pond: enabling spiking networks to see
SO FRONTIERS IN NEUROSCIENCE
DT Article
DE object recognition; spiking neural network; neuromorphic engineering;
   image transformation invariance; view invariance; polychronous network
ID PATTERN-RECOGNITION; COMPUTATIONAL PRINCIPLES; OBJECT RECOGNITION;
   VISUAL RECOGNITION; SPIRAL WAVES; MODEL; INVARIANT; VISION;
   SYNCHRONIZATION; ATTENTION
AB We present the biologically inspired Ripple Pond Network (RPN), a simply connected spiking neural network which performs a transformation converting two dimensional images to one dimensional temporal patterns (TP) suitable for recognition by temporal coding learning and memory networks. The RPN has been developed as a hardware solution linking previously implemented neuromorphic vision and memory structures such as frameless vision sensors and neuromorphic temporal coding spiking neural networks. Working together such systems are potentially capable of delivering end-to-end high-speed, low-power and low-resolution recognition for mobile and autonomous applications where slow, highly sophisticated and power hungry signal processing solutions are ineffective. Key aspects in the proposed approach include utilizing the spatial properties of physically embedded neural networks and propagating waves of activity therein for information processing, using dimensional collapse of imagery information into amenable TP and the use of asynchronous frames for information binding.
C1 [Afshar, Saeed; Cohen, Gregory K.; Wang, Runchun M.; Van Schaik, Andre; Tapson, Jonathan; Hamilton, Tara J.] Univ Western Sydney, MARCS Inst, Penrith, NSW 2751, Australia.
   [Afshar, Saeed; Lehmann, Torsten; Hamilton, Tara J.] Univ New S Wales, Sch Elect Engn & Telecommun, Sydney, NSW 2052, Australia.
RP Hamilton, TJ (corresponding author), Univ Western Sydney, MARCS Inst, Locked Bag 1797, Penrith, NSW 2751, Australia.
EM t.hamilton@uws.edu.au
CR Adamatzky A, 2002, PHYS LETT A, V297, P344, DOI 10.1016/S0375-9601(02)00289-X
   Afshari S, 2012, PROCEEDINGS OF THE 3RD INTERNATIONAL CONFERENCE ON INFORMATION MANAGEMENT AND EVALUATION, P1
   [Anonymous], 2001, ECHO STATE APPROACH
   [Anonymous], 2005, 23 PROBLEMS SYSTEMS
   Pérez-Carrasco JA, 2013, IEEE T PATTERN ANAL, V35, P2706, DOI 10.1109/TPAMI.2013.71
   Avarguès-Weber A, 2010, J EXP BIOL, V213, P593, DOI 10.1242/jeb.039263
   Bartolozzi C., 2012, 2012 IEEE COMP SOC C, P76, DOI [10.1109/CVPRW.2012.6238898, DOI 10.1109/CVPRW.2012.6238898]
   Bressloff PC, 2002, NEURAL COMPUT, V14, P473, DOI 10.1162/089976602317250861
   Brunel N, 2000, J COMPUT NEUROSCI, V8, P183, DOI 10.1023/A:1008925309027
   BULTHOFF HH, 1995, CEREB CORTEX, V5, P247, DOI 10.1093/cercor/5.3.247
   Buschman TJ, 2009, NEURON, V63, P386, DOI 10.1016/j.neuron.2009.06.020
   Caley MJ, 2003, P ROY SOC B-BIOL SCI, V270, P667, DOI 10.1098/rspb.2002.2263
   Carandini M, 2012, NAT REV NEUROSCI, V13, P51, DOI 10.1038/nrn3136
   CAVANAGH P, 1978, PERCEPTION, V7, P167, DOI 10.1068/p070167
   Chessa Manuela, 2011, Computer Vision Systems. Proceedings 8th International Conference (ICVS 2011), P41, DOI 10.1007/978-3-642-23968-7_5
   Chicca E, 2007, IEEE T CIRCUITS-I, V54, P981, DOI 10.1109/TCSI.2007.893509
   Choi TYW, 2005, IEEE T CIRCUITS-I, V52, P1049, DOI 10.1109/TCSI.2005.849136
   Dahlem MA, 1997, EXP BRAIN RES, V115, P319, DOI 10.1007/PL00005700
   Dahlem MA, 2004, PROG NEUROBIOL, V74, P351, DOI 10.1016/j.pneurobio.2004.10.003
   Dahlem MA, 2009, PLOS ONE, V4, DOI 10.1371/journal.pone.0005007
   Dehaene S, 2011, NEURON, V70, P200, DOI 10.1016/j.neuron.2011.03.018
   DILL M, 1993, NATURE, V365, P751, DOI 10.1038/365751a0
   Drazen D, 2011, EXP FLUIDS, V51, P1465, DOI 10.1007/s00348-011-1207-y
   Dudek P, 2005, IEEE T CIRCUITS-I, V52, P13, DOI 10.1109/TCSI.2004.840093
   Fang L, 2009, SPATIAL VISION, V22, P45, DOI 10.1163/156856809786618484
   Farabet C, 2012, FRONT NEUROSCI-SWITZ, V6, DOI 10.3389/fnins.2012.00032
   Faugeras O., 1993, 3 DIMENSIONAL COMPUT
   Fernando C, 2003, LECT NOTES ARTIF INT, V2801, P588
   Folowosele F, 2011, IEEE J EM SEL TOP C, V1, P516, DOI 10.1109/JETCAS.2012.2183409
   Fries P, 2009, ANNU REV NEUROSCI, V32, P209, DOI 10.1146/annurev.neuro.051508.135603
   Furber SB, 2013, IEEE T COMPUT, V62, P2454, DOI 10.1109/TC.2012.142
   Gao DS, 2009, NEURAL COMPUT, V21, P239, DOI 10.1162/neco.2009.11-06-391
   Gattass R, 2005, PHILOS T R SOC B, V360, P709, DOI 10.1098/rstb.2005.1629
   Gherardi F, 2012, ANIM COGN, V15, P745, DOI 10.1007/s10071-012-0513-y
   Gong BQ, 2012, PROC CVPR IEEE, P2066, DOI 10.1109/CVPR.2012.6247911
   Gregoriou GG, 2009, SCIENCE, V324, P1207, DOI 10.1126/science.1171402
   Gütig R, 2009, PLOS BIOL, V7, DOI 10.1371/journal.pbio.1000141
   Hall T. S., 2004, P 18 INT PAR DISTR P, V142, P26, DOI [10.1109/IPDPS.2004.130312, DOI 10.1109/IPDPS.2004.130312]
   Hamilton TJ, 2011, IEEE INT SYMP CIRC S, P865
   Horridge A, 2009, J INSECT PHYSIOL, V55, P499, DOI 10.1016/j.jinsphys.2009.03.006
   Huang XY, 2004, J NEUROSCI, V24, P9897, DOI 10.1523/JNEUROSCI.2705-04.2004
   Huerta R, 2009, NEURAL COMPUT, V21, P2123, DOI 10.1162/neco.2009.03-08-733
   Hussain S, 2012, 2012 IEEE ASIA PACIFIC CONFERENCE ON CIRCUITS AND SYSTEMS (APCCAS), P304, DOI 10.1109/APCCAS.2012.6419032
   Iftekharuddin KM, 2011, IEEE T NEURAL NETWOR, V22, P906, DOI 10.1109/TNN.2011.2132737
   Indiveri G, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00073
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Itti L, 2001, NAT REV NEUROSCI, V2, P194, DOI 10.1038/35058500
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Izhikevich EM, 2009, INT J BIFURCAT CHAOS, V19, P1733, DOI 10.1142/S0218127409023809
   Jhuang H, 2007, IEEE I CONF COMP VIS, P1253
   KOCH C, 1983, P NATL ACAD SCI-BIOL, V80, P2799, DOI 10.1073/pnas.80.9.2799
   KOHONEN T, 1982, BIOL CYBERN, V43, P59, DOI 10.1007/BF00337288
   Land MF, 1999, J COMP PHYSIOL A, V185, P341, DOI 10.1007/s003590050393
   Lazar AA, 2011, IEEE T NEURAL NETWOR, V22, P461, DOI 10.1109/TNN.2010.2103323
   Levy WB, 1996, NEURAL COMPUT, V8, P531, DOI 10.1162/neco.1996.8.3.531
   Loxley PN, 2011, EPL-EUROPHYS LETT, V93, DOI 10.1209/0295-5075/93/64001
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Maass W, 2007, LECT NOTES COMPUT SC, V4497, P507
   Martinez D, 2005, NEURAL COMPUT, V17, P2548, DOI 10.1162/089976605774320566
   McDonnell MD, 2012, BRAIN RES, V1434, P162, DOI 10.1016/j.brainres.2011.08.070
   Meador KJ, 2002, NEUROLOGY, V59, P847, DOI 10.1212/WNL.59.6.847
   Meng Y, 2011, IEEE T NEURAL NETWOR, V22, P1952, DOI 10.1109/TNN.2011.2171044
   Nakamura K, 2002, IEEE IJCNN, P2439, DOI 10.1109/IJCNN.2002.1007524
   Neri P, 2012, ANIM BEHAV, V84, P485, DOI 10.1016/j.anbehav.2012.06.005
   Norouzi M, 2009, PROC CVPR IEEE, P2727
   OLSHAUSEN BA, 1993, J NEUROSCI, V13, P4700
   Pardo F, 1998, IEEE J SOLID-ST CIRC, V33, P842, DOI 10.1109/4.678644
   Paugam-Moisy H, 2008, NEUROCOMPUTING, V71, P1143, DOI 10.1016/j.neucom.2007.12.027
   Pinto N, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.0040027
   Postma EO, 1997, NEURAL NETWORKS, V10, P993, DOI 10.1016/S0893-6080(97)00034-8
   Ranhel J, 2012, IEEE T NEUR NET LEAR, V23, P916, DOI 10.1109/TNNLS.2012.2190421
   Rasche C, 2007, IEEE T NEURAL NETWOR, V18, P520, DOI 10.1109/TNN.2006.884679
   REITBOECK HJ, 1984, BIOL CYBERN, V51, P113, DOI 10.1007/BF00357924
   Riesenhuber M, 1999, NAT NEUROSCI, V2, P1019, DOI 10.1038/14819
   Serrano-Gotarredona R, 2009, IEEE T NEURAL NETWOR, V20, P1417, DOI 10.1109/TNN.2009.2023653
   Serre T., 2005, 2005036 AI CBCL
   Seth AK, 2004, CEREB CORTEX, V14, P1185, DOI 10.1093/cercor/bhh079
   Shi B. E., 2006, P 2006 IEEE INT S CI, DOI [10.1109/ISCAS.2006.1693407, DOI 10.1109/ISCAS.2006.1693407]
   Sivilotti M., 1991, WIRING CONSIDERATION
   Sountsov P, 2011, FRONT COMPUT NEUROSC, V5, DOI 10.3389/fncom.2011.00053
   Tapson J, 2013, NEURAL NETWORKS, V45, P94, DOI 10.1016/j.neunet.2013.02.008
   Tapson JC, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00153
   Traver VJ, 2010, ROBOT AUTON SYST, V58, P378, DOI 10.1016/j.robot.2009.10.002
   Tricarico E, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0018710
   Van der Velden J, 2008, PLOS ONE, V3, DOI 10.1371/journal.pone.0001695
   Van Rullen R, 2001, NEURAL COMPUT, V13, P1255, DOI 10.1162/08997660152002852
   VanRullen R, 2007, P NATL ACAD SCI USA, V104, P19204, DOI 10.1073/pnas.0707316104
   Vogelstein RJ, 2007, NEURAL COMPUT, V19, P2281, DOI 10.1162/neco.2007.19.9.2281
   Volman V, 2010, PLOS COMPUT BIOL, V6, DOI 10.1371/journal.pcbi.1000973
   Wang R., 2011, 2011 7 INT C INT SEN, P97, DOI [10.1109/ISSNIP.2011.6146572, DOI 10.1109/ISSNIP.2011.6146572]
   Wang RC, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00014
   Wang XJ, 2010, PHYSIOL REV, V90, P1195, DOI 10.1152/physrev.00035.2008
   Wills S., 2004, THESIS U CAMBRIDGE C
   Wu JY, 2008, NEUROSCIENTIST, V14, P487, DOI 10.1177/1073858408317066
   Yu YF, 2012, P NATL ACAD SCI USA, V109, P2585, DOI 10.1073/pnas.1121111109
   Zelnik-Manor L., 2001, COMP VIS PATT REC 20
NR 96
TC 7
Z9 7
U1 0
U2 6
PY 2013
VL 7
AR 212
DI 10.3389/fnins.2013.00212
WC Neurosciences
DA 2023-11-11
ER

PT C
AU DUCHATEAU, G
   LANSNER, A
AF DUCHATEAU, G
   LANSNER, A
BE KOHONEN, T
   MAKISARA, K
   SIMULA, O
   KANGAS, J
TI A BAYESIAN ARTIFICIAL NEURAL NETWORK WITH SPIKING UNITS
SO ARTIFICIAL NEURAL NETWORKS, VOLS 1 AND 2
DT Proceedings Paper
CT INTERNATIONAL CONF ON ARTIFICIAL NEURAL NETWORKS ( ICANN-91 )
CY JUN 24-28, 1991
CL ESPOO, FINLAND
NR 0
TC 0
Z9 0
U1 0
U2 0
PY 1991
BP 525
EP 530
DA 2023-11-11
ER

PT J
AU Xiang, SY
   Zhang, TR
   Han, YA
   Guo, XX
   Zhang, YH
   Shi, YC
   Hao, Y
AF Xiang, Shuiying
   Zhang, Tianrui
   Han, Yanan
   Guo, Xingxing
   Zhang, Yahui
   Shi, Yuechun
   Hao, Yue
TI Neuromorphic Speech Recognition With Photonic Convolutional Spiking
   Neural Networks
SO IEEE JOURNAL OF SELECTED TOPICS IN QUANTUM ELECTRONICS
DT Article
DE Feature extraction; Photonics; Neurons; Convolution; Speech recognition;
   Spectrogram; Kernel; Photonic convolutional spiking neural network;
   speech recognition; supervised learning
ID INTELLIGENCE; FEATURES
AB Spiking neural network (SNN) have attracted lots of attention due to its event-driven nature and powerful computation capability. However, it is still limited to simple task due to the training difficulty. In this work, we propose a hybrid architecture of photonic convolutional spiking neural network (PCSNN) to realize the speech recognition task. In the PCSNN, the feature extraction is realized by a convolution SNN with unsupervised learning algorithm, the classification is realized by a photonic SNN with modified time-based supervised training algorithm. The TIDIGITS dataset is used to test the speech recognition performance of the proposed PCSNN, and the highest testing accuracy is 93.75%. The proposed PCSNN provides a solution for architecture and algorithm co-design for the speech recognition task, which is helpful for extending the applications of photonic SNN.
C1 [Xiang, Shuiying; Zhang, Tianrui; Han, Yanan; Guo, Xingxing; Zhang, Yahui] Xidian Univ, State Key Lab Integrated Serv Networks, Xian 710071, Peoples R China.
   [Xiang, Shuiying; Hao, Yue] Xidian Univ, Sch Microelect, State Key Discipline Lab Wide Bandgap Semicond Tec, Xian 710071, Peoples R China.
   [Shi, Yuechun] Yongjiang Lab, Ningbo 315202, Peoples R China.
RP Xiang, SY (corresponding author), Xidian Univ, State Key Lab Integrated Serv Networks, Xian 710071, Peoples R China.; Xiang, SY (corresponding author), Xidian Univ, Sch Microelect, State Key Discipline Lab Wide Bandgap Semicond Tec, Xian 710071, Peoples R China.
EM syxiang@xidian.edu.cn; trzhang98@gmail.com; 1356078782@qq.com;
   xidiangxx@126.com; 18332551054@163.com; yuechun-shi@ylab.ac.cn;
   yhao@xidian.edu.cn
CR [Anonymous], 2019, PROC IEEE INT JOINT
   Barbay S, 2011, OPT LETT, V36, P4476, DOI 10.1364/OL.36.004476
   Choi K, 2018, Arxiv, DOI arXiv:1709.04396
   Deng T, 2017, IEEE J SEL TOP QUANT, V23, DOI 10.1109/JSTQE.2017.2685140
   Dennis J, 2013, PATTERN RECOGN LETT, V34, P1085, DOI 10.1016/j.patrec.2013.02.015
   Dennis J, 2013, INT CONF ACOUST SPEE, P803, DOI 10.1109/ICASSP.2013.6637759
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Dong M, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0204596
   Feldmann J, 2019, NATURE, V569, P208, DOI 10.1038/s41586-019-1157-8
   Gütig R, 2009, PLOS BIOL, V7, DOI 10.1371/journal.pbio.1000141
   Han YN, 2021, PHOTONICS RES, V9, pB119, DOI 10.1364/PRJ.413742
   Huang CR, 2020, APL PHOTONICS, V5, DOI 10.1063/1.5144121
   Hurtado A, 2010, OPT EXPRESS, V18, P25170, DOI 10.1364/OE.18.025170
   Jackson Z., 2016, FREE SPOKEN DIGIT DA
   Jha A, 2022, J LIGHTWAVE TECHNOL, V40, P2901, DOI 10.1109/JLT.2022.3146157
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Lee C, 2019, IEEE T COGN DEV SYST, V11, P384, DOI 10.1109/TCDS.2018.2833071
   Leonard R.G., 1993, TIDIGITS SPEECH CORP
   Lostanlen V., 2016, ARXIV
   Masquelier T, 2007, PLOS COMPUT BIOL, V3, P247, DOI 10.1371/journal.pcbi.0030031
   Muda L, 2010, Arxiv, DOI arXiv:1003.4083
   Nahmias MA, 2013, IEEE J SEL TOP QUANT, V19, DOI 10.1109/JSTQE.2013.2257700
   Peng HT, 2018, IEEE J SEL TOP QUANT, V24, DOI 10.1109/JSTQE.2018.2840448
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Robertson J, 2020, IEEE J SEL TOP QUANT, V26, DOI 10.1109/JSTQE.2019.2931215
   Roy K, 2019, NATURE, V575, P607, DOI 10.1038/s41586-019-1677-2
   Shastri BJ, 2021, NAT PHOTONICS, V15, P102, DOI 10.1038/s41566-020-00754-y
   Shen YC, 2017, NAT PHOTONICS, V11, P441, DOI [10.1038/NPHOTON.2017.93, 10.1038/nphoton.2017.93]
   Song ZW, 2023, IEEE J SEL TOP QUANT, V29, DOI 10.1109/JSTQE.2022.3200942
   Srinivasan G, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00524
   Tait AN, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-07754-z
   Tait AN, 2014, J LIGHTWAVE TECHNOL, V32, P4029, DOI 10.1109/JLT.2014.2345652
   Tavanaei A, 2018, 2018 INT JOINT C NEU, DOI DOI 10.1109/IJCNN.2018.8489104
   Tavanaei A, 2017, LECT NOTES COMPUT SC, V10639, P899, DOI 10.1007/978-3-319-70136-3_95
   Tavanaei A, 2017, NEUROCOMPUTING, V240, P191, DOI 10.1016/j.neucom.2017.01.088
   Texas Instruments, 1991, 46 WORD SPEAK DEP IS
   Tian Y, 2022, NANOPHOTONICS-BERLIN, V11, P329, DOI 10.1515/nanoph-2021-0521
   Xiang JL, 2022, PHOTONICS RES, V10, P939, DOI 10.1364/PRJ.445954
   Xiang SY, 2023, OPTICA, V10, P162, DOI 10.1364/OPTICA.468347
   Xiang SY, 2021, IEEE J SEL TOP QUANT, V27, DOI 10.1109/JSTQE.2020.3005589
   Xiang SY, 2021, IEEE T NEUR NET LEAR, V32, P2494, DOI 10.1109/TNNLS.2020.3006263
   Xiang SY, 2019, IEEE J SEL TOP QUANT, V25, DOI 10.1109/JSTQE.2019.2911565
   Xiao R, 2017, COMM COM INF SC, V710, P584, DOI 10.1007/978-981-10-5230-9_57
   Yu Q, 2022, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2022.3165527
   Zhang ZX, 2021, IEEE SIGNAL PROC LET, V28, P484, DOI 10.1109/LSP.2021.3059172
   Zheng D., 2023, PHOTONICS RES, V11, P65
   Zhou HL, 2020, IEEE J SEL TOP QUANT, V26, DOI 10.1109/JSTQE.2019.2943347
NR 47
TC 0
Z9 0
U1 36
U2 55
PD NOV
PY 2023
VL 29
IS 6
AR 7600507
DI 10.1109/JSTQE.2023.3240248
WC Engineering, Electrical & Electronic; Quantum Science & Technology;
   Optics; Physics, Applied
DA 2023-11-11
ER

PT J
AU Bohte, SM
   La Poutré, H
   Kok, JN
AF Bohte, SM
   La Poutré, H
   Kok, JN
TI Unsupervised clustering with spiking neurons by sparse temporal coding
   and multilayer RBF networks
SO IEEE TRANSACTIONS ON NEURAL NETWORKS
DT Article
DE coarse coding; complex clusters; Hebbian-learning; high-dimensional
   clustering; sparse coding; spiking neurons; synchronous firing; temporal
   coding; unsupervised learning
ID PATTERN-RECOGNITION; RECEIVERS; CODE
AB We demonstrate that spiking neural networks encoding information in the timing of single spikes are capable of computing and learning clusters from realistic data. We show how a spiking neural network based on spike-time coding and Hebbian learning can successfully perform unsupervised clustering on real-world data, and we demonstrate how temporal synchrony in a multilayer network can induce hierarchical clustering. We develop a temporal encoding of continuously valued data to obtain adjustable clustering capacity and precision with an efficient use of neurons: input variables are encoded in a population code by neurons with graded and overlapping sensitivity profiles. We also discuss methods for enhancing scale-sensitivity of the network and show how the induced synchronization of neurons within early RBF layers allows for the subsequent detection of complex clusters.
C1 Netherlands Ctr Comp Sci & Math, Amsterdam, Netherlands.
   Leiden Univ, Leiden Inst Adv Comp Sci, Leiden, Netherlands.
RP Bohte, SM (corresponding author), Netherlands Ctr Comp Sci & Math, Amsterdam, Netherlands.
CR BALDI P, 1988, BIOL CYBERN, V59, P313, DOI 10.1007/BF00332921
   BOHTE SM, IN PRESS NEUROCOMPUT
   BOHTE SM, 2000, P ESANN 2000
   Buonomano DV, 1999, NEURAL COMPUT, V11, P103, DOI 10.1162/089976699300016836
   Campbell SR, 1999, NEURAL COMPUT, V11, P1595, DOI 10.1162/089976699300016160
   CHEN K, 1999, ADV NEURAL INFORMATI, V11
   Delorme A, 1999, NEUROCOMPUTING, V26-7, P989, DOI 10.1016/S0925-2312(99)00095-8
   EURICH CW, 2000, IN PRESS NEURAL COMP
   GERSTNER W, 1995, PHYS REV E, V51, P738, DOI 10.1103/PhysRevE.51.738
   Gerstner W, 1996, NATURE, V383, P76, DOI 10.1038/383076a0
   Guedalia ID, 1999, NEURAL COMPUT, V11, P521, DOI 10.1162/089976699300016755
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   KOENDERINK JJ, 1984, BIOL CYBERN, V50, P363, DOI 10.1007/BF00336961
   König P, 1991, NEURAL COMPUT, V3, P155, DOI 10.1162/neco.1991.3.2.155
   Maass W, 1997, NEURAL COMPUT, V9, P279, DOI 10.1162/neco.1997.9.2.279
   Mattia M, 2000, NEURAL COMPUT, V12, P2305, DOI 10.1162/089976600300014953
   Natschlager T, 1998, NETWORK-COMP NEURAL, V9, P319, DOI 10.1088/0954-898X/9/3/003
   Pouget A, 1999, NEURAL COMPUT, V11, P85, DOI 10.1162/089976699300016818
   SINGER W, 1995, ANNU REV NEUROSCI, V18, P555, DOI 10.1146/annurev.neuro.18.1.555
   SNIPPE HP, 1992, BIOL CYBERN, V67, P183, DOI 10.1007/BF00201025
   VANKEMENADE CHM, 1999, SPATIAL STAT REMOTE
   von der Malsburg C, 1999, NEURON, V24, P95, DOI 10.1016/S0896-6273(00)80825-9
   Zhang KC, 1999, NEURAL COMPUT, V11, P75, DOI 10.1162/089976699300016809
   Zhang KC, 1998, J NEUROPHYSIOL, V79, P1017, DOI 10.1152/jn.1998.79.2.1017
   Zurada J.M, 1992, INTRO ARTIFICIAL NEU, P8
NR 25
TC 160
Z9 171
U1 9
U2 22
PD MAR
PY 2002
VL 13
IS 2
BP 426
EP 435
AR PII S1045-9227(02)02406-2
DI 10.1109/72.991428
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Computer Science, Theory & Methods; Engineering,
   Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Kang, ZY
   Xiao, X
   Li, SM
   Wang, L
   Wang, Y
AF Kang, Ziyang
   Xiao, Xu
   Li, Shiming
   Wang, Lei
   Wang, Yao
GP IEEE
TI Hotspot Prediction of Network-on-Chip for Neuromorphic Processor with
   Liquid State Machine
SO 2022 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS (ISCAS 22)
SE IEEE International Symposium on Circuits and Systems
DT Proceedings Paper
CT IEEE International Symposium on Circuits and Systems (ISCAS)
CY MAY 28-JUN 01, 2022
CL Austin, TX
DE Spiking Neural Networks; Network-on-Chip; Liquid State Machine; Hotspot
   prediction
ID NEURAL-NETWORKS
AB The traffic patterns of Spiking Neural Networks (SNNs) are highly varying and unpredictable, which will cause elevated traffic hotspots on the Network-on-Chip (NoC). How to predict the occurrence of hotspots remains one of the most challenging issues for NoC design. This work presents the first attempt toward utilizing a Liquid State Machine (LSM) in the prediction of traffic hotspot on routers. We also adopt the heuristic algorithm to search the hyperparameter of the LSM to improve the performance. Results indicate that the predictor can forecast hotspot formation with an accuracy up to 89.36% and 90.19% for two spiking-based datasets, respectively.
C1 [Kang, Ziyang; Xiao, Xu; Li, Shiming; Wang, Lei; Wang, Yao] Natl Univ Def Technol, Coll Comp Sci & Technol, Changsha, Peoples R China.
RP Kang, ZY (corresponding author), Natl Univ Def Technol, Coll Comp Sci & Technol, Changsha, Peoples R China.
EM kangziyang14@nudt.edu.cn; xiaoxun@nudt.edu.cn; lishiming14@nudt.edu.cn;
   leiwang@nudt.edu.cn; wangyao@nudt.edu.cn
CR Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   [Anonymous], 1989, NEUR NETW 1989 IJCNN
   [Anonymous], 2011, P 5 ACM IEEE INT S
   [Anonymous], 2017, ARXIV171105255
   Balaji A, 2020, IEEE T VLSI SYST, V28, P76, DOI 10.1109/TVLSI.2019.2951493
   Burgsteiner H, 2007, APPL INTELL, V26, P99, DOI 10.1007/s10489-006-0007-1
   Cameron K, 2010, IEEE INT SYMP CIRC S, P365, DOI 10.1109/ISCAS.2010.5537767
   Chih-Hao Chao, 2010, 2010 ACM/IEEE International Symposium on Networks-on-Chip (NOCS), P223, DOI 10.1109/NOCS.2010.32
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   de Kamps M, 2001, NEURAL NETWORKS, V14, P941, DOI 10.1016/S0893-6080(01)00068-5
   Gupte A, 2009, 2009 INTERNATIONAL CONFERENCE ON RECONFIGURABLE COMPUTING AND FPGAS, P89, DOI 10.1109/ReConFig.2009.80
   Heeger D., 2000, HANDOUT U STANDFORD, V5, P76
   Jackson Z., 2016, TECHNICAL REPORT
   Javed A, 2020, IEEE INT SYMP CIRC S
   Kennedy J, 1995, 1995 IEEE INTERNATIONAL CONFERENCE ON NEURAL NETWORKS PROCEEDINGS, VOLS 1-6, P1942, DOI 10.1109/icnn.1995.488968
   Li Shiming, 2020, ARXIV200401639
   Ly C, 2015, J COMPUT NEUROSCI, V39, P311, DOI 10.1007/s10827-015-0578-0
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   MEAD C, 1990, P IEEE, V78, P1629, DOI 10.1109/5.58356
   Merkel A., 2006, Operating Systems Review, V40, P403, DOI 10.1145/1218063.1217974
   Orchard G, 2015, FRONT NEUROSCI-SWITZ, V9, DOI [10.3389/fnins.2015.00437, 10.3389/fhins.2015.00437]
   Peng Yu, 2009, Proceedings of the 2009 Fifth International Conference on Natural Computation (ICNC 2009), P464, DOI 10.1109/ICNC.2009.685
   Querlioz D, 2013, IEEE T NANOTECHNOL, V12, P288, DOI 10.1109/TNANO.2013.2250995
   Soteriou V, 2016, IEEE T COMPUT, V65, P819, DOI 10.1109/TC.2015.2435748
   Stimberg M, 2019, ELIFE, V8, DOI 10.7554/eLife.47314
   Wang J, 2021, EURASIP J IMAGE VIDE, V2021, DOI 10.1186/s13640-020-00542-2
   Zhu D, 2015, DES AUT TEST EUROPE, P1241
NR 27
TC 1
Z9 1
U1 0
U2 0
PY 2022
BP 2353
EP 2357
DI 10.1109/ISCAS48785.2022.9937553
WC Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Buice, MA
   Chow, CC
AF Buice, Michael A.
   Chow, Carson C.
TI Generalized activity equations for spiking neural network dynamics
SO FRONTIERS IN COMPUTATIONAL NEUROSCIENCE
DT Article
DE mean field theory; theta model; fokker-planck; correlations; finite size
   networks; wilson-cowan model; population rate; fluctuations
ID MEAN-FIELD ANALYSIS; LARGE-SCALE MODEL; ASYNCHRONOUS STATES; POPULATION;
   SYSTEMS; CHAOS
AB Much progress has been made in uncovering the computational capabilities of spiking neural networks. However, spiking neurons will always be more expensive to simulate compared to rate neurons beacause of the inherent disparity in time scales-the spike duration time is much shorter than the inter-spike time, which is much shorter than any learning time scale. In numerical analysis, this is a classic stiff problem. Spiking neurons are also much more difficult to study analytically. One possible approach to making spiking networks more tractable is to augment mean field activity models with some information about spiking correlations. For example, such a generalized activity model could carry information about spiking rates and correlations between spikes self-consistently. Here, we will show how this can be accomplished by constructing a complete formal probabilistic description of the network and then expanding around a small parameter such as the inverse of the number of neurons in the network. The mean field theory of the system gives a rate-like description. The first order terms in the perturbation expansion keep track of covariances.
C1 [Buice, Michael A.] Allen Inst Brain Sci, Modeling Anal & Theory Team, Seattle, WA USA.
   [Chow, Carson C.] NIDDK, Lab Biol Modeling, NIH, Bethesda, MD 20814 USA.
RP Chow, CC (corresponding author), NIDDK, Lab Biol Modeling, NIH, Bldg 12A Room 4007,12 South Dr, Bethesda, MD 20814 USA.
EM carsonc@mail.nih.gov
CR ABBOTT LF, 1993, PHYS REV E, V48, P1483, DOI 10.1103/PhysRevE.48.1483
   Ailamaki A., 2012, HUMAN BRAIN PROJECT
   Baladron J, 2012, J MATH NEUROSCI, V2, DOI 10.1186/2190-8567-2-10
   Bressloff PC, 2009, SIAM J APPL MATH, V70, P1488, DOI 10.1137/090756971
   Buice M. A., 2010, ARXIV10095966
   Buice MA, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1002872
   Buice MA, 2011, PHYS REV E, V84, DOI 10.1103/PhysRevE.84.051120
   Buice MA, 2010, NEURAL COMPUT, V22, P377, DOI 10.1162/neco.2009.02-09-960
   Buice MA, 2009, PROG BIOPHYS MOL BIO, V99, P53, DOI 10.1016/j.pbiomolbio.2009.07.003
   Buice MA, 2007, PHYS REV E, V76, DOI 10.1103/PhysRevE.76.031118
   Buicel MA, 2013, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2013/03/P03003
   DESAI RC, 1978, J STAT PHYS, V19, P1, DOI 10.1007/BF01020331
   Elgart V, 2004, PHYS REV E, V70, DOI 10.1103/PhysRevE.70.041106
   Eliasmith C, 2012, SCIENCE, V338, P1202, DOI 10.1126/science.1225266
   Ermentrout B, 1996, NEURAL COMPUT, V8, P979, DOI 10.1162/neco.1996.8.5.979
   ERMENTROUT GB, 1991, J MATH BIOL, V29, P195, DOI 10.1007/BF00160535
   ERMENTROUT GB, 1986, SIAM J APPL MATH, V46, P233, DOI 10.1137/0146017
   Faugeras O, 2009, FRONT COMPUT NEUROSC, V3, DOI 10.3389/neuro.10.001.2009
   GERSTNER W, 1995, PHYS REV E, V51, P738, DOI 10.1103/PhysRevE.51.738
   Gerstner W, 2000, NEURAL COMPUT, V12, P43, DOI 10.1162/089976600300015899
   Golomb D, 2000, NEURAL COMPUT, V12, P1095, DOI 10.1162/089976600300015529
   Hildebrand EJ, 2007, PHYS REV LETT, V98, DOI 10.1103/PhysRevLett.98.054101
   Hoppensteadt FC., 1997, WEAKLY CONNECTED NEU, P25, DOI [10.1007/978-1-4612-1828-9_2, DOI 10.1007/978-1-4612-1828-9_2, 10.1007/978-1-4612-1828-9, DOI 10.1007/978-1-4612-1828-9]
   Ichimaru S., 1973, BASIC PRINCIPLES PLA
   Izhikevich EM, 2008, P NATL ACAD SCI USA, V105, P3593, DOI 10.1073/pnas.0712231105
   Janssen HK, 2005, ANN PHYS-NEW YORK, V315, P147, DOI 10.1016/j.aop.2004.09.011
   Kuramoto Y., 1984, CHEM OSCILLATIONS WA, DOI [10.1007/978-3-642-69689-3, DOI 10.1038/srep02228, DOI 10.1007/978-3-642-69689-3]
   Liboff R. L., 2003, KINETIC THEORY
   MCKEAN HP, 1966, P NATL ACAD SCI USA, V56, P1907, DOI 10.1073/pnas.56.6.1907
   Nicholson D. R., 1993, INTRO PLASMA THEORY
   Salinas E, 2000, J NEUROSCI, V20, P6193, DOI 10.1523/JNEUROSCI.20-16-06193.2000
   SOMPOLINSKY H, 1988, PHYS REV LETT, V61, P259, DOI 10.1103/PhysRevLett.61.259
   STROGATZ SH, 1991, J STAT PHYS, V63, P613, DOI 10.1007/BF01029202
   Touboul J, 2012, PHYSICA D, V241, P1223, DOI 10.1016/j.physd.2012.03.010
   TREVES A, 1993, NETWORK-COMP NEURAL, V4, P259, DOI 10.1088/0954-898X/4/3/002
NR 35
TC 8
Z9 8
U1 0
U2 9
PD NOV 15
PY 2013
VL 7
AR 162
DI 10.3389/fncom.2013.00162
WC Mathematical & Computational Biology; Neurosciences
DA 2023-11-11
ER

PT C
AU Mayr, C
   Schüffny, R
AF Mayr, C
   Schüffny, R
GP ieee
TI Noise shaping in spiking neural nets -: Network design issues
SO 2004 47TH MIDWEST SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL II, CONFERENCE
   PROCEEDINGS
DT Proceedings Paper
CT 47th Midwest Symposium on Circuits and Systems (MWSCAS 2004)
CY JUL 25-28, 2004
CL Hiroshima Univ, Hiroshima, JAPAN
HO Hiroshima Univ
AB In recent years, there has been an increased focus on the mechanics of information transmission in spiking neural networks. Especially the Noise Shaping properties of these networks and their similarity to Delta-Sigma Modulators has received a lot of attention. However, very little of the research done in this area has focused on the effect the weights in these networks have on the Noise Shaping properties. This paper concerns itself with the various modes of network operation and beneficial as well as detrimental effects which the systematic generation of network weights can effect. Relevancy of this research to industrial application of neural nets as building blocks of oversampled A/D converters is shown. Also, further points of contention are listed, which must be thoroughly researched to add to the above mentioned applicability of spiking neural nets.
C1 Tech Univ Dresden, Infineon Technol AG, Lehrstuhl Hochparallele VLSI Syst & Neuromikroele, D-8027 Dresden, Germany.
RP Mayr, C (corresponding author), Tech Univ Dresden, Infineon Technol AG, Lehrstuhl Hochparallele VLSI Syst & Neuromikroele, D-8027 Dresden, Germany.
CR BLACK WC, 1980, IEEE J SOLID-ST CIRC, V15, P1022, DOI 10.1109/JSSC.1980.1051512
   CHIPPERFIELD, 1996, GENETIC ALGORITHM TO
   Coello CAC, 2001, LECT NOTES COMPUT SC, V1993, P21
   Gabbiani F, 1999, J EXP BIOL, V202, P1267
   Galton I, 1995, IEEE T CIRCUITS-II, V42, P773, DOI 10.1109/82.476175
   GERSTNER W, 1999, C PUBLICATION, V470, P7
   MARIENBORG JT, 1996, ISCAS 02
   Norsworthy S., 1996, DELTA SIGMA DATA CON
   Stanley KO, 2002, P 2002 C EV COMP CEC
NR 9
TC 0
Z9 0
U1 0
U2 2
PY 2004
BP 401
EP 404
WC Engineering, Electrical & Electronic; Telecommunications
DA 2023-11-11
ER

PT C
AU El-Sayed, SA
   Camuñas-Mesa, LA
   Linares-Barranco, B
   Stratigopoulos, HG
AF El-Sayed, Sarah A.
   Camunas-Mesa, Luis A.
   Linares-Barranco, Bernabe
   Stratigopoulos, Haralampos-G
GP IEEE
TI Self-Testing Analog Spiking Neuron Circuit
SO 2019 16TH INTERNATIONAL CONFERENCE ON SYNTHESIS, MODELING, ANALYSIS AND
   SIMULATION METHODS AND APPLICATIONS TO CIRCUIT DESIGN (SMACD 2019)
SE International Conference on Synthesis Modeling Analysis and Simulation
   Methods and Applications to Circuit Design
DT Proceedings Paper
CT 16th International Conference on Synthesis, Modeling, Analysis and
   Simulation Methods and Applications to Circuit Design (SMACD) / 15th
   Conference on PhD Research in Microelectronics and Electronics (PRIME)
CY JUL 15-18, 2019
CL Lausanne, SWITZERLAND
ID NETWORKS
AB Hardware-implemented neural networks are foreseen to play an increasing role in numerous applications. In this paper, we address the problem of post-manufacturing test and self-test of hardware-implemented neural networks. In particular, we propose a self-testable version of a spiking neuron circuit. The self-test wrapper is a compact circuit composed of a low-precision ramp generator and a small digital block. The self-test principle is demonstrated on a spiking neuron circuit design in 0.35 mu m CMOS technology.
C1 [El-Sayed, Sarah A.; Stratigopoulos, Haralampos-G] Sorbonne Univ, CNRS, LIP6, Paris, France.
   [Camunas-Mesa, Luis A.; Linares-Barranco, Bernabe] CSIC, Inst Microelect Sevilla IMSE CNM, Seville, Spain.
   [Camunas-Mesa, Luis A.; Linares-Barranco, Bernabe] Univ Seville, Seville, Spain.
RP El-Sayed, SA (corresponding author), Sorbonne Univ, CNRS, LIP6, Paris, France.
CR Anghel L, 2018, IEEE INT CONF VLSI, P176, DOI 10.1109/VLSI-SoC.2018.8644897
   [Anonymous], P IEEE VLSI TEST S
   [Anonymous], 2017, ARXIV170506963V1
   [Anonymous], IEEE T CIRCUITS SY 1
   [Anonymous], 2011, INTRO MIXED SIGNAL I
   [Anonymous], 2016, DEEP LEARNING
   [Anonymous], P IEEE EUR TEST S
   [Anonymous], 2016, P IEEE INT TEST C
   Bushnell M. L., 2000, ESSENTIALS ELECT TES
   CONNORS BW, 1990, TRENDS NEUROSCI, V13, P99, DOI 10.1016/0166-2236(90)90185-D
   Indiveri G, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00073
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maliuk D, 2010, INT TEST CONF P
   MEAD C, 1990, P IEEE, V78, P1629, DOI 10.1109/5.58356
   Mitra S, 2000, INT TEST CONF P, P985, DOI 10.1109/TEST.2000.894311
   Pandey S, 2018, IEEE INT ON LINE, P135, DOI 10.1109/IOLTS.2018.8474075
   Poon CS, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00108
   Serrano-Gotarredona R, 2009, IEEE T NEURAL NETWOR, V20, P1417, DOI 10.1109/TNN.2009.2023653
   Stratigopoulos HG, 2018, PROC EUR TEST SYMP
   Sunter S, 2016, IEEE T CIRCUITS-I, V63, P2313, DOI 10.1109/TCSI.2016.2616159
   Theis TN, 2017, COMPUT SCI ENG, V19, P41, DOI 10.1109/MCSE.2017.29
   Valle M, 2002, ANALOG INTEGR CIRC S, V33, P263, DOI 10.1023/A:1020717929709
   Volanis G, 2016, IEEE DES TEST, V33, P91, DOI 10.1109/MDAT.2016.2545159
   Wijekoon JHB, 2008, NEURAL NETWORKS, V21, P524, DOI 10.1016/j.neunet.2007.12.037
   Yu SM, 2018, P IEEE, V106, P260, DOI 10.1109/JPROC.2018.2790840
NR 26
TC 8
Z9 8
U1 1
U2 4
PY 2019
BP 81
EP 84
DI 10.1109/smacd.2019.8795234
WC Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Bae, JM
   Cho, UI
AF Bae, Jeong Min
   Cho, Ung In
TI The excitability by both electric and concentrative perturbation in CSTR
SO BULLETIN OF THE KOREAN CHEMICAL SOCIETY
DT Article
DE chemical excitability; chemical spiking neural network
ID COUPLED CHEMICAL OSCILLATORS; LIMIT-CYCLE BEHAVIOR; SPIKING NEURONS;
   SYSTEM; OREGONATOR; MODEL; NETWORKS
AB Excitability is one of the basic and fundamental mechanisms utilized for signal transmission in living organisms. With reference to the condition by Marek and the condition by Schneider, we found a condition in which excitability with similar shapes can appear by chemical and electric perturbation. Our condition is constructed with 3 chemical channels and 1 electric channel, and can be used as a condition for a chemical spiking neuron and as a unit of a chemical spiking neural network.
C1 Yonsei Univ, Dept Chem, Seoul 120749, South Korea.
RP Cho, UI (corresponding author), Yonsei Univ, Dept Chem, Seoul 120749, South Korea.
EM uicho@yonsei.ac.kr
CR BARELI K, 1987, J CHEM PHYS, V86, P1927, DOI 10.1063/1.452142
   Epstein I.R., 1998, INTRO NONLINEAR CHEM, DOI 10.1093/oso/9780195096705.001.0001
   Field R. J., 1985, OSCILLATIONS TRAVELI
   FIELD RJ, 1974, J CHEM PHYS, V60, P1877, DOI 10.1063/1.1681288
   FIELD RJ, 1972, J AM CHEM SOC, V94, P8649, DOI 10.1021/ja00780a001
   FINKEOVA J, 1990, J PHYS CHEM-US, V94, P4110, DOI 10.1021/j100373a042
   Gray P., 1994, CHEM OSCILLATIONS IN
   Hohmann W, 1999, J PHYS CHEM A, V103, P5742, DOI 10.1021/jp991224a
   Hohmann W, 1996, J PHYS CHEM-US, V100, P3221, DOI 10.1021/jp952506n
   Lee IH, 2001, PHYS CHEM CHEM PHYS, V3, P94, DOI 10.1039/b006784i
   Maass W, 1997, NEURAL COMPUT, V9, P279, DOI 10.1162/neco.1997.9.2.279
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Natschläger T, 2002, THEOR COMPUT SCI, V287, P251, DOI 10.1016/S0304-3975(02)00099-3
   RUOFF P, 1986, J CHEM PHYS, V84, P1413, DOI 10.1063/1.450484
   SHOWALTER K, 1978, J CHEM PHYS, V69, P2514, DOI 10.1063/1.436894
   Votrubová V, 1998, J PHYS CHEM A, V102, P1318, DOI 10.1021/jp973041z
NR 16
TC 1
Z9 1
U1 0
U2 2
PD AUG 20
PY 2006
VL 27
IS 8
BP 1145
EP 1148
WC Chemistry, Multidisciplinary
DA 2023-11-11
ER

PT C
AU Kubota, N
   Sasaki, H
AF Kubota, N
   Sasaki, H
GP IEEE
TI Genetic algorithm for a fuzzy spiking neural network of a mobile robot
SO 2005 IEEE International Symposium on Computational Intelligence in
   Robotics and Automation, Proceedings
DT Proceedings Paper
CT IEEE International Symposium on Computational Intelligence in Robotics
   and Automation
CY JUN 09-30, 2005
CL Espoo, FINLAND
DE genetic algorithm; spiking neural network; intelligent robotics;
   behavior learning; fuzzy theory
ID SYSTEM
AB It is very difficult to design the learning structure of a robot beforehand in an unknown and dynamic environment, because the dynamics of the environment is unknown. Therefore, this paper proposes a fuzzy spiking neural network (FSNN) for behavior learning of a mobile robot. Furthermore, the network structure of the FSNN should be adaptive to the environmental condition. In this paper, we apply a steady-state genetic algorithm for acquiring the suitable network structure through the interaction with the environment. The simulation results show the robot can update the network structure and learn the weights of FSNN according to the spatio-temporal context of the facing environment.
C1 Tokyo Metropolitan Univ, Dept Syst Design, Hachioji, Tokyo 1920397, Japan.
RP Kubota, N (corresponding author), Tokyo Metropolitan Univ, Dept Syst Design, 1-1 Minami Ohsawa, Hachioji, Tokyo 1920397, Japan.
CR Anderson J.A., 1988, NEUROCOMPUTING
   BROOKS RA, 1986, IEEE T ROBOTIC AUTOM, V2, P14, DOI 10.1109/JRA.1986.1087032
   FLOREANO D, 2001, P INT S EV ROB, P38
   Fogel D. B., 1995, EVOLUTIONARY COMPUTA, V3rd
   Fukuda T, 1999, P IEEE, V87, P1448, DOI 10.1109/5.784220
   Jang JSR, 1997, NEUROFUZZY SOFT COMP, P640
   Kubota N, 2001, MEASUREMENT, V29, P237, DOI 10.1016/S0263-2241(00)00044-0
   Kubota N., 2003, J MULT-VALUED LOG S, V9, P221
   KUBOTA N, 2003, P 12 YAL WORKSH AD L, P199
   MAASS W, 1999, PULSED NETWORK NETWO
   MILLER WT, 1990, NEURAL NETWORKS CONT
   NOLFI S, 2000, EVOLUTINARY ROBOTICS
   Pfeifer R., 1999, UNDERSTANDING INTELL
   Scheier C, 1998, NEURAL NETWORKS, V11, P1551, DOI 10.1016/S0893-6080(98)00084-7
   Syswerda G, 1991, FDN GENETIC ALGORITH, V1, P94
   TAKITA K, 2002, P INT JOINT C NEUR N
   Tani J, 1996, IEEE T SYST MAN CY B, V26, P421, DOI 10.1109/3477.499793
NR 17
TC 4
Z9 5
U1 1
U2 1
PY 2005
BP 321
EP 326
DI 10.1109/CIRA.2005.1554297
WC Automation & Control Systems; Computer Science, Artificial Intelligence;
   Robotics
DA 2023-11-11
ER

PT C
AU Akusok, A
   Björk, KM
   Leal, LE
   Miche, Y
   Hu, RJ
   Lendasse, A
AF Akusok, Anton
   Bjork, Kaj-Mikael
   Leal, Leonardo Espinosa
   Miche, Yoan
   Hu, Renjie
   Lendasse, Amaury
GP ACM
TI Spiking Networks for Improved Cognitive Abilities of Edge Computing
   Devices
SO 12TH ACM INTERNATIONAL CONFERENCE ON PERVASIVE TECHNOLOGIES RELATED TO
   ASSISTIVE ENVIRONMENTS (PETRA 2019)
DT Proceedings Paper
CT 12th ACM International Conference on PErvasive Technologies Related to
   Assistive Environments (PETRA)
CY JUN 05-07, 2019
CL Rhodes, GREECE
DE Edge computing; spiking neural networks; interactive computation
AB This concept paper highlights a recently opened opportunity for large scale analytical algorithms to be trained directly on edge devices. Such approach is a response to the arising need of processing data generated by natural person (a human being), also known as personal data. Spiking Neural networks are the core method behind it: suitable for a low latency energy-constrained hardware, enabling local training or re-training, while not taking advantage of scalability available in the Cloud.
C1 [Akusok, Anton] Arcada Univ Appl Sci, Helsinki, Finland.
   [Bjork, Kaj-Mikael] Hanken Sch Econ, Helsinki, Finland.
   [Leal, Leonardo Espinosa] Arcada UAS, Risklab, Helsinki, Finland.
   [Miche, Yoan] Nokia Bell Labs, Espoo, Finland.
   [Hu, Renjie] Univ Iowa, Iowa City, IA USA.
   [Lendasse, Amaury] Univ Houston, Houston, TX USA.
RP Akusok, A (corresponding author), Arcada Univ Appl Sci, Helsinki, Finland.
EM anton.akusok@arcada.fi; kaj-mikael.bjork@hanken.fi;
   leonardo.espinosaleal@arcada.fi; yoan.miche@nokia-bell-labs.com;
   renjie-hu@uiowa.edu; alendass@central.uh.edu
CR Akusok Anton., 2018, HIGH PERFORMANCE ELM
   [Anonymous], 2014, INTEL XEON PHI COPRO
   Hillar Gaston, 2011, DOBBS BLOGGERS 0624, V24
   Jeffers Jim., 2013, MODERN ACCELERATOR T, P25
   Jouppi NP, 2017, 44TH ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA 2017), P1, DOI 10.1145/3079856.3080246
   Li Q, 2013, IEEE SECUR PRIV, V11, P78, DOI 10.1109/MSP.2013.15
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Maass W, 2004, J COMPUT SYST SCI, V69, P593, DOI 10.1016/j jcss.2004.04.001
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   MIT Technology Review Insights, 2018, ON DEV PROC AL GO HA
   Palmer, 1980, P 7 ANN S COMP ARCH, P174
   Qiu Chenxi, 2018, IEEE T BIG DATA, V2018
   Schrauwen B, 2007, LECT NOTES COMPUT SC, V4668, P471
NR 13
TC 1
Z9 1
U1 0
U2 0
PY 2019
BP 307
EP 308
DI 10.1145/3316782.3321546
WC Computer Science, Cybernetics; Computer Science, Theory & Methods
DA 2023-11-11
ER

PT C
AU Sboev, A
   Davydov, Y
   Rybka, R
   Vlasov, D
   Serenko, A
AF Sboev, Alexander
   Davydov, Yury
   Rybka, Roman
   Vlasov, Danila
   Serenko, Alexey
BE Klimov, VV
   Kelley, DJ
TI A Comparison of Two Variants of Memristive Plasticity for Solving the
   Classification Problem of Handwritten Digits Recognition
SO BIOLOGICALLY INSPIRED COGNITIVE ARCHITECTURES 2021
SE Studies in Computational Intelligence
DT Proceedings Paper
CT 12th Annual International Conference of the
   Biologically-Inspired-Cognitive-Architectures-Society (BICA) on
   Biologically Inspired Cognitive Architectures
CY SEP 12-19, 2021
CL ELECTR NETWORK
DE Spiking neural networks; Spike-timing-dependent plasticity; Memristors;
   Classification; Machine learning
AB Nowadays, the task of creating and training spiking neural networks (SNN) is extremely relevant due to their high energy efficiency achieved by implementing such networks via neuromorphic hardware.
   Especially interesting is the possibility of building SNNs based on memristors, which have properties that potentially allow them to be used as analog synapses. With that in mind, it seems relevant to study spike networks built upon plasticity rules that correspond to the experimentally observed nonlinear laws of conductivity change in memristors.
   Earlier it was shown that spiking neural networks trained with a biologically inspired local STDP (Spike-Timing-Dependent Plasticity) rule are capable of solving classification problems successfully. In addition, it was also demonstrated that classification problems can also be solved with spiking neural networks operating with a plasticity rule that models the change in conductivity in nanocomposite (NC) memristors.
   This paper presents a continuation of the study of the applicability of memristive plasticity rules on the handwritten digit recognition problem. Two types of memristive plasticity are compared: for nanocomposite and PPX memristors. It is shown that both models can successfully solve the classification problem, and the key differences between them are identified.
C1 [Sboev, Alexander; Davydov, Yury; Rybka, Roman; Vlasov, Danila; Serenko, Alexey] Natl Res Ctr Kurchatov Inst, Moscow, Russia.
   [Sboev, Alexander] Natl Res Nucl Univ MEPhI, Moscow, Russia.
RP Sboev, A (corresponding author), Natl Res Ctr Kurchatov Inst, Moscow, Russia.
CR Camuñas-Mesa LA, 2019, MATERIALS, V12, DOI 10.3390/ma12172745
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Demin VA, 2021, NEURAL NETWORKS, V134, P64, DOI 10.1016/j.neunet.2020.11.005
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Emelyanov AV, 2020, NANOTECHNOLOGY, V31, DOI 10.1088/1361-6528/ab4a6d
   Hazan H, 2018, FRONT NEUROINFORM, V12, DOI 10.3389/fninf.2018.00089
   Indiveri G, 2015, 2015 IEEE INTERNATIONAL ELECTRON DEVICES MEETING (IEDM)
   Ismail M, 2022, J MATER SCI TECHNOL, V96, P94, DOI 10.1016/j.jmst.2021.04.025
   Klikauer T, 2016, TRIPLEC-COMMUN CAPIT, V14, P260
   Lapkin DA, 2018, MICROELECTRON ENG, V185, P43, DOI 10.1016/j.mee.2017.10.017
   Minnekhanov AA, 2019, ORG ELECTRON, V74, P89, DOI 10.1016/j.orgel.2019.06.052
   Paugam-Moisy H, 2012, HDB NATURAL COMPUTIN, V1, P1, DOI DOI 10.1007/978-3-540-92910-9_10
   Prudnikov NV, 2020, J PHYS D APPL PHYS, V53, DOI 10.1088/1361-6463/ab9262
   Qu LH, 2020, NEURAL COMPUT APPL, V32, P13479, DOI 10.1007/s00521-020-04755-4
   Querlioz D., 2011, 2011 IEEE/ACM International Symposium on Nanoscale Architectures (NANOARCH), P150, DOI 10.1109/NANOARCH.2011.5941497
   Rajendran B, 2019, IEEE SIGNAL PROC MAG, V36, P97, DOI 10.1109/MSP.2019.2933719
   Ryu JH, 2021, J ALLOY COMPD, V850, DOI 10.1016/j.jallcom.2020.156675
   Saïghi S, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00051
   Serrano-Gotarredona T, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00002
   Taherkhani A, 2020, NEURAL NETWORKS, V122, P253, DOI 10.1016/j.neunet.2019.09.036
   van Rossum MCW, 2000, J NEUROSCI, V20, P8812
   Wang ZR, 2018, NAT ELECTRON, V1, P137, DOI 10.1038/s41928-018-0023-2
NR 22
TC 1
Z9 1
U1 0
U2 0
PY 2022
VL 1032
BP 438
EP 446
DI 10.1007/978-3-030-96993-6_48
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods
DA 2023-11-11
ER

PT C
AU SoLatzis, RJ
   Afshar, S
   Hamilton, TJ
AF SoLatzis, Richard James
   Afshar, Saeed
   Hamilton, Tara Julia
GP IEEE
TI Rotationally Invariant Vision Recognition with Neuromorphic
   Transformation and Learning Networks
SO 2014 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS (ISCAS)
SE IEEE International Symposium on Circuits and Systems
DT Proceedings Paper
CT IEEE International Symposium on Circuits and Systems (ISCAS)
CY JUN 01-05, 2014
CL Melbourne, AUSTRALIA
DE neuromorphic engineering; rotational invariance; object recognition;
   spiking neural network; synaptic plasticity; delay plasticity; temporal
   coding; spatio-temporal spike pattern recognition
AB In this paper we present a biologically inspired rotationally-invariant end-to-end recognition system demonstrated in hardware with a bitmap camera and a Field Programmable Gate Array (FPGA). The system integrates the Ripple Pond Network (RPN), a neural network that performs image transformation from two dimensions to one dimensional rotationally invariant temporal patterns (TPs), and the Synaptic Kernel Adaptation Network (SKAN), a neural network capable of unsupervised learning of a spatio-temporal pattern of input spikes. Our results demonstrate rapid learning and recognition of simple hand gestures with no prior training and minimal usage of FPGA hardware.
C1 [SoLatzis, Richard James] Univ New South Wales, Sch Elect Engn & Telecommun, Sydney, NSW 2052, Australia.
   [Afshar, Saeed; Hamilton, Tara Julia] Univ Western Sydney, MARCS Inst, Kingswood, NSW, Australia.
RP SoLatzis, RJ (corresponding author), Univ New South Wales, Sch Elect Engn & Telecommun, Sydney, NSW 2052, Australia.
EM rjs@unswalumni.com; s.v.afshar@uws.edu.au; t.hamilton@uws.edu.au
CR Afshar S, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00212
   [Anonymous], CIRC SYST I IN PRESS
   Bonato V, 2008, IEEE T CIRC SYST VID, V18, P1703, DOI 10.1109/TCSVT.2008.2004936
   Cornelis N., 2008, COMPUTER VISION PATT, P23
   Pardo F, 1998, IEEE J SOLID-ST CIRC, V33, P842, DOI 10.1109/4.678644
NR 5
TC 4
Z9 4
U1 0
U2 0
PY 2014
BP 469
EP 472
WC Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Zheng, HH
   Yi, Y
AF Zheng, Honghao
   Yi, Yang
GP IEEE
TI Spiking Domain Feature Extraction with Temporal Dynamic Learning
SO 2023 24TH INTERNATIONAL SYMPOSIUM ON QUALITY ELECTRONIC DESIGN, ISQED
SE International Symposium on Quality Electronic Design
DT Proceedings Paper
CT 24th International Symposium on Quality Electronic Design (ISQED)
CY APR 05-07, 2023
CL ELECTR NETWORK
DE SNN; feature extraction; multiplexing; STDP
AB Spiking neural network (SNN) has attracted more and more research attention due to its event-based property. SNNs are more power efficient with such property than a conventional artificial neural network. For transferring the information to spikes, SNNs need an encoding process. With the temporal encoding schemes, SNN can extract the temporal patterns from the original information. A more advanced encoding scheme is a multiplexing temporal encoding which combines several encoding schemes with different timescales to have a larger information density and dynamic range. After that, the spike timing dependence plasticity (STDP) learning algorithm is utilized for training the SNN since the SNN can not be trained with regular training algorithms like backpropagation. In this work, a spiking domain feature extraction neural network with temporal multiplexing encoding is designed on EAGLE and fabricated on the PCB board. The testbench's power consumption is 400mW. From the test result, a conclusion can be drawn that the network on PCB can transfer the input information to multiplexing temporal encoded spikes and then utilize the spikes to adjust the synaptic weight voltage.
C1 [Zheng, Honghao; Yi, Yang] Virginia Tech, Dept Elect & Comp Engn, Blacksburg, VA USA.
RP Zheng, HH (corresponding author), Virginia Tech, Dept Elect & Comp Engn, Blacksburg, VA USA.
EM zhenghh@vt.edu; yangyi8@vt.edu
CR [Anonymous], 2010, DYNAMICAL SYSTEMS NE
   Anwani N, 2015, IEEE IJCNN
   Bai KJ, 2020, IEEE T VLSI SYST, V28, P62, DOI 10.1109/TVLSI.2019.2942267
   Gerstner W., 2002, SPIKING NEURON MODEL, DOI DOI 10.1017/CBO9780511815706
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Iakymchuk T, 2015, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-015-0059-4
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Munakata Y, 2004, DEVELOPMENTAL SCI, V7, P141, DOI 10.1111/j.1467-7687.2004.00331.x
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Nelson M., 1995, THE BOOK OF GENESIS, P27
   Nowshin F., 2022, 2022 23 INT S QUALIT, P1
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Teeter C, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-017-02717-4
   Zhao CY, 2016, IEEE T MULTI-SCALE C, V2, P265, DOI 10.1109/TMSCS.2016.2607164
   Zhao CY, 2017, IEEE T VLSI SYST, V25, P2193, DOI 10.1109/TVLSI.2017.2683260
   Zheng H., 2023, IEEE T VLSI SYST
NR 16
TC 0
Z9 0
U1 0
U2 0
PY 2023
BP 706
EP 710
DI 10.1109/ISQED57927.2023.10129326
WC Computer Science, Hardware & Architecture; Engineering, Electrical &
   Electronic
DA 2023-11-11
ER

PT J
AU Carnell, A
   Richardson, D
AF Carnell, Andrew
   Richardson, Daniel
TI Parallel computation in spiking neural nets
SO THEORETICAL COMPUTER SCIENCE
DT Article
DE spiking neural nets; oscillators; Turing completeness; parallel
   processing; arithmetic
ID LARGE NETWORKS
AB Numerical quantities can be represented as phase differences between equiperiodic oscillating subsystems in a spiking neural net. It is then possible to represent integer variables, and the increment and decrement operations, X := X + 1, X := X - 1. It is possible to represent the if construction, the while construction, and some other programming language constructions, including variants of the Seq, Par, and Alt constructors, which were used in Occam. We give a general purpose parallel programming language with integer variables which can be systematically implemented in spiking neural networks. Addition, subtraction and multiplication are done, albeit inefficiently, as examples. (c) 2007 Elsevier B.V. All rights reserved.
C1 Univ Bath, Bath BA2 7AY, Avon, England.
RP Richardson, D (corresponding author), Univ Bath, Bath BA2 7AY, Avon, England.
EM masdr@bath.ac.uk
CR [Anonymous], 1997, COMPLEXITY REAL COMP
   Bindal A, 2007, NANOTECHNOLOGY, V18, DOI 10.1088/0957-4484/18/9/095201
   Çesmeli E, 2000, IEEE T NEURAL NETWOR, V11, P935, DOI 10.1109/72.857773
   Delorme A, 1999, NEUROCOMPUTING, V26-7, P989, DOI 10.1016/S0925-2312(99)00095-8
   Gerstner W., 2002, SPIKING NEURON MODEL
   Herz AVM, 2006, SCIENCE, V314, P80, DOI 10.1126/science.1127240
   HOARE CAR, 1985, COMMNICATING SEQUENT
   *INM LTD, 1998, OCCAM2 REF MAN
   Maass W, 1996, NEURAL COMPUT, V8, P1, DOI 10.1162/neco.1996.8.1.1
   Mattia M, 2000, NEURAL COMPUT, V12, P2305, DOI 10.1162/089976600300014953
   Neto J. P., 2003, J OFTHE BRAZILIAN CO, V8, P58, DOI DOI 10.1590/S0104-65002003000100005
   Siegelmann H.T., 1999, NEURAL NETWORKS ANAL
NR 12
TC 5
Z9 5
U1 1
U2 2
PD OCT 28
PY 2007
VL 386
IS 1-2
BP 57
EP 72
DI 10.1016/j.tcs.2007.06.017
WC Computer Science, Theory & Methods
DA 2023-11-11
ER

PT C
AU Mukhopadhyay, AK
   Chakrabarti, I
   Sharad, M
AF Mukhopadhyay, Anand Kumar
   Chakrabarti, Indrajit
   Sharad, Mrigank
GP IEEE
TI Classification of Hand Movements by Surface Myoelectric Signal using
   Artificial-Spiking Neural Network Model
SO 2018 IEEE SENSORS
SE IEEE Sensors
DT Proceedings Paper
CT 17th IEEE SENSORS Conference
CY OCT 28-31, 2018
CL New Delhi, INDIA
DE surface electromyography signal; classification; artificial neural
   network; spiking neural network; prosthesis application
AB Real-time classification of the myoelectric signal has applications in the field of neuro-rehabilitation systems such as prosthesis. The classifier which is a human-computer-interaction (HCI) controller should be ideally fast and computationally less intensive. In this work, we have done a simulation-based study to estimate the performance of a deep artificial/spiking neural network (ANN) model for classification. The model parameters are tuned for a subject to get a 93.33 % and 89.39 % classification accuracy using the ANN and SNN classifiers respectively. A comparison between the two classifiers is studied in terms of computational complexity, external noise effect and trained parameters approximation.
C1 [Mukhopadhyay, Anand Kumar; Chakrabarti, Indrajit; Sharad, Mrigank] Indian Inst Technol, Dept Elect & Elect Commun Engn, Kharagpur 721302, W Bengal, India.
RP Mukhopadhyay, AK (corresponding author), Indian Inst Technol, Dept Elect & Elect Commun Engn, Kharagpur 721302, W Bengal, India.
EM anandmukh@iitkgp.ac.in; indrajit@ece.iitkgp.ernet.in;
   mrigank@ece.iitkgp.ernet.in
CR Chowdhury RH, 2013, SENSORS-BASEL, V13, P12431, DOI 10.3390/s130912431
   Diehl Peter U, 2015, 2015 INT JOINT C NEU, P1, DOI [DOI 10.1109/IJCNN.2015.7280696, 10.1109/IJCNN.2015.7280696]
   Englehart K, 2003, IEEE T BIO-MED ENG, V50, P848, DOI 10.1109/TBME.2003.813539
   Farrell TR, 2007, IEEE T NEUR SYS REH, V15, P111, DOI 10.1109/TNSRE.2007.891391
   Khushaba RN, 2016, PROCEEDINGS OF 2016 IEEE SYMPOSIUM SERIES ON COMPUTATIONAL INTELLIGENCE (SSCI), DOI 10.1109/SSCI.2016.7850064
   Khushaba RN, 2014, NEURAL NETWORKS, V55, P42, DOI 10.1016/j.neunet.2014.03.010
   Kuiken TA, 2009, JAMA-J AM MED ASSOC, V301, P619, DOI 10.1001/jama.2009.116
   Merletti R., 2016, SURFACE ELECTROMYOGR
   Oskoei MA, 2007, BIOMED SIGNAL PROCES, V2, P275, DOI 10.1016/j.bspc.2007.07.009
   Ravì D, 2017, IEEE J BIOMED HEALTH, V21, P4, DOI 10.1109/JBHI.2016.2636665
   Reaz MBI, 2006, BIOL PROCED ONLINE, V8, P11, DOI [10.1251/bpo115, 10.1251/bpo124]
NR 11
TC 1
Z9 1
U1 0
U2 2
PY 2018
BP 419
EP 422
WC Engineering, Electrical & Electronic; Remote Sensing
DA 2023-11-11
ER

PT C
AU OZDAMAR, O
   YAYLALI, I
   JAYAKAR, P
   LOPEZ, CN
AF OZDAMAR, O
   YAYLALI, I
   JAYAKAR, P
   LOPEZ, CN
BE BANKMAN, IN
   TSITLIK, JE
TI MULTILEVEL NEURAL NETWORK SYSTEM FOR EEG SPIKE DETECTION
SO COMPUTER-BASED MEDICA SYSTEMS
DT Proceedings Paper
CT 4TH ANNUAL IEEE SYMP ON COMPUTER-BASED MEDICAL SYSTEMS
CY MAY 12-14, 1991
CL BALTIMORE, MD
NR 0
TC 13
Z9 14
U1 0
U2 0
PY 1991
BP 272
EP 279
WC Engineering, Electrical & Electronic; Physics, Applied
DA 2023-11-11
ER

PT J
AU Nobukawa, S
   Nishimura, H
   Yamanishi, T
AF Nobukawa, Sou
   Nishimura, Haruhiko
   Yamanishi, Teruya
TI Temporal-specific complexity of spiking patterns in spontaneous activity
   induced by a dual complex network structure
SO SCIENTIFIC REPORTS
DT Article
ID ALZHEIMERS-DISEASE; MODEL; EEG; DYNAMICS; SIGNAL; BRAIN; CONNECTIVITY;
   COMPUTATION; NEURONS; MEG
AB Temporal fluctuation of neural activity in the brain has an important function in optimal information processing. Spontaneous activity is a source of such fluctuation. The distribution of excitatory postsynaptic potentials (EPSPs) between cortical pyramidal neurons can follow a log-normal distribution. Recent studies have shown that networks connected by weak synapses exhibit characteristics of a random network, whereas networks connected by strong synapses have small-world characteristics of small path lengths and large cluster coefficients. To investigate the relationship between temporal complexity spontaneous activity and structural network duality in synaptic connections, we executed a simulation study using the leaky integrate-and-fire spiking neural network with log-normal synaptic weight distribution for the EPSPs and duality of synaptic connectivity, depending on synaptic weight. We conducted multiscale entropy analysis of the temporal spiking activity. Our simulation demonstrated that, when strong synaptic connections approach a smallworld network, specific spiking patterns arise during irregular spatio-temporal spiking activity, and the complexity at the large temporal scale (i.e., slow frequency) is enhanced. Moreover, we confirmed through a surrogate data analysis that slow temporal dynamics reflect a deterministic process in the spiking neural networks. This modelling approach may improve the understanding of the spatio-temporal complex neural activity in the brain.
C1 [Nobukawa, Sou] Chiba Inst Technol, Dept Comp Sci, 2-17-1 Tsudanuma, Narashino, Chiba 2750016, Japan.
   [Nishimura, Haruhiko] Univ Hyogo, Grad Sch Appl Informat, Chuo Ku, Kobe, Hyogo 6508588, Japan.
   [Yamanishi, Teruya] Fukui Univ Technol, AI & IoT Ctr, Dept Management & Informat Sci, 3-6-1 Gakuen, Fukui 9108505, Japan.
RP Nobukawa, S (corresponding author), Chiba Inst Technol, Dept Comp Sci, 2-17-1 Tsudanuma, Narashino, Chiba 2750016, Japan.
EM nobukawa@cs.it-chiba.ac.jp
CR Adeli H, 2005, CLIN EEG NEUROSCI, V36, P131, DOI 10.1177/155005940503600303
   Amit DJ, 1997, CEREB CORTEX, V7, P237, DOI 10.1093/cercor/7.3.237
   Bhat S, 2015, EUR NEUROL, V74, P202, DOI 10.1159/000441447
   Brunel N, 2000, J COMPUT NEUROSCI, V8, P183, DOI 10.1023/A:1008925309027
   Cabral J, 2017, NEUROIMAGE, V160, P84, DOI 10.1016/j.neuroimage.2017.03.045
   Cao M, 2017, TRENDS NEUROSCI, V40, P494, DOI 10.1016/j.tins.2017.06.003
   Costa M, 2002, PHYS REV LETT, V89, DOI 10.1103/PhysRevLett.89.068102
   Delbeuck X, 2003, NEUROPSYCHOL REV, V13, P79, DOI 10.1023/A:1023832305702
   Denève S, 2017, NEURON, V94, P969, DOI 10.1016/j.neuron.2017.05.016
   Destexhe A, 2009, J COMPUT NEUROSCI, V27, P493, DOI 10.1007/s10827-009-0164-4
   Fell J, 2000, ACTA NEUROBIOL EXP, V60, P87
   Fiser J, 2004, NATURE, V431, P573, DOI 10.1038/nature02907
   Girvan M, 2002, P NATL ACAD SCI USA, V99, P7821, DOI 10.1073/pnas.122653799
   Glasser MF, 2016, NAT NEUROSCI, V19, P1175, DOI 10.1038/nn.4361
   Glasser MF, 2016, NATURE, V536, P171, DOI 10.1038/nature18933
   Goodman Dan FM, 2014, BMC NEUROSCI, V15, P1, DOI DOI 10.1186/1471-2202-15-S1-P199
   Guo DQ, 2010, IEEE T NEURAL NETWOR, V21, P895, DOI 10.1109/TNN.2010.2044419
   Hasegawa C, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00566
   Hiratani N, 2013, FRONT COMPUT NEUROSC, V6, DOI 10.3389/fncom.2012.00102
   Hromadka T, 2008, PLOS BIOL, V6, P124, DOI 10.1371/journal.pbio.0060016
   Jonke Z, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00118
   Koren V, 2017, PLOS COMPUT BIOL, V13, DOI 10.1371/journal.pcbi.1005355
   Lefort S, 2009, NEURON, V61, P301, DOI 10.1016/j.neuron.2008.12.020
   Lippé S, 2009, FRONT HUM NEUROSCI, V3, DOI 10.3389/neuro.09.048.2009
   Mammone N, 2017, INT J NEURAL SYST, V27, DOI 10.1142/S0129065717500204
   Martí D, 2018, PHYS REV E, V97, DOI 10.1103/PhysRevE.97.062314
   Mastrogiuseppe F, 2017, PLOS COMPUT BIOL, V13, DOI 10.1371/journal.pcbi.1005498
   McCormick DA, 1999, SCIENCE, V285, P541, DOI 10.1126/science.285.5427.541
   Mizuno T, 2010, CLIN NEUROPHYSIOL, V121, P1438, DOI 10.1016/j.clinph.2010.03.025
   Nakagawa TT, 2014, NEUROIMAGE, V87, P383, DOI 10.1016/j.neuroimage.2013.11.009
   Nobukawa S, 2018, PROCEEDINGS OF 2018 IEEE 17TH INTERNATIONAL CONFERENCE ON COGNITIVE INFORMATICS & COGNITIVE COMPUTING (ICCI*CC 2018), P159, DOI 10.1109/ICCI-CC.2018.8482070
   Omura Y, 2015, J NEUROSCI, V35, P14585, DOI 10.1523/JNEUROSCI.4944-14.2015
   Ostojic S, 2014, NAT NEUROSCI, V17, P594, DOI 10.1038/nn.3658
   Ostojic S, 2009, J COMPUT NEUROSCI, V26, P369, DOI 10.1007/s10827-008-0117-3
   PARK J, 2019, ENTROPY-SWITZ, V21, DOI DOI 10.3390/E21020214
   Riecke H, 2007, CHAOS, V17, DOI 10.1063/1.2743611
   Sakata S, 2009, NEURON, V64, P404, DOI 10.1016/j.neuron.2009.09.020
   Schreiber T, 1996, PHYS REV LETT, V77, P635, DOI 10.1103/PhysRevLett.77.635
   Shadlen MN, 1998, J NEUROSCI, V18, P3870
   Shanahan M, 2008, PHYS REV E, V78, DOI 10.1103/PhysRevE.78.041924
   SOFTKY WR, 1993, J NEUROSCI, V13, P334
   Song S, 2005, PLOS BIOL, V3, P507, DOI 10.1371/journal.pbio.0030068
   Stam CJ, 2005, CLIN NEUROPHYSIOL, V116, P2266, DOI 10.1016/j.clinph.2005.06.011
   Takahashi T, 2013, PROG NEURO-PSYCHOPH, V45, P258, DOI 10.1016/j.pnpbp.2012.05.001
   Teramae J, 2012, SCI REP-UK, V2, DOI 10.1038/srep00485
   van Vreeswijk C, 1998, NEURAL COMPUT, V10, P1321, DOI 10.1162/089976698300017214
   Vogels TP, 2005, J NEUROSCI, V25, P10786, DOI 10.1523/JNEUROSCI.3508-05.2005
   Watanabe Y, 2016, ASIA JT CONF INF SEC, P115, DOI 10.1109/AsiaJCIS.2016.26
   Watts DJ, 1998, NATURE, V393, P440, DOI 10.1038/30918
   Wieland S, 2015, PHYS REV E, V92, DOI 10.1103/PhysRevE.92.040901
   Xie Teng, 2011, Front Psychiatry, V2, P77, DOI 10.3389/fpsyt.2011.00077
   Yang AC, 2013, PROG NEURO-PSYCHOPH, V45, P253, DOI 10.1016/j.pnpbp.2012.09.015
NR 52
TC 14
Z9 14
U1 0
U2 3
PD SEP 4
PY 2019
VL 9
AR 12749
DI 10.1038/s41598-019-49286-8
WC Multidisciplinary Sciences
DA 2023-11-11
ER

PT J
AU Awadalla, MHA
   Sadek, MA
AF Awadalla, Medhat H. A.
   Sadek, M. Abdellatif
TI Spiking neural network-based control chart pattern recognition
SO ALEXANDRIA ENGINEERING JOURNAL
DT Article
DE Spiking neural network; Control chart pattern; recognition; Spikeprop
   algorithm
AB Due to an increasing competition in products, consumers have become more critical in choosing products. The quality of products has become more important. Statistical Process Control (SPC) is usually used to improve the quality of products. Control charting plays the most important role in SPC. Control charts help to monitor the behavior of the process to determine whether it is stable or not. Unnatural patterns in control charts mean that there are some unnatural causes for variations in SPC. Spiking neural networks (SNNs) are the third generation of artificial neural networks that consider time as an important feature for information representation and processing. In this paper, a spiking neural network architecture is proposed to be used for control charts pattern recognition (CCPR). Furthermore, enhancements to the SpikeProp learning algorithm are proposed. These enhancements provide additional learning rules for the synaptic delays, time constants and for the neurons thresholds. Simulated experiments have been conducted and the achieved results show a remarkable improvement in the overall performance compared with artificial neural networks. (C) 2012 Faculty of Engineering, Alexandria University. Production and hosting by Elsevier B. V. All rights reserved.
C1 [Awadalla, Medhat H. A.] Helwan Univ, Fac Engn, Communicat & Elect Dept, Cairo, Egypt.
   [Sadek, M. Abdellatif] Shorouk Acad, High Inst Engn, Informat Technol Dept, Cairo, Egypt.
RP Sadek, MA (corresponding author), Shorouk Acad, High Inst Engn, Informat Technol Dept, Cairo, Egypt.
EM awadalla_medhat@yahoo.co.uk; abdellatif_it_sha1@yahoo.com
CR [Anonymous], 2010, EUR J SCI RES
   Belhaouari Samir Brahim, 2009, WORLD ACAD SCI ENG T, V49, P1022
   Bohte S. M., 2000, SPIKEPROP ERROR BACK, P419
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Cheng H.P., 2009, J QUALITY, V16, P311
   Hodgkin A. L., 1992, J PHYSL, V117, P500
   Kiran NVNI, 2010, INT J COMPUT SCI NET, V10, P194
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Meftah R, 2008, AIP CONF PROC, V1019, P15, DOI 10.1063/1.2952970
   Natschlager T, 1998, NETWORK-COMP NEURAL, V9, P319, DOI 10.1088/0954-898X/9/3/003
   Pham D.T., 1994, CONTROL CHART PATTER
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Yang JH, 2005, COMPUT IND ENG, V48, P205, DOI 10.1016/j.cie.2005.01.008
NR 13
TC 17
Z9 21
U1 1
U2 1
PD MAR
PY 2012
VL 51
IS 1
BP 27
EP 35
DI 10.1016/j.aej.2012.07.004
WC Engineering, Multidisciplinary
DA 2023-11-11
ER

PT J
AU Belyaev, M
   Velichko, A
AF Belyaev, Maksim
   Velichko, Andrei
TI A Spiking Neural Network Based on the Model of VO<sub>2</sub>-Neuron
SO ELECTRONICS
DT Article
DE leaky integrate-and-fire neuron; vanadium dioxide; neural network;
   pattern recognition
ID METAL-INSULATOR-TRANSITION; FEATURES; DEVICE
AB In this paper, we present an electrical circuit of a leaky integrate-and-fire neuron with one VO2 switch, which models the properties of biological neurons. Based on VO2 neurons, a two-layer spiking neural network consisting of nine input and three output neurons is modeled in the SPICE simulator. The network contains excitatory and inhibitory couplings, and implements the winner-takes-all principle in pattern recognition. Using a supervised Spike-Timing-Dependent Plasticity training method and a timing method of information coding, the network was trained to recognize three patterns with dimensions of 3 x 3 pixels. The neural network is able to recognize up to 10(5) images per second, and has the potential to increase the recognition speed further.
C1 [Belyaev, Maksim; Velichko, Andrei] Petrozavodsk State Univ, Inst Phys & Technol, 31 Lenina Str, Petrozavodsk 185910, Russia.
RP Belyaev, M (corresponding author), Petrozavodsk State Univ, Inst Phys & Technol, 31 Lenina Str, Petrozavodsk 185910, Russia.
EM biomax89@yandex.ru; velichko@petrsu.ru
CR Abdel-Hamid O, 2014, IEEE-ACM T AUDIO SPE, V22, P1533, DOI 10.1109/TASLP.2014.2339736
   Amer S, 2019, IEEE J ELECTRON DEVI, V7, P18, DOI 10.1109/JEDS.2018.2875627
   [Anonymous], 2005, PROC CVPR IEEE
   Belyaev MA, 2018, PHYS SOLID STATE+, V60, P447, DOI 10.1134/S1063783418030046
   Beyeler M, 2015, NEURAL NETWORKS, V72, P75, DOI 10.1016/j.neunet.2015.09.005
   Beyeler M, 2013, NEURAL NETWORKS, V48, P109, DOI 10.1016/j.neunet.2013.07.012
   Bichler O, 2012, NEURAL NETWORKS, V32, P339, DOI 10.1016/j.neunet.2012.02.022
   Bonabi SY, 2014, FRONT NEUROSCI-SWITZ, V8, DOI 10.3389/fnins.2014.00379
   BORISKOV P, 2019, ELECTRONICS-SWITZ, V8, DOI DOI 10.3390/electronics8090922
   Brown BL, 2013, J APPL PHYS, V113, DOI 10.1063/1.4803551
   Cheung Kit, 2012, Artificial Neural Networks and Machine Learning - ICANN 2012. Proceedings of the 22nd International Conference on Artificial Neural Networks, P113, DOI 10.1007/978-3-642-33269-2_15
   Ciresan D, 2012, PROC CVPR IEEE, P3642, DOI 10.1109/CVPR.2012.6248110
   Crunteanu A, 2010, SCI TECHNOL ADV MAT, V11, DOI 10.1088/1468-6996/11/6/065002
   Cruz-Albrecht JM, 2012, IEEE T BIOMED CIRC S, V6, P246, DOI 10.1109/TBCAS.2011.2174152
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Du Y, 2015, PROC CVPR IEEE, P1110, DOI 10.1109/CVPR.2015.7298714
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Ignatov M, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00376
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Jeong DS, 2016, ADV ELECTRON MATER, V2, DOI 10.1002/aelm.201600090
   Jeong DS, 2012, REP PROG PHYS, V75, DOI 10.1088/0034-4885/75/7/076502
   Jeong H, 2019, J PHYS D APPL PHYS, V52, DOI 10.1088/1361-6463/aae223
   Jerry M, 2017, S VLSI TECH, pT186
   Jerry Matthew, 2016, 2016 74th Annual Device Research Conference (DRC), P1, DOI 10.1109/DRC.2016.7548503
   Karda K, 2018, IEEE T ELECTRON DEV, V65, P1672, DOI 10.1109/TED.2018.2817604
   Kim H, 2017, NANOTECHNOLOGY, V28, DOI 10.1088/1361-6528/aa86f8
   Kim S, 2018, NANOSCALE RES LETT, V13, DOI 10.1186/s11671-018-2660-9
   Kwon MW, 2018, J NANOSCI NANOTECHNO, V18, P6588, DOI 10.1166/jnn.2018.15700
   LeCun Y., MNIST HANDWRITTEN DI
   Lee C, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00435
   Lepage D, 2017, AIP ADV, V7, DOI 10.1063/1.4983175
   Li C, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351877
   Li YB, 2018, J PHYS D APPL PHYS, V51, DOI 10.1088/1361-6463/aade3f
   Lin CY, 2017, NANOSCALE, V9, P8586, DOI 10.1039/c7nr02305g
   Lin J., 2016, P 2016 IEEE INT EL D, P35
   Lin JQ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00856
   Liu WB, 2017, NEUROCOMPUTING, V234, P11, DOI 10.1016/j.neucom.2016.12.038
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Min EX, 2018, IEEE ACCESS, V6, P39501, DOI 10.1109/ACCESS.2018.2855437
   Nishitani Y, 2015, IEEE T NEUR NET LEAR, V26, P2999, DOI 10.1109/TNNLS.2015.2399491
   Oster M, 2009, NEURAL COMPUT, V21, P2437, DOI 10.1162/neco.2009.07-08-829
   Parihar A, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00210
   Patterson J, 2017, DEEP LEARNING PRACTI
   Pergament A, 2015, ADV COND MATTER PHYS, V2015, DOI 10.1155/2015/654840
   Pergament AL, 2010, J PHYS CHEM SOLIDS, V71, P874, DOI 10.1016/j.jpcs.2010.03.032
   Pergament A, 2018, PHYSICA B, V536, P239, DOI 10.1016/j.physb.2017.10.123
   Ponulak F, 2011, ACTA NEUROBIOL EXP, V71, P409
   Querlioz D, 2013, IEEE T NANOTECHNOL, V12, P288, DOI 10.1109/TNANO.2013.2250995
   Sakai J, 2008, J APPL PHYS, V103, DOI 10.1063/1.2930959
   Saunders D. J., 2018, 2018 INT JOINT C NEU, P1
   Savchenko AV, 2019, INFORM SCIENCES, V489, P18, DOI 10.1016/j.ins.2019.03.030
   Serrano-Gotarredona T, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00002
   Shukla A, 2017, IEEE IJCNN, P4657, DOI 10.1109/IJCNN.2017.7966447
   Sourikopoulos I, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00123
   Srinivasan G, 2016, SCI REP-UK, V6, DOI 10.1038/srep29545
   Sun C, 2014, J MATER CHEM C, V2, P9283, DOI 10.1039/c4tc00778f
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Tavanaei A, 2019, NEUROCOMPUTING, V330, P39, DOI 10.1016/j.neucom.2018.11.014
   Velichko A, 2018, SOLID STATE ELECTRON, V141, P40, DOI 10.1016/j.sse.2017.12.003
   Velichko A, 2018, SOLID STATE ELECTRON, V139, P8, DOI 10.1016/j.sse.2017.09.014
   Wang Z, 2018, INT EL DEVICES MEET
   Yi W, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-07052-w
   Yousefzadeh A, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00665
   Zhou ER, 2018, ELECTRONICS-SWITZ, V7, DOI 10.3390/electronics7120396
NR 64
TC 8
Z9 8
U1 0
U2 19
PD OCT
PY 2019
VL 8
IS 10
AR 1065
DI 10.3390/electronics8101065
WC Computer Science, Information Systems; Engineering, Electrical &
   Electronic; Physics, Applied
DA 2023-11-11
ER

PT C
AU Masumori, A
   Sinapayen, L
   Ikegami, T
AF Masumori, Atsushi
   Sinapayen, Lana
   Ikegami, Takashi
BE Knibbe, C
   Beslon, G
   Parsons, D
   Misevic, D
   RouzaudCornabas, J
   Bredeche, N
   Hassas, S
   Simonin, O
   Soula, H
TI Learning by Stimulation Avoidance Scales to Large Neural Networks
SO FOURTEENTH EUROPEAN CONFERENCE ON ARTIFICIAL LIFE (ECAL 2017)
DT Proceedings Paper
CT 14th European Conference on Artificial Life (ECAL)
CY SEP 04-08, 2017
CL Natl Inst Appl Sci Lyon, Lyon, FRANCE
HO Natl Inst Appl Sci Lyon
ID TIMING-DEPENDENT PLASTICITY; MODEL; DYNAMICS; NEURONS
AB Spiking neural networks with spike-timing dependent plasticity (STDP) can learn to avoid the external stimulations spontaneously. This principle is called "Learning by Stimulation Avoidance" (LSA) and can be used to reproduce learning experiments on cultured biological neural networks. LSA has promising potential, but its application and limitations have not be studied extensively. This paper focuses on the scalability of LSA for large networks and shows that LSA works well in small networks (100 neurons) and can be scaled to networks up to approximately 3,000 neurons.
C1 [Masumori, Atsushi; Sinapayen, Lana; Ikegami, Takashi] Univ Tokyo, Tokyo, Japan.
RP Masumori, A (corresponding author), Univ Tokyo, Tokyo, Japan.
EM masumori@sacral.c.u-tokyo.ac.jp
CR [Anonymous], 1943, J PHILOS
   [Anonymous], COM ADAP SY
   Ashby W.R., 1960, DESIGN BRAIN
   Bakkum DJ, 2008, J NEURAL ENG, V5, P310, DOI 10.1088/1741-2560/5/3/004
   Berkes P, 2011, SCIENCE, V331, P83, DOI 10.1126/science.1195870
   Brette R, 2007, J COMPUT NEUROSCI, V23, P349, DOI 10.1007/s10827-007-0038-6
   Canepari M, 1997, BIOL CYBERN, V77, P153, DOI 10.1007/s004220050376
   Caporale N, 2008, ANNU REV NEUROSCI, V31, P25, DOI 10.1146/annurev.neuro.31.060407.125639
   Cassenaer S, 2007, NATURE, V448, P709, DOI 10.1038/nature05973
   Dan Y, 2006, PHYSIOL REV, V86, P1033, DOI 10.1152/physrev.00030.2005
   DeMarse TB, 2005, IEEE IJCNN, P1548
   Di Paolo EA, 2008, BIOSYSTEMS, V91, P409, DOI 10.1016/j.biosystems.2007.05.016
   Doi I., 2017, 14 EUR C ART LIF ECA
   Friston K, 2016, NEUROSCI BIOBEHAV R, V68, P862, DOI 10.1016/j.neubiorev.2016.06.022
   Friston KJ, 2010, NAT REV NEUROSCI, V11, P127, DOI 10.1038/nrn2787
   Hebb D. O., 1949, ORG BEHAV A NEUROPSY
   Iizuka H, 2007, ADAPT BEHAV, V15, P359, DOI 10.1177/1059712307084687
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Kudoh SN, 2008, 2008 INTERNATIONAL CONFERENCE ON MECHATRONICS AND AUTOMATION: (ICMA), VOLS 1 AND 2, P374
   Marom S, 2002, Q REV BIOPHYS, V35, P63, DOI 10.1017/S0033583501003742
   Masumori A., 2016, P ANN M PHYS SOC JAP, V71, P3092
   Masumori A, 2015, ECAL 2015: THE THIRTEENTH EUROPEAN CONFERENCE ON ARTIFICIAL LIFE, P373, DOI 10.7551/978-0-262-33027-5-ch067
   Mongillo G, 2008, SCIENCE, V319, P1543, DOI 10.1126/science.1150769
   Potter S. M., 2006, ADV NETWORK ELECTROP, P215
   SEJNOWSKI TJ, 1988, SCIENCE, V241, P1299, DOI 10.1126/science.3045969
   Shahaf G, 2001, J NEUROSCI, V21, P8782, DOI 10.1523/JNEUROSCI.21-22-08782.2001
   Sinapayen L, 2015, 13 EUR C ART LIF ECA
   Sinapayen L, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0170388
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Wagenaar DA, 2005, J NEUROSCI, V25, P680, DOI 10.1523/JNEUROSCI.4209-04.2005
   Warwick K, 2010, ETHICS INF TECHNOL, V12, P223, DOI 10.1007/s10676-010-9218-6
NR 33
TC 4
Z9 4
U1 0
U2 2
PY 2017
BP 275
EP 282
WC Computer Science, Interdisciplinary Applications; Mathematical &
   Computational Biology
DA 2023-11-11
ER

PT J
AU Cariani, PA
AF Cariani, PA
TI Neural timing nets
SO NEURAL NETWORKS
DT Review
DE neural timing networks; time-delay neural networks; temporal coding;
   spiking neurons; scene analysis; temporal correlation; auditory
   neurocomputation
ID CONCURRENT VOWELS; AUDITORY PERIPHERY; PHASE SENSITIVITY;
   COMPUTER-MODEL; VIRTUAL PITCH; PERCEPTION; INFORMATION; PATTERNS;
   RECOGNITION; SYNCHRONY
AB Formulations of artificial neural networks are directly related to assumptions about neural coding in the brain. Traditional connectionist networks assume channel-based rate coding, while time-delay networks convert temporally-coded inputs into rate-coded outputs. Neural timing nets that operate on time structured input spike trains to produce meaningful time-structured outputs are proposed. Basic computational properties of simple feedforward and recurrent timing nets are outlined and applied to auditory computations. Feed-forward timing nets consist of arrays of coincidence detectors connected via tapped delay lines. These temporal sieves extract common spike patterns in their inputs that can subserve extraction of common fundamental frequencies (periodicity pitch) and common spectrum (timbre). Feedforward timing nets can also be used to separate time-shifted patterns, fusing patterns with similar internal temporal structure and spatially segregating different ones. Simple recurrent timing nets consisting of arrays of delay loops amplify and separate recurring time patterns. Single- and multichannel recurrent timing nets are presented that demonstrate the separation of concurrent, double vowels. Timing nets constitute a new and general neural network strategy for performing temporal computations on neural spike trains: extraction of common periodicities, detection of recurring temporal patterns, and formation and separation of invariant spike patterns that subserve auditory objects. (C) 2001 Elsevier Science Ltd. All rights reserved.
C1 Massachusetts Eye & Ear Infirm, Eaton Peabody Lab Auditory Physiol, Boston, MA 02114 USA.
RP Cariani, PA (corresponding author), Massachusetts Eye & Ear Infirm, Eaton Peabody Lab Auditory Physiol, 243 Charles St, Boston, MA 02114 USA.
EM peter@epl.meei.harvard.edu
CR ABELES M, 1982, ISRAEL J MED SCI, V18, P83
   ABELES M, 1990, CORTICONIS
   Abeles M., 1994, TEMPORAL CODING BRAI, P39
   AKEROYD MA, 2000, BRIT J AUDIOL, V33, P106
   [Anonymous], THESIS RIJKSUNIVERSI
   [Anonymous], 1967, CORRELATION TECHNIQU
   [Anonymous], NEUROSCI RES PROGRAM
   [Anonymous], FUNCTIONAL NEUROSCIE
   [Anonymous], HDB PERCEPTION
   [Anonymous], 1989, ANALOG VLSI NEURAL S
   [Anonymous], 1973, PSYCHOBIOLOGY SENSOR
   [Anonymous], 1986, BRAIN THEORY
   [Anonymous], 1949, THEORY HEARING
   ASSMANN PF, 1989, J ACOUST SOC AM, V85, P327, DOI 10.1121/1.397684
   ASSMANN PF, 1990, J ACOUST SOC AM, V88, P680, DOI 10.1121/1.399772
   BIALEK W, 1991, SCIENCE, V252, P1854, DOI 10.1126/science.2063199
   Boring E.G., 1942, SENSATION PERCEPTION
   Braitenberg V, 1967, Prog Brain Res, V25, P334
   BRAITENBERG V, 1961, NATURE, V190, P539, DOI 10.1038/190539b0
   Cariani P, 2001, NATO SCI S A LIF SCI, V312, P233
   Cariani P, 1999, NEURAL PLAST, V6, P147, DOI 10.1155/NP.1999.147
   CARIANI P, 1999, MUS TIM NETW P FWO R, P28
   CARIANI P, 1994, J ACOUST SOC AM, V95, P2842
   CARIANI P, 2000, ASS RES OT ABSTR, V23, P102
   CARIANI P, 1997, P AUD ENG SOC M AES
   CARIANI P, 1997, INTELLECTICA, V2, P95
   CARIANI P, 2001, IN PRESS J NEW MUSIC
   CARIANI P, 1993, ASS RES OT ABS, V16, P373
   Cariani P., 1995, COMMUN COGNIT ARTIF, P161
   Cariani P. A., 1996, Society for Neuroscience Abstracts, V22, P649
   Cariani P. A., 2001, Acoustical Science and Technology, V22, P77, DOI 10.1250/ast.22.77
   Cariani PA, 1996, J NEUROPHYSIOL, V76, P1698, DOI 10.1152/jn.1996.76.3.1698
   Cariani PA, 1997, COMPUTATIONAL NEUROSCIENCE: TRENDS IN RESEARCH, 1997, P591
   CARNEY T, 1995, VISION RES, V35, P1951, DOI 10.1016/0042-6989(94)00288-W
   CARR CE, 1993, ANNU REV NEUROSCI, V16, P223, DOI 10.1146/annurev.ne.16.030193.001255
   Cherry C., 1961, SENSORY COMMUNICATIO, P99, DOI [10.7551/mitpress/9780262518420.003.0006, DOI 10.7551/MITPRESS/9780262518420.003.0006]
   CHUNG SH, 1970, BRAIN BEHAV EVOLUT, V3, P72, DOI 10.1159/000125464
   COLBURN S, 1996, AUDITORY COMPUTATION
   Culling JF, 1998, J ACOUST SOC AM, V103, P3509, DOI 10.1121/1.423059
   de Cheveigné A, 1998, J ACOUST SOC AM, V103, P1261, DOI 10.1121/1.423232
   de Ribaupierre F., 1997, CENTRAL AUDITORY SYS, P317
   DEBOER E, 1976, HDB SENSORY PHYSL, V3, P479
   DILORENZO PM, 1993, BEHAV NEUROSCI, V107, P130, DOI 10.1037/0735-7044.107.1.130
   EGGERMONT J, 1993, CONCEPT NEUROSCI, V4, P105
   EGGERMONT JJ, 1990, CORRELATIVE BRAIN TH, V24
   Emmers R., 1981, PAIN SPIKE INTERVAL
   GHITZA O, 1988, J PHONETICS, V16, P109, DOI 10.1016/S0095-4470(19)30469-3
   GOLDSTEIN JL, 1977, PSYCHOPHYSICS PHYSL
   Grossberg S, 1988, ADAPTIVE BRAIN
   GROSSBERG S, 1995, MIND MOTION EXPLORAT, P449
   GROSSBERG S, 1988, ADAPTIVE BRAIN, V1
   HIRAHARA T, 1996, P EUR SPEECH COMM AS, P1
   HIRAHARA T, 1996, ASS RES OT ABSTR, V19, P80
   JEFFRESS LA, 1948, J COMP PHYSIOL PSYCH, V41, P35, DOI 10.1037/h0061495
   JONES MR, 1976, PSYCHOL REV, V83, P323, DOI 10.1037/0033-295X.83.5.323
   KAUER JS, 1974, J PHYSIOL-LONDON, V243, P695, DOI 10.1113/jphysiol.1974.sp010772
   Kiang NY., 1965, DISCHARGE PATTERNS S
   KOZAK WM, 1974, VISION RES, V14, P405, DOI 10.1016/0042-6989(74)90239-9
   Krishna BS, 2000, J NEUROPHYSIOL, V84, P255, DOI 10.1152/jn.2000.84.1.255
   LANGNER G, 1992, HEARING RES, V60, P115, DOI 10.1016/0378-5955(92)90015-F
   Laurent G, 1999, SCIENCE, V286, P723, DOI 10.1126/science.286.5440.723
   LICKLIDER JCR, 1951, EXPERIENTIA, V7, P128, DOI 10.1007/BF02156143
   Licklider JCR., 1959, PSYCHOL STUDY SCI, P41
   LONGUETHIGGINS H, 1987, MENTAL PROCESSES STU
   LONGUETHIGGINS HC, 1989, COMP NEUR S, P99
   Lyon R., 1996, AUDITORY COMPUTATION, P221, DOI [DOI 10.1007/978-1-4612-4070-9_6, 10.1007/978-1-4612-4070-96, DOI 10.1007/978-1-4612-4070-96]
   LYON R, 1995, AUDITORY COMPUTATION, P221
   LYON RF, 1984, IEEE INT C AC SPEECH
   MacKay D.M., 1962, Self-Organizing Systems 1962, P37, DOI 10.1016/j.biosystems.2017.07.008
   MACKAY DM, 1952, B MATH BIOPHYS, V14
   MEDDIS R, 1991, J ACOUST SOC AM, V89, P2866, DOI 10.1121/1.400725
   Meddis R, 1997, J ACOUST SOC AM, V102, P1811, DOI 10.1121/1.420088
   MEDDIS R, 1992, J ACOUST SOC AM, V91, P233, DOI 10.1121/1.402767
   MEDDIS R, 1991, J ACOUST SOC AM, V89, P2883, DOI 10.1121/1.400726
   MEYEREPPLER W, 1953, COMMUN THEORY, P183
   MICHELSON A, 1992, EVOLUTIONARY BIOL HE, P49
   Miller R.R., 1993, CURRENT DIRECTIONS P, V2, P106, DOI [10.1111/1467-8721.ep10772577, DOI 10.1111/1467-8721.EP10772577]
   Moore B. C., 1997, INTRO PSYCHOL HEARIN
   Mountcastle V., 1967, NEUROSCIENCES STUDY
   Pabst M., 1989, MODELS BRAIN FUNCTIO, P137
   Palmer AR, 1992, AUDITORY PROCESSING, P115
   PATTERSON RD, 1995, J ACOUST SOC AM, V98, P1890, DOI 10.1121/1.414456
   REES A, 1987, HEARING RES, V27, P129, DOI 10.1016/0378-5955(87)90014-1
   Reichardt W, 1961, SENS COMMUN, P303
   Reinagel P, 2000, J NEUROSCI, V20, P5392, DOI 10.1523/JNEUROSCI.20-14-05392.2000
   Reitboeck H. J., 1989, SENSORY PROCESSING M, P307
   REITBOEK HJ, 1988, COMPUTER SIMULATION
   Rieke F., 1997, SPIKES EXPLORING NEU
   ROSE JE, 1971, J NEUROPHYSIOL, V34, P685, DOI 10.1152/jn.1971.34.4.685
   Sejnowski TJ, 1998, PULSED NEURAL NETWORKS, pXIII
   SENEFF S, 1988, J PHONETICS, V16, P55, DOI 10.1016/S0095-4470(19)30466-8
   SENEFF S, 1985, UNPUB PITCH SPECTRAL
   Shamma S, 2000, J ACOUST SOC AM, V107, P2631, DOI 10.1121/1.428649
   SINGER W, 1995, TEMPORAL CODING BRAI, P51
   SLANEY M, 1993, VISUAL REPRESENTATIO, P95
   SUMMERFIELD Q, 1991, J ACOUST SOC AM, V89, P1364, DOI 10.1121/1.400659
   TROLAND LT, 1929, PRINCIPLES PSYCHOPHY, V1
   VANNOORDEN L, 1982, MUSIC MIND BRAIN, P251, DOI DOI 10.1007/978-1-4684-8917-0_13
   von Bekesy G., 1967, SENSORY INHIBITION
   Wasserman Gerald S., 1992, Biological Signals, V1, P117
   WEANG DL, 1995, NEURAL REPRESENTATIO, P53
   YOUNG ED, 1979, J ACOUST SOC AM, V66, P1381, DOI 10.1121/1.383532
   YOUNG RA, 1977, VISION RES, V17, P957, DOI 10.1016/0042-6989(77)90071-2
   ZWICKER UT, 1984, SPEECH COMMUN, V3, P265, DOI 10.1016/0167-6393(84)90023-2
NR 104
TC 32
Z9 32
U1 0
U2 10
PD JUL-SEP
PY 2001
VL 14
IS 6-7
SI SI
BP 737
EP 753
DI 10.1016/S0893-6080(01)00056-9
WC Computer Science, Artificial Intelligence; Neurosciences
DA 2023-11-11
ER

PT C
AU Mouraud, A
   Puzenat, D
AF Mouraud, Anthony
   Puzenat, Didier
BE Brown, DP
   Draganova, C
   Pimenidis, E
   Mouratidis, H
TI Simulation of Large Spiking Neural Networks on Distributed
   Architectures, The "DAMNED" Simulator
SO ENGINEERING APPLICATIONS OF NEURAL NETWORKS, PROCEEDINGS
SE Communications in Computer and Information Science
DT Proceedings Paper
CT 11th International Conference on Engineering Applications of Neural
   Networks
CY AUG 27-29, 2009
CL London, ENGLAND
ID EVENT-DRIVEN SIMULATION; EFFICIENT; DYNAMICS; NEURONS
AB Tlis paper presents a spiking neural network simulator suitable for biologically plausible large neural networks; named DAMNED for "Distributed And Multi-threaded Neural Event-Driven". The simulator is designed to ruts efficiently on a variety of hardware. DAMNED makes use of multi-threaded programming and non-blocking communications in order to optimize communications and computations overlap. This paper details the even-driven architecture of the simulator. Some original contributions are presented, such as the handling of a distributed virtual clock and an efficient; circular event queue taking into account spike propagation delays. DAMNED is evaluated out a cluster of computers for networks from 10(3) to 10(5) neurons. Simulation and network creation speedups are presented. Finally, scalability is discussed regarding number of processors, network size. and activity of the simulated NN.
C1 [Mouraud, Anthony; Puzenat, Didier] Univ Antilles Guyane, GRIMAAG Lab, Pointe A Pitre, Guadeloupe, France.
RP Mouraud, A (corresponding author), Univ Antilles Guyane, GRIMAAG Lab, Pointe A Pitre, Guadeloupe, France.
EM amouraud@univ-ag.fr; dpuzenat@univ-ag.fr
CR [Anonymous], ADV NEURAL INFORM PR
   Bohte Sander M., 2004, Natural Computing, V3, P195, DOI 10.1023/B:NACO.0000027755.02868.60
   CHANDY KM, 1979, IEEE T SOFTWARE ENG, V5, P440, DOI 10.1109/TSE.1979.230182
   Ferscha A., 1996, PARALLEL DISTRIBUTED, P1003
   Flynn MJ, 1996, ACM COMPUT SURV, V28, P67, DOI 10.1145/234313.234345
   Gerstner W., 2002, SPIKING NEURON MODEL
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   KNIGHT BW, 1972, J GEN PHYSIOL, V59, P734, DOI 10.1085/jgp.59.6.734
   Lobb CJ, 2005, Workshop on Principles of Advanced and Distributed Simulation, Proceedings, P16, DOI 10.1109/PADS.2005.18
   Makino T, 2003, NEURAL COMPUT APPL, V11, P210, DOI 10.1007/s00521-003-0358-z
   MARIN M, 2000, SCCC, P172
   Mattia M, 2000, NEURAL COMPUT, V12, P2305, DOI 10.1162/089976600300014953
   Morrison A, 2005, NEURAL COMPUT, V17, P1776, DOI 10.1162/0899766054026648
   Rudolph M, 2006, NEURAL COMPUT, V18, P2146, DOI 10.1162/neco.2006.18.9.2146
   Shelley MJ, 2001, J COMPUT NEUROSCI, V11, P111, DOI 10.1023/A:1012885314187
   SWADLOW HA, 1988, J NEUROPHYSIOL, V59, P1162, DOI 10.1152/jn.1988.59.4.1162
   *U TN, 1994, UTCS94230
NR 17
TC 3
Z9 3
U1 0
U2 0
PY 2009
VL 43
BP 359
EP +
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Shirsavar, SR
   Vahabie, AH
   Dehaqani, MRA
AF Shirsavar, Shahriar Rezghi
   Vahabie, Abdol-Hossein
   Dehaqani, Mohammad-Reza A.
TI Models developed for spiking neural networks
SO METHODSX
DT Article
DE Spiking neural network; Biological plausibility; STDP; R-STDP;
   Backpropagation; ANN-to-SNN
ID PLASTICITY; REPRESENTATION
AB Emergence of deep neural networks (DNNs) has raised enormous attention towards artificial neural networks (ANNs) once again. They have become the state-of-the-art models and have won different machine learning challenges. Although these networks are inspired by the brain, they lack biological plausibility, and they have structural differences compared to the brain. Spiking neural networks (SNNs) have been around for a long time, and they have been investigated to understand the dynamics of the brain. However, their application in real-world and complicated machine learning tasks were limited. Recently, they have shown great potential in solving such tasks. Due to their energy efficiency and temporal dynamics there are many promises in their future development. In this work, we reviewed the structures and performances of SNNs on image classification tasks. The comparisons illustrate that these networks show great capabilities for more complicated problems. Furthermore, the simple learning rules developed for SNNs, such as STDP and R-STDP, can be a potential alternative to replace the backpropagation algorithm used in DNNs. center dot Different building blocks of spiking neural networks are explained in this work. center dot Developed models for SNNs are introduced based on their characteristics and building blocks.
C1 [Shirsavar, Shahriar Rezghi; Vahabie, Abdol-Hossein; Dehaqani, Mohammad-Reza A.] Univ Tehran, Coll Engn, Sch Elect & Comp Engn, Tehran, Iran.
   [Shirsavar, Shahriar Rezghi] Inst Res Fundamental Sci IPM, Sch Cognit Sci, Tehran, Iran.
RP Dehaqani, MRA (corresponding author), Univ Tehran, Coll Engn, Sch Elect & Comp Engn, Tehran, Iran.
EM shahriar.rezghi@ut.ac.ir; h.vahabie@ut.ac.ir; dehaqani@ut.ac.ir
CR Abbott LF, 1999, BRAIN RES BULL, V50, P303, DOI 10.1016/S0361-9230(99)00161-6
   Adrian ED, 1926, J PHYSIOL-LONDON, V61, P465, DOI 10.1113/jphysiol.1926.sp002308
   Akbarzadeh-Sherbaf K, 2020, NEUROCOMPUTING, V412, P129, DOI 10.1016/j.neucom.2020.05.044
   Akbarzadeh-Sherbaf K, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00698
   Alcorn MA, 2019, PROC CVPR IEEE, P4840, DOI 10.1109/CVPR.2019.00498
   Beer M, 2020, 18 INT C INF PROC MA, V1237, P59, DOI [10.1007/978-3-030-50146-4_5, DOI 10.1007/978-3-030-50146-4_5]
   Beyeler M, 2013, NEURAL NETWORKS, V48, P109, DOI 10.1016/j.neunet.2013.07.012
   Bohte S., 2000, SPIKEPROP BACKPROPAG, P419
   Buzsaki G, 2011, RHYTHMS BRAIN, DOI DOI 10.1093/ACPROF:OSO/9780195301069.001.0001
   Chen TQ, 2015, Arxiv, DOI arXiv:1512.01274
   Cheng X, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1519
   Citri A, 2008, NEUROPSYCHOPHARMACOL, V33, P18, DOI 10.1038/sj.npp.1301559
   Collobert R, 2011, J MACH LEARN RES, V12, P2493
   Dehaqani MRA, 2018, CEREB CORTEX, V28, P3046, DOI 10.1093/cercor/bhy141
   Dehaqani MRA, 2016, J NEUROPHYSIOL, V116, P587, DOI 10.1152/jn.00018.2016
   Diehl Peter U, 2015, 2015 INT JOINT C NEU, P1, DOI [DOI 10.1109/IJCNN.2015.7280696, 10.1109/IJCNN.2015.7280696]
   Ding JH, 2021, Arxiv, DOI arXiv:2105.11654
   Drix D, 2020, NEURAL NETWORKS, V131, P37, DOI 10.1016/j.neunet.2020.06.007
   Frémaux N, 2016, FRONT NEURAL CIRCUIT, V9, DOI 10.3389/fncir.2015.00085
   Gautrais J, 1998, BIOSYSTEMS, V48, P57, DOI 10.1016/S0303-2647(98)00050-1
   Gewaltig M-O., 2007, SCHOLARPEDIA, V2, DOI [10.4249/scholarpedia.1430, DOI 10.4249/SCHOLARPEDIA.1430]
   Gollisch T, 2008, SCIENCE, V319, P1108, DOI 10.1126/science.1149639
   Graves A, 2005, NEURAL NETWORKS, V18, P602, DOI 10.1016/j.neunet.2005.06.042
   Hazan H, 2018, FRONT NEUROINFORM, V12, DOI 10.3389/fninf.2018.00089
   HEBB D. O., 1949
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Johansson RS, 2004, NAT NEUROSCI, V7, P170, DOI 10.1038/nn1177
   Jolivet R, 2003, LECT NOTES COMPUT SC, V2714, P846
   Huynh PK, 2022, Arxiv, DOI arXiv:2202.08897
   Kheradpisheh SR, 2022, NEURAL PROCESS LETT, V54, P1255, DOI 10.1007/s11063-021-10680-x
   Kheradpisheh SR, 2020, INT J NEURAL SYST, V30, DOI 10.1142/S0129065720500276
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Kingma D. P., 2014, ARXIV, DOI DOI 10.48550/ARXIV.1412.6980
   Kostakos V, 2017, UNDERST COMPLEX SYST, P69, DOI 10.1007/978-3-319-25658-0_4
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
   Laurent G, 1996, TRENDS NEUROSCI, V19, P489, DOI 10.1016/S0166-2236(96)10054-0
   Lee C, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00119
   Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508
   Li D, 2016, PROCEEDINGS OF 2016 IEEE INTERNATIONAL CONFERENCES ON BIG DATA AND CLOUD COMPUTING (BDCLOUD 2016) SOCIAL COMPUTING AND NETWORKING (SOCIALCOM 2016) SUSTAINABLE COMPUTING AND COMMUNICATIONS (SUSTAINCOM 2016) (BDCLOUD-SOCIALCOM-SUSTAINCOM 2016), P477, DOI 10.1109/BDCloud-SocialCom-SustainCom.2016.76
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   McCulloch W. S., 1943, B MATH BIOPHYS, V5, P115, DOI [10.1007/BF02478259, DOI 10.1007/BF02478259]
   Mozafari M, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00625
   Mozafari M, 2019, PATTERN RECOGN, V94, P87, DOI 10.1016/j.patcog.2019.05.015
   Nair V, 2010, INT C MACH LEARN HAI, V27, P807
   OKEEFE J, 1993, HIPPOCAMPUS, V3, P317, DOI 10.1002/hipo.450030307
   Paszke A., 2019, ADV NEURAL INFORM PR, P8024
   Pehle Christian-Gernot, 2021, Zenodo
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Saunders DJ, 2019, NEURAL NETWORKS, V119, P332, DOI 10.1016/j.neunet.2019.08.016
   Shen GB, 2022, PATTERNS, V3, DOI 10.1016/j.patter.2022.100522
   Shirsavar S.R., 2022, ARXIV, DOI [10.48550/arXiv.2210.17442, DOI 10.48550/ARXIV.2210.17442]
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Stimberg M, 2019, ELIFE, V8, DOI 10.7554/eLife.47314
   Sun C, 2017, IEEE I CONF COMP VIS, P843, DOI 10.1109/ICCV.2017.97
   Tan MX, 2020, Arxiv, DOI [arXiv:1905.11946, DOI 10.48550/ARXIV.1905.11946]
   TensorFlow Developers, 2022, Zenodo, DOI 10.5281/ZENODO.6574269
   Thorpe S, 1998, COMPUTATIONAL NEUROSCIENCE: TRENDS IN RESEARCH, P113
   Vreeken J, 2003, SPIKING NEURAL NETWO
   Wang TX, 2021, NEUROCOMPUTING, V425, P96, DOI 10.1016/j.neucom.2020.10.100
   Wiener MC, 2003, J NEUROSCI, V23, P2394
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Xu Q, 2020, NEURAL NETWORKS, V121, P512, DOI 10.1016/j.neunet.2019.08.034
   Xu ZH, 2017, NEUROCOMPUTING, V267, P141, DOI 10.1016/j.neucom.2017.05.016
   Zhang AG, 2019, NEUROCOMPUTING, V365, P102, DOI 10.1016/j.neucom.2019.07.009
   Zhang W, 2020, ADV NEURAL INFORM PR, V33, P12022, DOI DOI 10.48550/ARXIV.2002.10085
   Zhang Wenrui, 2019, Arxiv, DOI arXiv:1908.06378
   Zhao D., ARXIV
NR 69
TC 0
Z9 0
U1 2
U2 2
PY 2023
VL 10
AR 102157
DI 10.1016/j.mex.2023.102157
EA APR 2023
WC Multidisciplinary Sciences
DA 2023-11-11
ER

PT J
AU Ros, E
   Pelayo, FJ
   Martin-Smith, P
   Rojas, I
   Palomar, D
   Prieto, A
AF Ros, E
   Pelayo, FJ
   Martin-Smith, P
   Rojas, I
   Palomar, D
   Prieto, A
TI Competitive and temporal inhibition structures with spiking neurons
SO NEURAL PROCESSING LETTERS
DT Article
DE spiking neurons; competitive processing; temporal inhibition;
   attentional control mechanisms; bio-inspired neural systems
ID NEURAL NETWORKS; VLSI
AB The paper describes the implementation of competitive neural structures based on a spiking neural model that includes multiplicative or shunting synapses enabling non-saturated stable states in response to different stationary inputs as well as controllable transient responses. A VLSI-viable implementation of this model has been previously proposed and tested [1]. It has the possibility of modulating the output spike frequency by an additional input without affecting other neuron variables such as the membrane potential. This feature is exploited in the simulation of a Selective Temporal Inhibition network that is suitable for implementing attentional control systems.
C1 Univ Granada, Dept Architecture & Comp Technol, E-18071 Granada, Spain.
CR [Anonymous], 1991, THESIS CALTECH
   [Anonymous], ADAPTIVE BRAIN
   BOAHEN K, 1996, RETINOMORPHIC VISION, V2, P14
   GROSSBERG S, 1988, NEURAL NETWORKS, V1, P17, DOI 10.1016/0893-6080(88)90021-4
   GROSSBERG S, 1987, ADAPTIVE BRAIN, V2
   Mahowald M., 1992, THESIS CALTECH
   MARTINSMITH P, 1997, THESIS U GRANADA
   Morris TG, 1997, ANALOG INTEGR CIRC S, V13, P79, DOI 10.1023/A:1008275710961
   MORTARA A, 1994, IEEE T NEURAL NETWOR, V5, P459, DOI 10.1109/72.286916
   Mortara A, 1997, ANALOG INTEGR CIRC S, V13, P93, DOI 10.1023/A:1008232011870
   Niebur E, 1996, ADV NEUR IN, V8, P802
   Pelayo FJ, 1997, ANALOG INTEGR CIRC S, V13, P111, DOI 10.1023/A:1008240229616
NR 12
TC 1
Z9 1
U1 0
U2 0
PD JUN
PY 2000
VL 11
IS 3
BP 197
EP 208
DI 10.1023/A:1009611609606
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT C
AU Chang, CC
   Chen, PC
   Hudec, B
   Liu, PT
   Hou, TH
AF Chang, Che-Chia
   Chen, Pin-Chun
   Hudec, Boris
   Liu, Po-Tsun
   Hou, Tuo-Hung
GP IEEE
TI Interchangeable Hebbian and Anti-Hebbian STDP Applied to Supervised
   Learning in Spiking Neural Network
SO 2018 IEEE INTERNATIONAL ELECTRON DEVICES MEETING (IEDM)
SE IEEE International Electron Devices Meeting
DT Proceedings Paper
CT 64th IEEE Annual International Electron Devices Meeting (IEDM)
CY DEC 01-05, 2018
CL San Francisco, CA
AB This work provides a complete framework, including device, architecture, and algorithm, for implementing bio-inspired supervised spiking neural networks (SNNs) on hardware. An analog synapse with atypical dual bipolar resistive-switching (D-BRS) modes demonstrates interchangeable Hebbian spiking-timing-dependent plasticity (STDP) and anti-Hebbian STDP, and it is capable of implementing supervised ReSuMe SNNs in crossbar arrays. By using an "exchange" update scheme, accurate supervised learning (similar to 96% for MNIST) is achieved in a compact network.
C1 [Chang, Che-Chia; Chen, Pin-Chun; Hudec, Boris; Hou, Tuo-Hung] Natl Chiao Tung Univ, Dept Elect Engn, Hsinchu, Taiwan.
   [Chang, Che-Chia; Chen, Pin-Chun; Hudec, Boris; Hou, Tuo-Hung] Natl Chiao Tung Univ, Inst Elect, Hsinchu, Taiwan.
   [Liu, Po-Tsun] Natl Chiao Tung Univ, Dept Photon, Hsinchu, Taiwan.
RP Hou, TH (corresponding author), Natl Chiao Tung Univ, Dept Elect Engn, Hsinchu, Taiwan.; Hou, TH (corresponding author), Natl Chiao Tung Univ, Inst Elect, Hsinchu, Taiwan.
EM thhou@mail.nctu.edu.tw
CR [Anonymous], IEEE INT EL DEV M
   Chang C.-C., 2017, IEDM, P278
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   OConnor P., 2016, ARXIV160208323
   Pedretti G., 2017, IEDM, P653
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Roberts PD, 2002, BIOL CYBERN, V87, P392, DOI 10.1007/s00422-002-0361-y
   Yoon JH, 2015, ADV MATER, V27, P3811, DOI 10.1002/adma.201501167
   Yu S., 2015, IEDM, P451, DOI 10.1109/IEDM.2015.7409718
   Zamarreño-Ramos C, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00026
NR 11
TC 4
Z9 5
U1 0
U2 0
PY 2018
WC Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Matsubara, T
AF Matsubara, Takashi
TI Conduction Delay Learning Model for Unsupervised and Supervised
   Classification of Spatio-Temporal Spike Patterns
SO FRONTIERS IN COMPUTATIONAL NEUROSCIENCE
DT Article
DE spiking neural network; temporal coding; delay learning;
   activity-dependent myelination; spike timing-dependent plasticity;
   unsupervised learning
ID TIMING-DEPENDENT PLASTICITY; INTERAURAL TIME DIFFERENCES;
   NEURAL-NETWORK; NEURONS; POLYCHRONIZATION; MYELINATION; COINCIDENCE;
   MECHANISM; DYNAMICS
AB Precise spike timing is considered to play a fundamental role in communications and signal processing in biological neural networks. Understanding the mechanism of spike timing adjustment would deepen our understanding of biological systems and enable advanced engineering applications such as efficient computational architectures. However, the biological mechanisms that adjust and maintain spike timing remain unclear. Existing algorithms adopt a supervised approach, which adjusts the axonal conduction delay and synaptic efficacy until the spike timings approximate the desired timings. This study proposes a spike timing-dependent learning model that adjusts the axonal conduction delay and synaptic efficacy in both unsupervised and supervised manners. The proposed learning algorithm approximates the Expectation-Maximization algorithm, and classifies the input data encoded into spatio-temporal spike patterns. Even in the supervised classification, the algorithm requires no external spikes indicating the desired spike timings unlike existing algorithms. Furthermore, because the algorithm is consistent with biological models and hypotheses found in existing biological studies, it could capture the mechanism underlying biological delay learning.
C1 [Matsubara, Takashi] Kobe Univ, Grad Sch Syst Informat, Dept Computat Sci, Computat Intelligence Fundamentals Computat Sci, Kobe, Hyogo, Japan.
RP Matsubara, T (corresponding author), Kobe Univ, Grad Sch Syst Informat, Dept Computat Sci, Computat Intelligence Fundamentals Computat Sci, Kobe, Hyogo, Japan.
EM matsubara@phoenix.kobe-u.ac.jp
CR Abraham WC, 2008, NAT REV NEUROSCI, V9, P387, DOI 10.1038/nrn2356
   [Anonymous], ARXIV160902053
   [Anonymous], 2012, MACHINE LEARNING PRO
   Bakkum DJ, 2008, PLOS ONE, V3, DOI 10.1371/journal.pone.0002088
   Baraban M, 2016, BRAIN RES, V1641, P149, DOI 10.1016/j.brainres.2015.10.026
   Beyeler M, 2013, NEURAL NETWORKS, V48, P109, DOI 10.1016/j.neunet.2013.07.012
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Brader JM, 2007, NEURAL COMPUT, V19, P2881, DOI 10.1162/neco.2007.19.11.2881
   BURNS BD, 1976, PROC R SOC SER B-BIO, V194, P211, DOI 10.1098/rspb.1976.0074
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   CARR CE, 1990, J NEUROSCI, V10, P3227
   Desai NS, 1999, NAT NEUROSCI, V2, P515, DOI 10.1038/9165
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Eurich CW, 2000, NEUROCOMPUTING, V32, P741, DOI 10.1016/S0925-2312(00)00239-3
   Eurich CW, 1999, PHYS REV LETT, V82, P1594, DOI 10.1103/PhysRevLett.82.1594
   Fields RD, 2015, NAT REV NEUROSCI, V16, P756, DOI 10.1038/nrn4023
   Fields RD, 2005, NEUROSCIENTIST, V11, P528, DOI 10.1177/1073858405282304
   Fisher RA, 1936, ANN EUGENIC, V7, P179, DOI 10.1111/j.1469-1809.1936.tb02137.x
   Gerstner W, 1996, NATURE, V383, P76, DOI 10.1038/383076a0
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Gütig R, 2003, J NEUROSCI, V23, P3697
   Hansel D, 1998, NEURAL COMPUT, V10, P467, DOI 10.1162/089976698300017845
   Huning H, 1998, NEURAL COMPUT, V10, P555, DOI 10.1162/089976698300017665
   Izhikevich E M, 2007, DYNAM SYST, DOI [DOI 10.1017/S0143385704000173, DOI 10.7551/MITPRESS/2526.001.0001]
   Izhikevich EM, 2004, CEREB CORTEX, V14, P933, DOI 10.1093/cercor/bhh053
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Jamann N, 2018, NEUROSCIENCE, V368, P268, DOI 10.1016/j.neuroscience.2017.07.035
   Kappel D., 2015, ADV NEURAL INFORM PR, V28, P370
   Kappel D, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003511
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LEVINE MW, 1991, BIOL CYBERN, V65, P459, DOI 10.1007/BF00204659
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   Matsubara T, 2017, IEEE IJCNN, P1831, DOI 10.1109/IJCNN.2017.7966073
   Matsubara T, 2016, FRONT NEURAL CIRCUIT, V10, DOI [10.3389/fncir2016.00042, 10.3389/fncir.2016.00042]
   Matsubara T, 2016, IEEE T NEUR NET LEAR, V27, P836, DOI 10.1109/TNNLS.2015.2425893
   Matsubara T, 2013, IEEE T NEUR NET LEAR, V24, P736, DOI 10.1109/TNNLS.2012.2230643
   MIDDLEBROOKS JC, 1994, SCIENCE, V264, P842, DOI 10.1126/science.8171339
   Neftci E, 2014, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00272
   Nessler B., 2009, ADV NEURAL INFORM PR, P1357
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   Paugam-Moisy H, 2008, NEUROCOMPUTING, V71, P1143, DOI 10.1016/j.neucom.2007.12.027
   Pfister JP, 2006, NEURAL COMPUT, V18, P1318, DOI 10.1162/neco.2006.18.6.1318
   Ponulak F., 2005, REP
   Querlioz D, 2013, IEEE T NANOTECHNOL, V12, P288, DOI 10.1109/TNANO.2013.2250995
   Rezende DJ, 2014, FRONT COMPUT NEUROSC, V8, DOI 10.3389/fncom.2014.00038
   RUSHTON WAH, 1951, J PHYSIOL-LONDON, V115, P101, DOI 10.1113/jphysiol.1951.sp004655
   Sato M., 1999, FAST LEARNING ON LIN
   Seidl AH, 2010, J NEUROSCI, V30, P70, DOI 10.1523/JNEUROSCI.3464-09.2010
   Shouval HZ, 2010, FRONT COMPUT NEUROSC, V4, DOI 10.3389/fncom.2010.00019
   Shouval HZ, 2002, P NATL ACAD SCI USA, V99, P10831, DOI 10.1073/pnas.152343099
   Sporea I, 2013, NEURAL COMPUT, V25, P473, DOI 10.1162/NECO_a_00396
   Taherkhani A, 2015, IEEE T NEUR NET LEAR, V26, P3137, DOI 10.1109/TNNLS.2015.2404938
   Turrigiano GG, 2000, CURR OPIN NEUROBIOL, V10, P358, DOI 10.1016/S0959-4388(00)00091-X
   van Rossum MCW, 2000, J NEUROSCI, V20, P8812
   Van Rullen R, 2001, NEURAL COMPUT, V13, P1255, DOI 10.1162/08997660152002852
   Watt Alanna J, 2010, Front Synaptic Neurosci, V2, P5, DOI 10.3389/fnsyn.2010.00005
   WAXMAN SG, 1976, EXP NEUROL, V53, P115, DOI 10.1016/0014-4886(76)90287-9
   Wittenberg GM, 2006, J NEUROSCI, V26, P6610, DOI 10.1523/JNEUROSCI.5388-05.2006
   Yu Q, 2014, NEUROCOMPUTING, V138, P3, DOI 10.1016/j.neucom.2013.06.052
NR 60
TC 8
Z9 8
U1 1
U2 7
PD NOV 21
PY 2017
VL 11
AR 104
DI 10.3389/fncom.2017.00104
WC Mathematical & Computational Biology; Neurosciences
DA 2023-11-11
ER

PT C
AU Long, LN
AF Long, Lyle N.
BE Stoica, A
   Tunstel, E
   Huntsberger, T
   Arslan, T
   Vijayakumar, S
   ElRayis, AO
TI Scalable biologically inspired neural networks with spike time based
   learning
SO 2008 ECSIS SYMPOSIUM ON LEARNING AND ADAPTIVE BEHAVIORS FOR ROBOTIC
   SYSTEMS - LAB-RS 2008, PROCEEDINGS
DT Proceedings Paper
CT ECSIS Symposium on Learning and Adaptive Behaviors for Robotic Systems
CY AUG 06-08, 2008
CL Univ Edinburgh, Edinburgh, SCOTLAND
HO Univ Edinburgh
AB This paper describes the software and algorithmic issues involved in developing scalable large-scale biologically-inspired spiking neural networks. These neural networks are useful in object recognition and signal processing tasks, but will also be useful in simulations to help understand the human brain. The software is written using object oriented programming and is very general and usable for processing a wide range of sensor data and for data fusion.
C1 Penn State Univ, University Pk, PA 16802 USA.
RP Long, LN (corresponding author), Penn State Univ, 229 Hammond Bldg, University Pk, PA 16802 USA.
CR Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   DARWIN C. R., 1872, ORIGIN SPECIES 1859, V6th
   Dennett DC, 1996, DARWINS DANGEROUS ID
   FRITZ TE, 2004, J AIRCRAFT, V41
   FUKUSHIMA K, 1988, NEURAL NETWORKS, V1, P119, DOI 10.1016/0893-6080(88)90014-7
   GUPTA A, 2007, JOINT C NEUR NETW OR, V53
   HANFORD SD, 2008, 46 AIAA AER SCI M
   Hebb D. O., 1949, ORG BEHAV A NEUROPSY
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   KOCH C, 1999, BIOPHYS COMPUTATION
   Koch C., 2004, QUEST CONSCIOUSNESS
   Long L. N., 2008, CROSSTALK, V21, P6
   LONG LN, 2008, 46 AIAA AER SCI M RE
   LONG LN, 2008, J AEROSPACE COMPUTIN, V5
   LONG LN, 2008, SCI CONSC C TUCS AZ
   OCONNOR PD, 2006, AIAA JOINT PROP C
   Pinker S., 1999, MIND WORKS
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Traub RD, 2005, J NEUROPHYSIOL, V93, P2194, DOI 10.1152/jn.00983.2004
   Vreeken J, 2002, UUCS2003008 I INF CO
   Weldon TP, 1996, OPT ENG, V35, P2852, DOI 10.1117/1.600971
   2007, NEOCORTICAL SIMULATO
   2007, TOP500 COMPUTER LIST
NR 23
TC 0
Z9 0
U1 0
U2 0
PY 2008
BP 29
EP 34
DI 10.1109/LAB-RS.2008.24
WC Computer Science, Cybernetics; Computer Science, Theory & Methods;
   Engineering, Electrical & Electronic; Robotics
DA 2023-11-11
ER

PT J
AU Kholkin, V
   Druzhina, O
   Vatnik, V
   Kulagin, M
   Karimov, T
   Butusov, D
AF Kholkin, Vladislav
   Druzhina, Olga
   Vatnik, Valerii
   Kulagin, Maksim
   Karimov, Timur
   Butusov, Denis
TI Comparing Reservoir Artificial and Spiking Neural Networks in Machine
   Fault Detection Tasks
SO BIG DATA AND COGNITIVE COMPUTING
DT Article
DE artificial neural networks; spiking neural networks; reservoir
   computing; fault diagnosis
ID FIRE MODEL
AB For the last two decades, artificial neural networks (ANNs) of the third generation, also known as spiking neural networks (SNN), have remained a subject of interest for researchers. A significant difficulty for the practical application of SNNs is their poor suitability for von Neumann computer architecture, so many researchers are currently focusing on the development of alternative hardware. Nevertheless, today several experimental libraries implementing SNNs for conventional computers are available. In this paper, using the RCNet library, we compare the performance of reservoir computing architectures based on artificial and spiking neural networks. We explicitly show that, despite the higher execution time, SNNs can demonstrate outstanding classification accuracy in the case of complicated datasets, such as data from industrial sensors used for the fault detection of bearings and gears. For one of the test problems, namely, ball bearing diagnosis using an accelerometer, the accuracy of the classification using reservoir SNN almost reached 100%, while the reservoir ANN was able to achieve recognition accuracy up to only 61%. The results of the study clearly demonstrate the superiority and benefits of SNN classificators.
C1 [Kholkin, Vladislav; Vatnik, Valerii; Butusov, Denis] St Petersburg Electrotech Univ LETI, Dept Comp Aided Design, 5 Professora Popova St, St Petersburg 197022, Russia.
   [Druzhina, Olga; Kulagin, Maksim; Karimov, Timur] St Petersburg Electrotech Univ LETI, Youth Res Inst, 5 Professora Popova St, St Petersburg 197022, Russia.
RP Butusov, D (corresponding author), St Petersburg Electrotech Univ LETI, Dept Comp Aided Design, 5 Professora Popova St, St Petersburg 197022, Russia.; Karimov, T (corresponding author), St Petersburg Electrotech Univ LETI, Youth Res Inst, 5 Professora Popova St, St Petersburg 197022, Russia.
EM tikarimov@etu.ru; dnbutusov@etu.ru
CR Anwani N, 2020, NEUROCOMPUTING, V380, P67, DOI 10.1016/j.neucom.2019.10.104
   Bouvier M, 2019, ACM J EMERG TECH COM, V15, DOI 10.1145/3304103
   Brette R, 2005, J NEUROPHYSIOL, V94, P3637, DOI 10.1152/jn.00686.2005
   Case Western Reserve University Bearing Data Center, 2018, US
   Davidson S, 2021, FRONT NEUROSCI-SWITZ, V15, DOI 10.3389/fnins.2021.651141
   Deng L, 2020, NEURAL NETWORKS, V121, P294, DOI 10.1016/j.neunet.2019.09.005
   Gallicchio C, 2013, NEUROCOMPUTING, V101, P319, DOI 10.1016/j.neucom.2012.08.017
   Gearbox, 2020, FAULT DIAGN STACK DA
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   Gerstner W., 2002, SPIKING NEURON MODEL, DOI DOI 10.1017/CBO9780511815706
   Immovilli F, 2010, IEEE T IND APPL, V46, P1350, DOI 10.1109/TIA.2010.2049623
   Kansizoglou I, 2022, IEEE T AFFECT COMPUT, V13, P756, DOI 10.1109/TAFFC.2019.2961089
   Karlik B., 2011, INT J ARTIFICIAL INT, V1, P111
   Kim Y, 2022, INT CONF ACOUST SPEE, P71, DOI 10.1109/ICASSP43922.2022.9747906
   KLIMAN GB, 1992, ELECTR MACH POW SYST, V20, P463, DOI 10.1080/07313569208909609
   Kulkarni SR, 2018, NEURAL NETWORKS, V103, P118, DOI 10.1016/j.neunet.2018.03.019
   Liu YH, 2001, J COMPUT NEUROSCI, V10, P25, DOI 10.1023/A:1008916026143
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Moore S.C., 2002, THESIS U BATH BATH
   Moran S, 2017, PROCESS PLANT LAYOUT, 2ND EDITION, P1
   Oikonomou KM, 2023, IEEE ROBOT AUTOM LET, V8, P3007, DOI 10.1109/LRA.2023.3264836
   Pilarczyk K, 2018, COORDIN CHEM REV, V365, P23, DOI 10.1016/j.ccr.2018.03.018
   Schoen RR, 1995, IEEE T IND APPL, V31, P1274, DOI 10.1109/28.475697
   Sharma S., 2020, DATA SCI, V4, P7, DOI [10.33564/IJEAST.2020.v04i12.054, DOI 10.33564/IJEAST.2020.V04I12.054]
   Tanaka G, 2019, NEURAL NETWORKS, V115, P100, DOI 10.1016/j.neunet.2019.03.005
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Vuillaume D., 2020, ARXIV
   Wagner T., 2021, P PHM SOC EUR C TURI, VVolume 6, P9
   Wang XW, 2020, NEURAL NETWORKS, V125, P258, DOI 10.1016/j.neunet.2020.02.011
   Yan ZL, 2021, BIOMED SIGNAL PROCES, V63, DOI 10.1016/j.bspc.2020.102170
   Zhang SH, 2021, MECH SYST SIGNAL PR, V146, DOI 10.1016/j.ymssp.2020.106961
   Zuo L, 2022, RELIAB ENG SYST SAFE, V225, DOI 10.1016/j.ress.2022.108561
NR 32
TC 0
Z9 0
U1 3
U2 3
PD JUN
PY 2023
VL 7
IS 2
AR 110
DI 10.3390/bdcc7020110
WC Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Computer Science, Theory & Methods
DA 2023-11-11
ER

PT C
AU Bakhshiev, AV
   Fomin, IS
   Gundelakh, FV
   Demcheva, AA
   Korsakov, AM
AF Bakhshiev, A., V
   Fomin, I. S.
   Gundelakh, F., V
   Demcheva, A. A.
   Korsakov, A. M.
GP IOP
TI The architecture of a software platform for growing spiking neural
   networks simulator developing
SO II INTERNATIONAL SCIENTIFIC CONFERENCE ON APPLIED PHYSICS, INFORMATION
   TECHNOLOGIES AND ENGINEERING 25, PTS 1-5
SE Journal of Physics Conference Series
DT Proceedings Paper
CT 2nd International Scientific Conference on Applied Physics, Information
   Technologies and Engineering (APITECH)
CY SEP 25-OCT 04, 2020
CL Krasnoyarsk, RUSSIA
ID MODEL
AB The paper compares spike neural network simulators. It is shown that the existing simulation software are not adapted to describe growing neural networks with irregular connection topology. This problem is relevant for cases when the neuron model is not point-like but describes a dendritic tree. The architecture of a new software environment for modeling growing networks based on compartment models of neurons is proposed. The comparation of the characteristics of the developed simulator with existing solutions is shown.
C1 [Bakhshiev, A., V; Fomin, I. S.; Gundelakh, F., V; Korsakov, A. M.] Russian State Sci Ctr Robot & Tech Cybernet RTC, St Petersburg, Russia.
   [Bakhshiev, A., V; Gundelakh, F., V; Demcheva, A. A.] Peter Great St Petersburg Polytech Univ SPbPU, St Petersburg, Russia.
RP Bakhshiev, AV (corresponding author), Russian State Sci Ctr Robot & Tech Cybernet RTC, St Petersburg, Russia.; Bakhshiev, AV (corresponding author), Peter Great St Petersburg Polytech Univ SPbPU, St Petersburg, Russia.
EM alexab@rtc.ru
CR [Anonymous], 2015, SUPPLEMENTARY P 4 IN
   Brette R, 2005, J NEUROPHYSIOL, V94, P3637, DOI 10.1152/jn.00686.2005
   Brette R, 2007, J COMPUT NEUROSCI, V23, P349, DOI 10.1007/s10827-007-0038-6
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
NR 4
TC 0
Z9 0
U1 0
U2 0
PY 2020
VL 1679
AR 042001
DI 10.1088/1742-6596/1679/4/042001
WC Automation & Control Systems; Computer Science, Interdisciplinary
   Applications; Engineering, Multidisciplinary; Materials Science,
   Multidisciplinary; Physics, Applied
DA 2023-11-11
ER

PT J
AU Wu, QX
   McGinnity, TM
   Maguire, LP
   Glackin, B
   Belatreche, A
AF Wu, Q. X.
   McGinnity, T. M.
   Maguire, L. P.
   Glackin, B.
   Belatreche, A.
TI Learning under weight constraints in networks of temporal encoding
   spiking neurons
SO NEUROCOMPUTING
DT Article; Proceedings Paper
CT 1st International Conference on Brain Inspired Cognitive Systems (BICS
   2004)
CY AUG 29-SEP 01, 2004
CL Stirling, SCOTLAND
DE spiking neural network; temporal encoding; non-firing neuron; square
   cosine encoder
ID SYNCHRONIZATION; SEGMENTATION; PLASTICITY
AB Limits on synaptic efficiency are characteristic of biological neural networks. In this paper, weight limitation constraints are applied to the spike time error-backpropagation (SpikeProp) algorithm for temporally encoded networks of spiking neurons. A novel solution to the problem raised by non-firing neurons is presented which makes the learning algorithm converge reliably and efficiently. In addition a square cosine encoder is applied to the input neurons to reduce the number of input neurons required. The approach is demonstrated by application to the classical XOR-problem analysis, a function approximation experiment and benchmark data sets. Using input delay neurons and relative timing, the algorithm is also applied to solve a time series prediction problem. The experimental results show that the new approach produces comparable accuracy in classification with the original approach while utilising a smaller spiking neural network. (c) 2006 Elsevier B.V. All rights reserved.
C1 Univ Ulster, Sch Comp & Intelligent Syst, Derry BT48 7JL, North Ireland.
RP McGinnity, TM (corresponding author), Univ Ulster, Sch Comp & Intelligent Syst, Derry BT48 7JL, North Ireland.
EM tm.mcginnity@ulster.ac.uk
CR [Anonymous], 2003, PERSP NEURAL COMP
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Choe Y, 1998, NEUROCOMPUTING, V21, P139, DOI 10.1016/S0925-2312(98)00040-X
   Delorme A, 2001, NEUROCOMPUTING, V38, P539, DOI 10.1016/S0925-2312(01)00403-9
   Diesmann M, 1999, NATURE, V402, P529, DOI 10.1038/990101
   Gerstner W, 1998, PULSED NEURAL NETWORKS, P3
   Gerstner W, 1996, NATURE, V383, P76, DOI 10.1038/383076a0
   Knoblauch A, 2002, BIOL CYBERN, V87, P151, DOI 10.1007/s00422-002-0331-4
   KOSKELA T, 1998, P 6 EUR S ART NEUR N, P167
   Lysetskiy M, 2002, NEURAL PROCESS LETT, V15, P225, DOI 10.1023/A:1015773115997
   Riehle A, 1997, SCIENCE, V278, P1950, DOI 10.1126/science.278.5345.1950
   Roberts PD, 2002, BIOL CYBERN, V87, P392, DOI 10.1007/s00422-002-0361-y
   SOHN JW, 1999, P INT JOINT C NEUR N, V4, P2590
   Song S, 2001, NEURON, V32, P339, DOI 10.1016/S0896-6273(01)00451-2
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   van Rossum MCW, 2000, J NEUROSCI, V20, P8812
   VanRullen R, 2002, VISION RES, V42, P2593, DOI 10.1016/S0042-6989(02)00298-5
NR 18
TC 39
Z9 40
U1 0
U2 8
PD OCT
PY 2006
VL 69
IS 16-18
SI SI
BP 1912
EP 1922
DI 10.1016/j.neucom.2005.11.023
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT J
AU Nichols, E
   McDaid, LJ
   Siddique, N
AF Nichols, Eric
   McDaid, Liam J.
   Siddique, Nazmul
TI Biologically Inspired SNN for Robot Control
SO IEEE TRANSACTIONS ON CYBERNETICS
DT Article
DE Dynamic synapses; self-organization; spiking neural network (SNN);
   temporal difference (TD) learning rule
ID SPIKING NEURAL-NETWORK; DOPAMINE; MODEL; PLASTICITY; DYNAMICS
AB This paper proposes a spiking-neural-network-based robot controller inspired by the control structures of biological systems. Information is routed through the network using facilitating dynamic synapses with short-term plasticity. Learning occurs through long-term synaptic plasticity which is implemented using the temporal difference learning rule to enable the robot to learn to associate the correct movement with the appropriate input conditions. The network self-organizes to provide memories of environments that the robot encounters. A Pioneer robot simulator with laser and sonar proximity sensors is used to verify the performance of the network with a wall-following task, and the results are presented.
C1 [Nichols, Eric; McDaid, Liam J.; Siddique, Nazmul] Univ Ulster, Intelligent Syst Res Ctr, Derry BT48 J7L, North Ireland.
RP Nichols, E (corresponding author), Univ Ulster, Intelligent Syst Res Ctr, Magee Campus, Derry BT48 J7L, North Ireland.
EM ericjnichols@gmail.com; lj.mcdaid@ulster.ac.uk; nh.siddique@ulster.ac.uk
CR Abbott LF, 2004, NATURE, V431, P796, DOI 10.1038/nature03010
   Alnajjar F., 2005, P INT C COMPUTATIONA, V1, P1134, DOI 10.1109/CIMCA.2005.1631415
   Arena P., 2010, PROC INT JOINT C NEU, P1, DOI [10.1109/IJCNN.2010.5596513ieeexplore.ieee.org, DOI 10.1109/IJCNN.2010.5596542, 10.1109/IJCNN.2010.5596542]
   Arias-Carrión O, 2007, ACTA NEUROBIOL EXP, V67, P481
   Beck F., 1998, COGNITIVE STUD, V5, P295
   Bernroider G, 2005, P SOC PHOTO-OPT INS, V5841, P205, DOI 10.1117/12.609227
   Bressloff PC, 2000, NEURAL COMPUT, V12, P91, DOI 10.1162/089976600300015907
   Di Paolo EA, 2002, ADAPT BEHAV, V10, P243, DOI 10.1177/1059712302010003006
   Dimitrijevic MR, 1998, ANN NY ACAD SCI, V860, P360, DOI 10.1111/j.1749-6632.1998.tb09062.x
   Egelman DM, 1998, J COGNITIVE NEUROSCI, V10, P623, DOI 10.1162/089892998563022
   Girault JA, 2004, ARCH NEUROL-CHICAGO, V61, P641, DOI 10.1001/archneur.61.5.641
   GREENFIELD S, 1998, HUMAN BRAIN GUIDED T
   Hagras H, 2004, IEEE INT CONF ROBOT, P4620, DOI 10.1109/ROBOT.2004.1302446
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Hooper SL, 2000, CURR BIOL, V10, pR176, DOI 10.1016/S0960-9822(00)00367-5
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   Hopfield JJ, 2010, P NATL ACAD SCI USA, V107, P1648, DOI 10.1073/pnas.0913991107
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Kovacs G. L., 2004, J INT FED CLIN CHEM, V15, P1
   Kubota N, 2004, IEEE SYS MAN CYBERN, P5783
   Kubota N, 2009, 2009 IEEE WORKSHOP ON ROBOTIC INTELLIGENCE IN INFORMATIONALLY STRUCTURED SPACE, P107, DOI 10.1109/RIISS.2009.4937914
   Makarov VA, 2005, J NEUROSCI METH, V144, P265, DOI 10.1016/j.jneumeth.2004.11.013
   Markram H, 1998, P NATL ACAD SCI USA, V95, P5323, DOI 10.1073/pnas.95.9.5323
   Markram H, 2006, NAT REV NEUROSCI, V7, P153, DOI 10.1038/nrn1848
   Mejías JF, 2007, NEUROCOMPUTING, V70, P2026, DOI 10.1016/j.neucom.2006.10.097
   Mokhtar M, 2007, IEEE IJCNN, P813, DOI 10.1109/IJCNN.2007.4371062
   Montague PR, 2006, ANNU REV NEUROSCI, V29, P417, DOI 10.1146/annurev.neuro.29.051605.112903
   NAGUMO J, 1962, P IRE, V50, P2061, DOI 10.1109/JRPROC.1962.288235
   Nichols E, 2010, INT J NEURAL SYST, V20, P501, DOI 10.1142/S0129065710002577
   Nicoll RA, 2005, NAT REV NEUROSCI, V6, P863, DOI 10.1038/nrn1786
   Oberauer K, 2002, J EXP PSYCHOL LEARN, V28, P411, DOI 10.1037//0278-7393.28.3.411
   Potjans W, 2009, NEURAL COMPUT, V21, P301, DOI 10.1162/neco.2008.08-07-593
   Schultz W, 2002, NEURON, V36, P241, DOI 10.1016/S0896-6273(02)00967-4
   Shigang Cui, 2010, 2010 International Conference on Computer Application and System Modeling (ICCASM 2010), P233, DOI 10.1109/ICCASM.2010.5620069
   Summhammer J., 2007, QUANTUM ENTANGLEMENT
   Suri RE, 2002, NEURAL NETWORKS, V15, P523, DOI 10.1016/S0893-6080(02)00046-1
   Suri RE, 2001, NEURAL COMPUT, V13, P841, DOI 10.1162/089976601300014376
   Sutton R. S., 2015, REINFORCEMENT LEARNI, V2nd
   Tonnelier A, 2007, NEURAL COMPUT, V19, P3226, DOI 10.1162/neco.2007.19.12.3226
   Tsodyks M, 1998, NEURAL COMPUT, V10, P821, DOI 10.1162/089976698300017502
   Wade JJ, 2010, IEEE T NEURAL NETWOR, V21, P1817, DOI 10.1109/TNN.2010.2074212
   Wang XZ, 2009, STUD FUZZ SOFT COMP, V245, P1
NR 42
TC 34
Z9 34
U1 1
U2 20
PD FEB
PY 2013
VL 43
IS 1
BP 115
EP 128
DI 10.1109/TSMCB.2012.2200674
WC Automation & Control Systems; Computer Science, Artificial Intelligence;
   Computer Science, Cybernetics
DA 2023-11-11
ER

PT C
AU Hosaka, R
   Ikeguchi, T
   Nakamura, H
   Araki, O
AF Hosaka, R
   Ikeguchi, T
   Nakamura, H
   Araki, O
GP ieee
TI Information transformation from a spatiotemporal pattern to synchrony
   through STDP network
SO 2004 IEEE INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS, VOLS 1-4,
   PROCEEDINGS
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT IEEE International Joint Conference on Neural Networks (IJCNN)
CY JUL 25-29, 2004
CL Budapest, HUNGARY
ID SPIKING
AB Recent experimental results show the sensitivity of synaptic plasticity to the timing of synaptic events and postsynaptic firings (spike timing dependent plasticity: STDP). Although a number of studies have been made on STDP, little is known about the effect of STDP on the relation between external input patterns to the recurrent neural network and its output spikes. In this study, we examine this relation by computer simulations of a spiking neural network with STDR We have found that STDP organizes the neural network to transform an external spatiotemporal input pattern into a synchronous firing, and the synchrony is much dependent on the spatiotemporal structure of the external input. This result suggests that the original of the synchrony propagate through feed-forward network may be generated by the recurrent neural network respond to the external input.
C1 Saitama Univ, Grad Sch Sci & Engn, Sakura Ku, Saitama 3388570, Japan.
RP Hosaka, R (corresponding author), Saitama Univ, Grad Sch Sci & Engn, Sakura Ku, 255 Shimo Ohkubo, Saitama 3388570, Japan.
EM hosaka@nls.ics.saitama-u.ac.jp
CR Abbott LF, 2000, NAT NEUROSCI, V3, P1178, DOI 10.1038/81453
   [Anonymous], 1991, ANATOMY CORTEX
   Araki O, 2001, NEURAL COMPUT, V13, P2799, DOI 10.1162/089976601317098538
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Câteau H, 2003, NEURAL COMPUT, V15, P597, DOI 10.1162/089976603321192095
   Diesmann M, 1999, NATURE, V402, P529, DOI 10.1038/990101
   GERSTNER W, 1993, BIOL CYBERN, V69, P503, DOI 10.1007/BF01185422
   Gerstner W, 1996, NATURE, V383, P76, DOI 10.1038/383076a0
   Kitano K, 2002, NEUROREPORT, V13, P795, DOI 10.1097/00001756-200205070-00012
   Levy N, 2001, NEURAL NETWORKS, V14, P815, DOI 10.1016/S0893-6080(01)00044-2
   LILEY DTJ, 1994, NETWORK-COMP NEURAL, V5, P175, DOI 10.1088/0954-898X/5/2/004
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   Masuda N, 2003, PHYS LETT A, V311, P485, DOI 10.1016/S0375-9601(03)00498-5
   Masuda N, 2003, NEURAL COMPUT, V15, P103, DOI 10.1162/089976603321043711
   NAGUMO J, 1972, KYBERNETIK, V10, P155, DOI 10.1007/BF00290514
   Nishiyama M, 2000, NATURE, V408, P584, DOI 10.1038/35046067
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   van Rossum MCW, 2002, J NEUROSCI, V22, P1956, DOI 10.1523/JNEUROSCI.22-05-01956.2002
   Zhang LI, 1998, NATURE, V395, P37, DOI 10.1038/25665
   Zhigulin VP, 2003, PHYS REV E, V67, DOI 10.1103/PhysRevE.67.021901
NR 20
TC 3
Z9 3
U1 0
U2 0
PY 2004
BP 1475
EP 1480
WC Computer Science, Artificial Intelligence; Computer Science, Cybernetics
DA 2023-11-11
ER

PT J
AU Sinapayen, L
   Masumori, A
   Ikegami, T
AF Sinapayen, Lana
   Masumori, Atsushi
   Ikegami, Takashi
TI Reactive, Proactive, and Inductive Agents: An Evolutionary Path for
   Biological and Artificial Spiking Networks
SO FRONTIERS IN COMPUTATIONAL NEUROSCIENCE
DT Article
DE neural network; spiking neural network; predictive coding; LSA; STDP
ID TIMING-DEPENDENT PLASTICITY; MODEL
AB Complex environments provide structured yet variable sensory inputs. To best exploit information from these environments, organisms must evolve the ability to anticipate consequences of new stimuli, and act on these predictions. We propose an evolutionary path for neural networks, leading an organism from reactive behavior to simple proactive behavior and from simple proactive behavior to induction-based behavior. Based on earlier in-vitro and in-silico experiments, we define the conditions necessary in a network with spike-timing dependent plasticity for the organism to go from reactive to proactive behavior. Our results support the existence of specific evolutionary steps and four conditions necessary for embodied neural networks to evolve predictive and inductive abilities from an initial reactive strategy.
C1 [Sinapayen, Lana] Sony Comp Sci Labs Inc, Tokyo, Japan.
   [Sinapayen, Lana] Tokyo Inst Technol, Earth Life Sci Inst, Tokyo, Japan.
   [Masumori, Atsushi; Ikegami, Takashi] Univ Tokyo, Dept Gen Syst Studies, Grad Sch Arts & Sci, Tokyo, Japan.
RP Masumori, A (corresponding author), Univ Tokyo, Dept Gen Syst Studies, Grad Sch Arts & Sci, Tokyo, Japan.
EM masumori@sacral.c.u-tokyo.ac.jp
CR Brunel N, 2000, J COMPUT NEUROSCI, V8, P183, DOI 10.1023/A:1008925309027
   Buonomano DV, 2000, J NEUROSCI, V20, P1129, DOI 10.1523/JNEUROSCI.20-03-01129.2000
   Caporale N, 2008, ANNU REV NEUROSCI, V31, P25, DOI 10.1146/annurev.neuro.31.060407.125639
   Dennett Daniel C., 1995, DARWINS DANGEROUS ID
   Drew PJ, 2006, P NATL ACAD SCI USA, V103, P8876, DOI 10.1073/pnas.0600676103
   Edwards G, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-16093-y
   Földiák P, 1991, NEURAL COMPUT, V3, P194, DOI 10.1162/neco.1991.3.2.194
   Ji Ryang Chung, 2009, Proceedings 2009 International Joint Conference on Neural Networks (IJCNN 2009 - Atlanta), P571, DOI 10.1109/IJCNN.2009.5179065
   Klyubin AS, 2005, IEEE C EVOL COMPUTAT, P128
   Kwon J, 2008, INT C DEVEL LEARN, P109, DOI 10.1109/DEVLRN.2008.4640814
   Masumori A., 2019, THESIS
   Masumori A, 2017, FOURTEENTH EUROPEAN CONFERENCE ON ARTIFICIAL LIFE (ECAL 2017), P275
   Masumori A, 2018, 2018 CONFERENCE ON ARTIFICIAL LIFE (ALIFE 2018), P163
   Powers Albert R III, 2016, Biol Psychiatry Cogn Neurosci Neuroimaging, V1, P393, DOI 10.1016/j.bpsc.2016.04.003
   Raman R, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0151194
   Rao RPN, 1999, NAT NEUROSCI, V2, P79, DOI 10.1038/4580
   Rao RPN, 2001, NEURAL COMPUT, V13, P2221, DOI 10.1162/089976601750541787
   Sethi AK, 2012, FRONT PSYCHOL, V3, DOI 10.3389/fpsyg.2011.00395
   Sinapayen L, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0170388
   Suzuki K, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-16316-2
   Wacongne C, 2012, J NEUROSCI, V32, P3665, DOI 10.1523/JNEUROSCI.5003-11.2012
   Wagenaar DA, 2005, J NEUROSCI, V25, P680, DOI 10.1523/JNEUROSCI.4209-04.2005
   Watanabe E, 2018, FRONT PSYCHOL, V9, DOI 10.3389/fpsyg.2018.00345
NR 23
TC 2
Z9 2
U1 1
U2 1
PD JAN 22
PY 2020
VL 13
AR 88
DI 10.3389/fncom.2019.00088
WC Mathematical & Computational Biology; Neurosciences
DA 2023-11-11
ER

PT J
AU Lightheart, T
   Grainger, S
   Lu, TF
AF Lightheart, Toby
   Grainger, Steven
   Lu, Tien-Fu
TI Spike-Timing-Dependent Construction
SO NEURAL COMPUTATION
DT Article
ID VISUAL-PATTERN RECOGNITION; SYNAPTIC PLASTICITY; NEURAL-NETWORKS;
   COMPUTATIONAL POWER; ADULT NEUROGENESIS; LEARNING ALGORITHM; NEURONS;
   MODEL; REINFORCEMENT; ROBOTS
AB Spike-timing-dependent construction (STDC) is the production of new spiking neurons and connections in a simulated neural network in response to neuron activity. Following the discovery of spike-timing-dependent plasticity (STDP), significant effort has gone into the modeling and simulation of adaptation in spiking neural networks (SNNs). Limitations in computational power imposed by network topology, however, constrain learning capabilities through connection weight modification alone. Constructive algorithms produce new neurons and connections, allowing automatic structural responses for applications of unknown complexity and nonstationary solutions. A conceptual analogy is developed and extended to theoretical conditions for modeling synaptic plasticity as network construction. Generalizing past constructive algorithms, we propose a framework for the design of novel constructive SNNs and demonstrate its application in the development of simulations for the validation of developed theory. Potential directions of future research and applications of STDC for biological modeling and machine learning are also discussed.
C1 [Lightheart, Toby; Grainger, Steven; Lu, Tien-Fu] Univ Adelaide, Sch Mech Engn, Adelaide, SA 5005, Australia.
RP Lightheart, T (corresponding author), Univ Adelaide, Sch Mech Engn, Adelaide, SA 5005, Australia.
EM toby.lightheart@adelaide.edu.au; steven.grainger@adelaide.edu.au;
   tien-fu.lu@adelaide.edu.au
CR Abbott LF, 2000, NAT NEUROSCI, V3, P1178, DOI 10.1038/81453
   Abrous DN, 2005, PHYSIOL REV, V85, P523, DOI 10.1152/physrev.00055.2003
   AHA DW, 1991, MACH LEARN, V6, P37, DOI 10.1023/A:1022689900470
   Aimone JB, 2009, NEURON, V61, P187, DOI 10.1016/j.neuron.2008.11.026
   [Anonymous], [No title captured]
   [Anonymous], FRONT SYNAPTIC NEURO
   Arena P, 2009, IEEE T NEURAL NETWOR, V20, P202, DOI 10.1109/TNN.2008.2005134
   Becker S, 2005, HIPPOCAMPUS, V15, P722, DOI 10.1002/hipo.20095
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   BRUSKE J, 1995, NEURAL COMPUT, V7, P845, DOI 10.1162/neco.1995.7.4.845
   Caporale N, 2008, ANNU REV NEUROSCI, V31, P25, DOI 10.1146/annurev.neuro.31.060407.125639
   Carrillo RR, 2008, BIOSYSTEMS, V94, P18, DOI 10.1016/j.biosystems.2008.05.008
   Cirean D., 2012, IDSIA0412
   Delorme A, 2001, NEURAL NETWORKS, V14, P795, DOI 10.1016/S0893-6080(01)00049-1
   Di Paolo EA, 2003, PHILOS T R SOC A, V361, P2299, DOI 10.1098/rsta.2003.1256
   Floreano D, 2006, INT J INTELL SYST, V21, P1005, DOI 10.1002/int.20173
   Florian RV, 2007, NEURAL COMPUT, V19, P1468, DOI 10.1162/neco.2007.19.6.1468
   Fukushima K, 2011, NEURAL NETWORKS, V24, P767, DOI 10.1016/j.neunet.2011.03.017
   Gerstner W., 2002, SPIKING NEURON MODEL
   Gilson M, 2012, PLOS COMPUT BIOL, V8, DOI 10.1371/journal.pcbi.1002584
   Guyonneau R, 2005, NEURAL COMPUT, V17, P859, DOI 10.1162/0899766053429390
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Holtmaat A, 2009, NAT REV NEUROSCI, V10, P647, DOI 10.1038/nrn2699
   Huang GB, 1998, IEEE T NEURAL NETWOR, V9, P224, DOI 10.1109/72.655045
   Huemer A, 2009, STUD COMPUT INTELL, V258, P225
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Kasinski A., 2006, International Journal of Applied Mathematics and Computer Science, V16, P101
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Legenstein R, 2005, NEURAL COMPUT, V17, P2337, DOI 10.1162/0899766054796888
   Lightheart T., 2010, P 2010 AUSTR C ROB A
   Liu J, 2005, INT C DEVEL LEARN, P121
   Maass W, 1996, NEURAL COMPUT, V8, P1, DOI 10.1162/neco.1996.8.1.1
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maass W, 1997, ADV NEUR IN, V9, P211
   Masquelier T, 2008, PLOS ONE, V3, DOI 10.1371/journal.pone.0001377
   Masquelier T, 2007, PLOS COMPUT BIOL, V3, P247, DOI 10.1371/journal.pcbi.0030031
   Masquelier T, 2009, NEURAL COMPUT, V21, P1259, DOI 10.1162/neco.2008.06-08-804
   Morrison A, 2008, BIOL CYBERN, V98, P459, DOI 10.1007/s00422-008-0233-1
   Nessler B., 2009, ADV NEURAL INFORM PR, P1357
   Nicoletti MD, 2009, STUD COMPUT INTELL, V258, P1
   Pfister JP, 2006, J NEUROSCI, V26, P9673, DOI 10.1523/JNEUROSCI.1425-06.2006
   Salakhutdinov R, 2012, NEURAL COMPUT, V24, P1967, DOI 10.1162/NECO_a_00311
   Savin C, 2010, PLOS COMPUT BIOL, V6, DOI 10.1371/journal.pcbi.1000757
   Serre T, 2007, IEEE T PATTERN ANAL, V29, P411, DOI 10.1109/TPAMI.2007.56
   Seung HS, 2003, NEURON, V40, P1063, DOI 10.1016/S0896-6273(03)00761-X
   Soltic S, 2008, IEEE IJCNN, P2091, DOI 10.1109/IJCNN.2008.4634085
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Sutton R. S., 2015, REINFORCEMENT LEARNI, V2nd
   Takita K., 2005, Systems and Computers in Japan, V36, P42, DOI 10.1002/scj.10645
   Takita K, 2002, IEEE IJCNN, P1643, DOI 10.1109/IJCNN.2002.1007764
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   van Rossum MCW, 2000, J NEUROSCI, V20, P8812
   Watts MJ, 2009, IEEE T SYST MAN CY C, V39, P253, DOI 10.1109/TSMCC.2008.2012254
   Wysoski SG, 2006, LECT NOTES COMPUT SC, V4131, P61
   Wysoski SG, 2010, NEURAL NETWORKS, V23, P819, DOI 10.1016/j.neunet.2010.04.009
NR 58
TC 2
Z9 2
U1 0
U2 19
PD OCT
PY 2013
VL 25
IS 10
BP 2611
EP 2645
DI 10.1162/NECO_a_00501
WC Computer Science, Artificial Intelligence; Neurosciences
DA 2023-11-11
ER

PT J
AU Lu, D
   Li, WC
   Liu, J
   Chen, G
   Li, ZG
AF Lu, Di
   Li, Wenchang
   Liu, Jian
   Chen, Gang
   Li, Zhigang
TI Design of a Configurable Spike-Encoding Circuit Based on Focal Plane
   Array
SO APPLIED SCIENCES-BASEL
DT Article
DE spiking neural networks; spike encoding; focal plane array; artificial
   vision system
ID STIMULUS; SPEED; CODE
AB Featured Application A configurable readout circuit for the focal plane array is designed that transmits information in the form of spike-encoding and supports different encoding methods. Spiking neural networks with different structures can be matched according to the task.Abstract Spiking neural networks inspired by biological models are gaining popularity in artificial intelligence due to their ability to solve diverse problems while reducing energy consumption. As a result of the trade-off between the need to transmit large amounts of data and the power consumption of hardware deployment, artificial vision systems are particularly well-suited to construction using spiking neural networks (SNNs). How to communicate with the neuromorphic network effectively is one of the challenges associated with building systems that utilize SNN systems. It is necessary to convert the data to spike form before they can be processed by an SNN as input, unless neuromorphic or event-triggered sensing systems are employed. We present a configurable circuit based on a focal plane array (FPA) capable of providing spike-encoded readout data at the pixel level. With this type of circuit, the current signal of the photoelectric sensor can be encoded into two spike encodings with different precision, which are sent for processing to SNNs. This provides image information at two different scales for the artificial vision system based on SNNs. With this feature, we can use this circuit and different SNN structures to build an artificial target recognition system that is closer to the biological visual system.
C1 [Lu, Di; Li, Wenchang] Chinese Acad Sci, Lab Solid State Optoelect Informat Technol, Inst Semicond, Beijing 100083, Peoples R China.
   [Lu, Di; Li, Wenchang] Univ Chinese Acad Sci, Sch Integrated Circuits, Beijing 100049, Peoples R China.
   [Liu, Jian] Chinese Acad Sci, Inst Semicond, State Key Lab Superlatt & Microstruct, Beijing 100083, Peoples R China.
   [Liu, Jian] Univ Chinese Acad Sci, Ctr Mat Sci & Optoelect Engn, Beijing 100049, Peoples R China.
   [Chen, Gang; Li, Zhigang] Chinese Acad Sci, Key Lab Semicond Neural Network Intelligent Percep, Beijing 100083, Peoples R China.
RP Li, WC (corresponding author), Chinese Acad Sci, Lab Solid State Optoelect Informat Technol, Inst Semicond, Beijing 100083, Peoples R China.; Li, WC (corresponding author), Univ Chinese Acad Sci, Sch Integrated Circuits, Beijing 100049, Peoples R China.
EM ludi@semi.ac.cn; liwc@semi.ac.cn; liujian@semi.ac.cn;
   chengang08@semi.ac.cn; lizhg@semi.ac.cn
CR Abraham NM, 2004, NEURON, V44, P865, DOI 10.1016/S0896-6273(04)00753-6
   Adrian ED, 1926, J PHYSIOL-LONDON, V61, P465, DOI 10.1113/jphysiol.1926.sp002308
   Ahmad S, 2019, Arxiv, DOI [arXiv:1903.11257, DOI 10.48550/ARXIV.1903.11257]
   BAZES M, 1991, IEEE J SOLID-ST CIRC, V26, P165, DOI 10.1109/4.68134
   Blouw P., 2019, P 7 ANN NEURO INSPIR
   Eliasmith C, 2012, SCIENCE, V338, P1202, DOI 10.1126/science.1225266
   Galambos R, 1943, J NEUROPHYSIOL, V6, P39, DOI 10.1152/jn.1943.6.1.39
   Gautrais J, 1998, BIOSYSTEMS, V48, P57, DOI 10.1016/S0303-2647(98)00050-1
   Gawne TJ, 1996, J NEUROPHYSIOL, V76, P1356, DOI 10.1152/jn.1996.76.2.1356
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   Gerstner W., 2002, SPIKING NEURON MODEL, DOI DOI 10.1017/CBO9780511815706
   Gollisch T, 2008, SCIENCE, V319, P1108, DOI 10.1126/science.1149639
   GRAY CM, 1989, P NATL ACAD SCI USA, V86, P1698, DOI 10.1073/pnas.86.5.1698
   GRAY CM, 1989, NATURE, V338, P334, DOI 10.1038/338334a0
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   Johansson RS, 2004, NAT NEUROSCI, V7, P170, DOI 10.1038/nn1177
   Kayser C, 2009, NEURON, V61, P597, DOI 10.1016/j.neuron.2009.01.008
   Li M, 2017, FRONT CELL NEUROSCI, V11, DOI 10.3389/fncel.2017.00236
   Li Y, 2022, Arxiv, DOI arXiv:2207.02702
   Margrie TW, 2003, J PHYSIOL-LONDON, V546, P363, DOI 10.1113/jphysiol.2002.031245
   Montemurro MA, 2008, CURR BIOL, V18, P375, DOI 10.1016/j.cub.2008.02.023
   Olshausen BA, 2004, CURR OPIN NEUROBIOL, V14, P481, DOI 10.1016/j.conb.2004.07.007
   Oswald AMM, 2007, J NEUROPHYSIOL, V97, P2731, DOI 10.1152/jn.00987.2006
   Portelli G, 2016, ENEURO, V3, DOI 10.1523/ENEURO.0134-15.2016
   Shi C, 2023, IEEE T CIRCUITS-II, V70, P2665, DOI 10.1109/TCSII.2023.3239039
   Steinmetz PN, 2000, NATURE, V404, P187, DOI 10.1038/35004588
   Thorpe S, 1996, NATURE, V381, P520, DOI 10.1038/381520a0
   Thorpe S, 1998, COMPUTATIONAL NEUROSCIENCE: TRENDS IN RESEARCH, P113
   THORPE SJ, 1990, PARALLEL PROCESSING IN NEURAL SYSTEMS AND COMPUTERS, P91
   Wang TX, 2021, NEUROCOMPUTING, V425, P96, DOI 10.1016/j.neucom.2020.10.100
NR 30
TC 0
Z9 0
U1 1
U2 1
PD SEP
PY 2023
VL 13
IS 18
AR 10092
DI 10.3390/app131810092
WC Chemistry, Multidisciplinary; Engineering, Multidisciplinary; Materials
   Science, Multidisciplinary; Physics, Applied
DA 2023-11-11
ER

PT J
AU Suresh, B
   Boppidi, PKR
   Rao, BVVSNP
   Banerjee, S
   Kundu, S
AF Suresh, Bharathwaj
   Boppidi, Pavan Kumar Reddy
   Rao, B. V. V. S. N. Prabhakar
   Banerjee, Souri
   Kundu, Souvik
TI Realizing spike-timing dependent plasticity learning rule in
   Pt/Cu:ZnO/Nb:STO memristors for implementing single spike based
   denoising autoencoder
SO JOURNAL OF MICROMECHANICS AND MICROENGINEERING
DT Article
DE denoising autoencoder; Cu:ZnO; memristor; spiking neural network; STDP
   learning rule
ID NEURAL-NETWORKS; ALGORITHMS; SYNAPSES; HARDWARE; NEURONS
AB In this work, the Cu:ZnO based memristors were fabricated and modelled and its biological synaptic characteristics were realized. Phenomenon similar to long-term potentiation and long-term depression were observed in the proposed devices and spike timing dependent plasticity learning rule was established by engineering appropriate input voltage spikes, making the devices suitable for use in artificial neural networks. In order to demonstrate learning mechanism, a denoising autoencoder network was developed by incorporating the synaptic characteristics of the device along with the concept of rank coding. To evaluate the feasibility and performance of the network, images from the MNIST database for handwritten digits were employed. The training of the proposed network was accomplished by incorporating noisy images, and it was validated with images corrupted with Gaussian, Salt & Pepper and Speckle noises. Surprisingly, the obtained results demonstrated that the shape of the digits was recovered to a great extent and almost all noise in the background were removed. The accuracy of the denoising was found to be more than 90% for most cases. The proposed network shows the efficacy of ferroelectric Cu:ZnO memristors as artificial synapses in spiking neural networks, which opens up a new path towards developing future generation biologically compatible neuromorphic systems.
C1 [Suresh, Bharathwaj; Boppidi, Pavan Kumar Reddy; Rao, B. V. V. S. N. Prabhakar; Kundu, Souvik] Birla Inst Technol & Sci BITS Pilani, Dept Elect & Elect Engn, Hyderabad Campus, Hyderabad 500078, Telangana, India.
   [Banerjee, Souri] Birla Inst Technol & Sci BITS Pilani, Dept Phys, Hyderabad Campus, Hyderabad 500078, Telangana, India.
RP Kundu, S (corresponding author), Birla Inst Technol & Sci BITS Pilani, Dept Elect & Elect Engn, Hyderabad Campus, Hyderabad 500078, Telangana, India.
EM souvikelt@gmail.com
CR Abbott LF, 2004, NATURE, V431, P796, DOI 10.1038/nature03010
   [Anonymous], 2015, SPLST
   [Anonymous], 2012, NEURIPS 2012
   [Anonymous], 2016, P ADV NEUR INF PROC
   Babu AV, 2018, NEUROCOMPUTING, V321, P227, DOI 10.1016/j.neucom.2018.09.019
   Berdan R, 2016, SCI REP-UK, V6, DOI 10.1038/srep18639
   Bi GQ, 2001, ANNU REV NEUROSCI, V24, P139, DOI 10.1146/annurev.neuro.24.1.139
   Boppidi PKR, 2018, J APPL PHYS, V124, DOI 10.1063/1.5052619
   Boyat A, 2015, SIGNAL IMAGE PROCESS, P663
   Buades A, 2005, MULTISCALE MODEL SIM, V4, P490, DOI 10.1137/040616024
   Burbank KS, 2015, PLOS COMPUT BIOL, V11, DOI 10.1371/journal.pcbi.1004566
   Calimera A, 2013, FUNCT NEUROL, V28, P191, DOI 10.11138/FNeur/2013.28.3.191
   Cassidy AS, 2013, NEURAL NETWORKS, V45, P4, DOI 10.1016/j.neunet.2013.05.011
   Cruz-Albrecht JM, 2012, IEEE T BIOMED CIRC S, V6, P246, DOI 10.1109/TBCAS.2011.2174152
   Gayathri R, 2013, INT J ADV RES ELECT, V2, P1641
   Gondara L, 2016, INT CONF DAT MIN WOR, P241, DOI [10.1109/ICDMW.2016.102, 10.1109/ICDMW.2016.0041]
   Gonzalez R., 2008, DIGITAL IMAGE PROCES, V3rd
   Guan PY, 2017, MATER SCI TECH-LOND, V33, P2010, DOI 10.1080/02670836.2017.1366712
   Haron Nor Zaidi, 2008, Proceedings 3rd International Design and Test Workshop (IDT 2008), P98, DOI 10.1109/IDT.2008.4802475
   Hasan R, 2016, P IEEE NAT AER EL C, P327
   Hasan R, 2017, IEEE IJCNN, P3527, DOI 10.1109/IJCNN.2017.7966300
   Indiveri G, 2006, IEEE T NEURAL NETWOR, V17, P211, DOI 10.1109/TNN.2005.860850
   James CD, 2017, BIOL INSPIR COGN ARC, V19, P49, DOI 10.1016/j.bica.2016.11.002
   Jerry M, 2018, J PHYS D APPL PHYS, V51, DOI 10.1088/1361-6463/aad6f8
   KAU LS, 1989, J AM CHEM SOC, V111, P7103, DOI 10.1021/ja00200a032
   Kim MK, 2019, NANO LETT, V19, P2044, DOI 10.1021/acs.nanolett.9b00180
   Kim S, 2011, 2011 11TH INTERNATIONAL CONFERENCE ON CONTROL, AUTOMATION AND SYSTEMS (ICCAS), P1
   Kim S, 2018, NANOTECHNOLOGY, V29, DOI 10.1088/1361-6528/aad64c
   Kim S, 2017, ACS APPL MATER INTER, V9, P40420, DOI 10.1021/acsami.7b11191
   Kundu S, 2015, SCI REP-UK, V5, DOI 10.1038/srep12415
   Kvatinsky S, 2015, IEEE T CIRCUITS-II, V62, P786, DOI 10.1109/TCSII.2015.2433536
   La Barbera S, 2017, COGN SYST MONOGR, V31, P17, DOI 10.1007/978-81-322-3703-7_2
   LeCun Y., MNIST DATASET HANDWR
   Liu HY, 2015, PHYS CHEM CHEM PHYS, V17, P9098, DOI 10.1039/c5cp00086f
   Makhzani A., 2014, P ADV NEUR INF PROC, P2791
   Meftah B., 2013, ARTIF INTELL, P525
   Misra J, 2010, NEUROCOMPUTING, V74, P239, DOI 10.1016/j.neucom.2010.03.021
   Motwani M.C., 2004, P GSPX, V27, P27, DOI DOI 10.5120/9288-3488
   Muller J., 2013, TECHNICAL DIGEST INT
   Saunders D. J., 2018, 2018 INT JOINT C NEU, P1
   Sengupta A, 2015, APPL PHYS LETT, V106, DOI 10.1063/1.4914111
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   Vincent P., 2008, P 25 INT C MACH LEAR, P1096, DOI [DOI 10.1145/1390156.1390294, 10.1145/1390156.1390294]
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Wang L., 2014, J NANOMATER, V2014
   Wang W, 2018, SCI ADV, V4, DOI 10.1126/sciadv.aat4752
   Wang ZQ, 2012, ADV FUNCT MATER, V22, P2759, DOI 10.1002/adfm.201103148
   Williams RS, 2017, COMPUT SCI ENG, V19, P7, DOI 10.1109/MCSE.2017.31
NR 48
TC 14
Z9 14
U1 2
U2 33
PD AUG
PY 2019
VL 29
IS 8
AR 085006
DI 10.1088/1361-6439/ab235f
WC Engineering, Electrical & Electronic; Nanoscience & Nanotechnology;
   Instruments & Instrumentation; Physics, Applied
DA 2023-11-11
ER

PT J
AU Sanchez-Garcia, M
   Chauhan, T
   Cottereau, BR
   Beyeler, M
AF Sanchez-Garcia, Melani
   Chauhan, Tushar
   Cottereau, Benoit R.
   Beyeler, Michael
TI Efficient multi-scale representation of visual objects using a
   biologically plausible spike-latency code and winner-take-all inhibition
SO BIOLOGICAL CYBERNETICS
DT Article
DE Spiking neural networks; Spike-timing-dependent-plasticity; Multi-scale
   processing; Spike-latency code; Winner-take-all inhibition
ID TIMING-DEPENDENT PLASTICITY; RETINAL GANGLION-CELLS; RESPONSE
   PROPERTIES; FREQUENCY; CATEGORIZATION; STATISTICS; NETWORK; CORTEX
AB Deep neural networks have surpassed human performance in key visual challenges such as object recognition, but require a large amount of energy, computation, and memory. In contrast, spiking neural networks (SNNs) have the potential to improve both the efficiency and biological plausibility of object recognition systems. Here we present a SNN model that uses spike-latency coding and winner-take-all inhibition (WTA-I) to efficiently represent visual stimuli using multi-scale parallel processing. Mimicking neuronal response properties in early visual cortex, images were preprocessed with three different spatial frequency (SF) channels, before they were fed to a layer of spiking neurons whose synaptic weights were updated using spike-timing-dependent-plasticity. We investigate how the quality of the represented objects changes under different SF bands and WTA-I schemes. We demonstrate that a network of 200 spiking neurons tuned to three SFs can efficiently represent objects with as little as 15 spikes per neuron. Studying how core object recognition may be implemented using biologically plausible learning rules in SNNs may not only further our understanding of the brain, but also lead to novel and efficient artificial vision systems.
C1 [Sanchez-Garcia, Melani; Beyeler, Michael] Univ Calif Santa Barbara, Dept Comp Sci, Santa Barbara, CA 93106 USA.
   [Chauhan, Tushar] MIT, Picower Inst Learning & Memory, Dept Brain & Cognit Sci, Boston, MA USA.
   [Chauhan, Tushar; Cottereau, Benoit R.] Univ Toulouse III Paul Sabatier, CerCo CNRS UMR5549, Toulouse, France.
   [Cottereau, Benoit R.] IPAL, CNRS IRL 2955, Singapore, Singapore.
   [Beyeler, Michael] Univ Calif Santa Barbara, Dept Psychol & Brain Sci, Santa Barbara, CA USA.
RP Sanchez-Garcia, M (corresponding author), Univ Calif Santa Barbara, Dept Comp Sci, Santa Barbara, CA 93106 USA.
EM mesangar@ucsb.edu; tchauhan@mit.edu; benoit.cottereau@cnrs.fr;
   mbeyeler@ucsb.edu
CR Ales JM, 2013, NEUROIMAGE, V67, P77, DOI 10.1016/j.neuroimage.2012.10.044
   Beyeler M, 2019, PLOS COMPUT BIOL, V15, DOI 10.1371/journal.pcbi.1006908
   Beyeler M, 2016, J NEUROSCI, V36, P8399, DOI 10.1523/JNEUROSCI.0396-16.2016
   Beyeler M, 2013, NEURAL NETWORKS, V48, P109, DOI 10.1016/j.neunet.2013.07.012
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Bing ZS, 2019, FRONT NEUROROBOTICS, V13, DOI 10.3389/fnbot.2019.00018
   Brzosko Z, 2019, NEURON, V103, P563, DOI 10.1016/j.neuron.2019.05.041
   Burges, 2010, MNIST HANDWRITTEN DI, DOI DOI 10.1561/2400000035
   Campbell Fergus W, 1973, THEORETICAL PHYS BIO, P374
   Caporale N, 2008, ANNU REV NEUROSCI, V31, P25, DOI 10.1146/annurev.neuro.31.060407.125639
   Chang L, 2017, CELL, V169, P1013, DOI 10.1016/j.cell.2017.05.011
   Chauhan T, 2021, FRONT NEUROSCI-SWITZ, V15, DOI 10.3389/fnins.2021.727448
   Chauhan T, 2018, J NEUROSCI, V38, P9563, DOI 10.1523/JNEUROSCI.1259-18.2018
   Cichy RM, 2016, CEREB CORTEX, V26, P3563, DOI 10.1093/cercor/bhw135
   Delorme A, 2001, NEURAL NETWORKS, V14, P795, DOI 10.1016/S0893-6080(01)00049-1
   DERRINGTON AM, 1982, J PHYSIOL-LONDON, V333, P343, DOI 10.1113/jphysiol.1982.sp014457
   DERRINGTON AM, 1979, J PHYSIOL-LONDON, V289, P299, DOI 10.1113/jphysiol.1979.sp012738
   DEVALOIS RL, 1982, VISION RES, V22, P545, DOI 10.1016/0042-6989(82)90113-4
   DiCarlo JJ, 2012, NEURON, V73, P415, DOI 10.1016/j.neuron.2012.01.010
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   ENROTHCUGELL C, 1966, J PHYSIOL-LONDON, V187, P517, DOI 10.1113/jphysiol.1966.sp008107
   Falez P., 2019, 2019 INT JOINT C NEU, P1, DOI [DOI 10.1109/IJCNN.2019.8852346, 10.1109/IJCNN.2019.8852346]
   Feldman DE, 2012, NEURON, V75, P556, DOI 10.1016/j.neuron.2012.08.001
   FIELD DJ, 1987, J OPT SOC AM A, V4, P2379, DOI 10.1364/JOSAA.4.002379
   Fu Q, 2021, NEUROCOMPUTING, V419, P47, DOI 10.1016/j.neucom.2020.07.109
   Gerstner W., 2002, SPIKING NEURON MODEL, DOI DOI 10.1017/CBO9780511815706
   Ginsburg AP, 1986, HDB PERCEPTION HUMAN, V2
   Goel A, 2020, 2020 IEEE 6TH WORLD FORUM ON INTERNET OF THINGS (WF-IOT), DOI 10.1109/wf-iot48130.2020.9221198
   Gütig R, 2003, J NEUROSCI, V23, P3697
   Hao YZ, 2020, NEURAL NETWORKS, V121, P387, DOI 10.1016/j.neunet.2019.09.007
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   Henriksson L, 2008, J VISION, V8, DOI 10.1167/8.10.5
   Hughes HC, 1996, J COGNITIVE NEUROSCI, V8, P197, DOI 10.1162/jocn.1996.8.3.197
   Jiang PY, 2022, PROCEDIA COMPUT SCI, V199, P1066, DOI 10.1016/j.procs.2022.01.135
   Kauffmann L, 2014, FRONT INTEGR NEUROSC, V8, DOI 10.3389/fnint.2014.00037
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Liu D, 2016, IEEE IJCNN, P285, DOI 10.1109/IJCNN.2016.7727210
   Liu QH, 2020, IEEE T NEUR NET LEAR, V31, P5300, DOI 10.1109/TNNLS.2020.2966058
   Maass W, 2000, NEURAL COMPUT, V12, P2519, DOI 10.1162/089976600300014827
   Majaj NJ, 2015, J NEUROSCI, V35, P13402, DOI 10.1523/JNEUROSCI.5181-14.2015
   Masquelier T, 2007, PLOS COMPUT BIOL, V3, P247, DOI 10.1371/journal.pcbi.0030031
   Mozafari M, 2019, PATTERN RECOGN, V94, P87, DOI 10.1016/j.patcog.2019.05.015
   Nassi JJ, 2009, NAT REV NEUROSCI, V10, P360, DOI 10.1038/nrn2619
   Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7
   Samaria F. S., 1994, Proceedings of the Second IEEE Workshop on Applications of Computer Vision (Cat. No.94TH06742), P138, DOI 10.1109/ACV.1994.341300
   Sanchez-Garcia, 2022, ARXIV
   SHAPLEY R, 1985, ANNU REV NEUROSCI, V8, P547, DOI 10.1146/annurev.ne.08.030185.002555
   Solomon SG, 2002, J NEUROSCI, V22, P338, DOI 10.1523/JNEUROSCI.22-01-00338.2002
   Stivaktakis R, 2019, IEEE GEOSCI REMOTE S, V16, P1031, DOI 10.1109/LGRS.2019.2893306
   Stuijt J, 2021, FRONT NEUROSCI-SWITZ, V15, DOI 10.3389/fnins.2021.664208
   Sun Y., 2015, ARXIV
   TOLHURST DJ, 1992, OPHTHAL PHYSL OPT, V12, P229, DOI 10.1111/j.1475-1313.1992.tb00296.x
   Vigneron A, 2020, IEEE IJCNN, DOI 10.1109/ijcnn48605.2020.9207239
   Vinje W. E., 2000, Science, V287, P1273, DOI 10.1126/science.287.5456.1273
   Xiao H, 2017, ARXIV PREPRINT ARXIV
   Yu Q, 2013, IEEE T NEUR NET LEAR, V24, P1539, DOI 10.1109/TNNLS.2013.2245677
   Zhou Q, 2022, IEEE SENS J, V22, P16323, DOI 10.1109/JSEN.2022.3189679
NR 58
TC 1
Z9 1
U1 3
U2 5
PD APR
PY 2023
VL 117
IS 1-2
BP 95
EP 111
DI 10.1007/s00422-023-00956-x
EA APR 2023
WC Computer Science, Cybernetics; Neurosciences
DA 2023-11-11
ER

PT J
AU Burgsteiner, H
AF Burgsteiner, Harald
TI Imitation learning with spiking neural networks and real-world devices
SO ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE
DT Article
DE robotics; learning; spiking neural networks
ID COMPUTATION; NEURONS
AB This article is about a new approach in robotic learning systems. It provides a method to use a real-world device that operates in realtime, controlled through a simulated recurrent spiking neural network for robotic experiments. A randomly generated network is used as the main computational unit. Only the weights of the output units of this network are changed during training. It will be shown, that this simple type of a biological realistic spiking neural network-also known as a neural microcircuit-is able to imitate robot controllers like that incorporated in Braitenberg vehicles. A more non-linear type controller is imitated in a further experiment. In a different series of experiments that involve temporal memory reported in Burgsteiner et al. [2005. In: Proceedings of the 18th International Conference IEA/AIE. Lecture Notes in Artificial Intelligence. Springer, Berlin, pp. 121-130.] this approach also provided a basis for a movement prediction task. The results suggest that a neural microcircuit with a simple learning rule can be used as a sustainable robot controller for experiments in computational motor control. (c) 2006 Elsevier Ltd. All rights reserved.
C1 Graz Univ Appl Sci, Dept Informat Engn Info Med Hlth Care Engn, A-8020 Graz, Austria.
RP Burgsteiner, H (corresponding author), Graz Univ Appl Sci, Dept Informat Engn Info Med Hlth Care Engn, Eggenberger Allee 11, A-8020 Graz, Austria.
EM harald.burgsteiner@fh-joanneum.at
CR [Anonymous], 1989, GENETIC ALGORITHM SE
   [Anonymous], 2001, 148 GMD GERM NAT RES
   Billard A, 2001, CYBERNET SYST, V32, P155, DOI 10.1080/019697201300001849
   Bishop C.M., 1995, NEURAL NETWORKS PATT
   Burgsteiner H, 2005, LECT NOTES ARTIF INT, V3533, P121
   Burgsteiner Harald, 2005, P 9 INT C ENG APPL N, P129
   ECKMILLER R, 1991, P INT C ART NEUR NET, V1, P345
   ECKMILLER R, 1988, NEURAL COMPUTERS, P359
   ELMAN JL, 1990, COGNITIVE SCI, V14, P179, DOI 10.1207/s15516709cog1402_1
   Floreano D, 1996, IEEE T SYST MAN CY B, V26, P396, DOI 10.1109/3477.499791
   FLOREANO D, 2001, LECT NOTES COMPUTER, P38
   Gers FA, 2000, NEURAL COMPUT, V12, P2451, DOI 10.1162/089976600300015015
   Gupta A, 2000, SCIENCE, V287, P273, DOI 10.1126/science.287.5451.273
   HARTMANN G, 1997, P 6 INT C MICR NEUR, P130
   HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554
   HOPFIELD JJ, 1995, P NATL ACAD SCI USA, V92, P6655, DOI 10.1073/pnas.92.15.6655
   Jordan M. I., 1999, COGNITIVE NEUROSCIEN, P601
   JOSHI P, 2004, P BIO ADIT, P258
   Kohonen T, 2001, SELF ORG MAPS, P501, DOI [10.1007/978-3-642-56927-2, DOI 10.1007/978-3-642-56927-2]
   LOFFLER A, 1999, P 1 INT KHEP WORKSH, V64
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   MAASS W., 1999, PULSED NEURAL NETWOR
   Markram H, 1998, P NATL ACAD SCI USA, V95, P5323, DOI 10.1073/pnas.95.9.5323
   NATSCHLAGER T, 2003, NEUROSCIENCE DATABAS, P123, DOI DOI 10.1007/978-1-4615-1079-6_9
   PEARLMUTTER BA, 1995, IEEE T NEURAL NETWOR, V6, P1212, DOI 10.1109/72.410363
   Thomson AM, 2002, CEREB CORTEX, V12, P936, DOI 10.1093/cercor/12.9.936
NR 27
TC 14
Z9 14
U1 1
U2 10
PD OCT
PY 2006
VL 19
IS 7
SI SI
BP 741
EP 752
DI 10.1016/j.engappai.2006.05.007
WC Automation & Control Systems; Computer Science, Artificial Intelligence;
   Engineering, Multidisciplinary; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Naylor, M
   Fox, PJ
   Markettos, AT
   Moore, SW
AF Naylor, Matthew
   Fox, Paul J.
   Markettos, A. Theodore
   Moore, Simon W.
BE Cardoso, JMP
   Morrow, K
   Diniz, PC
TI A SPIKING NEURAL NETWORK ON A PORTABLE FPGA TABLET
SO 2013 23RD INTERNATIONAL CONFERENCE ON FIELD PROGRAMMABLE LOGIC AND
   APPLICATIONS (FPL 2013) PROCEEDINGS
SE International Conference on Field Programmable and Logic Applications
DT Proceedings Paper
CT 23rd International Conference on Field Programmable Logic and
   Applications (FPL)
CY SEP 02-04, 2013
CL U Porto, Porto, PORTUGAL
HO U Porto
C1 [Naylor, Matthew; Fox, Paul J.; Markettos, A. Theodore; Moore, Simon W.] Univ Cambridge, Comp Lab, Cambridge CB2 3QG, England.
RP Naylor, M (corresponding author), Univ Cambridge, Comp Lab, Pembroke St, Cambridge CB2 3QG, England.
EM matthew.naylor@cl.cam.ac.uk; paul.fox@cl.cam.ac.uk;
   theo.markettos@cl.cam.ac.uk; simon.moore@cl.cam.ac.uk
CR Eliasmith C, 2012, SCIENCE, V338, P1202, DOI 10.1126/science.1225266
   Moore S. W., P FCCM 2012, P133
   Naylor M., P FPL 2013 IN PRESS
NR 3
TC 0
Z9 0
U1 0
U2 0
PY 2013
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering
DA 2023-11-11
ER

PT C
AU Belhadj, B
   Tomas, J
   Malot, O
   Bornat, Y
   N'Kaoua, G
   Renaud, S
AF Belhadj, Bilel
   Tomas, Jean
   Malot, Olivia
   Bornat, Yannick
   N'Kaoua, Gilles
   Renaud, Sylvie
GP IEEE
TI Guaranteeing Spike Arrival Time in Multiboard & Multichip Spiking Neural
   Networks
SO 2010 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS
SE IEEE International Symposium on Circuits and Systems
DT Proceedings Paper
CT International Symposium on Circuits and Systems Nano-Bio Circuit Fabrics
   and Systems (ISCAS 2010)
CY MAY 30-JUN 02, 2010
CL Paris, FRANCE
AB Large-scale spiking neural networks (SNN) are generally run on distributed and parallel architectures with multiple computation nodes. These architectures induce extra delays due to the node-to-node communication process. In multiboard & multichip SNNs, important delays may affect spike arrival time and, thus, can alter simulation results. In this work, we propose a method aiming to guarantee spike arrival time with arbitrary prefixed deadlines. The communication architecture is based on the token-passing access policy to grant access to shared communication channels. We show that several network parameters must be set carefully if spikes have to meet their deadlines. Parameters are chosen by taking into account the communication channel bandwidth, the arbitrary deadlines and the worst case situation that can happen in generating neural activity in SNNs. As proof of concept, we have built a system that emulates up to 120 analog Hodgkin-Huxley neurons spread across 6 boards. Experimental results show that whatever it happens (unless there is a network fault), spikes reach their destination with a maximum delay of 5 microseconds.
C1 [Belhadj, Bilel; Tomas, Jean; Malot, Olivia; Bornat, Yannick; N'Kaoua, Gilles; Renaud, Sylvie] Bordeaux Univ, Dept Microelect, IMS Labs, Bordeaux, France.
RP Belhadj, B (corresponding author), Bordeaux Univ, Dept Microelect, IMS Labs, Bordeaux, France.
EM bilel.belhadj@ims-bordeaux.fr; jean.tomas@ims-bordeaux.fr
CR BELHADJ B, 2008, ICECS 08, P93
   BELHADJ B, 2009, P SOC PHOTO-OPT INS, V7365, P1
   BURY L, 2008, BIOMEDICAL CIRCUITS, P193
   CHICCA E, 2004, ISCAS 04 MAY, V5, P357
   CHOI TYW, 2004, ISCAS 04 MAY, V3, P13
   Malcolm N., 1993, Proceedings. 18th Conference on Local Computer Networks (Cat. No.93TH0582-7), P186, DOI 10.1109/LCN.1993.591219
   Renaud S, 2007, IEEE INT SYMP CIRC S, P3355, DOI 10.1109/ISCAS.2007.378286
   SEVCIK KC, 1987, IEEE T SOFTWARE ENG, V13, P376, DOI 10.1109/TSE.1987.233169
NR 8
TC 0
Z9 0
U1 0
U2 0
PY 2010
BP 377
EP 380
WC Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Szczesny, S
   Huderek, D
   Przyborowski, L
AF Szczesny, Szymon
   Huderek, Damian
   Przyborowski, Lukasz
TI Spiking Neural Network with Linear Computational Complexity for Waveform
   Analysis in Amperometry
SO SENSORS
DT Article
DE amperometry; edge computing; spiking neural network; exocytosis; vesicle
   fusion
ID EXOCYTOSIS
AB The paper describes the architecture of a Spiking Neural Network (SNN) for time waveform analyses using edge computing. The network model was based on the principles of preprocessing signals in the diencephalon and using tonic spiking and inhibition-induced spiking models typical for the thalamus area. The research focused on a significant reduction of the complexity of the SNN algorithm by eliminating most synaptic connections and ensuring zero dispersion of weight values concerning connections between neuron layers. The paper describes a network mapping and learning algorithm, in which the number of variables in the learning process is linearly dependent on the size of the patterns. The works included testing the stability of the accuracy parameter for various network sizes. The described approach used the ability of spiking neurons to process currents of less than 100 pA, typical of amperometric techniques. An example of a practical application is an analysis of vesicle fusion signals using an amperometric system based on Carbon NanoTube (CNT) sensors. The paper concludes with a discussion of the costs of implementing the network as a semiconductor structure.
C1 [Szczesny, Szymon; Huderek, Damian; Przyborowski, Lukasz] Poznan Univ Tech, Fac Comp & Telecommun, Inst Comp Sci, Piotrowo 3A St, PL-61138 Poznan, Poland.
RP Szczesny, S (corresponding author), Poznan Univ Tech, Fac Comp & Telecommun, Inst Comp Sci, Piotrowo 3A St, PL-61138 Poznan, Poland.
EM szymon.szczesny@put.poznan.pl; damian.huderek@put.poznan.pl;
   lukasz.i.przyborowski@doctorate.put.poznan.pl
CR Abusnaina Ahmed A., 2014, International Journal of Digital Content Technology and its Applications, V8, P14
   Acero C, 2017, IEEE T VLSI SYST, V25, P2949, DOI 10.1109/TVLSI.2017.2717844
   Auer M., 2020, P 2020 43 INT CONV I
   Banks C., 2015, ELECTROCHEMISTRY
   Basu A, 2010, IEEE T CIRCUITS-I, V57, P2938, DOI 10.1109/TCSI.2010.2048772
   Cao J., 2018, P 2018 IEEE INT S CI
   Cassell A.M., 2011, U.S. Patent, Patent No. [7939734B1, 7939734]
   Chen Y., 2017, P DES AUT TEST EUR C
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Elfaki E.A., 2013, P INT C COMP EL EL E
   Fathali H, 2018, PFLUG ARCH EUR J PHY, V470, P125, DOI 10.1007/s00424-017-2069-9
   Handkiewicz A., 2002, MIXED SIGNAL SYSTEMS
   Harrison R., 2014, MOSFET OPERATION WEA
   Hoogstraaten RI, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-67988-2
   Iakymchuk T, 2015, EURASIP J IMAGE VIDE, DOI 10.1186/s13640-015-0059-4
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Joubert A, 2012, IEEE IJCNN
   Li Y., 2019, P 2019 SIL NAN WORKS
   Lucien F, 2019, BIOCHEM SOC T, V47, P273, DOI 10.1042/BST20180253
   Lymer J, 2019, EXP BRAIN RES, V237, P1511, DOI 10.1007/s00221-019-05526-x
   Michels A, 2016, J THROMB HAEMOST, V14, P2274, DOI 10.1111/jth.13493
   Morie T, 2015, 2015 IEEE INTERNATIONAL MEETING FOR FUTURE OF ELECTRON DEVICES, KANSAI (IMFEDK)
   Nowak LG, 1997, CEREB CORTEX, V7, P487, DOI 10.1093/cercor/7.6.487
   Ponulak F, 2011, ACTA NEUROBIOL EXP, V71, P409
   Rowcliffe P, 2006, IEEE T NEURAL NETWOR, V17, P803, DOI 10.1109/TNN.2006.873274
   Schrauwen B, 2004, P 15 PRORISC WORKSH, P301
   Sourikopoulos I, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00123
   Stuijk S., 2021, P DRONESE RAPIDO 21
   Sulaiman M.B.G., 2020, IEEE INT C ELECTR TA, P1
   Szczesny S, 2020, IEEE SENS J, V20, P5733, DOI 10.1109/JSEN.2020.2974701
   Szczesny S, 2020, J COMPUT ELECTRON, V19, P242, DOI 10.1007/s10825-019-01431-2
   Szczesny S, 2017, CIRC SYST SIGNAL PR, V36, P2672, DOI 10.1007/s00034-016-0449-6
   Szczesny S, 2016, MICROELECTRON ENG, V165, P41, DOI [10.1016/j.mec.2016.08.010, 10.1016/j.mee.2016.08.010]
   Tang HY, 2019, IEEE T BIOMED CIRC S, V13, P1664, DOI 10.1109/TBCAS.2019.2945406
   Tazerart S, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-17861-7
   Thompson NC, 2020, COMPUTATIONAL LIMITS
   Tilevich E., 2019, P 2019 IEEE INT C ED
   To WKL, 2015, PLACENTA, V36, P759, DOI 10.1016/j.placenta.2015.04.005
   Trampert DC, 2018, BBA-MOL CELL RES, V1865, P1761, DOI 10.1016/j.bbamcr.2018.05.010
   Wu QH, 2019, J NEUROSCI, V39, P199, DOI 10.1523/JNEUROSCI.1255-18.2018
   Wu XY, 2015, IEEE T CIRCUITS-II, V62, P1088, DOI 10.1109/TCSII.2015.2456372
   Yuan XB, 2011, IEEE T ELECTRON DEV, V58, P335, DOI 10.1109/TED.2010.2090159
   Zoltowska KM, 2017, MOL NEURODEGENER, V12, DOI 10.1186/s13024-017-0159-y
NR 44
TC 2
Z9 2
U1 0
U2 5
PD MAY
PY 2021
VL 21
IS 9
AR 3276
DI 10.3390/s21093276
WC Chemistry, Analytical; Engineering, Electrical & Electronic; Instruments
   & Instrumentation
DA 2023-11-11
ER

PT J
AU Moon, J
   Wu, YT
   Zhu, XJ
   Lu, WD
AF Moon, John
   Wu, Yuting
   Zhu, Xiaojian
   Lu, Wei D.
TI Neural connectivity inference with spike-timing dependent plasticity
   network
SO SCIENCE CHINA-INFORMATION SCIENCES
DT Article
DE spike-timing dependent plasticity; neural connectivity; memristor;
   online learning; second-order memristor
AB Knowing the connectivity patterns in neural circuitry is essential to understand the operating mechanism of the brain, as it allows the analysis of how neural signals are processed and flown through the neural system. With the recent advances in neural recording technologies in terms of channel size and time resolution, a simple and efficient system to perform neural connectivity inference is highly desired, which will enable the process of high dimensional neural activity recording data and reduction of the computational time and cost. In this work, we show that the spike-timing dependent plasticity (STDP) algorithm can be used to reconstruct neural connectivity patterns in a biological neural network, with higher accuracy and efficiency than statistic-based inference methods. The biologically inspired STDP learning rules are natively implemented in a second-order memristor network and are used to estimate the type and the direction of neural connections. When stimulated by the recorded neural spike trains, the memristor device conductance is modulated by the proposed STDP learning rules, which in turn reflects the correlation of the spikes and the possibility of neural connections. By compensating for the different levels of neural activity, highly reliable inference performance can be achieved. The proposed approach offers real-time and local learning, resulting in reduced computational cost/time and strong tolerance to variations of the neural system.
C1 [Moon, John; Wu, Yuting; Zhu, Xiaojian; Lu, Wei D.] Univ Michigan, Dept Elect Engn & Comp Sci, Ann Arbor, MI 48109 USA.
RP Lu, WD (corresponding author), Univ Michigan, Dept Elect Engn & Comp Sci, Ann Arbor, MI 48109 USA.
EM wluee@umich.edu
CR Amunts K, 2016, NEURON, V92, P574, DOI 10.1016/j.neuron.2016.10.046
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Brown EN, 2004, NAT NEUROSCI, V7, P456, DOI 10.1038/nn1228
   Caporale N, 2008, ANNU REV NEUROSCI, V31, P25, DOI 10.1146/annurev.neuro.31.060407.125639
   de Abril IM, 2018, NEURAL NETWORKS, V102, P120, DOI 10.1016/j.neunet.2018.02.016
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Garofalo M, 2009, PLOS ONE, V4, DOI 10.1371/journal.pone.0006482
   Gewaltig M-O., 2007, SCHOLARPEDIA, V2, DOI [10.4249/scholarpedia.1430, DOI 10.4249/SCHOLARPEDIA.1430]
   Hoffmann JHO, 2015, CEREB CORTEX, V25, P4415, DOI 10.1093/cercor/bhv039
   Insel TR, 2013, SCIENCE, V340, P687, DOI 10.1126/science.1239276
   Ito SY, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0027431
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Kim S, 2015, NANO LETT, V15, P2203, DOI 10.1021/acs.nanolett.5b00697
   Kim S, 2014, ACS NANO, V8, P2369, DOI 10.1021/nn405827t
   Kobayashi R, 2019, NAT COMMUN, V10, DOI 10.1038/s41467-019-12225-2
   Kraskov A, 2004, PHYS REV E, V69, DOI 10.1103/PhysRevE.69.066138
   Lee SH, 2020, ACS APPL ELECTRON MA, V2, P701, DOI 10.1021/acsaelm.9b00792
   MATTHEWS BW, 1975, BIOCHIM BIOPHYS ACTA, V405, P442, DOI 10.1016/0005-2795(75)90109-9
   Okano H, 2016, NEURON, V92, P582, DOI 10.1016/j.neuron.2016.10.018
   Pastore VP, 2018, PLOS COMPUT BIOL, V14, DOI 10.1371/journal.pcbi.1006381
   Pillow JW, 2008, NATURE, V454, P995, DOI 10.1038/nature07140
   Salinas E, 2001, NAT REV NEUROSCI, V2, P539, DOI 10.1038/35086012
   Schreiber T, 2000, PHYS REV LETT, V85, P461, DOI 10.1103/PhysRevLett.85.461
   Song S, 2005, PLOS BIOL, V3, P507, DOI 10.1371/journal.pbio.0030068
   Zhu XJ, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-16261-1
NR 25
TC 4
Z9 5
U1 1
U2 21
PD JUN
PY 2021
VL 64
IS 6
AR 160405
DI 10.1007/s11432-021-3217-0
WC Computer Science, Information Systems; Engineering, Electrical &
   Electronic
DA 2023-11-11
ER

PT C
AU Dampfhoffer, M
   Mesquida, T
   Valentian, A
   Anghel, L
AF Dampfhoffer, Manon
   Mesquida, Thomas
   Valentian, Alexandre
   Anghel, Lorena
BE Pimenidis, E
   Angelov, P
   Jayne, C
   Papaleonidas, A
   Aydin, M
TI Investigating Current-Based and Gating Approaches for Accurate and
   Energy-Efficient Spiking Recurrent Neural Networks
SO ARTIFICIAL NEURAL NETWORKS AND MACHINE LEARNING - ICANN 2022, PT III
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 31st International Conference on Artificial Neural Networks (ICANN)
CY SEP 06-09, 2022
CL Univ W England, Bristol, ENGLAND
HO Univ W England
DE SNN; RNN; GRU; Speech recognition
ID MODEL
AB Spiking Neural Networks (SNNs) with spike-based computations and communications may be more energy-efficient than Artificial Neural Networks (ANNs) for embedded applications. However, SNNs have mostly been applied to image processing, although audio applications may better fit their temporal dynamics. We evaluate the accuracy and energy-efficiency of Leaky Integrate-and-Fire (LIF) models on spiking audio datasets compared to ANNs. We demonstrate that, for processing temporal sequences, the Current-based LIF (Cuba-LIF) outperforms the LIF. Moreover, gated recurrent networks have demonstrated superior accuracy than simple recurrent networks for such tasks. Therefore, we introduce SpikGRU, a gated version of the Cuba-LIF. SpikGRU achieves higher accuracy than other recurrent SNNs on the most difficult task studied in this work. The Cuba-LIF and SpikGRU reach state-of-theart accuracy, only <1.1% below the accuracy of the best ANNs, while showing up to a 49x reduction in the number of operations compared to ANNs, due to the high spike sparsity.
C1 [Dampfhoffer, Manon; Anghel, Lorena] Univ Grenoble Alpes, Grenoble INP, CEA,CNRS, INAC Spintec, F-38000 Grenoble, France.
   [Mesquida, Thomas; Valentian, Alexandre] Univ Grenoble Alpes, CEA List, F-38000 Grenoble, France.
RP Dampfhoffer, M (corresponding author), Univ Grenoble Alpes, Grenoble INP, CEA,CNRS, INAC Spintec, F-38000 Grenoble, France.
EM manon.dampfhoffer@cea.fr
CR Abbott LF, 1999, BRAIN RES BULL, V50, P303, DOI 10.1016/S0361-9230(99)00161-6
   Anumula J, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00023
   Bellec G., 2018, ADV NEURAL INFORM PR, P787
   BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181
   Bing Han, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13555, DOI 10.1109/CVPR42600.2020.01357
   Cho K., 2014, P 19 C EMPIRICAL MET, P1, DOI [10.3115/v1/D14-1179, DOI 10.3115/V1/D14-1179, 10.3115]
   Cramer B, 2022, IEEE T NEUR NET LEAR, V33, P2744, DOI 10.1109/TNNLS.2020.3044364
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Horowitz M, 2014, ISSCC DIG TECH PAP I, V57, P10, DOI 10.1109/ISSCC.2014.6757323
   Kingma D. P., 2014, ARXIV, DOI DOI 10.48550/ARXIV.1412.6980
   Lichtsteiner P, 2008, IEEE J SOLID-ST CIRC, V43, P566, DOI 10.1109/JSSC.2007.914337
   Lotfi Rezaabad A., 2020, INT C NEUR SYST, P1, DOI DOI 10.1145/3407197.3407211
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Pan ZH, 2020, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.01420
   Perez-Nieves N, 2021, NAT COMMUN, V12, DOI 10.1038/s41467-021-26022-3
   Ponghiran W, 2021, PROCEEDINGS OF THE 2021 DESIGN, AUTOMATION & TEST IN EUROPE CONFERENCE & EXHIBITION (DATE 2021), P581, DOI 10.23919/DATE51398.2021.9473953
   Ravanelli M, 2018, IEEE TETCI, V2, P92, DOI 10.1109/TETCI.2017.2762739
   Shrestha A, 2017, ICCAD-IEEE ACM INT, P631, DOI 10.1109/ICCAD.2017.8203836
   Sze V, 2017, P IEEE, V105, P2295, DOI 10.1109/JPROC.2017.2761740
   Le QV, 2015, Arxiv, DOI arXiv:1504.00941
   Yin B, 2020, IEEE INTERNET THINGS, V7, P8748, DOI [10.1109/JIOT.2020.2996562, 10.1145/3407197.3407225]
   Yin BJ, 2021, NAT MACH INTELL, V3, P905, DOI 10.1038/s42256-021-00397-w
NR 23
TC 0
Z9 0
U1 0
U2 2
PY 2022
VL 13531
BP 359
EP 370
DI 10.1007/978-3-031-15934-3_30
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods
DA 2023-11-11
ER

PT C
AU Nobukawa, S
   Nishimura, H
   Yamanishi, T
AF Nobukawa, Sou
   Nishimura, Haruhiko
   Yamanishi, Teruya
BE Cheng, L
   Leung, ACS
   Ozawa, S
TI Skewed and Long-Tailed Distributions of Spiking Activity in Coupled
   Network Modules with Log-Normal Synaptic Weight Distribution
SO NEURAL INFORMATION PROCESSING (ICONIP 2018), PT I
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 25th International Conference on Neural Information Processing (ICONIP)
CY DEC 13-16, 2018
CL Siem Reap, CAMBODIA
DE Spiking neural network; Log-normal distribution; Pattern alternation
ID BINOCULAR-RIVALRY; FIRING RATES; CORTEX; BRAIN
AB Recent studies with neuroimaging modalities have been elucidating a structure of a whole network of the brain and its functional activity. The characteristics of various functional neural activities and network structures exhibit skewed and long-tailed distributions. However, it remains unclear how heavy-tailed structural distribution affects functional distribution. In this study, we constructed spiking neural networks composed of two modules with excitatory post-synaptic potential (EPSP) following log-normal distribution. Through the evaluation of multi-scale entropy analysis and its surrogate data analysis, we reveal that the long-tailed synaptic weight distribution enhances the complexity of spiking activity at large temporal scales and emerges non-linear dynamics. Furthermore, we compared distribution of residence time in each spiking pattern between cases with/without large EPSPs. The results show that strong synapses are crucial in the heavy-tailed distribution of residence time.
C1 [Nobukawa, Sou] Chiba Inst Technol, Dept Comp Sci, 2-17-1 Tsudanuma, Narashino, Chiba 2750016, Japan.
   [Nishimura, Haruhiko] Univ Hyogo, Grad Sch Appl Informat, 7-1-28 Chuo Ku, Kobe, Hyogo 6508588, Japan.
   [Yamanishi, Teruya] Fukui Univ Technol, Dept Management Informat Sci, 3-6-1 Gakuen, Fukui, Fukui 9108505, Japan.
RP Nobukawa, S (corresponding author), Chiba Inst Technol, Dept Comp Sci, 2-17-1 Tsudanuma, Narashino, Chiba 2750016, Japan.
EM nobukawa@cs.it-chiba.ac.jp
CR Bassett DS, 2017, NAT NEUROSCI, V20, P353, DOI 10.1038/nn.4502
   Battaglia FP, 2005, NEURAL NETWORKS, V18, P1280, DOI 10.1016/j.neunet.2005.08.011
   Blake R, 2002, NAT REV NEUROSCI, V3, P13, DOI 10.1038/nrn701
   BORSELLINO A, 1972, KYBERNETIK, V10, P139, DOI 10.1007/BF00290512
   Bullmore ET, 2009, NAT REV NEUROSCI, V10, P186, DOI 10.1038/nrn2575
   Buzsáki G, 2014, NAT REV NEUROSCI, V15, P264, DOI 10.1038/nrn3687
   Costa M, 2002, PHYS REV LETT, V89, DOI 10.1103/PhysRevLett.89.068102
   Eguíluz VM, 2005, PHYS REV LETT, V94, DOI 10.1103/PhysRevLett.94.018102
   Fell J, 2000, ACTA NEUROBIOL EXP, V60, P87
   Glasser MF, 2016, NAT NEUROSCI, V19, P1175, DOI 10.1038/nn.4361
   Glasser MF, 2016, NATURE, V536, P171, DOI 10.1038/nature18933
   Hagmann P, 2008, PLOS BIOL, V6, P1479, DOI 10.1371/journal.pbio.0060159
   Hagmann P, 2007, PLOS ONE, V2, DOI 10.1371/journal.pone.0000597
   Hirase H, 2001, P NATL ACAD SCI USA, V98, P9386, DOI 10.1073/pnas.161274398
   Hromadka T, 2008, PLOS BIOL, V6, P124, DOI 10.1371/journal.pbio.0060016
   Kanamaru T, 2017, NEURAL COMPUT, V29, P1696, DOI 10.1162/NECO_a_00965
   Lefort S, 2009, NEURON, V61, P301, DOI 10.1016/j.neuron.2008.12.020
   LEHKY SR, 1995, P ROY SOC B-BIOL SCI, V259, P71, DOI 10.1098/rspb.1995.0011
   LEVELT WJM, 1967, BRIT J PSYCHOL, V58, P143, DOI 10.1111/j.2044-8295.1967.tb01068.x
   Mizuseki K, 2014, PHILOS T R SOC B, V369, DOI 10.1098/rstb.2012.0530
   Mizuseki K, 2013, CELL REP, V4, P1010, DOI 10.1016/j.celrep.2013.07.039
   Nagao N, 2000, NEURAL PROCESS LETT, V12, P267, DOI 10.1023/A:1026511124944
   O'Connor DH, 2010, NEURON, V67, P1048, DOI 10.1016/j.neuron.2010.08.026
   Peyrache A, 2012, P NATL ACAD SCI USA, V109, P1731, DOI 10.1073/pnas.1109895109
   Schreiber T, 1996, PHYS REV LETT, V77, P635, DOI 10.1103/PhysRevLett.77.635
   Shafi M, 2007, NEUROSCIENCE, V146, P1082, DOI 10.1016/j.neuroscience.2006.12.072
   She Q, 2016, SCI REP-UK, V6, DOI 10.1038/srep21468
   Song S, 2005, PLOS BIOL, V3, P507, DOI 10.1371/journal.pbio.0030068
   Sporns O, 2014, NAT NEUROSCI, V17, P652, DOI 10.1038/nn.3690
   Teramae J, 2012, SCI REP-UK, V2, DOI 10.1038/srep00485
   van den Heuvel M, 2008, J NEUROSCI, V28, P10844, DOI 10.1523/JNEUROSCI.2964-08.2008
   van den Heuvel MP, 2013, TRENDS COGN SCI, V17, P683, DOI 10.1016/j.tics.2013.09.012
   WALKER P, 1975, PERCEPT PSYCHOPHYS, V18, P467, DOI 10.3758/BF03204122
NR 33
TC 3
Z9 3
U1 0
U2 0
PY 2018
VL 11301
BP 535
EP 544
DI 10.1007/978-3-030-04167-0_48
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods
DA 2023-11-11
ER

PT C
AU Fernandez, JG
   Hortal, E
   Mehrkanoon, S
AF Fernandez, Jesus Garcia
   Hortal, Enrique
   Mehrkanoon, Siamak
GP IEEE
TI Towards biologically plausible learning in neural networks
SO 2021 IEEE SYMPOSIUM SERIES ON COMPUTATIONAL INTELLIGENCE (IEEE SSCI
   2021)
DT Proceedings Paper
CT IEEE Symposium Series on Computational Intelligence (IEEE SSCI)
CY DEC 05-07, 2021
CL ELECTR NETWORK
ID ERROR-BACKPROPAGATION; SPIKING; CORTEX; MODEL
AB Artificial neural networks are inspired by information processing performed by neural circuits in biology. While existing models are sufficient to solve many real-world tasks, they are far from reaching the potential of biological neural networks. These models are oversimplifications of their biological counterparts, omitting key features such as the spiking nature of their units or the locality during learning, among others. In this work, we, first, provide a short review of the most recent theories on biologically plausible learning and learning in Spiking Neural Networks. Then, aiming to give a step towards brain-inspired deep learning, we introduce a novel biologically plausible learning method. This approach achieves learning using only local information to each synapse, spiking units and unidirectional synaptic connections. We also propose a local solution to address the credit assignment problem based on target propagation. Finally, we evaluate our approach over three different tasks, i.e. boolean problems, image autoencoding and handwritten digit recognition.
C1 [Fernandez, Jesus Garcia; Hortal, Enrique; Mehrkanoon, Siamak] Maastricht Univ, Dept Data Sci & Knowledge Engn, Maastricht, Netherlands.
RP Fernandez, JG (corresponding author), Maastricht Univ, Dept Data Sci & Knowledge Engn, Maastricht, Netherlands.
CR [Anonymous], 2005, AISTATS BRIDGETOWN B
   [Anonymous], 2016, PROC 9 EAI INT C BIO
   [Anonymous], 1960, IRE WESCON CONVENTIO, DOI DOI 10.21236/AD0241531
   Badel L, 2008, J NEUROPHYSIOL, V99, P656, DOI 10.1152/jn.01107.2007
   Barrett DGT, 2019, CURR OPIN NEUROBIOL, V55, P55, DOI 10.1016/j.conb.2019.01.007
   Bengio Y., ARXIV PREPRINT ARXIV
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   BRIDGEMAN B, 1971, PSYCHOL REV, V78, P528, DOI 10.1037/h0031782
   Cadieu CF, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003963
   Clopath C, 2007, NEUROCOMPUTING, V70, P1668, DOI 10.1016/j.neucom.2006.10.047
   Comsa JM, 2020, INT CONF ACOUST SPEE, P8529, DOI [10.1109/ICASSP40776.2020.9053856, 10.1109/icassp40776.2020.9053856]
   Devlin J., 2018, PREPRINT
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Geirhos R., 2018, IMAGENET TRAINED CNN
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   Gollisch T, 2008, SCIENCE, V319, P1108, DOI 10.1126/science.1149639
   Guerguiev J, 2017, ELIFE, V6, DOI 10.7554/eLife.22901
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Hao YZ, 2020, NEURAL NETWORKS, V121, P387, DOI 10.1016/j.neunet.2019.09.007
   HEBB D. O., 1949
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Hussain S, 2014, IEEE INT SYMP CIRC S, P2640, DOI 10.1109/ISCAS.2014.6865715
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Jolivet R., 2006, INTEGRATE FIRE MODEL
   Kaiser J, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00424
   Kheradpisheh SR, 2020, INT J NEURAL SYST, V30, DOI 10.1142/S0129065720500276
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Le Cun Y., 1986, DISORDERED SYSTEMS B, P233
   Lee DH, 2015, LECT NOTES ARTIF INT, V9284, P498, DOI 10.1007/978-3-319-23528-8_31
   Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508
   Lillicrap TP, 2020, NAT REV NEUROSCI, V21, P335, DOI 10.1038/s41583-020-0277-3
   Lillicrap TP, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms13276
   Mostafa H, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00608
   Mozafari M., 2018, ARXIV180400227
   Naud R, 2008, BIOL CYBERN, V99, P335, DOI 10.1007/s00422-008-0264-7
   Noriega L., 2005, MULTILAYER PERCEPTRO, V4, P5
   OReilly RC, 1996, NEURAL COMPUT, V8, P895, DOI 10.1162/neco.1996.8.5.895
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Qian N, 1999, NEURAL NETWORKS, V12, P145, DOI 10.1016/S0893-6080(98)00116-6
   Rao RPN, 1999, NAT NEUROSCI, V2, P79, DOI 10.1038/4580
   Roelfsema PR, 2018, NAT REV NEUROSCI, V19, P166, DOI 10.1038/nrn.2018.6
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Sacramento J, 2018, ADV NEUR IN, V31
   Scellier B, 2017, FRONT COMPUT NEUROSC, V11, DOI 10.3389/fncom.2017.00024
   Shadlen MN, 1998, J NEUROSCI, V18, P3870
   Shrestha SB, 2018, ADV NEUR IN, V31
   Sjostrom J., 2010, SCHOLARPEDIA 52 REVI, V5, P1362, DOI [10.4249/scholarpedia.1362, 10.4249%2Fscholarpedia.1362, DOI 10.4249/SCHOLARPEDIA.1362]
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
   Tavanaei A, 2019, NEUROCOMPUTING, V330, P39, DOI 10.1016/j.neucom.2018.11.014
   Whittington JCR, 2019, TRENDS COGN SCI, V23, P235, DOI 10.1016/j.tics.2018.12.005
   Whittington JCR, 2017, NEURAL COMPUT, V29, P1229, DOI 10.1162/NECO_a_00949
   Wu JY, 2019, 2019 5TH INTERNATIONAL CONFERENCE ON EVENT-BASED CONTROL, COMMUNICATION, AND SIGNAL PROCESSING (EBCCSP), DOI 10.1109/ebccsp.2019.8836892
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Wunderlich TC, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-91786-z
   Zenke F, 2018, NEURAL COMPUT, V30, P1514, DOI 10.1162/neco_a_01086
   Zhao B, 2015, IEEE T NEUR NET LEAR, V26, P1963, DOI 10.1109/TNNLS.2014.2362542
NR 56
TC 0
Z9 0
U1 0
U2 3
PY 2021
DI 10.1109/SSCI50451.2021.9659539
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic; Operations Research & Management Science; Mathematics,
   Applied
DA 2023-11-11
ER

PT J
AU Li, QS
   Gao, Y
AF Li, Qianshu
   Gao, Yang
TI Control of spiking regularity in a noisy complex neural network
SO PHYSICAL REVIEW E
DT Article
ID ENHANCED COHERENCE RESONANCE; SMALL-WORLD NETWORKS; STOCHASTIC
   RESONANCE; NEOCORTICAL NEURONS; SIGNAL-TRANSDUCTION; IN-VIVO; SYSTEMS;
   ARRAY; SYNCHRONIZATION; DRIVEN
AB The effects of spatiotemporally correlated noise on the regularity of spiking oscillations are studied in a network composed of Fitz-Hugh-Nagumo neurons. The spiking regularity of the neural network becomes the best at a moderate noise intensity, indicating the occurrence of coherence resonance (CR). The CR in a Watts-Strogatz small-world network is further improved by adding a small fraction of long-range connections. Given a set of temporal correlation constant tau and spatial correlation length lambda of the noise, there exists an optimal network topology randomness, at which the spiking oscillations show the best regularity. The optimal randomness of the network topology at different tau and lambda varies in a narrow range. Changing lambda does not affect the optimal tau for achieving the most regular spike train, whereas varying tau, the best spiking regularity emerges at different optimal lambda.
C1 [Li, Qianshu; Gao, Yang] Beijing Inst Technol, Inst Chem Phys, Beijing 100081, Peoples R China.
   [Li, Qianshu] S China Normal Univ, Sch Chem & Environm, Guangzhou 510006, Peoples R China.
RP Li, QS (corresponding author), Beijing Inst Technol, Inst Chem Phys, Beijing 100081, Peoples R China.
EM qsli@bit.edu.cn
CR Albert R, 2002, REV MOD PHYS, V74, P47, DOI 10.1103/RevModPhys.74.47
   Alonso S, 2002, PHYS REV E, V65, DOI 10.1103/PhysRevE.65.066107
   Atilgan AR, 2004, BIOPHYS J, V86, P85, DOI 10.1016/S0006-3495(04)74086-2
   BENZI R, 1981, J PHYS A-MATH GEN, V14, pL453, DOI 10.1088/0305-4470/14/11/006
   BEZRUKOV SM, 1995, NATURE, V378, P362, DOI 10.1038/378362a0
   BRITTEN KH, 1992, J NEUROSCI, V12, P4745
   Buldú JM, 2001, PHYS REV E, V64, DOI 10.1103/PhysRevE.64.051109
   Buzsáki G, 2004, TRENDS NEUROSCI, V27, P186, DOI 10.1016/j.tins.2004.02.007
   Collins JJ, 1996, NATURE, V383, P770, DOI 10.1038/383770a0
   Destexhe A, 2003, NAT REV NEUROSCI, V4, P739, DOI 10.1038/nrn1198
   Destexhe A, 2001, NEUROSCIENCE, V107, P13, DOI 10.1016/S0306-4522(01)00344-X
   DOUGLASS JK, 1993, NATURE, V365, P337, DOI 10.1038/365337a0
   Gong PL, 2002, CHAOS SOLITON FRACT, V13, P885, DOI 10.1016/S0960-0779(01)00064-9
   Gong YB, 2005, CHEMPHYSCHEM, V6, P1042, DOI 10.1002/cphc.200500051
   Han SK, 1999, PHYS REV LETT, V83, P1771, DOI 10.1103/PhysRevLett.83.1771
   Hasegawa H, 2004, PHYS REV E, V70, DOI 10.1103/PhysRevE.70.066107
   Hauptmann C, 1999, INT J BIFURCAT CHAOS, V9, P1159, DOI 10.1142/S0218127499000808
   He DH, 2002, PHYS REV E, V65, DOI 10.1103/PhysRevE.65.055204
   Hu BB, 2000, PHYS REV E, V61, pR1001, DOI 10.1103/PhysRevE.61.R1001
   Jasch F, 2002, J CHEM PHYS, V117, P2474, DOI 10.1063/1.1489903
   Kaiser M, 2004, NEUROCOMPUTING, V58, P297, DOI 10.1016/j.neucom.2004.01.059
   Kashimori Y, 1998, BIOPHYS J, V75, P1700, DOI 10.1016/S0006-3495(98)77612-X
   Kwon O, 2005, PHYS REV E, V72, DOI 10.1103/PhysRevE.72.066121
   Levin JE, 1996, NATURE, V380, P165, DOI 10.1038/380165a0
   Lin M, 2005, PHYS REV E, V71, DOI 10.1103/PhysRevE.71.016133
   Lindner B, 2005, PHYS REV E, V72, DOI 10.1103/PhysRevE.72.061919
   Lindner B, 2000, PHYS REV E, V61, P6103, DOI 10.1103/PhysRevE.61.6103
   LINDNER JF, 1995, PHYS REV LETT, V75, P3, DOI 10.1103/PhysRevLett.75.3
   Liu F, 2001, PHYS REV E, V63, DOI 10.1103/PhysRevE.63.031907
   Manjarrez E, 2002, NEUROSCI LETT, V326, P93, DOI 10.1016/S0304-3940(02)00318-X
   Nozaki D, 1999, PHYS REV E, V60, P4637, DOI 10.1103/PhysRevE.60.4637
   Pei X, 1996, PHYS REV LETT, V77, P4679, DOI 10.1103/PhysRevLett.77.4679
   Pikovsky AS, 1997, PHYS REV LETT, V78, P775, DOI 10.1103/PhysRevLett.78.775
   Sancho J M, 1989, NOISE NONLINEAR DYNA
   Simonotto E, 1997, PHYS REV LETT, V78, P1186, DOI 10.1103/PhysRevLett.78.1186
   Wang MS, 2006, CHEMPHYSCHEM, V7, P579, DOI 10.1002/cphc.200500499
   Wang S, 2004, PHYS REV E, V69, DOI 10.1103/PhysRevE.69.011909
   Wang W, 1997, PHYS REV E, V55, P7379, DOI 10.1103/PhysRevE.55.7379
   Wang XN, 2004, PHYS REV E, V69, DOI 10.1103/PhysRevE.69.056223
   Watts D., 1999, SMALL WORLDS
   Watts DJ, 1998, NATURE, V393, P440, DOI 10.1038/30918
   Zhou CS, 2001, PHYS REV LETT, V87, DOI 10.1103/PhysRevLett.87.098101
NR 42
TC 12
Z9 13
U1 0
U2 12
PD MAR
PY 2008
VL 77
IS 3
AR 036117
DI 10.1103/PhysRevE.77.036117
PN 2
WC Physics, Fluids & Plasmas; Physics, Mathematical
DA 2023-11-11
ER

PT C
AU Hu, J
   Tang, HJ
   Tan, KC
AF Hu, Jun
   Tang, Huajin
   Tan, Kay Chen
GP IEEE
TI A Hierarchical Organized Memory Model Using Spiking Neurons
SO 2013 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY AUG 04-09, 2013
CL Dallas, TX
ID REPRESENTATION; OSCILLATIONS; INFORMATION; NETWORKS; SYSTEM; UNITS
AB The recent identification of neural cliques, which are network-level memory coding units in the hippocampus, enables population codes to be the neuronal representation of memory. It has been discovered that the timing of spikes plays an important role in the neural computation and information processing in the brain. Moreover, these memory-coding units have been observed organizing in a hierarchical manner in the brain. Inspired by these exciting findings, we present a hierarchically organized memory model with spiking neurons, which can store both associative memory and episodic memory with temporal population codes. The basic structure of the hierarchical model is composed of three layers with different functions and can be extended to more complicated networks by duplicating and connecting the basic three-layer network. With a spike-timing based learning algorithm, the spiking neural network with theta and gamma oscillations is able to store spatiotemporal memory items within gamma cycles, and links these memories into a sequence. The spiking-timing-dependent plasticity (STDP) contributes to the formation of both associative memory and episodic memory via fast and slow N-methyl-Daspartate (NMDA) channels, respectively.
C1 [Hu, Jun; Tan, Kay Chen] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 117576, Singapore.
   [Tang, Huajin] ASTAR, Inst Infocomm Res, Singapore 138632, Singapore.
RP Hu, J (corresponding author), Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 117576, Singapore.
EM junhu@nus.edu.sg; htang@i2r.a-star.edu.sg; eletankc@nus.edu.sg
CR Adrian E. D., 1928, BASIS SENSATION
   [Anonymous], 1998, PULSED NEURAL NETWOR
   Axmacher N, 2006, BRAIN RES REV, V52, P170, DOI 10.1016/j.brainresrev.2006.01.007
   Baeg EH, 2003, NEURON, V40, P177, DOI 10.1016/S0896-6273(03)00597-X
   Barak O, 2010, J NEUROSCI, V30, P9424, DOI 10.1523/JNEUROSCI.1875-10.2010
   Bi GQ, 2001, ANNU REV NEUROSCI, V24, P139, DOI 10.1146/annurev.neuro.24.1.139
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   BLISS TVP, 1993, NATURE, V361, P31, DOI 10.1038/361031a0
   Boerlin M, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1001080
   Buzsáki G, 2004, NAT NEUROSCI, V7, P446, DOI 10.1038/nn1233
   Buzsáki G, 2002, NEURON, V33, P325, DOI 10.1016/S0896-6273(02)00586-X
   CARR CE, 1993, ANNU REV NEUROSCI, V16, P223, DOI 10.1146/annurev.ne.16.030193.001255
   Cohen NJ, 1993, MEMORY AMNESIA HIPPO
   Dayan P., 2005, THEORETICAL NEUROSCI
   Eichenbaum H, 2000, NAT REV NEUROSCI, V1, P41, DOI 10.1038/35036213
   Eichenbaum H, 1996, P NATL ACAD SCI USA, V93, P13500, DOI 10.1073/pnas.93.24.13500
   GEORGOPOULOS AP, 1986, SCIENCE, V233, P1416, DOI 10.1126/science.3749885
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Hebb D. O., 1949, ORG BEHAV A NEUROPSY
   Jazayeri M, 2006, NAT NEUROSCI, V9, P690, DOI 10.1038/nn1691
   Jensen O, 1996, LEARN MEMORY, V3, P243, DOI 10.1101/lm.3.2-3.243
   Kiani R., 2007, J NEUROPHYSIOL, V97
   Klin'shov V. V., 2005, Radiophysics and Quantum Electronics, V48, P203, DOI 10.1007/s11141-005-0061-2
   Lega BC, 2012, HIPPOCAMPUS, V22, P748, DOI 10.1002/hipo.20937
   Lin LN, 2006, TRENDS NEUROSCI, V29, P48, DOI 10.1016/j.tins.2005.11.004
   Lin LN, 2005, P NATL ACAD SCI USA, V102, P6125, DOI 10.1073/pnas.0408233102
   MCNAUGHTON BL, 1983, J NEUROSCI METH, V8, P391, DOI 10.1016/0165-0270(83)90097-3
   MEISTER M, 1995, SCIENCE, V270, P1207, DOI 10.1126/science.270.5239.1207
   NICOLELIS MAL, 1995, SCIENCE, V268, P1353, DOI 10.1126/science.7761855
   Oertel D, 1999, ANNU REV PHYSIOL, V61, P497, DOI 10.1146/annurev.physiol.61.1.497
   OKEEFE J, 1971, BRAIN RES, V34, P171, DOI 10.1016/0006-8993(71)90358-1
   OKEEFE J, 1993, HIPPOCAMPUS, V3, P317, DOI 10.1002/hipo.450030307
   Pan XC, 2006, BIOL CYBERN, V95, P159, DOI 10.1007/s00422-006-0074-8
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Sanger TD, 2003, CURR OPIN NEUROBIOL, V13, P238, DOI 10.1016/S0959-4388(03)00034-5
   Schrader S, 2009, NEURAL NETWORKS, V22, P1055, DOI 10.1016/j.neunet.2009.07.021
   Wehr M, 1996, NATURE, V384, P162, DOI 10.1038/384162a0
   Wyss R, 2003, P NATL ACAD SCI USA, V100, P324, DOI 10.1073/pnas.0136977100
NR 38
TC 0
Z9 0
U1 0
U2 1
PY 2013
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Shapero, S
   Zhu, MC
   Hasler, J
   Rozell, C
AF Shapero, Samuel
   Zhu, Mengchen
   Hasler, Jennifer
   Rozell, Christopher
TI OPTIMAL SPARSE APPROXIMATION WITH INTEGRATE AND FIRE NEURONS
SO INTERNATIONAL JOURNAL OF NEURAL SYSTEMS
DT Article
DE Sparse coding; spiking neural networks; locally competitive algorithm
ID DYNAMICAL-SYSTEM; SPIKING NEURONS; NEURAL-NETWORKS; TRANSMISSION;
   CONVERGENCE; MODELS; ANALOG; CODE
AB Sparse approximation is a hypothesized coding strategy where a population of sensory neurons (e. g. V1) encodes a stimulus using as few active neurons as possible. We present the Spiking LCA (locally competitive algorithm), a rate encoded Spiking Neural Network (SNN) of integrate and fire neurons that calculate sparse approximations. The Spiking LCA is designed to be equivalent to the nonspiking LCA, an analog dynamical system that converges on a l(1)-norm sparse approximations exponentially. We show that the firing rate of the Spiking LCA converges on the same solution as the analog LCA, with an error inversely proportional to the sampling time. We simulate in NEURON a network of 128 neuron pairs that encode 8x8 pixel image patches, demonstrating that the network converges to nearly optimal encodings within 20ms of biological time. We also show that when using more biophysically realistic parameters in the neurons, the gain function encourages additional l(0)-norm sparsity in the encoding, relative both to ideal neurons and digital solvers.
C1 [Shapero, Samuel] Georgia Tech, Res Inst, Elect Syst Lab, Atlanta, GA 30318 USA.
   [Zhu, Mengchen] Georgia Inst Technol, Atlanta, GA 30332 USA.
   [Hasler, Jennifer; Rozell, Christopher] Georgia Inst Technol, Atlanta, GA 30332 USA.
RP Rozell, C (corresponding author), Georgia Inst Technol, 777 Atlantic Dr NW, Atlanta, GA 30332 USA.
EM samuel.shapero@gtri.gatech.edu; mczhu@gatech.edu;
   phasler@ece.gatech.edu; crozell@gatech.edu
CR [Anonymous], 2012, 2012 INT JOINT C NEU, DOI [DOI 10.1109/IJCNN.2012.6252637, 10.1109/IJCNN.2012.6252637]
   Balavoine A, 2013, IEEE T SIGNAL PROCES, V61, P4259, DOI 10.1109/TSP.2013.2271482
   Balavoine A, 2012, IEEE T NEUR NET LEAR, V23, P1377, DOI 10.1109/TNNLS.2012.2202400
   Barlow H.B., 1961, SENS COMMUN, V1, P1, DOI DOI 10.7551/MITPRESS/9780262518420.003.0013
   Berkes P., 2009, ADV NEURAL INFORM PR, V22, P108
   Borghi A, 2013, J SIGNAL PROCESS SYS, V71, P1, DOI 10.1007/s11265-012-0671-9
   Bruderle Daniel, 2009, Front Neuroinform, V3, P17, DOI 10.3389/neuro.11.017.2009
   Candès EJ, 2008, J FOURIER ANAL APPL, V14, P877, DOI 10.1007/s00041-008-9045-x
   Carnevale T., 2006, NEURON BOOK, DOI DOI 10.1017/CBO9780511541612
   Charles AS, 2012, NEURAL COMPUT, V24, P3317, DOI 10.1162/NECO_a_00372
   Chen SSB, 2001, SIAM REV, V43, P129, DOI 10.1137/S1064827596304010
   Elad M, 2010, P IEEE, V98, P972, DOI 10.1109/JPROC.2009.2037655
   FOLDIAK P, 1995, HDB BRAIN THEORY NEU, P165
   Gao PR, 2012, IEEE T CIRCUITS-I, V59, P2383, DOI 10.1109/TCSI.2012.2188956
   Gerstner W., 2002, SPIKING NEURON MODEL
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Haider B, 2010, NEURON, V65, P107, DOI 10.1016/j.neuron.2009.12.005
   HOPFIELD JJ, 1984, P NATL ACAD SCI-BIOL, V81, P3088, DOI 10.1073/pnas.81.10.3088
   HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554
   HOPFIELD JJ, 1995, P NATL ACAD SCI USA, V92, P6655, DOI 10.1073/pnas.92.15.6655
   Hu T, 2012, NEURAL COMPUT, V24, P2852, DOI 10.1162/NECO_a_00353
   Kaiser F, 2007, LECT NOTES COMPUT SC, V4668, P380
   Kim SJ, 2007, IEEE J-STSP, V1, P606, DOI 10.1109/JSTSP.2007.910971
   Lustig M, 2007, MAGN RESON MED, V58, P1182, DOI 10.1002/mrm.21391
   Maass W, 1997, NETWORK-COMP NEURAL, V8, P355, DOI 10.1088/0954-898X/8/4/002
   Olshausen BA, 2004, CURR OPIN NEUROBIOL, V14, P481, DOI 10.1016/j.conb.2004.07.007
   Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7
   Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0
   Perrinet L, 2004, NEUROCOMPUTING, V57, P125, DOI 10.1016/j.neucom.2004.01.010
   Rehn M, 2007, J COMPUT NEUROSCI, V22, P135, DOI 10.1007/s10827-006-0003-9
   Rosselló JL, 2012, INT J NEURAL SYST, V22, DOI 10.1142/S0129065712500141
   Rozell CJ, 2008, NEURAL COMPUT, V20, P2526, DOI 10.1162/neco.2008.03-07-486
   Sachdev RNS, 2012, FRONT NEURAL CIRCUIT, V6, DOI 10.3389/fncir.2012.00043
   Sarpeshkar R, 1998, NEURAL COMPUT, V10, P1601, DOI 10.1162/089976698300017052
   Schemmel J, 2010, IEEE INT SYMP CIRC S, P1947, DOI 10.1109/ISCAS.2010.5536970
   Shapero S., 2011, COMP SYST NEUR COSYN
   Shapero S, 2013, NEURAL NETWORKS, V45, P134, DOI 10.1016/j.neunet.2013.03.012
   Shapero S, 2012, IEEE J EM SEL TOP C, V2, P530, DOI 10.1109/JETCAS.2012.2214615
   Shriki O, 2003, NEURAL COMPUT, V15, P1809, DOI 10.1162/08997660360675053
   Tkacik G, 2010, P NATL ACAD SCI USA, V107, P14419, DOI 10.1073/pnas.1004906107
   Tolhurst DJ, 2009, J NEUROSCI, V29, P2355, DOI 10.1523/JNEUROSCI.3869-08.2009
   TUCKWELL HC, 1979, J THEOR BIOL, V77, P65, DOI 10.1016/0022-5193(79)90138-3
   Vasanawala SS, 2010, RADIOLOGY, V256, P607, DOI 10.1148/radiol.10091218
   Vinje WE, 2002, J NEUROSCI, V22, P2904, DOI 10.1523/JNEUROSCI.22-07-02904.2002
   Wong WK, 2012, INT J NEURAL SYST, V22, DOI 10.1142/S0129065712500177
   Wright J, 2010, P IEEE, V98, P1031, DOI 10.1109/JPROC.2010.2044470
   Zhu MC, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003191
   Zylberberg J, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003182
   Zylberberg J, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1002250
NR 49
TC 35
Z9 36
U1 2
U2 21
PD AUG
PY 2014
VL 24
IS 5
SI SI
AR 1440001
DI 10.1142/S0129065714400012
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT C
AU Zjajo, A
   Kumar, S
   van Leuken, R
AF Zjajo, Amir
   Kumar, Sumeet
   van Leuken, Rene
GP IEEE
TI Neuromorphic Spike Data Classifier for Reconfigurable Brain-Machine
   Interface
SO 2017 8TH INTERNATIONAL IEEE/EMBS CONFERENCE ON NEURAL ENGINEERING (NER)
SE International IEEE EMBS Conference on Neural Engineering
DT Proceedings Paper
CT 8th International IEEE/EMBS Conference on Neural Engineering (NER)
CY MAY 25-28, 2017
CL Shanghai, PEOPLES R CHINA
AB In this paper, we propose a reconfigurable neural spike classifier based on neuromorphic event-based networks that can be directly interfaced to neural signal conditioning and quantization circuits. The classifier is set as a heterogeneity based, multi-layer computational network to offer wide flexibility in the implementation of plastic and metaplastic interactions, and to increase efficacy in neural signal processing. Built-in temporal control mechanisms allow the implementation of homeostatic regulation in the resulting network. The results obtained in a 90 nm CMOS technology show that an efficient neural spike data classification can be obtained with a low power (9.4 mu W/core) and compact (0.54 mm 2 per core) structure.
C1 [Zjajo, Amir; Kumar, Sumeet; van Leuken, Rene] Delft Univ Technol, Circuits & Syst Grp, Mekelweg 4, NL-2628 CD Delft, Netherlands.
RP Zjajo, A (corresponding author), Delft Univ Technol, Circuits & Syst Grp, Mekelweg 4, NL-2628 CD Delft, Netherlands.
EM amir.zjajo@ieee.org
CR Ambard M, 2012, FRONT COMPUT NEUROSC, V6, DOI 10.3389/fncom.2012.00078
   [Anonymous], 1990, MAXIMUM ENTROPY MODE
   Bartolozzi C, 2007, NEURAL COMPUT, V19, P2581, DOI 10.1162/neco.2007.19.10.2581
   Corradi F., IEEE T BIOM IN PRESS
   Diorio C., 1997, THESIS
   Froemke RC, 2002, NATURE, V416, P433, DOI 10.1038/416433a
   Hussain S, 2016, IEEE INT SYMP CIRC S, P2527, DOI 10.1109/ISCAS.2016.7539107
   Jiang Z., 2015, INT J ADV MANUF TECH, P1
   Karkare V, 2013, IEEE J SOLID-ST CIRC, V48, P2230, DOI 10.1109/JSSC.2013.2264616
   Lee KH, 2013, IEEE J SOLID-ST CIRC, V48, P1625, DOI 10.1109/JSSC.2013.2253226
   Merolla P, 2004, ADV NEUR IN, V16, P995
   Segev I, 2000, SCIENCE, V290, P744, DOI 10.1126/science.290.5492.744
   Tamosiunaite M, 2007, J COMPUT NEUROSCI, V23, P113, DOI 10.1007/s10827-007-0021-2
   Tsung-Chuan Ma, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P3889, DOI 10.1109/ICASSP.2014.6854330
   Wang YX, 2011, IEEE T CIRCUITS-I, V58, P2159, DOI 10.1109/TCSI.2011.2112570
   Williams SR, 2003, TRENDS NEUROSCI, V26, P147, DOI 10.1016/S0166-2236(03)00035-3
   Zjajo A, 2016, 2016 3RD IEEE EMBS INTERNATIONAL CONFERENCE ON BIOMEDICAL AND HEALTH INFORMATICS, P489, DOI 10.1109/BHI.2016.7455941
NR 17
TC 2
Z9 2
U1 0
U2 0
PY 2017
BP 150
EP 153
WC Engineering, Biomedical; Neurosciences
DA 2023-11-11
ER

PT C
AU Chernetchenko, DV
   Romaniuk, RS
   Sawicki, D
   Yusupova, G
AF Chernetchenko, Dmytro V.
   Romaniuk, Ryszard S.
   Sawicki, Daniel
   Yusupova, Gulbahar
BE Romaniuk, RS
   Linczuk, M
TI Analysis of electrical patterns activity in artificial multi-stable
   neural networks
SO PHOTONICS APPLICATIONS IN ASTRONOMY, COMMUNICATIONS, INDUSTRY, AND
   HIGH-ENERGY PHYSICS EXPERIMENTS 2019
SE Proceedings of SPIE
DT Proceedings Paper
CT 44th WILGA Symposium on Photonics Applications and Web Engineering
CY MAY 26-JUN 02, 2019
CL Wilga, POLAND
DE spiking neurons models; spiking artificial neural networks; multi-stable
   spiking neurons; signal processing; FPGA; patterns classification
   methods
ID SPIKING NEURONS; ARCHITECTURE; SYSTEM; MODEL
AB An improved mathematical model of artificial neuron with active generation of action potentials was developed, and the behavior of an artificial network consisting of thousands of described neurons was investigated. The passive part of the neuron consists of soma and asymmetric dendritic branches that provide multi-stability. The active component of the neuron is described with the help of a simplified non-linear neuron model with mechanisms for the spiking generation. The presented model reproduces all types of electrical generations of known biological neurons, e.g. neocortical. The model combines the biological similarity of the Hodgkin-Huxley type dynamics and the computational efficiency of integrative-spiking neurons. It is shown that switching between different modes of generation is possible under the condition of structural three-stability of the neuron in common. A neural network consisting of multi-stable neurons is capable of generating synchronous regular spikes if all neurons in the network are in a similar electrical state. In the case where a part of the neurons at non-similar stable condition, the network generates asynchronous regular spikes, without adding any synaptic plasticity mechanisms or modulating stimulation processes. The obtained model can be used for studying the features of real-time data processing by artificial neural networks, which can be used for such modern tasks as recognition and classification of biophysical signal patterns or for the development of elements of artificial intelligence.
C1 [Chernetchenko, Dmytro V.] Oles Honchar Dnipro Natl Univ, Dnipro, Ukraine.
   [Romaniuk, Ryszard S.] Warsaw Univ Technol, Warsaw, Poland.
   [Sawicki, Daniel] Lublin Univ Technol, Lublin, Poland.
   [Yusupova, Gulbahar] Turan Univ, Alma Ata, Kazakhstan.
RP Chernetchenko, DV (corresponding author), Oles Honchar Dnipro Natl Univ, Dnipro, Ukraine.
EM rakon3@gmail.com
CR Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   Alaburda A, 2001, BIOFIZIKA+, V46, P337
   BINAS J, 2015, IEEE INT
   Borges R., 2015, COMMUNICATIONS NONLI, V34, P12
   BRODIN L, 1991, J NEUROPHYSIOL, V66, P473, DOI 10.1152/jn.1991.66.2.473
   Buonomano DV, 1999, NEURAL COMPUT, V11, P103, DOI 10.1162/089976699300016836
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Diehl Peter U, 2015, 2015 INT JOINT C NEU, P1, DOI [DOI 10.1109/IJCNN.2015.7280696, 10.1109/IJCNN.2015.7280696]
   Hines ML, 2004, NEUROCOMPUTING, V58, P1117, DOI 10.1016/j.neucom.2004.01.175
   Hsieh HY, 2012, IEEE T NEUR NET LEAR, V23, P1065, DOI 10.1109/TNNLS.2012.2195329
   Iannella N, 2001, NEURAL NETWORKS, V14, P933, DOI 10.1016/S0893-6080(01)00080-6
   Indiveri G, 2006, IEEE T NEURAL NETWOR, V17, P211, DOI 10.1109/TNN.2005.860850
   Kasabov N, 2013, NEURAL NETWORKS, V41, P188, DOI 10.1016/j.neunet.2012.11.014
   Kasabov NK, 2014, NEURAL NETWORKS, V52, P62, DOI 10.1016/j.neunet.2014.01.006
   Koprowski R, 2012, BIOMED ENG ONLINE, V11, DOI 10.1186/1475-925X-11-48
   Korogod SM, 2008, NEUROPHYSIOLOGY+, V40, P412, DOI 10.1007/s11062-009-9067-2
   Kotyra A, 2012, PRZ ELEKTROTECHNICZN, V88, P295
   Krak Yu V., 2017, Journal of Automation and Information Sciences, V49, P65
   Kryvonos IG, 2017, CYBERN SYST ANAL+, V53, P495, DOI 10.1007/s10559-017-9951-5
   Kvyetnyy R., 2017, P SPIE
   Kvyetnyy R, 2015, PROC SPIE, V9816, DOI 10.1117/12.2229103
   Maciejewski M, 2014, ANN AGR ENV MED, V21, P167
   Omiotek Z, 2013, EXPERT SYST APPL, V40, P6684, DOI 10.1016/j.eswa.2013.03.022
   Omiotek Z, 2017, P I MECH ENG H, V231, P774, DOI 10.1177/0954411917702682
   Romanyuk ON, 2015, PROC SPIE, V9816, DOI 10.1117/12.2229013
   Romanyuk S., 2015, CONTROL COMMUNICATIO
   Samadi A, 2017, NEURAL COMPUT, V29, P578, DOI 10.1162/NECO_a_00929
   Smolarz A, 2012, PRZ ELEKTROTECHNICZN, V88, P259
   Snezhko E., 2012, VESTNIK DNU
   Svirskis G, 1998, BIOFIZIKA+, V43, P87
   Wójcik W, 2004, P SOC PHOTO-OPT INS, V5484, P427, DOI 10.1117/12.569041
NR 31
TC 0
Z9 0
U1 0
U2 2
PY 2019
VL 11176
AR 111761R
DI 10.1117/12.2536462
WC Optics
DA 2023-11-11
ER

PT C
AU Lin, XC
   Shen, JR
   Wen, J
   Tang, HJ
AF Lin, Xiaocui
   Shen, Jiangrong
   Wen, Jun
   Tang, Huajin
GP IEEE
TI Bipolar Population Threshold Encoding for Audio Recognition with Deep
   Spiking Neural Networks
SO 2023 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS, IJCNN
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUN 18-23, 2023
CL Broadbeach, AUSTRALIA
DE spiking neural network; neural encoding; audio recognition;
   spatio-temporal
AB Spiking Neural Networks (SNNs) have been increasingly investigated for audio recognition due to the low power consumption on neuromorphic hardware by mimicking biological neural systems. Since the SNNs are learned from spikes, a critical step lies in the efficient neural encoding of real-valued sound signals to represent complex temporal patterns in speech and environmental sounds. In this paper, we propose a novel Bipolar Population Threshold (BPT) encoding model that effectively captures the trajectory information of time-series speech data by combining temporal and spatial dimensions. The bipolar encoding technique uses positive and negative neurons to capture the dynamic changes in the audio signal, while the threshold intervals allow for a sparse representation that focuses on encoding significant changes, resulting in an efficient and simplified recognition process. Extensively experimenting on three benchmark datasets including the TIDIGITS with speeches, RWCP with sounds, and MedleyDB with music, the numeric results show the superiority of the proposed method by consistently outperforming the state-of-the-art approaches while with fewer spikes, especially in capturing the complex spatiotemporal patterns of audio signals.
C1 [Lin, Xiaocui; Shen, Jiangrong; Tang, Huajin] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou, Peoples R China.
   [Wen, Jun] Harvard Med Sch, Dept Biomed Informat, Boston, MA 02115 USA.
RP Tang, HJ (corresponding author), Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou, Peoples R China.
EM xlin_ai@zju.edu.cn; jiangrongshen@zju.edu.cn; jun_wen@hms.harvard.edu;
   htang@zju.edu.cn
CR Almomani A, 2019, CLUSTER COMPUT, V22, P419, DOI 10.1007/s10586-018-02891-0
   Bear MF, 2007, NEUROSCIENCE EXPLORI
   Bittner R. M., 2014, P 15 INT SOC MUSIC I, P155
   Chen Y., 2022, INT C MACH LEARN ICM, P3701
   Comsa I.M., 2020, ICASSP 2020 2020 IEE, DOI DOI 10.1109/ICASSP40776.2020.9053856
   Cooke M, 2001, SPEECH COMMUN, V34, P267, DOI 10.1016/S0167-6393(00)00034-0
   Dean I, 2005, NAT NEUROSCI, V8, P1684, DOI 10.1038/nn1541
   Dennis J, 2013, INT CONF ACOUST SPEE, P803, DOI 10.1109/ICASSP.2013.6637759
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Diehl Peter U, 2015, 2015 INT JOINT C NEU, P1, DOI [DOI 10.1109/IJCNN.2015.7280696, 10.1109/IJCNN.2015.7280696]
   Eurich CW, 2000, NEURAL COMPUT, V12, P1519, DOI 10.1162/089976600300015240
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   GILBERT CD, 1992, NATURE, V356, P150, DOI 10.1038/356150a0
   Gu PJ, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1366
   Gütig R, 2016, SCIENCE, V351, DOI 10.1126/science.aab4113
   Hopfield JJ, 2004, P NATL ACAD SCI USA, V101, P6255, DOI 10.1073/pnas.0401125101
   Hopfield JJ, 2001, P NATL ACAD SCI USA, V98, P1282, DOI 10.1073/pnas.031567098
   Hopfield JJ, 1996, P NATL ACAD SCI USA, V93, P15440, DOI 10.1073/pnas.93.26.15440
   Jin YYZ, 2018, ADV NEUR IN, V31
   Kheradpisheh SR, 2020, INT J NEURAL SYST, V30, DOI 10.1142/S0129065720500276
   Lee JM, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00191
   Leonard R.G., 1993, TIDIGITS SPEECH CORP
   Loiselle S, 2005, IEEE IJCNN, P2076
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Nadasdy Z, 2009, FRONT SYST NEUROSCI, V3, DOI 10.3389/neuro.06.006.2009
   Nakamura Satoshi, 2000, ACOUSTICAL SOUND DAT
   Neil D., 2016, P 31 ANN ACM S APPL, P293
   Pan ZH, 2020, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.01420
   Pons J, 2017, EUR SIGNAL PR CONF, P2744, DOI 10.23919/EUSIPCO.2017.8081710
   Qi Y, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1597
   Samadi A, 2017, NEURAL COMPUT, V29, P578, DOI 10.1162/NECO_a_00929
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Shen J, 2022, IEEE T DEPEND SECURE, V19, P2198, DOI 10.1109/TDSC.2021.3050517
   Shen JR, 2021, NEURAL COMPUT, V33, P2971, DOI 10.1162/neco_a_01432
   Snippe HP, 1996, NEURAL COMPUT, V8, P511, DOI 10.1162/neco.1996.8.3.511
   Tamazin M, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9102166
   Tavanaei A, 2017, LECT NOTES COMPUT SC, V10639, P899, DOI 10.1007/978-3-319-70136-3_95
   Wu JC, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351221
   Wu JB, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00836
   Xu Q, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1646
   Xu QS, 2023, IEEE T PATTERN ANAL, V45, P4945, DOI 10.1109/TPAMI.2022.3200074
   Yang F, 2016, INT CONF SIGN PROCES, P584, DOI 10.1109/ICSP.2016.7877900
   Yu Q, 2019, IEEE T CYBERNETICS, V49, P2178, DOI 10.1109/TCYB.2018.2821692
   Zhang ML, 2022, IEEE T NEUR NET LEAR, V33, P1947, DOI 10.1109/TNNLS.2021.3110991
   Zhang Y, 2015, IEEE T NEUR NET LEAR, V26, P2635, DOI 10.1109/TNNLS.2015.2388544
NR 45
TC 0
Z9 0
U1 0
U2 0
PY 2023
DI 10.1109/IJCNN54540.2023.10191235
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Bibi, A
   Xu, F
   Adorna, HN
   Cabarle, FGC
AF Bibi, Alia
   Xu, Fei
   Adorna, Henry N.
   Cabarle, Francis George C.
TI Sequential Spiking Neural P Systems with Local Scheduled Synapses
   without Delay
SO COMPLEXITY
DT Article
ID RULES; WORKING
AB Spiking neural P systems with scheduled synapses are a class of distributed and parallel computational models motivated by the structural dynamism of biological synapses by incorporating ideas from nonstatic (i.e., dynamic) graphs and networks. In this work, we consider the family of spiking neural P systems with scheduled synapses working in the sequential mode: at each step the neuron(s) with the maximum/minimum number of spikes among the neurons that can spike will fire. The computational power of spiking neural P systems with scheduled synapses working in the sequential mode is investigated. Specifically, the universality (Turing equivalence) of such systems is obtained.
C1 [Bibi, Alia; Xu, Fei] Huazhong Univ Sci & Technol, Sch Automat, Educ Minist China, Key Lab Image Informat Proc & Intelligent Control, Wuhan 430074, Hubei, Peoples R China.
   [Bibi, Alia] Govt Girls Postgrad Coll, Kohat 26000, Khyber Pakhtunk, Pakistan.
   [Adorna, Henry N.; Cabarle, Francis George C.] Univ Philippines Diliman, Dept Comp Sci, Quezon City, Philippines.
   [Cabarle, Francis George C.] Xiamen Univ, Sch Informat Sci & Engn, Xiamen 361005, Peoples R China.
RP Xu, F (corresponding author), Huazhong Univ Sci & Technol, Sch Automat, Educ Minist China, Key Lab Image Informat Proc & Intelligent Control, Wuhan 430074, Hubei, Peoples R China.
EM fei_xu@hust.edu.cn
CR [Anonymous], 2018, IEEE T BIG DATA EARL
   Barbuti R, 2010, J LOGIC ALGEBR PROGR, V79, P304, DOI 10.1016/j.jlap.2010.03.011
   Cabarle FGC, 2017, IEEE T NANOBIOSCI, V16, P792, DOI 10.1109/TNB.2017.2762580
   Cabarle FGC, 2016, NEURAL COMPUT APPL, V27, P1337, DOI 10.1007/s00521-015-1937-5
   Cabarle FGC, 2015, NEURAL COMPUT APPL, V26, P1905, DOI 10.1007/s00521-015-1857-4
   Carandang J. P., 2017, ROMANIAN JOURNAL OF, V20, P57
   CHEN H, 2002, FUNDAMENTA INFORMATI, V75, P141
   Chen Haiming, 2008, Natural Computing, V7, P147, DOI 10.1007/s11047-006-9024-6
   Garca Arnau M., 2007, INT J UNCONV COMPUT, V5, P411
   Gexiang Zhang., 2017, REAL LIFE APPL MEMBR
   Ibarra OH, 2009, THEOR COMPUT SCI, V410, P2982, DOI 10.1016/j.tcs.2009.03.004
   Ibarra Oscar H., 2008, Natural Computing, V7, P3, DOI 10.1007/s11047-007-9043-y
   IONESCU M, FUNDAMENTA INFORMATI, V71, P279
   Jiang KQ, 2013, THEOR COMPUT SCI, V499, P88, DOI 10.1016/j.tcs.2013.07.006
   MINSKY M, 1967, COMPUTATION FINITE I
   Pan LQ, 2011, SCI CHINA INFORM SCI, V54, P1596, DOI 10.1007/s11432-011-4303-y
   Paun A, 2007, BIOSYSTEMS, V90, P48, DOI 10.1016/j.biosystems.2006.06.006
   Peng H, 2019, KNOWL-BASED SYST, V163, P875, DOI 10.1016/j.knosys.2018.10.016
   Peng H, 2017, NEURAL NETWORKS, V95, P66, DOI 10.1016/j.neunet.2017.08.003
   Peng H, 2013, INFORM SCIENCES, V235, P106, DOI 10.1016/j.ins.2012.07.015
   Pun G, 2010, OXFORD HDB MEMBRANE
   Pun Gh, 2012, MEMBRANE COMPUTING I
   Song T, 2018, IEEE T COGN DEV SYST, V10, P1106, DOI 10.1109/TCDS.2017.2785332
   Song T, 2015, IEEE T NANOBIOSCI, V14, P465, DOI 10.1109/TNB.2015.2402311
   Song T, 2015, IEEE T NANOBIOSCI, V14, P38, DOI 10.1109/TNB.2014.2367506
   Song T, 2014, THEOR COMPUT SCI, V529, P82, DOI 10.1016/j.tcs.2014.01.001
   Wang J, 2013, IEEE T FUZZY SYST, V21, P209, DOI 10.1109/TFUZZ.2012.2208974
   Wang J, 2010, NEURAL COMPUT, V22, P2615, DOI 10.1162/NECO_a_00022
   Wang T, 2015, IEEE T POWER SYST, V30, P1182, DOI 10.1109/TPWRS.2014.2347699
   Wu TF, 2016, THEOR COMPUT SCI, V623, P180, DOI 10.1016/j.tcs.2015.12.038
   Zhang GX, 2014, INT J NEURAL SYST, V24, DOI 10.1142/S0129065714400061
   Zhang XY, 2015, IEEE T NEUR NET LEAR, V26, P2816, DOI 10.1109/TNNLS.2015.2396940
NR 32
TC 8
Z9 8
U1 2
U2 20
PY 2019
AR 7313414
DI 10.1155/2019/7313414
WC Mathematics, Interdisciplinary Applications; Multidisciplinary Sciences
DA 2023-11-11
ER

PT C
AU Panchapakesan, S
   Fang, ZM
   Chandrachoodan, N
AF Panchapakesan, Sathish
   Fang, Zhenman
   Chandrachoodan, Nitin
GP IEEE Comp Soc
TI EASpiNN: Effective Automated Spiking Neural Network Evaluation on FPGA
SO 28TH IEEE INTERNATIONAL SYMPOSIUM ON FIELD-PROGRAMMABLE CUSTOM COMPUTING
   MACHINES (FCCM)
SE Annual IEEE Symposium on Field-Programmable Custom Computing Machines
DT Proceedings Paper
CT 28th IEEE International Symposium on Field-Programmable Custom Computing
   Machines (FCCM)
CY MAY 03-06, 2020
CL Fayetteville, AR
C1 [Panchapakesan, Sathish; Fang, Zhenman] Simon Fraser Univ, Sch Engn Sci, Burnaby, BC, Canada.
   [Chandrachoodan, Nitin] Indian Inst Technol Madras, Dept Elect Engn, Madras, Tamil Nadu, India.
RP Panchapakesan, S (corresponding author), Simon Fraser Univ, Sch Engn Sci, Burnaby, BC, Canada.
EM sathishp@sfu.ca; zhenman@sfu.ca; nitin@ee.iitm.ac.in
CR Cong J, 2018, ANN IEEE SYM FIELD P, P93, DOI 10.1109/FCCM.2018.00023
   Diehl PU, 2015, IEEE IJCNN
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
NR 3
TC 3
Z9 3
U1 0
U2 2
PY 2020
BP 242
EP 242
DI 10.1109/FCCM48280.2020.00075
WC Computer Science, Hardware & Architecture
DA 2023-11-11
ER

PT J
AU Li, XM
   Liu, H
   Xue, FZ
   Zhou, HJ
   Song, YD
AF Li, Xiumin
   Liu, Hui
   Xue, Fangzheng
   Zhou, Hongjun
   Song, Yongduan
TI Liquid computing of spiking neural network with multi-clustered and
   active-neuron-dominant structure
SO NEUROCOMPUTING
DT Article
DE spiking neural network; liquid computing; multi-cluster; STDP
ID CORTICAL NETWORKS; STATE MACHINES; PERFORMANCE; PLASTICITY;
   SYNCHRONIZATION; CLASSIFICATION; CONNECTIVITY; COMPUTATION; PREDICTION;
   MODELS
AB Liquid computing is an effective approach to intelligent computations of neural networks, especially for spiking neural networks. If the liquid network is embedded with a proper structure it can perform complex computational tasks. However, the modeling of self-organized neural networks with more biological characteristics is still an important open challenge, resulting in major constraints on improving the computational capability and dynamical diversity of the model. Here, we present a novel type of liquid computing model with both multi-clustered and active-neuron-dominant structure of spiking neural network, instead of the traditional random structure. The optimal parameter settings of the cluster number and time window size of clustering generation method had been considered. The synaptic weights in each cluster are further refined through the spike-timing-dependent plasticity rule to obtain an active neuron-dominant structure. The results show that this model has much better performance on liquid computing than the random model. The enhancement of information processing capability is achieved by improving two aspects, i.e. the activity synchrony and network sensitivity, based on the clustered structure and active-neuron-dominant structure, respectively. Statistical analysis demonstrates that both structure entropy and activity entropy of our proposed network are increased, indicating its high topological complexity and dynamical diversity. Therefore, the improvement in efficiency of signal transmission of this network is confirmed. (C) 2017 Elsevier B.V. All rights reserved.
C1 [Li, Xiumin; Liu, Hui; Xue, Fangzheng; Song, Yongduan] Chongqing Univ, Minist Educ, Key Lab Dependable Serv Comp Cyber Phys Soc, Chongqing 400044, Peoples R China.
   [Li, Xiumin; Liu, Hui; Xue, Fangzheng; Song, Yongduan] Chongqing Univ, Coll Automat, Chongqing 400044, Peoples R China.
   [Zhou, Hongjun] Chongqing Univ, Sch Econ & Business Adm, Chongqing 400044, Peoples R China.
RP Li, XM (corresponding author), Chongqing Univ, Coll Automat, Chongqing 400044, Peoples R China.
EM xmli@cqu.edu.cn
CR [Anonymous], 2014, CONTROLLING RECURREN
   Caporale N, 2008, ANNU REV NEUROSCI, V31, P25, DOI 10.1146/annurev.neuro.31.060407.125639
   Galla T., 2006, J PHYS A, V39, P3849
   Gerstner W., 2002, SPIKING NEURON MODEL
   Hausler S., 2003, Complexity, V8, P39, DOI 10.1002/cplx.10089
   Hazan H, 2012, EXPERT SYST APPL, V39, P1597, DOI 10.1016/j.eswa.2011.06.052
   Hourdakis E, 2013, NEUROCOMPUTING, V107, P40, DOI 10.1016/j.neucom.2012.07.032
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Jaeger H., 2001, GMD REPORT 148
   Ju H, 2013, NEURAL NETWORKS, V38, P39, DOI 10.1016/j.neunet.2012.11.003
   Kaiser M, 2007, NEW J PHYS, V9, DOI 10.1088/1367-2630/9/5/110
   Kaiser M, 2007, NEUROCOMPUTING, V70, P1829, DOI 10.1016/j.neucom.2006.10.060
   Kappel D, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003511
   Klampfl S, 2013, J NEUROSCI, V33, P11515, DOI 10.1523/JNEUROSCI.5044-12.2013
   Lazar A, 2007, NEURAL NETWORKS, V20, P312, DOI 10.1016/j.neunet.2007.04.020
   Lazar A, 2009, FRONT COMPUT NEUROSC, V3, DOI 10.3389/neuro.10.023.2009
   Legenstein R, 2007, NEURAL NETWORKS, V20, P323, DOI 10.1016/j.neunet.2007.04.017
   Li XM, 2010, NEW J PHYS, V12, DOI 10.1088/1367-2630/12/8/083045
   Li XM, 2009, CHAOS, V19, DOI 10.1063/1.3076394
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Maass W, 2004, J COMPUT SYST SCI, V69, P593, DOI 10.1016/j jcss.2004.04.001
   Maass W., 2015, COMPUTABILITY CONTEX, P275
   Maass W, 2007, PLOS COMPUT BIOL, V3, P15, DOI 10.1371/journal.pcbi.0020165
   Maass W, 2007, LECT NOTES COMPUT SC, V4497, P507
   Maass W, 2014, P IEEE, V102, P860, DOI 10.1109/JPROC.2014.2310593
   Markram Henry, 2011, Front Synaptic Neurosci, V3, P4, DOI 10.3389/fnsyn.2011.00004
   Matser J., 2011, STRUCTURED LIQUIDS L
   McDonnell MD, 2011, NAT REV NEUROSCI, V12, P415, DOI 10.1038/nrn3061
   NATSCHLAGER T, 2003, NEUROSCIENCE DATABAS, P123, DOI DOI 10.1007/978-1-4615-1079-6_9
   Nessler B, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003037
   Norton D, 2010, NEUROCOMPUTING, V73, P2893, DOI 10.1016/j.neucom.2010.08.005
   Pipa G., EXTENDED LIQUID COMP
   Rhéaume F, 2011, NEUROCOMPUTING, V74, P2842, DOI 10.1016/j.neucom.2011.03.033
   Shew WL, 2011, J NEUROSCI, V31, P55, DOI 10.1523/JNEUROSCI.4637-10.2011
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Wang QY, 2007, NEW J PHYS, V9, DOI 10.1088/1367-2630/9/10/383
   Wang SJ, 2012, NEW J PHYS, V14, DOI 10.1088/1367-2630/14/2/023005
   Zamora-López G, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00083
   Zhou CS, 2007, NEW J PHYS, V9, DOI 10.1088/1367-2630/9/6/178
NR 39
TC 7
Z9 8
U1 1
U2 32
PD JUN 21
PY 2017
VL 243
BP 155
EP 165
DI 10.1016/j.neucom.2017.03.022
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT C
AU Liu, SS
   Sock, NM
   Ching, SN
AF Liu, Sensen
   Sock, Noah M.
   Ching, ShiNung
GP IEEE
TI Learning-based Approaches for Controlling Neural Spiking
SO 2018 ANNUAL AMERICAN CONTROL CONFERENCE (ACC)
SE Proceedings of the American Control Conference
DT Proceedings Paper
CT American Control Conference
CY JUN 27-29, 2018
CL Milwaukee, WI
ID EXCITATION
AB We consider the problem of controlling populations of interconnected neurons using extrinsic stimulation. Such a problem, which is relevant to applications in both basic neuroscience as well as brain medicine, is challenging due to the nonlinearity of neuronal dynamics and the highly unpredictable structure of underlying neuronal networks. Compounding this difficulty is the fact that most neurostimulation technologies offer a single degree of freedom to actuate tens to hundreds of interconnected neurons. To meet these challenges, here we consider an adaptive, learning-based approach to controlling neural spike trains. Rather than explicitly modeling neural dynamics and designing optimal controls, we instead synthesize a so-called control network (CONET) that interacts with the spiking network by maximizing the Shannon mutual information between it and the realized spiking outputs. Thus, the CONET learns a representation of the spiking network that subsequently allows it to learn suitable control signals through a reinforcement-type mechanism. We demonstrate feasibility of the approach by controlling networks of stochastic spiking neurons, wherein desired patterns are induced for neuron-toactuator ratios in excess of 10 to 1.
C1 [Liu, Sensen; Ching, ShiNung] Washington Univ, Dept Elect & Syst Engn, St Louis, MO 63110 USA.
   [Ching, ShiNung] Washington Univ, Dept Biomed Engn, St Louis, MO 63110 USA.
   [Ching, ShiNung] Washington Univ, Div Biol & Biomed Sci, St Louis, MO 63110 USA.
   [Sock, Noah M.] CALTECH, Pasadena, CA 91125 USA.
RP Ching, SN (corresponding author), Washington Univ, Dept Elect & Syst Engn, St Louis, MO 63110 USA.
EM shinung@ese.wustl.edu
CR Ahmadian Y, 2011, J NEUROPHYSIOL, V106, P1038, DOI 10.1152/jn.00427.2010
   [Anonymous], 1997, NEURAL SYSTEMS CONTR
   [Anonymous], 2010, ALGORITHMS REINFORCE
   Ching SN, 2013, FRONT NEURAL CIRCUIT, V7, DOI 10.3389/fncir.2013.00054
   Cooper L., 2004, THEORY CORTICAL PLAS
   Dasanayake I, 2011, PHYS REV E, V83, DOI 10.1103/PhysRevE.83.061916
   Dayan P., 2005, THEORETICAL NEUROSCI
   FRANCIS BA, 1976, AUTOMATICA, V12, P457, DOI 10.1016/0005-1098(76)90006-6
   Gosavi A, 2009, INFORMS J COMPUT, V21, P178, DOI 10.1287/ijoc.1080.0305
   Grondman I, 2012, IEEE T SYST MAN CY C, V42, P1291, DOI 10.1109/TSMCC.2012.2218595
   Grosenick L, 2015, NEURON, V86, P106, DOI 10.1016/j.neuron.2015.03.034
   Haider B, 2006, J NEUROSCI, V26, P4535, DOI 10.1523/JNEUROSCI.5297-05.2006
   Kaelbling LP, 1996, J ARTIF INTELL RES, V4, P237, DOI 10.1613/jair.301
   Konda VR, 2003, SIAM J CONTROL OPTIM, V42, P1143, DOI 10.1137/S0363012901385691
   Liu SS, 2017, NEURAL COMPUT, V29, P2528, DOI [10.1162/neco_a_00993, 10.1162/NECO_a_00993]
   Nandi A., 2017, IEEE T CONTROL NETW, P1
   Nandi A, 2017, P AMER CONTR CONF, P2811, DOI 10.23919/ACC.2017.7963377
   Ritt JT, 2015, P AMER CONTR CONF, P3765, DOI 10.1109/ACC.2015.7171915
   Shu YS, 2003, NATURE, V423, P288, DOI 10.1038/nature01616
   Vogels TP, 2011, SCIENCE, V334, P1569, DOI 10.1126/science.1211095
   Wilson D, 2014, SIAM J APPL DYN SYST, V13, P276, DOI 10.1137/120901702
NR 21
TC 1
Z9 1
U1 0
U2 0
PY 2018
BP 2827
EP 2832
WC Automation & Control Systems; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT S
AU Petrovici, MA
AF Petrovici, Mihai Alexandru
BA Petrovici, MA
BF Petrovici, MA
TI Probabilistic Inference in Neural Networks
SO FORM VERSUS FUNCTION: THEORY AND MODELS FOR NEURONAL SUBSTRATES
SE Springer Theses-Recognizing Outstanding PhD Research
DT Article; Book Chapter
ID SPIKING NEURONS; MODEL; RELIABILITY
C1 [Petrovici, Mihai Alexandru] Heidelberg Univ, Kirchhoff Inst Phys, Dept Elect Vis, Heidelberg, Germany.
RP Petrovici, MA (corresponding author), Heidelberg Univ, Kirchhoff Inst Phys, Dept Elect Vis, Heidelberg, Germany.
CR [Anonymous], 2009, AISTATS
   [Anonymous], 1998, MNIST DATABASE HANDW
   [Anonymous], 2005, ADV NEURAL INF PROCE
   Azouz R, 1999, J NEUROSCI, V19, P2209
   Bengio Y, 2013, ARXIV13116184
   Berkes P, 2011, SCIENCE, V331, P83, DOI 10.1126/science.1195870
   Bill J, 2010, FRONT COMPUT NEUROSC, V4, DOI 10.3389/fncom.2010.00129
   Bishop C. M., 2009, PATTERN RECOGNITION, V1
   Brascamp JW, 2006, J VISION, V6, P1244, DOI 10.1167/6.11.8
   Breitwieser O., 2015, THESIS RUPRECHT KARL
   Brette R, 2007, J COMPUT NEUROSCI, V23, P349, DOI 10.1007/s10827-007-0038-6
   Bruderle D., 2010, SIMULATOR EXPLORATIO
   Brunel N, 1998, J THEOR BIOL, V195, P87, DOI 10.1006/jtbi.1998.0782
   Brunel N, 2000, J COMPUT NEUROSCI, V8, P183, DOI 10.1023/A:1008925309027
   Buesing L, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1002211
   Burkitt AN, 2006, BIOL CYBERN, V95, P97, DOI 10.1007/s00422-006-0082-8
   Chow CC, 1996, BIOPHYS J, V71, P3013, DOI 10.1016/S0006-3495(96)79494-8
   Ciresan D, 2012, NEURAL NETWORKS, V32, P333, DOI 10.1016/j.neunet.2012.02.023
   Ciresan D, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P1918, DOI 10.1109/IJCNN.2011.6033458
   Deneve S, 2008, NEURAL COMPUT, V20, P91, DOI 10.1162/neco.2008.20.1.91
   Destexhe A, 2003, NAT REV NEUROSCI, V4, P739, DOI 10.1038/nrn1198
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Habenschuss S., 2012, ADV NEURAL INF PROCE, V25
   Hinton G., 1986, LEARNING RELEARNING
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hinton Geoffrey E., 2012, NEURAL NETWORKS TRIC, P599, DOI [DOI 10.1007/978-3-642-35289-8_32, 10.1007/978-3-642-35289-8_32]
   HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554
   Ide JS, 2002, LECT NOTES ARTIF INT, V2507, P366
   Indiveri G, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00073
   Jaeger H., 2001, GMD REPORT 148
   Jeltsch S., 2014, THESIS U HEIDELBERG
   Jordan J., 2014, NEURAL NETWORKS SOUR
   Kappel D, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003511
   KNILL DC, 1991, NATURE, V351, P228, DOI 10.1038/351228a0
   Korcsak-Gorzo T., 2015, FIRING STATES RECURR
   Körding KP, 2004, NATURE, V427, P244, DOI 10.1038/nature02169
   Lansky P, 1997, PHYS REV E, V55, P2040, DOI 10.1103/PhysRevE.55.2040
   Leng L., 2014, THESIS RUPRECHT KARL
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   MAINEN ZF, 1995, SCIENCE, V268, P1503, DOI 10.1126/science.7770778
   Markram H, 2006, NAT REV NEUROSCI, V7, P153, DOI 10.1038/nrn1848
   Mitra S, 2009, IEEE T BIOMED CIRC S, V3, P32, DOI 10.1109/TBCAS.2008.2005781
   Moreno-Bote R, 2004, PHYS REV LETT, V92, DOI 10.1103/PhysRevLett.92.028102
   Naud R, 2008, BIOL CYBERN, V99, P335, DOI 10.1007/s00422-008-0264-7
   Neftci E., 2015, FRONT NEUROSCI, V9
   Nessler B., 2009, ADV NEURAL INFORM PR, P1357
   Nessler B, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003037
   Pecevski D, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1002294
   Petkov V., 2012, THESIS RUPRECHT KARL
   Petrovici M. A., 2013, STOCHASTIC INFERENCE, V1311.3211
   Petrovici MA, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0108590
   Pfeil T., 2014, ARXIV14117916
   Pfeil T, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00011
   Pouget A, 2013, NAT NEUROSCI, V16, P1170, DOI 10.1038/nn.3495
   Probst D., 2014, THESIS RUPRECHT KARL
   Probst D, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00013
   RICCIARDI LM, 1988, J APPL PROBAB, V25, P43, DOI 10.2307/3214232
   RICCIARDI LM, 1979, BIOL CYBERN, V35, P1, DOI 10.1007/BF01845839
   Rivkin B., 2014, THESIS RUPRECHT KARL
   Rolls E. T., 2010, NOISY BRAIN STOCHAST, V34
   Roth M., 2014, THESIS RUPRECHT KARL
   Salakhutdinov R., 2010, P 27 INT C MACH LEAR, P943
   Schmidt D., 2012, READOUT TRAINING LIQ
   Schneidman E, 1998, NEURAL COMPUT, V10, P1679, DOI 10.1162/089976698300017089
   Schwartz M.-O., 2013, THESIS U HEIDELBERG
   Steimer A, 2009, NEURAL COMPUT, V21, P2502, DOI 10.1162/neco.2009.08-08-837
   Stockel D., 2015, THESIS RUPRECHT KARL
   Sussillo D, 2007, J NEUROPHYSIOL, V97, P4079, DOI 10.1152/jn.01357.2006
   Tetzlaff T, 2012, PLOS COMPUT BIOL, V8, DOI 10.1371/journal.pcbi.1002596
   THOMAS MU, 1975, J APPL PROBAB, V12, P600, DOI 10.2307/3212877
   Weilbach C., 2015, THESIS RUPRECHT KARL
NR 71
TC 0
Z9 0
U1 0
U2 2
PY 2016
BP 219
EP 346
DI 10.1007/978-3-319-39552-4_6
D2 10.1007/978-3-319-39552-4
WC Mathematical & Computational Biology; Neurosciences
DA 2023-11-11
ER

PT J
AU Gueorguieva, N
   Valova, I
   Georgiev, G
AF Gueorguieva, N
   Valova, I
   Georgiev, G
TI Learning and data clustering with an RBF-based spiking neuron network
SO JOURNAL OF EXPERIMENTAL & THEORETICAL ARTIFICIAL INTELLIGENCE
DT Article
DE spiking neuron; neural networks; learning algorithm; radial basis
   function; data clustering
ID COMPONENT ANALYSIS
AB A spiking neuron is a simplified model of the biological neuron as the input, output, and internal representation of information based on the relative timing of individual spikes, and is closely related to the biological network. We extend the learning algorithms with spiking neurons developed by earlier workers. These algorithms explicitly concerned a single pair of pre- and postsynaptic spikes and cannot be applied to situations involving multiple spikes arriving at the same synapse. The aim of the algorithm presented here is to achieve synaptic plasticity by using relative timing between single pre- and postsynaptic spikes and therefore to improve the performance on large datasets. The learning algorithm is based on spike timing-dependent synaptic plasticity, which uses exact spike timing to optimize the information stream through the neural network as well as to enforce the competition between neurons during unsupervised Hebbian learning. We demonstrate the performance of the proposed spiking neuron model and learning algorithm on clustering and provide a comparative analysis with other state-of-the-art approaches.
C1 CUNY, CSI, Staten Isl, NY 10314 USA.
   Univ Massachusetts, N Dartmouth, MA 02747 USA.
   Univ Wisconsin, Oshkosh, WI 54901 USA.
RP Gueorguieva, N (corresponding author), CUNY, CSI, 2800 Victory Blvd, Staten Isl, NY 10314 USA.
EM natachag@postbox.csi.cuny.edu
CR [Anonymous], 1994, MACHINE LEARNING NEU
   Blatt M, 1997, NEURAL COMPUT, V9, P1805, DOI 10.1162/neco.1997.9.8.1805
   BOHTE SM, 2000, P IEEE INNS ENNS INT
   Delorme A, 1999, NEUROCOMPUTING, V26-7, P989, DOI 10.1016/S0925-2312(99)00095-8
   GERSTNER W, 1995, PHYS REV E, V51, P738, DOI 10.1103/PhysRevE.51.738
   Gerstner W, 1996, NATURE, V383, P76, DOI 10.1038/383076a0
   Guedalia ID, 1999, NEURAL COMPUT, V11, P521, DOI 10.1162/089976699300016755
   HABERLY LB, 1985, CHEM SENSES, V10, P219, DOI 10.1093/chemse/10.2.219
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   Maass W, 1996, NEURAL COMPUT, V8, P1, DOI 10.1162/neco.1996.8.1.1
   Maass W, 1997, NEURAL COMPUT, V9, P279, DOI 10.1162/neco.1997.9.2.279
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Markram H, 1996, NATURE, V382, P807, DOI 10.1038/382807a0
   Mattia M, 2000, NEURAL COMPUT, V12, P2305, DOI 10.1162/089976600300014953
   Natschlager T, 1998, NETWORK-COMP NEURAL, V9, P319, DOI 10.1088/0954-898X/9/3/003
   OKEEFE J, 1993, HIPPOCAMPUS, V3, P317, DOI 10.1002/hipo.450030307
   Rosipal R, 2001, NEURAL COMPUT, V13, P505, DOI 10.1162/089976601300014439
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   SOO YL, 1999, NEURAL PROCESS LETT, V10, P171
   Wentzell PD, 1997, J CHEMOMETR, V11, P339, DOI 10.1002/(SICI)1099-128X(199707)11:4<339::AID-CEM476>3.0.CO;2-L
   Yagar RR., 1994, J INTELL FUZZY SYST, V2, P209, DOI [10.3233/IFS-1994-2301, DOI 10.3233/IFS-1994-2301]
NR 21
TC 8
Z9 8
U1 0
U2 2
PD MAR
PY 2006
VL 18
IS 1
BP 73
EP 86
DI 10.1080/09528130600552888
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT C
AU Taherkhani, A
   Li, YH
   Belatreche, A
   Maguire, LP
AF Taherkhani, Aboozar
   Li, Yuhua
   Belatreche, Ammar
   Maguire, Liam P.
GP IEEE
TI Multi-DL-ReSuMe: Multiple neurons Delay Learning Remote Supervised
   Method
SO 2015 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUL 12-17, 2015
CL Killarney, IRELAND
DE classification; delay learning; spiking neural network; supervised
   learning; spatiotemporal patterns
ID SPIKING NEURAL-NETWORKS; PLASTICITY
AB Spikes are an important part of information transmission between neurons in the biological brain. Biological evidence shows that information is carried in the timing of individual action potentials, rather than only the firing rate. Spiking neural networks are devised to capture more biological characteristics of the brain to construct more powerful intelligent systems. In this paper, we extend our newly proposed supervised learning algorithm called DL-ReSuMe (Delay Learning Remote Supervised Method) to train multiple neurons to classify spatiotemporal spiking patterns. In this method, a number of neurons instead of a single neuron is trained to perform the classification task. The simulation results show that a population of neurons has significantly higher processing ability compared to a single neuron. It is also shown that the performance of Multi-DL-ReSuMe (Multiple DL-ReSuMe) is increased when the number of desired spikes is increased in the desired spike trains to an appropriate number.
C1 [Taherkhani, Aboozar] Univ Ulster, Intelligent Syst Res Ctr, Coleraine BT52 1SA, Londonderry, North Ireland.
   [Li, Yuhua] Univ Salford, Sch Comp Sci & Engn, Manchester, Lancs, England.
   [Belatreche, Ammar] Univ Ulster, Sch Comp & Intelligent Syst, Coleraine BT52 1SA, Londonderry, North Ireland.
   [Maguire, Liam P.] Univ Ulster, Fac Comp & Engn, Coleraine BT52 1SA, Londonderry, North Ireland.
RP Taherkhani, A (corresponding author), Univ Ulster, Intelligent Syst Res Ctr, Coleraine BT52 1SA, Londonderry, North Ireland.
EM taherkhani-a@emaile.ulster.ac.uk; y.li@salford.ac.uk;
   a.belatreche@ulster.ac.uk; Ip.maguire@ulster.ac.uk
CR Averbeck BB, 2006, NAT REV NEUROSCI, V7, P358, DOI 10.1038/nrn1888
   Belatreche A, 2007, SOFT COMPUT, V11, P239, DOI [10.1007/s00500-006-0065-7, 10.1007/S00500-006-0065-7]
   BIALEK W, 1991, SCIENCE, V252, P1854, DOI 10.1126/science.2063199
   Bohte Sander M., 2004, Natural Computing, V3, P195, DOI 10.1023/B:NACO.0000027755.02868.60
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Florian RV, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0040233
   Friedrich J., 2013, INT J NEURAL SYST
   Glackin C, 2011, NEURAL NETWORKS, V24, P247, DOI 10.1016/j.neunet.2010.11.008
   Heiligenberg W., 1991, NEURAL NETS ELECT FI
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Jörntell H, 2006, NEURON, V52, P227, DOI 10.1016/j.neuron.2006.09.032
   KUWABARA N, 1993, J NEUROPHYSIOL, V69, P1713, DOI 10.1152/jn.1993.69.5.1713
   Legenstein R, 2005, NEURAL COMPUT, V17, P2337, DOI 10.1162/0899766054796888
   Mohemmed A, 2013, NEUROCOMPUTING, V107, P3, DOI 10.1016/j.neucom.2012.08.034
   Mohemmed A, 2012, INT J NEURAL SYST, V22, DOI 10.1142/S0129065712500128
   Morrison A, 2008, BIOL CYBERN, V98, P459, DOI 10.1007/s00422-008-0233-1
   Paugam-Moisy H, 2012, HDB NATURAL COMPUTIN, V1, P1, DOI DOI 10.1007/978-3-540-92910-9_10
   Ponulak F., 2005, RESUME NEW SUPERVISE
   Ponulak F, 2011, ACTA NEUROBIOL EXP, V71, P409
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Pouget A, 2000, NAT REV NEUROSCI, V1, P125, DOI 10.1038/35039062
   Rieke P., 1997, NETWORK COMPUTATION, V8, P353
   Ruf B, 1997, NEURAL PROCESS LETT, V5, P9, DOI 10.1023/A:1009697008681
   Schreiber S, 2003, NEUROCOMPUTING, V52-4, P925, DOI 10.1016/S0925-2312(02)00838-X
   Sporea I, 2013, NEURAL COMPUT, V25, P473, DOI 10.1162/NECO_a_00396
   Taherkhani A., 2015, IEEE T NEUR IN PRESS
   Taherkhani A., 2014, P ESANN BRUG BELG
   Thorpe S, 1996, NATURE, V381, P520, DOI 10.1038/381520a0
   Xu Y, 2013, NEURAL NETWORKS, V43, P99, DOI 10.1016/j.neunet.2013.02.003
NR 30
TC 1
Z9 1
U1 0
U2 1
PY 2015
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Lee, J
   Cho, DH
AF Lee, Jeongman
   Cho, Dong-Ho
BE Shakshuki, EM
TI Capacity Analysis of Neuro-Spike Communication System for Nanonetworks
SO 5TH INTERNATIONAL CONFERENCE ON EMERGING UBIQUITOUS SYSTEMS AND
   PERVASIVE NETWORKS / THE 4TH INTERNATIONAL CONFERENCE ON CURRENT AND
   FUTURE TRENDS OF INFORMATION AND COMMUNICATION TECHNOLOGIES IN
   HEALTHCARE / AFFILIATED WORKSHOPS
SE Procedia Computer Science
DT Proceedings Paper
CT 4th International Conference on Current and Future Trends of Information
   and Communication Technologies in Healthcare (ICTH)
CY SEP 22-25, 2014
CL Halifax, CANADA
DE Molecular communication; neuro-spike communication; body area sensor
   network; neural network; nanonetworks
ID STIMULATION
AB Molecular communication is a new paradigm to solve the problems of conventional communication system, such as capacity or energy consumption. The neural network is one of the molecular communication mechanism valid for higher animals and considered as a highly advanced information transfer network in terms of capacity, reliability and energy consumption. In this paper, we provide the capacity of neuro-spike communication system, which is inspired from neural network. The neuro-spike communication consists of axon propagation, vesicle release and neurotransmitter diffusion. Through modeling and analysis of the above three parts, the capacity of the neuro-spike communication can be obtained. Numerical results show the trends of the capacity for main parameters, such as bandwidth, absolute refractory period and distance. The capacity is converged to certain value even though bandwidth of input signal becomes larger and larger. As a result, it can be seen that refractory period is a key parameter in neuro-spike communication system. (C) 2014 The Authors. Published by Elsevier B.V.
C1 [Lee, Jeongman; Cho, Dong-Ho] Korea Adv Inst Sci & Technol KAIST, Dept Elect Engn, Taejon, South Korea.
RP Cho, DH (corresponding author), Korea Adv Inst Sci & Technol KAIST, Dept Elect Engn, Taejon, South Korea.
EM dhcho@kaist.ac.kr
CR Akyildiz I.F., 2010, NANO COMMUN NETW, V1, P3, DOI [DOI 10.1016/J.NANCOM.2010.04.001, 10.1016/j. nancom.2010.04.001]
   Akyildiz IF, 2008, COMPUT NETW, V52, P2260, DOI 10.1016/j.comnet.2008.04.001
   Akyildiz IF, 2011, COMMUN ACM, V54, P84, DOI 10.1145/2018396.2018417
   Balasubramaniam S., 2011, NANO COMMUN NETW, V2, P150, DOI DOI 10.1016/J.NANC0M.2011.05.004
   Balevi E, 2013, IEEE T COMMUN, V61, P1178, DOI 10.1109/TCOMM.2012.010213.110093
   Dayan P., 2001, NEUROSCIENCE COMPUTA
   Dobrunz LE, 1997, NEURON, V18, P995, DOI 10.1016/S0896-6273(00)80338-4
   Faisal AA, 2007, PLOS COMPUT BIOL, V3, P783, DOI 10.1371/journal.pcbi.0030079
   Hiyama S., 2005, 2005 NSTI Nanotechnology Conference and Trade Show. NSTI Nanotech 2005, P391
   Malak D, 2013, IEEE INT CONF COMM, P771, DOI 10.1109/ICCW.2013.6649337
   Meeks JP, 2007, J NEUROPHYSIOL, V97, P3460, DOI 10.1152/jn.01288.2006
   Mesiti F, 2013, IEEE J SEL AREA COMM, V31, P695, DOI 10.1109/JSAC.2013.SUP2.1213002
   Pierobon M, 2013, IEEE T INFORM THEORY, V59, P942, DOI 10.1109/TIT.2012.2219496
   Raastad M, 2003, J PHYSIOL-LONDON, V548, P745, DOI 10.1113/jphysiol.2002.032706
   YEOMANS JS, 1979, PHYSIOL BEHAV, V22, P911, DOI 10.1016/0031-9384(79)90336-6
NR 15
TC 4
Z9 4
U1 0
U2 0
PY 2014
VL 37
BP 428
EP 433
DI 10.1016/j.procs.2014.08.064
WC Computer Science, Information Systems; Computer Science, Theory &
   Methods
DA 2023-11-11
ER

PT C
AU Bohte, SM
   Kok, JN
   La Poutré, H
AF Bohte, SM
   Kok, JN
   La Poutré, H
GP IEEE
   IEEE
TI Implementing position-invariant detection of feature-conjunctions in a
   network of spiking neurons
SO PROCEEDING OF THE 2002 INTERNATIONAL JOINT CONFERENCE ON NEURAL
   NETWORKS, VOLS 1-3
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN 02)
CY MAY 12-17, 2002
CL HONOLULU, HI
ID BINDING
AB The design of neural networks that are able to efficiently detect conjunctions of features is an important open challenge. We develop a feed-forward spiking neural network that requires a constant number of neurons for detecting a conjunction irrespective of the size of the retinal input field, and for up to four simultaneously present feature-conjunctions.
C1 CWI, NL-1090 GB Amsterdam, Netherlands.
RP Bohte, SM (corresponding author), CWI, Kruislaan 413,POB 94079, NL-1090 GB Amsterdam, Netherlands.
CR BOHTE SM, IN PRESS P IJCNN 200, V4, P249
   BOHTE SM, IN PRESS P ESANN 200, P419
   BOHTE SM, 2001, POUTRE BINDING SPARS
   FODOR JA, 1988, COGNITION, V28, P3, DOI 10.1016/0010-0277(88)90031-5
   Hebb D. O., 1949, ORG BEHAV
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   Luck SJ, 1997, NATURE, V390, P279, DOI 10.1038/36846
   MAASS W, 1999, PULSED NEURAL NETWOR, V1
   Mozer M. C., 1991, PERCEPTION MULTIPLE
   Natschlager T, 1998, NETWORK-COMP NEURAL, V9, P319, DOI 10.1088/0954-898X/9/3/003
   Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0
   Rachkovskij DA, 2001, NEURAL COMPUT, V13, P411, DOI 10.1162/089976601300014592
   Shadlen MN, 1999, NEURON, V24, P67, DOI 10.1016/S0896-6273(00)80822-3
   THORPE SJ, 1997, NEURAL INFORMATION P, P901
   von der Malsburg C, 1999, NEURON, V24, P95, DOI 10.1016/S0896-6273(00)80825-9
NR 15
TC 0
Z9 0
U1 0
U2 1
PY 2002
BP 1097
EP 1102
DI 10.1109/IJCNN.2002.1007647
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Webber, WRS
   Lesser, RP
AF Webber, W. R. S.
   Lesser, Ronald P.
TI Automated spike detection in EEG
SO CLINICAL NEUROPHYSIOLOGY
DT Editorial Material
ID ARTIFICIAL NEURAL-NETWORKS; RAW
C1 [Webber, W. R. S.; Lesser, Ronald P.] Johns Hopkin Hosp, Epilepsy Ctr, Baltimore, MD 21287 USA.
RP Webber, WRS (corresponding author), Johns Hopkin Hosp, Epilepsy Ctr, Baltimore, MD 21287 USA.
EM BobWeb@jhmi.edu
CR Brown MW, 2007, CLIN NEUROPHYSIOL, V118, P1744, DOI 10.1016/j.clinph.2007.04.017
   GABOR AJ, 1992, ELECTROEN CLIN NEURO, V83, P271, DOI 10.1016/0013-4694(92)90086-W
   GOTMAN J, 1975, ELECTROEN CLIN NEURO, V38, P623, DOI 10.1016/0013-4694(75)90163-7
   HOSTETLER WE, 1992, ELECTROEN CLIN NEURO, V83, P1, DOI 10.1016/0013-4694(92)90126-3
   Özdamar Ö, 1998, COMPUT BIOMED RES, V31, P122, DOI 10.1006/cbmr.1998.1475
   Scheuer ML, 2017, CLIN NEUROPHYSIOL, V128, P243, DOI 10.1016/j.clinph.2016.11.005
   Turing M., 1950, MIND, VLIX, P433, DOI [10.1093/mind/LIX.236.433, DOI 10.1093/MIND/LIX.236.433]
   WEBBER WRS, 1994, ELECTROEN CLIN NEURO, V91, P194, DOI 10.1016/0013-4694(94)90069-8
   WEBBER WRS, 1993, ELECTROEN CLIN NEURO, V87, P364, DOI 10.1016/0013-4694(93)90149-P
   Wilson SB, 2002, CLIN NEUROPHYSIOL, V113, P1873, DOI 10.1016/S1388-2457(02)00297-3
   Wilson SB, 1996, ELECTROEN CLIN NEURO, V98, P186, DOI 10.1016/0013-4694(95)00221-9
NR 11
TC 11
Z9 11
U1 0
U2 3
PD JAN
PY 2017
VL 128
IS 1
BP 241
EP 242
DI 10.1016/j.clinph.2016.11.018
WC Clinical Neurology; Neurosciences
DA 2023-11-11
ER

PT J
AU Shi, W
   Wang, YF
   Chen, YY
   Ma, JH
AF Shi, Wei
   Wang, Yufeng
   Chen, Yiyuan
   Ma, Jianhua
TI An effective Two-Stage Electricity Price forecasting scheme
SO ELECTRIC POWER SYSTEMS RESEARCH
DT Article
DE Electricity price forecasting; Spike occurrence prediction; Deep neural
   network; Variance stabilization transformation
ID WAVELET TRANSFORM; NEURAL-NETWORK; HYBRID MODEL; SPIKES; IMPACT; MARKET
AB With the development of the global power market reform, the monopoly of the power sector and government control pattern has gradually broken. Due to the unique properties of electricity, electricity prices show high volatility and uncertainty, bringing significant challenges to the accurate prediction of electricity prices. The sudden occurrence of a few spike prices in the electricity spot market has significantly affected electricity price forecasting accuracy. We propose a novel two-stage electricity price forecasting scheme (TSEP). A multi-source data-based spike occurrence prediction scheme is presented in the first stage, which adopts a deep neural network (DNN) to predict whether the price to be forecasted is a spike or not. Specifically, to alleviate the impact of low spike price samples, the oversampling method is used to synthesize some spikes at the data level. A loss function with a misclassification penalty to increase the cost of missing price spikes is designed at the algorithm level. Based on the outputs of the first stage, in the second stage, TSEP exploits the variance stabilizing transformations respectively suitable for pre-processing spike and normal prices and combines an artificial neural network (ANN) based spike calibration model to improve the accuracy of electricity price forecasting further. The experimental results on the European Power Exchange for France (EPEX-FR) demonstrate that TSEP increases spike occurrence prediction accuracy compared with the conventional models and significantly improves the accuracy of spike electricity price forecasting without affecting the accuracy of forecasting normal electricity price.
C1 [Shi, Wei; Wang, Yufeng; Chen, Yiyuan] Nanjing Univ Posts & Telecommun, Coll Telecommun & Informat Engn, Nanjing, Peoples R China.
   [Shi, Wei] Jiangsu Police Inst, Nanjing, Peoples R China.
   [Ma, Jianhua] Hosei Univ, Fac Comp & Informat Sci, Tokyo, Japan.
RP Wang, YF (corresponding author), Nanjing Univ Posts & Telecommun, Coll Telecommun & Informat Engn, Nanjing, Peoples R China.
EM wfwang1974@gmail.com
CR Afanasyev DO, 2019, APPL ENERG, V236, P196, DOI 10.1016/j.apenergy.2018.11.076
   Bento PMR, 2018, APPL ENERG, V210, P88, DOI 10.1016/j.apenergy.2017.10.058
   Borovkova S, 2006, STOCHASTIC FINANCE, P239, DOI 10.1007/0-387-28359-5_9
   Cartea A., 2005, APPL MATH FINANCE, V12, P313, DOI DOI 10.1080/13504860500117503
   Chen YY, 2019, ENERGIES, V12, DOI 10.3390/en12122241
   DIEBOLD FX, 1995, J BUS ECON STAT, V13, P253, DOI 10.2307/1392185
   Fanone E, 2013, ENERG ECON, V35, P22, DOI 10.1016/j.eneco.2011.12.006
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Han H, 2005, LECT NOTES COMPUT SC, V3644, P878, DOI 10.1007/11538059_91
   Janczura J, 2013, ENERG ECON, V38, P96, DOI 10.1016/j.eneco.2013.03.013
   Jiang P, 2016, APPL MATH MODEL, V40, P10631, DOI 10.1016/j.apm.2016.08.001
   Kyritsis E, 2017, ENERG POLICY, V101, P550, DOI 10.1016/j.enpol.2016.11.014
   Mount T., 2004, P 37 ANN HAW INT C S, DOI [10.1109/HICSS.2004.1265173, DOI 10.1109/HICSS.2004.1265173]
   Qiu XH, 2017, PROCEDIA COMPUT SCI, V108, P1308, DOI 10.1016/j.procs.2017.05.055
   Rafique SF, 2018, TSINGHUA SCI TECHNOL, V23, P254, DOI 10.26599/TST.2018.9010086
   Roy F.R.V, 2021, ADV SMART GRID TECHN, P245
   Sandhu HS, 2016, ELECTR POW SYST RES, V141, P450, DOI 10.1016/j.epsr.2016.08.005
   Sundaram A, 2018, ELECTR POW COMPO SYS, V46, P521, DOI 10.1080/15325008.2018.1460639
   Truck S, 2007, 4711 MPRA WROCL U TE
   Uniejewski B, 2018, IEEE T POWER SYST, V33, P2219, DOI 10.1109/TPWRS.2017.2734563
   Voronin S, 2013, ENERGIES, V6, P5897, DOI 10.3390/en6115897
   Wang DY, 2017, APPL ENERG, V190, P390, DOI 10.1016/j.apenergy.2016.12.134
   Wang Y, 2015, TSINGHUA SCI TECHNOL, V20, P117, DOI 10.1109/TST.2015.7085625
   Yang WD, 2019, APPL ENERG, V235, P1205, DOI 10.1016/j.apenergy.2018.11.034
   Yang Z, 2017, APPL ENERG, V190, P291, DOI 10.1016/j.apenergy.2016.12.130
   Zhang JL, 2019, INT J ELEC POWER, V105, P541, DOI 10.1016/j.ijepes.2018.08.025
   Zhang JL, 2020, APPL ENERG, V258, DOI 10.1016/j.apenergy.2019.114087
   Zhao JH, 2007, IEEE T POWER SYST, V22, P376, DOI 10.1109/TPWRS.2006.889139
   Ziel F, 2018, ENERG ECON, V70, P396, DOI 10.1016/j.eneco.2017.12.016
   Ziel F, 2015, ENERG ECON, V51, P430, DOI 10.1016/j.eneco.2015.08.005
NR 30
TC 15
Z9 15
U1 0
U2 16
PD OCT
PY 2021
VL 199
AR 107416
DI 10.1016/j.epsr.2021.107416
EA JUN 2021
WC Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Echtermeyer, C
   Smulders, TV
   Smith, VA
AF Echtermeyer, Christoph
   Smulders, Tom V.
   Smith, V. Anne
TI Causal pattern recovery from neural spike train data using the Snap Shot
   Score
SO JOURNAL OF COMPUTATIONAL NEUROSCIENCE
DT Article
DE Neuronal assembly analysis; Spike train; Causal network; Neural
   information flow
ID PARTIAL DIRECTED COHERENCE; DYNAMIC BAYESIAN NETWORK; PLANAR
   MULTIELECTRODE ARRAY; COOPERATIVE FIRING ACTIVITY; FUNCTIONAL
   CONNECTIVITY; BRAIN CONNECTIVITY; INFORMATION-FLOW; ENSEMBLE; MODEL;
   ALGORITHM
AB We present a new approach to learning directed in flow networks from multi-channel spike train data. A novel scoring function, the Snap Shot Score, is used to assess potential networks with respect to their quality of causal explanation for the data. Additionally, we suggest a generic concept of plausibility in order to assess network learning techniques under partial observability conditions. Examples demonstrate the assessment of networks with the Snap Shot Score, and neural network simulations show its performance in complex situations with partial observability. We discuss the application of the new score to real data and indicate how it can be modified to suit other neural data types.
C1 [Echtermeyer, Christoph; Smith, V. Anne] Univ St Andrews, Sch Biol, St Andrews KY16 9TS, Fife, Scotland.
   [Smulders, Tom V.] Newcastle Univ, Inst Neurosci, Newcastle Upon Tyne NE2 4HH, Tyne & Wear, England.
RP Smith, VA (corresponding author), Univ St Andrews, Sch Biol, St Andrews KY16 9TS, Fife, Scotland.
CR Abbott LF, 1999, BRAIN RES BULL, V50, P303, DOI 10.1016/S0361-9230(99)00161-6
   AERTSEN AMHJ, 1989, J NEUROPHYSIOL, V61, P900, DOI 10.1152/jn.1989.61.5.900
   Airoldi EM, 2007, PLOS COMPUT BIOL, V3, P2421, DOI 10.1371/journal.pcbi.0030252
   [Anonymous], 1988, C PROGRAMMING LANGUA
   [Anonymous], 1950, INTRO PROBABILITY TH
   Ashlock D, 2004, EVOLUTIONARY COMPUTA
   Astolfi L, 2006, IEEE T BIO-MED ENG, V53, P1802, DOI 10.1109/TBME.2006.873692
   B??ck T., 1996, EVOLUTION STRATEGIES, DOI DOI 10.1093/OSO/9780195099713.001.0001
   Baccalá LA, 2001, BIOL CYBERN, V84, P463, DOI 10.1007/PL00007990
   Borst A, 1999, NAT NEUROSCI, V2, P947, DOI 10.1038/14731
   Brown EN, 2004, NAT NEUROSCI, V7, P456, DOI 10.1038/nn1228
   Burge J, 2009, HUM BRAIN MAPP, V30, P122, DOI 10.1002/hbm.20490
   Cadotte AJ, 2008, PLOS ONE, V3, DOI 10.1371/journal.pone.0003355
   CASELLA G, 1992, AM STAT, V46, P167, DOI 10.2307/2685208
   CERNY V, 1985, J OPTIMIZ THEORY APP, V45, P41, DOI 10.1007/BF00940812
   CHORNOBOY ES, 1988, BIOL CYBERN, V59, P265, DOI 10.1007/BF00332915
   COOPER GF, 1992, MACH LEARN, V9, P309, DOI 10.1007/BF00994110
   Cormen T.H., 2001, INTRO ALGORITHMS, V1, P329
   COX RT, 1946, AM J PHYS, V14, P1, DOI 10.1119/1.1990764
   Dayan P., 2005, THEORETICAL NEUROSCI
   EBERHART R, 2001, SWARM INTELLIGENCE A
   Eichler M, 2006, BIOL CYBERN, V94, P469, DOI 10.1007/s00422-006-0062-z
   Eldawlatly S, 2008, IEEE ENG MED BIO, P5531, DOI 10.1109/IEMBS.2008.4650467
   Friedman N, 2000, J COMPUT BIOL, V7, P601, DOI 10.1089/106652700750050961
   Friedman Nir, 1997, P 14 INT C MACH LEAR, V97, P125
   Friston Karl J., 1994, Human Brain Mapping, V2, P56, DOI 10.1002/hbm.460020107
   GERSTEIN GL, 1985, J NEUROSCI, V5, P881
   GERSTEIN GL, 1969, SCIENCE, V164, P828, DOI 10.1126/science.164.3881.828
   GERSTEIN GL, 1985, J NEUROPHYSIOL, V54, P1513, DOI 10.1152/jn.1985.54.6.1513
   Gerstner W., 2002, SPIKING NEURON MODEL
   Ghahramani Z, 1998, LECT NOTES ARTIF INT, V1387, P168, DOI 10.1007/BFb0053999
   GRANGER CWJ, 1969, ECONOMETRICA, V37, P424, DOI 10.2307/1912791
   HASTINGS WK, 1970, BIOMETRIKA, V57, P97, DOI 10.1093/biomet/57.1.97
   Hebb D. O., 1949, ORG BEHAV
   HECKERMAN D, 1995, MACH LEARN, V20, P197, DOI 10.1007/BF00994016
   Heuschkel MO, 2002, J NEUROSCI METH, V114, P135, DOI 10.1016/S0165-0270(01)00514-3
   Jezzard P., 2001, FUNCTIONAL MRI INTRO
   Johnson JL, 2003, METHODS, V30, P64, DOI 10.1016/S1046-2023(03)00008-2
   KAMINSKI MJ, 1991, BIOL CYBERN, V65, P203, DOI 10.1007/BF00198091
   Kennedy J, 1995, 1995 IEEE INTERNATIONAL CONFERENCE ON NEURAL NETWORKS PROCEEDINGS, VOLS 1-6, P1942, DOI 10.1109/icnn.1995.488968
   Kim S, 2004, BIOSYSTEMS, V75, P57, DOI 10.1016/j.biosystems.2004.03.004
   KIRKPATRICK S, 1983, SCIENCE, V220, P671, DOI 10.1126/science.220.4598.671
   Lauritzen SL., 1996, GRAPH MODELS
   Li J, 2007, P ANN INT IEEE EMBS, P5992
   LI ZWJ, 2006, 3 IEEE INT S BIOM IM, P964
   Lindsey BG, 2006, J NEUROSCI METH, V150, P116, DOI 10.1016/j.jneumeth.2005.06.019
   MADIGAN D, 1994, J AM STAT ASSOC, V89, P1535, DOI 10.2307/2291017
   Makarov VA, 2005, J NEUROSCI METH, V144, P265, DOI 10.1016/j.jneumeth.2004.11.013
   Matthews PM, 2004, J NEUROL NEUROSUR PS, V75, P6
   METROPOLIS N, 1953, J CHEM PHYS, V21, P1087, DOI 10.1063/1.1699114
   Murphy K, 1999, MODELLING GENE EXPRE
   Murphy KP, 2002, THESIS
   Neyman J, 1933, PHILOS T R SOC LOND, V231, P289, DOI 10.1098/rsta.1933.0009
   Nunez P.L., 2007, SCHOLARPEDIA, V2, P1348, DOI [10.4249/scholarpedia.1348, DOI 10.4249/SCHOLARPEDIA.1348]
   Nykamp DQ, 2005, SIAM J APPL MATH, V65, P2005, DOI 10.1137/S0036139903437072
   Oka H, 1999, J NEUROSCI METH, V93, P61, DOI 10.1016/S0165-0270(99)00113-2
   Okatan M, 2005, NEURAL COMPUT, V17, P1927, DOI 10.1162/0899766054322973
   Pearl J., 2000, CAUSALITY
   PERKEL DH, 1967, BIOPHYS J, V7, P419, DOI 10.1016/S0006-3495(67)86597-4
   Perrin BE, 2003, BIOINFORMATICS, V19, pII138, DOI 10.1093/bioinformatics/btg1071
   Pillow JW, 2008, NATURE, V454, P995, DOI 10.1038/nature07140
   Rajapakse JC, 2008, IEEE T MED IMAGING, V27, P825, DOI 10.1109/TMI.2008.915672
   Rajapakse JC, 2007, NEUROIMAGE, V37, P749, DOI 10.1016/j.neuroimage.2007.06.003
   Rieke F., 1999, SPIKES EXPLORING NEU
   ROBERT CP, 2004, MONTE CARLO STAT MET, P337
   Sameshima K, 1999, J NEUROSCI METH, V94, P93, DOI 10.1016/S0165-0270(99)00128-4
   Sato T, 2007, J NEUROSCI METH, V160, P45, DOI 10.1016/j.jneumeth.2006.08.009
   SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136
   SMITH VA, 2006, PLOS COMPUTATIONAL B, V2, pE61
   Sporns O, 2004, TRENDS COGN SCI, V8, P418, DOI 10.1016/j.tics.2004.07.008
   STEIN RB, 1965, BIOPHYS J, V5, P173, DOI 10.1016/S0006-3495(65)86709-1
   Stosiek C, 2003, P NATL ACAD SCI USA, V100, P7319, DOI 10.1073/pnas.1232232100
   Takahashi DY, 2007, J APPL STAT, V34, P1259, DOI 10.1080/02664760701593065
   Truccolo W, 2005, J NEUROPHYSIOL, V93, P1074, DOI 10.1152/jn.00697.2004
   Tsytsarev V, 2006, J NEURAL ENG, V3, P293, DOI 10.1088/1741-2560/3/4/006
   Wai Lam, 1994, Computational Intelligence, V10, P269, DOI 10.1111/j.1467-8640.1994.tb00166.x
   WHITLEY D, 1994, STAT COMPUT, V4, P65, DOI 10.1007/BF00175354
   Zou M, 2005, BIOINFORMATICS, V21, P71, DOI 10.1093/bioinformatics/bth463
NR 78
TC 2
Z9 2
U1 0
U2 4
PD AUG-OCT
PY 2010
VL 29
IS 1-2
SI SI
BP 231
EP 252
DI 10.1007/s10827-009-0174-2
WC Mathematical & Computational Biology; Neurosciences
DA 2023-11-11
ER

PT J
AU Öniz, Y
   Ayyildiz, M
AF Oniz, Yesim
   Ayyildiz, Mehmet
TI Recognizing handwritten digits using spiking neural networks with
   learning algorithms based on sliding mode control theory
SO TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES
DT Article
DE Digit recognition; spiking neural networks; sliding mode control
ID ERROR-BACKPROPAGATION; TIME
AB In this paper, a spiking neural network (SNN) has been proposed for recognizing the digits written on the LCD screen of an experimental setup. The convergence of the learning algorithm has been ensured by using sliding mode control (SMC) theory and the Lyapunov stability method for the adaptation of the network parameters. The spike response model (SRM) has been utilized in the design of the SNN. The performance of the proposed learning scheme has been evaluated both on the experimental data and on the MNIST dataset. The simulated and experimental results of the SNN structure have been compared with the responses of a conventional neural network (ANN) for which the weight update rules have been also derived using SMC theory. The conducted simulations and experimental studies reveal that convergence can be ensured for the proposed learning scheme and the SNN yields higher recognition accuracy compared to a conventional ANN.
C1 [Oniz, Yesim; Ayyildiz, Mehmet] Istanbul Bilgi Univ, Fac Engn & Nat Sci, Dept Mechatron Engn, Istanbul, Turkiye.
RP Öniz, Y (corresponding author), Istanbul Bilgi Univ, Fac Engn & Nat Sci, Dept Mechatron Engn, Istanbul, Turkiye.
EM yesim.oniz@bilgi.edu.tr
CR Abiyev RH, 2012, IEEE ASME INT C ADV, P1030, DOI 10.1109/AIM.2012.6265983
   Abu Ghosh MM, 2017, 2017 INTERNATIONAL CONFERENCE ON PROMISING ELECTRONIC TECHNOLOGIES (ICPET 2017), P77, DOI 10.1109/ICPET.2017.20
   Abusnaina AA, 2019, NEURAL PROCESS LETT, V49, P661, DOI 10.1007/s11063-018-9846-0
   Ahlawat S, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20123344
   Belatreche A, 2006, NEW MATH NAT COMPUT, V2, P237, DOI 10.1142/S179300570600049X
   Beyeler M, 2013, NEURAL NETWORKS, V48, P109, DOI 10.1016/j.neunet.2013.07.012
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Farsa EZ, 2019, IEEE T CIRCUITS-II, V66, P1582, DOI 10.1109/TCSII.2019.2890846
   Haykin S., 2004, NEURAL NETWORKS COMP, V2, P41
   Javanshir A, 2022, NEURAL COMPUT, V34, P1289, DOI 10.1162/neco_a_01499
   Kampakis S, 2012, SOFT COMPUT, V16, P943, DOI 10.1007/s00500-011-0793-1
   Khan MM, 2008, IEEE IJCNN, P2849, DOI 10.1109/IJCNN.2008.4634199
   Kheradpisheh SR, 2022, NEURAL PROCESS LETT, V54, P1255, DOI 10.1007/s11063-021-10680-x
   Lillicrap TP, 2019, CURR OPIN NEUROBIOL, V55, P82, DOI 10.1016/j.conb.2019.01.011
   Mirsadeghi M, 2021, NEUROCOMPUTING, V427, P131, DOI 10.1016/j.neucom.2020.11.052
   Mostafa H, 2018, IEEE T NEUR NET LEAR, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Oniz Y, 2014, J FRANKLIN I, V351, P3269, DOI 10.1016/j.jfranklin.2014.03.002
   Parma GG, 1998, ELECTRON LETT, V34, P97, DOI 10.1049/el:19980062
   Ponulak F, 2011, ACTA NEUROBIOL EXP, V71, P409
   Qu LH, 2020, NEURAL COMPUT APPL, V32, P13479, DOI 10.1007/s00521-020-04755-4
   Ramzan M, 2018, INT J ADV COMPUT SC, V9, P519
   Saeed AM, 2015, INT J ENG RES APPL, V5, P46
   Schrauwen B, 2004, P 15 PRORISC WORKSH, P301
   Tang HY, 2019, IEEE T BIOMED CIRC S, V13, P1664, DOI 10.1109/TBCAS.2019.2945406
   Utkin V., 2017, SLIDING MODE CONTROL
   Whittington JCR, 2019, TRENDS COGN SCI, V23, P235, DOI 10.1016/j.tics.2018.12.005
   Xu Y, 2017, NEURAL NETWORKS, V93, P7, DOI 10.1016/j.neunet.2017.04.010
   Yamazaki K, 2022, BRAIN SCI, V12, DOI 10.3390/brainsci12070863
   Yang SM, 2019, IEEE T CYBERNETICS, V49, P2490, DOI 10.1109/TCYB.2018.2823730
   Yu XH, 2009, IEEE T IND ELECTRON, V56, P3275, DOI 10.1109/TIE.2009.2027531
   Zhao B, 2015, IEEE T NEUR NET LEAR, V26, P1963, DOI 10.1109/TNNLS.2014.2362542
   Zheng N, 2018, IEEE T NEUR NET LEAR, V29, P4287, DOI 10.1109/TNNLS.2017.2761335
NR 33
TC 0
Z9 0
U1 0
U2 0
PY 2023
VL 31
IS 5
BP 860
EP 875
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
DA 2023-11-11
ER

PT J
AU Sun, YQ
   Zeng, Y
   Zhang, TL
AF Sun, Yinqian
   Zeng, Yi
   Zhang, Tielin
TI Quantum superposition inspired spiking neural network
SO ISCIENCE
DT Article
ID PERCEPTRON
AB Despite advances in artificial intelligence models, neural networks still cannot achieve human performance, partly due to differences in how information is encoded and processed compared with human brain. Information in an artificial neural network (ANN) is represented using a statistical method and processed as a fitting function, enabling handling of structural patterns in image, text, and speech processing. However, substantial changes to the statistical characteristics of the data, for example, reversing the background of an image, dramatically reduce the performance. Here, we propose a quantum superposition spiking neural network (QS-SNN) inspired by quantum mechanisms and phenomena in the brain, which can handle reversal of image background color. The QS-SNN incorporates quantum theory with brain-inspired spiking neural network models from a computational perspective, resulting in more robust performance compared with traditional ANN models, especially when processing noisy inputs. The results presented here will inform future efforts to develop brain-inspired artificial intelligence.
C1 [Sun, Yinqian; Zeng, Yi; Zhang, Tielin] Chinese Acad Sci, Res Ctr Brain Inspired Intelligence, Inst Automat, Beijing 100190, Peoples R China.
   [Zeng, Yi] Chinese Acad Sci, Ctr Excellence Brain Sci & Intelligence Technol, Shanghai 200031, Peoples R China.
   [Zeng, Yi] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Sun, Yinqian; Zeng, Yi] Univ Chinese Acad Sci, Sch Future Technol, Beijing 100190, Peoples R China.
   [Zeng, Yi] Univ Chinese Acad Sci, Sch Artifidal Intelligence, Beijing 100190, Peoples R China.
RP Zeng, Y (corresponding author), Chinese Acad Sci, Res Ctr Brain Inspired Intelligence, Inst Automat, Beijing 100190, Peoples R China.; Zeng, Y (corresponding author), Chinese Acad Sci, Ctr Excellence Brain Sci & Intelligence Technol, Shanghai 200031, Peoples R China.; Zeng, Y (corresponding author), Univ Chinese Acad Sci, Sch Future Technol, Beijing 100190, Peoples R China.; Zeng, Y (corresponding author), Univ Chinese Acad Sci, Sch Artifidal Intelligence, Beijing 100190, Peoples R China.
EM yi.zeng@ia.ac.cn
CR Beer K, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-14454-2
   Burges, 2010, MNIST HANDWRITTEN DI, DOI DOI 10.1561/2400000035
   Cao Y., 2017, ARXIV ARXIV171111240
   Dan Y, 2004, NEURON, V44, P23, DOI 10.1016/j.neuron.2004.09.007
   Dendukuri A., 2018, ARXIV ARXIV181211042
   Frémaux N, 2016, FRONT NEURAL CIRCUIT, V9, DOI 10.3389/fncir.2015.00085
   Gao Huang ZL, 2017, PROC CVPR IEEE, P4700, DOI DOI 10.1109/CVPR.2017.243
   Gerstner W., 2001, FRAMEWORK SPIKING NE, V4, P469
   Gerstner W., 2002, SPIKING NEURON MODEL
   Grover L. K., 1996, Proceedings of the Twenty-Eighth Annual ACM Symposium on the Theory of Computing, P212, DOI 10.1145/237814.237866
   Harrow AW, 2009, PHYS REV LETT, V103, DOI 10.1103/PhysRevLett.103.150502
   Héricé C, 2016, J INTEGR NEUROSCI, V15, P515, DOI 10.1142/S021963521650028X
   Iyengar S.S., 2020, ARXIV ARXIV200801081
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90
   Khalil R, 2017, EUR J NEUROSCI, V46, P2445, DOI 10.1111/ejn.13712
   Kristensen L.B., 2019, ARXIV ARXIV190706269
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lloyd S, 2014, NAT PHYS, V10, P631, DOI [10.1038/nphys3029, 10.1038/NPHYS3029]
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Mangini S, 2020, MACH LEARN-SCI TECHN, V1, DOI 10.1088/2632-2153/abaf98
   Mastriani M., 2020, ARXIV ARXIV200204394
   Schuld M, 2015, PHYS LETT A, V379, P660, DOI 10.1016/j.physleta.2014.11.061
   Shor PW, 1997, SIAM J COMPUT, V26, P1484, DOI 10.1137/S0036144598347011
   Shrestha Sumit Bam, 2018, ADV NEURAL INFORM PR, P1412
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Torrontegu E, 2019, EPL-EUROPHYS LETT, V125, DOI 10.1209/0295-5075/125/30004
   Vaziri A, 2010, NEW J PHYS, V12, DOI 10.1088/1367-2630/12/8/085001
   Weingarten CP, 2016, FRONT HUM NEUROSCI, V10, DOI 10.3389/fnhum.2016.00541
   Xiao H, 2017, ARXIV PREPRINT ARXIV
   Xiao J, 2010, EXPERT SYST APPL, V37, P4966, DOI 10.1016/j.eswa.2009.12.017
   Zeng Y, 2017, SCI CHINA INFORM SCI, V60, DOI 10.1007/s11432-016-0439-4
   Zhao FF, 2017, LECT NOTES COMPUT SC, V10637, P182, DOI 10.1007/978-3-319-70093-9_19
NR 32
TC 7
Z9 7
U1 1
U2 13
PD AUG 20
PY 2021
VL 24
IS 8
AR 102880
DI 10.1016/j.isci.2021.102880
WC Multidisciplinary Sciences
DA 2023-11-11
ER

PT J
AU Faghihi, F
   Alashwal, H
   Moustafa, AA
AF Faghihi, Faramarz
   Alashwal, Hany
   Moustafa, Ahmed A. A.
TI A Synaptic Pruning-Based Spiking Neural Network for Hand-Written Digits
   Classification
SO FRONTIERS IN ARTIFICIAL INTELLIGENCE
DT Article
DE deep spiking neural network; back-propagation; synaptic pruning; MNIST
   database; feature detection
ID DEEP; NEUROSCIENCE; RECOGNITION
AB A spiking neural network model inspired by synaptic pruning is developed and trained to extract features of hand-written digits. The network is composed of three spiking neural layers and one output neuron whose firing rate is used for classification. The model detects and collects the geometric features of the images from the Modified National Institute of Standards and Technology database (MNIST). In this work, a novel learning rule is developed to train the network to detect features of different digit classes. For this purpose, randomly initialized synaptic weights between the first and second layers are updated using average firing rates of pre- and postsynaptic neurons. Then, using a neuroscience-inspired mechanism named, "synaptic pruning" and its predefined threshold values, some of the synapses are deleted. Hence, these sparse matrices named, "information channels" are constructed so that they show highly specific patterns for each digit class as connection matrices between the first and second layers. The "information channels" are used in the test phase to assign a digit class to each test image. In addition, the role of feed-back inhibition as well as the connectivity rates of the second and third neural layers are studied. Similar to the abilities of the humans to learn from small training trials, the developed spiking neural network needs a very small dataset for training, compared to the conventional deep learning methods that have shown a very good performance on the MNIST dataset. This work introduces a new class of brain-inspired spiking neural networks to extract the features of complex data images.
C1 [Faghihi, Faramarz] Univ Bremen, Machine Listening Lab, Bremen, Germany.
   [Alashwal, Hany] United Arab Emirates Univ, Coll Informat Technol, Al Ain, U Arab Emirates.
   [Moustafa, Ahmed A. A.] Bond Univ, Fac Soc & Design, Sch Psychol, Gold Coast, Qld, Australia.
   [Moustafa, Ahmed A. A.] Univ Johannesburg, Fac Hlth Sci, Dept Human Anat & Physiol, Johannesburg, South Africa.
RP Alashwal, H (corresponding author), United Arab Emirates Univ, Coll Informat Technol, Al Ain, U Arab Emirates.
EM halashwal@uaeu.ac.ae
CR Amir A, 2017, PROC CVPR IEEE, P7388, DOI 10.1109/CVPR.2017.781
   Arce-McShane FI, 2018, J NEUROPHYSIOL, V120, P226, DOI 10.1152/jn.00037.2018
   Baldominos A, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9153169
   Brunner J, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms13033
   Chechik G, 1998, NEURAL COMPUT, V10, P1759, DOI 10.1162/089976698300017124
   Ciresan DC, 2010, NEURAL COMPUT, V22, P3207, DOI 10.1162/NECO_a_00052
   Deco G, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.1000092
   Decoste D, 2002, MACH LEARN, V46, P161, DOI 10.1023/A:1012454411458
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Faghihi F, 2017, FRONT NEUROROBOTICS, V11, DOI 10.3389/fnbot.2017.00063
   Frémaux N, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003024
   Fu SY, 2012, COMPUT INTEL NEUROSC, V2012, DOI 10.1155/2012/946589
   Furui S, 2012, IEEE SIGNAL PROC MAG, V29, P16, DOI 10.1109/MSP.2012.2209906
   Gooya ES, 2020, INT CONF IMAG PROC
   Hassabis D, 2017, NEURON, V95, P245, DOI 10.1016/j.neuron.2017.06.011
   Kadam Suvarna, 2020, Intelligent Systems Design and Applications. Proceedings of 18th International Conference on Intelligent Systems Design and Applications (ISDA 2018). Advances in Intelligent Systems and Computing (AISC 940), P100, DOI 10.1007/978-3-030-16657-1_10
   Kasabov N., 2018, SPRINGER SERIES BIOA, DOI 10.1007/978-3-662-57715-8
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Kulkarni SR, 2018, NEURAL NETWORKS, V103, P118, DOI 10.1016/j.neunet.2018.03.019
   Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050
   Langner R, 2019, BRAIN COGNITION, V131, P74, DOI 10.1016/j.bandc.2018.09.007
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508
   Li JJ, 2019, IEEE T COGN DEV SYST, V11, P148, DOI 10.1109/TCDS.2019.2897618
   López-Vázquez G, 2019, COMPUT INTEL NEUROSC, V2019, DOI 10.1155/2019/4182639
   Mostafa H, 2018, IEEE T NEUR NET LEAR, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Najafabadi M M, 2015, J BIG DATA-GER, P1
   Navlakha S, 2018, TRENDS COGN SCI, V22, P64, DOI 10.1016/j.tics.2017.09.012
   Paolicelli RC, 2011, SCIENCE, V333, P1456, DOI 10.1126/science.1202529
   Patil P., 2020, INT J INNOV RES COMP, V8, DOI [10.21276/ijircst.2020.8.4.16, DOI 10.21276/IJIRCST.2020.8.4.16]
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Kheradpisheh SR, 2021, Arxiv, DOI arXiv:2007.04039
   Riesenhuber M, 1999, NAT NEUROSCI, V2, P1019, DOI 10.1038/14819
   Sakemi Y, 2023, IEEE T NEUR NET LEAR, V34, P394, DOI 10.1109/TNNLS.2021.3095068
   Seeman SC, 2018, ELIFE, V7, DOI 10.7554/eLife.37349
   Stewart K, 2020, IEEE J EM SEL TOP C, V10, P512, DOI 10.1109/JETCAS.2020.3032058
   Südhof TC, 2018, NEURON, V100, P276, DOI 10.1016/j.neuron.2018.09.040
   Sun QR, 2019, PROC CVPR IEEE, P403, DOI 10.1109/CVPR.2019.00049
   Suvarna Y, 2016, MOL NEUROBIOL, V53, P2572, DOI 10.1007/s12035-015-9280-5
   Tavanaei Amirhossein, 2015, International Journal of Advanced Research in Artificial Intelligence, V4, P1
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Tavanaei A, 2019, NEUROCOMPUTING, V330, P39, DOI 10.1016/j.neucom.2018.11.014
   Thiele JC, 2018, FRONT COMPUT NEUROSC, V12, DOI 10.3389/fncom.2018.00046
   Ullman S, 2019, SCIENCE, V363, P692, DOI 10.1126/science.aau6595
   Vogt N, 2018, NAT METHODS, V15, P33, DOI 10.1038/nmeth.4549
   Wan L., 2013, P 30 INT C MACH LEAR, V28, P1058
   Wang YQ, 2020, ACM COMPUT SURV, V53, DOI 10.1145/3386252
   Waters J, 2006, J NEUROSCI, V26, P8267, DOI 10.1523/JNEUROSCI.2152-06.2006
   Wu JY, 2020, MEASUREMENT, V166, DOI 10.1016/j.measurement.2020.108202
   Wu JY, 2019, 2019 5TH INTERNATIONAL CONFERENCE ON EVENT-BASED CONTROL, COMMUNICATION, AND SIGNAL PROCESSING (EBCCSP), DOI 10.1109/ebccsp.2019.8836892
   Wu YJ, 2022, NAT COMMUN, V13, DOI 10.1038/s41467-021-27653-2
   Wyss R, 2003, P NATL ACAD SCI USA, V100, P324, DOI 10.1073/pnas.0136977100
   Yang WG, 2013, J NEUROSCI, V33, P17373, DOI 10.1523/JNEUROSCI.2515-13.2013
   Zhang ML, 2020, Arxiv, DOI arXiv:2003.11837
   Zhou Q, 2020, IEEE ACCESS, V8, P224162, DOI 10.1109/ACCESS.2020.3044646
NR 56
TC 4
Z9 4
U1 1
U2 12
PD FEB 24
PY 2022
VL 5
AR 680165
DI 10.3389/frai.2022.680165
WC Computer Science, Artificial Intelligence; Computer Science, Information
   Systems
DA 2023-11-11
ER

PT C
AU Leichsenring, F
   Graf, W
   Kaliske, M
AF Leichsenring, Ferenc
   Graf, Wolfgang
   Kaliske, Michael
GP IEEE
TI Spiking Response Model for Uniaxial Carbon Concrete Experimental Data
SO PROCEEDINGS OF 2016 IEEE SYMPOSIUM SERIES ON COMPUTATIONAL INTELLIGENCE
   (SSCI)
DT Proceedings Paper
CT IEEE Symposium Series on Computational Intelligence (IEEE SSCI)
CY DEC 06-09, 2016
CL Athens, GREECE
ID NETWORKS
AB In engineering related tasks, multiple types of neural networks are common methods of solution. Beside the different kinds of artificial neural networks, spiking neural networks (SNN) represent a continuative development in information processing within the computational units of a net. The properties of this neural type is utilized in this contribution in order to evaluate a uniaxial tension test of carbon reinforced specimen regarding the appearance of cracks in the composite structure during the experiment. The crack detection is considered as showcase for further development of evaluation methods based on SNNs with the focal point to engineering related experiments. This contribution is divided into five main parts, whereas the initial brief introduction is devoted to give an overview of neural networks and their computational units, particularly with regard to the classification of spiking neural networks. Since the proposed application of SNNs targets the evaluation of experimental data - especially crack detection - the uniaxial tension test of carbon reinforced concrete specimen is introduced, which is the basis for the experimental data. The utilized spike response model (SRM) is further presented in order to conclusively apply the method to experimental data for the purpose of crack occurrence detection within the data.
C1 [Leichsenring, Ferenc; Graf, Wolfgang; Kaliske, Michael] Tech Univ Dresden, Inst Struct Anal, D-01062 Dresden, Germany.
RP Leichsenring, F (corresponding author), Tech Univ Dresden, Inst Struct Anal, D-01062 Dresden, Germany.
EM ferenc.leichsenring@tu-dresden.de; wolfgang.graf@tu-dresden.de;
   michael.kaliske@tu-dresden.de
CR Anderson T.L., 2004, FRACTURE MECH FUNDAM
   GERSTNER W, 1995, PHYS REV E, V51, P738, DOI 10.1103/PhysRevE.51.738
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Haykin S., 2010, NEURAL NETWORKS LEAR
   Kaliske M., 2011, SOFT COMPUTING METHO, P59
   Kistler WM, 1997, NEURAL COMPUT, V9, P1015, DOI 10.1162/neco.1997.9.5.1015
   KRUGER J, 1990, VISUAL NEUROSCI, V5, P135, DOI 10.1017/S0952523800000171
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   MAASS W., 1999, PULSED NEURAL NETWOR
   Perrett D., 1982, EXPT BRAIN RES, V47
   Sch?tze E., 2015, FERRO 11 11 INT S FE
   Scholzen A., 2015, FLACHIGE TRAGSTRUKTU
NR 12
TC 0
Z9 0
U1 0
U2 0
PY 2016
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
DA 2023-11-11
ER

PT C
AU Zhang, L
AF Zhang, Lei
GP IEEE
TI Neural Dynamics Analysis for A Novel Bio-inspired Logistic Spiking
   Neuron Model
SO 2023 IEEE INTERNATIONAL CONFERENCE ON CONSUMER ELECTRONICS, ICCE
SE International Conference on Consumer Electronics
DT Proceedings Paper
CT IEEE International Conference on Consumer Electronics (ICCE)
CY JAN 06-08, 2023
CL Las Vegas, NV
DE neural dynamics; nonlinear dynamics; Logistic Spiking Neuron Model
   (LSNM); stability; eigenvalue; bifurcation
AB The major goal of this research is to analyze and optimize a single spiking neuron model, which will be used to construct Spiking Neural Network (SNN). The hardware implementation of SNN has the benefit of low power consumption and fast speed, compared to conventional neural network. Therefore, it is more suitable for battery-powered consumer electronic systems. This paper presents the dynamical analysis of a novel logistic spiking neuron model; provides a step-by-step derivation for the first order differential equation of the model is provided; illustrates the effects of the model parameters on the neuron dynamics, including stability, fixed points; and demonstrates the bifurcation phenomenon by controlling the input stimulation current to the neuron. Additionally, a trigonometric representation of the spiking neuron model in polar coordinates is introduced to examine the periodic property of the neuron.
C1 [Zhang, Lei] Univ Regina, Regina, SK, Canada.
RP Zhang, L (corresponding author), Univ Regina, Regina, SK, Canada.
EM Lei.Zhang@uregina.ca
CR Börgers C, 2017, TEXTS APPL MATH, V66, P51, DOI 10.1007/978-3-319-51171-9_8
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Lu M, 2016, ASIA-PAC INT SYM ELE, P1115, DOI 10.1109/APEMC.2016.7522959
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Rice D. M., 2014, CALCULUS THOUGHT, P125
   Spivak M., 2006, CALCULUS, P382
   Wilson H. R., 1999, SPIKES DECISIONS ACT
   Zhang L., 2017, IOP C SERIES MAT SCI, V224
   Zhang LB, 2018, IEEE INT CON MULTI
   Zhang L, 2021, IEEE T CLOUD COMPUT, V9, P1117, DOI 10.1109/TCC.2019.2903254
NR 11
TC 1
Z9 1
U1 0
U2 0
PY 2023
DI 10.1109/ICCE56470.2023.10043551
WC Computer Science, Interdisciplinary Applications; Engineering,
   Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Johansen, AR
   Jin, J
   Maszczyk, T
   Dauwels, J
   Cash, SS
   Westover, MB
AF Johansen, Alexander Rosenberg
   Jin, Jing
   Maszczyk, Tomasz
   Dauwels, Justin
   Cash, Sydney S.
   Westover, M. Brandon
GP IEEE
TI EPILEPTIFORM SPIKE DETECTION VIA CONVOLUTIONAL NEURAL NETWORKS
SO 2016 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL
   PROCESSING PROCEEDINGS
SE International Conference on Acoustics Speech and Signal Processing
   ICASSP
DT Proceedings Paper
CT 41st IEEE International Conference on Acoustics, Speech and Signal
   Processing (ICASSP)
CY MAR 20-25, 2016
CL Shanghai, PEOPLES R CHINA
DE Epilepsy; Spike detection; EEG; Deep learning; Convolutional neural
   network
ID EEG; RECOGNITION
AB The EEG of epileptic patients often contains sharp waveforms called "spikes", occurring between seizures. Detecting such spikes is crucial for diagnosing epilepsy. In this paper, we develop a convolutional neural network (CNN) for detecting spikes in EEG of epileptic patients in an automated fashion. The CNN has a convolutional architecture with filters of various sizes applied to the input layer, leaky ReLUs as activation functions, and a sigmoid output layer. Balanced mini-batches were applied to handle the imbalance in the data set. Leave-one-patient-out cross-validation was carried out to test the CNN and benchmark models on EEG data of five epilepsy patients. We achieved 0.947 AUC for the CNN, while the best performing benchmark model, Support Vector Machines with Gaussian kernel, achieved an AUC of 0.912.
C1 [Johansen, Alexander Rosenberg] Tech Univ Denmark, DTU Compute, Lyngby, Denmark.
   [Johansen, Alexander Rosenberg; Jin, Jing; Maszczyk, Tomasz; Dauwels, Justin] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore, Singapore.
   [Cash, Sydney S.; Westover, M. Brandon] Massachusetts Gen Hosp, Dept Neurol, Boston, MA 02114 USA.
   [Cash, Sydney S.; Westover, M. Brandon] Harvard Med Sch, Boston, MA USA.
RP Johansen, AR (corresponding author), Tech Univ Denmark, DTU Compute, Lyngby, Denmark.; Johansen, AR (corresponding author), Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore, Singapore.
CR [Anonymous], 2010, P PYTHON SCI COMPUTI
   [Anonymous], 2015, CORR
   [Anonymous], 2015, LASAGNE 1 RELEASE
   [Anonymous], 2012, DEEP LEARNING UNSUPE
   [Anonymous], 2015, DEEP RESIDUAL LEARNI
   [Anonymous], ARXIV150301919
   Black MA, 2000, CLIN ELECTROENCEPHAL, V31, P122, DOI 10.1177/155005940003100304
   DAVEY BLK, 1989, MED BIOL ENG COMPUT, V27, P365, DOI 10.1007/BF02441427
   DEOLIVEIRA PG, 1983, ELECTROEN CLIN NEURO, V56, P97, DOI 10.1016/0013-4694(83)90011-1
   Epilepsy Foundation of America, 2014, EP BAS
   FAURE C, 1985, INT J BIOMED COMPUT, V16, P217, DOI 10.1016/0020-7101(85)90056-X
   GABOR AJ, 1992, ELECTROEN CLIN NEURO, V83, P271, DOI 10.1016/0013-4694(92)90086-W
   GOTMAN J, 1992, ELECTROEN CLIN NEURO, V83, P12, DOI 10.1016/0013-4694(92)90127-4
   GOTMAN J, 1991, ELECTROEN CLIN NEURO, V79, P11, DOI 10.1016/0013-4694(91)90151-S
   Hearst MA, 1998, IEEE INTELL SYST APP, V13, P18, DOI 10.1109/5254.708428
   Jin J, 2014, IEEE ENG MED BIO, P4435, DOI 10.1109/EMBC.2014.6944608
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
   Lawrence S, 1997, IEEE T NEURAL NETWOR, V8, P98, DOI 10.1109/72.554195
   Peterson LE., 2009, SCHOLARPEDIA, V4, P1883, DOI 10.4249/scholarpedia.1883
   Quinlan J. R., 1986, Machine Learning, V1, P81, DOI 10.1007/BF00116251
   Racette BA, 2014, NEUROLOGY, V82, P2254, DOI 10.1212/WNL.0000000000000509
   Ramabhadran B, 1999, J CLIN NEUROPHYSIOL, V16, P59, DOI 10.1097/00004691-199901000-00006
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Sutskever I., 2013, INT C MACH LEARN, P1139, DOI DOI 10.1007/S00287-015-0911-Z
   Svetnik V, 2003, J CHEM INF COMP SCI, V43, P1947, DOI 10.1021/ci034160g
   WEBBER WRS, 1994, ELECTROEN CLIN NEURO, V91, P194, DOI 10.1016/0013-4694(94)90069-8
   Wilson SB, 2002, CLIN NEUROPHYSIOL, V113, P1873, DOI 10.1016/S1388-2457(02)00297-3
   Wilson SB, 1999, CLIN NEUROPHYSIOL, V110, P404, DOI 10.1016/S1388-2457(98)00023-6
NR 29
TC 60
Z9 61
U1 2
U2 19
PY 2016
BP 754
EP 758
WC Acoustics; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Amirsoleimani, A
   Ahmadi, M
   Ahmadi, A
   Boukadoum, M
AF Amirsoleimani, Amirali
   Ahmadi, Majid
   Ahmadi, Arash
   Boukadoum, Mounir
GP IEEE
TI Brain-inspired Pattern Classification with Memristive Neural Network
   Using the Hodgkin-Huxley Neuron
SO 23RD IEEE INTERNATIONAL CONFERENCE ON ELECTRONICS CIRCUITS AND SYSTEMS
   (ICECS 2016)
SE IEEE International Conference on Electronics Circuits and Systems
DT Proceedings Paper
CT 23rd IEEE International Conference on Electronics, Circuits and Systems
   (ICECS)
CY DEC 11-14, 2016
CL MONACO
DE Memristor; Hudgkin Huxley; Spike-Timing-Dependent-Plasticity (STDP);
   Spiking Neural Network (SNN)
AB Recent findings about using memristor devices to mimic biological synapses in neuromorphic systems open a new vision in neuroscience. Ultra-dense learning architectures can be implemented through the Spike-Timing-Dependent-Plasticity (STDP) mechanism by exploiting these nanoscale nonvolatile devices. In this paper, a Spiking Neural Network (SNN) that uses biologically plausible mechanisms is implemented. The proposed SNN relies on Hodgkin-Huxley neurons and memristor-based synapses to implement a bio-inspired neuromorphic platform. The behavior of the proposed SNN and its learning mechanism are discussed, and test results are provided to show the effectiveness of the proposed design for pattern classification applications.
C1 [Amirsoleimani, Amirali; Ahmadi, Majid; Ahmadi, Arash] Univ Windsor, Res Ctr Integrated Microsyst, Windsor, ON, Canada.
   [Boukadoum, Mounir] Univ Quebec Montreal, Microelect Prototyping Res Lab, Montreal, PQ, Canada.
RP Amirsoleimani, A (corresponding author), Univ Windsor, Res Ctr Integrated Microsyst, Windsor, ON, Canada.
EM amirsol@uwindsor.ca; ahmadi@uwindsor.ca; aahmadi@uwindsor.ca;
   boukadoum.mounir@uqam.ca
CR Amirsoleimani A, 2016, IEEE INT SYMP CIRC S, P1242, DOI 10.1109/ISCAS.2016.7527472
   [Anonymous], CIRC THEOR DES ECCTD
   [Anonymous], NEURAL COMPUTING APP
   Biolek Z, 2009, RADIOENGINEERING, V18, P210
   Cantley KD, 2012, IEEE T NEUR NET LEAR, V23, P565, DOI 10.1109/TNNLS.2012.2184801
   Chua L, 2013, NANOTECHNOLOGY, V24, DOI 10.1088/0957-4484/24/38/383001
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Indiveri G, 2013, NANOTECHNOLOGY, V24, DOI 10.1088/0957-4484/24/38/384010
   Serrano-Gotarredona T, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00002
   Teimoory M, 2014, IEEE I C ELECT CIRC, P562, DOI 10.1109/ICECS.2014.7050047
   Yenpo Ho, 2009, Proceedings of the 2009 IEEE/ACM International Conference on Computer-Aided Design (ICCAD 2009), P485
NR 11
TC 5
Z9 5
U1 1
U2 16
PY 2016
BP 81
EP 84
WC Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Li, XM
   Luo, SY
   Xue, FZ
AF Li, Xiumin
   Luo, Shengyuan
   Xue, Fangzheng
TI Effects of synaptic integration on the dynamics and computational
   performance of spiking neural network
SO COGNITIVE NEURODYNAMICS
DT Article
DE Synaptic integration; Spiking neural network; Dynamics; Computational
   performance
ID NEURONAL NETWORKS; DENDRITES; NERVE; MODEL
AB Neurons in the brain receive thousands of synaptic inputs from other neurons. This afferent information is processed by neurons through synaptic integration, which is an important information processing mechanism in biological neural networks. Synaptic currents integrated from spiking trains of presynaptic neurons have complex nonlinear dynamics which endow neurons with significant computational abilities. However, in many computational studies of neural networks, external input currents are often simply taken as a direct current that is static. In this paper, the influences of synaptic and noise external currents on the dynamics of spiking neural network and its computational capability have been investigated in detail. Our results show that due to the nonlinear synaptic integration, both of fast and slow excitatory synaptic currents have much more complex and oscillatory fluctuations than the noise current with the same average intensity. Thus network driven by synaptic external current exhibits remarkably more complex dynamics than that driven by noise external current. Interestingly, the enhancement of network activity is beneficial for information transmission, which is further supported by two computational tasks conducted on the liquid state machine (LSM) network. LSM with synaptic external current displays considerably better performance in both nonlinear fitting and pattern classification than that with noise external current. Synaptic integration can significantly enhance the entropy of activity patterns and computational performance of LSM. Our results demonstrate that the complex dynamics of nonlinear synaptic integration play a critical role in the computational abilities of neural networks and should be more broadly considered in the modelling studies of spiking neural networks.
C1 [Li, Xiumin; Luo, Shengyuan; Xue, Fangzheng] Chongqing Univ, Coll Automat, Chongqing 400044, Peoples R China.
RP Li, XM (corresponding author), Chongqing Univ, Coll Automat, Chongqing 400044, Peoples R China.
EM xmli@cqu.edu.cn
CR Chen MJ, 2017, NONLINEAR DYNAM, V88, P2491, DOI 10.1007/s11071-017-3391-7
   Dan Y, 1996, J NEUROSCI, V16, P3351
   Destexhe A., 1998, METHODS NEURONAL MOD, V2, P1, DOI DOI 10.1111/J.1460-9568.2006.04992.X
   FITZHUGH R, 1961, BIOPHYS J, V1, P445, DOI 10.1016/S0006-3495(61)86902-6
   Garrigan P, 2010, PLOS COMPUT BIOL, V6, DOI 10.1371/journal.pcbi.1000677
   Gosak M, 2018, PHYS LIFE REV
   Gulledge AT, 2005, J NEUROBIOL, V64, P75, DOI 10.1002/neu.20144
   Guo DQ, 2018, EPL-EUROPHYS LETT, V124, DOI 10.1209/0295-5075/124/50001
   Guo DQ, 2016, EPL-EUROPHYS LETT, V114, DOI 10.1209/0295-5075/114/30001
   Guo DQ, 2016, SCI REP-UK, V6, DOI 10.1038/srep26096
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Hausler S., 2003, Complexity, V8, P39, DOI 10.1002/cplx.10089
   HODGKIN AL, 1990, B MATH BIOL, V52, P25, DOI 10.1016/S0092-8240(05)80004-7
   Howard MA, 2016, J NEUROPHYSIOL, V116, P472, DOI 10.1152/jn.00321.2016
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Justus D, 2017, NAT NEUROSCI, V20, P16, DOI 10.1038/nn.4447
   Kumamoto N, 2012, NAT NEUROSCI, V15, P399, DOI 10.1038/nn.3042
   LAUGHLIN S, 1981, Z NATURFORSCH C, V36, P910
   Li XM, 2017, PHILOS T R SOC A, V375, DOI 10.1098/rsta.2016.0286
   Li XM, 2017, NEUROCOMPUTING, V243, P155, DOI 10.1016/j.neucom.2017.03.022
   Li XM, 2016, COGN NEURODYNAMICS, V10, P415, DOI 10.1007/s11571-016-9387-z
   Li XM, 2014, COGN NEURODYNAMICS, V8, P81, DOI 10.1007/s11571-013-9252-2
   Li XM, 2013, J NEUROPHYSIOL, V109, P2739, DOI 10.1152/jn.00397.2012
   Li XM, 2011, CORTICAL OSCILLATION
   Liu SB, 2013, NONLINEAR DYNAM, V73, P1055, DOI 10.1007/s11071-013-0852-5
   Maass W, 2007, PLOS COMPUT BIOL, V3, P15, DOI 10.1371/journal.pcbi.0020165
   Majhi S, 2019, PHYS LIFE REV, V28, P100, DOI 10.1016/j.plrev.2018.09.003
   Natschlager T., 2002, SPECIAL ISSUE FDN IN, P39, DOI [DOI 10.1017/CBO9781107415324.004, 10.1017/CBO9781107415324.004]
   Neftci EO, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00241
   Shew WL, 2011, J NEUROSCI, V31, P55, DOI 10.1523/JNEUROSCI.4637-10.2011
   Spruston N, 2008, NAT REV NEUROSCI, V9, P206, DOI 10.1038/nrn2286
   Sultan S, 2015, NEURON, V88, DOI 10.1016/j.neuron.2015.10.037
   Van Vreeswijk C, 1994, J Comput Neurosci, V1, P313
   Vargas-Caballero M, 2004, J NEUROSCI, V24, P6171, DOI 10.1523/JNEUROSCI.1380-04.2004
   Wang R, 2017, NONLINEAR DYNAM, V88, P1855, DOI 10.1007/s11071-017-3348-x
   WANG XJ, 1992, NEURAL COMPUT, V4, P84, DOI 10.1162/neco.1992.4.1.84
   Williams SR, 2002, SYNAPTIC INTEGRATION
   Yilmaz E, 2015, PHYSICA A
   Yilmaz E, 2016, SCI REP-UK, V6, DOI 10.1038/srep30914
   Zhou J, 2009, IEEE T NEURAL NETWOR, V20, P1679, DOI 10.1109/TNN.2009.2029102
NR 40
TC 10
Z9 11
U1 1
U2 13
PD JUN
PY 2020
VL 14
IS 3
BP 347
EP 357
DI 10.1007/s11571-020-09572-y
EA FEB 2020
WC Neurosciences
DA 2023-11-11
ER

PT C
AU Hao, JY
   Hao, XY
   Wang, J
   Yang, SM
   Deng, B
   Yu, HT
AF Hao, Jingyi
   Hao, Xinyu
   Wang, Jiang
   Yang, Shuangming
   Deng, Bin
   Yu, Haitao
GP IEEE
BE Fu, M
   Sun, J
TI Behavior of a Hippocampal Spiking Network and FPGA Implementation
SO PROCEEDINGS OF THE 38TH CHINESE CONTROL CONFERENCE (CCC)
SE Chinese Control Conference
DT Proceedings Paper
CT 38th Chinese Control Conference (CCC)
CY JUL 27-30, 2019
CL Guangzhou, PEOPLES R CHINA
DE Hippocampus; spiking neural network; memory-related behavior; Field
   Programmable Gate Array
ID MEDIAL TEMPORAL-LOBE; RECOGNITION MEMORY; NEURON
AB The hippocampus in human brain is responsible for memory processing and tasks learning, which has long been one of the main interests of many researchers. In this paper, a spiking neural network of the hippocampus is realized based on a designed task. In the task, the model rat firstly gets familiar with the environment and finds item in one of the pots and finally gets a reward for making the correct response. Through the network, the feature of the hippocampal neurons can be simulated in software successfully. The three-layer network was built based on the spike-timing dependent synaptic plasticity and the synaptic weights will be modified between layers during task which can finally achieve the memory-related behavior of the model rat. Besides software realization, we also utilize Field Programmable Gate Array (FPGA) to reproduce the characteristics of the network in real time. The results show that the spiking neural network of the hippocampus can mimic the memory-related behavior of the model rat.
C1 [Hao, Jingyi; Hao, Xinyu; Wang, Jiang; Yang, Shuangming; Deng, Bin; Yu, Haitao] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
RP Yu, HT (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
EM htyu@tju.edu.cn
CR Barker GRI, 2011, J NEUROSCI, V31, P10721, DOI 10.1523/JNEUROSCI.6413-10.2011
   BERGER TW, 1983, BEHAV BRAIN RES, V8, P49, DOI 10.1016/0166-4328(83)90171-7
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Eichenbaum H, 2007, ANNU REV NEUROSCI, V30, P123, DOI 10.1146/annurev.neuro.30.051606.094328
   EICHENBAUM H, 1988, BEHAV NEUROSCI, V102, P331, DOI 10.1037/0735-7044.102.3.331
   Ferbinteanu J, 2003, NEURON, V40, P1227, DOI 10.1016/S0896-6273(03)00752-9
   Klausberger T., SCIENCE, V321
   Komorowski RW, 2009, J NEUROSCI, V29, P9918, DOI 10.1523/JNEUROSCI.1378-09.2009
   Lee I, 2008, LEARN MEMORY, V15, P357, DOI 10.1101/lm.902708
   Mak TST, 2006, IEEE T NEUR SYS REH, V14, P410, DOI 10.1109/TNSRE.2006.886727
   Manns JR, 2003, NEURON, V37, P171, DOI 10.1016/S0896-6273(02)01147-9
   Matsubara T, 2013, IEEE T NEUR NET LEAR, V24, P736, DOI 10.1109/TNNLS.2012.2230643
   Paolo B., 2013, FRONTIERS NEURAL CIR, V7
   Raudies F, 2014, FRONT SYST NEUROSCI, V8, DOI 10.3389/fnsys.2014.00178
   Squire LR, 2007, NAT REV NEUROSCI, V8, P872, DOI 10.1038/nrn2154
   SQUIRE LR, 1991, SCIENCE, V253, P1380, DOI 10.1126/science.1896849
   Weinstein RK, 2006, J NEURAL ENG, V3, P21, DOI 10.1088/1741-2560/3/1/003
   Wood ER, 2000, NEURON, V27, P623, DOI 10.1016/S0896-6273(00)00071-4
   Yang SM, 2018, NEUROCOMPUTING, V282, P262, DOI 10.1016/j.neucom.2017.12.031
   Yang SM, 2018, PHYSICA A, V494, P484, DOI 10.1016/j.physa.2017.11.155
   Yang SM, 2016, NEUROCOMPUTING, V177, P274, DOI 10.1016/j.neucom.2015.11.026
   Yang SM, 2015, NEURAL NETWORKS, V71, P62, DOI 10.1016/j.neunet.2015.07.017
NR 22
TC 1
Z9 1
U1 0
U2 0
PY 2019
BP 8433
EP 8438
DI 10.23919/chicc.2019.8866461
WC Automation & Control Systems; Computer Science, Artificial Intelligence;
   Engineering, Electrical & Electronic; Operations Research & Management
   Science; Mathematics, Applied
DA 2023-11-11
ER

PT J
AU Zhang, L
AF Zhang, Lei
TI Building Logistic Spiking Neuron Models Using Analytical Approach
SO IEEE ACCESS
DT Article
DE Analytical models; bifurcation; chaos; nonlinear dynamical systems;
   logistic map; logistic function; spiking neural model;
   integrate-and-fire; quadratic integrate-and-fire; differential equations
AB Spiking neuron models are inspired by biological neurons. They can simulate the neuronal activities of the mammalian brains, such as spiking (integrator) and periodic oscillation (resonator). A spiking neural network consisting of a cluster of spiking neurons can be used to simulate the collective dynamic behaviors of a brain neural network. This paper presents step-by-step analyses for the non-linear dynamics of mathematical spiking neuron models and sets forth a novel spiking model based on logistic function using an analytical approach. The logistic function is a well-known one-dimensional dynamical system and can generate spiking or periodic oscillation based on the system parameter. The novel spiking neural model is a combination of the integrate-and-fire and the quadratic integrate-and-fire neuron models with an added parameter to control the neural dynamics in order to generate stable, periodic, or chaotic neural behavior with fiexibility. The analytical approach presented in this paper can be applied extensively to the design and analyses of multi-dimensional neuron models. The goal of this research project is to understand the dynamical behaviors of biological neurons in order to design biologically inspired spiking neuron model for building artificial intelligence, treating cognitive disorders, and advancing the scientific frontiers of brain research.
C1 [Zhang, Lei] Univ Regina, Fac Engn & Appl Sci, Regina, SK S4S 0A2, Canada.
RP Zhang, L (corresponding author), Univ Regina, Fac Engn & Appl Sci, Regina, SK S4S 0A2, Canada.
EM lei.zhang@uregina.ca
CR Boeing G, 2016, SYSTEMS-BASEL, V4, DOI 10.3390/systems4040037
   Hashimoto S, 2010, IEEE T CIRCUITS-I, V57, P2168, DOI 10.1109/TCSI.2010.2041507
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Izhikevich E M, 2007, DYNAM SYST, DOI [DOI 10.1017/S0143385704000173, DOI 10.7551/MITPRESS/2526.001.0001]
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Ji HF, 2018, E ASIAN J APPL MATH, V8, P1, DOI 10.4208/eajam.160217.070717a
   Li X, 2018, ACS SUSTAIN CHEM ENG, V6, P10606, DOI 10.1021/acssuschemeng.8b01934
   Lynch S., 2015, U. S. Patent, Patent No. [20 130 093 458 A1, 20130093458]
   Lynch S., 2013, P IEEE 14 INT SUP EL, P1
   Maguire LP, 2007, NEUROCOMPUTING, V71, P13, DOI 10.1016/j.neucom.2006.11.029
   Mitchell Tom M., 1997, MACH LEARN, P96
   Weisstein EW, LOGISTIC EQUATION MA
   WULFRAM G, 2002, SPIKING NEURON MODEL
   Zhang L., 2018, P 17 IEEE INT C COGN, P145
   Zhang L., 2017, P THE 2 INT C NEUROS, P14
   Zhang L, 2018, 2018 JOINT 7TH INTERNATIONAL CONFERENCE ON INFORMATICS, ELECTRONICS & VISION (ICIEV) AND 2018 2ND INTERNATIONAL CONFERENCE ON IMAGING, VISION & PATTERN RECOGNITION (ICIVPR), P388, DOI 10.1109/ICIEV.2018.8641053
NR 16
TC 4
Z9 4
U1 1
U2 2
PY 2019
VL 7
BP 80443
EP 80452
DI 10.1109/ACCESS.2019.2921003
WC Computer Science, Information Systems; Engineering, Electrical &
   Electronic; Telecommunications
DA 2023-11-11
ER

PT J
AU Kang, P
   Banerjee, S
   Chopp, H
   Katsaggelos, A
   Cossairt, O
AF Kang, Peng
   Banerjee, Srutarshi
   Chopp, Henry
   Katsaggelos, Aggelos
   Cossairt, Oliver
TI Boost event-driven tactile learning with location spiking neurons
SO FRONTIERS IN NEUROSCIENCE
DT Article
DE Spiking Neural Networks; spiking neuron models; location spiking
   neurons; event-driven tactile learning; event-driven tactile object
   recognition; event-driven tactile slip detection; robotic manipulation
ID NETWORK
AB Tactile sensing is essential for a variety of daily tasks. Inspired by the event-driven nature and sparse spiking communication of the biological systems, recent advances in event-driven tactile sensors and Spiking Neural Networks (SNNs) spur the research in related fields. However, SNN-enabled event-driven tactile learning is still in its infancy due to the limited representation abilities of existing spiking neurons and high spatio-temporal complexity in the event-driven tactile data. In this paper, to improve the representation capability of existing spiking neurons, we propose a novel neuron model called "location spiking neuron," which enables us to extract features of event-based data in a novel way. Specifically, based on the classical Time Spike Response Model (TSRM), we develop the Location Spike Response Model (LSRM). In addition, based on the most commonly-used Time Leaky Integrate-and-Fire (TLIF) model, we develop the Location Leaky Integrate-and-Fire (LLIF) model. Moreover, to demonstrate the representation effectiveness of our proposed neurons and capture the complex spatio-temporal dependencies in the event-driven tactile data, we exploit the location spiking neurons to propose two hybrid models for event-driven tactile learning. Specifically, the first hybrid model combines a fully-connected SNN with TSRM neurons and a fully-connected SNN with LSRM neurons. And the second hybrid model fuses the spatial spiking graph neural network with TLIF neurons and the temporal spiking graph neural network with LLIF neurons. Extensive experiments demonstrate the significant improvements of our models over the state-of-the-art methods on event-driven tactile learning, including event-driven tactile object recognition and event-driven slip detection. Moreover, compared to the counterpart artificial neural networks (ANNs), our SNN models are 10x to 100x energy-efficient, which shows the superior energy efficiency of our models and may bring new opportunities to the spike-based learning community and neuromorphic engineering. Finally, we thoroughly examine the advantages and limitations of various spiking neurons and discuss the broad applicability and potential impact of this work on other spike-based learning applications.
C1 [Kang, Peng; Cossairt, Oliver] Northwestern Univ, Dept Comp Sci, Evanston, IL 60208 USA.
   [Banerjee, Srutarshi; Chopp, Henry; Katsaggelos, Aggelos] Northwestern Univ, Dept Elect & Comp Engn, Evanston, IL USA.
RP Kang, P (corresponding author), Northwestern Univ, Dept Comp Sci, Evanston, IL 60208 USA.
EM pengkang2022@u.northwestern.edu
CR Abbott LF, 1999, BRAIN RES BULL, V50, P303, DOI 10.1016/S0361-9230(99)00161-6
   Anumula J, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00023
   Baishya SS, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P8, DOI 10.1109/IROS.2016.7758088
   Bullmore ET, 2012, NAT REV NEUROSCI, V13, P336, DOI 10.1038/nrn3214
   Calandra R, 2018, IEEE ROBOT AUTOM LET, V3, P3300, DOI 10.1109/LRA.2018.2852779
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Chan V, 2007, IEEE T CIRCUITS-I, V54, P48, DOI 10.1109/TCSI.2006.887979
   Chankyu Lee, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12374), P366, DOI 10.1007/978-3-030-58526-6_22
   Cheng X, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1519
   Cho KYHY, 2014, Arxiv, DOI [arXiv:1406.1078, DOI 10.48550/ARXIV.1406.1078]
   Clevert DA, 2016, Arxiv, DOI [arXiv:1511.07289, DOI 10.48550/ARXIV.1511.07289]
   Davies M, 2021, P IEEE, V109, P911, DOI 10.1109/JPROC.2021.3067593
   Dosovitskiy A, 2021, Arxiv, DOI [arXiv:2010.11929, DOI 10.48550/ARXIV.2010.11929]
   Du J, 2018, Arxiv, DOI [arXiv:1710.10370, 10.48550/ARXIV.1710.10370]
   Felleman DJ, 1991, CEREB CORTEX, V1, P1, DOI 10.1093/cercor/1.1.1
   Fishel JA, 2012, P IEEE RAS-EMBS INT, P1122, DOI 10.1109/BioRob.2012.6290741
   Gallego G., 2020, IEEE T PAT ANAL MACH, V2020, P8405, DOI [10.48550/arXiv.1904.08405, DOI 10.48550/ARXIV.1904.08405]
   Gandarias JM, 2019, 2019 IEEE WORLD HAPTICS CONFERENCE (WHC), P551, DOI [10.1109/WHC.2019.8816162, 10.1109/whc.2019.8816162]
   GERSTNER W, 1995, PHYS REV E, V51, P738, DOI 10.1103/PhysRevE.51.738
   Gerstner W., 2002, SPIKING NEURON MODEL
   Gu FQ, 2020, IEEE INT C INT ROBOT, P9876, DOI 10.1109/IROS45743.2020.9341421
   Horowitz M, 2014, ISSCC DIG TECH PAP I, V57, P10, DOI 10.1109/ISSCC.2014.6757323
   Kangwarnchokchai P, 2022, BIOMED ENG INT CONF, DOI 10.1109/BMEiCON56653.2022.10012078
   Kappassov Z, 2015, ROBOT AUTON SYST, V74, P195, DOI 10.1016/j.robot.2015.07.015
   Leonard R.G., 1993, TIDIGITS SPEECH CORP
   Li D, 2016, PROCEEDINGS OF 2016 IEEE INTERNATIONAL CONFERENCES ON BIG DATA AND CLOUD COMPUTING (BDCLOUD 2016) SOCIAL COMPUTING AND NETWORKING (SOCIALCOM 2016) SUSTAINABLE COMPUTING AND COMMUNICATIONS (SUSTAINCOM 2016) (BDCLOUD-SOCIALCOM-SUSTAINCOM 2016), P477, DOI 10.1109/BDCloud-SocialCom-SustainCom.2016.76
   Li Wenshuo, 2022, P IEEECVF C COMPUTER, P783
   Maas AL, 2013, PROC ICML
   Maass W, 2001, PULSED NEURAL NETWOR
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Roy K, 2019, NATURE, V575, P607, DOI 10.1038/s41586-019-1677-2
   Sanchez J, 2018, IEEE INT C INT ROBOT, P504, DOI 10.1109/IROS.2018.8594314
   Schmitz A, 2010, IEEE INT C INT ROBOT, P2212, DOI 10.1109/IROS.2010.5648838
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Shrestha S. B., 2018, P 32 INT C NEUR INF, P1419, DOI [10.5555/3326943.3327073, DOI 10.5555/3326943.3327073]
   Soh H, 2014, IEEE T HAPTICS, V7, P512, DOI 10.1109/TOH.2014.2326159
   Strubell E, 2019, Arxiv, DOI [arXiv:1906.02243, DOI 10.48550/ARXIV.1906.02243]
   Taunyazov T, 2021, Arxiv, DOI arXiv:2106.00489
   Taunyazov T, 2020, IEEE INT C INT ROBOT, P9890, DOI 10.1109/IROS45743.2020.9340693
   Taunyazov T, 2019, IEEE INT CONF ROBOT, P4269, DOI [10.1109/icra.2019.8793967, 10.1109/ICRA.2019.8793967]
   Taunyazoz T., 2020, P ROB SCI SYST
   Tolstikhin I., 2021, ADV NEURAL INF PROCE, V34, P24261, DOI DOI 10.48550/ARXIV.2105.01601
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Xu B, 2015, Arxiv, DOI [arXiv:1505.00853, DOI 10.48550/ARXIV.1505.00853]
   Xu M., 2021, P 30 INT JOINT C ART, P3207
NR 46
TC 0
Z9 0
U1 9
U2 9
PD APR 21
PY 2023
VL 17
AR 1127537
DI 10.3389/fnins.2023.1127537
WC Neurosciences
DA 2023-11-11
ER

PT C
AU OZDAMAR, O
   ZHU, GL
   YAYLALI, I
   JAYAKAR, P
AF OZDAMAR, O
   ZHU, GL
   YAYLALI, I
   JAYAKAR, P
BE Morucci, JP
   Plonsey, R
   Coatrieux, JL
   Laxminarayan, S
TI REAL-TIME DETECTION OF EEG SPIKES USING NEURAL NETWORKS
SO PROCEEDINGS OF THE ANNUAL INTERNATIONAL CONFERENCE OF THE IEEE
   ENGINEERING IN MEDICINE AND BIOLOGY SOCIETY, VOL 14, PTS 1-7
SE PROCEEDINGS OF THE ANNUAL INTERNATIONAL CONFERENCE OF THE IEEE
   ENGINEERING IN MEDICINE AND BIOLOGY SOCIETY
DT Proceedings Paper
CT 14TH ANNUAL INTERNATIONAL CONF OF THE IEEE ENGINEERING IN MEDICINE AND
   BIOLOGY SOCIETY
CY OCT 29-NOV 01, 1992
CL PARIS, FRANCE
NR 0
TC 4
Z9 4
U1 0
U2 0
PY 1992
VL 14
BP 1022
EP 1023
WC Engineering, Biomedical
DA 2023-11-11
ER

PT J
AU Jia, SC
   Zhang, TL
   Cheng, X
   Liu, HX
   Xu, B
AF Jia, Shuncheng
   Zhang, Tielin
   Cheng, Xiang
   Liu, Hongxing
   Xu, Bo
TI Neuronal-Plasticity and Reward-Propagation Improved Recurrent Spiking
   Neural Networks
SO FRONTIERS IN NEUROSCIENCE
DT Article
DE spiking neural network; neuronal plasticity; synaptic plasticity; reward
   propagation; sparse connections
AB Different types of dynamics and plasticity principles found through natural neural networks have been well-applied on Spiking neural networks (SNNs) because of their biologically-plausible efficient and robust computations compared to their counterpart deep neural networks (DNNs). Here, we further propose a special Neuronal-plasticity and Reward-propagation improved Recurrent SNN (NRR-SNN). The historically-related adaptive threshold with two channels is highlighted as important neuronal plasticity for increasing the neuronal dynamics, and then global labels instead of errors are used as a reward for the paralleling gradient propagation. Besides, a recurrent loop with proper sparseness is designed for robust computation. Higher accuracy and stronger robust computation are achieved on two sequential datasets (i.e., TIDigits and TIMIT datasets), which to some extent, shows the power of the proposed NRR-SNN with biologically-plausible improvements.
C1 [Jia, Shuncheng; Zhang, Tielin; Cheng, Xiang; Liu, Hongxing; Xu, Bo] Chinese Acad Sci CASIA, Inst Automat, Res Ctr Brain Inspired Intelligence, Beijing, Peoples R China.
   [Jia, Shuncheng; Zhang, Tielin; Cheng, Xiang; Xu, Bo] Univ Chinese Acad Sci UCAS, Sch Artificial Intelligence, Beijing, Peoples R China.
   [Liu, Hongxing] Beijing Univ Technol, Fac Informat Technol, Beijing, Peoples R China.
   [Xu, Bo] Chinese Acad Sci, Ctr Excellence Brain Sci & Intelligence Technol, Shanghai, Peoples R China.
RP Zhang, TL; Xu, B (corresponding author), Chinese Acad Sci CASIA, Inst Automat, Res Ctr Brain Inspired Intelligence, Beijing, Peoples R China.; Zhang, TL; Xu, B (corresponding author), Univ Chinese Acad Sci UCAS, Sch Artificial Intelligence, Beijing, Peoples R China.; Xu, B (corresponding author), Chinese Acad Sci, Ctr Excellence Brain Sci & Intelligence Technol, Shanghai, Peoples R China.
EM tielin.zhang@ia.ac.cn; xubo@ia.ac.cn
CR Nguyen A, 2015, PROC CVPR IEEE, P427, DOI 10.1109/CVPR.2015.7298640
   Bellec G, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-17236-y
   Bengio Y, 2017, NEURAL COMPUT, V29, P555, DOI 10.1162/NECO_a_00934
   Bi GQ, 2001, ANNU REV NEUROSCI, V24, P139, DOI 10.1146/annurev.neuro.24.1.139
   Ciresan D, 2012, PROC CVPR IEEE, P3642, DOI 10.1109/CVPR.2012.6248110
   Dan Y, 2004, NEURON, V44, P23, DOI 10.1016/j.neuron.2004.09.007
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Dong M, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0204596
   Frenkel C., 2019, ARXIV PREPRINT ARXIV, DOI [10.3389/fnins.2021.629892, DOI 10.3389/FNINS.2021.629892]
   Garofolo J.S., 1993, LINGUIST DATA CONSOR, V1993, P15
   GERSTNER W, 1993, BIOL CYBERN, V69, P503, DOI 10.1007/BF01185422
   Gerstner W, 2018, SCHOLARPEDIA, V3, P1343
   Gerstner W., 2014, NEURONAL DYNAMICS SI, DOI [10.1017/CBO9781107447615, DOI 10.1017/CBO9781107447615]
   Hassabis D, 2017, NEURON, V95, P245, DOI 10.1016/j.neuron.2017.06.011
   He K., 2017, P IEEE INT C COMPUTE, DOI 10.1109/ICCV.2017.322
   HODGKIN AL, 1945, J PHYSIOL-LONDON, V104, P176, DOI 10.1113/jphysiol.1945.sp004114
   Hodgkin AL, 1939, NATURE, V144, P710, DOI 10.1038/144710a0
   ITO M, 1989, ANNU REV NEUROSCI, V12, P85, DOI 10.1146/annurev.neuro.12.1.85
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Kim J, 2017, IEEE SIGNAL PROC MAG, V34, P130, DOI 10.1109/MSP.2017.2736018
   Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508
   Leonard R.G., 1993, TIDIGITS LDC93S10
   Maesa A., 2012, J INFORM SECURITY, V3, P335
   NOBLE D, 1962, J PHYSIOL-LONDON, V160, P317, DOI 10.1113/jphysiol.1962.sp006849
   Pan Z., 2019, IEEE IJCNN, P1, DOI 10.1109/IJCNN.2019.8851858
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   TEYLER TJ, 1987, ANNU REV NEUROSCI, V10, P131, DOI 10.1146/annurev.ne.10.030187.001023
   Wu Jialin, 2020, Oncol Res, DOI 10.3727/096504020X16061317886881
   Wu JB, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00199
   Wu JB, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00836
   Yu Q, 2019, IEEE T CYBERNETICS, V49, P2178, DOI 10.1109/TCYB.2018.2821692
   Zeng Y, 2017, SCI CHINA INFORM SCI, V60, DOI 10.1007/s11432-016-0439-4
   Zenke F, 2018, NEURAL COMPUT, V30, P1514, DOI 10.1162/neco_a_01086
   Zhang M., 2020, INT C LEARN REPR
   Zhang T., 2018, 32 AAAI C ARTIFICIAL, P620
   Zhang T., 2018, INT JOINT C ART INT, P1653, DOI [10.24963/ijcai.2018/229, DOI 10.24963/IJCAI.2018/229]
   Zhang Tianyi, 2020, ICLR
   Zhang Y, 2015, IEEE T NEUR NET LEAR, V26, P2635, DOI 10.1109/TNNLS.2015.2388544
   Zhao DC, 2020, FRONT COMPUT NEUROSC, V14, DOI 10.3389/fncom.2020.576841
   ZUCKER RS, 1989, ANNU REV NEUROSCI, V12, P13, DOI 10.1146/annurev.ne.12.030189.000305
NR 41
TC 5
Z9 5
U1 1
U2 13
PD MAR 12
PY 2021
VL 15
AR 654786
DI 10.3389/fnins.2021.654786
WC Neurosciences
DA 2023-11-11
ER

PT J
AU York, LC
   Oram, M
   van Rossum, M
AF York, L. C.
   Oram, M.
   van Rossum, M.
TI Contrast dependent response latency in a spiking neural network
SO PERCEPTION
DT Meeting Abstract
C1 Univ Edinburgh, Edinburgh, Midlothian, Scotland.
   [Oram, M.] Univ St Andrews, St Andrews, Fife, Scotland.
EM lcyork@yahoo.co.uk
NR 0
TC 0
Z9 0
U1 0
U2 0
PY 2007
VL 36
SU S
BP 78
EP 79
WC Ophthalmology; Psychology; Psychology, Experimental
DA 2023-11-11
ER

PT J
AU AMIT, DJ
   TREVES, A
AF AMIT, DJ
   TREVES, A
TI ASSOCIATIVE MEMORY NEURAL NETWORK WITH LOW TEMPORAL SPIKING RATES
SO PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES OF THE UNITED STATES OF
   AMERICA
DT Article
C1 HEBREW UNIV JERUSALEM,RACAH INST PHYS,IL-91904 JERUSALEM,ISRAEL.
RP AMIT, DJ (corresponding author), UNIV ROME LA SAPIENZA,DEPARTIMENTO FIS,IST NAZL FIS NUCL,I-00185 ROME,ITALY.
CR Amit D. J., 1989, MODELING BRAIN FUNCT
   AMIT DJ, 1985, PHYS REV A, V32, P1007, DOI 10.1103/PhysRevA.32.1007
   AMIT DJ, 1988, P NATL ACAD SCI USA, V85, P2141, DOI 10.1073/pnas.85.7.2141
   AMIT DJ, 1987, PHYS REV A, V35, P2293, DOI 10.1103/PhysRevA.35.2293
   AMIT DJ, 1987, ANN PHYS-NEW YORK, V173, P30, DOI 10.1016/0003-4916(87)90092-3
   AMIT DJ, 1989, 89306 HEBR U JER RAC
   AMIT DJ, 1989, J PHYS A, V22, P2205
   ANDERSEN RA, 1983, J NEUROSCI, V3, P532
   Buhmann J, 1988, NEURAL NETWORKS MODE, P360
   Eccles JC, 1964, PHYSL SYNAPSES
   FATT P, 1953, J PHYSIOL-LONDON, V121, P374, DOI 10.1113/jphysiol.1953.sp004952
   GARDNER E, 1988, J PHYS A-MATH GEN, V21, P257, DOI 10.1088/0305-4470/21/1/030
   GLAUBER RJ, 1963, J MATH PHYS, V4, P294, DOI 10.1063/1.1703954
   GOLDBERG ME, 1985, VISION RES, V25, P471, DOI 10.1016/0042-6989(85)90072-0
   HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554
   KLEINFELD D, 1988, BIOPHYS J, V54, P1039, DOI 10.1016/S0006-3495(88)83041-8
   MIYASHITA Y, 1988, NATURE, V331, P68, DOI 10.1038/331068a0
   MORGESTERN I, 1987, HEIDELBERG C GLASSY
   SHINOMOTO S, 1987, BIOL CYBERN, V57, P197, DOI 10.1007/BF00364151
   SUR M, 1984, J NEUROPHYSIOL, V51, P724, DOI 10.1152/jn.1984.51.4.724
   TSODYKS MV, 1988, EUROPHYS LETT, V6, P101, DOI 10.1209/0295-5075/6/2/002
   VIRASORO MA, 1988, EUROPHYS LETT, V7, P293, DOI 10.1209/0295-5075/7/4/002
   WILLSHAW DJ, 1969, NATURE, V222, P960, DOI 10.1038/222960a0
NR 23
TC 34
Z9 35
U1 0
U2 1
PD OCT
PY 1989
VL 86
IS 20
BP 7871
EP 7875
DI 10.1073/pnas.86.20.7871
WC Multidisciplinary Sciences
DA 2023-11-11
ER

PT J
AU Xue, FZ
   Wang, W
   Li, N
   Yang, YC
AF Xue, Fangzheng
   Wang, Wei
   Li, Nan
   Yang, Yuchao
TI FPGA Implementation of Self-Organized Spiking Neural Network Controller
   for Mobile Robots
SO ADVANCES IN MECHANICAL ENGINEERING
DT Article
ID BASAL GANGLIA; NEURONS; MODEL; PLASTICITY; CIRCUITS
AB Spiking neural network, a computational model which uses spikes to process the information, is good candidate for mobile robot controller. In this paper, we present a novel mechanism for controlling mobile robots based on self-organized spiking neural network (SOSNN) and introduce a method for FPGA implementation of this SOSNN. The spiking neuron we used is Izhikevich model. Akey feature of this controller is that it can simulate the process of unconditioned reflex (avoid obstacles using infrared sensor signals) and conditioned reflex (make right choices in multiple T-maze) by spike timing-dependent plasticity (STDP) learning and dopamine-receptor modulation. Experimental results show that the proposed controller is effective and is easy to implement. The FPGA implementation method aims to build up a specific network using generic blocks designed in the MATLAB Simulink environment. The main characteristics of this original solution are: on-chip learning algorithm implementation, high reconfiguration capability, and operation under real time constraints. An extended analysis has been carried out on the hardware resources used to implement the whole SOSNN network, as well as each individual component block.
C1 [Xue, Fangzheng; Wang, Wei; Li, Nan; Yang, Yuchao] Chongqing Univ, Coll Automat, Chongqing 400044, Peoples R China.
RP Xue, FZ (corresponding author), Chongqing Univ, Coll Automat, Chongqing 400044, Peoples R China.
EM xuefangzheng@cqu.edu.cn
CR Alnajjar F., 2005, P INT C COMPUTATIONA, V1, P1134, DOI 10.1109/CIMCA.2005.1631415
   Bamford SA, 2012, IEEE T BIOMED CIRC S, V6, P385, DOI 10.1109/TBCAS.2012.2184285
   Bofill-i-Petit A, 2004, IEEE T NEURAL NETWOR, V15, P1296, DOI 10.1109/TNN.2004.832842
   Chersi F, 2013, NEURAL NETWORKS, V41, P212, DOI 10.1016/j.neunet.2012.11.009
   FITZHUGH R, 1961, BIOPHYS J, V1, P445, DOI 10.1016/S0006-3495(61)86902-6
   Floreano D., 2001, LNCS, P38
   Floreano D, 2006, INT J INTELL SYST, V21, P1005, DOI 10.1002/int.20173
   Hagras H, 2004, IEEE INT CONF ROBOT, P4620, DOI 10.1109/ROBOT.2004.1302446
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V116, P449, DOI 10.1113/jphysiol.1952.sp004717
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   Humphries MD, 2006, J NEUROSCI, V26, P12921, DOI 10.1523/JNEUROSCI.3486-06.2006
   Indiveri G, 2000, SCIENCE, V288, P1189, DOI 10.1126/science.288.5469.1189
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   KNIGHT BW, 1972, J GEN PHYSIOL, V59, P734, DOI 10.1085/jgp.59.6.734
   Li XM, 2010, NEW J PHYS, V12, DOI 10.1088/1367-2630/12/8/083045
   Li XM, 2009, CHAOS, V19, DOI 10.1063/1.3076394
   Maass W, 1999, INFORM COMPUT, V153, P26, DOI 10.1006/inco.1999.2806
   Mayr Christian G, 2010, Front Synaptic Neurosci, V2, P33, DOI 10.3389/fnsyn.2010.00033
   Singer W., 1990, CONCEPT NEUROSCI, V1, P1
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Surmeier DJ, 2007, TRENDS NEUROSCI, V30, P228, DOI 10.1016/j.tins.2007.03.008
   Wang XQ, 2008, NEUROCOMPUTING, V71, P655, DOI 10.1016/j.neucom.2007.08.025
NR 23
TC 3
Z9 3
U1 0
U2 37
PY 2014
AR 180620
DI 10.1155/2014/180620
WC Thermodynamics; Engineering, Mechanical
DA 2023-11-11
ER

PT C
AU Gupta, A
   Long, LN
AF Gupta, Ankur
   Long, Lyle N.
GP IEEE
TI Character recognition using spiking neural networks
SO 2007 IEEE INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS, VOLS 1-6
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks
CY AUG 12-17, 2007
CL Orlando, FL
ID NEURONS; FACE; PLASTICITY
AB A spiking neural network model is used to identify characters in a character set. The network is a two layered structure consisting of integrate-and-fire and active dendrite neurons. There are both excitatory and inhibitory connections in the network. Spike time dependent plasticity (STDP) is used for training. The winner take all mechanism is enforced by the lateral inhibitory connections. It is found that most of the characters are recognized in a character set consisting of 48 characters. The network is trained successfully with increased resolution of the characters. Also, addition of uniform random noise does not decrease its recognition capability.
C1 [Gupta, Ankur] Penn State Univ, Dept Comp Sci & Engn, University Pk, PA 16802 USA.
   [Long, Lyle N.] Penn State Univ, Aerosp Engn & Math, University Pk, PA 16802 USA.
RP Gupta, A (corresponding author), Penn State Univ, Dept Comp Sci & Engn, University Pk, PA 16802 USA.
EM azg139@psu.edu; lnl@psu.edu
CR [Anonymous], PULSED NEURAL NETWOR
   BAIG AR, 2004, EUR S ART NEUR NETW, P561
   Berninger B, 2002, BIOESSAYS, V24, P212, DOI 10.1002/bies.10060
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   BRUCE C, 1981, J NEUROPHYSIOL, V46, P369, DOI 10.1152/jn.1981.46.2.369
   Buonomano DV, 1999, NEURAL COMPUT, V11, P103, DOI 10.1162/089976699300016836
   Dan Y, 2004, NEURON, V44, P23, DOI 10.1016/j.neuron.2004.09.007
   Delorme A, 2001, NEURAL NETWORKS, V14, P795, DOI 10.1016/S0893-6080(01)00049-1
   Gerstner W., 2002, SPIKING NEURON MODEL
   Haykin S., 2004, NEURAL NETWORKS COMP, V2, P41
   JARAMILLO S, RECOGNITION DYNAMIC
   Jeffreys DA, 1996, VIS COGN, V3, P1, DOI 10.1080/713756729
   LONG L, 2005, AIAA INFOTECH AER C
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   Natschlager T, 1998, NETWORK-COMP NEURAL, V9, P319, DOI 10.1088/0954-898X/9/3/003
   Panchev C, 2006, CONNECT SCI, V18, P1, DOI 10.1080/09540090500132385
   Panchev C, 2004, NEUROCOMPUTING, V58, P365, DOI 10.1016/j.neucom.2004.01.068
   PANCHEV C, 2005, THESIS U SUNDERLAND
   PANCHEV C, 2002, P INT C ART NEUR NET, P896
   PERRETT DI, 1982, EXP BRAIN RES, V47, P329
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   Van Rullen R, 1998, BIOSYSTEMS, V48, P229, DOI 10.1016/S0303-2647(98)00070-7
NR 23
TC 38
Z9 39
U1 1
U2 3
PY 2007
BP 53
EP +
DI 10.1109/IJCNN.2007.4370930
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering
DA 2023-11-11
ER

PT C
AU Cios, KJ
   Sala, DM
AF Cios, KJ
   Sala, DM
BE Priddy, KL
   Keller, PE
   Fogel, DB
TI Advances in applications of spiking neuron networks
SO APPLICATIONS AND SCIENCE OF COMPUTATIONAL INTELLIGENCE III
SE PROCEEDINGS OF THE SOCIETY OF PHOTO-OPTICAL INSTRUMENTATION ENGINEERS
   (SPIE)
DT Proceedings Paper
CT 3rd Annual Conference on Applications and Science of Computational
   Intelligence
CY APR 24-27, 2000
CL ORLANDO, FL
DE networks of spiking neurons; modeling; solving graph algorithms
ID RECEPTIVE-FIELDS; SYNAPTIC EFFICACY; NERVE DOMINANCE; MONKEYS; CORTEX;
   MAPS
AB In this paper we present new findings in constructing and applications of artificial neural networks that use a biologically inspired spiking neuron model. The used model is a point neuron with the interaction between neurons described by postsynaptic potentials. The synaptic plasticity is achieved by using a temporal correlation learning rule, specified as a function of time difference between the firings of pre- and post-synaptic neurons. Using this rule we show how certain associations between neurons in a network of spiking neurons can be implemented. As an example we analyze the dynamic properties of networks of laterally connected spiking neurons and we show their capability to self-organize into topological maps in response to external stimulation. In another application we explore the capability networks of spiking neurons to solve graph algorithms by using temporal coding of distances in a given spatial configuration. The paper underlines the importance of temporal dimension in artificial neural network information processing.
C1 Univ Toledo, Toledo, OH 43606 USA.
RP Cios, KJ (corresponding author), Univ Toledo, 2801 W Bancroft St, Toledo, OH 43606 USA.
CR [Anonymous], NEURAL ORG
   [Anonymous], THEORETICAL MECH BIO
   BOWER JM, 1998, BOOK GENESIS
   CALFORD MB, 1991, SOMATOSENS MOT RES, V8, P249, DOI 10.3109/08990229109144748
   Chartrand G., 1993, APPL ALGORITHMIC GRA
   Dykes RW, 1998, J NEUROPHYSIOL, V80, P120, DOI 10.1152/jn.1998.80.1.120
   Evans JR, 1992, OPTIMIZATION ALGORIT
   HOPFIELD JJ, 1985, BIOL CYBERN, V52, P141
   KAAS JH, 1991, ANNU REV NEUROSCI, V14, P137, DOI 10.1146/annurev.ne.14.030191.001033
   Kaas Jon H., 1995, P51
   KOLARIK RC, 1994, J NEUROSCI, V14, P4269
   KUHN R, 1995, MODELS NEURAL NETWOR, V1, P201
   MacGregor R.J., 1987, NEURAL BAIN MODELING
   Markram H, 1996, NATURE, V382, P807, DOI 10.1038/382807a0
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   RECANZONE GH, 1992, J NEUROPHYSIOL, V67, P1031, DOI 10.1152/jn.1992.67.5.1031
   Sala D. M., 1998, Australian Journal of Intelligent Information Processing Systems, V5, P161
   Sala DM, 1999, IEEE T NEURAL NETWOR, V10, P953, DOI 10.1109/72.774270
   SALA DM, 1997, PROGR CONNECTIONIST, V1, P153
   Schroeder CE, 1997, J NEUROPHYSIOL, V77, P522, DOI 10.1152/jn.1997.77.1.522
   Sirosh J, 1994, BIOL CYBERN, V71, P66
   TAHA HA, 1992, OPERATIONS RES
   WANG XQ, 1995, NATURE, V378, P71, DOI 10.1038/378071a0
   WEINBERGER NM, 1995, ANNU REV NEUROSCI, V18, P129
   Xing J, 1996, J NEUROPHYSIOL, V75, P184, DOI 10.1152/jn.1996.75.1.184
NR 25
TC 3
Z9 3
U1 0
U2 1
PY 2000
VL 4055
BP 324
EP 336
DI 10.1117/12.380586
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Computer Science, Interdisciplinary Applications
DA 2023-11-11
ER

PT C
AU Vázquez, RA
   Garro, BA
AF Vazquez, Roberto A.
   Garro, Beatriz A.
BE Tan, Y
   Shi, Y
   Chai, Y
   Wang, G
TI Training Spiking Neurons by Means of Particle Swarm Optimization
SO ADVANCES IN SWARM INTELLIGENCE, PT I
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 2nd International Conference on Swarm Intelligence (ICSI)
CY JUN 12-15, 2011
CL Chongqing, PEOPLES R CHINA
ID NEURAL-NETWORKS; MODEL
AB Meta-heuristic algorithms inspired by nature have been used in a wide range of optimization problems. These types of algorithms have gained popularity in the field of artificial neural networks (ANN). On the other hand, spiking neural networks are a new type of ANN that simulates the behaviour of a biological neural network in a more realistic manner. Furthermore, these neural models have been applied to solve some pattern recognition problems. In this paper, it is proposed the use of the particle swarm optimization (PSO) algorithm to adjust the synaptic weights of a spiking neuron when it is applied to solve a pattern classification task. Given a set of input patterns belonging to K classes, each input pattern is transformed into an input signal. Then, the spiking neuron is stimulated during T ms and the firing rate is computed. After adjusting the synaptic weights of the neural model using the PSO algorithm, input patterns belonging to the same class will generate similar firing rates. On the contrary, input patterns belonging to other classes will generate firing rates different enough to discriminate among the classes. At last, a comparison between the PSO algorithm and a differential evolution algorithm is presented when the spiking neural model is applied to solve non-linear and real object recognition problems.
C1 [Vazquez, Roberto A.] La Salle Univ, Fac Engn, Intelligent Syst Grp, Benjam Franklin 47 Col Condesa, Mexico City 06140, DF, Mexico.
   [Garro, Beatriz A.] Ctr Comp Res, IPN, Mexico City 07738, DF, Mexico.
RP Vázquez, RA (corresponding author), La Salle Univ, Fac Engn, Intelligent Syst Grp, Benjam Franklin 47 Col Condesa, Mexico City 06140, DF, Mexico.
EM ravem@lasallistas.org.mx; bgarrol@ipn.mx
CR [Anonymous], P 14 INT C COMP SUPP
   [Anonymous], 2010, DYNAMICAL SYSTEMS NE
   Di Paolo EA, 2002, ADAPT BEHAV, V10, P243, DOI 10.1177/1059712302010003006
   Garro Beatriz A., 2009, Proceedings 2009 International Joint Conference on Neural Networks (IJCNN 2009 - Atlanta), P938, DOI 10.1109/IJCNN.2009.5178918
   Gudise VG, 2003, PROCEEDINGS OF THE 2003 IEEE SWARM INTELLIGENCE SYMPOSIUM (SIS 03), P110, DOI 10.1109/SIS.2003.1202255
   Hamed HNA, 2009, LECT NOTES COMPUT SC, V5864, P611, DOI 10.1007/978-3-642-10684-2_68
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Kamoi S, 2003, 2003 IEEE INTERNATIONAL SYMPOSIUM ON COMPUTATIONAL INTELLIGENCE IN ROBOTICS AND AUTOMATION, VOLS I-III, PROCEEDINGS, P977
   Kennedy J, 1995, 1995 IEEE INTERNATIONAL CONFERENCE ON NEURAL NETWORKS PROCEEDINGS, VOLS 1-6, P1942, DOI 10.1109/icnn.1995.488968
   Loiselle S, 2005, IEEE IJCNN, P2076
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Murphy P.M., 1994, UCI REPOSITORY MACHI
   Thorpe SJ, 2004, NEUROCOMPUTING, V58, P857, DOI 10.1016/j.neucom.2004.01.138
   de los Monteros RAVE, 2008, NEURAL PROCESS LETT, V28, P189, DOI 10.1007/s11063-008-9089-6
   Vazquez Roberto A., 2010, 2010 7th International Conference on Electrical Engineering, Computing Science and Automatic Control (CCE 2010) (Formerly known as ICEEE), P424, DOI 10.1109/ICEEE.2010.5608622
   Vazquez R.A., 2010, AUSTR J INTELLIGENT, V11, P35
   Vázquez RA, 2010, LECT NOTES ARTIF INT, V6433, P423
   Yu JB, 2007, NEURAL PROCESS LETT, V26, P217, DOI 10.1007/s11063-007-9053-x
   Zhao L, 2009, EXPERT SYST APPL, V36, P2805, DOI 10.1016/j.eswa.2008.01.061
NR 19
TC 11
Z9 11
U1 1
U2 4
PY 2011
VL 6728
BP 242
EP 249
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods
DA 2023-11-11
ER

PT J
AU Réda, A
   Abdel-Kader, G
   Aoued, B
   Ahmed, L
AF Reda, Adjoudj
   Abdel-Kader, Gafour
   Aoued, Boukelif
   Ahmed, Lehireche
TI A Comparison between Artificial and Spike Neural Network for Face
   Recognition
SO TRAITEMENT DU SIGNAL
DT Article
DE Biometric; Pattern Recognition; Artificial Neural Network; Spike Neural
   Network; Image processing
AB Face recognition is the process of the automatic recognition of the person's identity based on individual informations that are included in face image. This technique makes the face recognition possible, to use the person's image, to verify their identity and control access to services such as e-commerce, internet access, physical access control, cellular phones, border control, passport control, database access services, information services, security control for confidential information areas, and remote access to computers. This document demonstrates how a face recognition system can be designed by a conventional artificial neural network and by another more recent neural network, which is called Spike neural network. The latter is developed to capture the important characteristics of the face, to simulate the human visual system and to optimise the computational time, this last characteristic has been one of the driving forces behind the development of spike neural networks. This kind of neural networks has scored a perfect recognition rate of 92.10 %, 95.00 % and 93.10 %, with a maximum training time of 150ms for the three databases, compared to the case when the artificial neural network is applied, where the rate was 71.05 %, 77.50 % and 73.89 %, with a maximum training time of 2 hours 17 min 45 s. Note that the training process of the two networks, on different sets of noisy images, forced the two networks to learn how to deal. In this case, a common problem in the real world.
C1 [Reda, Adjoudj; Abdel-Kader, Gafour; Ahmed, Lehireche] Univ Djillali Liabes Sidi Bel Abbes, Dept Informat, Lab Syst Distribues & Ingn Evolut, EEDIS, Sidi Bel Abbes, Algeria.
   [Aoued, Boukelif] Univ Djillali Liabes Sidi Bel Abbes, Equipe Rech Tech Video, Dept Elect, Lab Telecommun & Traitement Numer Signal, Sidi Bel Abbes, Algeria.
RP Réda, A (corresponding author), Univ Djillali Liabes Sidi Bel Abbes, Dept Informat, Lab Syst Distribues & Ingn Evolut, EEDIS, Sidi Bel Abbes, Algeria.
EM AdjReda@yahoo.fr; gafour@yahoo.com; aboukelif@yahoo.fr; elhir@yahoo.com
CR ADJOUDJ R, 2002, THESIS U SIDI BELABB
   ADJOUDJ R, 2006, THESIS U SIDI BELABB
   ADJOUDJ R, 2003, RIST J, V13, P93
   ADJOUDJ R, 2005, COMMUNICATION SCI TE, V3, P78
   ADJOUDJ R, 2004, INT C COMP COMM CONT
   ADJOUDJ R, 2005, 3 INT C SCI EL TECHN, P174
   ADJOUDJ R, 2004, INT S CONTR COMM SIG
   [Anonymous], [No title captured]
   [Anonymous], 2008, THESIS MICHIGAN STAT
   [Anonymous], 1999, THESIS CARNEGIE MELL
   BELHIA S, 2008, INT C WEB INF TECHN
   BEYMER DJ, 1996, THESIS MIT US
   CARRILLO CM, 2003, THESIS NAVAL POSTGRA
   DAUGMAN J, 1997, FACE GESTURE RECOGNI, V19
   Delorme A, 2001, NEUROCOMPUTING, V38, P539, DOI 10.1016/S0925-2312(01)00403-9
   Demuth H., 2003, NEURAL NETWORK TOOLB
   GAMBOA H, 2007, BIOM S BCC BALT SEPT
   HAYKIN S, 1990, NEURAL NETWORKS COMP
   HSU RL, 2002, THESIS MICHIGAN STAT
   Jain A.K., 2007, HDB BIOMETRICS
   Jain AK, 2007, NATURE, V449, P38, DOI 10.1038/449038a
   Li ZP, 1998, NEURAL COMPUT, V10, P903, DOI 10.1162/089976698300017557
   LU X, 2007, IEEE T PATTERN ANAL
   Maltoni D, 2003, HDB FINGERPRINT RECO
   MITCHELL T, 1997, THESIS CARNEGIE MELL
   NANDAKUMAR K, 2007, IEEE T INFORM FORENS
   NORRIS JS, 1999, THESIS MIT US
   Thorpe S, 1996, NATURE, V381, P520, DOI 10.1038/381520a0
   Thorpe S, 1998, COMPUTATIONAL NEUROSCIENCE: TRENDS IN RESEARCH, P113
   TIAN YL, 1999, RECOGNIZING ACTION U
   VANRULLEN R, 1998, BIOSYSTEMS J, V22
NR 31
TC 1
Z9 1
U1 0
U2 6
PY 2008
VL 25
IS 6
BP 449
EP 458
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
DA 2023-11-11
ER

PT J
AU Calaim, N
   Dehmelt, FA
   Goncalves, PJ
   Machens, CK
AF Calaim, Nuno
   Dehmelt, Florian A.
   Goncalves, Pedro J.
   Machens, Christian K.
TI The geometry of robustness in spiking neural networks
SO ELIFE
DT Article
DE spiking neural networks; robustness; neural coding; None
ID MODEL; REPRESENTATION; DYNAMICS; NEURONS; SYSTEMS; CORTEX; MEMORY;
   DEATH; NOISE
AB Neural systems are remarkably robust against various perturbations, a phenomenon that still requires a clear explanation. Here, we graphically illustrate how neural networks can become robust. We study spiking networks that generate low-dimensional representations, and we show that the neurons' subthreshold voltages are confined to a convex region in a lower-dimensional voltage subspace, which we call a 'bounding box'. Any changes in network parameters (such as number of neurons, dimensionality of inputs, firing thresholds, synaptic weights, or transmission delays) can all be understood as deformations of this bounding box. Using these insights, we show that functionality is preserved as long as perturbations do not destroy the integrity of the bounding box. We suggest that the principles underlying robustness in these networks - low-dimensional representations, heterogeneity of tuning, and precise negative feedback - may be key to understanding the robustness of neural systems at the circuit level.
C1 [Calaim, Nuno; Dehmelt, Florian A.; Machens, Christian K.] Champalimaud Fdn, Champalimaud Neurosci Programme, Lisbon, Portugal.
   [Dehmelt, Florian A.] Tubingen Univ Hosp, Ctr Integrat Neurosci, Tubingen, Germany.
   [Goncalves, Pedro J.] Ctr Adv European Studies & Res CAESAR, Bonn, Germany.
   [Goncalves, Pedro J.] Tech Univ Munich, Dept Elect & Comp Engn, Computat Neuroengn, Munich, Germany.
   [Goncalves, Pedro J.] Tubingen Univ, Machine Learning Sci, Excellence Cluster Machine Learning, Tubingen, Germany.
RP Machens, CK (corresponding author), Champalimaud Fdn, Champalimaud Neurosci Programme, Lisbon, Portugal.
EM christian.machens@neuro.fchampalimaud.org
CR Barak O, 2017, CURR OPIN NEUROBIOL, V46, P1, DOI 10.1016/j.conb.2017.06.003
   Barrett DGT, 2019, CURR OPIN NEUROBIOL, V55, P55, DOI 10.1016/j.conb.2019.01.007
   Barrett DGT, 2016, ELIFE, V5, DOI 10.7554/eLife.12454
   Biggio Battista, 2013, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2013. Proceedings: LNCS 8190, P387, DOI 10.1007/978-3-642-40994-3_25
   Bishop C.M., 1999, NEURAL NETWORKS PATT
   Boahen K, 2017, COMPUT SCI ENG, V19, P14, DOI 10.1109/MCSE.2017.33
   Boerlin M, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003258
   Bourdoukan R., 2012, ADV NEURAL INFORM PR
   Bredesen DE, 2006, NATURE, V443, P796, DOI 10.1038/nature05293
   Brendel W, 2020, PLOS COMPUT BIOL, V16, DOI 10.1371/journal.pcbi.1007692
   Buxó CER, 2020, PLOS COMPUT BIOL, V16, DOI 10.1371/journal.pcbi.1008261
   Carlini N, 2019, Arxiv, DOI arXiv:1902.06705
   Chalk M, 2016, ELIFE, V5, DOI 10.7554/eLife.13824
   Coelho DS, 2018, CELL REP, V25, P3661, DOI 10.1016/j.celrep.2018.11.098
   Csete ME, 2002, SCIENCE, V295, P1664, DOI 10.1126/science.1069981
   Cunningham JP, 2014, NAT NEUROSCI, V17, P1500, DOI 10.1038/nn.3776
   Dalgleish HWP, 2020, ELIFE, V9, DOI 10.7554/eLife.58889
   Destexhe A, 2001, NEUROSCIENCE, V107, P13, DOI 10.1016/S0306-4522(01)00344-X
   Eliasmith C, 2005, NEURAL COMPUT, V17, P1276, DOI 10.1162/0899766053630332
   Eliasmith C., 2004, NEURAL ENG COMPUTATI
   Faisal AA, 2008, NAT REV NEUROSCI, V9, P292, DOI 10.1038/nrn2258
   Fèlix MA, 2015, NAT REV GENET, V16, P483, DOI 10.1038/nrg3949
   Fellous JM, 2003, NEUROSCIENCE, V122, P811, DOI 10.1016/j.neuroscience.2003.08.027
   Fetsch CR, 2018, ELIFE, V7, DOI 10.7554/eLife.36523
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   Goldman MS, 2003, CEREB CORTEX, V13, P1185, DOI 10.1093/cercor/bhg095
   Haddad SA, 2018, NEURON, V100, P609, DOI 10.1016/j.neuron.2018.08.035
   HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554
   Houweling AR, 2008, NATURE, V451, P65, DOI 10.1038/nature06447
   Hromadka T, 2008, PLOS BIOL, V6, P124, DOI 10.1371/journal.pbio.0060016
   Huber D, 2008, NATURE, V451, P61, DOI 10.1038/nature06445
   Keemink SW, 2019, CURR OPIN NEUROBIOL, V58, P112, DOI 10.1016/j.conb.2019.09.004
   Kitano H, 2004, NAT REV GENET, V5, P826, DOI 10.1038/nrg1471
   Koulakov AA, 2002, NAT NEUROSCI, V5, P775, DOI 10.1038/nn893
   Li N, 2016, NATURE, V532, P459, DOI 10.1038/nature17643
   Lim S, 2013, NAT NEUROSCI, V16, P1306, DOI 10.1038/nn.3492
   Löhne A, 2017, EUR J OPER RES, V260, P807, DOI 10.1016/j.ejor.2016.02.039
   Mancoo A, 2020, ADV NEURAL INFORM PR, V33
   Mastrogiuseppe F, 2018, NEURON, V99, P609, DOI 10.1016/j.neuron.2018.07.003
   Moreno E, 2015, CURR BIOL, V25, P955, DOI 10.1016/j.cub.2015.02.014
   Morrison JH, 1997, SCIENCE, V278, P412, DOI 10.1126/science.278.5337.412
   O'Leary T, 2016, CURR BIOL, V26, P2935, DOI 10.1016/j.cub.2016.08.061
   Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0
   Palop JJ, 2006, NATURE, V443, P768, DOI 10.1038/nature05289
   Morcos AS, 2018, Arxiv, DOI [arXiv:1803.06959, DOI 10.48550/ARXIV.1803.06959]
   Savin C., 2014, ADV NEURAL INFORM PR, P27
   Saxena S, 2019, CURR OPIN NEUROBIOL, V55, P103, DOI 10.1016/j.conb.2019.02.002
   Seung HS, 1996, P NATL ACAD SCI USA, V93, P13339, DOI 10.1073/pnas.93.23.13339
   Seung HS, 2000, NEURON, V26, P259, DOI 10.1016/S0896-6273(00)81155-1
   Sussillo D, 2009, NEURON, V63, P544, DOI 10.1016/j.neuron.2009.07.018
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Thalmeier D, 2016, PLOS COMPUT BIOL, V12, DOI 10.1371/journal.pcbi.1004895
   Trouche S, 2016, NAT NEUROSCI, V19, P564, DOI 10.1038/nn.4250
   Turrigiano G, 2012, CSH PERSPECT BIOL, V4, DOI 10.1101/cshperspect.a005736
   Vidaurre D., 2022, HMM MAR SWH 1 REV 7E
   Vogels TP, 2005, ANNU REV NEUROSCI, V28, P357, DOI 10.1146/annurev.neuro.28.061604.135637
   Vyas S, 2020, ANNU REV NEUROSCI, V43, P249, DOI 10.1146/annurev-neuro-092619-094115
   Weinberger, 2013, ADV NEURAL INFORM PR, P1538
   Whitacre James M., 2012, Frontiers in Genetics, V3, P5, DOI [10.3389/fgene.2012.00005, 10.3389/fgene.2012.00067]
   Wohrer A, 2013, PROG NEUROBIOL, V103, P156, DOI 10.1016/j.pneurobio.2012.09.004
   Wolff SBE, 2018, CURR OPIN NEUROBIOL, V49, P84, DOI 10.1016/j.conb.2018.01.004
   Zhang K, 1996, J NEUROSCI, V16, P2112
NR 62
TC 3
Z9 3
U1 3
U2 4
PD MAY 30
PY 2022
VL 11
AR e73276
DI 10.7554/eLife.73276
WC Biology
DA 2023-11-11
ER

PT C
AU Wang, DW
   Chundi, PK
   Kim, SJ
   Yang, MH
   Cerqueira, JP
   Kang, J
   Jung, S
   Kim, S
   Seok, M
AF Wang, Dewei
   Chundi, Pavan Kumar
   Kim, Sung Justin
   Yang, Minhao
   Cerqueira, Joao Pedro
   Kang, Joonsung
   Jung, Seungchul
   Kim, Sangjoon
   Seok, Mingoo
GP IEEE
TI Always-On, Sub-300-nW, Event-Driven Spiking Neural Network based on
   Spike-Driven Clock-Generation and Clock- and Power-Gating for an
   Ultra-Low-Power Intelligent Device
SO 2020 IEEE ASIAN SOLID-STATE CIRCUITS CONFERENCE (A-SSCC)
SE IEEE Asian Solid-State Circuits Conference Proceedings of Technical
   Papers
DT Proceedings Paper
CT IEEE Asian Solid-State Circuits Conference (A-SSCC)
CY NOV 09-11, 2020
CL ELECTR NETWORK
DE always-on device; neuromorphic hardware; spiking neural network;
   event-driven architecture; speech recognition; keyword spotting
AB Always-on artificial intelligent (AI) functions such as keyword spotting (KWS) and visual wake-up tend to dominate total power consumption in ultra-low power devices [1]. A key observation is that the signals to an always- on function are sparse in time, which a spiking neural network (SNN) classifier can leverage for power savings, because the switching activity and power consumption of SNNs tend to scale with spike rate. Toward this goal, we present a novel SNN classifier architecture for always-on functions, demonstrating sub-300nW power consumption at the competitive inference accuracy for a KWS and other always-on classification workloads.
C1 [Wang, Dewei; Chundi, Pavan Kumar; Kim, Sung Justin; Yang, Minhao; Cerqueira, Joao Pedro; Seok, Mingoo] Columbia Univ, New York, NY 10027 USA.
   [Kang, Joonsung; Jung, Seungchul; Kim, Sangjoon] Samsung Elect, Suwon, South Korea.
RP Wang, DW (corresponding author), Columbia Univ, New York, NY 10027 USA.
EM dewei.wang@columbia.edu
CR Badami K., 2015, IEEE INT SOL STAT CI
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Cerqueira JP, 2017, IEEE T VLSI SYST, V25, P189, DOI 10.1109/TVLSI.2016.2576280
   Chen GK, 2019, IEEE J SOLID-ST CIRC, V54, P992, DOI 10.1109/JSSC.2018.2884901
   Chen Yu, 2013, IEEE ACM INT S LOW P
   Coucke A, 2019, INT CONF ACOUST SPEE, P6351, DOI 10.1109/ICASSP.2019.8683474
   Giraldo JSP, 2018, PROC EUR SOLID-STATE, P166, DOI 10.1109/ESSCIRC.2018.8494342
   Guo RQ, 2019, SYMP VLSI CIRCUITS, pC120, DOI [10.23919/vlsic.2019.8778028, 10.23919/VLSIC.2019.8778028]
   Liu J, 2013, INT SYMP ASYNCHRON C, P1, DOI 10.1109/ASYNC.2013.29
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Park J, 2019, ISSCC DIG TECH PAP I, V62, P140, DOI 10.1109/ISSCC.2019.8662398
   Shan WW, 2020, ISSCC DIG TECH PAP I, P230, DOI 10.1109/ISSCC19947.2020.9063000
   Warden Pete, ARXIV180403209CSCL
   Yang MH, 2019, IEEE J SOLID-ST CIRC, V54, P1764, DOI 10.1109/JSSC.2019.2894360
   Yang MH, 2015, IEEE J SOLID-ST CIRC, V50, P2149, DOI 10.1109/JSSC.2015.2425886
NR 15
TC 2
Z9 2
U1 1
U2 3
PY 2020
DI 10.1109/a-sscc48613.2020.9336139
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Van Vaerenbergh, T
   Fiers, M
   Bienstman, P
   Dambre, J
AF Van Vaerenbergh, Thomas
   Fiers, Martin
   Bienstman, Peter
   Dambre, Joni
GP IEEE
TI Towards integrated optical spiking neural networks: delaying spikes on
   chip
SO 2013 SIXTH RIO DE LA PLATA WORKSHOP ON LASER DYNAMICS AND NONLINEAR
   PHOTONICS
SE Rio de la Plata Workshop on Laser Dynamics and Nonlinear Photonics
DT Proceedings Paper
CT 6th Rio De La Plata Workshop on Laser Dynamics and Nonlinear Photonics
CY DEC 09-12, 2013
CL Montevideo, URUGUAY
ID EXCITABILITY
AB To emulate Spiking Neural Networks (SNN) on an integrated photonic chip, one needs both excitable optical components, to mimic the spiking neurons, and delay lines acting as the connections that transfer the pulses between those neurons. Such an optical connection needs to have a delay in the order of magnitude of the internal timescale of the neuron. Recently, we showed how passive silicon-on-insulator microring resonators can be used as an all-optical spiking neuron. In this paper, we investigate how the dynamics of the microring can be used to obtain a meaningful spike delay. The possibility to delay spikes on chip paves the way towards fully integrated optical SNNs.
C1 [Van Vaerenbergh, Thomas; Fiers, Martin; Bienstman, Peter] Univ Ghent, IMEC, Dept Informat Technol, Photon Res Grp, B-9000 Ghent, Belgium.
   [Dambre, Joni] Univ Ghent, Comp Syst Lab, Elect & Informat Syst, B-9000 Ghent, Belgium.
RP Van Vaerenbergh, T (corresponding author), Univ Ghent, IMEC, Dept Informat Technol, Photon Res Grp, St Pietersnieuwstr 41, B-9000 Ghent, Belgium.
EM thomas.vanvaerenbergh@intec.ugent.be
CR Alexander K., EXCITABILITY O UNPUB, V4
   Brunstein M, 2012, PHYS REV A, V85, DOI 10.1103/PhysRevA.85.031803
   Coomans W, 2011, PHYS REV E, V84, DOI 10.1103/PhysRevE.84.036209
   Fiers M, 2012, J OPT SOC AM B, V29, P896, DOI 10.1364/JOSAB.29.000896
   Van Vaerenbergh T, 2013, OPT EXPRESS, V21, P28922, DOI 10.1364/OE.21.028922
   Van Vaerenbergh T, 2012, PHYS REV A, V86, DOI 10.1103/PhysRevA.86.063808
   Van Vaerenbergh T, 2012, OPT EXPRESS, V20, P20292, DOI 10.1364/OE.20.020292
   Xiao Z, 2013, OPT EXPRESS, V21, P21285, DOI 10.1364/OE.21.021285
   Yacomotti AM, 2006, PHYS REV LETT, V97, DOI 10.1103/PhysRevLett.97.143904
NR 9
TC 2
Z9 2
U1 0
U2 4
PY 2013
WC Engineering, Electrical & Electronic; Physics, Applied
DA 2023-11-11
ER

PT J
AU Lobov, SA
   Berdnikova, ES
   Zharinov, AI
   Kurganov, DP
   Kazantsev, VB
AF Lobov, Sergey A.
   Berdnikova, Ekaterina S.
   Zharinov, Alexey I.
   Kurganov, Dmitry P.
   Kazantsev, Victor B.
TI STDP-Driven Rewiring in Spiking Neural Networks under Stimulus-Induced
   and Spontaneous Activity
SO BIOMIMETICS
DT Article
DE spiking neural network; synaptic plasticity; structural plasticity;
   rewiring; learning; wiring vector field; weight vector field; activity
   vector field; STDP
ID STRUCTURAL PLASTICITY; SYNAPTIC PLASTICITY; ADULT; REORGANIZATION;
   SYNAPSES; CAPACITY; MODEL
AB Mathematical and computer simulation of learning in living neural networks have typically focused on changes in the efficiency of synaptic connections represented by synaptic weights in the models. Synaptic plasticity is believed to be the cellular basis for learning and memory. In spiking neural networks composed of dynamical spiking units, a biologically relevant learning rule is based on the so-called spike-timing-dependent plasticity or STDP. However, experimental data suggest that synaptic plasticity is only a part of brain circuit plasticity, which also includes homeostatic and structural plasticity. A model of structural plasticity proposed in this study is based on the activity-dependent appearance and disappearance of synaptic connections. The results of the research indicate that such adaptive rewiring enables the consolidation of the effects of STDP in response to a local external stimulation of a neural network. Subsequently, a vector field approach is used to demonstrate the successive "recording" of spike paths in both functional connectome and synaptic connectome, and finally in the anatomical connectome of the network. Moreover, the findings suggest that the adaptive rewiring could stabilize network dynamics over time in the context of activity patterns' reproducibility. A universal measure of such reproducibility introduced in this article is based on similarity between time-consequent patterns of the special vector fields characterizing both functional and anatomical connectomes.
C1 [Lobov, Sergey A.; Kazantsev, Victor B.] Moscow Inst Phys & Technol, Lab Neurobiomorph Technol, Moscow 117303, Russia.
   [Lobov, Sergey A.; Berdnikova, Ekaterina S.; Zharinov, Alexey I.; Kazantsev, Victor B.] Lobachevsky State Univ Nizhny Novgorod, Neurotechnol Dept, Nizhnii Novgorod 603022, Russia.
   [Kurganov, Dmitry P.; Kazantsev, Victor B.] Samara State Med Univ, Lab Neuromodeling, Samara 443079, Russia.
RP Lobov, SA (corresponding author), Moscow Inst Phys & Technol, Lab Neurobiomorph Technol, Moscow 117303, Russia.; Lobov, SA (corresponding author), Lobachevsky State Univ Nizhny Novgorod, Neurotechnol Dept, Nizhnii Novgorod 603022, Russia.
EM lobov@neuro.nnov.ru; berdnikova-k@mail.ru; zharinov@neuro.nnov.ru;
   dmi21v@gmail.com; kazantsev@neuro.nnov.ru
CR Abrego L, 2021, PHYS REV E, V103, DOI 10.1103/PhysRevE.103.022410
   [Anonymous], 2005, INFORM THEORY INFERE
   Butz M, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003259
   Calvo Tapia C, 2020, COMMUN NONLINEAR SCI, V82, DOI 10.1016/j.cnsns.2019.105065
   Chen Y, 2021, MAIN TRACK, P1713
   Chklovskii DB, 2004, NATURE, V431, P782, DOI 10.1038/nature03012
   DARIANSMITH C, 1994, NATURE, V368, P737, DOI 10.1038/368737a0
   Deger M, 2018, CEREB CORTEX, V28, P1396, DOI 10.1093/cercor/bhx339
   Diaz-Pier S, 2016, FRONT NEUROANAT, V10, DOI 10.3389/fnana.2016.00057
   Dityatev A, 2011, CURR OPIN NEUROBIOL, V21, P353, DOI 10.1016/j.conb.2010.12.006
   Faherty CJ, 2003, DEV BRAIN RES, V141, P55, DOI 10.1016/S0165-3806(02)00642-9
   Gallinaro JV, 2022, PLOS COMPUT BIOL, V18, DOI 10.1371/journal.pcbi.1009836
   Gilbert CD, 1998, PHYSIOL REV, V78, P467, DOI 10.1152/physrev.1998.78.2.467
   Gritsun TA, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0043352
   Grutzendler J, 2002, NATURE, V420, P812, DOI 10.1038/nature01276
   Haykin, 1998, NEURAL NETWORKS COMP
   HEBB D. O., 1949
   HEINEN SJ, 1991, EXP BRAIN RES, V83, P670
   Iglesias J, 2007, BIOSYSTEMS, V89, P287, DOI 10.1016/j.biosystems.2006.05.020
   Ikegaya Y, 2008, PLOS ONE, V3, DOI 10.1371/journal.pone.0003983
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Kazantsev V, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0041646
   Knott GW, 2002, NEURON, V34, P265, DOI 10.1016/S0896-6273(02)00663-3
   KOHONEN T, 1982, BIOL CYBERN, V43, P59, DOI 10.1007/BF00337288
   Kolb B, 1998, ANNU REV PSYCHOL, V49, P43, DOI 10.1146/annurev.psych.49.1.43
   Lazarevich I, 2020, PLOS ONE, V15, DOI 10.1371/journal.pone.0227917
   Limbacher T, 2020, FRONT COMPUT NEUROSC, V14, DOI 10.3389/fncom.2020.00057
   Llinas R. R., 2002, I VORTEX NEURONS SEL
   Lobov S, 2016, EUR PHYS J-SPEC TOP, V225, P29, DOI 10.1140/epjst/e2016-02614-y
   Lobov SA, 2023, MATHEMATICS-BASEL, V11, DOI 10.3390/math11010234
   Lobov SA, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21082678
   Makarov VA, 2022, FRONT COMPUT NEUROSC, V16, DOI 10.3389/fncom.2022.859874
   Makovkin S, 2022, SCI REP-UK, V12, DOI 10.1038/s41598-022-10649-3
   Markram Henry, 2011, Front Synaptic Neurosci, V3, P4, DOI 10.3389/fnsyn.2011.00004
   MERZENICH MM, 1984, J COMP NEUROL, V224, P591, DOI 10.1002/cne.902240408
   Miner D, 2016, PLOS COMPUT BIOL, V12, DOI 10.1371/journal.pcbi.1004759
   Morrison A, 2008, BIOL CYBERN, V98, P459, DOI 10.1007/s00422-008-0233-1
   Pimashkin A, 2011, FRONT COMPUT NEUROSC, V5, DOI 10.3389/fncom.2011.00046
   Poirazi P, 2001, NEURON, V29, P779, DOI 10.1016/S0896-6273(01)00252-5
   Poirazi P, 2000, NEURAL COMPUT, V12, P1189, DOI 10.1162/089976600300015556
   PONS TP, 1991, SCIENCE, V252, P1857, DOI 10.1126/science.1843843
   Raichman N, 2008, J NEUROSCI METH, V170, P96, DOI 10.1016/j.jneumeth.2007.12.020
   Rathi N, 2019, IEEE T COMPUT AID D, V38, P668, DOI 10.1109/TCAD.2018.2819366
   Rentzeperis I, 2021, FRONT SYST NEUROSCI, V15, DOI 10.3389/fnsys.2021.580569
   Shi Y, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00405
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Spiess R, 2016, FRONT COMPUT NEUROSC, V10, DOI 10.3389/fncom.2016.00093
   spneuro, SPIK NEUR NEURONET U
   Trachtenberg JT, 2002, NATURE, V420, P788, DOI 10.1038/nature01273
   Tsodyks M, 1998, NEURAL COMPUT, V10, P821, DOI 10.1162/089976698300017502
   Tsybina Y, 2022, NEURAL COMPUT APPL, V34, P9147, DOI 10.1007/s00521-022-06936-9
   TURNER AM, 1985, BRAIN RES, V329, P195, DOI 10.1016/0006-8993(85)90525-6
   Turrigiano GG, 2017, PHILOS T R SOC B, V372, DOI 10.1098/rstb.2016.0258
   Wagenaar DA, 2006, BMC NEUROSCI, V7, DOI 10.1186/1471-2202-7-11
   Yin J, 2015, FRONT CELL NEUROSCI, V8, DOI 10.3389/fncel.2014.00439
NR 56
TC 0
Z9 0
U1 1
U2 1
PD JUL
PY 2023
VL 8
IS 3
AR 320
DI 10.3390/biomimetics8030320
WC Engineering, Multidisciplinary; Materials Science, Biomaterials
DA 2023-11-11
ER

PT C
AU Hall, EC
   Willett, RM
AF Hall, Eric C.
   Willett, Rebecca M.
GP IEEE
TI Online Learning of Neural Network Structure from Spike Trains
SO 2015 7TH INTERNATIONAL IEEE/EMBS CONFERENCE ON NEURAL ENGINEERING (NER)
SE International IEEE EMBS Conference on Neural Engineering
DT Proceedings Paper
CT 7th Annual International IEEE EMBS Conference on Neural Engineering
   (NER)
CY APR 22-24, 2015
CL Montpellier, FRANCE
DE Autoregressive processes; Hawkes process; Network theory (graphs);
   Online learning
AB Cascading series of events are a salient feature of neural networks, where neuron spikes may stimulate or inhibit spike activity in other neurons. Only individual spike times associated with each neuron are observed, usually without knowledge of the underlying relationships among neurons. This paper addresses the challenge of tracking how spikes within such networks stimulate or influence future events. The proposed approach is an online learning framework well-suited to streaming data, using a multivariate Hawkes point process model to encapsulate autoregressive features of observed events within the network. Recent work on online learning in dynamic environments is leveraged not only to exploit the dynamics within the underlying network, but also to track that network structure as it evolves. Regret bounds and experimental results demonstrate that the proposed method performs nearly as well as an oracle or batch algorithm.
C1 [Hall, Eric C.] Duke Univ, Dept Elect & Comp Engn, Durham, NC 27708 USA.
   [Willett, Rebecca M.] Univ Wisconsin, Dept Elect & Comp Engn, Madison, WI 53706 USA.
RP Hall, EC (corresponding author), Duke Univ, Dept Elect & Comp Engn, Durham, NC 27708 USA.
EM ech11@duke.edu; willett@discovery.wisc.edu
CR Brown EN, 2004, NAT NEUROSCI, V7, P456, DOI 10.1038/nn1228
   Daley DJ, 2003, GEN THEORY STRUCTURE, V2nd
   Hall E., 2013, P INT C MACH LEARN I
   HAWKES AG, 1971, J ROYAL STAT SOC B, V58, P83
   Masud MS, 2011, J NEUROSCI METH, V196, P201, DOI 10.1016/j.jneumeth.2011.01.003
   Pillow JW, 2008, NATURE, V454, P995, DOI 10.1038/nature07140
   Reynaud-Bouret P, 2010, ANN STAT, V38, P2781, DOI 10.1214/10-AOS806
NR 7
TC 4
Z9 4
U1 0
U2 1
PY 2015
BP 930
EP 933
WC Engineering, Biomedical; Neurosciences
DA 2023-11-11
ER

PT S
AU Katayama, K
   Yano, M
   Horiguchi, T
AF Katayama, K
   Yano, M
   Horiguchi, T
BE Pal, NR
   Kasabov, N
   Mudi, RK
   Pal, S
   Parui, SK
TI Synchronous phenomena for two-layered neural network with chaotic
   neurons
SO NEURAL INFORMATION PROCESSING
SE LECTURE NOTES IN COMPUTER SCIENCE
DT Article; Proceedings Paper
CT 11th International Conference on Neural Information Processing
CY NOV 22-25, 2004
CL Calcutta, INDIA
ID SELECTIVE VISUAL-ATTENTION
AB We propose a mathematical model of visual selective attention using a two-layered neural network, based on an assumption proposed by Desimone and Duncan. We use a spiking neuron model proposed by Hayashi and Ishizuka, which generates periodic spikes, quasi-periodic spikes and chaotic spikes. The neural network consists of a layer of hippocampal formation and that of visual cortex. In order to clarify an attention shift, we solve numerically a set of the first-order ordinary differential equations, which describe a time-evolution of neurons. The visual selective attention is considered as the synchronous phenomena between the firing times of the neurons in the hippocampal formation and those in a part of the visual cortex in the present model.
C1 Tohoku Univ, Elect Commun Res Inst, Sendai, Miyagi 9808577, Japan.
   Tohoku Univ, Dept Comp & Math Sci, GSIS, Sendai, Miyagi 9808579, Japan.
RP Katayama, K (corresponding author), Tohoku Univ, Elect Commun Res Inst, Sendai, Miyagi 9808577, Japan.
CR CRICK F, 1984, P NATL ACAD SCI-BIOL, V81, P4586, DOI 10.1073/pnas.81.14.4586
   DESIMONE R, 1995, ANNU REV NEUROSCI, V18, P193, DOI 10.1146/annurev-psych-122414-033400
   HAYASHI H, 1992, J THEOR BIOL, V156, P269, DOI 10.1016/S0022-5193(05)80676-9
   LIJIMA T, 1996, SCIENCE, V272, P1176
   Wu ZH, 1999, BIOL CYBERN, V80, P205, DOI 10.1007/s004220050518
NR 5
TC 0
Z9 0
U1 0
U2 0
PY 2004
VL 3316
BP 19
EP 30
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods
DA 2023-11-11
ER

PT C
AU Oudjail, V
   Martinet, J
AF Oudjail, Veis
   Martinet, Jean
BE Tremeau, A
   Farinella, GM
   Braz, J
TI Bio-inspired Event-based Motion Analysis with Spiking Neural Networks
SO VISAPP: PROCEEDINGS OF THE 14TH INTERNATIONAL JOINT CONFERENCE ON
   COMPUTER VISION, IMAGING AND COMPUTER GRAPHICS THEORY AND APPLICATIONS,
   VOL 4
DT Proceedings Paper
CT 14th International Joint Conference on Computer Vision, Imaging and
   Computer Graphics Theory and Applications (VISAPP)
CY FEB 25-27, 2019
CL Prague, CZECH REPUBLIC
DE Video Analysis; Spiking Neural Networks; Motion Analysis; Address-event
   Representation
ID FEATURES
AB This paper presents an original approach to analyze the motion of a moving pattern with a Spiking Neural Network, using visual data encoded in the Address-Event Representation. Our objective is to identify a minimal network structure able to recognize the motion direction of a simple binary pattern. For this purpose, we generated synthetic data of 3 different patterns moving in 4 directions, and we designed several variants of a one-layer fully-connected feed-forward spiking neural network with varying number of neurons in the output layer. The networks are trained in an unsupervised manner by presenting the synthetic temporal data to the network for a few seconds. The experimental results show that such networks quickly converged to a state where input classes can be successfully distinguished for 2 of the considered patterns, no network configuration did converge for the third pattern. In the convergence cases, the network proved a remarkable stability for several output layer sizes. We also show that the sequential order of presentation of classes impacts the ability of the network to learn the input.
C1 [Oudjail, Veis; Martinet, Jean] Univ Lille, CNRS, Cent Lille, UMR 9189,CRIStAL, Lille, France.
RP Oudjail, V (corresponding author), Univ Lille, CNRS, Cent Lille, UMR 9189,CRIStAL, Lille, France.
CR Amir A, 2017, PROC CVPR IEEE, P7388, DOI 10.1109/CVPR.2017.781
   [Anonymous], 2015, FRONTIERS NEUROSCIEN, DOI [10.3389/fnins.2021.736888, DOI 10.3389/FNINS.2015.00481]
   Bichler O, 2012, NEURAL NETWORKS, V32, P339, DOI 10.1016/j.neunet.2012.02.022
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Goodman DFM, 2009, FRONT NEUROSCI-SWITZ, V3, P192, DOI 10.3389/neuro.01.026.2009
   Hopkins M, 2018, INTERFACE FOCUS, V8, DOI 10.1098/rsfs.2018.0007
   Kreiser R., 2017, BIOM CIRC SYST C BIO, P1
   Lee JM, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00191
   Li HM, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00309
   Lichtsteiner P, 2008, IEEE J SOLID-ST CIRC, V43, P566, DOI 10.1109/JSSC.2007.914337
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Masquelier T, 2007, PLOS COMPUT BIOL, V3, P247, DOI 10.1371/journal.pcbi.0030031
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Orchard G, 2015, FRONT NEUROSCI-SWITZ, V9, DOI [10.3389/fnins.2015.00437, 10.3389/fhins.2015.00437]
   Orchard G, 2013, BIOMED CIRC SYST C, P298, DOI 10.1109/BioCAS.2013.6679698
   Paugam-Moisy H, 2012, HDB NATURAL COMPUTIN, V1, P1, DOI DOI 10.1007/978-3-540-92910-9_10
   Ponulak F, 2011, ACTA NEUROBIOL EXP, V71, P409
   Sourikopoulos I, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00123
   Tavanaei A, 2017, IEEE IJCNN, P2023, DOI 10.1109/IJCNN.2017.7966099
   Zhao B, 2015, IEEE T NEUR NET LEAR, V26, P1963, DOI 10.1109/TNNLS.2014.2362542
NR 20
TC 2
Z9 2
U1 0
U2 0
PY 2019
BP 389
EP 394
DI 10.5220/0007397303890394
WC Computer Science, Software Engineering; Imaging Science & Photographic
   Technology
DA 2023-11-11
ER

PT J
AU Kheradpisheh, SR
   Masquelier, T
AF Kheradpisheh, Saeed Reza
   Masquelier, Timothee
TI Temporal Backpropagation for Spiking Neural Networks with One Spike per
   Neuron
SO INTERNATIONAL JOURNAL OF NEURAL SYSTEMS
DT Article
DE Spiking neural network; supervised learning; temporal backpropagation;
   single spike coding
ID VISUAL FEATURES; INPUT
AB We propose a new supervised learning rule for multilayer spiking neural networks (SNNs) that use a form of temporal coding known as rank-order-coding. With this coding scheme, all neurons fire exactly one spike per stimulus, but the firing order carries information. In particular, in the readout layer, the first neuron to fire determines the class of the stimulus. We derive a new learning rule for this sort of network, named S4NN, akin to traditional error backpropagation, yet based on latencies. We show how approximated error gradients can be computed backward in a feedforward network with any number of layers. This approach reaches state-of-the-art performance with supervised multi-fully connected layer SNNs: test accuracy of 97.4% for the MNIST dataset, and 99.2% for the ('altech Face/Motorbike dataset. Yet, the neuron model that we use, nonleaky integrate-and-fire, is much simpler than the one used in all previous works. The source codes of the proposed S4NN are publicly available at https://github.com/SRKH/S4NN.
C1 [Kheradpisheh, Saeed Reza] Shahid Beheshti Univ, Dept Comp & Data Sci, Fac Math Sci, Tehran, Iran.
   [Masquelier, Timothee] Univ Toulouse 3, CNRS, CERCO, UMR 5549, Toulouse, France.
RP Kheradpisheh, SR (corresponding author), Shahid Beheshti Univ, Dept Comp & Data Sci, Fac Math Sci, Tehran, Iran.
EM s_kheradpisheh@sbu.ac; timothee.masquelier@cnrs.fr
CR Adeli H, 2010, AUTOMATED EEG-BASED DIAGNOSIS OF NEUROLOGICAL DISORDERS: INVENTING THE FUTURE OF NEUROLOGY, P1
   [Anonymous], 2019, INT C LEARN REPR
   [Anonymous], 2017, 2017 IEEE INT S CIRC, DOI DOI 10.1109/ISCAS.2017.8050870
   [Anonymous], 2019, CORR
   Antonietti A, 2018, INT J NEURAL SYST, V28, DOI 10.1142/S012906571850020X
   Antunes G, 2018, INT J NEURAL SYST, V28, DOI 10.1142/S0129065717500587
   Bellec G., 2018, ADV NEURAL INFORM PR, P787
   Bengtsson F, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0056630
   Bernert M, 2019, INT J NEURAL SYST, V29, DOI 10.1142/S0129065718500594
   Bohte SM, 2011, LECT NOTES COMPUT SC, V6791, P60, DOI 10.1007/978-3-642-21735-7_8
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Brasselet R, 2011, NEURAL COMPUT, V23, P852, DOI 10.1162/NECO_a_00099
   Brette R, 2015, FRONT SYST NEUROSCI, V9, DOI 10.3389/fnsys.2015.00151
   Burkitt AN, 2006, BIOL CYBERN, V95, P1, DOI 10.1007/s00422-006-0068-6
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Chen MQ, 2018, INT J NEURAL SYST, V28, DOI 10.1142/S0129065718500193
   Comsa I. M., 2020, IEEE INT C AC SPEECH
   Diehl PU, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON REBOOTING COMPUTING (ICRC)
   Esser SK, 2016, P NATL ACAD SCI USA, V113, P11441, DOI 10.1073/pnas.1604850113
   Falez P., 2019, 2019 INT JOINT C NEU, P1, DOI [DOI 10.1109/IJCNN.2019.8852346, 10.1109/IJCNN.2019.8852346]
   Galán-Prado F, 2019, INT J NEURAL SYST, V29, DOI 10.1142/S0129065719500047
   Geminiani A, 2018, INT J NEURAL SYST, V28, DOI 10.1142/S0129065717500174
   Ghosh-Dastidar S, 2007, INTEGR COMPUT-AID E, V14, P187
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Goltz J., 2019, ARXIV191110124
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P29
   Hu RH, 2019, INT J NEURAL SYST, V29, DOI 10.1142/S0129065719500060
   Huh D, 2018, ADV NEUR IN, V31
   Hung CP, 2005, SCIENCE, V310, P863, DOI 10.1126/science.1117593
   Hunsberger E., ARXIV151008829
   Illing B, 2019, NEURAL NETWORKS, V118, P90, DOI 10.1016/j.neunet.2019.06.001
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Kheradpisheh SR, 2016, NEUROCOMPUTING, V205, P382, DOI 10.1016/j.neucom.2016.04.029
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee JM, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00191
   Maass W., 2019, ARXIV200101682, P1
   Masquelier T, 2007, PLOS COMPUT BIOL, V3, P247, DOI 10.1371/journal.pcbi.0030031
   Mostafa H, 2018, IEEE T NEUR NET LEAR, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Mozafari M., 2019, FRONT NEUROSCI, P1
   Mozafari M, 2019, PATTERN RECOGN, V94, P87, DOI 10.1016/j.patcog.2019.05.015
   Mozafari M, 2018, IEEE T NEUR NET LEAR, V29, P6178, DOI 10.1109/TNNLS.2018.2826721
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Neftci EO, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00324
   Orchard G, 2015, IEEE T PATTERN ANAL, V37, P2028, DOI 10.1109/TPAMI.2015.2392947
   Oster M, 2007, IEEE INT SYMP CIRC S, P853, DOI 10.1109/ISCAS.2007.378040
   Pan LQ, 2017, INT J NEURAL SYST, V27, DOI 10.1142/S0129065717500423
   Peng H, 2017, NEURAL NETWORKS, V95, P66, DOI 10.1016/j.neunet.2017.08.003
   Peng H, 2016, INT J NEURAL SYST, V26, DOI 10.1142/S0129065716500040
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Posch C, 2014, P IEEE, V102, P1470, DOI 10.1109/JPROC.2014.2346153
   Roy K, 2019, NATURE, V575, P607, DOI 10.1038/s41586-019-1677-2
   Rueckauer B, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351295
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Serrano-Gotarredona R, 2009, IEEE T NEURAL NETWOR, V20, P1417, DOI 10.1109/TNN.2009.2023653
   Shrestha Sumit Bam, 2018, ADV NEURAL INFORM PR, P1412
   Taherkhani A, 2020, NEURAL NETWORKS, V122, P253, DOI 10.1016/j.neunet.2019.09.036
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Tavanaei A, 2019, NEUROCOMPUTING, V330, P39, DOI 10.1016/j.neucom.2018.11.014
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   Thorpe S, 1998, COMPUTATIONAL NEUROSCIENCE: TRENDS IN RESEARCH, P113
   THORPE SJ, 1990, PARALLEL PROCESSING IN NEURAL SYSTEMS AND COMPUTERS, P91
   Vaila R., ARXIV190312272
   Vaila R., 2019, P INT C NEUR SYST, P1
   VanRullen R, 2005, TRENDS NEUROSCI, V28, P1, DOI 10.1016/j.tins.2004.10.010
   Wu T., 2017, IEEE T NEURAL NETW L, V29, P3349
   Wu TF, 2018, INT J NEURAL SYST, V28, DOI 10.1142/S0129065718500132
   Wu YJ, 2019, AAAI CONF ARTIF INTE, P1311
   Yousefzadeh A., 2015, IEEE, P1, DOI 10.1109/EBCCSP.2015.7300698
   Zenke F, 2018, NEURAL COMPUT, V30, P1514, DOI 10.1162/neco_a_01086
   Zimmer Romain, 2019, ARXIV191110124
NR 75
TC 92
Z9 96
U1 3
U2 36
PD JUN
PY 2020
VL 30
IS 6
AR 2050027
DI 10.1142/S0129065720500276
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT J
AU Takahashi, T
AF Takahashi, T
TI Analyses on the temporal patterns of spikes of auditory neurons of the
   macaque monkey by means of an artificial neural network and tree-based
   models
SO NEUROSCIENCE LETTERS
DT Article
DE auditory information processing; primary auditory cortex; single-unit
   recording; time scale; tree-based model; artificial neural network;
   cross-validation; temporal patterns of neuronal spikes
ID INFORMATION
AB The time scale over which information in the primary auditory cortex is processed was estimated. An artificial neural network was used to learn the temporal patterns of spikes. After learning, test patterns were input to the network. Comparison of the accuracy of the network with that of the maximum Likelihood function computed from the spike count reveals that the temporal patterns of spikes are closely related to stimulus discrimination. Next, a tree-based model from a subset of the spike trains with a fixed time resolution was constructed and validated the model with another. By repeating this for different bin widths, it was found that there are no simple models for the time bin width larger than 50 ms. This indicates that the time scale in the auditory cortex is not larger than 50 ms. (C) 1997 Elsevier Science ireland Ltd.
C1 JUSTSYST SCI INST,MINATO KU,TOKYO 107,JAPAN.
CR Breiman L., 1984, CLASSIFICATION REGRE, P18
   Clark L.A., 2017, STAT MODELS S, P377
   ESKANDAR EN, 1992, J NEUROPHYSIOL, V68, P1277, DOI 10.1152/jn.1992.68.4.1277
   HASTIE T, 1990, SHRINKING TREES, P11
   HELLER J, 1995, J COMPUT NEUROSCI, V2, P175, DOI 10.1007/BF00961433
   MIDDLEBROOKS JC, 1994, SCIENCE, V264, P842, DOI 10.1126/science.8171339
   OPTICAN LM, 1987, J NEUROPHYSIOL, V57, P162, DOI 10.1152/jn.1987.57.1.162
   Quinlan JR, 1993, C4 5 PROGRAMS MACHIN, P17
   Riquimaroux H., 1994, Journal of the Acoustical Society of Japan (E), V15, P171, DOI 10.1250/ast.15.171
   Takahashi T, 1996, INT J NEUROSCI, V88, P11, DOI 10.3109/00207459608999810
   TAKAHASHI T, 1995, NEUROSCI LETT, V186, P1
NR 11
TC 1
Z9 1
U1 0
U2 1
PD SEP 12
PY 1997
VL 233
IS 1
BP 17
EP 20
DI 10.1016/S0304-3940(97)00612-5
WC Neurosciences
DA 2023-11-11
ER

PT C
AU Dytckov, S
   Daneshtalab, M
   Ebrahimi, M
   Anwar, H
   Plosila, J
   Tenhunen, H
AF Dytckov, Sergei
   Daneshtalab, Masoud
   Ebrahimi, Masoumeh
   Anwar, Hassan
   Plosila, Juha
   Tenhunen, Hannu
GP IEEE
TI Efficient STDP Micro-Architecture for Silicon Spiking Neural Networks
SO 2014 17TH EUROMICRO CONFERENCE ON DIGITAL SYSTEM DESIGN (DSD)
DT Proceedings Paper
CT 17th Euromicro Conference on Digital System Design (DSD)
CY AUG 27-29, 2014
CL Verona, ITALY
DE Spiking Neural Network; Networks-on-Chip; STDP; Neuron Clustering
ID REINFORCEMENT; MODEL
AB Spiking neural networks (SNNs) are the closest approach to biological neurons in comparison with conventional artificial neural networks (ANN). SNNs are composed of neurons and synapses which are interconnected with a complex pattern. As communication in such massively parallel computational systems is getting critical, the network-on-chip (NoC) becomes a promising solution to provide a scalable and robust interconnection fabric. However, using NoC for large-scale SNNs arises a trade-off between scalability, throughput, neuron/router ratio (cluster size), and area overhead. In this paper, we tackle the trade-off using a clustering approach and try to optimize the synaptic resource utilization. An optimal cluster size can provide the lowest area overhead and power consumption. For the learning purposes, a phenomenon known as spike-timing-dependent plasticity (STDP) is utilized. The micro-architectures of the network, clusters, and the computational neurons are also described. The presented approach suggests a promising solution of integrating NoCs and STDP-based SNNs for the optimal performance based on the underlying application.
C1 [Dytckov, Sergei; Daneshtalab, Masoud; Ebrahimi, Masoumeh; Plosila, Juha; Tenhunen, Hannu] Univ Turku, SF-20500 Turku, Finland.
   [Daneshtalab, Masoud; Ebrahimi, Masoumeh; Tenhunen, Hannu] KTH Royal Inst Technol, Stockholm, Sweden.
   [Anwar, Hassan] Ecole Polytech Montreal, Montreal, PQ, Canada.
RP Dytckov, S (corresponding author), Univ Turku, SF-20500 Turku, Finland.
CR [Anonymous], 2010 INT JOINT C NEU
   [Anonymous], P INT WORKSH MAN EMB
   [Anonymous], P 14 INT C COMP SUPP
   Arthur J.V., 2006, ADV NEURAL INFORM PR, P75
   Carrillo S., 2012, 2012 Sixth IEEE/ACM International Symposium on Networks-on-Chip (NoCS), P83, DOI 10.1109/NOCS.2012.17
   Carrillo S, 2012, NEURAL NETWORKS, V33, P42, DOI 10.1016/j.neunet.2012.04.004
   Choudhary Swadesh, 2012, Artificial Neural Networks and Machine Learning - ICANN 2012. Proceedings of the 22nd International Conference on Artificial Neural Networks, P121, DOI 10.1007/978-3-642-33269-2_16
   Dugan K, 2013, IET COMPUT DIGIT TEC, V7, P115, DOI 10.1049/iet-cdt.2012.0139
   Emery R, 2009, 2009 3RD ACM/IEEE INTERNATIONAL SYMPOSIUM ON NETWORKS-ON-CHIP, P144, DOI 10.1109/NOCS.2009.5071462
   Florian RV, 2007, NEURAL COMPUT, V19, P1468, DOI 10.1162/neco.2007.19.6.1468
   Gerstner W., 2002, SPIKING NEURON MODEL
   Grossberg S, 2001, NEURAL NETWORKS, V14, P587, DOI 10.1016/S0893-6080(01)00102-2
   Harkin J, 2009, INT J RECONFIGURABLE, V2009, DOI 10.1155/2009/908740
   Indiveri G, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00073
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2007, CEREB CORTEX, V17, P2443, DOI 10.1093/cercor/bhl152
   Legenstein R, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.1000180
   Livi P, 2009, IEEE INT SYMP CIRC S, P2898, DOI 10.1109/ISCAS.2009.5118408
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Palesi M, 2014, ROUTING ALGORITHMS N
   Schemmel J, 2010, IEEE INT SYMP CIRC S, P1947, DOI 10.1109/ISCAS.2010.5536970
   Seung HS, 2003, NEURON, V40, P1063, DOI 10.1016/S0896-6273(03)00761-X
   Sjostrom J., 2010, SCHOLARPEDIA 52 REVI, V5, P1362, DOI [10.4249/scholarpedia.1362, 10.4249%2Fscholarpedia.1362, DOI 10.4249/SCHOLARPEDIA.1362]
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Theocharides T, 2004, IEEE INT SOC CONF, P191, DOI 10.1109/SOCC.2004.1362404
   Trappenberg T., 2010, FUNDAMENTALS COMPUTA
   Xie XH, 2004, PHYS REV E, V69, DOI 10.1103/PhysRevE.69.041909
NR 28
TC 1
Z9 2
U1 1
U2 1
PY 2014
BP 496
EP 503
DI 10.1109/DSD.2014.109
WC Computer Science, Theory & Methods; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Tamura, S
   Nishitani, Y
   Hosokawa, C
AF Tamura, Shinichi
   Nishitani, Yoshi
   Hosokawa, Chie
TI Feasibility of Multiplex Communication in a 2D Mesh Asynchronous Neural
   Network with Fluctuations
SO AIMS NEUROSCIENCE
DT Article
DE neural network; multiplex communication; BPN; learning; fluctuation of
   neuron
ID PROPAGATION
AB It remains a mystery how neural networks composed of neurons with fluctuating characteristics can reliably transmit information. In this study, we simulated a 9 x 9 2D mesh neural network consisting of an integrate-and-fire model without leak, and connection weights that were randomly generated. The characteristics of the refractory period and output delay of the neurons were fluctuated time to time. Spikes from transmitting neuron groups spread ( propagated as spike waves) to receiving neurons. For 9 to 1 multiplex communication with a back propagation neural network (BPN), the receiving neurons successfully classified which neuron group transmitted the spike at a rate of 99%. In other words, the activity of the neuron group is propagated in the neural network as spike waves in a broadcasting manner and the wave fragment is received by receiving neurons. Next, point-to-point signal transmission in the neural network is carried out by multi-path, multiplex communication, and diversity reception. Each neuron can function in 3 ways of transmit, relay ( transfer), and receive; however, most neurons act as a local relaying media. This type of mechanism is similar to sound propagation through air. Our research group studies the functions of neural networks by combining experiments with cultured neuronal networks with artificial neural network simulations. This current study corresponding to our previous work on the ability of remote receiving neurons to identify two transmitting neuron groups stimulated in a cultured neuronal network, i.e., 2 to 1 communication. These mechanisms may be the basis of higher cortical functions.
C1 [Tamura, Shinichi] NBL Technovator Co Ltd, Sennan, Osaka, Japan.
   [Nishitani, Yoshi] Osaka Univ, Grad Sch Med, Dept Radiol, Suita, Osaka 565, Japan.
   [Hosokawa, Chie] AIST, Biomed Res Inst, Ikeda, Osaka, Japan.
RP Tamura, S (corresponding author), NBL Technovator Co Ltd, Sennan, Osaka, Japan.
EM tamuras@nblmt.jp
CR Abeles M., 2009, SCHOLARPEDIA, V4, P1441, DOI [DOI 10.4249/SCH0LARPEDIA.1441.REVISI0N, 10.4249/scholarpedia.1441]
   Abeles M., 1982, LOCAL CORTICAL CIRCU, DOI DOI 10.1007/978-3-642-81708-3
   Bell AJ, 1997, VISION RES, V37, P3327, DOI 10.1016/S0042-6989(97)00121-1
   Cessac B, 2010, J PHYSIOL-PARIS, V104, P5, DOI 10.1016/j.jphysparis.2009.11.002
   Fujita K, 2007, BIOL CYBERN, V97, P293, DOI 10.1007/s00422-007-0175-z
   Havenith MN, 2011, J NEUROSCI, V31, P8570, DOI 10.1523/JNEUROSCI.2817-10.2011
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Kamimura T., 2015, AUTOMATION CONTROL I, V3, P63, DOI DOI 10.11648/J.ACIS.20150305.11
   Kliper O, 2004, NEUROCOMPUTING, V58, P1027, DOI 10.1016/j.neucom.2004.01.162
   Mizuno-Matsumoto Y, 2001, BRAIN TOPOGR, V13, P269, DOI 10.1023/A:1011176612377
   Mizuno-Matsumoto Y, 1999, IEEE T BIO-MED ENG, V46, P271, DOI 10.1109/10.748980
   Mohemmed A, 2013, NEUROCOMPUTING, V107, P3, DOI 10.1016/j.neucom.2012.08.034
   Nishitani Y, 2012, COMPUT INTEL NEUROSC, V2012, DOI 10.1155/2012/862579
   Nishitani Yoshi, 2016, INT J ACAD RES REFLE, V4, P11
   Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0
   Perc M, 2005, PHYS REV E, V72, DOI 10.1103/PhysRevE.72.016207
   Perc M, 2007, CHAOS SOLITON FRACT, V32, P1118, DOI 10.1016/j.chaos.2005.11.035
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Sakurai Y, 2013, EUR J NEUROSCI, V37, P623, DOI 10.1111/ejn.12070
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Tamura S., 2013, AUTOMATION CONTR INT, V1, P121, DOI [10.11648/j.acis.20130106.11, DOI 10.11648/J.ACIS.20130106.11]
   Tamura S, 2009, 5 INT C INT INF HID
   Tamura S, 2016, COMPUT INTEL NEUROSC, V2016, DOI 10.1155/2016/7186092
   Tamura S, 2016, COMPUT INTEL NEUROSC, V2016, DOI 10.1155/2016/7267691
   Tyukin I, 2009, NEURAL NETWORKS, V22, P425, DOI 10.1016/j.neunet.2009.01.014
   VAUGHAN RG, 1987, IEEE T VEH TECHNOL, V36, P149, DOI 10.1109/T-VT.1987.24115
   Zhang HH, 2013, COMMUN NONLINEAR SCI, V18, P601, DOI 10.1016/j.cnsns.2012.08.009
NR 27
TC 4
Z9 4
U1 0
U2 1
PY 2016
VL 3
IS 4
BP 385
EP 397
DI 10.3934/Neuroscience.2016.4.385
WC Neurosciences
DA 2023-11-11
ER

PT J
AU Maass, W
   Schmitt, M
AF Maass, W
   Schmitt, M
TI On the complexity of learning for spiking neurons with temporal coding
SO INFORMATION AND COMPUTATION
DT Article
ID VAPNIK-CHERVONENKIS DIMENSION; NEURAL-NETWORK MODELS; VC DIMENSION
AB Spiking neurons are models for the computational units in biological neural systems where information is considered to be encoded mainly in the temporal patterns of their activity. In a network of spiking neurons a new set of parameters becomes relevant which has no counterpart in traditional neural network models: the time that a pulse needs to travel through a connection between two neurons (also known as delay of a connection). It is known that these delays are tuned in biological neural systems through a variety of mechanisms. In this article we consider the arguably most simple model for a spiking neuron, which can also easily be implemented in pulsed VLSI. We investigate the Vapnik-Chervonenkis (VC) dimension of networks of spiking neurons, where the delays are viewed as programmable parameters and we prove tight bounds for this VC dimension. Thus, we get quantitative estimates for the diversity of functions that a network with fixed architecture can compute with different settings of its delays. In particular, it turns out that a network of spiking neurons with k adjustable delays is able to compute a much richer class of functions than a threshold circuit with k adjustable weights. The results also yield bounds for the number of training examples that an algorithm needs for tuning the delays of a network of spiking neurons. Results about the computational complexity of such algorithms are also given. (C) 1999 Academic Press.
C1 Graz Univ Technol, Inst Theoret Comp Sci, A-8010 Graz, Austria.
   Ruhr Univ Bochum, Fak Math, Lehrstuhl Math & Informat, D-44780 Bochum, Germany.
RP Schmitt, M (corresponding author), Graz Univ Technol, Inst Theoret Comp Sci, Klosterwiesgasse 32-2, A-8010 Graz, Austria.
EM maass@igi.tu-graz.ac.at; mschmitt@lmi.ruhr-uni-bochum.de
CR AGMONSNIR H, 1993, J NEUROPHYSIOL, V70, P2066, DOI 10.1152/jn.1993.70.5.2066
   [Anonymous], 1988, CAMBRIDGE STUD MATH
   Anthony M., 1992, CAMBRIDGE TRACTS THE
   BLUM AL, 1992, NEURAL NETWORKS, V5, P117, DOI 10.1016/S0893-6080(05)80010-3
   BLUMER A, 1989, J ACM, V36, P929, DOI 10.1145/76359.76371
   COVER TM, 1965, IEEE TRANS ELECTRON, VEC14, P326, DOI 10.1109/PGEC.1965.264137
   Garey M. R., 1979, COMPUTERS INTRACTABI
   GERSTNER W, 1995, PHYS REV E, V51, P738, DOI 10.1103/PhysRevE.51.738
   Gerstner W, 1996, NATURE, V383, P76, DOI 10.1038/383076a0
   GOLDBERG PW, 1995, MACH LEARN, V18, P131, DOI 10.1007/BF00993408
   HAUSSLER D, 1992, INFORM COMPUT, V100, P78, DOI 10.1016/0890-5401(92)90010-D
   HOFFGEN KU, 1995, J COMPUT SYST SCI, V50, P114, DOI 10.1006/jcss.1995.1011
   KEARNS M, 1994, J ASSOC COMPUT MACH, V41, P1298, DOI 10.1145/195613.195656
   KEARNS MJ, 1994, MACH LEARN, V17, P115, DOI 10.1007/BF00993468
   Koiran P, 1997, J COMPUT SYST SCI, V54, P190, DOI 10.1006/jcss.1997.1479
   Maass W, 1997, NEURAL COMPUT, V9, P279, DOI 10.1162/neco.1997.9.2.279
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   MAASS W., 1999, PULSED NEURAL NETWOR
   Macintyre A., 1993, Proceedings of the Twenty-Fifth Annual ACM Symposium on the Theory of Computing, P325, DOI 10.1145/167088.167192
   MUROGA S, 1966, IEEE TRANS ELECTRON, VEC15, P805, DOI 10.1109/PGEC.1966.264574
   Muroga Saburo, 1971, THRESHOLD LOGIC ITS
   MURRAY A, 1994, ANALOGUE NEURAL VLSI
   Schlafli L, 1901, THEORIE VIELFACHEN K
   VALIANT LG, 1984, COMMUN ACM, V27, P1134, DOI 10.1145/1968.1972
   Zador AM, 1996, NEURAL COMPUT, V8, P611, DOI 10.1162/neco.1996.8.3.611
NR 25
TC 34
Z9 34
U1 0
U2 3
PD AUG 25
PY 1999
VL 153
IS 1
BP 26
EP 46
DI 10.1006/inco.1999.2806
WC Computer Science, Theory & Methods; Mathematics, Applied
DA 2023-11-11
ER

PT J
AU Wang, T
   Wang, Y
   Shen, JM
   Wang, L
   Cao, LH
AF Wang, Tian
   Wang, Ye
   Shen, Jiamin
   Wang, Lei
   Cao, Lihong
TI Predicting Spike Features of Hodgkin-Huxley-Type Neurons With Simple
   Artificial Neural Network
SO FRONTIERS IN COMPUTATIONAL NEUROSCIENCE
DT Article
DE spike; Hodgkin-Huxley model; spike features prediction; artificial
   neural network; spike prediction module; feature prediction module
ID FIRE MODEL; MECHANISMS; CELLS
AB Hodgkin-Huxley (HH)-type model is the most famous computational model for simulating neural activity. It shows the highest accuracy in capturing neuronal spikes, and its model parameters have definite physiological meanings. However, HH-type models are computationally expensive. To address this problem, a previous study proposed a spike prediction module (SPM) to predict whether a spike will take place 1 ms later based on three voltage values with intervals of 1 ms. Although SPM does well, it fails to evaluate the informative features of the spike. In this study, the feature prediction module (FPM) based on simple artificial neural network (ANN) was proposed to predict spike features including maximum voltage, minimum voltage, and dropping interval. Nine different HH-type models were adopted whose firing patterns cover most of the firing behaviors observed in the brain. Voltage and spike feature samples under constant external input current were collected for training and testing. Experiment results illustrated that the combination of SPM and FPM can accurately predict the spiking part of different HH-type models and can generalize to unseen types of input current. The combination of SPM and FPM may offer a possible way to simulate the action potentials of biological neurons with high accuracy and efficiency.
C1 [Wang, Tian; Wang, Ye; Shen, Jiamin; Wang, Lei; Cao, Lihong] Commun Univ China, State Key Lab Media Convergence & Commun, Beijing, Peoples R China.
   [Wang, Tian; Wang, Ye; Cao, Lihong] Commun Univ China, Neurosci & Intelligent Media Inst, Beijing, Peoples R China.
   [Cao, Lihong] State Key Lab Math Engn & Adv Comp, Wuxi, Jiangsu, Peoples R China.
RP Cao, LH (corresponding author), Commun Univ China, State Key Lab Media Convergence & Commun, Beijing, Peoples R China.; Cao, LH (corresponding author), Commun Univ China, Neurosci & Intelligent Media Inst, Beijing, Peoples R China.; Cao, LH (corresponding author), State Key Lab Math Engn & Adv Comp, Wuxi, Jiangsu, Peoples R China.
EM lihong.cao@cuc.edu.cn
CR Abbott LF, 1999, BRAIN RES BULL, V50, P303, DOI 10.1016/S0361-9230(99)00161-6
   Bean BP, 2007, NAT REV NEUROSCI, V8, P451, DOI 10.1038/nrn2148
   Beniaguev D, 2021, NEURON, V109, P2727, DOI 10.1016/j.neuron.2021.07.002
   Berger SD, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00139
   Brette R, 2005, J NEUROPHYSIOL, V94, P3637, DOI 10.1152/jn.00686.2005
   Cao LH, 2018, SCI CHINA INFORM SCI, V61, DOI 10.1007/s11432-017-9379-4
   Ermentrout B, 1998, NEURAL COMPUT, V10, P1721, DOI 10.1162/089976698300017106
   FITZHUGH R, 1961, BIOPHYS J, V1, P445, DOI 10.1016/S0006-3495(61)86902-6
   Fohlmeister JF, 1997, J NEUROPHYSIOL, V78, P1935, DOI 10.1152/jn.1997.78.4.1935
   Gai Y, 2009, J NEUROPHYSIOL, V102, P3447, DOI 10.1152/jn.00538.2009
   Goldman MS, 2001, J NEUROSCI, V21, P5229, DOI 10.1523/JNEUROSCI.21-14-05229.2001
   Golomb D, 2007, PLOS COMPUT BIOL, V3, P1498, DOI 10.1371/journal.pcbi.0030156
   Golomb D, 2006, J NEUROPHYSIOL, V96, P1912, DOI 10.1152/jn.00205.2006
   Gouwens NW, 2010, PLOS COMPUT BIOL, V6, DOI 10.1371/journal.pcbi.1000951
   HINDMARSH JL, 1984, PROC R SOC SER B-BIO, V221, P87, DOI 10.1098/rspb.1984.0024
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Izhikevich EM, 2001, NEURAL NETWORKS, V14, P883, DOI 10.1016/S0893-6080(01)00078-8
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Lapique L., 1907, J PHYSL PATHOL GEN, V9, P620, DOI DOI 10.1007/S00422-007-0189-6
   Miles R, 1991, NEURONAL NETWORKS HI, DOI 10.1017/cbo9780511895401
   MORRIS C, 1981, BIOPHYS J, V35, P193, DOI 10.1016/S0006-3495(81)84782-0
   NAGUMO J, 1962, P IRE, V50, P2061, DOI 10.1109/JRPROC.1962.288235
   Rothman JS, 2003, J NEUROPHYSIOL, V89, P3097, DOI 10.1152/jn.00127.2002
   Rulkov NF, 2002, PHYS REV E, V65, DOI 10.1103/PhysRevE.65.041922
   STEIN RB, 1967, BIOPHYS J, V7, P37, DOI 10.1016/S0006-3495(67)86574-3
   Sun Y, 2009, J COMPUT NEUROSCI, V27, P369, DOI 10.1007/s10827-009-0151-9
   Wang XJ, 1996, J NEUROSCI, V16, P6402
   Yang SM, 2022, IEEE T NEUR NET LEAR, V33, P7126, DOI 10.1109/TNNLS.2021.3084250
   Yang SM, 2021, FRONT NEUROSCI-SWITZ, V15, DOI 10.3389/fnins.2021.601109
   Yang SM, 2022, IEEE T NEUR NET LEAR, V33, P4398, DOI [10.1109/TNNLS.2021.3057070, 10.4018/IJCINI.20211001.oa2]
NR 31
TC 2
Z9 2
U1 1
U2 10
PD FEB 7
PY 2022
VL 15
AR 800875
DI 10.3389/fncom.2021.800875
WC Mathematical & Computational Biology; Neurosciences
DA 2023-11-11
ER

PT C
AU Traub, M
   Butz, MV
   Legenstein, R
   Otte, S
AF Traub, Manuel
   Butz, Martin, V
   Legenstein, Robert
   Otte, Sebastian
BE Farkas, I
   Masulli, P
   Otte, S
   Wermter, S
TI Dynamic Action Inference with Recurrent Spiking Neural Networks
SO ARTIFICIAL NEURAL NETWORKS AND MACHINE LEARNING, ICANN 2021, PT V
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 30th International Conference on Artificial Neural Networks (ICANN)
CY SEP 14-17, 2021
CL ELECTR NETWORK
DE Recurrent spiking neural networks; Active inference; Temporal gradients
AB In this paper, we demonstrate that goal-directed behavior unfolds in recurrent spiking neural networks (RSNNs) when intentions are projected onto continuously progressing spike dynamics encoding the recent history of an agent's state. The projections, which can either be realized via backpropagation through time (BPTT) over a certain time window or even directly and temporally local in an online fashion using a biologically inspired inference rule. In contrast to previous studies that use, for instance, LSTM-like models, our approach is biologically more plausible as it fully relies on spike-based processing of sensorimotor experiences. Specifically, we show that precise control of a flying vehicle in a 3D environment is possible. Moreover, we show that more complex mental traces of foresighted movement imagination unfold that effectively help to circumvent learned obstacles.
C1 [Traub, Manuel; Butz, Martin, V; Otte, Sebastian] Univ Tubingen, Comp Sci Dept, Neuro Cognit Modeling, Sand 14, D-72076 Tubingen, Germany.
   [Legenstein, Robert] Graz Univ Technol, Fac Comp Sci & Biomed Engn, Inffeldgasse 16b, A-8010 Graz, Austria.
RP Otte, S (corresponding author), Univ Tubingen, Comp Sci Dept, Neuro Cognit Modeling, Sand 14, D-72076 Tubingen, Germany.
EM manuel.traub@uni-tuebingen.de; martin.butz@uni-tuebingen.de;
   robert.legenstein@igi.tugraz.at; sebastian.otte@uni-tuebingen.de
CR [Anonymous], 1997, NEURAL COMPUT
   Bellec G., 2018, ADV NEURAL INFORM PR, P787
   Bellec G, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-17236-y
   Butz MV, 2019, LECT NOTES COMPUT SC, V11727, P639, DOI 10.1007/978-3-030-30487-4_49
   Butz MV, 2019, NEURAL NETWORKS, V117, P135, DOI 10.1016/j.neunet.2019.05.001
   BUTZ MV, 2016, MIND COMES BEING INT
   Clark A, 2015, SURFING UNCERTAINTY
   Friston KJ, 2009, TRENDS COGN SCI, V13, P293, DOI 10.1016/j.tics.2009.04.005
   Mnih V., 2013, ARXIV, DOI DOI 10.1038/NATURE14236
   Nichol Alex, 2018, PREPRINT
   OpenAI, 2019, ARXIV191206680
   Otte S, 2019, LECT NOTES COMPUT SC, V11730, P543, DOI 10.1007/978-3-030-30490-4_44
   Otte S, 2017, LECT NOTES COMPUT SC, V10613, P227, DOI 10.1007/978-3-319-68600-4_27
   Schulman J., 2017, ARXIV, DOI DOI 10.1016/J.JDEVECO.2016.04.001
   Traub M., 2021, IEEE RSJ INT C INT R
   Yin BJ, 2021, NAT MACH INTELL, V3, P905, DOI 10.1038/s42256-021-00397-w
NR 16
TC 0
Z9 0
U1 1
U2 6
PY 2021
VL 12895
BP 233
EP 244
DI 10.1007/978-3-030-86383-8_19
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods; Imaging Science & Photographic Technology
DA 2023-11-11
ER

PT C
AU Mehrtash, N
   Jung, D
   Mar, H
AF Mehrtash, N
   Jung, D
   Mar, H
BE Callaos, N
   He, Y
   PerezPeraza, JA
TI SpinnSoftSim: A programme for simulating spiking neural networks
SO 6TH WORLD MULTICONFERENCE ON SYSTEMICS, CYBERNETICS AND INFORMATICS, VOL
   XVII, PROCEEDINGS: INDUSTRIAL SYSTEMS AND ENGINEERING III
DT Proceedings Paper
CT 6th World Multi-Conference on Systemics, Cybernetics and Informatics
   (SCI 2002)/8th International Conference on Information Systems Analysis
   and Synthesis (ISAS 2002)
CY JUL 14-18, 2002
CL ORLANDO, FL
DE spiking neural networks; dynamic synapses
ID CORTEX; SYNAPSES; CODE
AB Neuroscience has been developed significantly in the last decade. During these years the behavior description of the individual neuron changed from cellbiologic to microbiologic. In order to evaluate the hypothesis in a large neural network and to build some paradigms that can be applied in technical areas we are developing an accelerator system. This task includes all aspects of Hardware/Software-Codesign and System-on-Chip (SoC) realization. Sofar we designed the (SPINN)-I-2-chip (synaptic plasticity in spiking neural network) and SpinnSoftSim. At this stage of its developement SpinnSoftSim is a stand-alone simulator, but we are also going to gear it to our hardware.
C1 Tech Univ Berlin, Inst Comp Engn & Microelect, D-1000 Berlin, Germany.
RP Mehrtash, N (corresponding author), Tech Univ Berlin, Inst Comp Engn & Microelect, D-1000 Berlin, Germany.
CR [Anonymous], 2001, DELAY EFFECTS STABIL
   BIALEK W, 1991, SCIENCE, V252, P1854, DOI 10.1126/science.2063199
   Bringuier V, 1999, SCIENCE, V283, P695, DOI 10.1126/science.283.5402.695
   deCharms RC, 1998, P NATL ACAD SCI USA, V95, P15166, DOI 10.1073/pnas.95.26.15166
   Eckhorn R., 1989, NEURAL NETWORK FEATU
   Eric RKandel, 2000, PRINCIPLES NEURAL SC
   Felleman DJ, 1991, CEREB CORTEX, V1, P1, DOI 10.1093/cercor/1.1.1
   FRENCH AS, 1970, IEEE T BIO-MED ENG, VBM17, P248, DOI 10.1109/TBME.1970.4502739
   Friston KJ, 1997, NEUROIMAGE, V5, P213, DOI 10.1006/nimg.1997.0260
   GERSTNER W, 1993, BIOL CYBERN, V69, P503, DOI 10.1007/BF01185422
   Gerstner W, 1997, P NATL ACAD SCI USA, V94, P12740, DOI 10.1073/pnas.94.24.12740
   MAASS W, 1996, NETWORKS SPIKING NEU
   Makarenko V, 1998, P NATL ACAD SCI USA, V95, P15747, DOI 10.1073/pnas.95.26.15747
   MEHRTASH N, UNPUB IMAGE PREPROCE
   MEHRTASH N, UNPUB SYNAPTIC PLAST
   Senn W, 2001, NEURAL COMPUT, V13, P35, DOI 10.1162/089976601300014628
   Tsodyks M, 1998, NEURAL COMPUT, V10, P821, DOI 10.1162/089976698300017502
   VAADIA E, 1995, NATURE, V373, P515, DOI 10.1038/373515a0
   Varela JA, 1997, J NEUROSCI, V17, P7926
   WEHMEIER U, METHODS NEURONAL MOD, P335
NR 20
TC 1
Z9 1
U1 0
U2 0
PY 2002
BP 61
EP 64
WC Automation & Control Systems; Computer Science, Artificial Intelligence;
   Computer Science, Interdisciplinary Applications; Engineering,
   Industrial
DA 2023-11-11
ER

PT C
AU Wysoski, SG
   Benuskova, L
   Kasabov, N
AF Wysoski, Simei Gomes
   Benuskova, Lubica
   Kasabov, Nikola
BE Ishikawa, M
   Doya, K
   Miyamoto, H
   Yamakawa, T
TI Adaptive spiking neural networks for audiovisual pattern recognition
SO NEURAL INFORMATION PROCESSING, PART II
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 14th International Conference on Neural Information Processing (ICONIP
   2007)
CY NOV 13-16, 2007
CL Kitakyushu, JAPAN
DE spiking neural networks; multi-modal information processing; face and
   speaker recognition; visual and auditory integration
ID ASSOCIATIONS; ACTIVATION
AB The paper describes the integration of brain-inspired systems to perform audiovisual pattern recognition tasks. Individual sensory pathways as well as the integrative modules are implemented using a fast version of spiking neurons grouped in evolving spiking neural network (ESNN) architectures capable of lifelong adaptation. We design a new crossmodal integration system, where individual modalities can influence others before individual decisions are made, fact that resembles some characteristics of the biological brains. The system is applied to the person authentication problem. Preliminary results show that the integrated system can improve the accuracy in many operation points as well as it enables a range of multi-criteria optimizations.
C1 [Wysoski, Simei Gomes; Benuskova, Lubica; Kasabov, Nikola] Auckland Univ Technol, Knowledge Engn & Discovery Res Inst, Auckland, New Zealand.
RP Wysoski, SG (corresponding author), Auckland Univ Technol, Knowledge Engn & Discovery Res Inst, 581-585 Great S Rd, Auckland, New Zealand.
EM swysoski@aut.ac.nz; lbenusko@aut.ac.nz; nkasabov@aut.ac.nz
CR [Anonymous], 1993, MERGING SENSES MERGI
   [Anonymous], 1999, COMBINING ARTIFICIAL
   Bimbot F, 2004, EURASIP J APPL SIG P, V2004, P430, DOI 10.1155/S1110865704310024
   Burileanu C., 2002, International Journal of Speech Technology, V5, P247, DOI 10.1023/A:1020244924468
   BURTON AM, 1990, BRIT J PSYCHOL, V81, P361, DOI 10.1111/j.2044-8295.1990.tb02367.x
   Calvert GA, 2001, CEREB CORTEX, V11, P1110, DOI 10.1093/cercor/11.12.1110
   Delorme A, 1999, NEUROCOMPUTING, V26-7, P989, DOI 10.1016/S0925-2312(99)00095-8
   ELLIS AW, 1987, MODERN COGNITION
   Ellis HD, 1997, BRIT J PSYCHOL, V88, P143, DOI 10.1111/j.2044-8295.1997.tb02625.x
   Gonzalo D, 2000, NEUROIMAGE, V11, P243, DOI 10.1006/nimg.2000.0540
   Kasabov N, 2000, INFORM SCIENCES, V123, P127, DOI 10.1016/S0020-0255(99)00114-0
   McIntosh AR, 1998, J NEUROPHYSIOL, V80, P2790, DOI 10.1152/jn.1998.80.5.2790
   Reynolds DA, 2000, DIGIT SIGNAL PROCESS, V10, P19, DOI 10.1006/dspr.1999.0361
   Ross A, 2003, PATTERN RECOGN LETT, V24, P2115, DOI 10.1016/S0167-8655(03)00079-5
   Sanderson C, 2004, DIGIT SIGNAL PROCESS, V14, P449, DOI 10.1016/j.dsp.2004.05.001
   VanRullen R, 2001, NEUROCOMPUTING, V38, P1003, DOI 10.1016/S0925-2312(01)00445-3
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   von Kriegstein K, 2006, PLOS BIOL, V4, P1809, DOI 10.1371/journal.pbio.0040326
   WYSOSKI SG, 2007, NEUROCOMPUT IN PRESS
   Wysoski SG, 2007, LECT NOTES COMPUT SC, V4669, P758
   Wysoski SG, 2006, LECT NOTES COMPUT SC, V4131, P61
NR 21
TC 10
Z9 10
U1 0
U2 1
PY 2008
VL 4985
BP 406
EP 415
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods
DA 2023-11-11
ER

PT C
AU Smith, MR
   Hill, AJ
   Carlson, KD
   Vineyard, CM
   Donaldson, J
   Follett, DR
   Follett, PL
   Naegle, JH
   James, CD
   Aimone, JB
AF Smith, Michael R.
   Hill, Aaron J.
   Carlson, Kristofor D.
   Vineyard, Craig M.
   Donaldson, Jonathon
   Follett, David R.
   Follett, Pamela L.
   Naegle, John H.
   James, Conrad D.
   Aimone, James B.
GP IEEE
TI A Novel Digital Neuromorphic Architecture Efficiently Facilitating
   Complex Synaptic Response Functions Applied to Liquid State Machines
SO 2017 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY MAY 14-19, 2017
CL Anchorage, AK
ID TIME; RECOGNITION; NETWORK
AB Information in neural networks is represented as weighted connections, or synapses, between neurons. This poses a problem as the primary computational bottleneck for neural networks is the vector-matrix multiply when inputs are multiplied by the neural network weights. Conventional processing architectures are not well suited for simulating neural networks, often requiring large amounts of energy and time. Additionally, synapses in biological neural networks are not binary connections, but exhibit a nonlinear response function as neurotransmitters are emitted and diffuse between neurons. Inspired by neuroscience principles, we present a digital neuromorphic architecture, the Spiking Temporal Processing Unit (STPU), capable of modeling arbitrary complex synaptic response functions without requiring additional hardware components. We consider the paradigm of spiking neurons with temporally coded information as opposed to non-spiking rate coded neurons used in most neural networks. In this paradigm we examine liquid state machines applied to speech recognition and show how a liquid state machine with temporal dynamics maps onto the STPU-demonstrating the flexibility and efficiency of the STPU for instantiating neural algorithms.
C1 [Smith, Michael R.; Hill, Aaron J.; Carlson, Kristofor D.; Vineyard, Craig M.; Donaldson, Jonathon; Naegle, John H.; James, Conrad D.; Aimone, James B.] Sandia Natl Labs, POB 5800, Albuquerque, NM 87185 USA.
   [Follett, David R.; Follett, Pamela L.] Lewis Rhodes Labs, Concord, MA 01742 USA.
   [Follett, Pamela L.] Tufts Univ, Medford, MA 02155 USA.
RP Smith, MR (corresponding author), Sandia Natl Labs, POB 5800, Albuquerque, NM 87185 USA.
EM msmith4@sandia.gov; ajhill@sandia.gov; kdcarls@sandia.gov;
   cmviney@sandia.gov; jwdonal@sandia.gov; drfollett@earthlink.net;
   plfollett@earthlink.net; jhnaegl@sandia.gov; cdjame@sandia.gov;
   jbaimon@sandia.gov
CR [Anonymous], 2003, ADV NEURAL INFORM PR
   BACKUS J, 1978, COMMUN ACM, V21, P613, DOI 10.1145/359576.359579
   Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Buonomano DV, 2009, NAT REV NEUROSCI, V10, P113, DOI 10.1038/nrn2558
   Burgsteiner H, 2007, APPL INTELL, V26, P99, DOI 10.1007/s10489-006-0007-1
   Ciresan D, 2012, PROC CVPR IEEE, P3642, DOI 10.1109/CVPR.2012.6248110
   Dayan P., 2001, THEORETICAL NEUROSCI, DOI DOI 10.1016/j.neuron.2013.01.033
   Deng L, 2013, INT CONF ACOUST SPEE, P8599, DOI 10.1109/ICASSP.2013.6639344
   Follett PL, 2009, J CHILD NEUROL, V24, P1205, DOI 10.1177/0883073809338627
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   Furber SB, 2013, IEEE T COMPUT, V62, P2454, DOI 10.1109/TC.2012.142
   Hammami N., 2010, P IEEE INT C COMP SC
   Huang GB, 2006, NEUROCOMPUTING, V70, P489, DOI 10.1016/j.neucom.2005.12.126
   Lukosevicius M, 2009, COMPUT SCI REV, V3, P127, DOI 10.1016/j.cosrev.2009.03.005
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Norton D, 2010, NEUROCOMPUTING, V73, P2893, DOI 10.1016/j.neucom.2010.08.005
   Roy S., 2014, IEEE T BIOMEDICAL CI, V8
   Schemmel J, 2010, IEEE INT SYMP CIRC S, P1947, DOI 10.1109/ISCAS.2010.5536970
   Schemmel J, 2008, IEEE IJCNN, P431, DOI 10.1109/IJCNN.2008.4633828
   Schrauwen B, 2008, NEURAL NETWORKS, V21, P511, DOI 10.1016/j.neunet.2007.12.009
   SEJNOWSKI TJ, 1995, NATURE, V376, P21, DOI 10.1038/376021a0
   Severa W., 2016, NIPS WORKSH COMP SPI
   Socher R., 2013, P 2013 C EMPIRICAL M, P1631, DOI DOI 10.1371/JOURNAL.PONE.0073791
   Verzi S. J., COMPUTING SPIK UNPUB
   Verzi S. J., 2017, P IEEE INT IN PRESS
   Zhang Y, 2015, IEEE T NEUR NET LEAR, V26, P2635, DOI 10.1109/TNNLS.2015.2388544
NR 27
TC 6
Z9 6
U1 2
U2 5
PY 2017
BP 2421
EP 2428
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU de Santos-Sierra, D
   Sanchez-Jimenez, A
   Garcia-Vellisca, MA
   Navas, A
   Villacorta-Atienza, JA
AF de Santos-Sierra, Daniel
   Sanchez-Jimenez, Abel
   Garcia-Vellisca, Mariano A.
   Navas, Adrian
   Villacorta-Atienza, Jose A.
TI Effects of Spike Anticipation on the Spiking Dynamics of Neural Networks
SO FRONTIERS IN COMPUTATIONAL NEUROSCIENCE
DT Article
DE spike anticipation; information processing; neural networks;
   synchronization; chaotic dynamical systems
ID PHASE SYNCHRONIZATION; DEPENDENT PLASTICITY; SLEEP; MODULATION; NEURONS;
   PATHWAY; NUCLEI; CORTEX; MODES
AB Synchronization is one of the central phenomena involved in information processing in living systems. It is known that the nervous system requires the coordinated activity of both local and distant neural populations. Such an interplay allows to merge different information modalities in a whole processing supporting high-level mental skills as understanding, memory, abstraction, etc. Though, the biological processes underlying synchronization in the brain are not fully understood there have been reported a variety of mechanisms supporting different types of synchronization both at theoretical and experimental level. One of the more intriguing of these phenomena is the anticipating synchronization, which has been recently reported in a pair of unidirectionally coupled artificial neurons under simple conditions (Pyragiene and Pyragas, 2013), where the slave neuron is able to anticipate in time the behavior of the master one. In this paper, we explore the effect of spike anticipation over the information processing performed by a neural network at functional and structural level. We show that the introduction of intermediary neurons in the network enhances spike anticipation and analyse how these variations in spike anticipation can significantly change the firing regime of the neural network according to its functional and structural properties. In addition we show that the interspike interval (ISI), one of the main features of the neural response associated with the information coding, can be closely related to spike anticipation by each spike, and how synaptic plasticity can be modulated through that relationship. This study has been performed through numerical simulation of a coupled system of Hindmarsh Rose neurons.
C1 [de Santos-Sierra, Daniel; Villacorta-Atienza, Jose A.] Tech Univ Madrid, Grp Biometr Biosignals & Secur, Res Ctr Smart Bldg & Energy Efficiency CeDint, Madrid, Spain.
   [de Santos-Sierra, Daniel; Garcia-Vellisca, Mariano A.; Navas, Adrian] Tech Univ Madrid, Ctr Biomed Technol, Lab Computat Syst Biol, Madrid, Spain.
   [Sanchez-Jimenez, Abel] Univ Complutense Madrid, Sch Biol Sci, Dept Appl Math Biomath, Madrid, Spain.
   [Villacorta-Atienza, Jose A.] Univ Complutense Madrid, Sch Math, Dept Appl Math, Madrid, Spain.
RP Villacorta-Atienza, JA (corresponding author), Tech Univ Madrid, Grp Biometr Biosignals & Secur, Res Ctr Smart Bldg & Energy Efficiency CeDint, Madrid, Spain.
EM joseavillacorta@pluri.ucm.es
CR [Anonymous], 1959, MATH THEORY COMMUNIC
   [Anonymous], 2010, DYNAMICAL SYSTEMS NE
   ARTOLA A, 1993, TRENDS NEUROSCI, V16, P480, DOI 10.1016/0166-2236(93)90081-V
   Benito N, 2014, CEREB CORTEX, V24, P1738, DOI 10.1093/cercor/bht022
   Butts Daniel A, 2010, Front Synaptic Neurosci, V2, P30, DOI 10.3389/fnsyn.2010.00030
   Caporale N, 2008, ANNU REV NEUROSCI, V31, P25, DOI 10.1146/annurev.neuro.31.060407.125639
   Davison AP, 2006, J NEUROSCI, V26, P5604, DOI 10.1523/JNEUROSCI.5263-05.2006
   Daw MI, 2007, MOL CELL NEUROSCI, V34, P493, DOI 10.1016/j.mcn.2007.01.001
   DeFelipe J, 2012, FRONT NEUROANAT, V6, DOI [10.3389/fnana.2012.00022, 10.3389/fnsyn.2012.00002, 10.3389/fnana.2012.00005]
   Fanselow EE, 2001, P NATL ACAD SCI USA, V98, P15330, DOI 10.1073/pnas.261273898
   Fiete IR, 2010, NEURON, V65, P563, DOI 10.1016/j.neuron.2010.02.003
   Friedel P, 2008, BIOL CYBERN, V98, P597, DOI 10.1007/s00422-008-0236-y
   Froemke RC, 2002, NATURE, V416, P433, DOI 10.1038/416433a
   Grossberg S, 2008, BRAIN RES, V1218, P278, DOI 10.1016/j.brainres.2008.04.024
   Hänggi P, 2002, CHEMPHYSCHEM, V3, P285, DOI 10.1002/1439-7641(20020315)3:3<285::AID-CPHC285>3.0.CO;2-A
   Häusser M, 2003, CURR OPIN NEUROBIOL, V13, P372, DOI 10.1016/S0959-4388(03)00075-8
   HINDMARSH JL, 1984, PROC R SOC SER B-BIO, V221, P87, DOI 10.1098/rspb.1984.0024
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Kandel ER., 2021, PRINCIPLES NEURAL SC, V6
   Khazipov R, 2013, NEUROSCIENCE, V250, P240, DOI 10.1016/j.neuroscience.2013.07.019
   Koch Christof, 1999, P1
   Lin YW, 2003, J NEUROSCI, V23, P4173
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   Pais-Vieira M, 2013, J NEUROSCI, V33, P4076, DOI 10.1523/JNEUROSCI.1659-12.2013
   PECORA LM, 1990, PHYS REV LETT, V64, P821, DOI 10.1103/PhysRevLett.64.821
   Pikovsky A, 2003, SYNCHRONIZATION UNIV
   Pyragiene T, 2013, NONLINEAR DYNAM, V74, P297, DOI 10.1007/s11071-013-0968-7
   Rosenblum MG, 1996, PHYS REV LETT, V76, P1804, DOI 10.1103/PhysRevLett.76.1804
   RULKOV NF, 1995, PHYS REV E, V51, P980, DOI 10.1103/PhysRevE.51.980
   Sanchez-Jimenez A, 2009, NEUROSCIENCE, V160, P212, DOI 10.1016/j.neuroscience.2009.01.075
   Sanchez-Jimenez A, 2013, FRONT CELL NEUROSCI, V7, DOI 10.3389/fncel.2013.00079
   Sjöström PJ, 2001, NEURON, V32, P1149, DOI 10.1016/s0896-6273(01)00542-6
   STERIADE M, 1991, J NEUROSCI, V11, P3200
   Swadlow HA, 2005, PROG BRAIN RES, V149, P91, DOI 10.1016/S0079-6123(05)49008-1
   Tsoukatos J, 1997, EXP BRAIN RES, V113, P273, DOI 10.1007/BF02450325
   Varela F, 2001, NAT REV NEUROSCI, V2, P229, DOI 10.1038/35067550
   Viemari JC, 2013, FRONT NEURAL CIRCUIT, V7, DOI 10.3389/fncir.2013.00179
   Villacorta JA, 2005, BIOL CYBERN, V92, P339, DOI 10.1007/s00422-005-0554-2
   Voss HU, 2000, PHYS REV E, V61, P5115, DOI 10.1103/PhysRevE.61.5115
   Weyand TG, 2001, J NEUROPHYSIOL, V85, P1107, DOI 10.1152/jn.2001.85.3.1107
NR 40
TC 2
Z9 2
U1 0
U2 8
PD NOV 30
PY 2015
VL 9
AR 144
DI 10.3389/fncom.2015.00144
WC Mathematical & Computational Biology; Neurosciences
DA 2023-11-11
ER

PT C
AU Azimirad, V
   Sotubadi, SV
   Nasirlou, A
AF Azimirad, Vahid
   Sotubadi, Saleh Valizadeh
   Nasirlou, Ali
GP IEEE
TI Vision-based Learning: A Novel Machine Learning Method based on
   Convolutional Neural Networks and Spiking Neural Networks
SO 2021 9TH RSI INTERNATIONAL CONFERENCE ON ROBOTICS AND MECHATRONICS
   (ICROM)
SE RSI International Conference on Robotics and Mechatronics ICRoM
DT Proceedings Paper
CT 9th RSI International Conference on Robotics and Mechatronics (ICRoM)
CY NOV 17-19, 2021
CL Amirkabir Univ Technol, New Technologies Ctr, Tehran, IRAN
HO Amirkabir Univ Technol, New Technologies Ctr
DE Spiking Neural Networks; Convolutional Neural Networks; Non-holonomic
   Mobile Robots; Transfer Learning
AB The aim of this study is to autonomously control a Non-holonomic Mobile Robot using novel learning-based control approaches. A Spiking Neural Network (SNN) is modeled and developed to perform the decision making in the robot. A camera captures pictures from the environment and transfers them to the Convolutional Neural Network (CNN) to extract the image features. The extracted features are subsequently given to a Multi Layer Perceptron (MLP) neural network to perform the task of image classification. The results of the object recognition carried out by the image classifier unit are then given to the SNN for decision making and extracting the optimal policy based on the current states of the environment where the agent resides. The extracted policy is given to the agent to perform an action. The result of the action either results in a positive outcome or a negative outcome. The feedback of the policy execution (outcome) is given to the SNN as reward or punishment to optimize the policy extraction.
C1 [Azimirad, Vahid; Nasirlou, Ali] Univ Tabriz, Dept Mech Engn, Tabriz, Iran.
   [Sotubadi, Saleh Valizadeh] Michigan Technol Univ, Dept Mech Engn, Houghton, MI USA.
EM azimirad@tabrizu.ac.ir; svalizad@mtu.edu; ali.nasirlou74@gmail.com
CR Azimirad Vahid, 2020, Proceedings of the 10th International Conference on Computer and Knowledge Engineering (ICCKE 2020), P030, DOI 10.1109/ICCKE50421.2020.9303660
   Azimirad V., ROBOTICA, P1
   Azimirad V, 2018, RSI INT CONF ROBOT M, P126, DOI 10.1109/ICRoM.2018.8657598
   Dura-Bernal S, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00028
   He K., 2015, ARXIV
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V116, P449, DOI 10.1113/jphysiol.1952.sp004717
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2007, CEREB CORTEX, V17, P2443, DOI 10.1093/cercor/bhl152
   Kingma D. P., 2015, PROC 3 INT C LEARN R, DOI DOI 10.48550/ARXIV.1412.6980
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
   Martínez-Tenor A, 2018, EXPERT SYST APPL, V100, P246, DOI 10.1016/j.eswa.2017.11.011
   Mnih V., 2013, ARXIV, DOI DOI 10.1038/NATURE14236
   Pakizeh E, 2015, APPL INTELL, V43, P487, DOI 10.1007/s10489-015-0665-y
   Putra R. V. W., 2020, FSPINN OPTIMIZATION
   Ramezanlou Mohammad Tayefe, 2020, Proceedings of the 10th International Conference on Computer and Knowledge Engineering (ICCKE 2020), P544, DOI 10.1109/ICCKE50421.2020.9303687
   Sadhu AK, 2017, ROBOT AUTON SYST, V92, P66, DOI 10.1016/j.robot.2017.03.003
   Simonyan K., 2015, P 3 INT C LEARN REPR, P1
   Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594
NR 18
TC 5
Z9 5
U1 2
U2 7
PY 2021
BP 192
EP 197
DI 10.1109/ICRoM54204.2021.9663521
WC Engineering, Electrical & Electronic; Robotics
DA 2023-11-11
ER

PT C
AU Sen, S
   Venkataramani, S
   Raghunathan, A
AF Sen, Sanchari
   Venkataramani, Swagath
   Raghunathan, Anand
GP IEEE
TI Approximate Computing for Spiking Neural Networks
SO PROCEEDINGS OF THE 2017 DESIGN, AUTOMATION & TEST IN EUROPE CONFERENCE &
   EXHIBITION (DATE)
SE Design Automation and Test in Europe Conference and Exhibition
DT Proceedings Paper
CT 20th Conference and Exhibition on Design, Automation and Test in Europe
   (DATE)
CY MAR 27-31, 2017
CL EPFL Campus, Lausanne, SWITZERLAND
HO EPFL Campus
DE Approximate Computing; Spiking Neural Networks; Approximate Neural
   Networks
AB Spiking Neural Networks (SNNs) are widely regarded as the third generation of artificial neural networks, and are expected to drive new classes of recognition, data analytics and computer vision applications. However, large-scale SNNs (e.g., of the scale of the human visual cortex) are highly compute and data intensive, requiring new approaches to improve their efficiency. Complementary to prior efforts that focus on parallel software and the design of specialized hardware, we propose AxSNN, the first effort to apply approximate computing to improve the computational efficiency of evaluating SNNs. In SNNs, the inputs and outputs of neurons are encoded as a time series of spikes. A spike at a neuron's output triggers updates to the potentials (internal states) of neurons to which it is connected. AxSNN determines spike-triggered neuron updates that can be skipped with little or no impact on output quality and selectively skips them to improve both compute and memory energy. Neurons that can be approximated are identified by utilizing various static and dynamic parameters such as the average spiking rates and current potentials of neurons, and the weights of synaptic connections. Such a neuron is placed into one of many approximation modes, wherein the neuron is sensitive only to a subset of its inputs and sends spikes only to a subset of its outputs. A controller periodically updates the approximation modes of neurons in the network to achieve energy savings with minimal loss in quality. We apply AxSNN to both hardware and software implementations of SNNs. For hardware evaluation, we designed SNNAP, a Spiking Neural Network Approximate Processor that embodies the proposed approximation strategy, and synthesized it to 45nm technology. The software implementation of AxSNN was evaluated on a 2.7 GHz Intel Xeon server with 128 GB memory. Across a suite of 6 image recognition benchmarks, AxSNN, achieves 1.4-5.5x reduction in scalar operations for network evaluation, which translates to 1.2-3.62x and 1.26-3.9x improvement in hardware and software energies respectively, for no loss in application quality. Progressively higher energy savings are achieved with modest reductions in output quality.
C1 [Sen, Sanchari; Venkataramani, Swagath; Raghunathan, Anand] Purdue Univ, Sch Elect & Comp Engn, W Lafayette, IN 47907 USA.
   [Venkataramani, Swagath] IBM TJ Watson Res Ctr, Yorktown Hts, NY USA.
RP Sen, S (corresponding author), Purdue Univ, Sch Elect & Comp Engn, W Lafayette, IN 47907 USA.
EM sen9@purdue.edu; venkata@purdue.edu; raghunathan@purdue.edu
CR Ananthanarayanan R., 2007, P SUP SC 07
   [Anonymous], P IJCNN
   [Anonymous], 2014, P CVPR
   Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Cao Y., 2015, INT J COMPUT VISION, V113
   Cassidy A. S., 2013, NEURAL NETW, V45
   Diehl P., 2015, P IJCNN
   Du Z., 2015, IEEE TCAD
   Fidjeland A., 2010, IJCNN
   Furber SB, 2013, IEEE T COMPUT, V62, P2454, DOI 10.1109/TC.2012.142
   Izhikevich E. M., 2004, NEURAL NETWORKS IEEE, V15
   Krichmar J. L., 2015, JETC, V11
   Kuzum D, 2013, NANOTECHNOLOGY, V24, DOI 10.1088/0957-4484/24/38/382001
   Liu XX, 2016, IEEE T CIRCUITS-I, V63, P617, DOI 10.1109/TCSI.2016.2529279
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Morrison A, 2005, NEURAL COMPUT, V17, P1776, DOI 10.1162/0899766054026648
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Schoenauer T., TRANS NEUR NETW, V13
   Sengupta A., 2016, SCI REPORTS
   Venkataramani S., 2015, P DAC
   Venkataramani S., 2014, P ISLPED
   Vreeken J., 2003, SPIKING NEURAL NETWO
   Zhang Q., 2015, P DATE
NR 23
TC 29
Z9 29
U1 0
U2 2
PY 2017
BP 193
EP 198
WC Automation & Control Systems; Engineering, Industrial; Engineering,
   Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Dibazar, AA
   Yousefi, A
   Berger, TW
AF Dibazar, Alireza A.
   Yousefi, Ali
   Berger, Theodore W.
GP IEEE
TI Multi-Layer Spike-In Spike-Out Representation of Hippocampus Circuitry
SO 2013 6TH INTERNATIONAL IEEE/EMBS CONFERENCE ON NEURAL ENGINEERING (NER)
SE International IEEE EMBS Conference on Neural Engineering
DT Proceedings Paper
CT 6th International IEEE EMBS Conference on Neural Engineering (NER)
CY NOV 06-08, 2013
CL San Diego, CA
AB One of the frontier research of neuroscience focuses on replacing the damaged human Hippocampus regions with a prosthetic device replicating Hippocampus neural functionality. Since neural cognition in general and memory formation in particular are the result of neural processing in multiple layers of neural circuitries, it is crucial to develop neural models mimicking the same topology principals. This paper presents a two later spike-in-spike-out model of the Hippocampus neural circuitry with the emphasis on providing a methodology for tuning parameters of the model for DG -> CA1 spike-in-spike-out transformation. Volterra series expansion with Laguerre basis functions represents layers of the circuit, in which network layers are communication by spikes. Utilizing the input-output recordings of the Hippocampus, the novel model predicts spike timing of the CA1 neurons. The simulation result justifies that neurobiological recording of DG -> CA1 can be successfully generated by utilizing the methodology of this paper.
C1 [Dibazar, Alireza A.; Berger, Theodore W.] Univ Southern Calif, Dept Biomed Engn, 1042 Downey Way,DRB 140, Los Angeles, CA 90089 USA.
   [Yousefi, Ali] Univ Southern Calif, Los Angeles, CA USA.
RP Dibazar, AA (corresponding author), Univ Southern Calif, Dept Biomed Engn, 1042 Downey Way,DRB 140, Los Angeles, CA 90089 USA.
EM dibazar@usc.edu; ayousefi@usc.edu; berger@usc.edu
CR Berger TW, 2012, IEEE T NEUR SYS REH, V20, P198, DOI 10.1109/TNSRE.2012.2189133
   MAINEN ZF, 1995, SCIENCE, V268, P1503, DOI 10.1126/science.7770778
   OGURA H, 1972, IEEE T INFORM THEORY, V18, P473, DOI 10.1109/TIT.1972.1054856
   Song D, 2013, J COMPUT NEUROSCI, V35, P335, DOI 10.1007/s10827-013-0455-7
   Takahashi N, 2012, SCIENCE, V335, P353, DOI 10.1126/science.1210362
   YECKEL MF, 1990, P NATL ACAD SCI USA, V87, P5832, DOI 10.1073/pnas.87.15.5832
   Yousefi A, 2012, IEEE ENG MED BIO, P1362, DOI 10.1109/EMBC.2012.6346191
NR 7
TC 1
Z9 1
U1 0
U2 0
PY 2013
BP 613
EP 616
WC Engineering, Biomedical; Neurosciences
DA 2023-11-11
ER

PT J
AU Liu, C
   Shen, WH
   Zhang, L
   Du, YK
   Yuan, ZH
AF Liu, Chuang
   Shen, Wanghui
   Zhang, Le
   Du, Yingkui
   Yuan, Zhonghu
TI Spike Neural Network Learning Algorithm Based on an Evolutionary
   Membrane Algorithm
SO IEEE ACCESS
DT Article
DE Evolutionary membrane algorithm; P systems; spiking neural network;
   supervised classification
AB As one of the important artificial intelligence fields, brain-like computing attempts to give machines a higher intelligence level by studying and simulating the cognitive principles of the human brain. Compared with the traditional neural network, the spiking neural network (SNN) has better biogenesis and stronger computing power. In this paper, an SNN learning model based on an evolutionary membrane algorithm is proposed to solve the problem of supervised classification. The proposed algorithm uses the P system's object, reaction rules, and membrane structure to solve these problems. Specifically, the proposed algorithm can automatically adjust the learning parameters of the network by adjusting the synaptic weight in the learning stage of the spiking neural model according to different application data, providing a better solution model for balance exploration and exploitation. In the simulation experiment, effectiveness verification research is carried out. The simulation results show that compared with other experimental algorithms, the proposed algorithm has a competitive advantage in solving twelve supervised classification benchmark problems through learning curves and quantified classification results.
C1 [Liu, Chuang; Shen, Wanghui; Zhang, Le; Du, Yingkui; Yuan, Zhonghu] Shenyang Univ, Sch Informat Engn, Shenyang 110044, Peoples R China.
RP Liu, C (corresponding author), Shenyang Univ, Sch Informat Engn, Shenyang 110044, Peoples R China.
EM chuang.liu@syu.edu.cn
CR Abusnaina AA, 2019, NEURAL PROCESS LETT, V49, P661, DOI 10.1007/s11063-018-9846-0
   Asuncion A., 2007, UCI MACHINE LEARNING
   Coello CAC, 2004, IEEE T EVOLUT COMPUT, V8, P256, DOI [10.1109/TEVC.2004.826067, 10.1109/tevc.2004.826067]
   Das S, 2011, IEEE T EVOLUT COMPUT, V15, P4, DOI 10.1109/TEVC.2010.2059031
   Dora S, 2019, IEEE T CYBERNETICS, V49, P989, DOI 10.1109/TCYB.2018.2791282
   Fil J, 2020, NEURAL COMPUT, V32, P1408, DOI 10.1162/neco_a_01290
   Guo P, 2019, IEEE ACCESS, V7, P60774, DOI 10.1109/ACCESS.2019.2915550
   Guo SQ, 2019, IEEE T CYBERNETICS, V49, P133, DOI 10.1109/TCYB.2017.2768554
   HUSSAIN I, 2020, SCI REP-UK, V10, P1, DOI DOI 10.1038/S41598-019-56847-4
   Jin YC, 2007, LECT NOTES COMPUT SC, V4668, P370
   Karaboga D, 2007, LECT NOTES COMPUT SC, V4529, P789, DOI 10.1007/978-3-540-72950-1_77
   Lam AYS, 2010, IEEE T EVOLUT COMPUT, V14, P381, DOI 10.1109/TEVC.2009.2033580
   Liu C, 2020, IEEE ACCESS, V8, P6020, DOI 10.1109/ACCESS.2019.2939217
   Liu C, 2019, KNOWL-BASED SYST, V165, P306, DOI 10.1016/j.knosys.2018.12.001
   Liu C, 2016, OPTIK, V127, P1909, DOI 10.1016/j.ijleo.2015.11.140
   Lobo JL, 2020, NEURAL NETWORKS, V121, P88, DOI 10.1016/j.neunet.2019.09.004
   Omran MGH, 2008, APPL MATH COMPUT, V198, P643, DOI 10.1016/j.amc.2007.09.004
   Pavlidis NG, 2005, IEEE IJCNN, P2190
   Peng H, 2019, IEEE T NEUR NET LEAR, V30, P1672, DOI 10.1109/TNNLS.2018.2872999
   Pérez-Hurtado I, 2019, J MEMBRANE COMPUT, V1, P93, DOI 10.1007/s41965-019-00014-1
   Rathi N, 2019, IEEE T COMPUT AID D, V38, P668, DOI 10.1109/TCAD.2018.2819366
   Saleh AY., 2017, J TELECOMMUN ELECT C, V9, P23
   Schliebs S, 2013, EVOL SYST-GER, V4, P87, DOI 10.1007/s12530-013-9074-9
   Singh G, 2016, APPL SOFT COMPUT, V45, P27, DOI 10.1016/j.asoc.2016.03.020
   SRINIVAS M, 1994, COMPUTER, V27, P17, DOI 10.1109/2.294849
   Vazquez RA, 2015, COMPUT INTEL NEUROSC, V2015, DOI 10.1155/2015/947098
   Vazquez RA, 2011, IEEE C EVOL COMPUTAT, P679
   Wade JJ, 2010, IEEE T NEURAL NETWOR, V21, P1817, DOI 10.1109/TNN.2010.2074212
   Wang XW, 2020, NEURAL NETWORKS, V125, P258, DOI 10.1016/j.neunet.2020.02.011
   Yang SM, 2019, IEEE T CYBERNETICS, V49, P2490, DOI 10.1109/TCYB.2018.2823730
   Yang XS, 2014, NEURAL COMPUT APPL, V24, P169, DOI 10.1007/s00521-013-1367-1
   Zhang L, 2019, IEEE ACCESS, V7, P80443, DOI 10.1109/ACCESS.2019.2921003
   Zhang XY, 2015, IEEE T NEUR NET LEAR, V26, P2816, DOI 10.1109/TNNLS.2015.2396940
NR 33
TC 12
Z9 13
U1 3
U2 28
PY 2021
VL 9
BP 17071
EP 17082
DI 10.1109/ACCESS.2021.3053280
WC Computer Science, Information Systems; Engineering, Electrical &
   Electronic; Telecommunications
DA 2023-11-11
ER

PT C
AU Kasabov, N
   Benuskova, L
   Wysoski, SG
AF Kasabov, N
   Benuskova, L
   Wysoski, SG
GP IEEE
TI A computational neurogenetic model of a spiking neuron
SO Proceedings of the International Joint Conference on Neural Networks
   (IJCNN), Vols 1-5
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT IEEE International Joint Conference on Neural Networks (IJCNN 2005)
CY JUL 31-AUG 04, 2005
CL Montreal, CANADA
AB The paper presents a novel, biologically plausible spiking neuronal model that includes a dynamic gene network. Interactions of genes in neurons affect the dynamics of the neurons and the whole network through neuronal parameters that change as a function of gene expression. The proposed model is used to build a spiking neural network (SNN) illustrated on a real EEG data case study problem. The paper also presents a novel computational approach to brain neural network modeling that integrates dynamic gene networks with a neural network model. Interaction of genes in neurons affects the dynamics of the whole neural network through neuronal parameters, which are no longer constant, but change as a function of gene expression. Through optimization of the gene interaction network, initial gene/protein expression values and ANN parameters, particular target states of the neural network operation can be achieved, and statistics about gene intercation matrix can be extracted. It is illustrated by means of a simple neurogenetic model of a spiking neural network (SNN). The behavior of SNN is evaluated by means of the local field potential, thus making it possible to attempt modeling the role of genes in different brain states, where EEG data is available to test the model. We use standard signal processing techniques like FFT to evaluate the SNN output to compare it with real human EEG data.
C1 Auckland Univ Technol, Knowledge Engn & Discovery Res Inst, Auckland, New Zealand.
RP Kasabov, N (corresponding author), Auckland Univ Technol, Knowledge Engn & Discovery Res Inst, Auckland, New Zealand.
CR [Anonymous], 2004, BIRTH MIND TINY NUMB
   Destexhe A, 1999, J NEUROSCI, V19, P4595, DOI 10.1523/jneurosci.19-11-04595.1999
   Destexhe A, 1998, J NEUROSCI, V18, P9099
   FREEMAN JW, 1975, MASS ACTION NERVOUS
   Gerstner W., 2002, SPIKING NEURON MODEL
   KASABOV N, 2004, P IEEE INT JOINT C N, V2, P1203
   Kasabov N, 2004, J COMPUT THEOR NANOS, V1, P47, DOI 10.1166/jctn.2004.006
   Lodish H., 2000, MOL CELL BIOL, P100
   *NCBI, GEN DIS
   QUIROGA RQ, DATASET 3 TONIC CLON
   QUIROGA RQ, 1998, QUANTITATIVE ANAL EE
   van Beijsterveldt CEM, 2002, BIOL PSYCHOL, V61, P111, DOI 10.1016/S0301-0511(02)00055-8
   Weaver D C, 1999, Pac Symp Biocomput, P112
   Wendling F, 2002, EUR J NEUROSCI, V15, P1499, DOI 10.1046/j.1460-9568.2002.01985.x
   Wessels L F, 2001, Pac Symp Biocomput, P508
NR 15
TC 11
Z9 11
U1 1
U2 3
PY 2005
BP 446
EP 451
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT C
AU Crossman, LC
AF Crossman, Lisa C.
BE Chicco, D
   Facchiano, A
   Tavazzi, E
   Longato, E
   Vettoretti, M
   Bernasconi, A
   Avesani, S
   Cazzaniga, P
TI Deep Recurrent Neural Networks for the Generation of Synthetic
   Coronavirus Spike Protein Sequences
SO COMPUTATIONAL INTELLIGENCE METHODS FOR BIOINFORMATICS AND BIOSTATISTICS,
   CIBB 2021
SE Lecture Notes in Bioinformatics
DT Proceedings Paper
CT 17th International Conference on Computational Intelligence Methods for
   Bioinformatics and Biostatistics (CIBB)
CY NOV 15-17, 2021
CL ELECTR NETWORK
DE Coronavirus; Deep learning; Neural networks
AB With the advent of deep learning techniques for text generation, comes the possibility of generating fully simulated or synthetic genomes. For this study, the dataset of interest is that of coronaviruses. Coronaviridae are a family of positive-sense RNA viruses capable of infecting humans and animals. These viruses usually cause mild to moderate upper respiratory tract infection; however, they can also cause more severe symptoms, gastrointestinal and central nervous system diseases. The viruses are capable of flexibly adapting to newenvironments, hence health threats from coronavirus are constant and long-term. Immunogenic spike proteins are glycoproteins found on the surface of Coronaviridae particles that mediate entry to host cells. The aim of this study was to train deep learning neural networks to produce simulated spike protein sequences, which may be able to aid in knowledge and/or vaccine design by creating alternative possible spike sequences that could arise from zoonotic sources in future. Deep learning recurrent neural networks (RNN) were trained to provide computer-simulated coronavirus spike protein sequences in the style of previously known sequences and examine their characteristics. The deep generative model was created as a recurrent neural network employing text embedding and gated recurrent unit layers in TensorFlow Keras. Training used a dataset of alpha, beta, gamma, and delta coronavirus spike sequences. In a set of 100 simulated sequences, all 100 had most significant BLAST matches to Spike proteins in searches against NCBI non-redundant dataset (NR) and possessed the expected Pfam domain matches. Simulated sequences from the neural network may be able to guide us with future prospective targets for vaccine discovery in advance of a potential novel zoonosis.
C1 [Crossman, Lisa C.] NRP Innovat Ctr, SequenceAnal Co Uk, Norwich Res Pk, Norwich, Norfolk, England.
   [Crossman, Lisa C.] Univ East Anglia, Sch Biol Sci, Norwich, Norfolk, England.
RP Crossman, LC (corresponding author), NRP Innovat Ctr, SequenceAnal Co Uk, Norwich Res Pk, Norwich, Norfolk, England.; Crossman, LC (corresponding author), Univ East Anglia, Sch Biol Sci, Norwich, Norfolk, England.
EM l.crossman@uea.ac.uk
CR Cho K., 2014, P C EMP METH NAT LAN, P1724
   Crooks GE, 2004, GENOME RES, V14, P1188, DOI 10.1101/gr.849004
   Goodsell D, 2020, MOL MONTH SARS COV 2, DOI [10.2210/rcsb_pdb/mom_2020_6, DOI 10.2210/RCSB_PDB/MOM_2020_6]
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Lan J, 2020, NATURE, V581, P215, DOI 10.1038/s41586-020-2180-5
   Li F, 2016, ANNU REV VIROL, V3, P237, DOI 10.1146/annurev-virology-110615-042301
   Luan JW, 2020, BIOCHEM BIOPH RES CO, V526, P165, DOI 10.1016/j.bbrc.2020.03.047
   Organization WH, 2003, WHOCDSCSRGAR200311
   Wu ZG, 2016, J INFECT DIS, V213, P579, DOI 10.1093/infdis/jiv476
   Zaki AM, 2012, NEW ENGL J MED, V367, P1814, DOI 10.1056/NEJMoa1211721
   Zhou GY, 2020, INT J BIOL SCI, V16, P1718, DOI 10.7150/ijbs.45123
   Zhou Peng, 2020, Nature, V588, pE6, DOI 10.1038/s41586-020-2951-z
   Zhou P, 2018, NATURE, V556, P255, DOI 10.1038/s41586-018-0010-9
   Zhu N, 2020, NEW ENGL J MED, V382, P727, DOI 10.1056/NEJMoa2001017
NR 14
TC 0
Z9 0
U1 0
U2 0
PY 2022
VL 13483
BP 217
EP 226
DI 10.1007/978-3-031-20837-9_17
WC Biochemical Research Methods; Computer Science, Artificial Intelligence;
   Mathematical & Computational Biology
DA 2023-11-11
ER

PT J
AU Soares, GE
   Borges, HE
   Gomes, RM
   Zeferino, GM
   Braga, AP
AF Soares, Gabriela E.
   Borges, Henrique E.
   Gomes, Rogerio M.
   Zeferino, Gustavo M.
   Braga, Antonio P.
TI Emergence of synchronicity in a self-organizing spiking neuron network:
   an approach via genetic algorithms
SO NATURAL COMPUTING
DT Article
DE Spiking neurons; Neural networks; Genetic algorithm; Neuronal groups;
   TNGS
ID PATTERNS; DYNAMICS; MODEL
AB Based on the Theory of Neuronal Group Selection (TNGS), we have investigated the emergence of synchronicity in a network composed of spiking neurons via genetic algorithm. The TNGS establishes that a neuronal group is the most basic unit in the cortical area of the brain and, as a rule, it is not formed by a single neuron, but by a cluster of tightly coupled neural cells which fire and oscillate in synchrony at a predefined frequency. Thus, this paper describes a method of tuning the parameters of the Izhikevich spiking neuron model through genetic algorithm in order to enable the self-organization of the neural network. Computational experiments were performed considering a network composed of neurons of the same type and another composed of neurons of different types.
C1 [Soares, Gabriela E.; Borges, Henrique E.; Gomes, Rogerio M.; Zeferino, Gustavo M.] Lab Sistemas Inteligentes LSI CEFET MG, BR-30510000 Belo Horizonte, MG, Brazil.
   [Braga, Antonio P.] Univ Fed Minas Gerais, BR-31270010 Belo Horizonte, MG, Brazil.
RP Gomes, RM (corresponding author), Lab Sistemas Inteligentes LSI CEFET MG, Av Amazonas 7675, BR-30510000 Belo Horizonte, MG, Brazil.
EM gabriela@lsi.cefetmg.br; henrique@lsi.cefetmg.br; rogerio@lsi.cefetmg.b;
   zeferino@lsi.cefetmg.br; apbraga@ufmg.br
CR [Anonymous], 1997, SITUATED COGNITION H
   [Anonymous], 2010, DYNAMICAL SYSTEMS NE
   Bi GQ, 2001, ANNU REV NEUROSCI, V24, P139, DOI 10.1146/annurev.neuro.24.1.139
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   BRYANT HL, 1976, J PHYSIOL-LONDON, V260, P279, DOI 10.1113/jphysiol.1976.sp011516
   Burgsteiner Harald, 2005, P 9 INT C ENG APPL N, P129
   Chang EY, 2000, J NEUROPHYSIOL, V84, P1136, DOI 10.1152/jn.2000.84.3.1136
   EDELMAN GM, 1993, NEURON, V10, P115, DOI 10.1016/0896-6273(93)90304-A
   Edelman GM., 1987, NEURAL DARWINISM THE
   Florian RV, 2006, LECT NOTES COMPUT SC, V4095, P570
   Ikegaya Y, 2004, SCIENCE, V304, P559, DOI 10.1126/science.1093173
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2004, CEREB CORTEX, V14, P933, DOI 10.1093/cercor/bhh053
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2007, CEREB CORTEX, V17, P2443, DOI 10.1093/cercor/bhl152
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Lindsey BG, 1997, J NEUROPHYSIOL, V78, P1714, DOI 10.1152/jn.1997.78.3.1714
   MAINEN ZF, 1995, SCIENCE, V268, P1503, DOI 10.1126/science.7770778
   Mao BQ, 2001, NEURON, V32, P883, DOI 10.1016/S0896-6273(01)00518-9
   Mühlenbein H, 1993, EVOL COMPUT, V1, P25, DOI 10.1162/evco.1993.1.1.25
   Muresan RC, 2007, J NEUROPHYSIOL, V97, P1911, DOI 10.1152/jn.01043.2006
   Prut Y, 1998, J NEUROPHYSIOL, V79, P2857, DOI 10.1152/jn.1998.79.6.2857
   Reinagel P, 2002, J NEUROSCI, V22, P6837
   Soares G. E., 2010, Proceedings of the 2010 Eleventh Brazilian Symposium on Neural Networks (SBRN 2010), P43, DOI 10.1109/SBRN.2010.16
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   STREHLER BL, 1986, P NATL ACAD SCI USA, V83, P9812, DOI 10.1073/pnas.83.24.9812
   Tetko IV, 2001, J NEUROSCI METH, V105, P15, DOI 10.1016/S0165-0270(00)00337-X
   vanSteveninck RRD, 1997, SCIENCE, V275, P1805, DOI 10.1126/science.275.5307.1805
   Villa AEP, 1999, P NATL ACAD SCI USA, V96, P1106, DOI 10.1073/pnas.96.3.1106
NR 29
TC 4
Z9 4
U1 1
U2 15
PD SEP
PY 2012
VL 11
IS 3
SI SI
BP 405
EP 413
DI 10.1007/s11047-011-9288-3
WC Computer Science, Artificial Intelligence; Computer Science,
   Interdisciplinary Applications; Computer Science, Theory & Methods
DA 2023-11-11
ER

PT C
AU Chen, XY
   Zhu, WX
   Dai, YX
   Ren, QY
AF Chen, Xinyi
   Zhu, Wenxin
   Dai, Yunxiang
   Ren, Qinyuan
GP IEEE
TI A Bio-inspired Spiking Neural Network for Control of A 4-DoF Robotic Arm
SO PROCEEDINGS OF THE 15TH IEEE CONFERENCE ON INDUSTRIAL ELECTRONICS AND
   APPLICATIONS (ICIEA 2020)
SE IEEE Conference on Industrial Electronics and Applications
DT Proceedings Paper
CT 15th IEEE Conference on Industrial Electronics and Applications (ICIEA)
CY NOV 09-13, 2020
CL ELECTR NETWORK
DE Spiking neural network; robotic arm; tempotron; bio-inspiration
AB This paper explores the control task of a 4-DoF robotic arm via a Spiking Neural Network (SNN). Inspired from the biological neuron control mechanism, the SNN is proposed and analyzed for the robotic arm control. The SNN adopts a data-driven way 10 estimate the kinematic properties of the robotic arm and further spares the difficulty of analytical model building. Biologically, the desired target position and sensory information are processed into the network, and the patterns of motor commands are able to extract from the readout layer of the SNN. Finally, numerical studies are conducted to verify the effectiveness of the proposed SNN.
C1 [Chen, Xinyi; Zhu, Wenxin; Dai, Yunxiang; Ren, Qinyuan] Zhejiang Univ, Coll Control Sci & Engn, Hangzhou, Zhejiang, Peoples R China.
RP Ren, QY (corresponding author), Zhejiang Univ, Coll Control Sci & Engn, Hangzhou, Zhejiang, Peoples R China.
EM latepat@gmail.com
CR Bohte S. M., 2000, ERROR BACKPROPAGATIO, V48
   Borisyuk R, 2009, NEURAL NETWORKS, V22, P707, DOI 10.1016/j.neunet.2009.06.047
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Clawson TS, 2016, IEEE DECIS CONTR P, P3381, DOI 10.1109/CDC.2016.7798778
   Dennis J, 2013, INT CONF ACOUST SPEE, P803, DOI 10.1109/ICASSP.2013.6637759
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Florian RV, 2007, NEURAL COMPUT, V19, P1468, DOI 10.1162/neco.2007.19.6.1468
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Hebb D, 1949, ORG BEHAV NEUROPHYSI, P39
   Izhikevich EM, 2007, CEREB CORTEX, V17, P2443, DOI 10.1093/cercor/bhl152
   Joshi P, 2005, NEURAL COMPUT, V17, P1715, DOI 10.1162/0899766054026684
   Liang WY, 2019, IEEE-ASME T MECH, V24, P2862, DOI 10.1109/TMECH.2019.2945518
   Lin S, 2001, IEEE T NEURAL NETWOR, V12, P1121, DOI 10.1109/72.950141
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   MIYAMOTO H, 1988, NEURAL NETWORKS, V1, P251, DOI 10.1016/0893-6080(88)90030-5
   Nichols E, 2013, IEEE T CYBERNETICS, V43, P115, DOI 10.1109/TSMCB.2012.2200674
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Uzzell VJ, 2004, J NEUROPHYSIOL, V92, P780, DOI 10.1152/jn.01171.2003
   Nguyen VA, 2012, IEEE T NEUR NET LEAR, V23, P971, DOI 10.1109/TNNLS.2012.2191419
   Wang XQ, 2014, NEUROCOMPUTING, V134, P230, DOI 10.1016/j.neucom.2013.07.055
   Wiklendt L, 2009, NEURAL COMPUT APPL, V18, P369, DOI 10.1007/s00521-008-0187-1
NR 21
TC 0
Z9 0
U1 1
U2 6
PY 2020
BP 616
EP 621
WC Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Niu, LY
   Wei, Y
   Long, JY
   Liu, WB
AF Niu, Li-Ye
   Wei, Ying
   Long, Jun-Yu
   Liu, Wen-Bo
TI High-Accuracy Spiking Neural Network for Objective Recognition Based on
   Proportional Attenuating Neuron
SO NEURAL PROCESSING LETTERS
DT Article
DE Spiking neural network; RA-LIF neuron; Weight normalization; Spiking
   rate
ID FIRE MODEL
AB Spiking neural network (SNN) is one of the most successful methods to imitate biological brain behavior and learning potential. The information processing mechanism of SNN combines the concepts of time and space. To address the performance dropping problem during the conversion process from the artificial neural network (ANN) to SNN, this paper proposes a proportional attenuation leaky integrate-and-fire (RA-LIF) neuron model to solve the problem of membrane potential loss caused by leaky integrate-and-fire (LIF) neurons. The accuracy of the SNN network based on RA-LIF neurons on MNIST is 98.76%. This paper also proposes a weight normalization method to help adjust the network spiking rate to reduce the loss. We evaluate and analyze the performance of SNN networks based on LIF, RA-LIF, and AD-LIF. By analyzing the spike firing rate and convergence rate, the effects of spike frequency and neuron threshold on the performance of the network are discussed. So far, the SNN transformed by ANN can only achieve worse or similar performance compared with the original network. For the first time, the performance of the SNN transformed by the proposed RA-LIF neuron model and weight normalization method is better than the original network, which shows the test accuracy of 98.88% on MNIST. The SNN obtained by converting the LeNet network using the above method achieves a test accuracy of 98.91% on MNIST. In conclusion, the RA-LIF neuron model and normalization method proposed in this paper have promising applicability. Moreover, this method has the characteristics of high precision and fast calculation speed, which can provide a reference for the research of the SNN framework.
C1 [Niu, Li-Ye; Wei, Ying; Long, Jun-Yu; Liu, Wen-Bo] Northeastern Univ, Coll Informat Sci & Engn, Shenyang 110819, Peoples R China.
   [Wei, Ying] Northeastern Univ, Key Lab Med Imaging Calculat, Minist Educ, Shenyang 110179, Peoples R China.
   [Wei, Ying] Peking Univ, Informat Technol R&D Innovat Ctr, Shaoxing, Peoples R China.
RP Wei, Y (corresponding author), Northeastern Univ, Coll Informat Sci & Engn, Shenyang 110819, Peoples R China.; Wei, Y (corresponding author), Northeastern Univ, Key Lab Med Imaging Calculat, Minist Educ, Shenyang 110179, Peoples R China.; Wei, Y (corresponding author), Peking Univ, Informat Technol R&D Innovat Ctr, Shaoxing, Peoples R China.
EM weiying@ise.neu.edu.cn
CR [Anonymous], 2012, PREDICTION CANDIDATE
   BIENENSTOCK EL, 1982, J NEUROSCI, V2, P32, DOI 10.1523/jneurosci.02-01-00032.1982
   Brette R, 2005, J NEUROPHYSIOL, V94, P3637, DOI 10.1152/jn.00686.2005
   Buonocore A, 2016, MATH BIOSCI ENG, V13, P483, DOI 10.3934/mbe.2016002
   Burkitt AN, 2006, BIOL CYBERN, V95, P1, DOI 10.1007/s00422-006-0068-6
   Butts DA, 2007, NATURE, V449, P92, DOI [10.1038/nature06105, 10.1038/natureO6105]
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Cohen GK, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00184
   Danelljan M, 2014, PROC CVPR IEEE, P1090, DOI 10.1109/CVPR.2014.143
   DeFelipe J, 2012, FRONT NEUROANAT, V6, DOI [10.3389/fnana.2012.00022, 10.3389/fnsyn.2012.00002, 10.3389/fnana.2012.00005]
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Diehl Peter U, 2015, 2015 INT JOINT C NEU, P1, DOI [DOI 10.1109/IJCNN.2015.7280696, 10.1109/IJCNN.2015.7280696]
   Fu Q, 2021, NEUROCOMPUTING, V419, P47, DOI 10.1016/j.neucom.2020.07.109
   Hamidinekoo A, 2018, MED IMAGE ANAL, V47, P45, DOI 10.1016/j.media.2018.03.006
   Lee C, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00435
   Lee JH, 2016, NEURAL PROCESS LETT, V10
   Lee JM, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00191
   Li HM, 2018, NEUROCOMPUTING, V299, P1, DOI 10.1016/j.neucom.2018.02.019
   Lin XH, 2017, NEUROCOMPUTING, V237, P59, DOI 10.1016/j.neucom.2016.08.087
   Ma JQ, 2018, IEEE T MULTIMEDIA, V20, P3111, DOI 10.1109/TMM.2018.2818020
   Mishra D, 2006, NEURAL NETW WORLD, V16, P513
   Mozafari M, 2019, PATTERN RECOGN, V94, P87, DOI 10.1016/j.patcog.2019.05.015
   Neftci E, 2014, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00272
   Neil D., 2016, P 31 ANN ACM S APPL, P293
   Querlioz D, 2013, IEEE T NANOTECHNOL, V12, P288, DOI 10.1109/TNANO.2013.2250995
   Song ZY, 2019, LECT NOTES COMPUT SC, V11555, P361, DOI 10.1007/978-3-030-22808-8_36
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Urbanczik R, 2014, NEURON, V81, P521, DOI 10.1016/j.neuron.2013.11.030
   VEMURI V, 1992, J FRANKLIN I, V329, P241, DOI 10.1016/0016-0032(92)90031-B
   Xu Q, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1646
   Yang X, 2020, SCI CHINA INFORM SCI, V63, DOI 10.1007/s11432-019-1468-0
   Zhao B, 2015, IEEE T NEUR NET LEAR, V26, P1963, DOI 10.1109/TNNLS.2014.2362542
NR 32
TC 3
Z9 3
U1 6
U2 25
PD APR
PY 2022
VL 54
IS 2
BP 1055
EP 1073
DI 10.1007/s11063-021-10669-6
EA NOV 2021
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT C
AU Yu, Q
   Goh, SK
   Tang, HJ
   Tan, KC
AF Yu, Qiang
   Goh, Sim Kuan
   Tang, Huajin
   Tan, Kay Chen
BE Handa, H
   Ishibuchi, H
   Ong, YS
   Tan, KC
TI Application of Precise-Spike-Driven Rule in Spiking Neural Networks for
   Optical Character Recognition
SO PROCEEDINGS OF THE 18TH ASIA PACIFIC SYMPOSIUM ON INTELLIGENT AND
   EVOLUTIONARY SYSTEMS, VOL 2
DT Proceedings Paper
CT 18th Asia Pacific Symposium on Intelligent and Evolutionary Systems
CY NOV 10-12, 2014
CL Singapore, SINGAPORE
DE Spiking neural networks (SNNs); optical character recognition; encoding;
   learning
ID INFORMATION; TRAINS
AB Recently, the Precise-Spike-Driven (PSD) rule has been proposed to train neurons to associate spatiotemporal spike patterns. The PSD rule is not limited to association, but can also be used for recognition of spike patterns. This paper presents a new approach with the PSD rule for optical character recognition (OCR). A new encoding method is also proposed to convert the external images into spike patterns. This is an essential step for spiking neurons to process the image patterns. The simulation results show that our approach can perform the task well, and the applied PSD rule is also benchmarked with other learning rules on the given task.
C1 [Yu, Qiang; Goh, Sim Kuan; Tang, Huajin; Tan, Kay Chen] Natl Univ Singapore, Dept Elect & Comp Engn, 4 Engn Dr 3, Singapore 117576, Singapore.
RP Yu, Q (corresponding author), Natl Univ Singapore, Dept Elect & Comp Engn, 4 Engn Dr 3, Singapore 117576, Singapore.
CR Bair W, 1996, NEURAL COMPUT, V8, P1185, DOI 10.1162/neco.1996.8.6.1185
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Florian RV, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0040233
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Hu J, 2013, NEURAL COMPUT, V25, P450, DOI 10.1162/NECO_a_00395
   Jacobs J, 2007, J NEUROSCI, V27, P3839, DOI 10.1523/JNEUROSCI.4636-06.2007
   Kayser C, 2009, NEURON, V61, P597, DOI 10.1016/j.neuron.2009.01.008
   Koepsell K, 2009, FRONT SYST NEUROSCI, V3, DOI 10.3389/neuro.06.004.2009
   LLINAS RR, 1991, P NATL ACAD SCI USA, V88, P897, DOI 10.1073/pnas.88.3.897
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   MAINEN ZF, 1995, SCIENCE, V268, P1503, DOI 10.1126/science.7770778
   Mohemmed A, 2012, INT J NEURAL SYST, V22, DOI 10.1142/S0129065712500128
   Nadasdy Z, 2009, FRONT SYST NEUROSCI, V3, DOI 10.3389/neuro.06.006.2009
   Panzeri S, 2010, TRENDS NEUROSCI, V33, P111, DOI 10.1016/j.tins.2009.12.001
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Reinagel P, 2000, J NEUROSCI, V20, P5392, DOI 10.1523/JNEUROSCI.20-14-05392.2000
   Uzzell VJ, 2004, J NEUROPHYSIOL, V92, P780, DOI 10.1152/jn.01171.2003
   van Rossum MCW, 2001, NEURAL COMPUT, V13, P751, DOI 10.1162/089976601300014321
   VanRullen R, 2005, TRENDS NEUROSCI, V28, P1, DOI 10.1016/j.tins.2004.10.010
   Xu Y, 2013, NEURAL COMPUT, V25, P1472, DOI 10.1162/NECO_a_00450
   Yu Q, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0078318
   Yu Q, 2013, IEEE T NEUR NET LEAR, V24, P1539, DOI 10.1109/TNNLS.2013.2245677
NR 22
TC 2
Z9 2
U1 0
U2 2
PY 2015
BP 65
EP 75
DI 10.1007/978-3-319-13356-0_6
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods
DA 2023-11-11
ER

PT J
AU Stoliar, P
   Schneegans, O
   Rozenberg, MJ
AF Stoliar, Pablo
   Schneegans, Olivier
   Rozenberg, Marcelo J.
TI A Functional Spiking Neural Network of Ultra Compact Neurons
SO FRONTIERS IN NEUROSCIENCE
DT Article
DE spiking neural networks; neuron models; leaky-integrated-and-fire;
   artificial intelligence; neuromorphic electronic circuits; neuromorphic
   computers; Jeffress model
AB We demonstrate that recently introduced ultra-compact neurons (UCN) with a minimal number of components can be interconnected to implement a functional spiking neural network. For concreteness we focus on the Jeffress model, which is a classic neuro-computational model proposed in the 40's to explain the sound directionality detection by animals and humans. In addition, we introduce a long-axon neuron, whose architecture is inspired by the Hodgkin-Huxley axon delay-line and where the UCNs implement the nodes of Ranvier. We then interconnect two of those neurons to an output layer of UCNs, which detect coincidences between spikes propagating down the long-axons. This functional spiking neural neuron circuit with biological relevance is built from identical UCN blocks, which are simple enough to be made with off-the-shelf electronic components. Our work realizes a new, accessible and affordable physical model platform, where neuroscientists can construct arbitrary mid-size spiking neuronal networks in a lego-block like fashion that work in continuous time. This should enable them to address in a novel experimental manner fundamental questions about the nature of the neural code and to test predictions from mathematical models and algorithms of basic neurobiology research. The present work aims at opening a new experimental field of basic research in Spiking Neural Networks to a potentially large community, which is at the crossroads of neurobiology, dynamical systems, theoretical neuroscience, condensed matter physics, neuromorphic engineering, artificial intelligence, and complex systems.
C1 [Stoliar, Pablo] Natl Inst Adv Ind Sci & Technol, Tsukuba, Ibaraki, Japan.
   [Schneegans, Olivier] Univ Paris Saclay, Sorbonne Univ, Cent Supelec, CNRS,Lab Genie Elect & Elect Paris, Gif Sur Yvette, France.
   [Rozenberg, Marcelo J.] Univ Paris Saclay, CNRS, Lab Phys Solides, Orsay, France.
RP Rozenberg, MJ (corresponding author), Univ Paris Saclay, CNRS, Lab Phys Solides, Orsay, France.
EM marcelo.rozenberg@universite-paris-saclay.fr
CR Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   BHADKAMKAR N, 1993, 1993 IEEE INTERNATIONAL CONFERENCE ON NEURAL NETWORKS, VOLS 1-3, P1902, DOI 10.1109/ICNN.1993.298847
   BHADKAMKAR NA, 1994, 1994 IEEE INTERNATIONAL CONFERENCE ON NEURAL NETWORKS, VOL 1-7, P1866, DOI 10.1109/ICNN.1994.374442
   Burger RM, 2011, J NEUROPHYSIOL, V106, P4, DOI 10.1152/jn.00205.2011
   Cariani P., 2011, SCHOLARPEDIA, V6, DOI [10.4249/scholarpedia.2920, DOI 10.4249/SCHOLARPEDIA.2920]
   del Valle J, 2019, NATURE, V569, P388, DOI 10.1038/s41586-019-1159-6
   del Valle J, 2018, J APPL PHYS, V124, DOI 10.1063/1.5047800
   Furber S, 2016, J NEURAL ENG, V13, DOI 10.1088/1741-2560/13/5/051001
   Glackin B, 2010, FRONT COMPUT NEUROSC, V4, DOI 10.3389/fncom.2010.00018
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Indiveri G, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00073
   Izhikevich EM, 2008, P NATL ACAD SCI USA, V105, P3593, DOI 10.1073/pnas.0712231105
   Janod E, 2015, ADV FUNCT MATER, V25, P6287, DOI 10.1002/adfm.201500823
   JEFFRESS LA, 1948, J COMP PHYSIOL PSYCH, V41, P35, DOI 10.1037/h0061495
   Joris PX, 1998, NEURON, V21, P1235, DOI 10.1016/S0896-6273(00)80643-1
   KONISHI M, 1993, SCI AM, V268, P66, DOI 10.1038/scientificamerican0493-66
   Korn H, 2003, CR BIOL, V326, P787, DOI 10.1016/j.crvi.2003.09.011
   Lazzaro J, 1989, NEURAL COMPUT, V1, P47, DOI 10.1162/neco.1989.1.1.47
   Liu SC, 2015, EVENT-BASED NEUROMORPHIC SYSTEMS, P1, DOI 10.1002/9781118927601
   MAHOWALD M, 1991, NATURE, V354, P515, DOI 10.1038/354515a0
   Nobukawa S, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-017-18783-z
   Rabinovich MI, 2006, REV MOD PHYS, V78, P1213, DOI 10.1103/RevModPhys.78.1213
   Reyes AD, 2003, NAT NEUROSCI, V6, P593, DOI 10.1038/nn1056
   Rozenberg MJ, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-47348-5
   Rudolph M, 2007, NEUROCOMPUTING, V70, P1966, DOI 10.1016/j.neucom.2006.10.138
   Schemmel J, 2010, IEEE INT SYMP CIRC S, P1947, DOI 10.1109/ISCAS.2010.5536970
   Stimberg M, 2019, ELIFE, V8, DOI 10.7554/eLife.47314
   Stoliar P, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00421
   Stoliar P, 2017, ADV FUNCT MATER, V27, DOI 10.1002/adfm.201604740
   Thakur CS, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00891
   Tuma T, 2016, NAT NANOTECHNOL, V11, P693, DOI [10.1038/nnano.2016.70, 10.1038/NNANO.2016.70]
   Yang SM, 2020, IEEE T NEUR NET LEAR, V31, P148, DOI 10.1109/TNNLS.2019.2899936
   Yang SM, 2019, IEEE T FUZZY SYST, V27, P304, DOI 10.1109/TFUZZ.2018.2856182
   Yang SM, 2018, NEUROCOMPUTING, V314, P394, DOI 10.1016/j.neucom.2018.07.006
   Yang SM, 2015, NEURAL NETWORKS, V71, P62, DOI 10.1016/j.neunet.2015.07.017
   Yi W, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-07052-w
   Yin LP, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-07317-4
   YOUNG SR, 1983, J NEUROSCI, V3, P1373
NR 38
TC 4
Z9 4
U1 2
U2 20
PD FEB 25
PY 2021
VL 15
AR 635098
DI 10.3389/fnins.2021.635098
WC Neurosciences
DA 2023-11-11
ER

PT C
AU Oya, T
   Asai, T
   Amemiya, Y
   Schmid, A
   Leblebici, Y
AF Oya, T
   Asai, T
   Amemiya, Y
   Schmid, A
   Leblebici, Y
GP IEEE
TI Single-electron circuitfor inhibitory spiking neural network with
   fault-tolerant architecture
SO 2005 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS (ISCAS), VOLS
   1-6, CONFERENCE PROCEEDINGS
SE IEEE International Symposium on Circuits and Systems
DT Proceedings Paper
CT IEEE International Symposium on Circuits and Systems (ISCAS)
CY MAY 23-26, 2005
CL Kobe, JAPAN
AB An inhibitory spiking neural network that uses single-electron circuit devices is described. The network consists of a number of identical neuron circuits constructed from single-electron oscillators with coupling capacitors. The neurons are cross-connected in a competing fault-tolerant architecture. Computer simulations show that the proposed circuit is capable to overcome a high rate of random device failures.
C1 Hokkaido Univ, Dept Elect Engn, Sapporo, Hokkaido 060, Japan.
RP Oya, T (corresponding author), Hokkaido Univ, Dept Elect Engn, Sapporo, Hokkaido 060, Japan.
EM oya@sapiens-ei.eng.hokudai.ac.jp
CR Asai T, 1999, IEEE T NEURAL NETWOR, V10, P1222, DOI 10.1109/72.788661
   OYA T, 2005, IN PRESS INT J UNCON, V1
   OYA T, 2004, P 2004 INT S NONL TH, P235
   Schmid A, 2003, 2003 THIRD IEEE CONFERENCE ON NANOTECHNOLOGY, VOLS ONE AND TWO, PROCEEDINGS, P516
   Yamada T, 2001, NANOTECHNOLOGY, V12, P60, DOI 10.1088/0957-4484/12/1/311
NR 5
TC 1
Z9 1
U1 0
U2 0
PY 2005
BP 2535
EP 2538
DI 10.1109/ISCAS.2005.1465142
WC Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Zhu, L
   Wang, X
   Chang, Y
   Li, JN
   Huang, TJ
   Tian, YH
AF Zhu, Lin
   Wang, Xiao
   Chang, Yi
   Li, Jianing
   Huang, Tiejun
   Tian, Yonghong
GP IEEE COMP SOC
TI Event-based Video Reconstruction via Potential-assisted Spiking Neural
   Network
SO 2022 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION
   (CVPR 2022)
SE IEEE Conference on Computer Vision and Pattern Recognition
DT Proceedings Paper
CT IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CY JUN 18-24, 2022
CL New Orleans, LA
ID BACKPROPAGATION; CAMERAS
AB Neuromorphic vision sensor is a new bio-inspired imaging paradigm that reports asynchronous, continuously perpixel brightness changes called 'events' with high temporal resolution and high dynamic range. So far, the event-based image reconstruction methods are based on artificial neural networks (ANN) or hand-crafted spatiotemporal smoothing techniques. In this paper, we first implement the image reconstruction work via deep spiking neural network (SNN) architecture. As the bio-inspired neural networks, SNNs operating with asynchronous binary spikes distributed over time, can potentially lead to greater computational efficiency on event-driven hardware. We propose a novel Event-based Video reconstruction framework based on a fully Spiking Neural Network (EVSNN), which utilizes Leaky-Integrate-and-Fire (LIF) neuron and Membrane Potential (MP) neuron. We find that the spiking neurons have the potential to store useful temporal information (memory) to complete such time-dependent tasks. Further-more, to better utilize the temporal information, we propose a hybrid potential-assisted framework (PAEVSNN) using the membrane potential of spiking neuron. The proposed neuron is referred as Adaptive Membrane Potential (AMP) neuron, which adaptively updates the membrane potential according to the input spikes. The experimental results demonstrate that our models achieve comparable performance to ANN-based models on IJRR, MVSEC, and HQF datasets. The energy consumptions of EVSNN and PAEVSNN are 19.36x and 7.75x more computationally efficient than their ANN architectures, respectively. The code and pretrained model are available at https://sites.google.com/view/evsnn.
C1 [Zhu, Lin; Li, Jianing; Huang, Tiejun; Tian, Yonghong] Peking Univ, Beijing, Peoples R China.
   [Zhu, Lin; Wang, Xiao; Chang, Yi; Li, Jianing; Tian, Yonghong] Peng Cheng Lab, Shenzhen, Peoples R China.
RP Tian, YH (corresponding author), Peking Univ, Beijing, Peoples R China.; Tian, YH (corresponding author), Peng Cheng Lab, Shenzhen, Peoples R China.
CR Amir A., 2017, P IEEE C COMP VIS PA, P7243, DOI DOI 10.1109/CVPR.2017.781
   Bardow P, 2016, PROC CVPR IEEE, P884, DOI 10.1109/CVPR.2016.102
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Brandli C, 2014, IEEE J SOLID-ST CIRC, V49, P2333, DOI 10.1109/JSSC.2014.2342715
   Chankyu Lee, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12374), P366, DOI 10.1007/978-3-030-58526-6_22
   Cook M, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P770, DOI 10.1109/IJCNN.2011.6033299
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Fang Wei, 2020, SPIKINGJELLY
   Fang WJ, 2021, IEEE INT SYMP INFO, P3261, DOI 10.1109/ISIT45174.2021.9517911
   Cadena PRG, 2021, IEEE T IMAGE PROCESS, V30, P2488, DOI 10.1109/TIP.2021.3052070
   Gehrig M, 2020, IEEE INT CONF ROBOT, P4195, DOI [10.1109/icra40945.2020.9197133, 10.1109/ICRA40945.2020.9197133]
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Haessig G, 2018, IEEE T BIOMED CIRC S, V12, P860, DOI 10.1109/TBCAS.2018.2834558
   Hagenaars Jesse, 2021, 35 C NEUR INF PROC S
   Horowitz M, 2014, ISSCC DIG TECH PAP I, V57, P10, DOI 10.1109/ISSCC.2014.6757323
   Jin Y., 2018, ADV NEURAL INFORM PR
   Kaiser J, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00424
   Kim H, 2016, LECT NOTES COMPUT SC, V9910, P349, DOI 10.1007/978-3-319-46466-4_21
   Kingma D. P., 2014, ARXIV, DOI DOI 10.48550/ARXIV.1412.6980
   Lai WS, 2018, LECT NOTES COMPUT SC, V11219, P179, DOI 10.1007/978-3-030-01267-0_11
   Lee C, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00119
   Lee JM, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00191
   Lichtsteiner Patrick, 2008, IEEE Journal of Solid-State Circuits, V43, P566, DOI 10.1109/JSSC.2007.914337
   Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48
   Mahony R, 2018, ARXIV181100386, DOI [10.1007/978-3-030-20873-8, DOI 10.1007/978-3-030-20873-8]
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Mohemmed A, 2012, INT J NEURAL SYST, V22, DOI 10.1142/S0129065712500128
   Mueggler E, 2017, INT J ROBOT RES, V36, P142, DOI 10.1177/0278364917691115
   Munda G, 2018, INT J COMPUT VISION, V126, P1381, DOI 10.1007/s11263-018-1106-2
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Orchard G, 2015, IEEE T PATTERN ANAL, V37, P2028, DOI 10.1109/TPAMI.2015.2392947
   Parameshwara Chethan M., 2021, ARXIV210506562
   Paredes-Vallés F, 2021, PROC CVPR IEEE, P3445, DOI 10.1109/CVPR46437.2021.00345
   Paredes-Vallés F, 2020, IEEE T PATTERN ANAL, V42, P2051, DOI 10.1109/TPAMI.2019.2903179
   Paszke A., 2019, ADV NEURAL INFORM PR, P8024
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Ranc<comma>on Ulysse, 2021, ARXIV210913751, P2
   Rathi N, 2023, IEEE T NEUR NET LEAR, V34, P3174, DOI 10.1109/TNNLS.2021.3111897
   Rebecq H, 2019, PROC CVPR IEEE, P3852, DOI 10.1109/CVPR.2019.00398
   Rebecq H, 2017, IEEE ROBOT AUTOM LET, V2, P593, DOI 10.1109/LRA.2016.2645143
   Rebecq Henri, 2018, C ROB LEARN PMLR, P969
   Rebecq Henri, 2019, IEEE T PATTERN ANAL, V4, P5
   Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Scheerlinck C, 2020, IEEE WINT CONF APPL, P156, DOI 10.1109/WACV45572.2020.9093366
   Scheerlinck C, 2019, IEEE ROBOT AUTOM LET, V4, P816, DOI 10.1109/LRA.2019.2893427
   Shrestha SB, 2018, ADV NEUR IN, V31
   Stoffregen Timo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12372), P534, DOI 10.1007/978-3-030-58583-9_32
   Strohmer B, 2021, FRONT NEUROSCI-SWITZ, V15, DOI 10.3389/fnins.2021.633945
   Wang L, 2019, PROC CVPR IEEE, P10073, DOI 10.1109/CVPR.2019.01032
   Wang Xiao, 2021, VISEVENT RELIABLE OB, P1
   Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861
   Weng Wenming, 2021, P IEEECVF INT C COMP, P2563
   WERBOS PJ, 1990, P IEEE, V78, P1550, DOI 10.1109/5.58337
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Wu ZZ, 2022, IEEE T NEUR NET LEAR, V33, P6249, DOI 10.1109/TNNLS.2021.3073016
   Xing YN, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.590164
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zhang X, 2021, PROC CVPR IEEE, P14230, DOI 10.1109/CVPR46437.2021.01401
   Zheng H., 2020, ARXIV201105280
   Zheng YJ, 2021, PROC CVPR IEEE, P6354, DOI 10.1109/CVPR46437.2021.00629
   Zhu Alex Zihao, 2018, IEEE Robotics and Automation Letters, V3, P2032, DOI 10.1109/LRA.2018.2800793
   Zhu AZ, 2019, PROC CVPR IEEE, P989, DOI 10.1109/CVPR.2019.00108
   Zhu L, 2020, PROC CVPR IEEE, P1435, DOI 10.1109/CVPR42600.2020.00151
   Zhu L, 2019, IEEE INT CON MULTI, P1432, DOI 10.1109/ICME.2019.00248
NR 67
TC 10
Z9 10
U1 6
U2 10
PY 2022
BP 3584
EP 3594
DI 10.1109/CVPR52688.2022.00358
WC Computer Science, Artificial Intelligence; Imaging Science &
   Photographic Technology
DA 2023-11-11
ER

PT C
AU Pitti, A
   Gaussier, P
   Quoy, M
AF Pitti, Alex
   Gaussier, Philippe
   Quoy, Mathias
BE Cong, F
   Leung, A
   Wei, Q
TI INFERNO: A Novel Architecture for Generating Long Neuronal Sequences
   with Spikes
SO ADVANCES IN NEURAL NETWORKS, PT I
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 14th International Symposium on Neural Networks (ISNN)
CY JUN 21-26, 2017
CL JAPAN
DE Free-energy; Predictive coding; Working memory; Neuronal sequences;
   Spiking neurons; STDP; Basal ganglia; Cortico-basal loops; Habit
   learning
ID BRAIN
AB Human working memory is capable to generate dynamically robust and flexible neuronal sequences for action planning, problem solving and decision making. However, current neurocomputational models of working memory find hard to achieve these capabilities since intrinsic noise is difficult to stabilize over time and destroys global synchrony. As part of the principle of free-energy minimization proposed by Karl Friston, we propose a novel neural architecture to optimize the free-energy inherent to spiking recurrent neural networks to regulate their activity. We show for the first time that it is possible to stabilize iteratively the long-range control of a recurrent spiking neurons network over long sequences. We identify our architecture as the working memory composed by the Basal Ganglia and the Intra-Parietal Lobe for action selection and we make some comparisons with other networks such as deep neural networks and neural Turing machines. We name our architecture INFERNO for Iterative Free-Energy Optimization for Recurrent Neural Network. abstract environment.
C1 [Pitti, Alex; Gaussier, Philippe; Quoy, Mathias] Univ Cergy Pontoise, CNRS, UMR 8051, Lab ETIS,ENSEA, Cergy, France.
RP Pitti, A (corresponding author), Univ Cergy Pontoise, CNRS, UMR 8051, Lab ETIS,ENSEA, Cergy, France.
EM alexandre.pitti@u-cergy.fr
CR Benedek M, 2016, SCI REP-UK, V6, DOI 10.1038/srep22959
   Dehaene S, 2015, NEURON, V88, P2, DOI 10.1016/j.neuron.2015.09.019
   Friston KJ, 2006, J PHYSIOL-PARIS, V100, P70, DOI 10.1016/j.jphysparis.2006.10.001
   Friston KJ, 2005, PHILOS T R SOC B, V360, P815, DOI 10.1098/rstb.2005.1622
   Graves A, 2016, NATURE, V538, P471, DOI 10.1038/nature20101
   Guthrie M, 2013, J NEUROPHYSIOL, V109, P3025, DOI 10.1152/jn.00026.2013
   Koechlin E, 2016, CURR OPIN NEUROBIOL, V37, P1, DOI 10.1016/j.conb.2015.11.004
   Miller E, 2015, DIALOGUES CLIN NEURO, V15, P411
   Pitti A, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0173684
   Seger CA, 2010, ANNU REV NEUROSCI, V33, P203, DOI 10.1146/annurev.neuro.051508.135546
   Topalidou M, 2015, RESCIENCE, V1, P1
   Tsuda Ichiro, 2004, Journal of Integrative Neuroscience, V3, P159, DOI 10.1142/S021963520400049X
   Van Rullen R, 1998, BIOSYSTEMS, V48, P229, DOI 10.1016/S0303-2647(98)00070-7
   Zylberberg A, 2011, TRENDS COGN SCI, V15, P293, DOI 10.1016/j.tics.2011.05.007
NR 14
TC 0
Z9 0
U1 0
U2 3
PY 2017
VL 10261
BP 421
EP 428
DI 10.1007/978-3-319-59072-1_50
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods
DA 2023-11-11
ER

PT J
AU Polap, D
   Wozniak, M
   Holubowski, W
   Damasevicius, R
AF Polap, Dawid
   Wozniak, Marcin
   Holubowski, Waldemar
   Damasevicius, Robertas
TI A heuristic approach to the hyperparameters in training spiking neural
   networks using spike-timing-dependent plasticity
SO NEURAL COMPUTING & APPLICATIONS
DT Article
DE Spiking neural network; Heuristic; Hyperparameters; Federated learning;
   Image processing
ID OPTIMIZATION
AB The third type of neural network called spiking is developed due to a more accurate representation of neuronal activity in living organisms. Spiking neural networks have many different parameters that can be difficult to adjust manually to the current classification problem. The analysis and selection of coefficients' values in the network can be analyzed as an optimization problem. A practical method for automatic selection of them can decrease the time needed to develop such a model. In this paper, we propose the use of a heuristic approach to analyze and select coefficients with the idea of collaborative working. The proposed idea is based on parallel analyzing of different coefficients and choosing the best of them or average ones. This type of optimization problem allows the selection of all variables, which can significantly affect the convergence of the accuracy. Our proposal was tested using network simulators and popular databases to indicate the possibilities of the described approach. Five different heuristic algorithms were tested and the best results were reached by Cuckoo Search Algorithm, Grasshopper Optimization Algorithm, and Polar Bears Algorithm.
C1 [Polap, Dawid; Wozniak, Marcin; Holubowski, Waldemar; Damasevicius, Robertas] Silesian Tech Univ, Fac Appl Math, Kaszubska 23, PL-44100 Gliwice, Poland.
RP Polap, D (corresponding author), Silesian Tech Univ, Fac Appl Math, Kaszubska 23, PL-44100 Gliwice, Poland.
EM Dawid.Polap@polsl.pl; Marcin.Wozniak@polsl.pl;
   Waldemar.Holubowski@polsl.pl; Robertas.Damasevicius@polsl.pl
CR Averbeck BB, 2009, NEURON, V62, P310, DOI 10.1016/j.neuron.2009.04.021
   Balaji A, 2020, IEEE T VLSI SYST, V28, P76, DOI 10.1109/TVLSI.2019.2951493
   Balandat M., 2020, ADV NEURAL INFORM PR, DOI DOI 10.48550/ARXIV.1910.06403
   Bernert M, 2019, INT J NEURAL SYST, V29, DOI 10.1142/S0129065718500594
   Burges, 2010, MNIST HANDWRITTEN DI, DOI DOI 10.1561/2400000035
   Capecci E, 2019, EVOLV SYST, P1
   Cheng L, 2021, IEEE T COGN DEV SYST, V13, P151, DOI 10.1109/TCDS.2019.2918228
   Czerpak P, 2012, STUD P POL ASS KNOWL, P60
   Dabrowska D, 2016, ENVIRON EARTH SCI, V75, DOI 10.1007/s12665-016-5554-0
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Donati E, 2019, IEEE T BIOMED CIRC S, V13, P795, DOI 10.1109/TBCAS.2019.2925454
   Haessig G, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-40064-0
   Hazan H, 2018, FRONT NEUROINFORM, V12, DOI 10.3389/fninf.2018.00089
   Vu HT, 2019, J SUPERCOMPUT, V75, P5367, DOI 10.1007/s11227-019-02792-y
   Hyla T., 2021, P 54 HAWAII INT C SY, P7269
   Kristensen L, 2020, B AM PHYS SOC
   Kumarasinghe K, 2020, NEURAL NETWORKS, V121, P169, DOI 10.1016/j.neunet.2019.08.029
   Lee C, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00119
   Li LS, 2018, J MACH LEARN RES, V18
   Lin JCW, 2021, APPL INTELL, V51, P4806, DOI 10.1007/s10489-020-02080-w
   Mardani A, 2020, J CLEAN PROD, V275, DOI 10.1016/j.jclepro.2020.122942
   Mirjalili S, 2016, ADV ENG SOFTW, V95, P51, DOI 10.1016/j.advengsoft.2016.01.008
   Morrison A, 2007, NEURAL COMPUT, V19, P1437, DOI 10.1162/neco.2007.19.6.1437
   Nour M, 2021, NEURAL COMPUT APPL, V33, P15815, DOI 10.1007/s00521-021-06202-4
   OConnor Peter, 2019, 22 INT C ARTIFICIAL, V89
   Ozturk S., 2021, IOP Conference Series: Materials Science and Engineering, V1099, DOI 10.1088/1757-899X/1099/1/012072
   Polap D, 2017, SYMMETRY-BASEL, V9, DOI 10.3390/sym9100203
   Rongala UB, 2020, NEURAL NETWORKS, V123, P273, DOI 10.1016/j.neunet.2019.11.020
   Saremi S., 2020, NATURE INSPIRED OPTI
   Saunders DJ, 2019, NEURAL NETWORKS, V119, P332, DOI 10.1016/j.neunet.2019.08.016
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Tubishat M, 2020, EXPERT SYST APPL, V145, DOI 10.1016/j.eswa.2019.113122
   Wajda A, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21217247
   Wang W, 2019, IEEE ACCESS, V7, P117165, DOI 10.1109/ACCESS.2019.2936604
   Wlodarczyk-Sielicka M, 2017, 2017 BALTIC GEODETIC CONGRESS (BGC GEOMATICS), P193, DOI 10.1109/BGC.Geomatics.2017.67
   Xiao H, 2017, ARXIV PREPRINT ARXIV
   Yang L, 2020, NEUROCOMPUTING, V415, P295, DOI 10.1016/j.neucom.2020.07.061
   Yang XS, 2009, WOR CONG NAT BIOL, P210, DOI 10.1109/nabic.2009.5393690
NR 39
TC 7
Z9 7
U1 1
U2 8
PD AUG
PY 2022
VL 34
IS 16
SI SI
BP 13187
EP 13200
DI 10.1007/s00521-021-06824-8
EA DEC 2021
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT J
AU Rodrigues, DR
   Moukhader, R
   Luo, YX
   Fang, B
   Pontlevy, A
   Hamadeh, A
   Zeng, ZM
   Carpentieri, M
   Finocchio, G
AF Rodrigues, Davi R.
   Moukhader, Rayan
   Luo, Yanxiang
   Fang, Bin
   Pontlevy, Adrien
   Hamadeh, Abbas
   Zeng, Zhongming
   Carpentieri, Mario
   Finocchio, Giovanni
TI Spintronic Hodgkin-Huxley-Analogue Neuron Implemented with a Single
   Magnetic Tunnel Junction
SO PHYSICAL REVIEW APPLIED
DT Article
ID FIRING RATES; NETWORKS; CLASSIFICATION; GENERATION; DEPENDENCE;
   ALGORITHM; CIRCUIT; MEMORY; MODEL; STDP
AB Spiking neural networks aim to emulate the brain's properties to achieve similar parallelism and high processing power. A caveat of these neural networks is the high computational cost for emulation, while current proposals for analogue implementations are energy inefficient and not scalable. We propose a device based on a single magnetic tunnel junction to perform neuron firing for spiking neural networks without the need for any resetting procedure. We leverage two areas of physics, magnetism and thermal effects, to obtain biorealistic spiking behavior analogous to the Hodgkin-Huxley model of the neuron. The device is also able to emulate the simpler leaky-integrate-and-fire model. Numerical simulations using experimental-based parameters demonstrate firing frequency in the megahertz to gigahertz range under constant input at room temperature. The compactness, scalability, low cost, CMOS compatibility, and power efficiency of magnetic tunnel junctions advocates for their broad use in hardware implementations of spiking neural networks.
C1 [Rodrigues, Davi R.; Carpentieri, Mario] Politecn Bari, Dept Elect & Informat Engn, I-70125 Bari, Italy.
   [Moukhader, Rayan; Pontlevy, Adrien; Finocchio, Giovanni] Univ Messina, Dept Math & Comp Sci, Phys Sci & Earth Sci, I-98166 Messina, Italy.
   [Moukhader, Rayan] Lebanese Univ, Fac Sci, Multidisciplinary Phys Lab, Beirut 1500, Lebanon.
   [Luo, Yanxiang; Fang, Bin; Zeng, Zhongming] Chinese Acad Sci, Suzhou Inst Nanotech & Nanob, Key Lab Multifunct Nanomat & Smart Syst, Suzhou 215123, Jiangsu, Peoples R China.
   [Hamadeh, Abbas] Tech Univ Kaiserslautern, Fachbereich Phys & Landesforschungszentrum OPTIMAS, D-67663 Kaiserslautern, Germany.
RP Rodrigues, DR (corresponding author), Politecn Bari, Dept Elect & Informat Engn, I-70125 Bari, Italy.
EM davi.rodrigues@poliba.it; mario.carpentieri@poliba.it;
   gfinocchio@unime.it
CR Abbott L.F., 2008, STAT MECH NEURAL NET, P5
   ABBOTT LF, 1990, LECT NOTES PHYS, V368, P5
   ACKLEY DH, 1985, COGNITIVE SCI, V9, P147
   Afifi A, 2009, 2009 EUROPEAN CONFERENCE ON CIRCUIT THEORY AND DESIGN, VOLS 1 AND 2, P563, DOI 10.1109/ECCTD.2009.5275035
   Alzate JG, 2014, APPL PHYS LETT, V104, DOI 10.1063/1.4869152
   Amirsoleimani A, 2016, IEEE I C ELECT CIRC, P81, DOI 10.1109/ICECS.2016.7841137
   Bouvier M, 2019, ACM J EMERG TECH COM, V15, DOI 10.1145/3304103
   BROWN WF, 1963, PHYS REV, V130, P1677, DOI 10.1103/PhysRev.130.1677
   Burr GW, 2017, ADV PHYS-X, V2, P89, DOI 10.1080/23746149.2016.1259585
   Campbell K, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10020550
   Christensen DV, 2022, NEUROMORPH COMPUT EN, V2, DOI 10.1088/2634-4386/ac4a83
   Chua L, 2013, NANOTECHNOLOGY, V24, DOI 10.1088/0957-4484/24/38/383001
   Chua L, 2012, INT J BIFURCAT CHAOS, V22, DOI 10.1142/S021812741230011X
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Davis AH, 2001, J APPL PHYS, V89, P7567, DOI 10.1063/1.1357126
   De Rose R, 2018, IEEE T CIRCUITS-I, V65, P1086, DOI 10.1109/TCSI.2017.2762431
   Deco G, 2009, PROG NEUROBIOL, V88, P1, DOI 10.1016/j.pneurobio.2009.01.006
   Delorme A, 2003, NETWORK-COMP NEURAL, V14, P613, DOI 10.1088/0954-898X/14/4/301
   Dieny B, 2020, NAT ELECTRON, V3, P446, DOI 10.1038/s41928-020-0461-5
   Dutta S, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-07418-y
   Fang B, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms11259
   Feldmann J, 2019, NATURE, V569, P208, DOI 10.1038/s41586-019-1157-8
   Feng HF, 2001, NEURAL NETWORKS, V14, P955, DOI 10.1016/S0893-6080(01)00074-0
   Finocchio G, 2011, PHYS REV B, V83, DOI 10.1103/PhysRevB.83.134402
   Finocchio G., 2023, ARXIV
   Finocchio G, 2021, J MAGN MAGN MATER, V521, DOI 10.1016/j.jmmm.2020.167506
   Fukami S, 2018, J APPL PHYS, V124, DOI 10.1063/1.5042317
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   Ghosh-Dastidar S, 2007, INTEGR COMPUT-AID E, V14, P187
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Giordano A, 2012, J APPL PHYS, V111, DOI 10.1063/1.3673428
   Goodman Dan, 2008, Front Neuroinform, V2, P5, DOI 10.3389/neuro.11.005.2008
   Grollier J, 2016, P IEEE, V104, P2024, DOI 10.1109/JPROC.2016.2597152
   Gupta A, 2007, IEEE IJCNN, P53, DOI 10.1109/IJCNN.2007.4370930
   Haensch W, 2019, P IEEE, V107, P108, DOI 10.1109/JPROC.2018.2871057
   Hamadeh A, 2012, PHYS REV B, V85, DOI 10.1103/PhysRevB.85.140408
   Hebb D. O., 1949, ORG BEHAV
   Hejda M, 2022, PHYS REV APPL, V17, DOI 10.1103/PhysRevApplied.17.024072
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Hopfield JJ, 2001, P NATL ACAD SCI USA, V98, P1282, DOI 10.1073/pnas.031567098
   Hu XY, 2019, NONLINEAR DYNAM, V97, P1721, DOI 10.1007/s11071-019-05100-8
   Indiveri G., 2003, CIRC SYST 2003 P 200, V4, P4, DOI DOI 10.1109/ISCAS.2003.1206342
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2007, CEREB CORTEX, V17, P2443, DOI 10.1093/cercor/bhl152
   Jaiswal A, 2017, IEEE T ELECTRON DEV, V64, P1818, DOI 10.1109/TED.2017.2671353
   Jin X, 2010, COMPUT SCI ENG, V12, P91, DOI 10.1109/MCSE.2010.112
   Jin X, 2008, IEEE IJCNN, P2812, DOI 10.1109/IJCNN.2008.4634194
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Khymyn R, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-33697-0
   Kravtsov K, 2011, OPT EXPRESS, V19, P2133, DOI 10.1364/OE.19.002133
   Kurenkov A, 2019, ADV MATER, V31, DOI 10.1002/adma.201900636
   Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508
   Liyanagedera CM, 2017, PHYS REV APPL, V8, DOI 10.1103/PhysRevApplied.8.064017
   Lobo JL, 2020, NEURAL NETWORKS, V121, P88, DOI 10.1016/j.neunet.2019.09.004
   Lv WX, 2022, APPL PHYS LETT, V121, DOI 10.1063/5.0126392
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Markovic D, 2020, NAT REV PHYS, V2, P499, DOI 10.1038/s42254-020-0208-2
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   MILLER RN, 1981, BIOPHYS J, V34, P227, DOI 10.1016/S0006-3495(81)84847-3
   Natarajan A, 2018, IEEE T BIOMED CIRC S, V12, P918, DOI 10.1109/TBCAS.2018.2837055
   Nawrot M, 1999, J NEUROSCI METH, V94, P81, DOI 10.1016/S0165-0270(99)00127-2
   Puebla J, 2020, COMMUN MATER, V1, DOI 10.1038/s43246-020-0022-5
   Querlioz D, 2012, IEEE INT SYMP NANO, P203
   Rajabali M, 2023, PHYS REV APPL, V19, DOI 10.1103/PhysRevApplied.19.034070
   Rajasekharan D, 2022, IEEE T COMPUT AID D, V41, P2107, DOI 10.1109/TCAD.2021.3101407
   Romera M, 2022, NAT COMMUN, V13, DOI 10.1038/s41467-022-28159-1
   Roxin A, 2011, J NEUROSCI, V31, P16217, DOI 10.1523/JNEUROSCI.1677-11.2011
   Roy K, 2019, NATURE, V575, P607, DOI 10.1038/s41586-019-1677-2
   Sah MP, 2014, IEEE CIRC SYST MAG, V14, P12, DOI 10.1109/MCAS.2013.2296414
   Schemmel J., 2014, ENCY COMPUTATIONAL N, P1
   Schuman CD, 2022, NAT COMPUT SCI, V2, P10, DOI 10.1038/s43588-021-00184-y
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Sengupta A, 2016, PHYS REV APPL, V6, DOI 10.1103/PhysRevApplied.6.064003
   Sengupta A, 2016, SCI REP-UK, V6, DOI 10.1038/srep30039
   Sengupta A, 2016, PHYS REV APPL, V5, DOI 10.1103/PhysRevApplied.5.024012
   Serrano-Gotarredona T, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00002
   Shang CH, 1998, PHYS REV B, V58, pR2917, DOI 10.1103/PhysRevB.58.R2917
   Siegelmann H.T., 1999, NEURAL NETWORKS ANAL
   Siracusano G, 2018, IEEE T MAGN, V54, DOI 10.1109/TMAG.2018.2799856
   Slavin A, 2009, IEEE T MAGN, V45, P1875, DOI 10.1109/TMAG.2008.2009935
   Slonczewski JC, 1996, J MAGN MAGN MATER, V159, pL1, DOI 10.1016/0304-8853(96)00062-5
   Srinivasan G, 2016, SCI REP-UK, V6, DOI 10.1038/srep29545
   Taherkhani A, 2020, NEURAL NETWORKS, V122, P253, DOI 10.1016/j.neunet.2019.09.036
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Teeter C, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-017-02717-4
   Tomasello R, 2018, PHYS REV B, V97, DOI 10.1103/PhysRevB.97.060402
   Torrejon J, 2017, NATURE, V547, P428, DOI 10.1038/nature23011
   Umesh S, 2019, J SYST ARCHITECT, V97, P349, DOI 10.1016/j.sysarc.2018.11.005
   Vincent AF, 2015, IEEE T BIOMED CIRC S, V9, P166, DOI 10.1109/TBCAS.2015.2414423
   Wade JJ, 2010, IEEE T NEURAL NETWOR, V21, P1817, DOI 10.1109/TNN.2010.2074212
   Wan QZ, 2019, ADV MATER TECHNOL-US, V4, DOI 10.1002/admt.201900037
   Wu MH, 2022, PHYS REV APPL, V18, DOI 10.1103/PhysRevApplied.18.064034
   Xiao J, 2005, PHYS REV B, V72, DOI 10.1103/PhysRevB.72.014446
   Yang JQ, 2020, NANO ENERGY, V74, DOI 10.1016/j.nanoen.2020.104828
   Yang Q, 2022, NANO LETT, DOI 10.1021/acs.nanolett.2c02409
   Zeng ZM, 2012, APPL PHYS LETT, V101, DOI 10.1063/1.4744914
   Zeng ZM, 2013, SCI REP-UK, V3, DOI 10.1038/srep01426
   Zeng ZM, 2013, NANOSCALE, V5, P2219, DOI 10.1039/c2nr33407k
   Zhang S, 2002, PHYS REV LETT, V88, DOI 10.1103/PhysRevLett.88.236601
   Zink BR, 2018, J APPL PHYS, V124, DOI 10.1063/1.5042444
NR 100
TC 0
Z9 0
U1 4
U2 4
PD JUN 2
PY 2023
VL 19
IS 6
AR 064010
DI 10.1103/PhysRevApplied.19.064010
WC Physics, Applied
DA 2023-11-11
ER

PT C
AU Xu, Q
   Li, YX
   Shen, JR
   Liu, JK
   Tang, HJ
   Pan, G
AF Xu, Qi
   Li, Yaxin
   Shen, Jiangrong
   Liu, Jian K.
   Tang, Huajin
   Pan, Gang
GP IEEE
TI Constructing Deep Spiking Neural Networks from Artificial Neural
   Networks with Knowledge Distillation
SO 2023 IEEE/CVF CONFERENCE ON COMPUTER VISION AND PATTERN RECOGNITION,
   CVPR
SE IEEE Conference on Computer Vision and Pattern Recognition
DT Proceedings Paper
CT IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
CY JUN 17-24, 2023
CL Vancouver, CANADA
AB Spiking neural networks (SNNs) are well-known as brain-inspired models with high computing efficiency, due to a key component that they utilize spikes as information units, close to the biological neural systems. Although spiking based models are energy efficient by taking advantage of discrete spike signals, their performance is limited by current network structures and their training methods. As discrete signals, typical SNNs cannot apply the gradient descent rules directly into parameter adjustment as artificial neural networks (ANNs). Aiming at this limitation, here we propose a novel method of constructing deep SNN models with knowledge distillation (KD) that uses ANN as the teacher model and SNN as the student model. Through the ANN-SNN joint training algorithm, the student SNN model can learn rich feature information from the teacher ANN model through the KD method, yet it avoids training SNN from scratch when communicating with non-differentiable spikes. Our method can not only build a more efficient deep spiking structure feasibly and reasonably but use few time steps to train the whole model compared to direct training or ANN to SNN methods. More importantly, it has a superb ability of noise immunity for various types of artificial noises and natural signals. The proposed novel method provides efficient ways to improve the performance of SNN through constructing deeper structures in a high-throughput fashion, with potential usage for light and efficient brain-inspired computing of practical scenarios.
C1 [Xu, Qi; Li, Yaxin] Dalin Univ Technol, Sch Artificial Intelligence, Dalian, Peoples R China.
   [Shen, Jiangrong; Tang, Huajin; Pan, Gang] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou, Peoples R China.
   [Liu, Jian K.] Univ Leeds, Sch Comp, Leeds, W Yorkshire, England.
RP Shen, JR; Pan, G (corresponding author), Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou, Peoples R China.
EM jrshen@zju.edu.cn; gpan@zju.edu.cn
CR Bang D, 2021, INFORM SCIENCES, V576, P743, DOI 10.1016/j.ins.2021.08.020
   Bing Han, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13555, DOI 10.1109/CVPR42600.2020.01357
   Bu Tong, 2021, INT C LEARN REPR
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Cho JH, 2019, IEEE I CONF COMP VIS, P4793, DOI 10.1109/ICCV.2019.00489
   Deng Shikuang, 2021, INT C LEARN REPR
   Diehl Peter U, 2015, 2015 INT JOINT C NEU, P1, DOI [DOI 10.1109/IJCNN.2015.7280696, 10.1109/IJCNN.2015.7280696]
   Ding Jianhao, 2022, ADV NEURAL INFORM PR, V34, P1
   Ding Jianhao, 2021, P 30 INT JOINT C ART, P2328, DOI [DOI 10.24963/IJCAI.2021/321, 10.24963/ijcai.2021/321]
   Fang W., 2021, ADV NEURAL INFORM PR, V34, P21056, DOI DOI 10.48550/ARXIV.2102.04159
   Fang Wei, 2020, SPIKINGJELLY
   Garg Isha, 2021, P IEEECVF INT C COMP, P4671
   Gigante G, 2007, PHYS REV LETT, V98, DOI 10.1103/PhysRevLett.98.148101
   Hinton G., 2015, ARXIV150302531, DOI DOI 10.4140/TCP.N.2015.249
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Kim J., 2020, ADV NEURAL INFORM PR, V33, p19 534
   Kobayashi R, 2009, FRONT COMPUT NEUROSC, V3, DOI 10.3389/neuro.10.009.2009
   Kushawaha RK, 2021, INT C PATT RECOG, P4536, DOI 10.1109/ICPR48806.2021.9412147
   Li Haitong, 2015, EXPLORING KNOWLEDGE
   Mao HZ, 2017, IEEE COMPUT SOC CONF, P1927, DOI 10.1109/CVPRW.2017.241
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Ostojic S, 2014, NAT NEUROSCI, V17, P594, DOI 10.1038/nn.3658
   Rathi Nitin, 2020, INT C LEARN REPR
   Romero A., 2015, 3 INT C LEARNING REP, P1
   Roy K, 2019, NATURE, V575, P607, DOI 10.1038/s41586-019-1677-2
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Shen JR, 2021, NEURAL COMPUT, V33, P2971, DOI 10.1162/neco_a_01432
   Singh S, 2020, ANN I S COM, P363, DOI 10.1109/ISCA45697.2020.00039
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Wu H, 2021, AAAI CONF ARTIF INTE, V35, P10320
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Xu Q, 2022, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2022.3232106
   Xu Qi, 2018, IJCAI
   Xu Qi, 2021, IEEE T NEURAL NETWOR, V33, P1935
   Yu Q, 2022, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2022.3165527
   Zenke F, 2015, NAT COMMUN, V6, DOI 10.1038/ncomms7922
NR 37
TC 0
Z9 0
U1 0
U2 0
PY 2023
BP 7886
EP 7895
DI 10.1109/CVPR52729.2023.00762
WC Computer Science, Artificial Intelligence; Computer Science,
   Interdisciplinary Applications; Computer Science, Software Engineering;
   Computer Science, Theory & Methods
DA 2023-11-11
ER

PT C
AU Slade, S
   Zhang, L
AF Slade, Sam
   Zhang, Li
GP IEEE
TI Topological Evolution of Spiking Neural Networks
SO 2018 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUL 08-13, 2018
CL Rio de Janeiro, BRAZIL
ID PARTICLE SWARM; OPTIMIZATION; ALGORITHM; SIMULATOR; MODEL
AB Neuro-evolution is often used to generate the parameters, topology, and rules of artificial neural networks. This technique allows for automatic configuration of a neural network. In this paper we propose a method to generate Spiking Neural Networks (SNNs) automatically called NENG (NeuroEvolutionary Network Generation). The aim was to help alleviate the manual construction and optimization of neural network implementations. The results show the algorithm is successful at generating and improving the design of SNNs for a Classification task. After 812 generations with a population size of 20 the algorithm converges to model the Xor gate with 100% accuracy. The results show improvements to the algorithm execution time and number of neurons over time.
C1 [Slade, Sam; Zhang, Li] Northumbria Univ, Dept Comp & Informat Sci, Newcastle, England.
RP Slade, S (corresponding author), Northumbria Univ, Dept Comp & Informat Sci, Newcastle, England.
EM samslade_13@hotmail.com; li.zhang@northumbria.ac.uk
CR [Anonymous], 2017, SWARM EVOLUTIONARY C
   Bartlett P. L., 1999, RES SCH INFORM SCI E
   Brunel N, 2007, BIOL CYBERN, V97, P337, DOI 10.1007/s00422-007-0190-0
   Burkitt AN, 2006, BIOL CYBERN, V95, P1, DOI 10.1007/s00422-006-0068-6
   Chatzikonstantis G., 2017, IEEE T PARALLEL DIST
   Cheong D, 2017, APPL SOFT COMPUT, V61, P593, DOI 10.1016/j.asoc.2017.08.042
   Geng K., 2017, IEEE T NEURAL NETWOR
   Ghambari S., 2017, APPL SOFT COMPUTING
   Han B, 2018, ADV NEUR IN, V31, DOI 10.5555/3327757.3327944
   Hill AV, 1936, PROC R SOC SER B-BIO, V119, P305, DOI 10.1098/rspb.1936.0012
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V116, P497, DOI 10.1113/jphysiol.1952.sp004719
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Howard G, 2014, EVOL COMPUT, V22, P79, DOI 10.1162/EVCO_a_00103
   Islam M. J., 2017, SWARM EVOLUTIONARY C
   Izhikevich E M, 2007, DYNAM SYST, DOI [DOI 10.1017/S0143385704000173, DOI 10.7551/MITPRESS/2526.001.0001]
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Javidrad F, 2017, APPL SOFT COMPUT, V60, P634, DOI 10.1016/j.asoc.2017.07.023
   Joseph RK, 2016, CRIT POL ECON S ASIA, P1
   Kinghorn P, 2018, NEUROCOMPUTING, V272, P416, DOI 10.1016/j.neucom.2017.07.014
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
   Lee JM, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00191
   Lin J., 2017, APPL SOFT COMPUTING
   Liu C., 2017, IEEE T NEURAL NETWOR
   Liu PH, 2017, APPL SOFT COMPUT, V61, P256, DOI 10.1016/j.asoc.2017.08.022
   Luo JP, 2017, APPL SOFT COMPUT, V50, P235, DOI 10.1016/j.asoc.2016.11.014
   MANIEZZO V, 1994, IEEE T NEURAL NETWOR, V5, P39, DOI 10.1109/72.265959
   Minkovich K, 2014, IEEE T NEUR NET LEAR, V25, P316, DOI 10.1109/TNNLS.2013.2276056
   Mistry K, 2017, IEEE T CYBERNETICS, V47, P1496, DOI 10.1109/TCYB.2016.2549639
   Naveros F, 2015, IEEE T NEUR NET LEAR, V26, P1567, DOI 10.1109/TNNLS.2014.2345844
   Neoh SC, 2015, SCI REP-UK, V5, DOI 10.1038/srep14938
   Rakshit P, 2017, IEEE C EVOL COMPUTAT, P984, DOI 10.1109/CEC.2017.7969415
   Real E., 2017, ARIV170301041
   Rongala UB, 2017, IEEE T NEUR NET LEAR, V28, P819, DOI 10.1109/TNNLS.2015.2472477
   Saad A., 2017, SWARM EVOLUTIONARY C
   Sokolova M, 2009, INFORM PROCESS MANAG, V45, P427, DOI 10.1016/j.ipm.2009.03.002
   Srisukkham W, 2017, APPL SOFT COMPUT, V56, P405, DOI 10.1016/j.asoc.2017.03.024
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   Vargas DV, 2017, IEEE T NEUR NET LEAR, V28, P1759, DOI 10.1109/TNNLS.2016.2551748
   Xu X, 2017, IEEE T NEUR NET LEAR, V28, P934, DOI 10.1109/TNNLS.2015.2505084
   Ye WX, 2017, APPL SOFT COMPUT, V61, P832, DOI 10.1016/j.asoc.2017.08.051
   Yu VF, 2017, APPL SOFT COMPUT, V53, P119, DOI 10.1016/j.asoc.2016.12.027
   Zaheeruddin, 2017, SWARM EVOL COMPUT, V33, P85, DOI 10.1016/j.swevo.2016.10.004
   Zhang L, 2016, KNOWL-BASED SYST, V111, P248, DOI 10.1016/j.knosys.2016.08.018
NR 43
TC 1
Z9 1
U1 0
U2 0
PY 2018
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Lansner, A
   Holst, A
AF Lansner, A
   Holst, A
TI A higher order Bayesian neural network with spiking units
SO INTERNATIONAL JOURNAL OF NEURAL SYSTEMS
DT Article
ID VISUAL-CORTEX; CLASSIFICATION; INFORMATION; REDUCTION
AB We treat a Bayesian confidence propagation neural network, primarily in a classifier context. The one-layer version of the network implements a naive Bayesian classifier, which requires the input attributes to be independent. This limitation is overcome by a higher order network. The higher order Bayesian neural network is evaluated on a real world task of diagnosing a telephone exchange computer. By introducing stochastic spiking units, and soft interval coding, it is also possible to handle uncertain as well as continuous valued inputs.
C1 ROYAL INST TECHNOL,DEPT NUMER ANAL & COMP SCI,S-10044 STOCKHOLM,SWEDEN.
CR [Anonymous], 1992, BAYESIAN METHODS ADA
   [Anonymous], 1986, PARALLEL DISTRIB PRO
   BARLOW H, 1989, COMPUTING NEURON, pCH4
   Barlow H B, 1972, Perception, V1, P371, DOI 10.1068/p010371
   BATTITI R, 1994, IEEE T NEURAL NETWOR, V5, P537, DOI 10.1109/72.298224
   BELL AJ, 1995, INC9501 UCSD
   BENGTSSON M, 1991, ARTIFICIAL NEURAL NETWORKS, VOLS 1 AND 2, P169
   BIENENSTOCK EL, 1982, J NEUROSCI, V2, P32, DOI 10.1523/jneurosci.02-01-00032.1982
   BLUM AL, 1992, NEURAL NETWORKS, V5, P117, DOI 10.1016/S0893-6080(05)80010-3
   Broomhead D. S., 1988, Complex Systems, V2, P321
   DAUGMAN JG, 1989, IEEE T BIO-MED ENG, V36, P107, DOI 10.1109/10.16456
   DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x
   Duda R. O., 1973, PATTERN CLASSIFICATI, V3
   Fisher RA, 1936, ANN EUGENIC, V7, P179, DOI 10.1111/j.1469-1809.1936.tb02137.x
   FOLDIAK P, 1990, BIOL CYBERN, V64, P165, DOI 10.1007/BF02331346
   FUGLEVAND AJ, 1993, J NEUROPHYSIOL, V70, P2550
   GEORGOPOULOS AP, 1994, NEURAL COMPUT, V6, P19
   Ghosh J., 1993, International Journal of Neural Systems, V3, P323, DOI 10.1142/S0129065792000255
   GOODMAN RM, 1992, NEURAL COMPUT, V4, P781, DOI 10.1162/neco.1992.4.6.781
   GUSTAFSSON E, 1991, THESIS ROYAL I TECHN
   HOLST A, 1993, NEURAL NETW INNS, P147
   HOLST A, 1993, TRITANAP9325 ROYAL I
   HUBEL DH, 1977, PROC R SOC SER B-BIO, V198, P1, DOI 10.1098/rspb.1977.0085
   IVAKHNENKO AG, 1971, IEEE T SYST MAN CYB, VSMC1, P364, DOI 10.1109/TSMC.1971.4308320
   Kandel E. R., 1991, PRINCIPLES NEURAL SC
   KONONENKO I, 1989, BIOL CYBERN, V61, P361, DOI 10.1007/BF00200801
   Kononenko I., 1991, International Journal of Neural Networks - Research & Applications, V2, P43
   Lansner A., 1989, International Journal of Neural Systems, V1, P77, DOI 10.1142/S0129065789000499
   LANSNER A, 1992, NETWORK-COMP NEURAL, V3, P105, DOI 10.1088/0954-898X/3/2/002
   LANSNER A, 1991, ARTIFICIAL NEURAL NETWORKS, VOLS 1 AND 2, P247
   LANSNER A, 1987, P IEEE 1 ANN INT C N, V2, P549
   LANSNER A, 1994, IN PRESS COMPUTATION
   LEVIN B, 1995, THESIS ROYAL I TECHN
   LEVY WB, 1985, SYNAPTIC MODIFICATIO, pCH6
   Lewis II P.M., 1959, INFORM CONTROL, V2, P214, DOI DOI 10.1016/S0019-9958(59)90207-4
   McLachlan G.J., 1988, MIXTURE MODELS INFER
   MCNAUGHTON BL, 1989, NEURAL CONNECTIONS M, pCH9
   Minsky Marvin, 1969, PERCEPTRONS
   Moody J, 1989, NEURAL COMPUT, V1, P281, DOI 10.1162/neco.1989.1.2.281
   NADAL JP, 1993, NETWORK-COMP NEURAL, V4, P295, DOI 10.1088/0954-898X/4/3/004
   Namatame A., 1992, International Journal of Neural Systems, V3, P19, DOI 10.1142/S0129065792000036
   Pearl J., 1998, PROBABILISTIC REASON
   PROTZEL PW, 1991, ARTIFICIAL NEURAL NETWORKS, VOLS 1 AND 2, P273
   PSALTIS D, 1988, NEURAL NETWORKS, V1, P149, DOI 10.1016/0893-6080(88)90017-2
   PURVES D, 1992, TRENDS NEUROSCI, V15, P362, DOI 10.1016/0166-2236(92)90180-G
   REDDING NJ, 1991, ARTIFICIAL NEURAL NETWORKS, VOLS 1 AND 2, P25
   REDLICH AN, 1993, NEURAL COMPUT, V5, P289, DOI 10.1162/neco.1993.5.2.289
   Salinas E, 1994, J Comput Neurosci, V1, P89, DOI 10.1007/BF00962720
   SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P863, DOI 10.1162/neco.1992.4.6.863
   TRAVEN HGC, 1991, IEEE T NEURAL NETWOR, V2, P366, DOI 10.1109/72.97913
   UEDA N, 1994, NEURAL NETWORKS, V7, P1211, DOI 10.1016/0893-6080(94)90003-5
   UTTLEY A. M., 1962, BIOPHYS JOUR, V2, P169
   WEDELIN D, 1993, THESIS CHALMERS U TE
   WILLSHAW DJ, 1976, PROC R SOC SER B-BIO, V194, P431, DOI 10.1098/rspb.1976.0087
NR 54
TC 43
Z9 43
U1 9
U2 26
PD MAY
PY 1996
VL 7
IS 2
BP 115
EP 128
DI 10.1142/S0129065796000816
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT C
AU Wu, QX
   McGinnity, M
   Maguire, L
   Belatreche, A
   Glackin, B
AF Wu, QingXiang
   McGinnity, Martin
   Maguire, Liam
   Belatreche, Ammar
   Glackin, Brendan
BE Huang, DS
   Heutte, L
   Loog, M
TI Edge detection based on spiking neural network model
SO ADVANCED INTELLIGENT COMPUTING THEORIES AND APPLICATIONS, PROCEEDINGS:
   WITH ASPECTS OF ARTIFICIAL INTELLIGENCE
SE Lecture Notes in Artificial Intelligence
DT Proceedings Paper
CT 3rd International Conference on Intelligent Computing
CY AUG 21-24, 2007
CL Qingdao, PEOPLES R CHINA
DE edge detection; spiking neural networks; receptive field; attention;
   visual system
ID CONNECTED VISUAL AREAS; SCENE SEGMENTATION; SYNCHRONIZATION
AB Inspired by the behaviour of biological receptive fields and the human visual system, a network model based on spiking neurons is proposed to detect edges in a visual image. The structure and the properties of the network are detailed in this paper. Simulation results show that the network based on spiking neurons is able to perform edge detection within a time interval of 100 ms. This processing time is consistent with the human visual system. A firing rate map recorded in the simulation is comparable to Sobel and Canny edge graphics. In addition, the network can separate different edges using synapse plasticity, and the network provides an attention mechanism in which edges in an attention area can be enhanced.
C1 [Wu, QingXiang; McGinnity, Martin; Maguire, Liam; Belatreche, Ammar; Glackin, Brendan] Univ Ulster, Sch Comp & Intelligent Syst, Magee Campus, Derry BT48 7JL, North Ireland.
RP Wu, QX (corresponding author), Univ Ulster, Sch Comp & Intelligent Syst, Magee Campus, Derry BT48 7JL, North Ireland.
EM q.wu@ulster.ac.uk; tm.mcginnity@ulster.ac.uk; lp.maguire@ulster.ac.uk;
   a.belatreche@ulster.ac.uk; b.glackin@ulster.ac.uk
CR Abbott LF, 2004, NATURE, V431, P796, DOI 10.1038/nature03010
   [Anonymous], THESIS U HEIDELBERG
   Borisyuk RA, 2004, NEURAL NETWORKS, V17, P899, DOI 10.1016/j.neunet.2004.03.005
   Chen K, 2002, NEURAL NETWORKS, V15, P423, DOI 10.1016/S0893-6080(02)00028-X
   Choe Y, 2004, BIOL CYBERN, V90, P75, DOI 10.1007/s00422-003-0435-5
   Dayan P., 2001, THEORETICAL NEUROSCI
   Gerstner W., 2002, SPIKING NEURON MODEL
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Hosoya T, 2005, NATURE, V436, P71, DOI 10.1038/nature03689
   Jessell T. M, 1981, PRINCIPLES NEURAL SC
   Knoblauch A, 2002, BIOL CYBERN, V87, P168, DOI 10.1007/s00422-002-0332-3
   Knoblauch A, 2002, BIOL CYBERN, V87, P151, DOI 10.1007/s00422-002-0331-4
   Koch Christof, 1999, P1
   Purushothaman G, 1998, NATURE, V396, P424, DOI 10.1038/24766
   Wu QX, 2007, STUD COMPUT INTELL, V35, P171
   Wu QX, 2005, LECT NOTES COMPUT SC, V3610, P420
NR 16
TC 33
Z9 48
U1 0
U2 7
PY 2007
VL 4682
BP 26
EP 34
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods; Operations Research & Management Science
DA 2023-11-11
ER

PT C
AU Ventresca, M
   Ombuki, B
AF Ventresca, Mario
   Ombuki, Beatrice
GP IEEE
TI Search space analysis of recurrent spiking and continuous-time neural
   networks
SO 2006 IEEE INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORK PROCEEDINGS,
   VOLS 1-10
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT IEEE International Joint Conference on Neural Network
CY JUL 16-21, 2006
CL Vancouver, CANADA
AB The problem of designing recurrent continuous-time and spiking neural networks is NP-Hard. A common practice is to utilize stochastic searches, such as evolutionary algorithms, to automatically construct acceptable networks. The outcome of the stochastic search is related to its ability to navigate the search space of neural networks and discover those of high quality. In this paper we investigate the search space associated with designing the above recurrent neural networks in order to differentiate which network should be easier to automatically design via a stochastic search. Our investigation utilizes two popular dynamic systems problems; (1) the Henon map and (2) the inverted pendulum as a benchmark.
C1 [Ventresca, Mario] Univ Guelph, Dept Comp & Informat Sci, Guelph, ON N1G 2W1, Canada.
   [Ventresca, Mario] Univ Waterloo, Syst Design Engn, Pattern Anal & Machine Intelligence Lab, Waterloo, ON N2L 3G1, Canada.
   [Ombuki, Beatrice] Brock Univ, Dept Comp Sci, St Catharines, ON L2S 3A1, Canada.
RP Ventresca, M (corresponding author), Univ Guelph, Dept Comp & Informat Sci, Guelph, ON N1G 2W1, Canada.
EM mventres@pami.uwaterloo.ca; bombuki@brocku.ca
CR Alander JT, 2002, 2002 IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE SYSTEMS, PROCEEDINGS, P363, DOI 10.1109/ICAIS.2002.1048128
   Beer Randall D., 1992, Adaptive Behavior, V1, P91, DOI 10.1177/105971239200100105
   BEER RD, 1995, ADAPT BEHAV, V3, P469, DOI 10.1177/105971239500300405
   Diacu F., 2000, INTRO DIFFERENTIAL E
   FUNAHASHI K, 1993, NEURAL NETWORKS, V6, P801, DOI 10.1016/S0893-6080(05)80125-X
   Gerstner W., 2002, SPIKING NEURON MODEL
   HENON M, 1976, COMMUN MATH PHYS, V50, P69, DOI 10.1007/BF01608556
   Jones, 1995, THESIS U NEW MEXICO
   MAAS W, 2001, HDB BRAIN THEORY NEU, P1080
   MAAS W, 1999, PULSED NEURAL NETWOR
   MAAS W, 1997, T SOC COMPUT SIMUL, P1659
   Maass W, 1996, NEURAL COMPUT, V8, P1, DOI 10.1162/neco.1996.8.1.1
   MILLER WT, 1990, CHALLENGING CONTROL
   PASEMANN F, 1997, P 7 INT C ART NEUR N, P823
   Schiavinotto T, 2003, LECT NOTES COMPUT SC, V2611, P322
   Smith T, 2001, IEEE C EVOL COMPUTAT, P9, DOI 10.1109/CEC.2001.934364
   Stadler PF, 1995, LECT NOTES PHYS, V461, P78
   TEO J, 2002, P 6 AUSTR JAP JOINT, P54
   Vassilev VK, 2000, EVOL COMPUT, V8, P31, DOI 10.1162/106365600568095
   VASSILEV VK, 2003, NAT COMP SER, P3
   WEINBERGER E, 1990, BIOL CYBERN, V63, P325, DOI 10.1007/BF00202749
   WRIGHT SEWALL, 1932, PROC SIXTH INTERNAT CONGR GENETICS ITHACA NEW YORK, V1, P356
NR 22
TC 1
Z9 1
U1 0
U2 0
PY 2006
BP 4514
EP +
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT J
AU Li, XM
   Yi, H
   Luo, SY
AF Li, Xiumin
   Yi, Hao
   Luo, Shengyuan
TI Pattern Recognition of Spiking Neural Networks Based on Visual Mechanism
   and Supervised Synaptic Learning
SO NEURAL PLASTICITY
DT Article
ID MODEL
AB Electrophysiological studies have shown that mammalian primary visual cortex are selective for the orientations of visual stimuli. Inspired by this mechanism, we propose a hierarchical spiking neural network (SNN) for image classification. Grayscale input images are fed through a feed-forward network consisting of orientation-selective neurons, which then projected to a layer of downstream classifier neurons through the spiking-based supervised tempotron learning rule. Based on the orientation-selective mechanism of the visual cortex and tempotron learning rule, the network can effectively classify images of the extensively studied MNIST database of handwritten digits, which achieves 96% classification accuracy based on only 2000 training samples (traditional training set is 60000). Compared with other classification methods, our model not only guarantees the biological plausibility and the accuracy of image classification but also significantly reduces the needed training samples. Considering the fact that the most commonly used deep learning neural networks need big data samples and high power consumption in image recognition, this brain-inspired computational neural network model based on the layer-by-layer hierarchical image processing mechanism of the visual cortex may provide a basis for the wide application of spiking neural networks in the field of intelligent computing.
C1 [Li, Xiumin; Yi, Hao; Luo, Shengyuan] Chongqing Univ, Coll Automat, Chongqing 400044, Peoples R China.
RP Li, XM (corresponding author), Chongqing Univ, Coll Automat, Chongqing 400044, Peoples R China.
EM free_xmin@hotmail.com; 201713021035@cqu.edu.cn; 20161302032@cqu.edu.cn
CR Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596
   [Anonymous], 2011, FRONT NEUROINFORM
   Beyeler M, 2014, NEUROINFORMATICS, V12, P435, DOI 10.1007/s12021-014-9220-y
   Beyeler M, 2013, NEURAL NETWORKS, V48, P109, DOI 10.1016/j.neunet.2013.07.012
   Ciresan DC, 2011, FLEXIBLE HIGH PERFOR
   Diehl Peter U, 2015, 2015 INT JOINT C NEU, P1, DOI [DOI 10.1109/IJCNN.2015.7280696, 10.1109/IJCNN.2015.7280696]
   Gardner MW, 1998, ATMOS ENVIRON, V32, P2627, DOI 10.1016/S1352-2310(97)00447-0
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Han Bing, 2013, Journal of Software, V24, P2758, DOI 10.3724/SP.J.1001.2013.04481
   HUBEL DH, 1959, J PHYSIOL-LONDON, V148, P574, DOI 10.1113/jphysiol.1959.sp006308
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Jarrett K, 2009, IEEE I CONF COMP VIS, P2146, DOI 10.1109/ICCV.2009.5459469
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Kim SY, 2018, COGN NEURODYNAMICS, V12, P315, DOI 10.1007/s11571-017-9470-0
   Kulkarni SR, 2018, NEURAL NETWORKS, V103, P118, DOI 10.1016/j.neunet.2018.03.019
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Li XM, 2020, COGN NEURODYNAMICS, V14, P347, DOI 10.1007/s11571-020-09572-y
   Liao YH, 2002, COMPUT SECUR, V21, P439, DOI 10.1016/S0167-4048(02)00514-X
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   Moehlis J., 2008, SIAM REV, V50, P397
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   Panzeri S, 2010, TRENDS NEUROSCI, V33, P111, DOI 10.1016/j.tins.2009.12.001
   Rolls E.T., 2007, COMPUTATIONAL NEUROS
   Simoncelli EP, 1998, VISION RES, V38, P743, DOI 10.1016/S0042-6989(97)00183-1
   Suykens JAK, 1999, NEURAL PROCESS LETT, V9, P293, DOI 10.1023/A:1018628609742
   van Gemert Jan C., 2008, Computer Vision. Proceedings 10th European Conference on Computer Vision, ECCV 2008, P696, DOI 10.1007/978-3-540-88690-7_52
   Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517
   Yoon S., 2019, SPIKING YOLO SPIKING
   Yu Q., 2017, NEUROMORPHIC COGNITI
   Zeng Y, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P1154, DOI 10.1109/ICASSP.2018.8462502
   Zhang T, 2019, COGN NEURODYNAMICS, V13, P579, DOI 10.1007/s11571-019-09540-1
NR 32
TC 4
Z9 4
U1 1
U2 6
PD OCT 28
PY 2020
VL 2020
AR 8851351
DI 10.1155/2020/8851351
WC Neurosciences
DA 2023-11-11
ER

PT C
AU Nandakumar, SR
   Boybat, I
   Le Gallo, M
   Sebastian, A
   Rajendran, B
   Eleftheriou, E
AF Nandakumar, S. R.
   Boybat, I.
   Le Gallo, M.
   Sebastian, A.
   Rajendran, B.
   Eleftheriou, E.
GP IEEE
TI Supervised Learning in Spiking Neural Networks with MLC PCM Synapses
SO 2017 75TH ANNUAL DEVICE RESEARCH CONFERENCE (DRC)
SE IEEE Device Research Conference Proceedings
DT Proceedings Paper
CT 75th Annual Device Research Conference (DRC)
CY JUN 25-28, 2017
CL Univ Notre Dame, South Bend, IN
HO Univ Notre Dame
C1 [Nandakumar, S. R.; Rajendran, B.] New Jersey Inst Technol, Dept Elect & Comp Engn, Newark, NJ 07102 USA.
   [Boybat, I.; Le Gallo, M.; Sebastian, A.; Eleftheriou, E.] IBM Res Zurich, CH-8803 Ruschlikon, Switzerland.
RP Sebastian, A (corresponding author), IBM Res Zurich, CH-8803 Ruschlikon, Switzerland.
EM ase@zurich.ibm.com; bipin@njit.edu
CR Anwani N., 2015, IJCNN
   Brader J. M., 2007, NEURAL COMPUTATION, V19
   Close G., 2010, IEDM
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Suri M., 2011, IEDM
   Tuma T., 2016, NATURE NANO, V11
NR 6
TC 6
Z9 6
U1 1
U2 3
PY 2017
WC Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Yousefi, A
   Dibazar, AA
   Berger, TW
AF Yousefi, Ali
   Dibazar, Alireza A.
   Berger, Theodore W.
TI Synaptic dynamics: Linear model and adaptation algorithm
SO NEURAL NETWORKS
DT Article
DE Spiking neural networks; Plasticity; Learning; Bio-inspired models
ID TIMING-DEPENDENT PLASTICITY; SPIKING NEURAL-NETWORKS; RELIABILITY;
   SYNAPSES; STDP
AB In this research, temporal processing in brain neural circuitries is addressed by a dynamic model of synaptic connections in which the synapse model accounts for both pre- and post-synaptic processes determining its temporal dynamics and strength. Neurons, which are excited by the post-synaptic potentials of hundred of the synapses, build the computational engine capable of processing dynamic neural stimuli. Temporal dynamics in neural models with dynamic synapses will be analyzed, and learning algorithms for synaptic adaptation of neural networks with hundreds of synaptic connections are proposed.
   The paper starts by introducing a linear approximate model for the temporal dynamics of synaptic transmission. The proposed linear model substantially simplifies the analysis and training of spiking neural networks. Furthermore, it is capable of replicating the synaptic response of the non-linear facilitation depression model with an accuracy better than 92.5%. In the second part of the paper, a supervised spike-in-spike-out learning rule for synaptic adaptation in dynamic synapse neural networks (DSNN) is proposed. The proposed learning rule is a biologically plausible process, and it is capable of simultaneously adjusting both pre- and post-synaptic components of individual synapses. The last section of the paper starts with presenting the rigorous analysis of the learning algorithm in a system identification task with hundreds of synaptic connections which confirms the learning algorithm's accuracy, repeatability and scalability. The DSNN is utilized to predict the spiking activity of cortical neurons and pattern recognition tasks. The DSNN model is demonstrated Lobe a generative model capable of producing different cortical neuron spiking patterns and CA1 Pyramidal neurons recordings. A single-layer DSNN classifier on a benchmark pattern recognition task outperforms a 2-Layer Neural Network and GMM classifiers while having fewer numbers of free parameters and decides with a shorter observation of data. DSNN performance in the benchmark pattern recognition problem shows 96.7% accuracy in classifying three classes of spiking activity. (C) 2014 Elsevier Ltd. All rights reserved.
C1 [Yousefi, Ali] Univ So Calif, Dept Elect Engn, Los Angeles, CA 90089 USA.
   [Dibazar, Alireza A.; Berger, Theodore W.] Univ So Calif, Dept Biomed Engn, Los Angeles, CA 90089 USA.
RP Dibazar, AA (corresponding author), 1042 Downey Way,DRB 140, Los Angeles, CA 90089 USA.
EM ayousefi@usc.edu; dibazar@usc.edu; berger@usc.edu
CR Abbott LF, 1997, SCIENCE, V275, P220, DOI 10.1126/science.275.5297.221
   Bartlett P.L., 2011, 11060665 ARXIV
   Belatreche A, 2007, SOFT COMPUT, V11, P239, DOI [10.1007/s00500-006-0065-7, 10.1007/S00500-006-0065-7]
   Berry MJ, 1997, P NATL ACAD SCI USA, V94, P5411, DOI 10.1073/pnas.94.10.5411
   Bohte S. M., 2004, P ADV NEUR INF PROC, P270
   Bohte S.M., 2003, THESIS CTR MATH COMP
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Burkitt AN, 2004, NEURAL COMPUT, V16, P885, DOI 10.1162/089976604773135041
   Bush D, 2010, NEURAL COMPUT, V22, P2059, DOI 10.1162/NECO_a_00003-Bush
   Davis B. A., 2003, P INT JOINT C NEUR N, V4
   Deco G, 1999, NEURAL COMPUT, V11, P919, DOI 10.1162/089976699300016502
   Dittman JS, 2000, J NEUROSCI, V20, P1374
   Fang HJ, 2010, NEURAL COMPUT, V22, P1060, DOI 10.1162/neco.2009.10-08-885
   Fiete I.R., 2003, THESIS HARVARD U CAM
   Fiete IR, 2006, PHYS REV LETT, V97, DOI 10.1103/PhysRevLett.97.048104
   Gerstner W., 2006, SPIKING NEURON MODEL
   Ghaderi V. S., 2012, ANN INT C IEEE ENG M
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   Gupta A., 2009, INT JOINT C NEUR NET
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Hatsopoulos N, 2003, NEUROCOMPUTING, V52-4, P25, DOI 10.1016/S0925-2312(02)00773-7
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Izhikevich EM, 2003, NEURAL COMPUT, V15, P1511, DOI 10.1162/089976603321891783
   Jin YC, 2007, LECT NOTES COMPUT SC, V4668, P370
   Kreuz T, 2007, J NEUROSCI METH, V165, P151, DOI 10.1016/j.jneumeth.2007.05.031
   Lee K, 2008, NEUROCOMPUTING, V71, P3037, DOI 10.1016/j.neucom.2007.09.009
   Legenstein R, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.1000180
   Liaw JS, 1999, NEUROCOMPUTING, V26-7, P199, DOI 10.1016/S0925-2312(99)00063-6
   Liaw JS, 1996, HIPPOCAMPUS, V6, P591
   Maass W, 1999, NEURAL COMPUT, V11, P903, DOI 10.1162/089976699300016494
   MAINEN ZF, 1995, SCIENCE, V268, P1503, DOI 10.1126/science.7770778
   Masquelier T, 2008, PLOS ONE, V3, DOI 10.1371/journal.pone.0001377
   Mongillo G, 2008, SCIENCE, V319, P1543, DOI 10.1126/science.1150769
   Morrison A, 2008, BIOL CYBERN, V98, P459, DOI 10.1007/s00422-008-0233-1
   Pfister JP, 2006, NEURAL COMPUT, V18, P1318, DOI 10.1162/neco.2006.18.6.1318
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Rostro H., 2009, 09054810 ARXIV
   Senn W, 2001, NEURAL COMPUT, V13, P35, DOI 10.1162/089976601300014628
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Storck J, 2001, NEURAL NETWORKS, V14, P275, DOI 10.1016/S0893-6080(00)00101-5
   Strain T. J., 2006, INT JOINT C NEUR NET
   Tiesinga PHE, 2002, NEURAL COMPUT, V14, P1629, DOI 10.1162/08997660260028647
   Tsodyks M, 1998, NEURAL COMPUT, V10, P821, DOI 10.1162/089976698300017502
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696
   Xie XH, 2004, PHYS REV E, V69, DOI 10.1103/PhysRevE.69.041909
   Yousefi A., 2011, 2011 INT JOINT C NEU
   Zucker RS, 2002, ANNU REV PHYSIOL, V64, P355, DOI 10.1146/annurev.physiol.64.092501.114547
   [No title captured]
NR 48
TC 1
Z9 1
U1 0
U2 25
PD AUG
PY 2014
VL 56
BP 49
EP 68
DI 10.1016/j.neunet.2014.04.001
WC Computer Science, Artificial Intelligence; Neurosciences
DA 2023-11-11
ER

PT C
AU Kampakis, S
AF Kampakis, Stylianos
BE Merelo, JJ
   Rosa, A
   Cadenas, JM
   Dourado, A
   Madani, K
   Filipe, J
TI Neurons with Non-standard Behaviors Can Be Computationally Relevant
SO COMPUTATIONAL INTELLIGENCE, IJCCI 2014
SE Studies in Computational Intelligence
DT Proceedings Paper
CT 6th International Joint Conference on Computational Intelligence (IJCCI)
CY OCT 22-24, 2014
CL Rome, ITALY
ID SPIKING NEURONS; OPTIMIZATION; DIVERSITY; INTERNEURONS; ALGORITHM;
   NETWORKS; MODELS
AB Neurons can exhibit many different kinds of behaviors, such as bursting, oscillating or rebound spiking. However, research in spiking neural networks has largely focused on the neuron type known as "integrator". Recent researches have suggested that using neural networks equipped with neurons other than the integrator, might carry computational advantages. However, there still lacks an experimental validation of this idea. This study used a spiking neural network with a biologically realistic neuron model in order to provide experimental evidence on this hypothesis. The study contains two experiments. In the first experiment the optimization of the network is conducted by setting the weights to random values and then adjusting the parameters of the neurons in order to adapt the neural behaviors. In the second experiment, the parameter optimization is used in order to improve the network's performance after the weights have been trained. The results illustrate that neurons with non-standard behaviors can provide computational advantages for a network. Further implications of this study and suggestions for future research are discussed.
C1 [Kampakis, Stylianos] UCL, Dept Comp Sci, London, England.
RP Kampakis, S (corresponding author), UCL, Dept Comp Sci, London, England.
EM stylianos.kampakis@gmail.com
CR Achard P, 2006, PLOS COMPUT BIOL, V2, P794, DOI 10.1371/journal.pcbi.0020094
   Bohte S. M., 2001, IEEE T NEURAL NETW, VXX
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Buzsáki G, 2004, TRENDS NEUROSCI, V27, P186, DOI 10.1016/j.tins.2004.02.007
   Cohen S, 2002, PATTERN ANAL APPL, V5, P113, DOI 10.1007/s100440200010
   Fisher RA, 1936, ANN EUGENIC, V7, P179, DOI 10.1111/j.1469-1809.1936.tb02137.x
   Ghosh-Dastidar S., 2009, NEURAL NETW, V22
   Iannella N, 2001, NEURAL NETWORKS, V14, P933, DOI 10.1016/S0893-6080(01)00080-6
   Izhikevich E., 2003, IEEE T NEURAL NETW, V14
   Izhikevich E M, 2007, DYNAM SYST, DOI [DOI 10.1017/S0143385704000173, DOI 10.7551/MITPRESS/2526.001.0001]
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Kampakis S., 2013, J SOFT COMP IN PRESS
   Kampakis S., 2011, J SOFT COMPUT
   Kampakis S, 2013, NEURAL NETWORKS, V43, P41, DOI 10.1016/j.neunet.2013.01.011
   Keren N., 2006, J NEUROPHYSIOL, P3730
   Klausberger T, 2008, SCIENCE, V321, P53, DOI 10.1126/science.1149381
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maul T., 2013, EARLY EXPT NEU UNPUB
   Meftah B, 2010, NEURAL PROCESS LETT, V32, P131, DOI 10.1007/s11063-010-9149-6
   Moore CI, 2010, CELL, V142, P184, DOI 10.1016/j.cell.2010.07.005
   Potjans W, 2009, NEURAL COMPUT, V21, P301, DOI 10.1162/neco.2008.08-07-593
   Rawlins G. E., 1991, FDN GENETIC ALGORITH
   Taylor AM, 2004, J COMPUT NEUROSCI, V16, P139, DOI 10.1023/B:JCNS.0000014107.16610.2e
   Tutkun N, 2009, EXPERT SYST APPL, V36, P3342, DOI 10.1016/j.eswa.2008.01.060
   Valko M., 2005, EVOLUTIONARY FEATURE, P181
   Van Geit W, 2008, BIOL CYBERN, V99, P241, DOI 10.1007/s00422-008-0257-6
   Wade JJ, 2008, IEEE IJCNN, P2648, DOI 10.1109/IJCNN.2008.4634169
   Wang H., 2009, IMPROVEMENT IZHIKEVI
   Wu CH, 2009, EXPERT SYST APPL, V36, P4725, DOI 10.1016/j.eswa.2008.06.046
NR 29
TC 0
Z9 0
U1 0
U2 1
PY 2016
VL 620
BP 337
EP 349
DI 10.1007/978-3-319-26393-9_20
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT J
AU Bagley, BA
   Bordelon, B
   Moseley, B
   Wessel, R
AF Bagley, Bryce Allen
   Bordelon, Blake
   Moseley, Benjamin
   Wessel, Ralf
TI Pre-Synaptic Pool Modification (PSPM): A supervised learning procedure
   for recurrent spiking neural networks
SO PLOS ONE
DT Article
ID NEURONS; BACKPROPAGATION
AB Learning synaptic weights of spiking neural network (SNN) models that can reproduce target spike trains from provided neural firing data is a central problem in computational neuroscience and spike-based computing. The discovery of the optimal weight values can be posed as a supervised learning task wherein the weights of the model network are chosen to maximize the similarity between the target spike trains and the model outputs. It is still largely unknown whether optimizing spike train similarity of highly recurrent SNNs produces weight matrices similar to those of the ground truth model. To this end, we propose flexible heuristic supervised learning rules, termed Pre-Synaptic Pool Modification (PSPM), that rely on stochastic weight updates in order to produce spikes within a short window of the desired times and eliminate spikes outside of this window. PSPM improves spike train similarity for all-to-all SNNs and makes no assumption about the post-synaptic potential of the neurons or the structure of the network since no gradients are required. We test whether optimizing for spike train similarity entails the discovery of accurate weights and explore the relative contributions of local and homeostatic weight updates. Although PSPM improves similarity between spike trains, the learned weights often differ from the weights of the ground truth model, implying that connectome inference from spike data may require additional constraints on connectivity statistics. We also find that spike train similarity is sensitive to local updates, but other measures of network activity such as avalanche distributions, can be learned through synaptic homeostasis.
C1 [Bagley, Bryce Allen; Bordelon, Blake] Washington Univ, Dept Elect & Syst Engn, St Louis, MO 63110 USA.
   [Bagley, Bryce Allen; Bordelon, Blake; Wessel, Ralf] Washington Univ, Dept Phys, St Louis, MO 63130 USA.
   [Bagley, Bryce Allen; Moseley, Benjamin] Washington Univ, Dept Comp Sci, St Louis, MO 63130 USA.
   [Bagley, Bryce Allen] Stanford Univ, Stanford Inst Theoret Phys, Stanford, CA 94305 USA.
   [Moseley, Benjamin] Carnegie Mellon Univ, Dept Operat Res, Pittsburgh, PA 15213 USA.
RP Bagley, BA (corresponding author), Washington Univ, Dept Elect & Syst Engn, St Louis, MO 63110 USA.; Bagley, BA (corresponding author), Washington Univ, Dept Phys, St Louis, MO 63130 USA.; Bagley, BA (corresponding author), Washington Univ, Dept Comp Sci, St Louis, MO 63130 USA.; Bagley, BA (corresponding author), Stanford Univ, Stanford Inst Theoret Phys, Stanford, CA 94305 USA.
EM bbagley@stanford.edu
CR Abeles M., CORTICONICS NEURAL C
   Asai Y, 2012, BRAIN RES, V1434, P17, DOI 10.1016/j.brainres.2011.10.012
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Booij O, 2005, INFORM PROCESS LETT, V95, P552, DOI 10.1016/j.ipl.2005.05.023
   Bryant HL, J PHYSL
   Butts DA, 2007, NATURE, V449, P92, DOI [10.1038/nature06105, 10.1038/natureO6105]
   Cabessa J., PLOS ONE
   Cabessa J., CHAOS INTERDISCIPLIN
   Caudill MS, 2009, PHYS REV E, V80, DOI 10.1103/PhysRevE.80.051923
   Davies M., 2017, ARXIV170505475
   Dayhoff J. E., APPL J NEUROPHYSIOLO
   de Abril IM, 2018, NEURAL NETWORKS, V102, P120, DOI 10.1016/j.neunet.2018.02.016
   Delorme A, 2003, NETWORK-COMP NEURAL, V14, P613, DOI 10.1088/0954-898X/14/4/301
   Florian RV, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0040233
   Friston KJ, 2011, BRAIN CONNECT, V1, P13, DOI 10.1089/brain.2011.0008
   Gardner B, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0161335
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   Gupta A, 2007, IEEE IJCNN, P53, DOI 10.1109/IJCNN.2007.4370930
   Herikstad R, 2011, J NEUROSCI, V31, P15844, DOI 10.1523/JNEUROSCI.5153-10.2011
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Jin YC, 2007, LECT NOTES COMPUT SC, V4668, P370
   Jolivet R, 2003, LECT NOTES COMPUT SC, V2714, P846
   Karimipanah Y, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0182501
   Karimipanah Y, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0177396
   Keck T, 2017, PHILOS T R SOC B, V372, DOI 10.1098/rstb.2016.0158
   Kobayashi R., FRONTIERS COMPUTATIO
   Larremore DB, 2012, PHYS REV E, V85, DOI 10.1103/PhysRevE.85.066131
   Liaw JS, 1998, IEEE WORLD CONGRESS ON COMPUTATIONAL INTELLIGENCE, P2175, DOI 10.1109/IJCNN.1998.687197
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   MAINEN ZF, 1995, SCIENCE, V268, P1503, DOI 10.1126/science.7770778
   Masquelier T, 2007, PLOS COMPUT BIOL, V3, P247, DOI 10.1371/journal.pcbi.0030031
   MEAD C, 1990, P IEEE, V78, P1629, DOI 10.1109/5.58356
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Mohemmed A, 2011, LECT NOTES COMPUT SC, V7063, P718, DOI 10.1007/978-3-642-24958-7_83
   Nahmias MA, 2013, IEEE J SEL TOP QUANT, V19, DOI 10.1109/JSTQE.2013.2257700
   Pavlidis NG, 2005, IEEE IJCNN, P2190
   Pehlevan Cengiz, 2019, ARXIV190201429
   Pfister JP, 2006, NEURAL COMPUT, V18, P1318, DOI 10.1162/neco.2006.18.6.1318
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Russell A, 2010, IEEE T NEURAL NETWOR, V21, P1950, DOI 10.1109/TNN.2010.2083685
   Schrauwen B, 2004, IEEE IJCNN, P471, DOI 10.1109/IJCNN.2004.1379954
   Schrauwen B, 2006, IEEE IJCNN, P1797
   Segundo J. P., J EXPT BIOL
   Shreiber S, 2003, NEUROCOMPUTING, V52
   Sporns O, 2005, PLOS COMPUT BIOL, V1, P245, DOI 10.1371/journal.pcbi.0010042
   Tavanaei A., 2018, CORR
   Turrigiano GG, 2017, PHILOS T R SOC B, V372, DOI 10.1098/rstb.2016.0258
   van Rossum MCW, 2001, NEURAL COMPUT, V13, P751, DOI 10.1162/089976601300014321
   Villa A. E., NEUROREPORT INT J RA
   Wu Q, 2007, ICIC
   Wysoski SG, 2010, NEURAL NETWORKS, V23, P819, DOI 10.1016/j.neunet.2010.04.009
NR 52
TC 0
Z9 0
U1 1
U2 5
PD FEB 24
PY 2020
VL 15
IS 2
AR e0229083
DI 10.1371/journal.pone.0229083
WC Multidisciplinary Sciences
DA 2023-11-11
ER

PT J
AU Ge, CJ
   Kasabov, N
   Liu, Z
   Yang, J
AF Ge, Chenjie
   Kasabov, Nikola
   Liu, Zhi
   Yang, Jie
TI A spiking neural network model for obstacle avoidance in simulated
   prosthetic vision
SO INFORMATION SCIENCES
DT Article
DE Spiking neural networks; NeuCube; Visual prothesis; Obstacle avoidance;
   Simulated prosthetic vision
ID SPATIOTEMPORAL DATA; ARTIFICIAL VISION; EEG DATA; STIMULATION;
   CLASSIFICATION; RECOGNITION; METHODOLOGY; NEUCUBE; BRAIN; BLIND
AB Limited by visual percepts elicited by existing visual prosthesis, it's necessary to enhance its functionality to fulfill some challenging tasks for the blind such as obstacle avoidance. This paper argues that spiking neural networks (SNN) are effective techniques for object recognition and introduces for the first time a SNN model for obstacle recognition to assist blind people wearing prosthetic vision devices by modelling and classifying spatiotemporal (ST) video data. The proposed methodology is based on a novel spiking neural network architecture, called NeuCube as a general framework for video data modelling in simulated prosthetic vision. As an integrated environment including spiking trains encoding, input variable mapping, unsupervised reservoir training and supervised classifier training, the NeuCube consists of a spiking neural network reservoir (SNNr) and a dynamic evolving spiking neural network classifier (deSNN). First, input data is captured by visual prosthesis, then ST feature extraction is utilized in the low-resolution prosthetic vision generated by prostheses. Finally such ST features are fed to the NeuCube to output classification result of obstacle analysis for an early warning system to be activated. Experiments on collected video data and comparison with other computational intelligence methods indicate promising results. This makes it possible to directly utilize available neuromorphic hardware chips, embedded in visual prostheses, to enhance significantly their functionality. The proposed NeuCube-based obstacle avoidance methodology provides useful guidance to the blind, thus offering a significant improvement of current prostheses and potentially benefiting future prosthesis wearers. (C) 2017 Elsevier Inc. All rights reserved.
C1 [Ge, Chenjie; Yang, Jie] Shanghai Jiao Tong Univ, Inst Image Proc & Pattern Recognit, Shanghai, Peoples R China.
   [Kasabov, Nikola] Auckland Univ Technol, Knowledge Engn & Discovery Res Inst, Auckland 1010, New Zealand.
   [Liu, Zhi] Shanghai Univ, Sch Commun & Informat Engn, Shanghai, Peoples R China.
RP Yang, J (corresponding author), Shanghai Jiao Tong Univ, Inst Image Proc & Pattern Recognit, Shanghai, Peoples R China.
EM jieyang@sjtu.edu.cn
CR Aly R., 2013, AXES SUBMISSIONS TRE
   [Anonymous], P R SOC LOND B
   [Anonymous], 2007, RETRIEVED
   BORENSTEIN J, 1991, IEEE T ROBOTIC AUTOM, V7, P278, DOI 10.1109/70.88137
   Boureau Y L, 2010, P 27 INT C MACH LEAR, P111
   Capecci E, 2015, NEURAL NETWORKS, V68, P62, DOI 10.1016/j.neunet.2015.03.009
   CHA K, 1992, VISION RES, V32, P1367, DOI 10.1016/0042-6989(92)90229-C
   CHA K, 1992, J OPT SOC AM A, V9, P673, DOI 10.1364/JOSAA.9.000673
   CHA KH, 1992, ANN BIOMED ENG, V20, P439, DOI 10.1007/BF02368135
   Chai XY, 2007, ARTIF ORGANS, V31, P175, DOI 10.1111/j.1525-1594.2007.00362.x
   da Cruz L., 2013, BRIT J OPHTHALMOL
   Dakopoulos D, 2010, IEEE T SYST MAN CY C, V40, P25, DOI 10.1109/TSMCC.2009.2021255
   Dobelle WH, 2000, ASAIO J, V46, P3, DOI 10.1097/00002480-200001000-00002
   Doborjeh MG, 2016, IEEE T BIO-MED ENG, V63, P1830, DOI 10.1109/TBME.2015.2503400
   Ess A, 2009, IEEE INT CONF ROBOT, P4451
   Fornos AP, 2005, INVEST OPHTH VIS SCI, V46, P3906, DOI 10.1167/iovs.04-1173
   Fujikado T, 2011, INVEST OPHTH VIS SCI, V52, P4726, DOI 10.1167/iovs.10-6836
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   Fusi S, 2000, NEURAL COMPUT, V12, P2227, DOI 10.1162/089976600300014917
   Gerstner W, 2001, MATH MODELL, V13, P23
   Hall Mark, 2009, SIGKDD EXPLORATIONS, V11, P10, DOI DOI 10.1145/1656274.1656278
   Han TT, 2015, 2015 INTERNATIONAL SYMPOSIUM ON BIOELECTRONICS AND BIOINFORMATICS (ISBB), P172, DOI 10.1109/ISBB.2015.7344951
   Huang WH, 2006, ROBOT AUTON SYST, V54, P288, DOI 10.1016/j.robot.2005.11.004
   Humayun MS, 2012, OPHTHALMOLOGY, V119, P779, DOI 10.1016/j.ophtha.2011.09.028
   Humayun MS, 2003, VISION RES, V43, P2573, DOI 10.1016/S0042-6989(03)00457-7
   Indiveri G, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00073
   Kasabov N., 2015, NEURAL NETW
   Kasabov N, 2015, INFORM SCIENCES, V294, P565, DOI 10.1016/j.ins.2014.06.028
   Kasabov N, 2014, NEUROCOMPUTING, V134, P269, DOI 10.1016/j.neucom.2013.09.049
   Kasabov N, 2013, NEURAL NETWORKS, V41, P188, DOI 10.1016/j.neunet.2012.11.014
   Kasabov NK, 2014, NEURAL NETWORKS, V52, P62, DOI 10.1016/j.neunet.2014.01.006
   Kurtenbach A, 2013, DOC OPHTHALMOL, V126, P45, DOI 10.1007/s10633-012-9359-5
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Parikh N, 2013, J NEURAL ENG, V10, DOI 10.1088/1741-2560/10/2/026017
   Rizzo JF, 2003, INVEST OPHTH VIS SCI, V44, P5362, DOI 10.1167/iovs.02-0817
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Thorpe S, 1998, COMPUTATIONAL NEUROSCIENCE: TRENDS IN RESEARCH, P113
   Tu EM, 2017, IEEE T NEUR NET LEAR, V28, P1305, DOI 10.1109/TNNLS.2016.2536742
   Tu EM, 2014, IEEE IJCNN, P638, DOI 10.1109/IJCNN.2014.6889717
   Veraart C, 1998, BRAIN RES, V813, P181, DOI 10.1016/S0006-8993(98)00977-9
   Wang J., 2015, ARTIF ORGANS
   Wang J, 2014, J NEURAL ENG, V11, DOI 10.1088/1741-2560/11/4/046009
   Wang J, 2014, INFORM SCIENCES, V277, P512, DOI 10.1016/j.ins.2014.02.136
   Weiland JD, 2011, OPHTHALMOLOGY, V118, P2227, DOI 10.1016/j.ophtha.2011.08.042
   Xu ZW, 2015, PROC CVPR IEEE, P1798, DOI 10.1109/CVPR.2015.7298789
   Zhao Y, 2011, INVEST OPHTH VIS SCI, V52, P3404, DOI 10.1167/iovs.09-4234
   Zhou DD, 2013, IEEE INT CONF MULTI, DOI 10.1109/ICMEW.2013.6618428
   Zisserman A., 2014, 14091556 ARXIV
NR 48
TC 16
Z9 19
U1 1
U2 70
PD AUG
PY 2017
VL 399
BP 30
EP 42
DI 10.1016/j.ins.2017.03.006
WC Computer Science, Information Systems
DA 2023-11-11
ER

PT J
AU Zhou, SB
   Chen, Y
   Li, XH
   Sanyal, A
AF Zhou, Shibo
   Chen, Ying
   Li, Xiaohua
   Sanyal, Arindam
TI Deep SCNN-Based Real-Time Object Detection for Self-Driving Vehicles
   Using LiDAR Temporal Data
SO IEEE ACCESS
DT Article
DE Spiking convolutional neural network; LiDAR temporal data; energy
   consumption; real-time object detection
ID NEURAL-NETWORKS
AB Real-time accurate detection of three-dimensional (3D) objects is a fundamental necessity for self-driving vehicles. Most existing computer vision approaches are based on convolutional neural networks (CNNs). Although the CNN-based approaches can achieve high detection accuracy, their high energy consumption is a severe drawback. To resolve this problem, novel energy efficient approaches should be explored. Spiking neural network (SNN) is a promising candidate because it has orders-of-magnitude lower energy consumption than CNN. Unfortunately, the studying of SNN has been limited in small networks only. The application of SNN for large 3D object detection networks has remain largely open. In this paper, we integrate spiking convolutional neural network (SCNN) with temporal coding into the YOLOv2 architecture for real-time object detection. To take the advantage of spiking signals, we develop a novel data preprocessing layer that translates 3D point-cloud data into spike time data. We propose an analog circuit to implement the non-leaky integrate and fire neuron used in our SCNN, from which the energy consumption of each spike is estimated. Moreover, we present a method to calculate the network sparsity and the energy consumption of the overall network. Extensive experiments have been conducted based on the KITTI dataset, which show that the proposed network can reach competitive detection accuracy as existing approaches, yet with much lower average energy consumption. If implemented in dedicated hardware, our network could have a mean sparsity of 56.24% and extremely low total energy consumption of 0.247mJ only. Implemented in NVIDIA GTX 1080i GPU, we can achieve 35.7 fps frame rate, high enough for real-time object detection.
C1 [Zhou, Shibo; Li, Xiaohua] SUNY Binghamton, Dept Elect & Comp Engn, Binghamton, NY 13902 USA.
   [Chen, Ying] Harbin Inst Technol, Sch Management, Dept Management Sci & Engn, Harbin 150000, Peoples R China.
   [Sanyal, Arindam] SUNY Buffalo, Dept Elect Engn, Buffalo, NY 14260 USA.
RP Chen, Y (corresponding author), Harbin Inst Technol, Sch Management, Dept Management Sci & Engn, Harbin 150000, Peoples R China.
EM yingchen@hit.edu.cn
CR [Anonymous], ARXIV190407537
   [Anonymous], 2017, ARXIV170109175
   [Anonymous], 2017, INT C INT ROB SYST
   [Anonymous], 2017, ARXIV170602413
   [Anonymous], 2018, ARXIV180306199
   Behroozpour B, 2017, IEEE COMMUN MAG, V55, P135, DOI 10.1109/MCOM.2017.1700030
   Biswas A, 2018, ISSCC DIG TECH PAP I, P488, DOI 10.1109/ISSCC.2018.8310397
   Chen XZ, 2017, PROC CVPR IEEE, P6526, DOI 10.1109/CVPR.2017.691
   Comsa JM, 2020, INT CONF ACOUST SPEE, P8529, DOI [10.1109/ICASSP40776.2020.9053856, 10.1109/icassp40776.2020.9053856]
   Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693
   Dayan P., 2001, THEORETICAL NEUROSCI
   Deng J, 2019, IEEE INT C INTELL TR, P279, DOI 10.1109/ITSC.2019.8917126
   Engelcke Martin, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P1355, DOI 10.1109/ICRA.2017.7989161
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Goltz J., 2019, ARXIV PREPRINT ARXIV
   González A, 2015, IEEE INT VEH SYM, P356, DOI 10.1109/IVS.2015.7225711
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Kheradpisheh S. Reza, 2019, ARXIV191009495
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Mostafa Hesham, 2018, IEEE Trans Neural Netw Learn Syst, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Ponulak F, 2011, ACTA NEUROBIOL EXP, V71, P409
   Premebida C, 2014, IEEE INT C INT ROBOT, P4112, DOI 10.1109/IROS.2014.6943141
   Qi Charles R., 2017, FRUSTUM POINTNETS 3D
   Redmon J., 2016, P IEEE C COMP VIS PA, P779
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Shi SS, 2019, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2019.00086
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Valavi H, 2018, SYMP VLSI CIRCUITS, P141, DOI 10.1109/VLSIC.2018.8502421
   Wang D, 2015, 2015 IEEE 81 VEH TEC, P1, DOI [10.1109/ECOC.2015.7341753, DOI 10.1109/VTCSPRING.2015.7145910]
   Wang ZY, 2019, IEEE ACCESS, V7, P120449, DOI 10.1109/ACCESS.2019.2937676
   Zhou S., 2019, ARXIV190910837
   Zhou Y, 2018, PROC CVPR IEEE, P4490, DOI 10.1109/CVPR.2018.00472
NR 33
TC 16
Z9 16
U1 4
U2 18
PY 2020
VL 8
BP 76903
EP 76912
DI 10.1109/ACCESS.2020.2990416
WC Computer Science, Information Systems; Engineering, Electrical &
   Electronic; Telecommunications
DA 2023-11-11
ER

PT J
AU Rotermund, D
   Pawelzik, KR
AF Rotermund, David
   Pawelzik, Klaus R.
TI Back-Propagation Learning in Deep Spike-By-Spike Networks
SO FRONTIERS IN COMPUTATIONAL NEUROSCIENCE
DT Article
DE deep network (DN); spiking network model; sparseness; compressed sensing
   (CS); error back propagation (BP) neural network
ID FIRE NEURON MODEL; EXACT SIMULATION; SPARSITY
AB Artificial neural networks (ANNs) are important building blocks in technical applications. They rely on noiseless continuous signals in stark contrast to the discrete action potentials stochastically exchanged among the neurons in real brains. We propose to bridge this gap with Spike-by-Spike (SbS) networks which represent a compromise between non-spiking and spiking versions of generative models. What is missing, however, are algorithms for finding weight sets that would optimize the output performances of deep SbS networks with many layers. Here, a learning rule for feed-forward SbS networks is derived. The properties of this approach are investigated and its functionality is demonstrated by simulations. In particular, a Deep Convolutional SbS network for classifying handwritten digits achieves a classification performance of roughly 99.3% on the MNIST test data when the learning rule is applied together with an optimizer. Thereby it approaches the benchmark results of ANNs without extensive parameter optimization. We envision this learning rule for SBS networks to provide a new basis for research in neuroscience and for technical applications, especially when they become implemented on specialized computational hardware.
C1 [Rotermund, David; Pawelzik, Klaus R.] Univ Bremen, Inst Theoret Phys, Bremen, Germany.
RP Rotermund, D (corresponding author), Univ Bremen, Inst Theoret Phys, Bremen, Germany.
EM davrot@neuro.uni-bremen.de
CR [Anonymous], 2016, P IEEE C COMPUTER VI
   [Anonymous], 2004, P 21 INT C MACH LEAR
   [Anonymous], 2016, ARXIV160204283
   [Anonymous], 2015, STDP BIOLOGICALLY PL
   Anwani N., 2018, ARXIV181110678
   Azkarate Saiz A., 2015, DEEP LEARNING REV IT
   Basan M, 2015, MOL SYST BIOL, V11, DOI 10.15252/msb.20156178
   Bengio Y, 2014, PR MACH LEARN RES, V32, P226
   Brette R, 2006, NEURAL COMPUT, V18, P2004, DOI 10.1162/neco.2006.18.8.2004
   Brette R, 2007, NEURAL COMPUT, V19, P2604, DOI 10.1162/neco.2007.19.10.2604
   Bruckstein AM, 2008, IEEE T INFORM THEORY, V54, P4813, DOI 10.1109/TIT.2008.929920
   Burkitt AN, 2006, BIOL CYBERN, V95, P97, DOI 10.1007/s00422-006-0082-8
   Burkitt AN, 2006, BIOL CYBERN, V95, P1, DOI 10.1007/s00422-006-0068-6
   Candès EJ, 2006, COMMUN PUR APPL MATH, V59, P1207, DOI 10.1002/cpa.20124
   Capparelli F., 2019, BIORXIV, DOI [10.1101/555128, DOI 10.1101/555128]
   Ernst U, 2007, NEURAL COMPUT, V19, P1313, DOI 10.1162/neco.2007.19.5.1313
   Ganguli S, 2012, ANNU REV NEUROSCI, V35, P485, DOI 10.1146/annurev-neuro-062111-150410
   Ganguli S, 2010, PHYS REV LETT, V104, DOI 10.1103/PhysRevLett.104.188701
   Ghosh P., 2019, VARIATIONAL DETERMIN
   Guo YM, 2016, NEUROCOMPUTING, V187, P27, DOI 10.1016/j.neucom.2015.09.116
   Guo Z., 2017, ARXIV170709316
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Jouppi NP, 2018, IEEE MICRO, V38, P10, DOI 10.1109/MM.2018.032271057
   King DB, 2015, ACS SYM SER, V1214, P1
   Lagorce X, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00206
   Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565
   Lee DS, 2001, IEEE VTS VEH TECHNOL, P556, DOI 10.1109/VETECS.2001.944904
   Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508
   Lustig M, 2008, IEEE SIGNAL PROC MAG, V25, P72, DOI 10.1109/MSP.2007.914728
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Moreno-Bote R, 2015, SCI REP-UK, V5, DOI 10.1038/srep17531
   Oh JH, 1998, ADV NEUR IN, V10, P605
   Olshausen B. A., 2006, 23 PROBLEMS SYSTEMS, V23, P182, DOI DOI 10.1093/ACPROF:OSO/9780195148220.001.0001
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Rezende D. J., 2014, ARXIV PREPRINT ARXIV
   Rolinek M., 2018, ARXIV180205074
   ROSENBLATT F, 1958, PSYCHOL REV, V65, P386, DOI 10.1037/h0042519
   Rotermund D., 2018, BIORXIV PREPRINT, DOI [10.1101/500280, DOI 10.1101/500280]
   Rotermund D., 2019, BIORXIV PREPRINT, DOI [10.1101/569236, DOI 10.1101/569236]
   Rozell CJ, 2008, NEURAL COMPUT, V20, P2526, DOI 10.1162/neco.2008.03-07-486
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Salakhutdinov R, 2015, ANNU REV STAT APPL, V2, P361, DOI 10.1146/annurev-statistics-010814-020120
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Serrano-Gotarredona T, 2015, IEEE INT SYMP CIRC S, P2405, DOI 10.1109/ISCAS.2015.7169169
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Spanne A, 2015, TRENDS NEUROSCI, V38, P417, DOI 10.1016/j.tins.2015.05.005
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Sze V, 2017, P IEEE, V105, P2295, DOI 10.1109/JPROC.2017.2761740
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Wan L., 2013, P 30 INT C MACH LEAR, V28, P1058
   Wiedemann T, 2018, AUTON AGENT MULTI-AG, V32, P134, DOI 10.1007/s10458-017-9375-7
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Zeng XH, 2016, J OPTIM, V2016, DOI 10.1155/2016/5975120
NR 53
TC 9
Z9 9
U1 0
U2 8
PD AUG 13
PY 2019
VL 13
AR 55
DI 10.3389/fncom.2019.00055
WC Mathematical & Computational Biology; Neurosciences
DA 2023-11-11
ER

PT S
AU Thiem, J
   Wolff, C
   Hartmann, G
AF Thiem, J
   Wolff, C
   Hartmann, G
BE Lee, SW
   Bulthoff, HH
   Poggio, T
TI Biology-inspired early vision system for a spike processing
   neurocomputer
SO BIOLOGICALLY MOTIVATED COMPUTER VISION, PROCEEDING
SE LECTURE NOTES IN COMPUTER SCIENCE
DT Article; Proceedings Paper
CT 1st IEEE International Workshop on Biologically Motivated Computer
   Vision (BMCV 2000)
CY MAY 15-17, 2000
CL SEOUL, SOUTH KOREA
ID MODEL; NEURONS
AB Examinations of the early vision system of mammals have shown GABOR-like behaviour of the simple cell responses. Furthermore, the advantages of GABOR-like image filtering are evident in computer vision. This causes strong demand to achieve GABOR-like responses in biology inspired spiking neural networks and so we propose a neural vision network based on spiking neurons with GABOR-like Simple cell responses. The GABOR behaviour is theoretically derived and demonstrated with simulation results. Our network consists of a cascaded structure of photoreceptors, ganglion and horizontal neurons and simple cells. The receptors are arranged on a hexagonal grid. One main advantage of our approach compared to direct GABOR filtering is the availability of valuable intersignals. The network is designed as the preprocessing stage for pulse-coupled neural Vision networks simulated on the SPIKE neurocomputer architecture. Hence, the simple cells are implemented as ECKHORN neurons.
C1 Univ Gesamthsch Paderborn, Heinz Nixdorf Inst, D-4790 Paderborn, Germany.
RP Thiem, J (corresponding author), Univ Gesamthsch Paderborn, Heinz Nixdorf Inst, D-4790 Paderborn, Germany.
CR [Anonymous], 1997, PHYSL MENSCHEN
   DAUGMAN JG, 1980, VISION RES, V20, P847, DOI 10.1016/0042-6989(80)90065-6
   Dudgeon Dan E., 1984, MULTIDIMENSIONAL DIG
   Eckhorn R, 1990, NEURAL COMPUT, V2, P293, DOI 10.1162/neco.1990.2.3.293
   ECKHORN R, 1989, P ICNN, V1, P723
   Frank G, 1999, IEEE T NEURAL NETWOR, V10, P527, DOI 10.1109/72.761709
   HARTMANN G, 1982, BIOL CYBERN, V43, P199, DOI 10.1007/BF00319979
   HARTMANN G, 1997, P 6 INT C MICR NEUR, P130
   HUBEL D, 1990, AUGE GEHIRN SPEKTRUM
   Iglesias I, 1998, J OPT SOC AM A, V15, P326, DOI 10.1364/JOSAA.15.000326
   JONES JP, 1987, J NEUROPHYSIOL, V58, P1233, DOI 10.1152/jn.1987.58.6.1233
   MARCELJA S, 1980, J OPT SOC AM, V70, P1297, DOI 10.1364/JOSA.70.001297
   MARR D, 1980, P ROYAL SOC LONDON B, V207
   POLLEN DA, 1983, IEEE T SYST MAN CYB, V13, P907, DOI 10.1109/TSMC.1983.6313086
   Shah S, 1996, IEEE T SYST MAN CY B, V26, P259, DOI 10.1109/3477.485837
   WILLIAMS DR, 1985, VISION RES, V25, P195, DOI 10.1016/0042-6989(85)90113-0
   Wolff C, 1999, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON MICROELECTRONICS FOR NEURAL, FUZZY AND BIO-INSPIRED SYSTEMS, MICORNEURO'99, P324, DOI 10.1109/MN.1999.758882
   Yellot J. I., 1981, SCIENCE, V212, P382
NR 18
TC 5
Z9 5
U1 0
U2 0
PY 2000
VL 1811
BP 387
EP 396
WC Computer Science, Artificial Intelligence; Computer Science,
   Cybernetics; Computer Science, Theory & Methods
DA 2023-11-11
ER

PT C
AU Meftah, R
   Benyettou, A
   Lezoray, O
   Debakla, M
AF Meftah, R.
   Benyettou, A.
   Lezoray, O.
   Debakla, M.
BE Arioui, H
   Merzouki, R
   Abbassi, HA
TI Image segmentation with Spiking Neuron Network
SO INTELLIGENT SYSTEMS AND AUTOMATION
SE AIP Conference Proceedings
DT Proceedings Paper
CT 1st Mediterranean Conference on Intelligent Systems and Automation (CISA
   2008)
CY JUN 30-JUL 02, 2008
CL Annaba, ALGERIA
DE classification; clustering; learning; segmentation; spiking neuron
   network
AB The process of segmenting images is one of the most critical ones in automatic image analysis whose goal can be regarded as to find what objects are presented in images. Artificial neural networks have been well developed. First two generations of neural networks have a lot of successful applications. Spiking Neuron Networks (SNNs) are often referred to as the P generation of neural networks which have potential to solve problems related to biological stimuli. They derive their strength and interest from an accurate modeling of synaptic interactions between neurons, taking into account the time of spike emission. SNNs overcome the computational power of neural networks made of threshold or sigmoidal units. Based on dynamic event-driven processing, they open up new horizons for developing models with an exponential capacity of memorizing and a strong ability to fast adaptation. Moreover, SNNs add a new dimension, the temporal axis, to the representation capacity and the processing abilities of neuralnetworks. In this paper, we present how SNN can be applied with efficacy in image segmentation.
C1 [Meftah, R.; Debakla, M.] Ctr Univ Mustapha Stambouli, Equipe EDTEC LRSBG, Mascara, Algeria.
   [Benyettou, A.] Univ Mohamed Boudiaf USTO, Lab SIMPA, Oran, Algeria.
   [Lezoray, O.] Univ Caen, CNRS, GREYC, UMR 6072, F-14050 Caen, France.
RP Meftah, R (corresponding author), Ctr Univ Mustapha Stambouli, Equipe EDTEC LRSBG, Mascara, Algeria.
CR Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   BRAGA AP, 2000, LTC EDITORA
   Dayan P., 2004, THEORETICAL NEUROSCI
   Gerstner W., 2002, SPIKING NEURON MODEL
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Maass W, 2001, THEOR COMPUT SCI, V261, P157, DOI 10.1016/S0304-3975(00)00137-7
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655
   OSTER M, 2004, P 11 IEEE INT C EL C, V11, P203
   Paugam-Moisy H, 2006, SPIKING NEURON NETWO
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   Zhang YJ, 2001, ISSPA 2001: SIXTH INTERNATIONAL SYMPOSIUM ON SIGNAL PROCESSING AND ITS APPLICATIONS, VOLS 1 AND 2, PROCEEDINGS, P148, DOI 10.1109/ISSPA.2001.949797
NR 12
TC 3
Z9 3
U1 0
U2 1
PY 2008
VL 1019
BP 15
EP +
WC Automation & Control Systems; Computer Science, Artificial Intelligence;
   Robotics
DA 2023-11-11
ER

PT J
AU Zhou, ER
   Fang, L
   Liu, RL
   Tang, ZS
AF Zhou, Errui
   Fang, Liang
   Liu, Rulin
   Tang, Zhensen
TI Area-efficient memristor spiking neural networks and supervised learning
   method
SO SCIENCE CHINA-INFORMATION SCIENCES
DT Letter
C1 [Zhou, Errui; Fang, Liang; Liu, Rulin; Tang, Zhensen] Natl Univ Def Technol, Inst Quantum Informat, Coll Comp, Changsha 410073, Hunan, Peoples R China.
   [Zhou, Errui; Fang, Liang; Liu, Rulin; Tang, Zhensen] Natl Univ Def Technol, State Key Lab High Performance Comp, Coll Comp, Changsha 410073, Hunan, Peoples R China.
RP Fang, L (corresponding author), Natl Univ Def Technol, Inst Quantum Informat, Coll Comp, Changsha 410073, Hunan, Peoples R China.
EM lfang@nudt.edu.cn
CR Adam GC, 2017, IEEE T ELECTRON DEV, V64, P312, DOI 10.1109/TED.2016.2630925
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Maass W, 1996, NEURAL COMPUT, V8, P1, DOI 10.1162/neco.1996.8.1.1
   Nishitani Y, 2015, IEEE T NEUR NET LEAR, V26, P2999, DOI 10.1109/TNNLS.2015.2399491
   Strukov DB, 2008, NATURE, V453, P80, DOI 10.1038/nature06932
   Wu XY, 2015, IEEE T CIRCUITS-II, V62, P1088, DOI 10.1109/TCSII.2015.2456372
   Zhang TL, 2018, AAAI CONF ARTIF INTE, P620
   Zhou J, 2014, SCI CHINA INFORM SCI, V57, DOI 10.1007/s11432-013-4887-5
   Zhu X, 2014, CHINESE PHYS B, V23, DOI 10.1088/1674-1056/23/2/028501
NR 9
TC 4
Z9 5
U1 2
U2 18
PD SEP
PY 2019
VL 62
IS 9
AR 199103
DI 10.1007/s11432-018-9607-8
WC Computer Science, Information Systems; Engineering, Electrical &
   Electronic
DA 2023-11-11
ER

PT J
AU Han, IS
AF Han, Il Song
TI Mixed-signal neuron-synapse implementation for large-scale neural
   network
SO NEUROCOMPUTING
DT Article; Proceedings Paper
CT 1st International Conference on Brain Inspired Cognitive Systems (BICS
   2004)
CY AUG 29-SEP 01, 2004
CL Stirling, SCOTLAND
DE analogue-mixed VLSI neural network; pulse/spike-based neural
   computation; asynchronous operation; MOSFET resistance;
   voltage-controlled linear resistance
ID MODEL
AB This paper describes a mixed-signal neural networks VLSI for low power and asynchronous operation. The voltage-controlled transconductance produces the synaptic function of multiplication and summation of synaptic currents for neuron, by compensating the non-linearity of MOSFET resistance in the triode region.
   The flexible configuration of synapse accommodates the spike-based neural networks, inspired by the biological plausibility and low power requirement. The neuron with a combination of synapses demonstrates asynchronous spikes of integration-and-firing with a refractory period. The speed of individual synapse is simulated up to 300 Mega operations/s with the power consumption of less than 33 mu W, using 0.18 mu m CMOS. (c) 2006 Elsevier B.V. All rights reserved.
C1 Univ Sheffield, Dept Elect & Elect Engn, Sheffield S1 3JD, S Yorkshire, England.
RP Han, IS (corresponding author), Univ Sheffield, Dept Elect & Elect Engn, Mappin St, Sheffield S1 3JD, S Yorkshire, England.
EM i.s.han@sheffield.ac.uk
CR Asai T, 2003, IEEE T NEURAL NETWOR, V14, P1308, DOI 10.1109/TNN.2003.816357
   BARTOLOZZI C, 2004, P BICS STERL
   Bugmann G, 1997, BIOSYSTEMS, V40, P11, DOI 10.1016/0303-2647(96)01625-5
   Christodoulou C, 2002, NEURAL NETWORKS, V15, P891, DOI 10.1016/S0893-6080(02)00034-5
   Eckmiller R, 2004, LECT NOTES COMPUT SC, V3316, P10
   Floreano D., 2001, LNCS, P38
   Häusser M, 2000, NAT NEUROSCI, V3, P1165, DOI 10.1038/81426
   HAN I, 1997, P EANN 97, P299
   HAN I, 1997, KOREA TELECOM J, V2, P12
   HAN IS, 2004, P BICS STERL
   Horio Y, 2003, IEEE T NEURAL NETWOR, V14, P1393, DOI 10.1109/TNN.2003.816349
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   LINARES-BARRANCO B, 1991, IEEE J SOLID-ST CIRC, V26, P956, DOI 10.1109/4.92015
   MATINOIA S, 2004, IEEE T BIOMEDICAL EN, V51, P859
   Milev M, 2003, IEEE T NEURAL NETWOR, V14, P1187, DOI 10.1109/TNN.2003.816369
   PEARCE T, 2004, P BICS STERL
   Seiffert U, 2004, NEUROCOMPUTING, V57, P135, DOI 10.1016/j.neucom.2004.01.011
   Simoni ME, 2004, IEEE T BIO-MED ENG, V51, P342, DOI 10.1109/TBME.2003.820390
   TAYLOR J, 2004, COMMUNICATION
   Taylor JG, 2003, PROG NEUROBIOL, V71, P305, DOI 10.1016/j.pneurobio.2003.10.002
   TAYLOR N, 2004, P BICS STERL
NR 21
TC 13
Z9 13
U1 0
U2 1
PD OCT
PY 2006
VL 69
IS 16-18
SI SI
BP 1860
EP 1867
DI 10.1016/j.neucom.2005.11.013
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT C
AU Sun, N
   Luo, QM
   Li, XN
AF Sun, N
   Luo, QM
   Li, XN
BE Luo, Q
   Tuchin, VV
   Gu, M
   Wang, LV
TI Spike sorting in networks of cultured neurons on multi-electrode arrays
SO PHOTONICS AND IMAGING IN BIOLOGY AND MEDICINE
SE Proceedings of SPIE
DT Proceedings Paper
CT 3rd International Conference on Photonics and Imaging in Biology and
   Medicine
CY JUN 08-11, 2003
CL WUHAN, PEOPLES R CHINA
DE multi-electrode arrays; neuronal network; spike sorting; Principal
   Component Analysis (PCA); Cluster Analysis
ID CORTICAL-NEURONS
AB The technology of micro-electrode arrays is getting more and more important in the research of brain neural. networks and its dynamics because of the ability to stimulate and record more neurons' activities simultaneously. When the signals of many neurons with several noises in a local region are picked up with a microelectrode, a neurophysiologist may wish to sort these signals by assigning particular spikes to putative neurons with some degree of reliability. Spike sorting is a key step in whole data process and is a general problem in neural electrophysiology. Many algorithms for spike sorting have been brought forward based on the features of the spike waveforms from different neurons. In this article, I have accomplished spike sorting through analyzing the amplitude, shape and principal components of the spikes.
C1 Huazhong Univ Sci & Technol, Minist Educ, Key Lab Biomed Photon, Wuhan 430074, Peoples R China.
RP Luo, QM (corresponding author), Huazhong Univ Sci & Technol, Minist Educ, Key Lab Biomed Photon, Wuhan 430074, Peoples R China.
EM gluo@mail.hust.edu.cn
CR Egert U, 2002, J NEUROSCI METH, V117, P33, DOI 10.1016/S0165-0270(02)00045-6
   Jimbo Y, 2000, BIOL CYBERN, V83, P1, DOI 10.1007/PL00007970
   Jimbo Y, 1999, BIOPHYS J, V76, P670, DOI 10.1016/S0006-3495(99)77234-6
   Letelier JC, 2000, J NEUROSCI METH, V101, P93, DOI 10.1016/S0165-0270(00)00250-8
   Lewicki MS, 1998, NETWORK-COMP NEURAL, V9, pR53, DOI 10.1088/0954-898X/9/4/001
   Marom S, 2002, Q REV BIOPHYS, V35, P63, DOI 10.1017/S0033583501003742
   SAHARI M, 1999, THESIS CALIFORNIA I
   Shahaf G, 2001, J NEUROSCI, V21, P8782, DOI 10.1523/JNEUROSCI.21-22-08782.2001
   Sim AWK, 1998, P ANN INT IEEE EMBS, V20, P1381, DOI 10.1109/IEMBS.1998.747138
   Zhang JF, 2002, MED MULTIVARIATE STA
NR 10
TC 1
Z9 1
U1 0
U2 1
PY 2003
VL 5254
BP 513
EP 520
DI 10.1117/12.546560
WC Biochemical Research Methods; Medicine, Research & Experimental; Optics;
   Imaging Science & Photographic Technology; Radiology, Nuclear Medicine &
   Medical Imaging; Spectroscopy
DA 2023-11-11
ER

PT C
AU Huang, PL
   Hsu, CH
   Cheng, YH
   Li, ZF
   Lin, YH
   Tang, KT
AF Huang, Ping-Li
   Hsu, Chen-Han
   Cheng, Yu-Hsiang
   Li, Zhaofang
   Lin, Yu-Hsuan
   Tang, Kea-Tiong
GP IEEE
TI A 104.76-TOPS/W, Spike-Based Convolutional Neural Network Accelerator
   with Reduced On-Chip Memory Data Flow and Operation Unit Skipping
SO 2022 IEEE ASIA PACIFIC CONFERENCE ON CIRCUITS AND SYSTEMS, APCCAS
DT Proceedings Paper
CT IEEE Asia Pacific Conference on Circuits and Systems (APCCAS)
CY NOV 11-13, 2022
CL So Univ Sci & Technol, Shenzhen, PEOPLES R CHINA
HO So Univ Sci & Technol
DE Convolutional neural network; spiking neural network; accelerators;
   digital circuits
AB The energy efficiency of artificial intelligence networks must be increased if they are to be implemented on edge devices. Brain-inspired spiking neural networks (SNNs) are considered potential candidates for this purpose because they do not involve multiplication operations. SNNs only perform addition and shifting operations. SNNs can be used with a convolutional neural network (CNN) to reduce the required computational power. The combination of an SNN and a CNN is called a spiking CNN (SCNN). To achieve a high operation speed with an SCNN, a large memory, which occupies a relatively large area and consumes a relatively large amount of power, is often required. In this paper, a data flow method is proposed to reduce the required on-chip memory and power consumption and to eliminate the operation unit skipping of a high-sparsity SCNN. This method decreases the overall on-chip memory required by an SCNN and increases the network's energy efficiency. When using the proposed method in this study, an SCNN exhibited energy efficiency of 104.76 TOPS/W when processing the CIFA-10 dataset.
C1 [Huang, Ping-Li; Hsu, Chen-Han; Cheng, Yu-Hsiang; Li, Zhaofang; Lin, Yu-Hsuan; Tang, Kea-Tiong] Nation Tsinghua Univ, Dept Elect Engn, Hsinchu, Taiwan.
RP Huang, PL (corresponding author), Nation Tsinghua Univ, Dept Elect Engn, Hsinchu, Taiwan.
EM 1i242132@gmail.com; pass7955820@gmail.com; cheng1eo8548@gmail.com;
   xbllzf@gapp.nthu.edu.tw; hsuan10130607@gmail.com; kttang@mx.nthu.edudw
CR Chuang PY, 2020, DES AUT CON, DOI 10.1109/dac18072.2020.9218714
   Kim Sangyeob, 2022, ARXIV
   Lien HH, 2021, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS51556.2021.9401181
   Narayanan S, 2020, ANN I S COM, P349, DOI 10.1109/ISCA45697.2020.00038
   Wang B, 2020, DES AUT TEST EUROPE, P240, DOI 10.23919/DATE48585.2020.9116516
   Wang RC, 2016, IEEE INT SYMP CIRC S, P2082, DOI 10.1109/ISCAS.2016.7538989
   Zhang ML, 2022, IEEE T NEUR NET LEAR, V33, P1947, DOI 10.1109/TNNLS.2021.3110991
   Zhong Y, 2021, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS51556.2021.9401194
NR 8
TC 0
Z9 0
U1 1
U2 1
PY 2022
BP 41
EP 45
DI 10.1109/APCCAS55924.2022.10090309
WC Computer Science, Hardware & Architecture; Engineering, Electrical &
   Electronic
DA 2023-11-11
ER

PT C
AU Yusoff, N
   Grüning, A
AF Yusoff, Nooraini
   Gruening, Andre
BE Huang, T
   Zeng, Z
   Li, C
   Leung, CS
TI Learning Anticipation through Priming in Spatio-temporal Neural Networks
SO NEURAL INFORMATION PROCESSING, ICONIP 2012, PT I
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 19th International Conference on Neural Information Processing (ICONIP)
CY NOV 11-15, 2012
CL Doha, QATAR
DE Reward-based learning; Spiking neural networks; Priming effect;
   Associative learning
ID SPIKING NEURONS; MEMORY; MODEL
AB In this paper, we propose a reward-based learning model inspired by the findings from a behavioural study and biologically realistic properties of spatio-temporal neural networks. The model simulates the cognitive priming effect in stimulus-stimulus-response association. Synaptic plasticity is dependent on a global reward signal that enhances the synaptic changes derived from spike-timing dependent plasticity (STDP) process. We show that by priming a network with a cue stimulus can facilitate the response to a later stimulus. The network can be trained to associate a stimulus pair (with an inter-stimulus interval) to a response, as well as to recognise the temporal sequence of the stimulus presentation.
C1 [Yusoff, Nooraini] Univ Utara Malaysia, UUM Coll Arts & Sci, Sch Comp, Sintok 06010, Kedah, Malaysia.
   [Gruening, Andre] Univ Surrey, Fac Engn & Phys Sci, Dept Comp, Guildford GU2 7XH, Surrey, England.
RP Yusoff, N (corresponding author), Univ Utara Malaysia, UUM Coll Arts & Sci, Sch Comp, Sintok 06010, Kedah, Malaysia.
EM nooraini@uum.edu.my; a.gruning@surrey.ac.uk
CR Erickson CA, 1999, J NEUROSCI, V19, P10404
   Filippova MG, 2011, SPAN J PSYCHOL, V14, P20, DOI 10.5209/rev_SJOP.2011.v14.n1.2
   Florian RV, 2007, NEURAL COMPUT, V19, P1468, DOI 10.1162/neco.2007.19.6.1468
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2007, CEREB CORTEX, V17, P2443, DOI 10.1093/cercor/bhl152
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Mongillo G, 2003, EUR J NEUROSCI, V18, P2011, DOI 10.1046/j.1460-9568.2003.02908.x
   SCHACTER DL, 1992, J COGNITIVE NEUROSCI, V4, P244, DOI 10.1162/jocn.1992.4.3.244
   TULVING E, 1982, J EXP PSYCHOL LEARN, V8, P336, DOI 10.1037/0278-7393.8.4.336
NR 10
TC 1
Z9 1
U1 0
U2 1
PY 2012
VL 7663
BP 168
EP 175
WC Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Computer Science, Theory & Methods
DA 2023-11-11
ER

PT C
AU Schuman, CD
   Mitchell, JP
   Parsa, M
   Plank, JS
   Brown, SD
   Rose, GS
   Patton, RM
   Potok, TE
AF Schuman, Catherine D.
   Mitchell, J. Parker
   Parsa, Maryam
   Plank, James S.
   Brown, Samuel D.
   Rose, Garrett S.
   Patton, Robert M.
   Potok, Thomas E.
GP IEEE
TI Automated Design of Neuromorphic Networks for Scientific Applications at
   the Edge
SO 2020 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN) held as part
   of the IEEE World Congress on Computational Intelligence (IEEE WCCI)
CY JUL 19-24, 2020
CL ELECTR NETWORK
AB Designing spiking neural networks for neuromorphic deployment is a non-trivial task. It is further complicated when there are resource constraints for the neuromorphic implementation, such as size or power constraints, that may be present in edge applications. In this work, we utilize a previously presented approach, EONS, to design spiking neural networks for a memristive neuromorphic implementation for scientific data applications. We specifically use a multi-objective approach in EONS to maximize network accuracy on the scientific data application task, but also to minimize network size and energy. We illustrate that EONS determines both the network structure and the parameters, removing the burden from the user on determining the appropriate spiking neural network structure, and we show that the resulting networks are very different from the layered structure of typical neural networks. Finally, we show that the multi-objective approach produces smaller, more energy efficient networks than the original EONS approach and produces comparable accuracy to a back-propagation style training approach.
C1 [Schuman, Catherine D.; Mitchell, J. Parker; Patton, Robert M.; Potok, Thomas E.] Oak Ridge Natl Lab, Computat Data Analyt, Oak Ridge, TN 37830 USA.
   [Parsa, Maryam] Purdue Univ, W Lafayette, IN 47907 USA.
   [Plank, James S.; Brown, Samuel D.; Rose, Garrett S.] Univ Tennessee, Dept EECS, Knoxville, TN USA.
RP Schuman, CD (corresponding author), Oak Ridge Natl Lab, Computat Data Analyt, Oak Ridge, TN 37830 USA.
EM schumancd@ornl.gov; mitchelljp1@ornl.gov; mparsa@purdue.edu;
   jplank@utk.edu; sbrow109@utk.edu; garose@utk.edu; pattonrm@ornl.gov;
   potokte@ornl.gov
CR Adnan MM, 2018, INT SOC DESIGN CONF, P37, DOI 10.1109/SOCC.2018.8618553
   Ankit A., 2019, IEEE T COMPUTER AIDE
   Ankit A, 2017, ICCAD-IEEE ACM INT, P533, DOI 10.1109/ICCAD.2017.8203823
   [Anonymous], 2013, MATH PROBLEMS ENG
   Bruer G., 2018, INT C NEUR COMP SYST
   Buckley S, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON REBOOTING COMPUTING (ICRC), P36
   Chakma G, 2018, IEEE J EM SEL TOP C, V8, P125, DOI 10.1109/JETCAS.2017.2777181
   Chakraborty I, 2018, IEEE TETCI, V2, P335, DOI 10.1109/TETCI.2018.2829919
   Dheeru D., 2017, UCI MACHINE LEARNING
   Diehl PU, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON REBOOTING COMPUTING (ICRC)
   Dimovska M., 2019, 2019 IEEE ANN UB COM
   Disney A, 2016, BIOL INSPIR COGN ARC, V17, P49, DOI 10.1016/j.bica.2016.07.007
   Esser S.K., 2015, ADV NEURAL INFORM PR, P1117
   Guliashki Vassil, 2009, Problemy na Tekhnicheskata Kibernetika i Robotikata, V60, P42
   Hasan M. S., 2018, IEEE 13 DALL CIRC SY
   Hasan R, 2014, IEEE IJCNN, P21, DOI 10.1109/IJCNN.2014.6889893
   Ji Y, 2016, PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON NUCLEAR ENGINEERING, 2016, VOL 3
   Johnston T, 2019, PROCEEDINGS OF 2019 5TH IEEE/ACM WORKSHOP ON MACHINE LEARNING IN HIGH PERFORMANCE COMPUTING ENVIRONMENTS (MLHPC 2019), P9, DOI 10.1109/MLHPC49564.2019.00007
   Lee JM, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00191
   Liang L, 2018, IEEE ACCESS, V6, P58324, DOI 10.1109/ACCESS.2018.2874823
   Liu BY, 2015, DES AUT CON, DOI 10.1145/2744769.2744930
   Mitchell D, 2018, GEOGR ANN B, V100, P2, DOI 10.1080/04353684.2018.1445478
   Parsa M., 2019, ARXIV190608167
   Plank J S., 2018, IEEE LETT COMPUT SOC, V1, P17, DOI [DOI 10.1109/LOCS.2018.2885976, 10.1109/LOCS.2018.2885976]
   Rasmussen Daniel, 2019, NEUROINFORMATICS, P1
   Schuman CD, 2017, PROCEEDINGS OF NEUROMORPHIC COMPUTING SYMPOSIUM (NCS 2017), DOI 10.1145/3183584.3183612
   Schuman CD, 2016, IEEE IJCNN, P145, DOI 10.1109/IJCNN.2016.7727192
   Schumann CL, 2019, AIDS BEHAV, V23, P5, DOI 10.1007/s10461-017-1727-4
   Severa W, 2019, NAT MACH INTELL, V1, P86, DOI 10.1038/s42256-018-0015-y
   Shrestha Sumit Bam, 2018, ADV NEURAL INFORM PR, P1412
   Yuan G, 2019, I SYMPOS LOW POWER E
   Zhao G., 2019, P 44 ANN GOMACTECH C
NR 32
TC 0
Z9 0
U1 0
U2 2
PY 2020
DI 10.1109/ijcnn48605.2020.9207412
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture
DA 2023-11-11
ER

PT J
AU An, Y
   Nam, Y
AF An, Yujin
   Nam, Yoonkey
TI Closed-loop control of neural spike rate of cultured neurons using a
   thermoplasmonics-based photothermal neural stimulation
SO JOURNAL OF NEURAL ENGINEERING
DT Article
DE optical stimulation; closed-loop control; microelectrode array; gold
   nanorod; near-infrared; thermoplasmonics; cell culture
ID INHIBITION; ACTIVATION
AB Objective. Photothermal neural stimulation has been developed in a variety of interfaces as an alternative technology that can perturb neural activity. The demonstrations of these techniques have heavily relied on open-loop stimulation or complete suppression of neural activity. To extend the controllability of photothermal neural stimulation, combining it with a closed-loop system is required. In this work, we investigated whether photothermal suppression mechanism can be used in a closed-loop system to reliably modulate neural spike rate to non-zero setpoints. Approach. To incorporate the photothermal inhibition mechanism into the neural feedback system, we combined a thermoplasmonic stimulation platform based on gold nanorods (GNRs) and near-infrared illuminations (808 nm, spot size: 2 mm or 200 mu m in diameter) with a proportional-integral (PI) controller. The closed-loop feedback control system was implemented to track predetermined target spike rates of hippocampal neuronal networks cultured on GNR-coated microelectrode arrays. Main results. The closed-loop system for neural spike rate control was successfully implemented using a PI controller and the thermoplasmonic neural suppression platform. Compared to the open-loop control, the target-channel spike rates were precisely modulated to remain constant or change in a sinusoidal form in the range below baseline spike rates. The spike rate response behaviors were affected by the choice of the controller gain. We also demonstrated that the functional connectivity of a synchronized bursting network could be altered by controlling the spike rate of one of the participating channels. Significance. The thermoplasmonic feedback controller proved that it can precisely modulate neural spike rate of neural activity in vitro. This technology can be used for studying neuronal network dynamics and might provide insights in developing new neuromodulation techniques in clinical applications.
C1 [An, Yujin; Nam, Yoonkey] Korea Adv Inst Sci & Technol KAIST, Dept Bio & Brain Engn, 291 Daehak Ro, Daejeon 34141, South Korea.
   [Nam, Yoonkey] Korea Adv Inst Sci & Technol KAIST, KAIST Inst Hlth Sci & Technol, Daejeon 34141, South Korea.
RP Nam, Y (corresponding author), Korea Adv Inst Sci & Technol KAIST, Dept Bio & Brain Engn, 291 Daehak Ro, Daejeon 34141, South Korea.; Nam, Y (corresponding author), Korea Adv Inst Sci & Technol KAIST, KAIST Inst Hlth Sci & Technol, Daejeon 34141, South Korea.
EM ynam@kaist.ac.kr
CR Albert ES, 2012, J NEUROPHYSIOL, V107, P3227, DOI 10.1152/jn.00424.2011
   An Y., 2021, 43 ANN INT C IEEE EN
   Baffou G., 2017, THERMOPLASMONICS HEA, P36
   Bazard P, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-08141-4
   Bolus MF, 2018, J NEURAL ENG, V15, DOI 10.1088/1741-2552/aaa506
   Bolus MF., 2020, 2020 STATE SPACE OPT, DOI [10.1101/2020.06.25.171785, DOI 10.1101/2020.06.25.171785]
   Carvalho-de-Souza JL, 2015, NEURON, V86, P207, DOI 10.1016/j.neuron.2015.02.033
   Cayce JM, 2014, NEUROIMAGE, V84, P181, DOI 10.1016/j.neuroimage.2013.08.040
   Damnjanovic R, 2020, ACS NANO, V14, P10917, DOI 10.1021/acsnano.0c00722
   de Boer WDAM, 2018, LIGHT-SCI APPL, V7, DOI 10.1038/s41377-018-0103-0
   Eom K, 2016, BIOMED OPT EXPRESS, V7, P1614, DOI 10.1364/BOE.7.001614
   Eom K, 2014, SMALL, V10, P3853, DOI 10.1002/smll.201400599
   Farah N, 2013, J NEURAL ENG, V10, DOI 10.1088/1741-2560/10/5/056004
   Fekete Z, 2020, J NEURAL ENG, V17, DOI 10.1088/1741-2552/abb3b2
   Feyen P, 2016, SCI REP-UK, V6, DOI 10.1038/srep22718
   Hong N, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-20060-z
   Jiang B, 2019, NEUROPHOTONICS, V6, DOI 10.1117/1.NPh.6.3.035009
   Jung H, 2017, BIOMED OPT EXPRESS, V8, P2866, DOI 10.1364/BOE.8.002866
   Kang H, 2020, ACS NANO, V14, P11406, DOI 10.1021/acsnano.0c03703
   Kang H, 2018, ACS NANO, V12, P1128, DOI 10.1021/acsnano.7b06617
   Krook-Magnuson E, 2013, NAT COMMUN, V4, DOI 10.1038/ncomms2376
   Lee JW, 2018, NANOSCALE, V10, P9226, DOI 10.1039/c8nr01697f
   Lee JW, 2018, BIOMATERIALS, V153, P59, DOI 10.1016/j.biomaterials.2017.10.041
   Lyu Y, 2016, J AM CHEM SOC, V138, P9049, DOI 10.1021/jacs.6b05192
   Marom S, 2011, FRONT COMPUT NEUROSC, V5, DOI [10.3389/fncom.2011.00016, 10.3389/fneng.2011.00003]
   Martino N, 2015, SCI REP-UK, V5, DOI 10.1038/srep08911
   Migliori B, 2012, AIP ADV, V2, DOI 10.1063/1.4748955
   Miranda-Domínguez O, 2010, J NEURAL ENG, V7, DOI 10.1088/1741-2560/7/6/066004
   Miyako E, 2014, ANGEW CHEM INT EDIT, V53, P13121, DOI 10.1002/anie.201407169
   Nakatsuji H, 2015, ANGEW CHEM INT EDIT, V54, P11725, DOI 10.1002/anie.201505534
   Newman JP, 2015, ELIFE, V4, DOI 10.7554/eLife.07192
   Rastinehad AR, 2019, P NATL ACAD SCI USA, V116, P18590, DOI 10.1073/pnas.1906929116
   Shapiro MG, 2012, NAT COMMUN, V3, DOI 10.1038/ncomms1742
   Singh P, 2018, INT J MOL SCI, V19, DOI 10.3390/ijms19071979
   Tonnesen J, 2009, P NATL ACAD SCI USA, V106, P12162, DOI 10.1073/pnas.0901915106
   Wells J, 2005, OPT LETT, V30, P504, DOI 10.1364/OL.30.000504
   Yong J, 2014, ADV HEALTHC MATER, V3, P1862, DOI 10.1002/adhm.201400027
   Yoo S, 2019, ACS NANO, V13, P544, DOI 10.1021/acsnano.8b07277
   Yoo S, 2016, ACS NANO, V10, P4274, DOI 10.1021/acsnano.5b07747
   Yoo S, 2014, ACS NANO, V8, P8040, DOI 10.1021/nn5020775
NR 40
TC 5
Z9 5
U1 1
U2 10
PD DEC
PY 2021
VL 18
IS 6
AR 066002
DI 10.1088/1741-2552/ac3265
WC Engineering, Biomedical; Neurosciences
DA 2023-11-11
ER

PT J
AU KALAYCI, T
   OZDAMAR, O
AF KALAYCI, T
   OZDAMAR, O
TI WAVELET PREPROCESSING FOR AUTOMATED NEURAL-NETWORK DETECTION OF EEG
   SPIKES
SO IEEE ENGINEERING IN MEDICINE AND BIOLOGY MAGAZINE
DT Article
RP KALAYCI, T (corresponding author), UNIV MIAMI,DEPT BIOMED ENGN,POB 248294,CORAL GABLES,FL 33124, USA.
NR 0
TC 162
Z9 166
U1 0
U2 7
PD MAR-APR
PY 1995
VL 14
IS 2
BP 160
EP 166
DI 10.1109/51.376754
WC Engineering, Biomedical; Medical Informatics
DA 2023-11-11
ER

PT C
AU Gruel, A
   Martinet, J
AF Gruel, Amelie
   Martinet, Jean
GP IEEE
TI Bio-inspired visual attention for silicon retinas based on spiking
   neural networks applied to pattern classification
SO 2021 INTERNATIONAL CONFERENCE ON CONTENT-BASED MULTIMEDIA INDEXING
   (CBMI)
SE International Workshop on Content-Based Multimedia Indexing
DT Proceedings Paper
CT 18th International Conference on Content-Based Multimedia Indexing (IEEE
   CBMI)
CY JUN 28-30, 2021
CL ELECTR NETWORK
DE visual attention; silicon retinas; bio-inspiration; spiking neural
   networks
ID VISION
AB Visual attention can be defined as the behavioral and cognitive process of selectively focusing on a discrete aspect of sensory cues while disregarding other perceivable information. This biological mechanism, more specifically saliency detection, has long been used in multimedia indexing to drive the analysis only on relevant parts of images or videos for further processing.
   The recent advent of silicon retinas (or event cameras - sensors that measure pixel-wise changes in brightness and output asynchronous events accordingly) raises the question of how to adapt attention and saliency to the unconventional type of such sensors' output. Silicon retina aims to reproduce the biological retina behaviour. In that respect, they produce punctual events in time that can be construed as neural spikes and interpreted as such by a neural network.
   In particular, Spiking Neural Networks (SNNs) represent an asynchronous type of artificial neural network closer to biology than traditional artificial networks, mainly because they seek to mimic the dynamics of neural membrane and action potentials over time. SNNs receive and process information in the form of spike trains. Therefore, they make for a suitable candidate for the efficient processing and classification of incoming event patterns measured by silicon retinas. In this paper, we review the biological background behind the attentional mechanism, and introduce a case study of event videos classification with SNNs, using a biology-grounded low-level computational attention mechanism, with interesting preliminary results.
C1 [Gruel, Amelie; Martinet, Jean] Univ Cote dAzur, I3S, CNRS, Nice, France.
RP Gruel, A (corresponding author), Univ Cote dAzur, I3S, CNRS, Nice, France.
EM amelie.gruel@univ-cotedazur.fr; jean.martinet@univ-cotedazur.fr
CR Amir A, 2017, PROC CVPR IEEE, P7388, DOI 10.1109/CVPR.2017.781
   Anderson John R., 2005, COGNITIVE PSYCHOL IT
   Bernert M., 2018, INT J NEURAL SYSTEMS
   Bichot NP, 2015, NEURON, V88, P832, DOI 10.1016/j.neuron.2015.10.001
   Bogdan P., 2019, EMERGING TECHNOLOGY
   Cannici M., 2018, ATTENTION MECH OBJEC
   Chevallier S., 2005, PDCN 2005
   Chevallier S, 2008, LECT NOTES COMPUT SC, V5008, P56
   Chevallier S, 2010, LECT NOTES COMPUT SC, V6374, P257, DOI 10.1007/978-3-642-15910-7_29
   Chik D, 2009, NEURAL NETWORKS, V22, P890, DOI 10.1016/j.neunet.2009.02.002
   Gallego G, 2022, IEEE T PATTERN ANAL, V44, P154, DOI 10.1109/TPAMI.2020.3008413
   Gehrig D, 2018, LECT NOTES COMPUT SC, V11216, P766, DOI 10.1007/978-3-030-01258-8_46
   Gregor K, 2015, PR MACH LEARN RES, V37, P1462
   Itti L, 2001, NAT REV NEUROSCI, V2, P194, DOI 10.1038/35058500
   James W., 1890, PRINCIPLES PSYCHOL V, V2
   Jetley S., 2018, INT C LEARN REPR ICL, P1
   Katayama K, 2004, BIOL CYBERN, V91, P315, DOI 10.1007/s00422-004-0504-4
   Knudsen EI, 2018, TRENDS NEUROSCI, V41, P789, DOI 10.1016/j.tins.2018.06.006
   Le Callet P, 2013, P IEEE, V101, P2058, DOI 10.1109/JPROC.2013.2265801
   Lichtsteiner P, 2008, IEEE J SOLID-ST CIRC, V43, P566, DOI 10.1109/JSSC.2007.914337
   MAHOWALD MA, 1991, SCI AM, V264, P76, DOI 10.1038/scientificamerican0591-76
   Martinet J, 2009, IEEE PAC RIM S IM VI
   Moore T, 2017, ANNU REV PSYCHOL, V68, P47, DOI 10.1146/annurev-psych-122414-033400
   Renner Alpha, 2019, EVENT BASED ATTENTIO
   Shi MT, 2020, FRONT COMPUT NEUROSC, V14, DOI 10.3389/fncom.2020.00007
   Steffen L, 2019, FRONT NEUROROBOTICS, V13, DOI 10.3389/fnbot.2019.00028
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   TSOTSOS JK, 1990, BEHAV BRAIN SCI, V13, P423, DOI 10.1017/S0140525X00079577
   Vaswani A., 2017, P 31 INT C NEURAL IN
   Wu QX, 2013, NEUROCOMPUTING, V116, P3, DOI 10.1016/j.neucom.2012.01.046
NR 30
TC 0
Z9 0
U1 1
U2 3
PY 2021
BP 118
EP 123
DI 10.1109/CBMI50038.2021.9461882
WC Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Imaging Science & Photographic Technology
DA 2023-11-11
ER

PT J
AU Maguire, LP
   McGinnity, TM
   Glackin, B
   Ghani, A
   Belatreche, A
   Harkin, J
AF Maguire, L. P.
   McGinnity, T. M.
   Glackin, B.
   Ghani, A.
   Belatreche, A.
   Harkin, J.
TI Challenges for large-scale implementations of spiking neural networks on
   FPGAs
SO NEUROCOMPUTING
DT Article
DE field programmable gate arrays (FPGAs); hardware implementation; spiking
   neural network (SNN)
ID MODELING LARGE NETWORKS; SIMULATION; DESIGN; PLASTICITY; SPIKENET;
   NEURONS
AB The last 50 years has witnessed considerable research in the area of neural networks resulting in a range of architectures, learning algorithms and demonstrative applications. A more recent research trend has focused on the biological plausibility of such networks as a closer abstraction to real neurons may offer improved performance in an adaptable, real-time environment. This poses considerable challenges for engineers particularly in terms of the requirement to realise a low-cost embedded solution. Programmable hardware has been widely recognised as an ideal platform for the adaptable requirements of neural networks and there has been considerable research reported in the literature. This paper aims to review this body of research to identify the key lessons learned and, in particular, to identify the remaining challenges for large-scale implementations of spiking neural networks on FPGAs. (c) 2007 Elsevier B.V. All rights reserved.
C1 Univ Ulster, Sch Comp & Intelligent Syst, Intelligent Syst Engn Lab, Derry BT48 7JL, North Ireland.
RP Glackin, B (corresponding author), Univ Ulster, Sch Comp & Intelligent Syst, Intelligent Syst Engn Lab, Magee Campus, Derry BT48 7JL, North Ireland.
EM lp.maguire@ulster.ac.uk; b.glackin@ulster.ac.uk
CR [Anonymous], FDN PHYSL PSYCHOL
   Belatreche A, 2007, SOFT COMPUT, V11, P239, DOI [10.1007/s00500-006-0065-7, 10.1007/S00500-006-0065-7]
   Belatreche A, 2006, NEW MATH NAT COMPUT, V2, P237, DOI 10.1142/S179300570600049X
   BEUCHAT JL, 1998, 5 REC ARCH WORKSH RA
   Bi GQ, 2001, ANNU REV NEUROSCI, V24, P139, DOI 10.1146/annurev.neuro.24.1.139
   Blake JJ, 1998, INFORM SCIENCES, V112, P151, DOI 10.1016/S0020-0255(98)10029-4
   CHEN Y, 2005, P PREP U LANC, P71
   CHEN Y, 2006, 3 INT S NEUR NETW IS
   Chen YJ, 2006, IEEE IJCNN, P1511
   DEGARIS H, 2002, NEUROCOMPUTING J, V42
   Delorme A, 1999, NEUROCOMPUTING, V26-7, P989, DOI 10.1016/S0925-2312(99)00095-8
   Delorme A, 2003, NETWORK-COMP NEURAL, V14, P613, DOI 10.1088/0954-898X/14/4/301
   Deneve S, 2001, NAT NEUROSCI, V4, P826, DOI 10.1038/90541
   ELDREDGE JG, 1994, THESIS BRIGHAM YOUNG
   Ferrucci A. T., 1994, THESIS U CALIFORNIA
   FURBER S, 2006, IEEE S CIRC SYST KOS
   GHANI A, 2005, APPROACHES IMPLEMENT
   GHANI A, 2006, AREA EFFICIENT ARCHI
   Glackin B, 2005, LECT NOTES COMPUT SC, V3512, P552
   Glackin B, 2004, INFORM SCIENCES, V161, P1, DOI 10.1016/j.ins.2003.03.008
   GLACKIN B, 2005, IMPLEMENTATION BIOL
   GLACKIN B, UNPUB IEEE NEURAL NE
   GODDARD NH, 1988, 233 U ROCH COMP SCI
   Graas EL, 2004, NEUROINFORMATICS, V2, P417, DOI 10.1385/NI:2:4:417
   Grassmann C, 1999, Int J Neural Syst, V9, P473, DOI 10.1142/S0129065799000502
   HARTMANN G, 1997, MICRONEURO 97, P130
   Haykin S., 2004, NEURAL NETWORKS COMP, V2, P41
   HEBB D. O., 1949
   HELLMICH HH, 2004, 5 WSEAS INT C NEUR N, P930
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Indiveri G., 1997, Artificial Neural Networks - ICANN '97. 7th International Conference Proceedings, P811, DOI 10.1007/BFb0020254
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Jahnke A., 1996, Proceedings of the Fifth International Conference on Microelectronics for Neural Networks and Fuzzy Systems. MicroNeuro'96, P232, DOI 10.1109/MNNFS.1996.493796
   Johnston S, 2005, LECT NOTES COMPUT SC, V3696, P269, DOI 10.1007/11550822_43
   JOHNSTON SPJ, 2004, P IEEE SMC UK RI CHA, P90
   JORDAN MI, 1994, NEURAL COMPUT, V6, P181, DOI 10.1162/neco.1994.6.2.181
   Le Masson S, 1999, IEEE T BIO-MED ENG, V46, P638, DOI 10.1109/10.764940
   LEWIS MA, 2000, P IEEE INT C ROB AUT
   Ligon WB, 1998, IEEE SYMPOSIUM ON FPGAS FOR CUSTOM COMPUTING MACHINES, PROCEEDINGS, P206, DOI 10.1109/FPGA.1998.707898
   MAAS W, PULSED NEURAL NETWOR
   MARTIN MH, 1994, THESIS U CALIFORNIA
   MAYA S, 2000, LNCS, V1896, P270
   McCulloch Warren S., 1943, BULL MATH BIOPHYS, V5, P115, DOI 10.1007/BF02478259
   MODI S, BEHAV SIMULATION BIO
   NORDSTROM T, 1991, 199116 TULEA LUL U T
   NORDSTROM T, 1995, 199502 TULEA LUL U T
   NORDSTROM T, 1992, J PARALLEL DISTRIBUT, V14
   NORDSTROM T, 1995, THESIS LULEA U TECHN
   Ortigosa EM, 2006, MICROPROCESS MICROSY, V30, P435, DOI 10.1016/j.micpro.2006.03.004
   Panchev C, 2004, NEUROCOMPUTING, V58, P365, DOI 10.1016/j.neucom.2004.01.068
   Panchev C, 2002, LECT NOTES COMPUT SC, V2415, P896
   PANDYA V, 2005, INT C REC COMP FPGA
   PEREZURIBE A, 1999, THESIS SWISS FED I T
   PEREZURIBE A, 1999, THESIS EPFL
   Renaud-Le Masson S, 2004, INFORM SCIENCES, V161, P57, DOI 10.1016/j.ins.2003.03.007
   ROGGEN D, 2003, NASA DOD C EV HARDW
   ROGGEN D, HARDWARE SPIKING NEU
   ROLLS ET, 1995, J NEUROPHYSIOL, V73, P713, DOI 10.1152/jn.1995.73.2.713
   Ros E, 2003, LECT NOTES COMPUT SC, V2687, P145
   Ros E, 2006, IEEE T NEURAL NETWOR, V17, P1050, DOI 10.1109/TNN.2006.875980
   Roth L, 1995, LECT NOTES COMPUT SC, V930, P720
   SCHEMMEL J, 2004, P 2004 INT JOINT C N, P1711
   Schæfer M, 2002, NEUROCOMPUTING, V48, P647, DOI 10.1016/S0925-2312(01)00633-6
   SCHOENAUER T, 2000, P IEEE INT JOINT C N
   SCHOENAUER T, 1998, VIDYNN 98
   Skrbek M., 1999, Neural Network World, V9, P375
   SMITH LS, SPIKING NEURAL SIMUL
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   TAVENIKU M, 1995, THESIS CHALMERS U TE
   UPEGUI A, 2003, IWANN, V1, P136
   UPEGUI A, 2003, ICCI INT C COMP INT
   WOLFF C, 1999, MICRONEURO 99, P324
   Wu QX, 2005, LECT NOTES COMPUT SC, V3610, P420
   WU QX, IN PRESS INT J NEURO
   WULFRAM G, 2002, SPIKING EURON MODELI
   *XIL, 2005, MICROBLAZE PROC REF
   Yao X, 1999, P IEEE, V87, P1423, DOI 10.1109/5.784219
   ZHU J, 2003, 13 INT C FIELD PROGR, P1062
NR 79
TC 131
Z9 133
U1 0
U2 16
PD DEC
PY 2007
VL 71
IS 1-3
BP 13
EP 29
DI 10.1016/j.neucom.2006.11.029
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT J
AU Vogels, TP
   Rajan, K
   Abbott, LF
AF Vogels, TP
   Rajan, K
   Abbott, LF
TI Neural network dynamics
SO ANNUAL REVIEW OF NEUROSCIENCE
SE Annual Review of Neuroscience
DT Review; Book Chapter
DE balance; memory; signal propagation; states; sustained activity
ID SPATIAL WORKING-MEMORY; FIRE NEURONS; SYNCHRONOUS SPIKING; ASYNCHRONOUS
   STATES; RECURRENT NETWORK; SYNAPTIC INPUT; PROPAGATION; MODEL;
   OSCILLATIONS; VARIABILITY
AB Neural network modeling is often concerned with stimulus-driven responses, but most of the activity in the brain is internally generated. Here, we review network models of internally generated activity, focusing on three types of network dynamics: (a) sustained responses to transient stimuli, which provide a model of working memory; (b) oscillatory network activity; and (c) chaotic activity, which models complex patterns of background spiking in cortical and other circuits. We also review propagation of stimulus-driven activity through spontaneously active networks. Exploring these aspects of neural network dynamics is critical for understanding how neural circuits produce cognitive function.
C1 Brandeis Univ, Volen Ctr Complex Syst, Waltham, MA 02454 USA.
   Brandeis Univ, Dept Biol, Waltham, MA 02454 USA.
RP Vogels, TP (corresponding author), Brandeis Univ, Volen Ctr Complex Syst, Waltham, MA 02454 USA.
EM vogels@brandeis.edu
CR ABBOTT LF, 1993, PHYS REV E, V48, P1483, DOI 10.1103/PhysRevE.48.1483
   Abeles M., 1991, CORTICONICS NEURAL C
   Aertsen A, 1996, J PHYSIOLOGY-PARIS, V90, P243, DOI 10.1016/S0928-4257(97)81432-5
   Amit DJ, 1997, CEREB CORTEX, V7, P237, DOI 10.1093/cercor/7.3.237
   Anderson JS, 2000, SCIENCE, V290, P1968, DOI 10.1126/science.290.5498.1968
   Arieli A, 1996, SCIENCE, V273, P1868, DOI 10.1126/science.273.5283.1868
   Aviel Y, 2003, NEURAL COMPUT, V15, P1321, DOI 10.1162/089976603321780290
   Beggs JM, 2004, J NEUROSCI, V24, P5216, DOI 10.1523/JNEUROSCI.0540-04.2004
   Beggs JM, 2003, J NEUROSCI, V23, P11167
   BENYISHAI R, 1995, P NATL ACAD SCI USA, V92, P3844, DOI 10.1073/pnas.92.9.3844
   BenYishai R, 1997, J COMPUT NEUROSCI, V4, P57, DOI 10.1023/A:1008816611284
   Brunel N, 1998, J THEOR BIOL, V195, P87, DOI 10.1006/jtbi.1998.0782
   Brunel N, 1999, NEURAL COMPUT, V11, P1621, DOI 10.1162/089976699300016179
   Brunel N, 2000, J PHYSIOLOGY-PARIS, V94, P445, DOI 10.1016/S0928-4257(00)01084-6
   Burkitt AN, 1999, NEURAL COMPUT, V11, P871, DOI 10.1162/089976699300016485
   BURNS BD, 1976, PROC R SOC SER B-BIO, V194, P211, DOI 10.1098/rspb.1976.0074
   Buzsáki G, 2004, SCIENCE, V304, P1926, DOI 10.1126/science.1099745
   Camperi M, 1998, J COMPUT NEUROSCI, V5, P383, DOI 10.1023/A:1008837311948
   Câteau H, 2001, NEURAL NETWORKS, V14, P675, DOI 10.1016/S0893-6080(01)00065-X
   Chance FS, 2002, NEURON, V35, P773, DOI 10.1016/S0896-6273(02)00820-6
   Compte A, 2000, CEREB CORTEX, V10, P910, DOI 10.1093/cercor/10.9.910
   Dayan P., 2001, THEORETICAL NEUROSCI
   de Carvalho JX, 2000, PHYS REV LETT, V84, P4006, DOI 10.1103/PhysRevLett.84.4006
   DEAN AF, 1981, EXP BRAIN RES, V44, P437
   DESTEXHE A, 1994, PHYS REV E, V50, P1594, DOI 10.1103/PhysRevE.50.1594
   Diesmann M, 1999, NATURE, V402, P529, DOI 10.1038/990101
   ERMENTROUT GB, 1979, J MATH BIOL, V7, P265, DOI 10.1007/BF00275728
   Fusi S, 1999, NEURAL COMPUT, V11, P633, DOI 10.1162/089976699300016601
   Gerstner W, 2000, NEURAL COMPUT, V12, P43, DOI 10.1162/089976600300015899
   Golomb D, 1998, J NEUROPHYSIOL, V79, P1
   GOLOMB D, 1993, PHYS REV E, V48, P4810, DOI 10.1103/PhysRevE.48.4810
   Gray C M, 1994, J Comput Neurosci, V1, P11, DOI 10.1007/BF00962716
   Gutkin BS, 2001, J COMPUT NEUROSCI, V11, P121, DOI 10.1023/A:1012837415096
   Hahnloser RHR, 2003, NEURAL COMPUT, V15, P621, DOI 10.1162/089976603321192103
   Hansel D, 2003, NEURAL COMPUT, V15, P1, DOI 10.1162/089976603321043685
   Hansel D, 2001, PHYS REV LETT, V86, P4175, DOI 10.1103/PhysRevLett.86.4175
   HANSEL D, 2000, METHODS NEURONAL MOD, P499
   Harris TE., 1963, GRUNDLEHREN MATH WIS, DOI 10.1007/978-3-642-51866-9
   HERRMANN M, 1995, NETWORK-COMP NEURAL, V6, P403, DOI 10.1088/0954-898X/6/3/006
   Holter W, 1996, PEDIAT ALLERG IMM-UK, V7, P75, DOI 10.1111/j.1399-3038.1996.tb00110.x
   Laurent G, 1996, TRENDS NEUROSCI, V19, P489, DOI 10.1016/S0166-2236(96)10054-0
   Leopold DA, 2003, CEREB CORTEX, V13, P422, DOI 10.1093/cercor/13.4.422
   Lerchner A, 2004, NEUROCOMPUTING, V58, P935, DOI 10.1016/j.neucom.2004.01.149
   Litvak V, 2003, J NEUROSCI, V23, P3006
   Marder E, 1996, PHYSIOL REV, V76, P687, DOI 10.1152/physrev.1996.76.3.687
   Marsalek PR, 1997, P NATL ACAD SCI USA, V94, P735, DOI 10.1073/pnas.94.2.735
   Mehring C, 2003, BIOL CYBERN, V88, P395, DOI 10.1007/s00422-002-0384-4
   Ott E., 1993, CHAOS DYNAMICAL SYST
   Penttonen Markku, 2003, Thalamus & Related Systems, V2, P145, DOI 10.1017/S1472928803000074
   Prinz AA, 2004, NAT NEUROSCI, V7, P1345, DOI 10.1038/nn1352
   Renart A, 2003, NEURON, V38, P473, DOI 10.1016/S0896-6273(03)00255-1
   Reyes AD, 2003, NAT NEUROSCI, V6, P593, DOI 10.1038/nn1056
   Ricciardi LM., 1977, DIFFUSION PROCESSES
   Rinzel J., 1998, METHODS NEURONAL MOD, P251
   Salinas E, 2002, NEURAL COMPUT, V14, P2111, DOI 10.1162/089976602320264024
   Seung HS, 2000, NEURON, V26, P259, DOI 10.1016/S0896-6273(00)81155-1
   Shadlen Michael N., 1994, Current Opinion in Neurobiology, V4, P569, DOI 10.1016/0959-4388(94)90059-0
   Shriki O, 2003, NEURAL COMPUT, V15, P1809, DOI 10.1162/08997660360675053
   Silberberg G, 2004, J NEUROPHYSIOL, V91, P704, DOI 10.1152/jn.00415.2003
   SOFTKY WR, 1993, J NEUROSCI, V13, P334
   SONG S, 2005, IN PRESS HIGHLY NONR
   Strogatz S. H., 2019, NONLINEAR DYNAMICS C, V2nd
   Tegnér J, 2002, BIOL CYBERN, V87, P471, DOI 10.1007/s00422-002-0363-9
   Traub RD, 1997, J COMPUT NEUROSCI, V4, P141, DOI 10.1023/A:1008839312043
   TRAUB RD, 1989, SCIENCE, V243, P1319, DOI 10.1126/science.2646715
   Traub RD., 1999, FAST OSCILLATIONS CO
   Troyer TW, 1997, NEURAL COMPUT, V9, P971, DOI 10.1162/neco.1997.9.5.971
   TSODYKS M, 1995, NETWORK, V6, P1
   Tuckwell H.C., 1988, INTRO THEORETICAL NE, V2
   USHER M, 1994, NEURAL COMPUT, V6, P795, DOI 10.1162/neco.1994.6.5.795
   van Rossum MCW, 2002, J NEUROSCI, V22, P1956, DOI 10.1523/JNEUROSCI.22-05-01956.2002
   van Vreeswijk C, 1998, NEURAL COMPUT, V10, P1321, DOI 10.1162/089976698300017214
   van Vreeswijk C, 2000, PHYS REV LETT, V84, P5110, DOI 10.1103/PhysRevLett.84.5110
   vanVreeswijk C, 1996, SCIENCE, V274, P1724, DOI 10.1126/science.274.5293.1724
   VOGELS TP, 2004, SOC NEUR
   Wang X, 2003, CEREB CORTEX, V13, P1123, DOI 10.1093/cercor/bhg112
   Wang XJ, 1999, J NEUROSCI, V19, P9587
   White JA, 1998, J COMPUT NEUROSCI, V5, P5, DOI 10.1023/A:1008841325921
   Whittington MA, 2000, INT J PSYCHOPHYSIOL, V38, P315, DOI 10.1016/S0167-8760(00)00173-2
   WILSON HR, 1972, BIOPHYS J, V12, P1, DOI 10.1016/S0006-3495(72)86068-5
   ZAPPERI S, 1995, PHYS REV LETT, V75, P4071, DOI 10.1103/PhysRevLett.75.4071
NR 81
TC 339
Z9 354
U1 3
U2 88
PY 2005
VL 28
BP 357
EP 376
DI 10.1146/annurev.neuro.28.061604.135637
WC Neurosciences
DA 2023-11-11
ER

PT J
AU Kasabov, NK
AF Kasabov, Nikola K.
TI Spiking neural networks for deep learning and knowledge representation:
   Editorial
SO NEURAL NETWORKS
DT Editorial Material
C1 [Kasabov, Nikola K.] Auckland Univ Technol, Auckland, New Zealand.
RP Kasabov, NK (corresponding author), Auckland Univ Technol, Auckland, New Zealand.
EM nik.kasabov@aut.ac.nz
CR [Anonymous], 2007, RETRIEVED
   Benuskova L., 2007, TOP BIOMED ENG
   Bishop C. M., 2006, MACH LEARN
   Bohte Sander M., 2004, Natural Computing, V3, P195, DOI 10.1023/B:NACO.0000027755.02868.60
   Furber SB, 2013, IEEE T COMPUT, V62, P2454, DOI 10.1109/TC.2012.142
   Gerstner W, 1997, P NATL ACAD SCI USA, V94, P12740, DOI 10.1073/pnas.94.24.12740
   Gerstner W, 2012, SCIENCE, V338, P60, DOI 10.1126/science.1227356
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Indiveri G, 2015, P IEEE, V103, P1379, DOI 10.1109/JPROC.2015.2444094
   Kasabov N., 2018, TIME SPACE SPIKING N, P751
   Kasabov NK, 2014, NEURAL NETWORKS, V52, P62, DOI 10.1016/j.neunet.2014.01.006
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
NR 16
TC 4
Z9 4
U1 0
U2 15
PD NOV
PY 2019
VL 119
BP 341
EP 342
DI 10.1016/j.neunet.2019.08.019
WC Computer Science, Artificial Intelligence; Neurosciences
DA 2023-11-11
ER

PT J
AU Yu, M
   Xiang, TT
   Srivatsa, P
   Chu, KTN
   Amornpaisannon, B
   Tavva, Y
   Miriyala, VPK
   Carlson, TE
AF Yu, Miao
   Xiang, Tingting
   Srivatsa, P.
   Chu, Kyle Timothy Ng
   Amornpaisannon, Burin
   Tavva, Yaswanth
   Miriyala, Venkata Pavan Kumar
   Carlson, Trevor E.
TI A TTFS-based energy and utilization efficient neuromorphic CNN
   accelerator
SO FRONTIERS IN NEUROSCIENCE
DT Article
DE artificial neural networks (ANNs); brain-inspired networks; neuromorphic
   hardware; spiking neural networks (SNNs); time-to-first-spike
ID NEURAL-NETWORKS
AB Spiking neural networks (SNNs), which are a form of neuromorphic, brain-inspired AI, have the potential to be a power-efficient alternative to artificial neural networks (ANNs). Spikes that occur in SNN systems, also known as activations, tend to be extremely sparse, and low in number. This minimizes the number of data accesses typically needed for processing. In addition, SNN systems are typically designed to use addition operations which consume much less energy than the typical multiply and accumulate operations used in DNN systems. The vast majority of neuromorphic hardware designs support rate-based SNNs, where the information is encoded by spike rates. Generally, rate-based SNNs can be inefficient as a large number of spikes will be transmitted and processed during inference. One coding scheme that has the potential to improve efficiency is the time-to-first-spike (TTFS) coding, where the information isn't presented through the frequency of spikes, but instead through the relative spike arrival time. In TTFS-based SNNs, each neuron can only spike once during the entire inference process, and this results in high sparsity. The activation sparsity of TTFS-based SNNs is higher than rate-based SNNs, but TTFS-based SNNs have yet to achieve the same accuracy as rate-based SNNs. In this work, we propose two key improvements for TTFS-based SNN systems: (1) a novel optimization algorithm to improve the accuracy of TTFS-based SNNs and (2) a novel hardware accelerator for TTFS-based SNNs that uses a scalable and low-power design. Our work in TTFS coding and training improves the accuracy of TTFS-based SNNs to achieve state-of-the-art results on the MNIST and Fashion-MNIST datasets. Meanwhile, our work reduces the power consumption by at least 2.4x, 25.9x, and 38.4x over the state-of-the-art neuromorphic hardware on MNIST, Fashion-MNIST, and CIFAR10, respectively.
C1 [Yu, Miao; Xiang, Tingting; Amornpaisannon, Burin; Tavva, Yaswanth; Miriyala, Venkata Pavan Kumar; Carlson, Trevor E.] Natl Univ Singapore, Sch Comp, Dept Comp Sci, Singapore, Singapore.
   [Srivatsa, P.] Georgia Inst Technol, Sch Interact Comp, Atlanta, GA USA.
   [Chu, Kyle Timothy Ng] Natl Univ Singapore, Ctr Quantum Technol, Singapore, Singapore.
RP Carlson, TE (corresponding author), Natl Univ Singapore, Sch Comp, Dept Comp Sci, Singapore, Singapore.
EM tcarlson@comp.nus.edu.sg
CR Bouvier M, 2019, ACM J EMERG TECH COM, V15, DOI 10.1145/3304103
   Cattani A, 2015, Arxiv, DOI arXiv:1504.03954
   Chen QY, 2022, IEEE T COMPUT AID D, V41, P5732, DOI 10.1109/TCAD.2022.3158834
   Comsa JM, 2020, INT CONF ACOUST SPEE, P8529, DOI [10.1109/ICASSP40776.2020.9053856, 10.1109/icassp40776.2020.9053856]
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Esser SK, 2016, P NATL ACAD SCI USA, V113, P11441, DOI 10.1073/pnas.1604850113
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   Guo WZ, 2021, FRONT NEUROSCI-SWITZ, V15, DOI [10.3389/fnins.2021.638474, 10.1007/s11704-020-9230-x]
   He KM, 2015, Arxiv, DOI [arXiv:1502.01852, DOI 10.48550/ARXIV.1502.01852]
   Howard A, 2019, Arxiv, DOI [arXiv:1905.02244, DOI 10.48550/ARXIV.1905.02244]
   Khan MM, 2008, IEEE IJCNN, P2849, DOI 10.1109/IJCNN.2008.4634199
   Khoei MA, 2020, 2020 2ND IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE CIRCUITS AND SYSTEMS (AICAS 2020), P256, DOI [10.1109/AICAS48895.2020.9073827, 10.1109/aicas48895.2020.9073827]
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
   Kwon H, 2017, INT SYM PERFORM ANAL, P195, DOI 10.1109/ISPASS.2017.7975291
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lew D, 2022, PROCEEDINGS OF THE 59TH ACM/IEEE DESIGN AUTOMATION CONFERENCE, DAC 2022, P265, DOI 10.1145/3489517.3530457
   Luo T, 2017, IEEE T COMPUT, V66, P73, DOI 10.1109/TC.2016.2574353
   Mayberry M., 2017, INTELS NEW SELF LEAR
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Moreau T, 2019, Arxiv, DOI arXiv:1807.04188
   Mostafa H., 2016, SUPERVISED LEARNING, DOI [10.1109/TNNLS.2017.2726060, DOI 10.1109/TNNLS.2017.2726060]
   Narayanan S, 2020, ANN I S COM, P349, DOI 10.1109/ISCA45697.2020.00038
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Park S, 2020, DES AUT CON, DOI [10.1109/dac18072.2020.9218689, 10.1007/s00779-020-01476-2]
   Rueckauer B, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351295
   Shrestha SB, 2017, NEURAL NETWORKS, V96, P33, DOI 10.1016/j.neunet.2017.08.010
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Smith J. E., 1982, 9th Annual Symposium on Computer Architecture, P112
   Stöckl C, 2021, NAT MACH INTELL, V3, DOI 10.1038/s42256-021-00311-4
   Tan MX, 2019, PR MACH LEARN RES, V97
   Wang B, 2020, DES AUT TEST EUROPE, P240, DOI 10.23919/DATE48585.2020.9116516
   Wu YJ, 2019, AAAI CONF ARTIF INTE, P1311
   Xiao H, 2017, Arxiv, DOI [arXiv:1708.07747, DOI 10.48550/ARXIV.1708.07747]
   Yakopcic C, 2017, IEEE IJCNN, P1696, DOI 10.1109/IJCNN.2017.7966055
   Zhang J, 2022, IEEE T CIRCUITS-II, V69, P2271, DOI 10.1109/TCSII.2021.3137987
NR 35
TC 1
Z9 1
U1 3
U2 3
PD MAY 5
PY 2023
VL 17
AR 1121592
DI 10.3389/fnins.2023.1121592
WC Neurosciences
DA 2023-11-11
ER

PT J
AU Khanday, MA
   Bashir, F
   Khanday, FA
AF Khanday, Mudasir A.
   Bashir, Faisal
   Khanday, Farooq A.
TI Single Germanium MOSFET-Based Low Energy and Controllable Leaky
   Integrate-and-Fire Neuron for Spiking Neural Networks
SO IEEE TRANSACTIONS ON ELECTRON DEVICES
DT Article
DE Neurons; Germanium; Behavioral sciences; Logic gates; Integrated circuit
   modeling; Semiconductor process modeling; Mathematical models; Germanium
   (Ge); leaky integrate-and-fire (LIF) neuron; mosfet; spiking neural
   network (SNN)
ID ELECTRONIC REALIZATION; MODEL
AB In this work, a single transistor based on germanium (Ge) is used to construct a leaky integrate-and-fire (LIF) neuron with significant improvement in energy efficiency, area efficiency, and reduction in cost. Using 2-D calibrated simulation, we validated that Ge-mosfet LIF neuron is able to imitate the neuron behavior accurately. The Ge-mosfet shows low breakdown voltage, high impact ionization coefficient, and sharp breakdown. All these factors are responsible for achieving low energy per spike and higher spiking current. The proposed Ge-mosfet-based spiking LIF neuron needs only 8 pJ/spike of energy as compared to recently reported silicon-based silicon-on-insulator (SOI) mosfet, which needs 45 pJ/spike of energy. The use of gate voltage makes Ge-mosfet LIF neuron firing controllable, which can improve the energy efficiency of the spiking neural network (SNN) by inducing sparse action.
C1 [Khanday, Mudasir A.; Bashir, Faisal; Khanday, Farooq A.] Univ Kashmir, Dept Elect & Instrumentat Technol, Srinagar 190006, India.
RP Khanday, FA (corresponding author), Univ Kashmir, Dept Elect & Instrumentat Technol, Srinagar 190006, India.
EM mudasirmails@gmail.com; faisalbashir4161@gmail.com;
   farooqkhanday@kashmiruniversity.ac.in
CR [Anonymous], 2017, ATL TCAD DEV SIM
   Arthur JV, 2011, IEEE T CIRCUITS-I, V58, P1034, DOI 10.1109/TCSI.2010.2089556
   Chatterjee D, 2019, IEEE ELECTR DEVICE L, V40, P1301, DOI 10.1109/LED.2019.2924259
   Das B, 2018, IEEE ELECTR DEVICE L, V39, P1832, DOI 10.1109/LED.2018.2876684
   Dutta S, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-07418-y
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   Han JW, 2018, IEEE ELECTR DEVICE L, V39, P1457, DOI 10.1109/LED.2018.2856092
   Han JK, 2020, IEEE ELECTR DEVICE L, V41, P208, DOI 10.1109/LED.2019.2958623
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Indiveri G, 2006, IEEE T NEURAL NETWOR, V17, P211, DOI 10.1109/TNN.2005.860850
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Joubert A, 2012, IEEE IJCNN
   Kamal N, 2021, IEEE T ELECTRON DEV, V68, P1633, DOI 10.1109/TED.2021.3061036
   Kant NA, 2017, CIRC SYST SIGNAL PR, V36, P4844, DOI 10.1007/s00034-017-0615-5
   Khanday FA, 2018, IET CIRC DEVICE SYST, V12, P696, DOI 10.1049/iet-cds.2018.5033
   Khanday FA, 2019, IEEE T NEUR NET LEAR, V30, P2108, DOI 10.1109/TNNLS.2018.2877454
   Kibong Moon, 2016, 2016 International Symposium on VLSI Technology, Systems and Application (VLSI-TSA), P1, DOI 10.1109/VLSI-TSA.2016.7480499
   Kim WK, 2021, IEEE ELECTR DEVICE L, V42, P681, DOI 10.1109/LED.2021.3070334
   Kamal AK, 2020, IEEE T ELECTRON DEV, V67, P2600, DOI 10.1109/TED.2020.2985076
   Lashkare S, 2018, IEEE ELECTR DEVICE L, V39, P484, DOI 10.1109/LED.2018.2805822
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Mesaritakis C, 2016, SCI REP-UK, V6, DOI 10.1038/srep39317
   MIKAWA T, 1980, APPL PHYS LETT, V37, P387, DOI 10.1063/1.91932
   Nisar A, 2020, NANOTECHNOLOGY, V31, DOI 10.1088/1361-6528/abadc4
   Park JH, 2011, IEEE ELECTR DEVICE L, V32, P234, DOI 10.1109/LED.2010.2095827
   Pickett MD, 2013, NAT MATER, V12, P114, DOI [10.1038/NMAT3510, 10.1038/nmat3510]
   Pillarisetty R, 2011, NATURE, V479, P324, DOI 10.1038/nature10678
   Ponulak F, 2011, ACTA NEUROBIOL EXP, V71, P409
   Sze S.M., 2006, PHYS SEMICONDUCTOR D, DOI 10.1002/0470068329
   WANG Z, 2018, IEDM
   Wijekoon JHB, 2008, NEURAL NETWORKS, V21, P524, DOI 10.1016/j.neunet.2007.12.037
NR 31
TC 6
Z9 6
U1 1
U2 5
PD AUG
PY 2022
VL 69
IS 8
BP 4265
EP 4270
DI 10.1109/TED.2022.3186274
EA JUL 2022
WC Engineering, Electrical & Electronic; Physics, Applied
DA 2023-11-11
ER

PT J
AU Hu, RH
   Chang, S
   Wang, H
   He, J
   Huang, QJ
AF Hu, Ruihan
   Chang, Sheng
   Wang, Hao
   He, Jin
   Huang, Qijun
TI Efficient Multispike Learning for Spiking Neural Networks Using
   Probability-Modulated Timing Method
SO IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
DT Article
DE Clustering rule connection mechanism (CRCM); probability modulation;
   spiking neural network (SNN); supervised learning
ID EVENT-DRIVEN; ALGORITHM; MODEL; SIMULATOR; NEURONS
AB Error functions are normally based on the distance between output spikes and target spikes in supervised learning algorithms for spiking neural networks (SNNs). Due to the discontinuous nature of the internal state of spiking neuron, it is challenging to ensure that the number of output spikes and target spikes kept identical in multispike learning. This problem is conventionally dealt with by using the smaller of the number of desired spikes and that of actual output spikes in learning. However, if this approach is used, information is lost as some spikes are neglected. In this paper, a probability-modulated timing mechanism is built on the stochastic neurons, where the discontinuous spike patterns are converted to the likelihood of generating the desired output spike trains. By applying this mechanism to a probability-modulated spiking classifier, a probability-modulated SNN (PMSNN) is constructed. In its multilayer and multispike learning structure, more inputs are incorporated and mapped to the target spike trains. A clustering rule connection mechanism is also applied to a reservoir to improve the efficiency of information transmission among synapses, which can map the highly correlated inputs to the adjacent neurons. Results of comparisons between the proposed method and popular the SNN algorithms showed that the PMSNN yields higher efficiency and requires fewer parameters.
C1 [Hu, Ruihan; Chang, Sheng; Wang, Hao; He, Jin; Huang, Qijun] Wuhan Univ, Sch Phys & Technol, Wuhan 430072, Hubei, Peoples R China.
RP Chang, S (corresponding author), Wuhan Univ, Sch Phys & Technol, Wuhan 430072, Hubei, Peoples R China.
EM changsheng@whu.edu.cn
CR Adibi P, 2005, NEUROCOMPUTING, V64, P335, DOI 10.1016/j.neucom.2004.10.111
   [Anonymous], 2015, 2015 INT JOINT C NEU, DOI DOI 10.1109/IJCNN.2015.7280592
   Pérez-Carrasco JA, 2013, IEEE T PATTERN ANAL, V35, P2706, DOI 10.1109/TPAMI.2013.71
   Baravalle R, 2017, PHYSICA A, V486, P986, DOI 10.1016/j.physa.2017.06.016
   Beyeler M, 2014, NEUROINFORMATICS, V12, P435, DOI 10.1007/s12021-014-9220-y
   Blake C.L., UCI REPOSITORY MACHI
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   BUONOMANO DV, 1995, SCIENCE, V267, P1028, DOI 10.1126/science.7863330
   Cachóu A, 2015, NEUROCOMPUTING, V148, P187, DOI 10.1016/j.neucom.2012.11.059
   da Silva AB, 2013, LECT NOTES ARTIF INT, V7894, P183, DOI 10.1007/978-3-642-38658-9_17
   Dora S, 2016, NEUROCOMPUTING, V171, P1216, DOI 10.1016/j.neucom.2015.07.086
   Eurich CW, 2000, NEURAL COMPUT, V12, P1519, DOI 10.1162/089976600300015240
   Gerstner W., 2003, BIOLOGICAL, V87, P404
   Gerstner W., 2002, SPIKING NEURON MODEL, DOI DOI 10.1017/CBO9780511815706
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   Hsieh HY, 2013, IEEE T NEUR NET LEAR, V24, P2063, DOI 10.1109/TNNLS.2013.2271644
   Hu M, 2014, IEEE T NEUR NET LEAR, V25, P1864, DOI 10.1109/TNNLS.2013.2296777
   Kasabov N., 2012, Proceedings of the 2012 6th IEEE International Conference Intelligent Systems (IS), P27, DOI 10.1109/IS.2012.6335110
   Kasabov N, 2013, NEURAL NETWORKS, V41, P188, DOI 10.1016/j.neunet.2012.11.014
   Kasabov NK, 2014, NEURAL NETWORKS, V52, P62, DOI 10.1016/j.neunet.2014.01.006
   Kistler WM, 2002, BIOL CYBERN, V87, P416, DOI 10.1007/s00422-002-0359-5
   Kuntanapreeda S, 1996, IEEE T NEURAL NETWOR, V7, P745, DOI 10.1109/72.501730
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Masquelier T, 2009, NEURAL COMPUT, V21, P1259, DOI 10.1162/neco.2008.06-08-804
   Mathias AC, 2012, NEURAL NETWORKS, V34, P42, DOI 10.1016/j.neunet.2012.06.006
   McKennoch S, 2006, IEEE IJCNN, P3970
   Minkovich K, 2014, IEEE T NEUR NET LEAR, V25, P316, DOI 10.1109/TNNLS.2013.2276056
   Naveros F, 2015, IEEE T NEUR NET LEAR, V26, P1567, DOI 10.1109/TNNLS.2014.2345844
   Orchard G, 2015, FRONT NEUROSCI-SWITZ, V9, DOI [10.3389/fnins.2015.00437, 10.3389/fhins.2015.00437]
   Orchard G, 2015, IEEE T PATTERN ANAL, V37, P2028, DOI 10.1109/TPAMI.2015.2392947
   Perin R, 2011, P NATL ACAD SCI USA, V108, P5419, DOI 10.1073/pnas.1016051108
   Phillips PJ, 2000, IEEE T PATTERN ANAL, V22, P1090, DOI 10.1109/34.879790
   Ranhel J., 2011, Proceedings 2011 IEEE Symposium on Foundations of Computational Intelligence (FOCI 2011), P66, DOI 10.1109/FOCI.2011.5949465
   Roy S., 2015, IEEE T NEURAL NETWOR, V27, P1572
   Schrauwen B., 2004, P P PRORISC WORKSH, P1
   Schreiber S, 2003, NEUROCOMPUTING, V52-4, P925, DOI 10.1016/S0925-2312(02)00838-X
   Serrano-Gotarredona R, 2009, IEEE T NEURAL NETWOR, V20, P1417, DOI 10.1109/TNN.2009.2023653
   Silva SM, 2005, PROCEEDINGS OF THE 2005 INTERNATIONAL CONFERENCE ON NEURAL NETWORKS AND BRAIN, VOLS 1-3, P1354
   Tran TP, 2006, IEEE IJCNN, P2354
   Urbanczik R, 2009, NAT NEUROSCI, V12, P250, DOI 10.1038/nn.2264
   van Rossum MCW, 2001, NEURAL COMPUT, V13, P751, DOI 10.1162/089976601300014321
   VanRullen R, 2005, TRENDS NEUROSCI, V28, P1, DOI 10.1016/j.tins.2004.10.010
   Vazquez R. A., 2004, CUCKOO SEARCH FIREFL, P155
   Vazquez RA, 2015, COMPUT INTEL NEUROSC, V2015, DOI 10.1155/2015/947098
   Wade JJ, 2010, IEEE T NEURAL NETWOR, V21, P1817, DOI 10.1109/TNN.2010.2074212
   Wiggett A. J., 2010, J VIS, V6, P1031
   Wojcik GM, 2004, NEUROCOMPUTING, V58, P245, DOI 10.1016/j.neucom.2004.01.051
   Wu QX, 2006, NEUROCOMPUTING, V69, P1912, DOI 10.1016/j.neucom.2005.11.023
   Wysoski SG, 2006, LECT NOTES COMPUT SC, V4131, P61
   Xin JG, 2001, IEEE IJCNN, P1772, DOI 10.1109/IJCNN.2001.938430
   Xu H, 2015, IEEE T NEUR NET LEAR, V26, P472, DOI 10.1109/TNNLS.2014.2315622
   Xu Y, 2013, NEURAL NETWORKS, V43, P99, DOI 10.1016/j.neunet.2013.02.003
   Yedjour H, 2017, COGN PROCESS, V18, P315, DOI 10.1007/s10339-017-0803-z
   Yilmaz AS, 2009, EXPERT SYST APPL, V36, P9767, DOI 10.1016/j.eswa.2009.02.014
   Yu Q., 2013, PLOS ONE, V8, P65
   Yu Q, 2016, IEEE T NEUR NET LEAR, V27, P621, DOI 10.1109/TNNLS.2015.2416771
   Yu Q, 2014, NEUROCOMPUTING, V138, P3, DOI 10.1016/j.neucom.2013.06.052
   Zhao B, 2015, IEEE T NEUR NET LEAR, V26, P1963, DOI 10.1109/TNNLS.2014.2362542
NR 58
TC 11
Z9 13
U1 1
U2 26
PD JUL
PY 2019
VL 30
IS 7
BP 1984
EP 1997
DI 10.1109/TNNLS.2018.2875471
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Computer Science, Theory & Methods; Engineering,
   Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Liu, ZZ
   Chotibut, T
   Hillar, C
   Lin, SW
AF Liu, Zuozhu
   Chotibut, Thiparat
   Hillar, Christopher
   Lin, Shaowei
GP Assoc Advancement Artificial Intelligence
TI Biologically Plausible Sequence Learning with Spiking Neural Networks
SO THIRTY-FOURTH AAAI CONFERENCE ON ARTIFICIAL INTELLIGENCE, THE
   THIRTY-SECOND INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE
   CONFERENCE AND THE TENTH AAAI SYMPOSIUM ON EDUCATIONAL ADVANCES IN
   ARTIFICIAL INTELLIGENCE
SE AAAI Conference on Artificial Intelligence
DT Proceedings Paper
CT 34th AAAI Conference on Artificial Intelligence / 32nd Innovative
   Applications of Artificial Intelligence Conference / 10th AAAI Symposium
   on Educational Advances in Artificial Intelligence
CY FEB 07-12, 2020
CL New York, NY
AB Motivated by the celebrated discrete-time model of nervous activity outlined by McCulloch and Pitts in 1943, we propose a novel continuous-time model, the McCulloch-Pitts network (MPN), for sequence learning in spiking neural networks. Our model has a local learning rule, such that the synaptic weight updates depend only on the information directly accessible by the synapse. By exploiting asymmetry in the connections between binary neurons, we show that MPN can be trained to robustly memorize multiple spatiotemporal patterns of binary vectors, generalizing the ability of the symmetric Hopfield network to memorize static spatial patterns. In addition, we demonstrate that the model can efficiently learn sequences of binary pictures as well as generative models for experimental neural spike-train data. Our learning rule is consistent with spike-timing-dependent plasticity (STDP), thus providing a theoretical ground for the systematic design of biologically inspired networks with large and robust long-range sequence storage capacity.
C1 [Liu, Zuozhu] Natl Univ Singapore, Dept Stat & Appl Probabil, Singapore, Singapore.
   [Chotibut, Thiparat] Chulalongkorn Univ, Fac Sci, Dept Phys, Bangkok, Thailand.
   [Liu, Zuozhu; Chotibut, Thiparat; Lin, Shaowei] Singapore Univ Technol & Design, Engn Syst & Design, Singapore, Singapore.
   [Hillar, Christopher] Awecom Inc, Whittier, CA USA.
   [Hillar, Christopher] Univ Calif Berkeley, Redwood Ctr Theoret Neurosci, Berkeley, CA 94720 USA.
RP Chotibut, T (corresponding author), Chulalongkorn Univ, Fac Sci, Dept Phys, Bangkok, Thailand.; Chotibut, T (corresponding author), Singapore Univ Technol & Design, Engn Syst & Design, Singapore, Singapore.
EM lcowen.hn@gmail.com; thiparatc@gmail.com; hillarmath@gmail.com;
   shaowei_lin@sutd.edu.sg
CR AMARI SI, 1972, IEEE T COMPUT, VC 21, P1197, DOI 10.1109/T-C.1972.223477
   [Anonymous], 2015, STDP BIOLOGICALLY PL
   Bellec G., 2018, ADV NEURAL INFORM PR
   Bengio Y, 2017, NEURAL COMPUT, V29, P555, DOI 10.1162/NECO_a_00934
   Bi GQ, 2001, ANNU REV NEUROSCI, V24, P139, DOI 10.1146/annurev.neuro.24.1.139
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Brea J, 2013, J NEUROSCI, V33, P9565, DOI 10.1523/JNEUROSCI.4098-12.2013
   Ermentrout G.B., 2010, MATH FDN NEUROSCIENC, V35
   Faisal AA, 2008, NAT REV NEUROSCI, V9, P292, DOI 10.1038/nrn2258
   Gerstner W, 1996, NATURE, V383, P76, DOI 10.1038/383076a0
   Hillar C., 2012, ARXIV12042916
   Hillar CJ, 2018, J MATH NEUROSCI, V8, DOI 10.1186/s13408-017-0056-2
   HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554
   Liu Z., 2018, 32 AAAI C ART INT
   Markram H., 2007, SOC NEUR ABSTR, V21
   McCulloch W. S., 1943, B MATH BIOPHYS, V5, P115, DOI [10.1007/BF02478259, DOI 10.1007/BF02478259]
   Memmesheimer RM, 2014, NEURON, V82, P925, DOI 10.1016/j.neuron.2014.03.026
   Nasser H, 2013, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2013/03/P03006
   Nessler B, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003037
   Pecevski D., 2016, ENEURO0048
   Rezende Danilo J, 2011, ADV NEURAL INFORM PR, P136
   Scellier Benjamin, 2017, FRONTIERS COMPUTATIO
   Sohl-Dickstein Jascha, 2011, P 28 INT C INT C MAC, P905, DOI DOI 10.1103/PHYSREVLETT.107
   Tyrcha J, 2013, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2013/03/P03005
   Zador A, 1998, J NEUROPHYSIOL, V79, P1219, DOI 10.1152/jn.1998.79.3.1219
   Zeng HL, 2013, PHYS REV LETT, V110, DOI 10.1103/PhysRevLett.110.210601
NR 26
TC 2
Z9 2
U1 0
U2 2
PY 2020
VL 34
BP 1316
EP 1323
WC Computer Science, Artificial Intelligence; Computer Science,
   Interdisciplinary Applications; Education, Scientific Disciplines
DA 2023-11-11
ER

PT J
AU Voutsas, K
   Adamy, J
AF Voutsas, Kyriakos
   Adamy, Juergen
TI A biologically inspired spiking neural network for sound source
   lateralization
SO IEEE TRANSACTIONS ON NEURAL NETWORKS
DT Article
DE brain-like systems; computational neuroscience; Neural Network (NN);
   neuronal modeling; spiking neuron; sound source lateralization.
ID SUPERIOR OLIVARY COMPLEX; INTERAURAL TEMPORAL DISPARITIES; TIMING
   DEPENDENT PLASTICITY; HIGH-FREQUENCY NEURONS; INFERIOR COLLICULUS; FIRE
   NEURONS; MODEL; SENSITIVITY; SYNAPSES; VLSI
AB In this paper, a binaural sound source lateralization spiking neural network (NN) will be presented which is inspired by most recent neurophysiological studies on the role of certain nuclei in the superior olivary complex (SOC) and the inferior colliculus (IC). The binaural sound source lateralization neural network (BiSoLaNN) is a spiking NN based on neural mechanisms, utilizing complex neural models, and attempting to simulate certain parts of nuclei of the auditory system in detail. The BiSoLaNN utilizes both excitatory and inhibitory ipsilateral and contralateral influences arrayed in only one delay line originating in the contralateral side to achieve a sharp azimuthal localization. It will be shown that the proposed model can be used both for purposes of understanding the mechanisms of an NN of the auditory system and for sound source lateralization tasks in technical applications, e.g., its use with the Darmstadt robotic head (DRH).
C1 Tech Univ Darmstadt, Control Theory & Robot Lab, D-64283 Darmstadt, Germany.
RP Voutsas, K (corresponding author), Tech Univ Darmstadt, Control Theory & Robot Lab, D-64283 Darmstadt, Germany.
EM kvoutsas@rtr.tu-darmstadt.de
CR Adamy J., 2003, Automatisierungstechnik, V51, P387, DOI 10.1524/auto.51.9.387.22718
   ALBECK Y, 1996, HDB BRAIN THEORY NEU, P891
   Batra R, 1997, J NEUROPHYSIOL, V78, P1237, DOI 10.1152/jn.1997.78.3.1237
   Batra R, 1997, J NEUROPHYSIOL, V78, P1222, DOI 10.1152/jn.1997.78.3.1222
   BHADKAMKAR N, 1993, P IEEE INT C NEUR NE, V3, P1902
   BHADKAMKAR NA, 1994, P 1994 IEEE INT C NE, V3, P1866
   BLAUERT J, 1999, SPATIAL HEARING
   Cai HM, 1998, J ACOUST SOC AM, V103, P475, DOI 10.1121/1.421100
   Cameron K, 2005, IEEE T NEURAL NETWOR, V16, P1626, DOI 10.1109/TNN.2005.852238
   Chau W., 1995, P IEEE 29 AS C SIGN, V2, P1281
   DOUGLAS LO, 2001, MICROSC RES TECHNIQ, V51, P340
   Ehret G., 1997, CENTRAL AUDITORY SYS
   Fitzpatrick DC, 2002, HEARING RES, V168, P79, DOI 10.1016/S0378-5955(02)00359-3
   Gerstner W, 1998, PULSED NEURAL NETWORKS, P353
   GERSTNER W, 2003, HDB BRAIN THEORY NEU, P577
   Indiveri G, 2006, IEEE T NEURAL NETWOR, V17, P211, DOI 10.1109/TNN.2005.860850
   Izák R, 1999, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON MICROELECTRONICS FOR NEURAL, FUZZY AND BIO-INSPIRED SYSTEMS, MICORNEURO'99, P103, DOI 10.1109/MN.1999.758852
   JEFFRESS LA, 1948, J COMP PHYSIOL PSYCH, V61, P468
   Koch C., 1998, METHODS NEURONAL MOD
   Lim C., 1994, P IEEE 28 AS C SIGN, DOI [10.1109/acssc.1994.471484., DOI 10.1109/ACSSC.1994.471484]
   LINDEMANN W, 1985, THESIS TU BOCHUM BOC
   Liu SC, 2004, IEEE T NEURAL NETWOR, V15, P1305, DOI 10.1109/TNN.2004.832725
   MARTIN K, 1995, IEEE P ASSP WORKSH A, P96
   MEDDIS R, 1990, J ACOUST SOC AM, V87, P1813, DOI 10.1121/1.399379
   Mysore SP, 2005, IEEE IJCNN, P2766
   Patterson R., 1988, SPIRAL VOS FINAL R A
   Paulin MG, 2004, IEEE T NEURAL NETWOR, V15, P987, DOI 10.1109/TNN.2004.832814
   Ros E, 2006, IEEE T NEURAL NETWOR, V17, P1050, DOI 10.1109/TNN.2006.875980
   Rowcliffe P, 2006, IEEE T NEURAL NETWOR, V17, P803, DOI 10.1109/TNN.2006.873274
   SHAMMA SA, 1989, J ACOUST SOC AM, V86, P989, DOI 10.1121/1.398734
   SLANEY M, 1993, TR35 APPL COMP
   SLANEY M, 1988, TR13 APPL COMP
   Smith LS, 2004, IEEE T NEURAL NETWOR, V15, P1125, DOI 10.1109/TNN.2004.832831
   Voutsas K, 2005, IEEE T SYST MAN CY B, V35, P12, DOI 10.1109/TSMCB.2004.837751
   Willert V, 2006, IEEE T SYST MAN CY B, V36, P982, DOI 10.1109/TSMCB.2006.872263
   Yin TCT, 2002, SPR HDB AUD, V15, P99
   YIN TCT, 1990, J NEUROPHYSIOL, V64, P465, DOI 10.1152/jn.1990.64.2.465
NR 37
TC 29
Z9 42
U1 1
U2 5
PD NOV
PY 2007
VL 18
IS 6
BP 1785
EP 1799
DI 10.1109/TNN.2007.899623
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Computer Science, Theory & Methods; Engineering,
   Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Hwang, S
   Kim, H
   Kwon, MW
   Park, J
   Park, BG
AF Hwang, Sungmin
   Kim, Hyungjin
   Kwon, Min-Woo
   Park, Jungjin
   Park, Byung-Gook
TI An Online Learning Method Using Spike-Timing Dependent Plasticity for
   Neuromorphic Systems
SO JOURNAL OF NANOSCIENCE AND NANOTECHNOLOGY
DT Article
DE Neuromorphic; Spike-Timing Dependent Plasticity; Gradient Descent;
   Pattern Recognition
ID NEURAL-NETWORKS
AB In this study, we proposed an online learning method using spike-timing dependent plasticity (STDP) whose operation is analogous to gradient descent, the most successful learning algorithm for non-spiking artificial neural networks (ANNs). With a model of a 4-terminal synaptic transistor we previously reported, a single-layer neural network implemented on the cross-point array was simulated by MATLAB to train binary MNIST samples with gradient descent algorithm. In addition, a proposed pulse scheme based on STDP was used to train the same network by applying teaching pulses having positive and negative timing differences with respect to input pulses to the back gate of the synaptic transistors. By comparing the extracted synaptic weight maps from both methods, therefore, the network trained by gradient descent was almost equally reproduced by the proposed method which was performed fully on hardware without computer calculation.
C1 [Hwang, Sungmin; Kwon, Min-Woo; Park, Jungjin; Park, Byung-Gook] Seoul Natl Univ, ISRC, Seoul 151742, South Korea.
   [Hwang, Sungmin; Kwon, Min-Woo; Park, Jungjin; Park, Byung-Gook] Seoul Natl Univ, Dept Elect & Comp Engn, Seoul 151742, South Korea.
   [Kim, Hyungjin] Yeungnam Univ, Dept Elect Engn, 280 Daehak Ro, Gyongsan 38541, South Korea.
RP Park, BG (corresponding author), Seoul Natl Univ, ISRC, Seoul 151742, South Korea.; Park, BG (corresponding author), Seoul Natl Univ, Dept Elect & Comp Engn, Seoul 151742, South Korea.
CR Alibart F, 2013, NAT COMMUN, V4, DOI 10.1038/ncomms3072
   Almási AD, 2016, NEUROCOMPUTING, V174, P31, DOI 10.1016/j.neucom.2015.02.092
   [Anonymous], 2015, NATURE, DOI [10.1038/nature14539, 10.1038/, DOI 10.1038/NATURE14539]
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Kim HU, 2017, NANOTECHNOLOGY, V28, DOI 10.1088/1361-6528/aa67d1
   Kim H, 2016, IEEE ELECTR DEVICE L, V37, P249, DOI 10.1109/LED.2016.2521863
   Kuzum D, 2013, NANOTECHNOLOGY, V24, DOI 10.1088/0957-4484/24/38/382001
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   MEAD C, 1990, P IEEE, V78, P1629, DOI 10.1109/5.58356
   Misra J, 2010, NEUROCOMPUTING, V74, P239, DOI 10.1016/j.neucom.2010.03.021
   Park J, 2017, J SEMICOND TECH SCI, V17, P210, DOI 10.5573/JSTS.2017.17.2.210
   Park J, 2017, IEEE T ELECTRON DEV, V64, P2438, DOI 10.1109/TED.2017.2685519
   Park J, 2016, J NANOSCI NANOTECHNO, V16, P4709, DOI 10.1166/jnn.2016.12234
   Prezioso M, 2015, NATURE, V521, P61, DOI 10.1038/nature14441
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Sonoda K, 2004, IEEE T ELECTRON DEV, V51, P1726, DOI 10.1109/TED.2004.834915
NR 17
TC 1
Z9 1
U1 1
U2 31
PD OCT
PY 2019
VL 19
IS 10
BP 6776
EP 6780
DI 10.1166/jnn.2019.17120
WC Chemistry, Multidisciplinary; Nanoscience & Nanotechnology; Materials
   Science, Multidisciplinary; Physics, Applied; Physics, Condensed Matter
DA 2023-11-11
ER

PT J
AU Li, C
   Ma, L
   Furber, S
AF Li, Chen
   Ma, Lei
   Furber, Steve
TI Quantization Framework for Fast Spiking Neural Networks
SO FRONTIERS IN NEUROSCIENCE
DT Article
DE spiking neural networks; fast spiking neural networks; ANN-to-SNN
   conversion; inference latency; quantization; occasional noise
AB Compared with artificial neural networks (ANNs), spiking neural networks (SNNs) offer additional temporal dynamics with the compromise of lower information transmission rates through the use of spikes. When using an ANN-to-SNN conversion technique there is a direct link between the activation bit precision of the artificial neurons and the time required by the spiking neurons to represent the same bit precision. This implicit link suggests that techniques used to reduce the activation bit precision of ANNs, such as quantization, can help shorten the inference latency of SNNs. However, carrying ANN quantization knowledge over to SNNs is not straightforward, as there are many fundamental differences between them. Here we propose a quantization framework for fast SNNs (QFFS) to overcome these difficulties, providing a method to build SNNs with enhanced latency and reduced loss of accuracy relative to the baseline ANN model. In this framework, we promote the compatibility of ANN information quantization techniques with SNNs, and suppress "occasional noise" to minimize accuracy loss. The resulting SNNs overcome the accuracy degeneration observed previously in SNNs with a limited number of time steps and achieve an accuracy of 70.18% on ImageNet within 8 time steps. This is the first demonstration that SNNs built by ANN-to-SNN conversion can achieve a similar latency to SNNs built by direct training.
C1 [Li, Chen; Furber, Steve] Univ Manchester, Dept Comp Sci, Adv Processor Technol APT Grp, Manchester, England.
   [Ma, Lei] Beijing Acad Artificial Intelligence, Beijing, Peoples R China.
   [Ma, Lei] Peking Univ, Natl Biomed Imaging Ctr, Beijing, Peoples R China.
RP Li, C (corresponding author), Univ Manchester, Dept Comp Sci, Adv Processor Technol APT Grp, Manchester, England.
EM chen.li@manchester.ac.uk
CR Bing Han, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13555, DOI 10.1109/CVPR42600.2020.01357
   Chowdhury S.S., 2021, ARXIV, DOI [10.1109/IJCNN52387.2021.9534111, DOI 10.1109/IJCNN52387.2021.9534111]
   Davison Andrew P, 2008, Front Neuroinform, V2, P11, DOI 10.3389/neuro.11.011.2008
   Deng SK, 2021, Arxiv, DOI arXiv:2103.00476
   Diehl PU, 2015, IEEE IJCNN
   Ding J., 2021, INT C LEARNING REPRE
   Ding JH, 2021, Arxiv, DOI arXiv:2105.11654
   Fang W, 2022, Arxiv, DOI arXiv:2102.04159
   Ho ND, 2021, Arxiv, DOI [arXiv:2008.04509, 10.1109/DAC18074.2021.9586266]
   Hwang S, 2021, FRONT NEUROSCI-SWITZ, V15, DOI 10.3389/fnins.2021.629000
   Esser SK, 2020, Arxiv, DOI arXiv:1902.08153
   Krishnamoorthi R, 2018, Arxiv, DOI arXiv:1806.08342
   Li Yan, 2021, arXiv
   Li Y., 2021, FREE LUNCH ANN EFFIC
   Li Y, 2021, Arxiv, DOI arXiv:2105.12917
   Lu S, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00535
   Lui H.W., 2021, ARXIV
   Mueller E., 2021, 2021 INT JOINT C NEU, P1
   Qin HT, 2020, PATTERN RECOGN, V105, DOI 10.1016/j.patcog.2020.107281
   Rathi N, 2020, Arxiv, DOI arXiv:2008.03658
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Schaefer C. J., 2020, INT C NEUR SYST 2020, DOI [10.1145/3407197.3407203, DOI 10.1145/3407197.3407203]
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Warden P., 2019, TINYML MACHINE LEARN
   Wu JB, 2020, Arxiv, DOI arXiv:2007.01204
NR 25
TC 3
Z9 3
U1 1
U2 12
PD JUL 19
PY 2022
VL 16
AR 918793
DI 10.3389/fnins.2022.918793
WC Neurosciences
DA 2023-11-11
ER

PT J
AU Cocola, J
   Hand, P
   Voroninski, V
AF Cocola, Jorio
   Hand, Paul
   Voroninski, Vladislav
TI No Statistical-Computational Gap in Spiked Matrix Models with Generative
   Network Priors
SO ENTROPY
DT Article
DE spiked matrix models; generative networks; rank-one matrix recovery;
   statistical-computational gap
ID PRINCIPAL-COMPONENTS; SPARSE PCA; SEMIDEFINITE RELAXATIONS; PHASE
   RETRIEVAL; OPTIMAL RATES
AB We provide a non-asymptotic analysis of the spiked Wishart and Wigner matrix models with a generative neural network prior. Spiked random matrices have the form of a rank-one signal plus noise and have been used as models for high dimensional Principal Component Analysis (PCA), community detection and synchronization over groups. Depending on the prior imposed on the spike, these models can display a statistical-computational gap between the information theoretically optimal reconstruction error that can be achieved with unbounded computational resources and the sub-optimal performances of currently known polynomial time algorithms. These gaps are believed to be fundamental, as in the emblematic case of Sparse PCA. In stark contrast to such cases, we show that there is no statistical-computational gap under a generative network prior, in which the spike lies on the range of a generative neural network. Specifically, we analyze a gradient descent method for minimizing a nonlinear least squares objective over the range of an expansive-Gaussian neural network and show that it can recover in polynomial time an estimate of the underlying spike with a rate-optimal sample complexity and dependence on the noise level.
C1 [Cocola, Jorio; Hand, Paul] Northeastern Univ, Dept Math, Boston, MA 02115 USA.
   [Hand, Paul] Northeastern Univ, Khoury Coll Comp Sci, Boston, MA 02115 USA.
   [Voroninski, Vladislav] Helm Ai, Menlo Pk, CA 94025 USA.
RP Cocola, J (corresponding author), Northeastern Univ, Dept Math, Boston, MA 02115 USA.
EM cocola.j@northeastern.edu; p.hand@northeastern.edu; vlad@helm.ai
CR Abbe E, 2014, IEEE T NETW SCI ENG, V1, P10, DOI 10.1109/TNSE.2014.2368716
   Amini AA, 2008, IEEE INT SYMP INFO, P2454, DOI 10.1109/ISIT.2008.4595432
   Arous G.B., P C LEARN THEOR PMLR, P479
   Asim M, 2020, IEEE T COMPUT IMAG, V6, P1493, DOI 10.1109/TCI.2020.3032671
   Aubin B., P ADV NEURAL INF PRO, P8366
   Aubin B., P 1 MATH SCI MACH LE, P55
   Bandeira AS, 2020, INVERSE PROBL, V36, DOI 10.1088/1361-6420/ab7d2c
   Bandeira AS, 2018, PORT MATH, V75, P159, DOI 10.4171/PM/2014
   Berthet Q, 2013, ANN STAT, V41, P1780, DOI 10.1214/13-AOS1127
   Bora A, 2017, PR MACH LEARN RES, V70
   Bresler G., 2019, C LEARN THEOR COLT
   Cai TT, 2016, ANN STAT, V44, P2221, DOI 10.1214/16-AOS1443
   Cai TT, 2013, ANN STAT, V41, P3074, DOI 10.1214/13-AOS1178
   Chi YJ, 2019, IEEE T SIGNAL PROCES, V67, P5239, DOI 10.1109/TSP.2019.2937282
   Clason C., 2017, LECT NOTES
   Cocola J., P ADV NEURAL INFORM, V33
   d'Aspremont A, 2007, SIAM REV, V49, P434, DOI 10.1137/050645506
   Daskalakis C., 2020, ARXIV200604237
   Decelle A, 2011, PHYS REV E, V84, DOI 10.1103/PhysRevE.84.066106
   Deshpande Y., P ADV NEURAL INFORM, P334
   Deshpande Y, 2016, IEEE INT SYMP INFO, P185, DOI 10.1109/ISIT.2016.7541286
   Dhar M, 2018, PR MACH LEARN RES, V80
   Fan J., 2018, ARXIV180806996
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Hand P., 2016, ARXIV161105985
   Hand P., ARXIV200810579
   Hand P., P INT C LEARN REPR N
   Hand P., P ADV NEUR PROC SYST, P11535
   Hand P, 2020, IEEE T INFORM THEORY, V66, P401, DOI 10.1109/TIT.2019.2935447
   Hand P, 2018, ADV NEUR IN, V31
   Heckel R., 2020, ARXIV200503991
   Heckel R., 2018, ARXIV18050885
   Heckel R., 2019, ARXIV191014634
   Javanmard A, 2016, P NATL ACAD SCI USA, V113, pE2218, DOI 10.1073/pnas.1523097113
   Johnstone IM, 2009, J AM STAT ASSOC, V104, P682, DOI 10.1198/jasa.2009.0121
   Johnstone IM, 2001, ANN STAT, V29, P295, DOI 10.1214/aos/1009210544
   Krauthgamer R, 2015, ANN STAT, V43, P1300, DOI 10.1214/15-AOS1310
   Kunisky Dmitriy, 2019, ARXIV190711636
   Lesieur T, 2015, IEEE INT SYMP INFO, P1635, DOI 10.1109/ISIT.2015.7282733
   Li XD, 2013, SIAM J MATH ANAL, V45, P3019, DOI 10.1137/120893707
   Ma FC, 2018, ADV NEUR IN, V31
   MA T., 2015, ADV NEURAL INFORM PR, P1612
   McSherry F, 2001, ANN IEEE SYMP FOUND, P529, DOI 10.1109/SFCS.2001.959929
   Mixon D.G., ARXIV180309319
   Montanari A, 2016, IEEE T INFORM THEORY, V62, P1458, DOI 10.1109/TIT.2015.2457942
   Moore C, 2017, BULL EUR ASSOC THEOR, P26
   Ohlsson H., 2011, ARXIV11116323, DOI [10.3182/20120711-3-BE-2027.00415, DOI 10.3182/20120711-3-BE-2027.00415]
   Oymak S, 2015, IEEE T INFORM THEORY, V61, P2886, DOI 10.1109/TIT.2015.2401574
   Perry A, 2018, COMMUN PUR APPL MATH, V71, P2275, DOI 10.1002/cpa.21750
   Qiu Shuang, 2019, ARXIV190805368
   Richard E., 2014, ADV NEURAL INFORM PR, V27
   Rigollet P., 2013, ARXIV13040828
   Shah V, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P4609, DOI 10.1109/ICASSP.2018.8462233
   Sonderby C. Kaae, ARXIV161004490
   Song G., P ADV NEURAL INF PRO, P15034
   Vershynin R, 2018, CAMBRIDGE SERIES STA, DOI DOI 10.1017/9781108231596
   Vu V. Q., 2012, P 15 INT C ART INT S, P1278
   Wainwright MJ, 2019, CA ST PR MA, P1, DOI 10.1017/9781108627771
   Wang G, 2018, IEEE T SIGNAL PROCES, V66, P479, DOI 10.1109/TSP.2017.2771733
   Xue Y, 2018, NEUROINFORMATICS, V16, P383, DOI 10.1007/s12021-018-9377-x
   Yang G, 2018, IEEE T MED IMAGING, V37, P1310, DOI 10.1109/TMI.2017.2785879
   Yeh RA, 2017, PROC CVPR IEEE, P6882, DOI 10.1109/CVPR.2017.728
   Yuan ZY, 2019, J COMPUT APPL MATH, V355, P162, DOI 10.1016/j.cam.2019.01.009
   Zou H, 2006, J COMPUT GRAPH STAT, V15, P265, DOI 10.1198/106186006X113430
NR 64
TC 2
Z9 2
U1 0
U2 0
PD JAN
PY 2021
VL 23
IS 1
AR 115
DI 10.3390/e23010115
WC Physics, Multidisciplinary
DA 2023-11-11
ER

PT C
AU Sun, QY
   Wu, QX
   Wang, X
   Hou, L
AF Sun, QiYan
   Wu, QingXiang
   Wang, Xuan
   Hou, Lei
BE Huang, DS
   Han, K
TI A Spiking Neural Network for Extraction of Multi-features in Visual
   Processing Pathways
SO ADVANCED INTELLIGENT COMPUTING THEORIES AND APPLICATIONS, ICIC 2015, PT
   III
SE Lecture Notes in Artificial Intelligence
DT Proceedings Paper
CT 11th International Conference on Intelligent Computing (ICIC)
CY AUG 20-23, 2015
CL Fuzhou, PEOPLES R CHINA
DE Spiking neural network; Multi-features extraction; Visual processing
   pathways; Integrate-and-fire neuron model
AB Based on spiking neural network and colour visual processing mechanism, a hierarchical network is proposed to extract multi-features from a colour image. The network is constructed with a conductance-based integrate-and-fire neuron model and a set of receptive fields. Inspired by visual system, an image can be decomposed into multiple visual image channels and processed in hierarchical structures. The firing rate map of each channel is computed and recorded. Finally, multi-features are obtained from firing rate map. Simulation results show that the proposed method is successfully applied to recognize the target with a higher recognition rate compared with some other methods.
C1 [Sun, QiYan; Wu, QingXiang; Wang, Xuan; Hou, Lei] Fujian Normal Univ, Coll Photon & Elect Engn, Key Lab OptoElecron Sci & Technol Med, Minist Educ, Fuzhou 350007, Fujian, Peoples R China.
   [Sun, QiYan] Fujian Agr & Forestry Univ, Fuzhou 350007, Fujian, Peoples R China.
RP Wu, QX (corresponding author), Fujian Normal Univ, Coll Photon & Elect Engn, Key Lab OptoElecron Sci & Technol Med, Minist Educ, Fuzhou 350007, Fujian, Peoples R China.
EM sunqiyan99168@163.com; qxwu@fjnu.edu.cn
CR Borer S, 2002, CGIV'2002: FIRST EUROPEAN CONFERENCE ON COLOUR IN GRAPHICS, IMAGING, AND VISION, CONFERENCE PROCEEDINGS, P187
   Chatterjee S, 2003, NATURE, V426, P668, DOI 10.1038/nature02163
   DEVALOIS RL, 1965, COLD SPRING HARB SYM, V30, P567, DOI 10.1101/SQB.1965.030.01.055
   Gerstner W., 2002, SPIKING NEURON MODEL
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   LIVINGSTONE MS, 1984, J NEUROSCI, V4, P309
   Ratnasingam S., 2013, 2013 INT JOINT C NEU, P1
   Shapley R, 2011, VISION RES, V51, P701, DOI 10.1016/j.visres.2011.02.012
   Wu QX, 2008, NEUROCOMPUTING, V71, P2055, DOI 10.1016/j.neucom.2007.10.020
   Wu QX, 2013, NEUROCOMPUTING, V116, P3, DOI 10.1016/j.neucom.2012.01.046
   Wu QX, 2010, LECT NOTES COMPUT SC, V6215, P49
   Wysoski SG, 2008, NEUROCOMPUTING, V71, P2563, DOI 10.1016/j.neucom.2007.12.038
   [张悦 Zhang Yue], 2014, [高技术通讯, Chinese High Technology Letters], V24, P407
NR 13
TC 0
Z9 0
U1 0
U2 4
PY 2015
VL 9227
BP 275
EP 281
DI 10.1007/978-3-319-22053-6_29
WC Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Computer Science, Interdisciplinary Applications; Robotics
DA 2023-11-11
ER

PT C
AU Sayarkin, KS
   Popov, AV
   Zhilenkov, AA
AF Sayarkin, Konstantin S.
   Popov, Alexey V.
   Zhilenkov, Anton A.
GP IEEE
TI Spiking Neural Network Model MATLAB Implementation Based on Izhikevich
   Mathematical Model for Control Systems
SO PROCEEDINGS OF THE 2018 IEEE CONFERENCE OF RUSSIAN YOUNG RESEARCHERS IN
   ELECTRICAL AND ELECTRONIC ENGINEERING (EICONRUS)
DT Proceedings Paper
CT IEEE Conference of Russian Young Researchers in Electrical and
   Electronic Engineering (ElConRus)
CY JAN 29-FEB 01, 2018
CL Saint Petersburg Electrotechn Univ LETI, RUSSIA
HO Saint Petersburg Electrotechn Univ LETI
DE spiking neural network; neuron; MATLAB; Simulink; robotics
AB In article results of spiking neural network model realization on based on Izhikevich mathematical model in MATLAB environment are considered. It is known that complited mathematical model of a biological nervous cell from the point of view of display of its functionality is the model of Nobel laureates Hodzhkin and Huxley. However, it contains a large number of the differential equations that does it of little use for hardware or program realization. Especially at creation of big scale artificial neural networks. Izhikevich's model is less exacting to computing resources and at the same time rather precisely realizes functionality of biological neuron. A specific question is the problem of discrete hardware realization of this model.
C1 [Sayarkin, Konstantin S.; Popov, Alexey V.] Peter Great St Petersburg Polytech Univ, Inst Comp Sci & Technol, High Sch Cyberphys Syst & Control, St Petersburg, Russia.
   [Zhilenkov, Anton A.] ITMO Univ, Dept Control Syst & Informat, Fac Control Syst & Robot, St Petersburg, Russia.
RP Zhilenkov, AA (corresponding author), ITMO Univ, Dept Control Syst & Informat, Fac Control Syst & Robot, St Petersburg, Russia.
EM zhilenkovanton@gmail.com
CR ABBOTT LF, 1990, LECT NOTES PHYS, V368, P5
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Booij O, 2005, INFORM PROCESS LETT, V95, P552, DOI 10.1016/j.ipl.2005.05.023
   Booij O., 2004, THESIS
   Chernyi S., 2015, TRANSPORT TELECOMMUN, V16
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Karpov A., 2017, 2017 IEEE C RUSS YOU
   Karpov A., 2017, 2017 IEEE C RUSSIAN
   Lisitsa D., 2017, 2017 IEEE C RUSS YOU
   McCulloch Warren S., 1943, BULL MATH BIOPHYS, V5, P115, DOI 10.1007/BF02478259
   Nyrkov A., 2017, ADV SYSTEMS CONTROL, P387
   ROSENBLATT F, 1958, PSYCHOL REV, V65, P386, DOI 10.1037/h0042519
   Zhilenkov A., 2017, Solid State Phenomena, V265, P627, DOI 10.4028/www.scientific.net/SSP.265.627
   Zhilenkov Anton, 2016, Vibroengineering Procedia. 22nd International Conference on Vibroengineering, P17
   Zhilenkov A, 2015, PROCEDIA ENGINEER, V100, P1247, DOI 10.1016/j.proeng.2015.01.490
NR 15
TC 1
Z9 1
U1 0
U2 1
PY 2018
BP 979
EP 982
WC Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Johnson, C
   Roychowdhury, S
   Venayagamoorthy, GK
AF Johnson, C.
   Roychowdhury, S.
   Venayagamoorthy, G. K.
GP IEEE
TI A Reversibility Analysis of Encoding Methods for Spiking Neural Networks
SO 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUL 31-AUG 05, 2011
CL San Jose, CA
ID COMPUTATION
AB There is much excitement surrounding the idea of using spiking neural networks (SNNs) as the next generation of function-approximating neural networks. However, with the unique mechanism of communication (neural spikes) between neurons comes the challenge of transferring real-world data into the network to process. Many different encoding methods have been developed for SNNs, most temporal and some spatial. This paper analyzes three of them (Poisson rate encoding, Gaussian receptor fields, and a dual-neuron n-bit representation) and tests to see if the information is fully transformed into the spiking patterns. An oft-neglected consideration in encoding for SNNs is whether or not the real-world data is even truly being introduced to the network. By testing the reversibility of the encoding methods in this paper, the completeness of the information's presence in the pattern of spikes to serve as an input to an SNN is determined.
C1 [Johnson, C.; Roychowdhury, S.; Venayagamoorthy, G. K.] Missouri S&T, Real Time Power & Intelligent Syst Lab, Rolla, MO 65401 USA.
RP Johnson, C (corresponding author), Missouri S&T, Real Time Power & Intelligent Syst Lab, Rolla, MO 65401 USA.
EM cameron.e.johnson@gmail.com; sr6g7@mst.edu; gkumar@ieee.org
CR Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   Haykin S., 2001, NEURAL NETWORKS COMP
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Jaeger H., 2001, 148 GMD, V148, P1
   Johnson C., 2010, INT JOINT C NEUR NET, P1800
   Johnson C, 2009, NEURAL NETWORKS, V22, P833, DOI 10.1016/j.neunet.2009.06.033
   Ma WJ, 2010, VISION RES, V50, P2308, DOI 10.1016/j.visres.2010.08.035
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Rojas R., 1996, NEURAL NETWORKS SYST, P336
   Ross S. M., 1995, STOCHASTIC PROCESSES, P59
   Rowcliffe P, 2008, IEEE T NEURAL NETWOR, V19, P1626, DOI 10.1109/TNN.2008.2000999
   Zhang XJ, 2009, NEURAL COMPUT, V21, P3079, DOI 10.1162/neco.2009.06-08-807
NR 13
TC 6
Z9 6
U1 0
U2 1
PY 2011
BP 1802
EP 1809
WC Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Computer Science, Theory & Methods; Engineering, Electrical &
   Electronic
DA 2023-11-11
ER

PT C
AU Schmidgall, S
   Hays, J
AF Schmidgall, Samuel
   Hays, Joe
GP ACM
TI Stable Lifelong Learning: Spiking neurons as a solution to instability
   in plastic neural networks
SO PROCEEDINGS OF THE 2022 ANNUAL NEURO-INSPIRED COMPUTATIONAL ELEMENTS
   CONFERENCE (NICE 2022)
DT Proceedings Paper
CT Annual Neuro-Inspired Computational Elements Conference (NICE)
CY MAR 28-APR 01, 2022
CL ELECTR NETWORK
DE spiking neural networks; synaptic plasticity; plastic neural networks;
   lifelong learning; robotic learning
ID SYNAPTIC PLASTICITY; MEMORY
AB Synaptic plasticity poses itself as a powerful method of self-regulated unsupervised learning in neural networks. A recent resurgence of interest has developed in utilizing Artificial Neural Networks (ANNs) together with synaptic plasticity for intra-lifetime learning. Plasticity has been shown to improve the learning capabilities of these networks in generalizing to novel environmental circumstances. However, the long-term stability of these trained networks has yet to be examined. This work demonstrates that utilizing plasticity together with ANNs leads to instability beyond the pre-specified lifespan used during training. This instability can lead to the dramatic decline of reward seeking behavior, or quickly lead to reaching environment terminal states. This behavior is shown to hold consistent for several plasticity rules on two different environments across many training time-horizons: a cart-pole balancing problem and a quadrupedal locomotion problem. We present a solution to this instability through the use of spiking neurons.
C1 [Schmidgall, Samuel; Hays, Joe] US Naval Res Lab, Washington, DC 20375 USA.
RP Schmidgall, S (corresponding author), US Naval Res Lab, Washington, DC 20375 USA.
EM samuel.schmidgall@nrl.navy.mil; joe.hays@nrl.navy.mil
CR Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Citri A, 2008, NEUROPSYCHOPHARMACOL, V33, P18, DOI 10.1038/sj.npp.1301559
   Schuman CD, 2017, Arxiv, DOI arXiv:1705.06963
   FUORTES MGF, 1962, J GEN PHYSIOL, V45, P1163, DOI 10.1085/jgp.45.6.1163
   Jordan J, 2021, Arxiv, DOI arXiv:2005.14149
   Liu X, 2012, NATURE, V484, P381, DOI 10.1038/nature11028
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   Martin SJ, 2000, ANNU REV NEUROSCI, V23, P649, DOI 10.1146/annurev.neuro.23.1.649
   Miconi T, 2018, Arxiv, DOI arXiv:1804.02464
   Miconi T, 2020, Arxiv, DOI [arXiv:2002.10585, DOI 10.48550/ARXIV.2002.10585]
   Munakata Y, 2004, DEVELOPMENTAL SCI, V7, P141, DOI 10.1111/j.1467-7687.2004.00331.x
   Najarro E, 2022, Arxiv, DOI arXiv:2007.02686
   Neves G, 2008, NAT REV NEUROSCI, V9, P65, DOI 10.1038/nrn2303
   OJA E, 1982, J MATH BIOL, V15, P267, DOI 10.1007/BF00275687
   Pennartz CMA, 1997, NEUROSCIENCE, V81, P303, DOI 10.1016/S0306-4522(97)00118-8
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Salimans T, 2017, Arxiv, DOI arXiv:1703.03864
   Schmidgall Samuel, 2020, GECCO'20. Proceedings of the 2020 Genetic and Evolutionary Computation Conference Companion, P89, DOI 10.1145/3377929.3389901
   Schmidgall S., 2020, P 2020 GENETIC EVOLU, P89, DOI 10.1145/3377929.3389901
   Schmidgall S, 2021, FRONT NEUROROBOTICS, V15, DOI 10.3389/fnbot.2021.629210
   Soltoggio A, 2018, NEURAL NETWORKS, V108, P48, DOI 10.1016/j.neunet.2018.07.013
   Soltoggio A, 2007, IEEE C EVOL COMPUTAT, P2471, DOI 10.1109/CEC.2007.4424781
   Südhof TC, 2018, NEURON, V100, P276, DOI 10.1016/j.neuron.2018.09.040
   Tam S, 2020, J NEUROSCI, V40, P4363, DOI 10.1523/JNEUROSCI.2647-19.2020
   Zucker RS, 2002, ANNU REV PHYSIOL, V64, P355, DOI 10.1146/annurev.physiol.64.092501.114547
NR 25
TC 0
Z9 0
U1 0
U2 1
PY 2022
BP 1
EP 7
DI 10.1145/3517343.3517345
WC Computer Science, Artificial Intelligence; Computer Science,
   Cybernetics; Computer Science, Theory & Methods
DA 2023-11-11
ER

PT C
AU Min, JW
   Kim, J
AF Min, Jeong Woo
   Kim, Jaeha
GP IEEE
TI XSNN: a System-Level Simulator for Spiking Neural Network with Neuron
   Circuits and Synapse Devices
SO 2022 19TH INTERNATIONAL SOC DESIGN CONFERENCE (ISOCC)
SE International SoC Design Conference
DT Proceedings Paper
CT 19th International SoC Design Conference (ISOCC) - SoC Technology
   Towards a New Era of Innovation
CY OCT 19-22, 2022
CL Gangwon Do, SOUTH KOREA
DE spiking neural network (SNN); modeling; simulation
AB We propose a system-level simulation framework called XSNN that can accurately and efficiently predict the accuracy performance of spiking neural networks (SNN), reflecting the ideal and non-ideal characteristics of the analog/mixed-signal neuron circuits and synapse devices. The neurons and synapses are modeled with XMODEL primitives, and a Python script can automatically construct a system-level model of a chosen SNN architecture and accompanying simulation testbenches in SystemVerilog. The SNN model of a 100-10-10 fully-connected network with MNIST dataset achieves a 52.8x speed-up with an classification accuracy error of less than 2% compared to the equivalent HSPICE simulation.
C1 [Min, Jeong Woo; Kim, Jaeha] Seoul Natl Univ, Dept Elect & Comp Engn, Seoul, South Korea.
RP Min, JW (corresponding author), Seoul Natl Univ, Dept Elect & Comp Engn, Seoul, South Korea.
EM jwmin@mics.snu.ac.kr; jaeha@snu.ac.kr
CR Asghshahr MS, 2022, PROC INST CIV ENG-BR, V175, P228, DOI 10.1680/jbren.20.00034
   Diehl PU, 2015, IEEE IJCNN
   Oh S, 2022, IEEE ACCESS, V10, P24444, DOI 10.1109/ACCESS.2022.3149577
   Park J, 2017, IEEE T ELECTRON DEV, V64, P2438, DOI 10.1109/TED.2017.2685519
   Scientific Analog Inc., 2021, XMODEL REF MAN
   Wang W, 2020, ISCIENCE, V23, DOI 10.1016/j.isci.2020.101809
NR 6
TC 0
Z9 0
U1 0
U2 0
PY 2022
BP 390
EP 391
DI 10.1109/ISOCC56007.2022.10031532
WC Computer Science, Hardware & Architecture; Computer Science, Theory &
   Methods; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Dominguez-Morales, JP
   Rios-Navarro, A
   Gutierrez-Galan, D
   Tapiador-Morales, R
   Jimenez-Fernandez, A
   Cerezuela-Escudero, E
   Dominguez-Morales, M
   Linares-Barranco, A
AF Dominguez-Morales, J. P.
   Rios-Navarro, A.
   Gutierrez-Galan, D.
   Tapiador-Morales, R.
   Jimenez-Fernandez, A.
   Cerezuela-Escudero, E.
   Dominguez-Morales, M.
   Linares-Barranco, A.
GP IEEE
TI Live Demonstration - Multilayer spiking neural network for audio samples
   classification using SpiNNaker
SO 2017 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS (ISCAS)
SE IEEE International Symposium on Circuits and Systems
DT Proceedings Paper
CT IEEE International Symposium on Circuits and Systems (ISCAS)
CY MAY 28-31, 2017
CL Baltimore, MD
AB In this demonstration we present a spiking neural network architecture for audio samples classification using SpiNNaker. The network consists of different leaky integrate-and-fire neuron layers. The connections between them are trained using firing rate based algorithms. Tests use sets of pure tones with frequencies that range from 130.813 to 1396.91 Hz. Audio signals coming from the computer are converted to spikes using a Neuromorphic Auditory Sensor and, after that, this information is sent to the SpiNNaker board through a PCB that translates from AER to 2-of-7 protocol. The classification output obtained in the spiking neural network deployed on SpiNNaker is then shown in the computer screen. Different levels of random noise are added to the original audio signals in order to test the robustness of the classification system.
C1 [Dominguez-Morales, J. P.] Univ Seville, Dept Comp Architecture & Technol, Seville, Spain.
   ETS Ingn Informat, Avd Reina Mercedes S-N, Seville, Spain.
RP Dominguez-Morales, JP (corresponding author), Univ Seville, Dept Comp Architecture & Technol, Seville, Spain.
EM jpdominguez@atc.us.es
CR Jimenez-Fernandez A., IEEE T NEURAL NETWOR, P1
   Dominguez-Morales JP, 2016, LECT NOTES COMPUT SC, V9886, P45, DOI 10.1007/978-3-319-44778-0_6
NR 2
TC 1
Z9 1
U1 0
U2 0
PY 2017
BP 625
EP 625
WC Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Liu, Y
   Chen, LJ
   Chen, JW
   Chen, QH
   Fang, FK
AF Liu, Yan
   Chen, Liujun
   Chen, Jiawei
   Chen, Qinghua
   Fang, Fukang
BE Yu, L
   Lai, KK
   Mishra, SK
TI Dynamic Neural Mechanisms for Recognizing Spike Trains
SO INTERNATIONAL JOINT CONFERENCE ON COMPUTATIONAL SCIENCES AND
   OPTIMIZATION, VOL 1, PROCEEDINGS
DT Proceedings Paper
CT 2nd International Joint Conference on Computational Sciences and
   Optimization (CSO)
CY APR 24-26, 2009
CL Sanya, PEOPLES R CHINA
AB Dynamic neural networks are designed to discuss how the dynamic mechanisms in the neurons and synapses work in recognizing interspike intervals (ISIs). The threshold integration of post-synaptic membrane potentials, the refractory period of neurons, together with the spike-time-dependent plasticity (STDP) learning rule are discussed. Based on these dynamic mechanisms, the input inter-spike interval sequences are decomposed into isolated spikes. The synaptic delay times modulated by STDP learning rule is the key mechanism in the ISIs recognition, based on which the ISIs are learned and saved in the delay times. After learning, the neural networks could recognize whether different input sequences include the same consecutive ISIs.
C1 [Liu, Yan; Chen, Liujun; Chen, Jiawei; Chen, Qinghua] Beijing Normal Univ, Sch Management, Dept Syst Sci, Beijing 100875, Peoples R China.
   [Fang, Fukang] Beijing Normal Univ, State Key Lab Cognit Neurosci & Learning, Beijing 100875, Peoples R China.
RP Liu, Y (corresponding author), Beijing Normal Univ, Sch Management, Dept Syst Sci, Beijing 100875, Peoples R China.
EM bnuliuyan@bnu.edu.cn; fkfang@bnu.edu.cn
CR Abarbanel HDI, 2006, PHYS REV LETT, V96, DOI 10.1103/PhysRevLett.96.148104
   Abbott LF, 2004, NATURE, V431, P796, DOI 10.1038/nature03010
   [Anonymous], 2010, DYNAMICAL SYSTEMS NE
   Bi GQ, 2001, ANNU REV NEUROSCI, V24, P139, DOI 10.1146/annurev.neuro.24.1.139
   BUONOMANO DV, 1995, SCIENCE, V267, P1028, DOI 10.1126/science.7863330
   Hebb D. O., 1949, ORG BEHAV
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Jin DZ, 2004, PHYS REV E, V69, DOI 10.1103/PhysRevE.69.021905
   Liaw JS, 1999, NEUROCOMPUTING, V26-7, P199, DOI 10.1016/S0925-2312(99)00063-6
   Strogatz SH, 2001, NATURE, V410, P268, DOI 10.1038/35065725
NR 10
TC 0
Z9 0
U1 2
U2 3
PY 2009
BP 584
EP +
DI 10.1109/CSO.2009.173
WC Computer Science, Interdisciplinary Applications; Computer Science,
   Theory & Methods; Operations Research & Management Science
DA 2023-11-11
ER

PT J
AU Zhao, DC
   Zeng, Y
   Li, Y
AF Zhao, Dongcheng
   Zeng, Yi
   Li, Yang
TI BackEISNN: A deep spiking neural network with adaptive self-feedback and
   balanced excitatory-inhibitory neurons
SO NEURAL NETWORKS
DT Article
DE Spiking neural networks; Adaptive self-feedback connections; Dynamically
   balanced; excitatory-inhibitory neurons
AB Spiking neural networks (SNNs) transmit information through discrete spikes that perform well in processing spatial-temporal information. Owing to their nondifferentiable characteristic, difficulties persist in designing SNNs that deliver good performance. SNNs trained with backpropagation have recently exhibited impressive performance by using gradient approximation. However, their performance on complex tasks remains significantly inferior to that of deep neural networks. By taking inspiration from autapses in the brain that connect spiking neurons with a self-feedback connection, we apply adaptive time-delayed self-feedback to the membrane potential to regulate the precision of the spikes. We also strike a balance between the excitatory and inhibitory mechanisms of neurons to dynamically control the output of spiking neurons. By combining these two mechanisms, we propose a deep SNN with adaptive self-feedback and balanced excitatory and inhibitory neurons (BackEISNN). The results of experiments on several standard datasets show that the two modules not only accelerate the convergence of the network but also increase its accuracy. Our model achieved state-of-the-art performance on the MNIST, Fashion-MNIST, and N-MNIST datasets. The proposed BackEISNN also achieved remarkably good performance on the CIFAR10 dataset while using a relatively light structure that competes against state-of-the-art SNNs. (C) 2022 The Author( s). Published by Elsevier Ltd.
C1 [Zhao, Dongcheng; Zeng, Yi; Li, Yang] Chinese Acad Sci, Inst Automat, Res Ctr Brain Inspired Intelligence, Beijing, Peoples R China.
   [Zeng, Yi; Li, Yang] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing, Peoples R China.
   [Zeng, Yi] Chinese Acad Sci, Ctr Excellence Brain Sci & Intelligence Technol, Shanghai, Peoples R China.
   [Zeng, Yi] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing, Peoples R China.
RP Zeng, Y (corresponding author), Chinese Acad Sci, Inst Automat, Beijing, Peoples R China.
EM zhaodongcheng2016@ia.ac.cn; yi.zeng@ia.ac.cn; liyang2019@ia.ac.cn
CR Amodei D, 2016, PR MACH LEARN RES, V48
   [Anonymous], 2019, INT C LEARN REPR
   Bellec G, 2018, ADV NEUR IN, V31
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Bohte S. M., 2000, 8th European Symposium on Artificial Neural Networks. ESANN"2000. Proceedings, P419
   Burges, 2010, MNIST HANDWRITTEN DI, DOI DOI 10.1561/2400000035
   Cheng X, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1519
   Collobert R., 2008, P 25 INT C MACH LEAR, P160, DOI DOI 10.1145/1390156.1390177
   Dehghani N, 2016, SCI REP-UK, V6, DOI 10.1038/srep23176
   Diehl PU, 2015, IEEE IJCNN
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Falez Pierre, 2019, 2019 INT JOINT C NEU, P1
   Felleman DJ, 1991, CEREB CORTEX, V1, P1, DOI 10.1093/cercor/1.1.1
   Ikeda K, 2006, CURR BIOL, V16, pR308, DOI 10.1016/j.cub.2006.03.085
   Jin Yingyezhe, 2018, P 32 INT C NEUR INF
   Kingma D. P., 2014, ARXIV, DOI DOI 10.48550/ARXIV.1412.6980
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Machingal P, 2020, IEEE IJCNN, DOI 10.1109/ijcnn48605.2020.9207620
   OConnor P., 2016, DEEP SPIKING NETWORK
   Orchard G, 2015, FRONT NEUROSCI-SWITZ, V9, DOI [10.3389/fnins.2015.00437, 10.3389/fhins.2015.00437]
   Pan ZH, 2019, IEEE IJCNN
   Paszke A, 2019, ADV NEUR IN, V32
   Rubin R, 2017, P NATL ACAD SCI USA, V114, pE9366, DOI 10.1073/pnas.1705841114
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Shrestha SB, 2018, ADV NEUR IN, V31
   Sporns O, 2004, NEUROINFORMATICS, V2, P145, DOI 10.1385/NI:2:2:145
   Tavanaei A, 2017, Arxiv, DOI arXiv:1611.03000
   Tavanaei A, 2017, IEEE IJCNN, P2023, DOI 10.1109/IJCNN.2017.7966099
   Wang CN, 2017, COMPLEXITY, DOI 10.1155/2017/5436737
   Wang Y., 2018, IEEE T NEUR NET LEAR
   Wu JY, 2019, 2019 5TH INTERNATIONAL CONFERENCE ON EVENT-BASED CONTROL, COMMUNICATION, AND SIGNAL PROCESSING (EBCCSP), DOI 10.1109/ebccsp.2019.8836892
   Wu YJ, 2019, AAAI CONF ARTIF INTE, P1311
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Xiao H, 2017, Arxiv, DOI [arXiv:1708.07747, DOI 10.48550/ARXIV.1708.07747]
   Xu Q, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1646
   Yin LP, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-07317-4
   Zhang TL, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1653
   Zhang TL, 2018, AAAI CONF ARTIF INTE, P620
   Zhang W, 2020, ADV NEURAL INFORM PR, V33, P12022, DOI DOI 10.48550/ARXIV.2002.10085
   Zhang WR, 2019, ADV NEUR IN, V32
   Zhao DC, 2020, FRONT COMPUT NEUROSC, V14, DOI 10.3389/fncom.2020.576841
   Zhu J, 2011, PLOS BIOL, V9, DOI 10.1371/journal.pbio.1001032
NR 44
TC 3
Z9 4
U1 4
U2 17
PD OCT
PY 2022
VL 154
BP 68
EP 77
DI 10.1016/j.neunet.2022.06.036
EA JUL 2022
WC Computer Science, Artificial Intelligence; Neurosciences
DA 2023-11-11
ER

PT C
AU Bezugam, SS
   Shaban, A
   Suri, M
AF Bezugam, Sai Sukruth
   Shaban, Ahmed
   Suri, Manan
GP IEEE
TI Neuromorphic Recurrent Spiking Neural Networks for EMG Gesture
   Classification and Low Power Implementation on Loihi
SO 2023 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, ISCAS
SE IEEE International Symposium on Circuits and Systems
DT Proceedings Paper
CT 56th IEEE International Symposium on Circuits and Systems (ISCAS)
CY MAY 21-25, 2023
CL Monterey, CA
DE Spiking Neural Network; Neuromorphic hardware; RSNN; LOIHI; EMG; Gesture
   Recognition
AB In this work, we show an efficient Electromyograph (EMG) gesture recognition using Double Exponential Adaptive Threshold (DEXAT) neuron based Recurrent Spiking Neural Network (RSNN). Our network achieves a classification accuracy of 90% while using lesser number of neurons compared to the best reported prior art on Roshambo EMG dataset. Further, to illustrate the benefits of dedicated neuromorphic hardware, we show hardware implementation of DEXAT neuron using multi-compartment methodology on Intel's neuromorphic Loihi chip. RSNN implementation on Loihi (Nahuku 32) achieves significant energy/latency benefits of similar to 983X/19X compared to GPU for batch size = 50.
C1 [Bezugam, Sai Sukruth; Shaban, Ahmed; Suri, Manan] Indian Inst Technol Delhi, Dept Elect Engn, New Delhi, India.
RP Suri, M (corresponding author), Indian Inst Technol Delhi, Dept Elect Engn, New Delhi, India.
EM manansuri@ee.iitd.ac.in
CR Behrenbeck J, 2019, J NEURAL ENG, V16, DOI 10.1088/1741-2552/aafabc
   Bellec G, 2018, Arxiv, DOI arXiv:1803.09574
   Davies M, 2021, P IEEE, V109, P911, DOI 10.1109/JPROC.2021.3067593
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Donati Elisa, 2019, Zenodo, DOI 10.5281/ZENODO.3194792
   Donati E, 2019, IEEE T BIOMED CIRC S, V13, P795, DOI 10.1109/TBCAS.2019.2925454
   Garg N., 2021, ARXIV
   Intel, INT POW GADG WIND
   Ma YQ, 2020, IEEE J EM SEL TOP C, V10, P578, DOI 10.1109/JETCAS.2020.3037951
   Ma YQ, 2020, 2020 2ND IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE CIRCUITS AND SYSTEMS (AICAS 2020), P69, DOI [10.1109/aicas48895.2020.9073810, 10.1109/AICAS48895.2020.9073810]
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   NVIDIA, NVID SYST MAN INT PR
   Peng L, 2015, INT C ADV MECH SYST, P365, DOI 10.1109/ICAMechS.2015.7287090
   Plank P. J., 2018, THESIS GRAZ U TECHNO
   Rao AR, 2021, Arxiv, DOI arXiv:2107.03992
   Shaban A, 2021, NAT COMMUN, V12, DOI 10.1038/s41467-021-24427-8
NR 16
TC 0
Z9 0
U1 1
U2 1
PY 2023
DI 10.1109/ISCAS46773.2023.10181510
WC Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Xiang, SY
   Zhang, YH
   Gong, JK
   Guo, XX
   Lin, L
   Hao, Y
AF Xiang, Shuiying
   Zhang, Yahui
   Gong, Junkai
   Guo, Xingxing
   Lin, Lin
   Hao, Yue
TI STDP-Based Unsupervised Spike Pattern Learning in a Photonic Spiking
   Neural Network With VCSELs and VCSOAs
SO IEEE JOURNAL OF SELECTED TOPICS IN QUANTUM ELECTRONICS
DT Article
DE Photonic spiking neural network; vertical-cavity surface-emitting
   lasers; vertical-cavity semiconductor optical amplifiers; spike timing
   dependent plasticity; unsupervised spike pattern learning
ID TIMING-DEPENDENT PLASTICITY; FABRY-PEROT; NEURONS; IMPLEMENTATION;
   POLARIZATION; SPINNAKER; DYNAMICS; PARALLEL; SUBJECT; SYSTEM
AB We propose a photonic spiking neural network (SNN) consisting of photonic spiking neurons based on vertical-cavity surface-emitting lasers (VCSELs). The photonic spike timing dependent plasticity (STDP) is implemented in a vertical-cavity semi-conductor optical amplifier (VCSOA). A versatile computational model of the photonic SNN is presented based on the rate equation models. Through numerical simulation, a spike pattern learning and recognition task is performed based on the photonic STDP. The results show that the post-synaptic spike timing (PST) is eventually converged iteratively to the first spike timing of the input spike pattern via unsupervised learning. Additionally, the convergence rate of the PST can be accelerated for a photonic SNN with more pre-synaptic neurons. The effects of VCSOA parameters on the convergence performance of the unsupervised spike learning are also considered. To the hest of our knowledge, such a versatile computational model of photonic SNN for unsupervised learning and recognition of arbitrary spike pattern has not yet been reported, which would contribute one step forward toward numerical implementation of a large-scale energy-efficient photonic SNN, and hence is interesting for neuromorphic photonic systems and spiking information processing.
C1 [Xiang, Shuiying] Xidian Univ, Sch Microelect, State Key Lab Integrated Serv Networks, Xian 710071, Shaanxi, Peoples R China.
   [Xiang, Shuiying; Hao, Yue] Xidian Univ, Sch Microelect, State Key Discipline Lab Wide Bandgap Semicond Te, Xian 710071, Shaanxi, Peoples R China.
   [Zhang, Yahui; Gong, Junkai; Guo, Xingxing; Lin, Lin] Xidian Univ, State Key Lab Integrated Serv Networks, Xian 710071, Shaanxi, Peoples R China.
RP Xiang, SY (corresponding author), Xidian Univ, Sch Microelect, State Key Lab Integrated Serv Networks, Xian 710071, Shaanxi, Peoples R China.; Xiang, SY (corresponding author), Xidian Univ, Sch Microelect, State Key Discipline Lab Wide Bandgap Semicond Te, Xian 710071, Shaanxi, Peoples R China.
EM jxxsy@126.com; 18332551054@163.com; gjk01121014@163.com;
   xidiangxx@126.com; linlin@mail.xidian.edu.cn; yhao@xidian.edu.cn
CR Abbott LF, 2000, NAT NEUROSCI, V3, P1178, DOI 10.1038/81453
   Adams M. J., 1985, IEE Proceedings J (Optoelectronics), V132, P58, DOI 10.1049/ip-j.1985.0012
   Alibart F, 2013, NAT COMMUN, V4, DOI 10.1038/ncomms3072
   Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Bi GQ, 2001, ANNU REV NEUROSCI, V24, P139, DOI 10.1146/annurev.neuro.24.1.139
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Björlin ES, 2001, IEEE J QUANTUM ELECT, V37, P274, DOI 10.1109/3.903078
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Boybat I, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-04933-y
   Burr GW, 2015, IEEE T ELECTRON DEV, V62, P3498, DOI 10.1109/TED.2015.2439635
   Cheng ZG, 2017, SCI ADV, V3, DOI 10.1126/sciadv.1700160
   Coomans W, 2011, PHYS REV E, V84, DOI 10.1103/PhysRevE.84.036209
   Dai YT, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-017-02527-8
   Deng T, 2018, IEEE ACCESS, V6, P67951, DOI 10.1109/ACCESS.2018.2878940
   Deng T, 2017, IEEE J SEL TOP QUANT, V23, DOI 10.1109/JSTQE.2017.2685140
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Ferré P, 2018, FRONT COMPUT NEUROSC, V12, DOI 10.3389/fncom.2018.00024
   Fok MP, 2013, OPT LETT, V38, P419, DOI 10.1364/OL.38.000419
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   Gauss V, 2011, OPT COMMUN, V284, P2345, DOI 10.1016/j.optcom.2011.01.010
   Gautrais J, 1998, BIOSYSTEMS, V48, P57, DOI 10.1016/S0303-2647(98)00050-1
   Gholipour B, 2015, ADV OPT MATER, V3, P635, DOI 10.1002/adom.201400472
   Gollisch T, 2008, SCIENCE, V319, P1108, DOI 10.1126/science.1149639
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Guyonneau R, 2005, NEURAL COMPUT, V17, P859, DOI 10.1162/0899766053429390
   Hughes TW, 2018, OPTICA, V5, P864, DOI 10.1364/OPTICA.5.000864
   Hurtado A, 2007, J OPT NETW, V6, P434, DOI 10.1364/JON.6.000434
   Hurtado A, 2012, APPL PHYS LETT, V100, DOI 10.1063/1.3692726
   Hurtado A, 2007, OPT EXPRESS, V15, P9084, DOI 10.1364/OE.15.009084
   Hurtado A, 2010, OPT EXPRESS, V18, P25170, DOI 10.1364/OE.18.025170
   Indiveri G, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00073
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Li Q., 2016, P SOC PHOTO-OPT INS, V10019
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   MAINEN ZF, 1995, SCIENCE, V268, P1503, DOI 10.1126/science.7770778
   Masquelier T, 2008, PLOS ONE, V3, DOI 10.1371/journal.pone.0001377
   Masquelier T, 2007, PLOS COMPUT BIOL, V3, P247, DOI 10.1371/journal.pcbi.0030031
   Masquelier T, 2009, NEURAL COMPUT, V21, P1259, DOI 10.1162/neco.2008.06-08-804
   MEAD C, 1990, P IEEE, V78, P1629, DOI 10.1109/5.58356
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Mohemmed A, 2012, INT J NEURAL SYST, V22, DOI 10.1142/S0129065712500128
   Nahmias MA, 2013, IEEE J SEL TOP QUANT, V19, DOI 10.1109/JSTQE.2013.2257700
   Nawrocki RA, 2016, IEEE T ELECTRON DEV, V63, P3819, DOI 10.1109/TED.2016.2598413
   Nguyen THO, 2017, CLIN TRANSL IMMUNOL, V6, DOI 10.1038/cti.2017.4
   Painkras E, 2013, IEEE J SOLID-ST CIRC, V48, P1943, DOI 10.1109/JSSC.2013.2259038
   Park S, 2015, SCI REP-UK, V5, DOI 10.1038/srep10123
   Piprek J, 2001, IEEE J QUANTUM ELECT, V37, P127, DOI 10.1109/3.892734
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Prezioso M, 2015, NATURE, V521, P61, DOI 10.1038/nature14441
   Prucnal PR, 2016, ADV OPT PHOTONICS, V8, P228, DOI 10.1364/AOP.8.000228
   Qiao N, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00141
   Ren QS, 2015, OPT EXPRESS, V23, P25247, DOI 10.1364/OE.23.025247
   Robertson J, 2017, OPT LETT, V42, P1560, DOI 10.1364/OL.42.001560
   Romeira B, 2016, SCI REP-UK, V6, DOI 10.1038/srep19510
   Royo P, 2002, IEEE J QUANTUM ELECT, V38, P279, DOI 10.1109/3.985569
   Sánchez MD, 2003, OPT EXPRESS, V11, P2689, DOI 10.1364/OE.11.002689
   Schuman CD., 2017, ARXIV
   Selmi F, 2014, PHYS REV LETT, V112, DOI 10.1103/PhysRevLett.112.183902
   Shastri BJ, 2016, SCI REP-UK, V6, DOI 10.1038/srep19126
   Shen JC, 2016, SCI CHINA INFORM SCI, V59, DOI 10.1007/s11432-015-5511-7
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   TOMBLING C, 1994, IEEE J QUANTUM ELECT, V30, P2491, DOI 10.1109/3.333700
   Toole R, 2016, J LIGHTWAVE TECHNOL, V34, P470, DOI 10.1109/JLT.2015.2475275
   Toole R, 2015, OPT EXPRESS, V23, P16133, DOI 10.1364/OE.23.016133
   Van Vaerenbergh T, 2012, OPT EXPRESS, V20, P20292, DOI 10.1364/OE.20.020292
   Wang JL, 2014, NEUROCOMPUTING, V144, P526, DOI 10.1016/j.neucom.2014.04.017
   Xiang SY, 2018, IEEE J QUANTUM ELECT, V54, DOI 10.1109/JQE.2018.2879484
   Xiang SY, 2018, J LIGHTWAVE TECHNOL, V36, P4227, DOI 10.1109/JLT.2018.2818195
   Yao P, 2017, NAT COMMUN, V8, DOI 10.1038/ncomms15199
   Zhang YH, 2018, APPL OPTICS, V57, P1731, DOI 10.1364/AO.57.001731
   Zhang Y, 2018, EURASIP J WIREL COMM, DOI 10.1186/s13638-017-1014-0
NR 72
TC 89
Z9 89
U1 4
U2 91
PD NOV-DEC
PY 2019
VL 25
IS 6
AR 1700109
DI 10.1109/JSTQE.2019.2911565
WC Engineering, Electrical & Electronic; Quantum Science & Technology;
   Optics; Physics, Applied
DA 2023-11-11
ER

PT B
AU Adeli, H
   Ghosh-Dastidar, S
   Dadmehr, N
AF Adeli, Hojjat
   Ghosh-Dastidar, Samanwoy
   Dadmehr, Nahid
BA Adeli, H
   GhoshDastidar, S
BF Adeli, H
   GhoshDastidar, S
TI A New Supervised Learning Algorithm for Multiple Spiking Neural Networks
SO AUTOMATED EEG-BASED DIAGNOSIS OF NEUROLOGICAL DISORDERS: INVENTING THE
   FUTURE OF NEUROLOGY
DT Article; Book Chapter
RP Adeli, H (corresponding author), Ohio State Univ, Columbus, OH 43210 USA.
NR 0
TC 0
Z9 0
U1 0
U2 0
PY 2010
BP 305
EP 328
WC Engineering, Biomedical; Clinical Neurology; Neuroimaging
DA 2023-11-11
ER

PT C
AU Kubota, N
   Sasaki, H
AF Kubota, N
   Sasaki, H
BE Murase, K
   Sekiyama, K
   Kubota, N
   Naniwa, T
   Sitte, J
TI Spiking neural network for behavior learning of a mobile robot
SO PROCEEDINGS OF THE 3RD INTERNATIONAL SYMPOSIUM ON AUTONOMOUS MINIROBOTS
   FOR RESEARCH AND EDUTAINMENT (AMIRE 2005)
DT Proceedings Paper
CT 3rd International Symposium on Autonomous Minirobots for Research and
   Edutainment
CY SEP 20-22, 2005
CL Fukui, JAPAN
C1 Tokyo Metropolitan Univ, Dept Syst Design, 1-1 Minami Osawa, Tokyo 1920397, Japan.
   PRESTO, Japan Sci & Technol Agcy JST, Saitama, Japan.
   Univ Fukui, Dept Human & Art Intelligent Syst, 3-9-1 Bunkyo, Fukui 9108507, Japan.
RP Kubota, N (corresponding author), Tokyo Metropolitan Univ, Dept Syst Design, 1-1 Minami Osawa, Tokyo 1920397, Japan.
EM kubota@comp.metro-u.ac.jp; sasaki@iicx.ia.his.fukui-u.ac.jp
CR ANDERSON JA, NEUROCOMPUTING
   [Anonymous], 1997, NEURO FUZZY SOFT COM
   MAASS W., 1999, PULSED NEURAL NETWOR
   MILLER WT, 1990, NEURAL NETWORKS CONT
   Russell S., 1995, ARTI CIAL INTELLIGEN
   Scheier C, 1998, NEURAL NETWORKS, V11, P1551, DOI 10.1016/S0893-6080(98)00084-7
   Tani J, 1996, IEEE T SYST MAN CY B, V26, P421, DOI 10.1109/3477.499793
NR 7
TC 0
Z9 0
U1 0
U2 1
PY 2006
BP 267
EP +
DI 10.1007/3-540-29344-2_40
WC Robotics
DA 2023-11-11
ER

PT C
AU Vallejo-Mancero, B
   Nader, C
   Madrenas, J
   Zapata, M
AF Vallejo-Mancero, Bernardo
   Nader, Clement
   Madrenas, Jordi
   Zapata, Mireya
BE Pimenidis, E
   Angelov, P
   Jayne, C
   Papaleonidas, A
   Aydin, M
TI Real-Time Display of Spiking Neural Activity of SIMD Hardware Using an
   HDMI Interface
SO ARTIFICIAL NEURAL NETWORKS AND MACHINE LEARNING - ICANN 2022, PT III
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 31st International Conference on Artificial Neural Networks (ICANN)
CY SEP 06-09, 2022
CL Univ W England, Bristol, ENGLAND
HO Univ W England
DE Spiking neural network; FPGA; Raster plot; Real-time; HDMI display
AB Spiking neural networks (SNN) are considered the third generation of artificial networks and are powerful computational models inspired by the function and structure of biological neural networks, to solve different types of problems such as pattern recognition, classification, signal processing, among others.
   SNN have also aroused the interest of neuroscientists intending to obtain new knowledge about the functions of the neuronal system through the analysis of the patterns observed in spike trains. Therefore, in addition to the development of hardware solutions that allow the execution of the different neural models, it is important, to provide tools for the visualization and analysis of the spike trains and the evolution of the neural parameters of the affected neurons in real-time.
   This work describes a new solution that takes the hardware emulator of evolved neural spiking system (HEENS) as the starting point, which is a bio-inspired architecture that emulates SNN using reconfigurable hardware implemented in field-programmable gate arrays (FPGAs). Reported development includes new dedicated hardware modules to interface HEENS with the high definition multimedia interface (HDMI) port, ensuring execution cycles within a time window of at least 1 ms, a period considered real-time in many neural applications.
   Tests of the synthesized architecture including the new tool have been carried out, executing different types of applications. The result is a friendly and flexible tool that has successfully allowed the visualization of pulse trains and neural parameters and constitutes an alternative for the monitoring and supervision of the SNN in real-time.
C1 [Vallejo-Mancero, Bernardo; Nader, Clement; Madrenas, Jordi] Univ Politecn Cataluna, Dept Elect Engn, Barcelona, Spain.
   [Zapata, Mireya] Univ Tecnol Indoamerica, Ctr Invest Mecatron & Sistemas Interact MIST, Quito, Ecuador.
RP Vallejo-Mancero, B (corresponding author), Univ Politecn Cataluna, Dept Elect Engn, Barcelona, Spain.
EM bernardo.javier.vallejo@upc.edu; nader.clement@upc.edu;
   jordi.madrenas@upc.edu; mireyazapata@uti.edu.ec
CR Oltra-Oltra JA, 2021, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS51556.2021.9401615
   [Anonymous], ZYNQ 7000
   [Anonymous], 2006, HIGH DEFINITION MULT, P12
   [Anonymous], MATLAB SIMULINK
   [Anonymous], NEUROEXPLORER PLEXON
   Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Campos N., 2016, RGB YCBCR CONVERSION
   Caruso A., 2020, THESIS U POLITECNICA
   DeBole MV, 2019, COMPUTER, V52, P20, DOI 10.1109/MC.2019.2903009
   Derek-X-Wang, 2015, VGA TEXT GENERATOR
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   Guo WZ, 2021, FRONT NEUROSCI-SWITZ, V15, DOI [10.3389/fnins.2021.638474, 10.1007/s11704-020-9230-x]
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Kasabov N. K., 2019, TIME SPACE SPIKING N, DOI DOI 10.1007/978-3-662-57715-8
   Madrenas J, 2020, IEEE I C ELECT CIRC, DOI 10.1109/icecs49266.2020.9294982
   Nader C., 2022, THESIS U POLITECNICA
   Orchard G, 2021, IEEE WRK SIG PRO SYS, P254, DOI 10.1109/SiPS52927.2021.00053
   Somerville J, 2010, J NEUROSCI METH, V194, P158, DOI 10.1016/j.jneumeth.2010.09.009
   Spilger P., 2020, IOT STREAMS DATA DRI, P189, DOI [DOI 10.1007/978, 10.1007/978-3-030-66770-2_14, DOI 10.1007/978-3-030-66770-2_14]
   Zapata M., 2018, IEEE 3 EC TECHN CHAP, P1
   Zapata M, 2021, ADV INTELL SYST COMP, V1322, P489, DOI 10.1007/978-3-030-68017-6_73
   Zapata M, 2018, NASA ESA CONF, P241, DOI 10.1109/AHS.2018.8541463
NR 22
TC 0
Z9 0
U1 1
U2 2
PY 2022
VL 13531
BP 728
EP 739
DI 10.1007/978-3-031-15934-3_60
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods
DA 2023-11-11
ER

PT J
AU Haghiri, S
   Zahedi, A
   Naderi, A
   Ahmadi, A
AF Haghiri, Saeed
   Zahedi, Abdulhamid
   Naderi, Ali
   Ahmadi, Arash
TI Multiplierless Implementation of Noisy Izhikevich Neuron With Low-Cost
   Digital Design
SO IEEE TRANSACTIONS ON BIOMEDICAL CIRCUITS AND SYSTEMS
DT Article
DE Neuron; noisy Izhikevich model; coupling behaviors; FPGA
ID SPIKING NEURONS; ASTROCYTE; MODEL; REALIZATION
AB Fast speed and a high accuracy implementation of biological plausible neural networks are vital key objectives to achieve new solutions to model, simulate and cure the brain diseases. Efficient hardware implementation of spiking neural networks is a significant approach in biological neural networks. This paper presents a multiplierless noisy Izhikevich neuron (MNIN) model, which is used for the digital implementation of biological neural networks in large scale. Simulation results show that the MNIN model reproduces the same operations of the original noisy Izhikevich neuron. The proposed model has a low-cost hardware implementation property compared with the original neuron model. The field-programmable gate array realization results demonstrated that the MNIN model follows the different spiking patterns appropriately.
C1 [Haghiri, Saeed; Zahedi, Abdulhamid; Naderi, Ali] Kermanshah Univ Technol, Dept Elect Engn, Kermanshah 6715847141, Iran.
   [Ahmadi, Arash] Univ Windsor, Dept Elect & Comp Engn, Windsor, ON N9B 3P4, Canada.
RP Haghiri, S (corresponding author), Kermanshah Univ Technol, Dept Elect Engn, Kermanshah 6715847141, Iran.
EM s.haghiri@kut.ac.ir; zahedi@kut.ac.ir; a.naderi@kut.ac.ir;
   aahmadi70@gmail.com
CR Ambroise T., 2013, P 47 ANN C INF SCI S, P1, DOI [10.1109/CISS.2013.6616689.[29]H., DOI 10.1109/CISS.2013.6616689.[29]H]
   [Anonymous], 2011, 45 ANN C INFORM SCI
   [Anonymous], 2010, DYNAMICAL SYSTEMS NE
   Basu A, 2010, IEEE T BIOMED CIRC S, V4, P311, DOI 10.1109/TBCAS.2010.2055157
   Batista CAS, 2010, NEURAL NETWORKS, V23, P114, DOI 10.1016/j.neunet.2009.08.005
   FITZHUGH R, 1961, BIOPHYS J, V1, P445, DOI 10.1016/S0006-3495(61)86902-6
   Gerstner W., 2002, SPIKING NEURON MODEL
   Gomar S, 2014, IEEE T CIRCUITS-I, V61, P1206, DOI 10.1109/TCSI.2013.2286030
   Grassia F, 2016, J Physiol Paris, V110, P409, DOI 10.1016/j.jphysparis.2017.02.002
   Haghiri S, 2017, IEEE T BIOMED CIRC S, V11, P117, DOI 10.1109/TBCAS.2016.2583920
   Haghiri S, 2016, NEUROCOMPUTING, V214, P280, DOI 10.1016/j.neucom.2016.06.015
   Haghiri S, 2014, IRAN CONF ELECTR ENG, P88, DOI 10.1109/IranianCEE.2014.6999509
   Hayati M, 2016, IEEE T CIRCUITS-II, V63, P463, DOI 10.1109/TCSII.2015.2505258
   Hayati M, 2016, IEEE T BIOMED CIRC S, V10, P518, DOI 10.1109/TBCAS.2015.2450837
   Hayati M, 2015, IEEE T CIRCUITS-I, V62, P1805, DOI 10.1109/TCSI.2015.2423794
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Indiveri G, 2006, IEEE T NEURAL NETWOR, V17, P211, DOI 10.1109/TNN.2005.860850
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Khalil HK., 2012, NONLINEAR SYSTEMS
   MORRIS C, 1981, BIOPHYS J, V35, P193, DOI 10.1016/S0006-3495(81)84782-0
   Nazari S, 2014, NEUROSCI LETT, V582, P21, DOI 10.1016/j.neulet.2014.07.055
   Postnov DE, 2007, BIOSYSTEMS, V89, P84, DOI 10.1016/j.biosystems.2006.04.012
   Postnov DE, 2009, J BIOL PHYS, V35, P425, DOI 10.1007/s10867-009-9156-x
   Rice KL, 2009, 2009 INTERNATIONAL CONFERENCE ON RECONFIGURABLE COMPUTING AND FPGAS, P451, DOI 10.1109/ReConFig.2009.77
   Soleimani H, 2015, IEEE T NEUR NET LEAR, V26, P127, DOI 10.1109/TNNLS.2014.2311839
   Soleimani H, 2014, NEURAL NETWORKS, V51, P26, DOI 10.1016/j.neunet.2013.12.004
   Soleimani H, 2012, IEEE T CIRCUITS-I, V59, P2991, DOI 10.1109/TCSI.2012.2206463
   Szatmáry B, 2010, PLOS COMPUT BIOL, V6, DOI 10.1371/journal.pcbi.1000879
   Touboul J, 2008, BIOL CYBERN, V99, P319, DOI 10.1007/s00422-008-0267-4
   Yu T, 2011, IEEE T BIOMED CIRC S, V5, P420, DOI 10.1109/TBCAS.2011.2169794
   Zhou J, 2009, IEEE T NEURAL NETWOR, V20, P1679, DOI 10.1109/TNN.2009.2029102
NR 31
TC 26
Z9 27
U1 0
U2 5
PD DEC
PY 2018
VL 12
IS 6
BP 1422
EP 1430
DI 10.1109/TBCAS.2018.2868746
WC Engineering, Biomedical; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Wu, TF
   Lyu, Q
   Pan, LQ
AF Wu, Tingfang
   Lyu, Qiang
   Pan, Linqiang
TI Evolution-Communication Spiking Neural P Systems
SO INTERNATIONAL JOURNAL OF NEURAL SYSTEMS
DT Article
DE Membrane computing; neural computation; spiking neural network; spiking
   neural P system; computational power
ID COMPUTATIONAL POWER; UNIVERSALITY; COMPLEXITY; NETWORKS
AB Spiking neural P systems (SNP systems) are a class of distributed and parallel computation models, which are inspired by the way in which neurons process information through spikes, where the integrate-and-fire behavior of neurons and the distribution of produced spikes are achieved by spiking rules. In this work, a novel mechanism for separately describing the integrate-and-fire behavior of neurons and the distribution of produced spikes, and a novel variant of the SNP systems, named evolution-communication SNP (ECSNP) systems, is proposed. More precisely, the integrate-and-fire behavior of neurons is achieved by spike-evolution rules, and the distribution of produced spikes is achieved by spike-communication rules. Then, the computational power of ECSNP systems is examined. It is demonstrated that ECSNP systems are Turing universal as number-generating devices. Furthermore, the computational power of ECSNP systems with a restricted form, i.e. the quantity of spikes in each neuron throughout a computation does not exceed some constant, is also investigated, and it is shown that such restricted ECSNP systems can only characterize the family of semilinear number sets. These results manifest that the capacity of neurons for information storage (i.e. the quantity of spikes) has a critical impact on the ECSNP systems to achieve a desired computational power.
C1 [Wu, Tingfang; Lyu, Qiang] Soochow Univ, Sch Comp Sci & Technol, Suzhou 215006, Peoples R China.
   [Wu, Tingfang; Lyu, Qiang] Soochow Univ, Prov Key Lab Comp Informat Proc Technol, Suzhou 215006, Peoples R China.
   [Pan, Linqiang] Huazhong Univ Sci & Technol, Sch Artificial Intelligence & Automat, Key Lab Image Informat Proc & Intelligent Control, Educ Minist China,Inst Artificial Intelligence, Wuhan 430074, Hubei, Peoples R China.
RP Pan, LQ (corresponding author), Huazhong Univ Sci & Technol, Sch Artificial Intelligence & Automat, Key Lab Image Informat Proc & Intelligent Control, Educ Minist China,Inst Artificial Intelligence, Wuhan 430074, Hubei, Peoples R China.
EM lqpan@mail.hust.edu.cn
CR Acharya UR, 2018, COMPUT BIOL MED, V100, P270, DOI 10.1016/j.compbiomed.2017.09.017
   Acharya UR, 2018, COMPUT METH PROG BIO, V161, P103, DOI 10.1016/j.cmpb.2018.04.012
   Adeli H, 2010, AUTOMATED EEG-BASED DIAGNOSIS OF NEUROLOGICAL DISORDERS: INVENTING THE FUTURE OF NEUROLOGY, P1
   Alhazov A, 2018, NAT COMPUT, V17, P297, DOI 10.1007/s11047-017-9649-7
   ALON N, 1991, J ACM, V38, P495, DOI 10.1145/103516.103523
   Aman B, 2019, J MEMBRANE COMPUT, V1, P233, DOI 10.1007/s41965-019-00022-1
   [Anonymous], 2017, REAL LIFE APPL MEMBR
   Bernardini F, 2005, SOFT COMPUT, V9, P640, DOI 10.1007/s00500-004-0393-4
   Bernert M, 2019, INT J NEURAL SYST, V29, DOI 10.1142/S0129065718500594
   Cabarle FGC, 2019, INFORM SCIENCES, V501, P30, DOI 10.1016/j.ins.2019.05.070
   Cabarle FGC, 2018, IEEE T NANOBIOSCI, V17, P560, DOI 10.1109/TNB.2018.2879345
   Cabarle FGC, 2017, IEEE T NANOBIOSCI, V16, P792, DOI 10.1109/TNB.2017.2762580
   Cabessa J, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0223451
   Cabessa J, 2014, INT J NEURAL SYST, V24, DOI 10.1142/S0129065714500294
   Cavaliere M, 2003, LECT NOTES COMPUT SC, V2597, P134
   Chen ZH, 2018, NEURAL COMPUT APPL, V29, P695, DOI 10.1007/s00521-016-2489-z
   Díaz-Pernil D, 2019, J MEMBRANE COMPUT, V1, P58, DOI 10.1007/s41965-018-00002-x
   Duro RJ, 2019, INT J NEURAL SYST, V29, DOI 10.1142/S0129065718500533
   Fiesler E., 2020, HDB NEURAL COMPUTATI
   Gerstner W., 2002, SPIKING NEURON MODEL
   Ghosh-Dastidar S, 2007, INTEGR COMPUT-AID E, V14, P187
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Hopcroft JE, 2001, ACM SIGACT NEWS, V32, P60
   Horne BG, 1996, NEURAL NETWORKS, V9, P243, DOI 10.1016/0893-6080(95)00095-X
   Huang LY, 2018, INT J NEURAL SYST, V28, DOI 10.1142/S0129065717500319
   Ionescu M, 2006, FUND INFORM, V71, P279
   Krishna SN, 2017, THEOR COMPUT SCI, V701, P146, DOI 10.1016/j.tcs.2017.05.020
   LeCun Y, 2015, NATURE, V521, p7553 436 444, DOI [10.1038/nature14539, DOI 10.1038/NATURE14539]
   Leporati A, 2018, INT J ADV ENG SCI AP, V10, P221, DOI 10.1007/s12572-018-0227-8
   Leporati A, 2009, INT J UNCONV COMPUT, V5, P459
   Liu LC, 2019, THEOR COMPUT SCI, V785, P140, DOI 10.1016/j.tcs.2019.03.021
   Maass W, 1998, PULSED NEURAL NETWORKS, P55
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maass W, 2001, PULSED NEURAL NETWOR
   Martín-Vide C, 2003, THEOR COMPUT SCI, V296, P295, DOI 10.1016/S0304-3975(02)00659-X
   MCCULLOCH WS, 1990, B MATH BIOL, V52, P99, DOI 10.1016/S0092-8240(05)80006-0
   MINSKY M, 1967, COMPUTATION FINITE I
   Orellana-Martín D, 2020, THEOR COMPUT SCI, V805, P206, DOI 10.1016/j.tcs.2018.04.052
   Orellana-Martín D, 2019, J MEMBRANE COMPUT, V1, P85, DOI 10.1007/s41965-018-00004-9
   Pan LQ, 2017, INT J NEURAL SYST, V27, DOI 10.1142/S0129065717500423
   Pan LQ, 2017, IEEE T NANOBIOSCI, V16, P513, DOI 10.1109/TNB.2017.2722466
   Pan LQ, 2016, NAT COMPUT, V15, P575, DOI 10.1007/s11047-016-9567-0
   Pan LQ, 2011, SCI CHINA INFORM SCI, V54, P1596, DOI 10.1007/s11432-011-4303-y
   Paun A, 2007, BIOSYSTEMS, V90, P48, DOI 10.1016/j.biosystems.2006.06.006
   Päun G, 2000, J COMPUT SYST SCI, V61, P108, DOI 10.1006/jcss.1999.1693
   Paun Gh, 2010, OXFORD HDB MEMBRANE
   Paun G, 2006, FUND INFORM, V73, P213
   Paun G, 2006, INT J FOUND COMPUT S, V17, P975, DOI 10.1142/S0129054106004212
   Peng H, 2019, IEEE T NEUR NET LEAR, V30, P1672, DOI 10.1109/TNNLS.2018.2872999
   Peng H, 2018, IEEE T SMART GRID, V9, P4777, DOI 10.1109/TSG.2017.2670602
   Peng H, 2017, INTEGR COMPUT-AID E, V24, P401, DOI 10.3233/ICA-170552
   Peng H, 2016, INT J NEURAL SYST, V26, DOI 10.1142/S0129065716500040
   Pérez-Hurtado I, 2020, INTEGR COMPUT-AID E, V27, P121, DOI 10.3233/ICA-190616
   Rong HN, 2019, COMPLEXITY, V2019, DOI 10.1155/2019/2635714
   Rosselló JL, 2014, INT J NEURAL SYST, V24, DOI 10.1142/S0129065714300034
   Siegelmann H. T., 2012, NEURAL NETWORKS ANAL
   SIEGELMANN HT, 1995, J COMPUT SYST SCI, V50, P132, DOI 10.1006/jcss.1995.1013
   SIEGELMANN HT, 1991, APPL MATH LETT, V4, P77, DOI 10.1016/0893-9659(91)90080-F
   Song BS, 2017, INFORM SCIENCES, V378, P177, DOI 10.1016/j.ins.2016.10.046
   Song T, 2019, IEEE T NANOBIOSCI, V18, P176, DOI 10.1109/TNB.2019.2896981
   Song T, 2019, NEURAL PROCESS LETT, V50, P1485, DOI 10.1007/s11063-018-9947-9
   Song T, 2016, IEEE T NANOBIOSCI, V15, P666, DOI 10.1109/TNB.2016.2598879
   van Gerven Marcel, 2017, Front Comput Neurosci, V11, P114, DOI 10.3389/fncom.2017.00114
   Wang J, 2010, NEURAL COMPUT, V22, P2615, DOI 10.1162/NECO_a_00022
   Wang XY, 2016, INTEGR COMPUT-AID E, V23, P15, DOI 10.3233/ICA-150503
   Wu TF, 2019, THEOR COMPUT SCI, V777, P474, DOI 10.1016/j.tcs.2018.10.024
   Wu TF, 2018, INT J NEURAL SYST, V28, DOI 10.1142/S0129065718500132
   Wu TF, 2018, IEEE T NEUR NET LEAR, V29, P3349, DOI 10.1109/TNNLS.2017.2726119
   Wu TF, 2016, THEOR COMPUT SCI, V623, P180, DOI 10.1016/j.tcs.2015.12.038
   Xuayuan W., 2020, INTEGR COMPUT AIDED, P1
   Zhang GX, 2014, INT J NEURAL SYST, V24, DOI 10.1142/S0129065714400061
   Zhang XY, 2015, IEEE T NEUR NET LEAR, V26, P2816, DOI 10.1109/TNNLS.2015.2396940
   Zhang ZQ, 2018, SCI CHINA INFORM SCI, V61, DOI 10.1007/s11432-017-9103-5
NR 74
TC 16
Z9 16
U1 5
U2 53
PD FEB
PY 2021
VL 31
IS 2
AR 2050064
DI 10.1142/S0129065720500641
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT J
AU Wu, YJ
   Deng, L
   Li, GQ
   Zhu, J
   Shi, LP
AF Wu, Yujie
   Deng, Lei
   Li, Guoqi
   Zhu, Jun
   Shi, Luping
TI Spatio-Temporal Backpropagation for Training High-Performance Spiking
   Neural Networks
SO FRONTIERS IN NEUROSCIENCE
DT Article
DE spiking neural network (SNN); spatio-temporal recognition; leaky
   integrate-and-fire neuron; MNIST-DVS; MNIST; backpropagation;
   convolutional neural networks (CNN)
ID CLASSIFICATION
AB Spiking neural networks (SNNs) are promising in ascertaining brain-like behaviors since spikes are capable of encoding spatio-temporal information. Recent schemes, e.g., pre-training from artificial neural networks (ANNs) or direct training based on backpropagation (BP), make the high-performance supervised training of SNNs possible. However, these methods primarily fasten more attention on its spatial domain information, and the dynamics in temporal domain are attached less significance. Consequently, this might lead to the performance bottleneck, and scores of training techniques shall be additionally required. Another underlying problem is that the spike activity is naturally non-differentiable, raising more difficulties in supervised training of SNNs. In this paper, we propose a spatio-temporal backpropagation (STBP) algorithm for training high-performance SNNs. In order to solve the non-differentiable problem of SNNs, an approximated derivative for spike activity is proposed, being appropriate for gradient descent training. The STBP algorithm combines the layer-by-layer spatial domain (SD) and the timing-dependent temporal domain (TD), and does not require any additional complicated skill. We evaluate this method through adopting both the fully connected and convolutional architecture on the static MNIST dataset, a custom object detection dataset, and the dynamic N-MNIST dataset. Results bespeak that our approach achieves the best accuracy compared with existing state-of-the-art algorithms on spiking networks. This work provides a new perspective to investigate the high-performance SNNs for future brain-like computing paradigm with rich spatio-temporal dynamics.
C1 [Wu, Yujie; Deng, Lei; Li, Guoqi; Shi, Luping] Tsinghua Univ, Dept Precis Instrument, Ctr Brain Inspired Comp Res, Beijing Innovat Ctr Future Chip, Beijing, Peoples R China.
   [Deng, Lei] Univ Calif Santa Barbara, Dept Elect & Comp Engn, Santa Barbara, CA 93106 USA.
   [Zhu, Jun] Tsinghua Univ, Tsinghua Natl Lab Informat Sci & Technol, State Key Lab Intelligence Technol & Syst, Beijing, Peoples R China.
RP Shi, LP (corresponding author), Tsinghua Univ, Dept Precis Instrument, Ctr Brain Inspired Comp Res, Beijing Innovat Ctr Future Chip, Beijing, Peoples R China.; Zhu, J (corresponding author), Tsinghua Univ, Tsinghua Natl Lab Informat Sci & Technol, State Key Lab Intelligence Technol & Syst, Beijing, Peoples R China.
EM dcszj@mail.tsinghua.edu.cn; lpshi@tsinghua.edu.cn
CR Allen JN, 2009, NAECON: PROCEEDINGS OF THE IEEE 2009 NATIONAL AEROSPACE & ELECTRONICS CONFERENCE, P56, DOI 10.1109/NAECON.2009.5426652
   [Anonymous], 1 SPIKE BASED VISUAL
   [Anonymous], 2016, SELF DRIVING ROBOT U
   Bengio Y., 2015, COMPUT SCI
   Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Bohte S. M., 2000, 8th European Symposium on Artificial Neural Networks. ESANN"2000. Proceedings, P419
   Chung JY, 2015, PR MACH LEARN RES, V37, P2067
   Cohen GK, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00184
   DAVIS KH, 1952, J ACOUST SOC AM, V24, P637, DOI 10.1121/1.1906946
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Diehl Peter U, 2015, 2015 INT JOINT C NEU, P1, DOI [DOI 10.1109/IJCNN.2015.7280696, 10.1109/IJCNN.2015.7280696]
   Esser SK, 2016, P NATL ACAD SCI USA, V113, P11441, DOI 10.1073/pnas.1604850113
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   Garofolo J.S., 1993, 93 NASA STIRECON
   Gers FA, 2000, NEURAL COMPUT, V12, P2451, DOI 10.1162/089976600300015015
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Hunsberger E., 2015, COMPUT SCI
   Kasabov N, 2015, INFORM SCIENCES, V294, P565, DOI 10.1016/j.ins.2014.06.028
   Kheradpisheh SR, 2016, NEUROCOMPUTING, V205, P382, DOI 10.1016/j.neucom.2016.04.029
   Kingma D. P., 2014, ARXIV, DOI DOI 10.48550/ARXIV.1412.6980
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee JM, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00191
   Lichtsteiner Patrick, 2008, IEEE Journal of Solid-State Circuits, V43, P566, DOI 10.1109/JSSC.2007.914337
   McKennoch S, 2006, IEEE IJCNN, P3970
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Neftci E, 2014, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00272
   Neil D., 2016, PHASED LSTM ACCELERA
   Neil D, 2016, IEEE INT SYMP CIRC S, P2282, DOI 10.1109/ISCAS.2016.7539039
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   OConnor P, 2016, DEEP SPIKING NETWORK
   Orchard G, 2015, FRONT NEUROSCI-SWITZ, V9, DOI [10.3389/fnins.2015.00437, 10.3389/fhins.2015.00437]
   Ponulak F., 2005, RESUME NEW SUPERVISE
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Querlioz D, 2013, IEEE T NANOTECHNOL, V12, P288, DOI 10.1109/TNANO.2013.2250995
   Schrauwen B, 2004, IEEE IJCNN, P471, DOI 10.1109/IJCNN.2004.1379954
   Simard P. Y., 2003, INT C DOC AN REC ED
   Tavanaei A., 2017, BIOINSPIRED SPIKING
   Urbanczik R, 2009, NEURAL COMPUT, V21, P340, DOI 10.1162/neco.2008.09-07-605
   Vincent P., 2008, P 25 INT C MACH LEAR, P1096
   WERBOS PJ, 1990, P IEEE, V78, P1550, DOI 10.1109/5.58337
   Zhang B., 2016, SCIENCE, V354, P1445, DOI [10.1126/science.354.6318.1445-b, DOI 10.1126/SCIENCE.354.6318.1445-B]
   Zhang X, 2013, IEEE DECIS CONTR P, P6798, DOI 10.1109/CDC.2013.6760966
NR 43
TC 359
Z9 370
U1 24
U2 112
PD MAY 23
PY 2018
VL 12
AR 331
DI 10.3389/fnins.2018.00331
WC Neurosciences
HC Y
HP N
DA 2023-11-11
ER

PT C
AU Abed, BAR
   Ismail, AR
   Aziz, NA
AF Abed, Bassam Abdul-Rahman
   Ismail, Amelia Ritahani
   Aziz, Normaziah Abdul
GP IEEE
TI Real Time Astrocyte in Spiking Neural Network
SO 2015 SAI INTELLIGENT SYSTEMS CONFERENCE (INTELLISYS)
DT Proceedings Paper
CT SAI Intelligent Systems Conference (IntelliSys)
CY NOV 10-11, 2015
CL Sci & Informat Org, London, ENGLAND
HO Sci & Informat Org
DE Spiking Response Model; Spiking Neural Network; Astrocytes
ID DRESSED NEURONS; MODEL
AB Astrocytes, one type of glial cells, are considered to be an active partner to neurons in processing information of Central Nervous System (CNS). Therefore, studying the interaction between the astrocytes and neurons is important to create a novel model for Artificial Neuron-Glial Networks (ANGN). In this paper, a novel model for (ANGN) is proposed to model the real time interaction between Astrocytes and neurons by using Spiking Neural Networks (SNNs) and mathematical models for astrocyte-neuron interaction. How could this proposed model will be biologically inspired to model the real time interaction between astrocytes and neurons and to improve the performance of the SNN? However, these mathematical models are generalized and simplified to be used in the proposed network. The performance of the proposed network was compared with standard SNN and the simulation results showed that the proposed model evoked more spikes to fire whenever astrocytes were activating in a time window. This indicates that astrocytes are playing significant roles in processing information of the ANGN.
C1 [Abed, Bassam Abdul-Rahman; Ismail, Amelia Ritahani; Aziz, Normaziah Abdul] Int Islamic Univ Malaysia, Dept Comp Sci, Kuala Lumpur, Malaysia.
RP Abed, BAR (corresponding author), Int Islamic Univ Malaysia, Dept Comp Sci, Kuala Lumpur, Malaysia.
EM Bassamabed2000@gmail.com; amelia@iium.edu.my; naa@iium.edu.my
CR Alvarellos-Gonzalez A., 2012, COMPUTATIONAL MATH M, V2012
   Araque A, 1999, TRENDS NEUROSCI, V22, P208, DOI 10.1016/S0166-2236(98)01349-6
   Berger T. W., 2014, ENCY COMPUTATIONAL N, P1
   DESTEXHE A, 1994, NEURAL COMPUT, V6, P14, DOI 10.1162/neco.1994.6.1.14
   Gerstner W, 2001, NEURAL NETWORKS, V14, P599, DOI 10.1016/S0893-6080(01)00053-3
   Ghosh-Dastidar S, 2009, ADV INTEL SOFT COMPU, V61, P167
   Haghiri S., 2014, EL ENG ICEE 2014 22
   Haydon PG, 2006, PHYSIOL REV, V86, P1009, DOI 10.1152/physrev.00049.2005
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   Ikuta C., 2011, NEUR NETW IJCNN 2011
   IKUTA C, 2010, CIRC SYST ISCAS P 20
   Ikuta C., 2014, NEUR NETW IJCNN 2014
   Ikuta C., 2012, NEUR NETW IJCNN 2012
   Ikuta C., 2013, CIRC SYST ISCAS 2013
   Izhikevich EM, 2001, NEURAL NETWORKS, V14, P883, DOI 10.1016/S0893-6080(01)00078-8
   Kempter R, 1999, PHYS REV E, V59, P4498, DOI 10.1103/PhysRevE.59.4498
   Landolsi H., 2014, COMP INT COGN ALG MI
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Nadkarni S, 2004, PHYS BIOL, V1, P35, DOI 10.1088/1478-3967/1/1/004
   Perea G, 2009, TRENDS NEUROSCI, V32, P421, DOI 10.1016/j.tins.2009.05.001
   Porto-Pazos AB, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0019109
   Postnov DE, 2007, BIOSYSTEMS, V89, P84, DOI 10.1016/j.biosystems.2006.04.012
   Sajedinia Zahra, 2014, Unconventional Computation and Natural Computation. 13th International Conference. Proceedings: LNCS 8553, P316, DOI 10.1007/978-3-319-08123-6_26
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   Valenza G, 2011, NEURAL NETWORKS, V24, P679, DOI 10.1016/j.neunet.2011.03.013
   Valenza G., 2013, ENG MED BIOL SOC EMB
   Volman V, 2007, NEURAL COMPUT, V19, P303, DOI 10.1162/neco.2007.19.2.303
   Wade JJ, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0029445
NR 29
TC 2
Z9 2
U1 0
U2 5
PY 2015
BP 565
EP 570
WC Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Computer Science, Theory & Methods
DA 2023-11-11
ER

PT J
AU Göltz, J
   Kriener, L
   Baumbach, A
   Billaudelle, S
   Breitwieser, O
   Cramer, B
   Dold, D
   Kungl, AF
   Senn, W
   Schemmel, J
   Meier, K
   Petrovici, MA
AF Goeltz, J.
   Kriener, L.
   Baumbach, A.
   Billaudelle, S.
   Breitwieser, O.
   Cramer, B.
   Dold, D.
   Kungl, A. F.
   Senn, W.
   Schemmel, J.
   Meier, K.
   Petrovici, M. A.
TI Fast and energy-efficient neuromorphic deep learning with first-spike
   times
SO NATURE MACHINE INTELLIGENCE
DT Article
ID NEURAL-NETWORKS; INTELLIGENCE; BRAIN; MODEL
AB For a biological agent operating under environmental pressure, energy consumption and reaction times are of critical importance. Similarly, engineered systems are optimized for short time-to-solution and low energy-to-solution characteristics. At the level of neuronal implementation, this implies achieving the desired results with as few and as early spikes as possible. With time-to-first-spike coding, both of these goals are inherently emerging features of learning. Here, we describe a rigorous derivation of a learning rule for such first-spike times in networks of leaky integrate-and-fire neurons, relying solely on input and output spike times, and show how this mechanism can implement error backpropagation in hierarchical spiking networks. Furthermore, we emulate our framework on the BrainScaleS-2 neuromorphic system and demonstrate its capability of harnessing the system's speed and energy characteristics. Finally, we examine how our approach generalizes to other neuromorphic platforms by studying how its performance is affected by typical distortive effects induced by neuromorphic substrates.
   Spiking neural networks promise fast and energy-efficient information processing. The 'time-to-first-spike' coding scheme, where the time elapsed before a neuron's first spike is utilized as the main variable, is a particularly efficient approach and Goltz and Kriener et al. demonstrate that error backpropagation, an essential ingredient for learning in neural networks, can be implemented in this scheme.
C1 [Goeltz, J.; Baumbach, A.; Billaudelle, S.; Breitwieser, O.; Cramer, B.; Dold, D.; Kungl, A. F.; Schemmel, J.; Meier, K.; Petrovici, M. A.] Heidelberg Univ, Kirchhoff Inst Phys, Heidelberg, Germany.
   [Goeltz, J.; Kriener, L.; Senn, W.; Petrovici, M. A.] Univ Bern, Dept Physiol, Bern, Switzerland.
   [Dold, D.] Siemens AG Technol, Siemens AI Lab, Munich, Germany.
RP Göltz, J; Petrovici, MA (corresponding author), Heidelberg Univ, Kirchhoff Inst Phys, Heidelberg, Germany.; Göltz, J; Kriener, L; Petrovici, MA (corresponding author), Univ Bern, Dept Physiol, Bern, Switzerland.
EM julian.goeltz@kip.uni-heidelberg.de; laura.kriener@unibe.ch;
   mihai.petrovici@unibe.ch
CR Aamir SA, 2018, IEEE T CIRCUITS-I, V65, P4299, DOI 10.1109/TCSI.2018.2840718
   Aamir SA, 2018, IEEE T BIOMED CIRC S, V12, P1027, DOI 10.1109/TBCAS.2018.2848203
   Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   [Anonymous], 2016, 2016 IEE INT C REB C, DOI [DOI 10.1109/ICRC.2016.7738691, 10.1109/ICRC.2016.7738691]
   Billaudelle S, 2020, IEEE INT SYMP CIRC S, DOI [10.48350/149640, 10.1109/ISCAS45731.2020.9180741]
   Bohte S. M., 2000, 8th European Symposium on Artificial Neural Networks. ESANN"2000. Proceedings, P419
   Brooks R, 2012, NATURE, V482, P462, DOI 10.1038/482462a
   Brown T., 2020, PROC ADV NEUR INF PR, P1877
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Chen GK, 2019, IEEE J SOLID-ST CIRC, V54, P992, DOI 10.1109/JSSC.2018.2884901
   Comsa JM, 2020, INT CONF ACOUST SPEE, P8529, DOI [10.1109/ICASSP40776.2020.9053856, 10.1109/icassp40776.2020.9053856]
   Cramer B, 2020, TRAINING SPIKING MUL
   Davies M, 2019, NAT MACH INTELL, V1, P386, DOI 10.1038/s42256-019-0097-1
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Dold D, 2019, NEURAL NETWORKS, V119, P200, DOI 10.1016/j.neunet.2019.08.002
   Drenick R, 1981, SYSTEM MODELING OPTI, P762, DOI [DOI 10.1007/BFB0006203, 10.1007/BFb0006203]
   Esser S.K., 2015, ADV NEURAL INFORM PR, P1117
   Feldmann J, 2019, NATURE, V569, P208, DOI 10.1038/s41586-019-1157-8
   Friedmann S, 2017, IEEE T BIOMED CIRC S, V11, P128, DOI 10.1109/TBCAS.2016.2579164
   Furber S, 2019, COMMUNICATING PROCES, P277
   Gerstner W, 2001, MATH MODELL, V13, P23
   Gerstner W, 1998, SPIKING NEURONS
   Gerstner W, 2009, SCIENCE, V326, P379, DOI 10.1126/science.1181936
   Gollisch T, 2008, SCIENCE, V319, P1108, DOI 10.1126/science.1149639
   Goltz J., 2019, MASTERARBEIT
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Hassabis D, 2017, NEURON, V95, P245, DOI 10.1016/j.neuron.2017.06.011
   Hubara I, 2018, J MACH LEARN RES, V18
   Huh D, 2018, ADV NEUR IN, V31
   Hunsberger E., 2016, ARXIV161105141
   Illing B, 2019, NEURAL NETWORKS, V118, P90, DOI 10.1016/j.neunet.2019.06.001
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Johansson RS, 2004, NAT NEUROSCI, V7, P170, DOI 10.1038/nn1177
   Jordan J, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-54137-7
   Kheradpisheh SR, 2020, INT J NEURAL SYST, V30, DOI 10.1142/S0129065720500276
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Kingma DP., 2014, 3 INT C LEARN REPR I
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
   Kungl AF, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.01201
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Leng LZW, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-28999-2
   Linnainmaa S., 1970, REPRESENTATION CUMUL
   Maass W, 2016, CURR OPIN BEHAV SCI, V11, P81, DOI 10.1016/j.cobeha.2016.06.003
   MEAD C, 1990, P IEEE, V78, P1629, DOI 10.1109/5.58356
   Moradi S, 2018, IEEE T BIOMED CIRC S, V12, P106, DOI 10.1109/TBCAS.2017.2759700
   Mostafa Hesham, 2018, IEEE Trans Neural Netw Learn Syst, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Muller E., 2020, EXTENDING BRAINSCALE
   Naud R, BURST DEPENDENT SYNA, DOI [10.1101/2020.03.30.015511(2020, DOI 10.1101/2020.03.30.015511(2020]
   Neftci E, 2014, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00272
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Neftci EO, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00241
   Ng A., 2016, HARVARD BUS REV
   Paszke A., 2019, ADV NEURAL INFORM PR, P8024
   Pei J, 2019, NATURE, V572, P106, DOI 10.1038/s41586-019-1424-8
   Petrovici M. A, YIN YANG DATASET
   Petrovici M. A., 2013, STOCHASTIC INFERENCE, V1311.3211
   Petrovici MA, 2016, SPRINGER THESES-RECO, P1, DOI 10.1007/978-3-319-39552-4
   Petrovici MA, 2016, PHYS REV E, V94, DOI 10.1103/PhysRevE.94.042312
   Petrovici MA, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0108590
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Prodromakis T., 2010, Proceedings of the 2010 17th IEEE International Conference on Electronics, Circuits and Systems (ICECS 2010), P934, DOI 10.1109/ICECS.2010.5724666
   Rauch A, 2003, J NEUROPHYSIOL, V90, P1598, DOI 10.1152/jn.00293.2003
   Richards BA, 2019, NAT NEUROSCI, V22, P1761, DOI 10.1038/s41593-019-0520-2
   Roy K, 2019, NATURE, V575, P607, DOI 10.1038/s41586-019-1677-2
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Sacramento J, 2018, ADV NEUR IN, V31
   Schemmel J., 2020, ACCELERATED ANALOG N
   Schemmel J, 2010, IEEE INT SYMP CIRC S, P1947, DOI 10.1109/ISCAS.2010.5536970
   Schmitt S, 2017, IEEE IJCNN, P2227, DOI 10.1109/IJCNN.2017.7966125
   Sejnowski TJ, 2018, DEEP LEARNING REVOLUTION, P1
   Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270
   Sornborger A., BACKPROPAGATION ALGO
   Stromatias E, 2015, IEEE IJCNN
   Tavanaei A, 2018, 2018 INT JOINT C NEU, DOI DOI 10.1109/IJCNN.2018.8489104
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Teeter C, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-017-02717-4
   Thakur CS, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00891
   Thorpe S, 1996, NATURE, V381, P520, DOI 10.1038/381520a0
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   van de Burgt Y, 2018, NAT ELECTRON, V1, P386, DOI 10.1038/s41928-018-0103-3
   Wu JY, 2019, 2019 5TH INTERNATIONAL CONFERENCE ON EVENT-BASED CONTROL, COMMUNICATION, AND SIGNAL PROCESSING (EBCCSP), DOI 10.1109/ebccsp.2019.8836892
   Wunderlich T, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00260
   Zenke F, 2018, NEURAL COMPUT, V30, P1514, DOI 10.1162/neco_a_01086
NR 83
TC 35
Z9 35
U1 1
U2 32
PD SEP
PY 2021
VL 3
IS 9
BP 823
EP 835
DI 10.1038/s42256-021-00388-x
WC Computer Science, Artificial Intelligence; Computer Science,
   Interdisciplinary Applications
DA 2023-11-11
ER

PT C
AU Sharma, V
   Srinivasan, D
AF Sharma, V.
   Srinivasan, D.
GP IEEE
TI A Spiking Neural Network Based on Temporal Encoding for Electricity
   Price Time Series Forecasting in Deregulated Markets
SO 2010 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS IJCNN 2010
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT World Congress on Computational Intelligence (WCCI 2010)
CY 2010
CL Barcelona, SPAIN
ID NEURONS
AB In this paper a general methodology is proposed for development of spiking neural networks (SNN) as a time series modeling task. A continuous firing temporal encoding scheme is employed in the developed model for efficient handling of temporal correlations in high dimensional chaotic time series. The universal nonlinear function approximation property and unique ability of temporally encoded SNN is particularly advantageous in complex dynamics scenario. Rich dynamics of spiking neural networks are exploited for forecasting in electricity price time series system. The temporal encoding scheme proposed particularly for time series applications produced interesting results which encourage further research in this direction.
C1 [Sharma, V.; Srinivasan, D.] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 117576, Singapore.
RP Sharma, V (corresponding author), Natl Univ Singapore, Dept Elect & Comp Engn, 4 Engn Dr 3, Singapore 117576, Singapore.
EM g0600080@nus.edu.sg; elesd@nus.edu.sg
CR [Anonymous], NEURAL NETWORKS COMP
   Belatreche A, 2007, SOFT COMPUT, V11, P239, DOI [10.1007/s00500-006-0065-7, 10.1007/S00500-006-0065-7]
   Belatreche A, 2006, NEW MATH NAT COMPUT, V2, P237, DOI 10.1142/S179300570600049X
   Benini M, 2002, 2002 IEEE POWER ENGINEERING SOCIETY SUMMER MEETING, VOLS 1-3, CONFERENCE PROCEEDINGS, P1354, DOI 10.1109/PESS.2002.1043596
   Beyer H.G., 2001, NATURAL COMPUTING SE, VXIX
   BIALEK W, 1991, SCIENCE, V252, P1854, DOI 10.1126/science.2063199
   Bialek W., 1997, SPIKES EXPLORING NEU
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Duncombe J. U., 2001, NEURAL NETWORKS, V14, P933
   Ghosh-Dastidar S, 2007, INTEGR COMPUT-AID E, V14, P187
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   Gonzalez-Nalda P, 2008, NEUROCOMPUTING, V71, P721, DOI 10.1016/j.neucom.2007.07.032
   HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8
   Iannella N, 2001, NEURAL NETWORKS, V14, P933, DOI 10.1016/S0893-6080(01)00080-6
   Kanamaru T, 2006, INT J BIFURCAT CHAOS, V16, P3309, DOI 10.1142/S021812740601680X
   Li Z, 2006, WCICA 2006: SIXTH WORLD CONGRESS ON INTELLIGENT CONTROL AND AUTOMATION, VOLS 1-12, CONFERENCE PROCEEDINGS, P2681
   Maass W, 1997, NEURAL COMPUT, V9, P279, DOI 10.1162/neco.1997.9.2.279
   Maass W, 1997, ADV NEUR IN, V9, P211
   Manda P, 2006, INT J ELEC POWER, V28, P367, DOI 10.1016/j.ijepes.2005.12.007
   Mass W., 1999, PULSE NEURAL NETWORK
   McKennoch S., P INT JOINT C NEUR N, P3970
   Natschläger T, 1999, NEUROCOMPUTING, V26-7, P463, DOI 10.1016/S0925-2312(99)00052-1
   Sanger TD, 1998, NEURAL COMPUT, V10, P1567, DOI 10.1162/089976698300017313
   Wu QX, 2006, NEUROCOMPUTING, V69, P1912, DOI 10.1016/j.neucom.2005.11.023
   Wysoski SG, 2008, NEUROCOMPUTING, V71, P2563, DOI 10.1016/j.neucom.2007.12.038
   Zareipour H., 2006, IEEE POW ENG SOC GEN
NR 26
TC 2
Z9 2
U1 0
U2 0
PY 2010
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Computer Science, Theory & Methods; Engineering,
   Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Torikai, H
   Saito, T
AF Torikai, H
   Saito, T
GP IEEE
   IEEE
   IEEE
   IEEE
TI Various synchronization patterns from a pulse-coupled neural network of
   chaotic spiking oscillators
SO IJCNN'01: INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS, VOLS 1-4,
   PROCEEDINGS
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN 01)
CY JUL 15-19, 2001
CL WASHINGTON, D.C.
AB We present a pulse-coupled neural network (PCNN) of chaotic spiking oscillators (CSOs) with a spike-train input. The PCNN exhibits the following phenomena: each CSO synchronizes with a part of the spike-train input (a) without overlapping or (b) with overlapping. We clarify parameter regions for these phenomena. The PCNN has various synchronization states and exhibits one of them depending on the initial state. The synchronization stability is caused by compulsory firings by the input. Using a simple test circuit, typical phenomena are verified in the laboratory.
C1 Hosei Univ, EEE Dept, Koganei, Tokyo 1848584, Japan.
RP Torikai, H (corresponding author), Hosei Univ, EEE Dept, Koganei, Tokyo 1848584, Japan.
EM torikai@k.hosei.ac.jp; tsaito@k.hosei.ac.jp
CR Fujii H, 1996, NEURAL NETWORKS, V9, P1303, DOI 10.1016/S0893-6080(96)00054-8
   Fusi S, 2000, NEURAL COMPUT, V12, P2227, DOI 10.1162/089976600300014917
   HOPFIELD JJ, 1995, P NATL ACAD SCI USA, V92, P6655, DOI 10.1073/pnas.92.15.6655
   Izhikevich EM, 1999, IEEE T NEURAL NETWOR, V10, P508, DOI 10.1109/72.761708
   KAWASAKI Y, 2001, P IEEE ISCAS, V3, P133
   MIROLLO RE, 1990, SIAM J APPL MATH, V50, P1645, DOI 10.1137/0150098
   Torikai H, 1999, IEICE T FUND ELECTR, VE82A, P1336
   TORIKAI H, 2000, P ICONIP, V2, P964
   TORIKAI H, 2000, P IEEE INNS IJCNN, V3, P291
   Watanabe M, 1998, ICONIP'98: THE FIFTH INTERNATIONAL CONFERENCE ON NEURAL INFORMATION PROCESSING JOINTLY WITH JNNS'98: THE 1998 ANNUAL CONFERENCE OF THE JAPANESE NEURAL NETWORK SOCIETY - PROCEEDINGS, VOLS 1-3, P1370
NR 10
TC 0
Z9 0
U1 0
U2 0
PY 2001
BP 670
EP 674
WC Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Computer Science, Theory & Methods
DA 2023-11-11
ER

PT J
AU Panda, P
   Aketi, SA
   Roy, K
AF Panda, Priyadarshini
   Aketi, Sai Aparna
   Roy, Kaushik
TI Toward Scalable, Efficient, and Accurate Deep Spiking Neural Networks
   With Backward Residual Connections, Stochastic Softmax, and
   Hybridization
SO FRONTIERS IN NEUROSCIENCE
DT Article
DE spiking neural networks; energy-efficiency; backward residual
   connection; stochastic softmax; hybridization; improved accuracy
ID INTELLIGENCE
AB Spiking Neural Networks (SNNs) may offer an energy-efficient alternative for implementing deep learning applications. In recent years, there have been several proposals focused on supervised (conversion, spike-based gradient descent) and unsupervised (spike timing dependent plasticity) training methods to improve the accuracy of SNNs on large-scale tasks. However, each of these methods suffer fromscalability, latency, and accuracylimitations. In this paper, we propose novel algorithmic techniques of modifying the SNN configuration withbackward residual connections, stochastic softmax, and hybrid artificial-and-spiking neuronal activationsto improve the learning ability of the training methodologies to yield competitive accuracy, while, yielding large efficiency gains over their artificial counterparts. Note, artificial counterparts refer to conventional deep learning/artificial neural networks. Our techniques apply to VGG/Residual architectures, and are compatible with all forms of training methodologies. Our analysis reveals that the proposed solutions yield near state-of-the-art accuracy with significant energy-efficiency and reduced parameter overhead translating to hardware improvements on complex visual recognition tasks, such as, CIFAR10, Imagenet datatsets.
C1 [Panda, Priyadarshini] Yale Univ, Dept Elect Engn, New Haven, CT 06520 USA.
   [Aketi, Sai Aparna; Roy, Kaushik] Purdue Univ, Sch Elect & Comp Engn, W Lafayette, IN 47907 USA.
RP Panda, P (corresponding author), Yale Univ, Dept Elect Engn, New Haven, CT 06520 USA.
EM priya.panda@yale.edu
CR Abadi M., 2016, P OSDI 16 12 USENIX, V16, P265, DOI DOI 10.1038/NN.3331
   Ankit A, 2017, DES AUT CON, DOI 10.1145/3061639.3062311
   Bellec G., 2018, ADV NEURAL INFORM PR, P787
   Burges, 2010, MNIST HANDWRITTEN DI, DOI DOI 10.1561/2400000035
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Comsa I. M., 2019, ARXIV190713223, DOI [10.1109/ICASSP40776.2020.9053856, DOI 10.1109/ICASSP40776.2020.9053856]
   Courbariaux Matthieu, 2016, ABS160202830 CORR
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Diehl Peter U, 2015, 2015 INT JOINT C NEU, P1, DOI [DOI 10.1109/IJCNN.2015.7280696, 10.1109/IJCNN.2015.7280696]
   Han S., 2015, ARXIV151000149
   Huang XW, 2015, ACTA POLYM SIN, P1133
   Hunsberger Eric, 2015, ARXIV151008829
   Indiveri Giacomo, 2011, Front Neurosci, V5, P118, DOI 10.3389/fnins.2011.00118
   Indiveri G, 2015, 2015 IEEE INTERNATIONAL ELECTRON DEVICES MEETING (IEDM)
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90
   Kheradpisheh SR, 2020, INT J NEURAL SYST, V30, DOI 10.1142/S0129065720500276
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Kubilius Jonas, 2018, CORNET MODELING NEUR, DOI DOI 10.1101/408385
   Lee C., 2019, ARXIV190306379, DOI [10.3389/fnins.2020.00119, DOI 10.3389/FNINS.2020.00119]
   Lee C, 2019, IEEE T COGN DEV SYST, V11, P384, DOI 10.1109/TCDS.2018.2833071
   Lee C, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00435
   Lee H. B., 2018, ADV NEURAL INFORM PR, P919
   Lee JM, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00191
   Lu S, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00535
   Masquelier T, 2007, PLOS COMPUT BIOL, V3, P247, DOI 10.1371/journal.pcbi.0030031
   Masquelier T, 2009, NEURAL COMPUT, V21, P1259, DOI 10.1162/neco.2008.06-08-804
   Mostafa H, 2018, IEEE T NEUR NET LEAR, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   Panda P., 2019, ARXIV191013931
   Panda P, 2018, IEEE J EM SEL TOP C, V8, P51, DOI 10.1109/JETCAS.2017.2769684
   Panda P, 2016, IEEE IJCNN, P299, DOI 10.1109/IJCNN.2016.7727212
   Pei J, 2019, NATURE, V572, P106, DOI 10.1038/s41586-019-1424-8
   Pérez-Carrasco JA, 2010, IEEE INT SYMP CIRC S, P1659, DOI 10.1109/ISCAS.2010.5537484
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32
   Roy K, 2019, NATURE, V575, P607, DOI 10.1038/s41586-019-1677-2
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Sengupta A, 2017, APPL PHYS REV, V4, DOI 10.1063/1.5012763
   Sengupta A, 2016, PHYS REV APPL, V6, DOI 10.1103/PhysRevApplied.6.064003
   Severa W, 2019, NAT MACH INTELL, V1, P86, DOI 10.1038/s42256-018-0015-y
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Srinivasan G, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00189
   Srinivasan G, 2018, ACM J EMERG TECH COM, V14, DOI 10.1145/3266229
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   van de Burgt Y, 2017, NAT MATER, V16, P414, DOI [10.1038/nmat4856, 10.1038/NMAT4856]
   Voelker A.R., 2020, ARXIV200203553
   Wang ZR, 2017, NAT MATER, V16, P101, DOI [10.1038/nmat4756, 10.1038/NMAT4756]
   WERBOS PJ, 1990, P IEEE, V78, P1550, DOI 10.1109/5.58337
   Zamarreño-Ramos C, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00026
   Zhou S., 2019, ARXIV190910837
NR 53
TC 51
Z9 51
U1 0
U2 11
PD JUN 30
PY 2020
VL 14
AR 653
DI 10.3389/fnins.2020.00653
WC Neurosciences
DA 2023-11-11
ER

PT J
AU Liu, MX
   Feng, J
   Wang, YT
   Li, ZH
AF Liu, Mingxin
   Feng, Jing
   Wang, Yongtian
   Li, Zhaohui
TI Classification of overlapping spikes using convolutional neural networks
   and long short term memory
SO COMPUTERS IN BIOLOGY AND MEDICINE
DT Article
DE Spike sorting; Overlapping; Convolution neural networks; Long short term
   memory
ID FUTURE
AB Spike sorting is one of the key techniques to understand brain activity. In this paper, we propose a novel deep learning approach based on convolutional neural networks (CNN) and long short term memory (LSTM) to implement overlapping spike sorting. The results of the simulated data demonstrated that the clustering accuracy was greater than 99.9% and 99.0% for non-overlapping spikes and overlapping spikes, respectively. Moreover, the proposed method performed better than our previous deep learning approach named "1D-CNN". In addition, the experimental data recorded from the primary visual cortex of a macaque monkey were used to evaluate the proposed method in a practical application. It was shown that the method could successfully isolate most overlapping spikes of different neurons (ranging from two to five). In summary, the CNN + LSTM method proposed in this paper is of great advantage for overlapping spike sorting with high accuracy. It lays the foundation for application in more challenging works, such as distinguishing the simultaneous recordings of multichannel neuronal activities.
C1 [Liu, Mingxin] Guangdong Ocean Univ, Coll Elect & Informat Engn, Zhanjiang 524088, Peoples R China.
   [Feng, Jing; Wang, Yongtian; Li, Zhaohui] Yanshan Univ, Sch Informat Sci & Engn, Qinhuangdao 066004, Hebei, Peoples R China.
   [Li, Zhaohui] Yanshan Univ, Hebei Key Lab Informat Transmiss & Signal Proc, Qinhuangdao 066004, Hebei, Peoples R China.
RP Li, ZH (corresponding author), Yanshan Univ, Sch Informat Sci & Engn, Qinhuangdao 066004, Hebei, Peoples R China.; Li, ZH (corresponding author), Yanshan Univ, Hebei Key Lab Informat Transmiss & Signal Proc, Qinhuangdao 066004, Hebei, Peoples R China.
EM liumx@gdou.edu.ca; fengjing@stumai.ysu.edu; yongtian.w@stumai.ysu.edu;
   lizhaohui@ysu.edu.cn
CR Adamos DA, 2008, COMPUT METH PROG BIO, V91, P232, DOI 10.1016/j.cmpb.2008.04.011
   Azami H, 2015, J NEUROSCI METH, V239, P129, DOI 10.1016/j.jneumeth.2014.10.006
   Bhattacharyya D., 2010, Q J ECON, V4, P31
   Buzsáki G, 2004, NAT NEUROSCI, V7, P446, DOI 10.1038/nn1233
   Chah E, 2011, J NEURAL ENG, V8, DOI 10.1088/1741-2560/8/1/016006
   Chan HL, 2010, NEUROCOMPUTING, V73, P1513, DOI 10.1016/j.neucom.2009.11.006
   Chen PL, 2021, BIOMED SIGNAL PROCES, V70, DOI 10.1016/j.bspc.2021.103026
   Chu C.C., 2014, MULTIELECTRODE RECOR, DOI [10.6080/K0J1012K, DOI 10.6080/K0J1012K]
   Chu CCJ, 2014, VISION RES, V96, P113, DOI 10.1016/j.visres.2014.01.008
   deCharms RC, 2000, ANNU REV NEUROSCI, V23, P613, DOI 10.1146/annurev.neuro.23.1.613
   Einevoll GT, 2012, CURR OPIN NEUROBIOL, V22, P11, DOI 10.1016/j.conb.2011.10.001
   Franke F, 2012, FRONT NEURAL CIRCUIT, V6, DOI 10.3389/fncir.2012.00105
   Frey U, 2009, BIOSENS BIOELECTRON, V24, P2191, DOI 10.1016/j.bios.2008.11.028
   Fujita H, 2019, INFORM SCIENCES, V486, P231, DOI 10.1016/j.ins.2019.02.065
   Goldberg Y, 2016, J ARTIF INTELL RES, V57, P345, DOI 10.1613/jair.4992
   Greenspan H, 2016, IEEE T MED IMAGING, V35, P1153, DOI 10.1109/TMI.2016.2553401
   Han JG, 2013, IEEE T CYBERNETICS, V43, P1318, DOI 10.1109/TCYB.2013.2265378
   Harris KD, 2000, J NEUROPHYSIOL, V84, P401, DOI 10.1152/jn.2000.84.1.401
   Harris KD, 2016, NAT NEUROSCI, V19, P1165, DOI 10.1038/nn.4365
   Horton PM, 2007, J NEUROSCI METH, V160, P52, DOI 10.1016/j.jneumeth.2006.08.013
   Huang LB, 2019, IEEE T NANOBIOSCI, V18, P283, DOI 10.1109/TNB.2019.2909010
   Hulata E, 2002, J NEUROSCI METH, V117, P1, DOI 10.1016/S0165-0270(02)00032-8
   Jun JJ, 2017, NATURE, V551, P232, DOI 10.1038/nature24636
   Kadir SN, 2014, NEURAL COMPUT, V26, P2379, DOI 10.1162/NECO_a_00661
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lee J., 2017, 31 ANN C NEURAL INFO
   Lee J., 2020, BIORXIV
   Lefebvre Baptiste, 2016, J Physiol Paris, V110, P327, DOI 10.1016/j.jphysparis.2017.02.005
   Letelier JC, 2000, J NEUROSCI METH, V101, P93, DOI 10.1016/S0165-0270(00)00250-8
   Li ZH, 2020, BRAIN SCI, V10, DOI 10.3390/brainsci10110835
   Litjens G, 2017, MED IMAGE ANAL, V42, P60, DOI 10.1016/j.media.2017.07.005
   Nenadic Z, 2005, IEEE T BIO-MED ENG, V52, P74, DOI 10.1109/TBME.2004.839800
   Park IY, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10010301
   Petrantonakis PC, 2017, IEEE T NEUR SYS REH, V25, P323, DOI 10.1109/TNSRE.2016.2640858
   Quiroga R.Q., 2017, WAVE CLUS UNSUPERVIS
   Quiroga RQ, 2012, CURR BIOL, V22, pR45, DOI 10.1016/j.cub.2011.11.005
   Quiroga RQ, 2004, NEURAL COMPUT, V16, P1661, DOI 10.1162/089976604774201631
   Ravì D, 2017, IEEE J BIOMED HEALTH, V21, P4, DOI 10.1109/JBHI.2016.2636665
   Rey HG, 2015, BRAIN RES BULL, V119, P106, DOI 10.1016/j.brainresbull.2015.04.007
   Rossant C, 2016, NAT NEUROSCI, V19, P634, DOI 10.1038/nn.4268
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Saif-ur-Rehman M, 2021, J NEURAL ENG, V18, DOI 10.1088/1741-2552/abc8d4
   Sarikaya R, 2014, IEEE-ACM T AUDIO SPE, V22, P778, DOI 10.1109/TASLP.2014.2303296
   Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003
   Shoham S, 2003, J NEUROSCI METH, V127, P111, DOI 10.1016/S0165-0270(03)00120-1
   Song RQ, 2015, 2015 12TH INTERNATIONAL COMPUTER CONFERENCE ON WAVELET ACTIVE MEDIA TECHNOLOGY AND INFORMATION PROCESSING (ICCWAMTIP), P391, DOI 10.1109/ICCWAMTIP.2015.7494016
   Spira ME, 2013, NAT NANOTECHNOL, V8, P83, DOI [10.1038/NNANO.2012.265, 10.1038/nnano.2012.265]
   Sukiban J, 2019, NEUROSCIENCE, V414, P168, DOI 10.1016/j.neuroscience.2019.07.005
   Takekawa T, 2010, EUR J NEUROSCI, V31, P263, DOI 10.1111/j.1460-9568.2009.07068.x
   Tang Y., 2021, IEEE T KNOWL DATA EN, V33, P1
   Tang Y., 2022, IEEE T CYBERN, V52, P1
   Tariq T, 2019, COMPUT METH PROG BIO, V179, DOI 10.1016/j.cmpb.2019.104986
   Varior RR, 2016, LECT NOTES COMPUT SC, V9911, P135, DOI 10.1007/978-3-319-46478-7_9
   Voulodimos A, 2018, COMPUT INTEL NEUROSC, V2018, DOI 10.1155/2018/7068349
   Wu HF, 2018, IEEE T NEUR SYS REH, V26, P1516, DOI 10.1109/TNSRE.2018.2848463
   Wu T, 2018, J NEURAL ENG, V15, DOI 10.1088/1741-2552/aae18d
   Xu H, 2019, J NEUROSCI METH, V311, P111, DOI 10.1016/j.jneumeth.2018.10.019
   Yang K, 2017, J PHYS CONF SER, V910, DOI 10.1088/1742-6596/910/1/012062
   Zhang PM, 2003, PROCEEDINGS OF 2003 INTERNATIONAL CONFERENCE ON NEURAL NETWORKS & SIGNAL PROCESSING, PROCEEDINGS, VOLS 1 AND 2, P712
NR 59
TC 3
Z9 3
U1 7
U2 21
PD SEP
PY 2022
VL 148
AR 105888
DI 10.1016/j.compbiomed.2022.105888
EA JUL 2022
WC Biology; Computer Science, Interdisciplinary Applications; Engineering,
   Biomedical; Mathematical & Computational Biology
DA 2023-11-11
ER

PT J
AU Ghani, A
   McDaid, L
   Belatreche, A
   Hall, S
   Huang, S
   Marsland, J
   Dowrick, T
   Smith, A
AF Ghani, A.
   McDaid, L.
   Belatreche, A.
   Hall, S.
   Huang, S.
   Marsland, J.
   Dowrick, T.
   Smith, A.
TI Evaluating the generalisation capability of a CMOS based synapse
SO NEUROCOMPUTING
DT Article
DE CMOS synapses; Spiking neural networks; CMOS implementation of spiking
   neurons
ID OPTIC-NERVE SIGNALS; SPIKING NEURONS; NEURAL-NETWORKS; SILICON RETINA;
   CHIP; DESIGN; AMPLIFICATION; CIRCUIT; MODELS
AB The focus of this work is to investigate the generalisation capability of compact, solid-state synapses recently proposed by the authors. The synapses can be configured to yield a static or dynamic response. Empirical models of the Post Synaptic Response (PSP), derived from hardware simulations, are developed and embedded into the neural network toolbox in MATLAB. A network of these synapses was then used to solve benchmark problems using a well-established training algorithm where the performance metric was convergence time, accuracy and weight range; the Spike Response Model (SRM) was used to implement point neurons. Results are presented and compared with standard synaptic responses. (C) 2011 Elsevier B.V. All rights reserved.
C1 [Ghani, A.] Newcastle Univ, Sch Elect Elect & Comp Engn, Newcastle Upon Tyne NE1 7RU, Tyne & Wear, England.
   [Ghani, A.; McDaid, L.; Belatreche, A.] Univ Ulster, Intelligent Syst Res Ctr, Derry BT487JL, North Ireland.
   [Hall, S.; Huang, S.; Marsland, J.; Dowrick, T.; Smith, A.] Univ Liverpool, Dept Elect Engn & Elect, Liverpool L69 3GJ, Merseyside, England.
RP Ghani, A (corresponding author), Newcastle Univ, Sch Elect Elect & Comp Engn, Newcastle Upon Tyne NE1 7RU, Tyne & Wear, England.
EM arfan.ghani@newcastle.ac.uk; s.hall@liverpool.ac.uk
CR Belatreche A, 2007, SOFT COMPUT, V11, P239, DOI [10.1007/s00500-006-0065-7, 10.1007/S00500-006-0065-7]
   Bofill-i-Petit A, 2004, IEEE T NEURAL NETWOR, V15, P1296, DOI 10.1109/TNN.2004.832842
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Chen YJ, 2008, IEEE IJCNN, P1615, DOI 10.1109/IJCNN.2008.4634013
   DELBRUCK T, 1993, IEEE T NEURAL NETWOR, V4, P529, DOI 10.1109/72.217194
   DEYONG MR, 1992, IEEE T NEURAL NETWOR, V3, P363, DOI 10.1109/72.129409
   Diorio C, 2002, P IEEE, V90, P345, DOI 10.1109/5.993402
   Fisher RA, 1936, ANN EUGENIC, V7, P179, DOI 10.1111/j.1469-1809.1936.tb02137.x
   Fragniere E, 1997, ANALOG INTEGR CIRC S, V13, P19, DOI 10.1023/A:1008234622348
   Gerstner W., 2002, SPIKING NEURON MODEL
   Hahnloser RHR, 2000, NATURE, V405, P947, DOI 10.1038/35016072
   Horio Y, 2003, IEEE T NEURAL NETWOR, V14, P1393, DOI 10.1109/TNN.2003.816349
   Horiuchi T, 2001, AUTON ROBOT, V11, P241, DOI 10.1023/A:1012486906587
   Koickal TJ, 2007, IEEE T CIRCUITS-I, V54, P60, DOI 10.1109/TCSI.2006.888677
   Liu SC, 2004, IEEE T NEURAL NETWOR, V15, P1305, DOI 10.1109/TNN.2004.832725
   Maass W, 1998, LECT NOTES COMPUT SC, V1450, P72, DOI 10.1007/BFb0055758
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   MAASS W., 1999, PULSED NEURAL NETWOR
   MAASS W, 2003, HDB BRAIN THEORY NEU, P1080
   MAHOWALD MA, 1991, SCI AM, V264, P76, DOI 10.1038/scientificamerican0591-76
   Merolla PA, 2007, IEEE T CIRCUITS-I, V54, P301, DOI 10.1109/TCSI.2006.887474
   Mokhtar M, 2008, LECT NOTES COMPUT SC, V5216, P362
   Pelikan M, 2003, INFORM SCIENCES, V156, P147, DOI 10.1016/S0020-0255(03)00174-9
   Rasche C, 2007, IEEE T NEURAL NETWOR, V18, P520, DOI 10.1109/TNN.2006.884679
   Wen B, 2009, IEEE T BIOMED CIRC S, V3, P444, DOI 10.1109/TBCAS.2009.2027127
   WOLDBERG WW, 1990, P NATL ACAD SCI USA, V87, P9193
   Wu QX, 2006, NEUROCOMPUTING, V69, P1912, DOI 10.1016/j.neucom.2005.11.023
   Yao X, 1999, IEEE T EVOLUT COMPUT, V3, P82, DOI 10.1109/4235.771163
   Zaghloul KA, 2004, IEEE T BIO-MED ENG, V51, P667, DOI 10.1109/TBME.2003.821040
   Zaghloul KA, 2004, IEEE T BIO-MED ENG, V51, P657, DOI 10.1109/TBME.2003.821039
NR 30
TC 2
Z9 3
U1 0
U2 8
PD APR 15
PY 2012
VL 83
BP 188
EP 197
DI 10.1016/j.neucom.2011.12.010
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT C
AU Luo, YL
   Wan, L
   Liu, JX
   Zhang, JL
   Cao, Y
AF Luo, Yuling
   Wan, Lei
   Liu, Junxiu
   Zhang, Jinlei
   Cao, Yi
BE Liu, D
   Xie, S
   Li, Y
   Zhao, D
   ElAlfy, ESM
TI An Efficient Hardware Architecture for Multilayer Spiking Neural
   Networks
SO NEURAL INFORMATION PROCESSING (ICONIP 2017), PT VI
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 24th International Conference on Neural Information Processing (ICONIP)
CY NOV 14-18, 2017
CL Guangzhou, PEOPLES R CHINA
DE Spiking Neural Networks; Hardware architecture; FPGA
ID IMPLEMENTATIONS
AB Spiking Neural Network (SNN) is the most recent computational model that can emulate the behaviors of biological neuron system. This paper highlights and discusses an efficient hardware architecture for the hardware SNNs, which includes a layer-level tile architecture (LTA) for the neurons and synapses, and a novel routing architecture (NRA) for the interconnections between the neuron nodes. In addition, a visualization performance monitoring platform is designed, which is used as functional verification and performance monitoring for the SNN hardware system. Experimental results demonstrate that the proposed architecture is feasible and capable of scaling to large hardware multilayer SNNs.
C1 [Luo, Yuling; Wan, Lei; Liu, Junxiu; Zhang, Jinlei] Guangxi Normal Univ, Fac Elect Engn, Guangxi Key Lab Multisource Informat Min & Secur, Guilin 541004, Peoples R China.
   [Cao, Yi] Univ Surrey, Surrey Business Sch, Dept Business Transformat & Sustainable Enterpris, Surrey GU2 7XH, England.
RP Liu, JX (corresponding author), Guangxi Normal Univ, Fac Elect Engn, Guangxi Key Lab Multisource Informat Min & Secur, Guilin 541004, Peoples R China.
EM liujunxiu@mailbox.gxnu.edu.cn
CR Beuler Marcel, 2012, Artificial Neural Networks and Machine Learning - ICANN 2012. Proceedings of the 22nd International Conference on Artificial Neural Networks, P97, DOI 10.1007/978-3-642-33269-2_13
   Carrillo S, 2013, IEEE T PARALL DISTR, V24, P2451, DOI 10.1109/TPDS.2012.289
   Carrillo S, 2012, NEURAL NETWORKS, V33, P42, DOI 10.1016/j.neunet.2012.04.004
   Fidjeland Andreas K, 2010, 2010 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2010.5596678
   Iakymchuk T., 2012, 7 INT WORKSH REC COM, P1245
   Maguire LP, 2007, NEUROCOMPUTING, V71, P13, DOI 10.1016/j.neucom.2006.11.029
   Moctezuma JC, 2015, MICROPROCESS MICROSY, V39, P693, DOI 10.1016/j.micpro.2015.09.003
   Pande S, 2013, NEURAL PROCESS LETT, V38, P131, DOI 10.1007/s11063-012-9274-5
   Schemmel J, 2008, IEEE IJCNN, P431, DOI 10.1109/IJCNN.2008.4633828
   STEIN RB, 1965, BIOPHYS J, V5, P173, DOI 10.1016/S0006-3495(65)86709-1
   Tsodyks M, 1998, NEURAL COMPUT, V10, P821, DOI 10.1162/089976698300017502
   Wade JJ, 2010, IEEE T NEURAL NETWOR, V21, P1817, DOI 10.1109/TNN.2010.2074212
NR 12
TC 0
Z9 0
U1 1
U2 4
PY 2017
VL 10639
BP 786
EP 795
DI 10.1007/978-3-319-70136-3_83
PN VI
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods
DA 2023-11-11
ER

PT C
AU Polidori, E
   Camisa, G
   Mesri, A
   Ferrari, G
   Polidori, C
   Mastella, M
   Prati, E
AF Polidori, Elisabetta
   Camisa, Giovanni
   Mesri, Alireza
   Ferrari, Giorgio
   Polidori, Cristina
   Mastella, Michele
   Prati, Enrico
GP IEEE
TI Experimental validation of an analog spiking neural network with STDP
   learning rule in CMOS technology
SO 2022 IEEE INTERNATIONAL CONFERENCE ON METROLOGY FOR EXTENDED REALITY,
   ARTIFICIAL INTELLIGENCE AND NEURAL ENGINEERING (METROXRAINE)
DT Proceedings Paper
CT IEEE International Conference on Metrology for Extended Reality,
   Artificial Intelligence and Neural Engineering (IEEE MetroXRAINE)
CY OCT 26-28, 2022
CL Rome, ITALY
DE spiking neural network; floating gate; STDP; CMOS technology; analog
   circuits
AB We report the design in CMOS technology and the experimental characterization of an analog spiking neural network with on-chip unsupervised learning. Long-term synaptic memory is implemented using a floating-gate device in a standard 150 nm CMOS process. The neurons are operated with a voltage supply of only 0.4V, allowing an extremely low power dissipation with an energy dissipation per synaptic operation of about 55 fJ. The CMOS chip includes the circuits for implementing real-time learning of the network based on the Spike Time Dependent Plasticity algorithm. During the learning, the neurons produce pulses of +/- 4.5 V that change the synaptic weight by activating tunneling currents to change the charge in the floating gates.
C1 [Polidori, Elisabetta; Camisa, Giovanni; Mesri, Alireza; Polidori, Cristina] Politecn Milan, DEIB, Milan, Italy.
   [Ferrari, Giorgio] Politecn Milan, Phys Dept, Milan, Italy.
   [Mastella, Michele] Univ Groningen, Zernike Inst Adv Mat & CogniGron, Groningen, Netherlands.
   [Prati, Enrico] Univ Milan, Phys Dept, Milan, Italy.
RP Ferrari, G (corresponding author), Politecn Milan, Phys Dept, Milan, Italy.
EM giorgio.ferrari@polimi.it
CR Aamir SA, 2018, IEEE T BIOMED CIRC S, V12, P1027, DOI 10.1109/TBCAS.2018.2848203
   Basuki Akbari Indra, 2022, 2022 5th International Conference on Networking, Information Systems and Security: Envisage Intelligent Systems in 5g//6G-based Interconnected Digital Worlds (NISS), P1, DOI 10.1109/NISS55057.2022.10085020
   Bouvier M, 2019, ACM J EMERG TECH COM, V15, DOI 10.1145/3304103
   Covi E, 2021, FRONT NEUROSCI-SWITZ, V15, DOI 10.3389/fnins.2021.611300
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Dupeyroux J, 2021, IEEE INT CONF ROBOT, P96, DOI 10.1109/ICRA48506.2021.9560937
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   Hagenaars JJ, 2021, Arxiv, DOI arXiv:2106.01862
   Hasani R, 2021, NANO EXPRESS, V2, DOI 10.1088/2632-959X/abf2ae
   HEINRICH B, 1975, J COMP PHYSIOL, V96, P155, DOI 10.1007/BF00706595
   Indiveri G, 2015, 2015 IEEE INTERNATIONAL ELECTRON DEVICES MEETING (IEDM)
   Kornijcuk V, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-17889-8
   Loukola OJ, 2017, SCIENCE, V355, P834, DOI 10.1126/science.aag2360
   Maass W, 2014, P IEEE, V102, P860, DOI 10.1109/JPROC.2014.2310593
   Mastella M, 2020, 2020 2ND IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE CIRCUITS AND SYSTEMS (AICAS 2020), P213, DOI [10.1109/aicas48895.2020.9073965, 10.1109/AICAS48895.2020.9073965]
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Nease S, 2016, IEEE INT SYMP CIRC S, P2507, DOI 10.1109/ISCAS.2016.7539102
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Sharifshazileh M, 2021, NAT COMMUN, V12, DOI 10.1038/s41467-021-23342-2
   Usami K., 1995, Proceedings. 1995 International Symposium on Low Power Design, P3, DOI 10.1145/224081.224083
   Valle G, 2018, NEURON, V100, P37, DOI 10.1016/j.neuron.2018.08.033
   Vasas V, 2019, ISCIENCE, V11, P85, DOI 10.1016/j.isci.2018.12.009
   Vogginger B, 2022, FRONT NEUROSCI-SWITZ, V16, DOI 10.3389/fnins.2022.851774
   Zheng N., 2019, LEARNING ENERGY EFFI
NR 24
TC 0
Z9 0
U1 0
U2 0
PY 2022
BP 187
EP 192
DI 10.1109/MetroXRAINE54828.2022.9967583
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic; Instruments & Instrumentation
DA 2023-11-11
ER

PT J
AU AMIT, DJ
   TSODYKS, MV
AF AMIT, DJ
   TSODYKS, MV
TI QUANTITATIVE STUDY OF ATTRACTOR NEURAL NETWORK RETRIEVING AT LOW SPIKE
   RATES .1. SUBSTRATE SPIKES, RATES AND NEURONAL GAIN
SO NETWORK-COMPUTATION IN NEURAL SYSTEMS
DT Article
ID PRIMATE TEMPORAL CORTEX; LOW FIRING RATES; ASSOCIATIVE MEMORY;
   TERM-MEMORY; CORRELATE
AB We discuss the conversion of the description of the dynamics of a neural network from a temporal variation of synaptic currents driven by point spikes and modulated by a synaptic structure to a description of the current dynamics driven by spike rates.  The conditions for the validity of such a conversion are discussed in detail and are shown to be quite realistic in cortical conditions.  This is done in preparation for a discussion of a scenario of an attractor neural network, based on the interaction of synaptic currents and neural spike rates.
   The spike rates are then expressed in terms of the currents themselves to provide a closed set of dynamical equations for the currents.  The current-rate relation is expressed as a neuronal gain function, converting currents into spike rates.  It describes an integrate-and-fire element with noisy inputs, under explicit quantitative conditions which we argue to be plausible in a cortical situation.  In particular, it is shown that the gain of the current to rate transduction function, deduced from realistic parameters, does not exclude the possibility of a stable operation of the prospective ANN at low spike rates.  The actual integration into an associative memory network is left for the consecutive article.
RP AMIT, DJ (corresponding author), HEBREW UNIV JERUSALEM,RACAH INST PHYS,JERUSALEM,ISRAEL.
CR ABBOTT LF, 1991, NETWORK-COMP NEURAL, V2, P245, DOI 10.1088/0954-898X/2/3/002
   ABBOTT LF, 1990, STATISTICAL MECHANIC
   Amit D. J., 1989, MODELING BRAIN FUNCT
   AMIT DJ, 1991, NETWORK-COMP NEURAL, V2, P275, DOI 10.1088/0954-898X/2/3/004
   AMIT DJ, 1991, NETWORK, V1, P381
   AMIT DJ, 1989, P NATL ACAD SCI USA, V86, P7671
   BUHMANN J, 1989, PHYS REV A, V39, P2689, DOI 10.1103/PhysRevA.39.2689
   BUHMANN J, 1989, PHYS REV A, V40, P4145, DOI 10.1103/PhysRevA.40.4145
   COHEN MA, 1983, IEEE T SYST MAN CYB, V13, P813
   COWAN JD, 1972, STOCHASTIC MODEL NEU
   FROLOV AA, 1986, BIOFIZIKA+, V31, P304
   Gnedenko, 1962, THEORY PROBABILITY
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Holden AV., 1976, MODELS STOCHASTIC AC
   HOPFIELD JJ, 1984, P NATL ACAD SCI-BIOL, V81, P3088, DOI 10.1073/pnas.81.10.3088
   KRYUKOV VI, 1990, STOCHASTIC CELLULAR
   MCCORMICK DA, 1985, J NEUROPHYSIOL, V54, P782, DOI 10.1152/jn.1985.54.4.782
   MIYASHITA Y, 1988, NATURE, V331, P68, DOI 10.1038/331068a0
   MIYASHITA Y, 1988, NATURE, V335, P817, DOI 10.1038/335817a0
   NELKEN I, 1988, BIOL CYBERN, V59
   SNEDECOR GW, 1969, STATISTICAL METHODS
   TREVES A, 1989, J PHYS A-MATH GEN, V22, P2205, DOI 10.1088/0305-4470/22/12/020
   TREVES A, 1990, PHYS REV A, V42, P2418, DOI 10.1103/PhysRevA.42.2418
   TSODYKS MV, 1988, EUROPHYS LETT, V6, P101, DOI 10.1209/0295-5075/6/2/002
   TUCKWELL C, 1988, INTRO THEORETICAL NE, V2, P90501
   WILSON HR, 1972, BIOPHYS J, V12, P1, DOI 10.1016/S0006-3495(72)86068-5
NR 26
TC 192
Z9 194
U1 0
U2 10
PD AUG
PY 1991
VL 2
IS 3
BP 259
EP 273
DI 10.1088/0954-898X/2/3/003
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic; Neurosciences
DA 2023-11-11
ER

PT J
AU Zheng, N
   Mazumder, P
AF Zheng, Nan
   Mazumder, Pinaki
TI Hardware-Friendly Actor-Critic Reinforcement Learning Through Modulation
   of Spike-Timing-Dependent Plasticity
SO IEEE TRANSACTIONS ON COMPUTERS
DT Article
DE Reinforcement learning; spiking neural network; hardware neural network;
   spike-timing-dependent plasticity; and actor-critic network
ID NEURAL-NETWORK; ANALOG; IMPLEMENTATION; ELECTRONICS; MODEL
AB In this work, we propose a hardware-friendly reinforcement learning algorithm. The learning algorithm is based on an actor-critic structure implemented with spiking neural networks (SNNs). A biologically plausible and hardware-friendly spike-timing-dependent plasticity learning rule is formulated and employed in the training of SNNs. Several important aspects of applying the learning rule in a reinforcement learning context is studied, especially from the circuit designers' point of view. Pitfalls of potential noise mixing and correlated spikes are identified and properly addressed. To feature a low-power learning architecture, techniques such as down-sampling data for certain learning blocks, injecting quantization noise as noisy residues in neurons, and proper memory partitioning are proposed. A 1-D state-value function learning problem and a 2-D maze walking problem are examined in this paper to illustrate effectiveness of the proposed algorithm and learning rules. A low-power hardware architecture is proposed and examples are implemented with Verilog. Hardware complexity of the proposed algorithm is analyzed, and potential solutions to breaking memory bottleneck when the size of the problem gets large is also discussed.
C1 [Zheng, Nan; Mazumder, Pinaki] Univ Michigan, Dept EECS, Ann Arbor, MI 48105 USA.
RP Zheng, N (corresponding author), Univ Michigan, Dept EECS, Ann Arbor, MI 48105 USA.
EM zhengn@umich.edu; mazum@umich.edu
CR [Anonymous], 2011, 45 ANN C INFORM SCI
   Barangi M, 2015, IEEE T MAGN, V51, DOI 10.1109/TMAG.2014.2374556
   Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Boahen KA, 2000, IEEE T CIRCUITS-II, V47, P416, DOI 10.1109/82.842110
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Cruz-Albrecht JM, 2012, IEEE T BIOMED CIRC S, V6, P246, DOI 10.1109/TBCAS.2011.2174152
   Duan SK, 2015, IEEE T NEUR NET LEAR, V26, P1202, DOI 10.1109/TNNLS.2014.2334701
   Ebong IE, 2012, P IEEE, V100, P2050, DOI 10.1109/JPROC.2011.2173089
   Fan DL, 2015, IEEE T NANOTECHNOL, V14, P1013, DOI 10.1109/TNANO.2015.2437902
   Florian RV, 2007, NEURAL COMPUT, V19, P1468, DOI 10.1162/neco.2007.19.6.1468
   Frémaux N, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003024
   Hinton G., 2007, P NIPS 2007 DEEP LEA, P15
   Kim JY, 2010, IEEE J SOLID-ST CIRC, V45, P32, DOI 10.1109/JSSC.2009.2031768
   Knag P, 2015, IEEE J SOLID-ST CIRC, V50, P1070, DOI 10.1109/JSSC.2014.2386892
   Lian JM, 2008, IEEE T NEURAL NETWOR, V19, P460, DOI 10.1109/TNN.2007.909842
   Lu J, 2015, IEEE J SOLID-ST CIRC, V50, P270, DOI 10.1109/JSSC.2014.2356197
   Maeda Y, 2003, IEEE T NEURAL NETWOR, V14, P688, DOI 10.1109/TNN.2003.811357
   Markram H., 2012, FRONTIERS SYNAPTIC N, V4, P35
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Park J, 2014, IEEE T CIRCUITS-I, V61, P476, DOI 10.1109/TCSI.2013.2284188
   Park J, 2012, IEEE J SOLID-ST CIRC, V47, P2711, DOI 10.1109/JSSC.2012.2211691
   Patel ND, 2007, IEEE T NEURAL NETWOR, V18, P1488, DOI 10.1109/TNN.2007.895822
   Pearson MJ, 2007, IEEE T NEURAL NETWOR, V18, P1472, DOI 10.1109/TNN.2007.891203
   Potjans W, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1001133
   Potjans W, 2009, NEURAL COMPUT, V21, P301, DOI 10.1162/neco.2008.08-07-593
   Raicevic P, 2006, NEUROCOMPUTING, V69, P2171, DOI 10.1016/j.neucom.2005.07.008
   Sarpeshkar R, 1998, NEURAL COMPUT, V10, P1601, DOI 10.1162/089976698300017052
   Serrano-Gotarredona R, 2009, IEEE T NEURAL NETWOR, V20, P1417, DOI 10.1109/TNN.2009.2023653
   Sjostrom J., 2010, SPIKE TIMING DEPENDE, V5
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Spall JC, 1998, J HOPKINS APL TECH D, V19, P482
   Srinivasa N, 2012, IEEE PULSE, V3, P51, DOI 10.1109/MPUL.2011.2175639
   Sutton R. S., 1998, REINFORCEMENT LEARNI, V1
   Vasilaki E., 2009, PLOS COMPUT BIOL, V5
   Yilmaz Y., 2015, U.S. Patent, Patent No. [20 150 332 763, 20150332763]
NR 35
TC 11
Z9 12
U1 1
U2 15
PD FEB
PY 2017
VL 66
IS 2
BP 299
EP 311
DI 10.1109/TC.2016.2595580
WC Computer Science, Hardware & Architecture; Engineering, Electrical &
   Electronic
DA 2023-11-11
ER

PT J
AU Zhang, J
   Huang, LW
   Ma, ZY
   Zhou, HH
AF Zhang, Jie
   Huang, Liwei
   Ma, Zhengyu
   Zhou, Huihui
TI Predicting the temporal-dynamic trajectories of cortical neuronal
   responses in non-human primates based on deep spiking neural network
SO COGNITIVE NEURODYNAMICS
DT Article; Early Access
DE Neural prediction; Temporal dynamics; Spiking neural network;
   Convolutional neural network; Visual cortex
ID RECEPTIVE FIELDS
AB Deep convolutional neural networks (CNNs) are commonly used as computational models for the primate ventral stream, while deep spiking neural networks (SNNs) incorporated with both the temporal and spatial spiking information still lack investigation. We compared performances of SNN and CNN in prediction of visual responses to the naturalistic stimuli in area V4, inferior temporal (IT), and orbitofrontal cortex (OFC). The accuracies based on SNN were significantly higher than that of CNN in prediction of temporal-dynamic trajectory and averaged firing rate of visual response in V4 and IT. The temporal dynamics were captured by SNN for neurons with diverse temporal profiles and category selectivities, and most sensitively captured around the time of peak responses for each brain region. Consistently, SNN activities showed significantly stronger correlations with IT, V4 and OFC responses. In SNN, correlations with neural activities were stronger for later time-step features than early time-step features. The temporal-dynamic prediction was also significantly improved by considering preceding neural activities during the prediction. Thus, our study demonstrated SNN as a powerful temporal-dynamic model for cortical responses to complex naturalistic stimuli.
C1 [Zhang, Jie; Zhou, Huihui] Chinese Acad Sci, Shenzhen Inst Adv Technol, Brain Cognit & Brain Dis Inst, Shenzhen 518055, Peoples R China.
   [Zhang, Jie] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
   [Zhang, Jie; Huang, Liwei; Ma, Zhengyu; Zhou, Huihui] Peng Cheng Lab, Shenzhen 518000, Peoples R China.
   [Huang, Liwei] Peking Univ, Beijing 100871, Peoples R China.
RP Zhou, HH (corresponding author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Brain Cognit & Brain Dis Inst, Shenzhen 518055, Peoples R China.; Ma, ZY; Zhou, HH (corresponding author), Peng Cheng Lab, Shenzhen 518000, Peoples R China.
EM mazhy@pcl.ac.cn; zhouhh@pcl.ac.cn
CR Brincat SL, 2006, NEURON, V49, P17, DOI 10.1016/j.neuron.2005.11.026
   Cadena SA, 2019, PLOS COMPUT BIOL, V15, DOI 10.1371/journal.pcbi.1006897
   Cadieu CF, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003963
   Carandini M, 2005, J NEUROSCI, V25, P10577, DOI 10.1523/JNEUROSCI.3726-05.2005
   Chang L, 2021, CURR BIOL, V31, P2785, DOI 10.1016/j.cub.2021.04.014
   Dapello J., 2020, NEURIPS, V33, P13073, DOI DOI 10.1101/2020.06.16.154542
   David SV, 2005, NETWORK-COMP NEURAL, V16, P239, DOI 10.1080/09548980500464030
   Fang W., 2021, ADV NEURAL INFORM PR, V34, P21056, DOI DOI 10.48550/ARXIV.2102.04159
   Fang Wei, 2020, SPIKINGJELLY
   Higgins I, 2021, NAT COMMUN, V12, DOI 10.1038/s41467-021-26751-5
   Huang LW, 2023, Arxiv, DOI arXiv:2303.06060
   HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837
   HUBEL DH, 1959, J PHYSIOL-LONDON, V148, P574, DOI 10.1113/jphysiol.1959.sp006308
   Issa EB, 2018, ELIFE, V7, DOI 10.7554/eLife.42870
   Kar K, 2019, NAT NEUROSCI, V22, P974, DOI 10.1038/s41593-019-0392-5
   Kriegeskorte N, 2008, FRONT SYST NEUROSCI, V2, DOI 10.3389/neuro.06.004.2008
   Kubilius J, 2019, ADV NEUR IN, V32
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Meyer T, 2014, NAT NEUROSCI, V17, P1388, DOI 10.1038/nn.3794
   Mounier Eslam, 2021, Brain Inform, V8, P11, DOI 10.1186/s40708-021-00132-6
   Nayebi A, 2018, ADV NEUR IN, V31
   Oláh VJ, 2022, ELIFE, V11, DOI 10.7554/eLife.79535
   Paszke A, 2019, ADV NEUR IN, V32
   Pillow JW, 2008, NATURE, V454, P995, DOI 10.1038/nature07140
   Schrimpf Martin, 2020, BIORXIV, P8, DOI [10.1101/407007, DOI 10.1101/407007]
   Triplett MA, 2019, FRONT NEURAL CIRCUIT, V13, DOI 10.3389/fncir.2019.00001
   Winding M, 2023, SCIENCE, V379, P995, DOI 10.1126/science.add9330
   Yamins DLK, 2014, P NATL ACAD SCI USA, V111, P8619, DOI 10.1073/pnas.1403112111
   Zhang J, 2021, BIORXIV, V2021-11, DOI [10.1101/2021.11.22.469359, DOI 10.1101/2021.11.22.469359]
   Zheng HL, 2021, AAAI CONF ARTIF INTE, V35, P11062
   Zhuang CX, 2021, P NATL ACAD SCI USA, V118, DOI 10.1073/pnas.2014196118
NR 31
TC 0
Z9 0
U1 0
U2 0
PD 2023 SEP 5
PY 2023
DI 10.1007/s11571-023-09989-1
EA SEP 2023
WC Neurosciences
DA 2023-11-11
ER

PT J
AU Adibi, P
   Meybodi, MR
   Safabakhsh, R
AF Adibi, P
   Meybodi, MR
   Safabakhsh, R
TI Unsupervised learning of synaptic delays based on learning automata in
   an RBF-like network of spiking neurons for data clustering
SO NEUROCOMPUTING
DT Article
DE spiking neural networks; delay learning; learning automata; data
   clustering
ID SELF-ORGANIZATION
AB In this paper, a new delay shift approach for learning in an RBF-like neural network structure of spiking neurons is introduced. The synaptic connections between the input and the RBF neurons are single delayed connections and the delays are adapted during an unsupervised learning process. Each synaptic connection in this network is modeled by a learning automaton. The action of the automaton associated with each connection is considered as the delay of the corresponding synaptic connection. It is shown through simulations that the clustering precision of the proposed network is considerably higher than that of the existing similar neural networks. (c) 2004 Elsevier B.V. All rights reserved.
C1 Amirkabir Univ Technol, Dept Comp Engn, Soft Comp Lab, Tehran, Iran.
   Amirkabir Univ Technol, Dept Comp Engn, Computat Vis Intelligence Lab, Tehran, Iran.
RP Meybodi, MR (corresponding author), Amirkabir Univ Technol, Dept Comp Engn, Soft Comp Lab, Hafez Ave, Tehran, Iran.
EM meybodi@ce.aut.ac.ir
CR Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Bohte SM, 2000, IEEE IJCNN, P279, DOI 10.1109/IJCNN.2000.861316
   Choe Y, 1998, NEUROCOMPUTING, V21, P139, DOI 10.1016/S0925-2312(98)00040-X
   Eurich CW, 2000, NEUROCOMPUTING, V32, P741, DOI 10.1016/S0925-2312(00)00239-3
   Eurich CW, 1999, PHYS REV LETT, V82, P1594, DOI 10.1103/PhysRevLett.82.1594
   Gerstner W, 1998, PULSED NEURAL NETWORKS, P3
   HINTON GE, 2000, ADV NEURAL INFORMATI, V12
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   KOHONEN T, 2001, SELFORGANIZING MAPS
   Maass W, 1998, PULSED NEURAL NETWORKS, P55
   Maass W, 1997, NETWORK-COMP NEURAL, V8, P355, DOI 10.1088/0954-898X/8/4/002
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Murphy P. M., 2004, UCI REPOSITORY MACHI
   Narendra K., 1989, LEARNING AUTOMATA
   Natschlager T, 1998, NETWORK-COMP NEURAL, V9, P319, DOI 10.1088/0954-898X/9/3/003
   OKEEFE J, 1993, HIPPOCAMPUS, V3, P317, DOI 10.1002/hipo.450030307
   PERRETT DI, 1982, EXP BRAIN RES, V47, P329
   Ruf B, 1998, IEEE T NEURAL NETWOR, V9, P575, DOI 10.1109/72.668899
   RUF B, 1998, THESIS TU GRAZ AUSTR
   Sommer FT, 2001, NEURAL NETWORKS, V14, P825, DOI 10.1016/S0893-6080(01)00064-8
   THORPE SJ, 1989, CONNECTIONISM IN PERSPECTIVE, P63
   VONDERMALSBURG C, 1981, 812 MAX PLANCK I DEP
NR 22
TC 15
Z9 15
U1 0
U2 3
PD MAR
PY 2005
VL 64
BP 335
EP 357
DI 10.1016/j.neucom.2004.10.111
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT C
AU Hunter, R
   Cobb, S
   Graham, BP
AF Hunter, Russell
   Cobb, Stuart
   Graham, Bruce P.
BE Kurkova, V
   Neruda, R
   Koutnik, J
TI Improving associative memory in a network of spiking neurons
SO ARTIFICIAL NEURAL NETWORKS - ICANN 2008, PT II
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 18th International Conference on Artificial Neural Networks (ICANN 2008)
CY SEP 03-06, 2008
CL Prague, CZECH REPUBLIC
DE associative memory; mammalian hippocampus; neural networks; pattern
   recall; inhibition
ID HIPPOCAMPAL FUNCTION; INTERNEURONS; MODEL; OSCILLATIONS
AB Associative neural network models are a commonly used methodology when investigating the theory of associative memory in the brain. Comparisons between the mammalian hippocampus and neural network models of associative memory have been investigated [7]. Biologically based networks are complex systems built of neurons with a variety of properties. Here we compare and contrast associative memory function in a network of biologically-based spiking neurons [14] with previously published results for a simple artificial neural network model [6]. We investigate biologically plausible implementations of methods for improving recall under biologically realistic conditions, such as a sparsely connected network.
C1 [Hunter, Russell; Graham, Bruce P.] Univ Stirling, Dept Math & Comp Sci, Stirling FK9 4LA, Scotland.
   [Cobb, Stuart] Univ Glasgow, Div Neurosci & Biomed Syst, Glasgow G12 8QQ, Lanark, Scotland.
RP Hunter, R (corresponding author), Univ Stirling, Dept Math & Comp Sci, Stirling FK9 4LA, Scotland.
EM rhu@cs.stir.ac.uk; s.cobb@bio.gla.ac.uk; b.graham@cs.stir.ac.uk
CR [Anonymous], 2005, NEURON BOOK
   COBB SR, 1995, NATURE, V378, P75, DOI 10.1038/378075a0
   Connors BW, 2007, NAT NEUROSCI, V10, P808, DOI 10.1038/nn0707-808
   Fransen E, 1998, NETWORK-COMP NEURAL, V9, P235, DOI 10.1088/0954-898X/9/2/006
   GRAHAM B, 1995, BIOL CYBERN, V72, P337, DOI 10.1007/BF00202789
   Graham B, 1997, NETWORK-COMP NEURAL, V8, P35, DOI 10.1088/0954-898X/8/1/005
   Graham BP, 2001, NETWORK-COMP NEURAL, V12, P473, DOI 10.1088/0954-898X/12/4/304
   Jensen O, 1996, LEARN MEMORY, V3, P243, DOI 10.1101/lm.3.2-3.243
   Lewis DA, 2005, NAT REV NEUROSCI, V6, P312, DOI 10.1038/nrn1648
   Mann EO, 2005, J PHYSIOL-LONDON, V562, P55, DOI 10.1113/jphysiol.2004.078758
   Menschik ED, 1998, ARTIF INTELL MED, V13, P99, DOI 10.1016/S0933-3657(98)00006-2
   PINSKY P, 1994, J COMPUT NEUROSCI, V1, P3960
   Rolls ET, 2006, PROG NEUROBIOL, V79, P1, DOI 10.1016/j.pneurobio.2006.04.005
   Sommer FT, 2001, NEURAL NETWORKS, V14, P825, DOI 10.1016/S0893-6080(01)00064-8
   Wennekers T., 1995, SUPERCOMPUTING BRAIN, P301
   WILLSHAW D, 1971, THESIS J U EDINBURGH
NR 16
TC 5
Z9 5
U1 0
U2 0
PY 2008
VL 5164
BP 636
EP +
WC Computer Science, Theory & Methods
DA 2023-11-11
ER

PT J
AU Baumbach, A
   Korcsak-Gorzo, A
   Müller, MG
   Leng, LZW
   Breitwieser, OJ
   van Albada, S
   Senn, W
   Meier, K
   Legenstein, R
   Petrovici, MA
AF Baumbach, Andreas
   Korcsak-Gorzo, Agnes
   Mueller, Michael G.
   Leng, Luziwei
   Breitwieser, Oliver Julien
   van Albada, Sacha
   Senn, Walter
   Meier, Karlheinz
   Legenstein, Robert
   Petrovici, Mihai A.
TI Cortical oscillations support sampling-based computations in spiking
   neural networks
SO JOURNAL OF COMPUTATIONAL NEUROSCIENCE
DT Meeting Abstract
C1 [Baumbach, Andreas; Breitwieser, Oliver Julien; Meier, Karlheinz] Heidelberg Univ, Heidelberg, Germany.
   [Korcsak-Gorzo, Agnes; van Albada, Sacha; Legenstein, Robert] Julich Res Ctr, Neurosci & Med INM 6, INM 10, Julich, Germany.
   [Korcsak-Gorzo, Agnes; van Albada, Sacha] Inst Adv Simulat IAS 6, Julich, Germany.
   [Mueller, Michael G.; Legenstein, Robert] Graz Univ Technol, Graz, Austria.
   [Leng, Luziwei; Petrovici, Mihai A.] Heidelberg Univ, Heidelberg, Germany.
   [Leng, Luziwei; Senn, Walter; Petrovici, Mihai A.] Univ Bern, Bern, Switzerland.
EM andreas.baumbach@kip.uni-heidelberg.de
CR Berkes P, 2011, SCIENCE, V331, P83, DOI 10.1126/science.1195870
   Brunel N, 2000, J COMPUT NEUROSCI, V8, P183, DOI 10.1023/A:1008925309027
   Buzsaki G., 2006, RHYTHMS BRAIN, DOI 10.1093/acprof:oso/9780195301069.001.0001
   Korcsak-Gorzo A, 2020, ARXIV PREPRINT ARXIV
   Petrovici MA, 2016, PHYS REV E, V94, DOI 10.1103/PhysRevE.94.042312
NR 5
TC 0
Z9 0
U1 0
U2 1
PD DEC
PY 2021
VL 49
IS SUPPL 1
SU 1
SI SI
MA F1
BP S5
EP S6
WC Mathematical & Computational Biology; Neurosciences
DA 2023-11-11
ER

PT C
AU De Maria, E
   Di Giusto, C
   Ciatto, G
AF De Maria, Elisabetta
   Di Giusto, Cinzia
   Ciatto, Giovanni
GP ACM
TI Formal Validation of Neural Networks as Timed Automata
SO PROCEEDINGS OF THE EIGHTH INTERNATIONAL CONFERENCE ON COMPUTATIONAL
   SYSTEMS-BIOLOGY AND BIOINFORMATICS (CSBIO 2017)
DT Proceedings Paper
CT 8th International Conference on Computational Systems-Biology and
   Bioinformatics (CSBio)
CY DEC 07-08, 2017
CL Nha Trang, VIETNAM
DE Neural networks; Leaky Integrate and Fire Model; Timed Automata;
   Temporal Logic; Model Checking
ID SPIKING; MODEL
AB We propose a formalisation of spiking neural networks based on timed automata networks. Neurons are modelled as timed automata waiting for inputs on a number of different channels (synapses), for a given amount of time (the accumulation period). When this period is over, the current potential value is computed taking into account the current inputs and the previous decayed potential value. If the current potential overcomes a given threshold, the automaton emits a broadcast signal over its output channel, otherwise it restarts another accumulation period. After each emission, the automaton is constrained to remain inactive for a fixed refractory period. Spiking neural networks are formalised as sets of automata, one for each neuron, running in parallel and sharing channels according to the structure of the network. The model is then validated against some crucial properties defined via proper temporal logic formulae.
C1 [De Maria, Elisabetta; Di Giusto, Cinzia] Univ Cote dAzur, CNRS, I3S, Nice, France.
   [Ciatto, Giovanni] Univ Bologna, Bologna, Italy.
RP De Maria, E (corresponding author), Univ Cote dAzur, CNRS, I3S, Nice, France.
EM elisabetta.de-maria@unice.fr; cinzia.di-giusto@unice.fr;
   giovanni.ciatto@gmail.com
CR ALUR R, 1994, THEOR COMPUT SCI, V126, P183, DOI 10.1016/0304-3975(94)90010-8
   Aman B, 2016, THEOR COMPUT SCI, V623, P92, DOI 10.1016/j.tcs.2015.11.005
   [Anonymous], ADDITIONAL MAT
   [Anonymous], COMPUTING SPIKING NE
   Bengtsson J., 1996, Hybrid Systems III. Verification and Control, P232, DOI 10.1007/BFb0020949
   Clarke EM, 1999, MODEL CHECKING, P1
   Cybenko G., 1989, Mathematics of Control, Signals, and Systems, V2, P303, DOI 10.1007/BF02551274
   De Maria E., 2016, 5 INT WORKSH HYBR SY
   DiGiusto Cinzia, 2017, P ADV SYST SYNTH BIO, P55
   ERMENTROUT GB, 1986, SIAM J APPL MATH, V46, P233, DOI 10.1137/0146017
   Freund Y, 1999, MACH LEARN, V37, P277, DOI 10.1023/A:1007662407062
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Lapique L., 1907, J PHYSL PATHOL GEN, V9, P620, DOI DOI 10.1007/S00422-007-0189-6
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   McCulloch W. S., 1943, B MATH BIOPHYS, V5, P115, DOI [10.1007/BF02478259, DOI 10.1007/BF02478259]
NR 17
TC 2
Z9 2
U1 0
U2 0
PY 2017
BP 15
EP 22
DI 10.1145/3156346.3156350
WC Mathematical & Computational Biology
DA 2023-11-11
ER

PT J
AU Yang, SM
   Linares-Barranco, B
   Chen, BD
AF Yang, Shuangming
   Linares-Barranco, Bernabe
   Chen, Badong
TI Heterogeneous Ensemble-Based Spike-Driven Few-Shot Online Learning
SO FRONTIERS IN NEUROSCIENCE
DT Article
DE spiking neural network; few-shot learning; entropy-based learning;
   spike-driven learning; brain-inspired intelligence
ID MIXTURE CORRENTROPY; INTELLIGENCE
AB Spiking neural networks (SNNs) are regarded as a promising candidate to deal with the major challenges of current machine learning techniques, including the high energy consumption induced by deep neural networks. However, there is still a great gap between SNNs and the few-shot learning performance of artificial neural networks. Importantly, existing spike-based few-shot learning models do not target robust learning based on spatiotemporal dynamics and superior machine learning theory. In this paper, we propose a novel spike-based framework with the entropy theory, namely, heterogeneous ensemble-based spike-driven few-shot online learning (HESFOL). The proposed HESFOL model uses the entropy theory to establish the gradient-based few-shot learning scheme in a recurrent SNN architecture. We examine the performance of the HESFOL model based on the few-shot classification tasks using spiking patterns and the Omniglot data set, as well as the few-shot motor control task using an end-effector. Experimental results show that the proposed HESFOL scheme can effectively improve the accuracy and robustness of spike-driven few-shot learning performance. More importantly, the proposed HESFOL model emphasizes the application of modern entropy-based machine learning methods in state-of-the-art spike-driven learning algorithms. Therefore, our study provides new perspectives for further integration of advanced entropy theory in machine learning to improve the learning performance of SNNs, which could be of great merit to applied developments with spike-based neuromorphic systems.
C1 [Yang, Shuangming] Tianjin Univ, Sch Elect & Informat Engn, Tianjin, Peoples R China.
   [Linares-Barranco, Bernabe] Microelect Inst Seville, Seville, Spain.
   [Chen, Badong] Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian, Peoples R China.
RP Yang, SM (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin, Peoples R China.; Chen, BD (corresponding author), Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian, Peoples R China.
EM yangshuangming@tju.edu.cn; chenbd@mail.xjtu.edu.cn
CR Al Zoubi O, 2018, ARTIF INTELL MED, V86, P1, DOI 10.1016/j.artmed.2018.01.001
   Chen BD, 2021, IEEE T SYST MAN CY-S, V51, P4007, DOI 10.1109/TSMC.2019.2931403
   Chen BD, 2019, IEEE SIGNAL PROC LET, V26, P1212, DOI 10.1109/LSP.2019.2925692
   Chen BD, 2018, PATTERN RECOGN, V79, P318, DOI 10.1016/j.patcog.2018.02.010
   Ding J, 2021, PREPRINT, DOI [10.48550/arXiv.2105.11654, DOI 10.48550/ARXIV.2105.11654]
   Du B, 2019, IEEE T CYBERNETICS, V49, P1440, DOI 10.1109/TCYB.2018.2804326
   Esser SK, 2016, P NATL ACAD SCI USA, V113, P11441, DOI 10.1073/pnas.1604850113
   Falez P., 2019, 2019 INT JOINT C NEU, P1, DOI [DOI 10.1109/IJCNN.2019.8852346, 10.1109/IJCNN.2019.8852346]
   Fischer B, 2003, IEEE T PATTERN ANAL, V25, P1411, DOI 10.1109/TPAMI.2003.1240115
   Gidaris S, 2019, IEEE I CONF COMP VIS, P8058, DOI 10.1109/ICCV.2019.00815
   GOELET P, 1986, NATURE, V322, P419, DOI 10.1038/322419a0
   Heravi AR, 2018, IEEE T NEUR NET LEAR, V29, P6252, DOI 10.1109/TNNLS.2018.2827778
   Jiang RH, 2021, NEURAL COMPUT, V33, P2439, DOI 10.1162/neco_a_01423
   Kim Y., 2021, PREPRINT, DOI [10.48550/arXiv.2110.07742, DOI 10.48550/ARXIV.2110.07742]
   Kim Y., 2021, PREPRINT
   Kim Y, 2021, FRONT NEUROSCI-SWITZ, V15, DOI 10.3389/fnins.2021.773954
   Kim Y, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-98448-0
   Koch Gregory R., 2015, ICML DEEP LEARN WORK
   Lu Z, 2018, IEEE SIGNAL PROC LET, V25, P526, DOI 10.1109/LSP.2018.2810121
   Luo SY, 2018, I C CONT AUTOMAT ROB, P88, DOI 10.1109/ICARCV.2018.8581122
   Luo X, 2018, IEEE T IND INFORM, V14, P4963, DOI 10.1109/TII.2018.2854549
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Panda P, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00693
   Paredes-Vallés F, 2020, IEEE T PATTERN ANAL, V42, P2051, DOI 10.1109/TPAMI.2019.2903179
   Pei J, 2019, NATURE, V572, P106, DOI 10.1038/s41586-019-1424-8
   Qiao N, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00141
   Rodrigues CF, 2018, P INT C PAR DISTR PR, P375
   Roy K, 2019, NATURE, V575, P607, DOI 10.1038/s41586-019-1677-2
   Santoro A, 2016, PR MACH LEARN RES, V48
   Singh S, 2017, NATURE, V550, P336, DOI 10.1038/550336a
   Soures N, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00686
   Strack R, 2019, NAT METHODS, V16, P17, DOI 10.1038/s41592-018-0267-9
   Sun QR, 2019, PROC CVPR IEEE, P403, DOI 10.1109/CVPR.2019.00049
   Tolkach Y, 2020, NAT MACH INTELL, V2, P411, DOI 10.1038/s42256-020-0200-7
   Venkatesha Y, 2021, IEEE T SIGNAL PROCES, V69, P6183, DOI 10.1109/TSP.2021.3121632
   Wang JQ, 2021, IEEE SENS J, V21, P1779, DOI 10.1109/JSEN.2020.3016611
   Wang TL, 2022, IEEE INTELL SYST, V37, P69, DOI 10.1109/MIS.2021.3122958
   Wang YQ, 2020, ACM COMPUT SURV, V53, DOI 10.1145/3386252
   Wijesinghe P, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00504
   Xing L, 2021, IEEE T CYBERNETICS, V51, P3298, DOI 10.1109/TCYB.2019.2952398
   Yang SM, 2022, IEEE T NEUR NET LEAR, V33, P7126, DOI 10.1109/TNNLS.2021.3084250
   Yang SM, 2021, FRONT NEUROSCI-SWITZ, V15, DOI 10.3389/fnins.2021.601109
   Zadeh SG, 2021, IEEE T PATTERN ANAL, V43, P3126, DOI 10.1109/TPAMI.2020.2979450
   Zhang J, 2021, IEEE T PATTERN ANAL, V43, P2866, DOI 10.1109/TPAMI.2020.3046486
   Zheng H., 2020, PREPRINT, DOI [10.48550/arXiv.2011.05280, DOI 10.48550/ARXIV.2011.05280]
   Zheng YF, 2022, IEEE T NEUR NET LEAR, V33, P811, DOI 10.1109/TNNLS.2020.3029198
   Zou J, 2019, NAT GENET, V51, P12, DOI 10.1038/s41588-018-0295-5
NR 47
TC 43
Z9 43
U1 16
U2 34
PD MAY 9
PY 2022
VL 16
AR 850932
DI 10.3389/fnins.2022.850932
WC Neurosciences
HC Y
HP N
DA 2023-11-11
ER

PT C
AU Wysoski, SG
   Benuskova, L
   Kasabov, N
AF Wysoski, Simei Gomes
   Benuskova, Lubica
   Kasabov, Nikola
BE MarquesDeSa, J
   Alexandre, LA
   Duch, W
   Mandic, D
TI Text-independent speaker authentication with spiking neural networks
SO ARTIFICIAL NEURAL NETWORKS - ICANN 2007, PT 2, PROCEEDINGS
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 17th International Conference on Artificial Neural Networks (ICANN 2007)
CY SEP 09-13, 2007
CL Oporto, PORTUGAL
DE spiking neural network; speaker authentication; brain-like pattern
   recognition; similarity domain normalization
ID SPEECH
AB This paper presents a novel system that performs text-independent speaker authentication using new spiking neural network (SNN) architectures. Each speaker is represented by a set of prototype vectors that is trained with standard Hebbian rule and winner-takes-all approach. For every speaker there is a separated spiking network that computes normalized similarity scores of MFCC (Mel Frequency Cepstrum Coefficients) features considering speaker and background models. Experiments with the VidTimit dataset show similar performance of the system when compared with a benchmark method based on vector quantization. As the main property, the system enables optimization in terms of performance, speed and energy efficiency. A procedure to create/merge neurons is also presented, which enables adaptive and on-line training in an evolvable
C1 [Wysoski, Simei Gomes; Benuskova, Lubica; Kasabov, Nikola] Auckland Univ Technol, Knowledge Engn & Discovery Res Inst, 581-585 Great S Rd, Auckland, New Zealand.
RP Wysoski, SG (corresponding author), Auckland Univ Technol, Knowledge Engn & Discovery Res Inst, 581-585 Great S Rd, Auckland, New Zealand.
EM swysoski@aut.ac.nz; lbenusko@aut.ac.nz; nkasabov@aut.ac.nz
CR [Anonymous], 2000, SPEECH AUDIO SIGNAL
   Bimbot F, 2004, EURASIP J APPL SIG P, V2004, P430, DOI 10.1155/S1110865704310024
   Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   Burileanu C., 2002, International Journal of Speech Technology, V5, P247, DOI 10.1023/A:1020244924468
   Delorme A, 1999, NEUROCOMPUTING, V26-7, P989, DOI 10.1016/S0925-2312(99)00095-8
   Gerstner W., 2002, SPIKING NEURON MODEL
   KUROYANAGI S, 1994, IEICE T INF SYST, VE77D, P466
   LOISELLE S, 2005, EXPLORATION RANK ORD, P2076
   Rabiner L., 1993, FUNDAMENTALS SPEECH
   Reynolds DA, 2000, DIGIT SIGNAL PROCESS, V10, P19, DOI 10.1006/dspr.1999.0361
   Rouat J, 2005, LECT NOTES ARTIF INT, V3445, P317
   Sanderson C, 2004, DIGIT SIGNAL PROCESS, V14, P449, DOI 10.1016/j.dsp.2004.05.001
   Wysoski SG, 2006, LECT NOTES COMPUT SC, V4131, P61
NR 13
TC 13
Z9 13
U1 0
U2 0
PY 2007
VL 4669
BP 758
EP +
WC Computer Science, Artificial Intelligence; Computer Science,
   Interdisciplinary Applications; Computer Science, Theory & Methods
DA 2023-11-11
ER

PT J
AU Tang, DL
   Botzheim, J
   Kubota, N
AF Tang, Dalai
   Botzheim, Janos
   Kubota, Naoyuki
TI Supervised Learning Based Multi-modal Perception for Robot Partners
   using Smart Phones
SO ACTA POLYTECHNICA HUNGARICA
DT Article
DE Robot Partners; Multi-modal Perception; Computational Intelligence;
   Human Robot Interaction
AB This paper proposes a method for multi-modal perception of human-friendly robot partners based on various types of sensors built in a smart phone. The proposed method can estimate human interaction modes by fuzzy spiking neural network. The learning method of the spiking neural network based on the time series of the measured data is explained as well. Evolution strategy is used for optimizing the parameters of the fuzzy spiking neural network. Several experimental results are presented for confirming the effectiveness of the proposed technique. Finally, the future direction on this research is discussed.
C1 [Tang, Dalai; Botzheim, Janos; Kubota, Naoyuki] Tokyo Metropolitan Univ, Grad Sch Syst Design, Hino, Tokyo 1910065, Japan.
RP Tang, DL (corresponding author), Tokyo Metropolitan Univ, Grad Sch Syst Design, 6-6 Asahigaoka, Hino, Tokyo 1910065, Japan.
EM tang@ed.tmu.ac.jp; botzheim@tmu.ac.jp; kubota@tmu.ac.jp
CR Anderson J.A., 1988, NEUROCOMPUTING
   [Anonymous], 2010, P IEEE WORLD C COMP
   [Anonymous], 1991, FDN GENETIC ALGORITH, DOI DOI 10.1016/B978-0-08-050684-5.50009-4
   Botzheim J, 2013, PROCEDIA COMPUT SCI, V22, P883, DOI 10.1016/j.procs.2013.09.171
   Gerstner W, 1998, PULSED NEURAL NETWORKS, P3
   Gerstner W., 2002, SPIKING NEURON MODEL
   Hashimoto T, 2010, LECT NOTES ARTIF INT, V6425, P505
   ISHIGURO H, 2005, P WORKSH NETW ROB SY
   KEMMOTSU K, 2005, P 1 JAP KOR JOINT S
   Khemapech I., 2005, P 6 ANN POSTGRADUATE
   Kimura H., 2010, MEC 2010, V2010, P610
   Kubota Naoyuki, 2009, RO-MAN 2009 - The 18th IEEE International Symposium on Robot and Human Interactive Communication, P346, DOI 10.1109/ROMAN.2009.5326082
   KUBOTA N, 2009, P 2009 IEEE INT S CO, P165
   Kubota N., 2011, IEEE S SER COMP INT
   Kubota N., 2009, ROBOT HUMAN INTERACT
   Kubota N., 2009, P ICROS SICE INT JOI
   Kubota N., 2009, P INT WORKSH ADV COM
   Kubota N, 2012, IEEE T SYST MAN CY C, V42, P1142, DOI 10.1109/TSMCC.2012.2213810
   Morioka K., 2004, 2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566), P199
   Obo T., 2010, P 2010 IEEE WORLD C, P2215
   Pollack M., 2005, AI MAGAZINE
   Remagnino P, 2005, AMBIENT INTELLIGENCE: NOVEL PARADIGM, P1, DOI 10.1007/0-387-22991-4_1
   SATOH I, 2006, INT J DIGITAL LIB
   Satomi M., P IEEE S SER COMP IN
   Schwefel HP., 1981, NUMERICAL OPTIMIZATI
   Statistics Bureau the Ministry on Internal Affairs and Communications., 2012, POP EST
   Steventon A., 2005, INTELLIGENT SPACES A, P389
   Tang D., 2010, P WORLD AUT C WAC 20
   Tang D, 2013, IEEE INT WORKS GENET, P36, DOI 10.1109/GEFS.2013.6601053
   TANG DL, 2010, P 17 INT C NEUR INF, V6443, P25
   TANG DL, 2012, P IEEE C CONTR SYST, P196
   Yorita A, 2011, IEEE T AUTON MENT DE, V3, P64, DOI 10.1109/TAMD.2011.2105868
NR 32
TC 2
Z9 2
U1 0
U2 1
PY 2014
VL 11
IS 8
BP 139
EP 159
WC Engineering, Multidisciplinary
DA 2023-11-11
ER

PT C
AU Kaufmann, P
   Gómez, G
AF Kaufmann, Pascal
   Gomez, Gabriel
BE Arai, T
   Pfeifer, R
   Balch, T
   Yokoi, H
TI Growing Virtual Neural Tissue: Binding Spiking Neurons through Sensory
   Input
SO INTELLIGENT AUTONOMOUS SYSTEMS 9
DT Proceedings Paper
CT 9th International Conference on Intelligent Autonomous Systems (IAS-9)
CY MAR 07-09, 2006
CL Univ Tokyo, Tokyo, JAPAN
HO Univ Tokyo
DE Virtual neural tissue; spike time dependent plasticity; synchronizity;
   binding problem
AB Based on the earlier introduced Virtual Neural Tissue (VNT) architecture, growing artificial neural networks underlying spike-timing dependent plasticity (STDP) are provided with sensory input. Both environmental cues and neural morphology are exploited in order to minimize computational expenses. Applying recent neuroscientific insight, it is demonstrated that implemented spike-timing dependent plasticity leads to the functional reshaping of neural circuitries in response to either intrinsic neural activity or sensory stimulation. It is found that intrinsically generated neural activity leads to a fine tuning of the underlying neural circuitry. This is reflected by dynamically segregating and forming groups of neurons that engage in synchronously locked firing patterns.
C1 [Kaufmann, Pascal; Gomez, Gabriel] Univ Zurich, Artificial Intelligence Lab, Dept Informat, CH-8050 Zurich, Switzerland.
RP Kaufmann, P (corresponding author), Univ Zurich, Artificial Intelligence Lab, Dept Informat, Andreasstr 15, CH-8050 Zurich, Switzerland.
EM kpascal@ifi.unizh.ch
CR Abbott LF, 2000, NAT NEUROSCI, V3, P1178, DOI 10.1038/81453
   Dan Y, 2004, NEURON, V44, P23, DOI 10.1016/j.neuron.2004.09.007
   Edelman G, 2000, CONSCIOUSNESS MATTER
   KAUFMANN P, 2003, THESIS U ZURICH
   Koch C, 2000, NAT NEUROSCI, V3, P1171, DOI 10.1038/81444
   LEVY WB, 1983, NEUROSCIENCE, V8, P791, DOI 10.1016/0306-4522(83)90010-6
   Rolls E. T., 1998, NEURAL NETWORKS BRAI
NR 7
TC 1
Z9 1
U1 0
U2 1
PY 2006
BP 443
EP 451
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods
DA 2023-11-11
ER

PT J
AU Yoo, D
   Jeong, DS
AF Yoo, Donghyung
   Jeong, Doo Seok
TI DTS-SNN: Spiking Neural Networks With Dynamic Time-Surfaces
SO IEEE ACCESS
DT Article
DE Lightweight spiking neural network; spiking neural network; dynamic
   time-surfaces; event-based data
ID ARCHITECTURE
AB Convolution helps spiking neural networks (SNNs) capture the spatio-temporal structures of neuromorphic (event) data as evident in the convolution-based SNNs (C-SNNs) with the state-of-the-art classification-accuracies on various datasets. However, the efficacy aside, the efficiency of C-SNN is questionable. In this regard, we propose SNNs with novel trainable dynamic time-surfaces (DTS-SNNs) as efficient alternatives to convolution. The novel dynamic time-surface proposed in this work features its high responsiveness to moving objects given the use of the zero-sum temporal kernel that is motivated by the simple cells' receptive fields in the early stage visual pathway. We evaluated the performance and computational complexity of our DTS-SNNs on three real-world event-based datasets (DVS128 Gesture, Spiking Heidelberg dataset, N-Cars). The results highlight high classification accuracies and significant improvements in computational efficiency, e.g., merely 1.51% behind of the state-of-the-art result on DVS128 Gesture but a x 18 improvement in efficiency. The code is available online (https://github.com/dooseokjeong/DTS-SNN).
C1 [Yoo, Donghyung; Jeong, Doo Seok] Hanyang Univ, Div Mat Sci & Engn, Seoul 14763, South Korea.
RP Jeong, DS (corresponding author), Hanyang Univ, Div Mat Sci & Engn, Seoul 14763, South Korea.
EM dooseokj@hanyang.ac.kr
CR Amir A, 2017, PROC CVPR IEEE, P7388, DOI 10.1109/CVPR.2017.781
   Cramer B, 2022, IEEE T NEUR NET LEAR, V33, P2744, DOI 10.1109/TNNLS.2020.3044364
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Dayan P., 2005, THEORETICAL NEUROSCI
   Fang W., 2021, P IEEE CVF INT C COM, P2661, DOI 10.1109/ICCV48922.2021.00266
   He WH, 2020, NEURAL NETWORKS, V132, P108, DOI 10.1016/j.neunet.2020.08.001
   Jaderberg M, 2017, PR MACH LEARN RES, V70
   Jeong DS, 2018, J APPL PHYS, V124, DOI 10.1063/1.5042243
   Kaiser J, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00424
   King DB, 2015, ACS SYM SER, V1214, P1
   Kornijcuk V, 2019, ADV INTELL SYST-GER, V1, DOI 10.1002/aisy.201900030
   Kugele A, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00439
   Lagorce X, 2017, IEEE T PATTERN ANAL, V39, P1346, DOI 10.1109/TPAMI.2016.2574707
   Manderscheid J, 2019, PROC CVPR IEEE, P10237, DOI 10.1109/CVPR.2019.01049
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Moradi S, 2018, IEEE T BIOMED CIRC S, V12, P106, DOI 10.1109/TBCAS.2017.2759700
   Neckar A, 2019, P IEEE, V107, P144, DOI 10.1109/JPROC.2018.2881432
   Paszke A., 2019, ADV NEURAL INFORM PR, P8024
   Perez-Nieves N, 2021, NAT COMMUN, V12, DOI 10.1038/s41467-021-26022-3
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Shrestha SB, 2018, ADV NEUR IN, V31
   Sironi A, 2018, PROC CVPR IEEE, P1731, DOI 10.1109/CVPR.2018.00186
   Viale A., 2021, PROC INT JOINT C NEU, P1
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Xing YN, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.590164
   Yin BJ, 2021, NAT MACH INTELL, V3, P905, DOI 10.1038/s42256-021-00397-w
   Zenke F, 2021, NEURAL COMPUT, V33, P899, DOI 10.1162/neco_a_01367
   Zheng HL, 2021, AAAI CONF ARTIF INTE, V35, P11062
NR 28
TC 0
Z9 0
U1 1
U2 6
PY 2022
VL 10
BP 102659
EP 102668
DI 10.1109/ACCESS.2022.3209671
WC Computer Science, Information Systems; Engineering, Electrical &
   Electronic; Telecommunications
DA 2023-11-11
ER

PT J
AU Xie, XD
   Wen, SP
   Yan, Z
   Huang, TW
   Chen, YR
AF Xie, Xudong
   Wen, Shiping
   Yan, Zheng
   Huang, Tingwen
   Chen, Yiran
TI Designing pulse-coupled neural networks with
   spike-synchronization-dependent plasticity rule: image segmentation and
   memristor circuit application
SO NEURAL COMPUTING & APPLICATIONS
DT Article
DE Spike-synchronization-dependent plasticity; Pulse-coupled neural
   network; Memristor-based circuit; Biomedical image segmentation
ID GAMMA-BAND SYNCHRONIZATION; SOMATOSENSORY CORTEX; FEATURE-LINKING;
   MODEL; AREA; MODULATION; PASSIVITY; PCNN
AB Pulse-coupled neural network (PCNN) is a powerful unsupervised learning model with many parameters to be determined empirically. In particular, the weight matrix is invariable in the iterative process, which is inconsistent with the actual biological system. Based on the existing research foundation of biology and neural network, we propose a spike-synchronization-dependent plasticity (SSDP) rule. In this paper, the mathematical model and algorithm of SSDP are presented. Furthermore, a novel memristor-based circuit model of SSDP is designed. Finally, experimental results demonstrate that SSDP has greatly improved the image processing capabilities of PCNN.
C1 [Xie, Xudong] Huazhong Univ Sci & Technol, Sch Elect Informat & Commun, Wuhan 430074, Peoples R China.
   [Wen, Shiping] Univ Elect Sci & Technol China, Comp Sci & Engn, Sci & Technol China, Chengdu, Peoples R China.
   [Yan, Zheng] Univ Technol Sydney, Ctr Artificial Intelligence, Ultimo 2007, Australia.
   [Huang, Tingwen] Texas A&M Univ Qatar, Sci Program, Doha 23874, Qatar.
   [Chen, Yiran] Duke Univ, Dept Elect & Comp Engn, Durham, NC 27708 USA.
RP Wen, SP (corresponding author), Univ Elect Sci & Technol China, Comp Sci & Engn, Sci & Technol China, Chengdu, Peoples R China.
EM wenshiping@uestc.edu.cn
CR [Anonymous], 1949, ORG BEHAV NEUROPSYCH
   Bauer M, 2006, J NEUROSCI, V26, P490, DOI 10.1523/JNEUROSCI.5228-04.2006
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Bichot NP, 2005, SCIENCE, V308, P529, DOI 10.1126/science.1109676
   Boybat I, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-04933-y
   Brunet NM, 2014, P NATL ACAD SCI USA, V111, P3626, DOI 10.1073/pnas.1309714111
   Cao YY, 2019, NEURAL NETWORKS, V109, P159, DOI 10.1016/j.neunet.2018.10.004
   Cao YT, 2019, NEURAL NETWORKS, V119, P178, DOI 10.1016/j.neunet.2019.08.011
   Chen YL, 2015, IEEE T NEUR NET LEAR, V26, P1682, DOI 10.1109/TNNLS.2014.2351418
   CHUA LO, 1971, IEEE T CIRCUITS SYST, VCT18, P507, DOI 10.1109/TCT.1971.1083337
   Ding SB, 2017, NEURAL COMPUT APPL, V28, P4011, DOI 10.1007/s00521-016-2291-y
   Dong MH, 2019, NEUROCOMPUTING, V331, P465, DOI 10.1016/j.neucom.2018.11.079
   Eckhorn R, 1990, NEURAL COMPUT, V2, P293, DOI 10.1162/neco.1990.2.3.293
   Ekblad U, 2004, NUCL INSTRUM METH A, V525, P392, DOI 10.1016/j.nima.2004.03.102
   Eryilmaz SB, 2016, INT SYM QUAL ELECT, P118
   Fell J, 2003, BRAIN RES REV, V42, P265, DOI 10.1016/S0165-0173(03)00178-4
   Fries P, 2009, ANNU REV NEUROSCI, V32, P209, DOI 10.1146/annurev.neuro.051508.135603
   Fu JC, 2010, COMPUT MED IMAG GRAP, V34, P308, DOI 10.1016/j.compmedimag.2009.12.002
   Gelasca ED, 2008, IEEE IMAGE PROC, P1816, DOI 10.1109/ICIP.2008.4712130
   GRAY CM, 1989, NATURE, V338, P334, DOI 10.1038/338334a0
   Gross J, 2007, PLOS BIOL, V5, P1168, DOI 10.1371/journal.pbio.0050133
   Gu XD, 2005, IEEE T NEURAL NETWOR, V16, P692, DOI 10.1109/TNN.2005.844902
   HANLEY JA, 1982, RADIOLOGY, V143, P29, DOI 10.1148/radiology.143.1.7063747
   Hua YK, 2017, 2017 13TH INTERNATIONAL CONFERENCE ON NATURAL COMPUTATION, FUZZY SYSTEMS AND KNOWLEDGE DISCOVERY (ICNC-FSKD), P879, DOI 10.1109/FSKD.2017.8393392
   Huang W, 2007, PATTERN RECOGN LETT, V28, P1123, DOI 10.1016/j.patrec.2007.01.013
   Jin Yingyezhe, 2018, P 32 INT C NEUR INF
   JOHNSON JL, 1994, APPL OPTICS, V33, P6239, DOI 10.1364/AO.33.006239
   Johnson JL, 1999, IEEE T NEURAL NETWOR, V10, P480, DOI 10.1109/72.761706
   JOHNSON JL, 1994, ADAPTIVE COMPUTING M
   Kasabov N., 2018, SPRINGER SERIES BIOA, DOI 10.1007/978-3-662-57715-8
   Kinser JM, 1996, P SOC PHOTO-OPT INS, V2760, P563, DOI 10.1117/12.235951
   Li TS, 2018, NEURAL COMPUT APPL, V30, P1939, DOI 10.1007/s00521-016-2715-8
   Li ZL, 2019, NEUROCOMPUTING, V350, P53, DOI 10.1016/j.neucom.2019.04.028
   MA YD, 2001, P 30 INT C NEUR INF
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   Ota Y, 2002, IEEE IND ELEC, P3221, DOI 10.1109/IECON.2002.1182914
   Panda P., 2019, ARXIV191013931
   Qiling Ni, 2014, 2014 International Joint Conference on Neural Networks (IJCNN), P340, DOI 10.1109/IJCNN.2014.6889424
   Ren GH, 2018, NEUROCOMPUTING, V286, P11, DOI 10.1016/j.neucom.2018.01.046
   Riehle A, 1997, SCIENCE, V278, P1950, DOI 10.1126/science.278.5345.1950
   Strukov DB, 2008, NATURE, V453, P80, DOI 10.1038/nature06932
   Sun SZ, 2020, INT J EPIDEMIOL, V49, P142, DOI 10.1093/ije/dyz184
   Taha AA, 2015, BMC MED IMAGING, V15, DOI 10.1186/s12880-015-0068-x
   Vinck M, 2013, NEURON, V80, P1077, DOI 10.1016/j.neuron.2013.08.019
   Waldemark J, 2000, Int J Neural Syst, V10, P171, DOI 10.1016/S0129-0657(00)00015-6
   WANG M, 2017, INT J WIREL MOB COMP, V13, P72
   Wang SB, 2019, APPL MATH COMPUT, V361, P294, DOI 10.1016/j.amc.2019.05.040
   Wang SQ, 2020, NEURAL NETWORKS, V121, P140, DOI 10.1016/j.neunet.2019.09.001
   Wang YK, 2019, ACTA GEOTECH, V14, P1379, DOI 10.1007/s11440-018-0735-5
   Wang ZB, 2018, NEURAL COMPUT APPL, V29, P1101, DOI [10.1007/s00521-017-3262-7, 10.1007/s00521-016-2633-9]
   Wen SP, 2020, IEEE T NETW SCI ENG, V7, P1431, DOI 10.1109/TNSE.2019.2934357
   Wen SP, 2019, IEEE T CIRC SYST VID, V29, P2337, DOI 10.1109/TCSVT.2018.2867934
   Wen SP, 2018, NEURAL NETWORKS, V103, P142, DOI 10.1016/j.neunet.2018.03.015
   Wen SP, 2017, IEEE T SYST MAN CY-S, V47, P24, DOI 10.1109/TSMC.2016.2564930
   Wen SP, 2016, IEEE T FUZZY SYST, V24, P1048, DOI 10.1109/TFUZZ.2015.2501412
   Xiao SX, 2018, NEUROCOMPUTING, V272, P677, DOI 10.1016/j.neucom.2017.08.014
   Xie XD, 2019, CIRC SYST SIGNAL PR, V38, P1452, DOI 10.1007/s00034-018-0926-1
   Xie XD, 2018, NEUROCOMPUTING, V284, P10, DOI 10.1016/j.neucom.2018.01.024
   Xiong Y, 2010, IEEE INT C BIO BIO W, P283, DOI 10.1109/BIBMW.2010.5703813
   Xu XZ, 2017, NEURAL COMPUT APPL, V28, pS671, DOI 10.1007/s00521-016-2397-2
   Yan Z, 2019, IEEE ACCESS, V7, P98005, DOI 10.1109/ACCESS.2019.2929512
   Yin M, 2019, IEEE T INSTRUM MEAS, V68, P49, DOI 10.1109/TIM.2018.2838778
   Yonekawa M, 2009, LECT NOTES COMPUT SC, V5768, P834, DOI 10.1007/978-3-642-04274-4_86
   Zeng XF, 2018, NEURAL COMPUT APPL, V30, P503, DOI 10.1007/s00521-016-2700-2
   Zhan K, 2017, ARCH COMPUT METHOD E, V24, P573, DOI 10.1007/s11831-016-9182-3
   Zhan K, 2016, NEURAL COMPUT, V28, P1072, DOI 10.1162/NECO_a_00832
   Zhan K, 2015, IEEE IJCNN
   Zhan K, 2009, IEEE T NEURAL NETWOR, V20, P1980, DOI 10.1109/TNN.2009.2030585
   Zhu S, 2017, NEUROCOMPUTING, V227, P149, DOI 10.1016/j.neucom.2016.07.068
NR 69
TC 8
Z9 8
U1 3
U2 29
PD SEP
PY 2020
VL 32
IS 17
BP 13441
EP 13452
DI 10.1007/s00521-020-04752-7
EA FEB 2020
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT J
AU Yang, J
   Zhang, PP
   Liu, Y
AF Yang, Jie
   Zhang, Pingping
   Liu, Yan
TI Robustness of classification ability of spiking neural networks
SO NONLINEAR DYNAMICS
DT Article
DE Robustness; Spiking neural networks; Gaussian perturbation;
   Classification
ID NEURONS; MODEL
AB The robustness of an artificial neural network is important for its application. In this paper, we focus on the robustness of the classification ability of spiking neural networks with respect to perturbation of inputs according to the probability distribution. Two typical types of perturbations, sinusoidal and Gaussian perturbations, are considered, which have rarely been investigated for SNNs in the existing literature. In particular, some of the perturbations are allowed to be large, rather than all the perturbations are uniformly small as in the existing literature. Numerical experiments are carried out by using the SpikeProp algorithm on classical XOR problem and other three benchmark datasets. The numerical results show that the classification ability of SNN is robust with respect to sinusoidal and Gaussian perturbations of input signals.
C1 [Yang, Jie; Zhang, Pingping] Dalian Univ Technol, Sch Math Sci, Dalian 116024, Peoples R China.
   [Liu, Yan] Dalian Polytech Univ, Sch Informat Sci & Engn, Dalian 116034, Peoples R China.
RP Zhang, PP (corresponding author), Dalian Univ Technol, Sch Math Sci, Dalian 116024, Peoples R China.
EM jssxzhpp@mail.dlut.edu.cn
CR Bao HB, 2011, NEURAL NETWORKS, V24, P19, DOI 10.1016/j.neunet.2010.09.010
   BIENENSTOCK E, 1995, NETWORK-COMP NEURAL, V6, P179, DOI 10.1088/0954-898X/6/2/004
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Buonomano DV, 1999, NEURAL COMPUT, V11, P103, DOI 10.1162/089976699300016836
   Fisher RA, 1936, ANN EUGENIC, V7, P179, DOI 10.1111/j.1469-1809.1936.tb02137.x
   FUKUSHIMA K, 1980, BIOL CYBERN, V36, P193, DOI 10.1007/BF00344251
   Gerstner W., 2002, SPIKING NEURON MODEL
   Ghosh-Dastidar S, 2007, INTEGR COMPUT-AID E, V14, P187
   Graves A, 2009, IEEE T PATTERN ANAL, V31, P855, DOI 10.1109/TPAMI.2008.137
   Li Y.B., 2010, J HARBIN U NAT SCI E, V15, P63
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maass W, 1997, ADV NEUR IN, V9, P211
   Maass W, 2014, P IEEE, V102, P860, DOI 10.1109/JPROC.2014.2310593
   Mo Y.Z., 2010, J CHONGQING U NAT SC, V22, P817
   Riesenhuber M, 1999, NAT NEUROSCI, V2, P1019, DOI 10.1038/14819
   ROCHESTER N, 1956, IRE T INFORM THEOR, V2, P80, DOI 10.1109/TIT.1956.1056810
   Taylor C, 1994, MACHINE LEARNING NEU, V13, P1, DOI DOI 10.1080/00401706.1995.10484383
   Thorpe S, 1996, NATURE, V381, P520, DOI 10.1038/381520a0
   VOGL TP, 1988, BIOL CYBERN, V59, P257, DOI 10.1007/BF00332914
   Werbos P., 1974, THESIS HARVARD U
   WOLBERG WH, 1991, CANC DATASET OBTAINE
   Yang J, 2012, APPL MATH LETT, V25, P1118, DOI 10.1016/j.aml.2012.02.016
   Yang J, 2011, NEURAL NETW WORLD, V21, P45, DOI 10.14311/NNW.2011.21.003
NR 23
TC 7
Z9 7
U1 0
U2 21
PD OCT
PY 2015
VL 82
IS 1-2
BP 723
EP 730
DI 10.1007/s11071-015-2190-2
WC Engineering, Mechanical; Mechanics
DA 2023-11-11
ER

PT J
AU Sterne, P
AF Sterne, Philip
TI Information Recall Using Relative Spike Timing in a Spiking Neural
   Network
SO NEURAL COMPUTATION
DT Article
ID COMPUTATION; PROPAGATION; DENDRITES; PATTERNS
AB We present a neural network that is capable of completing and correcting a spiking pattern given only a partial, noisy version. It operates in continuous time and represents information using the relative timing of individual spikes. The network is capable of correcting and recalling multiple patterns simultaneously. We analyze the network's performance in terms of information recall. We explore two measures of the capacity of the network: one that values the accurate recall of individual spike times and another that values only the presence or absence of complete patterns. Both measures of information are found to scale linearly in both the number of neurons and the period of the patterns, suggesting these are natural measures of network information. We show a smooth transition from encodings that provide precise spike times to flexible encodings that can encode many scenes. This makes it plausible that many diverse tasks could be learned with such an encoding.
C1 Univ Cambridge, Cavendish Lab, Cambridge CB2 3RF, England.
RP Sterne, P (corresponding author), Univ Cambridge, Cavendish Lab, Cambridge CB2 3RF, England.
EM sterne@fias.uni-frankfurt.de
CR Abeles M., 1991, CORTICONICS NEURAL C
   [Anonymous], [No title captured]
   BIENENSTOCK E, 1995, NETWORK-COMP NEURAL, V6, P179, DOI 10.1088/0954-898X/6/2/004
   Diesmann M, 1999, NATURE, V402, P529, DOI 10.1038/990101
   Faisal AA, 2008, NAT REV NEUROSCI, V9, P292, DOI 10.1038/nrn2258
   Faisal AA, 2007, PLOS COMPUT BIOL, V3, P783, DOI 10.1371/journal.pcbi.0030079
   GERSTNER W, 1993, BIOL CYBERN, V69, P503, DOI 10.1007/BF01185422
   Gerstner W, 1996, NATURE, V383, P76, DOI 10.1038/383076a0
   Golding NL, 2002, NATURE, V418, P326, DOI 10.1038/nature00854
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Konig P, 1996, TRENDS NEUROSCI, V19, P130, DOI 10.1016/S0166-2236(96)80019-1
   Liu GS, 2004, NAT NEUROSCI, V7, P373, DOI 10.1038/nn1206
   London M, 2005, ANNU REV NEUROSCI, V28, P503, DOI 10.1146/annurev.neuro.28.061604.135703
   Polsky A, 2004, NAT NEUROSCI, V7, P621, DOI 10.1038/nn1253
   Simoncelli EP, 2004, COGNITIVE NEUROSCIENCES III, THIRD EDITION, P327
   SOFTKY W, 1994, NEUROSCIENCE, V58, P13, DOI 10.1016/0306-4522(94)90154-6
   Vertes PE, 2010, HFSP J, V4, P153, DOI 10.2976/1.3386761
   Wills S., 2004, THESIS CAMBRIDGE U
NR 19
TC 2
Z9 2
U1 0
U2 8
PD AUG
PY 2012
VL 24
IS 8
BP 2053
EP 2077
DI 10.1162/NECO_a_00306
WC Computer Science, Artificial Intelligence; Neurosciences
DA 2023-11-11
ER

PT J
AU Amiri, M
   Jafari, AH
   Makkiabadi, B
   Nazari, S
   Van Hulle, MM
AF Amiri, Masoud
   Jafari, Amir Homayoun
   Makkiabadi, Bahador
   Nazari, Soheila
   Van Hulle, Marc M.
TI A novel un-supervised burst time dependent plasticity learning approach
   for biologically pattern recognition networks
SO INFORMATION SCIENCES
DT Article
DE Spiking Neural Network (SNN); Burst; Spatial learning; AMPA and GABA
   neurotransmitters; BTDP
ID NEURAL-NETWORKS; INFORMATION; MODEL; IMAGE
AB Bio-inspired computing is an appropriate platform for developing artificial intelligent machines based on the behavioral and functional principles of the brain. Bio-inspired machines have been proven to play a significant role in the development of intelligent sys-tems with spike-based operation being a key feature. However, spikes by themselves do not contain much information and may not cross the synapse and stimulate the post -synaptic neuron while bursts consisting of short trains of high-frequency spikes provide more potent information coding facilities. In this study, a pattern recognition network is proposed that consists of an input layer (adapted from a retinal model), middle layer (bio-inspired spiking neural network with bursting neurons and excitatory and inhibitory AMPA (alpha-amino-3-hydroxy-5-methyl-4-isoxazole-propionic acid) and GABA (Gamma-aminobutyric acid) synapses) and output layer (pyramidal neurons as classifying neurons). For the first time, a novel unsupervised burst-based learning algorithm inspired by spike -time-dependent-plasticity (STDP) is developed, called Burst Time Dependent Plasticity (BTDP). Compared to STDP, BTDP yields a higher performance accuracy and faster conver-gence rate of spiking pattern recognition networks when classifying EMNIST and CIFAR10 datasets compared to existing spiking networks. The proposed spiking network, trained by the novel unsupervised learning algorithm, is able to compete with advanced deep net-works in recognizing complex patterns while being amenable to implementation on neu-romorphic hardware platforms.(c) 2022 Elsevier Inc. All rights reserved.
C1 [Amiri, Masoud; Jafari, Amir Homayoun; Makkiabadi, Bahador] Univ Tehran Med Sci, Sch Med, Dept Med Phys & Biomed Engn, Tehran, Iran.
   [Amiri, Masoud; Jafari, Amir Homayoun; Makkiabadi, Bahador] Univ Tehran Med Sci, Adv Med Technol & Equipment Inst AMTEI, Res Ctr Biomed Technol & Robot RCBTR, Tehran, Iran.
   [Nazari, Soheila] Shahid Beheshti Univ, Fac Elect Engn, Tehran, Iran.
   [Van Hulle, Marc M.] KU Leuven Univ Leuven, Dept Neurosci, Lab Neuro & Psychophysiol, Leuven, Belgium.
RP Jafari, AH (corresponding author), Univ Tehran Med Sci, Sch Med, Dept Med Phys & Biomed Engn, Tehran, Iran.; Jafari, AH (corresponding author), Univ Tehran Med Sci, Adv Med Technol & Equipment Inst AMTEI, Res Ctr Biomed Technol & Robot RCBTR, Tehran, Iran.; Nazari, S (corresponding author), Shahid Beheshti Univ, Fac Elect Engn, Tehran, Iran.
CR Amiri M, 2019, INT J CIRC THEOR APP, V47, P483, DOI 10.1002/cta.2596
   Baldominos A, 2019, APPL SCI-BASEL, V9, DOI 10.3390/app9153169
   Baldominos A, 2019, COMPLEXITY, DOI 10.1155/2019/2952304
   Cavalin Paulo, 2019, Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications. 23rd Iberoamerican Congress, CIARP 2018. Proceedings: Lecture Notes in Computer Science (LNCS 11401), P271, DOI 10.1007/978-3-030-13469-3_32
   Cohen G, 2017, IEEE IJCNN, P2921, DOI 10.1109/IJCNN.2017.7966217
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Ding JH, 2021, Arxiv, DOI arXiv:2105.11654
   Draganski B, 2006, J NEUROSCI, V26, P6314, DOI 10.1523/JNEUROSCI.4628-05.2006
   Eshraghian JK, 2018, IEEE T VLSI SYST, V26, P2816, DOI 10.1109/TVLSI.2018.2829918
   FOHLMEISTER JF, 1990, BRAIN RES, V510, P343, DOI 10.1016/0006-8993(90)91388-W
   Furui S, 2012, IEEE SIGNAL PROC MAG, V29, P16, DOI 10.1109/MSP.2012.2209906
   Hartley M, 2006, NEUROCOMPUTING, V69, P2005, DOI 10.1016/j.neucom.2005.11.021
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Ji BJ, 2021, MOL BRAIN, V14, DOI 10.1186/s13041-020-00717-y
   Kabir H., 2020, ARXIV200703347
   Huynh PK, 2022, Arxiv, DOI arXiv:2202.08897
   Lee C, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00119
   Li Y, 2021, ADV FUNCT MATER, V31, DOI 10.1002/adfm.202100042
   Liang M, 2015, PROC CVPR IEEE, P3367, DOI 10.1109/CVPR.2015.7298958
   Liao ZB, 2015, Arxiv, DOI arXiv:1508.00330
   Mazzoni A, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.1000239
   Mishkin D, 2016, 2016 INT C LEARN REP
   Nazari S, 2022, ENG APPL ARTIF INTEL, V116, DOI 10.1016/j.engappai.2022.105430
   Nazari S, 2020, IEEE T NEUR NET LEAR, V31, P464, DOI 10.1109/TNNLS.2019.2905003
   Nazari S, 2019, INFORM SCIENCES, V477, P80, DOI 10.1016/j.ins.2018.10.041
   Nazari S, 2019, NEUROCOMPUTING, V330, P196, DOI 10.1016/j.neucom.2018.10.066
   Nazari S, 2018, NEURAL NETWORKS, V99, P68, DOI 10.1016/j.neunet.2017.12.009
   Querlioz D, 2013, IEEE T NANOTECHNOL, V12, P288, DOI 10.1109/TNANO.2013.2250995
   Rathi N, 2020, Arxiv, DOI arXiv:2005.01807
   Rathi N, 2023, IEEE T NEUR NET LEAR, V34, P3174, DOI 10.1109/TNNLS.2021.3111897
   Sabour S, 2017, ADV NEUR IN, V30
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Shepherd JD, 2007, ANNU REV CELL DEV BI, V23, P613, DOI 10.1146/annurev.cellbio.23.090506.123516
   Singh S, 2017, 2017 IEEE 3RD INTERNATIONAL CONFERENCE ON SENSING, SIGNAL PROCESSING AND SECURITY (ICSSS), P379, DOI 10.1109/SSPS.2017.8071623
   Stuijt J, 2021, FRONT NEUROSCI-SWITZ, V15, DOI 10.3389/fnins.2021.664208
   Vaila R, 2022, IEEE TETCI, V6, P124, DOI 10.1109/TETCI.2020.3035164
   Vlachou Styliani, 2022, Curr Top Behav Neurosci, V52, P291, DOI 10.1007/7854_2021_231
   Werginz P, 2015, VISION RES, V111, P170, DOI 10.1016/j.visres.2014.12.002
   Wu TF, 2022, INFORM SCIENCES, V588, P1, DOI 10.1016/j.ins.2021.12.074
   Xu M, 2020, ADV FUNCT MATER, V30, DOI 10.1002/adfm.202003419
   Yamazaki K, 2022, BRAIN SCI, V12, DOI 10.3390/brainsci12070863
   Yang G, 2020, INFORM SCIENCES, V533, P108, DOI 10.1016/j.ins.2020.05.038
   Yang ZC, 2015, IEEE I CONF COMP VIS, P1476, DOI 10.1109/ICCV.2015.173
   Yusoff N, 2018, INFORM SCIENCES, V451, P143, DOI 10.1016/j.ins.2018.03.043
   Zeldenrust F, 2018, FRONT COMPUT NEUROSC, V12, DOI 10.3389/fncom.2018.00048
   Zhang AG, 2022, INFORM SCIENCES, V585, P543, DOI 10.1016/j.ins.2021.11.065
   Zhang WR, 2021, Arxiv, DOI arXiv:2002.10085
   Zhao DC, 2022, INFORM SCIENCES, V610, P1, DOI 10.1016/j.ins.2022.07.152
NR 48
TC 4
Z9 4
U1 1
U2 16
PD APR
PY 2023
VL 622
BP 1
EP 15
DI 10.1016/j.ins.2022.11.162
EA DEC 2022
WC Computer Science, Information Systems
DA 2023-11-11
ER

PT C
AU Vázquez, RA
AF Vazquez, Roberto A.
BE KuriMorales, A
   Simari, GR
TI Pattern Recognition Using Spiking Neurons and Firing Rates
SO ADVANCES IN ARTIFICIAL INTELLIGENCE - IBERAMIA 2010
SE Lecture Notes in Artificial Intelligence
DT Proceedings Paper
CT 12th Ibero-American Conference on AI
CY NOV 01-05, 2010
CL Bahia Blanca, ARGENTINA
ID MODEL; NETWORKS; TIME
AB Different varieties of artificial neural networks have proved their power in several pattern recognition problems, particularly feed-forward neural networks. Nevertheless, these kinds of neural networks require of several neurons and layers in order to success when they are applied to solve non-linear problems. In this paper is shown how a spiking neuron can be applied to solve different linear and non-linear pattern recognition problems. A spiking neuron is stimulated during T ms with an input signal and fires when its membrane potential reaches a specific value generating an action potential (spike) or a train of spikes. Given a set of input patterns belonging to K classes, each input pattern is transformed into an input signal, then the spiking neuron is stimulated during T ms and finally the firing rate is computed. After adjusting the synaptic weights of the neuron model, we expect that input patterns belonging to the same class generate almost the same firing rate and input patterns belonging to different classes generate firing rates different enough to discriminate among the different classes. At last, a comparison between a feed-forward neural network and a spiking neuron is presented when they are applied to solve non-linear and real object recognition problems.
C1 La Salle Univ, Escuela Ingn, Mexico City 06140, DF, Mexico.
RP Vázquez, RA (corresponding author), La Salle Univ, Escuela Ingn, Benjamin Franklin 47 Col Condesa, Mexico City 06140, DF, Mexico.
EM ravem@lasallistas.org.mx
CR Anderson A., 1995, INTRO NEURAL NETWORK, DOI [10.7551/mitpress/3905.001.0001, DOI 10.7551/MITPRESS/3905.001.0001]
   [Anonymous], 2010, DYNAMICAL SYSTEMS NE
   Azhar H, 2005, IEEE IJCNN, P3075
   Di Paolo EA, 2002, ADAPT BEHAV, V10, P243, DOI 10.1177/1059712302010003006
   Floreano D, 2005, ARTIF LIFE, V11, P121, DOI 10.1162/1064546053278900
   Frias-Martinez E, 2007, MIND MACH, V17, P287, DOI 10.1007/s11023-007-9070-6
   Garro Beatriz A., 2009, Proceedings 2009 International Joint Conference on Neural Networks (IJCNN 2009 - Atlanta), P938, DOI 10.1109/IJCNN.2009.5178918
   Gerstner W., 2002, SPIKING NEURON MODEL
   Hasselmo ME, 2002, NEURAL COMPUT, V14, P793, DOI 10.1162/089976602317318965
   Hendrickson E., 2007, BMC NEUROSCI, V8, pP122
   Hopfield IJ, 2000, P NATL ACAD SCI USA, V97, P13919, DOI 10.1073/pnas.250483697
   HU M, 1962, IRE T INFORM THEOR, V8, P179, DOI 10.1109/tit.1962.1057692
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Jain R., 1995, MACHINE VISION, V5
   Loiselle S, 2005, IEEE IJCNN, P2076
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Murphy P.M., 1994, UCI REPOSITORY MACHI
   OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076
   Price K., 2005, DIFFENTENTIAL EVOLUT
   Rieke F., 1997, SPIKES EXPLORING NEU
   Thorpe SJ, 2004, NEUROCOMPUTING, V58, P857, DOI 10.1016/j.neucom.2004.01.138
   de los Monteros RAVE, 2008, NEURAL PROCESS LETT, V28, P189, DOI 10.1007/s11063-008-9089-6
   WERBOS PJ, 1990, P IEEE, V78, P1550, DOI 10.1109/5.58337
NR 24
TC 13
Z9 13
U1 0
U2 2
PY 2010
VL 6433
BP 423
EP 432
WC Computer Science, Artificial Intelligence; Computer Science,
   Interdisciplinary Applications
DA 2023-11-11
ER

PT J
AU Ilyasov, AI
   Nikiruy, KE
   Emelyanov, AV
   Chernoglazov, KY
   Sitnikov, AV
   Rylkov, VV
   Demin, VA
AF Ilyasov, A. I.
   Nikiruy, K. E.
   Emelyanov, A., V
   Chernoglazov, K. Yu
   Sitnikov, A., V
   Rylkov, V. V.
   Demin, V. A.
TI Arrays of Nanocomposite Crossbar Memristors for the Implementation of
   Formal and Spiking Neuromorphic Systems
SO NANOBIOTECHNOLOGY REPORTS
DT Article
ID NETWORKS; STATE; LAYER
AB Memristors are promising devices for use as synapses in hardware-based artificial formal and spiking neural networks. Their use can increase the energy efficiency of such networks when solving various cognitive tasks. At the same time, for the implementation of hardware-based neural networks, a large number of rather densely packed structures are required, which can be achieved by creating memristive matrices in a crossbar architecture. Crossbar arrays of nanocomposite memristors on silicon substrates with protection against edge effects are developed and manufactured, their resistive switching is investigated, a formal neural network is created on their basis, which is capable of recognizing simple patterns after loading a pretrained weight map, and the possibility of precise adjustment of the conductivity of the crossbar memristors by various means applicable in the creation of more complex spiking and formal neural networks is demonstrated.
C1 [Ilyasov, A. I.; Nikiruy, K. E.; Emelyanov, A., V; Chernoglazov, K. Yu; Sitnikov, A., V; Rylkov, V. V.; Demin, V. A.] Natl Res Ctr, Kurchatov Inst, Moscow 117556, Russia.
   [Ilyasov, A. I.] Moscow MV Lomonosov State Univ, Moscow, Russia.
   [Sitnikov, A., V] Voronezh State Tech Univ, Voronezh, Russia.
   [Rylkov, V. V.] Kotelnikov Inst Radio Engn & Elect, Fryazino Branch, Fryazino, Russia.
RP Ilyasov, AI (corresponding author), Natl Res Ctr, Kurchatov Inst, Moscow 117556, Russia.; Ilyasov, AI (corresponding author), Moscow MV Lomonosov State Univ, Moscow, Russia.
EM sashailyasov99@gmail.com; emelyanov_av@nrcki.ru
CR Bobylev AN, 2016, RUSS MICROELECTRON+, V45, P396, DOI 10.1134/S1063739716060020
   Emelyanov AV, 2020, NANOTECHNOLOGY, V31, DOI 10.1088/1361-6528/ab4a6d
   Emelyanov AV, 2016, AIP ADV, V6, DOI 10.1063/1.4966257
   Feldmann J, 2019, NATURE, V569, P208, DOI 10.1038/s41586-019-1157-8
   Hubara I, 2018, J MACH LEARN RES, V18
   Jouppi NP, 2017, 44TH ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA 2017), P1, DOI 10.1145/3079856.3080246
   Li C, 2019, NAT MACH INTELL, V1, P49, DOI 10.1038/s42256-018-0001-4
   Martyshov MN, 2020, PHYS REV APPL, V14, DOI 10.1103/PhysRevApplied.14.034016
   Mikhaylov AN, 2021, CHAOS SOLITON FRACT, V144, DOI 10.1016/j.chaos.2021.110723
   Mikhaylov A.N., 2018, EMERG TOP COMPUT INT, V2, P371, DOI 10.1109/TETCI.2018.2829922
   Mikheev V, 2020, NANOTECHNOLOGY, V31, DOI 10.1088/1361-6528/ab746d
   Moon J, 2019, NAT ELECTRON, V2, P480, DOI 10.1038/s41928-019-0313-3
   Nikiruy KE, 2019, J COMMUN TECHNOL EL+, V64, P1135, DOI 10.1134/S1064226919100103
   Nikiruy KE, 2018, TECH PHYS LETT+, V44, P416, DOI 10.1134/S106378501805022X
   Prezioso M, 2015, NATURE, V521, P61, DOI 10.1038/nature14441
   Querlioz D, 2013, IEEE T NANOTECHNOL, V12, P288, DOI 10.1109/TNANO.2013.2250995
   Rylkov VV, 2020, J EXP THEOR PHYS+, V131, P160, DOI 10.1134/S1063776120070109
   Ryu JH, 2020, CHAOS SOLITON FRACT, V140, DOI 10.1016/j.chaos.2020.110236
   Strukov DB, 2008, NATURE, V453, P80, DOI 10.1038/nature06932
   Surazhevsky IA, 2021, CHAOS SOLITON FRACT, V146, DOI 10.1016/j.chaos.2021.110890
   Wang W, 2016, SCI REP-UK, V6, DOI 10.1038/srep31224
   Xia QF, 2019, NAT MATER, V18, P309, DOI 10.1038/s41563-019-0291-x
NR 22
TC 2
Z9 2
U1 0
U2 3
PD FEB
PY 2022
VL 17
IS 1
BP 118
EP 125
DI 10.1134/S2635167622010050
WC Nanoscience & Nanotechnology
DA 2023-11-11
ER

PT J
AU Yu, ZF
   Guo, SQ
   Deng, F
   Yan, Q
   Huang, KK
   Liu, JK
   Chen, F
AF Yu, Zhaofei
   Guo, Shangqi
   Deng, Fei
   Yan, Qi
   Huang, Keke
   Liu, Jian K.
   Chen, Feng
TI Emergent Inference of Hidden Markov Models in Spiking Neural Networks
   Through Winner-Take-All
SO IEEE TRANSACTIONS ON CYBERNETICS
DT Article
DE Hidden Markov models; Biological neural networks; Mathematical model;
   Neurons; Cybernetics; Markov processes; Brain modeling; Hidden Markov
   models (HMMs); neural implementation; posterior inference; spiking
   neural network; winner-take-all (WTA) circuits
ID NEURONS; UNCERTAINTY; BRAIN; COMPUTATION; ADAPTATION; PLASTICITY;
   CIRCUITS
AB Hidden Markov models (HMMs) underpin the solution to many problems in computational neuroscience. However, it is still unclear how to implement inference of HMMs with a network of neurons in the brain. The existing methods suffer from the problem of being nonspiking and inaccurate. Here, we build a precise equivalence between the inference equation of HMMs with time-invariant hidden variables and the dynamics of spiking winner-take-all (WTA) neural networks. We show that the membrane potential of each spiking neuron in the WTA circuit encodes the logarithm of the posterior probability of the hidden variable in each state, and the firing rate of each neuron is proportional to the posterior probability of the HMMs. We prove that the time course of the neural firing rate can implement posterior inference of HMMs. Theoretical analysis and experimental results show that the proposed WTA circuit can get accurate inference results of HMMs.
C1 [Yu, Zhaofei] Peking Univ, Sch Elect Engn & Comp Sci, Natl Engn Lab Video Technol, Beijing 100871, Peoples R China.
   [Yu, Zhaofei; Guo, Shangqi; Deng, Fei; Yan, Qi; Chen, Feng] Tsinghua Univ, Ctr Brain Inspired Comp Res, Dept Automat, Beijing 100084, Peoples R China.
   [Guo, Shangqi; Deng, Fei; Yan, Qi; Chen, Feng] Tsinghua Univ, Beijing Innovat Ctr Future Chip, Beijing 100084, Peoples R China.
   [Guo, Shangqi; Deng, Fei; Yan, Qi; Chen, Feng] Tsinghua Univ, LSBDPA Beijing Key Lab, Beijing 100084, Peoples R China.
   [Huang, Keke] Cent South Univ, Sch Informat Sci & Engn, Changsha 410083, Peoples R China.
   [Liu, Jian K.] Univ Leicester, Dept Neurosci Psychol & Behav, Ctr Syst Neurosci, Leicester LE1 7HA, Leics, England.
   [Liu, Jian K.] Graz Univ Technol, Inst Theoret Comp Sci, A-8010 Graz, Austria.
RP Chen, F (corresponding author), Tsinghua Univ, Ctr Brain Inspired Comp Res, Dept Automat, Beijing 100084, Peoples R China.; Liu, JK (corresponding author), Univ Leicester, Dept Neurosci Psychol & Behav, Ctr Syst Neurosci, Leicester LE1 7HA, Leics, England.
EM yzf714@126.com; gsq15@mails.tsinghua.edu.cn;
   dengf15@mails.tsinghua.edu.cn; qyan15@mails.tsinghua.edu.cn;
   huangkeke@csu.edu.cn; jian.liu@leicester.ac.uk; chenfeng@tsinghua.edu.cn
CR Beck JM, 2007, NEURAL COMPUT, V19, P1344, DOI 10.1162/neco.2007.19.5.1344
   Bobrowski O, 2009, NEURAL COMPUT, V21, P1277, DOI 10.1162/neco.2008.01-08-692
   Cao Y, 2015, IEEE T NEUR NET LEAR, V26, P318, DOI 10.1109/TNNLS.2014.2315042
   Cao YQ, 2012, NEURAL NETWORKS, V26, P75, DOI 10.1016/j.neunet.2011.10.010
   Carandini M, 2012, NAT REV NEUROSCI, V13, P51, DOI 10.1038/nrn3136
   Chandrasekaran C, 2017, CURR OPIN NEUROBIOL, V43, P25, DOI 10.1016/j.conb.2016.11.002
   Corbett-Detig R, 2017, PLOS GENET, V13, DOI 10.1371/journal.pgen.1006529
   Deneve S., 2005, ADV NEURAL INFORM PR, V17, P353
   Deneve S, 2008, NEURAL COMPUT, V20, P91, DOI 10.1162/neco.2008.20.1.91
   Douglas RJ, 2004, ANNU REV NEUROSCI, V27, P419, DOI 10.1146/annurev.neuro.27.070203.144152
   Ernst MO, 2002, NATURE, V415, P429, DOI 10.1038/415429a
   Frémaux N, 2010, J NEUROSCI, V30, P13326, DOI 10.1523/JNEUROSCI.6249-09.2010
   Gammelmark S, 2014, PHYS REV A, V89, DOI 10.1103/PhysRevA.89.043839
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   Gold JI, 2007, ANNU REV NEUROSCI, V30, P535, DOI 10.1146/annurev.neuro.29.051605.113038
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Jolivet R, 2006, J COMPUT NEUROSCI, V21, P35, DOI 10.1007/s10827-006-7074-5
   Kappel D, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003511
   Kasabov NK, 2014, NEURAL NETWORKS, V52, P62, DOI 10.1016/j.neunet.2014.01.006
   Knill DC, 2004, TRENDS NEUROSCI, V27, P712, DOI 10.1016/j.tins.2004.10.007
   Koller D., 2009, PROBABILISTIC GRAPHI
   Lee LM, 2013, IEEE T CYBERNETICS, V43, P2114, DOI 10.1109/TCYB.2013.2240450
   Lu C, 2013, IEEE T CYBERNETICS, V43, P806, DOI 10.1109/TSMCB.2012.2216872
   Ma WJ, 2006, NAT NEUROSCI, V9, P1432, DOI 10.1038/nn1790
   Ma WJ, 2014, ANNU REV NEUROSCI, V37, P205, DOI 10.1146/annurev-neuro-071013-014017
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Marco E, 2017, NAT COMMUN, V8, DOI 10.1038/ncomms15011
   Moreno-Bote R, 2015, SCI REP-UK, V5, DOI 10.1038/srep17531
   Murphy KP, 2012, MACHINE LEARNING: A PROBABILISTIC PERSPECTIVE, P27
   Nessler B, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003037
   O'reilly R.C., 2000, COMPUTATIONAL EXPLOR
   Oster M, 2009, NEURAL COMPUT, V21, P2437, DOI 10.1162/neco.2009.07-08-829
   Pfister JP, 2006, NEURAL COMPUT, V18, P1318, DOI 10.1162/neco.2006.18.6.1318
   Pouget A, 2016, NAT NEUROSCI, V19, P366, DOI 10.1038/nn.4240
   Rao RPN, 2004, NEURAL COMPUT, V16, P1, DOI 10.1162/08997660460733976
   Robinson M, 2005, IEEE T NEURAL NETWOR, V16, P447, DOI 10.1109/TNN.2004.841805
   Roy S, 2017, IEEE T NEUR NET LEAR, V28, P900, DOI 10.1109/TNNLS.2016.2582517
   Sahani M, 2003, NEURAL COMPUT, V15, P2255, DOI 10.1162/089976603322362356
   Seilheimer RL, 2014, CURR OPIN NEUROBIOL, V25, P38, DOI 10.1016/j.conb.2013.11.008
   Tavanaei A, 2018, J SIGNAL PROCESS SYS, V90, P211, DOI 10.1007/s11265-016-1153-2
   Tymoshchuk P, 2005, NEUROCOMPUTING, V64, P375, DOI 10.1016/j.neucom.2004.08.002
   Urdapilleta E, 2011, PHYS REV E, V84, DOI 10.1103/PhysRevE.84.041904
   Ursino M, 2014, NEURAL NETWORKS, V60, P141, DOI 10.1016/j.neunet.2014.08.003
   Valera I, 2016, IEEE T PATTERN ANAL, V38, P1816, DOI 10.1109/TPAMI.2015.2498931
   Yang T, 2007, NATURE, V447, P1075, DOI 10.1038/nature05852
   Yedidia JS, 2005, IEEE T INFORM THEORY, V51, P2282, DOI 10.1109/TIT.2005.850085
   Yu Q, 2016, IEEE T NEUR NET LEAR, V27, P621, DOI 10.1109/TNNLS.2015.2416771
   Yu ZF, 2018, IEEE T NEUR NET LEAR, V29, P5761, DOI 10.1109/TNNLS.2018.2805813
   Yu ZF, 2016, NEUROCOMPUTING, V175, P155, DOI 10.1016/j.neucom.2015.10.045
NR 50
TC 25
Z9 27
U1 4
U2 30
PD MAR
PY 2020
VL 50
IS 3
BP 1347
EP 1354
DI 10.1109/TCYB.2018.2871144
WC Automation & Control Systems; Computer Science, Artificial Intelligence;
   Computer Science, Cybernetics
DA 2023-11-11
ER

PT C
AU Pastorelli, E
   Paolucci, PS
   Simula, F
   Biagioni, A
   Capuani, F
   Cretaro, P
   De Bonis, G
   Lo Cicero, F
   Lonardo, A
   Martinelli, M
   Pontisso, L
   Vicini, P
   Ammendola, R
AF Pastorelli, Elena
   Paolucci, Pier Stanislao
   Simula, Francesco
   Biagioni, Andrea
   Capuani, Fabrizio
   Cretaro, Paolo
   De Bonis, Giulia
   Lo Cicero, Francesca
   Lonardo, Alessandro
   Martinelli, Michele
   Pontisso, Luca
   Vicini, Piero
   Ammendola, Roberto
BE Merelli, I
   Lio, P
   Kotenko, I
TI Gaussian and exponential lateral connectivity on distributed spiking
   neural network simulation
SO 2018 26TH EUROMICRO INTERNATIONAL CONFERENCE ON PARALLEL, DISTRIBUTED,
   AND NETWORK-BASED PROCESSING (PDP 2018)
SE Euromicro Conference on Parallel, Distributed and Network-Based
   Processing
DT Proceedings Paper
CT 26th Euromicro International Conference on Parallel, Distributed, and
   Network-Based Processing (PDP)
CY MAR 21-23, 2018
CL Univ Cambridge, Comp Lab, Cambridge, ENGLAND
HO Univ Cambridge, Comp Lab
DE cortical simulation; distributed computing; spiking neural network;
   lateral synaptic connectivity; hardware/software co-design
ID HORIZONTAL CONNECTIONS; PHYSIOLOGY; NEURONS; SLOW
AB We measured the impact of long-range exponentially decaying intra-areal lateral connectivity on the scaling and memory occupation of a distributed spiking neural network simulator compared to that of short-range Gaussian decays. While previous studies adopted short-range connectivity, recent experimental neurosciences studies are pointing out the role of longer-range intra-areal connectivity with implications on neural simulation platforms. Two-dimensional grids of cortical columns composed by up to 11 M point-like spiking neurons with spike frequency adaption were connected by up to 30 G synapses using short-and long-range connectivity models. The MPI processes composing the distributed simulator were run on up to 1024 hardware cores, hosted on a 64 nodes server platform. The hardware platform was a cluster of IBM NX360 M5 16-core compute nodes, each one containing two Intel Xeon Haswell 8-core E5-2630 v3 processors, with a clock of 2.40 G Hz, interconnected through an InfiniBand network, equipped with 4x QDR switches.
C1 [Pastorelli, Elena; Paolucci, Pier Stanislao; Simula, Francesco; Biagioni, Andrea; Capuani, Fabrizio; Cretaro, Paolo; De Bonis, Giulia; Lo Cicero, Francesca; Lonardo, Alessandro; Martinelli, Michele; Pontisso, Luca; Vicini, Piero] INFN, Sez Roma, Rome, Italy.
   [Pastorelli, Elena] Sapienza Univ Rome, PhD Program Behav Neurosci, Rome, Italy.
   [Ammendola, Roberto] INFN, Sez Tor Vergata, Rome, Italy.
   [Ammendola, Roberto] Univ Roma Tor Vergata, Elect Engn Dept, Rome, Italy.
RP Pastorelli, E (corresponding author), INFN, Sez Roma, Rome, Italy.; Pastorelli, E (corresponding author), Sapienza Univ Rome, PhD Program Behav Neurosci, Rome, Italy.
EM elena.pastorelli@roma1.infn.it; pier.paolucci@roma1.infn.it;
   francesco.simula@roma1.infn.it; andrea.biagioni@roma1.infn.it;
   fabrizio.capuani@roma1.infn.it; paolo.cretaro@roma1.infn.it;
   giulia.debonis@roma1.infn.it; francesca.locicero@roma1.infn.it;
   alessandro.lonardo@roma1.infn.it; michele.martinelli@roma1.infn.it;
   luca.pontisso@roma1.infn.it; piero.vicini@roma1.infn.it;
   roberto.ammendola@roma2.infn.it
CR [Anonymous], 2017, CORR
   [Anonymous], COMPETITIVE HEBBIAN
   [Anonymous], 2015, ARXIV150503015
   [Anonymous], 5 ANN HUM BRAIN PROJ
   [Anonymous], SILICON AUDITORY PRO
   [Anonymous], ADV NEUR IN
   [Anonymous], 2017, NEST 2 12 10
   Boucsein C, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00032
   Brette R, 2007, J COMPUT NEUROSCI, V23, P349, DOI 10.1007/s10827-007-0038-6
   Capone C., 2017, CEREB CORTEX, P1
   Furber SB, 2013, IEEE T COMPUT, V62, P2454, DOI 10.1109/TC.2012.142
   Gal E, 2017, NAT NEUROSCI, V20, P1004, DOI 10.1038/nn.4576
   Gewaltig M-O., 2007, SCHOLARPEDIA, V2, DOI [10.4249/scholarpedia.1430, DOI 10.4249/SCHOLARPEDIA.1430]
   Gigante G, 2007, PHYS REV LETT, V98, DOI 10.1103/PhysRevLett.98.148101
   Gütig R, 2003, J NEUROSCI, V23, P3697
   Izhikevich EM, 2008, P NATL ACAD SCI USA, V105, P3593, DOI 10.1073/pnas.0712231105
   Katevenis M, 2016, 19TH EUROMICRO CONFERENCE ON DIGITAL SYSTEM DESIGN (DSD 2016), P60, DOI 10.1109/DSD.2016.106
   Kunkel S, 2014, FRONT NEUROINFORM, V8, DOI 10.3389/fninf.2014.00078
   Litwin-Kumar A, 2012, NAT NEUROSCI, V15, P1498, DOI 10.1038/nn.3220
   Mattia M, 2000, NEURAL COMPUT, V12, P2305, DOI 10.1162/089976600300014953
   Modha DS, 2011, COMMUN ACM, V54, P62, DOI 10.1145/1978542.1978559
   Morrison A, 2005, NEURAL COMPUT, V17, P1776, DOI 10.1162/0899766054026648
   Nageswaran JM, 2009, NEURAL NETWORKS, V22, P791, DOI 10.1016/j.neunet.2009.06.028
   Paolucci PS, 2016, J SYST ARCHITECT, V69, P29, DOI 10.1016/j.sysarc.2015.11.008
   Potjans TC, 2014, CEREB CORTEX, V24, P785, DOI 10.1093/cercor/bhs358
   Ruiz-Mejias M, 2011, J NEUROPHYSIOL, V106, P2910, DOI 10.1152/jn.00440.2011
   Schnepel P, 2015, CEREB CORTEX, V25, P3818, DOI 10.1093/cercor/bhu265
   Schüz A, 2006, CEREB CORTEX, V16, P1474, DOI 10.1093/cercor/bhj085
   Stepanyants A, 2009, P NATL ACAD SCI USA, V106, P3555, DOI 10.1073/pnas.0810390106
   Stroh A, 2013, NEURON, V77, P1136, DOI 10.1016/j.neuron.2013.01.031
NR 30
TC 6
Z9 6
U1 0
U2 2
PY 2018
BP 658
EP 665
DI 10.1109/PDP2018.2018.00110
WC Computer Science, Hardware & Architecture; Computer Science, Theory &
   Methods; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Cohen-Duwek, H
   Tsur, EE
AF Cohen-Duwek, Hadar
   Tsur, Elishai Ezra
GP IEEE
TI BIOLOGICALLY PLAUSIBLE ILLUSIONARY CONTRAST PERCEPTION WITH SPIKING
   NEURAL NETWORKS
SO 2022 IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, ICIP
SE IEEE International Conference on Image Processing ICIP
DT Proceedings Paper
CT IEEE International Conference on Image Processing (ICIP)
CY OCT 16-19, 2022
CL Bordeaux, FRANCE
DE computational cognition; visual illusions; image reconstruction; neural
   engineering framework; visual perception
AB Illusionary visual perception has been long used to shed light on biological vision pathways and mechanisms. In this work, we propose a biologically plausible spiking neural network with which spike events are used for iterative image reconstruction in which illusionary contrast perception, long known to manifest in human vision, is apparent. This parametric implementation allows us to examine this visual phenomenon in a biologically plausible computational framework, which may also account for differences in individual visual perception.
C1 [Cohen-Duwek, Hadar; Tsur, Elishai Ezra] Open Univ Israel, Dept Math & Comp Sci, Neurobiomorph Engn Lab NBEL, Raanana, Israel.
RP Tsur, EE (corresponding author), Open Univ Israel, Dept Math & Comp Sci, Neurobiomorph Engn Lab NBEL, Raanana, Israel.
EM elishai@nbel-lab.com
CR Adelson E. H., 2005, CHECKERSHADOW ILLUSI
   Anderson BL, 2003, PERCEPTION, V32, P269, DOI 10.1068/p3216
   Cohen Duwek H., 2021, P ANN M COGN SCI SOC, V43
   Eliasmith C., 2003, NEURAL ENG COMPUTATI
   Heydt V. D., 2003, FILLING IN PERCEPTUA, P106, DOI DOI 10.1093/ACPROF:OSO/9780195140132.003.0006
   Huang X, 2008, J NEUROPHYSIOL, V100, P539, DOI 10.1152/jn.00997.2007
   Komatsu H, 2006, NAT REV NEUROSCI, V7, P220, DOI 10.1038/nrn1869
   Marr D., 1982, Vision. A computational investigation into the human representation and processing of visual information
   Tsur E. E., 2021, NEUROMORPHIC ENG SCI, DOI 10.1201/9781003143499
   Tsur EE, 2020, NEUROCOMPUTING, V374, P54, DOI 10.1016/j.neucom.2019.09.072
   Zaidel Y, 2021, FRONT NEUROROBOTICS, V15, DOI 10.3389/fnbot.2021.631159
   Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068
   Zweig S, 2015, J NEUROSCI, V35, P12103, DOI 10.1523/JNEUROSCI.1334-15.2015
NR 13
TC 0
Z9 0
U1 0
U2 0
PY 2022
BP 1586
EP 1590
DI 10.1109/ICIP46576.2022.9897264
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
DA 2023-11-11
ER

PT J
AU Eshraghian, JK
   Ward, M
   Neftci, EO
   Wang, XX
   Lenz, G
   Dwivedi, G
   Bennamoun, M
   Jeong, DS
   Lu, WD
AF Eshraghian, Jason K.
   Ward, Max
   Neftci, Emre O.
   Wang, Xinxin
   Lenz, Gregor
   Dwivedi, Girish
   Bennamoun, Mohammed
   Jeong, Doo Seok
   Lu, Wei D.
TI Training Spiking Neural Networks Using Lessons From Deep Learning
SO PROCEEDINGS OF THE IEEE
DT Article
DE | Deep learning; neural code; neuromorphic; online learning; spiking
   neural networks (SNNs)
ID EVENT-CAMERA DATASET; GRADIENT DESCENT; AI SYSTEM; FEEDFORWARD; CHIP;
   REINFORCEMENT; MODEL; BRAIN; CATEGORIZATION; INTELLIGENCE
AB The brain is the perfect place to look for inspiration to develop more efficient neural networks. The inner workings of our synapses and neurons provide a glimpse at what the future of deep learning might look like. This article serves as a tutorial and perspective showing how to apply the lessons learned from several decades of research in deep learning, gradient descent, backpropagation, and neuroscience to biologically plausible spiking neural networks (SNNs). We also explore the delicate interplay between encoding data as spikes and the learning process; the challenges and solutions of applying gradient-based learning to SNNs; the subtle link between temporal backpropagation and spike timing-dependent plasticity; and how deep learning might move toward biologically plausible online learning. Some ideas are well accepted and commonly used among the neuromorphic engineering community, while others are presented or justified for the first time here. A series of companion interactive tutorials complementary to this article using our Python package, snnTorch, are also made available: https://snntorch.readthedocs.io/en/latest/tutorials/index.html.
C1 [Eshraghian, Jason K.; Wang, Xinxin; Lu, Wei D.] Univ Michigan, Dept Elect Engn & Comp Sci, Ann Arbor, MI 48109 USA.
   [Eshraghian, Jason K.] Univ Calif Santa Cruz, Dept Elect & Comp Engn, Santa Cruz, CA 95064 USA.
   [Ward, Max] Harvard Univ, Dept Mol & Cellular Biol, Cambridge, MA 02138 USA.
   [Ward, Max; Bennamoun, Mohammed] Univ Western Australia, Dept Comp Sci & Software Engn, Crawley, WA 6009, Australia.
   [Neftci, Emre O.] Univ Calif Irvine UC Irvine, Dept Comp Sci, Irvine, CA 92697 USA.
   [Lenz, Gregor] SynSense AG, CH-8050 Zurich, Switzerland.
   [Dwivedi, Girish] Univ Western Australia, Sch Med, Crawley, WA 6009, Australia.
   [Jeong, Doo Seok] Hanyang Univ, Div Mat Sci & Engn, Seoul 04763, South Korea.
RP Eshraghian, JK (corresponding author), Univ Calif Santa Cruz, Dept Elect & Comp Engn, Santa Cruz, CA 95064 USA.
EM Jeshragh@ucsc.edu
CR AERTSEN AMHJ, 1981, BIOL CYBERN, V42, P133, DOI 10.1007/BF00336731
   Afshar S, 2020, IEEE SENS J, V20, P15117, DOI 10.1109/JSEN.2020.3009687
   Amir A., 2017, P IEEE C COMP VIS PA, P7243, DOI DOI 10.1109/CVPR.2017.781
   Amit D. J., 1992, MODELING BRAIN FUNCT
   Amodei D., 2019, AI AND COMPUTE
   [Anonymous], 2001, HDB BIOL PHYS
   Anthony L. F. W., 2020, ARXIV
   Pérez-Carrasco JA, 2013, IEEE T PATTERN ANAL, V35, P2706, DOI 10.1109/TPAMI.2013.71
   Anumula J, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00023
   ARROW C, 2021, PROC IEEE INT S CIRC, P1
   Azghadi MR, 2020, IEEE T BIOMED CIRC S, V14, P1138, DOI 10.1109/TBCAS.2020.3036081
   Azghadi MR, 2020, ADV INTELL SYST-GER, V2, DOI 10.1002/aisy.201900189
   Baek S, 2020, 2020 2ND IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE CIRCUITS AND SYSTEMS (AICAS 2020), P79, DOI [10.1109/AICAS48895.2020.9073963, 10.1109/aicas48895.2020.9073963]
   Bartolozzi C, 2022, NAT COMMUN, V13, DOI 10.1038/s41467-022-28487-2
   Bartunov S, 2018, ADV NEUR IN, V31
   Bauer FC, 2019, IEEE T BIOMED CIRC S, V13, P1575, DOI 10.1109/TBCAS.2019.2953001
   Bellec G., 2018, ARXIV
   Bellec G, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-17236-y
   Bengio Y, 2006, ADV NEURAL INFORM PR, P19, DOI DOI 10.7551/MITPRESS/7503.003.0024
   Bengio Y., 2014, ARXIV
   Bengio Y., 2015, ARXIV
   Benucci A, 2013, NAT NEUROSCI, V16, P724, DOI 10.1038/nn.3382
   Beyeler M, 2013, NEURAL NETWORKS, V48, P109, DOI 10.1016/j.neunet.2013.07.012
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Bi Y, 2019, IEEE I CONF COMP VIS, P491, DOI 10.1109/ICCV.2019.00058
   Bohnstingl T., 2020, ARXIV
   Bohnstingl T, 2022, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2022.3153985
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Boi F, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00563
   Booij O, 2005, INFORM PROCESS LETT, V95, P552, DOI 10.1016/j.ipl.2005.05.023
   Brader JM, 2007, NEURAL COMPUT, V19, P2881, DOI 10.1162/neco.2007.19.11.2881
   Brandli C, 2014, IEEE J SOLID-ST CIRC, V49, P2333, DOI 10.1109/JSSC.2014.2342715
   Brette R, 2015, FRONT SYST NEUROSCI, V9, DOI 10.3389/fnsys.2015.00151
   Brette R, 2012, PLOS COMPUT BIOL, V8, DOI 10.1371/journal.pcbi.1002561
   Brown T., 2020, PROC ADV NEUR INF PR, P1877
   Brunel N, 2007, BIOL CYBERN, V97, P337, DOI 10.1007/s00422-007-0190-0
   Burkitt AN, 2006, BIOL CYBERN, V95, P1, DOI 10.1007/s00422-006-0068-6
   Cai FX, 2020, NAT ELECTRON, V3, P409, DOI 10.1038/s41928-020-0436-6
   Callaway EM, 2004, NEURAL NETWORKS, V17, P625, DOI 10.1016/j.neunet.2004.04.004
   Ceolini E, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00637
   Chan W, 2016, INT CONF ACOUST SPEE, P4960, DOI 10.1109/ICASSP.2016.7472621
   Chi P, 2016, CONF PROC INT SYMP C, P27, DOI 10.1109/ISCA.2016.13
   Cho K., 2014, P C EMP METH NAT LAN, P1724
   Cichy RM, 2016, SCI REP-UK, V6, DOI 10.1038/srep27755
   Cohen G, 2022, IEEE SPECTRUM, V59, P44, DOI 10.1109/MSPEC.2022.9729948
   Cohen G, 2019, J ASTRONAUT SCI, V66, P125, DOI 10.1007/s40295-018-00140-5
   Collobert R., 2008, P 25 ICML, P160, DOI [DOI 10.1145/1390156.1390177, 10.1145/1390156.1390177]
   Comsa JM, 2020, INT CONF ACOUST SPEE, P8529, DOI [10.1109/ICASSP40776.2020.9053856, 10.1109/icassp40776.2020.9053856]
   Contreras L. F. H, 2023, ARXIV
   Corradi F, 2015, IEEE T BIOMED CIRC S, V9, P699, DOI 10.1109/TBCAS.2015.2479256
   Cramer B, 2022, IEEE T NEUR NET LEAR, V33, P2744, DOI 10.1109/TNNLS.2020.3044364
   CRICK F, 1989, NATURE, V337, P129, DOI 10.1038/337129a0
   Davies M, 2021, P IEEE, V109, P911, DOI 10.1109/JPROC.2021.3067593
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Dayan P., 2001, THEORETICAL NEUROSCI
   Deb K., 2014, SEARCH METHODOLOGIES, P403, DOI [10.1007/978-1-4614-6940-7_15, DOI 10.1007/978-1-4614-6940-7_15]
   Dehaene S, 2003, TRENDS COGN SCI, V7, P145, DOI 10.1016/S1364-6613(03)00055-X
   Dellaferrera G., 2022, P 39 INT C MACHINE L, P4937
   Denève S, 2017, NEURON, V94, P969, DOI 10.1016/j.neuron.2017.05.016
   Dhar P, 2020, NAT MACH INTELL, V2, P423, DOI 10.1038/s42256-020-0219-9
   Diehl PU, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON REBOOTING COMPUTING (ICRC)
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Drenick R, 1981, SYSTEM MODELING OPTI, P762, DOI [DOI 10.1007/BFB0006203, 10.1007/BFb0006203]
   Duan C., 2022, P ADV NEUR INF PROC, V35, P34377
   Dupeyroux J, 2021, IEEE INT CONF ROBOT, P96, DOI 10.1109/ICRA48506.2021.9560937
   Eshraghian J. K., 2021, SNNTORCH
   Eshraghian JK, 2020, NAT MACH INTELL, V2, P157, DOI 10.1038/s42256-020-0161-x
   Eshraghian JK, 2020, IEEE INSTRU MEAS MAG, V23, P21, DOI 10.1109/MIM.2020.8979519
   Eshraghian JK, 2018, INT J NEURAL SYST, V28, DOI 10.1142/S0129065718500041
   Eshraghian JK, 2018, IEEE T VLSI SYST, V26, P2816, DOI 10.1109/TVLSI.2018.2829918
   Esser SK, 2016, P NATL ACAD SCI USA, V113, P11441, DOI 10.1073/pnas.1604850113
   Fang W., 2021, ADV NEURAL INFORM PR, V34, P21056, DOI DOI 10.48550/ARXIV.2102.04159
   Frenkel C, 2023, P IEEE, V111, P623, DOI 10.1109/JPROC.2023.3273520
   Frenkel C, 2021, FRONT NEUROSCI-SWITZ, V15, DOI 10.3389/fnins.2021.629892
   Furber SB, 2004, NEURAL NETWORKS, V17, P1437, DOI 10.1016/j.neunet.2004.07.003
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   Gaba S, 2014, IEEE INT SYMP CIRC S, P2592, DOI 10.1109/ISCAS.2014.6865703
   Gallego G., 2019, ARXIV
   Gallego G, 2022, IEEE T PATTERN ANAL, V44, P154, DOI 10.1109/TPAMI.2020.3008413
   Gehrig M, 2021, IEEE ROBOT AUTOM LET, V6, P4947, DOI 10.1109/LRA.2021.3068942
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169
   Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81
   Glorot X., 2010, P JMLR WORKSH C P 13, P249, DOI DOI 10.1177/1753193409103364.
   Godbole V., 2023, DEEP LEARNING TUNING
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Graves A, 2014, PR MACH LEARN RES, V32, P1764
   Guerguiev J, 2017, ELIFE, V6, DOI 10.7554/eLife.22901
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   HAESSIG G, 2020, IEEE INT SYMP CIRC S, pNI341, DOI DOI 10.1109/iscas45731.2020.9180442
   Hamilton T, 2021, NAT MACH INTELL, V3, P194, DOI 10.1038/s42256-021-00315-0
   Hannun AY, 2019, NAT MED, V25, P65, DOI 10.1038/s41591-018-0268-3
   He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123
   He T, 2019, PROC CVPR IEEE, P558, DOI 10.1109/CVPR.2019.00065
   He YM, 2022, IEEE J SOLID-ST CIRC, V57, P3058, DOI 10.1109/JSSC.2022.3193846
   HEBB D. O., 1949
   Hecht S, 1942, J GEN PHYSIOL, V25, P819, DOI 10.1085/jgp.25.6.819
   Henkes A., 2022, ARXIV
   Hinton G., 2022, ARXIV
   Hinton G., 2016, P STANF U C COMP SYS
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Hu YT, 2018, ARXIV
   Hu YH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00405
   Huang YP, 2011, WIRES COGN SCI, V2, P580, DOI 10.1002/wcs.142
   HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837
   Huh D., 2017, ARXIV
   Hunsberger E., 2015, ARXIV
   Hussaini S, 2022, IEEE ROBOT AUTOM LET, V7, P4094, DOI 10.1109/LRA.2022.3149030
   Ioffe S., 2015, ICML, DOI DOI 10.1007/S13398-014-0173-7.2
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Jin SM, 2023, COMPLEX INTELL SYST, V9, P4959, DOI 10.1007/s40747-023-00983-y
   Jun JJ, 2017, NATURE, V551, P232, DOI 10.1038/nature24636
   Kag A, 2021, PR MACH LEARN RES, V139
   kaggle, US
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90
   Kaiser J, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00424
   Kanitscheider I., 2018, P INT C LEARN REPR, P1
   Kell AJE, 2018, NEURON, V98, P630, DOI 10.1016/j.neuron.2018.03.044
   Kheradpisheh SR, 2020, INT J NEURAL SYST, V30, DOI 10.1142/S0129065720500276
   Kim Y, 2021, FRONT NEUROSCI-SWITZ, V15, DOI 10.3389/fnins.2021.773954
   Kingma D. P., 2014, ARXIV, DOI DOI 10.48550/ARXIV.1412.6980
   Kornijcuk V, 2019, ADV INTELL SYST-GER, V1, DOI 10.1002/aisy.201900030
   Krips R, 2009, NEURAL COMPUT, V21, P2524, DOI 10.1162/neco.2009.07-07-563
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
   Kushnir L., 2019, ARXIV
   Lammie C, 2021, IEEE T CIRCUITS-II, V68, P1650, DOI 10.1109/TCSII.2021.3065932
   Lapique L., 1907, J PHYSL PATHOL GEN, V9, P620, DOI DOI 10.1007/S00422-007-0189-6
   Laskin M., 2020, ARXIV
   Levy W. B., 2020, BIORXIV
   Lichtsteiner P., 2006, IEEE INT SOL STAT CI, P2060, DOI DOI 10.1109/ISSCC.2006.1696265
   Lillicrap T. P., 2014, ARXIV
   Lillicrap TP, 2020, NAT REV NEUROSCI, V21, P335, DOI 10.1038/s41583-020-0277-3
   Linnainmaa S., 1970, REPRESENTATION CUMUL
   Liu Y, 2016, IEEE INT SYMP CIRC S, P538, DOI 10.1109/ISCAS.2016.7527296
   Luboeinski J, 2021, COMMUN BIOL, V4, DOI 10.1038/s42003-021-01778-y
   Luong M.T., 2015, EFFECTIVE APPROACHES
   Marblestone AH, 2016, FRONT COMPUT NEUROSC, V10, DOI 10.3389/fncom.2016.00094
   Marschall O, 2020, J MACH LEARN RES, V21
   McKinney SM, 2020, NATURE, V577, P89, DOI 10.1038/s41586-019-1799-6
   Mehonic A., 2023, APL MACH LEARN, V1
   Mehta MR, 2002, NATURE, V417, P741, DOI 10.1038/nature00807
   Mello GBM, 2015, CURR BIOL, V25, P1113, DOI 10.1016/j.cub.2015.02.036
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Mikolov T., 2013, P 26 INT C NEURAL IN, P3111, DOI DOI 10.1162/JMLR.2003.3.4-5.951
   Mikolov T, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2, P1045
   Miller J., 2019, ARXIV
   Modaresi F., 2023, ARXIV
   Moskovitz T. H., 2018, ARXIV
   Mostafa H, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00608
   Mueggler E, 2017, INT J ROBOT RES, V36, P142, DOI 10.1177/0278364917691115
   Mujika A, 2018, ADV NEUR IN, V31
   Murray JM, 2019, ELIFE, V8, DOI 10.7554/eLife.43299
   Neckar A, 2019, P IEEE, V107, P144, DOI 10.1109/JPROC.2018.2881432
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Neftci EO, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00324
   Olshausen B. A., 2006, 23 PROBLEMS SYSTEMS, V23, P182, DOI DOI 10.1093/ACPROF:OSO/9780195148220.001.0001
   Orchard G, 2015, FRONT NEUROSCI-SWITZ, V9, DOI [10.3389/fnins.2015.00437, 10.3389/fhins.2015.00437]
   Ortner T., 2023, ARXIV
   Ottati F., 2023, ARXIV
   Paszke A., 2017, P NEUR INF PROC SYST, P1, DOI DOI 10.1017/CBO9781107707221.009
   Paun G., 2005, DNA COMPUTING NEW CO
   Payvand M, 2020, IEEE J EM SEL TOP C, V10, P522, DOI 10.1109/JETCAS.2020.3040248
   Pei J, 2019, NATURE, V572, P106, DOI 10.1038/s41586-019-1424-8
   Perez-Nieves N., 2021, ARXIV
   Perez-Nieves N., 2021, NATURE COMMUN, V12, P1
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Qian L, 2011, NATURE, V475, P368, DOI 10.1038/nature10262
   Querlioz D, 2013, IEEE T NANOTECHNOL, V12, P288, DOI 10.1109/TNANO.2013.2250995
   Quintana F. M., 2023, ARXIV
   Rajalingham R, 2018, J NEUROSCI, V38, P7255, DOI 10.1523/JNEUROSCI.0388-18.2018
   Rathi N., 2019, P INT C LEARN REPR, P1
   Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91
   Ren SQ, 2017, IEEE T PATTERN ANAL, V39, P1137, DOI 10.1109/TPAMI.2016.2577031
   Renart A, 2003, NEURON, V38, P473, DOI 10.1016/S0896-6273(03)00255-1
   Renart A, 2004, MATH COMP BIOL SER, P431
   Richards BA, 2019, NAT NEUROSCI, V22, P1761, DOI 10.1038/s41593-019-0520-2
   Rieke F, 1998, REV MOD PHYS, V70, P1027, DOI 10.1103/RevModPhys.70.1027
   Rigotti M, 2010, FRONT COMPUT NEUROSC, V4, DOI 10.3389/fncom.2010.00024
   Robertson DA, 2023, PSYCHOL MED, V53, P3238, DOI 10.1017/S0033291721003743
   ROSENBLATT F, 1958, PSYCHOL REV, V65, P386, DOI 10.1037/h0042519
   Roy K, 2019, NATURE, V575, P607, DOI 10.1038/s41586-019-1677-2
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Rüedi PF, 2003, IEEE J SOLID-ST CIRC, V38, P2325, DOI 10.1109/JSSC.2003.819169
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Sandamirskaya Y, 2022, SCI ROBOT, V7, DOI 10.1126/scirobotics.abl8419
   Sanhueza M, 2013, MOL BRAIN, V6, DOI 10.1186/1756-6606-6-10
   Schmidgall S., 2023, ARXIV
   Schrauwen B, 2004, IEEE IJCNN, P471, DOI 10.1109/IJCNN.2004.1379954
   Schrimpf Martin, 2020, BIORXIV, P8, DOI [10.1101/407007, DOI 10.1101/407007]
   Schultz W, 1997, SCIENCE, V275, P1593, DOI 10.1126/science.275.5306.1593
   Serrano-Gotarredona T, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00481
   Seung HS, 2003, NEURON, V40, P1063, DOI 10.1016/S0896-6273(03)00761-X
   Sharifshazileh M, 2021, NAT COMMUN, V12, DOI 10.1038/s41467-021-23342-2
   Shinomoto S, 2007, STAT MED, V26, P4032, DOI 10.1002/sim.2932
   Shrestha SB, 2018, ADV NEUR IN, V31
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Sjöström PJ, 2001, NEURON, V32, P1149, DOI 10.1016/s0896-6273(01)00542-6
   Steinmetz NA, 2021, SCIENCE, V372, P258, DOI 10.1126/science.abf4588
   Stöckl C, 2021, NAT MACH INTELL, V3, DOI 10.1038/s42256-021-00311-4
   Sun P.-S. V., 2022, ARXIV
   Taherkhani A, 2015, IEEE T NEUR NET LEAR, V26, P3137, DOI 10.1109/TNNLS.2015.2404938
   Tallec C., 2018, P INT C LEARN REPR I, P1
   Thompson N. C., 2020, ARXIV
   Thorpe S, 1998, COMPUTATIONAL NEUROSCIENCE: TRENDS IN RESEARCH, P113
   Timcheck J, 2023, NEUROMORPH COMPUT EN, V3, DOI 10.1088/2634-4386/ace737
   VANDERVELDEN HA, 1946, OPHTHALMOLOGICA, V111, P321, DOI 10.1159/000300352
   Vaswani A., 2017, P 31 INT C NEURAL IN
   Vinyals O, 2019, NATURE, V575, P350, DOI 10.1038/s41586-019-1724-z
   Voelker A., 2019, P ADV NEUR INF PROC, V32, P1
   Vogels TP, 2005, J NEUROSCI, V25, P10786, DOI 10.1523/JNEUROSCI.3508-05.2005
   Wark B, 2007, CURR OPIN NEUROBIOL, V17, P423, DOI 10.1016/j.conb.2007.07.001
   Watt Alanna J, 2010, Front Synaptic Neurosci, V2, P5, DOI 10.3389/fnsyn.2010.00005
   Wei DX, 2021, APPL ENERG, V292, DOI 10.1016/j.apenergy.2021.116842
   Werfel J, 2005, NEURAL COMPUT, V17, P2699, DOI 10.1162/089976605774320539
   Williams R. J., 1988, NUCCS883
   WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696
   Williams RJ, 1989, NEURAL COMPUT, V1, P270, DOI 10.1162/neco.1989.1.2.270
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Wunderlich TC, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-91786-z
   Xiao W., 2018, ARXIV
   Xu Y, 2013, NEURAL NETWORKS, V43, P99, DOI 10.1016/j.neunet.2013.02.003
   Yan ZL, 2021, BIOMED SIGNAL PROCES, V63, DOI 10.1016/j.bspc.2020.102170
   Yang YK, 2023, NEUROMORPH COMPUT EN, V3, DOI 10.1088/2634-4386/acbab8
   Yang YK, 2022, ROY SOC OPEN SCI, V9, DOI 10.1098/rsos.220374
   Yang YK, 2022, IEEE J BIOMED HEALTH, V26, P3529, DOI 10.1109/JBHI.2022.3157877
   Yik J., 2023, ARXIV
   Yin BJ, 2023, NAT MACH INTELL, V5, P518, DOI 10.1038/s42256-023-00650-4
   Zenke F., 2019, SPYTORCH
   Zenke F, 2021, P IEEE, V109, P935, DOI 10.1109/JPROC.2020.3045625
   Zenke F, 2021, NEURAL COMPUT, V33, P899, DOI 10.1162/neco_a_01367
   Zhang M., 2020, ARXIV
   Zhang Y, 2017, INT CONF ACOUST SPEE, P4845, DOI 10.1109/ICASSP.2017.7953077
   Zhao B, 2015, IEEE T NEUR NET LEAR, V26, P1963, DOI 10.1109/TNNLS.2014.2362542
   Zhu Alex Zihao, 2018, IEEE Robotics and Automation Letters, V3, P2032, DOI 10.1109/LRA.2018.2800793
   Zhu R., 2023, ARXIV
NR 235
TC 0
Z9 0
U1 8
U2 8
PD SEP
PY 2023
VL 111
IS 9
BP 1016
EP 1054
DI 10.1109/JPROC.2023.3308088
EA SEP 2023
WC Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Shaaban, A
   Furtner, W
   Weigel, R
   Lurz, F
AF Shaaban, Ahmed
   Furtner, Wolfgang
   Weigel, Robert
   Lurz, Fabian
GP IEEE
TI Spiking Neural Networks for Gesture Recognition Using Time Domain Radar
   Data
SO 2022 19TH EUROPEAN RADAR CONFERENCE (EURAD)
SE European Radar Conference EuRAD
DT Proceedings Paper
CT 19th European Radar Conference (EuRAD) as part of 25th European
   Microwave Week
CY SEP 25-30, 2022
CL Milan, ITALY
DE Spiking Neural Networks; Radar Gesture Recognition; Convolutional Neural
   Networks; FMCW Radar; Raw Radar Data; Time Domain Radar Data
AB Gesture recognition using luminance invariant radar sensors is vital due to its extensive use in human-machine interfaces. However, the necessity for computationally expensive radar data pre-processing steps represented by fast Fourier transforms to get range and Doppler features are regarded as a contemporary concern. In this work, we present a solution for gesture recognition that relies on time-domain radar data applied to an event-driven, sparse, and end-to-end trained spiking neural network architecture. Using the proposed solution, it is possible to discriminate between 10 different gestures in a gesture dataset recorded using a 60 GHz frequency-modulated continuous-wave radar sensor, with a mean test accuracy of 93.1%.
C1 [Shaaban, Ahmed; Furtner, Wolfgang] Infineon Technol AG, Munich, Germany.
   [Shaaban, Ahmed; Weigel, Robert; Lurz, Fabian] Univ Erlangen Nurnberg, Inst Elect Engn, Erlangen, Germany.
RP Shaaban, A (corresponding author), Infineon Technol AG, Munich, Germany.; Shaaban, A (corresponding author), Univ Erlangen Nurnberg, Inst Elect Engn, Erlangen, Germany.
EM Ahmed.Shaaban@infineon.com; Wolfgang.Furtner@infineon.com;
   Robert.Weigel@fau.de; Fabian.Lurz@fau.de
CR Akiba T, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P2623, DOI 10.1145/3292500.3330701
   Amin MG, 2019, IEEE RAD CONF, DOI [10.1109/radar.2019.8835661, 10.1109/pst47121.2019.8949029]
   Arsalan Muhammad, 2021, 2021 IEEE MTT-S International Microwave Symposium (IMS), P78, DOI 10.1109/IMS19712.2021.9574994
   Auge D, 2021, IEEE INT CONF AUTOMA, DOI 10.1109/FG52635.2021.9666988
   Eshraghian JK, 2023, Arxiv, DOI [arXiv:2109.12894, DOI 10.48550/ARXIV.2109.12894]
   Gerstner W., 2002, SPIKING NEURON MODEL
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Hazra S, 2019, IEEE ACCESS, V7, P125623, DOI 10.1109/ACCESS.2019.2938725
   Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508
   Lien JM, 2016, ACM T GRAPHIC, V35, DOI [10.1145/2897824.2925953, 10.1145/9999997.9999999]
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Richards M.A., 2010, PRINCIPLES MODERN RA
   Tsang I. J., 2021, RSF-RUS SAGE J SOC S, V10
   Zhang ZY, 2018, IEEE SENS J, V18, P3278, DOI 10.1109/JSEN.2018.2808688
   Zimmermann C, 2017, IEEE I CONF COMP VIS, P4913, DOI 10.1109/ICCV.2017.525
NR 16
TC 0
Z9 0
U1 1
U2 2
PY 2022
BP 33
EP 36
WC Engineering, Electrical & Electronic; Telecommunications
DA 2023-11-11
ER

PT C
AU Smith, JE
AF Smith, James E.
GP IEEE
TI Space-Time Algebra: A Model for Neocortical Computation
SO 2018 ACM/IEEE 45TH ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER
   ARCHITECTURE (ISCA)
SE Conference Proceedings Annual International Symposium on Computer
   Architecture
DT Proceedings Paper
CT 45th ACM/IEEE Annual International Symposium on Computer Architecture
   (ISCA)
CY JUN 01-06, 2018
CL Los Angeles, CA
DE temporal neural networks; spiking neurons; space-time algebra; race
   logic
ID VISUAL FEATURES; SPIKING NEURONS; NEURAL-NETWORK; INTEGRATE
AB A proposed first step in replicating the computational methods used in the brain's neocortex is the development of a feedforward computing paradigm based on temporal relationships among inter-neuron voltage spikes. A "space-time" algebra captures the essential features of such a paradigm. The space-time algebra supports biologically plausible neural networks, as envisioned by theoretical neuroscientists. It also supports a generalization of previously proposed "race logic". A key feature of race logic is that it can be directly implemented with off-the-shelf CMOS digital circuits. This opens the possibility of designing brain-like neural networks in the neuroscience domain and implementing them directly in the CMOS digital circuit domain.
C1 [Smith, James E.] Univ Wisconsin Madison, Dept Elect & Comp Engn, Madison, WI 53706 USA.
RP Smith, JE (corresponding author), Univ Wisconsin Madison, Dept Elect & Comp Engn, Madison, WI 53706 USA.
EM jes@ece.wisc.edu
CR Abeles M., 1991, CORTICONICS NEURAL C
   [Anonymous], 2010, 2010 INT JOINT C NEU
   [Anonymous], 2016, ARXIV161101421
   [Anonymous], 1988, RSREMEMO4148
   [Anonymous], 2012, 2012 INT JOINT C NEU, DOI [DOI 10.1109/IJCNN.2012.6252637, 10.1109/IJCNN.2012.6252637]
   [Anonymous], 2011, FRONTIERS NEUROSCIEN
   Batcher K. E., 1968, AFIPS CONF P, P307, DOI [DOI 10.1145/1468075.1468121, 10.1145/1468075.1468121]
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Bichler O, 2012, NEURAL NETWORKS, V32, P339, DOI 10.1016/j.neunet.2012.02.022
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   Brette R, 2015, FRONT SYST NEUROSCI, V9, DOI 10.3389/fnsys.2015.00151
   Casasanto D, 2008, COGNITION, V106, P579, DOI 10.1016/j.cognition.2007.03.004
   Deiss SR, 1998, PULSED NEURAL NETWORKS, P157
   Delorme A, 1999, NEUROCOMPUTING, V26-7, P989, DOI 10.1016/S0925-2312(99)00095-8
   Diehl Peter U, 2015, 2015 INT JOINT C NEU, P1, DOI [DOI 10.1109/IJCNN.2015.7280696, 10.1109/IJCNN.2015.7280696]
   Eliasmith C, 2012, SCIENCE, V338, P1202, DOI 10.1126/science.1225266
   Fries P, 2007, TRENDS NEUROSCI, V30, P309, DOI 10.1016/j.tins.2007.05.005
   Fusi S, 1999, NEURAL COMPUT, V11, P633, DOI 10.1162/089976699300016601
   Gerstner W, 1996, NATURE, V383, P76, DOI 10.1038/383076a0
   Gütig R, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0053063
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Guyonneau R, 2004, J PHYSIOL-PARIS, V98, P487, DOI 10.1016/j.jphysparis.2005.09.004
   Guyonneau R, 2005, NEURAL COMPUT, V17, P859, DOI 10.1162/0899766053429390
   Hoare T, 2005, COMPUT J, V48, P49, DOI 10.1093/comjnl/bxh065
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   Institute of Medicine (US) Forum on Neuroscience and Nervous System Disorders, 2008, MOL MINDS CHALL 21 C
   Irwin M. J., 2005, REVITALIZING COMPUTE
   Karnani MM, 2014, CURR OPIN NEUROBIOL, V26, P96, DOI 10.1016/j.conb.2013.12.015
   Kheradpisheh SR, 2016, NEUROCOMPUTING, V205, P382, DOI 10.1016/j.neucom.2016.04.029
   Kistler WM, 1997, NEURAL COMPUT, V9, P1015, DOI 10.1162/neco.1997.9.5.1015
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Madhavan A, 2015, IEEE MICRO, V35, P48, DOI 10.1109/MM.2015.43
   MAINEN ZF, 1995, SCIENCE, V268, P1503, DOI 10.1126/science.7770778
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   Masquelier T, 2007, PLOS COMPUT BIOL, V3, P247, DOI 10.1371/journal.pcbi.0030031
   Morrison A, 2008, BIOL CYBERN, V98, P459, DOI 10.1007/s00422-008-0233-1
   Mountcastle VB, 1997, BRAIN, V120, P701, DOI 10.1093/brain/120.4.701
   NAE Grand Challenges for Engineering, REV ENG BRAIN
   Natschlager T, 1998, NETWORK-COMP NEURAL, V9, P319, DOI 10.1088/0954-898X/9/3/003
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   Pfeil T, 2012, FRONT NEUROSCI-SWITZ, V6, DOI 10.3389/fnins.2012.00090
   Probst Dimitri, 2012, Artificial Neural Networks and Machine Learning - ICANN 2012. Proceedings of the 22nd International Conference on Artificial Neural Networks, P209, DOI 10.1007/978-3-642-33269-2_27
   Querlioz D, 2013, IEEE T NANOTECHNOL, V12, P288, DOI 10.1109/TNANO.2013.2250995
   Schrauwen B, 2004, IEEE IJCNN, P471, DOI 10.1109/IJCNN.2004.1379954
   Singer W., 2010, DYNAMIC COORDINATION
   Smith J. E, 2017, SYNTHESIS LECT COMPU
   STEIN RB, 1965, BIOPHYS J, V5, P173, DOI 10.1016/S0006-3495(65)86709-1
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   THORPE SJ, 1989, CONNECTIONISM IN PERSPECTIVE, P63
   TUCKWELL HC, 1979, J THEOR BIOL, V77, P65, DOI 10.1016/0022-5193(79)90138-3
   VanRullen R, 2005, TRENDS NEUROSCI, V28, P1, DOI 10.1016/j.tins.2004.10.010
   Weidenbacher U, 2008, LECT NOTES ARTIF INT, V5078, P123, DOI 10.1007/978-3-540-69369-7_14
   Wysoski SG, 2010, NEURAL NETWORKS, V23, P819, DOI 10.1016/j.neunet.2010.04.009
   Yuste R, 2015, NAT REV NEUROSCI, V16, P487, DOI 10.1038/nrn3962
   Zhao B, 2015, IEEE T NEUR NET LEAR, V26, P1963, DOI 10.1109/TNNLS.2014.2362542
NR 57
TC 14
Z9 15
U1 0
U2 0
PY 2018
BP 289
EP 300
DI 10.1109/ISCA.2018.00033
WC Computer Science, Hardware & Architecture; Engineering, Electrical &
   Electronic
DA 2023-11-11
ER

PT J
AU Surazhevsky, IA
   Demin, VA
   Ilyasov, AI
   Emelyanov, AV
   Nikiruy, KE
   Rylkov, VV
   Shchanikov, SA
   Bordanov, IA
   Gerasimova, SA
   Guseinov, DV
   Malekhonova, NV
   Pavlov, DA
   Belov, AI
   Mikhaylov, AN
   Kazantsev, VB
   Valenti, D
   Spagnolo, B
   Kovalchuk, MV
AF Surazhevsky, I. A.
   Demin, V. A.
   Ilyasov, A. I.
   Emelyanov, A., V
   Nikiruy, K. E.
   Rylkov, V. V.
   Shchanikov, S. A.
   Bordanov, I. A.
   Gerasimova, S. A.
   Guseinov, D., V
   Malekhonova, N., V
   Pavlov, D. A.
   Belov, A., I
   Mikhaylov, A. N.
   Kazantsev, V. B.
   Valenti, D.
   Spagnolo, B.
   Kovalchuk, M., V
TI Noise-assisted persistence and recovery of memory state in a memristive
   spiking neuromorphic network
SO CHAOS SOLITONS & FRACTALS
DT Article
ID DEPENDENT PLASTICITY; STOCHASTIC RESONANCE; NEURAL-NETWORKS; MODEL;
   TIME; CLASSIFICATION; TRANSPORT; SYNAPSES; LIFETIME; DEVICE
AB We investigate the constructive role of an external noise signal, in the form of a low-rate Poisson sequence of pulses supplied to all inputs of a spiking neural network, consisting in maintaining for a long time or even recovering a memory trace (engram) of the image without its direct renewal (or rewriting). In particular, this unique dynamic property is demonstrated in a single-layer spiking neural network consisting of simple integrate-and-fire neurons and memristive synaptic weights. This is carried out by preserving and even fine-tuning the conductance values of memristors in terms of dynamic plasticity, specifically spike-timing-dependent plasticity-type, driven by overlapping pre- and postsynaptic voltage spikes. It has been shown that the weights can be to a certain extent unreliable, due to such characteristics as the limited retention time of resistive state or the variation of switching voltages. Such a noise-assisted persistence of memory , on one hand, could be a prototypical mechanism in a biological nervous system and, on the other hand, brings one step closer to the possibility of building reliable spiking neural networks composed of unreliable analog elements. (C) 2021 Elsevier Ltd. All rights reserved.
C1 [Surazhevsky, I. A.; Demin, V. A.; Ilyasov, A. I.; Emelyanov, A., V; Nikiruy, K. E.; Rylkov, V. V.; Kovalchuk, M., V] Kurchatov Inst, Natl Res Ctr, Moscow 123182, Russia.
   [Ilyasov, A. I.; Kovalchuk, M., V] Lomonosov Moscow State Univ, Fac Phys, Moscow 119991, Russia.
   [Emelyanov, A., V] State Univ, Moscow Inst Phys & Technol, Dolgoprudnyi 141700, Moscow Region, Russia.
   [Rylkov, V. V.] Kotelnikov Inst Radio Engn & Elect RAS, Fryazino 141190, Moscow Region, Russia.
   [Shchanikov, S. A.; Bordanov, I. A.] Vladimir State Univ, Dept Informat Technol, Vladimir 600000, Russia.
   [Shchanikov, S. A.; Gerasimova, S. A.; Guseinov, D., V; Malekhonova, N., V; Pavlov, D. A.; Belov, A., I; Mikhaylov, A. N.; Kazantsev, V. B.; Valenti, D.; Spagnolo, B.] Lobachevsky Univ, Nizhnii Novgorod 603950, Russia.
   [Valenti, D.; Spagnolo, B.] Univ Palermo, Grp Interdisciplinary Theoret Phys, Dipartimento Fis & Chim Emilio Segre, Viale Sci,Edif 18, I-90128 Palermo, Italy.
   [Spagnolo, B.] Ist Nazl Fis Nucl, Sez Catania, Via S Sofia 64, I-95123 Catania, Italy.
RP Surazhevsky, IA (corresponding author), Kurchatov Inst, Natl Res Ctr, Moscow 123182, Russia.
EM isurazhevsky@yandex.ru
CR Agudov NV, 2020, J STAT MECH-THEORY E, V2020, DOI 10.1088/1742-5468/ab684a
   [Anonymous], 2016, ARXIV161104465
   Berggren K, 2021, NANOTECHNOLOGY, V32, DOI 10.1088/1361-6528/aba70f
   Brivio S, 2019, NANOTECHNOLOGY, V30, DOI 10.1088/1361-6528/aae81c
   Brivio S, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-41497-3
   Brunetti R, 2007, J COMPUT ELECTRON, V6, P391, DOI 10.1007/s10825-006-0140-4
   Cai FX, 2020, NAT ELECTRON, V3, P409, DOI 10.1038/s41928-020-0436-6
   Covi E, 2018, J PHYS D APPL PHYS, V51, DOI 10.1088/1361-6463/aad361
   Denaro G, 2013, ECOL COMPLEX, V13, P21, DOI 10.1016/j.ecocom.2012.10.002
   DOERING CR, 1992, PHYS REV LETT, V69, P2318, DOI 10.1103/PhysRevLett.69.2318
   Dubkov AA, 2008, EUR PHYS J B, V65, P361, DOI 10.1140/epjb/e2008-00337-0
   Dubkov AA, 2005, PHYS REV E, V72, DOI 10.1103/PhysRevE.72.041104
   Emelyanov AV, 2020, NANOTECHNOLOGY, V31, DOI 10.1088/1361-6528/ab4a6d
   Emelyanov AV, 2019, MICROELECTRON ENG, V215, DOI 10.1016/j.mee.2019.110988
   Emelyanov AV, 2016, AIP ADV, V6, DOI 10.1063/1.4966257
   Falci G, 2013, PHYS REV B, V87, DOI 10.1103/PhysRevB.87.214515
   FATT P, 1952, J PHYSIOL-LONDON, V117, P109
   Filatov DO, 2019, J STAT MECH-THEORY E, V2019, DOI 10.1088/1742-5468/ab5704
   Gammaitoni L, 1998, REV MOD PHYS, V70, P223, DOI 10.1103/RevModPhys.70.223
   Ge J, 2020, NANOSCALE, V12, P720, DOI 10.1039/c9nr08001e
   Gerasimova SA, 1959, AIP C P
   Giuffrida A, 2009, EUR FOOD RES TECHNOL, V228, P767, DOI 10.1007/s00217-008-0988-6
   Guarcello C, 2016, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2016/05/054012
   Guseinov DV, 2017, INT J NANOTECHNOL, V14, P604, DOI 10.1504/IJNT.2017.083436
   Hurtado PI, 2006, PHYS REV E, V74, DOI 10.1103/PhysRevE.74.050101
   Ielmini D, 2016, SEMICOND SCI TECH, V31, DOI 10.1088/0268-1242/31/6/063002
   Izhikevich EM, 2003, NEURAL COMPUT, V15, P1511, DOI 10.1162/089976603321891783
   Juzekaeva E, 2019, ADV MATER TECHNOL-US, V4, DOI 10.1002/admt.201800350
   Kim S, 2019, NANOSCALE, V11, P237, DOI 10.1039/c8nr06694a
   Kosko B, 2003, NEURAL NETWORKS, V16, P755, DOI 10.1016/S0893-6080(03)00128-X
   Kvatinsky S, 2015, IEEE T CIRCUITS-II, V62, P786, DOI 10.1109/TCSII.2015.2433536
   Lanzara E, 1997, AM J PHYS, V65, P341, DOI 10.1119/1.18520
   LAUGER P, 1984, EUR BIOPHYS J BIOPHY, V11, P117, DOI 10.1007/BF00276627
   Li C, 2019, NAT MACH INTELL, V1, P49, DOI 10.1038/s42256-018-0001-4
   Li C, 2018, NAT ELECTRON, V1, P52, DOI 10.1038/s41928-017-0002-z
   Li CG, 2006, PLOS COMPUT BIOL, V2, P925, DOI 10.1371/journal.pcbi.0020103
   Li JH, 2010, PHYS REV E, V82, DOI 10.1103/PhysRevE.82.041104
   Lobov SA, 2017, MATH MODEL NAT PHENO, V12, P109, DOI 10.1051/mmnp/201712409
   MANTEGNA RN, 1995, NUOVO CIMENTO D, V17, P873, DOI 10.1007/BF02451845
   Mantegna RN, 2001, PHYS REV E, V63, DOI 10.1103/PhysRevE.63.011101
   Mantegna RN, 1998, INT J BIFURCAT CHAOS, V8, P783, DOI 10.1142/S0218127498000577
   Martyshov MN, 2020, PHYS REV APPL, V14, DOI 10.1103/PhysRevApplied.14.034016
   Matsukatova AN, 2020, J COMMUN TECHNOL EL+, V65, P1198, DOI 10.1134/S1064226920090077
   MCNAMARA B, 1988, PHYS REV LETT, V60, P2626, DOI 10.1103/PhysRevLett.60.2626
   Mikhaylov AN, 2016, PHYS STATUS SOLIDI C, V13, P870, DOI 10.1002/pssc.201600083
   Mikhaylov A, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00358
   Mikhaylov A, 2020, ADV MATER TECHNOL-US, V5, DOI 10.1002/admt.201900607
   Mikhaylov AN, 2018, IEEE TETCI, V2, P371, DOI 10.1109/TETCI.2018.2829922
   Milo V, 2020, MATERIALS, V13, DOI 10.3390/ma13010166
   Minnekhanov AA, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-47263-9
   Moon J, 2019, NAT ELECTRON, V2, P480, DOI 10.1038/s41928-019-0313-3
   Nikiruy KE, 2019, AIP ADV, V9, DOI 10.1063/1.5111083
   Nikiruy KE, 2019, TECH PHYS LETT+, V45, P386, DOI 10.1134/S1063785019040278
   Nikiruy KE, 2018, TECH PHYS LETT+, V44, P416, DOI 10.1134/S106378501805022X
   Nikiruy KE, 2020, PHYS STATUS SOLIDI A, V217, DOI 10.1002/pssa.201900938
   Orlandi JG, 2013, NAT PHYS, V9, P582, DOI [10.1038/nphys2686, 10.1038/NPHYS2686]
   Pankratov AL, 2004, PHYS REV LETT, V93, DOI 10.1103/PhysRevLett.93.177001
   Pankratov EL, 2005, EUR PHYS J B, V46, P15, DOI 10.1140/epjb/e2005-00233-1
   Pérez E, 2019, MICROELECTRON ENG, V214, P104, DOI 10.1016/j.mee.2019.05.004
   Pizzolato N, 2010, PHYS BIOL, V7, DOI 10.1088/1478-3975/7/3/034001
   Prezioso M, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-07757-y
   Prezioso M, 2016, SCI REP-UK, V6, DOI 10.1038/srep21331
   Prezioso M, 2015, NATURE, V521, P61, DOI 10.1038/nature14441
   Prudnikov NV, 2020, J PHYS D APPL PHYS, V53, DOI 10.1088/1361-6463/ab9262
   Querlioz D, 2013, IEEE T NANOTECHNOL, V12, P288, DOI 10.1109/TNANO.2013.2250995
   Ramirez JM, 2004, CURR OPIN NEUROBIOL, V14, P665, DOI 10.1016/j.conb.2004.10.011
   Roberson ED, 1999, LEARN MEMORY, V6, P381
   Roldan JB, 2020, INT RELIAB PHY SYM, DOI 10.1109/irps45951.2020.9129147
   Rylkov VV, 2020, J EXP THEOR PHYS+, V131, P160, DOI 10.1134/S1063776120070109
   Rylkov VV, 2018, J EXP THEOR PHYS+, V126, P353, DOI 10.1134/S1063776118020152
   Ryu JH, 2020, CHAOS SOLITON FRACT, V140, DOI 10.1016/j.chaos.2020.110236
   Sboev A, 2020, MATH METHOD APPL SCI, V43, P7802, DOI 10.1002/mma.6241
   Serb A, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms12611
   Smolen P, 2019, LEARN MEMORY, V26, P133, DOI 10.1101/lm.049395.119
   Spagnolo B, 2007, ACTA PHYS POL B, V38, P1925
   Spagnolo B, 2004, EUR PHYS J B, V40, P273, DOI 10.1140/epjb/e2004-00268-8
   Spagnolo B, 2015, CHAOS SOLITON FRACT, V81, P412, DOI 10.1016/j.chaos.2015.07.023
   Spagnolo B, 2017, ENTROPY-SWITZ, V19, DOI 10.3390/e19010020
   Spagnolo B, 2008, INT J BIFURCAT CHAOS, V18, P2775, DOI 10.1142/S0218127408022007
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Strukov DB, 2012, APPL PHYS A-MATER, V107, P509, DOI 10.1007/s00339-012-6902-x
   Sun Z, 2019, P NATL ACAD SCI USA, V116, P4123, DOI 10.1073/pnas.1815682116
   Surazhevsky IA, 2020, NANOINDUSTRY, V96
   Tikhov S, 2018, ADV COND MATTER PHYS, V2018, DOI 10.1155/2018/2028491
   Upadhyay NK, 2020, ADV ELECTRON MATER, V6, DOI 10.1002/aelm.201901411
   Ushakov YV, 2011, PHYS REV LETT, V107, DOI 10.1103/PhysRevLett.107.108103
   Ushakov YV, 2010, PHYS REV E, V81, DOI 10.1103/PhysRevE.81.041911
   Valenti D, 2015, PHYS REV B, V91, DOI 10.1103/PhysRevB.91.235412
   Vorobeva NS, 2017, NEUROSCI BEHAV PHYSI, V47, P780, DOI [10.1007/s11055- 017- 0467-2., DOI 10.1007/S11055-017-0467-2]
   Wang ZR, 2018, NAT ELECTRON, V1, P137, DOI 10.1038/s41928-018-0023-2
   Xia QF, 2019, NAT MATER, V18, P309, DOI 10.1038/s41563-019-0291-x
   Yakimov AV, 2019, APPL PHYS LETT, V114, DOI 10.1063/1.5098066
   Yao P, 2017, NAT COMMUN, V8, DOI 10.1038/ncomms15199
   Zahari F, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-71334-x
   Zhang WQ, 2020, NAT ELECTRON, V3, P371, DOI 10.1038/s41928-020-0435-7
   Zhang Y, 2020, APPL PHYS REV, V7, DOI 10.1063/1.5124027
   Zhu JD, 2020, APPL PHYS REV, V7, DOI 10.1063/1.5118217
   Zhu XJ, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-16261-1
NR 98
TC 59
Z9 59
U1 2
U2 31
PD MAY
PY 2021
VL 146
AR 110890
DI 10.1016/j.chaos.2021.110890
EA MAR 2021
WC Mathematics, Interdisciplinary Applications; Physics, Multidisciplinary;
   Physics, Mathematical
DA 2023-11-11
ER

PT J
AU Memmesheimer, RM
   Timme, M
AF Memmesheimer, Raoul-Martin
   Timme, Marc
TI STABLE AND UNSTABLE PERIODIC ORBITS IN COMPLEX NETWORKS OF SPIKING
   NEURONS WITH DELAYS
SO DISCRETE AND CONTINUOUS DYNAMICAL SYSTEMS
DT Article
DE Spiking neural network; hybrid dynamical system; periodic orbit;
   stability; local cortical circuits; synchronization; attractor neural
   networks
ID PULSE-COUPLED OSCILLATORS; SYNCHRONIZATION; ATTRACTORS; DYNAMICS;
   STATES; HIPPOCAMPUS; SEQUENCES; EXISTENCE; TIME; CODE
AB Is a periodic orbit underlying a periodic pattern of spikes in a heterogeneous neural network stable or unstable? We analytically assess this question in neural networks with delayed interactions by explicitly studying the microscopic time evolution of perturbations. We show that in purely inhibitorily coupled networks of neurons with normal dissipation (concave rise function), such as common leaky integrate-and-fire neurons, a l l orbits underlying non-degenerate periodic spike patterns are stable. In purely inhibitorily coupled networks with strongly connected topology and normal dissipation (strictly concave rise function), they are even asymptotically stable. In contrast, for the same type of individual neurons, all orbits underlying such patterns are unstable if the coupling is excitatory. For networks of neurons with anomalous dissipation ((strictly) convex rise function), the reverse statements hold. For the stable dynamics, we give an analytical lower bound on the local size of the basin of attraction. Numerical simulations of networks with different integrate-and-fire type neurons illustrate our results.
C1 [Memmesheimer, Raoul-Martin] Harvard Univ, Ctr Brain Sci, Fac Arts & Sci, Cambridge, MA 02138 USA.
   [Memmesheimer, Raoul-Martin] Radboud Univ Nijmegen, Dept Neuroinformat, NL-6525 ED Nijmegen, Netherlands.
   [Timme, Marc] Max Planck Inst Dynam & Self Org MPIDS, Network Dynam Grp, Gottingen, Germany.
   [Timme, Marc] BCCN, Gottingen, Germany.
   [Timme, Marc] Univ Gottingen, Dept Phys, Gottingen, Germany.
RP Memmesheimer, RM (corresponding author), Harvard Univ, Ctr Brain Sci, Fac Arts & Sci, Cambridge, MA 02138 USA.
EM r.memmesheimer@science.ru.nl; timme@nld.ds.mpg.de
CR ABBOTT LF, 1993, PHYS REV E, V48, P1483, DOI 10.1103/PhysRevE.48.1483
   Abeles M, 2004, SCIENCE, V304, P523, DOI 10.1126/science.1097725
   ABELES M, 1993, J NEUROPHYSIOL, V70, P1629, DOI 10.1152/jn.1993.70.4.1629
   [Anonymous], 1997, FUNKTIONALANALYSIS
   Ashwin P, 2005, NONLINEARITY, V18, P2035, DOI 10.1088/0951-7715/18/5/009
   BAINOV DD, 1989, STABILITY THEORY APP
   Bloch IJM, 2002, PHYS REV E, V66, DOI 10.1103/PhysRevE.66.036127
   Branicky M. S., 2005, HDB NETWORKED EMBEDD
   Brunel N, 1999, NEURAL COMPUT, V11, P1621, DOI 10.1162/089976699300016179
   Brunel N, 2000, J COMPUT NEUROSCI, V8, P183, DOI 10.1023/A:1008925309027
   Denker M, 2004, PHYS REV LETT, V92, DOI 10.1103/PhysRevLett.92.074103
   DENKER M, 1998, PHYS REV E, V57, P2150
   ERNST U, 1995, PHYS REV LETT, V74, P1570, DOI 10.1103/PhysRevLett.74.1570
   Gansel K, 2005, SOC NEUR ABSTR
   GANSEL K, 2006, SOC NEUR ABSTR
   GREBOGI C, 1982, PHYS REV LETT, V48, P1507, DOI 10.1103/PhysRevLett.48.1507
   Hahnloser RHR, 2002, NATURE, V419, P65, DOI 10.1038/nature00974
   Hansel D, 2001, PHYS REV LETT, V86, P4175, DOI 10.1103/PhysRevLett.86.4175
   Jahnke S, 2008, PHYS REV LETT, V100, DOI 10.1103/PhysRevLett.100.048102
   Jahnke S, 2009, FRONT COMPUT NEUROSC, V3, DOI 10.3389/neuro.10.013.2009
   Jin DZ, 2002, PHYS REV LETT, V89, DOI 10.1103/PhysRevLett.89.208102
   Katok A, 1995, INTRO MODERN THEORY, DOI [10.1017/CBO9780511809187, DOI 10.1017/CBO9780511809187]
   Kumar A, 2008, NEURAL COMPUT, V20, P1, DOI 10.1162/neco.2008.20.1.1
   Lal Mehta M., 1989, MATRIX THEORY SELECT
   Lapique L., 1907, J PHYSL PATHOL GEN, V9, P620, DOI DOI 10.1007/S00422-007-0189-6
   Lee AK, 2002, NEURON, V36, P1183, DOI 10.1016/S0896-6273(02)01096-6
   Memmesheimer RM, 2006, PHYS REV LETT, V97, DOI 10.1103/PhysRevLett.97.188101
   Memmesheimer RM, 2006, PHYSICA D, V224, P182, DOI 10.1016/j.physd.2006.09.037
   MEMMESHEIMER RM, 2008, THESIS GEORGAUGUST U
   MIROLLO RE, 1990, SIAM J APPL MATH, V50, P1645, DOI 10.1137/0150098
   Nádasdy Z, 1999, J NEUROSCI, V19, P9497
   Pipa G, 2007, NEUROCOMPUTING, V70, P2064, DOI 10.1016/j.neucom.2006.10.142
   Singer W, 1999, NEURON, V24, P49, DOI 10.1016/S0896-6273(00)80821-1
   STROGATZ SH, 1993, PHYS REV E, V47, P220, DOI 10.1103/PhysRevE.47.220
   Timme M, 2006, EUROPHYS LETT, V76, P367, DOI 10.1209/epl/i2006-10289-y
   Timme M, 2003, CHAOS, V13, P377, DOI 10.1063/1.1501274
   Timme M, 2002, PHYS REV LETT, V89, DOI 10.1103/PhysRevLett.89.154105
   Timme M, 2008, NONLINEARITY, V21, P1579, DOI 10.1088/0951-7715/21/7/011
   TSODYKS M, 1993, PHYS REV LETT, V71, P1280, DOI 10.1103/PhysRevLett.71.1280
   van Vreeswijk C, 2000, PHYS REV LETT, V84, P5110, DOI 10.1103/PhysRevLett.84.5110
   Van Vreeswijk C, 1994, J Comput Neurosci, V1, P313
   vanVreeswijk C, 1996, PHYS REV E, V54, P5522, DOI 10.1103/PhysRevE.54.5522
   ZHANG J, 2000, HYBRID SYSTEMS COMPU
   Zillmer R, 2006, PHYS REV E, V74, DOI 10.1103/PhysRevE.74.036203
   Zillmer R, 2009, PHYS REV E, V79, DOI 10.1103/PhysRevE.79.031909
   Zumdieck A, 2004, PHYS REV LETT, V93, DOI 10.1103/PhysRevLett.93.244103
NR 46
TC 6
Z9 6
U1 0
U2 8
PD DEC
PY 2010
VL 28
IS 4
BP 1555
EP 1588
DI 10.3934/dcds.2010.28.1555
WC Mathematics, Applied; Mathematics
DA 2023-11-11
ER

PT C
AU Vatajelu, EI
   Di Natale, G
   Anghel, L
AF Vatajelu, Elena-Ioana
   Di Natale, Giorgio
   Anghel, Lorena
GP IEEE
TI Special Session: Reliability of Hardware-Implemented Spiking Neural
   Networks (SNN)
SO 2019 IEEE 37TH VLSI TEST SYMPOSIUM (VTS)
SE IEEE VLSI Test Symposium
DT Proceedings Paper
CT 37th IEEE VLSI Test Symposium (VTS)
CY APR 23-25, 2019
CL Monterey, CA
DE fault modeling; fault tolerance; spiking neural networks; emerging
   memories
ID ARCHITECTURE
AB The research work presented in this paper deals with the fault analysis in hardware-implemented Spiking Neural Networks with special emphasis on circuits designed to perform unsupervised, on-line learning. The paper describes the benefits of such neuromorphic systems, the possibilities of their hardware integration, but more importantly, it underlines the main concerns related to their resilience face to different types of faults. An overview of pertinent fault models and a methodology for conducting fault injection campaigns is described and different scenarios of faulty behaviors occurring after/before the STDP learning are shown.
C1 [Vatajelu, Elena-Ioana; Di Natale, Giorgio; Anghel, Lorena] Univ Grenoble Alpes, CNRS, Grenoble INP, Inst Engn,TIMA, Grenoble, France.
RP Vatajelu, EI (corresponding author), Univ Grenoble Alpes, CNRS, Grenoble INP, Inst Engn,TIMA, Grenoble, France.
EM ioana.vatajelu@univ-grenoble-alpes.fr;
   giorgio.di-natale@univ-grenoble-alpes.fr;
   lorena.anghel@univ-grenoble-alpes.fr
CR Agrawal A., 2019, IEEE T COMPUTERS
   Anghel L., 2018, 26 IFIP IEEE INT C V
   [Anonymous], 2015, NIPS
   [Anonymous], IEEE T BIOMEDICAL CI
   [Anonymous], 2017, IEEE TETC
   Buonomano D., 2009, ENCY NEUROSCIENCE, P265, DOI DOI 10.1016/B978-008045046-9.00822-6
   Burr GW, 2017, ADV PHYS-X, V2, P89, DOI 10.1080/23746149.2016.1259585
   Cassidy AS, 2013, IEEE IJCNN
   Chicca E, 2014, P IEEE, V102, P1367, DOI 10.1109/JPROC.2014.2313954
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Degraeve R, 2015, MICROELECTRON ENG, V147, P171, DOI 10.1016/j.mee.2015.04.025
   Di Mauro A, 2017, DES AUT CON, DOI 10.1145/3061639.3062201
   Diehl P. U., 2015, IEEE T NEURAL NETWOR
   Goodman Dan, 2008, Front Neuroinform, V2, P5, DOI 10.3389/neuro.11.005.2008
   Indiveri G., 2003, P INT S CIRC SYST IS
   Jin X, 2010, COMPUT SCI ENG, V12, P91, DOI 10.1109/MCSE.2010.112
   Jo SH, 2010, NANO LETT, V10, P1297, DOI 10.1021/nl904092h
   Joubert A., 2012, P 2012 INT JOINT C N, P1, DOI [10.1109/ijcnn.2012.6252600, DOI 10.1109/IJCNN.2012.6252600, 10.1109/IJCNN.2012.6252600]
   Khacef L, 2018, IEEE IJCNN
   Khodagholy D, 2015, NAT NEUROSCI, V18, P310, DOI 10.1038/nn.3905
   Kim S., 2018, MATIC LEARNING ERROR
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee C., 2019, IEEE T COGNITIVE DEV
   LI XY, 2016, J COMPUTER SCI TECHN, V31
   Liu Z, 2018, DES AUT CON, DOI 10.1145/3195970.3196022
   Masquelier T. E., 2007, PLOS COMPUTATIONAL B
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Mitra S, 2009, IEEE T BIOMED CIRC S, V3, P32, DOI 10.1109/TBCAS.2008.2005781
   Moradi S, 2014, IEEE T BIOMED CIRC S, V8, P98, DOI 10.1109/TBCAS.2013.2255873
   Neckar A, 2019, P IEEE, V107, P144, DOI 10.1109/JPROC.2018.2881432
   Nishitani Y, 2015, IEEE T NEUR NET LEAR, V26, P2999, DOI 10.1109/TNNLS.2015.2399491
   Querlioz D., 2013, IEEE T NANOTECHNOLOG
   Rathi N, 2019, IEEE T COMPUT AID D, V38, P668, DOI 10.1109/TCAD.2018.2819366
   Sengupta A, 2016, PHYS REV APPL, V6, DOI 10.1103/PhysRevApplied.6.064003
   Sengupta A, 2016, SCI REP-UK, V6, DOI 10.1038/srep30039
   Sengupta A, 2016, IEEE T ELECTRON DEV, V63, P2963, DOI 10.1109/TED.2016.2568762
   Srinivasan G., 2017, INT JOINT C NEUR NET
   Tchernev EB, 2005, NEURAL COMPUT, V17, P1646, DOI 10.1162/0899766053723096
   Vatajelu E. I., 2017, ACM IEEE INT S NAN A
   Vreeken J., 2002, S2003008 UUC I INF C
   Wijesinghe P, 2018, IEEE TETCI, V2, P345, DOI 10.1109/TETCI.2018.2829924
   Zheng N, 2018, IEEE T NANOTECHNOL, V17, P520, DOI 10.1109/TNANO.2018.2821131
NR 42
TC 13
Z9 13
U1 1
U2 3
PY 2019
WC Computer Science, Hardware & Architecture; Engineering, Electrical &
   Electronic
DA 2023-11-11
ER

PT C
AU Ibarra, OH
   Paun, A
   Rodríguez-Patón, A
AF Ibarra, Oscar H.
   Paun, Andrei
   Rodriguez-Paton, Alfonso
BE Goel, A
   Simmel, FC
   Sosik, P
TI Sequentiality Induced by Spike Number in SNP Systems
SO DNA COMPUTING
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 14th International Meeting on DNA Computing
CY JUN 02-09, 2008
CL Prague, CZECH REPUBLIC
ID NEURAL-P-SYSTEMS; MEMBRANES; PROTEINS
AB The spiking neural P systems are a class of computing devices recently introduced as a bridge between spiking neural nets and membrane computing. In this paper we consider sequential SNP systems where the sequentiality of the system is induced by a simple choice: the neuron with the maximum number of spikes out of the neurons that can spike at one step will fire. This corresponds to a global view of the whole network that makes the system sequential. We study the properties of this restriction.
C1 [Paun, Andrei] Louisiana Tech Univ, Dept Comp Sci, POB 10348, Ruston, LA 71272 USA.
   [Ibarra, Oscar H.] Univ Calif Santa Barbara, Dept Comp Sci, Santa Barbara, CA 93106 USA.
   [Paun, Andrei; Rodriguez-Paton, Alfonso] Univ Politecn Madrid, Fac Informat, Dept Inteligencia Artificial, Madrid 28660, Spain.
   [Paun, Andrei] Natl Inst Res & Dev Biolog Sci, Bioinformat Dept, Bucharest, Romania.
RP Paun, A (corresponding author), Louisiana Tech Univ, Dept Comp Sci, POB 10348, Ruston, LA 71272 USA.
EM ibarra@cs.ucsb.edu; apaun@latech.edu; arpaton@fi.upm.es
CR Gerstner W., 2002, SPIKING NEURON MODEL
   Ibarra OH, 2007, THEOR COMPUT SCI, V372, P196, DOI 10.1016/j.tcs.2006.11.025
   Ibarra OH, 2006, LECT NOTES COMPUT SC, V4135, P113
   Ionescu M, 2006, FUND INFORM, V71, P279
   MAASS W, 2002, SPECIAL ISSUE FDN IN, V8, P32
   MAASS W., 1999, PULSED NEURAL NETWOR
   MINSKY M, 1967, COMPUTATION FINITE I
   Paun A, 2006, LECT NOTES COMPUT SC, V4036, P292
   Paun A, 2006, FUND INFORM, V72, P467
   PAUN G, 2006, INFINITE SPIKE UNPUB
   Paun Gh., 2002, MEMBRANE COMPUTING I
   Paun G, 2006, INT J FOUND COMPUT S, V17, P975, DOI 10.1142/S0129054106004212
   Rozenberg G., 1997, HDB FORMAL LANGUAGES, V3
NR 13
TC 1
Z9 1
U1 0
U2 1
PY 2009
VL 5347
BP 179
EP +
WC Computer Science, Theory & Methods; Mathematical & Computational Biology
DA 2023-11-11
ER

PT C
AU Fu, Q
   Luo, YL
   Liu, JX
   Bi, JJ
   Qiu, SH
   Cao, Y
   Ding, XM
AF Fu, Qiang
   Luo, Yuling
   Liu, Junxiu
   Bi, Jinjie
   Qiu, Senhui
   Cao, Yi
   Ding, Xuemei
GP IEEE
TI Improving Learning Algorithm Performance for Spiking Neural Networks
SO 2017 17TH IEEE INTERNATIONAL CONFERENCE ON COMMUNICATION TECHNOLOGY
   (ICCT 2017)
DT Proceedings Paper
CT IEEE 17th International Conference on Communication Technology (ICCT)
CY OCT 27-30, 2017
CL Chengdu, PEOPLES R CHINA
DE spiking neural network; optimization method; learning performance
ID NEURONS
AB This paper proposes three methods to improve the learning algorithm for spiking neural networks (SNNs). The aim is to improve learning performance in SNNs where neurons are allowed to fire multiple times. The performance is analyzed based on the convergence rate, the concussion condition in the training period and the error between actual output and desired output. The exclusive-or (XOR) and Wisconsin breast cancer (WBC) classification tasks are employed to validate the proposed optimized methods. Experimental results demonstrate that compared to original learning algorithm, all three methods have less iterations, higher accuracy, and more stable in the training period.
C1 [Fu, Qiang; Luo, Yuling; Liu, Junxiu; Bi, Jinjie; Qiu, Senhui] Guangxi Normal Univ, Fac Elect Engn, Guangxi Key Lab Multisource Informat Min & Secur, Guilin 541004, Peoples R China.
   [Qiu, Senhui] Guangxi Expt Ctr Informat Sci, Guilin, Peoples R China.
   [Cao, Yi] Univ Surrey, Surrey Business Sch, Dept Business Transformat & Sustainable Enterpris, Guildford, Surrey, England.
   [Ding, Xuemei] Fujian Normal Univ, Fac Software, Fuzhou 350108, Fujian, Peoples R China.
   [Ding, Xuemei] Ulster Univ, Fac Comp & Engn, Sch Comp & Intelligent Syst, Coleraine BT48 7JL, Londonderry, North Ireland.
RP Luo, YL (corresponding author), Guangxi Normal Univ, Fac Elect Engn, Guangxi Key Lab Multisource Informat Min & Secur, Guilin 541004, Peoples R China.
EM yuling0616@mailbox.gxnu.edu.cn
CR [Anonymous], 2017, IEEE T NEURAL NETW L
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Booij O, 2005, INFORM PROCESS LETT, V95, P552, DOI 10.1016/j.ipl.2005.05.023
   JACOBS RA, 1988, NEURAL NETWORKS, V1, P295, DOI 10.1016/0893-6080(88)90003-2
   Lin XH, 2017, NEUROCOMPUTING, V237, P59, DOI 10.1016/j.neucom.2016.08.087
   Liu J., 2016, INT JOINT C NEUR NET, P1
   Luo YL, 2017, LECT NOTES COMPUT SC, V10305, P569, DOI 10.1007/978-3-319-59153-7_49
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   McKennoch S, 2006, IEEE IJCNN, P3970
   Schrauwen B, 2004, IEEE IJCNN, P471, DOI 10.1109/IJCNN.2004.1379954
   Sen S, 2017, DES AUT TEST EUROPE, P193, DOI 10.23919/DATE.2017.7926981
   WOLBERG WH, 1990, P NATL ACAD SCI USA, V87, P9193, DOI 10.1073/pnas.87.23.9193
   Xie XR, 2017, NEUROCOMPUTING, V241, P152, DOI 10.1016/j.neucom.2017.01.086
   Xin JG, 2001, IEEE IJCNN, P1772, DOI 10.1109/IJCNN.2001.938430
NR 14
TC 7
Z9 7
U1 0
U2 1
PY 2017
BP 1916
EP 1919
WC Engineering, Electrical & Electronic; Telecommunications
DA 2023-11-11
ER

PT C
AU Mishra, D
   Yadav, A
   Dwivedi, A
   Kalra, PK
AF Mishra, Deepak
   Yadav, Abhishek
   Dwivedi, Ashutosh
   Kalra, Prem K.
GP IEEE
TI A neural network using single multiplicative spiking neuron for function
   approximation and classification
SO 2006 IEEE INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORK PROCEEDINGS,
   VOLS 1-10
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT IEEE International Joint Conference on Neural Network
CY JUL 16-21, 2006
CL Vancouver, CANADA
ID INFORMAX PRINCIPLE
AB In this paper, learning algorithm for a single multiplicative spiking neuron (MSN) is proposed and tested for various applications where a multilayer perceptron (MLP) neural network is conventionally used. It is found that a single MSN is sufficient for the applications that require a number of neurons in different hidden layers of a conventional neural network. Several benchmark and real-life problems of classification and function-approximation are illustrated. It has been observed that the inclusion of few more biological phenomenon in artificial neural networks can make them more prevailing.
C1 [Mishra, Deepak; Yadav, Abhishek; Dwivedi, Ashutosh; Kalra, Prem K.] Indian Inst Technol, Dept Elect Engn, Kanpur 208016, Uttar Pradesh, India.
RP Mishra, D (corresponding author), Indian Inst Technol, Dept Elect Engn, Kanpur 208016, Uttar Pradesh, India.
EM dkmishra@iitk.ac.in
CR [Anonymous], 1988, NONLINEAR STOCHASTIC
   Bohte S, 2000, P EUR S ART NEUR NET, P419
   Chandra P, 2004, IEEE T NEURAL NETWOR, V15, P1350, DOI 10.1109/TNN.2004.831198
   FENG J, 2001, J PHYS A, V24, P1649
   Feng JF, 2003, IEEE T NEURAL NETWOR, V14, P326, DOI 10.1109/TNN.2003.809419
   Feng JF, 2002, J PHYS A-MATH GEN, V35, P2379, DOI 10.1088/0305-4470/35/10/304
   FREEMAN W, 1988, P IEEE INT C NEUR NE, V2, P1
   GERSTNER W, 1995, PHYS REV E, V51, P738, DOI 10.1103/PhysRevE.51.738
   Gerstner W., 2002, SPIKING NEURON MODEL
   Hebb D. O., 1949, ORG BEHAV
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   IANNELLA N, 1999, P 1999 IEEE SIGN PRO, P139
   Koch C., 1992, MULTIPLYING SYNAPSES, P315
   Koch Christof, 1999, P1
   Liu SC, 2004, IEEE T NEURAL NETWOR, V15, P1305, DOI 10.1109/TNN.2004.832725
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   McCulloch W. S., 1943, B MATH BIOPHYS, V5, P115, DOI [10.1007/BF02478259, DOI 10.1007/BF02478259]
   Mishra D, 2004, LECT NOTES COMPUT SC, V3316, P868
   SCHOLLES M, 1993, P 1993 INT JOINT C N, V3, P2300
   SINHA M, 2002, J IE, V83
   Widrow B., 1985, ADAPTIVE SIGNAL PROC
   Widrow B., 1960, P IRE WESCON NEW YOR, V4, P96
   Yadav A, 2005, IEEE IJCNN, P2156
   Yadav RN, 2003, INDIN 2003: IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL INFORMATICS, PROCEEDINGS, P124, DOI 10.1109/INDIN.2003.1300258
NR 25
TC 2
Z9 2
U1 0
U2 0
PY 2006
BP 396
EP +
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT C
AU Chevtchenko, SF
   Ludermir, TB
AF Chevtchenko, Sergio F.
   Ludermir, Teresa B.
GP IEEE
TI Learning from Sparse and Delayed Rewards with a Multilayer Spiking
   Neural Network
SO 2020 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN) held as part
   of the IEEE World Congress on Computational Intelligence (IEEE WCCI)
CY JUL 19-24, 2020
CL ELECTR NETWORK
DE Reinforcement learning; spiking neural networks; reward-modulated STDP
ID STDP
AB Biological brains still far exceed artificial intelligence systems, both in terms of control capabilities and power consumption. Spiking neural networks (SNNs) are a promising model, inspired by neuroscience and functionally closer to the way neurons process information. While recent advancements in neuromorphic hardware allow energy efficient synthesis of spiking networks, the training of such networks remains an open problem. In this work we focus on reinforcement learning with sparse and delayed rewards. The proposed architecture has four distinct layers and addresses the limitation of previous models in terms of scalability with input dimensions. Our SNN is evaluated on classical reinforcement learning and control tasks and is compared to two common RL algorithms: Q-learning and deep Q-network (DQN). Experiments demonstrate that the proposed network outperforms Q-learning on a task with six-dimensional observation space and compares favorably to the evaluated DQN configurations in terms of stability and memory requirements.
C1 [Chevtchenko, Sergio F.; Ludermir, Teresa B.] Univ Fed Pernambuco UFPE, Ctr Informat CIn, Recife, PE, Brazil.
RP Chevtchenko, SF (corresponding author), Univ Fed Pernambuco UFPE, Ctr Informat CIn, Recife, PE, Brazil.
EM sfc@cin.ufpe.br; tbl@cin.ufpe.br
CR Bergstra J., 2011, ADV NEURAL INFORM PR, V24, DOI 10.5555/2986459.2986743
   Bing ZS, 2019, IEEE INT CONF ROBOT, P9645, DOI [10.1109/icra.2019.8793774, 10.1109/ICRA.2019.8793774]
   Bing ZS, 2018, IEEE INT CONF ROBOT, P4725
   Bing ZS, 2018, FRONT NEUROROBOTICS, V12, DOI 10.3389/fnbot.2018.00035
   Brockman Greg, 2016, arXiv
   Cully A, 2015, NATURE, V521, P503, DOI 10.1038/nature14422
   Dhariwal Prafulla, 2017, OPENAI BASELINES
   Feldmann J, 2019, NATURE, V569, P208, DOI 10.1038/s41586-019-1157-8
   Florian RV, 2007, NEURAL COMPUT, V19, P1468, DOI 10.1162/neco.2007.19.6.1468
   Frémaux N, 2016, FRONT NEURAL CIRCUIT, V9, DOI 10.3389/fncir.2015.00085
   Frémaux N, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003024
   Hazan H, 2018, FRONT NEUROINFORM, V12, DOI 10.3389/fninf.2018.00089
   Hwangbo J, 2019, SCI ROBOT, V4, DOI 10.1126/scirobotics.aau5872
   Izhikevich EM, 2007, CEREB CORTEX, V17, P2443, DOI 10.1093/cercor/bhl152
   Mamad O, 2015, FRONT BEHAV NEUROSCI, V9, DOI 10.3389/fnbeh.2015.00166
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Mozafari M, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00625
   Nakano T, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0115620
   OKEEFE J, 1979, BEHAV BRAIN SCI, V2, P487, DOI 10.1017/S0140525X00063949
   Potjans W, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1001133
   Qiu HN, 2018, 2018 IEEE SYMPOSIUM SERIES ON COMPUTATIONAL INTELLIGENCE (IEEE SSCI), P1367, DOI 10.1109/SSCI.2018.8628848
   Raichle ME, 2002, P NATL ACAD SCI USA, V99, P10237, DOI 10.1073/pnas.172399499
   Rasmussen D, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0180234
   Rueckert E, 2016, SCI REP-UK, V6, DOI 10.1038/srep21142
   Stanley KO, 2002, EVOL COMPUT, V10, P99, DOI 10.1162/106365602320169811
   Sutton R. S., 1988, Machine Learning, V3, P9, DOI 10.1007/BF00115009
   Sutton RS, 2018, ADAPT COMPUT MACH LE, P1
   Tanneberg D, 2016, IEEE-RAS INT C HUMAN, P656, DOI 10.1109/HUMANOIDS.2016.7803344
   Tavanaei A., 2018, ARXIV180408150
   Thakur CS, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00891
   Watkins C.J.C.H., 1989, THESIS U CAMBRIDGE
   Wilson DG, 2017, FOURTEENTH EUROPEAN CONFERENCE ON ARTIFICIAL LIFE (ECAL 2017), P585
   Wunderlich T, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00260
   Yuan MW, 2019, NEURAL COMPUT, V31, P2368, DOI 10.1162/neco_a_01238
NR 34
TC 2
Z9 2
U1 1
U2 2
PY 2020
DI 10.1109/ijcnn48605.2020.9206846
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture
DA 2023-11-11
ER

PT J
AU Johnson, L
   Alekseichuk, I
   Krieg, J
   Doyle, A
   Yu, Y
   Vitek, J
   Johnson, M
   Opitz, A
AF Johnson, Luke
   Alekseichuk, Ivan
   Krieg, Jordan
   Doyle, Alex
   Yu, Ying
   Vitek, Jerrold
   Johnson, Matthew
   Opitz, Alexander
TI Dose-dependent effects of transcranial alternating current stimulation
   on spike timing in awake nonhuman primates
SO SCIENCE ADVANCES
DT Article
ID ELECTRIC-FIELDS; OSCILLATIONS; NETWORK; ALPHA; EXCITABILITY; PLACEMENT;
   NEURONS; SIGNALS; SAFETY; EEG
AB Weak extracellular electric fields can influence spike timing in neural networks. Approaches to noninvasively impose these fields on the brain have high therapeutic potential in neurology and psychiatry. Transcranial alternating current stimulation (TACS) is hypothesized to affect spike timing and cause neural entrainment. However, the conditions under which these effects occur in vivo are unknown. Here, we recorded single-unit activity in the neocortex in awake nonhuman primates during TACS and found dose-dependent neural entrainment to the stimulation waveform. Cluster analysis of changes in interspike intervals identified two main types of neural responses to TACS-increased burstiness and phase entrainment. Our results uncover key mechanisms of TACS and show that the stimulation affects spike timing in the awake primate brain at intensities feasible in humans. Thus, novel TACS protocols tailored to ongoing brain activity may be a tool to normalize spike timing in maladaptive brain networks and neurological disease.
C1 [Johnson, Luke; Yu, Ying; Vitek, Jerrold; Opitz, Alexander] Univ Minnesota, Dept Neurol, Minneapolis, MN 55455 USA.
   [Alekseichuk, Ivan; Krieg, Jordan; Johnson, Matthew] Univ Minnesota, Dept Biomed Engn, Minneapolis, MN 55455 USA.
   [Doyle, Alex] Univ Minnesota, Dept Neurosci, Minneapolis, MN 55455 USA.
RP Opitz, A (corresponding author), Univ Minnesota, Dept Neurol, Minneapolis, MN 55455 USA.
EM aopitz@umn.edu
CR Alekseichuk I, 2020, BRAIN STIMUL, V13, P474, DOI 10.1016/j.brs.2019.12.019
   Alekseichuk I, 2019, NAT COMMUN, V10, DOI 10.1038/s41467-019-10581-7
   Alekseichuk I, 2019, NEUROIMAGE, V194, P136, DOI 10.1016/j.neuroimage.2019.03.044
   Anastassiou CA, 2015, CURR OPIN NEUROBIOL, V31, P95, DOI 10.1016/j.conb.2014.09.002
   Anastassiou CA, 2011, NAT NEUROSCI, V14, P217, DOI 10.1038/nn.2727
   Antal A, 2017, CLIN NEUROPHYSIOL, V128, P1774, DOI 10.1016/j.clinph.2017.06.001
   Asamoah B, 2019, NAT COMMUN, V10, DOI 10.1038/s41467-018-08183-w
   Aspart F, 2018, PLOS COMPUT BIOL, V14, DOI 10.1371/journal.pcbi.1006124
   Beaulieu-Laroche L, 2018, CELL, V175, P643, DOI 10.1016/j.cell.2018.08.045
   Benchenane K, 2010, NEURON, V66, P921, DOI 10.1016/j.neuron.2010.05.013
   Berens P, 2009, J STAT SOFTW, V31, P1, DOI 10.18637/jss.v031.i10
   Bikson M, 2004, J PHYSIOL-LONDON, V557, P175, DOI 10.1113/jphysiol.2003.055772
   BINDMAN LJ, 1962, NATURE, V196, P584, DOI 10.1038/196584a0
   Bingmer M, 2011, J NEUROSCI METH, V201, P426, DOI 10.1016/j.jneumeth.2011.08.013
   Borst A, 1999, NAT NEUROSCI, V2, P947, DOI 10.1038/14731
   Buzsáki G, 2012, NAT REV NEUROSCI, V13, P407, DOI 10.1038/nrn3241
   Deister CA, 2009, J NEUROSCI, V29, P15888, DOI 10.1523/JNEUROSCI.4053-09.2009
   Francis JT, 2003, J NEUROSCI, V23, P7255
   Fröhlich F, 2010, NEURON, V67, P129, DOI 10.1016/j.neuron.2010.06.005
   Haegens S, 2011, P NATL ACAD SCI USA, V108, P19377, DOI 10.1073/pnas.1117190108
   Helfrich RF, 2014, PLOS BIOL, V12, DOI 10.1371/journal.pbio.1002031
   Herrmann CS, 2013, FRONT HUM NEUROSCI, V7, DOI 10.3389/fnhum.2013.00279
   Huang Y, 2017, ELIFE, V6, DOI 10.7554/eLife.18834
   JEFFERYS JGR, 1981, J PHYSIOL-LONDON, V319, P143, DOI 10.1113/jphysiol.1981.sp013897
   Kasten FH, 2016, FRONT HUM NEUROSCI, V10, DOI [10.3389/fnhum.2016.00245, 10.3389/fnhum.2016.00135]
   Khatoun A, 2019, P NATL ACAD SCI USA, V116, P22438, DOI 10.1073/pnas.1912927116
   Krause MR, 2019, P NATL ACAD SCI USA, V116, P22440, DOI 10.1073/pnas.1914483116
   Krause MR, 2019, P NATL ACAD SCI USA, V116, P5747, DOI 10.1073/pnas.1815958116
   Lafon B, 2017, NAT COMMUN, V8, DOI 10.1038/s41467-017-01045-x
   Liu AL, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-07233-7
   Miocinovic S, 2007, ACTA NEUROCHIR SUPPL, V97, P561
   Neuling T, 2017, NEUROIMAGE, V147, P960, DOI 10.1016/j.neuroimage.2016.11.022
   Newman MEJ, 2006, P NATL ACAD SCI USA, V103, P8577, DOI 10.1073/pnas.0601602103
   Nitsche MA, 2017, BRAIN STIMUL, V10, P541, DOI 10.1016/j.brs.2017.03.002
   Noury N, 2018, NEUROIMAGE, V167, P53, DOI 10.1016/j.neuroimage.2017.11.023
   Noury N, 2017, NEUROIMAGE, V158, P406, DOI 10.1016/j.neuroimage.2017.07.010
   Noury N, 2016, NEUROIMAGE, V140, P99, DOI 10.1016/j.neuroimage.2016.03.065
   Opitz A, 2018, NEUROIMAGE, V181, P560, DOI 10.1016/j.neuroimage.2018.07.027
   Opitz A, 2017, P NATL ACAD SCI USA, V114, P5243, DOI 10.1073/pnas.1617024114
   Opitz A, 2016, SCI REP-UK, V6, DOI 10.1038/srep31236
   Opitz A, 2015, NEUROIMAGE, V109, P140, DOI 10.1016/j.neuroimage.2015.01.033
   Ozen S, 2010, J NEUROSCI, V30, P11476, DOI 10.1523/JNEUROSCI.5252-09.2010
   Peterchev AV, 2012, BRAIN STIMUL, V5, P435, DOI 10.1016/j.brs.2011.10.001
   Polania R, 2018, NAT NEUROSCI, V21, P174, DOI 10.1038/s41593-017-0054-4
   Radman T, 2007, J NEUROSCI, V27, P3030, DOI 10.1523/JNEUROSCI.0095-07.2007
   Radman T, 2009, BRAIN STIMUL, V2, P215, DOI 10.1016/j.brs.2009.03.007
   Rahman A, 2013, J PHYSIOL-LONDON, V591, P2563, DOI 10.1113/jphysiol.2012.247171
   Ramanathan DS, 2018, NAT MED, V24, P1257, DOI 10.1038/s41591-018-0058-y
   Reato D, 2010, J NEUROSCI, V30, P15067, DOI 10.1523/JNEUROSCI.2059-10.2010
   Rubinov M, 2010, NEUROIMAGE, V52, P1059, DOI 10.1016/j.neuroimage.2009.10.003
   Schwab B., 2019, BRAIN STIMUL, V12, P423
   Singer W, 2018, EUR J NEUROSCI, V48, P2389, DOI 10.1111/ejn.13796
   TERZUOLO CA, 1956, P NATL ACAD SCI USA, V42, P687, DOI 10.1073/pnas.42.9.687
   Uhlhaas PJ, 2010, NAT REV NEUROSCI, V11, P100, DOI 10.1038/nrn2774
   Vieira P, 2019, BIORXIV, DOI [10.1101/691022, DOI 10.1101/691022]
   Vinck M, 2010, NEUROIMAGE, V51, P112, DOI 10.1016/j.neuroimage.2010.01.073
   Vöröslakos M, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-02928-3
   Vossen A, 2015, BRAIN STIMUL, V8, P499, DOI 10.1016/j.brs.2014.12.004
   Wöstmann M, 2018, BRAIN STIMUL, V11, P752, DOI 10.1016/j.brs.2018.04.006
   Zaehle T, 2010, PLOS ONE, V5, DOI 10.1371/journal.pone.0013766
NR 60
TC 70
Z9 71
U1 1
U2 11
PD SEP
PY 2020
VL 6
IS 36
AR eaaz2747
DI 10.1126/sciadv.aaz2747
WC Multidisciplinary Sciences
DA 2023-11-11
ER

PT J
AU Kakani, V
   Li, XY
   Cui, XA
   Kim, H
   Kim, BS
   Kim, H
AF Kakani, Vijay
   Li, Xingyou
   Cui, Xuenan
   Kim, Heetak
   Kim, Byung-Soo
   Kim, Hakil
TI Implementation of Field-Programmable Gate Array Platform for Object
   Classification Tasks Using Spike-Based Backpropagated Deep Convolutional
   Spiking Neural Networks
SO MICROMACHINES
DT Article
DE field-programmable gate arrays; neuromorphic image processing; object
   classification performance; spiking neural networks
ID RECOGNITION; PROCESSOR; MODEL
AB This paper investigates the performance of deep convolutional spiking neural networks (DCSNNs) trained using spike-based backpropagation techniques. Specifically, the study examined temporal spike sequence learning via backpropagation (TSSL-BP) and surrogate gradient descent via backpropagation (SGD-BP) as effective techniques for training DCSNNs on the field programmable gate array (FPGA) platform for object classification tasks. The primary objective of this experimental study was twofold: (i) to determine the most effective backpropagation technique, TSSL-BP or SGD-BP, for deeper spiking neural networks (SNNs) with convolution filters across various datasets; and (ii) to assess the feasibility of deploying DCSNNs trained using backpropagation techniques on low-power FPGA for inference, considering potential configuration adjustments and power requirements. The aforementioned objectives will assist in informing researchers and companies in this field regarding the limitations and unique perspectives of deploying DCSNNs on low-power FPGA devices. The study contributions have three main aspects: (i) the design of a low-power FPGA board featuring a deployable DCSNN chip suitable for object classification tasks; (ii) the inference of TSSL-BP and SGD-BP models with novel network architectures on the FPGA board for object classification tasks; and (iii) a comparative evaluation of the selected spike-based backpropagation techniques and the object classification performance of DCSNNs across multiple metrics using both public (MNIST, CIFAR10, KITTI) and private (INHA_ADAS, INHA_KLP) datasets.
C1 [Kakani, Vijay] Inha Univ, Integrated Syst Engn, 100 Inharo, Incheon 22212, South Korea.
   [Li, Xingyou; Kim, Hakil] Inha Univ, Elect & Comp Engn, 100 Inharo, Incheon 22212, South Korea.
   [Cui, Xuenan] Inha Univ, Informat & Commun Engn, 100 Inharo, Incheon 22212, South Korea.
   [Kim, Heetak; Kim, Byung-Soo] Korea Elect Technol Inst, Res & Dev, 25 KETI, Seongnam Si 13509, South Korea.
RP Kim, H (corresponding author), Inha Univ, Elect & Comp Engn, 100 Inharo, Incheon 22212, South Korea.
EM vjkakani@inha.ac.kr; 22202326@inha.edu; xncui@inha.ac.kr;
   htkim@keti.re.kr; bskim4k@keti.re.kr; hikim@inha.ac.kr
CR Abbott LF, 2008, NEURON, V60, P489, DOI 10.1016/j.neuron.2008.10.019
   Abdelsalam AM, 2018, PROC INT CONF RECON
   Abdigapporov S, 2023, IEEE ACCESS, V11, P37637, DOI 10.1109/ACCESS.2023.3266284
   Abdigapporov S, 2022, INT C CONTR AUTOMAT, P819, DOI 10.23919/ICCAS55662.2022.10003816
   Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   Aung MTL, 2021, I C FIELD PROG LOGIC, P28, DOI 10.1109/FPL53798.2021.00013
   Bellec G., 2018, ADV NEURAL INFORM PR, P787
   Bing ZS, 2019, FRONT NEUROROBOTICS, V13, DOI 10.3389/fnbot.2019.00018
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Brette R, 2005, J NEUROPHYSIOL, V94, P3637, DOI 10.1152/jn.00686.2005
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Cardenas A, 2012, IEEE T POWER SYST, V27, P1343, DOI 10.1109/TPWRS.2012.2186468
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Diehl Peter U, 2015, 2015 INT JOINT C NEU, P1, DOI [DOI 10.1109/IJCNN.2015.7280696, 10.1109/IJCNN.2015.7280696]
   Fang HW, 2020, ICCAD-IEEE ACM INT, DOI 10.1145/3400302.3415608
   Fotis G, 2022, ELECTRONICS-SWITZ, V11, DOI 10.3390/electronics11121858
   Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074
   Gerstner W., 2002, SPIKING NEURON MODEL, DOI DOI 10.1017/CBO9780511815706
   Ghimire A, 2023, IEEE ACCESS, V11, P51930, DOI 10.1109/ACCESS.2023.3278974
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V116, P449, DOI 10.1113/jphysiol.1952.sp004717
   Huang XH, 2021, PROF DEV EDUC, DOI 10.1080/19415257.2021.1942143
   Huh D, 2018, ADV NEUR IN, V31
   Indiveri Giacomo, 2011, Front Neurosci, V5, P118, DOI 10.3389/fnins.2011.00118
   Irmak H, 2021, J LOW POWER ELECT AP, V11, DOI 10.3390/jlpea11030032
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Jin Yingyezhe, 2018, P 32 INT C NEUR INF
   Juraev S, 2022, IEEE ACCESS, V10, P94249, DOI 10.1109/ACCESS.2022.3203174
   Kakani V, 2022, 2022 IEEE REG 10 S T, P1
   Kakani V, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21051920
   Kakani V, 2020, J AGR FOOD RES, V2, DOI 10.1016/j.jafr.2020.100033
   Kakani V, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20030894
   Kakani V, 2019, SENSORS-BASEL, V19, DOI 10.3390/s19153369
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Khodamoradi Alireza, 2021, FPGA '21: The 2021 ACM/SIGDA International Symposium on Field-Programmable, P194, DOI 10.1145/3431920.3439283
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
   LeCun Y, 1999, LECT NOTES COMPUT SC, V1681, P319, DOI 10.1007/3-540-46805-6_19
   Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508
   Lent R, 2020, INT CONF WIREL SPAC, P106, DOI 10.1109/WiSEE44079.2020.9262700
   Li SX, 2021, IEEE T CIRCUITS-I, V68, P1543, DOI 10.1109/TCSI.2021.3052885
   Lu H, 2021, NEUROCOMPUTING, V458, P308, DOI 10.1016/j.neucom.2021.06.027
   Ma CL, 2023, INT MATER REV, V68, P82, DOI 10.1080/09506608.2022.2047420
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Miraliev S, 2023, IEEE T INTELL VEHICL
   Mostafa H, 2018, IEEE T NEUR NET LEAR, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Mozafari M, 2019, PATTERN RECOGN, V94, P87, DOI 10.1016/j.patcog.2019.05.015
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Pagoli A, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22114083
   Panchapakesan S, 2020, ANN IEEE SYM FIELD P, P242, DOI 10.1109/FCCM48280.2020.00075
   Pavlatos C, 2023, TECHNOLOGIES, V11, DOI 10.3390/technologies11030070
   Pham Q.T., 2021, 2021 INT C MULTIMEDI, P1
   Ponulak F, 2011, ACTA NEUROBIOL EXP, V71, P409
   Rahman MA, 2023, MICROMACHINES-BASEL, V14, DOI 10.3390/mi14030508
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Shrestha SB, 2018, ADV NEUR IN, V31
   Sommer J, 2022, Arxiv, DOI [arXiv:2203.12437, 10.1109/TCAD.2022.3197512, DOI 10.1109/TCAD.2022.3197512]
   Syed T, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21093240
   Tehreem S., 2021, 2021 IEEE REGION 10, P1
   Tsai HF, 2023, MICROMACHINES-BASEL, V14, DOI 10.3390/mi14040826
   Wang SQ, 2020, J COMPUT SCI TECH-CH, V35, P475, DOI 10.1007/s11390-020-9686-z
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Zenke F, 2018, NEURAL COMPUT, V30, P1514, DOI 10.1162/neco_a_01086
   Zhang W, 2020, ADV NEURAL INFORM PR, V33, P12022, DOI DOI 10.48550/ARXIV.2002.10085
   Zhang WR, 2019, ADV NEUR IN, V32
NR 63
TC 0
Z9 0
U1 2
U2 2
PD JUL
PY 2023
VL 14
IS 7
AR 1353
DI 10.3390/mi14071353
WC Chemistry, Analytical; Nanoscience & Nanotechnology; Instruments &
   Instrumentation; Physics, Applied
DA 2023-11-11
ER

PT C
AU Slepova, LO
   Berkhova, EM
AF Slepova, Liudmila O.
   Berkhova, Evdokiia M.
GP IEEE
TI Development of a System Providing the Interaction of the Transducer with
   the Nervous System for Sensory Prosthetics
SO PROCEEDINGS OF THE 2019 IEEE CONFERENCE OF RUSSIAN YOUNG RESEARCHERS IN
   ELECTRICAL AND ELECTRONIC ENGINEERING (EICONRUS)
SE IEEE NW Russia Young Researchers in Electrical and Electronic
   Engineering Conference
DT Proceedings Paper
CT IEEE Conference of Russian Young Researchers in Electrical and
   Electronic Engineering (EIConRus)
CY JAN 28-31, 2019
CL St Petersburg Electrotechn Univ, RUSSIA
HO St Petersburg Electrotechn Univ
DE nervous system; sensory prosthetics; neural coding; spiking; LIF-model
AB The problem of electronic sensor interaction with the nervous system of a living organism in sensory prosthetics devices is described. The processing, storing and transmission of information by a spiking neuron and a spiking neural network are analyzed. The basic principles of functioning of a neural cell, which uses an electric pulse for coding and transmission of information, are investigated. The possibility of developing an encoder for transmitting information from the sensor to the biological neuron of spiking neural networks is considered. The existing methods and means of sensory prosthetics and problems of hardware development for invasive and non-invasive systems are described.
C1 [Slepova, Liudmila O.] ITMO Univ, Fac Control Syst & Robot, St Petersburg, Russia.
   [Berkhova, Evdokiia M.] St Petersburg State Marine Tech Univ, Dept Marine Elect, St Petersburg, Russia.
RP Berkhova, EM (corresponding author), St Petersburg State Marine Tech Univ, Dept Marine Elect, St Petersburg, Russia.
EM marine_electronics@corp.smtu.ru
CR Dang B, 2018, AIP CONF PROC, V2034, DOI 10.1063/1.5067350
   Ivanov AV, 2018, PROCEEDINGS OF THE 2018 IEEE CONFERENCE OF RUSSIAN YOUNG RESEARCHERS IN ELECTRICAL AND ELECTRONIC ENGINEERING (EICONRUS), P890, DOI 10.1109/EIConRus.2018.8317231
   Kotlyarevskaya MV, 2018, PROCEEDINGS OF THE 2018 IEEE CONFERENCE OF RUSSIAN YOUNG RESEARCHERS IN ELECTRICAL AND ELECTRONIC ENGINEERING (EICONRUS), P894, DOI 10.1109/EIConRus.2018.8317232
   Popov AV, 2018, PROCEEDINGS OF THE 2018 IEEE CONFERENCE OF RUSSIAN YOUNG RESEARCHERS IN ELECTRICAL AND ELECTRONIC ENGINEERING (EICONRUS), P962, DOI 10.1109/EIConRus.2018.8317249
   Slepova LO, 2018, PROCEEDINGS OF THE 2018 IEEE CONFERENCE OF RUSSIAN YOUNG RESEARCHERS IN ELECTRICAL AND ELECTRONIC ENGINEERING (EICONRUS), P992, DOI 10.1109/EIConRus.2018.8317256
   Zhilenkov Anton, 2016, Vibroengineering Procedia. 22nd International Conference on Vibroengineering, P17
   Zhilenkov AA, 2018, PROCEEDINGS OF THE 2018 IEEE CONFERENCE OF RUSSIAN YOUNG RESEARCHERS IN ELECTRICAL AND ELECTRONIC ENGINEERING (EICONRUS), P400, DOI 10.1109/EIConRus.2018.8317117
   Zhilenkov AA, 2017, IEEE NW RUSS YOUNG, P1100, DOI 10.1109/EIConRus.2017.7910747
NR 8
TC 0
Z9 0
U1 0
U2 1
PY 2019
BP 1233
EP 1236
DI 10.1109/eiconrus.2019.8657210
WC Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Sun, HQ
   Yang, Y
   Sourina, O
   Huang, GB
AF Sun, Haoqi
   Yang, Yan
   Sourina, Olga
   Huang, Guang-Bin
GP IEEE
TI Runtime Detection of Activated Polychronous Neuronal Group towards its
   Spatiotemporal Analysis
SO 2015 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUL 12-17, 2015
CL Killarney, IRELAND
ID SYNAPTIC PLASTICITY; SPIKING NEURONS; NETWORKS; POLYCHRONIZATION; MODELS
AB Due to the precise spike timing in neural coding, spiking neural network (SNN) possesses richer spatiotemporal dynamics compared to neural networks with firing rate coding. One of the distinct features of SNN, polychronous neuronal group (PNG), receives much attention from both computational neuroscience and machine learning communities. However, all existing algorithms detect PNGs from the spike recording collected after simulation in an offline manner. There is currently no algorithm that detects PNGs actually being activated in runtime (online manner), which could be potentially used as inputs to higher level neural processing. We propose a runtime detection algorithm particularly for activated PNGs, using PNG readout neurons, to fill this gap. The proposed algorithm can reveal the spatiotemporal PNG patterns embedded in spike trains, which is higher level neuronal dynamics. We demonstrate through an example that for composed input patterns, new PNGs except the constituent PNGs can be easily found using the proposed algorithm. As an important interpretation, we give further insights on how to use PNG readout neurons to construct layered network structure.
C1 [Sun, Haoqi; Yang, Yan] Nanyang Technol Univ, Interdisciplinary Grad Sch, Energy Res Inst NTU ERI N, Singapore 639798, Singapore.
   [Sun, Haoqi; Huang, Guang-Bin] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
   [Sun, Haoqi; Sourina, Olga] Nanyang Technol Univ, Fraunhofer IDM NTU, Singapore 639798, Singapore.
RP Sun, HQ (corresponding author), Nanyang Technol Univ, Interdisciplinary Grad Sch, Energy Res Inst NTU ERI N, Singapore 639798, Singapore.
CR Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Brette R, 2007, J COMPUT NEUROSCI, V23, P349, DOI 10.1007/s10827-007-0038-6
   Chrol-Cannon J., 2012, NEUR NETW IJCNN 2012, P1
   Clair W. B. S., 2015, COGNITIVE PROCESSING, P1
   Daily M. J., 2014, U.S. Patent, Patent No. [8 756 183, 8756183]
   Gerstner W., 2002, SPIKING NEURON MODEL
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Guise M., 2014, NEURAL COMPUT, P1
   Guise M., 2013, AI 2013 ADV ARTIFICI, P86
   Howard M., 2010, IC AI, P325
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Kistler WM, 2000, NEURAL COMPUT, V12, P385, DOI 10.1162/089976600300015844
   Legenstein R, 2005, NEURAL COMPUT, V17, P2337, DOI 10.1162/0899766054796888
   Lukosevicius M, 2009, COMPUT SCI REV, V3, P127, DOI 10.1016/j.cosrev.2009.03.005
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maier W. L., 2008, ARXIV08061070
   Markram H., 2012, FRONTIERS SYNAPTIC N, V4
   Martinez R, 2009, LECT NOTES COMPUT SC, V5769, P75, DOI 10.1007/978-3-642-04277-5_8
   Masquelier T, 2007, PLOS COMPUT BIOL, V3, P247, DOI 10.1371/journal.pcbi.0030031
   Morrison A, 2008, BIOL CYBERN, V98, P459, DOI 10.1007/s00422-008-0233-1
   Paugam-Moisy H, 2008, NEUROCOMPUTING, V71, P1143, DOI 10.1016/j.neucom.2007.12.027
   Poggio T., 2013, SCHOLARPEDIA, V8, P3516, DOI [DOI 10.4249/SCHOLARPEDIA.3516, 10.4249/scholarpedia.3516]
   Sjostrom J., 2011, FRONTIERS SYNAPTIC N, P35
   Szatmáry B, 2010, PLOS COMPUT BIOL, V6, DOI 10.1371/journal.pcbi.1000879
NR 24
TC 1
Z9 1
U1 0
U2 1
PY 2015
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT S
AU Chen, YJ
   Hall, S
   McDaid, L
   Buiu, O
   Kelly, P
AF Chen, Yajie
   Hall, Steve
   McDaid, Liam
   Buiu, Octavian
   Kelly, Peter
BE Wang, J
   Yi, Z
   Zurada, JM
   Lu, BL
   Yin, H
TI A silicon synapse based on a charge transfer device for spiking neural
   network application
SO ADVANCES IN NEURAL NETWORKS - ISNN 2006, PT 3, PROCEEDINGS
SE LECTURE NOTES IN COMPUTER SCIENCE
DT Article; Proceedings Paper
CT 3rd International Symposium on Neural Networks (ISNN 2006)
CY MAY 28-31, 2006
CL Chengdu, PEOPLES R CHINA
ID COUPLED DEVICES; NEURONS
AB We propose a silicon synapse for spiking neural network application. In this endeavor, two major issues are addressed: the structure of the synapse and the associated behavior. This synaptic structure is basically a charge transfer device comprising of two Metal-Oxide- Semiconductor (MOS) capacitors the first of which stores the weight and the second controls its reading. In this work, simulation results prove that the proposed synapse captures the intrinsic dynamics of the biological synapse and exhibits a spike characteristic. The device operates at very low power and offers the potential for scaling to massively parallel third generation hardware neural networks.
C1 Univ Liverpool, Dept Elect Engn & Elect, Liverpool L69 3GJ, Merseyside, England.
   Univ Ulster, Sch Comp & Intelligent Syst, Londonderry BT48 7JL, North Ireland.
RP Chen, YJ (corresponding author), Univ Liverpool, Dept Elect Engn & Elect, Liverpool L69 3GJ, Merseyside, England.
CR Bofill-i-Petit A, 2004, IEEE T NEURAL NETWOR, V15, P1296, DOI 10.1109/TNN.2004.832842
   CARNES JE, 1971, IEEE J SOLID-ST CIRC, VSC 6, P322, DOI 10.1109/JSSC.1971.1050194
   CARNES JE, 1972, IEEE T ELECTRON DEV, VED19, P798, DOI 10.1109/T-ED.1972.17497
   Chicca E, 2003, IEEE T NEURAL NETWOR, V14, P1297, DOI 10.1109/TNN.2003.816367
   Diorio C, 1996, IEEE T ELECTRON DEV, V43, P1972, DOI 10.1109/16.543035
   Diorio C, 2002, P IEEE, V90, P345, DOI 10.1109/5.993402
   Gerstner W., 2002, SPIKING NEURON MODEL
   Heemskerk J.N.H., 1995, THESIS LEIDEN U NETH
   MAASS W, 1996, TR96031 TU GRAZ I TH
   MAASS W., 1999, PULSED NEURAL NETWOR
   STRAIN RJ, 1971, AT&T TECH J, V50, P1721, DOI 10.1002/j.1538-7305.1971.tb02579.x
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
NR 12
TC 0
Z9 0
U1 0
U2 2
PY 2006
VL 3973
BP 1366
EP 1373
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods
DA 2023-11-11
ER

PT J
AU Rostro-Gonzalez, H
   Cessac, B
   Vieville, T
AF Rostro-Gonzalez, H.
   Cessac, B.
   Vieville, T.
TI Parameter estimation in spiking neural networks: a reverse-engineering
   approach
SO JOURNAL OF NEURAL ENGINEERING
DT Article
ID TIMING-DEPENDENT PLASTICITY; NEURONS; TIME; COMPUTATION; PRECISION;
   DYNAMICS; SYSTEMS; CORTEX; MODEL
AB This paper presents a reverse engineering approach for parameter estimation in spiking neural networks (SNNs). We consider the deterministic evolution of a time-discretized network with spiking neurons, where synaptic transmission has delays, modeled as a neural network of the generalized integrate and fire type. Our approach aims at by-passing the fact that the parameter estimation in SNN results in a non-deterministic polynomial-time hard problem when delays are to be considered. Here, this assumption has been reformulated as a linear programming (LP) problem in order to perform the solution in a polynomial time. Besides, the LP problem formulation makes the fact that the reverse engineering of a neural network can be performed from the observation of the spike times explicit. Furthermore, we point out how the LP adjustment mechanism is local to each neuron and has the same structure as a 'Hebbian' rule. Finally, we present a generalization of this approach to the design of input-output (I/O) transformations as a practical method to 'program' a spiking network, i.e. find a set of parameters allowing us to exactly reproduce the network output, given an input. Numerical verifications and illustrations are provided.
C1 [Rostro-Gonzalez, H.] Univ Cyprus, KIOS Res Ctr, CY-1678 Nicosia, Cyprus.
   [Rostro-Gonzalez, H.] Univ Cyprus, Holist Elect Res Lab, Dept Elect & Comp Engn, CY-1678 Nicosia, Cyprus.
   [Rostro-Gonzalez, H.] Univ Guanajuato, Div Ingn Campus Irapuato Salamanca, Salamanca Gto 36885, Mexico.
   [Rostro-Gonzalez, H.; Cessac, B.] UNSA LJAD, NeuroMathComp Project Team, INRIA, ENS Paris, F-06902 Sophia Antipolis, France.
   [Vieville, T.] INRIA Cortex Project Team, F-06902 Sophia Antipolis, France.
RP Rostro-Gonzalez, H (corresponding author), Univ Cyprus, KIOS Res Ctr, 75 Kallipoleos Ave,POB 20537, CY-1678 Nicosia, Cyprus.
EM hrosgonz@ucy.ac.cy
CR Albers DJ, 2006, PHYSICA D, V223, P194, DOI 10.1016/j.physd.2006.09.004
   Albers DJ, 2006, NONLINEARITY, V19, P1801, DOI 10.1088/0951-7715/19/8/005
   Baudot P, 2007, THESIS U PARIS
   Bixby R. E., 1992, ORSA Journal on Computing, V4, P267, DOI 10.1287/ijoc.4.3.267
   Bohte SM, 2007, NEURAL COMPUT, V19, P371, DOI 10.1162/neco.2007.19.2.371
   Cessac B, 2008, J MATH BIOL, V56, P311, DOI 10.1007/s00285-007-0117-3
   Cessac B, 2008, NEUR 2008, V2008
   Cessac B, 2008, 6924 INRIA
   Cessac B, 2008, P NEUROCOMP, P302
   Cessac B, 2008, FRONT COMPUT NEUROSC, V2, DOI 10.3389/neuro.10.002.2008
   Chazottes JR, 1998, J STAT PHYS, V90, P697, DOI 10.1023/A:1023220802597
   Chechik G, 2003, NEURAL COMPUT, V15, P1481, DOI 10.1162/089976603321891774
   Cooper L., 2004, THEORY CORTICAL PLAS
   Darst R. B., 1990, INTRO LINEAR PROGRAM
   Delorme A, 2001, NEUROCOMPUTING, V38, P539, DOI 10.1016/S0925-2312(01)00403-9
   Destexhe A, 1997, NEURAL COMPUT, V9, P503, DOI 10.1162/neco.1997.9.3.503
   Gantmatcher FR, 1977, MATRIX THEORY
   Gerstner W, 2002, BIOL CYBERN, V87, P404, DOI 10.1007/s00422-002-0353-y
   Grammont F, 2003, BIOL CYBERN, V88, P360, DOI 10.1007/s00422-002-0385-3
   Grün S, 1999, J NEUROSCI METH, V94, P67, DOI 10.1016/S0165-0270(99)00126-0
   Guyonneau R, 2005, NEURAL COMPUT, V17, P859, DOI 10.1162/0899766053429390
   HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8
   Jaeger H., 2003, ADV NEURAL INFORM PR, P609
   Leporati A, 2007, WMC8 MEMBR COMP INT
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Maass W, 1997, NETWORK-COMP NEURAL, V8, P355, DOI 10.1088/0954-898X/8/4/002
   Maass W, 1997, NEURAL COMPUT, V9, P279, DOI 10.1162/neco.1997.9.2.279
   Maass W, 2001, THEOR COMPUT SCI, V261, P157, DOI 10.1016/S0304-3975(00)00137-7
   Maass W, 2003, PULSED NEURAL NETWOR
   Nemenman I, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.1000025
   Orchard G, 2008, IEEE INT SYMP CIRC S, P1048, DOI 10.1109/ISCAS.2008.4541601
   Paugam-Moisy H, 2008, NEUROCOMPUTING, V71, P1143, DOI 10.1016/j.neucom.2007.12.027
   Paun G, 2006, INT J FOUND COMPUT S, V17, P975, DOI 10.1142/S0129054106004212
   Riehle A, 2000, J PHYSIOL-PARIS, V94, P569, DOI 10.1016/S0928-4257(00)01100-1
   Rudolph M, 2006, NEURAL COMPUT, V18, P2146, DOI 10.1162/neco.2006.18.9.2146
   Schäfer AM, 2006, LECT NOTES COMPUT SC, V4131, P632
   Schrauwen B, 2007, THESIS U GENT BELGIU
   Síma J, 2005, NEURAL COMPUT, V17, P2635, DOI 10.1162/089976605774320601
   Soula H, 2007, NEURAL COMPUT, V19, P3262, DOI 10.1162/neco.2007.19.12.3262
   Sutton R. S., 2015, REINFORCEMENT LEARNI, V2nd
   Toyoizumi T, 2005, P NATL ACAD SCI USA, V102, P5239, DOI 10.1073/pnas.0500495102
   Toyoizumi T, 2007, NEURAL COMPUT, V19, P639, DOI 10.1162/neco.2007.19.3.639
   Van Rullen R, 2001, NEURAL COMPUT, V13, P1255, DOI 10.1162/08997660152002852
   Verstraeten D, 2007, NEURAL NETWORKS, V20, P391, DOI 10.1016/j.neunet.2007.04.003
   Victor JD, 1996, J NEUROPHYSIOL, V76, P1310, DOI 10.1152/jn.1996.76.2.1310
   Wang J, 2010, NEURAL COMPUT, V22, P2615, DOI 10.1162/NECO_a_00022
NR 46
TC 6
Z9 7
U1 0
U2 10
PD APR
PY 2012
VL 9
IS 2
AR 026024
DI 10.1088/1741-2560/9/2/026024
WC Engineering, Biomedical; Neurosciences
DA 2023-11-11
ER

PT J
AU Zhang, YS
   Lv, HY
   Zhao, YC
   Feng, Y
   Liu, HL
   Bi, GL
AF Zhang, Yisa
   Lv, Hengyi
   Zhao, Yuchen
   Feng, Yang
   Liu, Hailong
   Bi, Guoling
TI Event-Based Optical Flow Estimation with Spatio-Temporal Backpropagation
   Trained Spiking Neural Network
SO MICROMACHINES
DT Article
DE event camera; optical flow estimation; spiking neural network;
   spatio-temporal backpropagation
ID SENSOR
AB The advantages of an event camera, such as low power consumption, large dynamic range, and low data redundancy, enable it to shine in extreme environments where traditional image sensors are not competent, especially in high-speed moving target capture and extreme lighting conditions. Optical flow reflects the target's movement information, and the target's detailed movement can be obtained using the event camera's optical flow information. However, the existing neural network methods for optical flow prediction of event cameras has the problems of extensive computation and high energy consumption in hardware implementation. The spike neural network has spatiotemporal coding characteristics, so it can be compatible with the spatiotemporal data of an event camera. Moreover, the sparse coding characteristic of the spike neural network makes it run with ultra-low power consumption on neuromorphic hardware. However, because of the algorithmic and training complexity, the spike neural network has not been applied in the prediction of the optical flow for the event camera. For this case, this paper proposes an end-to-end spike neural network to predict the optical flow of the discrete spatiotemporal data stream for the event camera. The network is trained with the spatio-temporal backpropagation method in a self-supervised way, which fully combines the spatiotemporal characteristics of the event camera while improving the network performance. Compared with the existing methods on the public dataset, the experimental results show that the method proposed in this paper is equivalent to the best existing methods in terms of optical flow prediction accuracy, and it can save 99% more power consumption than the existing algorithm, which is greatly beneficial to the hardware implementation of the event camera optical flow prediction., laying the groundwork for future low-power hardware implementation of optical flow prediction for event cameras.
C1 [Zhang, Yisa; Lv, Hengyi; Zhao, Yuchen; Feng, Yang; Liu, Hailong; Bi, Guoling] Chinese Acad Sci, Changchun Inst Opt Fine Mech & Phys, Changchun 130033, Peoples R China.
   [Zhang, Yisa] Univ Chinese Acad Sci, Coll Mat Sci & Optoelect Technol, Beijing 100049, Peoples R China.
RP Lv, HY (corresponding author), Chinese Acad Sci, Changchun Inst Opt Fine Mech & Phys, Changchun 130033, Peoples R China.
EM lv_hengyi@163.com
CR Aung MT, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351588
   Barranco F, 2015, LECT NOTES COMPUT SC, V9094, P309, DOI 10.1007/978-3-319-19258-1_27
   Benosman R, 2014, IEEE T NEUR NET LEAR, V25, P407, DOI 10.1109/TNNLS.2013.2273537
   Benosman R, 2012, NEURAL NETWORKS, V27, P32, DOI 10.1016/j.neunet.2011.11.001
   Brandli C, 2014, IEEE J SOLID-ST CIRC, V49, P2333, DOI 10.1109/JSSC.2014.2342715
   Brosch T, 2015, FRONT NEUROSCI-SWITZ, V9, DOI [10.3389/fnins.7015.00137, 10.3389/fnins.2015.00137]
   Chen G, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00073
   Chin TJ, 2019, IEEE COMPUT SOC CONF, P1646, DOI 10.1109/CVPRW.2019.00208
   Colonnier F, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21237840
   Delbruckl T, 2016, PROC EUR S-STATE DEV, P7, DOI 10.1109/ESSDERC.2016.7599576
   Ding ZL, 2022, AAAI CONF ARTIF INTE, P525
   Gallego G, 2022, IEEE T PATTERN ANAL, V44, P154, DOI 10.1109/TPAMI.2020.3008413
   Gallego G, 2018, PROC CVPR IEEE, P3867, DOI 10.1109/CVPR.2018.00407
   Gehrig D, 2019, IEEE I CONF COMP VIS, P5632, DOI 10.1109/ICCV.2019.00573
   Gehrig M, 2021, INT CONF 3D VISION, P197, DOI 10.1109/3DV53792.2021.00030
   Glover A, 2017, IEEE INT C INT ROBOT, P3769, DOI 10.1109/IROS.2017.8206226
   Glover A, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P2203, DOI 10.1109/IROS.2016.7759345
   Horowitz M, 2014, ISSCC DIG TECH PAP I, V57, P10, DOI 10.1109/ISSCC.2014.6757323
   Kingma D. P., 2014, ARXIV, DOI DOI 10.48550/ARXIV.1412.6980
   Lee Chankyu, 2022, 2022 International Conference on Robotics and Automation (ICRA), P6504, DOI 10.1109/ICRA46639.2022.9811821
   Lee C., 2020, P EUROPEAN C COMPUTE
   Li ZR, 2022, PROD OPER MANAG, V31, P239, DOI 10.1111/poms.13530
   Liu M.-Y., 2018, ARXIV
   Molchanov P., 2016, ARXIV
   Parameshwara CM, 2021, IEEE INT C INT ROBOT, P3414, DOI 10.1109/IROS51168.2021.9636506
   Paredes-Valles F., 2021, P IEEECVF C COMPUTER, P3446
   Piatkowska, 2012, IEEE COMP SOC C COMP, DOI DOI 10.1109/CVPRW.2012.6238892
   Posch C, 2011, IEEE J SOLID-ST CIRC, V46, P259, DOI 10.1109/JSSC.2010.2085952
   Stoffregen Timo, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12372), P534, DOI 10.1007/978-3-030-58583-9_32
   Sun DQ, 2014, INT J COMPUT VISION, V106, P115, DOI 10.1007/s11263-013-0644-x
   Won JY, 2015, IEEE T IND ELECTRON, V62, P536, DOI 10.1109/TIE.2014.2334667
   Wu YJ, 2019, AAAI CONF ARTIF INTE, P1311
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Ye CX, 2020, IEEE INT C INT ROBOT, P5831, DOI 10.1109/IROS45743.2020.9341224
   Zhang YS, 2022, SENSORS-BASEL, V22, DOI 10.3390/s22072614
   Zheng HL, 2021, AAAI CONF ARTIF INTE, V35, P11062
   Zhu Alex Zihao, 2018, IEEE Robotics and Automation Letters, V3, P2032, DOI 10.1109/LRA.2018.2800793
   Zhu Alex Zihao, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P4465, DOI 10.1109/ICRA.2017.7989517
   Zhu A.Z., 2018, ARXIV
   Zhu AZ, 2019, PROC CVPR IEEE, P989, DOI 10.1109/CVPR.2019.00108
NR 40
TC 2
Z9 2
U1 5
U2 7
PD JAN
PY 2023
VL 14
IS 1
AR 203
DI 10.3390/mi14010203
WC Chemistry, Analytical; Nanoscience & Nanotechnology; Instruments &
   Instrumentation; Physics, Applied
DA 2023-11-11
ER

PT J
AU Turova, TS
   Villa, AEP
AF Turova, Tatyana S.
   Villa, Alessandro E. P.
TI On a phase diagram for random neural networks with embedded spike timing
   dependent plasticity
SO BIOSYSTEMS
DT Article; Proceedings Paper
CT 6th International Workshop on Neural Coding
CY AUG 23-28, 2005
CL Marburg, GERMANY
DE random network; spike timing dependent synaptic plasticity; spiking
   neural network; graph theory
ID SYNAPTIC DENSITY; VISUAL-CORTEX; DYNAMICS
AB This paper presents an original mathematical framework based on graph theory which is a first attempt to investigate the dynamics of a model of neural networks with embedded spike timing dependent plasticity. The neurons correspond to integrate-and-fire units located at the vertices of a finite subset of 2D lattice. There are two types of vertices, corresponding to the inhibitory and the excitatory neurons. The edges are directed and labelled by the discrete values of the synaptic strength. We assume that there is an initial firing pattern corresponding to a subset of units that generate a spike. The number of activated externally vertices is a small fraction of the entire network. The model presented here describes how such pattern propagates throughout the network as a random walk on graph. Several results are compared with computational simulations and new data are presented for identifying critical parameters of the model. (C) 2006 Elsevier Ireland Ltd. All rights reserved.
C1 Lund Univ, Ctr Math, S-22100 Lund, Sweden.
   Univ Grenoble 1, INSERM, U318, Lab Neurobiophys, F-38041 Grenoble, France.
   Univ Lausanne, Inst Comp Sci & Org INFORGE, Neuroheurist Res Grp, CH-1015 Lausanne, Switzerland.
RP Turova, TS (corresponding author), Lund Univ, Ctr Math, S-22100 Lund, Sweden.
EM tatyana@maths.lth.se; Alessandro.Villa@ujf-grenoble.fr
CR Bell CC, 1997, NATURE, V387, P278, DOI 10.1038/387278a0
   Bollobas B, 1998, RANDOM GRAPHS
   BOURGEOIS JP, 1993, J NEUROSCI, V13, P2801
   Braitenberg V, 1988, CORTEX STAT GEOMETRY
   Eriksson J, 2003, LECT NOTES COMPUT SC, V2606, P165
   GREILICH H, 1984, THESIS U TUBINGEN
   Harkany T, 2004, J NEUROSCI, V24, P4978, DOI 10.1523/JNEUROSCI.4884-03.2004
   HUTTENLOCHER PR, 1982, NEUROSCI LETT, V33, P247, DOI 10.1016/0304-3940(82)90379-2
   HUTTENLOCHER PR, 1979, BRAIN RES, V163, P195
   Iglesias J, 2005, LECT NOTES COMPUT SC, V3704, P59, DOI 10.1007/11565123_6
   Iglesias J, 2005, BIOSYSTEMS, V79, P11, DOI 10.1016/j.biosystems.2004.09.016
   Izhikevich EM, 2004, CEREB CORTEX, V14, P933, DOI 10.1093/cercor/bhh053
   Montgomery JM, 2004, TRENDS NEUROSCI, V27, P744, DOI 10.1016/j.tins.2004.10.006
   Song S, 2001, NEURON, V32, P339, DOI 10.1016/S0896-6273(01)00451-2
   SZENTAGOTHAI J, 1975, BRAIN RES, V95, P475, DOI 10.1016/0006-8993(75)90122-5
NR 15
TC 13
Z9 13
U1 0
U2 4
PD MAY-JUN
PY 2007
VL 89
IS 1-3
SI SI
BP 280
EP 286
DI 10.1016/j.biosystems.2006.05.019
WC Biology; Mathematical & Computational Biology
DA 2023-11-11
ER

PT J
AU Cho, H
   Son, H
   Kim, JS
   Kim, B
   Park, HJ
   Sim, JY
AF Cho, Hwasuk
   Son, Hyunwoo
   Kim, Jun-Seok
   Kim, Byungsub
   Park, Hong-June
   Sim, Jae-Yoon
TI Design of Digital CMOS Neuromorphic IC with Current-starved SRAM Synapse
   for Unsupervised Stochastic Learning
SO JOURNAL OF SEMICONDUCTOR TECHNOLOGY AND SCIENCE
DT Article
DE Spiking neural network (SNN); spike-timing dependent plasticity (STDP);
   unsupervised learning; neural networks
ID NEURAL-NETWORK; SPIKING NEURONS; DEVICE
AB This paper proposes a current-starved SRAM circuit for use as a synapse to support a biological learning of spike-timing dependent plasticity. If the driving strength of SRAM is limited both when writing a new state and restoring the previous state, then programming the opposite synaptic weight requires several densely-repeated write operations. The synapse takes only stochastically-meaningful stimuli for training, and rejects random and noisy inputs; as a result it achieves successful learning even with binary SRAM. The proposed synapse is applied to a three-layer neuromorphic network of spiking neurons that includes a receptive synaptic array and a projective synaptic array. The network is designed with a 28-nm digital CMOS process. Applying an unsupervised learning to only the projective synapses with the receptive synapses fixed at randomly given, the network successfully memorizes multiple overlapped image patterns and recalls the trained patterns if the images are distorted.
C1 [Cho, Hwasuk; Son, Hyunwoo; Kim, Byungsub; Park, Hong-June; Sim, Jae-Yoon] POSTECH, Dept Elect Engn, Pohang, South Korea.
   [Kim, Jun-Seok] Samsung Elect, Kiheung, South Korea.
RP Sim, JY (corresponding author), POSTECH, Dept Elect Engn, Pohang, South Korea.
EM jysim@postech.ac.kr
CR Alibart F, 2012, NANOTECHNOLOGY, V23, DOI 10.1088/0957-4484/23/7/075201
   [Anonymous], 2011, P 28 INT C INT C MAC
   [Anonymous], 2012, 2012 INT JOINT C NEU, DOI [DOI 10.1109/IJCNN.2012.6252637, 10.1109/IJCNN.2012.6252637]
   [Anonymous], 2013, 2013 INT JOINT C NEU
   Azghadi MR, 2014, P IEEE, V102, P717, DOI 10.1109/JPROC.2014.2314454
   Brader JM, 2007, NEURAL COMPUT, V19, P2881, DOI 10.1162/neco.2007.19.11.2881
   Chang T, 2013, IEEE CIRC SYST MAG, V13, P56, DOI 10.1109/MCAS.2013.2256260
   Cruz-Albrecht JM, 2012, IEEE T BIOMED CIRC S, V6, P246, DOI 10.1109/TBCAS.2011.2174152
   Dan Y, 2004, NEURON, V44, P23, DOI 10.1016/j.neuron.2004.09.007
   Esser S., 2010, INT JOINT C NEUR NET, P1
   Indiveri G, 2006, IEEE T NEURAL NETWOR, V17, P211, DOI 10.1109/TNN.2005.860850
   Indiveri G, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00073
   Jarrett K, 2009, IEEE I CONF COMP VIS, P2146, DOI 10.1109/ICCV.2009.5459469
   Jo SH, 2010, NANO LETT, V10, P1297, DOI 10.1021/nl904092h
   Kim S, 2011, 2011 11TH INTERNATIONAL CONFERENCE ON CONTROL, AUTOMATION AND SYSTEMS (ICCAS), P1
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Mitra S, 2009, IEEE T BIOMED CIRC S, V3, P32, DOI 10.1109/TBCAS.2008.2005781
   Querlioz D, 2013, IEEE T NANOTECHNOL, V12, P288, DOI 10.1109/TNANO.2013.2250995
   Snider GS, 2008, 2008 IEEE INTERNATIONAL SYMPOSIUM ON NANOSCALE ARCHITECTURES, P85, DOI 10.1109/NANOARCH.2008.4585796
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Strukov DB, 2008, NATURE, V453, P80, DOI 10.1038/nature06932
   van Schaik A, 2001, NEURAL NETWORKS, V14, P617, DOI 10.1016/S0893-6080(01)00067-3
   Wu X, 2015, INT JOINT C NEUR NET, V2015, P1
   Wu XY, 2015, IEEE J EM SEL TOP C, V5, P254, DOI 10.1109/JETCAS.2015.2433552
NR 25
TC 2
Z9 2
U1 0
U2 5
PD FEB
PY 2018
VL 18
IS 1
SI SI
BP 65
EP 77
DI 10.5573/JSTS.2018.18.1.065
WC Engineering, Electrical & Electronic; Physics, Applied
DA 2023-11-11
ER

PT J
AU Kim, S
   Kim, S
   Um, S
   Kim, S
   Lee, J
   Yoo, HJ
AF Kim, Sangyeob
   Kim, Sangjin
   Um, Soyeon
   Kim, Soyeon
   Lee, Juhyoung
   Yoo, Hoi-Jun
TI SNPU: An Energy-Efficient Spike Domain Deep-Neural-Network Processor
   With Two-Step Spike Encoding and Shift-and-Accumulation Unit
SO IEEE JOURNAL OF SOLID-STATE CIRCUITS
DT Article; Early Access
DE Deep learning application-specific integrated circuit (ASIC); deep
   neural network (DNN); sparsity exploration; spiking neural network
   (SNN); zero skipping
ID FACE RECOGNITION; ACCELERATOR; POWER
AB In this article, an energy-efficient spike domain deep-neural-network processor (SNPU) is proposed. Recently, many sparsity-aware processors were proposed to increase energy efficiency. However, because the activation of the real-world network was not high compared to the ideal condition, they were unable to completely employ integrated zero skipping logic. In addition, they employed weight sparsity by pruning, but their zero skipping logic was designed to perform best only under specific sparsity condition. We adopt two-step encoding (neural and computing encoding) and an omni-sparsity handling unit to solve these issues. The input neural encoding eliminates pseudo-random number generators (PRNGs) for the previous encoding, and the output neural encoding reduces redundant memory access. As a result, weight can be reused by a delayed factor, while input spike encoding consumes 99.4% less power compared to the Poisson encoder. Also, SNPU uses computing encoding with multi-level encoding and spiketrain decomposition. The multi-level encoding reduces the time-window size by merging several spikes, and SNPU uses a single shift-and-accumulation (SAC) rather than several accumulations, reducing 43.5% of operations compared to single-level encoding. In addition, the spike-train decomposition is proposed to decrease the number of SACs by computation reuse, resulting in a reduction of 71.8% operations compared to previous spiking neural-network (SNN) processing, which cannot reuse computation result. As a result, the computing encoding reduces the operations by 83.3% compared to SNPU without multi-level encoding and spike-train decomposition. Moreover, SAC units in SNN PE process multi-level spikes while consuming 80.3% less power than accumulator arrays that do not use the two-step encoding. Also, the neuron link and spike-train allocator handle variable time-window sizes, and they can increase PE utilization of SNPU by 23.1%. To sum it up, SNPU shows the state-of-the-art energy efficiency of 13.7 TOPS/W for object detection and 31.5 TOPS/W for ImageNet classification. Moreover, SNPU shows the state-of-the-art power consumption of 63.2 mu W for face recognition.
C1 [Kim, Sangyeob; Kim, Sangjin; Um, Soyeon; Kim, Soyeon; Lee, Juhyoung; Yoo, Hoi-Jun] Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon 34141, South Korea.
RP Yoo, HJ (corresponding author), Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon 34141, South Korea.
EM hjyoo@kaist.ac.kr
CR Bing Han, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13555, DOI 10.1109/CVPR42600.2020.01357
   Bong K, 2017, ISSCC DIG TECH PAP I, P248, DOI 10.1109/ISSCC.2017.7870354
   Chen GK, 2018, SYMP VLSI CIRCUITS, P255, DOI 10.1109/VLSIC.2018.8502423
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Han D, 2021, IEEE J SOLID-ST CIRC, V56, P2858, DOI 10.1109/JSSC.2021.3066400
   Kang S, 2020, ISSCC DIG TECH PAP I, P140, DOI 10.1109/ISSCC19947.2020.9062989
   Kang S, 2018, SYMP VLSI CIRCUITS, P137, DOI 10.1109/VLSIC.2018.8502266
   Kim S, 2020, IEEE T CIRCUITS-I, V67, P1181, DOI 10.1109/TCSI.2020.2966243
   Kim Y, 2020, IEEE T CIRCUITS-II, V67, P846, DOI 10.1109/TCSII.2020.2980022
   Lee J, 2019, IEEE SOLID-ST CIRC L, V2, P232, DOI 10.1109/LSSC.2019.2937440
   Lee J, 2019, ISSCC DIG TECH PAP I, V62, P142, DOI 10.1109/ISSCC.2019.8662302
   Liu Y., 2022, IEEE INT SOLID STATE, P372
   Mo H., 2019, PROC 56 ACMIEEE DESI, P1
   Park J, 2020, IEEE J SOLID-ST CIRC, V55, P108, DOI 10.1109/JSSC.2019.2942367
   Rathi N, 2020, Arxiv, DOI arXiv:2008.03658
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Tu FB, 2021, IEEE J SOLID-ST CIRC, V56, P658, DOI 10.1109/JSSC.2020.3021661
   Uenohara S, 2022, IEEE ACCESS, V10, P48338, DOI 10.1109/ACCESS.2022.3170579
   Wang, 2021, S VLSI CIRCUITS, P1
   Wu JB, 2022, IEEE T PATTERN ANAL, V44, P7824, DOI 10.1109/TPAMI.2021.3114196
   Yoon JH, 2020, ISSCC DIG TECH PAP I, P478, DOI 10.1109/ISSCC19947.2020.9063142
NR 21
TC 0
Z9 0
U1 9
U2 9
PD 2023 MAY 9
PY 2023
DI 10.1109/JSSC.2023.3270442
EA MAY 2023
WC Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Yang, SM
   Tan, JT
   Chen, BD
AF Yang, Shuangming
   Tan, Jiangtong
   Chen, Badong
TI Robust Spike-Based Continual Meta-Learning Improved by Restricted
   Minimum Error Entropy Criterion
SO ENTROPY
DT Article
DE spiking neural network; meta-learning; information theoretic learning;
   minimum error entropy; artificial general intelligence
AB The spiking neural network (SNN) is regarded as a promising candidate to deal with the great challenges presented by current machine learning techniques, including the high energy consumption induced by deep neural networks. However, there is still a great gap between SNNs and the online meta-learning performance of artificial neural networks. Importantly, existing spike-based online meta-learning models do not target the robust learning based on spatio-temporal dynamics and superior machine learning theory. In this invited article, we propose a novel spike-based framework with minimum error entropy, called MeMEE, using the entropy theory to establish the gradient-based online meta-learning scheme in a recurrent SNN architecture. We examine the performance based on various types of tasks, including autonomous navigation and the working memory test. The experimental results show that the proposed MeMEE model can effectively improve the accuracy and the robustness of the spike-based meta-learning performance. More importantly, the proposed MeMEE model emphasizes the application of the modern information theoretic learning approach on the state-of-the-art spike-based learning algorithms. Therefore, in this invited paper, we provide new perspectives for further integration of advanced information theory in machine learning to improve the learning performance of SNNs, which could be of great merit to applied developments with spike-based neuromorphic systems.
C1 [Yang, Shuangming; Tan, Jiangtong] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
   [Chen, Badong] Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian 710049, Peoples R China.
RP Chen, BD (corresponding author), Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian 710049, Peoples R China.
EM yangshuangming@tju.edu.cn; shuangmingyang@tju.edu.cn;
   chenbd@mail.xjtu.edu.cn
CR [Anonymous], 2021, ENTROPY-SWITZ, DOI DOI 10.3390/E23121620
   [Anonymous], 2019, ADV NEURAL INFORM PR
   Bellec G., 2017, ARXIV171105136
   Bellec G., 2018, ADV NEURAL INF PROCE, V31, P247
   Cao P., 2019, ADV NEURAL INF PROCE, V32, P76
   Chen BD, 2021, IEEE T SYST MAN CY-S, V51, P4007, DOI 10.1109/TSMC.2019.2931403
   Chen BD, 2020, IEEE T SYST MAN CY-S, V50, P4557, DOI 10.1109/TSMC.2018.2855106
   Chen BD, 2018, IEEE T NEUR NET LEAR, V29, P731, DOI 10.1109/TNNLS.2016.2636160
   Chen BD, 2012, IEEE T SIGNAL PROCES, V60, P1184, DOI 10.1109/TSP.2011.2178406
   Chen HY, 2019, IEEE I CONF COMP VIS, P4880, DOI 10.1109/ICCV.2019.00498
   Chen J., 2018, INT C MACHINE LEARNI, V80, P883
   Cho SW, 2021, ADV INTELL SYST-GER, V3, DOI 10.1002/aisy.202000162
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   DeBole MV, 2019, COMPUTER, V52, P20, DOI 10.1109/MC.2019.2903009
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   Grienberger C, 2017, NAT NEUROSCI, V20, P417, DOI 10.1038/nn.4486
   Jiang RH, 2021, NEURAL COMPUT, V33, P2439, DOI 10.1162/neco_a_01423
   Kingma D. P., 2014, ARXIV, DOI DOI 10.48550/ARXIV.1412.6980
   Krestinskaya O, 2020, IEEE T NEUR NET LEAR, V31, P4, DOI 10.1109/TNNLS.2019.2899262
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
   Lee C, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00435
   Li Y, 2022, IEEE T NEUR NET LEAR, V33, P4851, DOI 10.1109/TNNLS.2021.3061432
   Li YM, 2022, IEEE T NEUR NET LEAR, V33, P4228, DOI 10.1109/TNNLS.2021.3056188
   Muñoz W, 2017, SCIENCE, V355, P954, DOI 10.1126/science.aag2599
   Parisi GI, 2019, NEURAL NETWORKS, V113, P54, DOI 10.1016/j.neunet.2019.01.012
   Pei J, 2019, NATURE, V572, P106, DOI 10.1038/s41586-019-1424-8
   Poleg-Polsky A, 2018, CELL REP, V22, P2898, DOI 10.1016/j.celrep.2018.02.064
   Rachdi M, 2020, ENTROPY-SWITZ, V22, DOI 10.3390/e22030260
   Ranganathan GN, 2018, NAT NEUROSCI, V21, P1583, DOI 10.1038/s41593-018-0254-6
   Schulman J., 2017, ARXIV, DOI DOI 10.1016/J.JDEVECO.2016.04.001
   Serrà J, 2018, PR MACH LEARN RES, V80
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   van de Ven GM, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-17866-2
   Vasilaki E, 2009, PLOS COMPUT BIOL, V5, DOI 10.1371/journal.pcbi.1000586
   Wolff MJ, 2017, NAT NEUROSCI, V20, P864, DOI 10.1038/nn.4546
   Xia QF, 2019, NAT MATER, V18, P309, DOI 10.1038/s41563-019-0291-x
   Yang SM, 2022, IEEE T NEUR NET LEAR, V33, P2801, DOI 10.1109/TNNLS.2020.3045492
   Yang SM, 2021, FRONT NEUROSCI-SWITZ, V15, DOI 10.3389/fnins.2021.601109
   Yang SM, 2022, IEEE T NEUR NET LEAR, V33, P4398, DOI [10.1109/TNNLS.2021.3057070, 10.4018/IJCINI.20211001.oa2]
   Yang SM, 2019, IEEE T CYBERNETICS, V49, P2490, DOI 10.1109/TCYB.2018.2823730
   Yao H., 2020, ADV NEURAL INFORM PR, V33, P6779
   Yoo J, 2021, CURR OPIN BIOTECH, V72, P95, DOI 10.1016/j.copbio.2021.10.012
   Zeng GX, 2019, NAT MACH INTELL, V1, P364, DOI 10.1038/s42256-019-0080-x
NR 43
TC 81
Z9 81
U1 9
U2 59
PD APR
PY 2022
VL 24
IS 4
AR 455
DI 10.3390/e24040455
WC Physics, Multidisciplinary
HC Y
HP Y
DA 2023-11-11
ER

PT C
AU Gyöngyössy, NM
   Domonkos, M
   Botzheim, J
   Korondi, P
AF Gyongyossy, Natabara Mate
   Domonkos, Mark
   Botzheim, Janos
   Korondi, Peter
GP IEEE
TI Supervised Learning with Small Training Set for Gesture Recognition by
   Spiking Neural Networks
SO 2019 IEEE SYMPOSIUM SERIES ON COMPUTATIONAL INTELLIGENCE (IEEE SSCI
   2019)
DT Proceedings Paper
CT IEEE Symposium Series on Computational Intelligence (SSCI)
CY DEC 06-09, 2019
CL Xiamen, PEOPLES R CHINA
DE spiking neural networks; supervised learning; gesture recognition;
   human-robot interaction
AB This paper proposes a novel supervised learning algorithm for spiking neural networks. The algorithm combines Hebbian learning and least mean squares method and it works well for small training datasets and short training cycles. The proposed method is applied in human-robot interaction for recognizing musical hand gestures based on the work of Zoltan Kodaly. The MNIST dataset is also used as a benchmark test to verify the proposed algorithm's capability to outperform shallow ANN architectures. Experiments with the robot also provided promising results by recognizing the human hand signs correctly.
C1 [Gyongyossy, Natabara Mate; Domonkos, Mark; Botzheim, Janos; Korondi, Peter] Budapest Univ Technol & Econ, Fac Mech Engn, Dept Mechatron Opt & Mech Engn Informat, 4-6 Bertalan Lajos St, H-1111 Budapest, Hungary.
RP Gyöngyössy, NM (corresponding author), Budapest Univ Technol & Econ, Fac Mech Engn, Dept Mechatron Opt & Mech Engn Informat, 4-6 Bertalan Lajos St, H-1111 Budapest, Hungary.
EM natabara@gyongyossy.hu; domonkos@mogi.bme.hu; botzheim@mogi.bme.hu;
   korondi@mogi.bme.hu
CR [Anonymous], KODL LIF WORK
   Atre P., 2017, P 2 INT C INT COMP C
   Ballard L., 2011, ROBOT CONGERS
   Botzheim J, 2012, JOINT INT CONF SOFT, P1954, DOI 10.1109/SCIS-ISIS.2012.6505305
   Burges, 2010, MNIST HANDWRITTEN DI, DOI DOI 10.1561/2400000035
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Fasola J, 2013, J HUM-ROBOT INTERACT, V2, P3, DOI 10.5898/JHRI.2.2.Fasola
   Gerstner W., 2002, SPIKING NEURON MODEL
   HEBB D. O., 1949
   Hegyi E., 1975, SOLFEGE ACCORDING KO
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Hoffman G, 2010, 2010 IEEE RO-MAN, P718, DOI 10.1109/ROMAN.2010.5598690
   Hoffman G, 2010, IEEE INT CONF ROBOT, P582, DOI 10.1109/ROBOT.2010.5509182
   Horvth C. M., 2017, RECENT INNOVATIONS M, V4, P1
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Knox WB, 2013, LECT NOTES ARTIF INT, V8239, P460, DOI 10.1007/978-3-319-02675-6_46
   Niu D, 2018, LECT NOTES COMPUT SC, V11301, P582, DOI 10.1007/978-3-030-04167-0_53
   Roman RC, 2018, PROCEDIA COMPUT SCI, V139, P372, DOI 10.1016/j.procs.2018.10.277
   Tamiru H. G., 2018, 8 INT C MAN SCI ENG, P48
   Vaila R., 2019, MALAYSIAN J COMPUTER
   Widrow B., 1960, IRE WESCON COINVENTI, DOI DOI 10.21236/AD0241531
   Widrow B, 2015, IEEE COMPUT INTELL M, V10, P37, DOI 10.1109/MCI.2015.2471216
   Woo J, 2017, MALAYS J COMPUT SCI, V30, P258
NR 23
TC 8
Z9 8
U1 0
U2 2
PY 2019
BP 2201
EP 2206
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT J
AU Nishitani, Y
   Kaneko, Y
   Ueda, M
AF Nishitani, Yu
   Kaneko, Yukihiro
   Ueda, Michihito
TI Supervised Learning Using Spike-Timing-Dependent Plasticity of
   Memristive Synapses
SO IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
DT Article
DE Memristor; spike-timing-dependent plasticity (STDP); spiking neural
   network (SNN) hardware; supervised learning
ID NETWORKS; CIRCUITS; NEURONS
AB We propose a supervised learning model that enables error backpropagation for spiking neural network hardware. The method is modeled by modifying an existing model to suit the hardware implementation. An example of a network circuit for the model is also presented. In this circuit, a three-terminal ferroelectric memristor (3T-FeMEM), which is a field-effect transistor with a gate insulator composed of ferroelectric materials, is used as an electric synapse device to store the analog synaptic weight. Our model can be implemented by reflecting the network error to the write voltage of the 3T-FeMEMs and introducing a spike-timing-dependent learning function to the device. An XOR problem was successfully demonstrated as a benchmark learning by numerical simulations using the circuit properties to estimate the learning performance. In principle, the learning time per step of this supervised learning model and the circuit is independent of the number of neurons in each layer, promising a high-speed and low-power calculation in large-scale neural networks.
C1 [Nishitani, Yu; Kaneko, Yukihiro; Ueda, Michihito] Panasonic Corp, Adv Res Div, Kyoto 6190237, Japan.
RP Nishitani, Y (corresponding author), Panasonic Corp, Adv Res Div, Kyoto 6190237, Japan.
EM nishitani.yu@jp.panasonic.com; kaneko.yukihiro001@jp.panasonic.com;
   ueda.michihito@jp.panasonic.com
CR Abbott LF, 2000, NAT NEUROSCI, V3, P1178, DOI 10.1038/81453
   [Anonymous], IEEE IEDM TECH DIG 2, DOI DOI 10.1109/IEDM.2012.6478962
   [Anonymous], 1989, ANALOG VLSI NEURAL S
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Bichler O, 2012, IEEE T ELECTRON DEV, V59, P2206, DOI 10.1109/TED.2012.2197951
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Chanthbouala A, 2012, NAT MATER, V11, P860, DOI [10.1038/nmat3415, 10.1038/NMAT3415]
   Gerstner W., 2002, SPIKING NEURON MODEL
   Ghosh-Dastidar S, 2007, INTEGR COMPUT-AID E, V14, P187
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   ISHIWARA H, 1993, JPN J APPL PHYS 1, V32, P442, DOI 10.1143/JJAP.32.442
   Jo SH, 2010, NANO LETT, V10, P1297, DOI 10.1021/nl904092h
   Kandel E. R., 1991, PRINCIPLES NEURAL SC
   Kaneko Y, 2014, IEEE T ELECTRON DEV, V61, P2827, DOI 10.1109/TED.2014.2331707
   Kaneko Y, 2011, APPL PHYS LETT, V99, DOI 10.1063/1.3657413
   Kato Y, 2008, JPN J APPL PHYS, V47, P2719, DOI 10.1143/JJAP.47.2719
   Kuzum D, 2012, IEEE T ELECTRON DEV, V59, P3489, DOI 10.1109/TED.2012.2217146
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   McKennoch S, 2006, IEEE IJCNN, P3970
   Nishitani Y, 2012, J APPL PHYS, V111, DOI 10.1063/1.4729915
   Nishitani Y, 2013, JPN J APPL PHYS, V52, DOI 10.7567/JJAP.52.04CE06
   Nishiyama M, 2000, NATURE, V408, P584, DOI 10.1038/35046067
   Rajendran B, 2013, IEEE T ELECTRON DEV, V60, P246, DOI 10.1109/TED.2012.2227969
   Ramakrishnan S, 2011, IEEE T BIOMED CIRC S, V5, P244, DOI 10.1109/TBCAS.2011.2109000
   RIEDMILLER M, 1993, 1993 IEEE INTERNATIONAL CONFERENCE ON NEURAL NETWORKS, VOLS 1-3, P586, DOI 10.1109/ICNN.1993.298623
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   SCHUETZE SM, 1983, TRENDS NEUROSCI, V6, P164, DOI 10.1016/0166-2236(83)90078-4
   Sharad M, 2012, IEEE T NANOTECHNOL, V11, P843, DOI 10.1109/TNANO.2012.2202125
   Shi J, 2013, NAT COMMUN, V4, DOI 10.1038/ncomms3676
   Snider GS, 2008, 2008 IEEE INTERNATIONAL SYMPOSIUM ON NANOSCALE ARCHITECTURES, P85, DOI 10.1109/NANOARCH.2008.4585796
   Tanaka H, 2009, IEICE T FUND ELECTR, VE92A, P1690, DOI 10.1587/transfun.E92.A.1690
   Ueda M, 2011, J APPL PHYS, V110, DOI 10.1063/1.3653830
   Yoon SM, 1999, IEEE ELECTR DEVICE L, V20, P526, DOI 10.1109/55.791931
   Yoon SM, 1999, IEEE ELECTR DEVICE L, V20, P229, DOI 10.1109/55.761023
   Zhu LQ, 2014, NAT COMMUN, V5, DOI 10.1038/ncomms4158
NR 35
TC 39
Z9 41
U1 2
U2 104
PD DEC
PY 2015
VL 26
IS 12
BP 2999
EP 3008
DI 10.1109/TNNLS.2015.2399491
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Computer Science, Theory & Methods; Engineering,
   Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Tavanaei, A
   Maida, AS
AF Tavanaei, Amirhossein
   Maida, Anthony S.
GP IEEE
TI Multi-Layer Unsupervised Learning in a Spiking Convolutional Neural
   Network
SO 2017 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY MAY 14-19, 2017
CL Anchorage, AK
ID SPARSE CODE; MODEL
AB Spiking neural networks (SNNs) have advantages over traditional, non-spiking networks with respect to bio-realism, potential for low-power hardware implementations, and theoretical computing power. However, in practice, spiking networks with multi-layer learning have proven difficult to train. This paper explores a novel, bio-inspired spiking convolutional neural network (CNN) that is trained in a greedy, layer-wise fashion. The spiking CNN consists of a convolutional/pooling layer followed by a feature discovery layer, both of which undergo bio-inspired learning. Kernels for the convolutional layer are trained using a sparse, spiking auto-encoder representing primary visual features. The feature discovery layer uses a probabilistic spike-timing-dependent plasticity (STDP) learning rule. This layer represents complex visual features using WTA-thresholded, leaky, integrate-and-fire (LIF) neurons. The new model is evaluated on the MNIST digit dataset using clean and noisy images. Intermediate results show that the convolutional layer is stack-admissible, enabling it to support a multi-layer learning architecture. The recognition performance for clean images is above 98%. This performance is accounted for by the independent and informative visual features extracted in a hierarchy of convolutional and feature discovery layers. The performance loss for recognizing the noisy images is in the range 0.1% to 8.5%. This level of performance loss indicates that the network is robust to additive noise.
C1 [Tavanaei, Amirhossein; Maida, Anthony S.] Univ Louisiana Lafayette, Ctr Adv Comp Studies, Lafayette, LA 70504 USA.
RP Tavanaei, A (corresponding author), Univ Louisiana Lafayette, Ctr Adv Comp Studies, Lafayette, LA 70504 USA.
EM tavanaei@louisiana.edu; maida@louisiana.edu
CR [Anonymous], EUR C COMP VIS
   [Anonymous], 2016, ARXIV161101421
   [Anonymous], 2015, INT J COMPUT VISION, DOI DOI 10.1007/s11263-014-0788-3
   [Anonymous], J NEUROSCIENCE
   [Anonymous], ADV NEURAL INFORM PR
   [Anonymous], 2013, FRONTIERS NEUROSCIEN
   [Anonymous], 2015, P INT JOINT C NEUR N
   [Anonymous], ENEURO
   [Anonymous], 2011, ADV NEURAL INF PROCE
   Bell AJ, 1997, VISION RES, V37, P3327, DOI 10.1016/S0042-6989(97)00121-1
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Beyeler M, 2013, NEURAL NETWORKS, V48, P109, DOI 10.1016/j.neunet.2013.07.012
   Buesing L, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1002211
   Burbank KS, 2015, PLOS COMPUT BIOL, V11, DOI 10.1371/journal.pcbi.1004566
   Dan Y, 2006, PHYSIOL REV, V86, P1033, DOI 10.1152/physrev.00030.2005
   Diehl Peter U, 2015, 2015 INT JOINT C NEU, P1, DOI [DOI 10.1109/IJCNN.2015.7280696, 10.1109/IJCNN.2015.7280696]
   FOLDIAK P, 1990, BIOL CYBERN, V64, P165, DOI 10.1007/BF02331346
   Kheradpisheh SR, 2016, NEUROCOMPUTING, V205, P382, DOI 10.1016/j.neucom.2016.04.029
   King PD, 2013, J NEUROSCI, V33, P5475, DOI 10.1523/JNEUROSCI.4188-12.2013
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508
   Masquelier T, 2007, PLOS COMPUT BIOL, V3, P247, DOI 10.1371/journal.pcbi.0030031
   Matsugu M, 2002, ICONIP'02: PROCEEDINGS OF THE 9TH INTERNATIONAL CONFERENCE ON NEURAL INFORMATION PROCESSING, P660
   Neftci E, 2014, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00272
   Nessler B., 2009, ADV NEURAL INFORM PR, P1357
   Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0
   Panda P, 2016, IEEE IJCNN, P299, DOI 10.1109/IJCNN.2016.7727212
   Sainath TN, 2013, INT CONF ACOUST SPEE, P8614, DOI 10.1109/ICASSP.2013.6639347
   Savin C, 2010, PLOS COMPUT BIOL, V6, DOI 10.1371/journal.pcbi.1000757
   Tavanaei A., 2016, J SIGNAL PROCESS SYS, P1
   Tavanaei A, 2016, IEEE IJCNN, P307, DOI 10.1109/IJCNN.2016.7727213
   Wysoski SG, 2008, NEUROCOMPUTING, V71, P2563, DOI 10.1016/j.neucom.2007.12.038
   Zylberberg J, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1002250
NR 34
TC 42
Z9 44
U1 1
U2 12
PY 2017
BP 2023
EP 2030
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Kang, P
   Hu, H
   Banerjee, S
   Chopp, H
   Katsaggelos, A
   Cossairt, O
AF Kang, Peng
   Hu, Hao
   Banerjee, Srutarshi
   Chopp, Henry
   Katsaggelos, Aggelos
   Cossairt, Oliver
GP IEEE
TI HUMAN VISION-LIKE ROBUST OBJECT RECOGNITION
SO 2021 IEEE INTERNATIONAL CONFERENCE ON IMAGE PROCESSING (ICIP)
SE IEEE International Conference on Image Processing ICIP
DT Proceedings Paper
CT IEEE International Conference on Image Processing (ICIP)
CY SEP 19-22, 2021
CL ELECTR NETWORK
DE human vision; robust object recogniton; Spiking Neural Networks;
   Artificial Neural Networks
ID MODELS
AB Previous research always solely utilizes Artificial Neural Networks (ANNs) or Spiking Neural Networks (SNNs) for object recognition. However, evidence in neuroscience suggests that the visual processing in human vision is performed hierarchically in the combination of analog and digital processing. To construct a more human vision-like object recognition system, we propose a general hierarchical ANN-SNN model. We evaluate our model and its variants on two popular datasets to show its effectiveness, robustness, efficiency, and generality. Extensive experiments clearly demonstrate the superiority of our proposed models for robust object recognition.
C1 [Kang, Peng; Banerjee, Srutarshi; Chopp, Henry; Katsaggelos, Aggelos; Cossairt, Oliver] Northwestern Univ, Evanston, IL 60208 USA.
   [Hu, Hao] Univ British Columbia, Vancouver, BC V6T 2A1, Canada.
RP Kang, P (corresponding author), Northwestern Univ, Evanston, IL 60208 USA.
CR [Anonymous], 2000, PRINCIPLES NEURAL SC, DOI DOI 10.1007/SPRINGERREFERENCE_183113
   Bullmore ET, 2012, NAT REV NEUROSCI, V13, P336, DOI 10.1038/nrn3214
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Cheng X, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1519
   Dayan P., 2005, THEORETICAL NEUROSCI
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Felleman DJ, 1991, CEREB CORTEX, V1, P1, DOI 10.1093/cercor/1.1.1
   Gollisch T, 2010, NEURON, V65, P150, DOI 10.1016/j.neuron.2009.12.009
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   Hood DC, 1998, ANNU REV PSYCHOL, V49, P503, DOI 10.1146/annurev.psych.49.1.503
   JIN Y, 2018, ARXIV PREPRINT ARXIV
   LeCun Y, 2015, NATURE, V521, p7553 436 444, DOI [10.1038/nature14539, DOI 10.1038/NATURE14539]
   Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508
   Neil D, 2016, IEEE INT SYMP CIRC S, P2282, DOI 10.1109/ISCAS.2016.7539039
   Neil Daniel, 2016, P 30 INT C NEUR INF, P3882, DOI DOI 10.48550/ARXIV.1610.09513
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Purves D, 2001, TYPES EYE MOVEMENTS
   Riesenhuber M, 1999, NAT NEUROSCI, V2, P1019, DOI 10.1038/14819
   Roy K, 2019, NATURE, V575, P607, DOI 10.1038/s41586-019-1677-2
   Saszik S, 2012, J NEUROSCI, V32, P297, DOI 10.1523/JNEUROSCI.2739-08.2012
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Shrestha SB, 2018, ADV NEUR IN, V31
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
NR 23
TC 0
Z9 0
U1 0
U2 1
PY 2021
BP 709
EP 713
DI 10.1109/ICIP42928.2021.9506331
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Engineering, Electrical & Electronic; Imaging Science &
   Photographic Technology
DA 2023-11-11
ER

PT J
AU Parasuraman, K
   Elshorbagy, A
   Carey, SK
AF Parasuraman, Kamban
   Elshorbagy, Amin
   Carey, Sean K.
TI Spiking modular neural networks: A neural network modeling approach for
   hydrological processes
SO WATER RESOURCES RESEARCH
DT Article
ID REFERENCE EVAPOTRANSPIRATION; EVAPORATION RATES; EDDY-COVARIANCE; WATER;
   SOIL; HEAT; FLUX; PREDICTION; ALBEDO
AB Artificial Neural Networks (ANNs) have been widely used for modeling hydrological processes that are embedded with high nonlinearity in both spatial and temporal scales. The input-output functional relationship does not remain the same over the entire modeling domain, varying at different spatial and temporal scales. In this study, a novel neural network model called the spiking modular neural networks (SMNNs) is proposed. An SMNN consists of an input layer, a spiking layer, and an associator neural network layer. The modular nature of the SMNN helps in finding domain-dependent relationships. The performance of the model is evaluated using two distinct case studies. The first case study is that of streamflow modeling, and the second case study involves modeling of eddy covariance-measured evapotranspiration. Two variants of SMNNs were analyzed in this study. The first variant employs a competitive layer as the spiking layer, and the second variant employs a self-organizing map as the spiking layer. The performance of SMNNs is compared to that of a regular feed forward neural network (FFNN) model. Results from the study demonstrate that SMNNs performed better than FFNNs for both the case studies. Results from partitioning analysis reveal that, compared to FFNNs, SMNNs are effective in capturing the dynamics of high flows. In modeling evapotranspiration, it is found that net radiation and ground temperature alone can be used to model the evaporation flux effectively. The SMNNs are shown to be effective in discretizing the complex mapping space into simpler domains that can be learned with relative ease.
C1 Univ Saskatchewan, Dept Civil & Geol Engn, CANSIM, Saskatoon, SK S7N 5A9, Canada.
   Carleton Univ, Dept Geog & Environm Studies, Ottawa, ON K1S 5B6, Canada.
RP Parasuraman, K (corresponding author), Univ Saskatchewan, Dept Civil & Geol Engn, CANSIM, Saskatoon, SK S7N 5A9, Canada.
EM amin.elshorbagy@usask.ca
CR ABBOTT MB, 1986, J HYDROL, V87, P45, DOI 10.1016/0022-1694(86)90114-9
   [Anonymous], 1994, NEUROFUZZY ADAPTIVE
   [Anonymous], 1989, HYDROLOGIC SYSTEMS W
   Baker JM, 2005, AGR FOREST METEOROL, V128, P163, DOI 10.1016/j.agrformet.2004.11.005
   BLANEY HF, 1950, 96 USDA NAT RES CONS
   Bowden GJ, 2005, J HYDROL, V301, P75, DOI 10.1016/j.jhydrol.2004.06.021
   Brutsaert W., 1982, EVAPORATION ATMOSPHE, DOI DOI 10.1007/978-94-017-1497-6
   CAI SQ, 1994, CAN J CHEM ENG, V72, P440, DOI 10.1002/cjce.5450720308
   DEMUTH H, 2001, NEURAL NETWORK TOOLB
   Drexler JZ, 2004, HYDROL PROCESS, V18, P2071, DOI 10.1002/hyp.1462
   Eltahir EAB, 1998, WATER RESOUR RES, V34, P765, DOI 10.1029/97WR03499
   Entekhabi D, 1996, J HYDROL, V184, P3, DOI 10.1016/0022-1694(95)02965-6
   Govindaraju RS, 2000, J HYDROL ENG, V5, P124
   Govindaraju RS, 2000, J HYDROL ENG, V5, P115
   HARGREAVES GH, 1982, J IRR DRAIN DIV-ASCE, V108, P225
   Haykin S., 2004, NEURAL NETWORKS COMP, V2, P41
   HOLDRIDGE LR, 1962, ECOLOGY, V43, P1, DOI 10.2307/1932033
   Hong Y, 2005, WATER RESOUR RES, V41, DOI 10.1029/2004WR003142
   Hsu KL, 2002, WATER RESOUR RES, V38, DOI 10.1029/2001WR000795
   JACKSON RD, 1976, WATER RESOUR RES, V12, P23, DOI 10.1029/WR012i001p00023
   JARVIS PG, 1986, ADV ECOL RES, V15, P1, DOI 10.1016/S0065-2504(08)60119-1
   JENSEN KH, 1981, MODELING COMPONENTS, P235
   KARUNANITHI N, 1994, J COMPUT CIVIL ENG, V8, P201, DOI 10.1061/(ASCE)0887-3801(1994)8:2(201)
   Kohonen T., 1989, SELF ORG ASS MEMORY
   Kumar M, 2002, J IRRIG DRAIN ENG, V128, P224, DOI 10.1061/(ASCE)0733-9437(2002)128:4(224)
   Lakshmi V, 2001, HYDROL PROCESS, V15, P877, DOI 10.1002/hyp.193
   Leuning R, 1996, GLOBAL CHANGE BIOL, V2, P241, DOI 10.1111/j.1365-2486.1996.tb00076.x
   LINACRE ET, 1977, AGR METEOROL, V18, P409, DOI 10.1016/0002-1571(77)90007-3
   MacKay D. J., 1992, BAYESIAN METHODS ADA
   Maier HR, 2000, ENVIRON MODELL SOFTW, V15, P101, DOI 10.1016/S1364-8152(99)00007-9
   Minns AW, 1996, HYDROLOG SCI J, V41, P399, DOI 10.1080/02626669609491511
   Monteith J L, 1965, Symp Soc Exp Biol, V19, P205
   PENMAN HL, 1948, PROC R SOC LON SER-A, V193, P120, DOI 10.1098/rspa.1948.0037
   PRIESTLEY CHB, 1972, MON WEATHER REV, V100, P81, DOI 10.1175/1520-0493(1972)100<0081:OTAOSH>2.3.CO;2
   Sajikumar N, 1999, J HYDROL, V216, P32, DOI 10.1016/S0022-1694(98)00273-X
   Salvucci GD, 1997, WATER RESOUR RES, V33, P111, DOI 10.1029/96WR02858
   SAXTON KE, 1981, MODELING COMPONENTS, P183
   SCHOTANUS P, 1983, BOUND-LAY METEOROL, V26, P81, DOI 10.1007/BF00164332
   STEPHENS J. C., 1963, Publ. Int. Ass. sci. Hydrol. 62 gen. Assembly Berkeley, P123
   Sudheer KP, 2003, J IRRIG DRAIN ENG, V129, P214, DOI 10.1061/(ASCE)0733-9437(2003)129:3(214)
   Sudheer KP, 2002, HYDROL PROCESS, V16, P3189, DOI 10.1002/hyp.1096
   Temesgen B, 2005, J IRRIG DRAIN ENG, V131, P73, DOI 10.1061/(ASCE)0733-9437(2005)131:1(73)
   Thornthwaite CW, 1948, GEOGR REV, V38, P55, DOI 10.2307/210739
   Tokar AS, 2000, J HYDROL ENG, V5, P156, DOI 10.1061/(ASCE)1084-0699(2000)5:2(156)
   Trajkovic S, 2003, J IRRIG DRAIN ENG, V129, P454, DOI 10.1061/(ASCE)0733-9437(2003)129:6(454)
   Twine TE, 2000, AGR FOREST METEOROL, V103, P279, DOI 10.1016/S0168-1923(00)00123-4
   Wang JF, 2004, WATER RESOUR RES, V40, DOI 10.1029/2004WR003087
   WEBB EK, 1980, Q J ROY METEOR SOC, V106, P85, DOI 10.1002/qj.49710644707
   Zhang B, 2000, WATER RESOUR RES, V36, P753, DOI 10.1029/1999WR900264
NR 49
TC 30
Z9 31
U1 0
U2 9
PD MAY 9
PY 2006
VL 42
IS 5
AR W05412
DI 10.1029/2005WR004317
WC Environmental Sciences; Limnology; Water Resources
DA 2023-11-11
ER

PT C
AU Nuntalid, N
   Dhoble, K
   Kasabov, N
AF Nuntalid, Nuttapod
   Dhoble, Kshitij
   Kasabov, Nikola
BE Lu, BL
   Zhang, LQ
   Kwok, J
TI EEG Classification with BSA Spike Encoding Algorithm and Evolving
   Probabilistic Spiking Neural Network
SO NEURAL INFORMATION PROCESSING, PT I
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 18th International Conference on Neural Information Processing (ICONIP)
CY NOV 13-17, 2011
CL Shanghai, PEOPLES R CHINA
DE Spatio-Temporal Patterns; Electroencephalograms (EEG); Stochastic neuron
   models; evolving probabilistic spiking neural networks
ID FRAMEWORK
AB This study investigates the feasibility of Bens Spike Algorithm (BSA) to encode continuous EEG spatio-temporal data into input spike streams for a classification in a spiking neural network classifier. A novel evolving probabilistic spiking neural network reservoir (epSNNr) architecture is used for the purpose of learning and classifying the EEG signals after the BSA transformation. Experiments are conducted with EEG data measuring a cognitive state of a single individual under 4 different stimuli. A comparison is drawn between using traditional machine learning algorithms and using BSA plus epSNNr, when different probabilistic models of neurons are utilised. The comparison demonstrates that: (1) The BSA is a suitable transformation for EEG data into spike trains; (2) The performance of the epSNNr improves when a probabilistic model of a neuron is used, compared to the use of a deterministic LIF model of a neuron; (3) The classification accuracy of the EEG data in an epSNNr depends on the type of the probabilistic neuronal model used. The results suggest that an epSNNr can be optimised in terms of neuronal models used and parameters that would better match the noise and the dynamics of EEG data. Potential applications of the proposed method for BCI and medical studies are briefly discussed.
C1 [Nuntalid, Nuttapod; Dhoble, Kshitij; Kasabov, Nikola] Auckland Univ Technol, Knowledge Engn & Discovery Res Inst, Private Bag 92006, Auckland 1010, New Zealand.
   [Kasabov, Nikola] Univ Zurich, Inst Neuroinformat, Zurich, Switzerland.
   [Kasabov, Nikola] Univ Zurich, ETH Zurich, Zurich, Switzerland.
RP Nuntalid, N (corresponding author), Auckland Univ Technol, Knowledge Engn & Discovery Res Inst, Private Bag 92006, Auckland 1010, New Zealand.
EM nnuntali@aut.ac.nz; kdhoble@aut.ac.nz; nkasabov@aut.ac.nz
CR Bellas F, 2010, IEEE T AUTON MENT DE, V2, P340, DOI 10.1109/TAMD.2010.2086453
   Buteneers P, 2009, LECT NOTES COMPUT SC, V5506, P56, DOI 10.1007/978-3-642-02490-0_7
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   Goodman DFM, 2009, FRONT NEUROSCI-SWITZ, V3, P192, DOI 10.3389/neuro.01.026.2009
   Indiveri G, 2009, COGN COMPUT, V1, P119, DOI 10.1007/s12559-008-9003-6
   Indiveri G, 2010, IEEE INT SYMP CIRC S, P1951, DOI 10.1109/ISCAS.2010.5536980
   Kasabov N., 2011, IEEE T AUTONOMOUS ME
   Kasabov N., 2007, EVOLVING CONNECTIONI
   Kasabov N, 2010, NEURAL NETWORKS, V23, P16, DOI 10.1016/j.neunet.2009.08.010
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Marcel S, 2007, IEEE T PATTERN ANAL, V29, P743, DOI 10.1109/TPAMI.2007.1012
   Niedermeyer E., 2005, ELECTROENCEPHALOGRAP
   Palaniappan R, 2007, J VLSI SIG PROC SYST, V49, P243, DOI 10.1007/s11265-007-0078-1
   Rutishauser U., 2011, NEURAL COMPUT, P1
   Schliebs S, 2010, LECT NOTES COMPUT SC, V6443, P163, DOI 10.1007/978-3-642-17537-4_21
   Schliebs S, 2010, INT J NEURAL SYST, V20, P481, DOI 10.1142/S0129065710002565
   Schrauwen B, 2003, IEEE IJCNN, P2825
   Tatum W., 2007, HDB EEG INTERPRETATI
NR 18
TC 25
Z9 25
U1 3
U2 8
PY 2011
VL 7062
BP 451
EP +
PN I
WC Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Computer Science, Theory & Methods
DA 2023-11-11
ER

PT J
AU Xu, ZT
   Ma, YM
   Pan, ZK
   Zheng, XY
AF Xu, Zongtang
   Ma, Yumei
   Pan, Zhenkuan
   Zheng, Xiaoyang
TI Deep Spiking Residual Shrinkage Network for Bearing Fault Diagnosis
SO IEEE TRANSACTIONS ON CYBERNETICS
DT Article; Early Access
DE Bearing fault diagnosis; deep learning; soft thresh-olding; spiking
   neural network (SNN)
ID NEURAL-NETWORKS
AB Bearing fault diagnosis of electrical equipment has been a popular research area in recent years because there are often some faults during continuous operation in production due to the harsh working environment. However, the traditional fault signal processing methods rely on highly expert experience, and some parameters are difficult to be optimized by machine-learning methods. Thus, the satisfactory recognition accuracy of fault diagnosis cannot be achieved in the above methods. In this article, a new model based on the spiking neural network (SNN) is proposed, which is called deep the spiking residual shrinkage network (DSRSN) for bearing fault diagnosis. In the model, attention mechanisms and soft thresholding are introduced to improve the recognition rate under a high-level noise background. The higher recognition accuracy is obtained in the proposed model which is tested on the fault signal dataset under different noise intensities. Meanwhile, the training time is about treble as fast as the training time of the artificial neural network, which is reflecting the high efficiency of SNN.
C1 [Xu, Zongtang; Ma, Yumei; Pan, Zhenkuan; Zheng, Xiaoyang] Qingdao Univ, Coll Comp Sci & Technol, Qingdao 266071, Shandong, Peoples R China.
RP Ma, YM (corresponding author), Qingdao Univ, Coll Comp Sci & Technol, Qingdao 266071, Shandong, Peoples R China.
EM 2020025805@qdu.edu.cn; mayumei_qdu@163.com; zkpan@qdu.edu.cn;
   2020025808@qdu.edu.cn
CR Arpaia P, 2020, MEASUREMENT, V151, DOI 10.1016/j.measurement.2019.107126
   Bonnett AH, 2000, IEEE T IND APPL, V36, P1435, DOI 10.1109/28.871294
   Caesarendra W, 2018, APPL SCI-BASEL, V8, DOI 10.3390/app8122656
   Chen ZW, 2022, IEEE T CYBERNETICS, V52, P9157, DOI 10.1109/TCYB.2021.3059002
   Dai XW, 2013, IEEE T IND INFORM, V9, P2226, DOI 10.1109/TII.2013.2243743
   Gao Z., 2015, IEEE T IND ELECTRON, V62, P3768, DOI DOI 10.1109/TIE.2015.2419013
   Han YM, 2022, IEEE T CYBERNETICS, V52, P7504, DOI 10.1109/TCYB.2020.3041850
   Hinton GE., 2008, JMLR, V9, P2579, DOI DOI 10.1007/S10479-011-0841-3
   [胡晓依 HU Xiaoyi], 2008, [振动与冲击, Journal of Vibration and Shock], V27, P82
   Huang DJ, 2023, IEEE T CYBERNETICS, V53, P443, DOI 10.1109/TCYB.2021.3123667
   Kim J, 2018, NEUROCOMPUTING, V311, P373, DOI 10.1016/j.neucom.2018.05.087
   Kugele A., 2021, HYBRID SNN ANN ENERG, V3024, P297
   [李志农 Li Zhinong], 2014, [仪器仪表学报, Chinese Journal of Scientific Instrument], V35, P2423
   Ma S, 2019, MECH SYST SIGNAL PR, V127, P190, DOI 10.1016/j.ymssp.2019.02.055
   O'Donnell P., 1985, IEEE Transactions on Industry Applications, VIA-21, P865, DOI 10.1109/TIA.1985.349533
   Rathore SS, 2017, KNOWL-BASED SYST, V119, P232, DOI 10.1016/j.knosys.2016.12.017
   Rojas A, 2005, MACHINE LEARN SIGN P, P153, DOI 10.1109/MLSP.2005.1532891
   Roy K, 2019, NATURE, V575, P607, DOI 10.1038/s41586-019-1677-2
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   THORSEN OV, 1995, IEEE T IND APPL, V31, P1186, DOI 10.1109/28.464536
   Wu YL, 2020, CATENA, V187, DOI 10.1016/j.catena.2019.104396
   Wu YJ, 2019, AAAI CONF ARTIF INTE, P1311
   [于生宝 Yu Shengbao], 2016, [电工技术学报, Transactions of China Electrotechnical Society], V31, P102
   Zhan QG, 2022, IEEE T CYBERNETICS, V52, P13323, DOI 10.1109/TCYB.2021.3079097
   [张驰 Zhang Chi], 2022, [计算机应用研究, Application Research of Computers], V39, P593
   Zhang PJ, 2011, IEEE T IND APPL, V47, P34, DOI 10.1109/TIA.2010.2090839
   Zhang W, 2019, ISA T, V95, P295, DOI 10.1016/j.isatra.2018.12.025
   Zhang Z, 2020, APPL THERM ENG, V164, DOI 10.1016/j.applthermaleng.2019.114516
   Zhao MH, 2020, IEEE T IND INFORM, V16, P4681, DOI 10.1109/TII.2019.2943898
   Zhu DQ, 2022, IEEE T CYBERNETICS, V52, P9414, DOI 10.1109/TCYB.2021.3055770
NR 32
TC 0
Z9 0
U1 24
U2 46
PD 2022 DEC 20
PY 2022
DI 10.1109/TCYB.2022.3227363
EA DEC 2022
WC Automation & Control Systems; Computer Science, Artificial Intelligence;
   Computer Science, Cybernetics
DA 2023-11-11
ER

PT J
AU Liu, JX
   Lu, H
   Luo, YL
   Yang, S
AF Liu, Junxiu
   Lu, Hao
   Luo, Yuling
   Yang, Su
TI Spiking neural network-based multi-task autonomous learning for mobile
   robots
SO ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE
DT Article
DE Autonomous learning; Reinforcement learning; Spiking neural networks;
   Reward signals
ID SYNAPTIC PLASTICITY; LATERAL INHIBITION; TRACKING CONTROL; MODULATION;
   SPEED; STDP
AB Spiking Neural Networks (SNNs) are the new generation of artificial neural networks that closely mimic the time encoding and information processing aspects of the human brain. In this work, a multi-task autonomous learning paradigm is proposed for the mobile robot application, which employs a SNN to construct the controlling system of the mobile robot. The Reward-modulated Spiking-time-dependent Plasticity learning rule is developed for the SNN-based controller, which aims to achieve the capability of autonomous learning under multiple tasks. Reward signals are generated based on the instantaneous frequencies of pre- and post-synaptic spikes, which adapts to the sensory stimuli and environmental feedback. Meanwhile, inspired by lateral inhibition connections, a task switch mechanism is designed to enable the controller to switch the operations between multiple tasks. Two tasks of obstacle avoidance and target tracking are used for performance evaluation and results demonstrate that the mobile robot with the proposed paradigm is able to autonomously learn, switch and complete the tasks.
C1 [Liu, Junxiu; Lu, Hao; Luo, Yuling] Guangxi Normal Univ, Sch Elect Engn, Guilin, Peoples R China.
   [Yang, Su] Univ West London, Sch Comp & Engn, London, England.
RP Luo, YL (corresponding author), Guangxi Normal Univ, Sch Elect Engn, Guilin, Peoples R China.
EM yuling0616@gxnu.edu.cn
CR Belta C, 2007, IEEE ROBOT AUTOM MAG, V14, P61, DOI 10.1109/MRA.2007.339624
   Bing ZS, 2019, FRONT NEUROROBOTICS, V13, DOI 10.3389/fnbot.2019.00018
   Bing ZS, 2018, IEEE INT CONF ROBOT, P4725
   Bohte S. M., 2000, 8th European Symposium on Artificial Neural Networks. ESANN"2000. Proceedings, P419
   Borst A, 1999, NAT NEUROSCI, V2, P947, DOI 10.1038/14731
   Burkitt AN, 2006, BIOL CYBERN, V95, P1, DOI 10.1007/s00422-006-0068-6
   Cao CS, 2018, AAAI CONF ARTIF INTE, P6690
   Cao ZQ, 2015, NEURAL COMPUT APPL, V26, P1839, DOI 10.1007/s00521-015-1848-5
   Clawson TS, 2016, IEEE DECIS CONTR P, P3381, DOI 10.1109/CDC.2016.7798778
   Farooq Umar, 2014, International Journal of Computer and Electrical Engineering, V6, P83, DOI 10.7763/IJCEE.2014.V6.799
   Farsa EZ, 2019, IEEE T CIRCUITS-II, V66, P1582, DOI 10.1109/TCSII.2019.2890846
   Fernandes BJT, 2013, IEEE T CYBERNETICS, V43, P2082, DOI 10.1109/TCYB.2013.2240295
   Florian RV, 2007, NEURAL COMPUT, V19, P1468, DOI 10.1162/neco.2007.19.6.1468
   Gu DB, 2002, ROBOT AUTON SYST, V39, P73, DOI 10.1016/S0921-8890(02)00172-0
   HARTLINE HK, 1956, J GEN PHYSIOL, V39, P651, DOI 10.1085/jgp.39.5.651
   Huang CJ, 2019, INT J ROBOT AUTOM, V34, P84, DOI 10.2316/J.2019.206-5422
   Huang YC, 2021, NEUROCOMPUTING, V423, P336, DOI 10.1016/j.neucom.2020.10.106
   Izhikevich EM, 2007, CEREB CORTEX, V17, P2443, DOI 10.1093/cercor/bhl152
   Junxiu Liu, 2020, 2020 IEEE 22nd International Conference on High Performance Computing and Communications; IEEE 18th International Conference on Smart City; IEEE 6th International Conference on Data Science and Systems (HPCC/SmartCity/DSS), P593, DOI 10.1109/HPCC-SmartCity-DSS50907.2020.00075
   Kayser C, 2009, NEURON, V61, P597, DOI 10.1016/j.neuron.2009.01.008
   Liu F, 2012, OPTIK, V123, P1955, DOI 10.1016/j.ijleo.2011.09.052
   Liu JX, 2019, NEUROCOMPUTING, V331, P473, DOI 10.1016/j.neucom.2018.11.078
   Liu JX, 2019, IEEE T NEUR NET LEAR, V30, P865, DOI 10.1109/TNNLS.2018.2854291
   Liu JX, 2018, IEEE T NEUR NET LEAR, V29, P1287, DOI 10.1109/TNNLS.2017.2673021
   Loiselle S, 2005, IEEE IJCNN, P2076
   Luo YL, 2018, NEURAL PROCESS LETT, V48, P1777, DOI 10.1007/s11063-018-9797-5
   Luo YL, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00857
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Mahadevuni A, 2017, IEEE IJCNN, P2243, DOI 10.1109/IJCNN.2017.7966127
   Martínez-García M, 2021, IEEE T CYBERNETICS, V51, P3913, DOI 10.1109/TCYB.2020.3020217
   Martínez-García M, 2021, HUM FACTORS, V63, P210, DOI 10.1177/0018720819881008
   Martínez-García M, 2018, IEEE SYS MAN CYBERN, P1245, DOI 10.1109/SMC.2018.00218
   Morrison A, 2008, BIOL CYBERN, V98, P459, DOI 10.1007/s00422-008-0233-1
   Nagy I, 2014, ACTA POLYTECH HUNG, V11, P39
   Neftci EO, 2019, NAT MACH INTELL, V1, P133, DOI 10.1038/s42256-019-0025-4
   Nichols E, 2013, IEEE T CYBERNETICS, V43, P115, DOI 10.1109/TSMCB.2012.2200674
   Otmakhova NA, 1996, J NEUROSCI, V16, P7478
   Rafiq F, 2012, 2012 INT C EM TECHN, P1, DOI [10.1109/ICET.2012.6375453, DOI 10.1109/ICET.2012.6375453]
   Raja P., 2018, International Journal of Advanced Intelligence Paradigms, V11, P348
   Rausch V, 2017, P AMER CONTR CONF, P4914, DOI 10.23919/ACC.2017.7963716
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Rusu P, 2003, IEEE T INSTRUM MEAS, V52, P1335, DOI 10.1109/TIM.2003.816846
   Saha I, 2014, IEEE INT C INT ROBOT, P1525, DOI 10.1109/IROS.2014.6942758
   Schemmel J, 2006, IEEE IJCNN, P1
   Schrauwen B, 2003, IEEE IJCNN, P2825
   Seamans JK, 2004, PROG NEUROBIOL, V74, P1, DOI 10.1016/j.pneurobio.2004.05.006
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Tang Guangzhi, 2018, P INT C NEUROMORPHIC, DOI [10.1145/3229884.3229888, DOI 10.1145/3229884.3229888]
   Thorpe S, 1996, NATURE, V381, P520, DOI 10.1038/381520a0
   van Rossum MCW, 2000, J NEUROSCI, V20, P8812
   Vitanza A, 2015, J FRANKLIN I, V352, P3122, DOI 10.1016/j.jfranklin.2015.04.014
   Wai RJ, 2009, IEEE T IND ELECTRON, V56, P2667, DOI 10.1109/TIE.2009.2020077
   Wan L, 2017, MICROPROCESS MICROSY, V53, P21, DOI 10.1016/j.micpro.2017.07.005
   Zapata O, 2015, IEEE LAT AM T, V13, P18, DOI 10.1109/TLA.2015.7040623
NR 54
TC 6
Z9 7
U1 7
U2 22
PD SEP
PY 2021
VL 104
AR 104362
DI 10.1016/j.engappai.2021.104362
EA JUL 2021
WC Automation & Control Systems; Computer Science, Artificial Intelligence;
   Engineering, Multidisciplinary; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Plesa, MI
   Gheoghe, M
   Ipate, F
   Zhang, GX
AF Plesa, Mihail-Iulian
   Gheoghe, Marian
   Ipate, Florentin
   Zhang, Gexiang
TI A key agreement protocol based on spiking neural P systems with
   anti-spikes
SO JOURNAL OF MEMBRANE COMPUTING
DT Article
DE Spiking neural P system; Anti-spikes; Tree parity machine; Cryptography;
   Key agreement protocol
ID REPRESENTATION; INFORMATION
AB Spiking neural P systems, SN P systems for short, have found various applications over time. Perhaps the most important application to date is in the area of artificial intelligence where SN P systems are significant models of the third generation of neural networks. Another application of SN P systems that has not been researched much is cryptography. SN P systems can be used as computational devices on which various cryptographic algorithms can be implemented. Many of the machine learning algorithms that are applied in cryptography are based on neural networks which can be implemented using SN P systems. In this paper, we propose a new type of SN P system called Anti-Spiking Neural Tree Parity Machine. The system is inspired by how a Tree Parity Machine works and is constructed using SN P systems with anti-spikes. Based on the new system, we propose a novel key agreement protocol that allows two parties to communicate over a public channel and obtain a secret shared key. We perform multiple experiments in which we show the efficiency of our protocol and its security.
C1 [Plesa, Mihail-Iulian; Ipate, Florentin] Univ Bucharest, Dept Comp Sci, Bucharest, Romania.
   [Gheoghe, Marian] Univ Bradford, Sch Elect Engn & Comp Sci, Bradford, England.
   [Zhang, Gexiang] Chengdu Univ Informat Technol, Sch Automat, Chengdu 610225, Peoples R China.
RP Plesa, MI (corresponding author), Univ Bucharest, Dept Comp Sci, Bucharest, Romania.
EM mihail-iulian.plesa@s.unibuc.ro
CR Adorna HN, 2020, J MEMBRANE COMPUT, V2, P230, DOI 10.1007/s41965-020-00059-7
   ALHAZOV A, 2006, APPL MEMBRANE COMPUT, P215
   Alhazov A, 2021, J MEMBRANE COMPUT, V3, P1, DOI 10.1007/s41965-020-00068-6
   Allam Ahmed M., 2009, Proceedings 2009 International Joint Conference on Neural Networks (IJCNN 2009 - Atlanta), P121, DOI 10.1109/IJCNN.2009.5178851
   Allam AM, 2010, IEEE T NEURAL NETWOR, V21, P1915, DOI 10.1109/TNN.2010.2079948
   Bao TT, 2020, J MEMBRANE COMPUT, V2, P255, DOI 10.1007/s41965-020-00060-0
   Carandang JPA, 2017, ROM J INF SCI TECH, V20
   Cavaliere M, 2009, THEOR COMPUT SCI, V410, P2352, DOI 10.1016/j.tcs.2009.02.031
   Chen YH, 2021, INT J UNCONV COMPUT, V16, P173
   Díaz-Pernil D, 2019, J MEMBRANE COMPUT, V1, P58, DOI 10.1007/s41965-018-00002-x
   Dong T, 2020, IEEE T NEUR NET LEAR, V31, P4999, DOI 10.1109/TNNLS.2019.2955165
   Dupaya AGS, 2022, J MEMBRANE COMPUT, V4, P21, DOI 10.1007/s41965-022-00092-8
   Fernandez ADC, 2021, PROCESSES, V9, DOI 10.3390/pr9010072
   Ganbaatar G, 2021, J MEMBRANE COMPUT, V3, P22, DOI 10.1007/s41965-021-00073-3
   García-Quismondo M, 2010, LECT NOTES COMPUT SC, V5957, P264, DOI 10.1007/978-3-642-11467-0_20
   Gheorghe M, 2021, J MEMBRANE COMPUT, V3, P133, DOI 10.1007/s41965-021-00075-1
   Hinze T, 2020, J MEMBRANE COMPUT, V2, P121, DOI 10.1007/s41965-020-00041-3
   Hoffstein Jeffrey, 2008, INTRO MATH CRYPTOGRA, V1
   Ionescu M, 2006, FUND INFORM, V71, P279
   Ipate F., 2013, ADV INTELLIGENT SYST, P1081, DOI DOI 10.1007/978-3-642-37502-6_126
   Javurek M., 2016, 2016 NEW TRENDS SIGN, P1, DOI DOI 10.1109/NTSP.2016.7747782
   Jeong S, 2021, SECUR COMMUN NETW, V2021, DOI 10.1155/2021/6680782
   Kanter I, 2002, EUROPHYS LETT, V57, P141, DOI 10.1209/epl/i2002-00552-9
   Klein E., 2004, ADV NEURAL INFORM PR, V17
   Lv ZQ, 2021, J MEMBRANE COMPUT, V3, P270, DOI 10.1007/s41965-021-00089-9
   Martín-Vide C, 2003, THEOR COMPUT SCI, V296, P295, DOI 10.1016/S0304-3975(02)00659-X
   Mayne R, 2019, J MEMBRANE COMPUT, V1, P20, DOI 10.1007/s41965-018-00006-7
   Mi SH, 2021, J MEMBRANE COMPUT, V3, P284, DOI 10.1007/s41965-021-00087-x
   MICHEL O, 2006, APPL MEMBRANE COMPUT, P283
   Mislovaty R, 2002, PHYS REV E, V66, DOI 10.1103/PhysRevE.66.066102
   Pan LQ, 2017, INT J NEURAL SYST, V27, DOI 10.1142/S0129065717500423
   Pan LQ, 2012, NEURAL COMPUT, V24, P805, DOI 10.1162/NECO_a_00238
   Pan LQ, 2009, INT J COMPUT COMMUN, V4, P273, DOI 10.15837/ijccc.2009.3.2435
   Päun G, 2000, J COMPUT SYST SCI, V61, P108, DOI 10.1006/jcss.1999.1693
   Dorokhin ÉS, 2019, SECUR COMMUN NETW, DOI 10.1155/2019/8214681
   Shor PW, 1997, SIAM J COMPUT, V26, P1484, DOI 10.1137/S0036144598347011
   Song T, 2019, IEEE T NANOBIOSCI, V18, P176, DOI 10.1109/TNB.2019.2896981
   Song T, 2018, IEEE T COGN DEV SYST, V10, P1106, DOI 10.1109/TCDS.2017.2785332
   Song XX, 2018, BIOSYSTEMS, V169, P13, DOI 10.1016/j.biosystems.2018.05.004
   Stypinski M., 2021, ARXIV
   Valencia-Cabrera L, 2020, J MEMBRANE COMPUT, V2, P95, DOI 10.1007/s41965-020-00037-z
   Valencia-Cabrera L, 2020, J MEMBRANE COMPUT, V2, P392, DOI 10.1007/s41965-020-00056-w
   Wang HF, 2020, INT J UNCONV COMPUT, V15, P37
   Wang X, 2017, ROM J INF SCI TECH, V20
   Wu TF, 2021, J MEMBRANE COMPUT, V3, P221, DOI 10.1007/s41965-020-00069-5
   Wu Tingfang, 2018, IEEE Trans Neural Netw Learn Syst, V29, P3349, DOI 10.1109/TNNLS.2017.2726119
   Yahya R.I., 2016, INT C BIOINSPIRED CO, V681, P314
   Zandron C, 2001, DISCRETE MATH & THEO, P289
   Zhang GX, 2022, INT J NEURAL SYST, V32, DOI 10.1142/S012906572250023X
   Zhang XL, 2022, ENTROPY-SWITZ, V24, DOI 10.3390/e24040568
NR 50
TC 2
Z9 2
U1 4
U2 6
PD DEC
PY 2022
VL 4
IS 4
BP 341
EP 351
DI 10.1007/s41965-022-00110-9
EA DEC 2022
WC Computer Science, Theory & Methods
DA 2023-11-11
ER

PT J
AU Muhammad, Y
   Steuber, V
   Wróbel, B
AF Muhammad, Yaqoob
   Steuber, Volker
   Wrobel, Borys
TI Spiking neural networks as finite state transducers for temporal pattern
   recognition
SO JOURNAL OF COMPUTATIONAL NEUROSCIENCE
DT Meeting Abstract
C1 [Muhammad, Yaqoob] Univ Hertfordshire, Comp Sci, Hatfield, England.
   [Steuber, Volker] Univ Hertfordshire, Biocomputat Res Grp, Hatfield, England.
   [Wrobel, Borys] Adam Mickiewicz Univ, Evolving Syst Lab, Poznan, Poland.
EM m.yaqoob3@herts.ac.uk
CR ALUR R, 1994, THEOR COMPUT SCI, V126, P183, DOI 10.1016/0304-3975(94)90010-8
   De Maria E, 2020, NAT COMPUT, V19, P135, DOI 10.1007/s11047-019-09727-9
   Yaqoob M, 2018, 2018 CONFERENCE ON ARTIFICIAL LIFE (ALIFE 2018), P665
NR 3
TC 0
Z9 0
U1 0
U2 0
PD JAN
PY 2023
VL 51
SU 1
MA P115
BP S96
EP S97
WC Mathematical & Computational Biology; Neurosciences
DA 2023-11-11
ER

PT C
AU Eskandari, E
   Ahmadi, A
   Gomar, S
   Ahmadi, M
   Saif, M
AF Eskandari, Elahe
   Ahmadi, Arash
   Gomar, Shaghayegh
   Ahmadi, Majid
   Saif, Mehrdad
GP IEEE
TI Evolving Spiking Neural Networks of Artificial Creatures Using Genetic
   Algorithm
SO 2016 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUL 24-29, 2016
CL Vancouver, CANADA
AB This paper presents a Genetic Algorithm (GA) based evolution framework in which Spiking Neural Network (SNN) of single or a colony of artificial creatures are evolved for higher chance of survival in a virtual environment. The artificial creatures are composed of randomly connected Izhikevich spiking reservoir neural networks. Inspired by biological neurons, the neuronal connections are considered with different axonal conduction delays. Simulation results prove that the evolutionary algorithm has the capability to find or synthesis artificial creatures which can survive in the environment successfully and also simulations verify that colony approach has a better performance in comparison with a single complex creature.
C1 [Eskandari, Elahe; Ahmadi, Arash; Gomar, Shaghayegh; Ahmadi, Majid; Saif, Mehrdad] Univ Windsor, Windsor, ON, Canada.
RP Eskandari, E (corresponding author), Univ Windsor, Windsor, ON, Canada.
CR [Anonymous], 1989, GENETIC ALGORITHM SE
   [Anonymous], 2001, EVOLUTIONARY ROBOTIC
   [Anonymous], 1992, ARTIFICIAL NEURAL NE
   [Anonymous], 1998, INTRO GENETIC ALGORI
   [Anonymous], 1991, HDB GENETIC ALGORITH
   [Anonymous], 2005, NEURAL NETWORKS METH, DOI DOI 10.1007/3-540-28847-3
   Bailey JA, 2011, NEUROCOMPUTING, V74, P2392, DOI 10.1016/j.neucom.2011.04.001
   Baldassarre G., 2007, IEEE T SYST MAN CYB, V37
   Brbisson A.D., 2012, THESIS
   Brunel N., 2007, BIOL CYBERN, V97
   Cliff D, 1992, ISSUES EVOLUTIONARY
   Coley D.A., 1998, INTRO GENETIC ALGORI
   Colley M., 2002, P 2004 IEEE INT C RO, P4620
   Gerstner W., 2002, SPIKING NEURON MODEL
   Hodgkin A.L., 1952, PHYSL J, V117
   HOLLAND J, 1998, EMERGENCE
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Jin X., 2010, PARALLEL SIMULATION
   Judd J. S., 1990, NEURAL NETWORK DESIG
   Kasabov N, 2010, NEURAL NETWORKS, V23, P16, DOI 10.1016/j.neunet.2009.08.010
   Moutarde F., 2008, P IEEE S ART LIF C
   Oros N., 2009, P IEEE S ART LIF C
   Paugam-Moisy H, 2010, HDB NATURAL COMPUTIN
   Paugam-Moisy H, 2008, NEUROCOMPUTING, V71, P1143, DOI 10.1016/j.neucom.2007.12.027
   Poli R., 2008, FIELD GUIDE GENETIC
   Saggie-Wexler K, 2006, ARTIF LIFE, V12, P1, DOI 10.1162/106454606775186428
   Schrauwen B., 2007, P 15 EUR S ADV COMP
   Soares G., 2010, P 2010 11 BRAZ S NEU, P4348
   Su Dan, 2010, THESIS
   Urbanczik R, 2009, NAT NEUROSCI, V12, P250, DOI 10.1038/nn.2264
   Vreeken J, 2003, SPIKING NEURAL NETWO
NR 32
TC 6
Z9 6
U1 0
U2 0
PY 2016
BP 411
EP 418
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Hwang, S
   Yu, J
   Lee, GH
   Song, MS
   Chang, J
   Min, KK
   Jang, T
   Lee, JH
   Park, BG
   Kim, H
AF Hwang, Sungmin
   Yu, Junsu
   Lee, Geun Ho
   Song, Min Suk
   Chang, Jeesoo
   Min, Kyung Kyu
   Jang, Taejin
   Lee, Jong-Ho
   Park, Byung-Gook
   Kim, Hyungjin
TI Capacitor-Based Synaptic Devices for Hardware Spiking Neural Networks
SO IEEE ELECTRON DEVICE LETTERS
DT Article
DE Capacitor-based synaptic device; capacitive neural network; MOS
   capacitor; NAND flash memory; neuromorphic system; spiking neural
   network (SNN)
AB In this work, we present a hardware neural network with capacitor-based synaptic devices. A capacitor-based synaptic device was developed using a MOS capacitor structure with a charge trapping layer. Due to the flat band voltage shift by charge trapping and its non-linear C- V characteristics, multilevel weight values could be implemented by the charge occurring when charging and discharging the capacitor. The vector-matrix multiplication (VMM) function was also experimentally verified using a fabricated synapse array based on NAND flash structure.
C1 [Hwang, Sungmin; Yu, Junsu; Chang, Jeesoo; Min, Kyung Kyu; Jang, Taejin; Lee, Jong-Ho; Park, Byung-Gook] Seoul Natl Univ, Interuniv Semicond Res Ctr, Seoul 08826, South Korea.
   [Hwang, Sungmin; Yu, Junsu; Chang, Jeesoo; Min, Kyung Kyu; Jang, Taejin; Lee, Jong-Ho; Park, Byung-Gook] Seoul Natl Univ, Dept Elect Engn & Comp Sci, Seoul 08826, South Korea.
   [Lee, Geun Ho; Song, Min Suk; Kim, Hyungjin] Inha Univ, Dept Elect Engn, Incheon 22212, South Korea.
RP Park, BG (corresponding author), Seoul Natl Univ, Interuniv Semicond Res Ctr, Seoul 08826, South Korea.; Park, BG (corresponding author), Seoul Natl Univ, Dept Elect Engn & Comp Sci, Seoul 08826, South Korea.; Kim, H (corresponding author), Inha Univ, Dept Elect Engn, Incheon 22212, South Korea.
EM bgpark@snu.ac.kr; hkim@inha.ac.kr
CR Burr GW, 2017, ADV PHYS-X, V2, P89, DOI 10.1080/23746149.2016.1259585
   Chen W, 2019, IEEE INT MEM WORKSH, P95, DOI 10.1109/imw.2019.8739707
   Chicca E, 2020, APPL PHYS LETT, V116, DOI 10.1063/1.5142089
   Choi HS, 2020, IEEE ELECTR DEVICE L, V41, P1653, DOI 10.1109/LED.2020.3025587
   Choi W, 2021, IEEE ELECTR DEVICE L, V42, P763, DOI 10.1109/LED.2021.3065367
   CILINGIROGLU U, 1991, IEEE T CIRCUITS SYST, V38, P210, DOI 10.1109/31.68299
   Engeler W. E, 1991, U.S. Patent, Patent No. 5039871
   Guo X., 2017, IEDM, P6
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Ielmini D, 2020, NANOTECHNOLOGY, V31, DOI 10.1088/1361-6528/ab554b
   Jeong DS, 2016, ADV ELECTRON MATER, V2, DOI 10.1002/aelm.201600090
   Jeong Y, 2018, IEEE T NANOTECHNOL, V17, P184, DOI 10.1109/TNANO.2017.2784364
   Kim H, 2021, NAT COMMUN, V12, DOI 10.1038/s41467-021-25455-0
   Kim H, 2018, IEEE ELECTR DEVICE L, V39, P630, DOI 10.1109/LED.2018.2809661
   Kim H, 2017, NANOTECHNOLOGY, V28, DOI 10.1088/1361-6528/aa86f8
   Kim H, 2016, J SEMICOND TECH SCI, V16, P657, DOI 10.5573/JSTS.2016.16.5.657
   Kim J., ADV ELECTRON MATER, V6
   Kim SK, 2020, IEEE ELECTR DEVICE L, V41, P605, DOI 10.1109/LED.2020.2971321
   Kim TH, 2021, CHAOS SOLITON FRACT, V153, DOI 10.1016/j.chaos.2021.111587
   Kim TH, 2021, NANOTECHNOLOGY, V32, DOI 10.1088/1361-6528/abf0cc
   Kim TH, 2020, APPL PHYS LETT, V117, DOI 10.1063/5.0021626
   Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386
   Kwon D, 2020, IEEE ELECTR DEVICE L, V41, P493, DOI 10.1109/LED.2020.2969695
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee ST, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.571292
   Lee ST, 2019, IEEE J ELECTRON DEVI, V7, P1085, DOI 10.1109/JEDS.2019.2947316
   Luo Y, 2021, IEDM
   Merrikh Bayat, 2016, PROC DRC, P1
   Nawrocki RA, 2016, IEEE T ELECTRON DEV, V63, P3819, DOI 10.1109/TED.2016.2598413
   Prezioso M, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-07757-y
   ROSENBLATT F, 1958, PSYCHOL REV, V65, P386, DOI 10.1037/h0042519
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707
   Shibayama Y, 2021, IEEE ELECTR DEVICE L, V42, P1014, DOI 10.1109/LED.2021.3082083
   Suh K.-D., IEEE J SOLID-ST CIRC
   Upadhyay NK, 2019, ADV MATER TECHNOL-US, V4, DOI 10.1002/admt.201800589
   Zhou JT, 2014, IEEE T ELECTRON DEV, V61, P1369, DOI 10.1109/TED.2014.2310200
NR 36
TC 14
Z9 14
U1 3
U2 13
PD APR
PY 2022
VL 43
IS 4
BP 549
EP 552
DI 10.1109/LED.2022.3149029
WC Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Dezelak, K
   Stumberger, G
   Klopcic, B
   Dolinar, D
   Pihler, J
AF Dezelak, Klemen
   Stumberger, Gorazd
   Klopcic, Beno
   Dolinar, Drago
   Pihler, Joze
TI Iron core saturation detector supplemented by an artificial neural
   network
SO PRZEGLAD ELEKTROTECHNICZNY
DT Article
DE iron-core saturation; artificial neural network; transformer
AB This work presents an algorithm for detection of saturation level in the iron core of a welding transformer supplemented by an artificial neural network. The DC component in the iron core flux density can cause saturation of the transformer's iron core which can cause current spikes in the transformers primary current. In order to prevent current spikes the iron core saturation level detection is needed. In this work the iron core saturation level detection is based on dynamic inductances and an artificial neural network as a supplement to the existing method.
C1 [Dezelak, Klemen; Stumberger, Gorazd; Dolinar, Drago; Pihler, Joze] Univ Maribor, Fac Elect Engn & Comp Sci, SLO-2000 Maribor, Slovenia.
   [Klopcic, Beno] Indramat Elektromotorji Doo, Skofja Loka 4220, Slovenia.
RP Dezelak, K (corresponding author), Univ Maribor, Fac Elect Engn & Comp Sci, Smetanova Ulica 17, SLO-2000 Maribor, Slovenia.
EM klemen.dezelak@uni-mb.si; gorazd.stumberger@uni-mb.si;
   beno.klopcic@boschrexroth.si; dolinar@uni-mb.si; joze.pihler@uni-mb.si
CR BROWN BM, 1987, WELD J, V66, P18
   DEZELAK K, 2006, THESIS U MARIBOR
   Dezelak K, 2008, J MAGN MAGN MATER, V320, pE878, DOI 10.1016/j.jmmm.2008.04.099
   KLOPCIC B, 2005, THESIS U MARIBOR
   Klopcic B, 2008, IEEE T POWER ELECTR, V23, P144, DOI 10.1109/TPEL.2007.911851
   LI W, 2004, SHEET MET WELD C STE, V11
   Pihler J, 1997, IEEE T POWER DELIVER, V12, P1128, DOI 10.1109/61.636919
   Stumberger G, 2005, IEEE T MAGN, V41, P4030, DOI 10.1109/TMAG.2005.854992
NR 8
TC 4
Z9 4
U1 0
U2 1
PY 2008
VL 84
IS 12
BP 157
EP 159
WC Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Saunders, DJ
   Patel, D
   Hazan, H
   Siegelmann, HT
   Kozma, R
AF Saunders, Daniel J.
   Patel, Devdhar
   Hazan, Hananel
   Siegelmann, Hava T.
   Kozma, Robert
TI Locally connected spiking neural networks for unsupervised feature
   learning
SO NEURAL NETWORKS
DT Article
DE Spiking neural network; Machine learning; Spike-timing-dependent
   plasticity; BindsNET; Unsupervised learning; Image classification
AB In recent years, spiking neural networks (SNNs) have demonstrated great success in completing various machine learning tasks. We introduce a method for learning image features with locally connected layers in SNNs using a spike-timing-dependent plasticity (STDP) rule. In our approach, sub-networks compete via inhibitory interactions to learn features from different locations of the input space. These locally-connected SNNs (LC-SNNs) manifest key topological features of the spatial interaction of biological neurons. We explore a biologically inspired n-gram classification approach allowing parallel processing over various patches of the image space. We report the classification accuracy of simple two-layer LC-SNNs on two image datasets, which respectively match state-of-art performance and are the first results to date. LC-SNNs have the advantage of fast convergence to a dataset representation, and they require fewer learnable parameters than other SNN approaches with unsupervised learning. Robustness tests demonstrate that LC-SNNs exhibit graceful degradation of performance despite the random deletion of large numbers of synapses and neurons. Our results have been obtained using the BindsNET library, which allows efficient machine learning implementations of spiking neural networks. (C) 2019 Elsevier Ltd. All rights reserved.
C1 [Saunders, Daniel J.; Patel, Devdhar; Hazan, Hananel; Siegelmann, Hava T.; Kozma, Robert] Univ Massachusetts, Biol Inspired Neural & Dynam Syst Lab BINDS, Coll Comp & Informat Sci, 140 Governors Dr, Amherst, MA 01003 USA.
   [Kozma, Robert] Univ Memphis, Dept Math, Ctr Large Scale Intelligent Optimizat & Networks, Memphis, TN 38152 USA.
RP Saunders, DJ; Kozma, R (corresponding author), Univ Massachusetts, Biol Inspired Neural & Dynam Syst Lab BINDS, Coll Comp & Informat Sci, 140 Governors Dr, Amherst, MA 01003 USA.
EM djsaunde@cs.umass.edu; devdharpatel@cs.umass.edu; hhazan@cs.umass.edu;
   hava@cs.umass.edu; rkozma@cs.umass.edu
CR Allred JM, 2016, IEEE IJCNN, P2492, DOI 10.1109/IJCNN.2016.7727509
   [Anonymous], ARXIV E PRINTS
   Bekolay T, 2014, FRONT NEUROINFORM, V7, DOI 10.3389/fninf.2013.00048
   Burges, 2010, MNIST HANDWRITTEN DI, DOI DOI 10.1561/2400000035
   Carnevale N.T., 2006, NEURON BOOK, DOI [DOI 10.1017/CBO9780511541612, 10.1017/CBO9780511541612]
   Chen Y.-H., 2015, P INTERSPEECH, P1
   Cornelis H, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0029018
   Dan Goodman F. M., 2009, FRONTIERS NEUROSCIEN, V3, DOI [10.3389/neuro.01.026.2009/full, DOI 10.3389/NEURO.01.026.2009/FULL]
   Demin V, 2018, FRONT NEUROINFORM, V12, DOI 10.3389/fninf.2018.00079
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Diehl Peter U, 2015, 2015 INT JOINT C NEU, P1, DOI [DOI 10.1109/IJCNN.2015.7280696, 10.1109/IJCNN.2015.7280696]
   Florian RV, 2007, NEURAL COMPUT, V19, P1468, DOI 10.1162/neco.2007.19.6.1468
   Gewaltig M-O., 2007, SCHOLARPEDIA, V2, DOI [10.4249/scholarpedia.1430, DOI 10.4249/SCHOLARPEDIA.1430]
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Goodman D. F. M., 2009, FRONTIERS COMPUTATIO
   Gross CG, 2002, NEUROSCIENTIST, V8, P512, DOI 10.1177/107385802237175
   Hazan H, 2018, FRONT NEUROINFORM, V12, DOI 10.3389/fninf.2018.00089
   Kasabov NK, 2014, NEURAL NETWORKS, V52, P62, DOI 10.1016/j.neunet.2014.01.006
   Kauderer-Abrams E., 2016, CORR
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Lee C, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/TCDS.2018.2833071
   Lee C, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00435
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Mozafari M., 2018, IEEE T NEURAL NETWOR
   Panda P., 2016, CORR
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707
   Saunders D. J., 2018, 2018 INT JOINT C NEU, P1
   Sengupta A., 2018, CORR
   Siegelmann H, 2018, 2018 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2018.8489673
   Tavanaei A., 2018, CORR
   Tavanaei A., 2015, MINIMAL SPIKING NEUR
   Tavanaei A., 2016, CORR
   Tavanaei A, 2018, 2018 INT JOINT C NEU, DOI DOI 10.1109/IJCNN.2018.8489104
   Tavanaei A, 2017, IEEE IJCNN, P2023, DOI 10.1109/IJCNN.2017.7966099
   Vitay J, 2015, FRONT NEUROINFORM, V9, DOI 10.3389/fninf.2015.00019
   Zambrano D., 2017, CORR
   Zappacosta S, 2018, PLOS COMPUT BIOL, V14, DOI 10.1371/journal.pcbi.1006227
NR 38
TC 32
Z9 35
U1 0
U2 25
PD NOV
PY 2019
VL 119
BP 332
EP 340
DI 10.1016/j.neunet.2019.08.016
WC Computer Science, Artificial Intelligence; Neurosciences
DA 2023-11-11
ER

PT C
AU THORPE, SJ
AF THORPE, SJ
BE ECKMILLER, R
   HARTMANN, G
   HAUSKE, G
TI SPIKE ARRIVAL TIMES - A HIGHLY EFFICIENT CODING SCHEME FOR NEURAL
   NETWORKS
SO PARALLEL PROCESSING IN NEURAL SYSTEMS AND COMPUTERS
DT Proceedings Paper
CT 10TH CYBERNETICS CONGRESS OF THE DEUTSCHE-GESELLSCHAFT-FUR-KYBERNETIK :
   INTERNATIONAL CONF ON PARALLEL PROCESSING IN NEURAL SYSTEMS AND
   COMPUTERS ( ICNC )
CY MAR 19-21, 1990
CL DUSSELDORF, FED REP GER
RP THORPE, SJ (corresponding author), UNIV PARIS 06,INST NEUROSCI,DEPT NEUROSCI VIS,QUAI ST BERNARD,F-75230 PARIS 05,FRANCE.
NR 0
TC 114
Z9 116
U1 0
U2 3
PY 1990
BP 91
EP 94
WC Engineering, Electrical & Electronic; Neurosciences; Optics; Physics,
   Applied
DA 2023-11-11
ER

PT C
AU Philipp, S
   Grübl, A
   Meier, K
   Schemmel, J
AF Philipp, Stefan
   Gruebl, Andreas
   Meier, Karlheinz
   Schemmel, Johannes
BE Sandoval, F
   Prieto, A
   Cabestany, J
   Grana, M
TI Interconnecting VLSI spiking neural networks using isochronous
   connections
SO COMPUTATIONAL AND AMBIENT INTELLIGENCE
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 9th International Work-Conference on Artificial Neural Networks
CY JUN 20-22, 2007
CL San Sebastian, SPAIN
AB This paper presents a network architecture to interconnect mixed-signal VLSI1 integrate-and-fire neural networks in a way that the timing of the neural network data is preserved. The architecture uses isochronous connections to reserve network bandwidth and is optimized for the small data event packets that have to be exchanged in spiking hardware neural networks. End-to-end delay is reduced to the minimum by retaining 100 % throughput. As buffering is avoided wherever possible, the resulting jitter is independent of the number of neural network chips used. This allows to experiment with neural networks of thousands of artificial neurons with a speedup of up to 10(5) compared to biology. Simulation results are presented. The work focuses on the interconnection of hardware neural networks. In addition to this, the proposed architecture is suitable for any application where bandwidth requirements are known and constant low delay is needed.
C1 [Philipp, Stefan; Gruebl, Andreas; Meier, Karlheinz; Schemmel, Johannes] Heidelberg Univ, Kirchhoff Inst Phys, Neuenheimer Feld 227, D-69120 Heidelberg, Germany.
RP Philipp, S (corresponding author), Heidelberg Univ, Kirchhoff Inst Phys, Neuenheimer Feld 227, D-69120 Heidelberg, Germany.
EM sphilipp@kip.uni-heidelberg.de
CR BRELAZ D, 1979, COMMUN ACM, V22, P251, DOI 10.1145/359094.359101
   EHRLICH M, 2007, P IEEE SSD07 HAMM TU
   Fieres J., 2004, P 2004 BRAIN INSP CO
   Hung A, 1998, THIRD IEEE SYMPOSIUM ON COMPUTERS AND COMMUNICATIONS, PROCEEDINGS, P331, DOI 10.1109/ISCC.1998.702542
   Indiveri G, 2006, IEEE T NEURAL NETWOR, V17, P211, DOI 10.1109/TNN.2005.860850
   Li SZ, 1999, IEEE INFOCOM SER, P1152, DOI 10.1109/INFCOM.1999.751671
   *NEST, 2007, NEUR SIM TECHN IN HO
   Schemmel J, 2004, ANALOG INTEGR CIRC S, V38, P233, DOI 10.1023/B:ALOG.0000011170.92377.6e
   Schemmel J, 2006, P 2006 INT JOINT C N
   TANENBAUM AS, 2004, COMPUTER NETWORKS
   *XIL INC, 2003, 670 XIL INC
   *XIL INC, 2002, VIRTEX 2 PRO PLATF F
   *XIL INC, 2003, ROCK TRANSC US GUID
NR 13
TC 4
Z9 4
U1 0
U2 0
PY 2007
VL 4507
BP 471
EP +
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods
DA 2023-11-11
ER

PT C
AU Schuman, CD
   Plank, JS
   Disney, A
   Reynolds, J
AF Schuman, Catherine D.
   Plank, James S.
   Disney, Adam
   Reynolds, John
GP IEEE
TI An Evolutionary Optimization Framework for Neural Networks and
   Neuromorphic Architectures
SO 2016 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUL 24-29, 2016
CL Vancouver, CANADA
ID ERROR-BACKPROPAGATION; SPIKING NEURONS; CLASSIFICATION
AB As new neural network and neuromorphic architectures are being developed, new training methods that operate within the constraints of the new architectures are required. Evolutionary optimization (EO) is a convenient training method for new architectures. In this work, we review a spiking neural network architecture and a neuromorphic architecture, and we describe an EO training framework for these architectures. We present the results of this training framework on four classification data sets and compare those results to other neural network and neuromorphic implementations. We also discuss how this EO framework may be extended to other architectures.
C1 [Schuman, Catherine D.] Oak Ridge Natl Lab, Computat Data Analyt, Oak Ridge, TN 37831 USA.
   [Plank, James S.; Disney, Adam; Reynolds, John] Univ Tennessee, EECS Dept, Knoxville, TN 37996 USA.
RP Schuman, CD (corresponding author), Oak Ridge Natl Lab, Computat Data Analyt, Oak Ridge, TN 37831 USA.
EM schumancd@ornl.gov; jplank@uatk.edu; adisney1@vols.utk.edu;
   jreyno40@vols.utk.edu
CR Abbass HA, 2002, ARTIF INTELL MED, V25, P265, DOI 10.1016/S0933-3657(02)00028-3
   Alba E, 2004, LECT NOTES COMPUT SC, V3102, P852
   ANGELINE PJ, 1994, IEEE T NEURAL NETWOR, V5, P54, DOI 10.1109/72.265960
   [Anonymous], P WORKSH MACH LEARN
   [Anonymous], 2014, P 2014 BIOM SCI ENG
   Bako L., 2010, BRIEF BIOINFORM
   Batllori R, 2011, PROCEDIA COMPUT SCI, V6, DOI 10.1016/j.procs.2011.08.060
   Belatreche A, 2007, SOFT COMPUT, V11, P239, DOI [10.1007/s00500-006-0065-7, 10.1007/S00500-006-0065-7]
   BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181
   Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Booij O, 2005, INFORM PROCESS LETT, V95, P552, DOI 10.1016/j.ipl.2005.05.023
   Cawley S, 2011, GENET PROGRAM EVOL M, V12, P257, DOI 10.1007/s10710-011-9130-9
   Dasgupta D., 1992, COGANN-92. International Workshop on Combinations of Genetic Algorithms and Neural Networks (Cat. No.92TH0435-8), P87, DOI 10.1109/COGANN.1992.273946
   Dean Mark E., 2014, Unconventional Computation and Natural Computation. 13th International Conference. Proceedings: LNCS 8553, P129, DOI 10.1007/978-3-319-08123-6_11
   Fieldsend JE, 2005, IEEE T NEURAL NETWOR, V16, P338, DOI 10.1109/TNN.2004.841794
   Floreano D., 2001, LNCS, P38
   FOGEL DB, 1990, BIOL CYBERN, V63, P487, DOI 10.1007/BF00199581
   FOGEL DB, 1995, CANCER LETT, V96, P49, DOI 10.1016/0304-3835(95)03916-K
   Furber SB, 2013, IEEE T COMPUT, V62, P2454, DOI 10.1109/TC.2012.142
   García-Pedrajas N, 2006, NEURAL NETWORKS, V19, P514, DOI 10.1016/j.neunet.2005.08.014
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   Gomez F., 1998, ICANN 98. Proceedings of the 8th International Conference on Artificial Neural Networks, P425
   Gomez F, 2008, J MACH LEARN RES, V9, P937
   Gomez F, 2006, LECT NOTES COMPUT SC, V4212, P654
   Hagras H, 2004, IEEE INT CONF ROBOT, P4620, DOI 10.1109/ROBOT.2004.1302446
   Hasan R, 2014, IEEE IJCNN, P21, DOI 10.1109/IJCNN.2014.6889893
   Hsieh HY, 2013, IEEE T NEUR NET LEAR, V24, P2063, DOI 10.1109/TNNLS.2013.2271644
   Igel C, 2003, IEEE C EVOL COMPUTAT, P2588
   Islam MM, 2008, STUD COMPUT INTELL, V115, P851
   Jin YC, 2007, LECT NOTES COMPUT SC, V4668, P370
   Kasabov N, 2014, NEUROCOMPUTING, V134, P269, DOI 10.1016/j.neucom.2013.09.049
   Leung FHF, 2003, IEEE T NEURAL NETWOR, V14, P79, DOI 10.1109/TNN.2002.804317
   Lichman M., 2013, UCI MACHINE LEARNING
   Liu Yong, 1996, Chinese Journal of Advanced Software Research, V3, P54
   MANIEZZO V, 1994, IEEE T NEURAL NETWOR, V5, P39, DOI 10.1109/72.265959
   McKennoch S, 2009, NEURAL COMPUT, V21, P9, DOI 10.1162/neco.2008.09-07-610
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Montana D. J., 1989, IJCAI-89 Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, P762
   Moriarty DE, 1996, MACH LEARN, V22, P11, DOI 10.1007/BF00114722
   Palmes PP, 2005, IEEE T NEURAL NETWOR, V16, P587, DOI 10.1109/TNN.2005.844858
   Pavlidis NG, 2005, IEEE IJCNN, P2190
   Pfeil T, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00011
   Pujol JCF, 1998, APPL INTELL, V8, P73, DOI 10.1023/A:1008272615525
   Rabinovich MI, 2006, REV MOD PHYS, V78, P1213, DOI 10.1103/RevModPhys.78.1213
   SARAVANAN N, 1995, IEEE EXPERT, V10, P23, DOI 10.1109/64.393139
   Schuman CD, 2014, PROCEDIA COMPUT SCI, V41, P89, DOI 10.1016/j.procs.2014.11.089
   Schuman CD, 2013, BIOL INSPIR COGN ARC, V6, P126, DOI 10.1016/j.bica.2013.05.001
   Schuman CD, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0080455
   Seiffert U, 2002, STUD FUZZ SOFT COMP, V78, P45
   Siebel N.T., 2007, INT J HYBRID INTELL, V4, P171, DOI [10.3233/HIS-2007-4304, DOI 10.3233/HIS-2007-4304]
   Soudry D., 2015, MEMRISTOR BASED MULT
   Stanley KO, 2002, EVOL COMPUT, V10, P99, DOI 10.1162/106365602320169811
   Suri M, 2015, IEEE T NANOTECHNOL, V14, P963, DOI 10.1109/TNANO.2015.2441112
   Tang K., 1995, GENETIC STRUCTURE NN
   Temam O, 2012, CONF PROC INT SYMP C, P356, DOI 10.1109/ISCA.2012.6237031
   Tianshi Chen, 2012, 2012 IEEE International Symposium on Workload Characterization (IISWC 2012), P36, DOI 10.1109/IISWC.2012.6402898
   Valko M, 2005, 2005 Portuguese Conference on Artificial Intelligence, Proceedings, P181, DOI 10.1109/EPIA.2005.341291
   Vazquez Roberto A., 2010, 2010 7th International Conference on Electrical Engineering, Computing Science and Automatic Control (CCE 2010) (Formerly known as ICEEE), P424, DOI 10.1109/ICEEE.2010.5608622
   Vázquez RA, 2010, LECT NOTES ARTIF INT, V6433, P423
   Vazquez RA, 2011, IEEE C EVOL COMPUTAT, P679
   White D., 1993, LECT NOTES COMPUTER, V686, P322
   Wieland A. P., 1991, IJCNN-91-Seattle: International Joint Conference on Neural Networks (Cat. No.91CH3049-4), P667, DOI 10.1109/IJCNN.1991.155416
   Wysoski SG, 2008, NEUROCOMPUTING, V71, P2563, DOI 10.1016/j.neucom.2007.12.038
   Xin JG, 2001, IEEE IJCNN, P1772, DOI 10.1109/IJCNN.2001.938430
   Yao X, 1999, P IEEE, V87, P1423, DOI 10.1109/5.784219
   Yao X, 1997, IEEE T NEURAL NETWOR, V8, P694, DOI 10.1109/72.572107
   Yeung D., 2015, NEURAL NETWORKS LEAR, P1
NR 68
TC 35
Z9 35
U1 0
U2 4
PY 2016
BP 145
EP 154
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Dummer, B
   Wieland, S
   Lindner, B
AF Dummer, Benjamin
   Wieland, Stefan
   Lindner, Benjamin
TI Self-consistent determination of the spike-train power spectrum in a
   neural network with sparse connectivity
SO FRONTIERS IN COMPUTATIONAL NEUROSCIENCE
DT Article
DE neural noise; recurrent neural networks; non-Poissonian spiking;
   spike-train statistics; spike-train power spectrum
ID FIRE NEURONS; ASYNCHRONOUS STATES; RESPONSE VARIABILITY; CORRELATED
   NOISE; SYNAPTIC INPUT; COLORED-NOISE; CHANNEL NOISE; ART.; STATISTICS;
   MODEL
AB A major source of random variability in cortical networks is the quasi-random arrival of presynaptic action potentials from many other cells. In network studies as well as in the study of the response properties of single cells embedded in a network, synaptic background input is often approximated by Poissonian spike trains. However, the output statistics of the cells is in most cases far from being Poisson. This is inconsistent with the assumption of similar spike-train statistics for pre- and postsynaptic cells in a recurrent network. Here we tackle this problem for the popular class of integrate-and-fire neurons and study a self-consistent statistics of input and output spectra of neural spike trains. Instead of actually using a large network, we use an iterative scheme, in which we simulate a single neuron over several generations. In each of these generations, the neuron is stimulated with surrogate stochastic input that has a similar statistics as the output of the previous generation. For the surrogate input, we employ two distinct approximations: (i) a superposition of renewal spike trains with the same interspike interval density as observed in the previous generation and (ii) a Gaussian current with a power spectrum proportional to that observed in the previous generation. For input parameters that correspond to balanced input in the network, both the renewal and the Gaussian iteration procedure converge quickly and yield comparable results for the self-consistent spike-train power spectrum. We compare our results to large-scale simulations of a random sparsely connected network of leaky integrate-and-fire neurons (Brunel, 2000) and show that in the asynchronous regime close to a state of balanced synaptic input from the network, our iterative schemes provide an excellent approximations to the autocorrelation of spike trains in the recurrent network.
C1 [Dummer, Benjamin; Wieland, Stefan; Lindner, Benjamin] Bernstein Ctr Computat Neurosci, Berlin, Germany.
   [Dummer, Benjamin; Wieland, Stefan; Lindner, Benjamin] Humboldt Univ, Dept Phys, D-10115 Berlin, Germany.
RP Lindner, B (corresponding author), Humboldt Univ, Dept Phys, Philippstr 13,Haus 2, D-10115 Berlin, Germany.
EM benjamin.lindner@physik.hu-berlin.de
CR ABBOTT LF, 1993, PHYS REV E, V48, P1483, DOI 10.1103/PhysRevE.48.1483
   Alijani AK, 2011, PHYS REV E, V84, DOI 10.1103/PhysRevE.84.011919
   [Anonymous], 1967, TOPICS THEORY RANDOM
   Bahar S, 2001, EUROPHYS LETT, V56, P454, DOI 10.1209/epl/i2001-00540-7
   BAIR W, 1994, J NEUROSCI, V14, P2870
   Bauermeister C, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003170
   BILLAH KYR, 1990, PHYS REV A, V42, P7492, DOI 10.1103/PhysRevA.42.7492
   Branco T, 2009, NAT REV NEUROSCI, V10, P373, DOI 10.1038/nrn2634
   Brenner N, 2002, PHYS REV E, V66, DOI 10.1103/PhysRevE.66.031907
   Brette R, 2009, NEURAL COMPUT, V21, P188, DOI [10.1162/neco.2009.12-07-657, 10.1162/neco.2008.12-07-657]
   Brunel N, 1998, J THEOR BIOL, V195, P87, DOI 10.1006/jtbi.1998.0782
   Brunel N, 1999, NEURAL COMPUT, V11, P1621, DOI 10.1162/089976699300016179
   Brunel N, 2001, PHYS REV LETT, V86, P2186, DOI 10.1103/PhysRevLett.86.2186
   Brunel N, 2000, J COMPUT NEUROSCI, V8, P183, DOI 10.1023/A:1008925309027
   Brunel N, 2006, NEURAL COMPUT, V18, P1066, DOI 10.1162/neco.2006.18.5.1066
   Brunel N, 2008, CHAOS, V18, DOI 10.1063/1.2779858
   Burkitt AN, 2006, BIOL CYBERN, V95, P97, DOI 10.1007/s00422-006-0082-8
   Câteau H, 2006, PHYS REV LETT, V96, DOI 10.1103/PhysRevLett.96.058101
   Compte A, 2003, J NEUROPHYSIOL, V90, P3441, DOI 10.1152/jn.00949.2002
   Cox DR., 1962, RENEWAL THEORY
   Deger M, 2012, J COMPUT NEUROSCI, V32, P443, DOI 10.1007/s10827-011-0362-8
   Destexhe A, 2003, NAT REV NEUROSCI, V4, P739, DOI 10.1038/nrn1198
   Doiron B, 2004, PHYS REV LETT, V93, DOI 10.1103/PhysRevLett.93.048101
   Droste F, 2013, FRONT COMPUT NEUROSC, V7, DOI 10.3389/fncom.2013.00086
   Fisch K, 2012, J NEUROSCI, V32, P17332, DOI 10.1523/JNEUROSCI.6231-11.2012
   Fusi S, 1999, NEURAL COMPUT, V11, P633, DOI 10.1162/089976699300016601
   GERSTNER W, 1995, PHYS REV E, V51, P738, DOI 10.1103/PhysRevE.51.738
   Gerstner W., 2002, SPIKING NEURON MODEL
   Grytskyy D, 2013, FRONT COMPUT NEUROSC, V7, DOI 10.3389/fncom.2013.00131
   Guz SA, 1998, PHYS LETT A, V240, P43, DOI 10.1016/S0375-9601(98)00009-7
   Hansel D, 2003, NEURAL COMPUT, V15, P1, DOI 10.1162/089976603321043685
   Helias M, 2013, NEW J PHYS, V15, DOI 10.1088/1367-2630/15/2/023002
   Hennequin G, 2012, PHYS REV E, V86, DOI 10.1103/PhysRevE.86.011909
   Koch Christof, 1999, P1
   Lapique L., 1907, J PHYSL PATHOL GEN, V9, P620, DOI DOI 10.1007/S00422-007-0189-6
   Latham PE, 2000, J NEUROPHYSIOL, V83, P808, DOI 10.1152/jn.2000.83.2.808
   Leibold C, 2004, PHYS REV LETT, V93, DOI 10.1103/PhysRevLett.93.208104
   Lerchner A, 2006, NEURAL COMPUT, V18, P634, DOI 10.1162/089976606775623261
   Lindner B, 2006, PHYS REV E, V73, DOI 10.1103/PhysRevE.73.022901
   Lindner B, 2005, PHYS REV E, V72, DOI 10.1103/PhysRevE.72.061919
   Lindner B, 2005, PHYS REV E, V72, DOI 10.1103/PhysRevE.72.021911
   Lindner B, 2004, PHYS REV E, V69, DOI 10.1103/PhysRevE.69.022901
   Lindner B, 2002, PHYS REV E, V66, DOI 10.1103/PhysRevE.66.031916
   London M, 2010, NATURE, V466, P123, DOI 10.1038/nature09086
   Ly C, 2009, NEURAL COMPUT, V21, P360, DOI 10.1162/neco.2008.03-08-743
   Manwani A, 1999, NEURAL COMPUT, V11, P1797, DOI 10.1162/089976699300015972
   Masquelier T, 2013, FRONT COMPUT NEUROSC, V7, DOI 10.3389/fncom.2013.00007
   Middleton JW, 2003, PHYS REV E, V68, DOI 10.1103/PhysRevE.68.021920
   Moreno-Bote R, 2006, PHYS REV LETT, V96, DOI 10.1103/PhysRevLett.96.028101
   Neiman AB, 2005, PHYS REV E, V71, DOI 10.1103/PhysRevE.71.061915
   Ostojic S, 2014, NAT NEUROSCI, V17, P594, DOI 10.1038/nn.3658
   Pernice V, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1002059
   Richardson MJE, 2010, PHYS REV LETT, V105, DOI 10.1103/PhysRevLett.105.178102
   Schneidman E, 1998, NEURAL COMPUT, V10, P1679, DOI 10.1162/089976698300017089
   Stein RB, 2005, NAT REV NEUROSCI, V6, P389, DOI 10.1038/nrn1668
   Tchumatchenko T, 2010, PHYS REV LETT, V104, DOI 10.1103/PhysRevLett.104.058102
   Trousdale J, 2012, PLOS COMPUT BIOL, V8, DOI 10.1371/journal.pcbi.1002408
   vanSteveninck RRD, 1997, SCIENCE, V275, P1805, DOI 10.1126/science.275.5307.1805
   vanVreeswijk C, 1996, SCIENCE, V274, P1724, DOI 10.1126/science.274.5293.1724
   Vilela RD, 2009, PHYS REV E, V80, DOI 10.1103/PhysRevE.80.031909
   Wang ST, 2006, PHYS REV LETT, V96, DOI 10.1103/PhysRevLett.96.018103
   White JA, 2000, TRENDS NEUROSCI, V23, P131, DOI 10.1016/S0166-2236(99)01521-0
NR 62
TC 39
Z9 39
U1 0
U2 8
PD SEP 18
PY 2014
VL 8
AR 104
DI 10.3389/fncom.2014.00104
WC Mathematical & Computational Biology; Neurosciences
DA 2023-11-11
ER

PT C
AU PARNAS, BR
   LEWIS, ER
AF PARNAS, BR
   LEWIS, ER
BE NAGEL, JH
   SMITH, WM
TI ON BIOLOGICAL NEURAL NETWORKS - CRITERIA FOR SELECTION OF SPIKE
   INITIATOR MODELS
SO PROCEEDINGS OF THE ANNUAL INTERNATIONAL CONFERENCE OF THE IEEE
   ENGINEERING IN MEDICINE AND BIOLOGY SOCIETY, VOL 13, PTS 1-5
DT Proceedings Paper
CT 1991 ANNUAL INTERNATIONAL CONF OF THE IEEE ENGINEERING IN MEDICINE AND
   BIOLOGY SOC
CY OCT 31-NOV 03, 1991
CL ORLANDO, FL
NR 0
TC 0
Z9 0
U1 0
U2 0
PY 1991
BP 1442
EP 1443
DI 10.1109/IEMBS.1991.684537
WC Engineering, Biomedical
DA 2023-11-11
ER

PT J
AU Perrig, S
   Iglesias, J
   Shaposhnyk, V
   Chibirova, O
   Dutoit, P
   Cabessa, J
   Espa-Cervena, K
   Pelletier, L
   Berger, F
   Villa, AEP
AF Perrig, Stephen
   Iglesias, Javier
   Shaposhnyk, Vladislav
   Chibirova, Olga
   Dutoit, Pierre
   Cabessa, Jeremie
   Espa-Cervena, Katerina
   Pelletier, Laurent
   Berger, Francois
   Villa, Alessandro E. P.
TI Functional Interactions in Hierarchically Organized Neural Networks
   Studied with Spatiotemporal Firing Patterns and Phase-Coupling
   Frequencies
SO CHINESE JOURNAL OF PHYSIOLOGY
DT Article
DE apoptosis; spike timing dependent plasticity; synaptic pruning;
   preferred firing sequence; insomnia; EEG
ID COGNITIVE-BEHAVIORAL THERAPY; NEURONAL SPIKE TRAINS; GROUPING ALGORITHM;
   CONNECTIONS; EEG; PLASTICITY; INSOMNIA; RAT; MECHANISM; DYNAMICS
AB A scalable hardware/software hybrid module called Ubidule- endowed with bio-inspired ontogenetic and epigenetic features is configured to run a neural networks simulation with developmental and evolvable capabilities. We simulated the activity of hierarchically organized spiking neural networks characterized by an initial developmental phase featuring cell death followed by spike timing dependent synaptic plasticity in presence of background noise. An upstream 'sensory' network received a spatiotemporally organized external input and downstream networks were activated only via the upstream network. Precise firing sequences, formed by recurrent patterns of spikes intervals above chance levels, were observed in all recording conditions, thus suggesting the build-up of a connectivity able to sustain temporal information processing. The activity of a Ubinet -a network of Ubidules- is analyzed by means of virtual electrodes that recorded neural signals similar to EEG. The analysis of these signals was compared with a small set of human recordings and revealed common patterns of shift in quadratic phase coupling. The results suggest some interpretations of changes and plasticity of functional interactions between cortical areas driven by external stimuli and by learning/cognitive paradigms.
C1 [Villa, Alessandro E. P.] Univ Grenoble 1, GIN, INSERM, NeuroHeurist Res Grp,UMR S 836,Equipe 7, Grenoble, France.
   [Perrig, Stephen; Espa-Cervena, Katerina; Villa, Alessandro E. P.] Hop Univ Geneve, Dept Psychiat, Sleep Res Lab, Chene Bourg, Switzerland.
   [Iglesias, Javier] Univ Politecn Cataluna, Dept Fis & Engn Nucl, Terrassa, Spain.
   [Villa, Alessandro E. P.] Univ Lausanne, Informat Syst Dept ISI, Neuroheurist Res Grp, CH-1015 Lausanne, Switzerland.
RP Villa, AEP (corresponding author), Univ Grenoble 1, GIN, INSERM, NeuroHeurist Res Grp,UMR S 836,Equipe 7, Grenoble, France.
EM Alessandro.Villa@neuroheuristic.org
CR [Anonymous], CORTICONICS NZURAL C
   [Anonymous], COMPUTATIONAL NEUROG
   [Anonymous], NUMERICAL RECIPES C
   [Anonymous], PRINCIPLES PRACTICE
   [Anonymous], AHS 07
   [Anonymous], [No title captured]
   [Anonymous], TIME BRAIN
   [Anonymous], POST NATAL DEV HUMAN
   Ashikaga M, 2007, J ROBOT MECHATRON, V19, P466, DOI 10.20965/jrm.2007.p0466
   Bellifemine F.L., 2007, DEVELOPING MULTIAGEN
   Bonnet M H, 1997, Sleep Med Rev, V1, P97, DOI 10.1016/S1087-0792(97)90012-5
   Braitenberg V, 1998, CORTEX STAT GEOMETRY, V2nd
   BRILLINGER DR, 1965, ANN MATH STAT, V36, P1351, DOI 10.1214/aoms/1177699896
   Chechik G, 1999, NEURAL COMPUT, V11, P2061, DOI 10.1162/089976699300016089
   Doya K, 2005, ADAPT BEHAV, V13, P149, DOI 10.1177/105971230501300206
   DUMERMUTH G, 1971, ELECTROEN CLIN NEURO, V31, P137, DOI 10.1016/0013-4694(71)90183-0
   Edinger JD, 2005, CLIN PSYCHOL REV, V25, P539, DOI 10.1016/J.CPR.2005.04.003
   Elston GN, 2002, J NEUROCYTOL, V31, P317, DOI 10.1023/A:1024182228103
   Fouks JD, 2004, NEUROPHYSIOL CLIN, V34, P59, DOI 10.1016/j.neucli.2004.01.005
   FREEDMAN RR, 1986, ELECTROEN CLIN NEURO, V63, P408, DOI 10.1016/0013-4694(86)90122-7
   GAILLARD JM, 1978, SLEEP, V1, P133, DOI 10.1093/sleep/1.2.133
   Hill SL, 1997, NETWORK-COMP NEURAL, V8, P165, DOI 10.1088/0954-898X/8/2/004
   HUBEL DH, 1977, PHILOS T ROY SOC B, V278, P377, DOI 10.1098/rstb.1977.0050
   HUTTENLOCHER PR, 1979, BRAIN RES, V163, P195
   Iglesias J, 2005, BIOSYSTEMS, V79, P11, DOI 10.1016/j.biosystems.2004.09.016
   Iglesias J, 2008, INT J NEURAL SYST, V18, P267, DOI 10.1142/S0129065708001580
   Iglesias J, 2007, LECT NOTES COMPUT SC, V4668, P579
   Iglesias J, 2007, BIOSYSTEMS, V89, P287, DOI 10.1016/j.biosystems.2006.05.020
   Innocenti GM, 2005, NAT REV NEUROSCI, V6, P955, DOI 10.1038/nrn1790
   INNOCENTI GM, 1995, TRENDS NEUROSCI, V18, P397, DOI 10.1016/0166-2236(95)93936-R
   Lamarche CH, 1997, SLEEP, V20, P724
   Levitt P, 2003, J PEDIATR-US, V143, pS35, DOI 10.1067/S0022-3476(03)00400-1
   Low LK, 2006, PHILOS T R SOC B, V361, P1531, DOI 10.1098/rstb.2006.1883
   Merica H, 1998, EUR J NEUROSCI, V10, P1826, DOI 10.1046/j.1460-9568.1998.00189.x
   Mizuno H, 2007, J NEUROSCI, V27, P6760, DOI 10.1523/JNEUROSCI.1215-07.2007
   Montgomery JM, 2004, TRENDS NEUROSCI, V27, P744, DOI 10.1016/j.tins.2004.10.006
   Morin CM, 2009, JAMA-J AM MED ASSOC, V301, P2005, DOI 10.1001/jama.2009.682
   Nofzinger EA, 2004, AM J PSYCHIAT, V161, P2126, DOI 10.1176/appi.ajp.161.11.2126
   Nunez P, 2006, ELECT FIELDS BRAIN
   Perlis ML, 2001, SLEEP, V24, P110, DOI 10.1093/sleep/24.1.110
   Ramakers Ger J A, 2005, Prog Brain Res, V147, P1
   Roberts PD, 2002, BIOL CYBERN, V87, P392, DOI 10.1007/s00422-002-0361-y
   Shaposhnyk V, 2009, LECT NOTES COMPUT SC, V5768, P277, DOI 10.1007/978-3-642-04274-4_29
   SHATZ CJ, 1990, NEURON, V5, P745, DOI 10.1016/0896-6273(90)90333-B
   Tetko IV, 2001, J NEUROSCI METH, V105, P15, DOI 10.1016/S0165-0270(00)00337-X
   Tetko IV, 2001, J NEUROSCI METH, V105, P1, DOI 10.1016/S0165-0270(00)00336-8
   Torres O, 2004, BIOSYSTEMS, V76, P201, DOI 10.1016/j.biosystems.2004.05.012
   Villa AEP, 1999, P NATL ACAD SCI USA, V96, P1106, DOI 10.1073/pnas.96.3.1106
   Villa AEP, 2000, BIOSYSTEMS, V58, P219, DOI 10.1016/S0303-2647(00)00126-X
   Villa AEP, 1999, J NEUROSCI METH, V86, P161, DOI 10.1016/S0165-0270(98)00164-2
   Yamamori T, 2006, NEUROSCI RES, V55, P11, DOI 10.1016/j.neures.2006.02.006
NR 51
TC 3
Z9 3
U1 0
U2 11
PD DEC 31
PY 2010
VL 53
IS 6
SI SI
BP 382
EP 395
DI 10.4077/CJP.2010.AMM039
WC Physiology
DA 2023-11-11
ER

PT C
AU Knudsen, M
   Hendseth, S
   Tufte, G
   Sandvig, A
AF Knudsen, Martinius
   Hendseth, Sverre
   Tufte, Gunnar
   Sandvig, Axel
BE McQuillan, I
   Seki, S
TI Viewing Rate-Based Neurons as Biophysical Conductance Outputting Models
SO UNCONVENTIONAL COMPUTATION AND NATURAL COMPUTATION, UCNC 2019
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 18th International Conference on Unconventional Computation and Natural
   Computation (UCNC)
CY JUN 03-07, 2019
CL Tokyo, JAPAN
DE Artificial neural network; Spiking neural network; Computational
   neuroscience; Conductance models
ID NETWORKS; BRAIN
AB In the field of computational neuroscience, spiking neural network models are generally preferred over rate-based models due to their ability to model biological dynamics. Within AI, rate-based artificial neural networks have seen success in a wide variety of applications. In simplistic spiking models, information between neurons is transferred through discrete spikes, while rate-based neurons transfer information through continuous firing-rates. Here, we argue that while the spiking neuron model, when viewed in isolation, may be more biophysically accurate than rate-based models, the roles reverse when we also consider information transfer between neurons. In particular we consider the biological importance of continuous synaptic signals. We show how synaptic conductance relates to the common rate-based model, and how this relation elevates these models in terms of their biological soundness. We shall see how this is a logical relation by investigating mechanisms known to be present in biological synapses. We coin the term 'conductance-outputting neurons' to differentiate this alternative view from the standard firing-rate perspective. Finally, we discuss how this fresh view of rate-based models can open for further neuro-AI collaboration.
C1 [Knudsen, Martinius; Hendseth, Sverre] NTNU, Dept Engn Cybernet, Trondheim, Norway.
   [Tufte, Gunnar] NTNU, Dept Comp Sci, Trondheim, Norway.
   [Sandvig, Axel] NTNU, Dept Neuromed & Movement Sci, Trondheim, Norway.
RP Hendseth, S (corresponding author), NTNU, Dept Engn Cybernet, Trondheim, Norway.; Tufte, G (corresponding author), NTNU, Dept Comp Sci, Trondheim, Norway.; Sandvig, A (corresponding author), NTNU, Dept Neuromed & Movement Sci, Trondheim, Norway.
EM martinius.knudsen@ntnu.no; sverre.hendseth@ntnu.no;
   gunnar.tufte@ntnu.no; axel.sandvig@ntnu.no
CR Aaser P, 2017, FOURTEENTH EUROPEAN CONFERENCE ON ARTIFICIAL LIFE (ECAL 2017), P430
   [Anonymous], 2015, POPULATION PYRAMIDS
   [Anonymous], 2013, PRINCIPLES NEURAL SC, DOI DOI 10.1036/0838577016
   Attneave F., 1950, AJP, V63, P633, DOI [DOI 10.1002/SCE.37303405110.-633, DOI 10.2307/1418888, 10.2307/1418888]
   Brette R, 2015, FRONT SYST NEUROSCI, V9, DOI 10.3389/fnsys.2015.00151
   Buchanan Katherine A, 2010, Front Synaptic Neurosci, V2, P11, DOI 10.3389/fnsyn.2010.00011
   Burkitt AN, 2006, BIOL CYBERN, V95, P1, DOI 10.1007/s00422-006-0068-6
   Clopath Claudia, 2010, Front Synaptic Neurosci, V2, P25, DOI 10.3389/fnsyn.2010.00025
   de Kamps M, 2001, NEURAL NETWORKS, V14, P941, DOI 10.1016/S0893-6080(01)00068-5
   DeFelipe J, 2012, FRONT NEUROANAT, V6, DOI [10.3389/fnana.2012.00022, 10.3389/fnsyn.2012.00002, 10.3389/fnana.2012.00005]
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Gerstner W, 1997, P NATL ACAD SCI USA, V94, P12740, DOI 10.1073/pnas.94.24.12740
   HONORE T, 1982, J NEUROCHEM, V38, P173, DOI 10.1111/j.1471-4159.1982.tb10868.x
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Kheradpisheh SR, 2016, SCI REP-UK, V6, DOI 10.1038/srep32672
   Kheradpisheh SR, 2016, NEUROCOMPUTING, V205, P382, DOI 10.1016/j.neucom.2016.04.029
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   MAINEN ZF, 1995, SCIENCE, V268, P1503, DOI 10.1126/science.7770778
   Meldrum BS, 2000, J NUTR, V130, p1007S, DOI 10.1093/jn/130.4.1007S
   Nair V., 2010, ICML, P807
   RUMELHART DE, 1994, COMMUN ACM, V37, P87, DOI 10.1145/175247.175256
   Shouval HZ, 2010, FRONT COMPUT NEUROSC, V4, DOI 10.3389/fncom.2010.00019
   Sompolinsky H, 2014, CURR OPIN NEUROBIOL, V25, pXIII, DOI 10.1016/j.conb.2014.02.002
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Sterratt D., 2011, PRINCIPLES COMPUTATI, DOI [10.1109/MPUL.2012.2196841, DOI 10.1109/MPUL.2012.2196841]
   WOLFRAM S, 1984, NATURE, V311, P419, DOI 10.1038/311419a0
   WURTZ RH, 1969, J NEUROPHYSIOL, V32, P727, DOI 10.1152/jn.1969.32.5.727
NR 27
TC 0
Z9 0
U1 0
U2 0
PY 2019
VL 11493
BP 164
EP 177
DI 10.1007/978-3-030-19311-9_14
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods; Mathematics, Applied
DA 2023-11-11
ER

PT J
AU Amiri, M
   Amiri, M
   Nazari, S
   Faez, K
AF Amiri, Masoud
   Amiri, Mahmood
   Nazari, Soheila
   Faez, Karim
TI A new bio-inspired stimulator to suppress hyper-synchronized neural
   firing in a cortical network
SO JOURNAL OF THEORETICAL BIOLOGY
DT Article
DE DBS; Spiking neural network; Bio-inspired stimulator; Astrocyte
ID DEEP BRAIN-STIMULATION; DIGITAL IMPLEMENTATION; EPILEPSY; ASTROCYTES;
   CIRCUITS; MODEL; OSCILLATIONS; EXCITATION; GLIA
AB Hyper-synchronous neural oscillations are the character of several neurological diseases such as epilepsy. On the other hand, glial cells and particularly astrocytes can influence neural synchronization. Therefore, based on the recent researches, a new bio-inspired stimulator is proposed which basically is a dynamical model of the astrocyte biophysical model. The performance of the new stimulator is investigated on a large-scale, cortical network. Both excitatory and inhibitory synapses are also considered in the simulated spiking neural network. The simulation results show that the new stimulator has a good performance and is able to reduce recurrent abnormal excitability which in turn avoids the hyper-synchronous neural firing in the spiking neural network. In this way, the proposed stimulator has a demand controlled characteristic and is a good candidate for deep brain stimulation (DBS) technique to successfully suppress the neural hyper-synchronization. (C) 2016 Elsevier Ltd. All rights reserved.
C1 [Amiri, Masoud; Amiri, Mahmood; Nazari, Soheila] Kermanshah Univ Med Sci, Med Biol Res Ctr, Kermanshah, Iran.
   [Nazari, Soheila; Faez, Karim] Amirkabir Univ Technol, Dept Elect Engn, Tehran, Iran.
RP Amiri, M (corresponding author), Kermanshah Univ Med Sci, Parastar Ave, Kermanshah, Iran.
EM ma.amiri@ece.ut.ac.ir
CR Amiri M, 2013, NEURAL NETWORKS, V44, P157, DOI 10.1016/j.neunet.2013.03.018
   Amiri M, 2013, J COMPUT NEUROSCI, V34, P489, DOI 10.1007/s10827-012-0432-6
   Amiri M, 2012, J COMPUT NEUROSCI, V33, P285, DOI 10.1007/s10827-012-0386-8
   Amiri M, 2012, NEUROSCI RES, V72, P172, DOI 10.1016/j.neures.2011.11.006
   Amiri M, 2012, J THEOR BIOL, V292, P60, DOI 10.1016/j.jtbi.2011.09.013
   Amiri M, 2011, BIOL CYBERN, V105, P153, DOI 10.1007/s00422-011-0455-5
   Amiri M, 2011, MATH COMPUT SIMULAT, V81, P2471, DOI 10.1016/j.matcom.2011.03.012
   Annan K., 2014, J MATH SYST SCI, V4, P116
   Barardi A, 2014, PHILOS T R SOC B, V369, DOI 10.1098/rstb.2013.0533
   Bertram EH, 2013, EXP NEUROL, V244, P67, DOI 10.1016/j.expneurol.2012.01.028
   Beuter A, 2014, CLIN NEUROPHYSIOL, V125, P874, DOI 10.1016/j.clinph.2014.01.006
   Carron R., 2013, FRONTIERS SYSTEMS NE, V13, P1
   Cavallari S, 2014, FRONT NEURAL CIRCUIT, V8, DOI 10.3389/fncir.2014.00012
   Chao D., 2015, NEURAL FUNCTIONS DEL, P583
   Chao D. M., 2012, CURRENT RES ACUPUNCT, P129
   Clarke LE, 2013, NAT REV NEUROSCI, V14, P311, DOI 10.1038/nrn3484
   Coenen V. A., 2015, DTSCH AERZTEBLATT IN, V112
   Destexhe A, 2008, COMPUTATIONAL NEUROSCIENCE IN EPILEPSY, P184, DOI 10.1016/B978-012373649-9.50016-8
   Devinsky O, 2013, TRENDS NEUROSCI, V36, P174, DOI 10.1016/j.tins.2012.11.008
   Farajnia S, 2014, P NATL ACAD SCI USA, V111, P9627, DOI 10.1073/pnas.1319820111
   Fountas KN, 2005, STEREOT FUNCT NEUROS, V83, P153, DOI 10.1159/000088656
   Fröhlich F, 2008, COMPUTATIONAL NEUROSCIENCE IN EPILEPSY, P419, DOI 10.1016/B978-012373649-9.50029-6
   Graeber MB, 2010, ACTA NEUROPATHOL, V119, P89, DOI 10.1007/s00401-009-0622-0
   Hauptmann C, 2009, J NEURAL ENG, V6, DOI 10.1088/1741-2560/6/1/016004
   Hirata M, 2011, IEICE T COMMUN, VE94B, P2448, DOI 10.1587/transcom.E94.B.2448
   Hoffman R. E., 2002, AM J PSYCHIAT
   Hosain MK, 2014, AUSTRALAS PHYS ENG S, V37, P619, DOI 10.1007/s13246-014-0297-2
   Kandel E. R., 1991, PRINCIPLES NEURAL SC
   Kudela P, 2003, BIOL CYBERN, V88, P276, DOI 10.1007/s00422-002-0381-7
   Laxpati NG, 2014, NEUROTHERAPEUTICS, V11, P508, DOI 10.1007/s13311-014-0279-9
   Linne ML, 2014, PROG MOL BIOL TRANSL, V123, P191, DOI 10.1016/B978-0-12-397897-4.00005-X
   Liu JB, 2011, J NEURAL ENG, V8, DOI 10.1088/1741-2560/8/4/045002
   Marceglia S, 2007, EXPERT REV MED DEVIC, V4, P605, DOI 10.1586/17434440.4.5.605
   Mazzoni A., 2012, ARXIV12060560
   Mazzoni A, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.1000239
   McCormick DA, 2001, ANNU REV PHYSIOL, V63, P815, DOI 10.1146/annurev.physiol.63.1.815
   McDonnell MD, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0088254
   McDonnell MD, 2012, BRAIN RES, V1434, P162, DOI 10.1016/j.brainres.2011.08.070
   Montaseri G., 2014, NEURAL NETW
   Montaseri G., 2011, P IEEE 3 WORLD C NAB, P195
   Nazari S, 2015, NEUROCOMPUTING, V164, P281, DOI 10.1016/j.neucom.2015.02.041
   Nazari S, 2015, NEURAL NETWORKS, V66, P79, DOI 10.1016/j.neunet.2015.01.005
   Nazari S, 2014, NEUROSCI LETT, V582, P21, DOI 10.1016/j.neulet.2014.07.055
   Pallud J, 2014, SCI TRANSL MED, V6, DOI 10.1126/scitranslmed.3008065
   Pekny M., 2015, ACTA NEUROPATHOL, P1
   Pekny M, 2016, ACTA NEUROPATHOL, V131, P323, DOI 10.1007/s00401-015-1513-1
   Pérez-Alvarez A, 2013, CURR DRUG TARGETS, V14, P1220
   Postnov DE, 2009, J BIOL PHYS, V35, P425, DOI 10.1007/s10867-009-9156-x
   Priori A, 2013, EXP NEUROL, V245, P77, DOI 10.1016/j.expneurol.2012.09.013
   Rolls A, 2009, NAT REV NEUROSCI, V10, P235, DOI 10.1038/nrn2591
   Rosin B, 2011, NEURON, V72, P370, DOI 10.1016/j.neuron.2011.08.023
   Sancristóbal B, 2014, J COMPUT NEUROSCI, V37, P193, DOI 10.1007/s10827-014-0495-7
   Santaniello S, 2011, IEEE T NEUR SYS REH, V19, P15, DOI 10.1109/TNSRE.2010.2081377
   Schafer DP, 2012, NEURON, V74, P691, DOI 10.1016/j.neuron.2012.03.026
   Seifert G, 2013, EXP NEUROL, V244, P4, DOI 10.1016/j.expneurol.2011.08.024
   Weinberger M, 2009, EXP NEUROL, V219, P58, DOI 10.1016/j.expneurol.2009.05.014
   Wetherington J, 2008, NEURON, V58, P168, DOI 10.1016/j.neuron.2008.04.002
   Winestone JS, 2012, J NEUROSCI METH, V206, P34, DOI 10.1016/j.jneumeth.2012.02.002
   Wu JX, 2013, NEURAL REGEN RES, V8, P745, DOI 10.3969/j.issn.1673-5374.2013.08.009
NR 59
TC 7
Z9 7
U1 0
U2 20
PD DEC 7
PY 2016
VL 410
BP 107
EP 118
DI 10.1016/j.jtbi.2016.09.007
WC Biology; Mathematical & Computational Biology
DA 2023-11-11
ER

PT C
AU Hitron, Y
   Parter, M
AF Hitron, Yael
   Parter, Merav
BE Bender, MA
   Svensson, O
   Herman, G
TI Counting to Ten with Two Fingers: Compressed Counting with Spiking
   Neurons
SO 27TH ANNUAL EUROPEAN SYMPOSIUM ON ALGORITHMS (ESA 2019)
SE Leibniz International Proceedings in Informatics
DT Proceedings Paper
CT 27th Annual European Symposium on Algorithms (ESA)
CY SEP 09-11, 2019
CL Garching, GERMANY
DE stochastic neural networks; approximate counting; synchronizer
ID ASYNCHRONY; SYNCHRONY
AB We consider the task of measuring time with probabilistic threshold gates implemented by bio-inspired spiking neurons. In the model of spiking neural networks, network evolves in discrete rounds, where in each round, neurons fire in pulses in response to a sufficiently high membrane potential. This potential is induced by spikes from neighboring neurons that fired in the previous round, which can have either an excitatory or inhibitory effect.
   Discovering the underlying mechanisms by which the brain perceives the duration of time is one of the largest open enigma in computational neuro-science. To gain a better algorithmic understanding onto these processes, we introduce the neural timer problem. In this problem, one is given a time parameter t, an input neuron x, and an output neuron y. It is then required to design a minimum sized neural network (measured by the number of auxiliary neurons) in which every spike from x in a given round i, makes the output y fire for the subsequent t consecutive rounds.
   We first consider a deterministic implementation of a neural timer and show that Theta(log t) (deterministic) threshold gates are both sufficient and necessary. This raised the question of whether randomness can be leveraged to reduce the number of neurons. We answer this question in the affirmative by considering neural timers with spiking neurons where the neuron y is required to fire for t consecutive rounds with probability at least 1 - delta, and should stop firing after at most 2t rounds with probability 1 - delta for some input parameter delta is an element of (0, 1). Our key result is a construction of a neural timer with O(log log 1/ delta) spiking neurons. Interestingly, this construction uses only one spiking neuron, while the remaining neurons can be deterministic threshold gates. We complement this construction with a matching lower bound of Omega(min{log log 1/delta, log t}) neurons. This provides the first separation between deterministic and randomized constructions in the setting of spiking neural networks.
   Finally, we demonstrate the usefulness of compressed counting networks for synchronizing neural networks. In the spirit of distributed synchronizers [Awerbuch-Peleg, FOCS'90], we provide a general transformation (or simulation) that can take any synchronized network solution and simulate it in an asynchronous setting (where edges have arbitrary response latencies) while incurring a small overhead w.r.t the number of neurons and computation time.
C1 [Hitron, Yael; Parter, Merav] Weizmann Inst Sci, Dept Comp Sci & Appl Math, IL-76100 Rehovot, Israel.
RP Hitron, Y (corresponding author), Weizmann Inst Sci, Dept Comp Sci & Appl Math, IL-76100 Rehovot, Israel.
EM yael.hitron@weizmann.ac.il; merav.parter@weizmann.ac.il
CR Adrian ED, 1926, J PHYSIOL-LONDON, V61, P49, DOI 10.1113/jphysiol.1926.sp002273
   Allman MJ, 2014, ANNU REV PSYCHOL, V65, P743, DOI 10.1146/annurev-psych-010213-115117
   ARMSTRON.DB, 1969, IEEE T COMPUT, VC 18, P1110, DOI 10.1109/T-C.1969.222594
   Awerbuch B., 1990, Proceedings. 31st Annual Symposium on Foundations of Computer Science (Cat. No.90CH2925-6), P514, DOI 10.1109/FSCS.1990.89572
   Bjerregaard T, 2006, ACM COMPUT SURV, V38, P1, DOI 10.1145/1132952.1132953
   Chou Chi-Ning, 2019, 10 INN THEOR COMP SC
   DeVille REL, 2008, B MATH BIOL, V70, P1608, DOI 10.1007/s11538-008-9311-8
   Fan HW, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-017-19028-9
   Finnerty GT, 2015, J NEUROSCI, V35, P13912, DOI 10.1523/JNEUROSCI.2654-15.2015
   FLAJOLET P, 1985, BIT, V25, P113, DOI 10.1007/BF01934993
   Gerstner W, 1997, P NATL ACAD SCI USA, V94, P12740, DOI 10.1073/pnas.94.24.12740
   HAUCK S, 1995, P IEEE, V83, P69, DOI 10.1109/5.362752
   Ikeda K, 2006, CURR BIOL, V16, pR308, DOI 10.1016/j.cub.2006.03.085
   Kuhn F, 2010, PROC APPL MATH, V135, P949
   Legenstein Robert A., 2018, 9 INN THEOR COMP SCI
   Lindner B, 2009, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2009/01/P01008
   Lynch N., 2018, ARXIV180803884
   Lynch Nancy, 2017, P 8 C INN THEOR COMP
   Lynch Nancy, 2017, 5 WORKSH BIOL DISTR
   Lynch Nancy A., 2017, 31 INT S DISTR COMP
   Ma J, 2015, CHAOS SOLITON FRACT, V80, P31, DOI 10.1016/j.chaos.2015.02.005
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maass Wolfgang, 1994, ELECT C COMPUTATIONA, V1
   Maass Wolfgang, 1996, ADV NEURAL INFORM PR, V8
   Manohar R, 2017, INT SYMP ASYNCHRON C, P102, DOI 10.1109/ASYNC.2017.15
   Merchant H, 2013, ANNU REV NEUROSCI, V36, P313, DOI 10.1146/annurev-neuro-062012-170349
   MORRIS R, 1978, COMMUN ACM, V21, P840, DOI 10.1145/359619.359627
   Papadimitriou Christos H, 2018, 10 INN THEOR COMP SC
   Sparso J, 2001, PRINCIPLES OF ASYNCHRONOUS CIRCUIT DESIGN: A SYSTEMS PERSPECTIVE, P3
   Tsodyks MV, 1997, P NATL ACAD SCI USA, V94, P719, DOI 10.1073/pnas.94.2.719
   Wang Barbeeba, 2019, ARXIV190301217
NR 31
TC 0
Z9 0
U1 0
U2 0
PY 2019
VL 144
AR 57
DI 10.4230/LIPIcs.ESA.2019.57
WC Computer Science, Theory & Methods; Mathematics, Applied
DA 2023-11-11
ER

PT J
AU Hu, B
   Xu, MB
   Wang, ZZ
   Xia, ZN
   Wang, DJ
AF Hu, B.
   Xu, M. B.
   Wang, Z. Z.
   Xia, Z. N.
   Wang, D. J.
TI The inhibition mechanism of epilepsy seizures in a spiking neural
   network
SO BASIC & CLINICAL PHARMACOLOGY & TOXICOLOGY
DT Meeting Abstract
C1 [Hu, B.; Xu, M. B.; Wang, Z. Z.; Xia, Z. N.; Wang, D. J.] Zhejiang Univ Technol, Dept Appl Math, Hangzhou 310023, Peoples R China.
EM djhubingst@163.com; wangdingj@126.com
NR 0
TC 0
Z9 0
U1 0
U2 0
PD NOV
PY 2020
VL 127
SU 4
SI SI
MA 027
BP 17
EP 18
WC Pharmacology & Pharmacy; Toxicology
DA 2023-11-11
ER

PT C
AU Long, LN
AF Long, Lyle N.
GP IEEE
TI Adaptive Spiking Neural Networks with Hodgkin-Huxley Neurons and Hebbian
   Learning
SO 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUL 31-AUG 05, 2011
CL San Jose, CA
CR Dan Y, 2004, NEURON, V44, P23, DOI 10.1016/j.neuron.2004.09.007
   Gupta A., 2009, IEEE INT JOINT C NEU
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Long L. N., 2010, AIAA INFOTECH AER C
   Palm G., J COMP NEUROLOGY, V286, P442
NR 5
TC 0
Z9 0
U1 0
U2 1
PY 2011
BP 165
EP 165
WC Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Computer Science, Theory & Methods; Engineering, Electrical &
   Electronic
DA 2023-11-11
ER

PT C
AU INCHIOSA, ME
AF INCHIOSA, ME
BE Handel, PH
   Chung, AL
TI THE EFFECT OF NOISE ON A NEURAL-NETWORK WITH SPIKING NEURONS
SO NOISE IN PHYSICAL SYSTEMS AND 1/F FLUCTUATIONS
SE AIP Conference Proceedings
DT Proceedings Paper
CT INTERNATIONAL CONF ON NOISE IN PHYSICAL SYSTEMS AND 1/F FLUCTUATION (
   ICNF 93 )
CY 1993
CL UNIV MISSOURI, ST LOUIS, MO
HO UNIV MISSOURI
C1 USN, CTR COMMAND CONTROL & OCEAN SURVEILLANCE, DIV RDT & E, SAN DIEGO, CA 92152 USA.
NR 0
TC 0
Z9 0
U1 0
U2 0
PY 1993
IS 285
BP 741
EP 744
WC Physics, Applied; Physics, Condensed Matter
DA 2023-11-11
ER

PT C
AU Espinal, A
   Carpio, M
   Ornelas, M
   Puga, H
   Melín, P
   Sotelo-Figueroa, M
AF Espinal, Andres
   Carpio, Martin
   Ornelas, Manuel
   Puga, Hector
   Melin, Patricia
   Sotelo-Figueroa, Marco
BE MartinezTrinidad, JF
   CarrascoOchoa, JA
   OlveraLopez, JA
   SalasRodriguez, J
   Suen, CY
TI Developing Architectures of Spiking Neural Networks by Using Grammatical
   Evolution Based on Evolutionary Strategy
SO PATTERN RECOGNITION, MCPR 2014
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 6th Mexican Conference on Pattern Recognition (MCPR)
CY JUN 25-28, 2014
CL Cancun, MEXICO
ID PARTICLE SWARM OPTIMIZATION
AB The Artificial Neural Networks (ANNs) have been used for solving problems in many theoretical and practical areas. Advances on the field of ANNs have derived in Spiking Neural Networks (SNNs); which are considered as the third generation of ANNs. SNNs receive/send the information by timing of events (spikes) instead by the spike rate; as their predecessors do. Although SNNs are capable to solve some functions with fewer neurons than networks of previous generations, there aren't rules to set the architecture of any kind of ANN for solving a specific task; usually the architecture is set empirically based on the designer's experience and the neural network's performance over the problem. Recently, metaheuristic algorithms are being implemented to optimize some aspect on ANNs such as weight, connections and even the architecture. This work proposes a generic framework for automatic construction of Fully-Connected Feed-Forward Spiking Neural Networks through an indirect representation by means of Grammatical Evolution (GE) based on Evolutionary Strategy (ES) algorithm. Two well-known benchmarks datasets of pattern recognition were used for testing the proposal of this paper.
C1 [Espinal, Andres; Carpio, Martin; Ornelas, Manuel; Puga, Hector; Sotelo-Figueroa, Marco] Inst Tecnol Leon, Av Tecnol S-N, Leon, Gto, Mexico.
   [Melin, Patricia] Inst Technol Tijuana, Tijuana, Mexico.
RP Espinal, A (corresponding author), Inst Tecnol Leon, Av Tecnol S-N, Leon, Gto, Mexico.
EM andres.espinal@itleon.edu.mx
CR [Anonymous], 2005, SEARCH METHODOLOGIES
   [Anonymous], NEURAL NETWORK MODEL
   [Anonymous], P 14 INT C COMP SUPP
   [Anonymous], 2010, BIOL INSPIRED NEURAL
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   López LFD, 2012, J COMPUT SCI-NETH, V3, P46, DOI 10.1016/j.jocs.2011.12.005
   Dempsey I., 2009, SCI, V194
   Ding SF, 2013, ARTIF INTELL REV, V39, P251, DOI 10.1007/s10462-011-9270-6
   FANG HL, 1993, PROCEEDINGS OF THE FIFTH INTERNATIONAL CONFERENCE ON GENETIC ALGORITHMS, P375
   Garro BA, 2009, IEEE IJCNN, P2363
   GERSTNER W, 1995, PHYS REV E, V51, P738, DOI 10.1103/PhysRevE.51.738
   Gerstner W., 2002, SPIKING NEURON MODEL
   Holland J. H., 1992, ADAPTATION NATURAL A
   Johnson C, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P1802, DOI 10.1109/IJCNN.2011.6033443
   Kohavi R., 1995, INT JOINT C ART INT, V14, P1137, DOI [DOI 10.1067/MOD.2000.109031, 10.1067/mod.2000.109031]
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maass W., 1996, NEURAL INFORM PROCES, P211
   O'Neill M., 2006, INT C ART INT ICAI 2
   Rechenberg I., 1973, EVOLUTIONS STRATEGIE
   Ryan C., 1998, Genetic Programming. First European Workshop, EuroGP'98. Proceedings, P83, DOI 10.1007/BFb0055930
   Yao X, 1999, P IEEE, V87, P1423, DOI 10.1109/5.784219
NR 21
TC 3
Z9 3
U1 0
U2 1
PY 2014
VL 8495
BP 71
EP +
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods; Robotics
DA 2023-11-11
ER

PT C
AU Sengupta, A
   Ankit, A
   Roy, K
AF Sengupta, Abhronil
   Ankit, Aayush
   Roy, Kaushik
GP IEEE
TI Performance Analysis and Benchmarking of All-Spin Spiking Neural
   Networks
SO 2017 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY MAY 14-19, 2017
CL Anchorage, AK
DE Spiking Neural Network; Spintronics; Domain-Wall Motion; All-Spin
   Crossbar Architectures
ID DEVICE
AB Spiking Neural Network based brain-inspired computing paradigms are becoming increasingly popular tools for various cognitive tasks. The sparse event-driven processing capability enabled by such networks can be potentially appealing for implementation of low-power neural computing platforms. However, the parallel and memory-intensive computations involved in such algorithms is in complete contrast to the sequential fetch, decode, execute cycles of conventional von-Neumann processors. Recent proposals have investigated the design of spintronic "in-memory" crossbar based computing architectures driving "spin neurons" that can potentially alleviate the memory-access bottleneck of CMOS based systems and simultaneously offer the prospect of low-power inner product computations. In this article, we perform a rigorous system-level simulation study of such All-Spin Spiking Neural Networks on a benchmark suite of 6 recognition problems ranging in network complexity from 10k-7.4M synapses and 195-9.2k neurons. System level simulations indicate that the proposed spintronic architecture can potentially achieve similar to 1292x energy efficiency and similar to 235x speedup on average over the benchmark suite in comparison to an optimized CMOS implementation at 45nm technology node.
C1 [Sengupta, Abhronil; Ankit, Aayush; Roy, Kaushik] Purdue Univ, Sch Elect & Comp Engn, W Lafayette, IN 47907 USA.
RP Sengupta, A (corresponding author), Purdue Univ, Sch Elect & Comp Engn, W Lafayette, IN 47907 USA.
EM asengup@purdue.edu
CR [Anonymous], BIOM CIRC SYST C BIO
   [Anonymous], 2011, 2011 INT EL DEV M WA
   Diehl Peter U, 2015, 2015 INT JOINT C NEU, P1, DOI [DOI 10.1109/IJCNN.2015.7280696, 10.1109/IJCNN.2015.7280696]
   Emori S, 2014, PHYS REV B, V90, DOI 10.1103/PhysRevB.90.184427
   Emori S, 2013, NAT MATER, V12, P611, DOI [10.1038/NMAT3675, 10.1038/nmat3675]
   Hinton G, 2009, LEARNING MULTIPLE LA
   Jo SH, 2010, NANO LETT, V10, P1297, DOI 10.1021/nl904092h
   Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lin C., 2009, ELECT DEVICES M IEDM, P1
   Martinez E, 2014, J APPL PHYS, V115, DOI 10.1063/1.4881778
   Muralimanohar N, 2007, INT SYMP MICROARCH, P3, DOI 10.1109/MICRO.2007.33
   Netzer Yuval, 2011, P NIPS
   Prezioso M, 2016, IEEE INT SYMP CIRC S, P177, DOI 10.1109/ISCAS.2016.7527199
   Prezioso M, 2015, NATURE, V521, P61, DOI 10.1038/nature14441
   Ryu KS, 2013, NAT NANOTECHNOL, V8, P527, DOI [10.1038/NNANO.2013.102, 10.1038/nnano.2013.102]
   Sengupta A, 2016, IEEE T CIRCUITS-I, V63, P2267, DOI 10.1109/TCSI.2016.2615312
   Vansteenkiste A, 2014, AIP ADV, V4, DOI 10.1063/1.4899186
NR 18
TC 10
Z9 11
U1 0
U2 2
PY 2017
BP 4557
EP 4563
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Schliebs, S
   Defoin-Platel, M
   Worner, S
   Kasabov, N
AF Schliebs, Stefan
   Defoin-Platel, Michael
   Worner, Sue
   Kasabov, Nikola
TI Integrated feature and parameter optimization for an evolving spiking
   neural network: Exploring heterogeneous probabilistic models
SO NEURAL NETWORKS
DT Article; Proceedings Paper
CT International Joint Conference on Neural Networks
CY JUN 14-19, 2009
CL Atlanta, GA
DE Evolving spiking neural network; Quantum-inspired evolutionary
   algorithm; Multiple probabilistic model; Estimation of distribution
   algorithm
AB This study introduces a quantum-inspired spiking neural network (QiSNN) as an integrated connectionist system, in which the features and parameters of an evolving spiking neural network are optimized together with the use of a quantum-inspired evolutionary algorithm. We propose here a novel optimization method that uses different representations to explore the two search spaces: A binary representation for optimizing feature subsets and a continuous representation for evolving appropriate real-valued configurations of the spiking network. The properties and characteristics of the improved framework are Studied on two different synthetic benchmark datasets. Results are compared to traditional methods, namely a multi-layer-perceptron and a naive Bayesian classifier (NBC). A previously used real world ecological dataset on invasive species establishment prediction is revisited and new results are obtained and analyzed by an ecological expert. The proposed method results in a much faster convergence to an Optimal Solution (or a close to it), in a better accuracy, and in a more informative set of features selected. (C) 2009 Elsevier Ltd. All rights reserved.
C1 [Schliebs, Stefan; Kasabov, Nikola] Auckland Univ Technol, KEDRI, Auckland, New Zealand.
   [Worner, Sue] Lincoln Univ, Ctr Bioprotect, Canterbury, New Zealand.
RP Schliebs, S (corresponding author), Auckland Univ Technol, KEDRI, Auckland, New Zealand.
EM sschlieb@aut.ac.nz; michael.defoinplatel@gmail.com;
   worner@lincoln.ac.nz; nkasabov@aut.ac.nz
CR [Anonymous], CROP PROT COMP
   Benuskova L., 2007, TOP BIOMED ENG
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Bosman P. A. N., 2000, Parallel Problem Solving from Nature PPSN VI. 6th International Conference. Proceedings (Lecture Notes in Computer Science Vol.1917), P767
   DEFOINPLATEL M, 2009, IEEE T EVOL IN PRESS
   Estévez PA, 2009, IEEE T NEURAL NETWOR, V20, P189, DOI 10.1109/TNN.2008.2005601
   Gallagher M, 2005, EVOL COMPUT, V13, P29, DOI 10.1162/1063656053583478
   Gallagher M, 1999, GECCO-99: PROCEEDINGS OF THE GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P840
   Gerstner W., 2002, SPIKING NEURON MODEL
   GEWEKE J, 1991, COMPUTING SCIENCE AND STATISTICS, P571
   Han KH, 2002, IEEE T EVOLUT COMPUT, V6, P580, DOI 10.1109/TEVC.2002.804320
   Iannella N, 2005, INFORM PROCESS LETT, V95, P545, DOI 10.1016/j.ipl.2005.05.022
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Kasabov N., 2007, EVOLVING CONNECTIONI
   Kasabov N, 2008, IEEE COMPUT INTELL M, V3, P23, DOI 10.1109/MCI.2008.926584
   Kasabov Nikola, 2009, Natural Computing, V8, P199, DOI 10.1007/s11047-008-9066-z
   Knoblauch A, 2005, INFORM PROCESS LETT, V95, P537, DOI 10.1016/j.ipl.2005.05.021
   Lang KJ., 1988, P 1988 CONN MOD SUMM, P52
   Lozano JA., 2006, NEW EVOLUTIONARY COM, DOI 10.1007/3-540-32494-1
   MAASS W, 1999, COMPUTING SPIKING NE
   Mininno E, 2008, IEEE T EVOLUT COMPUT, V12, P203, DOI 10.1109/TEVC.2007.896689
   Mühlenbein H, 1999, J HEURISTICS, V5, P215, DOI 10.1023/A:1009689913453
   Platel MD, 2007, IEEE C EVOL COMPUTAT, P423
   Potter MA, 2000, EVOL COMPUT, V8, P1, DOI 10.1162/106365600568086
   Schliebs S, 2009, INT JOINT C NEUR NET, P2833
   SCHLIEBS S, 2009, LNCS
   Sebag M, 1998, LECT NOTES COMPUT SC, V1498, P418, DOI 10.1007/BFb0056884
   Servet I, 1998, LECT NOTES COMPUT SC, V1363, P137, DOI 10.1007/BFb0026596
   Soltic S., 2008, IEEE WORLD C COMP IN
   THORPE SJ, 1997, ESANN
   VALKO M, 2005, P 2005 PORT C ART IN
   VanRullen R, 2001, PERCEPTION, V30, P655, DOI 10.1068/p3029
   Vera MT, 2002, ENVIRON ENTOMOL, V31, P1009, DOI 10.1603/0046-225X-31.6.1009
   Verstraeten D., 2005, ESANN, V5, P435
   Watts MJ, 2006, IEEE IJCNN, P1840
   WORNER S, 2008, ECOLOGICAL INFORM
   Wysoski S. G., 2008, THESIS AUCKLAND U TE
   WYSOSKI SG, 2006, ICANN, V1, P61
   Wysoski SG, 2008, NEUROCOMPUTING, V71, P2563, DOI 10.1016/j.neucom.2007.12.038
   Yuan B, 2003, IEEE C EVOL COMPUTAT, P443, DOI 10.1109/CEC.2003.1299609
NR 40
TC 53
Z9 55
U1 3
U2 12
PD JUL-AUG
PY 2009
VL 22
IS 5-6
SI SI
BP 623
EP 632
DI 10.1016/j.neunet.2009.06.038
WC Computer Science, Artificial Intelligence; Neurosciences
DA 2023-11-11
ER

PT J
AU Salustri, M
   Micheletto, R
AF Salustri, Marcello
   Micheletto, Ruggero
TI Heterogeneous Axonal Delay Improves the Spiking Activity Propagation on
   a Toroidal Network
SO COGNITIVE COMPUTATION
DT Article
DE Noise delay; Spike; Stochastic resonance
AB Several studies have looked into how noise affects neural networks and actual brains as evidenced by transcranial random noise stimulation, which improves cognitive performance. This research aims to broaden this understanding by concentrating on the network structural heterogeneity realized by adding noise to a neural model network's axonal propagation delay. We utilized the pyNEST neural network simulator to model a network of 400 artificial Izhikevich neurons connected by a folded von Neumann neighborhood to form a toroidal shape where axonal propagation noise simulates a variable spatial spacing between neurons. In this network only one neuron is regularly spiking at first because it is specifically stimulated by a 10mA external current, while all the other neurons have no external input and are stimulated solely by the activity of their neighbors. The forward propagation of the spiking wave from the original neuron to its neighbors, and then to distant nodes on the toroidal network, was investigated. For each simulation, we recorded the activity of all the network changing several parameters to verify differences of spike activity in different positions on the torus. By manipulating heterogeneity, we discovered that adding noise helps the signal reach distant neurons in 20% less time, compared to when there is no heterogeneity. We demonstrated for the first time that structural heterogeneity in a neural network can favor the propagation of spiking waves. This result is in line with other findings that suggest that a certain level of noise is good for the brain, extending this concept to the network physical structure.
C1 [Salustri, Marcello; Micheletto, Ruggero] Yokohama City Univ, Grad Sch Nanobiosci, Cognit Informat Lab, Kanazawa Ku, 22-2 Seto, Yokohama, Kanagawa 2360027, Japan.
RP Micheletto, R (corresponding author), Yokohama City Univ, Grad Sch Nanobiosci, Cognit Informat Lab, Kanazawa Ku, 22-2 Seto, Yokohama, Kanagawa 2360027, Japan.
EM ruggero@yokohama-cu.ac.jp
CR Asl MM, 2017, SCI REP-UK, V7, DOI 10.1038/srep39682
   Chang W, 2008, J SYST ENG ELECTRON, V19, P694, DOI 10.1016/S1004-4132(08)60141-3
   Chrysafides SM, 2021, PHYSL RESTING POTENT
   Coli M, 2000, ISCAS 2000: IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS - PROCEEDINGS, VOL IV, P137, DOI 10.1109/ISCAS.2000.858707
   Eppler Jochen Martin, 2008, Front Neuroinform, V2, P12, DOI 10.3389/neuro.11.012.2008
   Expert P, 2019, NETW NEUROSCI, V3, P653, DOI 10.1162/netn_e_00096
   Faisal AA, 2008, NAT REV NEUROSCI, V9, P292, DOI 10.1038/nrn2258
   Gardner RJ, 2022, NATURE, V602, P123, DOI 10.1038/s41586-021-04268-7
   Gewaltig M. O., 2012, COMPUTATIONAL SYSTEM, P533, DOI [10.1007/978-94-007-3858-4_18, DOI 10.1007/978-94-007-3858-4_18]
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Koch C, 1999, SCIENCE, V284, P96, DOI 10.1126/science.284.5411.96
   Meriney SD, 2019, SYNAPTIC TRANSMISSION, P449, DOI 10.1016/B978-0-12-815320-8.00021-1
   Moret B, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-51553-7
   Nobukawa S, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-017-18783-z
   Perez-Nieves N, 2021, NAT COMMUN, V12, DOI 10.1038/s41467-021-26022-3
   Schneider A, 2010, DTSCH ARZTEBL INT, V107, P776, DOI [10.3238/arztebl.2010.0776, 10.3238/arztebl.2010.0799]
   Schwiening CJ, 2012, J PHYSIOL-LONDON, V590, P2571, DOI 10.1113/jphysiol.2012.230458
   Shannon C. E., 1948, BELL SYST TECH J, VVol. 27, P623, DOI [DOI 10.1002/J.1538-7305.1948.TB00917.X, DOI 10.1002/J.1538-7305.1948.TB01338.X]
   Stacey WC, 2000, J NEUROPHYSIOL, V83, P1394, DOI 10.1152/jn.2000.83.3.1394
   Suwanda R., 2020, Journal of Physics: Conference Series, V1566, DOI 10.1088/1742-6596/1566/1/012058
   Tozzi A, 2016, COGN NEURODYNAMICS, V10, P189, DOI 10.1007/s11571-016-9379-z
   Tzedakis G, 2015, CANCER INFORM, V14, P67, DOI 10.4137/CIN.S19343
   Von Neumann J., 1956, AUTOMATA STUDIES, P45
   Winfree AT, 2001, GEOMETRY BIOL TIME, V12, P29714, DOI [10.1038/35065725, DOI 10.1038/35065725]
NR 24
TC 2
Z9 2
U1 0
U2 3
PD JUL
PY 2023
VL 15
IS 4
BP 1231
EP 1242
DI 10.1007/s12559-022-10034-2
EA JUN 2022
WC Computer Science, Artificial Intelligence; Neurosciences
DA 2023-11-11
ER

PT J
AU Yang, Z
   Nai, W
   Li, D
AF Yang, Zan
   Nai, Wei
   Li, Dan
TI Application of Triple Generative Adversarial Network with Spiking Neural
   Network in Tumor Medical Image Enhancement for Lung Cancer Diagnosis
SO BASIC & CLINICAL PHARMACOLOGY & TOXICOLOGY
DT Meeting Abstract
C1 [Yang, Zan; Li, Dan] Tongji Zhejiang Coll, Fac Sci, Jiaxing, Peoples R China.
   [Nai, Wei] Tongji Zhejiang Coll, Dept Elect & Informat Engn, Jiaxing, Peoples R China.
NR 0
TC 0
Z9 0
U1 0
U2 7
PD AUG
PY 2020
VL 127
SU 1
SI SI
MA 081
BP 48
EP 48
WC Pharmacology & Pharmacy; Toxicology
DA 2023-11-11
ER

PT J
AU Göltz, J
   Kriener, L
   Sabado, V
   Petrovici, MA
AF Goeltz, Julian
   Kriener, Laura
   Sabado, Virginie
   Petrovici, Mihai A.
TI Fast and Energy-efficient deep Neuromorphic Learning
SO ERCIM NEWS
DT Article
AB Many neuromorphic platforms promise fast and energy-efficient emulation of spiking neural networks, but unlike artificial neural networks, spiking networks have lacked a powerful universal training algorithm for more challenging machine learning applications. Such a training scheme has recently been proposed and using it together with a biologically inspired form of information coding shows state-of-the-art results in terms of classification accuracy, speed and energy consumption.
C1 [Goeltz, Julian; Petrovici, Mihai A.] Heidelberg Univ, Heidelberg, Germany.
   [Goeltz, Julian; Kriener, Laura; Sabado, Virginie; Petrovici, Mihai A.] Univ Bern, Bern, Switzerland.
RP Göltz, J (corresponding author), Univ Bern, NeuroTMA Grp, Dept Physiol, Bern, Switzerland.; Göltz, J (corresponding author), Heidelberg Univ, Kirchhoff Inst Phys, Heidelberg, Germany.
EM julian.goeltz@kip.uni-heidelberg.de
CR BILLAUDELLE S, 2020, 2020 IEEE INT S CIRC
   Goltz J., 2019, ARXIV PREPRINT ARXIV
   Kriener L., 2021, ARXIV210208211
NR 3
TC 0
Z9 0
U1 0
U2 0
PD APR
PY 2021
IS 125
SI SI
BP 17
EP 18
WC Computer Science, Interdisciplinary Applications
DA 2023-11-11
ER

PT J
AU Nakano, T
   Otsuka, M
   Yoshimoto, J
   Doya, K
AF Nakano, Takashi
   Otsuka, Makoto
   Yoshimoto, Junichiro
   Doya, Kenji
TI A Spiking Neural Network Model of Model-Free Reinforcement Learning with
   High-Dimensional Sensory Input and Perceptual Ambiguity
SO PLOS ONE
DT Article
ID REPRESENTATION; CATEGORIES
AB A theoretical framework of reinforcement learning plays an important role in understanding action selection in animals. Spiking neural networks provide a theoretically grounded means to test computational hypotheses on neurally plausible algorithms of reinforcement learning through numerical simulation. However, most of these models cannot handle observations which are noisy, or occurred in the past, even though these are inevitable and constraining features of learning in real environments. This class of problem is formally known as partially observable reinforcement learning (PORL) problems. It provides a generalization of reinforcement learning to partially observable domains. In addition, observations in the real world tend to be rich and high-dimensional. In this work, we use a spiking neural network model to approximate the free energy of a restricted Boltzmann machine and apply it to the solution of PORL problems with high-dimensional observations. Our spiking network model solves maze tasks with perceptually ambiguous high-dimensional observations without knowledge of the true environment. An extended model with working memory also solves history-dependent tasks. The way spiking neural networks handle PORL problems may provide a glimpse into the underlying laws of neural information processing which can only be discovered through such a top-down approach.
C1 [Nakano, Takashi] Okinawa Inst Sci & Technol, Neurobiol Res Unit, Kunigami, Okinawa 9040495, Japan.
   [Otsuka, Makoto; Yoshimoto, Junichiro; Doya, Kenji] Okinawa Inst Sci & Technol, Neural Computat Unit, Kunigami, Okinawa 9040495, Japan.
RP Nakano, T (corresponding author), Okinawa Inst Sci & Technol, Neurobiol Res Unit, 1919-1 Tancha, Kunigami, Okinawa 9040495, Japan.
EM nakano@oist.jp
CR Bakker B, 2002, ADV NEUR IN, V14, P1475
   Belavkin RV, 2011, COGN SYST RES, V12, P93, DOI 10.1016/j.cogsys.2010.08.003
   Boerlin M, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1001080
   Doya K, 2002, NEURAL NETWORKS, V15, P495, DOI 10.1016/S0893-6080(02)00044-8
   Elfwing S, 2010, LECT NOTES COMPUT SC, V6443, P215, DOI 10.1007/978-3-642-17537-4_27
   Florian RV, 2007, NEURAL COMPUT, V19, P1468, DOI 10.1162/neco.2007.19.6.1468
   Freedman DJ, 2006, NATURE, V443, P85, DOI 10.1038/nature05078
   Freedman DJ, 2001, SCIENCE, V291, P312, DOI 10.1126/science.291.5502.312
   Gerstner W., 2002, SPIKING NEURON MODEL
   Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018
   Hollensen P, 2011, 21 ANN C JAP NEUR NE
   Izhikevich EM, 2007, CEREB CORTEX, V17, P2443, DOI 10.1093/cercor/bhl152
   Kwee I, 2001, LECT NOTES COMPUT SC, V2130, P865
   Matsuda W, 2009, J NEUROSCI, V29, P444, DOI 10.1523/JNEUROSCI.4029-08.2009
   Miller EK, 2002, PHILOS T R SOC B, V357, P1123, DOI 10.1098/rstb.2002.1099
   Otsuka M., 2010, P EUR S ART NEUR NET, P541
   Otsuka M, 2008, LECT NOTES COMPUT SC, V5163, P377, DOI 10.1007/978-3-540-87536-9_39
   Potjans W, 2009, NEURAL COMPUT, V21, P301, DOI 10.1162/neco.2008.08-07-593
   Reynolds JNJ, 2001, NATURE, V413, P67, DOI 10.1038/35092560
   Rezende DJ, 2014, FRONTIERS COMPUTATIO, V8
   Roberts PD, 2008, BIOL CYBERN, V99, P517, DOI 10.1007/s00422-008-0265-6
   Saeb S, 2009, NEURAL NETWORKS, V22, P586, DOI 10.1016/j.neunet.2009.06.049
   Sallans B, 2004, J MACH LEARN RES, V5, P1063
   Samejima K, 2005, SCIENCE, V310, P1337, DOI 10.1126/science.1115270
   Schmidhuber J, 2014, ABS14047828 CORR
   Schultz W, 1997, SCIENCE, V275, P1593, DOI 10.1126/science.275.5306.1593
   Sutton R. S., 2015, REINFORCEMENT LEARNI, V2nd
   Szatmáry B, 2010, PLOS COMPUT BIOL, V6, DOI 10.1371/journal.pcbi.1000879
   WHITEHEAD SD, 1995, ARTIF INTELL, V73, P271, DOI 10.1016/0004-3702(94)00012-P
NR 29
TC 12
Z9 13
U1 1
U2 13
PD MAR 3
PY 2015
VL 10
IS 3
AR e0115620
DI 10.1371/journal.pone.0115620
WC Multidisciplinary Sciences
DA 2023-11-11
ER

PT C
AU Lewden, P
   Vincent, AF
   Meyer, C
   Tomas, J
   Siami, S
   Saïghi, S
AF Lewden, Pierre
   Vincent, Adrien F.
   Meyer, Charly
   Tomas, Jean
   Siami, Shidoush
   Saighi, Sylvain
GP IEEE
TI Hardware Spiking Neural Networks: Slow Tasks Resilient Learning with
   Longer Term-Memory Bits
SO 2019 IEEE BIOMEDICAL CIRCUITS AND SYSTEMS CONFERENCE (BIOCAS 2019)
SE Biomedical Circuits and Systems Conference
DT Proceedings Paper
CT IEEE Biomedical Circuits and Systems Conference (BioCAS)
CY OCT 17-19, 2019
CL Nara, JAPAN
DE neuromorphic systems; spiking neural networks; memristors; spike
   timing-dependent plasticity
AB When aiming at efficient and low-power processing of event-based data, hardware implementations of spiking neural networks that co-integrate analog silicon neurons with memristive synaptic crossbar arrays are a promising framework. Fully analog systems however commonly make it difficult to learn patterns with real-world timescales, which are typically beyond the millisecond, due to intrinsic constraints of the underlying technologies. In this work, we propose to alleviate this issue by supplementing each presynaptic unit with a single memory bit, which allows to implement a hardware-friendly Spike Timing-Dependent Plasticity. By simulation means on the N-MNIST dataset, we illustrate the potential of this concept and show its robustness to postsynaptic neuron variability. We also discuss how to circumvent challenges raised by initial weight distribution. These results could facilitate the emergence of embedded smart systems directly fed by event-based sensors.
C1 [Lewden, Pierre; Vincent, Adrien F.; Meyer, Charly; Tomas, Jean; Saighi, Sylvain] Univ Bordeaux, Bordeaux INP, CNRS, Lab Integrat Materiau Syst, Talence, France.
   [Siami, Shidoush] Univ Pau & Pays Adour, IUT Pays Adour, Commun & Networking Dept, Pau, France.
RP Lewden, P (corresponding author), Univ Bordeaux, Bordeaux INP, CNRS, Lab Integrat Materiau Syst, Talence, France.
EM pierre.lewden@ims-bordeaux.fr; adrien.vincent@ims-bordeaux.fr;
   charly.meyer@ims-bordeaux.fr; jean.tomas@ims-bordeaux.fr;
   ssiami@univ-pau.fr; sylvain.saighi@ims-bordeaux.fr
CR [Anonymous], 2018, NATURE, V554
   Boyn S, 2017, NAT COMMUN, V8, DOI 10.1038/ncomms14736
   Chanthbouala A., 2012, NATURE MAT, V11
   Friedmann S, 2017, IEEE T BIOMED CIRC S, V11, P128, DOI 10.1109/TBCAS.2016.2579164
   Indiveri G, 2015, P IEEE, V103, P1379, DOI 10.1109/JPROC.2015.2444094
   Iyer L. R., 2017, INT JOINT C NEUR NET
   Kuzum D, 2013, NANOTECHNOLOGY, V24, DOI 10.1088/0957-4484/24/38/382001
   Lecerf G, 2014, IEEE INT SYMP CIRC S, P1568, DOI 10.1109/ISCAS.2014.6865448
   Moradi S, 2018, IEEE T BIOMED CIRC S, V12, P106, DOI 10.1109/TBCAS.2017.2759700
   Nessler B, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003037
   Orchard G, 2015, FRONT NEUROSCI-SWITZ, V9, DOI [10.3389/fnins.2015.00437, 10.3389/fhins.2015.00437]
   Posch C., 2014, P IEEE, V102
   Prezioso M., 2018, NATURE COMMUNICATION, V9
   Querlioz D., 2013, IEEE T NANOTECHNOLOG, V12
   Querlioz D, 2015, P IEEE, V103, P1398, DOI 10.1109/JPROC.2015.2437616
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707
   Serrano-Gotarredona T, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00002
   Thakur CS, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00891
NR 18
TC 0
Z9 0
U1 0
U2 0
PY 2019
DI 10.1109/biocas.2019.8918992
WC Computer Science, Information Systems; Engineering, Biomedical;
   Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Stoecker, M
   Reitboeck, HJ
AF Stoecker, M
   Reitboeck, HJ
TI A neural network for position invariant pattern recognition combining
   spiking neurons with the Fourier-transform
SO INTERNATIONAL JOURNAL OF NEURAL SYSTEMS
DT Article
ID VISUAL-CORTEX; OSCILLATIONS; CAT
AB We present an approach for position invariant recognition of individual objects in composite scenes, combining neural networks and algorithmic methods. A dynamic network of spiking neurons is used to generate object definition and figure/ground separation via temporal signal correlations. A shift invariant representation of the network spike activity distribution is subsequently realized via the amplitude spectrum of the Fourier-transform. Objects and their transformed representations are therefore linked in the time domain. The model segregates scenes and classifies individual patterns independent of their position in the input scene.
C1 UNIV MARBURG, APPL PHYS & NEUROPHYS DEPT, D-35032 MARBURG, GERMANY.
CR Eckhorn R, 1990, NEURAL COMPUT, V2, P293, DOI 10.1162/neco.1990.2.3.293
   ECKHORN R, 1993, NEUROREPORT, V4, P243, DOI 10.1097/00001756-199303000-00004
   ECKHORN R, 1988, BIOL CYBERN, V60, P121, DOI 10.1007/BF00202899
   ENGEL AK, 1991, P NATL ACAD SCI USA, V88, P9136, DOI 10.1073/pnas.88.20.9136
   GRAY CM, 1989, NATURE, V338, P334, DOI 10.1038/338334a0
   Maass W, 1996, NEURAL COMPUT, V8, P1, DOI 10.1162/neco.1996.8.1.1
   REITBOECK H, 1969, INFORM CONTROL, V15, P130, DOI 10.1016/S0019-9958(69)90387-8
   Reitboeck H. J., 1989, SENSORY PROCESSING M, P307
   REITBOECK HJ, 1993, P IEEE ICNN, V2, P638
   REITBOECK HJ, 1983, SYNERGETICS BRAIN, P174
   REITBOECK HJ, 1980, THESIS PHILIPPS U MA
   Roth L, 1995, LECT NOTES COMPUT SC, V930, P720
   Stoecker M, 1996, NEUROCOMPUTING, V11, P123, DOI 10.1016/0925-2312(94)00054-9
   STOECKER M, 1993, THESIS PHILIPPS U MA
   VONDERMALSBURG C, 1981, 812 DEP NEUR
NR 15
TC 1
Z9 1
U1 0
U2 1
PD DEC
PY 1996
VL 7
IS 6
BP 727
EP 733
DI 10.1142/S0129065796000695
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT J
AU Gast, R
   Solla, SA
   Kennedy, A
AF Gast, Richard
   Solla, Sara A.
   Kennedy, Ann
TI Macroscopic dynamics of neural networks with heterogeneous spiking
   thresholds
SO PHYSICAL REVIEW E
DT Article
ID MODEL; INHIBITION
AB Mean-field theory links the physiological properties of individual neurons to the emergent dynamics of neural population activity. These models provide an essential tool for studying brain function at different scales; however, for their application to neural populations on large scale, they need to account for differences between distinct neuron types. The Izhikevich single neuron model can account for a broad range of different neuron types and spiking patterns, thus rendering it an optimal candidate for a mean-field theoretic treatment of brain dynamics in heterogeneous networks. Here we derive the mean-field equations for networks of all-to-all coupled Izhikevich neurons with heterogeneous spiking thresholds. Using methods from bifurcation theory, we examine the conditions under which the mean-field theory accurately predicts the dynamics of the Izhikevich neuron network. To this end, we focus on three important features of the Izhikevich model that are subject here to simplifying assumptions: (i) spike-frequency adaptation, (ii) the spike reset conditions, and (iii) the distribution of single-cell spike thresholds across neurons. Our results indicate that, while the mean-field model is not an exact model of the Izhikevich network dynamics, it faithfully captures its different dynamic regimes and phase transitions. We thus present a mean-field model that can represent different neuron types and spiking dynamics. The model comprises biophysical state variables and parameters, incorporates realistic spike resetting conditions, and accounts for heterogeneity in neural spiking thresholds. These features allow for a broad applicability of the model as well as for a direct comparison to experimental data.
C1 [Gast, Richard; Solla, Sara A.; Kennedy, Ann] Northwestern Univ, Feinberg Sch Med, Dept Neurosci, Chicago, IL 60611 USA.
RP Gast, R (corresponding author), Northwestern Univ, Feinberg Sch Med, Dept Neurosci, Chicago, IL 60611 USA.
EM richard.gast@northwestern.edu
CR Amit DJ, 1997, CEREB CORTEX, V7, P237, DOI 10.1093/cercor/7.3.237
   Bick C, 2020, J MATH NEUROSCI, V10, DOI 10.1186/s13408-020-00086-9
   Brunel N, 2000, J COMPUT NEUROSCI, V8, P183, DOI 10.1023/A:1008925309027
   Cakan Caglar, 2020, PLoS Comput Biol, V16, pe1007822, DOI 10.1371/journal.pcbi.1007822
   Chen L, 2022, J COMPUT NEUROSCI, V50, P445, DOI 10.1007/s10827-022-00825-9
   Chialvo DR, 2010, NAT PHYS, V6, P744, DOI 10.1038/NPHYS1803
   Coombes S, 2010, NEUROIMAGE, V52, P731, DOI 10.1016/j.neuroimage.2010.01.045
   Coombes S, 2019, POLITO SPR SER, P1, DOI 10.1007/978-3-319-71048-8_1
   Dayan P., 2001, THEORETICAL NEUROSCI
   Deco G, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.1000092
   Devalle F, 2017, PLOS COMPUT BIOL, V13, DOI 10.1371/journal.pcbi.1005881
   Doedel E.J., 2007, AUTO07P
   El Boustani S, 2009, NEURAL COMPUT, V21, P46, DOI 10.1162/neco.2009.02-08-710
   Engel AK, 2013, NEURON, V80, P867, DOI 10.1016/j.neuron.2013.09.038
   Fuhrmann G, 2002, J NEUROPHYSIOL, V88, P761, DOI 10.1152/jn.2002.88.2.761
   Gasior K, 2022, J COMPUT NEUROSCI, V50, P145, DOI 10.1007/s10827-021-00799-0
   Gast R, 2022, Arxiv, DOI arXiv:2206.08813
   Gast R, 2021, PHYS REV E, V104, DOI 10.1103/PhysRevE.104.044310
   Gast R, 2020, NEURAL COMPUT, V32, P1615, DOI 10.1162/neco_a_01300
   Gast R, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0225900
   Gigante G, 2007, PHYS REV LETT, V98, DOI 10.1103/PhysRevLett.98.148101
   Guerreiro I, 2022, Arxiv, DOI arXiv:2206.10370
   Hodge RD, 2019, NATURE, V573, P61, DOI 10.1038/s41586-019-1506-7
   Humphries MD, 2009, NEURAL NETWORKS, V22, P1174, DOI 10.1016/j.neunet.2009.07.018
   Izhikevich EM, 2001, NEURAL NETWORKS, V14, P883, DOI 10.1016/S0893-6080(01)00078-8
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2007, DYNAMICAL SYSTEMS NE, DOI 10.7551/mitpress/2526.001.0001
   Izhikevich EM, 2008, P NATL ACAD SCI USA, V105, P3593, DOI 10.1073/pnas.0712231105
   JANSEN BH, 1995, BIOL CYBERN, V73, P357, DOI 10.1007/BF00199471
   Jonke Z, 2017, J NEUROSCI, V37, P8511, DOI 10.1523/JNEUROSCI.2078-16.2017
   Kuznetsov Y.A., 2013, ELEMENTS APPL BIFURC
   Lau D, 2000, J NEUROSCI, V20, P9071
   LOPESDAS.FH, 1974, KYBERNETIK, V15, P27, DOI 10.1007/BF00270757
   Luke TB, 2013, NEURAL COMPUT, V25, P3207, DOI 10.1162/NECO_a_00525
   Montbrió E, 2020, PHYS REV LETT, V125, DOI 10.1103/PhysRevLett.125.248101
   Montbrió E, 2015, PHYS REV X, V5, DOI 10.1103/PhysRevX.5.021028
   Neske GT, 2015, J NEUROSCI, V35, P1089, DOI 10.1523/JNEUROSCI.2279-14.2015
   Nicola W, 2013, J COMPUT NEUROSCI, V35, P87, DOI 10.1007/s10827-013-0442-z
   Ott E, 2008, CHAOS, V18, DOI 10.1063/1.2930766
   Pietras B, 2019, PHYS REV E, V100, DOI 10.1103/PhysRevE.100.042412
   Potjans TC, 2014, CEREB CORTEX, V24, P785, DOI 10.1093/cercor/bhs358
   Rich S, 2022, CELL REP, V39, DOI 10.1016/j.celrep.2022.110863
   Robinson PA, 1997, PHYS REV E, V56, P826, DOI 10.1103/PhysRevE.56.826
   Schwalger T, 2017, PLOS COMPUT BIOL, V13, DOI 10.1371/journal.pcbi.1005507
   Spiegler A, 2010, NEUROIMAGE, V52, P1041, DOI 10.1016/j.neuroimage.2009.12.081
   Taher H, 2020, PLOS COMPUT BIOL, V16, DOI 10.1371/journal.pcbi.1008533
   van Vreeswijk C, 1998, NEURAL COMPUT, V10, P1321, DOI 10.1162/089976698300017214
   Vohryzek J, 2022, PHILOS T R SOC A, V380, DOI 10.1098/rsta.2021.0247
   Wang Y, 2004, J PHYSIOL-LONDON, V561, P65, DOI 10.1113/jphysiol.2004.073353
   WILSON HR, 1972, BIOPHYS J, V12, P1, DOI 10.1016/S0006-3495(72)86068-5
   Yang WG, 2013, J NEUROSCI, V33, P17373, DOI 10.1523/JNEUROSCI.2515-13.2013
NR 52
TC 0
Z9 0
U1 1
U2 1
PD FEB 16
PY 2023
VL 107
IS 2
AR 024306
DI 10.1103/PhysRevE.107.024306
WC Physics, Fluids & Plasmas; Physics, Mathematical
DA 2023-11-11
ER

PT J
AU Diambra, L
   Capurro, A
   Plastino, A
AF Diambra, L
   Capurro, A
   Plastino, A
TI Neural networks that learn how to detect epileptic spikes
SO PHYSICS LETTERS A
DT Article
ID CHAOS
AB We advance a method based on a neural network environment to detect epileptic spikes in EEG. The concomitant training process of the pertinent feed-forward neural networks involves information theoretic tools. The resulting technique can be used to predict the one-step-forward behavior of the EEG time series. An optimal number of neurons is determined that allows for the maximum correlation between EEG signals and the neural network output. Our approach is illustrated with reference to EEG time series of the interictal activity from focal epileptic patients. (C) 1998 Elsevier Science B.V.
C1 Natl Univ La Plata, Dept Fis, RA-1900 La Plata, Argentina.
   Inst Invest Biol Clemente Estable, Montevideo, Uruguay.
RP Diambra, L (corresponding author), Natl Univ La Plata, Dept Fis, CC 67, RA-1900 La Plata, Argentina.
CR ALBERT A, 1972, REGRESSION MOOREPENR
   [Anonymous], 1986, PARALLEL DISTRIBUTED, DOI DOI 10.7551/MITPRESS/5236.001.0001
   BABLOYANTZ A, 1986, P NATL ACAD SCI USA, V83, P3513, DOI 10.1073/pnas.83.10.3513
   DESTEXHE A, 1988, PHYS LETT A, V132, P101, DOI 10.1016/0375-9601(88)90262-9
   DIAMBRA L, 1995, PHYS REV E, V52, P4557, DOI 10.1103/PhysRevE.52.4557
   FRANK GW, 1990, PHYSICA D, V46, P427, DOI 10.1016/0167-2789(90)90103-V
   Gevins A., 1987, HDB ELECTROENCEPHALO, V1
   GOTMAN J, 1993, COMPUTER APPL SURG T
   GRASSBERGER P, 1983, PHYS REV LETT, V50, P346, DOI 10.1103/PhysRevLett.50.346
   JAYNES ET, 1957, PHYS REV, V106, P620, DOI 10.1103/PhysRev.106.620
   JAYNES ET, 1957, PHYS REV, V108, P171, DOI 10.1103/PhysRev.108.171
   LAPEDES A, 1987, NEURAL INFORMATION P, P442, DOI DOI 10.1142/9789814434102_0012
   Lapedes A., 1987, PREDICTION SYSTEM MO
   Levine RD, 1978, MAXIMUM ENTROPY PRIN
   MATUS IJ, 1991, PHYS REV A, V43, P5683, DOI 10.1103/PhysRevA.43.5683
   NIEDERMEYER E, 1982, ELECTROENCEPHALOGRAP
   PIJN JP, 1991, ELECTROEN CLIN NEURO, V79, P371, DOI 10.1016/0013-4694(91)90202-F
   ROSEMBLATT F, 1962, PRINCIPLS NEURODYNAM
   SHANNON CE, 1949, MATH THEORY INFORM
   Theiler J, 1996, ELECTROEN CLIN NEURO, V98, P213, DOI 10.1016/0013-4694(95)00240-5
   TORRES ME, 1996, INRIA RAPP RECH PROG, V5, P2812
   TORRES ME, 1995, P 1995 IEEE WORKSH N, V2, P791
NR 22
TC 6
Z9 6
U1 0
U2 1
PD APR 20
PY 1998
VL 241
IS 1-2
BP 61
EP 66
DI 10.1016/S0375-9601(98)00089-9
WC Physics, Multidisciplinary
DA 2023-11-11
ER

PT J
AU Chen, JK
   Qiu, XL
   Ding, CB
   Wu, YR
AF Chen, Jiankun
   Qiu, Xiaolan
   Ding, Chibiao
   Wu, Yirong
TI SAR image classification based on spiking neural network through
   spike-time dependent plasticity and gradient descent
SO ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING
DT Article
DE Spiking Neural Network (SNN); SAR image classification; Spike-Time
   Dependent Plasticity (STDP); Gradient descent
ID ERROR-BACKPROPAGATION; INTEGRATION; SYSTEMS
AB At present, the Synthetic Aperture Radar (SAR) image classification method based on Convolution Neural Network (CNN) has faced some problems such as poor noise resistance and generalization ability. Spiking Neural Network (SNN) is one of the core components of brain-like intelligence and has good application prospects. This article constructs a complete SAR image classifier based on unsupervised and supervised learning of SNN by using spike sequences with complex spatio-temporal information. We firstly expound on the spiking neuron model, the receptive field of SNN, and the construction of spike sequence. Then we put forward an unsupervised learning algorithm based on STDP and a supervised learning algorithm based on gradient descent in series. The average classification accuracy of single layer and bilayer unsupervised learning SNN in three categories images on MSTAR dataset is 81.1% and 82.9%, respectively. Furthermore, the convergent output spike sequences of unsupervised learning can be used as teaching signals. Based on the TensorFlow framework, a single layer supervised learning SNN is built from the bottom, and the classification accuracy reaches 90.2%. By comparing noise resistance and model parameters between SNNs and CNNs, the effectiveness and outstanding advantages of SNN are verified. Code to reproduce our experiments is available at https://github.com/Jiankun-chen/Supervis ed-SNN-with-GD.
C1 [Chen, Jiankun] Univ Chinese Acad Sci, Beijing, Peoples R China.
   [Chen, Jiankun; Qiu, Xiaolan; Ding, Chibiao] Aerosp Informat Res Inst, Inst Elect, Natl Key Lab Sci & Technol Microwave Imaging, Beijing, Peoples R China.
   [Chen, Jiankun; Qiu, Xiaolan] Chinese Acad Sci, Aerosp Informat Res Inst, Key Lab Technol Geospatial Informat Proc & Applica, Beijing, Peoples R China.
   [Wu, Yirong] Chinese Acad Sci, Aerosp Informat Res Inst, Beijing 100190, Peoples R China.
   [Qiu, Xiaolan] Chinese Acad Sci, Aerosp Informat Res Inst, Suzhou 215123, Peoples R China.
RP Qiu, XL (corresponding author), Aerosp Informat Res Inst, Inst Elect, Natl Key Lab Sci & Technol Microwave Imaging, Beijing, Peoples R China.
EM xlqiu@mail.ie.ac.cn
CR Bentes C, 2018, IEEE J OCEANIC ENG, V43, P258, DOI 10.1109/JOE.2017.2767106
   Blasch E, DARPA DATA COLLECTIO
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   Chen JK, 2021, Preprints, DOI [10.20944/preprints202102.0083.v1, 10.20944/preprints202102.0083.v1, DOI 10.20944/PREPRINTS202102.0083.V1]
   Chen SZ, 2016, IEEE T GEOSCI REMOTE, V54, P4806, DOI 10.1109/TGRS.2016.2551720
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   ENGEL AK, 1992, TRENDS NEUROSCI, V15, P218, DOI 10.1016/0166-2236(92)90039-B
   GERSTNER W, 1995, PHYS REV E, V51, P738, DOI 10.1103/PhysRevE.51.738
   Gerstner W., 2002, SPIKING NEURON MODEL
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   Gupta S., 2018, GITHUB REPOSITORY
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Kasabov N, 2016, NEURAL NETWORKS, V78, P1, DOI 10.1016/j.neunet.2015.09.011
   Kasabov NK, 2014, NEURAL NETWORKS, V52, P62, DOI 10.1016/j.neunet.2014.01.006
   Kheradpisheh SR, 2016, NEUROCOMPUTING, V205, P382, DOI 10.1016/j.neucom.2016.04.029
   Kistler WM, 1997, NEURAL COMPUT, V9, P1015, DOI 10.1162/neco.1997.9.5.1015
   Kistler WM, 2000, NEURAL COMPUT, V12, P385, DOI 10.1162/089976600300015844
   KNUDSEN EI, 1994, J NEUROSCI, V14, P3985
   Lapique L., 1907, J PHYSL PATHOL GEN, V9, P620, DOI DOI 10.1007/S00422-007-0189-6
   Lv Q, 2015, J SENSORS, V2015, DOI 10.1155/2015/538063
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   McAnally KI, 1996, P ROY SOC B-BIOL SCI, V263, P961, DOI 10.1098/rspb.1996.0142
   MCCULLOCH WS, 1990, B MATH BIOL, V52, P99, DOI 10.1016/S0092-8240(05)80006-0
   McKennoch S, 2006, IEEE IJCNN, P3970
   McKennoch S, 2009, NEURAL COMPUT, V21, P9, DOI 10.1162/neco.2008.09-07-610
   Morris RGM, 1999, BRAIN RES BULL, V50, P437, DOI 10.1016/S0361-9230(99)00182-3
   Oniz Y, 2014, J FRANKLIN I, V351, P3269, DOI 10.1016/j.jfranklin.2014.03.002
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Seth AK, 2015, CURR BIOL, V25, pR110, DOI 10.1016/j.cub.2014.12.043
   Sjostrom J., 2010, SCHOLARPEDIA 52 REVI, V5, P1362, DOI [10.4249/scholarpedia.1362, 10.4249%2Fscholarpedia.1362, DOI 10.4249/SCHOLARPEDIA.1362]
   THEUNISSEN F, 1995, J COMPUT NEUROSCI, V2, P149, DOI 10.1007/BF00961885
   Tino P, 2006, NEURAL COMPUT, V18, P591, DOI 10.1162/089976606775623360
   Wang, 2021, GITHUB REPOSITORY
   Wang L, 2021, IEEE T GEOSCI REMOTE, V59, P9257, DOI 10.1109/TGRS.2021.3051024
   Wang ZR, 2003, NEURON, V37, P463, DOI 10.1016/S0896-6273(02)01189-3
   Wu Y, 2011, PATTERN RECOGN LETT, V32, P1532, DOI 10.1016/j.patrec.2011.04.009
   Xie HM, 2014, INT GEOSCI REMOTE SE, DOI 10.1109/IGARSS.2014.6947062
   [徐丰 Xu Feng], 2017, [雷达学报, Journal of Radars], V6, P136
   Xu Y, 2013, NEURAL NETWORKS, V43, P99, DOI 10.1016/j.neunet.2013.02.003
   Xue RH, 2021, IEEE T GEOSCI REMOTE, V59, P1250, DOI 10.1109/TGRS.2020.2997288
   Zhang TL, 2016, IEEE SYS MAN CYBERN, P2301, DOI 10.1109/SMC.2016.7844581
   Zhu XX, 2017, IEEE GEOSC REM SEN M, V5, P8, DOI 10.1109/MGRS.2017.2762307
NR 43
TC 6
Z9 6
U1 5
U2 27
PD JUN
PY 2022
VL 188
BP 109
EP 124
DI 10.1016/j.isprsjprs.2022.03.021
EA APR 2022
WC Geography, Physical; Geosciences, Multidisciplinary; Remote Sensing;
   Imaging Science & Photographic Technology
DA 2023-11-11
ER

PT C
AU Kayraklioglu, E
   El-Ghazawi, T
   Bozkus, Z
AF Kayraklioglu, Engin
   El-Ghazawi, Tarek
   Bozkus, Zeki
BE Wu, YL
   Min, GY
   Georgalas, N
   Hu, J
   Atzori, L
   Jin, XL
   Jarvis, S
   Liu, L
   Calvo, RA
TI Accelerating Brain Simulations on Graphical Processing Units
SO CIT/IUCC/DASC/PICOM 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER AND
   INFORMATION TECHNOLOGY - UBIQUITOUS COMPUTING AND COMMUNICATIONS -
   DEPENDABLE, AUTONOMIC AND SECURE COMPUTING - PERVASIVE INTELLIGENCE AND
   COMPUTING
DT Proceedings Paper
CT IEEE International Conference on Computer and Information
CY OCT 26-28, 2015
CL Liverpool, UNITED KINGDOM
DE Brain simulation; CUDA; accelerators
ID SPIKING; NETWORKS
AB NEural Simulation Tool(NEST) is a large scale spiking neuronal network simulator of the brain. In this work, we present a CUDA(R) implementation of NEST. We were able to gain a speedup of factor 20 for the computational parts of NEST execution using a different data structure than NEST's default. Our partial implementation shows the potential gains and limitations of such possible port. We discuss possible novel approaches to be able to adapt generic spiking neural network simulators such as NEST to run on commodity or high-end GPGPUs.
C1 [Kayraklioglu, Engin; El-Ghazawi, Tarek] George Washington Univ, Washington, DC USA.
   [Bozkus, Zeki] Kadir Has Univ, Istanbul, Turkey.
RP Kayraklioglu, E (corresponding author), George Washington Univ, Washington, DC USA.
EM engin@gwu.edu; tarek@gwu.edu; zeki.bozkus@khas.edu.tr
CR [Anonymous], BIOM SCI ENG C BSEC
   [Anonymous], 1998, BOOK GENESIS EXPLORI, DOI DOI 10.1007/978-1-4612-1634-63
   Brette R, 2012, NETWORK-COMP NEURAL, V23, P167, DOI 10.3109/0954898X.2012.730170
   Brunel N, 2000, J COMPUT NEUROSCI, V8, P183, DOI 10.1023/A:1008925309027
   Carnevale T., 2006, NEURON BOOK, DOI DOI 10.1017/CBO9780511541612
   Davison Andrew P, 2008, Front Neuroinform, V2, P11, DOI 10.3389/neuro.11.011.2008
   Eppler JM, 2008, FRONT NEUROINFORM, V2
   Fidjeland Andreas K, 2010, 2010 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2010.5596678
   Gewaltig M.-O., 2013, NEST EXAMPLE INTRO N
   Gewaltig M-O., 2007, SCHOLARPEDIA, V2, DOI [10.4249/scholarpedia.1430, DOI 10.4249/SCHOLARPEDIA.1430]
   Kunkel S, 2012, FRONT NEUROINFORM, V5, DOI 10.3389/fninf.2011.00035
   Kunkel S, 2014, FRONT NEUROINFORM, V8, DOI 10.3389/fninf.2014.00078
   Minkovich K, 2014, IEEE T NEUR NET LEAR, V25, P316, DOI 10.1109/TNNLS.2013.2276056
   Schenck W., 2014, PERFORMANCE MODEL LA
NR 14
TC 1
Z9 1
U1 0
U2 1
PY 2015
BP 556
EP 560
DI 10.1109/CIT/IUCC/DASC/PICOM.2015.79
WC Computer Science, Hardware & Architecture; Computer Science, Software
   Engineering; Computer Science, Theory & Methods
DA 2023-11-11
ER

PT C
AU Laña, I
   Capecci, E
   Del Ser, J
   Lobo, JL
   Kasabov, N
AF Lana, Ibai
   Capecci, Elisa
   Del Ser, Javier
   Lobo, Jesus L.
   Kasabov, Nikola
BE DelSer, J
   Osaba, E
   Bilbao, MN
   SanchezMedina, JJ
   Vecchio, M
   Yang, XS
TI Road Traffic Forecasting Using NeuCube and Dynamic Evolving Spiking
   Neural Networks
SO INTELLIGENT DISTRIBUTED COMPUTING XII
SE Studies in Computational Intelligence
DT Proceedings Paper
CT 12th International Symposium on Intelligent Distributed Computing (IDC)
CY OCT 15-17, 2018
CL Bilbao, SPAIN
DE Traffic forecasting; Spiking neural networks; NeuCube
AB This paper presents a new approach for spatio-temporal road traffic forecasting that relies on the adoption of the NeuCube architecture based on spiking neural networks. The NeuCube platform was originally conceived and designed to process electroencephalographic (EEG) signals considering their temporal component and their spatial source within the brain. Its neural representation allows for a visual analysis of connectivity among different locations, and also provides a prediction tool harnessing the predictive learning capabilities of dynamic evolving Spiking Neural Networks (deSNNs). Taking advantage of the NeuCube features, this work focuses on the potential of spatially-aware traffic variable forecasts, as well as on the exploration of the spatio-temporal relationships among different sensor locations within a traffic network. Its performance, assessed over real traffic data collected in 51 locations in the center of Madrid (Spain), is superior to that of other machine learning techniques in terms of forecasting accuracy. Moreover, we discuss on the interactions and relationships among sensors of the network provided by Neucube, which may provide valuable insights on the traffic dynamics of the city under study towards enhancing its management.
C1 [Lana, Ibai; Lobo, Jesus L.] TECNALIA, Derio 48160, Spain.
   [Capecci, Elisa; Kasabov, Nikola] Auckland Univ Technol, KEDRI, Auckland 1010, New Zealand.
   [Del Ser, Javier] Univ Basque Country, UPV EHU, TECNALIA, Bizkaia, Spain.
   [Del Ser, Javier] BCAM, Bizkaia, Spain.
RP Laña, I (corresponding author), TECNALIA, Derio 48160, Spain.
EM ibai.lana@tecnalia.com; ecapecci@aut.ac.nz; javier.delser@tecnalia.com;
   jesus.lopez@tecnalia.com; nkasabov@aut.ac.nz
CR Abadi A, 2015, IEEE T INTELL TRANSP, V16, P653, DOI 10.1109/TITS.2014.2337238
   Ahmed M.S., 1979, ANAL FREEWAY TRAFFIC, V722
   [Anonymous], 2018, MADRID OPEN DATA POR
   [Anonymous], 2013, P INT C BIOM
   Bose P, 2016, IEEE T GEOSCI REMOTE, V54, P6563, DOI 10.1109/TGRS.2016.2586602
   Cai PL, 2016, TRANSPORT RES C-EMER, V62, P21, DOI 10.1016/j.trc.2015.11.002
   Capano V, 2015, SCI REP-UK, V5, DOI 10.1038/srep09895
   Capecci E, 2015, NEURAL NETWORKS, V68, P62, DOI 10.1016/j.neunet.2015.03.009
   Espinosa-Ramos J. I, 2017, IEEE T COGN DEV SYST, V99, P1
   Fusi S, 2002, LECT NOTES COMPUT SC, V2415, P241
   Gerstner W, 2001, PLAUSIBLE NEURAL NET
   Hebb D. O., 1949, ORG BEHAV A NEUROPSY
   Kasabov N., 2007, EVOLVING CONNECTIONI
   Kasabov N, 2016, NEURAL NETWORKS, V78, P1, DOI 10.1016/j.neunet.2015.09.011
   Kasabov N, 2013, NEURAL NETWORKS, V41, P188, DOI 10.1016/j.neunet.2012.11.014
   Kulkarni S, 2013, APPL SOFT COMPUT, V13, P3628, DOI 10.1016/j.asoc.2013.04.007
   Laña I, 2018, TRANSPORT RES C-EMER, V90, P18, DOI 10.1016/j.trc.2018.02.021
   Laña I, 2018, IEEE INTEL TRANSP SY, V10, P93, DOI 10.1109/MITS.2018.2806634
   Levin M., 1980, TRANSPORT RES REC, V773, P47
   Lin Yang, 2011, 2011 IEEE 3rd International Conference on Communication Software and Networks (ICCSN 2011), P171, DOI 10.1109/ICCSN.2011.6014244
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Stathopoulos A, 2003, TRANSPORT RES C-EMER, V11, P121, DOI 10.1016/S0968-090X(03)00004-4
   Thorpe S, 1998, COMPUTATIONAL NEUROSCIENCE: TRENDS IN RESEARCH, P113
   Tu EM, 2017, IEEE T NEUR NET LEAR, V28, P1305, DOI 10.1109/TNNLS.2016.2536742
   Van Hinsbergen C, 2007, WORLD C INT TRANSP S
   VanArem B, 1997, INT J FORECASTING, V13, P1, DOI 10.1016/S0169-2070(96)00695-4
   Vlahogianni EI, 2004, TRANSPORT REV, V24, P533, DOI 10.1080/0144164042000195072
   Vlahogianni EI, 2014, TRANSPORT RES C-EMER, V43, P3, DOI 10.1016/j.trc.2014.01.005
   Wu Y., 2015, J INTELL TRANSPORT S, V2450, P1
   Wysoski SG, 2010, NEURAL NETWORKS, V23, P819, DOI 10.1016/j.neunet.2010.04.009
   Xu J, 2015, IEEE J-STSP, V9, P702, DOI 10.1109/JSTSP.2015.2389196
   Yixiong Chen, 2013, Neural Information Processing. 20th International Conference, ICONIP 2013. Proceedings: LNCS 8228, P70, DOI 10.1007/978-3-642-42051-1_10
   Zhu Z, 2016, J ADV TRANSPORT, V50, P1111, DOI 10.1002/atr.1392
NR 33
TC 4
Z9 4
U1 0
U2 9
PY 2018
VL 798
BP 192
EP 203
DI 10.1007/978-3-319-99626-4_17
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods
DA 2023-11-11
ER

PT J
AU Perrinet, L
   Delorme, A
   Samuelides, M
   Thorpe, SJ
AF Perrinet, L
   Delorme, A
   Samuelides, M
   Thorpe, SJ
TI Networks of integrate-and-fire neuron using rank order coding A: How to
   implement spike time dependent Hebbian plasticity
SO NEUROCOMPUTING
DT Article; Proceedings Paper
CT 9th Annual Computational Neuroscience Meeting (CNS*00)
CY JUL, 2000
CL BRUGGE, BELGIUM
DE spiking neural networks; Hebb rule; spike time dependent Hebbian
   plasticity; kinetic model; rank order coding
AB Based on neurophysiological observations on the behavior of synapses, spike time dependent Hebbian plasticity is a novel extension to the modeling of the Hebb rule. This rule has enormous importance in the learning of spiking neural networks (SNN) but its mechanisms and computational properties are still to be explored.
   In this article, we present a generative model for spike time dependent plasticity based on a simplified model of the synaptic kinetic. We then explore the fitting of this model to experimental data and review some of its dynamical properties. Finally, we extend this model to a simplified model of integrate-and-fire (IF) neurons network using rank order coding. (C) 2001 Elsevier Science B.V. All rights reserved.
C1 ONERA, DITM, F-31055 Toulouse, France.
   Fac Med Rangueil, Ctr Rech Cerveau & Cognit, UMR 5549, F-31062 Toulouse, France.
RP Perrinet, L (corresponding author), ONERA, DITM, 2 Av E Belin,BP 4026, F-31055 Toulouse, France.
CR Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   BRUNEL N, 1998, J THEOR BIOL
   DESTEXHE A, 1996, NEURAL COMPUT
   Gerstner W, 1996, NATURE, V383, P76, DOI 10.1038/383076a0
   Hebb D. O., 1949, ORG BEHAV
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   RUBIN J, 2000, ESF S NEUR PLAST DYN
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Thorpe S, 1996, NATURE, V381, P520, DOI 10.1038/381520a0
NR 9
TC 19
Z9 19
U1 0
U2 13
PD JUN
PY 2001
VL 38
BP 817
EP 822
DI 10.1016/S0925-2312(01)00460-X
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT C
AU Zhang, J
   Zhang, L
AF Zhang, Jin
   Zhang, Lei
GP IEEE
TI Spiking Neural Network Implementation on FPGA for Multiclass
   Classification
SO 2023 IEEE INTERNATIONAL SYSTEMS CONFERENCE, SYSCON
SE Annual IEEE Systems Conference
DT Proceedings Paper
CT 17th Annual IEEE International Systems Conference (SysCon)
CY APR 17-20, 2023
CL Vancouver, CANADA
DE Spiking Neural Network; FPGA; Spiking Exponential Function; Spiking
   SoftMax Function; Spiking Multiplier; Spiking Divider
AB Spiking Neural Network (SNN) is a particular Artificial Neural Networks (ANN) form. An SNN has similar features as an ANN, but an SNN has a different information system that will allow SNN to have higher energy efficiency than an ANN. This paper presents the design and implementation of an SNN on FPGA. The model of the SNN is designed to be lower power consumption than existing SNN models in the aspect of FPGA implementation and lower accuracy loss than the existing training method in the part of the algorithm. The coding scheme of the SNN model proposed in this paper is the rate coding scheme. This paper introduces a conversion method to directly map the trained parameters from ANN to SNN with negligible classification accuracy loss. Also, this paper demonstrates the technique of FPGA implementation for Spiking Exponential Function, Spiking SoftMax Function and Dynamic Adder Tree. This paper also presents the Time Division Component Reuse technic for lower resource utilization in the FPGA implementation of SNN. The proposed model has a power efficiency of 8841.7 frames per watt with negligible accuracy loss. The benchmark SNN model has a power efficiency of 337.6 frames per watt with an accuracy loss of 1.42 percent. The reference accuracy of the ANN model is 90.36 percent. For comparison, the specific model of the SNN has an accuracy of 90.39 percent.
C1 [Zhang, Jin; Zhang, Lei] Univ Regina, Fac Engn & Appl Sci, Regina, SK, Canada.
RP Zhang, J (corresponding author), Univ Regina, Fac Engn & Appl Sci, Regina, SK, Canada.
EM jinzhang@uregina.ca; lei.zhang@uregina.ca
CR [Anonymous], NORMAL DISTRIBUTION
   [Anonymous], 2008, 7542008 IEEE, DOI 10.1109/IEEESTD.2008.4610935
   Balaji A, 2018, J LOW POWER ELECTRON, V14, P508, DOI 10.1166/jolpe.2018.1582
   Biswas B., 2013, ELECT J MATH ANAL AP, V1, P2090
   Bodo R., 2017, FRONT NEUROSCI-SWITZ, V11
   Brownlee J., GENTLE INTRO RECTIFI
   Dekking F. M., SPRINGER TEXTS STAT, P25, DOI DOI 10.1007/1-84628-168-7
   Diehl PU, 2015, IEEE IJCNN
   Emre N., 2014, FRONT NEUROSCI-SWITZ, V7
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Han JH, 2020, TSINGHUA SCI TECHNOL, V25, P479, DOI 10.26599/TST.2019.9010019
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Schliebs S, 2013, EVOL SYST-GER, V4, P87, DOI 10.1007/s12530-013-9074-9
   Van Rullen R, 2001, NEURAL COMPUT, V13, P1255, DOI 10.1162/08997660152002852
   Zhang L., 2017, 2017 IEEE 30 CANADIA, P1, DOI [10.1109/CCECE.2017.7946635, DOI 10.1109/CCECE.2017.7946635]
NR 17
TC 0
Z9 0
U1 6
U2 6
PY 2023
DI 10.1109/SysCon53073.2023.10131076
WC Engineering, Multidisciplinary
DA 2023-11-11
ER

PT J
AU Sakai, Y
   Pedroni, BU
   Joshi, S
   Tanabe, S
   Akinin, A
   Cauwenberghs, G
AF Sakai, Yasufumi
   Pedroni, Bruno U.
   Joshi, Siddharth
   Tanabe, Satoshi
   Akinin, Abraham
   Cauwenberghs, Gert
TI Dropout and DropConnect for Reliable Neuromorphic Inference Under
   Communication Constraints in Network Connectivity
SO IEEE JOURNAL ON EMERGING AND SELECTED TOPICS IN CIRCUITS AND SYSTEMS
DT Article; Proceedings Paper
CT 1st IEEE International Conference on Artificial Intelligence Circuits
   and Systems (AICAS)
CY MAR 18-20, 2019
CL Hsinchu, TAIWAN
DE Neuromorphics; Convolutional neural networks; Training; Robustness;
   Inference mechanisms; Neuromorphic hardware; spiking neural networks;
   timeout error; Dropout; DropConnect
ID NEURAL-NETWORKS
AB Dropout and DropConnect are known as effective methods to improve on the generalization performance of neural networks, by either dropping states of neural units or dropping weights of synaptic connections randomly selected at each time instance throughout the training process. In this paper, we extend on the use of these methods in the design of neuromorphic spiking neural networks (SNN) hardware to improve further on the reliability of inference as impacted by resource constrained errors in network connectivity. Such energy and bandwidth constraints arise for low-power operation in the communication between neural units, which cause dropped spike events due to timeout errors in the transmission. The Dropout and DropConnect processes during training of the network are aligned with a statistical model of the network during inference that accounts for these random errors in the transmission of neural states and synaptic connections. The use of Dropout and DropConnect during training hence allows to simultaneously meet two design objectives: improving robustness of inference to dropped spike events due to timeout communication constraints in network connectivity, while maximizing time-to-decision bandwidth and hence minimizing inference energy in the neuromorphic hardware. Simulations with 5-layer fully connected 784-500-500-500-10 SNN on the MNIST task show a 3.42-fold and 7.06-fold decrease in inference energy at 90 test accuracy, by using Dropout and DropConnect respectively during backpropagation training. Also the simulation with convolutional neural networks on the CIFAR-10 task show a 1.24-fold decrease in inference energy at 60 test accuracy by using Dropout during backpropagation training.
C1 [Sakai, Yasufumi; Tanabe, Satoshi] Fujitsu Labs Ltd, Kawasaki, Kanagawa 2118588, Japan.
   [Pedroni, Bruno U.; Akinin, Abraham; Cauwenberghs, Gert] Univ Calif San Diego, Dept Bioengn, San Diego, CA 92093 USA.
   [Joshi, Siddharth] Univ Notre Dame, Dept Comp Sci & Engn, Notre Dame, IN 46556 USA.
RP Sakai, Y (corresponding author), Fujitsu Labs Ltd, Kawasaki, Kanagawa 2118588, Japan.
EM sakaiyasufumi@fujitsu.wm; bpedronik@ucsd.edu; sjoshi2@nd.edu;
   tanabe.s@fujitsu.com; akinin@gmail.com; gert@ucsd.edu
CR Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   [Anonymous], TECH REP
   Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Boahen KA, 2000, IEEE T CIRCUITS-II, V47, P416, DOI 10.1109/82.842110
   Ciresan DC, 2010, NEURAL COMPUT, V22, P3207, DOI 10.1162/NECO_a_00052
   Deiss SR, 1998, PULSED NEURAL NETWORKS, P157
   Diehl PU, 2016, IEEE IJCNN, P4278, DOI 10.1109/IJCNN.2016.7727758
   Diehl Peter U, 2015, 2015 INT JOINT C NEU, P1, DOI [DOI 10.1109/IJCNN.2015.7280696, 10.1109/IJCNN.2015.7280696]
   Indiveri G, 2006, IEEE T NEURAL NETWOR, V17, P211, DOI 10.1109/TNN.2005.860850
   Indiveri G, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00073
   Khan MM, 2008, IEEE IJCNN, P2849, DOI 10.1109/IJCNN.2008.4634199
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Mahowald M., 1994, ANALOG VLSI SYSTEM S, V265
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   Park J, 2017, IEEE T NEUR NET LEAR, V28, P2408, DOI 10.1109/TNNLS.2016.2572164
   Sakai Y, 2019, 2019 IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE CIRCUITS AND SYSTEMS (AICAS 2019), P76, DOI [10.1109/aicas.2019.8771533, 10.1109/AICAS.2019.8771533]
   Schemmel J, 2010, IEEE INT SYMP CIRC S, P1947, DOI 10.1109/ISCAS.2010.5536970
   Sivilotti M. A., 1991, THESIS
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Wan L., 2013, P 30 INT C MACH LEAR, V28, P1058
NR 20
TC 3
Z9 4
U1 1
U2 10
PD DEC
PY 2019
VL 9
IS 4
BP 658
EP 667
DI 10.1109/JETCAS.2019.2952642
WC Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Liu, HJ
   Li, JW
   Li, ZW
   Li, QJ
   Diao, JT
AF Liu, Hai-jun
   Li, Ji-wei
   Li, Zhi-wei
   Li, Qing-jiang
   Diao, Jie-tao
GP Destech Publicat Inc
TI A CMOS-based Neuron Circuit for Spiking Neural Networks with Memristive
   Synapse
SO 2018 INTERNATIONAL CONFERENCE ON COMPUTER, COMMUNICATIONS AND
   MECHATRONICS ENGINEERING (CCME 2018)
SE DEStech Transactions on Computer Science and Engineering
DT Proceedings Paper
CT International Conference on Computer, Communications and Mechatronics
   Engineering (CCME)
CY DEC 22-23, 2018
CL Shanghai, PEOPLES R CHINA
DE CMOS; Neuron; Spiking; Memristor; STDP
AB The brained-inspired spiking neural networks (SNNs), which can be applied on visual information processing and speech recognition, is attracting great attention especially when combined with emerging electronic synapse (i.e. memristor). The neuron, transmitting and receiving spiking signals, is a vital component to realize the biological rules of SNNs, such as leaky-integrate-and-fire (LIF), inhibitory period, winner takes all (WTA) and bidirectional transmission. In this work, we propose and implement a novel spiking neuron circuit based on complementary-metal-oxide-semiconductor (CMOS) technology. Experimental results show that the neuron circuit can generate spiking pulses, realize lateral inhibition and change the weight of synapse.
C1 [Liu, Hai-jun; Li, Ji-wei; Li, Zhi-wei; Li, Qing-jiang; Diao, Jie-tao] Natl Univ Def Technol, Coll Elect Sci, Changsha, Hunan, Peoples R China.
RP Liu, HJ (corresponding author), Natl Univ Def Technol, Coll Elect Sci, Changsha, Hunan, Peoples R China.
CR Ambrogio S, 2016, IEEE T ELECTRON DEV, V63, P1508, DOI 10.1109/TED.2016.2526647
   [Anonymous], 2017, IEEE T NEURAL NETW L
   Boyn S., 2017, LEARNING FERROELECTR
   Chu M, 2015, IEEE T IND ELECTRON, V62, P2410, DOI 10.1109/TIE.2014.2356439
   Covi E, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00482
   Gruning A., 2014, ESANN
   Hasler J, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00118
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Mahalanabis D, 2016, IEEE INT SYMP CIRC S, P2314, DOI 10.1109/ISCAS.2016.7539047
   Narasimman G, 2016, IEEE INT SYMP CIRC S, P914, DOI 10.1109/ISCAS.2016.7527390
   Yu SM, 2013, ADV MATER, V25, P1774, DOI 10.1002/adma.201203680
NR 11
TC 0
Z9 0
U1 0
U2 4
PY 2018
VL 332
BP 550
EP 555
WC Computer Science, Theory & Methods; Engineering, Multidisciplinary;
   Telecommunications
DA 2023-11-11
ER

PT C
AU Joldos, M
   Muntean, IL
AF Joldos, Marius
   Muntean, Ioan Lucian
GP IEEE
TI Multicore Asynchronous Simulation of Spiking Neural Networks on the Grid
SO 2013 ROEDUNET INTERNATIONAL CONFERENCE (ROEDUNET): NETWORKING IN
   EDUCATION, 11TH EDITION
DT Proceedings Paper
CT 11th RoEduNet International Conference - Networking in Education and
   Research
CY JAN 17-19, 2013
CL Sinaia, ROMANIA
DE Grid computing; Spiking Neural Networks; Asynchronous Simulation; Globus
   Toolkit; Globus Online Client
ID DYNAMICS; BRAIN; CHAOS
AB Dynamics analysis studies of spiking neural network behavior entails the computation of a large number of simulation scenarios. Moreover, when the simulated neural microcircuits are fairly large, the use of multiple cores in a simulation tends to be beneficial. In this paper we focus on the parallelization of an asynchronous simulation strategy with OpenMP threads and on the management of the neural simulations on compute grids. The first results show a parallel efficiency in the ranges of 0.31 - 0.98, with hyper-threading disabled or enabled, resp. By using the grid as underlying middleware for performing the simulations, we could generate and manage hundreds of simulation scenarios in an easy way.
C1 [Joldos, Marius; Muntean, Ioan Lucian] Tech Univ Cluj Napoca, Dept Comp Sci, Cluj Napoca, Romania.
RP Joldos, M (corresponding author), Tech Univ Cluj Napoca, Dept Comp Sci, Cluj Napoca, Romania.
EM Marius.Joldos@cs.utcluj.ro; Ioan.Lucian.Muntean@cs.utcluj.ro
CR Allen B, 2012, COMMUN ACM, V55, P81, DOI 10.1145/2076450.2076468
   [Anonymous], 2005, ADV NEURAL INF PROCE
   Faure P, 2001, CR ACAD SCI III-VIE, V324, P773, DOI 10.1016/S0764-4469(01)01377-4
   Foster I, 1997, INT J SUPERCOMPUT AP, V11, P115, DOI 10.1177/109434209701100205
   Gerstner W., 2002, SPIKING NEURON MODEL
   Korn H, 2003, CR BIOL, V326, P787, DOI 10.1016/j.crvi.2003.09.011
   Mattia M, 2000, NEURAL COMPUT, V12, P2305, DOI 10.1162/089976600300014953
   Muntean I. L., 2012, 2012 Sixth International Conference on Complex, Intelligent, and Software Intensive Systems (CISIS), P449, DOI 10.1109/CISIS.2012.196
   Muntean I. L., 2011, Proceedings 2011 10th International Symposium on Parallel and Distributed Computing (ISPDC 2011), P266, DOI 10.1109/ISPDC.2011.47
   Muntean I. L., 2012, 14 INT S SY IN PRESS
   Muresan R. C., 2004, INT C INT ENG SYST
   OpenMP Architecture Review Board, 2008, OPENMP APPL PROGR IN
   Sundell H, 2005, J PARALLEL DISTR COM, V65, P609, DOI 10.1016/j.jpdc.2004.12.005
   The Blue Brain Project, 2012, BLUE BRAIN PROJ
   Watts DJ, 1998, NATURE, V393, P440, DOI 10.1038/30918
NR 15
TC 1
Z9 1
U1 0
U2 2
PY 2013
WC Computer Science, Information Systems; Computer Science,
   Interdisciplinary Applications; Telecommunications
DA 2023-11-11
ER

PT J
AU Aertsen, A
   Diesmann, M
   Gewaltig, MO
AF Aertsen, A
   Diesmann, M
   Gewaltig, MO
TI Propagation of synchronous spiking activity in feedforward neural
   networks
SO JOURNAL OF PHYSIOLOGY-PARIS
DT Article
DE synchrony; cell assembly; synfire chain; pulse packet; propagation;
   neocortex; neural dynamics
AB 'Synfire' activity has been proposed as a model for the experimentally observed accurate spike patterns in cortical activity. We investigated the structural and dynamical aspects of this theory. To quantify the degree of spnchrony in neural activity, we introduced the concept of 'pulse packets'. This enabled us to derive a novel neural transmission function which was used to assess the role of the single neuron dynamics and to characterize the stability conditions for propagating synfire activity. Thus, we could demonstrate that the cortical network is able to sustain synchronous spiking activity using local feedforward (synfire) connections. This new approach opens the way for a quantitative description of neural network dynamics, and enables us to rest the synfire hypothesis on physiological data.
RP Aertsen, A (corresponding author), UNIV FREIBURG,INST BIOL 3,DEPT NEUROBIOL & BIOPHYS,SCHANZLESTR 1,D-79104 FREIBURG,GERMANY.
CR ABELES M, 1982, ISRAEL J MED SCI, V18, P83
   ABELES M, 1993, J NEUROPHYSIOL, V70, P1629, DOI 10.1152/jn.1993.70.4.1629
   ABELES M, 1993, BRAIN THEORY, P149
   Abeles M., 1991, CORTICONICS NEURAL C
   AERTSEN A, 1995, SUPERCOMPUTERS BRAIN, P213
   [Anonymous], UNITARY JOINT EVENTS
   BERNANDER O, 1994, NEURAL COMPUT, V6, P622, DOI 10.1162/neco.1994.6.4.622
   BIENENSTOCK E, 1995, NETWORK-COMP NEURAL, V6, P179, DOI 10.1088/0954-898X/6/2/004
   Diesmann M., 1995, SYNOD ENV NEURAL SYS
   Diesmann M, 1996, COMPUTATIONAL NEUROS, P59
   DIESMANN M, 1994, GOTTINGEN NEUROBIOLO, P560
   FETZ E, 1991, CEREB CORTEX, V9, P1
   Gewaltig MO, 1995, NEURAL NETWORKS ARTI, P37
   GEWALTIG O, 1994, GOTTINGEN NEUROBIOLO, P559
   GRUN S, 1994, P 17 M EUR NEUR ASS, P11
   MURPHY VN, 1994, NEURAL COMOP, V6, P1111
   RIEHLE A, 1996, SARTIFICIAL NEURAL N, P673
   RIEHLE A, 1995, SUPERCOMPUTERS BRAIN, P281
   Shadlen Michael N., 1994, Current Opinion in Neurobiology, V4, P569, DOI 10.1016/0959-4388(94)90059-0
NR 19
TC 99
Z9 99
U1 1
U2 4
PY 1996
VL 90
IS 3-4
BP 243
EP 247
DI 10.1016/S0928-4257(97)81432-5
WC Neurosciences; Physiology
DA 2023-11-11
ER

PT C
AU Mastella, M
   Chicca, E
AF Mastella, Michele
   Chicca, Elisabetta
GP IEEE
TI A Hardware-friendly Neuromorphic Spiking Neural Network for Frequency
   Detection and Fine Texture Decoding
SO 2021 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS (ISCAS)
SE IEEE International Symposium on Circuits and Systems
DT Proceedings Paper
CT IEEE International Symposium on Circuits and Systems (IEEE ISCAS)
CY MAY 22-28, 2021
CL Daegu, SOUTH KOREA
DE Spiking Neural Network; Phase-Locked Loop; texture; active touch;
   neuromorphic
ID TOUCH
AB Humans can distinguish fabrics by their textures, even when they are finer than the density of tactile sensors. Evidence suggests that this ability is produced by the nervous system using an active touch strategy. When the finger slides over a texture, the nervous system converts the texture's spatial period into an equivalent spiking frequency. Many studies focused on modeling the biological encoding part that translates the spatial frequency into a temporal spiking frequency, but few explored the decoding part. In this work, we propose a novel approach based on a spiking neural network able to detect the frequency of an input signal. Inspired by biological evidence, our architecture detects the range in which the encoded frequency dwells and could therefore decode the texture's spatial period. The network has been designed to be composed of existing neuromorphic spiking primitives. This property enables a straightforward implementation on integrated silicon circuits, allowing the texture decoding at the edge of the sensor.
C1 [Mastella, Michele; Chicca, Elisabetta] Univ Groningen, Bioinspired Circuits & Syst BICS Lab, Zernike Inst Adv Mat, Nijenborgh 4, NL-9747 AG Groningen, Netherlands.
   [Mastella, Michele; Chicca, Elisabetta] Univ Groningen, CogniGron Groningen Cognit Syst & Mat Ctr, Nijenborgh 4, NL-9747 AG Groningen, Netherlands.
RP Mastella, M (corresponding author), Univ Groningen, Bioinspired Circuits & Syst BICS Lab, Zernike Inst Adv Mat, Nijenborgh 4, NL-9747 AG Groningen, Netherlands.; Mastella, M (corresponding author), Univ Groningen, CogniGron Groningen Cognit Syst & Mat Ctr, Nijenborgh 4, NL-9747 AG Groningen, Netherlands.
CR Ahissar E, 2001, NEURON, V32, P185, DOI 10.1016/S0896-6273(01)00466-4
   Ahissar E, 1998, NEURAL COMPUT, V10, P597, DOI 10.1162/089976698300017683
   Ahissar E, 1997, P NATL ACAD SCI USA, V94, P11633, DOI 10.1073/pnas.94.21.11633
   Ahissar E., 2003, US Patent, Patent No. [6 581 046B1, 6581046]
   Biswas A., 2014, IEEE T HAPTICS
   Brunel N, 2007, BIOL CYBERN, V97, P341, DOI 10.1007/s00422-007-0189-6
   Chicca E, 2014, P IEEE, V102, P1367, DOI 10.1109/JPROC.2014.2313954
   D'Angelo G, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00451
   Greenspon CM, 2020, J R SOC INTERFACE, V17, DOI [10.1098/rsif.2019.0892, 10.1098/rsif.20190892]
   Gutierrez-galan D., 2020, EVENT BASED DIGITAL
   Harvey MA, 2013, PLOS BIOL, V11, DOI 10.1371/journal.pbio.1001558
   Hollins M, 2007, CAN J EXP PSYCHOL, V61, P184, DOI 10.1037/cjep2007020
   Hsieh GC, 1996, IEEE T IND ELECTRON, V43, P609, DOI 10.1109/41.544547
   Kwon MW, 2018, J APPL PHYS, V124, DOI 10.1063/1.5031929
   Mackevicius EL, 2012, J NEUROSCI, V32, P15309, DOI 10.1523/JNEUROSCI.2161-12.2012
   Meier M, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P5579, DOI 10.1109/IROS.2016.7759820
   Milde MB, 2018, NEURAL COMPUT, V30, P2384, DOI 10.1162/neco_a_01112
   Oddo CM, 2011, SENSORS-BASEL, V11, P5596, DOI 10.3390/s110605596
   Rongala UB, 2017, IEEE T NEUR NET LEAR, V28, P819, DOI 10.1109/TNNLS.2015.2472477
   Saal HP, 2014, TRENDS NEUROSCI, V37, P689, DOI 10.1016/j.tins.2014.08.012
   Schössler T, 2019, INT RELIAB PHY SYM, DOI [10.1109/irps.2019.8720464, 10.1109/CLEOE-EQEC.2019.8872698]
   Stimberg M, 2019, ELIFE, V8, DOI 10.7554/eLife.47314
   Su PE, 2009, IEEE T CIRCUITS-II, V56, P881, DOI 10.1109/TCSII.2009.2035258
   Weber AI, 2013, P NATL ACAD SCI USA, V110, P17107, DOI 10.1073/pnas.1305509110
NR 24
TC 0
Z9 0
U1 0
U2 0
PY 2021
DI 10.1109/ISCAS51556.2021.9401377
WC Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Masuda, N
   Kori, H
AF Masuda, Naoki
   Kori, Hiroshi
GP IEEE
TI STDP enhances frequency synchrony in neural networks with a pacemaker
SO 2007 IEEE INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS, VOLS 1-6
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks
CY AUG 12-17, 2007
CL Orlando, FL
ID DEPENDENT SYNAPTIC PLASTICITY; NEURONS; PROPAGATION
AB Spike- timing-dependent plasticity (STDP) is a general rule of synaptic plasticity based on precise spike timings. The common form of STDP strengthens synaptic weights most when a presynaptic spike time precedes a postsynaptic spike time by a small amount of time. Even though various neural computations can be implemented by STDP, the relation between STDP and synchronous firing remains elusive. With synchrony kept in mind, here we analyze neural networks driven by a pacemaker in the oscillatory scheme. We show that STDP promotes formation of a feedforward network whose root is the pacemaker. Neurons fire slightly after the pacemaker does, and therefore frequency synchrony is achieved. Remarkably, the synaptic weights necessary for frequency synchrony are much smaller with STDP than without STDP.
C1 [Masuda, Naoki] Univ Tokyo, Grad Sch Informat Sci & Technol, 7-1-1 Hongo, Tokyo 1138656, Japan.
   [Kori, Hiroshi] Hokkaido Univ, Dept Math, Hokkaido, Japan.
RP Masuda, N (corresponding author), Univ Tokyo, Grad Sch Informat Sci & Technol, 7-1-1 Hongo, Tokyo 1138656, Japan.
EM masuda@mist.i.u-tokyo.ac.jp; kori@nsc.es.hokudai.acjp
CR Abbott LF, 2000, NAT NEUROSCI, V3, P1178, DOI 10.1038/81453
   Bell CC, 1997, NATURE, V387, P278, DOI 10.1038/387278a0
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Diesmann M, 1999, NATURE, V402, P529, DOI 10.1038/990101
   Gerstner W, 1996, NATURE, V383, P76, DOI 10.1038/383076a0
   Gerstner W., 2002, SPIKING NEURON MODEL
   HANSEL D, 1993, EUROPHYS LETT, V23, P367, DOI 10.1209/0295-5075/23/5/011
   HANSEL D, 1995, NEURAL COMPUT, V7, P307, DOI 10.1162/neco.1995.7.2.307
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Karbowski J, 2002, PHYS REV E, V65, DOI 10.1103/PhysRevE.65.031902
   Kori H, 2004, PHYS REV LETT, V93, DOI 10.1103/PhysRevLett.93.254101
   Kori H, 2001, PHYS REV E, V63, DOI 10.1103/PhysRevE.63.046214
   Kori H, 2006, PHYS REV E, V74, DOI 10.1103/PhysRevE.74.066115
   KURAMOTO Y, 1991, PHYSICA D, V50, P15, DOI 10.1016/0167-2789(91)90075-K
   Kuramoto Y., 2003, CHEM OSCILLATIONS WA
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   Masuda N, 2004, NEURAL COMPUT, V16, P627, DOI 10.1162/089976604772744938
   MASUDA N, 2006, UNPUB FORMATION FEED
   Nowotny T, 2003, J NEUROSCI, V23, P9776
   Pikovsky A., 2003, SYNCHRONIZATION UNIV
   Plenz D, 1999, NATURE, V400, P677, DOI 10.1038/23281
   Ramirez JM, 2004, CURR OPIN NEUROBIOL, V14, P665, DOI 10.1016/j.conb.2004.10.011
   Reyes AD, 2003, NAT NEUROSCI, V6, P593, DOI 10.1038/nn1056
   Ritz R, 1997, CURR OPIN NEUROBIOL, V7, P536, DOI 10.1016/S0959-4388(97)80034-7
   SINGER W, 1995, ANNU REV NEUROSCI, V18, P555, DOI 10.1146/annurev.neuro.18.1.555
   Song S, 2001, NEURON, V32, P339, DOI 10.1016/S0896-6273(01)00451-2
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Vogels TP, 2005, J NEUROSCI, V25, P10786, DOI 10.1523/JNEUROSCI.3508-05.2005
   Zhang LI, 1998, NATURE, V395, P37, DOI 10.1038/25665
   Zhigulin VP, 2004, NEUROCOMPUTING, V58, P373, DOI 10.1016/j.neucom.2004.01.069
   Zhigulin VP, 2003, PHYS REV E, V67, DOI 10.1103/PhysRevE.67.021901
NR 31
TC 0
Z9 0
U1 0
U2 2
PY 2007
BP 96
EP +
DI 10.1109/IJCNN.2007.4370937
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering
DA 2023-11-11
ER

PT C
AU KOCH, C
AF KOCH, C
GP INT NEURAL NETWORK SOC
TI TEMPORAL ANALYSIS OF SPIKE PATTERNS IN MONKEYS AND ARTIFICIAL NEURAL
   NETWORKS
SO WORLD CONGRESS ON NEURAL NETWORKS-SAN DIEGO - 1994 INTERNATIONAL NEURAL
   NETWORK SOCIETY ANNUAL MEETING, VOL 4
DT Proceedings Paper
CT 1994 International-Neural-Network-Society Annual Meeting - World
   Congress on Neural Networks-San Diego
CY JUN 05-09, 1994
CL SAN DIEGO, CA
C1 CALTECH,PASADENA,CA 91125.
NR 0
TC 0
Z9 0
U1 0
U2 0
PY 1994
BP D353
EP D353
WC Computer Science, Artificial Intelligence; Neurosciences
DA 2023-11-11
ER

PT C
AU Kugler, M
   Iwasa, K
   Benso, VAP
   Kuroyanagi, S
   Iwata, A
AF Kugler, Mauricio
   Iwasa, Kaname
   Benso, Victor Alberto Parcianello
   Kuroyanagi, Susumu
   Iwata, Akira
BE Ishikawa, M
   Doya, K
   Miyamoto, H
   Yamakawa, T
TI A complete hardware implementation of an integrated sound localization
   and classification system based on spiking neural networks
SO NEURAL INFORMATION PROCESSING, PART II
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 14th International Conference on Neural Information Processing (ICONIP
   2007)
CY NOV 13-16, 2007
CL Kitakyushu, JAPAN
ID TIME
AB Several applications would emerge from the development of artificial systems able to accurately localize and identify sound sources. This paper proposes an integrated sound localization and classification system based on the human auditory system and a respective compact hardware implementation. The proposed models are based on spiking neurons, which are suitable for processing time series data, like sound signals, and can be easily implemented in hardware. The system uses two microphones, extracting the time difference between the two channels with a chain of coincidence detection spiking neurons. A spiking neural networks process the time-delay pattern, giving a single directional output. Simultaneously, an independent spiking neural network process the spectral information of on audio channel in order to classify the source. Experimental results show that a the proposed system could successfully locate and identify several sound sources in real time with high accuracy.
C1 [Kugler, Mauricio; Iwasa, Kaname; Benso, Victor Alberto Parcianello; Kuroyanagi, Susumu; Iwata, Akira] Nagoya Inst Technol, Dept Comp Sci & Engn, Showa Ku, Nagoya, Aichi 4668555, Japan.
RP Kugler, M (corresponding author), Nagoya Inst Technol, Dept Comp Sci & Engn, Showa Ku, Nagoya, Aichi 4668555, Japan.
EM mauricio@kugler.com; kaname@mars.elcom.nitech.ac.jp;
   benso@mars.elcom.nitech.ac.jp; bw@nitech.ac.jp; iwata@nitech.ac.jp
CR COWLING M, 2002, P 6 INT S DIG SIGN P, P16
   IWASA K, 2007, P 20 INT JOINT C NEU, P1252
   JEFFRESS LA, 1948, J COMP PHYSIOL PSYCH, V41, P35, DOI 10.1037/h0061495
   KNAPP CH, 1976, IEEE T ACOUST SPEECH, V24, P320, DOI 10.1109/TASSP.1976.1162830
   KUROYANAGI S, 1994, IEICE T INF SYST, VE77D, P466
   Kuroyanagi S, 2002, ICONIP'02: PROCEEDINGS OF THE 9TH INTERNATIONAL CONFERENCE ON NEURAL INFORMATION PROCESSING, P348
   KUROYANAGI S, 2002, NC2001210 NAG I TECH, V113
   Maass W, 2001, PULSED NEURAL NETWOR
   Ponca M., 2001, P 5 INT C ART NEUR N, P22
   SAKAGUCHI S, 1999, NC9970 NAG I TECHN, P61
   Schauer C, 1999, Int J Neural Syst, V9, P447, DOI 10.1142/S0129065799000460
   SCHAUER C, 2001, P INT JOINT C NEUR N, V2, P1132
   TURK O, 2002, P 7 INT C SPOK LANG, P641
NR 13
TC 7
Z9 7
U1 0
U2 0
PY 2008
VL 4985
BP 577
EP 587
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods
DA 2023-11-11
ER

PT J
AU Cessac, B
   Cofré, R
AF Cessac, B.
   Cofre, R.
TI Spike train statistics and Gibbs distributions
SO JOURNAL OF PHYSIOLOGY-PARIS
DT Article
DE Neural networks dynamics; Spike train statistics; Gibbs distributions;
   Higher order correlations; Generalized linear models
ID TEMPORAL CORRELATIONS; RESPONSES; NETWORKS; STATES
AB This paper is based on a lecture given in the LACONEU summer school, Valparaiso, January 2012. We introduce Gibbs distribution in a general setting, including non stationary dynamics, and present then three examples of such Gibbs distributions, in the context of neural networks spike train statistics: (i) maximum entropy model with spatio-temporal constraints; (ii) generalized linear models; and (iii) conductance based integrate and fire model with chemical synapses and gap junctions. 2013 Elsevier Ltd. All rights reserved.
C1 [Cessac, B.; Cofre, R.] UNSA LJAD, NeuroMathComp Team INRIA, F-06902 Sophia Antipolis, France.
RP Cofré, R (corresponding author), UNSA LJAD, NeuroMathComp Team INRIA, 2004 Route Lucioles, F-06902 Sophia Antipolis, France.
EM bruno.cessac@inria.fr; rodrigo.cofre_torres@inria.fr
CR Ahmadian Y, 2011, NEURAL COMPUT, V23, P46, DOI 10.1162/NECO_a_00059
   [Anonymous], 1988, DEGRUYTER STUDIES MA
   [Anonymous], MARKOV FIELDS UNPUB
   [Anonymous], 1975, LECT NOTES MATH
   [Anonymous], 1990, DISORDER PHYS SYSTEM
   BESAG J, 1974, J ROY STAT SOC B MET, V36, P192
   BRILLINGER DR, 1992, J AM STAT ASSOC, V87, P260, DOI 10.2307/2290256
   BRILLINGER DR, 1988, BIOL CYBERN, V59, P189, DOI 10.1007/BF00318010
   Brown E.N., 2003, COMPUT NEUROSCI A
   Bruno Cessac, 2012, CURRENT MATH PROBLEM
   Cessac B, 2011, J MATH BIOL, V62, P863, DOI 10.1007/s00285-010-0358-4
   Cessac B, 2011, J MATH NEUROSCI, V1, DOI 10.1186/2190-8567-1-8
   Cessac B, 2008, FRONT COMPUT NEUROSC, V2, DOI 10.3389/neuro.10.002.2008
   Cessac B, 2010, J PHYSIOL-PARIS, V104, P5, DOI 10.1016/j.jphysparis.2009.11.002
   Chazottes J., 2011, MATH COMPLEXITY DYNA, P1422, DOI [DOI 10.1007/978-1-4614-1806-1-90, 10.1007/978-1-4614-1806-1-90]
   Chichilnisky EJ, 2001, NETWORK-COMP NEURAL, V12, P199, DOI 10.1088/0954-898X/12/2/306
   Cofre Rodrigo, CHAOS SOLIT IN PRESS, DOI DOI 10.1016/J.CHA0S.2012.12.006
   Cornfeld I. P., 2012, ERGODIC THEORY
   Dudik M., 2004, P 17 ANN C COMP LEAR
   Fernández R, 2005, J STAT PHYS, V118, P555, DOI 10.1007/s10955-004-8821-5
   Ganmor E, 2011, P NATL ACAD SCI USA, V108, P9679, DOI 10.1073/pnas.1019641108
   Ganmor E, 2011, J NEUROSCI, V31, P3044, DOI 10.1523/JNEUROSCI.3682-10.2011
   Gantmacher FR., 1998, THEORY MATRICES, Vvols. 1 2
   Gerwinn S, 2009, FRONT COMPUT NEUROSC, V3, DOI 10.3389/neuro.10.021.2009
   JAYNES ET, 1957, PHYS REV, V106, P620, DOI 10.1103/PhysRev.106.620
   Jenkinson O, 2005, J STAT PHYS, V119, P765, DOI 10.1007/s10955-005-3035-z
   KEANE M, 1972, INVENT MATH, V16, P309, DOI 10.1007/BF01425715
   Kirst C, 2009, FRONT NEUROSCI-SWITZ, V3, P2, DOI 10.3389/neuro.01.009.2009
   Koyama S, 2010, ANN I STAT MATH, V62, P37, DOI 10.1007/s10463-009-0249-x
   Maillard G., 2007, INTRO CHAINS COMPLET
   Marre O, 2009, PHYS REV LETT, V102, DOI 10.1103/PhysRevLett.102.138101
   McCullagh P., 1989, GEN LINEAR MODELS
   Miller M.I., 1991, RANDOM POINT PROCESS
   MOUSSOUR.J, 1974, J STAT PHYS, V10, P11, DOI 10.1007/BF01011714
   Nasser H, 2013, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2013/03/P03006
   Ohiorhenuan IE, 2010, NATURE, V466, P617, DOI 10.1038/nature09178
   Paninski L, 2004, NETWORK-COMP NEURAL, V15, P243, DOI 10.1088/0954-898X/15/4/002
   Paninski L, 2004, J NEUROSCI, V24, P8551, DOI 10.1523/JNEUROSCI.0919-04.2004
   PARRY W, 1990, ASTERISQUE, P9
   Pillow JW, 2008, NATURE, V454, P995, DOI 10.1038/nature07140
   Pillow JW, 2011, NEURAL COMPUT, V23, P1, DOI 10.1162/NECO_a_00058
   Pillow JW, 2005, J NEUROSCI, V25, P11003, DOI 10.1523/JNEUROSCI.3305-05.2005
   Rieke F., 1996, SPIKES EXPLORING NEU
   Roudi Yasser, 2011, PHYS REV LETT, V106
   Rudolph M, 2006, NEURAL COMPUT, V18, P2146, DOI 10.1162/neco.2006.18.9.2146
   Ruelle D., 1978, THERMODYNAMIC FORMAL
   Schneidman E, 2006, NATURE, V440, P1007, DOI 10.1038/nature04701
   Seneta E, 2006, SPRINGER SER STAT, P1, DOI 10.1007/0-387-32792-4
   Shlens J, 2006, J NEUROSCI, V26, P8254, DOI 10.1523/JNEUROSCI.1282-06.2006
   Simoncelli E.P., 2004, COGNITIVE NEUROSCI
   Tang A, 2008, J NEUROSCI, V28, P505, DOI 10.1523/JNEUROSCI.3359-07.2008
   Theunissen FE, 2001, NETWORK-COMP NEURAL, V12, P289, DOI 10.1088/0954-898X/12/3/304
   Truccolo W, 2005, J NEUROPHYSIOL, V93, P1074, DOI 10.1152/jn.00697.2004
   Vasquez JC, 2012, J PHYSIOL-PARIS, V106, P120, DOI 10.1016/j.jphysparis.2011.11.001
   Vere-Jones D., 2003, INTRO THEORY POINT P
   Yu Shan, 2008, CEREB CORTEX
NR 56
TC 5
Z9 5
U1 1
U2 4
PD NOV
PY 2013
VL 107
IS 5
SI SI
BP 360
EP 368
DI 10.1016/j.jphysparis.2013.03.001
WC Neurosciences; Physiology
DA 2023-11-11
ER

PT J
AU Sasaki, K
   Morie, T
   Iwata, A
AF Sasaki, Kan'ya
   Morie, Takashi
   Iwata, Atsushi
TI A VLSI spiking feedback neural network with negative thresholding and
   its application to associative memory
SO IEICE TRANSACTIONS ON ELECTRONICS
DT Article
DE spiking neuron model; feedback network; global excitatory unit; negative
   thresholding; associative memory
AB An integrate-and-fire-type spiking feedback network is discussed in this paper. In our spiking neuron model, analog information expressing processing results is given by the relative relation of spike firing. Therefore, for spiking feedback networks, all neurons should fire (pseudo-)periodically. However, an integrate-and-fire-type neuron generates no spike unless its internal potential exceeds the threshold. To solve this problem, we propose negative thresholding operation. In this paper, this operation is achieved by a global excitatory unit. This unit operates immediately after receiving the first spike input. We have designed a CMOS spiking feedback network VLSI circuit with the global excitatory unit for Hopfield-type associative memory. The circuit simulation results show that the network achieves correct association operation.
C1 Hiroshima Univ, Grad Sch Adv Sci Matter, Higashihiroshima 7398530, Japan.
   Kyushu Inst Technol, Grad Sch Life Sci & Syst Engn, Kitakyushu, Fukuoka 8080196, Japan.
RP Sasaki, K (corresponding author), Hiroshima Univ, Grad Sch Adv Sci Matter, Higashihiroshima 7398530, Japan.
EM kanya@dsl.hiroshima-u.ac.jp
CR Hasegawa H, 2001, J PHYS SOC JPN, V70, P2210, DOI 10.1143/JPSJ.70.2210
   Hirai Y., 1996, Progress in Neural Information Processing. Proceedings of the International Conference on Neural Information Processing, P1251
   Li Y, 2004, IEEE COMP SOC ANN, P321, DOI 10.1109/ISVLSI.2004.1339571
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   MAASS W., 1999, PULSED NEURAL NETWOR
   MORIE T, 1994, IEEE J SOLID-ST CIRC, V29, P1086, DOI 10.1109/4.309904
   MURRAY A, 1994, ANALOGUE NEURAL VLSI
   RUMELHART D, 1986, PARALLEL DISTRIBUTE
   SASAKI K, 2004, 2004 IEEE INT MIDW S
   Thorpe SJ, 1997, ADV NEUR IN, V9, P901
   WALLACE BG, 2001, NEURON BRAIN
NR 11
TC 5
Z9 5
U1 0
U2 6
PD NOV
PY 2006
VL E89C
IS 11
BP 1637
EP 1644
DI 10.1093/ietele/e89-c.11.1637
WC Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Ulbrich, S
   Steward, T
   Peric, I
   Roennau, A
   Zöllner, JM
   Dillmann, R
AF Ulbrich, Stefan
   Steward, Terrence
   Peric, Igor
   Roennau, Arne
   Zoellner, J. Marius
   Dillmann, Ruediger
BE Howard, N
   Wang, Y
   Hussain, A
   Hamdy, F
   Widrow, B
   Zadeh, LA
TI Model-based Polynomial Function Approximation with Spiking Neural
   Networks
SO 2017 IEEE 16TH INTERNATIONAL CONFERENCE ON COGNITIVE INFORMATICS &
   COGNITIVE COMPUTING (ICCI*CC)
DT Proceedings Paper
CT IEEE 16th International Conference on Cognitive Informatics and
   Cognitive Computing (ICCI*CC)
CY JUL 26-28, 2017
CL Oxford, ENGLAND
DE Spiking neural networks; Neural engineering; Polynomial function
   approximation; Neurocomputing
ID BRAIN; MULTIPLICATION
AB Artificial neural networks are known to perform function approximation but with increasingly large non-redundant input spaces, the number of required neurons grows drastically. Functions have to be sampled densely leading to large data sets which imposes problems for applications such as neurorobotics, and requires a long time for training. Furthermore, they perform poorly on extrapolation as there are no model assumptions about the target function. This paper presents a novel network architecture of spiking neural networks for efficient model-based function approximation and prediction based on the concept of multivariate polynomial function approximation. This approach reduces the number of both training samples and required neurons, provides generalization and extrapolation depending on the chosen basis, and is capable of supervised learning. The network is implemented using the Neural Engineering Framework in the Nengo simulator and is centered around a mechanism for efficiently computing products of many input signals. We present the construction of the compound network, performance evaluation and propose a use case of its application.
C1 [Ulbrich, Stefan; Peric, Igor; Roennau, Arne; Zoellner, J. Marius; Dillmann, Ruediger] FZI Res Ctr Informat Technol, Haid & Neu Str 10-14, D-76131 Karlsruhe, Germany.
   [Steward, Terrence] Univ Waterloo, Computat Neurosci Res Grp, Waterloo, ON, Canada.
RP Ulbrich, S (corresponding author), FZI Res Ctr Informat Technol, Haid & Neu Str 10-14, D-76131 Karlsruhe, Germany.
EM sulbrich@fzi.de; tcstewar@uwaterloo.ca; peric@fzi.de; roennau@fzi.de;
   zoellner@fzi.de; dillmann@fzi.de
CR Ahmed F. Y., 2014, INT J ADV SOFT COMPU, V6
   Bekolay T, 2014, FRONT NEUROINFORM, V7, DOI 10.3389/fninf.2013.00048
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Davison AP, 2009, FRONT NEUROSCI-SWITZ, V3, P374, DOI 10.3389/neuro.01.036.2009
   Eliasmith C., 2003, NEURAL ENG COMPUTATI
   Eliasmith C, 2012, SCIENCE, V338, P1202, DOI 10.1126/science.1225266
   Farin G., 2001, CURVES SURFACES CAGD, V5th ed
   Furber SB, 2013, IEEE T COMPUT, V62, P2454, DOI 10.1109/TC.2012.142
   Gabbiani F, 2004, J PHYSIOLOGY-PARIS, V98, P19, DOI 10.1016/j.jphysparis.2004.03.001
   Jones AR, 2009, NAT REV NEUROSCI, V10, P821, DOI 10.1038/nrn2722
   Legenstein R, 2003, REV NEUROSCIENCE, V14, P5
   Markram H, 2012, SCI AM, V306, P50, DOI 10.1038/scientificamerican0612-50
   Markram H, 2011, PROCEDIA COMPUT SCI, V7, P39, DOI 10.1016/j.procs.2011.12.015
   Mishra D, 2006, IEEE IJCNN, P396
   Mundy A., 2015, P INT JOINT C NEUR N, P1, DOI DOI 10.1109/IJCNN.2015.7280390
   Nezis P, 2011, J NEURAL ENG, V8, DOI 10.1088/1741-2560/8/3/034005
   Peña JL, 2001, SCIENCE, V292, P249, DOI 10.1126/science.1059201
   Richter C., 2016, ARXIV E PRINTS
   Serrano-Gotarredona T, 2015, IEEE INT SYMP CIRC S, P2405, DOI 10.1109/ISCAS.2015.7169169
   SRINIVASAN MV, 1976, BIOL CYBERN, V21, P227, DOI 10.1007/BF00344168
   Stewart TC, 2014, P IEEE, V102, P881, DOI 10.1109/JPROC.2014.2306061
   Ulbrich S, 2012, IEEE T SYST MAN CY B, V42, P1215, DOI 10.1109/TSMCB.2012.2188507
   Vannucci L, 2015, IEEE-RAS INT C HUMAN, P1179, DOI 10.1109/HUMANOIDS.2015.7363512
   Walter J, 1996, NEUROCOMPUTING, V12, P131, DOI 10.1016/0925-2312(95)00117-4
NR 24
TC 0
Z9 1
U1 0
U2 1
PY 2017
BP 22
EP 27
WC Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Engineering, Biomedical; Imaging Science & Photographic
   Technology
DA 2023-11-11
ER

PT J
AU Uenohara, S
   Aihara, K
AF Uenohara, Seiji
   Aihara, Kazuyuki
TI A 18.7 TOPS/W Mixed-Signal Spiking Neural Network Processor With 8-bit
   Synaptic Weight On-Chip Learning That Operates in the Continuous-Time
   Domain
SO IEEE ACCESS
DT Article
DE Neurons; Synapses; Clocks; Computer architecture; Hardware; Transistors;
   Registers; ReSuMe; on-chip learning; compute-in-memory; CIM; spiking
   neural networks; SNN
ID NEURONS; MACRO; CMOS
AB We present a mixed-signal spiking neural networks processor with 8-bit synaptic weight on-chip learning in 40 nm CMOS that consists of a 10k mixed-signal synapse circuit and 100 analog leaky integrate-and-fire (LIF) neuron circuits. The processor has no clock signal except in peripheral circuits for I/O, and neuron and synapse circuits can operate asynchronously in the continuous-time domain, just like biological neurons. We demonstrate the energy efficiency of 6.24-18.7 TOPS/W in a multitarget spike learning task.
C1 [Uenohara, Seiji] Univ Tokyo, Inst Ind Sci, Tokyo 1538505, Japan.
   [Uenohara, Seiji] Kyushu Inst Technol, Grad Sch Life Sci & Syst Engn, Kitakyushu, Fukuoka 8080196, Japan.
   [Aihara, Kazuyuki] Univ Tokyo, Int Res Ctr Neurointelligence, Tokyo 1138654, Japan.
RP Uenohara, S (corresponding author), Univ Tokyo, Inst Ind Sci, Tokyo 1538505, Japan.; Uenohara, S (corresponding author), Kyushu Inst Technol, Grad Sch Life Sci & Syst Engn, Kitakyushu, Fukuoka 8080196, Japan.
EM s.uenohara@gmail.com
CR Agrawal A, 2021, IEEE SOLID-ST CIRC L, V4, P137, DOI 10.1109/LSSC.2021.3092727
   Akopyan F., IEEE T COMPUT AIDED, V34
   Bankman D, 2018, ISSCC DIG TECH PAP I, P222, DOI 10.1109/ISSCC.2018.8310264
   Buhler FN, 2017, SYMP VLSI CIRCUITS, pC30, DOI 10.23919/VLSIC.2017.8008536
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Dong Q, 2020, ISSCC DIG TECH PAP I, P242, DOI [10.1109/ISSCC19947.2020.9062985, 10.1109/isscc19947.2020.9062985]
   Gerstner W., 2002, SPIKING NEURON MODEL
   Gonugondla SK, 2018, ISSCC DIG TECH PAP I, P490, DOI 10.1109/ISSCC.2018.8310398
   Guo RQ, 2021, ISSCC DIG TECH PAP I, V64, P242, DOI 10.1109/ISSCC42613.2021.9365989
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Hoppner S., 2021, ARXIV210308392
   Hsieh HY, 2012, IEEE T NEUR NET LEAR, V23, P1065, DOI 10.1109/TNNLS.2012.2195329
   Indiveri G., 2015, IEDM, V2015, P2
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Khwa W.-S., IEEE INT SOLID STATE
   Kohno T, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00273
   Lau JH, 2009, 2009 IEEE 59TH ELECTRONIC COMPONENTS AND TECHNOLOGY CONFERENCE, VOLS 1-4, P635, DOI 10.1109/ECTC.2009.5074080
   Liu Q, 2020, ISSCC DIG TECH PAP I, P500, DOI 10.1109/ISSCC19947.2020.9062953
   Mochida R, 2018, 2018 IEEE SYMPOSIUM ON VLSI TECHNOLOGY, P175, DOI 10.1109/VLSIT.2018.8510676
   Nandakumar SR, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-64878-5
   Okumura S, 2019, SYMP VLSI CIRCUITS, pC248
   Park JH, 2019, IEEE SYS MAN CYBERN, P140, DOI 10.1109/SMC.2019.8914320
   Pei J, 2019, NATURE, V572, P106, DOI 10.1038/s41586-019-1424-8
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Qiao N, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00141
   Sakemi Y, 2020, SUPERVISED LEARNING
   Schemmel J, 2010, IEEE INT SYMP CIRC S, P1947, DOI 10.1109/ISCAS.2010.5536970
   Schoenauer T, 2002, IEEE T NEURAL NETWOR, V13, P205, DOI 10.1109/72.977304
   Si X, 2019, ISSCC DIG TECH PAP I, V62, P396, DOI 10.1109/ISSCC.2019.8662392
   Su J.-W., IEEE INT SOLID STATE
   Tanaka H, 2009, IEICE T FUND ELECTR, VE92A, P1690, DOI 10.1587/transfun.E92.A.1690
   Xue C.-X., IEEE INT SOLIDSTATE
   Yoon JH, 2020, ISSCC DIG TECH PAP I, P478, DOI 10.1109/ISSCC19947.2020.9063142
NR 33
TC 3
Z9 3
U1 3
U2 9
PY 2022
VL 10
BP 48338
EP 48348
DI 10.1109/ACCESS.2022.3170579
WC Computer Science, Information Systems; Engineering, Electrical &
   Electronic; Telecommunications
DA 2023-11-11
ER

PT C
AU Indiveri, G
   Fusi, S
AF Indiveri, Giacomo
   Fusi, Stefano
GP IEEE
TI Spike-based learning in VLSI networks of integrate- and-fire neurons
SO 2007 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOLS 1-11
SE IEEE International Symposium on Circuits and Systems
DT Proceedings Paper
CT IEEE International Symposium on Circuits and Systems
CY MAY 27-30, 2007
CL New Orleans, LA
ID SYNAPTIC PLASTICITY; PATTERNS
AB As the number of VLSI implementations of spike-based neural networks is steadily increasing, and the development of spike-based multi-chip systems is becoming more popular it is important to design spike-based learning algorithms and circuits, compatible with existing solutions, that endow these systems with adaptation and classification capabilities. We propose a spike-based learning algorithm that is highly effective in classifying complex patterns in semi-supervised fashion, and present neuromorphic circuits that support its VLSI implementation. We describe the architecture of a spike-based learning neural network, the analog circuits that implement the synaptic learning mechanism, and present results from a prototype VLSI chip comprising a full network of integrate-and-fire neurons and plastic synapses. We demonstrate how the VLSI circuits proposed reproduce the learning model's properties and fulfill its basic requirements for classifying complex patterns of mean firing rates.
C1 [Indiveri, Giacomo; Fusi, Stefano] Univ ETH Zurich, Inst Neuroinformat, CH-8057 Zurich, Switzerland.
RP Indiveri, G (corresponding author), Univ ETH Zurich, Inst Neuroinformat, Winterthurerstr 190, CH-8057 Zurich, Switzerland.
EM giacomo@ini.phys.ethz.ch
CR Abbott LF, 2000, NAT NEUROSCI, V3, P1178, DOI 10.1038/81453
   Alvado L, 2004, NEUROCOMPUTING, V58, P109, DOI 10.1016/j.neucom.2004.01.030
   [Anonymous], IEEE ISSCC
   [Anonymous], ADV NEURAL INFORM PR
   Badoni D, 2006, IEEE INT SYMP CIRC S, P1227
   Bartolozzi C., 2006, BRAIN INSPIRED COGNI, P1
   BARTOLOZZI C, 2007, IN PRESS NEURAL COMP
   Boahen KA, 2000, IEEE T CIRCUITS-II, V47, P416, DOI 10.1109/82.842110
   BRADER J, 2006, UNPUB NEURAL COMPUTA
   CHICCA E, 2006, IN PRESS IEEE T CIRC, V1
   Choi TYW, 2005, IEEE T CIRCUITS-I, V52, P1049, DOI 10.1109/TCSI.2005.849136
   Fusi S, 2002, BIOL CYBERN, V87, P459, DOI 10.1007/s00422-002-0356-8
   FUSI S, 2006, IN PRESS NATURE NEUR
   Gomez-Rodriguez F, 2006, IEEE INT SYMP CIRC S, P3253, DOI 10.1109/ISCAS.2006.1693319
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Indiveri G, 2006, IEEE T NEURAL NETWOR, V17, P211, DOI 10.1109/TNN.2005.860850
   Mallik U, 2005, IEEE INT SYMP CIRC S, P1919, DOI 10.1109/ISCAS.2005.1464988
   PETIT AB, 2003, P IEEE INT S CIRC SY, V5, P817
   Rasche C, 2000, ANALOG INTEGR CIRC S, V23, P227, DOI 10.1023/A:1008357931826
   Riis HK, 2004, 2004 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL 5, PROCEEDINGS, P393
   Senn W, 2005, NEURAL COMPUT, V17, P2106, DOI 10.1162/0899766054615644
   Serrao FV, 2005, PHYS THER SPORT, V6, P15, DOI 10.1016/j.ptsp.2004.03.001
   Turrigiano GG, 2004, NAT REV NEUROSCI, V5, P97, DOI 10.1038/nrn1327
   van Schaik A, 2005, IEEE INT SYMP CIRC S, P4213, DOI 10.1109/ISCAS.2005.1465560
NR 24
TC 26
Z9 27
U1 0
U2 3
PY 2007
BP 3371
EP 3374
DI 10.1109/ISCAS.2007.378290
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Engineering, Biomedical; Engineering, Electrical &
   Electronic; Mathematical & Computational Biology; Nanoscience &
   Nanotechnology; Imaging Science & Photographic Technology;
   Telecommunications
DA 2023-11-11
ER

PT J
AU Cao, LH
   Shen, JM
   Wang, L
   Wang, Y
AF Cao, Lihong
   Shen, Jiamin
   Wang, Lei
   Wang, Ye
TI Predicting spikes with artificial neural network
SO SCIENCE CHINA-INFORMATION SCIENCES
DT Letter
ID K+ CURRENT; NEURONS; MECHANISMS; CELLS
C1 [Cao, Lihong; Shen, Jiamin; Wang, Lei; Wang, Ye] Commun Univ China, Neurosci & Intelligent Media Inst, Beijing 100024, Peoples R China.
RP Cao, LH (corresponding author), Commun Univ China, Neurosci & Intelligent Media Inst, Beijing 100024, Peoples R China.
EM lihong.cao@cuc.edu.cn
CR Bean BP, 2007, NAT REV NEUROSCI, V8, P451, DOI 10.1038/nrn2148
   Ermentrout B, 1998, NEURAL COMPUT, V10, P1721, DOI 10.1162/089976698300017106
   Fohlmeister JF, 1997, J NEUROPHYSIOL, V78, P1935, DOI 10.1152/jn.1997.78.4.1935
   Gai Y, 2009, J NEUROPHYSIOL, V102, P3447, DOI 10.1152/jn.00538.2009
   Golomb D, 2007, PLOS COMPUT BIOL, V3, P1498, DOI 10.1371/journal.pcbi.0030156
   Golomb D, 2006, J NEUROPHYSIOL, V96, P1912, DOI 10.1152/jn.00205.2006
   Gouwens NW, 2010, PLOS COMPUT BIOL, V6, DOI 10.1371/journal.pcbi.1000951
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Miles R, 1991, NEURONAL NETWORKS HI, DOI 10.1017/cbo9780511895401
   Rothman JS, 2003, J NEUROPHYSIOL, V89, P3097, DOI 10.1152/jn.00127.2002
   Sun Y, 2009, J COMPUT NEUROSCI, V27, P369, DOI 10.1007/s10827-009-0151-9
   Wang XJ, 1996, J NEUROSCI, V16, P6402
NR 12
TC 4
Z9 4
U1 0
U2 5
PD JUN
PY 2018
VL 61
IS 6
AR 060428
DI 10.1007/s11432-017-9379-4
WC Computer Science, Information Systems; Engineering, Electrical &
   Electronic
DA 2023-11-11
ER

PT J
AU Andreev, AV
   Ivanchenko, MV
   Pisarchik, AN
   Hramov, AE
AF Andreev, Andrey, V
   Ivanchenko, Mikhail, V
   Pisarchik, Alexander N.
   Hramov, Alexander E.
TI Stimulus classification using chimera-like states in a spiking neural
   network
SO CHAOS SOLITONS & FRACTALS
DT Article
DE Bistability; Chimera state; Classifier; Hodgkin-Huxley model; Spiking
   neural network
ID NONLINEAR DYNAMICS; NOISE; PERCEPTION
AB A complex network of bistable Hodgkin-Huxley (HH) neurons with excitatory coupling can exhibit a partially spiking chimera behavior. We propose to use this chimera-like state for classification of the entering stimulus amplitude in the neural network with coexisting resting and spiking states. Due to different additive noise applied to each neuron in the network, the neurons are nonidentical. Therefore, depending on the amplitude of the external current, a part of the neurons stays in the resting state, while another part oscillates. Keeping fixed the coupling strength between neurons inside the network, we train the neural network on external pulses with two different amplitudes to adjust the coupling strength between the network neurons and two output neurons. We consider two variants of the classifier, in the presence and in the absence of inhibitory coupling between output neurons, and study how the output neurons respond to the external pulses of different amplitudes. The accuracy of the proposed classifier reaches 100% when the output neurons are inhibitory coupled, so that only one of these neurons is activated. (C) 2020 Elsevier Ltd. All rights reserved.
C1 [Andreev, Andrey, V; Pisarchik, Alexander N.; Hramov, Alexander E.] Innopolis Univ, Neurosci & Cognit Technol Lab, Univ Skaya 1, Innopolis 420500, Russia.
   [Ivanchenko, Mikhail, V] Lobachevsky State Univ Nizhny Novgorod, Math Future Technol Ctr, Dept Appl Math, Gagarina Av 23, Nizhnii Novgorod 603950, Russia.
   [Pisarchik, Alexander N.] Tech Univ Madrid, Ctr Biomed Technol, Campus Montegancedo, Madrid 28223, Spain.
RP Pisarchik, AN (corresponding author), Innopolis Univ, Neurosci & Cognit Technol Lab, Univ Skaya 1, Innopolis 420500, Russia.; Pisarchik, AN (corresponding author), Tech Univ Madrid, Ctr Biomed Technol, Campus Montegancedo, Madrid 28223, Spain.
EM alexander.pisarchik@ctb.upm.es
CR Andreev AV, 2019, PHYS REV E, V100, DOI 10.1103/PhysRevE.100.022224
   [Anonymous], 2013, BMC NEUROSCI
   Bandyopadhyay A, 2018, APPL MATH COMPUT, V333, P194, DOI 10.1016/j.amc.2018.03.084
   Barabási AL, 1999, SCIENCE, V286, P509, DOI 10.1126/science.286.5439.509
   Bera BK, 2019, CHAOS, V29, DOI 10.1063/1.5088833
   Bichler O, 2011, 2011 INT JOINT C NEU
   Bing ZS, 2018, FRONT NEUROROBOTICS, V12, DOI 10.3389/fnbot.2018.00035
   Carroll TL, 2007, CHAOS, V17, DOI 10.1063/1.2722870
   Fan YJ, 2018, NONLINEAR DYNAM, V93, P611, DOI 10.1007/s11071-018-4213-2
   Frolov N, 2019, CHAOS, V29, DOI 10.1063/1.5117263
   Glaze TA, 2016, CHAOS, V26, DOI 10.1063/1.4961122
   Hahne JM, 2014, IEEE T NEUR SYS REH, V22, P269, DOI 10.1109/TNSRE.2014.2305520
   Hassabis D, 2017, NEURON, V95, P245, DOI 10.1016/j.neuron.2017.06.011
   Hizanidis J, 2016, SCI REP-UK, V6, DOI 10.1038/srep19845
   Hizanidis J, 2014, INT J BIFURCAT CHAOS, V24, DOI 10.1142/S0218127414500308
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Hosny A, 2018, NAT REV CANCER, V18, P500, DOI 10.1038/s41568-018-0016-5
   Hossain MS, 2019, INFORM FUSION, V49, P69, DOI 10.1016/j.inffus.2018.09.008
   Hramov A. E., 2015, SPRINGER SERIES SYNE, DOI [10.1007/978-3-662-43850-3, DOI 10.1007/978-3-662-43850-3]
   Hramov A.E., BRAIN BEHAV COMPUTIN
   Hramov AE, 2019, CHAOS, V29, DOI 10.1063/1.5113844
   Hramov AE, 2018, CHAOS, V28, DOI 10.1063/1.5002892
   Hramov AE, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00674
   Ingrand F, 2014, AI COMMUN, V27, P63, DOI 10.3233/AIC-130578
   Kapitaniak T, 2014, SCI REP-UK, V4, DOI 10.1038/srep06379
   Kasabov N., 2007, EVOLVING CONNECTIONI
   Kuramoto Y., 2002, Nonlinear Phenomena in Complex Systems, V5, P380
   Lobov SA, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00088
   Lobov SA, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20020500
   Majhi S, 2019, PHYS LIFE REV, V28, P100, DOI 10.1016/j.plrev.2018.09.003
   Makarov VV, 2019, COMMUN NONLINEAR SCI, V71, P118, DOI 10.1016/j.cnsns.2018.11.015
   Maksimenko VA, 2018, COMPLEXITY, DOI 10.1155/2018/9385947
   Maksimenko VA, 2016, PHYS REV E, V94, DOI 10.1103/PhysRevE.94.052205
   Omelchenko I, 2015, PHYS REV E, V91, DOI 10.1103/PhysRevE.91.022917
   Omelchenko I, 2011, PHYS REV LETT, V106, DOI 10.1103/PhysRevLett.106.234102
   Özkaynak F, 2018, NONLINEAR DYNAM, V92, P305, DOI 10.1007/s11071-018-4056-x
   Pankratova EV, 2005, EUR PHYS J B, V45, P391, DOI 10.1140/epjb/e2005-00187-2
   Pisarchik AN, 2014, BIOL CYBERN, V108, P397, DOI 10.1007/s00422-014-0607-5
   Quiroga RQ, 2013, PRINCIPLES OF NEURAL CODING, P1, DOI 10.1201/b14756
   Roche AD, 2014, CURR SURG REP, V2, DOI 10.1007/s40137-013-0044-8
   Runnova AE, 2016, CHAOS SOLITON FRACT, V93, P201, DOI 10.1016/j.chaos.2016.11.001
   Scafati FT, 2018, NONLINEAR SYSTEMS CI
   Soltani R, 2017, J IND MANAG OPTIM, V13, P1701, DOI 10.3934/jimo.2017014
   Topol EJ, 2019, NAT MED, V25, P44, DOI 10.1038/s41591-018-0300-7
   Triggiani AI, 2017, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00604
   Tsigkri-DeSmedt ND, 2017, EUR PHYS J B, V90, DOI 10.1140/epjb/e2017-80162-0
   Voutsas K, 2007, IEEE T NEURAL NETWOR, V18, P1785, DOI 10.1109/TNN.2007.899623
   WANG GP, 2005, IEEE IJCNN, P416
   Yu D., 2016, AUTOMATIC SPEECH REC
NR 49
TC 15
Z9 17
U1 2
U2 13
PD OCT
PY 2020
VL 139
AR 110061
DI 10.1016/j.chaos.2020.110061
WC Mathematics, Interdisciplinary Applications; Physics, Multidisciplinary;
   Physics, Mathematical
DA 2023-11-11
ER

PT C
AU Jin, X
   Galluppi, F
   Patterson, C
   Rast, A
   Davies, S
   Temple, S
   Furber, S
AF Jin, Xin
   Galluppi, Francesco
   Patterson, Cameron
   Rast, Alexander
   Davies, Sergio
   Temple, Steve
   Furber, Steve
GP IEEE
TI Algorithm and Software for Simulation of Spiking Neural Networks on the
   Multi-Chip SpiNNaker System
SO 2010 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS IJCNN 2010
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT World Congress on Computational Intelligence (WCCI 2010)
CY 2010
CL Barcelona, SPAIN
ID MODEL
AB This paper presents the algorithm and software developed for parallel simulation of spiking neural networks on multiple SpiNNaker universal neuromorphic chips. It not only describes approaches to simulating neural network models, such as dynamics, neural representations, and synaptic delays, but also presents the software design of loading a neural application and initial a simulation on the multi-chip SpiNNaker system. A series of sub-issues are also investigated, such as neuron-processor allocation, synapses distribution, and route planning. The platform is verified by running spiking neural applications on both the SoC Designer model and the physical SpiNNaker Test Chip. This work sums the problems we have solved and highlights those requiring further investigations, and therefore it forms the foundation of the software design on SpiNNaker, leading the future development towards a universal platform for real-time simulations of extreme large-scale neural systems.
C1 [Jin, Xin; Galluppi, Francesco; Patterson, Cameron; Rast, Alexander; Davies, Sergio; Temple, Steve; Furber, Steve] Univ Manchester, Sch Comp Sci, Manchester, Lancs, England.
RP Jin, X (corresponding author), Univ Manchester, Sch Comp Sci, Manchester, Lancs, England.
EM jinxa@cs.man.ac.uk
CR FURBER S, 2009, P ACSD 09
   GLOVER M, 2009, 7 INT C MICR NEUR FU
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2008, P NATL ACAD SCI USA, V105, P3593, DOI 10.1073/pnas.0712231105
   JIN X, 2008, P 2008 INT JOINT C N
   Jin X., 2010, P 2010 INT JOINT C N, P1
   KHAN M, 2009, P 1 INT WORKSH RAP S
   KHAN MM, 2009, THESIS U MANCHESTER
   Maguire LP, 2007, NEUROCOMPUTING, V71, P13, DOI 10.1016/j.neucom.2006.11.029
   Markram H, 2006, NAT REV NEUROSCI, V7, P153, DOI 10.1038/nrn1848
   MEROLLA PA, 2007, IEEE T CIRCUITS SYST
   RAST AD, 2010, ACM INT C COMP FRONT
   RAST AD, 2008, P 2008 INT JOINT C N
NR 13
TC 0
Z9 0
U1 0
U2 1
PY 2010
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Computer Science, Theory & Methods; Engineering,
   Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Huemer, A
   Elizondo, D
   Gongora, M
AF Huemer, Andreas
   Elizondo, David
   Gongora, Mario
BE Franco, L
   Elizondo, DA
   Jerez, JM
TI A Constructive Neural Network for Evolving a Machine Controller in
   Real-Time
SO CONSTRUCTIVE NEURAL NETWORKS
SE Studies in Computational Intelligence
DT Proceedings Paper
CT 18th International Conference on Artificial Neural Networks
CY SEP, 2008
CL Prague, CZECH REPUBLIC
DE Constructive Neural Network; Spiking Neural Network; Reinforcement
   Learning; Growing Machine Controller
AB A novel method is presented to allow a machine controller to evolve while the machine is acting in its environment. The method uses a single spiking neural network with a minimum number of neurons and no initial connections. New connections and neurons are grown by evaluating reward values which can represent either the internal state of the machine or the rating of its task performance. This way the topology and the level of connectivity of the network are kept to a minimum. The method will be applied to a controller for an autonomous mobile robot.
C1 [Huemer, Andreas] De Montfort Univ, Inst Creat Technol, Leicester LE1 9BH, Leics, England.
   [Elizondo, David; Gongora, Mario] De Montfort Univ, Ctr Computat Intelligence, Leicester LE1 9BH, Leics, England.
RP Huemer, A (corresponding author), De Montfort Univ, Inst Creat Technol, Leicester LE1 9BH, Leics, England.
EM ahuemer@dmu.ac.uk; elizondo@dmu.ac.uk; mgongora@dmu.ac.uk
CR Alnajjar F., 2005, P INT C COMPUTATIONA, V1, P1134, DOI 10.1109/CIMCA.2005.1631415
   ALNAJJAR F, 2008, 2008 INT JOINT C NEU
   [Anonymous], 2001, ECHO STATE APPROACH
   [Anonymous], [No title captured]
   [Anonymous], 1991, ARTIFICIAL NEURAL NE
   [Anonymous], ADAPTIVE COMPUTATION
   DAUC E, 2006, HEBBIAN LEARNING LAR
   ELIZONDO D, 1995, INT C NEUR NETW 1995, V26, P290
   Elizondo D, 2006, IEEE IJCNN, P1776
   Florian RV, 2007, NEURAL COMPUT, V19, P1468, DOI 10.1162/neco.2007.19.6.1468
   FRITZKE B, 1994, NEURAL PROCESS LETT, V1, P2, DOI 10.1007/BF02312392
   Greenwood GW, 2008, LECT NOTES COMPUT SC, V5050, P368
   Hebb D. O., 1949, ORG BEHAV A NEUROPSY
   Hotz P. E., 2004, P 4 INT WORKSH EP RO, P119
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   IZHIKEVICH EM, 2007, CEREB CORTEX, V10, P1093
   KATIC D, 2006, LEAKY INTEGRATE AND
   Kohonen T., 1989, SELF ORG ASS MEMORY
   Liu J, 2005, INT C DEVEL LEARN, P121
   LIU J, 2006, T I SYSTEMS CONTROL, V19, P169
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Maes P., 1990, AAAI-90 Proceedings. Eighth National Conference on Artificial Intelligence, P796
   STANLEY KO, 2009, ARTIFICIAL IN PRESS
   Tajine M, 1998, NEURAL NETWORKS, V11, P1571, DOI 10.1016/S0893-6080(98)00054-9
   Vreeken J, 2003, SPIKING NEURAL NETWO
NR 25
TC 4
Z9 4
U1 0
U2 0
PY 2009
VL 258
BP 225
EP +
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic
DA 2023-11-11
ER

PT J
AU Ranganathan, GN
   Koester, HJ
AF Ranganathan, Gayathri Nattar
   Koester, Helmut Joachim
TI Correlations decrease with propagation of spiking activity in the mouse
   barrel cortex
SO FRONTIERS IN NEURAL CIRCUITS
DT Article
DE propagation; correlations; acousto-optical deflectors
ID RAT SOMATOSENSORY CORTEX; NEURONAL NETWORKS; SYNCHRONOUS SPIKING; SIGNAL
   PROPAGATION; CORTICAL COLUMN; FIRING RATE; IN-VITRO; DYNAMICS;
   CONNECTIVITY; PRECISION
AB Propagation of suprathreshold spiking activity through neuronal populations is important for the function of the central nervous system. Neural correlations have an impact on cortical function particularly on the signaling of information and propagation of spiking activity. Therefore we measured the change in correlations as suprathreshold spiking activity propagated between recurrent neuronal networks of the mammalian cerebral cortex. Using optical methods we recorded spiking activity from large samples of neurons from two neural populations simultaneously. The results indicate that correlations decreased as spiking activity propagated from layer 4 to layer 2/3 in the rodent barrel cortex.
C1 [Ranganathan, Gayathri Nattar; Koester, Helmut Joachim] Univ Texas Austin, Inst Neurosci, Neurobiol Sect, Ctr Learning & Memory, Austin, TX 78712 USA.
RP Ranganathan, GN (corresponding author), Univ Texas Austin, Inst Neurosci, Neurobiol Sect, Ctr Learning & Memory, 1 Univ Stn,C7000, Austin, TX 78712 USA.
EM gayathri.nattar@mail.utexas.edu
CR AERTSEN AMHJ, 1989, J NEUROPHYSIOL, V61, P900, DOI 10.1152/jn.1989.61.5.900
   AGMON A, 1991, NEUROSCIENCE, V41, P365, DOI 10.1016/0306-4522(91)90333-J
   Ahissar E, 2003, CEREB CORTEX, V13, P53, DOI 10.1093/cercor/13.1.53
   Bureau I, 2006, PLOS BIOL, V4, P2361, DOI 10.1371/journal.pbio.0040382
   de la Rocha J, 2007, NATURE, V448, P802, DOI 10.1038/nature06028
   Diesmann M, 1999, NATURE, V402, P529, DOI 10.1038/990101
   Feldmeyer D, 2002, J PHYSIOL-LONDON, V538, P803, DOI 10.1113/jphysiol.2001.012959
   Göbel W, 2007, PHYSIOLOGY, V22, P358, DOI 10.1152/physiol.00032.2007
   Grewe BF, 2010, NAT METHODS, V7, P399, DOI 10.1038/nmeth.1453
   Hirase H, 2004, PLOS BIOL, V2, P494, DOI 10.1371/journal.pbio.0020096
   Kimpo RR, 2003, J NEUROSCI, V23, P5750
   Koester HJ, 1999, BIOPHYS J, V77, P2226, DOI 10.1016/S0006-3495(99)77063-3
   KORALEK KA, 1988, BRAIN RES, V463, P346, DOI 10.1016/0006-8993(88)90408-8
   Kumar A, 2010, NAT REV NEUROSCI, V11, P615, DOI 10.1038/nrn2886
   London M, 2010, NATURE, V466, P123, DOI 10.1038/nature09086
   LU SM, 1993, SOMATOSENS MOT RES, V10, P1, DOI 10.3109/08990229309028819
   Lübke J, 2007, BRAIN STRUCT FUNCT, V212, P3, DOI 10.1007/s00429-007-0144-2
   Mehring C, 2003, BIOL CYBERN, V88, P395, DOI 10.1007/s00422-002-0384-4
   Ozer M, 2010, NEUROREPORT, V21, P338, DOI 10.1097/WNR.0b013e328336ee62
   Padmanabhan K, 2010, NAT NEUROSCI, V13, P1276, DOI 10.1038/nn.2630
   Panzeri S, 2001, NEURON, V29, P769, DOI 10.1016/S0896-6273(01)00251-3
   Ranganathan GN, 2010, J NEUROPHYSIOL, V104, P1812, DOI 10.1152/jn.00197.2010
   Renart A, 2010, SCIENCE, V327, P587, DOI 10.1126/science.1179850
   Reyes AD, 2003, NAT NEUROSCI, V6, P593, DOI 10.1038/nn1056
   Rosenbaum RJ, 2010, FRONT COMPUT NEUROSC, V4, DOI 10.3389/fncom.2010.00009
   ROSENBERG JR, 1989, PROG BIOPHYS MOL BIO, V53, P1, DOI 10.1016/0079-6107(89)90004-7
   Shea-Brown E, 2008, PHYS REV LETT, V100, DOI 10.1103/PhysRevLett.100.108102
   Stosiek C, 2003, P NATL ACAD SCI USA, V100, P7319, DOI 10.1073/pnas.1232232100
   Tetzlaff T, 2008, NEURAL COMPUT, V20, P2133, DOI 10.1162/neco.2008.05-07-525
   Thomson D. J., 1991, ADV SPECTRUM ANAL AR, P58
   van Rossum MCW, 2002, J NEUROSCI, V22, P1956, DOI 10.1523/JNEUROSCI.22-05-01956.2002
   Vogel A, 2007, J NEUROPHYSIOL, V97, P3376, DOI 10.1152/jn.00796.2006
   Vogels TP, 2005, J NEUROSCI, V25, P10786, DOI 10.1523/JNEUROSCI.3508-05.2005
   Vogels TP, 2005, ANNU REV NEUROSCI, V28, P357, DOI 10.1146/annurev.neuro.28.061604.135637
   Wiechert MT, 2010, NAT NEUROSCI, V13, P1003, DOI 10.1038/nn.2591
NR 35
TC 4
Z9 4
U1 0
U2 9
PD MAY 16
PY 2011
VL 5
AR 8
DI 10.3389/fncir.2011.00008
WC Neurosciences
DA 2023-11-11
ER

PT C
AU Li, RY
   Luo, XL
   Wang, J
AF Li Runyu
   Luo Xiaoling
   Wang Jun
GP IEEE
TI AN INTELLIGENT AND TRANSPARENT INFERENCE: SPIKING NEURAL NETWORK FOR
   CAUSAL REASONING
SO 2022 19TH INTERNATIONAL COMPUTER CONFERENCE ON WAVELET ACTIVE MEDIA
   TECHNOLOGY AND INFORMATION PROCESSING (ICCWAMTIP)
SE International Computer Conference on Wavelet Active Media Technology and
   Information Processing
DT Proceedings Paper
CT 19th IEEE International Computer Conference on Wavelet Active Media
   Technology and Information Processing (ICCWAMTIP)
CY DEC 16-18, 2022
CL Univ Elect Sci & Technol China, Chengdu, PEOPLES R CHINA
HO Univ Elect Sci & Technol China
DE Spiking neural network; Causal reasoning; STDP; Transparent inference
AB In light of mining large amounts of data, artificial intelligence (AI) has learned a very strong correlation between objects. However, its limitations lie in that it can't summarize the causality between objects like human beings and it forms the blind box association mechanism. In this paper, we address these limitations and make the contributions: We propose an implementation of causal reasoning based on spiking neural network (SNN), which simulates causality using information processing with spiking activities. And the spike-timing-dependent plastic rules (STDP) is utilized as a method of causal reasoning, which is based on the topological structure of causal graph and can make the process visible. Through experiments, our model completes the inference of causal ladder proposed by Judea Pearl, and experiments prove that it can complete more complex causal reasoning under the condition of integrating multiple causality.
C1 [Li Runyu; Luo Xiaoling] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 610054, Peoples R China.
   [Wang Jun] Univ Elect Sci & Technol China, Sch Publ Affairs & Adm, Chengdu 610054, Peoples R China.
RP Wang, J (corresponding author), Univ Elect Sci & Technol China, Sch Publ Affairs & Adm, Chengdu 610054, Peoples R China.
EM lry@std.uestc.edu.cn; luoxiaoling@std.uestc.edu.cn;
   wangjun190229@163.com
CR BALKE A, 1994, PROCEEDINGS OF THE TWELFTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE, VOLS 1 AND 2, P230
   Caporale N, 2008, ANNU REV NEUROSCI, V31, P25, DOI 10.1146/annurev.neuro.31.060407.125639
   de Haan Pim, 2019, NEURIPS
   Fang H, 2021, 2021 INT JOINT C NEU, P1
   Geirhos R, 2019, Arxiv, DOI [arXiv:1811.12231, DOI 10.48550/ARXIV.1811.12231]
   Gopnik A, 2000, CHILD DEV, V71, P1205, DOI 10.1111/1467-8624.00224
   Liu YH, 2001, J COMPUT NEUROSCI, V10, P25, DOI 10.1023/A:1008916026143
   Luo X., 2022, IEEE T NEUR NET LEAR
   Pearl J., 2018, THE BOOK OF WHY
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Schreiber S, 2003, NEUROCOMPUTING, V52-4, P925, DOI 10.1016/S0925-2312(02)00838-X
   Schulz L, 2012, TRENDS COGN SCI, V16, P382, DOI 10.1016/j.tics.2012.06.004
   Zhang ML, 2022, IEEE T NEUR NET LEAR, V33, P1947, DOI 10.1109/TNNLS.2021.3110991
   Zhang ML, 2020, NEUROCOMPUTING, V409, P103, DOI 10.1016/j.neucom.2020.03.079
   Zhang ML, 2019, IEEE T NEUR NET LEAR, V30, P123, DOI 10.1109/TNNLS.2018.2833077
   Zhang W, 2020, J ADV NURS, V76, P504, DOI 10.1111/jan.14247
NR 16
TC 0
Z9 0
U1 2
U2 2
PY 2022
DI 10.1109/ICCWAMTIP56608.2022.10016487
WC Computer Science, Information Systems; Computer Science, Theory &
   Methods; Telecommunications
DA 2023-11-11
ER

PT J
AU Liu, C
   Wang, HJ
   Du, YK
   Yuan, ZH
AF Liu, Chuang
   Wang, Haojie
   Du, Yingkui
   Yuan, Zhonghu
TI A Predictive Model for Student Achievement Using Spiking Neural Networks
   Based on Educational Data
SO APPLIED SCIENCES-BASEL
DT Article
DE student achievement; educational data mining; higher education; machine
   learning; spiking neural network
AB Student achievement prediction is one of the most important research directions in educational data mining. Student achievement directly reflects students' course mastery and lecturers' teaching level. Especially for the achievement prediction of college students, it not only plays an early warning and timely correction role for students and teachers, but also provides a method for university decision-makers to evaluate the quality of courses. Based on the existing research and experimental results, this paper proposes a student achievement prediction model based on evolutionary spiking neural network. On the basis of fully analyzing the relationship between course attributes and student attributes, a student achievement prediction model based on spiking neural network is established. The evolutionary membrane algorithm is introduced to learn hyperparameters of the model, so as to improve the accuracy of the model in predicting student achievement. Finally, the proposed model is used to predict student achievement on two benchmark student datasets, and the performance of the prediction model proposed in this paper is analyzed by comparing with other experimental algorithms. The experimental results show that the model based on spiking neural network can effectively improve the prediction accuracy of student achievement.
C1 [Liu, Chuang; Wang, Haojie; Du, Yingkui; Yuan, Zhonghu] Shenyang Univ, Sch Informat Engn, Shenyang 110044, Peoples R China.
RP Liu, C (corresponding author), Shenyang Univ, Sch Informat Engn, Shenyang 110044, Peoples R China.
EM chuang.liu@syu.edu.cn; mayang7566@163.com; yikui.du.cn@gmail.com;
   syyzh62@163.com
CR Ang KLM, 2020, IEEE ACCESS, V8, P116392, DOI 10.1109/ACCESS.2020.2994561
   Arora N., 2013, INT J INNOV RES SCI, V2, P4425
   Chaparro-Peláez J, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10010163
   Cortez P, 2008, 15TH EUROPEAN CONCURRENT ENGINEERING CONFERENCE/5TH FUTURE BUSINESS TECHNOLOGY CONFERENCE, P5
   Demertzis K, 2020, NEURAL COMPUT APPL, V32, P4303, DOI 10.1007/s00521-019-04363-x
   Dutti A, 2017, IEEE ACCESS, V5, P15991, DOI 10.1109/ACCESS.2017.2654247
   Ezz M, 2020, EDUC INF TECHNOL, V25, P2733, DOI 10.1007/s10639-019-10049-7
   Hooshyar D, 2020, ENTROPY-SWITZ, V22, DOI 10.3390/e22010012
   Injadat M, 2020, KNOWL-BASED SYST, V200, DOI 10.1016/j.knosys.2020.105992
   Karthikeyan VG, 2020, SOFT COMPUT, V24, P18477, DOI 10.1007/s00500-020-05075-4
   Khan A, 2021, EDUC INF TECHNOL, V26, P205, DOI 10.1007/s10639-020-10230-3
   Son LH, 2019, APPL INTELL, V49, P172, DOI 10.1007/s10489-018-1262-7
   Liu C, 2021, IEEE ACCESS, V9, P17071, DOI 10.1109/ACCESS.2021.3053280
   Liu C, 2019, KNOWL-BASED SYST, V165, P306, DOI 10.1016/j.knosys.2018.12.001
   Lobo JL, 2020, NEURAL NETWORKS, V121, P88, DOI 10.1016/j.neunet.2019.09.004
   Ma YL, 2019, SCI CHINA INFORM SCI, V62, DOI 10.1007/s11432-017-9371-y
   Mourad N, 2021, IET SIGNAL PROCESS, V15, P323, DOI 10.1049/sil2.12033
   Namoun A, 2021, APPL SCI-BASEL, V11, DOI 10.3390/app11010237
   Oh S, 2021, IEEE ACCESS, V9, P78098, DOI 10.1109/ACCESS.2021.3083056
   Pimentel JS, 2021, STATS-BASEL, V4, P682, DOI 10.3390/stats4030041
   Ramar K., 2013, INT J COMPUTER APPL, V63, P35, DOI DOI 10.5120/10489-5242
   Rastrollo-Guerrero JL, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10031042
   Romero C, 2020, WIRES DATA MIN KNOWL, V10, DOI 10.1002/widm.1355
   Salal YK, 2019, INT J ENG ADV TECHNO, V8, P54
   Salt L, 2020, IEEE T NEUR NET LEAR, V31, P3305, DOI 10.1109/TNNLS.2019.2941506
   Sokkhey Phauk, 2020, IEIE Transactions on Smart Processing & Computing, V9, P217, DOI 10.5573/IEIESPC.2020.9.3.217
   Taherkhani A, 2020, NEURAL NETWORKS, V122, P253, DOI 10.1016/j.neunet.2019.09.036
   Tan C, 2021, NEUROCOMPUTING, V434, P137, DOI 10.1016/j.neucom.2020.12.098
   Tsiakmaki M, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10010090
   Xing YY, 2010, IEEE T RELIAB, V59, P309, DOI 10.1109/TR.2010.2044539
   Yousafzai BK, 2021, SUSTAINABILITY-BASEL, V13, DOI 10.3390/su13179775
   Zhou Y, 2020, NEUROCOMPUTING, V406, P12, DOI 10.1016/j.neucom.2020.04.079
NR 32
TC 4
Z9 4
U1 6
U2 35
PD APR
PY 2022
VL 12
IS 8
AR 3841
DI 10.3390/app12083841
WC Chemistry, Multidisciplinary; Engineering, Multidisciplinary; Materials
   Science, Multidisciplinary; Physics, Applied
DA 2023-11-11
ER

PT J
AU Upegui, A
   Peña-Reyes, CA
   Sanchez, E
AF Upegui, A
   Peña-Reyes, CA
   Sanchez, E
TI An FPGA platform for on-line topology exploration of spiking neural
   networks
SO MICROPROCESSORS AND MICROSYSTEMS
DT Article
DE neural hardware; spiking neuron; evolvable hardware; topology evolution;
   dynamic reconfiguration; FPGA
ID OPTIMIZATION; EVOLUTION; NEURONS
AB In this paper we present a platform for evolving spiking neural networks on FPGAs. Embedded intelligent applications require both high performance, so as to exhibit real-time behavior, and flexibility, to cope with the adaptivity requirements. While hardware solutions offer performance, and software solutions offer flexibility, reconfigurable computing arises between these two types of solutions providing a tradeoff between flexibility and performance. Our platform is described as a combination of three parts: a hardware substrate, a computing engine, and an adaptation mechanism. We present, also, results about the performance and synthesis of the neural network implementation on an FPGA. (C) 2004 Elsevier B.V. All rights reserved.
C1 Swiss Fed Inst Technol, IN Ecublens, Log Syst Lab, CH-1015 Lausanne, Switzerland.
RP Upegui, A (corresponding author), Swiss Fed Inst Technol, IN Ecublens, Log Syst Lab, CH-1015 Lausanne, Switzerland.
EM andres.upegui@epfl.ch; carlos.pena@epfl.ch; eduardo.sanchez@epfl.ch
CR Abbass HA, 2003, NEURAL COMPUT, V15, P2705, DOI 10.1162/089976603322385126
   Floreano D, 2000, NEURAL NETWORKS, V13, P431, DOI 10.1016/S0893-6080(00)00032-0
   Gerstner W., 2002, SPIKING NEURON MODEL
   Goldberg D.E, 1989, GENETIC ALGORITHMS S, V27, P27
   Gould S. J., 2002, STRUCTURE EVOLUTIONA
   Haykin S., 1999, NEURAL NETWORKS COMP, V2nd, DOI DOI 10.1142/S0129065794000372
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Hüsken M, 2002, CONNECT SCI, V14, P219, DOI 10.1080/0954009021000047892
   Igel C, 2003, NEUROCOMPUTING, V55, P347, DOI 10.1016/S0925-2312(02)00628-8
   Kennedy J., 2001, SWARM INTELLIGENCE, DOI DOI 10.1016/B978-155860595-4/50007-3
   Langton CG., 1997, ARTIFICIAL LIFE OVER
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   MAASS W., 1999, PULSED NEURAL NETWOR
   MITCHELL T, 1989, ANNU REV COMPUT SCI, V4, P417
   Niv Y, 2002, ADAPT BEHAV, V10, P5, DOI 10.1177/10597123020101001
   PENAREYES CA, 2002, THESIS EPFL LAUSANNE, P148
   PEREZURIBE A, 1999, THESIS EPFL LAUSANNE
   REED R, 1993, IEEE T NEURAL NETWOR, V4, P740, DOI 10.1109/72.248452
   Ridley M, 2004, EVOLUTION
   ROGGEN D, 2003, 5 NASA DOD WORKSH EV
   Ros E, 2003, LECT NOTES COMPUT SC, V2687, P145
   Torres O, 2003, LECT NOTES COMPUT SC, V2687, P113
   Upegui A, 2003, LECT NOTES COMPUT SC, V2686, P136
   UPEGUI A, 2004, INT WORKSH BIOL INSP
   Vose M. D., 1999, COM ADAP SY
   *XIL CORP, 2002, XAPP 290 2 FLOWS PAR
   Yao X, 1999, P IEEE, V87, P1423, DOI 10.1109/5.784219
NR 27
TC 56
Z9 72
U1 0
U2 17
PD JUN 1
PY 2005
VL 29
IS 5
BP 211
EP 223
DI 10.1016/j.micpro.2004.08.012
WC Computer Science, Hardware & Architecture; Computer Science, Theory &
   Methods; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Mozafari, M
   Ganjtabesh, M
   Nowzari-Dalini, A
   Masquelíer, T
AF Mozafari, Milad
   Ganjtabesh, Mohammad
   Nowzari-Dalini, Abbas
   Masquelier, Timothee
TI SpykeTorch: Efficient Simulation of Convolutional Spiking Neural
   Networks With at Most One Spike per Neuron
SO FRONTIERS IN NEUROSCIENCE
DT Article
DE convolutional spiking neural networks; time-to-first-spike coding; one
   spike per neuron; STDP; reward-modulated STDP; tensor-based computing;
   GPU acceleration
ID VISUAL FEATURES
AB Application of deep convolutional spiking neural networks (SNNs) to artificial intelligence (AI) tasks has recently gained a lot of interest since SNNs are hardware-friendly and energy-efficient. Unlike the non-spiking counterparts, most of the existing SNN simulation frameworks are not practically efficient enough for large-scale AI tasks. In this paper; we introduce SpykeTorch, an open-source high-speed simulation framework based on PyTorch. This framework simulates convolutional SNNs with at most one spike per neuron and the rank-order encoding scheme. In terms of learning rules, both spike-timing-dependent plasticity (STDP) and reward-modulated STDP (R-STDP) are implemented, but other rules could be implemented easily. Apart from the aforementioned properties, SpykeTorch is highly generic and capable of reproducing the results of various studies. Computations in the proposed framework are tensor-based and totally done by PyTorch functions, which in turn brings the ability of just-in-time optimization for running on CPUs, GPUs, or Multi-GPU platforms.
C1 [Mozafari, Milad; Ganjtabesh, Mohammad; Nowzari-Dalini, Abbas] Univ Tehran, Sch Math Stat & Comp Sci, Dept Comp Sci, Tehran, Iran.
   [Mozafari, Milad; Masquelier, Timothee] Univ Toulouse 3, CNRS, CERCO UMR 5549, Toulouse, France.
RP Ganjtabesh, M (corresponding author), Univ Tehran, Sch Math Stat & Comp Sci, Dept Comp Sci, Tehran, Iran.
EM mgtabesh@ut.ac.ir
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   [Anonymous], 2019, ARXIV190109948
   [Anonymous], 2016, ARXIV161103000
   [Anonymous], 2017, 2017 IEEE INT S CIRC, DOI DOI 10.1109/ISCAS.2017.8050870
   Bekolay T, 2014, FRONT NEUROINFORM, V7, DOI 10.3389/fninf.2013.00048
   Bellec G., 2018, ADV NEURAL INFORM PR, P795
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Brzosko Z, 2017, ELIFE, V6, P1, DOI 10.7554/eLife.27756
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Carnevale T., 2006, NEURON BOOK, DOI DOI 10.1017/CBO9780511541612
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Diehl Peter U, 2015, 2015 INT JOINT C NEU, P1, DOI [DOI 10.1109/IJCNN.2015.7280696, 10.1109/IJCNN.2015.7280696]
   Falez P., 2019, ARXIV190401908
   Ferré P, 2018, FRONT COMPUT NEUROSC, V12, DOI 10.3389/fncom.2018.00024
   Florian RV, 2007, NEURAL COMPUT, V19, P1468, DOI 10.1162/neco.2007.19.6.1468
   Frémaux N, 2016, FRONT NEURAL CIRCUIT, V9, DOI 10.3389/fncir.2015.00085
   Furber S, 2016, J NEURAL ENG, V13, DOI 10.1088/1741-2560/13/5/051001
   Gerstner W, 1996, NATURE, V383, P76, DOI 10.1038/383076a0
   Gewaltig M-O., 2007, SCHOLARPEDIA, V2, DOI [10.4249/scholarpedia.1430, DOI 10.4249/SCHOLARPEDIA.1430]
   Gu JX, 2018, PATTERN RECOGN, V77, P354, DOI 10.1016/j.patcog.2017.10.013
   Hazan H, 2018, FRONT NEUROINFORM, V12, DOI 10.3389/fninf.2018.00089
   Hussain S, 2014, IEEE INT SYMP CIRC S, P2640, DOI 10.1109/ISCAS.2014.6865715
   Kasabov NK, 2014, NEURAL NETWORKS, V52, P62, DOI 10.1016/j.neunet.2014.01.006
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Kheradpisheh SR, 2016, NEUROCOMPUTING, V205, P382, DOI 10.1016/j.neucom.2016.04.029
   Liu T, 2017, ICCAD-IEEE ACM INT, P450, DOI 10.1109/ICCAD.2017.8203812
   Masquelier T, 2007, PLOS COMPUT BIOL, V3, P247, DOI 10.1371/journal.pcbi.0030031
   MINK JW, 1981, AM J PHYSIOL, V241, pR203, DOI 10.1152/ajpregu.1981.241.3.R203
   Mostafa Hesham, 2018, IEEE Trans Neural Netw Learn Syst, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Mozafari M, 2019, PATTERN RECOGN, V94, P87, DOI 10.1016/j.patcog.2019.05.015
   Mozafari M, 2018, IEEE T NEUR NET LEAR, V29, P6178, DOI 10.1109/TNNLS.2018.2826721
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   Paszke A., 2017, NEURIPS
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Rawat W, 2017, NEURAL COMPUT, V29, P2352, DOI [10.1162/NECO_a_00990, 10.1162/neco_a_00990]
   Shrestha S. B., 2018, ADV NEURAL INFORM PR
   Stimberg M, 2014, FRONT NEUROINFORM, V8, DOI 10.3389/fninf.2014.00006
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Thiele JC, 2018, FRONT COMPUT NEUROSC, V12, DOI 10.3389/fncom.2018.00046
   Thorpe S, 1996, NATURE, V381, P520, DOI 10.1038/381520a0
   Vaila R., 2019, ARXIV190312272
   Vitay J, 2015, FRONT NEUROINFORM, V9, DOI 10.3389/fninf.2015.00019
   Wu YT, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00935
   Yu Q, 2013, IEEE T NEUR NET LEAR, V24, P1539, DOI 10.1109/TNNLS.2013.2245677
   Zenke F, 2018, NEURAL COMPUT, V30, P1514, DOI 10.1162/neco_a_01086
NR 46
TC 47
Z9 49
U1 4
U2 24
PD JUL 12
PY 2019
VL 13
AR 625
DI 10.3389/fnins.2019.00625
WC Neurosciences
DA 2023-11-11
ER

PT C
AU Indiveri, G
   Corradi, F
   Qiao, N
AF Indiveri, Giacomo
   Corradi, Federico
   Qiao, Ning
GP IEEE
TI Neuromorphic Architectures for Spiking Deep Neural Networks
SO 2015 IEEE INTERNATIONAL ELECTRON DEVICES MEETING (IEDM)
DT Proceedings Paper
CT IEEE International Electron Devices Meeting (IEDM)
CY DEC 07-09, 2015
CL Washington, DC
AB We present a full custom hardware implementation of a deep neural network, built using multiple neuromorphic VLSI devices that integrate analog neuron and synapse circuits together with digital asynchronous logic circuits. The deep network comprises an event-based convolutional stage for feature extraction connected to a spike-based learning stage for feature classification. We describe the properties of the chips used to implement the network and present preliminary experimental results that validate the approach proposed.
C1 [Indiveri, Giacomo] Univ Zurich, Inst Neuroinformat, Zurich, Switzerland.
   Swiss Fed Inst Technol, Zurich, Switzerland.
RP Indiveri, G (corresponding author), Univ Zurich, Inst Neuroinformat, Zurich, Switzerland.
EM giacomo@ini.uzh.ch
CR Brader JM, 2007, NEURAL COMPUT, V19, P2881, DOI 10.1162/neco.2007.19.11.2881
   Breiman L, 2001, MACH LEARN, V45, P5, DOI 10.1023/A:1010933404324
   Brette R, 2005, J NEUROPHYSIOL, V94, P3637, DOI 10.1152/jn.00686.2005
   Chicca E, 2014, P IEEE, V102, P1367, DOI 10.1109/JPROC.2014.2313954
   Deng  Lei, 2015, SCI REPORTS, V5
   Indiveri G, 2015, P IEEE, V103, P1379, DOI 10.1109/JPROC.2015.2444094
   Indiveri G, 2013, NANOTECHNOLOGY, V24, DOI 10.1088/0957-4484/24/38/384010
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lichtsteiner P, 2008, IEEE J SOLID-ST CIRC, V43, P566, DOI 10.1109/JSSC.2007.914337
   MEAD C, 1990, P IEEE, V78, P1629, DOI 10.1109/5.58356
   Moradi S., 2013, CIRC THEOR DES ECCTD, P1, DOI [DOI 10.1109/ECCTD.2013.6662203, 10.1109/ECCTD.2013.6662203]
   Moradi  S., 2015, European patent application, Patent No. [EP 15/165272, 15165272]
   Naud R, 2008, BIOL CYBERN, V99, P335, DOI 10.1007/s00422-008-0264-7
   Qiao N, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00141
   Yu SM, 2011, IEEE T ELECTRON DEV, V58, P2729, DOI 10.1109/TED.2011.2147791
NR 15
TC 59
Z9 62
U1 1
U2 3
PY 2015
WC Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Lew, D
   Lee, K
   Park, J
AF Lew, Dongwoo
   Lee, Kyungchul
   Park, Jongsun
GP ACM
TI A Time-to-first-spike Coding and Conversion Aware Training for
   Energy-Efficient Deep Spiking Neural Network Processor Design
SO PROCEEDINGS OF THE 59TH ACM/IEEE DESIGN AUTOMATION CONFERENCE, DAC 2022
DT Proceedings Paper
CT 59th ACM/IEEE Design Automation Conference (DAC) - From Chips to Systems
   - Learn Today, Create Tomorrow
CY JUL 10-14, 2022
CL San Francisco, CA
DE Spiking Neural Network; ANN-to-SNN Conversion; Temporal Coding;
   Logarithmic Computations
AB In this paper, we present an energy-efficient SNN architecture, which can seamlessly run deep spiking neural networks (SNNs) with improved accuracy. First, we propose a conversion aware training (CAT) to reduce ANN-to-SNN conversion loss without hardware implementation overhead. In the proposed CAT, the activation function developed for simulating SNN during ANN training, is efficiently exploited to reduce the data representation error after conversion. Based on the CAT technique, we also present a time-to-first-spike coding that allows lightweight logarithmic computation by utilizing spike time information. The SNN processor design that supports the proposed techniques has been implemented using 28nm CMOS process. The processor achieves the top-1 accuracies of 91.7%, 67.9% and 57.4% with inference energy of 486.7uJ, 503.6uJ, and 1426uJ to process CIFAR10, CIFAR-100, and Tiny-ImageNet, respectively, when running VGG-16 with 5bit logarithmic weights.
C1 [Lew, Dongwoo; Lee, Kyungchul; Park, Jongsun] Korea Univ, Sch Elect Engn, Seoul, South Korea.
RP Lew, D (corresponding author), Korea Univ, Sch Elect Engn, Seoul, South Korea.
EM wwe9712@korea.ac.kr; 12251kc@korea.ac.kr; jongsun@korea.ac.kr
CR Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   Baek E, 2019, MICRO'52: THE 52ND ANNUAL IEEE/ACM INTERNATIONAL SYMPOSIUM ON MICROARCHITECTURE, P304, DOI 10.1145/3352460.3358268
   Bouvier M, 2019, ACM J EMERG TECH COM, V15, DOI 10.1145/3304103
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Deng L, 2020, IEEE J SOLID-ST CIRC, V55, P2228, DOI 10.1109/JSSC.2020.2970709
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Jacob B, 2018, PROC CVPR IEEE, P2704, DOI 10.1109/CVPR.2018.00286
   Jouppi NP, 2017, 44TH ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA 2017), P1, DOI 10.1145/3079856.3080246
   Miyashita D, 2016, Arxiv, DOI arXiv:1603.01025
   Narayanan S, 2020, ANN I S COM, P349, DOI 10.1109/ISCA45697.2020.00038
   O'Connor M, 2017, 50TH ANNUAL IEEE/ACM INTERNATIONAL SYMPOSIUM ON MICROARCHITECTURE (MICRO), P41, DOI 10.1145/3123939.3124545
   Park S, 2020, DES AUT CON, DOI [10.1109/dac18072.2020.9218689, 10.1007/s00779-020-01476-2]
   Roy K, 2019, NATURE, V575, P607, DOI 10.1038/s41586-019-1677-2
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Vogel S, 2018, ICCAD-IEEE ACM INT, DOI 10.1145/3240765.3240803
   Wu YJ, 2019, AAAI CONF ARTIF INTE, P1311
NR 16
TC 3
Z9 3
U1 1
U2 1
PY 2022
BP 265
EP 270
DI 10.1145/3489517.3530457
WC Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Karimipanah, Y
   Ma, ZY
   Wessel, R
AF Karimipanah, Yahya
   Ma, Zhengyu
   Wessel, Ralf
TI Criticality predicts maximum irregularity in recurrent networks of
   excitatory nodes
SO PLOS ONE
DT Article
ID SELF-ORGANIZED CRITICALITY; NEURONAL AVALANCHES; CORTICAL NETWORKS;
   DYNAMICS; POPULATIONS; CORTEX; RANGE; CHAOS
AB A rigorous understanding of brain dynamics and function requires a conceptual bridge between multiple levels of organization, including neural spiking and network-level population activity. Mounting evidence suggests that neural networks of cerebral cortex operate at a critical regime, which is defined as a transition point between two phases of short lasting and chaotic activity. However, despite the fact that criticality brings about certain functional advantages for information processing, its supporting evidence is still far from conclusive, as it has been mostly based on power law scaling of size and durations of cascades of activity. Moreover, to what degree such hypothesis could explain some fundamental features of neural activity is still largely unknown. One of the most prevalent features of cortical activity in vivo is known to be spike irregularity of spike trains, which is measured in terms of the coefficient of variation (CV) larger than one. Here, using a minimal computational model of excitatory nodes, we show that irregular spiking (CV > 1) naturally emerges in a recurrent network operating at criticality. More importantly, we show that even at the presence of other sources of spike irregularity, being at criticality maximizes the mean coefficient of variation of neurons, thereby maximizing their spike irregularity. Furthermore, we also show that such a maximized irregularity results in maximum correlation between neuronal firing rates and their corresponding spike irregularity (measured in terms of CV). On the one hand, using a model in the universality class of directed percolation, we propose new hallmarks of criticality at single-unit level, which could be applicable to any network of excitable nodes. On the other hand, given the controversy of the neural criticality hypothesis, we discuss the limitation of this approach to neural systems and to what degree they support the criticality hypothesis in real neural networks. Finally, we discuss the limitations of applying our results to real networks and to what degree they support the criticality hypothesis.
C1 [Karimipanah, Yahya; Ma, Zhengyu; Wessel, Ralf] Washington Univ, Dept Phys, St Louis, MO 63130 USA.
RP Karimipanah, Y (corresponding author), Washington Univ, Dept Phys, St Louis, MO 63130 USA.
EM yahyakp@gmail.com
CR Arviv O, 2015, J NEUROSCI, V35, P13927, DOI 10.1523/JNEUROSCI.0477-15.2015
   BAK P, 1987, PHYS REV LETT, V59, P381, DOI 10.1103/PhysRevLett.59.381
   Beggs JM, 2003, J NEUROSCI, V23, P11167
   Beggs JM, 2012, FRONT PHYSIOL, V3, DOI 10.3389/fphys.2012.00163
   Bellay T, 2015, ELIFE, V4, DOI 10.7554/eLife.07224
   Benayoun M, 2010, PLOS COMPUT BIOL, V6, DOI 10.1371/journal.pcbi.1000846
   Bialek W, 2014, P NATL ACAD SCI USA, V111, P7212, DOI 10.1073/pnas.1324045111
   Brunel N, 2000, J COMPUT NEUROSCI, V8, P183, DOI 10.1023/A:1008925309027
   Buice MA, 2009, PROG BIOPHYS MOL BIO, V99, P53, DOI 10.1016/j.pbiomolbio.2009.07.003
   CHEN DM, 1995, J PHYS A-MATH GEN, V28, P5177, DOI 10.1088/0305-4470/28/18/009
   Denève S, 2016, NAT NEUROSCI, V19, P375, DOI 10.1038/nn.4243
   di Santo S, 2017, PHYS REV E, V95, DOI 10.1103/PhysRevE.95.032115
   Eurich CW, 2002, PHYS REV E, V66, DOI 10.1103/PhysRevE.66.066137
   Fakhraei L, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0173658
   Fraiman D, 2009, PHYS REV E, V79, DOI 10.1103/PhysRevE.79.061922
   Friedman N, 2012, PHYS REV LETT, V108, DOI 10.1103/PhysRevLett.108.208102
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   Hahn G, 2010, J NEUROPHYSIOL, V104, P3312, DOI 10.1152/jn.00953.2009
   Hesse J, 2014, FRONT SYST NEUROSCI, V8, DOI 10.3389/fnsys.2014.00166
   Kadmon J, 2017, PLOS ONE, V12
   Karimipanah Y, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0177396
   Kastner DB, 2015, P NATL ACAD SCI USA, V112, P2533, DOI 10.1073/pnas.1418092112
   Kinouchi O, 2006, NAT PHYS, V2, P348, DOI 10.1038/nphys289
   Klaus A, 2013, PLOS ONE, V6
   Larremore DB, 2012, PHYS REV E, V85, DOI 10.1103/PhysRevE.85.066131
   Larremore DB, 2011, PHYS REV LETT, V106, DOI 10.1103/PhysRevLett.106.058101
   Legenstein R, 2007, NEURAL NETWORKS, V20, P323, DOI 10.1016/j.neunet.2007.04.017
   Mora T, 2011, J STAT PHYS, V144, P268, DOI 10.1007/s10955-011-0229-4
   Niebur E., 2014, REV NONLINEAR DYNAMI
   Nykter M, 2009, PHYS REV E, V79
   Okun M, 2015, NATURE, V521, P511, DOI 10.1038/nature14273
   Ostojic S, 2014, NAT NEUROSCI, V17, P594, DOI 10.1038/nn.3658
   Scott G, 2014, J NEUROSCI, V34, P16611, DOI 10.1523/JNEUROSCI.3474-14.2014
   Shew WL, 2015, NAT PHYS, V11, P659, DOI 10.1038/NPHYS3370
   Shew WL, 2011, J NEUROSCI, V31, P55, DOI 10.1523/JNEUROSCI.4637-10.2011
   Shew WL, 2009, J NEUROSCI, V29, P15595, DOI 10.1523/JNEUROSCI.3864-09.2009
   Shriki O, 2011, J NEUROSCI, V33, P7079
   SOFTKY WR, 1993, J NEUROSCI, V13, P334
   Sole RV, 1999, TRENDS ECOL EVOL, V14, P156, DOI 10.1016/S0169-5347(98)01518-3
   Stepp N, 2015, PLOS COMPUT BIOL, V11, DOI 10.1371/journal.pcbi.1004043
   Stevens CF, 1998, NAT NEUROSCI, V1, P210, DOI 10.1038/659
   Sussillo D, 2009, NEURON, V63, P544, DOI 10.1016/j.neuron.2009.07.018
   Tagliazucchi E, 2012, FRONT PHYSIOL, V3, DOI 10.3389/fphys.2012.00015
   Taillefumier T, 2013, P NATL ACAD SCI USA, V110, pE1438, DOI 10.1073/pnas.1212479110
   Tkacik G, 2015, P NATL ACAD SCI USA, V112, P11508, DOI 10.1073/pnas.1514188112
   vanVreeswijk C, 1996, SCIENCE, V274, P1724, DOI 10.1126/science.274.5293.1724
NR 46
TC 5
Z9 5
U1 0
U2 5
PD AUG 17
PY 2017
VL 12
IS 8
AR e0182501
DI 10.1371/journal.pone.0182501
WC Multidisciplinary Sciences
DA 2023-11-11
ER

PT C
AU Zapata, M
   Madrenas, J
   Zapata, M
   Alvarez, J
AF Zapata, Mireya
   Madrenas, Jordi
   Zapata, Miroslava
   Alvarez, Jorge
BE Ahram, T
TI Axonal Delay Controller for Spiking Neural Networks Based on FPGA
SO ADVANCES IN ARTIFICIAL INTELLIGENCE, SOFTWARE AND SYSTEMS ENGINEERING
SE Advances in Intelligent Systems and Computing
DT Proceedings Paper
CT 10th Int Conf on Appl Human Factors and Ergon (AHFE) / AHFE Int Conf
   Human Factors in Artificial Intelligence and Social Comp / AHFE Int Conf
   on Human Factors, Software, Serv and Syst Engn / AHFE Int Conf of Human
   Factors in Energy
CY JUL 24-28, 2019
CL Washington, DC
DE Axonal delay; FPGA; Spiking Neural Networks
ID DYNAMICS; MODEL
AB In this paper, the implementation of a programmable Axonal Delay Controller (ADyC) mapped on a hardware Neural Processor (NP) FPGA-based is reported. It is possible to define axonal delays between 1 to 31 emulation cycles to global and local pre-synaptic spikes generated by NP, extending the temporal characteristics supported by this architecture. The prototype presented in this work contributes to the realism of the network, which mimics the temporal biological characteristics of spike propagation through the cortex. The contribution of temporal information is strongly related to the learning process. ADyC operation is transparent for the rest of the system and neither affects the remaining tasks executed by the NP nor the emulation time period. In addition, an example implemented on hardware of a neural oscillator with programmable delays configured for a set of neurons is presented in order to demonstrate full platform functionality and operability.
C1 [Zapata, Mireya; Madrenas, Jordi] Univ Politecn Cataluna, Dept Elect Engn, Jordi Girona 1-3,Edif C4, ES-08034 Barcelona, Catalunya, Spain.
   [Zapata, Mireya; Alvarez, Jorge] Univ Tecnol Indoamer, Res Ctr Mechatron & Interact Syst, Quito, Ecuador.
   [Zapata, Miroslava] Univ Armed Forces ESPE, Dept Elect Engn, Av Gen Ruminahui, Sangolqui, Ecuador.
RP Zapata, M; Madrenas, J (corresponding author), Univ Politecn Cataluna, Dept Elect Engn, Jordi Girona 1-3,Edif C4, ES-08034 Barcelona, Catalunya, Spain.; Zapata, M; Alvarez, J (corresponding author), Univ Tecnol Indoamer, Res Ctr Mechatron & Interact Syst, Quito, Ecuador.; Zapata, M (corresponding author), Univ Armed Forces ESPE, Dept Elect Engn, Av Gen Ruminahui, Sangolqui, Ecuador.
EM miyarezapata@uti.edu.ec; jordi.madrenas@upc.edu; mazapata@espe.edu.ec;
   jorgealvarez@uti.edu.ec
CR Bringuier V, 1999, SCIENCE, V283, P695, DOI 10.1126/science.283.5402.695
   Brunel N, 2000, J COMPUT NEUROSCI, V8, P183, DOI 10.1023/A:1008925309027
   Dorta T, 2016, NEUROCOMPUTING, V171, P1684, DOI 10.1016/j.neucom.2015.07.080
   Ghosh-Dastidar S, 2009, ADV INTEL SOFT COMPU, V61, P167
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2004, CEREB CORTEX, V14, P933, DOI 10.1093/cercor/bhh053
   Madrenas J, 2009, PROCEEDINGS OF THE 2009 NASA/ESA CONFERENCE ON ADAPTIVE HARDWARE AND SYSTEMS, P376, DOI 10.1109/AHS.2009.31
   Roxin A, 2005, PHYS REV LETT, V94, DOI 10.1103/PhysRevLett.94.238103
   Scholze S, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00117
   Sripad A, 2018, NEURAL NETWORKS, V97, P28, DOI 10.1016/j.neunet.2017.09.011
   Wang RCM, 2014, FRONT NEUROSCI-SWITZ, V8, DOI 10.3389/fnins.2014.00051
   Zapata M., 2016, 25 INT C ART NEUR NE, V8681, P222, DOI [10.1007/978-3-319-11179-7, DOI 10.1007/978-3-319-11179-7]
   Zapata M, 2018, NASA ESA CONF, P241, DOI 10.1109/AHS.2018.8541463
NR 13
TC 0
Z9 0
U1 0
U2 1
PY 2020
VL 965
BP 284
EP 292
DI 10.1007/978-3-030-20454-9_29
WC Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Computer Science, Software Engineering
DA 2023-11-11
ER

PT C
AU Dutson, M
   Li, Y
   Gupta, M
AF Dutson, Matthew
   Li, Yin
   Gupta, Mohit
GP IEEE
TI Spike-Based Anytime Perception
SO 2023 IEEE/CVF WINTER CONFERENCE ON APPLICATIONS OF COMPUTER VISION
   (WACV)
SE IEEE Winter Conference on Applications of Computer Vision
DT Proceedings Paper
CT 23rd IEEE/CVF Winter Conference on Applications of Computer Vision
   (WACV)
CY JAN 03-07, 2023
CL Waikoloa, HI
ID NEURAL-NETWORK; COINCIDENCE; NEURONS
AB In many emerging computer vision applications, it is critical to adhere to stringent latency and power constraints. The current neural network paradigm of frame-based, floating-point inference is often ill-suited to these resource-constrained applications. Spike-based perception - enabled by spiking neural networks (SNNs) - is one promising alternative. Unlike conventional neural networks (ANNs), spiking networks exhibit smooth tradeoffs between latency, power, and accuracy. SNNs are the archetype of an "anytime algorithm" whose accuracy improves smoothly over time. This property allows SNNs to adapt their computational investment in response to changing resource constraints. Unfortunately, mainstream algorithms for training SNNs (i.e., those based on ANN-to-SNN conversion) tend to produce models that are inefficient in practice. To mitigate this problem, we propose a set of principled optimizations that reduce latency and power consumption by 1-2 orders of magnitude in converted SNNs. These optimizations leverage a set of novel efficiency metrics designed for anytime algorithms. We also develop a state-of-the-art simulator, SaRNN, which can simulate SNNs using commodity GPU hardware and neuromorphic platforms. We hope that the proposed optimizations, metrics, and tools will facilitate the future development of spike-based vision systems.
C1 [Dutson, Matthew; Li, Yin; Gupta, Mohit] Univ Wisconsin Madison, Madison, WI 53706 USA.
RP Dutson, M (corresponding author), Univ Wisconsin Madison, Madison, WI 53706 USA.
EM dutson@wisc.org; yin.li@wisc.org; mgupta37@wisc.org
CR Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265
   Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   Pérez-Carrasco JA, 2013, IEEE T PATTERN ANAL, V35, P2706, DOI 10.1109/TPAMI.2013.71
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Bing Han, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13555, DOI 10.1109/CVPR42600.2020.01357
   BODDY M, 1994, ARTIF INTELL, V67, P245, DOI 10.1016/0004-3702(94)90054-X
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Burges, 2010, MNIST HANDWRITTEN DI, DOI DOI 10.1561/2400000035
   Canziani Alfredo, 2017, ARXIV160507678CS
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Carnevale N.T., 2006, NEURON BOOK, DOI [DOI 10.1017/CBO9780511541612, 10.1017/CBO9780511541612]
   Chin Ting-Wu, 2019, ARXIV190202910CS
   Clarke Lyndon, 1994, PROGRAMMING ENV MASS, P213
   Courbariaux M., 2015, ADV NEURAL INFORM PR, V28, P3123, DOI [DOI 10.5555/2969442.2969588, DOI 10.1109/TWC.2016.2633262]
   Dalcín L, 2005, J PARALLEL DISTR COM, V65, P1108, DOI 10.1016/j.jpdc.2005.03.010
   Dally W.J., 2015, ADV NEURAL INFORM PR, P1135
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Davison Andrew P., 2009, FRONTIERS NEUROINFOR, V2
   Deng Shikuang, 2022, INT C LEARN REPR FEB
   Diehl Peter U, 2015, 2015 INT JOINT C NEU, P1, DOI [DOI 10.1109/IJCNN.2015.7280696, 10.1109/IJCNN.2015.7280696]
   Dutton NAW, 2016, IEEE T ELECTRON DEV, V63, P189, DOI 10.1109/TED.2015.2464682
   Farabet C, 2012, FRONT NEUROSCI-SWITZ, V6, DOI 10.3389/fnins.2012.00032
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   Hassibi B., 1992, P ADV NEUR INF PROC, V5, DOI DOI 10.5555/645753.668069
   He YX, 2023, TRANSPORTMETRICA A, V19, DOI 10.1080/23249935.2022.2033348
   Howard A. G., 2017, ARXIV
   Hunsberger Eric, 2016, ARXIV161105141CS
   Hwang K, 2014, IEEE WRK SIG PRO SYS, P174
   Iandola F. N., 2016, ARXIV
   Ioffe S., 2015, ICML, DOI DOI 10.1007/S13398-014-0173-7.2
   Johnson S.G., NLOPT NONLINEAR OPTI
   Kahneman D., 2011, THINKING FAST SLOW, DOI DOI 10.1017/CBO9780511808098.004
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   LeCun Y., 1990, ADV NEURAL INFORM PR, P598, DOI DOI 10.5555/109230.109298
   Lee JM, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00191
   Li H., 2017, INT C LEARN REPR, DOI DOI 10.48550/ARXIV.1608.08710
   Lichtsteiner Patrick, 2008, IEEE Journal of Solid-State Circuits, V43, P566, DOI 10.1109/JSSC.2007.914337
   Liu W, 2016, LECT NOTES COMPUT SC, V9905, P21, DOI 10.1007/978-3-319-46448-0_2
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   Mostafa H, 2018, IEEE T NEUR NET LEAR, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Neftci EO, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00324
   Neil D., 2016, P 31 ANN ACM S APPL, P293
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32
   Redmon J., 2016, YOU ONLY LOOK ONCE U, DOI DOI 10.1109/CVPR.2016.91
   Rhodes Oliver, 2018, FRONTIERS NEUROSCIEN, V12
   Rowan Thomas Harvey, 1991, THESIS
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Samaddar R, 2019, GLOBAL GOVERNANCE AND INDIA'S NORTH-EAST: LOGISTICS, INFRASTRUCTURE AND SOCIETY, P1
   Schemmel J, 2010, IEEE INT SYMP CIRC S, P1947, DOI 10.1109/ISCAS.2010.5536970
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Teerapittayanon S, 2016, INT C PATT RECOG, P2464, DOI 10.1109/ICPR.2016.7900006
   Tsung-Yi Lin, 2017, 2017 IEEE International Conference on Computer Vision (ICCV), P2999, DOI 10.1109/ICCV.2017.324
   Vanhoucke Vincent, 2011, DEEP LEARN UNS FEAT
   Veit A, 2018, LECT NOTES COMPUT SC, V11205, P3, DOI 10.1007/978-3-030-01246-5_1
   Wu YJ, 2019, AAAI CONF ARTIF INTE, P1311
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Wu ZX, 2018, PROC CVPR IEEE, P8817, DOI 10.1109/CVPR.2018.00919
   Xie YP, 2020, HUM CELL, V33, P220, DOI 10.1007/s13577-019-00305-w
   Xu R, 2020, PR IEEE COMP DESIGN, P494, DOI 10.1109/ICCD50377.2020.00089
   Yang L, 2020, PROC CVPR IEEE, P2366, DOI 10.1109/CVPR42600.2020.00244
   Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716
   Zilberstein S, 1996, AI MAG, V17, P73
NR 66
TC 0
Z9 0
U1 0
U2 0
PY 2023
BP 5283
EP 5293
DI 10.1109/WACV56688.2023.00526
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic; Imaging Science & Photographic Technology
DA 2023-11-11
ER

PT J
AU Cofré, R
   Cessac, B
AF Cofre, R.
   Cessac, B.
TI Exact computation of the maximum-entropy potential of spiking
   neural-network models
SO PHYSICAL REVIEW E
DT Article
ID GANGLION-CELLS; RETINA; TRAINS; DYNAMICS; NOISE
AB Understanding how stimuli and synaptic connectivity influence the statistics of spike patterns in neural networks is a central question in computational neuroscience. The maximum-entropy approach has been successfully used to characterize the statistical response of simultaneously recorded spiking neurons responding to stimuli. However, in spite of good performance in terms of prediction, the fitting parameters do not explain the underlying mechanistic causes of the observed correlations. On the other hand, mathematical models of spiking neurons (neuromimetic models) provide a probabilistic mapping between the stimulus, network architecture, and spike patterns in terms of conditional probabilities. In this paper we build an exact analytical mapping between neuromimetic and maximum-entropy models.
C1 [Cofre, R.] Univ Nice, INRIA, NeuroMathComp Team, F-06902 Sophia Antipolis, France.
   Univ Nice, Lab Math JA Dieudonne, F-06902 Sophia Antipolis, France.
RP Cofré, R (corresponding author), Univ Nice, INRIA, NeuroMathComp Team, 2004 Route Lucioles, F-06902 Sophia Antipolis, France.
CR Ahmadian Y, 2011, NEURAL COMPUT, V23, P46, DOI 10.1162/NECO_a_00059
   [Anonymous], 1988, DEGRUYTER STUDIES MA
   [Anonymous], 1983, GEOMETRICAL METHODS
   BESAG J, 1974, J ROY STAT SOC B MET, V36, P192
   Bialek W., ARXIV07124397
   Bowen R, 2008, LECT NOTES MATH, V470, P1, DOI 10.1007/978-3-540-77695-6
   BRILLINGER DR, 1988, BIOL CYBERN, V59, P189, DOI 10.1007/BF00318010
   Cessac B, 2011, J MATH BIOL, V62, P863, DOI 10.1007/s00285-010-0358-4
   Cessac B., 2013, 8329 INRIA
   Chichilnisky EJ, 2001, NETWORK-COMP NEURAL, V12, P199, DOI 10.1088/0954-898X/12/2/306
   Cocco S, 2009, P NATL ACAD SCI USA, V106, P14058, DOI 10.1073/pnas.0906705106
   Cofré R, 2013, CHAOS SOLITON FRACT, V50, P13, DOI 10.1016/j.chaos.2012.12.006
   Fernández R, 2005, J STAT PHYS, V118, P555, DOI 10.1007/s10955-004-8821-5
   Ganmor E, 2011, J NEUROSCI, V31, P3044, DOI 10.1523/JNEUROSCI.3682-10.2011
   Gantmacher F. R., 1959, THEORY MATRICES 2, V2
   Gerstner W., 2002, SPIKING NEURON MODEL
   Granot-Atedgi E, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1002922
   Grimmett GR., 1973, B LOND MATH SOC, V5, P81, DOI [10.1112/blms/5.1.81, DOI 10.1112/BLMS/5.1.81]
   Hammersley J. M., UNPUB
   JAYNES ET, 1957, PHYS REV, V106, P620, DOI 10.1103/PhysRev.106.620
   Keller G., 1998, EQUILIBRIUM STATES E
   Livsic A.N., 1972, MATH USSR IZVESTIYA, V6, P1278, DOI [/10.1070/IM1972v006n06ABEH001919, DOI 10.1070/IM1972V006N06ABEH001919]
   Marre O, 2009, PHYS REV LETT, V102, DOI 10.1103/PhysRevLett.102.138101
   Pollicott M, 2003, COMMUN MATH PHYS, V240, P457, DOI 10.1007/s00220-003-0905-6
   Rieke F., 1997, SPIKES EXPLORING NEU
   Schneidman E, 2006, NATURE, V440, P1007, DOI 10.1038/nature04701
   Shadlen Michael N., 1994, Current Opinion in Neurobiology, V4, P569, DOI 10.1016/0959-4388(94)90059-0
   Shlens J, 2006, J NEUROSCI, V26, P8254, DOI 10.1523/JNEUROSCI.1282-06.2006
   Soula H, 2006, NEURAL COMPUT, V18, P60, DOI 10.1162/089976606774841567
   Tkacik G, 2013, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2013/03/P03011
   Vasquez JC, 2012, J PHYSIOL-PARIS, V106, P120, DOI 10.1016/j.jphysparis.2011.11.001
NR 31
TC 17
Z9 17
U1 0
U2 7
PD MAY 12
PY 2014
VL 89
IS 5
AR 052117
DI 10.1103/PhysRevE.89.052117
WC Physics, Fluids & Plasmas; Physics, Mathematical
DA 2023-11-11
ER

PT J
AU She, XY
   Mukhopadhyay, S
AF She, Xueyuan
   Mukhopadhyay, Saibal
TI SPEED: Spiking Neural Network With Event-Driven Unsupervised Learning
   and Near-Real-Time Inference for Event-Based Vision
SO IEEE SENSORS JOURNAL
DT Article
DE Dynamicvision sensor (DVS); event-drivenprocess; neuromorphic vision
   sensor; spiking neural network; unsupervised learning
AB A fully event-based image processing pipeline containing neuromorphic vision sensors and spiking neural network has the potential to achieve high throughput, low latency and high dynamic range vision processing. In this work, we present an end-to-end SNN unsupervised learning inference framework to achievenear-real-time processingper-formance. The design uses fully event-driven operations that significantly improve learning and inference speed: over 100 x increase of inference throughput on CPU and near-real-time inference on GPU for neuromorphic vision sensors can be achieved. The event-driven processing method supports unsupervised spike-timing-dependent plasticity learning of convolutional SNN. When labels are limited, it achieves higher accuracy than supervised training approaches. In addition, the proposed method improves robustness for low-precision SNN as it reduces spiking activity distortion and achieves higher learning accuracy than regular discrete-time simulated low-precision networks.
C1 [She, Xueyuan; Mukhopadhyay, Saibal] Georgia Inst Technol, Sch Elect & Comp Engn, Atlanta, GA 30332 USA.
RP She, XY (corresponding author), Georgia Inst Technol, Sch Elect & Comp Engn, Atlanta, GA 30332 USA.
EM xshe@gatech.edu; saibal.mukhopadhyay@ece.gatech.edu
CR Afshar S, 2020, IEEE SENS J, V20, P15117, DOI 10.1109/JSEN.2020.3009687
   Amir A, 2017, PROC CVPR IEEE, P7388, DOI 10.1109/CVPR.2017.781
   Brandli C, 2014, IEEE J SOLID-ST CIRC, V49, P2333, DOI 10.1109/JSSC.2014.2342715
   Cannici M, 2019, IEEE COMPUT SOC CONF, P1656, DOI 10.1109/CVPRW.2019.00209
   Chen G, 2020, IEEE SENS J, V20, P10211, DOI 10.1109/JSEN.2020.2990752
   Chen G, 2020, IEEE SENS J, V20, P6170, DOI 10.1109/JSEN.2020.2973049
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Delbruck T, 2007, IEEE INT SYMP CIRC S, P845, DOI 10.1109/ISCAS.2007.378038
   Gallego G, 2022, IEEE T PATTERN ANAL, V44, P154, DOI 10.1109/TPAMI.2020.3008413
   Gehrig D, 2019, IEEE I CONF COMP VIS, P5632, DOI 10.1109/ICCV.2019.00573
   Gerstner W, 2002, BIOL CYBERN, V87, P404, DOI 10.1007/s00422-002-0353-y
   Gerstner W., 2002, SPIKING NEURON MODEL
   Hazan H, 2018, FRONT NEUROINFORM, V12, DOI 10.3389/fninf.2018.00089
   Jang H, 2020, ARXIV201208300
   Kaiser J, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00424
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Khoei MA, 2020, 2020 2ND IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE CIRCUITS AND SYSTEMS (AICAS 2020), P256, DOI [10.1109/AICAS48895.2020.9073827, 10.1109/aicas48895.2020.9073827]
   Kulkarni SR, 2018, NEURAL NETWORKS, V103, P118, DOI 10.1016/j.neunet.2018.03.019
   Lee C, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00435
   Lichtsteiner P, 2008, IEEE J SOLID-ST CIRC, V43, P566, DOI 10.1109/JSSC.2007.914337
   Lu S, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00535
   Lytton WW, 2005, NEURAL COMPUT, V17, P903, DOI 10.1162/0899766053429453
   Massa R., ARXIV200609985
   Messikommer Nico, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12353), P415, DOI 10.1007/978-3-030-58598-3_25
   Moreno-Bote R, 2015, SCI REP-UK, V5, DOI 10.1038/srep17531
   Nageswaran JM, 2009, IEEE IJCNN, P3201
   Naveros F, 2017, FRONT NEUROINFORM, V11, DOI 10.3389/fninf.2017.00007
   Paredes-Vallés F, 2020, IEEE T PATTERN ANAL, V42, P2051, DOI 10.1109/TPAMI.2019.2903179
   Pecevski D, 2014, FRONT NEUROINFORM, V8, DOI 10.3389/fninf.2014.00070
   Ramesh B, 2020, IEEE T PATTERN ANAL, V42, P2767, DOI 10.1109/TPAMI.2019.2919301
   Rebecq H, 2021, IEEE T PATTERN ANAL, V43, P1964, DOI 10.1109/TPAMI.2019.2963386
   She X., FRONTIERS NEUROSCI, V14
   She XY, 2019, DES AUT TEST EUROPE, P450, DOI [10.23919/DATE.2019.8714846, 10.23919/date.2019.8714846]
   Shrestha SB, 2018, ADV NEUR IN, V31
   Sironi A, 2018, PROC CVPR IEEE, P1731, DOI 10.1109/CVPR.2018.00186
   Stromatias E, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00222
   Wu YJ, 2019, AAAI CONF ARTIF INTE, P1311
   Xing YN, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.590164
NR 38
TC 3
Z9 3
U1 2
U2 12
PD SEP 15
PY 2021
VL 21
IS 18
BP 20578
EP 20588
DI 10.1109/JSEN.2021.3098013
WC Engineering, Electrical & Electronic; Instruments & Instrumentation;
   Physics, Applied
DA 2023-11-11
ER

PT C
AU Saranirad, V
   McGinnity, TM
   Dora, S
   Coyle, D
AF Saranirad, Vahid
   McGinnity, T. M.
   Dora, Shirin
   Coyle, Damien
GP IEEE
TI DoB-SNN: A New Neuron Assembly-inspired Spiking Neural Network for
   Pattern Classification
SO 2021 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUL 18-22, 2021
CL ELECTR NETWORK
DE spiking neural network; SNN; degree of belonging; DoB; spiking neurons;
   Integrate and Fire
AB Spiking neural networks (SNNs) as the third generation of artificial neural networks are closer to their biological counterparts than their predecessors. SNNs have a higher computational capacity and lower power requirements than networks of sigmoidal neurons. In this paper, a new spiking neural network for pattern classification referred to as Degree of Belonging SNN (DoB-SNN) is introduced. DoB-SNN is inspired by a neuronal assembly where each neuron has a degree of belonging to every class of data being process. DoB-SNN clusters the neurons during the training process using DoBs to allocate a group of neurons to each class. A new training algorithm is presented to adjust DoBs along with the network's synaptic weights, based on Spike-Timing Dependent Plasticity (STDP) and neurons' activity for training samples. The performance of DoB-SNN is evaluated on five datasets from the UCI machine learning repository. Nested Cross-Validation is employed to determine the network's hyperparameters for each dataset and thoroughly assess generalisation capability. A detailed comparison on these datasets with three other supervised learning algorithms, including SpikeProp, SWAT, and SRESN is provided. The results show that no algorithm significantly outperforms DoB-SNN, Whereas DoB-SNN has significantly better performance than others for Liver disorders dataset (>6.10%, p<0.01). Accuracies obtained by DoB-SNN are significantly greater than SWAT for both Iris and Breast Cancer (>1.69%, p<0.001) and significantly better than SpikeProp for Iris (1.62%, p=0.04). In all comparisons, DoB-SNN used the smallest network, among others. DoB-SNN therefore offers significant potential as alternative SNN architecture and learning algorithm.
C1 [Saranirad, Vahid; McGinnity, T. M.; Dora, Shirin; Coyle, Damien] Ulster Univ, Intelligent Syst Res Ctr, Derry, North Ireland.
RP Saranirad, V (corresponding author), Ulster Univ, Intelligent Syst Res Ctr, Derry, North Ireland.
EM saranirad-v@ulster.ac.uk; tm.mcginnity@ulster.ac.uk;
   s.dora@ulster.ac.uk; dh.coyle@ulster.ac.uk
CR Almomani A, 2019, CLUSTER COMPUT, V22, P419, DOI 10.1007/s10586-018-02891-0
   Bing ZS, 2019, IEEE INT CONF ROBOT, P9645, DOI [10.1109/icra.2019.8793774, 10.1109/ICRA.2019.8793774]
   Bing ZS, 2019, FRONT NEUROROBOTICS, V13, DOI 10.3389/fnbot.2019.00018
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Dheeru D., 2017, UCI MACHINE LEARNING
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   DIEHL PU, 2015, PROC INT JT CONF NEU
   DORA S, 2016, NEUROCOMPUTING
   Fisher R., 1921, METRON, V1, P3, DOI DOI 10.1093/BIOMET/9.1-2.22
   Fisher R.A., 1949, DESIGN EXPT, V5
   Frémaux N, 2016, FRONT NEURAL CIRCUIT, V9, DOI 10.3389/fncir.2015.00085
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   Gerstner W, 2018, FRONT NEURAL CIRCUIT, V12, DOI 10.3389/fncir.2018.00053
   Hebb DO, ORG BEHAV NEUROPSYCH
   Izhikevich E. M., 2005, DYNAMICAL SYSTEMS NE
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   KHERADPISHEH SR, 2018, NEURAL NETWORKS
   KOHAVI R, 1995, A STUDY OF CROSSVALI
   LeCun Y., MNIST HANDWRITTEN DI
   Lee C, 2019, IEEE T COGN DEV SYST, V11, P384, DOI 10.1109/TCDS.2018.2833071
   Maass W, 1997, NEURAL COMPUT, V9, P279, DOI 10.1162/neco.1997.9.2.279
   Machingal P, 2020, IEEE IJCNN, DOI 10.1109/ijcnn48605.2020.9207620
   Markram Henry, 2011, Front Synaptic Neurosci, V3, P4, DOI 10.3389/fnsyn.2011.00004
   MOZAFARI M, 2018, FIRST SPIKE BASED VI
   Papadimitriou CH, 2020, P NATL ACAD SCI USA, V117, P14464, DOI 10.1073/pnas.2001893117
   Querlioz D, 2013, IEEE T NANOTECHNOL, V12, P288, DOI 10.1109/TNANO.2013.2250995
   SHRESTHA A, 2017, PROC INT JT CONF NEU, P1999
   Shrestha SB, 2018, ADV NEUR IN, V31
   SRINIVASAN G, 2017, PROC INT JT CONF NEU, P1847
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Thiele JC, 2018, FRONT COMPUT NEUROSC, V12, DOI 10.3389/fncom.2018.00046
   Wade JJ, 2010, IEEE T NEURAL NETWOR, V21, P1817, DOI 10.1109/TNN.2010.2074212
NR 32
TC 0
Z9 0
U1 3
U2 8
PY 2021
DI 10.1109/IJCNN52387.2021.9534283
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Liu, Q
   Long, LF
   Peng, H
   Wang, J
   Yang, Q
   Song, XX
   Riscos-Núñez, A
   Pérez-Jiménez, MJ
AF Liu, Qian
   Long, Lifan
   Peng, Hong
   Wang, Jun
   Yang, Qian
   Song, Xiaoxiao
   Riscos-Nunez, Agustin
   Perez-Jimenez, Mario J.
TI Gated Spiking Neural P Systems for Time Series Forecasting
SO IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
DT Article
DE Gated spiking neural P (GSNP) systems; prediction model; spiking neural
   P (SNP) systems; time series forecasting
ID MULTIVARIATE; NETWORK
AB Spiking neural P (SNP) systems are a class of neural-like computing models, abstracted by the mechanism of spiking neurons. This article proposes a new variant of SNP systems, called gated spiking neural P (GSNP) systems, which are composed of gated neurons. Two gated mechanisms are introduced in the nonlinear spiking mechanism of GSNP systems, consisting of a reset gate and a consumption gate. The two gates are used to control the updating of states in neurons. Based on gated neurons, a prediction model for time series is developed, known as the GSNP model. Several benchmark univariate and multivariate time series are used to evaluate the proposed GSNP model and to compare several state-of-the-art prediction models. The comparison results demonstrate the availability and effectiveness of GSNP for time series forecasting.
C1 [Liu, Qian; Long, Lifan; Peng, Hong; Yang, Qian] Xihua Univ, Sch Comp & Software Engn, Chengdu 610039, Peoples R China.
   [Wang, Jun; Song, Xiaoxiao] Xihua Univ, Sch Elect Engn & Elect Informat, Chengdu 610039, Peoples R China.
   [Riscos-Nunez, Agustin; Perez-Jimenez, Mario J.] Univ Seville, Res Inst Informat Engn I3US, Res Grp Nat Comp, Seville 41012, Spain.
RP Peng, H (corresponding author), Xihua Univ, Sch Comp & Software Engn, Chengdu 610039, Peoples R China.
EM ph.xhu@hotmail.com; wj.xhu@hotmail.com
CR Aboagye-Sarfo P, 2015, J BIOMED INFORM, V57, P62, DOI 10.1016/j.jbi.2015.06.022
   Bai SJ, 2018, Arxiv, DOI [arXiv:1803.01271, DOI 10.48550/ARXIV.1803.01271]
   Box G. E. P., 1968, J AM STAT ASSOC, V65, P1509, DOI DOI 10.1080/01621459.1970.10481180
   Das M, 2017, PATTERN RECOGN LETT, V93, P192, DOI 10.1016/j.patrec.2017.01.002
   datareportal, US
   Dey R, 2017, MIDWEST SYMP CIRCUIT, P1597, DOI 10.1109/MWSCAS.2017.8053243
   Du SD, 2020, NEUROCOMPUTING, V388, P269, DOI 10.1016/j.neucom.2019.12.118
   Gers FA, 1999, IEE CONF PUBL, P850, DOI [10.1162/089976600300015015, 10.1049/cp:19991218]
   Geva AB, 1998, IEEE T NEURAL NETWOR, V9, P1471, DOI 10.1109/72.728396
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Han M, 2019, IEEE T CYBERNETICS, V49, P1160, DOI 10.1109/TCYB.2018.2789686
   Hu J, 2020, NEUROCOMPUTING, V383, P122, DOI 10.1016/j.neucom.2019.11.060
   Ionescu M, 2006, FUND INFORM, V71, P279
   JANG JSR, 1993, IEEE T SYST MAN CYB, V23, P665, DOI 10.1109/21.256541
   Kuznetsov V, 2019, Arxiv, DOI arXiv:1805.03714
   Li B, 2021, SIGNAL PROCESS, V178, DOI 10.1016/j.sigpro.2020.107793
   Li B, 2021, INT J NEURAL SYST, V31, DOI 10.1142/S0129065720500501
   Li B, 2020, KNOWL-BASED SYST, V196, DOI 10.1016/j.knosys.2020.105794
   Li JC, 2021, KNOWL-BASED SYST, V211, DOI 10.1016/j.knosys.2020.106508
   Liu PH, 2020, KNOWL-BASED SYST, V203, DOI 10.1016/j.knosys.2020.106081
   MACKEY MC, 1977, SCIENCE, V197, P287, DOI 10.1126/science.267326
   Pai PF, 2010, EXPERT SYST APPL, V37, P4261, DOI 10.1016/j.eswa.2009.11.076
   Pan LQ, 2017, INT J NEURAL SYST, V27, DOI 10.1142/S0129065717500423
   Pan LQ, 2009, INT J COMPUT COMMUN, V4, P273, DOI 10.15837/ijccc.2009.3.2435
   Paum G, 2007, J UNIVERS COMPUT SCI, V13, P1707
   Paun Gh, 2010, OXFORD HDB MEMBRANE
   Peng H, 2021, COMPUT VIS IMAGE UND, V210, DOI 10.1016/j.cviu.2021.103228
   Peng H, 2020, NEURAL NETWORKS, V127, P110, DOI 10.1016/j.neunet.2020.04.014
   Peng H, 2020, INT J NEURAL SYST, V30, DOI 10.1142/S0129065720500082
   Peng H, 2020, KNOWL-BASED SYST, V188, DOI 10.1016/j.knosys.2019.105064
   Peng H, 2019, KNOWL-BASED SYST, V163, P875, DOI 10.1016/j.knosys.2018.10.016
   Peng H, 2019, IEEE T NEUR NET LEAR, V30, P1672, DOI 10.1109/TNNLS.2018.2872999
   Peng H, 2018, IEEE T SMART GRID, V9, P4777, DOI 10.1109/TSG.2017.2670602
   Peng H, 2017, INTEGR COMPUT-AID E, V24, P401, DOI 10.3233/ICA-170552
   Peng H, 2017, NEURAL NETWORKS, V95, P66, DOI 10.1016/j.neunet.2017.08.003
   Peng H, 2013, INFORM SCIENCES, V235, P106, DOI 10.1016/j.ins.2012.07.015
   Renaud O, 2005, IEEE T SYST MAN CY B, V35, P1241, DOI 10.1109/TSMCB.2005.850182
   Song T, 2014, THEOR COMPUT SCI, V529, P82, DOI 10.1016/j.tcs.2014.01.001
   Song XX, 2021, INT J NEURAL SYST, V31, DOI 10.1142/S0129065720500422
   UCI Machine Learning Repository, US
   Wang J, 2013, IEEE T FUZZY SYST, V21, P209, DOI 10.1109/TFUZZ.2012.2208974
   Wang K, 2019, NEUROCOMPUTING, V360, P107, DOI 10.1016/j.neucom.2019.05.023
   Wang T, 2015, IEEE T POWER SYST, V30, P1182, DOI 10.1109/TPWRS.2014.2347699
   Wu TF, 2018, IEEE T NEUR NET LEAR, V29, P3349, DOI 10.1109/TNNLS.2017.2726119
   Yahoo!, 2017, GSPC HIST PRIC S P 5
   Yao SC, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P351, DOI 10.1145/3038912.3052577
   Zhang Y, 2014, INT J NEURAL SYST, V24, DOI 10.1142/S0129065714500038
   Zheng G., 1999, J COMPUTATIONAL INTE, V7, P18
NR 48
TC 25
Z9 25
U1 23
U2 49
PD SEP
PY 2023
VL 34
IS 9
BP 6227
EP 6236
DI 10.1109/TNNLS.2021.3134792
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Computer Science, Theory & Methods; Engineering,
   Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Kaiser, F
   Feldbusch, F
AF Kaiser, Florian
   Feldbusch, Fridtjof
BE MarquesDeSa, J
   Alexandre, LA
   Duch, W
   Mandic, DP
TI Building a bridge between spiking and artificial neural networks
SO ARTIFICIAL NEURAL NETWORKS - ICANN 2007, PT 1, PROCEEDINGS
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 17th International Conference on Artificial Neural Networks (ICANN 2007)
CY SEP 09-13, 2007
CL Oporto, PORTUGAL
AB Spiking neural networks (SNN) are a promising approach for the detection of patterns with a temporal component. However they provide more parameters than conventional artificial neural networks (ANN) which make them hard to handle. Many error-gradient-based approaches work with a time-to-firstspike code because the explicit calculation of a gradient in SNN is - due to the nature of spikes - very difficult. In this paper, we present the estimation of such an error-gradient based on the gain function of the neurons. This is done by interpreting spike trains as rate codes in a given time interval. This way a bridge is built between SNN and ANN. This bridge allows us to train the SNN with the well-known error back-propagation algorithm for ANN.
C1 [Kaiser, Florian; Feldbusch, Fridtjof] Univ Karlsruhe, CES, Karlsruhe, Germany.
RP Kaiser, F (corresponding author), Univ Karlsruhe, CES, Karlsruhe, Germany.
EM fkaiser@ira.uka.de; feldbusch@ira.uka.de
CR [Anonymous], PROBENL SET NEURAL N
   Belatreche A, 2003, P IEEE CYB INT CHALL, P39
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Gerstner W., 2002, SPIKING NEURON MODEL
   Kasinski A., 2006, International Journal of Applied Mathematics and Computer Science, V16, P101
   Konig P, 1996, TRENDS NEUROSCI, V19, P130, DOI 10.1016/S0166-2236(96)80019-1
   Marian I, 2002, THESIS U COLL DUBLIN
   RIEDMILLER M, 1992, RPROP FAST ADAPTIVE
   Ruf B, 1997, NEURAL PROCESS LETT, V5, P9, DOI 10.1023/A:1009697008681
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   WAIBEL A, 1989, IEEE T ACOUST SPEECH, V37, P328, DOI 10.1109/29.21701
   WAN EA, 1993, P NATO ADV WORKSH TI
NR 12
TC 5
Z9 5
U1 0
U2 1
PY 2007
VL 4668
BP 380
EP +
PN I
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods
DA 2023-11-11
ER

PT C
AU Bautembach, D
   Oikonomidis, I
   Kyriazis, N
   Argyros, A
AF Bautembach, Dennis
   Oikonomidis, Iason
   Kyriazis, Nikolaos
   Argyros, Antonis
GP IEEE
TI Faster and Simpler SNN Simulation with Work Queues
SO 2020 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN) held as part
   of the IEEE World Congress on Computational Intelligence (IEEE WCCI)
CY JUL 19-24, 2020
CL ELECTR NETWORK
DE spiking neural networks; SNN; simulation; DSL; work queues
ID EVENT-DRIVEN SIMULATION; SPIKING NEURONS; NETWORKS; DYNAMICS
AB We present a clock-driven Spiking Neural Network simulator which is up to 3x faster than the state of the art while, at the same time, being more general and requiring less programming effort on both the user's and maintainer's side. This is made possible by designing our pipeline around "work queues" which act as interfaces between stages and greatly reduce implementation complexity. We evaluate our work using three well-established SNN models on a series of benchmarks.
C1 [Bautembach, Dennis; Oikonomidis, Iason; Kyriazis, Nikolaos; Argyros, Antonis] FORTH ICS, Iraklion, Greece.
   [Bautembach, Dennis; Argyros, Antonis] CSD UOC, Iraklion, Greece.
RP Bautembach, D (corresponding author), FORTH ICS, Iraklion, Greece.; Bautembach, D (corresponding author), CSD UOC, Iraklion, Greece.
EM denniskb@ics.forth.gr; oikonom@ics.forth.gr; kyriazis@ics.forth.gr;
   argyros@ics.forth.gr
CR Ahmad N., 2018, BIORXIV, P461160, DOI [10.1101/461160, DOI 10.1101/461160]
   [Anonymous], 2017, ICLR
   Brette R, 2007, J COMPUT NEUROSCI, V23, P349, DOI 10.1007/s10827-007-0038-6
   Brunel N, 2000, J COMPUT NEUROSCI, V8, P183, DOI 10.1023/A:1008925309027
   Chou T. S., 2018, IEEE INT JOINT C NEU, P1158
   Eppler Jochen Martin, 2008, Front Neuroinform, V2, P12, DOI 10.3389/neuro.11.012.2008
   Fernandez A, 2008, 2008 11TH INTERNATIONAL WORKSHOP ON CELLULAR NEURAL NETWORKS AND THEIR APPLICATIONS, P208, DOI 10.1109/CNNA.2008.4588679
   Fidjeland Andreas K, 2010, 2010 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2010.5596678
   Fujita K, 2018, COMPUTING, V100, P907, DOI 10.1007/s00607-018-0590-0
   Furber SB, 2013, IEEE T COMPUT, V62, P2454, DOI 10.1109/TC.2012.142
   Garrido J. A., 2011, EVENT TIME DRIVEN HY
   Goodman D. F. M., 2010, FRONTIERS NEUROSCIEN, V3, P192
   Harlap A., 2018, ARXIV180603377
   Hazan H., 2018, BINDSNET MACHINE LEA, V12, P1, DOI [DOI 10.3389/FNINF.2018.00089, 10.3389]
   Kasap B, 2018, NEUROCOMPUTING, V302, P55, DOI 10.1016/j.neucom.2018.04.007
   Kempter R, 1999, PHYS REV E, V59, P4498, DOI 10.1103/PhysRevE.59.4498
   Kozdon K, 2018, 2018 CONFERENCE ON ARTIFICIAL LIFE (ALIFE 2018), P276
   Lee D, 2018, CONF PROC INT SYMP C, P275, DOI 10.1109/ISCA.2018.00032
   Maass W, 1996, NEURAL COMPUT, V8, P1, DOI 10.1162/neco.1996.8.1.1
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Marsaglia G, 2003, J STAT SOFTW, V8, DOI DOI 10.18637/JSS.V008.I18
   Mattia M, 2000, NEURAL COMPUT, V12, P2305, DOI 10.1162/089976600300014953
   McCulloch W. S., 1943, B MATH BIOPHYS, V5, P115, DOI [10.1007/BF02478259, DOI 10.1007/BF02478259]
   Morrison A, 2007, NEURAL COMPUT, V19, P47, DOI 10.1162/neco.2007.19.1.47
   Mozafari M., 2019, FRONT NEUROSCI, P1
   Nah J.-H., 2011, P ACM SIGGRAPH S HIG, V30, DOI DOI 10.1145/2018323.2018333
   Nvidia, 2019, NVIDIA CUDA C PROGR
   Pecevski D, 2014, FRONT NEUROINFORM, V8, DOI 10.3389/fninf.2014.00070
   Pelayo FJ, 1997, ANALOG INTEGR CIRC S, V13, P111, DOI 10.1023/A:1008240229616
   Reutimann J., 2002, NEURAL COMPUTATION
   Ros E, 2006, NEURAL COMPUT, V18, P2959, DOI 10.1162/neco.2006.18.12.2959
   Ros E, 2006, IEEE T NEURAL NETWOR, V17, P1050, DOI 10.1109/TNN.2006.875980
   Rudolph M, 2006, NEURAL COMPUT, V18, P2146, DOI 10.1162/neco.2006.18.9.2146
   Schemmel J, 2010, IEEE INT SYMP CIRC S, P1947, DOI 10.1109/ISCAS.2010.5536970
   Sripad A, 2018, NEURAL NETWORKS, V97, P28, DOI 10.1016/j.neunet.2017.09.011
   Stimberg M., 2018, BIORXIV
   Tavanaei A., 2018, NEURAL NETWORKS 2018
   Taylor M., 1973, S AFR J PSYCHOL, V3, P23
   Thibeault C. M., 2011, Proceedings of the ISCA 3rd International Conference on Bioinformatics and Computational Biology, P146
   Tomlinson M. S., 1990, SPIKE TRANSIMISSION
   Vogels TP, 2005, J NEUROSCI, V25, P10786, DOI 10.1523/JNEUROSCI.3508-05.2005
   YAVUZ E, 2015, SCI REPORTS, V6, P1
NR 42
TC 4
Z9 4
U1 0
U2 0
PY 2020
DI 10.1109/ijcnn48605.2020.9206752
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture
DA 2023-11-11
ER

PT J
AU Kim, KH
   Kim, SJ
AF Kim, KH
   Kim, SJ
TI Neural spike sorting under nearly 0-dB signal-to-noise ratio using
   nonlinear energy operator and artificial neural-network classifier
SO IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING
DT Article
DE extracellular recording; neural-network classifier; neural spike
   sorting; nonlinear energy operator; signal-to-noise ratio
ID ACTION-POTENTIALS; MULTIUNIT; RECORDINGS; DISCRIMINATION; RESOLUTION
AB We report a result on neural spike sorting under conditions where the signal-to-noise ratio is very low The use of nonlinear energy operator enables the detection of an action potential, even when the SNR is so poor that a typical amplitude thresholding method cannot be applied. The superior detection ability facilitates the collection of a training set under lower SNR than that of the methods which employ simple amplitude thresholding. Thus, the statistical characteristics of the input vectors can be better represented in the neural-network classifier. The trained neural-network classifiers yield the correct classification ratio higher than 90% when the SNR is as low as 1.2 (0.8 dB) when applied to data obtained from extracellular recording from Aplysia abdominal ganglia using a semiconductor microelectrode array.
C1 Seoul Natl Univ, Sch Elect Engn, Seoul 151742, South Korea.
RP Kim, SJ (corresponding author), Seoul Natl Univ, Sch Elect Engn, San 56-1, Seoul 151742, South Korea.
EM khkim@helios.snu.ac.kr; kim@helios.snu.ac.kr
CR [Anonymous], 1995, ELEMENTS SIGNAL DETE
   [Anonymous], 1996, STAT DIGITAL SIGNAL
   BANKMAN IN, 1992, P IEEE EMBS, V14, P2852
   BANKMAN IN, 1995, P 17 ANN C IEEE EMBS, P963
   BEMENT SL, 1986, IEEE T BIO-MED ENG, V33, P230, DOI 10.1109/TBME.1986.325895
   Bezdek J.C., 1981, PATTERN RECOGN
   BIALEK W, 1991, SCIENCE, V252, P1854, DOI 10.1126/science.2063199
   Chandra R, 1997, IEEE T BIO-MED ENG, V44, P403, DOI 10.1109/10.568916
   GOZANI SN, 1994, IEEE T BIO-MED ENG, V41, P358, DOI 10.1109/10.284964
   HAYKIN S., 1999, NEURAL NETWORK COMPR
   Holmstrom L, 1997, IEEE T NEURAL NETWOR, V8, P5, DOI 10.1109/72.554187
   JI J, 1992, IEEE J SOLID-ST CIRC, V27, P433, DOI 10.1109/4.121568
   JONES DL, 1992, IEEE T SIGNAL PROCES, V40, P413, DOI 10.1109/78.124951
   Kim KH, 2000, IEEE T BIO-MED ENG, V47, P1097, DOI 10.1109/10.855938
   Lewicki MS, 1998, NETWORK-COMP NEURAL, V9, pR53, DOI 10.1088/0954-898X/9/4/001
   MARAGOS P, 1993, IEEE T SIGNAL PROCES, V41, P1532, DOI 10.1109/78.212729
   MIRFAKHRAEI K, 1994, IEEE T BIO-MED ENG, V41, P89, DOI 10.1109/10.277276
   Mukhopadhyay S, 1998, IEEE T BIO-MED ENG, V45, P180, DOI 10.1109/10.661266
   Ohberg F, 1996, J NEUROSCI METH, V64, P181, DOI 10.1016/0165-0270(95)00132-8
   PERKEL DH, 1967, BIOPHYS J, V7, P419, DOI 10.1016/S0006-3495(67)86597-4
   Ruck D W, 1990, IEEE Trans Neural Netw, V1, P296, DOI 10.1109/72.80266
   TSIVIDIS YP, 1989, OPERATION MODELING M
   WHEELER BC, 1982, IEEE T BIO-MED ENG, V29, P752, DOI 10.1109/TBME.1982.324870
   YANG XW, 1988, IEEE T BIO-MED ENG, V35, P806, DOI 10.1109/10.7287
NR 24
TC 172
Z9 185
U1 0
U2 11
PD OCT
PY 2000
VL 47
IS 10
BP 1406
EP 1411
WC Engineering, Biomedical
DA 2023-11-11
ER

PT J
AU Lee, WW
   Kukreja, SL
   Thakor, NV
AF Lee, Wang Wei
   Kukreja, Sunil L.
   Thakor, Nitish V.
TI CONE: Convex-Optimized-Synaptic Efficacies for Temporally Precise Spike
   Mapping
SO IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
DT Article
DE Spatiotemporal coding; spike pattern mapping; spiking neural network
   (SNN); supervised learning
ID NEURAL-NETWORKS; TACTILE AFFERENTS; ALGORITHMS; PERCEPTRON; 1ST
AB Spiking neural networks are well suited to perform time-dependent pattern recognition problems by encoding the temporal dimension in precise spike times. With an appropriate set of weights, a spiking neuron can emit precisely timed action potentials in response to spatiotemporal input spikes. However, deriving supervised learning rules for spike mapping is nontrivial due to the increased complexity. Existing methods rely on heuristic approaches that do not guarantee a convex objective function and, therefore, may not converge to a global minimum. In this paper, we present a novel technique to obtain the weights of spiking neurons by formulating the problem in a convex optimization framework, rendering it be compatible with the established methods. We introduce techniques to influence the weight distribution and membrane trajectory, and then study how these factors affect robustness in the presence of noise. In addition, we show how the existence of a solution can be determined and assess memory capacity limits of a neuron model using synthetic examples. The practical utility of our technique is further assessed by its application to gait-event detection using the experimental data.
C1 [Lee, Wang Wei] Natl Univ Singapore, Grad Sch Integrat Sci & Engn, Singapore 119077, Singapore.
   [Kukreja, Sunil L.] Natl Univ Singapore, Singapore Inst Neurotechnol, Singapore 117456, Singapore.
   [Thakor, Nitish V.] Natl Univ Singapore, Singapore Inst Neurotechnol, Dept Elect & Bioengn, Singapore 119077, Singapore.
   [Thakor, Nitish V.] Johns Hopkins Univ, Dept Biomed Engn, Baltimore, MD 21218 USA.
RP Kukreja, SL (corresponding author), Natl Univ Singapore, Singapore Inst Neurotechnol, Singapore 117456, Singapore.
EM lee_wang_wei@u.nus.edu; sunil.kukreja@mail.mcgill.ca;
   sinapsedirector@gmail.com
CR Afshar Saeed, 2014, Front Neurosci, V8, P377, DOI 10.3389/fnins.2014.00377
   Airan RD, 2009, NATURE, V458, P1025, DOI 10.1038/nature07926
   [Anonymous], 2014, GUR OPT REF MAN
   [Anonymous], 2009, CONVEX OPTIMIZATION
   Barbour B, 2007, TRENDS NEUROSCI, V30, P622, DOI 10.1016/j.tins.2007.09.005
   Bologna LL, 2011, J PHYSIOL-PARIS, V105, P25, DOI 10.1016/j.jphysparis.2011.08.002
   Boyd S., 2004, CONVEX OPTIMIZATION, DOI 10.1017/CBO9780511804441
   Brunel N, 2004, NEURON, V43, P745, DOI 10.1016/j.neuron.2004.08.023
   Burnfield J.M., 2010, J PEDIATR ORTHOPED, V9, P353, DOI DOI 10.1097/01241398-199211000-00023
   Dantzig G.B., 1955, PAC J MATH, V5, P183, DOI DOI 10.2140/PJM.1955.5.183
   De Rossi SMM, 2012, P IEEE RAS-EMBS INT, P361, DOI 10.1109/BioRob.2012.6290278
   Dethier J, 2013, J NEURAL ENG, V10, DOI 10.1088/1741-2560/10/3/036008
   Florian RV, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0040233
   Gerstner W., 2002, SPIKING NEURON MODEL
   Ghoussayni S. N., 2004, P 9 ANN C IFESS SOC, P398
   Grant M., 2018, CVX MATLAB SOFTWARE
   Gütig R, 2014, CURR OPIN NEUROBIOL, V25, P134, DOI 10.1016/j.conb.2014.01.004
   Gütig R, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0053063
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Hreljac A, 2000, J BIOMECH, V33, P783, DOI 10.1016/S0021-9290(00)00014-2
   Hussain S, 2014, NEUROCOMPUTING, V138, P14, DOI 10.1016/j.neucom.2013.09.052
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Johansson RS, 2009, NAT REV NEUROSCI, V10, P345, DOI 10.1038/nrn2621
   Johansson RS, 2004, NAT NEUROSCI, V7, P170, DOI 10.1038/nn1177
   KARMARKAR N, 1984, COMBINATORICA, V4, P373, DOI 10.1007/BF02579150
   La Camera G, 2004, NEURAL COMPUT, V16, P2101, DOI 10.1162/0899766041732468
   Lee WW, 2014, P IEEE RAS-EMBS INT, P899, DOI 10.1109/BIOROB.2014.6913895
   Legenstein R, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.1000180
   Liu SC, 2010, CURR OPIN NEUROBIOL, V20, P288, DOI 10.1016/j.conb.2010.03.007
   Lofberg J., 2004, IEEE INT C ROB AUT I, DOI DOI 10.1109/CACSD.2004.1393890
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Maass W, 2004, J COMPUT SYST SCI, V69, P593, DOI 10.1016/j jcss.2004.04.001
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maass W, 2007, PLOS COMPUT BIOL, V3, P15, DOI 10.1371/journal.pcbi.0020165
   Meng XL, 2013, IEEE ENG MED BIO, P4907, DOI 10.1109/EMBC.2013.6610648
   Mohemmed A, 2012, INT J NEURAL SYST, V22, DOI 10.1142/S0129065712500128
   MOSEK ApS, 2010, MOSEK OPT SOFTW
   Nessler B., 2009, ADV NEURAL INFORM PR, P1357
   Oja E., 1989, International Journal of Neural Systems, V1, P61, DOI 10.1142/S0129065789000475
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Pruszynski JA, 2014, NAT NEUROSCI, V17, P1404, DOI 10.1038/nn.3804
   Rice KL, 2009, 2009 INTERNATIONAL CONFERENCE ON RECONFIGURABLE COMPUTING AND FPGAS, P451, DOI 10.1109/ReConFig.2009.77
   Schrauwen B, 2008, NEURAL NETWORKS, V21, P511, DOI 10.1016/j.neunet.2007.12.009
   Shadlen Michael N., 1994, Current Opinion in Neurobiology, V4, P569, DOI 10.1016/0959-4388(94)90059-0
   Sturm JF, 1999, OPTIM METHOD SOFTW, V11-2, P625, DOI 10.1080/10556789908805766
   Tapson JC, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00153
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   Toh KC, 1999, OPTIM METHOD SOFTW, V11-2, P545, DOI 10.1080/10556789908805762
   VanRullen R, 2005, TRENDS NEUROSCI, V28, P1, DOI 10.1016/j.tins.2004.10.010
   Victor JD, 1996, J NEUROPHYSIOL, V76, P1310, DOI 10.1152/jn.1996.76.2.1310
   Victor JD, 1997, NETWORK-COMP NEURAL, V8, P127, DOI 10.1088/0954-898X/8/2/003
   Wang RCM, 2014, FRONT NEUROSCI-SWITZ, V8, DOI 10.3389/fnins.2014.00051
   WIDROW B, 1990, P IEEE, V78, P1415, DOI 10.1109/5.58323
   Yu QF, 2013, PLOS ONE, V8, DOI [10.1371/journal.pone.0059627, 10.1371/journal.pone.0078318]
NR 54
TC 9
Z9 10
U1 1
U2 8
PD APR
PY 2017
VL 28
IS 4
BP 849
EP 861
DI 10.1109/TNNLS.2015.2509479
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Computer Science, Theory & Methods; Engineering,
   Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Ayuso-Martinez, A
   Casanueva-Morato, D
   Dominguez-Morales, JP
   Jimenez-Fernandez, A
   Jimenez-Moreno, G
AF Ayuso-Martinez, Alvaro
   Casanueva-Morato, Daniel
   Dominguez-Morales, Juan P.
   Jimenez-Fernandez, Angel
   Jimenez-Moreno, Gabriel
GP IEEE
TI Spike-based building blocks for performing logic operations using
   Spiking Neural Networks on SpiNNaker
SO 2022 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT IEEE International Conference on Fuzzy Systems (FUZZ-IEEE) / IEEE World
   Congress on Computational Intelligence (IEEE WCCI) / International Joint
   Conference on Neural Networks (IJCNN) / IEEE Congress on Evolutionary
   Computation (IEEE CEC)
CY JUL 18-23, 2022
CL Padua, ITALY
DE Bio-inspired building blocks; Spiking logic gates; Spiking Neural
   Networks; Neuromorphic engineering; SpiNNaker
AB One of the most interesting and still growing scientific fields is neuromorphic engineering, which is focused on studying and designing hardware and software with the purpose of mimicking the basic principles of biological nervous systems. Currently, there are many research groups developing practical applications based on neuroscientific knowledge. This work provides researchers with a novel toolkit of building blocks based on Spiking Neural Networks that emulate the behavior of different logic gates. These could be very useful in many spike-based applications, since logic gates are the basis of digital circuits. The designs and models proposed are presented and implemented on a SpiNNaker hardware platform. Different experiments were performed in order to validate the expected behavior, and the obtained results are discussed. The functionality of traditional logic gates and the proposed blocks is studied, and the feasibility of the presented approach is discussed.
C1 [Ayuso-Martinez, Alvaro; Casanueva-Morato, Daniel; Dominguez-Morales, Juan P.; Jimenez-Fernandez, Angel; Jimenez-Moreno, Gabriel] Univ Seville, Robot & Technol Comp Lab, Seville, Spain.
RP Ayuso-Martinez, A (corresponding author), Univ Seville, Robot & Technol Comp Lab, Seville, Spain.
EM aayuso@us.es
CR Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   Batres-Mendoza P, 2021, IEEE ACCESS, V9, P35766, DOI 10.1109/ACCESS.2021.3062329
   Davidson S, 2021, FRONT NEUROSCI-SWITZ, V15, DOI 10.3389/fnins.2021.651141
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Davison Andrew P, 2008, Front Neuroinform, V2, P11, DOI 10.3389/neuro.11.011.2008
   DOMINGUEZNAVARRO J, 2018, 2018 IJCNN
   Douglas RJ, 2007, CURR BIOL, V17, pR496, DOI 10.1016/j.cub.2007.04.024
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   Gewaltig M-O., 2007, SCHOLARPEDIA, V2, DOI [10.4249/scholarpedia.1430, DOI 10.4249/SCHOLARPEDIA.1430]
   Glatz S, 2019, IEEE INT CONF ROBOT, P9631, DOI [10.1109/icra.2019.8794145, 10.1109/ICRA.2019.8794145]
   Goodman Dan, 2008, Front Neuroinform, V2, P5, DOI 10.3389/neuro.11.005.2008
   Gutierrez-Galan D, 2020, NEUROCOMPUTING, V381, P10, DOI 10.1016/j.neucom.2019.11.007
   Hines ML, 1997, NEURAL COMPUT, V9, P1179, DOI 10.1162/neco.1997.9.6.1179
   Markram H, 2013, FUNCT NEUROL, V28, P145, DOI 10.11138/FNeur/2013.28.3.145
   MEAD C, 1990, P IEEE, V78, P1629, DOI 10.1109/5.58356
   Rowley AGD, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00231
   Roy K, 2019, NATURE, V575, P607, DOI 10.1038/s41586-019-1677-2
   Schoepe T, 2020, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS45731.2020.9181257
   Soman S., 2016, BIG DATA ANAL, V1
   Stagsted R., 2020, RSS
   Wu JB, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00199
   YIN BJ, 2021, P 2021 DRON SYST PER, P9
   Zhu JD, 2020, APPL PHYS REV, V7, DOI 10.1063/1.5118217
NR 23
TC 0
Z9 0
U1 0
U2 1
PY 2022
DI 10.1109/IJCNN55064.2022.9892479
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Engineering, Electrical & Electronic; Neurosciences
DA 2023-11-11
ER

PT C
AU Lv, MY
   Shao, CP
   Li, HY
   Li, J
   Sun, TF
AF Lv, Mengyu
   Shao, Cuiping
   Li, Huiyun
   Li, Jun
   Sun, Tianfu
GP IEEE
TI A novel spiking neural network with the learning strategy of biomimetic
   structure
SO 2021 ASIA-PACIFIC CONFERENCE ON COMMUNICATIONS TECHNOLOGY AND COMPUTER
   SCIENCE (ACCTCS 2021)
DT Proceedings Paper
CT Asia-Pacific Conference on Communications Technology and Computer
   Science (ACCTCS)
CY JAN 22-24, 2021
CL Shenyang, PEOPLES R CHINA
DE spiking neural network; biomimetic structure teaming; STDP teaming rule;
   WTA competition mechanism
ID MODEL
AB Spiking neural network (SNN) is a new artificial neural network computing model inspired by the brain, which is an appropriate tool for processing complex spatiotemporal information with low power consumption. However, due to the discontinuous, non-differentiable mechanism and complicated calculation model of SNN, there are some challenges such as the difficult training process and low learning efficiency of SNN. Inspired by the self-organization process in biological nervous system and based on the spike time difference between presynaptic and postsynaptic neurons, a kind of biomimetic structure learning algorithm is proposed in this paper. We use the leaky integrate-and-fire (LIF) neurons, the conductance-based synapses and based on the spike-timing dependent plasticity (STDP) learning rules construct a four-layer feedforward network in this paper. Different from the fully connected network, based on the precise spike time difference between presynaptic and postsynaptic neurons, we propose a method to establish new connections between neurons as the network is trained. And the winner-take-all (WTA) competition mechanism is introduced in the output layer to ensure the specificity of network learning features. Experiments results show that the classification ability of our network could achieve 92.1% with on the MNIST dataset. And compared with the fully connected SNN, the training time is significantly shortened by 2.6 hours, accounting for 82% of the fully connected network. Finally, combined with relevant experimental data, we also prove that the power consumption of SNN is much smaller than the ANN.
C1 [Lv, Mengyu; Li, Jun] Guilin Univ Elect Technol, Sch Elect Engn & Automat, Guilin 541004, Peoples R China.
   [Lv, Mengyu; Shao, Cuiping; Li, Huiyun; Sun, Tianfu] Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen 518055, Peoples R China.
RP Shao, CP (corresponding author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen 518055, Peoples R China.
EM cp.shao@siat.ac.cn
CR Brette R, 2005, J NEUROPHYSIOL, V94, P3637, DOI 10.1152/jn.00686.2005
   Caporale N, 2008, ANNU REV NEUROSCI, V31, P25, DOI 10.1146/annurev.neuro.31.060407.125639
   Deng L, 2020, NEURAL NETWORKS, V121, P294, DOI 10.1016/j.neunet.2019.09.005
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Dou Y, 2020, INFORM TECHNOLOGY NE, V38, P96
   HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554
   Illing B, 2019, NEURAL NETWORKS, V118, P90, DOI 10.1016/j.neunet.2019.06.001
   Kim SY, 2019, COGN NEURODYNAMICS, V13, P53, DOI 10.1007/s11571-018-9505-1
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
   Lee C, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00119
   Li XM, 2018, PHYSICA A, V491, P716, DOI 10.1016/j.physa.2017.08.053
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Poo MM, 2016, NEURON, V92, P591, DOI 10.1016/j.neuron.2016.10.050
   Raghava G, 2020, COMPUTER SCI
   ROSENBLATT F, 1958, PSYCHOL REV, V65, P386, DOI 10.1037/h0042519
   Roy K, 2019, NATURE, V575, P607, DOI 10.1038/s41586-019-1677-2
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Shang Y, 2020, COMPUTER ENG DESIGN, V41, P1390
   Shrestha A, 2017, IEEE IJCNN, P1999, DOI 10.1109/IJCNN.2017.7966096
   Taherkhani A, 2018, IEEE T NEUR NET LEAR, V29, P5394, DOI 10.1109/TNNLS.2018.2797801
   Wei J, 2019, CHINA INTERATED CIRC, V7, P18
   Yang Gang, 2013, Control and Decision, V28, P1702
   Zeng Y, 2017, SCI CHINA INFORM SCI, V60, DOI 10.1007/s11432-016-0439-4
NR 23
TC 0
Z9 0
U1 0
U2 5
PY 2021
BP 69
EP 74
DI 10.1109/ACCTCS52002.2021.00022
WC Computer Science, Artificial Intelligence; Telecommunications
DA 2023-11-11
ER

PT J
AU WEGMAN, EJ
   HABIB, MK
AF WEGMAN, EJ
   HABIB, MK
TI STOCHASTIC METHODS FOR NEURAL SYSTEMS
SO JOURNAL OF STATISTICAL PLANNING AND INFERENCE
DT Article
DE ARTIFICIAL NEURON SYSTEMS; NEURAL SPIKE TRAINS; SEMI-MARTINGALES;
   CENTRAL NERVOUS SYSTEM
ID PATTERN-RECOGNITION; SPIKE TRAINS; NETWORK; NEURONS; MODELS; CORTEX;
   NEOCOGNITRON; ALGORITHM
AB This paper is a survey of recent developments in the application of stochastic modeling and statistical analysis of biological and artificial neuron systems. We focus first on a general description of information processing in the central nervous system including the basic physiology of synaptic communications. We discuss some of the current models of and inference for membrane potential including temporal models. We discuss some of the stochastic modeling and analysis of neural spike trains, From a focus on communications between individual pairs of neurons, we turn our attention to broader scale neural network considerations. closing our discussion with models of networks with hidden layers.
C1 GEORGE MASON UNIV, CTR COMPUTAT STAT, FAIRFAX, VA 22030 USA.
CR AERTSEN AMHJ, 1989, J NEUROPHYSIOL, V61, P900, DOI 10.1152/jn.1989.61.5.900
   ALKON DL, 1990, BIOL CYBERN, V62, P363, DOI 10.1007/BF00197642
   ANDERSON JA, 1986, DISORDERED SYSTEMS B
   [Anonymous], 1976, NEURON BRAIN
   [Anonymous], 1984, SELF ORG ASS MEMORY
   BACH M, 1986, EXP BRAIN RES, V61, P451
   BORISYUK GN, 1985, BIOL CYBERN, V52, P301, DOI 10.1007/BF00355752
   BRILLINGER DR, 1988, BIOL CYBERN, V59, P189, DOI 10.1007/BF00318010
   CERVANTES JH, 1987, 1ST P IEEE INT C NEU, P657
   CHORNOBOY ES, 1988, BIOL CYBERN, V59, P265, DOI 10.1007/BF00332915
   COPE DK, 1979, J THEOR BIOL, V80, P1, DOI 10.1016/0022-5193(79)90174-7
   DEKWAADSTENIET JW, 1982, MATH BIOSCI, V60, P17, DOI 10.1016/0025-5564(82)90031-1
   FERNALD RD, 1971, BIOPHYS J, V11, P323, DOI 10.1016/S0006-3495(71)86218-5
   FIENBERG SE, 1974, BIOMETRICS, V30, P399, DOI 10.2307/2529198
   FUKUSHIMA K, 1978, BIOL CYBERN, V28, P201, DOI 10.1007/BF00344267
   FUKUSHIMA K, 1988, NEURAL NETWORKS, V1, P119, DOI 10.1016/0893-6080(88)90014-7
   FUKUSHIMA K, 1975, BIOL CYBERN, V20, P121, DOI 10.1007/BF00342633
   FUKUSHIMA K, 1982, PATTERN RECOGN, V15, P455, DOI 10.1016/0031-3203(82)90024-3
   GEMAN S, 1982, ANN STAT, V10, P401, DOI 10.1214/aos/1176345782
   GERSTEIN GL, 1972, BIOPHYS J, V12, P453, DOI 10.1016/S0006-3495(72)86097-1
   Grenander U., 1981, ABSTRACT INFERENCE
   GROSSBERG S, 1987, COGNITIVE SCI, V11, P23, DOI 10.1111/j.1551-6708.1987.tb00862.x
   Grossberg S., 1982, STUDIES MIND BRAIN N
   HABIB MK, 1990, APPL MATH COMPUT, V38, P51, DOI 10.1016/0096-3003(90)90080-M
   HABIB MK, 1992, J STAT PLAN INFER, V33, P143, DOI 10.1016/0378-3758(92)90099-E
   HABIB MK, 1985, MIMEO SERIES U N CAR, V1492
   HABIB MK, 1985, BIOSTATISTICS STATIS, P481
   HEBB DO, 1949, ORG BEHAVIOR
   HEYDE CC, 1992, J STAT PLAN INFER, V33, P121, DOI 10.1016/0378-3758(92)90097-C
   HOPFIELD JJ, 1986, SCIENCE, V233, P625, DOI 10.1126/science.3755256
   HOPFIELD JJ, 1984, P NATL ACAD SCI-BIOL, V81, P3088, DOI 10.1073/pnas.81.10.3088
   HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554
   Johannesma P.I.M., 1968, NEURAL NETWORKS, P116
   JOHNSON DH, 1983, J ACOUST SOC AM, V74, P493, DOI 10.1121/1.389815
   JULIANO SL, 1981, J NEUROPHYSIOL, V46, P1260, DOI 10.1152/jn.1981.46.6.1260
   KALLIANPUR G, 1984, APPL MATH OPT, V12, P125, DOI 10.1007/BF01449039
   KALLIANPUR G, 1983, CONTRIBUTIONS STATIS
   KOHONEN T, 1989, PHYS SCRIPTA, V39, P168, DOI 10.1088/0031-8949/39/1/027
   LECAM L, 1986, STATISTICAL METHODS
   LEVY BC, 1987, P IEEE INT C NEURAL
   MALSBURG CV, 1973, KYBERNETIK, V14, P85, DOI 10.1007/BF00288907
   McCulloch W. S., 1943, B MATH BIOPHYS, V5, P115, DOI [10.1007/BF02478259, DOI 10.1007/BF02478259]
   MCKEAGUE IW, 1986, ANN STAT, V14, P579, DOI 10.1214/aos/1176349939
   MICHALSKI A, 1983, EXP BRAIN RES, V51, P97
   Minsky Marvin, 1969, PERCEPTRONS
   NEHER E, 1977, ANNU REV BIOPHYS BIO, V6, P345, DOI 10.1146/annurev.bb.06.060177.002021
   NGUYEN HT, 1982, SIAM J CONTROL OPTIM, V20, P603, DOI 10.1137/0320045
   PALM G, 1988, BIOL CYBERN, V59, P1, DOI 10.1007/BF00336885
   PERKEL DH, 1967, BIOPHYS J, V7, P419, DOI 10.1016/S0006-3495(67)86597-4
   RALL W, 1977, HDB PHYSL I, V1
   Rall W, 1964, NEURAL THEORY MODELI
   RICCIARDI LM, 1979, BIOL CYBERN, V35, P1, DOI 10.1007/BF01845839
   ROSENBLATT F, 1959, MECHANIZATION THOUGH, V1, P1
   Rumelhart DE., 1988, PARALLEL DISTRIBUTED
   SEJNOWSKI TJ, 1986, PHYSICA D, V22, P260, DOI 10.1016/0167-2789(86)90245-9
   TANAKA K, 1983, J NEUROPHYSIOL, V49, P1303, DOI 10.1152/jn.1983.49.6.1303
   THAVANESWARAN A, 1986, J APPL PROBAB, V23, P409, DOI 10.2307/3214183
   THAVANESWARAN A, 1988, J APPL MATH COMPUT, V19, P1901
   TOYAMA K, 1981, J NEUROPHYSIOL, V46, P191, DOI 10.1152/jn.1981.46.2.191
   Tuckwell HC., 1989, STOCHASTIC PROCESSES
   WACHOLDER E, 1989, BIOL CYBERN, V61, P11, DOI 10.1007/BF00204755
   WEGMAN EJ, 1975, SANKHYA A, V37, P211
   WHITE H, 1989, J AM STAT ASSOC, V84, P1003, DOI 10.2307/2290076
   WHITSEL BL, 1972, J NEUROPHYSIOL, V35, P691, DOI 10.1152/jn.1972.35.5.691
   YANG GL, 1978, MATH BIOSCI, V38, P1, DOI 10.1016/0025-5564(78)90015-9
   YANG GL, 1992, J STAT PLAN INFER, V33, P107, DOI 10.1016/0378-3758(92)90096-B
NR 66
TC 3
Z9 3
U1 0
U2 4
PD OCT
PY 1992
VL 33
IS 1
BP 5
EP 25
DI 10.1016/0378-3758(92)90092-7
WC Statistics & Probability
DA 2023-11-11
ER

PT C
AU Yamaguchi, A
   Yamaguti, Y
   Kubo, M
AF Yamaguchi, Akihiro
   Yamaguti, Yutaka
   Kubo, Masao
BE DelgadoGarcia, JM
   Pan, X
   SanchezCampusano, R
   Wang, R
TI Decomposition of Superimposed Chaotic Spike Sequences by Using the
   Bifurcating Neuron
SO ADVANCES IN COGNITIVE NEURODYNAMICS (VI)
SE Advances in Cognitive Neurodynamics
DT Proceedings Paper
CT 6th International Conference on Cognitive Neurodynamics (ICCN)
CY AUG 01-05, 2017
CL Carmona, SPAIN
DE Chaotic synchronization; Bifurcating neuron; Neural coding
AB In this study, decomposition of superimposed chaotic spike sequence was investigated from the view point of neural information coding. We construct simple network of bifurcating neuron and introduce the coupling model to decompose superimposed chaotic spike sequences. The decomposing performance was demonstrated by the numerical simulation and evaluated by the ratio of synchronized spikes. As a result, for the superimposed two chaotic spike sequences, approximately 90% of spikes were correctly decomposed.
C1 [Yamaguchi, Akihiro; Yamaguti, Yutaka] Fukuoka Inst Technol, Fac Informat Engn, Fukuoka, Japan.
   [Kubo, Masao] Natl Def Acad Japan, Dept Comp Sci, Yokosuka, Kanagawa, Japan.
RP Yamaguchi, A (corresponding author), Fukuoka Inst Technol, Fac Informat Engn, Fukuoka, Japan.
EM aki@fit.ac.jp
CR Fujiwara M., 2016, J ROBOTICS NETWORKIN, V2, P26
   Gerstner W., 2002, SPIKING NEURON MODEL
   Lee G, 2001, NEURAL NETWORKS, V14, P115, DOI 10.1016/S0893-6080(00)00083-6
   Yamaguchi A, 2016, J ROBOT NETW ARTIF L, V2, P230, DOI 10.2991/jrnal.2016.2.4.6
NR 4
TC 2
Z9 2
U1 0
U2 0
PY 2018
BP 3
EP 9
DI 10.1007/978-981-10-8854-4_1
WC Neurosciences; Neuroimaging
DA 2023-11-11
ER

PT C
AU Sadovsky, E
   Jakubec, M
   Jarinova, D
   Jarina, R
AF Sadovsky, Erik
   Jakubec, Maros
   Jarinova, Darina
   Jarina, Roman
BE Pidanic, J
TI Dataset Conversion for Spiking Neural Networks
SO 2023 33RD INTERNATIONAL CONFERENCE RADIOELEKTRONIKA, RADIOELEKTRONIKA
DT Proceedings Paper
CT 33rd International Conference on Radioelektronika (RADIOELEKTRONIKA)
CY APR 19-20, 2023
CL Pardubice, CZECH REPUBLIC
DE biological neurons; SNNs; poisson process; data conversion; GTZAN
AB Spiking Neural Networks (SNN) are still a relatively new research area, and there are only a few publicly available datasets suitable for SNN training and testing. Datasets designed for traditional neural networks are not suitable as an input to SNNs because they rely on the timing of spikes, thus, requiring data to be preprocessed in a specific way. This work presents a pipeline for converting existing conventional datasets into rate-encoded spikes matching the requirements for SNN processing. The implementation is based on the Python snntorch library. The functionality of the proposed encoder pipeline is evaluated on a music genre classification task using the GTZAN dataset. However, the proposed procedure has general applicability and may be used for data of any form.
C1 [Sadovsky, Erik; Jakubec, Maros; Jarinova, Darina; Jarina, Roman] Univ Zilina, FEIT, Dept Multimedia & Informat Commun Technol, Zilina, Slovakia.
RP Sadovsky, E (corresponding author), Univ Zilina, FEIT, Dept Multimedia & Informat Commun Technol, Zilina, Slovakia.
EM erik.sadovsky@uniza.sk
CR [Anonymous], 2016, TRENDS PHARMACOL SCI, DOI [DOI 10.1016/j.tips.2016.05.001, DOI 10.3389/FNINS.2017.00073]
   [Anonymous], GOOGLE DOCS
   [Anonymous], GTZAN TENSORFLOW DAT
   [Anonymous], LAUSCHER FLEX AUD SP
   Auge D, 2021, NEURAL PROCESS LETT, V53, P4693, DOI 10.1007/s11063-021-10562-2
   Bouvier M, 2019, ACM J EMERG TECH COM, V15, DOI 10.1145/3304103
   Cramer B, 2022, IEEE T NEUR NET LEAR, V33, P2744, DOI 10.1109/TNNLS.2020.3044364
   Eshraghian JK, 2023, Arxiv, DOI [arXiv:2109.12894, DOI 10.48550/ARXIV.2109.12894]
   garrickorchard, G ORCHARD N MNIST
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   github, LIBR
   github, SLAYERPYTORCH
   Heeger P. D, POISSON MODEL SPIKE
   ibm, DVS128 GEST DAT IBM
   Liu SC, 2014, IEEE T BIOMED CIRC S, V8, P453, DOI 10.1109/TBCAS.2013.2281834
   Lobo JL, 2020, NEURAL NETWORKS, V121, P88, DOI 10.1016/j.neunet.2019.09.004
   Lütcke H, 2013, FRONT NEURAL CIRCUIT, V7, DOI 10.3389/fncir.2013.00201
   Makarov VA, 2005, J NEUROSCI METH, V144, P265, DOI 10.1016/j.jneumeth.2004.11.013
   Mansouri-Benssassi E, 2021, SOFT COMPUT, V25, P1717, DOI 10.1007/s00500-020-05501-7
   Orchard G, 2015, FRONT NEUROSCI-SWITZ, V9, DOI [10.3389/fnins.2015.00437, 10.3389/fhins.2015.00437]
   Palanisamy K, 2020, Arxiv, DOI [arXiv:2007.11154, DOI 10.48550/ARXIV.2007.11154]
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Prieto A, 2016, NEUROCOMPUTING, V214, P242, DOI 10.1016/j.neucom.2016.06.014
   Riad M. O. F., 2022, EURASIA PROC SCI TEC, V19, P87, DOI [10.55549/epstem.1219174, DOI 10.55549/EPSTEM.1219174]
   scikit-learn, SCIKIT LEARN MACHINE
   Senac C, 2017, PROCEEDINGS OF THE 15TH INTERNATIONAL WORKSHOP ON CONTENT-BASED MULTIMEDIA INDEXING (CBMI), DOI 10.1145/3095713.3095733
   Srivastava KH, 2017, P NATL ACAD SCI USA, V114, P1171, DOI 10.1073/pnas.1611734114
   Sturm B. L., 2012, P 2 INT ACM WORKSH M, P7, DOI [10.1145/2390848.2390851, DOI 10.1145/2390848.2390851]
   Taherkhani A, 2020, NEURAL NETWORKS, V122, P253, DOI 10.1016/j.neunet.2019.09.036
   Tzanetakis G, 2002, IEEE T SPEECH AUDI P, V10, P293, DOI 10.1109/TSA.2002.800560
   Wang XW, 2020, NEURAL NETWORKS, V125, P258, DOI 10.1016/j.neunet.2020.02.011
NR 31
TC 0
Z9 0
U1 1
U2 1
PY 2023
AR 251_388
DI 10.1109/RADIOELEKTRONIKA57919.2023.10109048
WC Engineering, Electrical & Electronic; Imaging Science & Photographic
   Technology
DA 2023-11-11
ER

PT J
AU Ikegawa, S
   Saiin, R
   Sawada, Y
   Natori, N
AF Ikegawa, Shin-ichi
   Saiin, Ryuji
   Sawada, Yoshihide
   Natori, Naotake
TI Rethinking the Role of Normalization and Residual Blocks for Spiking
   Neural Networks
SO SENSORS
DT Article
DE spiking neural networks; normalization; pre-activation residual blocks
ID NEURONS
AB Biologically inspired spiking neural networks (SNNs) are widely used to realize ultralow-power energy consumption. However, deep SNNs are not easy to train due to the excessive firing of spiking neurons in the hidden layers. To tackle this problem, we propose a novel but simple normalization technique called postsynaptic potential normalization. This normalization removes the subtraction term from the standard normalization and uses the second raw moment instead of the variance as the division term. The spike firing can be controlled, enabling the training to proceed appropriately, by conducting this simple normalization to the postsynaptic potential. The experimental results show that SNNs with our normalization outperformed other models using other normalizations. Furthermore, through the pre-activation residual blocks, the proposed model can train with more than 100 layers without other special techniques dedicated to SNNs.
C1 [Ikegawa, Shin-ichi; Sawada, Yoshihide; Natori, Naotake] Aisin Corp, Tokyo Res Ctr, Chiyoda Ku, Akihabara Daibiru 7F 1-18-13, Tokyo 1010021, Japan.
   [Saiin, Ryuji] AISIN Software Co Ltd, Adv Sq Kariya 7F 1-1-1,Aioicho, Kariya, Aichi 4480027, Japan.
RP Ikegawa, S (corresponding author), Aisin Corp, Tokyo Res Ctr, Chiyoda Ku, Akihabara Daibiru 7F 1-18-13, Tokyo 1010021, Japan.
EM shinichi.ikegawa@aisin.co.jp; ryuji.saiin@aisin-software.co.jp;
   yoshihide.sawada@aisin.co.jp; naotake.natori@aisin.co.jp
CR Aertsen A., 1996, BRAIN THEORY BIOLOGI
   Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   Ba J. L., 2016, ARXIV160706450
   Comsa JM, 2020, INT CONF ACOUST SPEE, P8529, DOI [10.1109/ICASSP40776.2020.9053856, 10.1109/icassp40776.2020.9053856]
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Diehl PU, 2015, IEEE IJCNN
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Fang W., 2021, P IEEE CVF INT C COM, P2661, DOI 10.1109/ICCV48922.2021.00266
   Fang W., 2021, ARXIV2021210204159
   FRANKENH.B, 1965, ACTA PHYSIOL SCAND, V63, P1, DOI 10.1111/j.1748-1716.1965.tb04037.x
   Gerstner W., 2002, SPIKING NEURON MODEL
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Kim Y, 2021, FRONT NEUROSCI-SWITZ, V15, DOI 10.3389/fnins.2021.773954
   Lapique L., 1907, J PHYSL PATHOL GEN, V9, P620, DOI DOI 10.1007/S00422-007-0189-6
   Ledinauskas E., 2020, ARXIV2020200604436
   Lee C, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00119
   Li Y., 2021, P INT C MACH LEARN I, V139, P6316
   Li Y., 2021, ADV NEURAL INF PROCE, V34, P12022
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maguire LP, 2007, NEUROCOMPUTING, V71, P13, DOI 10.1016/j.neucom.2006.11.029
   Orchard G, 2015, FRONT NEUROSCI-SWITZ, V9, DOI [10.3389/fnins.2015.00437, 10.3389/fhins.2015.00437]
   RALL W, 1967, J NEUROPHYSIOL, V30, P1138, DOI 10.1152/jn.1967.30.5.1138
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Santurkar S, 2018, ADV NEUR IN, V31
   SCHLUE WR, 1974, J NEUROPHYSIOL, V37, P303, DOI 10.1152/jn.1974.37.2.303
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Shen Y, 2021, NEURAL COMPUT, V33, P3179, DOI 10.1162/neco_a_01439
   Shrestha SB, 2018, ADV NEUR IN, V31
   STAFSTROM CE, 1984, J NEUROPHYSIOL, V52, P244, DOI 10.1152/jn.1984.52.2.244
   STEIN RB, 1965, BIOPHYS J, V5, P173, DOI 10.1016/S0006-3495(65)86709-1
   Ulyanov Dmitry, 2016, ABS160708022 ARXIV
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Wu YX, 2018, LECT NOTES COMPUT SC, V11217, P3, DOI 10.1007/978-3-030-01261-8_1
   Xiao H, 2017, ARXIV PREPRINT ARXIV
   Zenke F, 2018, NEURAL COMPUT, V30, P1514, DOI 10.1162/neco_a_01086
   Zhang W, 2020, ADV NEURAL INFORM PR, V33, P12022, DOI DOI 10.48550/ARXIV.2002.10085
   Zheng HL, 2021, AAAI CONF ARTIF INTE, V35, P11062
NR 42
TC 3
Z9 3
U1 1
U2 12
PD APR
PY 2022
VL 22
IS 8
AR 2876
DI 10.3390/s22082876
WC Chemistry, Analytical; Engineering, Electrical & Electronic; Instruments
   & Instrumentation
DA 2023-11-11
ER

PT C
AU Atsumi, M
AF Atsumi, M
GP ieee
TI Scene learning and glance recognizability based on competitively growing
   spiking neural network
SO 2004 IEEE INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS, VOLS 1-4,
   PROCEEDINGS
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT IEEE International Joint Conference on Neural Networks (IJCNN)
CY JUL 25-29, 2004
CL Budapest, HUNGARY
ID VISUAL-ATTENTION; MODEL
AB We have been building the competitively growing spiking neural network for quick one-shot object learning and glance object recognition, which is the core of our saliency-based scene memory model. This neural network represents objects using latency-based temporal coding and grows size and recognizability through learning and self-organization. Through simulation experiments of a robot equipped with a camera, it is shown that object and scene learning and glance object recognition are well performed by our model.
C1 Soka Univ, Fac Engn, Dept Informat Syst Sci, Tokyo 1928577, Japan.
RP Atsumi, M (corresponding author), Soka Univ, Fac Engn, Dept Informat Syst Sci, 1-236 Tangi Cho, Tokyo 1928577, Japan.
EM matsumi@t.soka.ac.jp
CR ASTUMI M, 2003, SMC 2003 C P 2003 IE, P2863
   Atsumi M., 2003, P WORKSH SELF ORG MA, P299
   Braitenberg V., 1984, VEHICLES EXPT SYNTHE
   Breazeal C, 1999, IJCAI-99: PROCEEDINGS OF THE SIXTEENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, VOLS 1 & 2, P1146
   Gerstner W., 2002, SPIKING NEURON MODEL
   Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558
   Kohonen T., 1995, SELF ORG MAPS
   MICHEL O, WEBOTS
   Niebur E, 1998, ATTENTIVE BRAIN, P163
   OLSHAUSEN BA, 1993, J NEUROSCI, V13, P4700
   Ruf B, 1998, COMPUTATIONAL NEUROSCIENCE: TRENDS IN RESEARCH, P509
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   Webster MJ, 1998, ATTENTIVE BRAIN, P19
NR 13
TC 1
Z9 1
U1 0
U2 0
PY 2004
BP 2859
EP 2864
WC Computer Science, Artificial Intelligence; Computer Science, Cybernetics
DA 2023-11-11
ER

PT J
AU Juárez-Lora, A
   García-Sebastián, LM
   Ponce-Ponce, VH
   Rubio-Espino, E
   Molina-Lozano, H
   Sossa, H
AF Juarez-Lora, Alejandro
   Garcia-Sebastian, Luis M.
   Ponce-Ponce, Victor H.
   Rubio-Espino, Elsa
   Molina-Lozano, Heron
   Sossa, Humberto
TI Implementation of Kalman Filtering with Spiking Neural Networks
SO SENSORS
DT Article
DE Kalman filter; artificial intelligence; spiking neural networks;
   robotics; dynamics
AB A Kalman filter can be used to fill space-state reconstruction dynamics based on knowledge of a system and partial measurements. However, its performance relies on accurate modeling of the system dynamics and a proper characterization of the uncertainties, which can be hard to obtain in real-life scenarios. In this work, we explore how the values of a Kalman gain matrix can be estimated by using spiking neural networks through a combination of biologically plausible neuron models with spike-time-dependent plasticity learning algorithms. The performance of proposed neural architecture is verified with simulations of some representative nonlinear systems, which show promising results. This approach traces a path for its implementation in neuromorphic analog hardware that can learn and reconstruct partial and changing dynamics of a system without the massive power consumption that is typically needed in a Von Neumann-based computer architecture.
C1 [Juarez-Lora, Alejandro; Garcia-Sebastian, Luis M.; Ponce-Ponce, Victor H.; Rubio-Espino, Elsa; Molina-Lozano, Heron; Sossa, Humberto] Inst Politecn Nacl, Ctr Invest Comp, Mexico City 07738, DF, Mexico.
RP Juárez-Lora, A (corresponding author), Inst Politecn Nacl, Ctr Invest Comp, Mexico City 07738, DF, Mexico.
EM jjuarezl2020@cic.ipn.mx
CR Bing ZS, 2019, IEEE INT CONF ROBOT, P9645, DOI [10.1109/icra.2019.8793774, 10.1109/ICRA.2019.8793774]
   Bing ZS, 2018, FRONT NEUROROBOTICS, V12, DOI 10.3389/fnbot.2018.00035
   Brunton SL., 2022, DATA DRIVEN SCI ENG, DOI [10.1017/9781009089517, DOI 10.1017/9781009089517]
   Davies M, 2021, P IEEE, V109, P911, DOI 10.1109/JPROC.2021.3067593
   Eshraghian JK, 2023, Arxiv, DOI [arXiv:2109.12894, DOI 10.48550/ARXIV.2109.12894]
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   Guo WZ, 2021, FRONT NEUROSCI-SWITZ, V15, DOI [10.3389/fnins.2021.638474, 10.1007/s11704-020-9230-x]
   Harris CR, 2020, NATURE, V585, P357, DOI 10.1038/s41586-020-2649-2
   Haykin S, 2001, ADAPT LEARN SYST SIG, P1
   Javanshir A, 2022, NEURAL COMPUT, V34, P1289, DOI 10.1162/neco_a_01499
   Juarez-Lora A, 2022, FRONT NEUROROBOTICS, V16, DOI 10.3389/fnbot.2022.904017
   Kaheman K, 2020, P ROY SOC A-MATH PHY, V476, DOI 10.1098/rspa.2020.0279
   Kaiser E, 2018, P ROY SOC A-MATH PHY, V474, DOI 10.1098/rspa.2018.0335
   Kalman RE., 1961, J BASIC ENG-T ASME, V83, P95, DOI 10.1115/1.3658902
   Kendall JD, 2020, APPL PHYS REV, V7, DOI 10.1063/1.5129306
   Kim H, 2021, NAT COMMUN, V12, DOI 10.1038/s41467-021-25455-0
   Kimura M, 2022, SCI REP-UK, V12, DOI 10.1038/s41598-022-09443-y
   Li YS, 2021, ADV INTELL SYST-GER, V3, DOI 10.1002/aisy.202000137
   Meurer A, 2017, PEERJ COMPUT SCI, DOI 10.7717/peerj-cs.103
   Modha D.S., 2022, PRODUCTS AKIDA NEURA
   Modha D.S., 2016, BRAINS ARCHITECTURE
   Payvand M, 2022, NAT COMMUN, V13, DOI 10.1038/s41467-022-33476-6
   Revach G, 2022, IEEE T SIGNAL PROCES, V70, P1532, DOI 10.1109/TSP.2022.3158588
   Saito T, 2020, IEICE NONLINEAR THEO, V11, P373, DOI 10.1587/nolta.11.373
   Sandamirskaya Y, 2022, SCI ROBOT, V7, DOI 10.1126/scirobotics.abq3909
   Sandamirskaya Y, 2022, SCI ROBOT, V7, DOI 10.1126/scirobotics.abl8419
   Schuman CD, 2022, NAT COMPUT SCI, V2, P10, DOI 10.1038/s43588-021-00184-y
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Teng Q, 2019, AIP ADV, V9, DOI 10.1063/1.5100558
   Thompson N. C., 2020, ARXIV
   Volinski A, 2022, PATTERNS, V3, DOI 10.1016/j.patter.2021.100391
   Zaidel Y, 2021, FRONT NEUROROBOTICS, V15, DOI 10.3389/fnbot.2021.631159
   Zhang XM, 2021, SCI BULL, V66, P1624, DOI 10.1016/j.scib.2021.04.014
NR 33
TC 1
Z9 1
U1 3
U2 5
PD NOV
PY 2022
VL 22
IS 22
AR 8845
DI 10.3390/s22228845
WC Chemistry, Analytical; Engineering, Electrical & Electronic; Instruments
   & Instrumentation
DA 2023-11-11
ER

PT C
AU Morita, K
   Morita, N
AF Morita, Kenta
   Morita, Naoki
GP IEEE
TI A spiking neural network that extracts frequent symbol patterns from
   time-series data with different symbol appearance intervals
SO 2022 JOINT 12TH INTERNATIONAL CONFERENCE ON SOFT COMPUTING AND
   INTELLIGENT SYSTEMS AND 23RD INTERNATIONAL SYMPOSIUM ON ADVANCED
   INTELLIGENT SYSTEMS (SCIS&ISIS)
SE Joint International Conference on Soft Computing and Intelligent Systems
   SCIS and International Symposium on Advanced Intelligent Systems ISIS
DT Proceedings Paper
CT Joint 12th International Conference on Soft Computing and Intelligent
   Systems / 23rd International Symposium on Advanced Intelligent Systems
   (SCIS and ISIS)
CY NOV 29-DEC 02, 2022
CL Ise, JAPAN
DE Spiking neural network; Time-series information processing; Pattern
   mining
AB In this research, we aim to extract frequent symbol patterns from symbol sequences. In the target time series data, the appearance timing of symbols is different. The purpose is to absorb the deviation of the appearance timing and extract frequent symbol patterns. This paper proposes a spiking neural network that extracts frequent symbol patterns. The structure of the proposed network grows automatically each time a symbol is given. By learning the network, the output unit will only fire when given a frequent symbol pattern, and the network will extract the frequent symbol patterns. As a result of a simple experiment, it was confirmed that the proposed network can extract frequent symbol patterns from the target time series data.
C1 [Morita, Kenta] Suzuka Univ Med Sci, Fac Med Engn, Tsu, Mie, Japan.
   [Morita, Naoki] Tokai Univ, Sch Informat Telecommun Engn, Tokyo, Japan.
RP Morita, K (corresponding author), Suzuka Univ Med Sci, Fac Med Engn, Tsu, Mie, Japan.
EM morita@suzuka-u.ac.jp; morita@tokai.ac.jp
CR AGRAWAL R., 1994, SER VLDB 94, P478, DOI DOI 10.5555/645920.672836
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Han JW, 2000, SIGMOD RECORD, V29, P1
   Hara K, 2017, IEEE INT CONF COMP V, P3154, DOI 10.1109/ICCVW.2017.373
   He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38
   Huang DA, 2018, PROC CVPR IEEE, P7366, DOI 10.1109/CVPR.2018.00769
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
   Lotter W, 2017, Arxiv, DOI [arXiv:1605.08104, 10.48550/arXiv.1605.08104]
   Morita K, 2019, PROCEDIA COMPUT SCI, V159, P363, DOI 10.1016/j.procs.2019.09.191
   Pei J, 2001, 2001 IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P441, DOI 10.1109/ICDM.2001.989550
   Simonyan K, 2014, ADV NEUR IN, V27
NR 11
TC 0
Z9 0
U1 0
U2 0
PY 2022
DI 10.1109/SCISISIS55246.2022.10002157
WC Automation & Control Systems; Computer Science, Artificial Intelligence;
   Robotics
DA 2023-11-11
ER

PT J
AU Cho, MW
AF Cho, Myoung Won
TI Simulations in a Spiking Neural Network Model Based on the Free Energy
   Principle
SO JOURNAL OF THE KOREAN PHYSICAL SOCIETY
DT Article
DE Neural network dynamics and learning; Free energy principle;
   Computational simulation method
AB The Feynman machine is a neural network model in which the spike-timing-dependent firing process is described through a path integral formulation. In addition, the gradient descent in the free energy is proposed as an ideal learning rule of the model system. The unique formulation of the Feynman machine is useful for studying the substance of the firing and the learning process in a spiking neural network; however, the implementation of the Feynman machine is not a plan problem because of the difficulty in calculating the free energy. We here introduce how to perform the simulation of both the firing and the learning processes in the Feynman machine through the Monte Carlo or the numerical integral method. We demonstrate the adequacy of the methods by applying them to the firing and the learning processes in some neural systems.
C1 [Cho, Myoung Won] Sungshin Womens Univ, Dept Global Med Sci, Seoul 01133, South Korea.
RP Cho, MW (corresponding author), Sungshin Womens Univ, Dept Global Med Sci, Seoul 01133, South Korea.
EM mwcho@sungshin.ac.kr
CR Abbott LF, 2000, NAT NEUROSCI, V3, P1178, DOI 10.1038/81453
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Chichilnisky EJ, 2001, NETWORK-COMP NEURAL, V12, P199, DOI 10.1088/0954-898X/12/2/306
   Cho MW, 2019, J KOREAN PHYS SOC, V74, P63, DOI 10.3938/jkps.74.63
   Cho M, 2018, J KOREAN PHYS SOC, V73, P1385, DOI 10.3938/jkps.73.1385
   Cho Myoung Won, 2017, [New Physics: Sae Mulli, 새물리], V67, P862
   Cho MW, 2017, J KOREAN PHYS SOC, V71, P222, DOI 10.3938/jkps.71.222
   Cho MW, 2016, EPL-EUROPHYS LETT, V115, DOI 10.1209/0295-5075/115/38001
   Cho Myoung Won, 2016, [New Physics: Sae Mulli, 새물리], V66, P786
   Cho MW, 2014, J KOREAN PHYS SOC, V64, P1213, DOI 10.3938/jkps.64.1213
   Cho MW, 2014, NEURAL NETWORKS, V49, P51, DOI 10.1016/j.neunet.2013.09.005
   Cho MW, 2007, PHYS REV LETT, V99, DOI 10.1103/PhysRevLett.99.208102
   Dayan P., 2001, THEORETICAL NEUROSCI
   Gerstner W, 1996, NATURE, V383, P76, DOI 10.1038/383076a0
   Gerstner W., 2002, SPIKING NEURON MODEL
   Landau D, 2021, GUIDE MONTE CARLO SI
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   Park M-H, 1996, J KOREAN PHYS SOC, V9, P29
   Schwartz O, 2006, J VISION, V6, P484, DOI 10.1167/6.4.13
   Song S, 2001, NEURON, V32, P339, DOI 10.1016/S0896-6273(01)00451-2
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   STEIN RB, 1967, BIOPHYS J, V7, P37, DOI 10.1016/S0006-3495(67)86574-3
   TORRIE GM, 1977, J COMPUT PHYS, V23, P187, DOI 10.1016/0021-9991(77)90121-8
NR 23
TC 6
Z9 6
U1 1
U2 5
PD AUG
PY 2019
VL 75
IS 3
BP 261
EP 270
DI 10.3938/jkps.75.261
WC Physics, Multidisciplinary
DA 2023-11-11
ER

PT C
AU Gruel, A
   Vitale, A
   Martinet, J
   Magno, M
AF Gruel, Amelie
   Vitale, Antonio
   Martinet, Jean
   Magno, Michele
GP IEEE
TI Neuromorphic Event-Based Spatio-temporal Attention using Adaptive
   Mechanisms
SO 2022 IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE CIRCUITS
   AND SYSTEMS (AICAS 2022): INTELLIGENT TECHNOLOGY IN THE POST-PANDEMIC
   ERA
DT Proceedings Paper
CT IEEE International Conference on Artificial Intelligence Circuits and
   Systems (AICAS) - Intelligent Technology in the Post-Pandemic Era
CY JUN 13-15, 2022
CL Incheon, SOUTH KOREA
DE Event-Based Vision; Online Adaptation; Neuromorphic Hardware; Spiking
   Neural Networks
AB Contrary to RGB cameras, Dynamic Vision Sensor (DVS) output visual data in the form of an asynchronous events stream by recording pixel-wise luminance changes at microsecond resolution. While conventional computer vision approaches utilise frame-based input data, thus failing to take full advantage of the high temporal resolution, novel approaches use spiking neural networks Spiking Neural Networks (SNNs) which are more compatible to handle event-based data since these bioinspired neural models intrinsically encode information in a sparse manner using activation spikes trains. This paper presents an attentional mechanism which detects regions with higher event density by using inherent SNN dynamics combined with online weight and threshold adaptation. We implemented the network directly on Intel's research neuromorphic chip Loihi and evaluate our proposed method on the open DVS128 Gesture Dataset. Our system is able to process 1 ms of event-data in 6 ms and reject more than 50% of incoming unwanted events occurring only 20 ms after activity onset.
C1 [Gruel, Amelie; Martinet, Jean] Univ Cote dAzur, I3S CNRS, Sophia Antipolis, France.
   [Vitale, Antonio; Magno, Michele] Swiss Fed Inst Technol, PBL Ctr, Zurich, Switzerland.
RP Gruel, A (corresponding author), Univ Cote dAzur, I3S CNRS, Sophia Antipolis, France.
EM amelie.gruel@univ-cotedazur.fr; antonio.vitale@pbl.ee.ethz.ch;
   jean.martinet@univ-cotedazur.fr; michele.magno@pbl.ee.ethz.ch
CR Amir A, 2017, PROC CVPR IEEE, P7388, DOI 10.1109/CVPR.2017.781
   Borji A, 2013, IEEE T PATTERN ANAL, V35, P185, DOI 10.1109/TPAMI.2012.89
   Davies M, 2021, P IEEE, V109, P911, DOI 10.1109/JPROC.2021.3067593
   Gallego G, 2022, IEEE T PATTERN ANAL, V44, P154, DOI 10.1109/TPAMI.2020.3008413
   Gruel A., 2021, CONTENT BASED MULTIM
   Haessig G, 2019, 2019 IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE CIRCUITS AND SYSTEMS (AICAS 2019), P86, DOI [10.1109/aicas.2019.8771512, 10.1109/AICAS.2019.8771512]
   Hagenaars JJ, 2021, Arxiv, DOI arXiv:2106.01862
   Hebb D. O., 1949, ORG BEHAV
   Mayr C, 2019, Arxiv, DOI arXiv:1911.02385
   Peng JZ, 2019, 2019 IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE CIRCUITS AND SYSTEMS (AICAS 2019), P171, DOI [10.1109/aicas.2019.8771615, 10.1109/AICAS.2019.8771615]
   Renner A, 2019, IEEE COMPUT SOC CONF, P1709, DOI 10.1109/CVPRW.2019.00220
   Song SJ, 2019, 2019 IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE CIRCUITS AND SYSTEMS (AICAS 2019), P179, DOI [10.1109/aicas.2019.8771542, 10.1109/AICAS.2019.8771542]
   Vitale Antonio, 2021, 2021 IEEE International Conference on Robotics and Automation (ICRA), P103, DOI 10.1109/ICRA48506.2021.9560881
   Zhu AZ, 2018, ROBOTICS: SCIENCE AND SYSTEMS XIV
NR 14
TC 4
Z9 4
U1 1
U2 7
PY 2022
BP 379
EP 382
DI 10.1109/AICAS54282.2022.9869977
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Iacob, S
   Kwisthout, J
   Thill, S
AF Iacob, Stefan
   Kwisthout, Johan
   Thill, Serge
BE Vouloutsi, V
   Mura, A
   Tauber, F
   Speck, T
   Prescott, TJ
   Verschure, PFMJ
TI From Models of Cognition to Robot Control and Back Using Spiking Neural
   Networks
SO BIOMIMETIC AND BIOHYBRID SYSTEMS, LIVING MACHINES 2020
SE Lecture Notes in Artificial Intelligence
DT Proceedings Paper
CT 9th International Conference on Biomimetic and Biohybrid Systems (Living
   Machines)
CY JUL 28-30, 2020
CL Freiburg, GERMANY
DE Spiking neural networks; Robot control; Bio-plausible models
ID SIMULATION
AB With the recent advent of neuromorphic hardware there has been a corresponding rise in interest in spiking neural network models for the control of real-world artificial agents such as robots. Although models of cognitive mechanisms instantiated in spiking neural networks are nothing new, very few of them are translated onto real robot platforms. In this paper, we attempt such a translation: we implement an existing, biologically plausible model of reaching (the REACH model) demonstrated in 2D simulation on a UR5e robot arm. We are interested in particular in how well such a translation works since this has implications for similar exercises with a vast library of existing models of cognition. In this particular case, after extensions to operations in 3D and for the particular hardware used, we do find that the model is able to learn on the real platform as it did in the original simulation, albeit without reaching the same levels of performance.
C1 [Iacob, Stefan; Kwisthout, Johan; Thill, Serge] Radboud Univ Nijmegen, Donders Inst Brain Cognit & Behav, NL-6525 HR Nijmegen, Netherlands.
RP Thill, S (corresponding author), Radboud Univ Nijmegen, Donders Inst Brain Cognit & Behav, NL-6525 HR Nijmegen, Netherlands.
EM s.iacob@student.ru.nl; j.kwisthout@donders.ru.nl; s.thill@donders.ru.nl
CR Andersen T.T., 2020, UNIVERSAL ROBOTS ROS
   [Anonymous], 2019, UNIVERSAL ROBOTS UR5
   [Anonymous], 2019, BIOL INSPIRED ALTERN
   Barsalou LW, 1999, BEHAV BRAIN SCI, V22, P577, DOI 10.1017/S0140525X99532147
   Bekolay T, 2014, FRONT NEUROINFORM, V7, DOI 10.3389/fninf.2013.00048
   Bing ZS, 2018, FRONT NEUROROBOTICS, V12, DOI 10.3389/fnbot.2018.00035
   Blouw P, 2020, PROCEEDINGS OF THE 2019 7TH ANNUAL NEURO-INSPIRED COMPUTATIONAL ELEMENTS WORKSHOP (NICE 2019), DOI 10.1145/3320288.3320304
   Bouganis A., 2010, P 2010 INT JOINT C N, P1, DOI DOI 10.1109/IJCNN.2010.5596525
   Da Lio M, 2017, IEEE INT C INTELL TR
   Demiris Y., 2005, MOTOR BABBLING HIERA
   DeWolf T, 2016, P ROY SOC B-BIOL SCI, V283, DOI 10.1098/rspb.2016.2134
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Eliasmith C., 2013, BUILD BRAIN NEURAL A, DOI DOI 10.1093/ACPROF:OSO/9780199794546.001.0001
   Eliasmith C, 2002, NEURAL ENG COMPUTATI
   Eliasmith C, 2012, SCIENCE, V338, P1202, DOI 10.1126/science.1225266
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Hesslow G, 2002, TRENDS COGN SCI, V6, P242, DOI 10.1016/S1364-6613(02)01913-7
   Hesslow G, 2012, BRAIN RES, V1428, P71, DOI 10.1016/j.brainres.2011.06.026
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Jirenhed D.A., 2001, EXPLORING INTERNAL S, V86, P107
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   SHADMEHR R, 1994, J NEUROSCI, V14, P3208
   Stramandinoli F, 2012, NEURAL NETWORKS, V32, P165, DOI 10.1016/j.neunet.2012.02.012
   Thill Serge, 2011, Artificial General Intelligence. Proceedings 4th International Conference, AGI 2011, P247, DOI 10.1007/978-3-642-22887-2_26
   Thill S, 2016, NEUROCOMPUTATIONAL M, P115, DOI [10.1142/9789814699341-0008, DOI 10.1142/9789814699341-0008]
   Tieck JCV, 2018, PROCEEDINGS OF 2018 IEEE 17TH INTERNATIONAL CONFERENCE ON COGNITIVE INFORMATICS & COGNITIVE COMPUTING (ICCI*CC 2018), P111, DOI 10.1109/ICCI-CC.2018.8482049
   Vernon D., 2014, ARTIFICIAL COGNITIVE
   Voelker A.R., 2015, SOLUTION DYNAMICS PR
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Ziemke T, 2005, NEUROCOMPUTING, V68, P85, DOI 10.1016/j.neucom.2004.12.005
NR 31
TC 1
Z9 1
U1 0
U2 0
PY 2020
VL 12413
BP 176
EP 191
DI 10.1007/978-3-030-64313-3_18
WC Cell & Tissue Engineering; Computer Science, Cybernetics; Robotics
DA 2023-11-11
ER

PT C
AU Li, X
   Wu, QX
   Kou, Y
   Hou, L
   Yang, H
AF Li, Xue
   Wu, Qingxiang
   Kou, Yu
   Hou, Lei
   Yang, Heng
BE Liang, J
   Zeng, B
   Wang, L
   Shao, L
   Hui, X
   Tao, Z
   Lin, S
TI Lane Detection Based on Spiking Neural Network and Hough Transform
SO 2015 8TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING (CISP)
DT Proceedings Paper
CT Proceedings 2015 8 International Congress on Image and Signal Processing
   (CISP)
CY OCT 14-16, 2015
CL Shenyang, PEOPLES R CHINA
DE lane detection; region of intrests; spiking neural network; hough
   transform
AB In the field of the unmanned automobile and the automobile auxiliary driving system, the real-time and accurate detection of the lane is very important. Based on the previous research on the lane detection, the paper introduces the spiking neural network with the parallel mechanism to detect the lane. Firstly, the region of interests (ROI) is set on the origin image that collected by a vehicle on-board camera. In order to reduce processing time, areas outside the road are excluded in the ROI. Then the image preprocessing is applied to the ROI, including RGB to grayscale, gray stretch and median filtering to eliminate noise. Edge detection of the lane is the key to determine whether the Hough transform can detect the lane. In this paper, the spiking neural network is used to detect the edge of the lane. Finally, Hough transform is used to detect the lane. Experimental results show that this method is more accurate and robust than other methods.
C1 [Li, Xue; Wu, Qingxiang; Kou, Yu; Hou, Lei; Yang, Heng] Fujian Normal Univ, Coll Photon & Elect Engn, Minist Educ, Key Lab OptoElect Sci & Technol Med, Fuzhou, Peoples R China.
RP Li, X (corresponding author), Fujian Normal Univ, Coll Photon & Elect Engn, Minist Educ, Key Lab OptoElect Sci & Technol Med, Fuzhou, Peoples R China.
EM 1026044914@qq.com; qxwu@fjnu.edu.cn
CR [Anonymous], [No title captured]
   [Anonymous], 2014, BRAZILIAN J BIOMEDIC, V30, P205
   Cai J.-R., 2006, J JIANGSU U NAT SCI, V27, P6
   Dai Y. M., 2006, J HANGZHOU DIANZI U, V18, P91
   Dayan P, 2001, THEORETICAL NEUROSCI, P248
   Guo J, 2015, 2015 IEEE 12TH INTERNATIONAL SYMPOSIUM ON AUTONOMOUS DECENTRALIZED SYSTEMS ISADS 2015, P285, DOI 10.1109/ISADS.2015.24
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Hosoya T, 2005, NATURE, V436, P71, DOI 10.1038/nature03689
   Jin Z. L., 2009, INSTRUMENTATION MEAS, V28, P88
   Kan E.R., 2000, PRINCIPLES NEURAL SC
   Knoblauch A, 2002, LOCAL EFFECTS CORTIC, V87, P151
   Liu X. R., 2012, PHOTOELECTRON LASER, V18, P1834
   Schwiening CJ, 2012, J PHYSIOL-LONDON, V590, P2571, DOI 10.1113/jphysiol.2012.230458
   Wang JG, 2010, EXPERT SYST APPL, V37, P113, DOI 10.1016/j.eswa.2009.05.026
   Wu QX, 2008, LECT NOTES COMPUT SC, V5227, P76
   Yi SC, 2015, COMPUT ELECTR ENG, V42, P23, DOI 10.1016/j.compeleceng.2015.01.002
   Yuan Y., 2013, J CHENGDU U TECHNOLO, V31, P126
   Zhao Yan, 2012, Journal of Jilin University (Science Edition), V50, P740
NR 18
TC 9
Z9 10
U1 0
U2 1
PY 2015
BP 626
EP 630
WC Engineering, Electrical & Electronic; Imaging Science & Photographic
   Technology
DA 2023-11-11
ER

PT J
AU Zhang, AG
   Zhu, W
   Li, JY
AF Zhang, Anguo
   Zhu, Wei
   Li, Junyu
TI Spiking Echo State Convolutional Neural Network for Robust Time Series
   Classification
SO IEEE ACCESS
DT Article
DE Echo state network; convolutional neural network; robust time series
   classification
ID CHAOTIC SYSTEMS
AB In this paper, a novel high-accuracy and robust computing framework for time series classification tasks is presented. The framework consists of a feature extraction module and a classification module, where the feature extraction is implemented by reservoir computing method of spiking neurons, and the classification result is obtained by the state-of-the-art analog convolutional neural networks (CNNs). The original time series input is first converted to multi-channel spike streams, then fed into the spiking reservoir layer to produce intermediate spike output, and subsequently, the spike output is transformed into a 2D mapping image, and deep CNN model is applied to classify the mapping image. The proposed model has the following three significant advantages: long-and-short term memory brought by the echo state of reservoir component, robustness to noise brought by the spiking encoding method, and high-accuracy performance brought by the deep CNN model. The experiments conducted on both synthetic time series data set and UCR time series data sets showed that our approach achieved highly competitive accuracy and robustness over other existing methods.
C1 [Zhang, Anguo; Zhu, Wei] Ruijie Networks Co Ltd, Res Inst Ruijie, Fuzhou 350002, Fujian, Peoples R China.
   [Li, Junyu] Hefei Technol Coll, Hefei 230012, Anhui, Peoples R China.
RP Li, JY (corresponding author), Hefei Technol Coll, Hefei 230012, Anhui, Peoples R China.
EM lijunyu_htc@126.com
CR Baydogan MG, 2013, IEEE T PATTERN ANAL, V35, P2796, DOI 10.1109/TPAMI.2013.72
   Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   Chaovalitwongse WA, 2011, IEEE T SYST MAN CY A, V41, P977, DOI 10.1109/TSMCA.2011.2106118
   Chen Q, 2020, NEURAL COMPUT APPL, V32, P3685, DOI 10.1007/s00521-018-3747-z
   Deng HT, 2013, INFORM SCIENCES, V239, P142, DOI 10.1016/j.ins.2013.02.030
   Fulcher BD, 2014, IEEE T KNOWL DATA EN, V26, P3026, DOI 10.1109/TKDE.2014.2316504
   García-Treviño ES, 2014, IEEE T CYBERNETICS, V44, P1978, DOI 10.1109/TCYB.2014.2322310
   Han SI, 2014, IEEE T IND ELECTRON, V61, P1099, DOI 10.1109/TIE.2013.2253072
   Haselsteiner E, 2000, IEEE T REHABIL ENG, V8, P457, DOI 10.1109/86.895948
   Hayashi H, 2015, IEEE T NEUR NET LEAR, V26, P3021, DOI 10.1109/TNNLS.2015.2400448
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Hills J, 2014, DATA MIN KNOWL DISC, V28, P851, DOI 10.1007/s10618-013-0322-1
   Jaeger H, 2004, SCIENCE, V304, P78, DOI 10.1126/science.1091277
   Jaeger H., 2001, GMD REPORT 148
   Kampouraki A, 2009, IEEE T INF TECHNOL B, V13, P512, DOI 10.1109/TITB.2008.2003323
   Karim F, 2018, IEEE ACCESS, V6, P1662, DOI 10.1109/ACCESS.2017.2779939
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li GQ, 2012, KNOWL-BASED SYST, V35, P35, DOI 10.1016/j.knosys.2012.04.019
   Lines J., 2012, PROC 18 ACM SIGKDD I, P289
   Liu DR, 2013, IEEE T CYBERNETICS, V43, P779, DOI 10.1109/TSMCB.2012.2216523
   Lukosevicius Mantas, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P659, DOI 10.1007/978-3-642-35289-8_36
   Ma Y, 2018, IEEE INT CONF ELECTR, P40, DOI 10.1109/ICEIEC.2018.8473552
   Mueen A, 2011, P 17 ACM SIGKDD INT, P1154, DOI DOI 10.1145/2020408.2020587
   Samiee K, 2015, IEEE T BIO-MED ENG, V62, P541, DOI 10.1109/TBME.2014.2360101
   Sheng CY, 2012, NEUROCOMPUTING, V82, P186, DOI 10.1016/j.neucom.2011.11.021
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Skowronski MD, 2007, IEEE T AUDIO SPEECH, V15, P1724, DOI 10.1109/TASL.2007.896669
   Suh S, 2016, IEEE IJCNN, P1015, DOI 10.1109/IJCNN.2016.7727309
   Tang YJ, 2016, INT CONF DAT MIN WOR, P503, DOI [10.1109/ICDMW.2016.0078, 10.1109/ICDMW.2016.65]
   Tanisaro P, 2016, 2016 15TH IEEE INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA 2016), P831, DOI [10.1109/ICMLA.2016.0149, 10.1109/ICMLA.2016.166]
   Tong MH, 2007, NEURAL NETWORKS, V20, P424, DOI 10.1016/j.neunet.2007.04.013
   Wei QL, 2012, NEURAL NETWORKS, V32, P236, DOI 10.1016/j.neunet.2012.02.027
   Xue FZ, 2016, ADV COGN NEURODYN, P699, DOI 10.1007/978-981-10-0207-6_94
   Yusof MH, 2016, INFORM SCIENCES, V364, P184, DOI 10.1016/j.ins.2015.11.017
   Zhang AG, 2017, CHIN AUTOM CONGR, P6189, DOI 10.1109/CAC.2017.8243892
   Zhao BD, 2017, J SYST ENG ELECTRON, V28, P162, DOI 10.21629/JSEE.2017.01.18
   Zhu XL, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON AGENTS (ICA), P145, DOI 10.1109/AGENTS.2017.8015322
NR 38
TC 17
Z9 18
U1 1
U2 16
PY 2019
VL 7
BP 4927
EP 4935
DI 10.1109/ACCESS.2018.2887354
WC Computer Science, Information Systems; Engineering, Electrical &
   Electronic; Telecommunications
DA 2023-11-11
ER

PT C
AU Sengupta, N
   Scott, N
   Kasabov, N
AF Sengupta, Neelava
   Scott, Nathan
   Kasabov, Nikola
BE Ravi, V
   Panigrahi, BK
   Das, S
   Suganthan, PN
TI Framework for Knowledge Driven Optimisation Based Data Encoding for
   Brain Data Modelling Using Spiking Neural Network Architecture
SO PROCEEDINGS OF THE FIFTH INTERNATIONAL CONFERENCE ON FUZZY AND NEURO
   COMPUTING (FANCCO - 2015)
SE Advances in Intelligent Systems and Computing
DT Proceedings Paper
CT 5th International Conference on Fuzzy and Neuro Computing (FANCCO)
CY DEC 17-19, 2015
CL Inst Dev & Res Banking Technol, Hyderabad, INDIA
HO Inst Dev & Res Banking Technol
DE Spike encoding; Spiking neural network; Mixed integer optimisation;
   Neucube
ID RESPONSES
AB From it's initiation, the field of artificial intelligence has been inspired primarily by the human brain. Recent advances and collaboration of computational neuroscience and artificial intelligence has led to the development of spiking neural networks (SNN) that can very closely mimic behaviour of the human brain. These networks use spike codes or synaptic potentials as a source of information. On the contrary, most of the real world data sources including brain data are continuous and analogue in nature. For an SNN based pattern recognition tool, it is imperative to have a data encoding mechanism that transforms the streaming analogue spatiotemporal information to train of spikes. In this article, we propose a generalised background knowledge-driven optimisation framework for encoding brain data (fMRI, EEG and others). Further, we also formalise and implement a mixed-integer genetic algorithm based optimisation for background knowledge-driven data encoding for fMRI data and compare the performance with existing data encoding method like temporal contrast and Ben Spiker Algorithm.
C1 [Sengupta, Neelava; Scott, Nathan; Kasabov, Nikola] Auckland Univ Technol, Knowledge Engn & Discovery Res Inst, Auckland, New Zealand.
RP Sengupta, N (corresponding author), Auckland Univ Technol, Knowledge Engn & Discovery Res Inst, Auckland, New Zealand.
EM neelava.sengupta@aut.ac.nz
CR [Anonymous], 2011, STAT ANAL FMRI DATA
   Babu BV, 2003, IEEE C EVOL COMPUTAT, P2696
   Boynton GM, 1996, J NEUROSCI, V16, P4207, DOI 10.1523/jneurosci.16-13-04207.1996
   Chen C., 1995, LINEAR SYSTEM THEORY
   de Garis H., 1999, IJCNN'99. International Joint Conference on Neural Networks. Proceedings (Cat. No.99CH36339), P438, DOI 10.1109/IJCNN.1999.831535
   Deb K, 2000, COMPUT METHOD APPL M, V186, P311, DOI 10.1016/S0045-7825(99)00389-8
   Deep K, 2009, APPL MATH COMPUT, V212, P505, DOI 10.1016/j.amc.2009.02.044
   DEGARIS H, 1994, NEW GENERAT COMPUT, V12, P215, DOI 10.1007/BF03037343
   Dorigo M, 1996, IEEE T SYST MAN CY B, V26, P29, DOI 10.1109/3477.484436
   Fransson P, 1999, NEUROIMAGE, V9, P611, DOI 10.1006/nimg.1999.0438
   Friston KJ, 1998, MAGNET RESON MED, V39, P41, DOI 10.1002/mrm.1910390109
   Gabbiani F, 1996, NEURAL COMPUT, V8, P44, DOI 10.1162/neco.1996.8.1.44
   Gabbiani F, 1999, J EXP BIOL, V202, P1267
   Gerstner W., 2002, SPIKING NEURON MODEL
   Glover GH, 1999, NEUROIMAGE, V9, P416, DOI 10.1006/nimg.1998.0419
   Hafiz F., 2012, 2012 International Conference on Computer and Communication Engineering (ICCCE), P132, DOI 10.1109/ICCCE.2012.6271167
   Hough M, 1999, INT C ROB ART LIF CI
   Iakymchuk T., 2014, 2014 9 SO C PROGR LO, P1
   Kasabov Nikola, 2012, Artificial Neural Networks in Pattern Recognition. Proceedings of the 5th INNS IAPR TC 3 GIRPR Workshop, ANNPR 2012, P225, DOI 10.1007/978-3-642-33212-8_21
   Kasabov N, 2013, NEURAL NETWORKS, V41, P188, DOI 10.1016/j.neunet.2012.11.014
   Luo YQ, 2007, COMPUT CHEM ENG, V31, P153, DOI 10.1016/j.compchemeng.2006.05.016
   Maass W, 2001, PULSED NEURAL NETWOR
   McCulloch Warren S., 1943, BULL MATH BIOPHYS, V5, P115, DOI 10.1007/BF02478259
   Richard S. J., 2004, HUMAN BRAIN FUNCTION
   Schrauwen B, 2003, IEEE IJCNN, P2825
   Vazquez AL, 1998, NEUROIMAGE, V7, P108, DOI 10.1006/nimg.1997.0316
NR 26
TC 8
Z9 8
U1 0
U2 9
PY 2015
VL 415
BP 109
EP 118
DI 10.1007/978-3-319-27212-2_9
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods
DA 2023-11-11
ER

PT C
AU Kamoi, S
   Iwai, R
   Kinjo, H
   Yamamoto, T
AF Kamoi, S
   Iwai, R
   Kinjo, H
   Yamamoto, T
GP IEEE
   IEEE
TI Pulse pattern training of spiking neural networks using improved genetic
   algorithm
SO 2003 IEEE INTERNATIONAL SYMPOSIUM ON COMPUTATIONAL INTELLIGENCE IN
   ROBOTICS AND AUTOMATION, VOLS I-III, PROCEEDINGS
DT Proceedings Paper
CT IEEE International Symposium on Computational Intelligence in Robotics
   and Automation
CY JUL 16-20, 2003
CL KOBE, JAPAN
AB In this paper, a training method of a spike train for spiking neural networks (SNNs) by the use of genetic algorithms (GAs) is proposed. There have been some reports on training methods of artificial neural networks. However, there are only a few reports on SNNs. SNNS are known to be highly similar to biological neural networks because they treat spike trains. SNNs process information based on pulse signals. In SNNs, spiking neurons receive spike pulses from and fire spike pulses to other neurons. The spiking neurons have a characteristic of suddenly changing the membrane potential immediately before and after firing. The characteristics of the potential behavior cause some difficulties in training SNNs. Many currently used training methods of SNNs apply the Hebb rule or gradient methods. However, under the application of the Hebb rule, SNNs sometimes failed the training test. Furthermore, the gradient methods include complicated calculations.
   In a previous study, we proposed a training method of SNN using a GA. It is reported that a use of Dot only connecting weights of SNN but also parameters of spike neuron is effective for SNN training. However, the successful rate of the training is not so good. In this paper, we apply an improved GA which adaptively changes the number of offspring and the mutation rate according to the diversity of the population to the SNN training. Simulation shows that the improved GA has better training performance than the traditional GA.
C1 Univ Ryukyus, Grad Sch Engn, Nishihara, Okinawa 90301, Japan.
RP Kamoi, S (corresponding author), Univ Ryukyus, Grad Sch Engn, Nishihara, Okinawa 90301, Japan.
CR [Anonymous], BIOPHYSICS COMPUTATI
   [Anonymous], 1989, GENETIC ALGORITHM SE
   [Anonymous], 1991, HDB GENETIC ALGORITH
   Fogel D. B., 1995, EVOLUTIONARY COMPUTA, V3rd
   HOLLAND JH, 1992, ADAPTATION NATURAL A
   KAMOI S, 2003, P AROB 8 03 OIT JAP, V2, P637
   MASS W, 1999, PULSED NEURAL NETWOR
   Rolls E. T., 1998, NEURAL NETWORKS BRAI
   Selvaratnam K., 2000, Transactions of the Institute of Systems, Control and Information Engineers, V13, P95, DOI 10.5687/iscie.13.3_95
   Shepherd G. M., 1998, SYNAPTIC ORG BRAIN
   WANG B, 2003, IEEJ T EIS, V123, P983
   2001, NEURAL NETWORKS, V14
NR 12
TC 4
Z9 4
U1 0
U2 0
PY 2003
BP 977
EP 981
WC Automation & Control Systems; Computer Science, Artificial Intelligence;
   Robotics
DA 2023-11-11
ER

PT J
AU Stasenko, SV
   Kazantsev, VB
AF Stasenko, Sergey V. V.
   Kazantsev, Victor B. B.
TI Bursting Dynamics of Spiking Neural Network Induced by Active
   Extracellular Medium
SO MATHEMATICS
DT Article
DE spiking neural network; tetrapartite synapse; neuron; ECM
ID NEURONAL SYNCHRONIZATION; MATRIX; COMMUNICATION; PLASTICITY; GAMMA
AB We propose a mathematical model of a spiking neural network (SNN) that interacts with an active extracellular field formed by the brain extracellular matrix (ECM). The SNN exhibits irregular spiking dynamics induced by a constant noise drive. Following neurobiological facts, neuronal firing leads to the production of the ECM that occupies the extracellular space. In turn, active components of the ECM can modulate neuronal signaling and synaptic transmission, for example, through the effect of so-called synaptic scaling. By simulating the model, we discovered that the ECM-mediated regulation of neuronal activity promotes spike grouping into quasi-synchronous population discharges called population bursts. We investigated how model parameters, particularly the strengths of ECM influence on synaptic transmission, may facilitate SNN bursting and increase the degree of neuronal population synchrony.
C1 [Stasenko, Sergey V. V.; Kazantsev, Victor B. B.] Moscow Inst Phys & Technol, Moscow 117303, Russia.
   [Stasenko, Sergey V. V.; Kazantsev, Victor B. B.] Lobachevsky State Univ Nizhny Novgorod, Neurotechnol Dept, Nizhnii Novgorod 603022, Russia.
RP Kazantsev, VB (corresponding author), Moscow Inst Phys & Technol, Moscow 117303, Russia.; Kazantsev, VB (corresponding author), Lobachevsky State Univ Nizhny Novgorod, Neurotechnol Dept, Nizhnii Novgorod 603022, Russia.
EM stasenko@neuro.nnov.ru; kazantsev@neuro.nnov.ru
CR Baldauf D, 2014, SCIENCE, V344, P424, DOI 10.1126/science.1247003
   Barabash N, 2023, EUR PHYS J-SPEC TOP, V232, P529, DOI 10.1140/epjs/s11734-023-00778-9
   Bikbaev A, 2015, SCI REP-UK, V5, DOI 10.1038/srep14527
   Bisong E., 2019, BUILDING MACHINE LEA, P151
   Bonneh-Barkay D, 2009, BRAIN PATHOL, V19, P573, DOI 10.1111/j.1750-3639.2008.00195.x
   Broekaart DWM, 2021, J CLIN INVEST, V131, DOI 10.1172/JCI138332
   de Jong JM, 2022, BIOMEDICINES, V10, DOI 10.3390/biomedicines10102475
   Dityatev A, 2011, CURR OPIN NEUROBIOL, V21, P353, DOI 10.1016/j.conb.2010.12.006
   Dityatev A, 2010, EPILEPSIA, V51, P61, DOI 10.1111/j.1528-1167.2010.02612.x
   Dityatev A, 2008, NEURON GLIA BIOL, V4, P235, DOI 10.1017/S1740925X09000118
   Duarte M., 2020, GITHUB REPOS
   Fawcett JW, 2022, MOL PSYCHIATR, V27, P3192, DOI 10.1038/s41380-022-01634-3
   Fell J, 2011, NAT REV NEUROSCI, V12, P105, DOI 10.1038/nrn2979
   Fries P, 2005, TRENDS COGN SCI, V9, P474, DOI 10.1016/j.tics.2005.08.011
   Fries P, 2001, SCIENCE, V291, P1560, DOI 10.1126/science.1055465
   Frischknecht R, 2012, ADV EXP MED BIOL, V970, P153, DOI 10.1007/978-3-7091-0932-8_7
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Izhikevich E. M., 2006, DYNAMICAL SYSTEMS NE, DOI [10.7551/mitpress/2526.001.0001, DOI 10.7551/MITPRESS/2526.001.0001]
   Kazantsev V, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0041646
   Kepecs A, 2003, NETWORK-COMP NEURAL, V14, P103, DOI 10.1088/0954-898X/14/1/306
   Khoshneviszadeh M., 2022, MICROVASCULAR DAMAGE
   Kim JH, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-018-37915-7
   Kochlamazashvili G, 2010, NEURON, V67, P116, DOI 10.1016/j.neuron.2010.05.030
   Lam D, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-40128-1
   Lazarevich IA, 2017, JETP LETT+, V105, P210, DOI 10.1134/S0021364017030092
   Lazarevich I, 2020, PLOS ONE, V15, DOI 10.1371/journal.pone.0227917
   Lobov SA, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21082678
   Lundqvist M, 2016, NEURON, V90, P152, DOI 10.1016/j.neuron.2016.02.028
   Makovkin S, 2022, SCI REP-UK, V12, DOI 10.1038/s41598-022-10649-3
   Muthukumaraswamy SD, 2009, P NATL ACAD SCI USA, V106, P8356, DOI 10.1073/pnas.0900728106
   Nelli F., 2015, PYTHON DATA ANAL DAT, P63
   Pankratova EV, 2019, NONLINEAR DYNAM, V97, P647, DOI 10.1007/s11071-019-05004-7
   Pasquale V, 2008, NEUROSCIENCE, V153, P1354, DOI 10.1016/j.neuroscience.2008.03.050
   Pikovsky A., 2003, SYNCHRONIZATION UNIV
   Pimashkin A, 2011, FRONT COMPUT NEUROSC, V5, DOI 10.3389/fncom.2011.00046
   PRINCE DA, 1978, ANNU REV NEUROSCI, V1, P395, DOI 10.1146/annurev.ne.01.030178.002143
   Rich MM, 2007, TRENDS NEUROSCI, V30, P119, DOI 10.1016/j.tins.2007.01.004
   Rozhnova MA, 2021, CHAOS SOLITON FRACT, V151, DOI 10.1016/j.chaos.2021.111253
   Schnitzler A, 2005, NAT REV NEUROSCI, V6, P285, DOI 10.1038/nrn1650
   Simonov AY, 2014, JETP LETT+, V98, P632, DOI 10.1134/S0021364013230136
   Sokolov I, 2016, ADV INTELL SYST, V449, P241, DOI 10.1007/978-3-319-32554-5_31
   Stasenko Sergey, 2022, 2022 Fourth International Conference Neurotechnologies and Neurointerfaces (CNN), P176, DOI 10.1109/CNN56452.2022.9912507
   Stasenko Sergey, 2022, 2022 Fourth International Conference Neurotechnologies and Neurointerfaces (CNN), P180, DOI 10.1109/CNN56452.2022.9912561
   Stasenko Sergey V., 2020, Procedia Computer Science, V169, P704, DOI 10.1016/j.procs.2020.02.175
   Stasenko SV, 2023, SCI REP-UK, V13, DOI 10.1038/s41598-023-33622-0
   Stasenko SV, 2023, MATHEMATICS-BASEL, V11, DOI 10.3390/math11030561
   Sterratt D., 2011, PRINCIPLES COMPUTATI
   Stimberg M, 2019, ELIFE, V8, DOI 10.7554/eLife.47314
   Timofeev I., 2012, JASPERS BASIC MECH E, V4
   Tsybina Yuliya, 2022, 2022 Fourth International Conference Neurotechnologies and Neurointerfaces (CNN), P206, DOI 10.1109/CNN56452.2022.9912521
   Turrigiano G, 2007, CURR OPIN NEUROBIOL, V17, P318, DOI 10.1016/j.conb.2007.04.004
   Ulbrich P, 2021, EUR J NEUROSCI, V53, P3811, DOI 10.1111/ejn.14887
   Van Rossum G., 1995, PYTHON TUTORIAL
   Velazquez JLP, 2003, PHYSICA D, V186, P205, DOI 10.1016/j.physd.2003.07.002
   Virtanen P, 2020, NAT METHODS, V17, P261, DOI 10.1038/s41592-019-0686-2
   Wagenaar DA, 2006, BMC NEUROSCI, V7, DOI 10.1186/1471-2202-7-11
   Wagenaar DA, 2005, J NEUROSCI, V25, P680, DOI 10.1523/JNEUROSCI.4209-04.2005
   Wagenaar DA, 2006, PHYS REV E, V73, DOI 10.1103/PhysRevE.73.051907
   Wang XJ, 2010, PHYSIOL REV, V90, P1195, DOI 10.1152/physrev.00035.2008
   Zeitler M, 2009, PHYS REV E, V79, DOI 10.1103/PhysRevE.79.065203
   Zeldenrust F, 2018, FRONT COMPUT NEUROSC, V12, DOI 10.3389/fncom.2018.00048
NR 62
TC 3
Z9 3
U1 3
U2 3
PD APR 28
PY 2023
VL 11
IS 9
AR 2109
DI 10.3390/math11092109
WC Mathematics
DA 2023-11-11
ER

PT J
AU Hazan, A
   Tsur, EE
AF Hazan, Avi
   Ezra Tsur, Elishai
TI Neuromorphic Analog Implementation of Neural Engineering
   Framework-Inspired Spiking Neuron for High-Dimensional Representation
SO FRONTIERS IN NEUROSCIENCE
DT Article
DE neural engineering framework; spiking neural networks; neuromorphic
   electronics; neuromorphic engineering; brain-inspired electronics
AB Brain-inspired hardware designs realize neural principles in electronics to provide high-performing, energy-efficient frameworks for artificial intelligence. The Neural Engineering Framework (NEF) brings forth a theoretical framework for representing high-dimensional mathematical constructs with spiking neurons to implement functional large-scale neural networks. Here, we present OZ, a programable analog implementation of NEF-inspired spiking neurons. OZ neurons can be dynamically programmed to feature varying high-dimensional response curves with positive and negative encoders for a neuromorphic distributed representation of normalized input data. Our hardware design demonstrates full correspondence with NEF across firing rates, encoding vectors, and intercepts. OZ neurons can be independently configured in real-time to allow efficient spanning of a representation space, thus using fewer neurons and therefore less power for neuromorphic data representation.
C1 [Hazan, Avi; Ezra Tsur, Elishai] Open Univ Israel, Dept Math & Comp Sci, Neurobiomorph Engn Lab, Raanana, Israel.
RP Tsur, EE (corresponding author), Open Univ Israel, Dept Math & Comp Sci, Neurobiomorph Engn Lab, Raanana, Israel.
EM elishai@NBEL-lab.com
CR Aamir S.A., 2017, P IEEE BIOM CIRC SYS, DOI [10.1109/BIOCAS.2017.8325167, DOI 10.1109/BIOCAS.2017.8325167]
   Amara A, 2006, MICROELECTRON J, V37, P669, DOI 10.1016/j.mejo.2005.11.003
   Analog Devices, 2008, LTSPICE SIM
   Ankri L, 2020, CELL REP, V31, DOI 10.1016/j.celrep.2020.107608
   Bartolozzi C, 2007, NEURAL COMPUT, V19, P2581, DOI 10.1162/neco.2007.19.10.2581
   Bekolay T, 2014, FRONT NEUROINFORM, V7, DOI 10.3389/fninf.2013.00048
   Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Boahen K, 2017, COMPUT SCI ENG, V19, P14, DOI 10.1109/MCSE.2017.33
   Burkitt AN, 2006, BIOL CYBERN, V95, P1, DOI 10.1007/s00422-006-0068-6
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   DeBole MV, 2019, COMPUTER, V52, P20, DOI 10.1109/MC.2019.2903009
   DeWolf T, 2020, FRONT NEUROROBOTICS, V14, DOI 10.3389/fnbot.2020.568359
   Eliasmith C., 2003, NEURAL ENG COMPUTATI
   Eliasmith C, 2014, CURR OPIN NEUROBIOL, V25, P1, DOI 10.1016/j.conb.2013.09.009
   Eliasmith C, 2012, SCIENCE, V338, P1202, DOI 10.1126/science.1225266
   Fischl KD, 2018, BIOMED CIRC SYST C, P587
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   Gosmann J, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0149928
   Indiveri G, 2000, SCIENCE, V288, P1189, DOI 10.1126/science.288.5469.1189
   Indiveri G, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00073
   Krestinskaya O, 2020, IEEE T NEUR NET LEAR, V31, P4, DOI 10.1109/TNNLS.2019.2899262
   Krichmar J.L., 2011, NEUROMORPHIC BRAIN B, DOI [10.1017/CBO9780511994838, DOI 10.1017/CBO9780511994838]
   Lin CK, 2018, COMPUTER, V51, P52, DOI 10.1109/MC.2018.157113521
   Liu SC, 2015, EVENT-BASED NEUROMORPHIC SYSTEMS, P1, DOI 10.1002/9781118927601
   Liu SC, 2010, CURR OPIN NEUROBIOL, V20, P288, DOI 10.1016/j.conb.2010.03.007
   Mayr CG, 2014, FRONT NEUROSCI-SWITZ, V8, DOI 10.3389/fnins.2014.00201
   MEAD CA, 1989, ADVANCED RESEARCH IN VLSI : PROCEEDINGS OF THE DECENNIAL CALTECH CONFERENCE ON VLSI, P1
   Merolla P, 2004, ADV NEUR IN, V16, P995
   Mundy A, 2015, IEEE IJCNN
   NICHOLS KG, 1994, IEE P-CIRC DEV SYST, V141, P242, DOI 10.1049/ip-cds:19941246
   Pederson D., 1973, M382 UCBERL
   Stewart TC, 2014, P IEEE, V102, P881, DOI 10.1109/JPROC.2014.2306061
   Tripathi A., 2019, P IEEE INT S CIRC SY, DOI [10.1109/ISCAS.2019.8702492, DOI 10.1109/ISCAS.2019.8702492]
   Tsur EE, 2020, NEUROCOMPUTING, V374, P54, DOI 10.1016/j.neucom.2019.09.072
   van Schaik A, 2001, NEURAL NETWORKS, V14, P617, DOI 10.1016/S0893-6080(01)00067-3
   Wang RC, 2017, IEEE T BIOMED CIRC S, V11, P574, DOI 10.1109/TBCAS.2017.2666883
   Yang SM, 2020, IEEE T NEUR NET LEAR, V31, P148, DOI 10.1109/TNNLS.2019.2899936
   Yang SM, 2018, NEUROCOMPUTING, V314, P394, DOI 10.1016/j.neucom.2018.07.006
   Yang SM, 2019, IEEE T CYBERNETICS, V49, P2490, DOI 10.1109/TCYB.2018.2823730
   Zaidel Y, 2021, FRONT NEUROROBOTICS, V15, DOI 10.3389/fnbot.2021.631159
   Zhang WQ, 2020, NAT ELECTRON, V3, P371, DOI 10.1038/s41928-020-0435-7
NR 41
TC 11
Z9 11
U1 0
U2 9
PD FEB 22
PY 2021
VL 15
AR 627221
DI 10.3389/fnins.2021.627221
WC Neurosciences
DA 2023-11-11
ER

PT C
AU Tan, J
   Lim, JH
   Kwon, JH
   Naik, VB
   Raghavan, N
   Pey, KL
AF Tan, J.
   Lim, J. H.
   Kwon, J. H.
   Naik, V. B.
   Raghavan, N.
   Pey, K. L.
GP IEEE
TI Backhopping-based STT-MRAM Poisson Spiking Neuron for Neuromorphic
   Computation
SO 2023 IEEE INTERNATIONAL RELIABILITY PHYSICS SYMPOSIUM, IRPS
SE International Reliability Physics Symposium
DT Proceedings Paper
CT 61st IEEE International Reliability Physics Symposium (IRPS)
CY MAR 26-30, 2023
CL Monterey, CA
DE Backhopping; Neuromorphic; Poisson; Spiking Neural Network; STT-MRAM
AB Spin-transfer-torque magnetic random-access memory (STT-MRAM) is a proven technology for embedded non-volatile memory applications. The backhopping phenomena in STT-MRAM, whereby the resistance of the device oscillates under higher current, has been recently explored for emerging spiking neural network applications. We report a detailed characterization of backhopping in foundry compatible STTMRAM having similar to 15kb bit-cell arrays by analyzing the behavior of backhopping spike rate versus applied current and temperature. Our study shows that the backhopping in STT-MRAM exhibits the Poisson statistics with a controllable spike rate with current that displays three regimes: non-backhopping, exponential and linear. This mimics the behavior of a rectified linear unit (ReLU) neuron, a commonly used activation function in deep learning models. A spiking neural network (SNN) communication channel is simulated using the derived statistics and a first principles mathematical framework to analyze the reliability performance of backhopping- based SNN in terms of trading-off the accuracy and applied current.
C1 [Tan, J.; Raghavan, N.; Pey, K. L.] Singapore Univ Technol & Design, 8 Somapah Rd, Singapore 487372, Singapore.
   [Tan, J.; Lim, J. H.; Kwon, J. H.; Naik, V. B.] GLOBALFOUNDRIES Singapore Pte Ltd, 60 Woodlands Ind Pk D St 2, Singapore 738406, Singapore.
RP Tan, J (corresponding author), Singapore Univ Technol & Design, 8 Somapah Rd, Singapore 487372, Singapore.; Tan, J (corresponding author), GLOBALFOUNDRIES Singapore Pte Ltd, 60 Woodlands Ind Pk D St 2, Singapore 738406, Singapore.
EM joel.tan@globalfoundries.com
CR Apicella A, 2021, NEURAL NETWORKS, V138, P14, DOI 10.1016/j.neunet.2021.01.026
   Boahen K, 2017, COMPUT SCI ENG, V19, P14, DOI 10.1109/MCSE.2017.33
   Buhrmester V, 2021, MACH LEARN KNOW EXTR, V3, P966, DOI 10.3390/make3040048
   Butts DA, 2006, PLOS BIOL, V4, P639, DOI 10.1371/journal.pbio.0040092
   Clevert DA, 2016, Arxiv, DOI [arXiv:1511.07289, DOI 10.48550/ARXIV.1511.07289]
   David M., 2021, 2021 Conference on Lasers and Electro-Optics Europe & European Quantum Electronics Conference (CLEO/Europe-EQEC), DOI 10.1109/CLEO/Europe-EQEC52157.2021.9542741
   El-Sayed SA, 2020, IEEE INT ON LINE, DOI 10.1109/iolts50870.2020.9159745
   Eliasmith C., 2003, NEURAL ENG COMPUTATI
   Furui S, 2012, IEEE SIGNAL PROC MAG, V29, P16, DOI 10.1109/MSP.2012.2209906
   Krizhevsky V., CIFAR 10 CANADIAN I
   Lee K, 2021, INT EL DEVICES MEET, DOI 10.1109/IEDM19574.2021.9720537
   Lee TY, 2020, INT EL DEVICES MEET, DOI 10.1109/IEDM13553.2020.9372015
   Liang F.-X., 2020, 2020 INT S VLSI TECH, P151, DOI [DOI 10.1109/VLSI-TSA48913.2020.9203701, 10.1109/VLSI-TSA48913.2020.9203701]
   Liang FX, 2021, ADV INTELL SYST-GER, V3, DOI 10.1002/aisy.202100007
   Naik VB, 2020, INT EL DEVICES MEET, DOI 10.1109/IEDM13553.2020.9371935
   Pei J, 2019, NATURE, V572, P106, DOI 10.1038/s41586-019-1424-8
   Shih YC, 2020, INT EL DEVICES MEET, DOI 10.1109/IEDM13553.2020.9372115
   Vatajelu EI, 2019, IEEE VLSI TEST SYMP
   Wu M. -H., 2020, S VLSI TECH, P1, DOI 10.1109/VLSITechnology18217.2020.9265033
   Wu MH, 2022, PHYS REV APPL, V18, DOI 10.1103/PhysRevApplied.18.064034
   Wu MH, 2019, S VLSI TECH, pT34, DOI [10.23919/VLSIT.2019.8776569, 10.23919/vlsit.2019.8776569]
NR 21
TC 0
Z9 0
U1 0
U2 0
PY 2023
DI 10.1109/IRPS48203.2023.10118343
WC Engineering, Multidisciplinary; Engineering, Electrical & Electronic;
   Physics, Applied
DA 2023-11-11
ER

PT J
AU Xue, XH
   Wimmer, RD
   Halassa, MM
   Chen, ZS
AF Xue, Xiaohe
   Wimmer, Ralf D.
   Halassa, Michael M.
   Chen, Zhe Sage
TI Spiking Recurrent Neural Networks Represent Task-Relevant Neural
   Sequences in Rule-Dependent Computation
SO COGNITIVE COMPUTATION
DT Article; Early Access
DE Prefrontal cortex; Spiking recurrent neural network; Neural sequence;
   Neural oscillations
ID DYNAMICS; TIME
AB Prefrontal cortical neurons play essential roles in performing rule-dependent tasks and working memory-based decision making. Motivated by PFC recordings of task-performing mice, we developed an excitatory-inhibitory spiking recurrent neural network (SRNN) to perform a rule-dependent two-alternative forced choice (2AFC) task. We imposed several important biological constraints onto the SRNN and adapted spike frequency adaptation (SFA) and SuperSpike gradient methods to train the SRNN efficiently. The trained SRNN produced emergent rule-specific tunings in single-unit representations, showing rule-dependent population dynamics that resembled experimentally observed data. Under various test conditions, we manipulated the SRNN parameters or configuration in computer simulations, and we investigated the impacts of rulecoding error, delay duration, recurrent weight connectivity and sparsity, and excitation/inhibition (E/I) balance on both task performance and neural representations. Overall, our modeling study provides a computational framework to understand neuronal representations at a fine timescale during working memory and cognitive control and provides new experimentally testable hypotheses in future experiments.
C1 [Xue, Xiaohe] NYU, Courant Inst Math Sci, New York, NY 10016 USA.
   [Wimmer, Ralf D.; Halassa, Michael M.] MIT, Dept Brain & Cognit Sci, E25-618, Cambridge, MA 02139 USA.
   [Chen, Zhe Sage] NYU, Dept Psychiat, Sch Med, One Pk Ave,Rm 8-226, New York, NY 10016 USA.
   [Chen, Zhe Sage] NYU, Dept Neurosci & Physiol, Sch Med, New York, NY 10016 USA.
   [Chen, Zhe Sage] NYU, Neurosci Inst, Sch Med, New York, NY 10016 USA.
RP Chen, ZS (corresponding author), NYU, Dept Psychiat, Sch Med, One Pk Ave,Rm 8-226, New York, NY 10016 USA.; Chen, ZS (corresponding author), NYU, Dept Neurosci & Physiol, Sch Med, New York, NY 10016 USA.; Chen, ZS (corresponding author), NYU, Neurosci Inst, Sch Med, New York, NY 10016 USA.
EM zhe.chen@nyulangone.org
CR Bellec G., 2018, ADV NEURAL INFORM PR, P787
   Bellec G, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-17236-y
   Bolkan SS, 2017, NAT NEUROSCI, V20, P987, DOI 10.1038/nn.4568
   Bush D, 2010, PLOS COMPUT BIOL, V6, DOI 10.1371/journal.pcbi.1000839
   Fiete IR, 2010, NEURON, V65, P563, DOI 10.1016/j.neuron.2010.02.003
   Fujisawa S, 2008, NAT NEUROSCI, V11, P823, DOI 10.1038/nn.2134
   Gillett M, 2020, P NATL ACAD SCI USA, V117, P29948, DOI 10.1073/pnas.1918674117
   Glorot X., 2010, P JMLR WORKSH C P 13, P249, DOI DOI 10.1177/1753193409103364.
   Goudar V, 2018, ELIFE, V7, DOI 10.7554/eLife.31134
   Hao YZ, 2020, NEURAL NETWORKS, V121, P387, DOI 10.1016/j.neunet.2019.09.007
   Hardy NF, 2018, NEURAL COMPUT, V30, P378, DOI [10.1162/NECO_a_01041, 10.1162/neco_a_01041]
   Harvey CD, 2012, NATURE, V484, P62, DOI 10.1038/nature10918
   Ingrosso A, 2019, PLOS ONE, V14, DOI 10.1371/journal.pone.0220547
   Kao JC, 2019, J NEUROPHYSIOL, V122, P2504, DOI 10.1152/jn.00467.2018
   Kim CM, 2021, NEURAL COMPUT, V33, P1199, DOI 10.1162/neco_a_01379
   Kim R, 2021, NAT NEUROSCI, V24, P129, DOI 10.1038/s41593-020-00753-w
   Kim R, 2019, P NATL ACAD SCI USA, V116, P22811, DOI 10.1073/pnas.1905926116
   King DB, 2015, ACS SYM SER, V1214, P1
   Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114
   Lee C, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00435
   Li YH, 2021, NEURAL COMPUT, V33, P3264, DOI 10.1162/neco_a_01409
   Lillicrap TP, 2020, NAT REV NEUROSCI, V21, P335, DOI 10.1038/s41583-020-0277-3
   Lindén H, 2014, FRONT NEUROINFORM, V7, DOI 10.3389/fninf.2013.00041
   Lobov SA, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00088
   Lundqvist M, 2018, J NEUROSCI, V38, P7013, DOI 10.1523/JNEUROSCI.2485-17.2018
   Lundqvist M, 2016, NEURON, V90, P152, DOI 10.1016/j.neuron.2016.02.028
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maes A, 2020, PLOS COMPUT BIOL, V16, DOI [10.1371/journal.pcbi.1007606, 10.1371/journal.pcbi.1007606.r001, 10.1371/journal.pcbi.1007606.r002, 10.1371/journal.pcbi.1007606.r003, 10.1371/journal.pcbi.1007606.r004, 10.1371/journal.pcbi.1007606.r005, 10.1371/journal.pcbi.1007606.r006]
   Maheswaranathan N, 2019, ADV NEUR IN, V32
   Mante V, 2013, NATURE, V503, P78, DOI 10.1038/nature12742
   Marton TF, 2018, J NEUROSCI, V38, P2569, DOI 10.1523/JNEUROSCI.1728-17.2018
   Mazzoni A, 2015, PLOS COMPUT BIOL, V11, DOI 10.1371/journal.pcbi.1004584
   Miller EK, 2018, NEURON, V100, P463, DOI 10.1016/j.neuron.2018.09.023
   Mukherjee A, 2021, NATURE, V600, P100, DOI 10.1038/s41586-021-04056-3
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Nicola W, 2017, NAT COMMUN, V8, DOI 10.1038/s41467-017-01827-3
   Okun M., 2009, SCHOLARPEDIA, V4, P7467, DOI DOI 10.4249/SCHOLARPEDIA.7467
   Orhan AE, 2019, NAT NEUROSCI, V22, P275, DOI 10.1038/s41593-018-0314-y
   Panda P, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00693
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Rajakumar A, 2021, NEURAL COMPUT, V33, P2603, DOI 10.1162/neco_a_01418
   Rajan K, 2006, PHYS REV LETT, V97, DOI 10.1103/PhysRevLett.97.188104
   Rajan K, 2016, NEURON, V90, P128, DOI 10.1016/j.neuron.2016.02.009
   Rezende DJ, 2014, FRONT COMPUT NEUROSC, V8, DOI 10.3389/fncom.2014.00038
   Rikhye RV, 2018, NAT NEUROSCI, V21, P1753, DOI 10.1038/s41593-018-0269-z
   Schmitt LI, 2017, NATURE, V545, P219, DOI 10.1038/nature22073
   Shenoy KV, 2021, NAT COMMUN, V12, DOI 10.1038/s41467-020-20371-1
   Shrestha SB, 2015, NEURAL NETWORKS, V63, P185, DOI 10.1016/j.neunet.2014.12.001
   Song HF, 2016, PLOS COMPUT BIOL, V12, DOI 10.1371/journal.pcbi.1004792
   Song S, 2001, NEURON, V32, P339, DOI 10.1016/S0896-6273(01)00451-2
   Sporea I, 2013, NEURAL COMPUT, V25, P473, DOI 10.1162/NECO_a_00396
   Stokes MG, 2013, NEURON, V78, P364, DOI 10.1016/j.neuron.2013.01.039
   Sussillo D, 2015, NAT NEUROSCI, V18, P1025, DOI 10.1038/nn.4042
   Sussillo D, 2009, NEURON, V63, P544, DOI 10.1016/j.neuron.2009.07.018
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Yang GR, 2019, NAT NEUROSCI, V22, P297, DOI 10.1038/s41593-018-0310-2
   Zenke F, 2017, PR MACH LEARN RES, V70
   Zenke F, 2018, NEURAL COMPUT, V30, P1514, DOI 10.1162/neco_a_01086
   Zhang XH, 2021, ISCIENCE, V24, DOI 10.1016/j.isci.2021.102919
NR 60
TC 2
Z9 2
U1 3
U2 16
PD 2022 FEB 5
PY 2022
DI 10.1007/s12559-022-09994-2
EA FEB 2022
WC Computer Science, Artificial Intelligence; Neurosciences
DA 2023-11-11
ER

PT J
AU Hwang, S
   Hwang, Y
   Kim, D
   Lee, J
   Choe, HK
   Lee, J
   Kang, H
   Kung, J
AF Hwang, Sangwoo
   Hwang, Yujin
   Kim, Duhee
   Lee, Junhee
   Choe, Han Kyoung
   Lee, Junghyup
   Kang, Hongki
   Kung, Jaeha
TI ReplaceNet: real-time replacement of a biological neural circuit with a
   hardware-assisted spiking neural network
SO FRONTIERS IN NEUROSCIENCE
DT Article
DE brain-chip interface; dynamic synapses; hardware implementation; spiking
   neural network; online learning
ID DEPENDENT SYNAPTIC PLASTICITY; LEARNING ALGORITHM; MODEL
AB Recent developments in artificial neural networks and their learning algorithms have enabled new research directions in computer vision, language modeling, and neuroscience. Among various neural network algorithms, spiking neural networks (SNNs) are well-suited for understanding the behavior of biological neural circuits. In this work, we propose to guide the training of a sparse SNN in order to replace a sub-region of a cultured hippocampal network with limited hardware resources. To verify our approach with a realistic experimental setup, we record spikes of cultured hippocampal neurons with a microelectrode array (in vitro). The main focus of this work is to dynamically cut unimportant synapses during SNN training on the fly so that the model can be realized on resource-constrained hardware, e.g., implantable devices. To do so, we adopt a simple STDP learning rule to easily select important synapses that impact the quality of spike timing learning. By combining the STDP rule with online supervised learning, we can precisely predict the spike pattern of the cultured network in real-time. The reduction in the model complexity, i.e., the reduced number of connections, significantly reduces the required hardware resources, which is crucial in developing an implantable chip for the treatment of neurological disorders. In addition to the new learning algorithm, we prototype a sparse SNN hardware on a small FPGA with pipelined execution and parallel computing to verify the possibility of real-time replacement. As a result, we can replace a sub-region of the biological neural circuit within 22 mu s using 2.5 x fewer hardware resources, i.e., by allowing 80% sparsity in the SNN model, compared to the fully-connected SNN model. With energy-efficient algorithms and hardware, this work presents an essential step toward real-time neuroprosthetic computation.
C1 [Hwang, Sangwoo; Hwang, Yujin; Kim, Duhee; Lee, Junhee; Lee, Junghyup; Kang, Hongki] DGIST, Dept Elect Engn & Comp Sci, Daegu, South Korea.
   [Choe, Han Kyoung] DGIST, Dept Brain Sci, Daegu, South Korea.
   [Kung, Jaeha] Korea Univ, Sch Elect Engn, Seoul, South Korea.
RP Kung, J (corresponding author), Korea Univ, Sch Elect Engn, Seoul, South Korea.
EM jhkung@korea.ac.kr
CR Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   Baek E, 2019, MICRO'52: THE 52ND ANNUAL IEEE/ACM INTERNATIONAL SYMPOSIUM ON MICROARCHITECTURE, P304, DOI 10.1145/3352460.3358268
   Belle AM, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-28950-5
   Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Bing Han, 2020, Computer Vision - ECCV 2020. 16th European Conference. Proceedings. Lecture Notes in Computer Science (LNCS 12355), P388, DOI 10.1007/978-3-030-58607-2_23
   Bing Han, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13555, DOI 10.1109/CVPR42600.2020.01357
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Bruzzone A, 2015, IEEE ENG MED BIO, P3391, DOI 10.1109/EMBC.2015.7319120
   Buccelli S, 2019, ISCIENCE, V19, P402, DOI 10.1016/j.isci.2019.07.046
   Cheung K, 2016, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00516
   Chou ZN, 2015, IEEE ENG MED BIO, P3949, DOI 10.1109/EMBC.2015.7319258
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Doborjeh Z, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-42863-x
   Dominguez-Morales JP, 2021, NEUROCOMPUTING, V449, P422, DOI 10.1016/j.neucom.2021.03.109
   Dong Song, 2018, IEEE Transactions on Neural Systems and Rehabilitation Engineering, V26, P272, DOI 10.1109/TNSRE.2016.2604423
   Fang W., 2021, P NEURIPS
   Hampson RE, 2018, J NEURAL ENG, V15, DOI 10.1088/1741-2552/aaaed7
   Kistler WM, 2002, BIOL CYBERN, V87, P416, DOI 10.1007/s00422-002-0359-5
   Kumarasinghe K, 2021, SCI REP-UK, V11, DOI 10.1038/s41598-021-81805-4
   Lee D, 2018, CONF PROC INT SYMP C, P275, DOI 10.1109/ISCA.2018.00032
   Lee WW, 2019, SCI ROBOT, V4, DOI 10.1126/scirobotics.aax2198
   Li W., 2021, P EMBC GUAD, P6198, DOI [10.1109/EMBC46164.2021.9630019, DOI 10.1109/EMBC46164.2021.9630019]
   Lobo JL, 2020, NEURAL NETWORKS, V121, P88, DOI 10.1016/j.neunet.2019.09.004
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   Mohemmed A, 2013, NEUROCOMPUTING, V107, P3, DOI 10.1016/j.neucom.2012.08.034
   Pfister JP, 2006, J NEUROSCI, V26, P9673, DOI 10.1523/JNEUROSCI.1425-06.2006
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Richards BA, 2019, NAT NEUROSCI, V22, P1761, DOI 10.1038/s41593-019-0520-2
   Roberts PD, 2002, BIOL CYBERN, V87, P392, DOI 10.1007/s00422-002-0361-y
   Schreiber S, 2003, NEUROCOMPUTING, V52-4, P925, DOI 10.1016/S0925-2312(02)00838-X
   She XW, 2022, J NEUROSCI METH, V370, DOI 10.1016/j.jneumeth.2022.109492
   Song D, 2007, IEEE T BIO-MED ENG, V54, P1053, DOI 10.1109/TBME.2007.891948
   Sripad A, 2018, NEURAL NETWORKS, V97, P28, DOI 10.1016/j.neunet.2017.09.011
   Sun JJ, 2010, EUR J NEUROSCI, V32, P1289, DOI 10.1111/j.1460-9568.2010.07383.x
   Wang XW, 2020, NEURAL NETWORKS, V125, P258, DOI 10.1016/j.neunet.2020.02.011
   Xu Y, 2013, NEURAL COMPUT, V25, P1472, DOI 10.1162/NECO_a_00450
   Xu Y, 2013, NEURAL NETWORKS, V43, P99, DOI 10.1016/j.neunet.2013.02.003
   Yang SM, 2022, IEEE T NEUR NET LEAR, V33, P2801, DOI 10.1109/TNNLS.2020.3045492
   Yang SM, 2022, IEEE T NEUR NET LEAR, V33, P4398, DOI [10.1109/TNNLS.2021.3057070, 10.4018/IJCINI.20211001.oa2]
   Yang SM, 2020, IEEE T NEUR NET LEAR, V31, P148, DOI 10.1109/TNNLS.2019.2899936
   Yu QF, 2013, PLOS ONE, V8, DOI [10.1371/journal.pone.0059627, 10.1371/journal.pone.0078318]
   Zeldenrust F, 2018, FRONT COMPUT NEUROSC, V12, DOI 10.3389/fncom.2018.00048
   Zhang ML, 2019, IEEE T NEUR NET LEAR, V30, P123, DOI 10.1109/TNNLS.2018.2833077
   Zhang ML, 2018, IEEE T COGN DEV SYST, V10, P151, DOI 10.1109/TCDS.2017.2651943
   Zhang ML, 2020, NAT ELECTRON, V3, P191, DOI 10.1038/s41928-020-0390-3
   Zheng H., 2021, P AAAI, DOI [10.1609/aaai.v35i12.17320, DOI 10.1609/AAAI.V35I12.17320]
NR 47
TC 0
Z9 0
U1 1
U2 1
PD AUG 10
PY 2023
VL 17
AR 1161592
DI 10.3389/fnins.2023.1161592
WC Neurosciences
DA 2023-11-11
ER

PT J
AU Strain, TJ
   McDaid, LJ
   McGinnity, TM
   Maguire, LP
   Sayers, HM
AF Strain, T. J.
   McDaid, L. J.
   McGinnity, T. M.
   Maguire, L. P.
   Sayers, H. M.
TI AN STDP TRAINING ALGORITHM FOR A SPIKING NEURAL NETWORK WITH DYNAMIC
   THRESHOLD NEURONS
SO INTERNATIONAL JOURNAL OF NEURAL SYSTEMS
DT Article
DE STDP; supervised learning; dynamic threshold neurons; SNNs
ID SPECIFICITY
AB This paper proposes a supervised training algorithm for Spiking Neural Networks (SNNs) which modifies the Spike Timing Dependent Plasticity (STDP) learning rule to support both local and network level training with multiple synaptic connections and axonal delays. The training algorithm applies the rule to two and three layer SNNs, and is benchmarked using the Iris and Wisconsin Breast Cancer (WBC) data sets. The effectiveness of hidden layer dynamic threshold neurons is also investigated and results are presented.
C1 [Strain, T. J.; McDaid, L. J.; McGinnity, T. M.; Maguire, L. P.; Sayers, H. M.] Univ Ulster, Intelligent Syst Res Ctr, Sch Comp & Intelligent Syst, Derry BT48 7JL, North Ireland.
RP McDaid, LJ (corresponding author), Univ Ulster, Intelligent Syst Res Ctr, Sch Comp & Intelligent Syst, Magee Campus, Derry BT48 7JL, North Ireland.
EM lj.mcdaid@ulster.ac.uk
CR BELATRECHE A, 2003, JCIS 2003, P1524
   Belatreche A, 2003, P IEEE CYB INT CHALL, P39
   BELATRECHE A, 2005, SOFT COMPUT, V11, P239
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   BIENENSTOCK EL, 1982, J NEUROSCI, V2, P32, DOI 10.1523/jneurosci.02-01-00032.1982
   Bishop C.M., 1995, NEURAL NETWORKS PATT
   Bohte S. M., 2000, 8th European Symposium on Artificial Neural Networks. ESANN"2000. Proceedings, P419
   Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   CLIFTON CR, 2004, J NEUROPHYSIOL, V91, P2273
   Eggert J, 2001, NEUROCOMPUTING, V38, P191, DOI 10.1016/S0925-2312(01)00505-7
   Ghosh-Dastidar S, 2007, INTEGR COMPUT-AID E, V14, P187
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Haykin S., 2004, NEURAL NETWORKS COMP, V2, P41
   HEBB D. O., 1949
   Iglesias J, 2008, INT J NEURAL SYST, V18, P267, DOI 10.1142/S0129065708001580
   *IST, IST200134712
   Kempter R, 1999, PHYS REV E, V59, P4498, DOI 10.1103/PhysRevE.59.4498
   Maass W, 1997, NEURAL COMPUT, V9, P279, DOI 10.1162/neco.1997.9.2.279
   Maass W, 1997, ADV NEUR IN, V9, P211
   Natschlager T, 1998, NETWORK-COMP NEURAL, V9, P319, DOI 10.1088/0954-898X/9/3/003
   Nishiyama M, 2000, NATURE, V408, P584, DOI 10.1038/35046067
   Panchev C., 2001, P WORLD C NEUR AUSTR, P378
   Ripley B. D., 1996, PATTERN RECOGNITION
   Roberts PD, 2002, BIOL CYBERN, V87, P392, DOI 10.1007/s00422-002-0361-y
   Rossello JL, 2009, INT J NEURAL SYST, V19, P465, DOI 10.1142/S0129065709002166
   Senn W, 2002, NEURAL COMPUT, V14, P583, DOI 10.1162/089976602317250915
   Shouval HZ, 2002, P NATL ACAD SCI USA, V99, P10831, DOI 10.1073/pnas.152343099
   Song S, 2001, NEURON, V32, P339, DOI 10.1016/S0896-6273(01)00451-2
   Song S, 2000, NEUROCOMPUTING, V32, P523, DOI 10.1016/S0925-2312(00)00208-3
   STRAIN TJ, 2004, IEEE SMC UK RI 04, P202
   van Rossum MCW, 2000, J NEUROSCI, V20, P8812
   WU QX, 2003, NEUROCOMPUTING, V55, P523
NR 33
TC 26
Z9 27
U1 1
U2 15
PD DEC
PY 2010
VL 20
IS 6
BP 463
EP 480
DI 10.1142/S0129065710002553
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT S
AU Aertsen, A
   Diesmann, M
   Gewaltig, MO
   Grün, S
   Rotter, S
AF Aertsen, A
   Diesmann, M
   Gewaltig, MO
   Grün, S
   Rotter, S
BE Bock, G
   Goode, J
TI Neural dynamics in cortical networks -: precision of joint-spiking
   events
SO COMPLEXITY IN BIOLOGICAL INFORMATION PROCESSING
SE NOVARTIS FOUNDATION SYMPOSIUM
DT Article; Proceedings Paper
CT Symposium on Complexity in Biological Information Processing
CY JUL 04-06, 2000
CL BERLIN, GERMANY
ID VISUAL-CORTEX; NEOCORTICAL NEURONS; SYNCHRONOUS SPIKING; BEHAVING
   MONKEYS; AUDITORY-CORTEX; FRONTAL-CORTEX; MOTOR CORTEX; SYNCHRONIZATION;
   OSCILLATIONS; INTEGRATION
AB Electrophysiological studies of cortical function on the basis of multiple single-neuron recordings reveal neuronal interactions which depend on stimulus context and behavioural events. These interactions exhibit dynamics on different time scales, with time constants down to the millisecond range. Mechanisms underlying such dynamic organization of the cortical network were investigated by experimental and theoretical approaches, We review some recent results from these studies, concentrating on the occurrence of precise joint-spiking events in cortical activity, both in physiological and in model neural networks. These findings suggest that a combinatorial neural code, based on rapid associations of groups of neurons co-ordinating their activity at the single spike level, is biologically feasible.
C1 Univ Freiburg, Inst Biol 3, D-7800 Freiburg, Germany.
   MPI Strommungsforsch, Dept Nonlinear Dynam, Gottingen, Germany.
   Honda R&D Europe, Future Technol Res, Offenbach, Germany.
   MPI Brain Res, Dept Neurophysiol, Frankfurt, Germany.
RP Aertsen, A (corresponding author), Univ Freiburg, Inst Biol 3, Schanzlestr 1, D-7800 Freiburg, Germany.
CR ABELES M, 1982, ISRAEL J MED SCI, V18, P83
   ABELES M, 1993, J NEUROPHYSIOL, V70, P1629, DOI 10.1152/jn.1993.70.4.1629
   ABELES M, 1993, BRAIN THEORY, P149
   Abeles M., 1982, LOCAL CORTICAL CIRCU, DOI DOI 10.1007/978-3-642-81708-3
   Aertsen A, 1996, J PHYSIOLOGY-PARIS, V90, P243, DOI 10.1016/S0928-4257(97)81432-5
   AERTSEN A, 1994, PHYSICA D, V75, P103, DOI 10.1016/0167-2789(94)90278-X
   AERTSEN A, 1991, J HIRNFORSCH, V32, P735
   Aertsen A, 1993, Curr Opin Neurobiol, V3, P586, DOI 10.1016/0959-4388(93)90060-C
   AHISSAR M, 1992, J NEUROPHYSIOL, V67, P203, DOI 10.1152/jn.1992.67.1.203
   [Anonymous], 1991, ANATOMY CORTEX
   [Anonymous], UNITARY JOINT EVENTS
   [Anonymous], 1991, CORTICONICS
   [Anonymous], 1986, BRAIN THEORY
   Arieli A., 1996, Society for Neuroscience Abstracts, V22, P2022
   Arieli A, 1996, SCIENCE, V273, P1868, DOI 10.1126/science.273.5283.1868
   Azouz R, 1999, J NEUROSCI, V19, P2209
   Barlow H B, 1972, Perception, V1, P371, DOI 10.1068/p010371
   Barlow H.B., 1992, INFORMATION PROCESSI, P169
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   BOVEN KH, 1990, PARALLEL PROCESSING IN NEURAL SYSTEMS AND COMPUTERS, P53
   Collins JJ, 1996, PHYS REV E, V54, P5575, DOI 10.1103/PhysRevE.54.5575
   deCharms RC, 1996, NATURE, V381, P610, DOI 10.1038/381610a0
   Diesmann M, 1999, NATURE, V402, P529, DOI 10.1038/990101
   DIESMANN M, 2000, SOC NEUR ABSTR, V26, P2201
   Diesmann M., 1995, GCAA953 WEIZM I SCI
   Diesmann M, 1996, COMPUTATIONAL NEUROS, P59
   ECKHORN R, 1988, BIOL CYBERN, V60, P121, DOI 10.1007/BF00202899
   EGGERMONT JJ, 1994, J NEUROPHYSIOL, V71, P246, DOI 10.1152/jn.1994.71.1.246
   ENGEL AK, 1992, TRENDS NEUROSCI, V15, P218, DOI 10.1016/0166-2236(92)90039-B
   Fetz EE, 1997, SCIENCE, V278, P1901, DOI 10.1126/science.278.5345.1901
   GEORGOPOULOS AP, 1993, SCIENCE, V260, P47, DOI 10.1126/science.8465199
   GERSTEIN GL, 1989, IEEE T BIO-MED ENG, V36, P4, DOI 10.1109/10.16444
   GEWALTIG MO, 1999, EVOLUTION SYNCHRONOU
   GEWALTIG MO, 1997, MEMBRANE MIND, P620
   GEWALTIG MO, 2000, IN PRESS NEUROCOMPUT
   GEWALTIG MO, 2001, IN PRESS NEURAL NERW
   GRAY CM, 1989, NATURE, V338, P334, DOI 10.1038/338334a0
   Grün S, 1999, J NEUROSCI METH, V94, P67, DOI 10.1016/S0165-0270(99)00126-0
   GRUN S, 2001, NEURAL COMPUT
   GRUN S, 1994, P 17 M EUR NEUR ASS, P11
   GRUN S, 2001, IN PRESS NEURAL COMP
   GUTIG R, 2001, IN PRESS NEURAL COMP
   GUTIG R, 2001, IN PRESS GOTT NEUR R
   Hebb D. O., 1949, ORG BEHAV A NEUROPSY
   HEHL U, 2001, IN PRESS GOTTINGEN N
   Hellwig B, 2000, BIOL CYBERN, V82, P111, DOI 10.1007/PL00007964
   HUBEL DH, 1977, PROC R SOC SER B-BIO, V198, P1, DOI 10.1098/rspb.1977.0085
   MAINEN ZF, 1995, SCIENCE, V268, P1503, DOI 10.1126/science.7770778
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   MOHNS M, 1999, MOL NEUROBIOLOGY CLI, P100
   MURTHY VN, 1992, P NATL ACAD SCI USA, V89, P5670, DOI 10.1073/pnas.89.12.5670
   NEWSOME WT, 1989, NATURE, V341, P52, DOI 10.1038/341052a0
   NICOLELIS MAL, 1995, SCIENCE, V268, P1353, DOI 10.1126/science.7761855
   Nowak LG, 1997, CEREB CORTEX, V7, P487, DOI 10.1093/cercor/7.6.487
   Palm G., 1990, CONCEPT NEUROSCI, V1, P133
   PERKEL DH, 1968, NEUROSCI RES PROGRAM, V6
   Prut Y, 1998, J NEUROPHYSIOL, V79, P2857, DOI 10.1152/jn.1998.79.6.2857
   Riehle A, 1997, SCIENCE, V278, P1950, DOI 10.1126/science.278.5345.1950
   Roelfsema PR, 1996, J COGNITIVE NEUROSCI, V8, P603, DOI 10.1162/jocn.1996.8.6.603
   Rotter S, 1996, NATO ADV SCI I A-LIF, V289, P355
   ROTTER S, 2000, CHAOS BRAIN, P3
   ROTTER S, 1994, WECHSELWIRKENDE STOC
   ROTTER S, 1997, MEMBRANE MIND, P622
   Rotter Stefan, 1995, P69
   Rubin J, 2001, PHYS REV LETT, V86, P364, DOI 10.1103/PhysRevLett.86.364
   Sakurai Y, 1996, NEUROSCI RES, V26, P1
   SANES JN, 1993, P NATL ACAD SCI USA, V90, P4470, DOI 10.1073/pnas.90.10.4470
   SINGER W, 1993, ANNU REV PHYSIOL, V55, P349, DOI 10.1146/annurev.physiol.55.1.349
   SINGER W, 1995, ANNU REV NEUROSCI, V18, P555, DOI 10.1146/annurev.neuro.18.1.555
   SOFTKY WR, 1993, J NEUROSCI, V13, P334
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Stevens CF, 1998, NAT NEUROSCI, V1, P210, DOI 10.1038/659
   VAADIA E, 1995, NATURE, V373, P515, DOI 10.1038/373515a0
   Volgushev M, 1998, NEUROSCIENCE, V83, P15, DOI 10.1016/S0306-4522(97)00380-1
   VONDERMALSBURG C, 1981, CORRELATION THEORY B, P81
NR 75
TC 4
Z9 4
U1 0
U2 1
PY 2001
VL 239
BP 193
EP 207
WC Behavioral Sciences; Medicine, General & Internal
DA 2023-11-11
ER

PT J
AU Ge, XK
   Hu, XZ
   Dai, XC
AF Ge, Xiaokai
   Hu, Xianzhi
   Dai, Xuchu
TI Unsupervised Learning Feature Estimation for MISO Beamforming by Using
   Spiking Neural Networks
SO IEEE COMMUNICATIONS LETTERS
DT Article
DE Encoding; Array signal processing; Estimation; Decoding; Training;
   Neurons; Unsupervised learning; Spiking neural networks;
   energy-efficiency; unsupervised learning; beamforming
AB Deep learning (DL)-based beamforming frameworks have the advantage of low computational complexity. However, existing works are based on artificial neural networks (ANNs) and have high energy cost. To improve this situation, this letter proposes a feature estimation framework based on spiking neural networks (SNNs) for sum rate maximization (SRM). Specifically, we first devise a coding method at the input side to convert channel state information into binary spike sequences that can be processed by SNNs. Then, at the output side, the output binary sequences are decoded into the features that are used to generate the beamforming weights. Based on the characteristics of the decoding method, unsupervised learning can be used to train the SNNs. Simulation results show that compared to existing feature estimation frameworks using ANNs, the SNNs-based framework reduces energy cost by orders of magnitude and has comparable sum rate performance.
C1 [Ge, Xiaokai; Hu, Xianzhi; Dai, Xuchu] Chinese Acad Sci, Univ Sci & Technol China, Sch Informat Sci & Technol, Key Lab Wireless Opt Commun, Hefei 230026, Peoples R China.
RP Ge, XK (corresponding author), Chinese Acad Sci, Univ Sci & Technol China, Sch Informat Sci & Technol, Key Lab Wireless Opt Commun, Hefei 230026, Peoples R China.
EM gxk1225@mail.ustc.edu.cn; hxz123@mail.ustc.edu.cn; daixc@ustc.edu.cn
CR Björnson E, 2014, IEEE SIGNAL PROC MAG, V31, P142, DOI 10.1109/MSP.2014.2312183
   Christensen SS, 2008, IEEE T WIREL COMMUN, V7, P4792, DOI 10.1109/T-WC.2008.070851
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Horowitz M, 2014, ISSCC DIG TECH PAP I, V57, P10, DOI 10.1109/ISSCC.2014.6757323
   Kim S, 2020, AAAI CONF ARTIF INTE, V34, P11270
   Kim Youngeun, 2022, ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P71, DOI 10.1109/ICASSP43922.2022.9747906
   Kingma D. P., 2014, ARXIV, DOI DOI 10.48550/ARXIV.1412.6980
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Meng QY, 2022, NEURAL NETWORKS, V153, P254, DOI 10.1016/j.neunet.2022.06.001
   Panda P, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00653
   Roy K, 2019, NATURE, V575, P607, DOI 10.1038/s41586-019-1677-2
   Shi QJ, 2011, IEEE T SIGNAL PROCES, V59, P4331, DOI 10.1109/TSP.2011.2147784
   Shi YM, 2018, IEEE T SIGNAL PROCES, V66, P5180, DOI 10.1109/TSP.2018.2865408
   Sun HR, 2018, IEEE T SIGNAL PROCES, V66, P5438, DOI 10.1109/TSP.2018.2866382
   Wu YJ, 2019, AAAI CONF ARTIF INTE, P1311
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Xia WC, 2020, IEEE T COMMUN, V68, P1866, DOI 10.1109/TCOMM.2019.2960361
NR 17
TC 0
Z9 0
U1 1
U2 1
PD APR
PY 2023
VL 27
IS 4
BP 1165
EP 1169
DI 10.1109/LCOMM.2023.3246052
WC Telecommunications
DA 2023-11-11
ER

PT C
AU Luo, C
   Ying, ZZ
   Zhu, XL
   Chen, LL
AF Luo, Chong
   Ying, Zhaozhong
   Zhu, Xiaolei
   Chen, Longlong
GP IEEE
TI A Mixed-Signal Spiking Neuromorphic Architecture for Scalable Neural
   Network
SO 2017 NINTH INTERNATIONAL CONFERENCE ON INTELLIGENT HUMAN-MACHINE SYSTEMS
   AND CYBERNETICS (IHMSC 2017), VOL 1
SE International Conference on Intelligent Human-Machine Systems and
   Cybernetics
DT Proceedings Paper
CT 9th International Conference on Intelligent Human-Machine Systems and
   Cybernetics (IHMSC)
CY AUG 26-27, 2017
CL Hangzhou, PEOPLES R CHINA
DE Spiking Neural Network (SNN); mixed-signal; I&F; multi-chip
   communication; silicon neuron; silicon synapse
ID PLASTICITY; NEURONS
AB A reconfigurable mixed-signal spiking neuromorphic architecture is proposed. The chip comprises a total of 256x256 Static Random Access Memory (SRAM) cells, 256x256 Content Addressable Memory (CAM) cells, 2x256 synapses and 256 neurons. The integrate-and-fire (I&F) neuron implements spiking frequency adaptation and adjustable refractory period. In this network, programmable synaptic weight values are stored in the SRAM which is interfaced with current-mode digital to analog converter (DAC). Multi-chip communication is realized by the implementation of the router which is based on Address Event Representation (AER) communication protocol. Simulation results show the correct operation of proposed reconfigurable spiking neuromorphic system.
C1 [Luo, Chong; Ying, Zhaozhong; Zhu, Xiaolei; Chen, Longlong] Zhejiang Univ, Inst VLSI Design, Hangzhou 310027, Zhejiang, Peoples R China.
RP Zhu, XL (corresponding author), Zhejiang Univ, Inst VLSI Design, Hangzhou 310027, Zhejiang, Peoples R China.
EM zhuxl@vlsi.zju.edu.cn
CR [Anonymous], 2015, COMPUTER SCI
   Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Chicca E, 2014, P IEEE, V102, P1367, DOI 10.1109/JPROC.2014.2313954
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   Indiveri G, 2006, IEEE T NEURAL NETWOR, V17, P211, DOI 10.1109/TNN.2005.860850
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Murali S, 2009, LECT NOTES ELECTR EN, V34, P1, DOI 10.1007/978-1-4020-9757-7
   Schemmel J, 2007, IEEE INT SYMP CIRC S, P3367, DOI 10.1109/ISCAS.2007.378289
NR 8
TC 5
Z9 5
U1 0
U2 2
PY 2017
BP 179
EP 182
DI 10.1109/IHMSC.2017.47
WC Computer Science, Artificial Intelligence; Computer Science,
   Cybernetics; Robotics
DA 2023-11-11
ER

PT C
AU Asai, T
   Amemiya, Y
AF Asai, T
   Amemiya, Y
GP IEEE
   IEEE
TI Frequency- and temporal-domain neural competition in analog
   integrate-and-fire neurochips
SO PROCEEDING OF THE 2002 INTERNATIONAL JOINT CONFERENCE ON NEURAL
   NETWORKS, VOLS 1-3
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN 02)
CY MAY 12-17, 2002
CL HONOLULU, HI
AB In this report, we present an inhibitory neural network implemented on analog CMOS chips, whose neurons compete with each other in the frequency and time domains. The circuit for each neuron was designed to produce sequences in time of identically shaped pulses, called spikes. The results of experiments and simulations revealed that the network more efficiently achieved the selective activation and inactivation of the neural circuits on the basis of spike timing than on the basis of firing rates. The results indicate that neural processing based on the spike timing of neural circuits provides a possible way to overcome the low-tolerance problems of analog devices in noisy environments.
C1 Hokkaido Univ, Dept Elect Engn, Kita Ku, Sapporo, Hokkaido 0608628, Japan.
RP Asai, T (corresponding author), Hokkaido Univ, Dept Elect Engn, Kita Ku, Kita 13,Nishi 8, Sapporo, Hokkaido 0608628, Japan.
CR [Anonymous], 1988, NONLINEAR STOCHASTIC
   [Anonymous], NEUROMORPHIC SYSTEMS
   Fukai T, 1996, BIOL CYBERN, V75, P453, DOI 10.1007/s004220050310
   MAINEN ZF, 1995, SCIENCE, V268, P1503, DOI 10.1126/science.7770778
   RIEKE F, 1989, SPIKES EXPLORING NEU
   VITTOZ EA, 1985, DESIGN MOS VLSI CIRC, P104
NR 6
TC 5
Z9 5
U1 0
U2 0
PY 2002
BP 1337
EP 1341
DI 10.1109/IJCNN.2002.1007689
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Iakymchuk, T
   Rosado, A
   Frances, JV
   Bataller, M
AF Iakymchuk, Taras
   Rosado, Alfredo
   Frances, Jose V.
   Bataller, Manuel
BE Indrusiak, LS
   Gogniat, G
   Voros, N
TI Fast Spiking Neural Network architecture for low-cost FPGA devices
SO 2012 7TH INTERNATIONAL WORKSHOP ON RECONFIGURABLE AND
   COMMUNICATION-CENTRIC SYSTEMS-ON-CHIP (RECOSOC)
DT Proceedings Paper
CT 7th International Workshop on Reconfigurable Communication-Centric
   Systems-on-Chip (ReCoSoC)
CY JUL 09-11, 2012
CL York, ENGLAND
AB Spiking Neural Networks (SNN) consist of fully interconnected computation units (neurons) based on spike processing. This type of networks resembles those found in biological systems studied by neuroscientists. This paper shows a hardware implementation for SNN. First, SNN require the inputs to be spikes, being necessary a conversion system (encoding) from digital values into spikes. For travelling spikes, each neuron interconnection is characterized by weights and delays, requiring an internal neuron processing by a Postsynaptic Potential (PSP) function and membrane potential threshold evaluation for a post-synaptic output spike generation. In order to model a real biological system by artificial SNN, the number of required neurons is very high (thousands). In this work, we propose a SNN architecture able to adapt big size networks using reduced hardware resources. While spikes are processed at 1ms time, inter spike time is used for internal calculations, a mixed serial-parallel structure allows optimized computation of all neuron output values. Results show that SNN can be accommodated using a medium-size FPGA device such as Xilinx Spartan 3 with processing speed comparable to fully parallel implementations with up to 70% resource reduction.
C1 [Iakymchuk, Taras; Rosado, Alfredo; Frances, Jose V.; Bataller, Manuel] Univ Valencia, Digital Signal Proc Grp GPDS, Dpt Elect Engn, ETSE, Valencia, Spain.
RP Iakymchuk, T (corresponding author), Univ Valencia, Digital Signal Proc Grp GPDS, Dpt Elect Engn, ETSE, Valencia, Spain.
EM taras.yakymchuk@uv.es
CR [Anonymous], IFAC P VOLUMES
   Pérez-Carrasco JA, 2010, IEEE T NEURAL NETWOR, V21, P609, DOI 10.1109/TNN.2009.2039943
   Arbib MA, 2002, HDB BRAIN THEORY NEU
   Bouganis A., 2010, P 2010 INT JOINT C N, P1, DOI DOI 10.1109/IJCNN.2010.5596525
   Floreano D., 2001, LNCS, P38
   Gerstner W., 2002, SPIKING NEURON MODEL
   Goldberg DH, 2007, NEURAL COMPUT, V19, P2797, DOI 10.1162/neco.2007.19.10.2797
   Guerrero-Martinez J., 2012, IFAC P, V45, DOI [10.3182/20120403-3-DE-3010.00074, DOI 10.3182/20120403-3-DE-3010.00074]
   Hagras H, 2004, IEEE INT CONF ROBOT, P4620, DOI 10.1109/ROBOT.2004.1302446
   Meftah B, 2008, IEEE IJCNN, P681, DOI 10.1109/IJCNN.2008.4633868
   PAUGAMMOISY H, 2009, HDB NATURAL COMPUTIN
   Perrinet LU, 2008, PROC SPIE, V7000, DOI 10.1117/12.787076
   Roggen D., 2003, EH 03, P199
   Tommiska MT, 2003, IEE P-COMPUT DIG T, V150, P403, DOI 10.1049/ip-cdt:20030965
   Vogelstein R.J., 2005, ADV NEURAL INF PROCE, P1457
   Wang XQ, 2008, ICNC 2008: FOURTH INTERNATIONAL CONFERENCE ON NATURAL COMPUTATION, VOL 4, PROCEEDINGS, P125, DOI 10.1109/ICNC.2008.718
   Wu QX, 2008, NEUROCOMPUTING, V71, P2055, DOI 10.1016/j.neucom.2007.10.020
   Wysoski SG, 2008, NEUROCOMPUTING, V71, P2563, DOI 10.1016/j.neucom.2007.12.038
   [No title captured]
NR 19
TC 1
Z9 1
U1 0
U2 4
PY 2012
WC Computer Science, Theory & Methods; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU BRANAGAN, LA
AF BRANAGAN, LA
TI INTRODUCTORY USE OF PROBABILISTIC NEURAL NETWORKS FOR SPIKE DETECTION
   FROM AN ONLINE VIBRATION DIAGNOSTIC SYSTEM
SO JOURNAL OF INTELLIGENT MATERIAL SYSTEMS AND STRUCTURES
DT Article
AB We developed a two-part algorithm for detecting spikes in trended data from a vibration diagnostic monitoring system. Deviation from an exponential forecasting technique provides an initial screening of data for abnormal amplitudes. These abnormal events are classified as NORMAL (i.e., no spikes), positive-going spikes (PSPIKE), or negative-going spikes (NSPIKE) by a PNN calculation trained on synthetic data using a Gaussian probability density function (PDF) estimation. This screening reduces computational time and results in fewer false spike classifications compared with a probabilistic neural network (PNN) calculation alone. Results from sample field data sets showed that spike detection can be readily achieved, but that steps in the data lead to false spike classifications.
RP BRANAGAN, LA (corresponding author), PACIFIC GAS & ELECT CO,3400 CROW CANYON RD,SAN RAMON,CA 94583, USA.
CR BRANAGAN LA, 1990, 4TH EL POW RES I INC
   BRANAGAN LA, 1992, INTELLIGENT ENG SYST, V2, P719
   Montgomery D.C., 1990, FORECASTING TIME SER
   Specht D. F., 1992, P IEEE INT JOINT C N
   SPECHT DF, 1990, NEURAL NETWORKS, V3, P109, DOI 10.1016/0893-6080(90)90049-Q
   WASSERMAN PD, 1989, NEURAL COMPUTING
   1992, MATCHCAD 31 USERS GU
NR 7
TC 0
Z9 0
U1 0
U2 1
PD MAR
PY 1994
VL 5
IS 2
BP 165
EP 171
DI 10.1177/1045389X9400500203
WC Materials Science, Multidisciplinary
DA 2023-11-11
ER

PT S
AU Fiasché, M
   Taisch, M
AF Fiasche, Maurizio
   Taisch, Marco
BE Bassis, S
   Esposito, A
   Morabito, FC
TI On the Use of Quantum-inspired Optimization Techniques for Training
   Spiking Neural Networks: A New Method Proposed
SO ADVANCES IN NEURAL NETWORKS: COMPUTATIONAL AND THEORETICAL ISSUES
SE Smart Innovation, Systems and Technologies
DT Article; Book Chapter
DE Spiking Neural Network (SNN); evolving SNN (eSNN); Evolutionary
   Algorithms (EA); Quantum EA (QEA); Quantum Particle Swarm Optimization
   (QPSO)
ID EVOLUTIONARY ALGORITHM
AB Spiking neural networks (SNN) are brain-like connectionist methods, where the output activation function is represented as a train of spikes and not as a potential. This and other reasons make SNN models biologically closer to brain principles than any of the alternative Artificial Neural networks (ANN) models proposed. In fact, they have great potential for solving complicated time-dependent pattern recognition problems defined by time series because of their inherent dynamical representation. A lot of works have been presented in the last decade about SNN which promote these models as third generation ANN. Nevertheless, several still open challenges have been reported in these studies. In this paper we analyze a particular type of SNN, the evolving SNN (eSNN), mainly focusing on their weights, parameters and features optimization using a new evolutionary strategy.
C1 [Fiasche, Maurizio; Taisch, Marco] Politecn Milan, Dept Management Econ & Ind Engn, Milan, Italy.
RP Fiasché, M (corresponding author), Politecn Milan, Dept Management Econ & Ind Engn, Milan, Italy.
EM maurizio.fiasche@polimi.it
CR Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Eberhart R., 1999, P 6 INT S MICROMACHI, DOI [10.1109/MHS.1995.494215, DOI 10.1109/MHS.1995.494215]
   Estévez PA, 2009, IEEE T NEURAL NETWOR, V20, P189, DOI 10.1109/TNN.2008.2005601
   Fiasché M, 2012, LECT NOTES COMPUT SC, V7665, P686, DOI 10.1007/978-3-642-34487-9_83
   Hamed HNA, 2011, EVOLUTIONARY ALGORITHMS, P133
   Hamed HNA, 2009, 2009 INTERNATIONAL CONFERENCE OF SOFT COMPUTING AND PATTERN RECOGNITION, P695, DOI 10.1109/SoCPaR.2009.139
   Han KH, 2002, IEEE T EVOLUT COMPUT, V6, P580, DOI 10.1109/TEVC.2002.804320
   Kasabov N, 2007, EVOLVING CONNECTIONI, V2
   Kasabov N, 2009, LECT NOTES COMPUT SC, V5506, P3, DOI 10.1007/978-3-642-02490-0_1
   Kasabov NK, 2014, NEURAL NETWORKS, V52, P62, DOI 10.1016/j.neunet.2014.01.006
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Platel MD, 2009, IEEE T EVOLUT COMPUT, V13, P1218, DOI 10.1109/TEVC.2008.2003010
   Platel MD, 2007, IEEE C EVOL COMPUTAT, P423
   Schliebs S, 2009, NEURAL NETWORKS, V22, P623, DOI 10.1016/j.neunet.2009.06.038
   Soltic S., 2008, IEEE WORLD C COMP IN
   Sun J, 2004, IEEE C EVOL COMPUTAT, P325
   Thorpe S.J., 1997, P EUR S ART NEUR NET
   VanRullen R, 2005, TRENDS NEUROSCI, V28, P1, DOI 10.1016/j.tins.2004.10.010
   Wysoski SG, 2006, LECT NOTES COMPUT SC, V4131, P61
   Wysoski SG, 2010, STUD COMPUT INTELL, V266, P15
NR 20
TC 3
Z9 3
U1 0
U2 0
PY 2015
VL 37
BP 359
EP 368
DI 10.1007/978-3-319-18164-6_35
D2 10.1007/978-3-319-18164-6
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT J
AU Wong, WK
   Wang, Z
   Zhen, B
   Leung, S
AF Wong, W. K.
   Wang, Z.
   Zhen, B.
   Leung, Sys.
TI RELATIONSHIP BETWEEN APPLICABILITY OF CURRENT-BASED SYNAPSES AND
   UNIFORMITY OF FIRING PATTERNS
SO INTERNATIONAL JOURNAL OF NEURAL SYSTEMS
DT Article
DE Current-based synapse; spiking neural networks; synaptic currents;
   neuron
ID SPIKING NEURAL-NETWORKS; GAMMA OSCILLATIONS; MODEL; NEURONS; INHIBITION;
   DYNAMICS; SYNCHRONIZATION; ROBUSTNESS; SIMULATION
AB The purpose of this paper is to identify situations in neural network modeling where current-based synapses are applicable. The applicability of current-based synapse model for studying post-transient behavior of neural networks is discussed in terms of average synaptic current strength induced by per spike during one firing cycle of a neuron (or briefly per spike synaptic current strength). It was found that current-based synapse models are applicable in both situations where both the interspike intervals of the neurons and the distribution of firing times of the neurons are uniform, and where the firing of all neurons is synchronized. If neither the interspike intervals nor the distribution of firing times of the neurons is uniform or the reversal potential is between the rest and threshold potentials, current-based synapse models may be oversimplified.
C1 [Wong, W. K.; Zhen, B.; Leung, Sys.] Hong Kong Polytech Univ, Inst Text & Clothing, Kowloon, Hong Kong, Peoples R China.
   [Wang, Z.] Donghua Univ, Coll Informat Sci & Technol, Shanghai, Peoples R China.
RP Wong, WK (corresponding author), Hong Kong Polytech Univ, Inst Text & Clothing, Kowloon, Hong Kong, Peoples R China.
EM tcwongca@inet.polyu.edu.hk; wangzj@dhu.edu.cn
CR [Anonymous], 1998, PULSED NEURAL NETWOR
   Badoual M, 2006, INT J NEURAL SYST, V16, P79, DOI 10.1142/S0129065706000524
   Bathellier B, 2008, NEURAL COMPUT, V20, P2973, DOI 10.1162/neco.2008.11-07-636
   Bressloff PC, 2000, NEURAL COMPUT, V12, P91, DOI 10.1162/089976600300015907
   Brette R, 2006, NEURAL COMPUT, V18, P2004, DOI 10.1162/neco.2006.18.8.2004
   Brette R, 2007, J COMPUT NEUROSCI, V23, P349, DOI 10.1007/s10827-007-0038-6
   Brunel N, 2003, J NEUROPHYSIOL, V90, P415, DOI 10.1152/jn.01095.2002
   Burkitt AN, 2006, BIOL CYBERN, V95, P1, DOI 10.1007/s00422-006-0068-6
   Chakravarthy N, 2007, INT J NEURAL SYST, V17, P123, DOI 10.1142/S0129065707000993
   CHAPEAUBLONDEAU F, 1995, NEURAL COMPUT, V7, P713, DOI 10.1162/neco.1995.7.4.713
   Chiappalone M, 2007, INT J NEURAL SYST, V17, P87, DOI 10.1142/S0129065707000968
   Chizhov AV, 2008, PHYS REV E, V77, DOI 10.1103/PhysRevE.77.011910
   Dayan P., 2001, THEORETICAL NEUROSCI
   Destexhe A, 1994, J Comput Neurosci, V1, P195, DOI 10.1007/BF00961734
   Gerstner W., 2002, SPIKING NEURON MODEL
   Ghosh-Dastidar S, 2007, INTEGR COMPUT-AID E, V14, P187
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Hertz J, 2010, NEURAL COMPUT, V22, P427, DOI 10.1162/neco.2009.06-08-806
   Iglesias J, 2008, INT J NEURAL SYST, V18, P267, DOI 10.1142/S0129065708001580
   JAHR CE, 1990, J NEUROSCI, V10, P1830
   Kohn A, 2005, J NEUROSCI, V25, P3661, DOI 10.1523/JNEUROSCI.5106-04.2005
   Luque NR, 2011, INT J NEURAL SYST, V21, P385, DOI 10.1142/S0129065711002900
   Neltner L, 2000, NEURAL COMPUT, V12, P1607, DOI 10.1162/089976600300015286
   Nichols E, 2010, INT J NEURAL SYST, V20, P501, DOI 10.1142/S0129065710002577
   Schliebs S, 2010, INT J NEURAL SYST, V20, P481, DOI 10.1142/S0129065710002565
   Shriki O, 2003, NEURAL COMPUT, V15, P1809, DOI 10.1162/08997660360675053
   Strain TJ, 2010, INT J NEURAL SYST, V20, P463, DOI 10.1142/S0129065710002553
   Vida I, 2006, NEURON, V49, P107, DOI 10.1016/j.neuron.2005.11.036
   Vidybida A, 2011, INT J NEURAL SYST, V21, P187, DOI 10.1142/S0129065711002742
   Wang XJ, 2010, PHYSIOL REV, V90, P1195, DOI 10.1152/physrev.00035.2008
   Wang XJ, 1996, J NEUROSCI, V16, P6402
   Wang Z, 2011, PHYS REV E, V83, DOI 10.1103/PhysRevE.83.051905
   Wu W, 2009, INT J NEURAL SYST, V19, P425, DOI 10.1142/S0129065709002129
   ZOHARY E, 1994, NATURE, V370, P140, DOI 10.1038/370140a0
NR 35
TC 16
Z9 16
U1 0
U2 7
PD AUG
PY 2012
VL 22
IS 4
AR 1250017
DI 10.1142/S0129065712500177
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT C
AU Johnston, SP
   Prasad, G
   Maguire, L
   McGinnity, TM
AF Johnston, S. P.
   Prasad, G.
   Maguire, L.
   McGinnity, T. M.
GP IEEE
TI A hybrid learning algorithm fusing STDP with GA based explicit delay
   learning for spiking neurons
SO 2006 3RD INTERNATIONAL IEEE CONFERENCE INTELLIGENT SYSTEMS, VOLS 1 AND 2
DT Proceedings Paper
CT 3rd IEEE International Conference on Intelligent Systems
CY SEP 04-06, 2006
CL Univ Westminister, London, ENGLAND
HO Univ Westminister
DE STDP; explicit delay learning; spike trains; genetic algorithm
AB This paper presents a hybrid learning algorithm for spiking neural networks (SNNs), referred to as an evolvable spiking neural network (ESNN) paradigm. The algorithm integrates a supervised and unsupervised learning approach. The unsupervised approach exploits a Spike Timing Dependent Plasticity (STDP) mechanism with explicit delay learning for multiple connections between neurons. Supervision of the synaptic delays and the excitatory/inhibitory connections is governed by a genetic algorithm (GA), while the STDP rule is free to operate in its normal unsupervised manner. A spike train encoding/decoding scheme is developed for the algorithm. The approach is validated by application to the Iris classification problem.
C1 [Johnston, S. P.; Prasad, G.; Maguire, L.; McGinnity, T. M.] Univ Ulster, Intelligent Syst Engn Lab, Derry, North Ireland.
RP Johnston, SP (corresponding author), Univ Ulster, Intelligent Syst Engn Lab, Derry, North Ireland.
EM s.johnston@ulster.ac.uk
CR Abbott LF, 2000, NAT NEUROSCI, V3, P1178, DOI 10.1038/81453
   ADRAIN E, 1926, J PHYSIOL-LONDON, V61, P49
   BELATRECH A, 2003, IEEE SMC UK ROI CHAP
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   DIPAOLO EA, 2003, PHILOS T R SOC A, V361, P92319
   FLORIAN RV, 2005, ICCN BOST MA, P27
   HUDEL D, 1977, P ROYAL SOC B, V198, P1
   JOHNSTON S, 2005, ICANN, V1, P269
   Lestienne R, 1996, BIOL CYBERN, V74, P55, DOI 10.1007/BF00199137
   Natschlager T, 1998, NETWORK-COMP NEURAL, V9, P319, DOI 10.1088/0954-898X/9/3/003
   Senn W, 2002, NEURAL COMPUT, V14, P583, DOI 10.1162/089976602317250915
   SOFTKY WR, 1993, J NEUROSCI, V13, P334
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   STRAIN T, 2004, IEEE SMC UK ROI CHAP
   TAO X, 2004, ICAI, V21, P168
   WULFRAM G, 2002, SPIKING NEURON MODEL
NR 17
TC 1
Z9 1
U1 0
U2 0
PY 2006
BP 621
EP 626
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT C
AU Foderaro, G
   Henriquez, C
   Ferrari, S
AF Foderaro, Greg
   Henriquez, Craig
   Ferrari, Silvia
GP IEEE
TI Indirect Training of a Spiking Neural Network for Flight Control via
   Spike-Timing-Dependent Synaptic Plasticity
SO 49TH IEEE CONFERENCE ON DECISION AND CONTROL (CDC)
SE IEEE Conference on Decision and Control
DT Proceedings Paper
CT 49th IEEE Conference on Decision and Control (CDC)
CY DEC 15-17, 2010
CL Atlanta, GA
AB Recently, spiking neural networks (SNNs) have been shown capable of approximating the dynamics of biological neuronal networks, and of being trainable by biologically-plausible learning mechanisms, such as spike-timing-dependent synaptic plasticity. Numerical simulations also support the possibility that they may possess universal function approximation abilities. However the effectiveness of training algorithms to date is far inferior to those of other artificial neural networks. Moreover, they rely on directly manipulating the SNN weights, which may not be feasible in a number of their potential applications. This paper presents a novel indirect training approach to modulate spike-timing-dependent plasticity (STDP) in an action SNN that serves as a flight controller without directly manipulating its weights. A critic SNN is directly trained with a reward-based Hebbian approach to send spike trains to the action SNN, which in turn controls the aircraft and learns via STDP. The approach is demonstrated by training the action SNN to act as a flight controller for stability augmentation. Its performance and dynamics are analyzed before and after training through numerical simulations and Poincare maps.
C1 [Foderaro, Greg; Ferrari, Silvia] Duke Univ, Dept Mech Engn, LISC, Durham, NC 27708 USA.
   [Henriquez, Craig] Duke Univ, Dept Biomed Engn, Durham, NC 27708 USA.
RP Foderaro, G (corresponding author), Duke Univ, Dept Mech Engn, LISC, Durham, NC 27708 USA.
EM greg.foderaro@duke.edu; ch@duke.edu; sferrari@duke.edu
CR [Anonymous], 1952, J PHYSL
   [Anonymous], CONNECTIONISM PERSPE
   [Anonymous], NEURAL MICROCIRCUITS
   Burgsteiner H., 2006, ENG APPL ARTIFICIAL, V19
   Ferrari S, 2008, IEEE IJCNN, P1780, DOI 10.1109/IJCNN.2008.4634039
   FLETCHER TL, 1991, J NEUROSCI, V11, P1617
   Florian R. V., 2007, NEURAL COMPUTATION, V19, P2007
   Hugh GS, 2002, NEUROCOMPUTING, V44, P847, DOI 10.1016/S0925-2312(02)00482-4
   Jack JJB, 1975, ELECT CURRENT FLOW E
   Legenstein R., 2005, NEURAL COMPUTATION, V17
   Maass W., 1997, ADV NEURAL INFORM PR, V9
   MAINEN ZF, 1995, SCIENCE, V268, P1503, DOI 10.1126/science.7770778
   Nelson R. C., 1998, FLIGHT STABILITY AUT
   Pennartz CMA, 1997, NEUROSCIENCE, V81, P303, DOI 10.1016/S0306-4522(97)00118-8
   Perrett D., 1982, EXPT BRAIN RES, V47
   Pfister J. P., 2006, NEURAL COMPUTATION, V18
   Salinas E., 2001, NATURE REV NEUROSCIE, V2
   Wysoski S. G., 2006, LECT NOTES COMPUTER, V4179
NR 18
TC 7
Z9 7
U1 0
U2 2
PY 2010
BP 911
EP 917
DI 10.1109/CDC.2010.5717260
WC Automation & Control Systems; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Stöckel, A
   Eliasmith, C
AF Stoeckel, Andreas
   Eliasmith, Chris
TI Passive Nonlinear Dendritic Interactions as a Computational Resource in
   Spiking Neural Networks
SO NEURAL COMPUTATION
DT Article
ID MODEL; SYNAPSES; NEURONS; GABA
AB Nonlinear interactions in the dendritic tree play a key role in neural computation. Nevertheless, modeling frameworks aimed at the construction of large-scale, functional spiking neural networks, such as the Neural Engineering Framework, tend to assume a linear superposition of postsynaptic currents. In this letter, we present a series of extensions to the Neural Engineering Framework that facilitate the construction of networks incorporating Dale's principle and nonlinear conductance-based synapses. We apply these extensions to a two-compartment LIF neuron that can be seen as a simple model of passive dendritic computation. We show that it is possible to incorporate neuron models with input-dependent nonlinearities into the Neural Engineering Framework without compromising high-level function and that nonlinear postsynaptic currents can be systematically exploited to compute a wide variety of multivariate, band-limited functions, including the Euclidean norm, controlled shunting, and nonnegative multiplication. By avoiding an additional source of spike noise, the function approximation accuracy of a single layer of two-compartment LIF neurons is on a par with or even surpasses that of two-layer spiking neural networks up to a certain target function bandwidth.
C1 [Stoeckel, Andreas; Eliasmith, Chris] Univ Waterloo, Ctr Theoret Neurosci, Waterloo, ON N2L 3G1, Canada.
RP Stöckel, A (corresponding author), Univ Waterloo, Ctr Theoret Neurosci, Waterloo, ON N2L 3G1, Canada.
EM astoecke@uwaterloo.ca; celiasmith@uwaterloo.ca
CR Alemi A., 2018, P 32 AAAI C ART INT
   Bekolay T, 2014, FRONT NEUROINFORM, V7, DOI 10.3389/fninf.2013.00048
   Berzish M, 2016, LECT NOTES COMPUT SC, V9886, P349, DOI 10.1007/978-3-319-44778-0_41
   Blouw P., 2018, ARXIV181201739
   Boahen K, 2017, COMPUT SCI ENG, V19, P14, DOI 10.1109/MCSE.2017.33
   Bobier B, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003577
   Boerlin M, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003258
   Boerlin M, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1001080
   Broomhead D, 1988, RADIAL BASIS FUNCTIO
   Capaday Charles, 2006, Journal of Integrative Neuroscience, V5, P199, DOI 10.1142/S021963520600115X
   CAPOCELLI RM, 1971, KYBERNETIK, V8, P214, DOI 10.1007/BF00288750
   Choudhary Swadesh, 2012, Artificial Neural Networks and Machine Learning - ICANN 2012. Proceedings of the 22nd International Conference on Artificial Neural Networks, P121, DOI 10.1007/978-3-642-33269-2_16
   Duggins P., 2017, THESIS U WATERLOO
   Duggins P, 2017, TOP COGN SCI, V9, P117, DOI 10.1111/tops.12247
   Eliasmith C., 2016, ARXIV160205220
   Eliasmith C., 2013, BUILD BRAIN NEURAL A, DOI DOI 10.1093/ACPROF:OSO/9780199794546.001.0001
   Eliasmith C., 2003, NEURAL ENG COMPUTATI
   Eliasmith C, 2012, SCIENCE, V338, P1202, DOI 10.1126/science.1225266
   GABBOTT PLA, 1986, EXP BRAIN RES, V61, P323
   Gosmann J., 2020, PSYCHOL REV, DOI 10.1037/rev0000250
   Gupta A, 2000, SCIENCE, V287, P273, DOI 10.1126/science.287.5451.273
   HENDRY SHC, 1981, J NEUROSCI, V1, P390
   HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8
   Hunsberger E, 2014, NEURAL COMPUT, V26, P1600, DOI 10.1162/NECO_a_00621
   Hunsberger Eric, 2015, ARXIV151008829
   JONAS P, 1993, J PHYSIOL-LONDON, V472, P615, DOI 10.1113/jphysiol.1993.sp019965
   Koch Christof, 1999, P1
   Komer B, 2016, CURR OPIN BEHAV SCI, V11, P14, DOI 10.1016/j.cobeha.2016.03.006
   Kozlov M, 1980, USSR COMP MATH MATH, V20, P223, DOI DOI 10.1016/0041-5553(80)90098-1
   Kreutz-Delgado K., 2015, ARXIV150104032
   Kuo PD, 2005, BIOL CYBERN, V93, P178, DOI 10.1007/s00422-005-0576-9
   London M, 2005, ANNU REV NEUROSCI, V28, P503, DOI 10.1146/annurev.neuro.28.061604.135703
   MacNeil D, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0022885
   MARR D, 1969, J PHYSIOL-LONDON, V202, P437, DOI 10.1113/jphysiol.1969.sp008820
   MEL BW, 1994, NEURAL COMPUT, V6, P1031, DOI 10.1162/neco.1994.6.6.1031
   MUNDY A, 2015, IEEE IJCNN
   Neckar A, 2019, P IEEE, V107, P144, DOI 10.1109/JPROC.2018.2881432
   Nicola W, 2017, NAT COMMUN, V8, DOI 10.1038/s41467-017-01827-3
   Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5
   Parisien C, 2008, NEURAL COMPUT, V20, P1473, DOI 10.1162/neco.2008.07-06-295
   Polsky A, 2004, NAT NEUROSCI, V7, P621, DOI 10.1038/nn1253
   Roth A, 2009, COMPUT NEUROSCI-MIT, P139
   Schemmel J, 2017, IEEE IJCNN, P2217, DOI 10.1109/IJCNN.2017.7966124
   Schwemmer MA, 2015, J NEUROSCI, V35, P10112, DOI 10.1523/JNEUROSCI.4951-14.2015
   Stellato B, 2020, MATH PROGRAM COMPUT, V12, P637, DOI 10.1007/s12532-020-00179-2
   Stewart TC, 2012, FRONT NEUROSCI-SWITZ, V6, DOI 10.3389/fnins.2012.00002
   Stockel A., 2017, ARXIV171007659
   Strata P, 1999, BRAIN RES BULL, V50, P349, DOI 10.1016/S0361-9230(99)00100-8
   Sussillo D, 2009, NEURON, V63, P544, DOI 10.1016/j.neuron.2009.07.018
   Thalmeier D, 2016, PLOS COMPUT BIOL, V12, DOI 10.1371/journal.pcbi.1004895
   Tripp B., 2009, THESIS U WATERLOO
   Tripp B, 2007, CEREB CORTEX, V17, P1830, DOI 10.1093/cercor/bhl092
   Vandenberghe L., 2010, CVXOPT LINEAR QUADRA
   Voelker AR, 2018, NEURAL COMPUT, V30, P569, DOI [10.1162/neco_a_01046, 10.1162/NECO_a_01046]
   VU ET, 1993, J NEUROSCI, V13, P4379
NR 55
TC 11
Z9 11
U1 0
U2 32
PD JAN
PY 2021
VL 33
IS 1
BP 96
EP 128
DI 10.1162/neco_a_01338
WC Computer Science, Artificial Intelligence; Neurosciences
DA 2023-11-11
ER

PT C
AU Kerr, D
   Coleman, SA
   McGinnity, TM
   Wu, Q
   Clogenson, M
AF Kerr, D.
   Coleman, S. A.
   McGinnity, T. M.
   Wu, Q.
   Clogenson, M.
GP IEEE
TI A Novel Approach to Robot Vision using a Hexagonal Grid and Spiking
   Neural Networks
SO 2012 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUN 10-15, 2012
CL Brisbane, AUSTRALIA
DE component; range image; hexagonal imaging; spiking neural network
ID EDGE-DETECTION; RANGE IMAGES; SYSTEM
AB Many robots use range data to obtain an almost 3-dimensional description of their environment. Feature driven segmentation of range images has been primarily used for 3D object recognition, and hence the accuracy of the detected features is a prominent issue. Inspired by the structure and behaviour of the human visual system, we present an approach to feature extraction in range data using spiking neural networks and a biologically plausible hexagonal pixel arrangement. Standard digital images are converted into a hexagonal pixel representation and then processed using a spiking neural network with hexagonal shaped receptive fields; this approach is a step towards developing a robotic eye that closely mimics the human eye. The performance is compared with receptive fields implemented on standard rectangular images. Results illustrate that, using hexagonally shaped receptive fields, performance is improved over standard rectangular shaped receptive fields.
C1 [Kerr, D.; Coleman, S. A.; McGinnity, T. M.; Wu, Q.] Univ Ulster, Intelligent Syst Res Ctr, Magee, Londonderry, North Ireland.
   [Clogenson, M.] CPR Lyon, F-69616 Villebrumier, France.
RP Kerr, D (corresponding author), Univ Ulster, Intelligent Syst Res Ctr, Magee, Londonderry, North Ireland.
EM d.kerr@ulster.ac.uk; sa.coleman@ulster.ac.uk; tm.mcginnity@ulster.ac.uk;
   q.wu@ulster.ac.uk; marine.clogenson@cpe.fr
CR ABDOU IE, 1979, P IEEE, V67, P753, DOI 10.1109/PROC.1979.11325
   Allen J.D., 2003, SIGNAL SOLUTIONS
   Bakker M. De, 2000, THESIS
   BELLON O, 2000, P WORKSH ART INT COM
   Besl P. J., 1988, Machine Vision and Applications, V1, P127, DOI 10.1007/BF01212277
   CURCIO CA, 1990, J COMP NEUROL, V292, P497, DOI 10.1002/cne.902920402
   DIAS P, 2003, P IEEE INT C IM PROC
   Egmont-Petersen M, 2002, PATTERN RECOGN, V35, P2279, DOI 10.1016/S0031-3203(01)00178-9
   Floreano D, 2005, ARTIF LIFE, V11, P121, DOI 10.1162/1064546053278900
   Floreano D., 2001, LNCS, P38
   Gamez D, 2007, LECT NOTES COMPUT SC, V4668, P360
   Gerstner W., 2002, SPIKING NEURON MODEL
   Hagras H., 2004, P IEEE INT C ROB AUT
   He X., 2005, P 1 INT C INF COMM T, P52, DOI DOI 10.1109/ICICT.2005.1598543
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Huang CH, 2007, IEEE T CIRCUITS-I, V54, P35, DOI 10.1109/TCSI.2006.887975
   Hugues E, 2002, LECT NOTES COMPUT SC, V2525, P60
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Jarvis R. J., 1993, 3 DIMENSIONAL OBJECT, P17
   Jiang XY, 1999, COMPUT VIS IMAGE UND, V73, P183, DOI 10.1006/cviu.1998.0715
   JIANG XY, 1994, MACH VISION APPL, V7, P115, DOI 10.1007/BF01215806
   Kunkle D.R., 2002, PULSED NEURAL NETWOR
   Lazdins E., 2011, P INT WORKSH BIOINSP
   Masland RH, 2001, NAT NEUROSCI, V4, P877, DOI 10.1038/nn0901-877
   Masuta H., 2008, SICE ANN C
   Meftah B, 2010, NEURAL PROCESS LETT, V32, P131, DOI 10.1007/s11063-010-9149-6
   Middleton L, 2001, IMAGE VISION COMPUT, V19, P1071, DOI 10.1016/S0262-8856(01)00067-1
   Roggen D., 2003, P NASA DOD C EV HARD
   Shimonomura K, 2007, IEEE INT CONF ROBOT, P4867, DOI 10.1109/ROBOT.2007.364229
   Takami R, 2005, IEEE INT SYMP CIRC S, P2771, DOI 10.1109/ISCAS.2005.1465201
   Wu QX, 2007, LECT NOTES ARTIF INT, V4682, P26
   WUTHRICH CA, 1991, CVGIP-GRAPH MODEL IM, V53, P324, DOI 10.1016/1049-9652(91)90036-J
NR 32
TC 2
Z9 2
U1 0
U2 6
PY 2012
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT J
AU Fu, Q
   Dong, HB
AF Fu, Qiang
   Dong, Hongbin
TI Breast Cancer Recognition Using Saliency-Based Spiking Neural Network
SO WIRELESS COMMUNICATIONS & MOBILE COMPUTING
DT Article
ID CLASSIFICATION; ALGORITHMS; DIAGNOSIS; MODEL
AB The spiking neural networks (SNNs) use event-driven signals to encode physical information for neural computation. SNN takes the spiking neuron as the basic unit. It modulates the process of nerve cells from receiving stimuli to firing spikes. Therefore, SNN is more biologically plausible. Although the SNN has more characteristics of biological neurons, SNN is rarely used for medical image recognition due to its poor performance. In this paper, a reservoir spiking neural network is used for breast cancer image recognition. Due to the difficulties of extracting the lesion features in medical images, a salient feature extraction method is used in image recognition. The salient feature extraction network is composed of spiking convolution layers, which can effectively extract the features of lesions. Two temporal encoding manners, namely, linear time encoding and entropy-based time encoding methods, are used to encode the input patterns. Readout neurons use the ReSuMe algorithm for training, and the Fruit Fly Optimization Algorithm (FOA) is employed to optimize the network architecture to further improve the reservoir SNN performance. Three modality datasets are used to verify the effectiveness of the proposed method. The results show an accuracy of 97.44% for the BreastMNIST database. The classification accuracy is 98.27% on the mini-MIAS database. And the overall accuracy is 95.83% for the BreaKHis database by using the saliency feature extraction, entropy-based time encoding, and network optimization.
C1 [Fu, Qiang; Dong, Hongbin] Harbin Engn Univ, Coll Comp Sci & Technol, Harbin 150001, Peoples R China.
RP Dong, HB (corresponding author), Harbin Engn Univ, Coll Comp Sci & Technol, Harbin 150001, Peoples R China.
EM donghongbin@hrbeu.edu.cn
CR Abdelsamea MM, 2019, CANCER INFORM, V18, DOI 10.1177/1176935119857570
   Al-Dhabyani W, 2020, DATA BRIEF, V28, DOI 10.1016/j.dib.2019.104863
   Anitha J, 2017, COMPUT METH PROG BIO, V138, P93, DOI 10.1016/j.cmpb.2016.10.026
   Benhammou Y, 2020, NEUROCOMPUTING, V375, P9, DOI 10.1016/j.neucom.2019.09.044
   Borji A, 2013, IEEE T IMAGE PROCESS, V22, P55, DOI 10.1109/TIP.2012.2210727
   Boudraa S, 2020, EVOL SYST-GER, V11, P697, DOI 10.1007/s12530-019-09322-4
   Dabass J., 2020, INFORM MED UNLOCKED, V20
   Ekici S, 2020, MED HYPOTHESES, V137, DOI 10.1016/j.mehy.2019.109542
   Enel P, 2016, PLOS COMPUT BIOL, V12, DOI 10.1371/journal.pcbi.1004967
   Feurer M, 2019, SPRING SER CHALLENGE, P113, DOI 10.1007/978-3-030-05318-5_6
   Florescu D, 2019, NEURAL COMPUT, V31, P1825, DOI 10.1162/neco_a_01218
   Fu Q, 2021, NEUROCOMPUTING, V419, P47, DOI 10.1016/j.neucom.2020.07.109
   Gallego-Ortiz C, 2019, MED IMAGE ANAL, V51, P116, DOI 10.1016/j.media.2018.10.011
   Gecer B, 2018, PATTERN RECOGN, V84, P345, DOI 10.1016/j.patcog.2018.07.022
   Han ZY, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-04075-z
   Hao YZ, 2020, NEURAL NETWORKS, V121, P387, DOI 10.1016/j.neunet.2019.09.007
   He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90
   Jiao ZC, 2018, PATTERN RECOGN, V75, P292, DOI 10.1016/j.patcog.2017.07.008
   Jin HF, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1946, DOI 10.1145/3292500.3330648
   Khan S, 2019, PATTERN RECOGN LETT, V125, P1, DOI 10.1016/j.patrec.2019.03.022
   KOCH C, 1985, HUM NEUROBIOL, V4, P219
   Koch K, 2006, CURR BIOL, V16, P1428, DOI 10.1016/j.cub.2006.05.056
   Kumar A, 2021, IEEE INTERNET THINGS, V8, P17778, DOI 10.1109/JIOT.2021.3119520
   Lee C, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00119
   Lee C, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00435
   Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508
   Lei BY, 2020, MED IMAGE ANAL, V64, DOI 10.1016/j.media.2020.101753
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Maicas G, 2019, MED IMAGE ANAL, V58, DOI 10.1016/j.media.2019.101562
   Nicola W, 2017, NAT COMMUN, V8, DOI 10.1038/s41467-017-01827-3
   Pan WT, 2012, KNOWL-BASED SYST, V26, P69, DOI 10.1016/j.knosys.2011.07.001
   Parvin F, 2020, IEEE REGION 10 SYMP, P945
   Ponghiran W, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00883
   Ponulak F, 2008, INT J APPL MATH COMP, V18, P117, DOI 10.2478/v10006-008-0011-1
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Qi XF, 2019, MED IMAGE ANAL, V52, P185, DOI 10.1016/j.media.2018.12.006
   Rabidas R, 2018, IEEE J BIOMED HEALTH, V22, P826, DOI 10.1109/JBHI.2017.2715021
   Soures N, 2019, IEEE SIGNAL PROC MAG, V36, P78, DOI 10.1109/MSP.2019.2931479
   Spanhol FA, 2016, IEEE T BIO-MED ENG, V63, P1455, DOI 10.1109/TBME.2015.2496264
   Spanhol FA, 2016, IEEE IJCNN, P2560, DOI 10.1109/IJCNN.2016.7727519
   SUCKLING J, 1994, INT CONGR SER, V1069, P375
   Wang XW, 2020, NEURAL NETWORKS, V125, P258, DOI 10.1016/j.neunet.2020.02.011
   Xie LZ, 2020, KNOWL-BASED SYST, V208, DOI 10.1016/j.knosys.2020.106465
   Yamazaki T, 2007, NEURAL NETWORKS, V20, P290, DOI 10.1016/j.neunet.2007.04.004
   Yan R, 2020, METHODS, V173, P52, DOI 10.1016/j.ymeth.2019.06.014
   Yang J., MEDMNIST CLASSIFICAT
NR 46
TC 0
Z9 0
U1 2
U2 15
PD MAR 24
PY 2022
VL 2022
AR 8369368
DI 10.1155/2022/8369368
WC Computer Science, Information Systems; Engineering, Electrical &
   Electronic; Telecommunications
DA 2023-11-11
ER

PT J
AU Cyr, A
   Thériault, F
   Chartier, S
AF Cyr, Andre
   Theriault, Frederic
   Chartier, Sylvain
TI Revisiting the XOR problem: a neurorobotic implementation
SO NEURAL COMPUTING & APPLICATIONS
DT Article
DE XOR; Neurorobotic; Spiking neural networks; Reinforcement learning;
   Spike timing-dependent plasticity; Operant conditioning
ID SPIKING NEURONS; REWARD; PLASTICITY; CATEGORIES; NETWORKS
AB The exclusive-or (XOR) classification task still represents a challenge in the study of cognition since the precise neural circuit sustaining the general ability to learn nonlinear problems remains to be discovered in natural organisms. As such, this paper focuses on a neurorobotic application embedding a specific spiking neural network built to solve these types of tasks. This experiment proposes a 2-bit task (XOR) with visual compound binary images acting as inputs and a left/right action for the output. The robot learns to solve it in both virtual and real environments from an operant conditioning procedure. Furthermore, the robot also adapts its behavior from learning all other simpler associative rules, even when switching them at runtime. Finally, this study explores the impact on the neural architecture, when passing from a 2-bit to a 3-bit task.
C1 [Cyr, Andre; Chartier, Sylvain] Ottawa Univ, 75 Laurier Ave East, Ottawa, ON K1N 6N5, Canada.
   [Theriault, Frederic] Cegep Vieux Montreal, 255 Ontario Est, Montreal, PQ H2X 1X6, Canada.
RP Cyr, A (corresponding author), Ottawa Univ, 75 Laurier Ave East, Ottawa, ON K1N 6N5, Canada.
EM acyr2@uottawa.ca; ftheriault@cvm.qc.ca; sylvain.chartier@uottawa.ca
CR [Anonymous], 2015, ARXIV150309129
   Behrends A, 2012, J EXP BIOL, V215, P1076, DOI 10.1242/jeb.063297
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Caporale N, 2008, ANNU REV NEUROSCI, V31, P25, DOI 10.1146/annurev.neuro.31.060407.125639
   Casacuberta D, 2012, MACHINE LEARNING CON
   Cyr A, 2014, FRONT NEUROROBOTICS, V8, P1, DOI 10.3389/fnbot.2014.00021
   Cyr A, 2009, NEURAL COMPUT APPL, V18, P431, DOI 10.1007/s00521-009-0254-2
   Deisig N, 2001, LEARN MEMORY, V8, P70, DOI 10.1101/lm.8.2.70
   El-Laithy K, 2011, COMPUT INTEL NEUROSC, V2011, P4
   Frémaux N, 2010, J NEUROSCI, V30, P13326, DOI 10.1523/JNEUROSCI.6249-09.2010
   Gerstner W., 2002, SPIKING NEURON MODEL
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Giurfa M, 2001, NATURE, V410, P930, DOI 10.1038/35073582
   Grand C, 2008, J EXP PSYCHOL-ANIM B, V34, P486, DOI 10.1037/0097-7403.34.4.486
   Hammer M, 1997, TRENDS NEUROSCI, V20, P245, DOI 10.1016/S0166-2236(96)01019-3
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Jitsumori M, 2011, LEARN BEHAV, V39, P271, DOI 10.3758/s13420-011-0028-4
   Krichmar JL, 2018, FRONT NEUROROBOTICS, V12, DOI 10.3389/fnbot.2018.00042
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   Minsky M., 1969, CAMBRIDGE TIASS HIT
   Paul C, 2006, ROBOT AUTON SYST, V54, P619, DOI 10.1016/j.robot.2006.03.003
   Pawlak V, 2010, FRONT SYNAPTIC NEURO, V2, P138
   Pezzulo G, 2011, FRONT PSYCHOL, V2, DOI 10.3389/fpsyg.2011.00005
   Reljan-Delaney M, 2017, 2017 COMPUTING CONFERENCE, P701, DOI 10.1109/SAI.2017.8252173
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Schultz W, 1998, J NEUROPHYSIOL, V80, P1, DOI 10.1152/jn.1998.80.1.1
   Seung HS, 2003, NEURON, V40, P1063, DOI 10.1016/S0896-6273(03)00761-X
   Smith JD, 2011, J EXP PSYCHOL-ANIM B, V37, P20, DOI 10.1037/a0019497
   Unoki S, 2006, EUR J NEUROSCI, V24, P2031, DOI 10.1111/j.1460-9568.2006.05099.x
   Vassiliades V, 2016, NEURAL NETWORKS, V74, P35, DOI 10.1016/j.neunet.2015.11.001
   Zbeda R, 2005, J COMPUTING SCI COLL, V20, P144
   Zeigler BP, 2017, SYSTEMS, V5, DOI 10.3390/systems5010007
NR 33
TC 4
Z9 4
U1 1
U2 7
PD JUL
PY 2020
VL 32
IS 14
BP 9965
EP 9973
DI 10.1007/s00521-019-04522-0
EA OCT 2019
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT C
AU Poulsen, TM
   Moore, RK
AF Poulsen, Thomas M.
   Moore, Roger K.
BE Alippi, C
   Polycarppou, M
   Panayiotou, C
   Ellinas, G
TI Evolving Spiking Neural Parameters for Behavioral Sequences
SO ARTIFICIAL NEURAL NETWORKS - ICANN 2009, PT II
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 19th International Conference on Artificial Neural Networks (ICANN 2009)
CY SEP 14-17, 2009
CL Limassol, CYPRUS
ID MEMORY; TIME
AB Sequential behavior has been the subject of numerous studies that, involve agent simulations. In such research, investigators often develop and examine neural networks that attempt to produce a sequence of outputs. Results have provided important insights into neural network designs but they offer a limited understanding of the underlying neural mechanisms. It is therefore still unclear how relevant neural parameters can advantageously be employed to alter motor output throughout a sequence of behavior. Here we implement a biologically based spiking neural network for different sequential tasks and investigate some of the neural mechanisms involved. It is demonstrated how a genetic algorithm can be employed to successfully evolve a range of neural parameters for different sequential tasks.
C1 [Poulsen, Thomas M.; Moore, Roger K.] Univ Sheffield, Dept Comp Sci, Sheffield S1 4DP, S Yorkshire, England.
RP Poulsen, TM (corresponding author), Univ Sheffield, Dept Comp Sci, 211 Portobello, Sheffield S1 4DP, S Yorkshire, England.
EM t.poulsen@dcs.shef.ac.uk; r.k.moore@dcs.shef.ac.uk
CR Abarbanel HDI, 2004, J NEUROPHYSIOL, V92, P96, DOI 10.1152/jn.01146.2003
   [Anonymous], FDN GENETIC ALGORITH
   BOHTE SM, 2000, NAT COMPUT, V2, P195
   Capi G, 2005, ADAPT BEHAV, V13, P53, DOI 10.1177/105971230501300103
   DANCE E, 2002, BIO CYB, V87, P185
   Floreano D., 2001, LNCS, P38
   Fuster JM, 2001, NEURON, V30, P319, DOI 10.1016/S0896-6273(01)00285-9
   GEORGOPOULOS AP, 1983, EXP BRAIN RES, V49, P327
   Gonzalez-Nalda P, 2008, NEUROCOMPUTING, V71, P721, DOI 10.1016/j.neucom.2007.07.032
   KINNEY GA, 1994, J NEUROPHYSIOL, V72, P1588, DOI 10.1152/jn.1994.72.4.1588
   Kistler WM, 2002, NEURAL COMPUT, V14, P2597, DOI 10.1162/089976602760407991
   Kistler WM, 1997, NEURAL COMPUT, V9, P1015, DOI 10.1162/neco.1997.9.5.1015
   Koch C, 1996, CEREB CORTEX, V6, P93, DOI 10.1093/cercor/6.2.93
   Poulsen TM, 2007, 2007 IEEE SYMPOSIUM ON FOUNDATIONS OF COMPUTATIONAL INTELLIGENCE, VOLS 1 AND 2, P350, DOI 10.1109/FOCI.2007.371495
   POULSEN TM, THESIS
   Rieke F., 1997, SPIKES EXPLORING NEU
   Saggie-Wexler K, 2006, ARTIF LIFE, V12, P1, DOI 10.1162/106454606775186428
   Squire L., 2008, FUNDAMENTAL NEUROSCI
   STORM JF, 1989, J PHYSIOL-LONDON, V409, P171, DOI 10.1113/jphysiol.1989.sp017491
   Xing J, 2000, J NEUROPHYSIOL, V84, P651, DOI 10.1152/jn.2000.84.2.651
   Yamauchi Brian M., 1994, Adaptive Behavior, V2, P219, DOI 10.1177/105971239400200301
   ZIPSER D, 1993, J NEUROSCI, V13, P3406
NR 22
TC 1
Z9 1
U1 0
U2 0
PY 2009
VL 5769
BP 784
EP 793
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods
DA 2023-11-11
ER

PT J
AU Nie, YM
   Fellous, JM
   Tatsuno, M
AF Nie, Yimin
   Fellous, Jean-Marc
   Tatsuno, Masami
TI Information-geometric measures estimate neural interactions during
   oscillatory brain states
SO FRONTIERS IN NEURAL CIRCUITS
DT Article
DE information geometry; spikes; spiking neuron model; oscillation; neural
   networks
ID THETA-OSCILLATIONS; PREFRONTAL CORTEX; SPIKING ACTIVITY; UNITARY EVENTS;
   MEMORY; MULTIPLE; PATTERNS; NETWORK; SLEEP; REACTIVATION
AB The characterization of functional network structures among multiple neurons is essential to understanding neural information processing. Information geometry (IG), a theory developed for investigating a space of probability distributions has recently been applied to spike-train analysis and has provided robust estimations of neural interactions. Although neural firing in the equilibrium state is often assumed in these studies, in reality, neural activity is non-stationary. The brain exhibits various oscillations depending on cognitive demands or when an animal is asleep. Therefore, the investigation of the IG measures during oscillatory network states is important for testing how the IG method can be applied to real neural data. Using model networks of binary neurons or more realistic spiking neurons, we studied how the single- and pairwise-IG measures were influenced by oscillatory neural activity. Two general oscillatory mechanisms, externally driven oscillations and internally induced oscillations, were considered. In both mechanisms, we found that the single-IG measure was linearly related to the magnitude of the external input, and that the pairwise-IG measure was linearly related to the sum of connection strengths between two neurons. We also observed that the pairwise-IG measure was not dependent on the oscillation frequency. These results are consistent with the previous findings that were obtained under the equilibrium conditions. Therefore, we demonstrate that the IG method provides useful insights into neural interactions under the oscillatory condition that can often be observed in the real brain.
C1 [Nie, Yimin; Tatsuno, Masami] Univ Lethbridge, Canadian Ctr Behav Neurosci, Dept Neurosci, Lethbridge, AB T1K 3M4, Canada.
   [Fellous, Jean-Marc] Univ Arizona, Dept Psychol, Program Appl Math, Tucson, AZ 85721 USA.
RP Tatsuno, M (corresponding author), Univ Lethbridge, Canadian Ctr Behav Neurosci, Dept Neurosci, 4401 Univ Dr W, Lethbridge, AB T1K 3M4, Canada.
EM tatsuno@uleth.ca
CR ABELES M, 1988, J NEUROPHYSIOL, V60, P909, DOI 10.1152/jn.1988.60.3.909
   Adini Y, 1997, P NATL ACAD SCI USA, V94, P10426, DOI 10.1073/pnas.94.19.10426
   AERTSEN AMHJ, 1989, J NEUROPHYSIOL, V61, P900, DOI 10.1152/jn.1989.61.5.900
   Amari S, 2003, NEURAL COMPUT, V15, P127, DOI 10.1162/089976603321043720
   Amari S, 2001, IEEE T INFORM THEORY, V47, P1701, DOI 10.1109/18.930911
   Amari S, 2009, NEURAL COMPUT, V21, P960, DOI 10.1162/neco.2008.03-08-729
   Baker SN, 1997, J PHYSIOL-LONDON, V501, P225, DOI 10.1111/j.1469-7793.1997.225bo.x
   BLAND BH, 1986, PROG NEUROBIOL, V26, P1, DOI 10.1016/0301-0082(86)90019-5
   Brown EN, 2004, NAT NEUROSCI, V7, P456, DOI 10.1038/nn1228
   Buzsáki G, 2004, NAT NEUROSCI, V7, P446, DOI 10.1038/nn1233
   BUZSAKI G, 1992, SCIENCE, V256, P1025, DOI 10.1126/science.1589772
   Buzsáki G, 2002, NEURON, V33, P325, DOI 10.1016/S0896-6273(02)00586-X
   Buzsáki G, 2012, ANNU REV NEUROSCI, V35, P203, DOI 10.1146/annurev-neuro-062111-150444
   Chapin JK, 1999, NAT NEUROSCI, V2, P664, DOI 10.1038/10223
   Contreras EJB, 2013, NEURON, V79, P555, DOI 10.1016/j.neuron.2013.06.013
   Crunelli V, 2010, NAT NEUROSCI, V13, P10, DOI 10.1038/nn.2445
   Czanner G, 2005, NEURAL COMPUT, V17, P1456, DOI 10.1162/0899766053723041
   Davidson TJ, 2009, NEURON, V63, P497, DOI 10.1016/j.neuron.2009.07.027
   Diekelmann S, 2010, NAT REV NEUROSCI, V11, P114, DOI 10.1038/nrn2762
   Dragoi G, 1999, J NEUROSCI, V19, P6191, DOI 10.1523/JNEUROSCI.19-14-06191.1999
   Dragoi G, 2013, P NATL ACAD SCI USA, V110, P9100, DOI 10.1073/pnas.1306031110
   Dragoi G, 2011, NATURE, V469, P397, DOI 10.1038/nature09633
   Ego-Stengel V, 2010, HIPPOCAMPUS, V20, P1, DOI 10.1002/hipo.20707
   Eleuteri A, 2005, NEURAL NETWORKS, V18, P1309, DOI 10.1016/j.neunet.2005.01.008
   Euston DR, 2007, SCIENCE, V318, P1147, DOI 10.1126/science.1148979
   Fellous JM, 2004, J NEUROSCI, V24, P2989, DOI 10.1523/JNEUROSCI.4649-03.2004
   GERSTEIN GL, 1969, SCIENCE, V164, P828, DOI 10.1126/science.164.3881.828
   Gilestro GF, 2009, SCIENCE, V324, P109, DOI 10.1126/science.1166673
   GINZBURG I, 1994, PHYS REV E, V50, P3171, DOI 10.1103/PhysRevE.50.3171
   Girardeau G, 2009, NAT NEUROSCI, V12, P1222, DOI 10.1038/nn.2384
   GRAY CM, 1989, NATURE, V338, P334, DOI 10.1038/338334a0
   Grün S, 2002, NEURAL COMPUT, V14, P81, DOI 10.1162/089976602753284464
   Grün S, 2002, NEURAL COMPUT, V14, P43, DOI 10.1162/089976602753284455
   HANSEL D, 1995, NEURAL COMPUT, V7, P307, DOI 10.1162/neco.1995.7.2.307
   Hoffman KL, 2002, SCIENCE, V297, P2070, DOI 10.1126/science.1073538
   Huber R, 2004, NATURE, V430, P78, DOI 10.1038/nature02663
   Ikeda K, 2005, NEURAL COMPUT, V17, P2719, DOI 10.1162/089976605774320593
   Ince RAA, 2010, NEURAL NETWORKS, V23, P713, DOI 10.1016/j.neunet.2010.05.008
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Kudrimoti HS, 1999, J NEUROSCI, V19, P4090
   Laubach M, 2000, NATURE, V405, P567, DOI 10.1038/35014604
   Lee AK, 2002, NEURON, V36, P1183, DOI 10.1016/S0896-6273(02)01096-6
   Lopes-dos-Santos V, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0020996
   Lovette IJ, 2012, MOL PHYLOGENET EVOL, V63, P219, DOI 10.1016/j.ympev.2011.07.009
   Luczak A, 2007, P NATL ACAD SCI USA, V104, P347, DOI 10.1073/pnas.0605643104
   Meltzer JA, 2008, CEREB CORTEX, V18, P1843, DOI 10.1093/cercor/bhm213
   MIROLLO RE, 1990, SIAM J APPL MATH, V50, P1645, DOI 10.1137/0150098
   Miura K, 2006, NEURAL COMPUT, V18, P2359, DOI 10.1162/neco.2006.18.10.2359
   Nagaoka H., 2000, METHODS INFORM GEOME, V191
   Nakahara H, 2006, NEURAL COMPUT, V18, P545, DOI 10.1162/089976606775623289
   Nakahara H, 2002, NEURAL COMPUT, V14, P2269, DOI 10.1162/08997660260293238
   Nie YM, 2012, NEURAL COMPUT, V24, P3213, DOI 10.1162/NECO_a_00367
   Panzeri S, 2001, NEURAL COMPUT, V13, P1311, DOI 10.1162/08997660152002870
   PAVLIDES C, 1989, J NEUROSCI, V9, P2907
   Peyrache A, 2010, J COMPUT NEUROSCI, V29, P309, DOI 10.1007/s10827-009-0154-6
   Peyrache A, 2009, NAT NEUROSCI, V12, P919, DOI 10.1038/nn.2337
   Raghavachari S, 2001, J NEUROSCI, V21, P3175, DOI 10.1523/JNEUROSCI.21-09-03175.2001
   Schneidman E, 2006, NATURE, V440, P1007, DOI 10.1038/nature04701
   Shimazaki H, 2007, NEURAL COMPUT, V19, P1503, DOI 10.1162/neco.2007.19.6.1503
   Shimokawa T, 2009, NEURAL COMPUT, V21, P1931, DOI 10.1162/neco.2009.08-08-841
   SINGER W, 1995, ANNU REV NEUROSCI, V18, P555, DOI 10.1146/annurev.neuro.18.1.555
   STERIADE M, 1993, J NEUROSCI, V13, P3252, DOI 10.1523/jneurosci.13-08-03252.1993
   Stickgold R, 2005, NATURE, V437, P1272, DOI 10.1038/nature04286
   Tang A, 2008, J NEUROSCI, V28, P505, DOI 10.1523/JNEUROSCI.3359-07.2008
   Tatsuno M, 2004, NEURAL COMPUT, V16, P737, DOI 10.1162/089976604322860686
   Tatsuno M, 2006, J NEUROSCI, V26, P10727, DOI 10.1523/JNEUROSCI.3317-06.2006
   Tatsuno M, 2009, NEURAL COMPUT, V21, P2309, DOI 10.1162/neco.2009.04-08-748
   Tyler AL, 2012, J NEUROSCI, V32, P11365, DOI 10.1523/JNEUROSCI.1516-12.2012
   Urban NN, 2001, HIPPOCAMPUS, V11, P408, DOI 10.1002/hipo.1055.abs
   VANDERWOLF CH, 1969, ELECTROEN CLIN NEURO, V26, P407, DOI 10.1016/0013-4694(69)90092-3
   WILSON MA, 1994, SCIENCE, V265, P676, DOI 10.1126/science.8036517
   WILSON MA, 1993, SCIENCE, V261, P1055, DOI 10.1126/science.8351520
   Zhang KC, 1998, J NEUROPHYSIOL, V79, P1017, DOI 10.1152/jn.1998.79.2.1017
NR 74
TC 4
Z9 4
U1 0
U2 12
PD FEB 24
PY 2014
VL 8
AR 11
DI 10.3389/fncir.2014.00011
WC Neurosciences
DA 2023-11-11
ER

PT J
AU Serrano-Fernández, L
   Beirán, M
   Parga, N
AF Serrano-Fernandez, Luis
   Beiran, Manuel
   Parga, Nestor
TI Bayesian computations in recurrent spiking neural networks trained to
   discriminate time intervals
SO JOURNAL OF COMPUTATIONAL NEUROSCIENCE
DT Meeting Abstract
C1 [Serrano-Fernandez, Luis; Parga, Nestor] Univ Autonoma Madrid, Dept Theoret Phys, Madrid, Spain.
   [Beiran, Manuel] PSL Univ, Lab Neurosci Cognit Computationnelle, Ecole Normale Super, Paris, France.
EM nestor.parga@uam.es
CR Egger SW, 2019, NAT NEUROSCI, V22, P1871, DOI 10.1038/s41593-019-0500-6
   Remington ED, 2018, NEURON, V98, P1005, DOI 10.1016/j.neuron.2018.05.020
NR 2
TC 0
Z9 0
U1 0
U2 0
PD DEC
PY 2021
VL 49
IS SUPPL 1
SU 1
SI SI
MA P136
BP S141
EP S143
WC Mathematical & Computational Biology; Neurosciences
DA 2023-11-11
ER

PT C
AU Hsieh, HY
   Li, PY
   Tang, KT
AF Hsieh, Hung-Yi
   Li, Pin-Yi
   Tang, Kea-Tiong
BE Liu, D
   Xie, S
   Li, Y
   Zhao, D
   ElAlfy, ESM
TI An Analog Probabilistic Spiking Neural Network with On-Chip Learning
SO NEURAL INFORMATION PROCESSING (ICONIP 2017), PT VI
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 24th International Conference on Neural Information Processing (ICONIP)
CY NOV 14-18, 2017
CL Guangzhou, PEOPLES R CHINA
DE Probabilistic spiking neural network (PSNN); Analog implementation;
   On-Chip learning
ID IMPLEMENTATION; HARDWARE
AB Portable or biomedical applications typically require signal processing, learning, and classification in conditions involving limited area and power consumption. Analog implementations of learning algorithms can satisfy these requirements and are thus attracting increasing attention. Probabilistic spiking neural network (PSNN) is a hardware friendly algorithm that is relax in weight resolution requirements and insensitive to noise and VLSI process variation. In this study, the probabilistic spiking neural network was implemented using analog very-large-scale integration (VLSI) to verify their hardware compatibility. The circuit was fabricated using 0.18 mu m CMOS technology. The power consumption of the chip was less than 10 mu W with a 1 V supply and the core area of chip was 0.43 mm(2). The chip can classify the electronic nose data with 92.3% accuracy and classify the electrocardiography data with 100% accuracy. The low power and high learning performance features make the chip suitable for portable or biomedical applications.
C1 [Hsieh, Hung-Yi; Li, Pin-Yi; Tang, Kea-Tiong] Natl Tsing Hua Univ, Dept Elect Engn, Neuromorph & Biomed Engn Lab, Hsinchu, Taiwan.
RP Tang, KT (corresponding author), Natl Tsing Hua Univ, Dept Elect Engn, Neuromorph & Biomed Engn Lab, Hsinchu, Taiwan.
EM hyhsieh@larc.ee.nthu.edu.tw; pinyili@foxmail.com; kttang@ee.nthu.edu.tw
CR Goldberger AL, 2000, CIRCULATION, V101, pE215, DOI 10.1161/01.CIR.101.23.e215
   Hsieh HY, 2013, IEEE T NEUR NET LEAR, V24, P2063, DOI 10.1109/TNNLS.2013.2271644
   Hsieh HY, 2012, IEEE T NEUR NET LEAR, V23, P1065, DOI 10.1109/TNNLS.2012.2195329
   Kasabov N, 2010, NEURAL NETWORKS, V23, P16, DOI 10.1016/j.neunet.2009.08.010
   Misra J, 2010, NEUROCOMPUTING, V74, P239, DOI 10.1016/j.neucom.2010.03.021
   Pouille F, 2009, NAT NEUROSCI, V12, P1577, DOI 10.1038/nn.2441
   Sun Q, 2011, IEEE T NEURAL NETWOR, V22, P858, DOI 10.1109/TNN.2011.2125986
   Sun YW, 2012, COMPUT BIOL MED, V42, P751, DOI 10.1016/j.compbiomed.2012.04.007
   Tarassenko L, 2001, NEURAL PROCESS LETT, V14, P15, DOI 10.1023/A:1011373923479
NR 9
TC 3
Z9 3
U1 0
U2 2
PY 2017
VL 10639
BP 777
EP 785
DI 10.1007/978-3-319-70136-3_82
PN VI
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods
DA 2023-11-11
ER

PT C
AU Aziza, H
   Bazzi, H
   Postel-Pellerin, J
   Canet, P
   Moreau, M
   Harb, A
AF Aziza, H.
   Bazzi, H.
   Postel-Pellerin, J.
   Canet, P.
   Moreau, M.
   Harb, A.
GP IEEE
TI An Augmented OxRAM Synapse for Spiking Neural Network (SNN) Circuits
SO 2019 14TH IEEE INTERNATIONAL CONFERENCE ON DESIGN & TECHNOLOGY OF
   INTEGRATED SYSTEMS IN NANOSCALE ERA (DTIS 2019)
DT Proceedings Paper
CT 14th IEEE International Conference on Design and Technology of
   Integrated Systems In Nanoscale Era (DTIS)
CY APR 16-18, 2019
CL Mykonos, GREECE
DE Memristor; Neuromorphic circuits; OxRAM; Synapses; CMOS neurons, Neural
   networks
AB In this paper, the conductance modulation of OxRAM memristive devices is evaluated based on experimental data to reveal the memristor inherent analog synaptic behavior. Simulation results are presented to validate the use of OxRAMs as synapses at a circuit level in a spiking neural network context. In the proposed approach, the OxRAM synapse is augmented with a shift register associated with current compliance control transistors to provide an efficient monitoring of the OxRAM conductance.
C1 [Aziza, H.; Bazzi, H.; Postel-Pellerin, J.; Canet, P.; Moreau, M.] Aix Marseille Univ, Univ Toulon, CNRS, IM2NP, Marseille, France.
   [Bazzi, H.] Lebanese Int Univ, Dept Elect & Elect Engn, Beirut, Lebanon.
   [Harb, A.] Int Univ Beirut, Dept Elect & Elect Engn, Beirut, Lebanon.
RP Aziza, H (corresponding author), Aix Marseille Univ, Univ Toulon, CNRS, IM2NP, Marseille, France.
CR Aitken R., 2014, P S VER LARG SCAL IN, P1, DOI [DOI 10.1109/VLSIT.2014.6894339, 10.1109/VLSIT.2014.6894339]
   Aziza H, 2018, SOLID STATE ELECTRON, V142, P52, DOI 10.1016/j.sse.2018.02.005
   Aziza H., 2016, 2016 International Conference on Design and Technology of Integrated Systems in Nanoscale Era (DTIS), P1, DOI 10.1109/DTIS.2016.7483892
   Aziza H., 2019, J ADV SCI TECHNOLOGY, V3, P11
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Bocquet M, 2014, IEEE T ELECTRON DEV, V61, P674, DOI 10.1109/TED.2013.2296793
   Chen WB, 2015, SEMICOND SCI TECH, V30, DOI 10.1088/0268-1242/30/7/075002
   Gamrat C., 1991, Proceedings of the 2nd International Conference on Microelectronics for Neural Networks, P463
   Hamdioui S., 2014, 2014 9 IEEE INT C, P1
   Hamdioui S, 2017, DES AUT TEST EUROPE, P722, DOI 10.23919/DATE.2017.7927083
   Indiveri Giacomo, 2011, Front Neurosci, V5, P118, DOI 10.3389/fnins.2011.00118
   Linn E, 2012, NANOTECHNOLOGY, V23, DOI 10.1088/0957-4484/23/30/305205
   Portal JM, 2017, IEEE T NANOTECHNOL, V16, P677, DOI 10.1109/TNANO.2017.2703985
   RUMELHART DE, 1985, COGNITIVE SCI, V9, P75, DOI 10.1016/S0364-0213(85)80010-0
   Simard PY, 2003, PROC INT CONF DOC, P958
   Vatajelu EI, 2014, INT DES TEST SYMP, P61, DOI 10.1109/IDT.2014.7038588
   Vianello E., 2014, IEEE INT EL DEV M IE
   Yu SM, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00186
NR 18
TC 11
Z9 11
U1 0
U2 0
PY 2019
DI 10.1109/dtis.2019.8735057
WC Computer Science, Theory & Methods; Engineering, Electrical &
   Electronic; Nanoscience & Nanotechnology
DA 2023-11-11
ER

PT J
AU Galanis, I
   Anagnostopoulos, I
   Nguyen, C
   Bares, G
AF Galanis, Ioannis
   Anagnostopoulos, Iraklis
   Nguyen, Chinh
   Bares, Guillermo
TI Efficient Deployment of Spiking Neural Networks on SpiNNaker
   Neuromorphic Platform
SO IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS II-EXPRESS BRIEFS
DT Article
DE Artificial neural networks; neural network hardware; neuromorphics
AB Spiking Neural Networks (SNNs) have emerged as serious competitors of the traditional Convolutional Neural Networks (CNNs), as they unlock new potential of implementing less complex and more energy efficient neural networks. Current deep CNNs can be converted to SNNs for fast deployment on neuromorphic devices, however existing methods do not investigate the impact of hardware-related parameters that directly affect the accuracy of an SNN. In this brief, we target the SpiNNaker neuromorphic platform and we demonstrate a fast exploration framework that effectively decides the configuration of the target board, in order to achieve the highest possible accuracy. Experimental results show that our method reaches 98.85% SNN accuracy on MNIST dataset, while reducing the exploration time by a factor of 3x compared to exhaustive search.
C1 [Galanis, Ioannis; Anagnostopoulos, Iraklis] Southern Illinois Univ, Dept Elect Comp & Biomed Engn, Carbondale, IL 62901 USA.
   [Nguyen, Chinh; Bares, Guillermo] Ford Res & Innovat Ctr, Dearborn, MI 48124 USA.
RP Galanis, I (corresponding author), Southern Illinois Univ, Dept Elect Comp & Biomed Engn, Carbondale, IL 62901 USA.
EM ioannis.galanis@siu.edu
CR Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   Furber SB, 2013, IEEE T COMPUT, V62, P2454, DOI 10.1109/TC.2012.142
   Hutter F, 2019, SPRING SER CHALLENGE, P1, DOI 10.1007/978-3-030-05318-5
   Liu C, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00840
   Massa R., 2020, ARXIV200609985
   Mundy A., 2015, P INT JOINT C NEUR N, P1, DOI DOI 10.1109/IJCNN.2015.7280390
   Painkras E, 2013, IEEE J SOLID-ST CIRC, V48, P1943, DOI 10.1109/JSSC.2013.2259038
   Patiño-Saucedo A, 2020, NEURAL NETWORKS, V121, P319, DOI 10.1016/j.neunet.2019.09.008
   Rhodes O, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00816
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Scheidegger F, 2021, VISUAL COMPUT, V37, P1593, DOI 10.1007/s00371-020-01922-5
   Stromatias E, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00222
   Tan MX, 2019, PROC CVPR IEEE, P2815, DOI [arXiv:1807.11626, 10.1109/CVPR.2019.00293]
   Urgese G., 2018, P CASES ESWEEK, P1
   Vineyard C. M., 2019, P 7 ANN NEUR COMP EL, P1
NR 15
TC 0
Z9 0
U1 0
U2 4
PD JUN
PY 2021
VL 68
IS 6
BP 1937
EP 1941
DI 10.1109/TCSII.2020.3047425
WC Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Pigareva, Y
   Malishev, E
   Gladkov, A
   Kolpakov, V
   Bukatin, A
   Mukhina, I
   Kazantsev, V
   Pimashkin, A
AF Pigareva, Y.
   Malishev, E.
   Gladkov, A.
   Kolpakov, V.
   Bukatin, A.
   Mukhina, I.
   Kazantsev, V.
   Pimashkin, A.
GP IOP
TI Neural signal registration and analysis of axons grown in microchannels
SO 3RD INTERNATIONAL SCHOOL AND CONFERENCE ON OPTOELECTRONICS, PHOTONICS,
   ENGINEERING AND NANOSTRUCTURES (SAINT PETERSBURG OPEN 2016)
SE Journal of Physics Conference Series
DT Proceedings Paper
CT 3rd International School and Conference on Optoelectronics, Photonics,
   Engineering and Nanostructures (Saint Petersburg OPEN)
CY MAR 28-30, 2016
CL Russian Acad Sci, St Petersburg Acad Univ, St Petersburg, RUSSIA
HO Russian Acad Sci, St Petersburg Acad Univ
AB Registration of neuronal bioelectrical signals remains one of the main physical tools to study fundamental mechanisms of signal processing in the brain. Neurons generate spiking patterns which propagate through complex map of neural network connectivity. Extracellular recording of isolated axons grown in microchannels provides amplification of the signal for detailed study of spike propagation. In this study we used neuronal hippocampal cultures grown in microfluidic devices combined with microelectrode arrays to investigate a changes of electrical activity during neural network development. We found that after 5 days in vitro after culture plating the spiking activity appears first in microchannels and on the next 2-3 days appears on the electrodes of overall neural network. We conclude that such approach provides a convenient method to study neural signal processing and functional structure development on a single cell and network level of the neuronal culture.
C1 [Pigareva, Y.; Gladkov, A.; Kolpakov, V.; Mukhina, I.; Kazantsev, V.; Pimashkin, A.] Lobachevsky State Univ Nizhny Novgorod, Ctr Translat Technol, Dept Neuroengn, Nizhny, Novgorod, Russia.
   [Malishev, E.; Bukatin, A.] St Petersburg Acad Univ, RAS, Nanotechnol Res & Educ Ctr, St Petersburg, Russia.
   [Gladkov, A.; Mukhina, I.] Nizhny Novgorod State Med Acad, Cell Technol Dept, Cent Res Lab, Nizhny, Novgorod, Russia.
RP Pigareva, Y (corresponding author), Lobachevsky State Univ Nizhny Novgorod, Ctr Translat Technol, Dept Neuroengn, Nizhny, Novgorod, Russia.
EM pigareva@neuro.nnov.ru
CR FitzGerald JJ, 2008, IEEE T BIO-MED ENG, V55, P1136, DOI 10.1109/TBME.2007.909533
   Habibey R, 2015, NEUROTECHNOLOGY ELEC, P13
   Le Feber J, 2015, FRONTIERS NEUROSCIEN, V9, P1
   Lewandowska M, 2015, PLOS ONE, V10, P1
   Malishev E, 2015, J PHYS CONF SER, V643, DOI 10.1088/1742-6596/643/1/012025
   Pan L, 2013, IEEE T NEUR SYS REH, V22, P453, DOI [10.1109/TNSRE.2013.2289911, DOI 10.1109/TNSRE.2013.2289911]
   Pimashkin A, 2013, FRONT NEURAL CIRCUIT, V7, DOI 10.3389/fncir.2013.00087
NR 7
TC 1
Z9 1
U1 0
U2 3
PY 2016
VL 741
AR 012057
DI 10.1088/1742-6596/741/1/012057
WC Engineering, Electrical & Electronic; Optics; Physics, Applied
DA 2023-11-11
ER

PT C
AU Ezenwe, I
   Joshi, A
   Wong-Lin, K
AF Ezenwe, Ifeatu
   Joshi, Alok
   Wong-Lin, KongFatt
GP IEEE
TI Genetic Algorithmic Parameter Optimisation of a Recurrent Spiking Neural
   Network Model
SO 2020 31ST IRISH SIGNALS AND SYSTEMS CONFERENCE (ISSC)
SE Irish Signals and Systems Conference
DT Proceedings Paper
CT 31st Irish Signals and Systems Conference (ISSC)
CY JUN 11-12, 2020
CL Letterkenny Inst Technol, ELECTR NETWORK
HO Letterkenny Inst Technol
DE Model parameter optimisation; genetic algorithm; recurrent spiking
   neural network model; Izhikevich neuronal model
ID NEURONS
AB Neural networks are complex algorithms that loosely model the behaviour of the human brain. They play a significant role in computational neuroscience and artificial intelligence. The next generation of neural network models is based on the spike timing activity of neurons: spiking neural networks (SNNs). However, model parameters in SNNs are difficult to search and optimise. Previous studies using genetic algorithm (GA) optimisation of SNNs were focused mainly on simple, feedforward, or oscillatory networks, but not much work has been done on optimising cortex-like recurrent SNNs. In this work, we investigated the use of GAs to search for optimal parameters in recurrent SNNs to reach targeted neuronal population firing rates, e.g. as in experimental observations. We considered a cortical column based SNN comprising 1000 Izhikevich spiking neurons for computational efficiency and biologically realism. The model parameters explored were the neuronal biased input currents. First, we found for this particular SNN, the optimal parameter values for targeted population averaged firing activities, and the convergence of algorithm by similar to 100 generations. We then showed that the GA optimal population size was within similar to 16-20 while the crossover rate that returned the best fitness value was similar to 0.95. Overall, we have successfully demonstrated the feasibility of implementing GA to optimize model parameters in a recurrent cortical based SNN.
C1 [Ezenwe, Ifeatu; Joshi, Alok; Wong-Lin, KongFatt] Ulster Univ, Intelligent Syst Res Ctr, Sch Comp Engn & Intelligent Syst, Magee Campus, Derry Londonderry, North Ireland.
RP Ezenwe, I (corresponding author), Ulster Univ, Intelligent Syst Res Ctr, Sch Comp Engn & Intelligent Syst, Magee Campus, Derry Londonderry, North Ireland.
EM ifeatuezenwe@gmail.com; ajoshi@ulster.ac.uk; k.wong-lin@ulster.ac.uk
CR [Anonymous], 2010, DYNAMICAL SYSTEMS NE
   [Anonymous], 1991, HDB GENETIC ALGORITH
   Brette R, 2005, J NEUROPHYSIOL, V94, P3637, DOI 10.1152/jn.00686.2005
   Chipperfield A., 1995, P IEE C APPL CONTR T, DOI DOI 10.1049/IC:19950061
   Dayan P., 2001, THEORETICAL NEUROSCI
   Douglas RJ, 2007, NEURON, V56, P226, DOI 10.1016/j.neuron.2007.10.017
   Ermentrout B, 1996, NEURAL COMPUT, V8, P979, DOI 10.1162/neco.1996.8.5.979
   Ezenwe I., 2019, THESIS ULSTER U
   FITZHUGH R, 1961, BIOPHYS J, V1, P445, DOI 10.1016/S0006-3495(61)86902-6
   Furber SB, 2013, IEEE T COMPUT, V62, P2454, DOI 10.1109/TC.2012.142
   Gerstner W., 2002, SPIKING NEURON MODEL
   Goldberg D.E, 1989, GENETIC ALGORITHMS S, V27, P27
   Grillner S, 1997, NEURONS NETWORKS MOT
   Hizak J, 2016, TEH GLAS, V10, P55
   Huh D, 2018, ADV NEUR IN, V31
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Jin YC, 2007, LECT NOTES COMPUT SC, V4668, P370
   Kasabov N, 2008, IEEE COMPUT INTELL M, V3, P23, DOI 10.1109/MCI.2008.926584
   Kochenderfer M. J., 2019, ALGORITHMS OPTIMIZAT
   Lapique L., 1907, J PHYSL PATHOL GEN, V9, P620, DOI DOI 10.1007/S00422-007-0189-6
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   McCall J, 2005, J COMPUT APPL MATH, V184, P205, DOI 10.1016/j.cam.2004.07.034
   Mihalas S, 2009, NEURAL COMPUT, V21, P704, DOI 10.1162/neco.2008.12-07-680
   Mitchell M., 2002, INTRO GENETIC ALGORI
   MORRIS C, 1981, BIOPHYS J, V35, P193, DOI 10.1016/S0006-3495(81)84782-0
   NAGUMO J, 1962, P IRE, V50, P2061, DOI 10.1109/JRPROC.1962.288235
   Negnevitsky M., 2005, ARTIFICIAL INTELLIGE, V2nd, P219
   Oros N, 2009, IEEE SYMP ART LIFE, P116, DOI 10.1109/ALIFE.2009.4937702
   Pei J, 2019, NATURE, V572, P106, DOI 10.1038/s41586-019-1424-8
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Ponulak F, 2011, ACTA NEUROBIOL EXP, V71, P409
   Potjans TC, 2014, CEREB CORTEX, V24, P785, DOI 10.1093/cercor/bhs358
   Russell A, 2010, IEEE T NEURAL NETWOR, V21, P1950, DOI 10.1109/TNN.2010.2083685
   Sussillo D, 2009, NEURON, V63, P544, DOI 10.1016/j.neuron.2009.07.018
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Tononi G, 1999, P NATL ACAD SCI USA, V96, P3257, DOI 10.1073/pnas.96.6.3257
   WILSON HR, 1972, BIOPHYS J, V12, P1, DOI 10.1016/S0006-3495(72)86068-5
   Wong-Lin K, 2012, NEURAL NETWORKS, V32, P15, DOI 10.1016/j.neunet.2012.02.009
   Wrobel B., 2012, EVONET2012 EV NETW S, P19
NR 40
TC 0
Z9 0
U1 0
U2 0
PY 2020
BP 202
EP 207
WC Automation & Control Systems; Computer Science, Artificial Intelligence;
   Computer Science, Information Systems; Computer Science, Software
   Engineering; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Kim, J
   Lee, S
   Kim, CH
   Park, BG
   Lee, JH
AF Kim, Jangsaeng
   Lee, Soochang
   Kim, Chul-Heung
   Park, Byung-Gook
   Lee, Jong-Ho
TI Analog synaptic devices applied to spiking neural networks for
   reinforcement learning applications
SO SEMICONDUCTOR SCIENCE AND TECHNOLOGY
DT Article
DE hardware-based spiking neural networks (SNNs); reinforcement learning
   (RL); deep Q-learning algorithm; neuromorphic; TFT-type flash synaptic
   device
ID MEMORY
AB In this work, we implement hardware-based spiking neural network (SNN) using the thin-film transistor (TFT)-type flash synaptic devices. A hardware-based SNN architecture with synapse arrays and integrate-and-fire (I&F) neuron circuits is presented for executing reinforcement learning (RL). Two problems were used to evaluate the applicability of the proposed hardware-based SNNs to off-chip RL: the Cart Pole balancing problem and the Rush Hour problem. The neural network was trained using a deep Q-learning algorithm. The proposed hardware-based SNNs using the synapse model with measured characteristics successfully solve the two problems and show high performance, implying that the networks are suitable for executing RL. Furthermore, the effect of variations in non-ideal synaptic devices and neurons on the performance was investigated.
C1 [Kim, Jangsaeng; Lee, Soochang; Kim, Chul-Heung; Park, Byung-Gook; Lee, Jong-Ho] Seoul Natl Univ, Dept Elect & Comp Engn, Seoul 08826, South Korea.
   [Lee, Jong-Ho] Seoul Natl Univ, Interuniv Semicond Res Ctr ISRC, Seoul 08826, South Korea.
RP Lee, JH (corresponding author), Seoul Natl Univ, Dept Elect & Comp Engn, Seoul 08826, South Korea.; Lee, JH (corresponding author), Seoul Natl Univ, Interuniv Semicond Res Ctr ISRC, Seoul 08826, South Korea.
EM jhl@snu.ac.kr
CR Ambrogio S, 2018, NATURE, V558, P60, DOI 10.1038/s41586-018-0180-5
   Diehl Peter U, 2015, 2015 INT JOINT C NEU, P1, DOI [DOI 10.1109/IJCNN.2015.7280696, 10.1109/IJCNN.2015.7280696]
   Ernoult M, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-018-38181-3
   Gong N, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-04485-1
   Ielmini D, 2020, ADV INTELL SYST-GER, V2, DOI 10.1002/aisy.202000040
   Ielmini D, 2020, NANOTECHNOLOGY, V31, DOI 10.1088/1361-6528/ab554b
   Ielmini D, 2018, NAT ELECTRON, V1, P333, DOI 10.1038/s41928-018-0092-2
   Indiveri G, 2015, P IEEE, V103, P1379, DOI 10.1109/JPROC.2015.2444094
   Kang WM, 2019, IEEE IJCNN
   Kim CH, 2018, IEEE T ELECTRON DEV, V65, P1774, DOI 10.1109/TED.2018.2817266
   Kim SJ, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-21621-5
   Kuzum D, 2013, NANOTECHNOLOGY, V24, DOI 10.1088/0957-4484/24/38/382001
   Kwon D, 2019, IEEE T ELECTRON DEV, V66, P395, DOI 10.1109/TED.2018.2879821
   Lee JH, 2019, IEEE T ELECTRON DEV, V66, P2172, DOI 10.1109/TED.2019.2906249
   Masquelier T., 2010, INT JOINT C NEURAL N, V1 8 1-8
   McKee S A., 2004, COMPUTING FRONTIERS, P162, DOI DOI 10.1145/977091.977115
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Querlioz D., 2011, 2011 IEEE/ACM International Symposium on Nanoscale Architectures (NANOARCH), P150, DOI 10.1109/NANOARCH.2011.5941497
   Querlioz D, 2013, IEEE T NANOTECHNOL, V12, P288, DOI 10.1109/TNANO.2013.2250995
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Shafiee A, 2016, CONF PROC INT SYMP C, P14, DOI 10.1109/ISCA.2016.12
   Sidler S, 2017, LECT NOTES COMPUT SC, V10613, P281, DOI 10.1007/978-3-319-68600-4_33
   Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270
   Spanò S, 2019, IEEE ACCESS, V7, P186340, DOI 10.1109/ACCESS.2019.2961174
   Sun J.(, 2015, IEEE I CONF COMP VIS, P1026, DOI DOI 10.1109/ICCV.2015.123
   Sun XY, 2019, IEEE J EM SEL TOP C, V9, P570, DOI 10.1109/JETCAS.2019.2933148
   Suri M, 2011, 2011 IEEE INTERNATIONAL ELECTRON DEVICES MEETING (IEDM)
   Tavanaei A, 2019, NEUROCOMPUTING, V330, P39, DOI 10.1016/j.neucom.2018.11.014
   van Hasselt H, 2016, AAAI CONF ARTIF INTE, P2094
   Volodymyr Mnih, 2013, ARXIV PREPRINT ARXIV, DOI DOI 10.48550/ARXIV.1312.5602
   Wang Z., 2016, ARXIV151106581, V1, P115
   Wu N., 2020, ARXIV200106930, P110
   Yu S., 2017, IEDM
   Yu SM, 2018, P IEEE, V106, P260, DOI 10.1109/JPROC.2018.2790840
   Yu SM, 2012, 2012 IEEE INTERNATIONAL ELECTRON DEVICES MEETING (IEDM)
NR 35
TC 0
Z9 0
U1 4
U2 23
PD JUL 1
PY 2022
VL 37
IS 7
AR 075002
DI 10.1088/1361-6641/ac6ae0
WC Engineering, Electrical & Electronic; Materials Science,
   Multidisciplinary; Physics, Condensed Matter
DA 2023-11-11
ER

PT J
AU Comsa, IM
   Versari, L
   Fischbacher, T
   Alakuijala, J
AF Comsa, Iulia-Maria
   Versari, Luca
   Fischbacher, Thomas
   Alakuijala, Jyrki
TI Spiking Autoencoders With Temporal Coding
SO FRONTIERS IN NEUROSCIENCE
DT Article
DE spiking networks; temporal coding; latency coding; backpropagation;
   autoencoders; inhibition; biologically-inspired artificial intelligence
ID NETWORKS; NEURONS
AB Spiking neural networks with temporal coding schemes process information based on the relative timing of neuronal spikes. In supervised learning tasks, temporal coding allows learning through backpropagation with exact derivatives, and achieves accuracies on par with conventional artificial neural networks. Here we introduce spiking autoencoders with temporal coding and pulses, trained using backpropagation to store and reconstruct images with high fidelity from compact representations. We show that spiking autoencoders with a single layer are able to effectively represent and reconstruct images from the neuromorphically-encoded MNIST and FMNIST datasets. We explore the effect of different spike time target latencies, data noise levels and embedding sizes, as well as the classification performance from the embeddings. The spiking autoencoders achieve results similar to or better than conventional non-spiking autoencoders. We find that inhibition is essential in the functioning of the spiking autoencoders, particularly when the input needs to be memorised for a longer time before the expected output spike times. To reconstruct images with a high target latency, the network learns to accumulate negative evidence and to use the pulses as excitatory triggers for producing the output spikes at the required times. Our results highlight the potential of spiking autoencoders as building blocks for more complex biologically-inspired architectures. We also provide open-source code for the model.
C1 [Comsa, Iulia-Maria; Versari, Luca; Fischbacher, Thomas; Alakuijala, Jyrki] Google Res, Zurich, Switzerland.
RP Comsa, IM (corresponding author), Google Res, Zurich, Switzerland.
EM iuliacomsa@google.com
CR Abadi M., 2016, P OSDI 16 12 USENIX, V16, P265, DOI DOI 10.1038/NN.3331
   Abbott LF, 2016, NAT NEUROSCI, V19, P350, DOI 10.1038/nn.4241
   Ahmed FYH, 2013, MATH PROBL ENG, V2013, DOI 10.1155/2013/257085
   [Anonymous], 2015, STDP BIOLOGICALLY PL
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Blouw P, 2020, INT CONF ACOUST SPEE, P8534, DOI [10.1109/ICASSP40776.2020.9053043, 10.1109/icassp40776.2020.9053043]
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Booij O, 2005, INFORM PROCESS LETT, V95, P552, DOI 10.1016/j.ipl.2005.05.023
   Brette R, 2015, FRONT SYST NEUROSCI, V9, DOI 10.3389/fnsys.2015.00151
   Burbank KS, 2015, PLOS COMPUT BIOL, V11, DOI 10.1371/journal.pcbi.1004566
   Buzsáki G, 2002, NEURON, V33, P325, DOI 10.1016/S0896-6273(02)00586-X
   Comsa IM, 2022, IEEE T NEUR NET LEAR, V33, P5939, DOI 10.1109/TNNLS.2021.3071976
   Corless RM, 1996, ADV COMPUT MATH, V5, P329, DOI 10.1007/BF02124750
   Denève S, 2016, NAT NEUROSCI, V19, P375, DOI 10.1038/nn.4243
   Falez P, 2019, PATTERN RECOGN, V93, P418, DOI 10.1016/j.patcog.2019.04.016
   Glorot X., 2010, P 13 INT C ARTIFICIA, V13, P249, DOI DOI 10.1.1/207.2059
   Gollisch T, 2008, SCIENCE, V319, P1108, DOI 10.1126/science.1149639
   Golovin D, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1487, DOI 10.1145/3097983.3098043
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hong CF, 2020, IEEE T NEUR NET LEAR, V31, P1285, DOI 10.1109/TNNLS.2019.2919662
   Johansson RS, 2009, NAT REV NEUROSCI, V10, P345, DOI 10.1038/nrn2621
   Jones LM, 2004, SCIENCE, V304, P1986, DOI 10.1126/science.1097779
   Kheradpisheh SR, 2020, INT J NEURAL SYST, V30, DOI 10.1142/S0129065720500276
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Kingma DP, 2019, FOUND TRENDS MACH LE, V12, P4, DOI 10.1561/2200000056
   Krauss P, 2020, FRONT COMPUT NEUROSC, V14, DOI 10.3389/fncom.2020.556544
   Lambert J.H., 1758, ACTA HELVETICA, V3, P128
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   LeCun Y, 2015, NATURE, V521, p7553 436 444, DOI [10.1038/nature14539, DOI 10.1038/NATURE14539]
   Lillicrap TP, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms13276
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Masquelier T, 2007, PLOS COMPUT BIOL, V3, P247, DOI 10.1371/journal.pcbi.0030031
   Maziarz K., 2018, EVOLUTIONARY NEURAL
   McKennoch S, 2006, IEEE IJCNN, P3970
   Mostafa Hesham, 2018, IEEE Trans Neural Netw Learn Syst, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   RALL W, 1967, J NEUROPHYSIOL, V30, P1138, DOI 10.1152/jn.1967.30.5.1138
   Reinagel P, 2000, J NEUROSCI, V20, P5392, DOI 10.1523/JNEUROSCI.20-14-05392.2000
   Roy D, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00621
   Sakemi Y, 2020, SUPERVISED LEARNING
   Schrauwen B, 2004, IEEE IJCNN, P471, DOI 10.1109/IJCNN.2004.1379954
   Smith J.E., 2021, TEMPORAL NEURAL NETW
   Sterratt D., 2011, PRINCIPLES COMPUTATI, P172, DOI 10.1017/CBO9780511975899.008
   THORPE SJ, 1989, CONNECTIONISM IN PERSPECTIVE, P63
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Van Rullen R, 2001, NEURAL COMPUT, V13, P1255, DOI 10.1162/08997660152002852
   Vaswani A, 2017, ADV NEUR IN, V30
   Vincent P., 2008, P 25 INT C MACH LEAR, P1096, DOI [DOI 10.1145/1390156.1390294, 10.1145/1390156.1390294]
   Vincent P, 2010, J MACH LEARN RES, V11, P3371
   Wang JL, 2017, IEEE T NEUR NET LEAR, V28, P30, DOI 10.1109/TNNLS.2015.2501322
   Xiao Han, 2017, ARXIV170807747, P4321
   Yamins DLK, 2016, NAT NEUROSCI, V19, P356, DOI 10.1038/nn.4244
   Zenke F, 2021, NEURON, V109, P571, DOI 10.1016/j.neuron.2021.01.009
   Zhang M., 2020, ARXIV PREPRINT ARXIV
NR 55
TC 3
Z9 3
U1 0
U2 10
PD AUG 13
PY 2021
VL 15
AR 712667
DI 10.3389/fnins.2021.712667
WC Neurosciences
DA 2023-11-11
ER

PT C
AU Kiselev, I
   Neil, D
   Liu, SC
AF Kiselev, Ilya
   Neil, Daniel
   Liu, Shih-Chii
GP IEEE
TI Event-Driven Deep Neural Network Hardware System for Sensor Fusion
SO 2016 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS (ISCAS)
SE IEEE International Symposium on Circuits and Systems
DT Proceedings Paper
CT IEEE International Symposium on Circuits and Systems (ISCAS)
CY MAY 22-25, 2016
CL Montreal, CANADA
DE Spiking Deep Networks; Dynamic Vision Sensor; event-driven sensors;
   sensor fusion; hardware spiking network
AB This paper presents a real-time multi-modal spiking Deep Neural Network (DNN) implemented on an FPGA platform. The hardware DNN system, called n-Minitaur, demonstrates a 4-fold improvement in computational speed over the previous DNN FPGA system. The proposed system directly interfaces two different event-based sensors: a Dynamic Vision Sensor (DVS) and a Dynamic Audio Sensor (DAS). The DNN for this bimodal hardware system is trained on the MNIST digit dataset and a set of unique audio tones for each digit. When tested on the spikes produced by each sensor alone, the classification accuracy is around 70% for DVS spikes generated in response to displayed MNIST images, and 60% for DAS spikes generated in response to noisy tones. The accuracy increases to 98% when spikes from both modalities are provided simultaneously. In addition, the system shows a fast latency response of only 5ms.
C1 [Kiselev, Ilya] Univ Zurich, Inst Neuroinformat, CH-8057 Zurich, Switzerland.
   Swiss Fed Inst Technol, CH-8057 Zurich, Switzerland.
RP Kiselev, I (corresponding author), Univ Zurich, Inst Neuroinformat, CH-8057 Zurich, Switzerland.
CR Chan V, 2012, FRONT NEUROSCI, V6, P1
   Lichtsteiner P, 2008, IEEE J SOLID-ST CIRC, V43, P566, DOI 10.1109/JSSC.2007.914337
   Liu SC, 2014, IEEE T BIOMED CIRC S, V8, P453, DOI 10.1109/TBCAS.2013.2281834
   Neil D, 2014, IEEE T VLSI SYST, V22, P2621, DOI 10.1109/TVLSI.2013.2294916
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   Stromatias E, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00222
   Zamarreño-Ramos C, 2013, IEEE T BIOMED CIRC S, V7, P82, DOI 10.1109/TBCAS.2012.2195725
NR 7
TC 12
Z9 12
U1 0
U2 6
PY 2016
BP 2495
EP 2498
WC Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Hulea, M
   Uleru, GI
   Caruntu, CF
AF Hulea, Mircea
   Uleru, George Iulian
   Caruntu, Constantin Florin
TI Adaptive SNN for Anthropomorphic Finger Control
SO SENSORS
DT Article
DE spiking neural networks; neuromorphic hardware; Hebbian learning;
   anthropomorphic finger
ID SPIKING NEURAL-NETWORKS; MUSCLE-SPINDLE; MODEL
AB Anthropomorphic hands that mimic the smoothness of human hand motions should be controlled by artificial units of high biological plausibility. Adaptability is among the characteristics of such control units, which provides the anthropomorphic hand with the ability to learn motions. This paper presents a simple structure of an adaptive spiking neural network implemented in analogue hardware that can be trained using Hebbian learning mechanisms to rotate the metacarpophalangeal joint of a robotic finger towards targeted angle intervals. Being bioinspired, the spiking neural network drives actuators made of shape memory alloy and receives feedback from neuromorphic sensors that convert the joint rotation angle and compression force into the spiking frequency. The adaptive SNN activates independent neural paths that correspond to angle intervals and learns in which of these intervals the rotation the finger rotation is stopped by an external force. Learning occurs when angle-specific neural paths are stimulated concurrently with the supraliminar stimulus that activates all the neurons that inhibit the SNN output stopping the finger. The results showed that after learning, the finger stopped in the angle interval in which the angle-specific neural path was active, without the activation of the supraliminar stimulus. The proposed concept can be used to implement control units for anthropomorphic robots that are able to learn motions unsupervised, based on principles of high biological plausibility.
C1 [Hulea, Mircea; Uleru, George Iulian; Caruntu, Constantin Florin] Gheorghe Asachi Tech Univ Iasi, Fac Automat Control & Comp Engn, Iasi 700050, Romania.
RP Hulea, M (corresponding author), Gheorghe Asachi Tech Univ Iasi, Fac Automat Control & Comp Engn, Iasi 700050, Romania.
EM mhulea@tuiasi.ro; george-iulian.uleru@academic.tuiasi.ro;
   caruntuc@ac.tuiasi.ro
CR ALFALAHE NA, 1990, BRAIN, V113, P325, DOI 10.1093/brain/113.2.325
   Almusawi ARJ, 2016, COMPUT INTEL NEUROSC, V2016, DOI 10.1155/2016/5720163
   Andrianesis K, 2015, J INTELL ROBOT SYST, V78, P257, DOI 10.1007/s10846-014-0061-6
   Bing ZS, 2018, FRONT NEUROROBOTICS, V12, DOI 10.3389/fnbot.2018.00035
   Blum KP, 2017, PLOS COMPUT BIOL, V13, DOI 10.1371/journal.pcbi.1005767
   Bouganis A., 2010, P 2010 INT JOINT C N, P1, DOI DOI 10.1109/IJCNN.2010.5596525
   Brailovski V, 2010, PHYSCS PROC, V10, P197, DOI 10.1016/j.phpro.2010.11.098
   BULLOCK D, 1993, J COGNITIVE NEUROSCI, V5, P408, DOI 10.1162/jocn.1993.5.4.408
   Camilo V.T.J., 2018, LECT NOTES COMPUTER, V1139
   Chadderdon GL, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0047251
   Chakravarthy VS, 2010, BIOL CYBERN, V103, P237, DOI 10.1007/s00422-010-0401-y
   Chou TS, 2015, FRONT NEUROROBOTICS, V9, DOI 10.3389/fnbot.2015.00006
   Clawson TS, 2016, IEEE DECIS CONTR P, P3381, DOI 10.1109/CDC.2016.7798778
   Coral W, 2012, SMART ACTUATION AND SENSING SYSTEMS - RECENT ADVANCES AND FUTURE CHALLENGES, P53, DOI 10.5772/50209
   Ulloa CC, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10207160
   Demin VA, 2021, NEURAL NETWORKS, V134, P64, DOI 10.1016/j.neunet.2020.11.005
   Dimitriou M, 2016, CURR BIOL, V26, P1062, DOI 10.1016/j.cub.2016.02.030
   Ligutan DD, 2018, I C HUMANOID NANOTEC, DOI 10.1109/HNICEM.2018.8666292
   Dunne L., 2020, P 2020 8 IEEE RAS EM
   Garcia-Cordova F., 1999, IEEE SMC'99 Conference Proceedings. 1999 IEEE International Conference on Systems, Man, and Cybernetics (Cat. No.99CH37028), P794, DOI 10.1109/ICSMC.1999.825363
   Ge SS, 1997, IEEE T IND ELECTRON, V44, P746, DOI 10.1109/41.649934
   Grillner S, 2003, NAT REV NEUROSCI, V4, P573, DOI 10.1038/nrn1137
   Grillner S, 2020, COMPR PHYSIOL, V10, P1241, DOI 10.1002/cphy.c190045
   Hao YZ, 2020, NEURAL NETWORKS, V121, P387, DOI 10.1016/j.neunet.2019.09.007
   Hulea M., 2018, Patent, Patent No. [RO126249(A2), 1262492]
   Hulea M., 2020, 12 INT S COMMUN SYST, P1
   Hulea M, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20216119
   Hulea M, 2019, INT J HUM ROBOT, V16, DOI 10.1142/S0219843619500129
   Hulea M, 2016, STUD SYST DECIS CONT, V40, P13, DOI 10.1007/978-3-319-26230-7_2
   Hulea M, 2014, INT CONF SYST THEO, P163, DOI 10.1109/ICSTCC.2014.6982409
   HULLIGER M, 1984, REV PHYSIOL BIOCH P, V101, P1, DOI 10.1007/BFb0027694
   Jani JM, 2014, MATER DESIGN, V56, P1078, DOI 10.1016/j.matdes.2013.11.084
   Je-sung Koh, 2011, 2011 8th International Conference on Ubiquitous Robots and Ambient Intelligence (URAI 2011), DOI 10.1109/URAI.2011.6145989
   Kim J, 2020200602642 ARXIV
   Kumarasinghe K, 2020, NEURAL NETWORKS, V121, P169, DOI 10.1016/j.neunet.2019.08.029
   Lester BT, 2015, ACTA MECH, V226, P3907, DOI 10.1007/s00707-015-1433-0
   Lobo JL, 2020, NEURAL NETWORKS, V121, P88, DOI 10.1016/j.neunet.2019.09.004
   Lobov SA, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00088
   Maciag PS, 2021, NEURAL NETWORKS, V139, P118, DOI 10.1016/j.neunet.2021.02.017
   Nichols E, 2013, IEEE T CYBERNETICS, V43, P115, DOI 10.1109/TSMCB.2012.2200674
   Payvand M, 2020, IEEE J EM SEL TOP C, V10, P522, DOI 10.1109/JETCAS.2020.3040248
   Peters RM, 2017, NEUROSCIENCE, V349, P98, DOI 10.1016/j.neuroscience.2017.02.034
   Qiu Q., P INT JOINT C ARTIFI
   Qu LH, 2020, NEURAL COMPUT APPL, V32, P13479, DOI 10.1007/s00521-020-04755-4
   Shi MT, 2020, FRONT COMPUT NEUROSC, V14, DOI 10.3389/fncom.2020.00007
   Shim Y, 2016, PLOS COMPUT BIOL, V12, DOI 10.1371/journal.pcbi.1005137
   Skatchkovsky N, 2021, IEEE COMMUN LETT, V25, P1741, DOI 10.1109/LCOMM.2021.3050242
   Sugita K, 2016, IEEE ASME INT C ADV, P431, DOI 10.1109/AIM.2016.7576805
   Taherkhani A, 2020, NEURAL NETWORKS, V122, P253, DOI 10.1016/j.neunet.2019.09.036
   Tang G., 2021, P INT C INT ROB, P6090
   Tieck CV, 2019, PROCEEDINGS OF THE 2019 IEEE 18TH INTERNATIONAL CONFERENCE ON COGNITIVE INFORMATICS & COGNITIVE COMPUTING (ICCI*CC 2019), P54, DOI 10.1109/ICCICC46617.2019.9146079
   Tieck J.C.V., 2018, P 2018 IEEE 17 INT C
   Tieck JCV, 2017, LECT NOTES COMPUT SC, V10613, P43, DOI 10.1007/978-3-319-68600-4_6
   VALLBO AB, 1990, J PHYSIOL-LONDON, V421, P553, DOI 10.1113/jphysiol.1990.sp017961
   VALLBO AB, 1974, ACTA PHYSIOL SCAND, V90, P303, DOI 10.1111/j.1748-1716.1974.tb05593.x
   Ventura C., 2009, P BIOENG BIOINSP SYS
   Voos H., 2016, P 2016 8 INT C EL CO
   Wang XW, 2020, NEURAL NETWORKS, V125, P258, DOI 10.1016/j.neunet.2020.02.011
   Xu B., ARXIV2020201003140
   Yang J, 2018, COMPLEXITY, DOI 10.1155/2018/7131562
   Yellakuor BE, 2020, IEEE ACCESS, V8, P72360, DOI 10.1109/ACCESS.2020.2985257
   Yu Q., ARXIV 2020 200503231
NR 62
TC 4
Z9 4
U1 3
U2 18
PD APR
PY 2021
VL 21
IS 8
AR 2730
DI 10.3390/s21082730
WC Chemistry, Analytical; Engineering, Electrical & Electronic; Instruments
   & Instrumentation
DA 2023-11-11
ER

PT B
AU Gotman, J
AF Gotman, Jean
BE Fisch, BJ
TI COMPUTERIZED SIGNAL ANALYSIS AND EVENT DETECTION
SO EPILEPSY AND INTENSIVE CARE MONITORING: PRINCIPLES AND PRACTICE
DT Article; Book Chapter
ID ARTIFICIAL NEURAL-NETWORK; AUTOMATIC SEIZURE DETECTION; TRAUMATIC
   BRAIN-INJURY; DEPENDENT SPIKE DETECTION; EPILEPTIC SEIZURES; INTERICTAL
   SPIKING; NONCONVULSIVE SEIZURES; WAVELET ANALYSIS; EEG RECORDINGS;
   WARNING SYSTEM
C1 McGill Univ, Montreal Neurol Inst, Montreal, PQ, Canada.
RP Gotman, J (corresponding author), McGill Univ, Montreal Neurol Inst, Montreal, PQ, Canada.
CR Aarabi A, 2006, CLIN NEUROPHYSIOL, V117, P328, DOI 10.1016/j.clinph.2005.10.006
   Aarabi A, 2007, CLIN NEUROPHYSIOL, V118, P2781, DOI 10.1016/j.clinph.2007.08.012
   Acir N, 2004, COMPUT BIOL MED, V34, P561, DOI 10.1016/j.compbiomed.2003.08.003
   Alkan A, 2005, J NEUROSCI METH, V148, P167, DOI 10.1016/j.jneumeth.2005.04.009
   Altenburg J, 2003, CLIN NEUROPHYSIOL, V114, P50, DOI 10.1016/S1388-2457(02)00322-X
   [Anonymous], 1974, Electroencephalogr Clin Neurophysiol, V37, P538
   Celka P, 2002, IEEE T BIO-MED ENG, V49, P455, DOI 10.1109/10.995684
   Claassen J, 2004, NEUROLOGY, V62, P1743, DOI 10.1212/01.WNL.0000125184.88797.62
   DAttellis CE, 1997, ANN BIOMED ENG, V25, P286, DOI 10.1007/BF02648043
   Flanagan D, 2003, CLIN NEUROPHYSIOL, V114, P38, DOI 10.1016/S1388-2457(02)00296-1
   GABOR AJ, 1992, ELECTROEN CLIN NEURO, V83, P271, DOI 10.1016/0013-4694(92)90086-W
   Gabor AJ, 1996, ELECTROEN CLIN NEURO, V99, P257, DOI 10.1016/0013-4694(96)96001-0
   GLOVER JR, 1989, IEEE T BIO-MED ENG, V36, P519, DOI 10.1109/10.24253
   GOTMAN J, 1992, ELECTROEN CLIN NEURO, V83, P12, DOI 10.1016/0013-4694(92)90127-4
   GOTMAN J, 1991, ELECTROEN CLIN NEURO, V79, P11, DOI 10.1016/0013-4694(91)90151-S
   GOTMAN J, 1979, ELECTROEN CLIN NEURO, V46, P510, DOI 10.1016/0013-4694(79)90004-X
   GOTMAN J, 1991, CAN J NEUROL SCI, V18, P573, DOI 10.1017/S031716710003273X
   GOTMAN J, 1985, ANN NEUROL, V17, P597, DOI 10.1002/ana.410170612
   GOTMAN J, 1989, ELECTROEN CLIN NEURO, V72, P7, DOI 10.1016/0013-4694(89)90026-6
   GOTMAN J, 1982, ELECTROEN CLIN NEURO, V54, P530, DOI 10.1016/0013-4694(82)90038-4
   GOTMAN J, 1990, ELECTROEN CLIN NEURO, V76, P317, DOI 10.1016/0013-4694(90)90032-F
   Gotman J, 1997, ELECTROEN CLIN NEURO, V103, P356, DOI 10.1016/S0013-4694(97)00003-9
   Greene BR, 2007, CLIN NEUROPHYSIOL, V118, P1348, DOI 10.1016/j.clinph.2007.02.015
   Grewal S, 2005, CLIN NEUROPHYSIOL, V116, P2460, DOI 10.1016/j.clinph.2005.05.020
   HARDING GW, 1993, ELECTROEN CLIN NEURO, V86, P428, DOI 10.1016/0013-4694(93)90138-L
   Hassanpour H, 2004, PHYSIOL MEAS, V25, P935, DOI 10.1088/0967-3334/25/4/012
   Hellmann G, 1999, CLIN NEUROPHYSIOL, V110, P887, DOI 10.1016/S1388-2457(99)00040-1
   Hopfengärtner R, 2007, CLIN NEUROPHYSIOL, V118, P2332, DOI 10.1016/j.clinph.2007.07.017
   JANDO G, 1993, ELECTROEN CLIN NEURO, V86, P100, DOI 10.1016/0013-4694(93)90082-7
   Jirsch J, 2007, CLIN NEUROPHYSIOL, V118, P1660, DOI 10.1016/j.clinph.2006.11.312
   Khan YU, 2003, CLIN NEUROPHYSIOL, V114, P898, DOI 10.1016/S1388-2457(03)00035-X
   Klatchko A, 1998, ELECTROEN CLIN NEURO, V106, P52, DOI 10.1016/S0013-4694(97)00092-8
   Ko CW, 2000, CLIN NEUROPHYSIOL, V111, P477, DOI 10.1016/S1388-2457(99)00284-9
   Kurth C, 2000, ANN BIOMED ENG, V28, P1362, DOI 10.1114/1.1331312
   Latka M, 2003, PHYS REV E, V67, DOI 10.1103/PhysRevE.67.052902
   LIU A, 1992, ELECTROEN CLIN NEURO, V82, P30, DOI 10.1016/0013-4694(92)90179-L
   Mormann F, 2007, BRAIN, V130, P314, DOI 10.1093/brain/awl241
   Nasraway SA, 2002, CRIT CARE MED, V30, P117, DOI 10.1097/00003246-200201000-00019
   Osorio I, 1998, EPILEPSIA, V39, P615, DOI 10.1111/j.1528-1157.1998.tb01430.x
   Özdamar Ö, 1998, COMPUT BIOMED RES, V31, P122, DOI 10.1006/cbmr.1998.1475
   PIETILA T, 1994, ELECTROEN CLIN NEURO, V90, P438, DOI 10.1016/0013-4694(94)90134-1
   Qu H, 1997, IEEE T BIO-MED ENG, V44, P115, DOI 10.1109/10.552241
   QU H, 1993, ELECTROEN CLIN NEURO, V86, P79, DOI 10.1016/0013-4694(93)90079-B
   Qu H, 1995, NEUROLOGY, V45, P2250, DOI 10.1212/WNL.45.12.2250
   Roessgen M, 1998, IEEE T BIO-MED ENG, V45, P673, DOI 10.1109/10.678601
   Ronne-Engstrom E, 2006, ACTA NEUROL SCAND, V114, P47, DOI 10.1111/j.1600-0404.2006.00652.x
   Saab ME, 2005, CLIN NEUROPHYSIOL, V116, P427, DOI 10.1016/j.clinph.2004.08.004
   SAMMARITANO M, 1991, NEUROLOGY, V41, P290, DOI 10.1212/WNL.41.2_Part_1.290
   Sartoretto F, 1999, CLIN NEUROPHYSIOL, V110, P239, DOI 10.1016/S0013-4694(98)00116-3
   Scheuer ML, 2004, J CLIN NEUROPHYSIOL, V21, P353
   Schindler K, 2002, CLIN NEUROPHYSIOL, V113, P604, DOI 10.1016/S1388-2457(02)00032-9
   Senhadji L, 2002, NEUROPHYSIOL CLIN, V32, P175, DOI 10.1016/S0987-7053(02)00304-0
   Shoeb A, 2004, EPILEPSY BEHAV, V5, P483, DOI 10.1016/j.yebeh.2004.05.005
   SPENCER SS, 2008, EPILEPSIA
   Srinivasan V, 2005, J Med Syst, V29, P647, DOI 10.1007/s10916-005-6133-1
   Valenti P, 2006, J NEUROSCI METH, V150, P105, DOI 10.1016/j.jneumeth.2005.06.005
   Vespa PM, 1999, J NEUROSURG, V91, P750, DOI 10.3171/jns.1999.91.5.0750
   Vespa PM, 1997, ELECTROEN CLIN NEURO, V103, P607, DOI 10.1016/S0013-4694(97)00071-0
   Vespa PM, 2002, J NEUROSURG, V97, P84, DOI 10.3171/jns.2002.97.1.0084
   Webber WRS, 1996, ELECTROEN CLIN NEURO, V98, P250, DOI 10.1016/0013-4694(95)00277-4
   WEBBER WRS, 1994, ELECTROEN CLIN NEURO, V91, P194, DOI 10.1016/0013-4694(94)90069-8
   Wilson SB, 2005, CLIN NEUROPHYSIOL, V116, P1785, DOI 10.1016/j.clinph.2005.04.025
   Wilson SB, 2004, CLIN NEUROPHYSIOL, V115, P2280, DOI 10.1016/j.clinph.2004.05.018
   Wilson SB, 1999, CLIN NEUROPHYSIOL, V110, P404, DOI 10.1016/S1388-2457(98)00023-6
   Wright WL, 2007, J NEUROL SCI, V261, P10, DOI 10.1016/j.jns.2007.04.027
   Young GB, 1996, NEUROLOGY, V47, P83
NR 66
TC 1
Z9 1
U1 0
U2 0
PY 2010
BP 77
EP 88
WC Clinical Neurology; Neurosciences
DA 2023-11-11
ER

PT C
AU Wang, JL
   Belatreche, A
   Maguire, LP
   McGinnity, TM
AF Wang, Jinling
   Belatreche, Ammar
   Maguire, Liam P.
   McGinnity, T. Martin
BE Arik, S
   Huang, T
   Lai, WK
   Liu, Q
TI SpikeComp: An Evolving Spiking Neural Network with Adaptive Compact
   Structure for Pattern Classification
SO NEURAL INFORMATION PROCESSING, PT II
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 22nd International Conference on Neural Information Processing (ICONIP)
CY NOV 09-12, 2015
CL Istanbul, TURKEY
DE Spiking neurons; Supervised learning; Adaptive structure; Classification
AB This paper presents a new supervised learning algorithm (SpikeComp) with an adaptive compact structure for Spiking Neural Networks (SNNs). SpikeComp consists of two layers of spiking neurons: an encoding layer which temporally encodes real valued features into spatio-temporal spike patterns, and an output layer of dynamically grown neurons which perform spatio-temporal pattern classification. The weights between the neurons in the encoding layer and the new added neuron in the output layer are initialised based on the precise spiking times in the encoding layer. New strategies are proposed to either add a new neuron, or update the network parameters when a new sample is presented to the network. The proposed learning algorithm was demonstrated on several benchmark classification datasets and the obtained results show that SpikeComp can perform pattern classification with a comparable performance and a much compact network structure compared with other existing SNN training algorithm.
C1 [Wang, Jinling; Belatreche, Ammar; Maguire, Liam P.; McGinnity, T. Martin] Univ Ulster, Fac Comp & Engn, ISRC, Magee Campus,Northland Rd, Derry BT48 7JL, North Ireland.
   [McGinnity, T. Martin] Nottingham Trent Univ, Sch Sci & Technol, Nottingham, England.
RP Wang, JL (corresponding author), Univ Ulster, Fac Comp & Engn, ISRC, Magee Campus,Northland Rd, Derry BT48 7JL, North Ireland.
EM Wang-J1@email.ulster.ac.uk; A.Belatreche@ulster.ac.uk;
   LP.Maguire@ulster.ac.uk; TM.McGinnity@ulster.ac.uk
CR Belatreche A, 2006, NEW MATH NAT COMPUT, V2, P237, DOI 10.1142/S179300570600049X
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Dora S., 2015, NEUROCOMPUT IN PRESS
   Kasabov N, 2013, NEURAL NETWORKS, V41, P188, DOI 10.1016/j.neunet.2012.11.014
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   VanRullen R, 2005, TRENDS NEUROSCI, V28, P1, DOI 10.1016/j.tins.2004.10.010
   Wang JL, 2014, NEUROCOMPUTING, V144, P526, DOI 10.1016/j.neucom.2014.04.017
NR 7
TC 3
Z9 3
U1 0
U2 0
PY 2015
VL 9490
BP 259
EP 267
DI 10.1007/978-3-319-26535-3_30
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods
DA 2023-11-11
ER

PT J
AU Bianchi, S
   Pedretti, G
   Muñoz-Martin, I
   Calderoni, A
   Ramaswamy, N
   Ambrogio, S
   Ielmini, D
AF Bianchi, Stefano
   Pedretti, Giacomo
   Munoz-Martin, Irene
   Calderoni, Alessandro
   Ramaswamy, Nirmal
   Ambrogio, Stefano
   Ielmini, Daniele
TI A Compact Model for Stochastic Spike-Timing-Dependent Plasticity (STDP)
   Based on Resistive Switching Memory (RRAM) Synapses
SO IEEE TRANSACTIONS ON ELECTRON DEVICES
DT Article
DE Neuromorphic engineering; resistive switching memory (RRAM);
   spike-timing-dependent plasticity (STDP); stochastic learning;
   unsupervised learning
ID NETWORKS; NEURONS; DEVICE
AB Resistive switching memory (RRAM) devices have been proposed to boost the density and the biorealistic plasticity in neural networks. One of the main limitations to the development of neuromorphic systems with RRAM devices is the lack of compact models for the simulation of spiking neural networks, including neuron spike processing, synaptic plasticity, and stochastic learning. Here, we present a predictive model for neuromorphic networks with unsupervised spike timing-dependent plasticity (STDP) in HfO2 RRAM devices. Our compact model can predict the learning behavior of experimental networks and can speed up the simulation of unsupervised learning compared to Monte Carlo (MC) approaches. The model can be used to optimize the classification accuracy of data sets, such as MNIST, and to estimate the time of learning and the energy consumption.
C1 [Bianchi, Stefano; Pedretti, Giacomo; Munoz-Martin, Irene; Ambrogio, Stefano; Ielmini, Daniele] Politecn Milan, Dipartimento Elettron Informaz & Bioingn, I-20133 Milan, Italy.
   [Bianchi, Stefano; Pedretti, Giacomo; Munoz-Martin, Irene; Ielmini, Daniele] Politecn Milan, Italian Univ Nanoelect Team, I-20133 Milan, Italy.
   [Calderoni, Alessandro; Ramaswamy, Nirmal] Micron Technol Inc, Boise, ID 83707 USA.
RP Ielmini, D (corresponding author), Politecn Milan, Dipartimento Elettron Informaz & Bioingn, I-20133 Milan, Italy.; Ielmini, D (corresponding author), Politecn Milan, Italian Univ Nanoelect Team, I-20133 Milan, Italy.
EM stefano1.bianchi@polimi.it; giacomo.pedretti@polimi.it;
   irene.munoz@polimi.it; acaldero@micron.com; dramaswamy@micron.com;
   stefano.ambrogio@ibm.com; daniele.ielmini@polimi.it
CR Abbott LF, 2000, NAT NEUROSCI, V3, P1178, DOI 10.1038/81453
   Ambrogio S, 2016, IEEE T ELECTRON DEV, V63, P1508, DOI 10.1109/TED.2016.2526647
   [Anonymous], 2015, IEDM, DOI DOI 10.1109/IEDM.2015.7409716
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Bianchi S, 2019, SYMP VLSI CIRCUITS, pT172, DOI 10.23919/VLSIC.2019.8778001
   Chicca E, 2014, P IEEE, V102, P1367, DOI 10.1109/JPROC.2014.2313954
   Clopath Claudia, 2010, Front Synaptic Neurosci, V2, P25, DOI 10.3389/fnsyn.2010.00025
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Indiveri G, 2015, P IEEE, V103, P1379, DOI 10.1109/JPROC.2015.2444094
   Jo SH, 2010, NANO LETT, V10, P1297, DOI 10.1021/nl904092h
   Kuzum D, 2012, NANO LETT, V12, P2179, DOI 10.1021/nl201040y
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Maass W, 2014, P IEEE, V102, P860, DOI 10.1109/JPROC.2014.2310593
   Masquelier T, 2007, PLOS COMPUT BIOL, V3, P247, DOI 10.1371/journal.pcbi.0030031
   Muñoz-Martín I, 2019, IEEE J EXPLOR SOLID-, V5, P58, DOI 10.1109/JXCDC.2019.2911135
   Pedretti G, 2017, INT EL DEVICES MEET
   Pedretti G, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-05480-0
   Prezioso M, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-07757-y
   Raoux S, 2010, CHEM REV, V110, P240, DOI 10.1021/cr900040x
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Wong HSP, 2012, P IEEE, V100, P1951, DOI 10.1109/JPROC.2012.2190369
   Yu SM, 2018, P IEEE, V106, P260, DOI 10.1109/JPROC.2018.2790840
   Yu SM, 2011, IEEE T ELECTRON DEV, V58, P2729, DOI 10.1109/TED.2011.2147791
NR 23
TC 9
Z9 9
U1 2
U2 28
PD JUL
PY 2020
VL 67
IS 7
BP 2800
EP 2806
DI 10.1109/TED.2020.2992386
WC Engineering, Electrical & Electronic; Physics, Applied
DA 2023-11-11
ER

PT C
AU Bang, S
   Oh, MH
   Kim, MH
   Kim, TH
   Lee, DK
   Choi, YJ
   Kim, CS
   Hong, K
   Cho, S
   Kim, S
   Park, BG
AF Bang, Suhyun
   Oh, Min-Hye
   Kim, Min-Hwi
   Kim, Tae-Hyeon
   Lee, Dong Keun
   Choi, Yeon-Joon
   Kim, Chae Soo
   Hong, Kyungho
   Cho, Seongjae
   Kim, Sungjun
   Park, Byung-Gook
GP IEEE
TI Validation of Spiking Neural Networks Using Resistive-Switching Synaptic
   Device with SpikeRate-Dependent Plasticity
SO 2020 INTERNATIONAL CONFERENCE ON ELECTRONICS, INFORMATION, AND
   COMMUNICATION (ICEIC)
DT Proceedings Paper
CT 19th International Conference on Electronics, Information, and
   Communication (ICEIC)
CY JAN 19-22, 2020
CL Barcelona, SPAIN
DE resistive-switching random-access memory; synaptic device; spiking
   neural network; spike-rate-dependent plasticity
ID BEHAVIOR; RRAM
AB In this work, we have developed a spiking neural network (SNN) using gradual resistive- switching random-access memory (RRAM) synaptic device. The fabricated RRAM devices demonstrated the characteristics of gradually changing conductance with voltage pulses under both positive and negative polarities, which is suitable for imitating the potentiation and depression functions of a biological synapse by an electron device. Featuring the gradual switching characteristics, spikerate-dependent plasticity (SRDP) inspired by Bienenstock, Cooper, and Munro (BCM) learning rule was confirmed and modeled for synaptic modification in the SNN. Then, the supervised learning of MNIST patterns was performed on the simulated SNNs, by which it has been validated that the proposed resistive-switching synaptic device and SRDP synaptic modification rule can adjust weights accurately in cooperation without necessitating the conventional calculation-based learning scheme in the artificial neural networks (ANNs), such as error backpropagation.
C1 [Bang, Suhyun; Oh, Min-Hye; Kim, Min-Hwi; Kim, Tae-Hyeon; Lee, Dong Keun; Choi, Yeon-Joon; Kim, Chae Soo; Hong, Kyungho; Park, Byung-Gook] Seoul Natl Univ, Interuniv Semicond Res Ctr ISRC, Seoul 08826, South Korea.
   [Bang, Suhyun; Oh, Min-Hye; Kim, Min-Hwi; Kim, Tae-Hyeon; Lee, Dong Keun; Choi, Yeon-Joon; Kim, Chae Soo; Hong, Kyungho; Park, Byung-Gook] Seoul Natl Univ, Dept Elect & Comp Engn, Seoul 08826, South Korea.
   [Cho, Seongjae] Gachon Univ, Dept Elect Engn, 1342 Seongnamdaero, Seongnam Si 13120, Gyeonggi Do, South Korea.
   [Kim, Sungjun] Chungbuk Natl Univ, Sch Elect Engn, Cheongju 28644, South Korea.
RP Park, BG (corresponding author), Seoul Natl Univ, Interuniv Semicond Res Ctr ISRC, Seoul 08826, South Korea.; Park, BG (corresponding author), Seoul Natl Univ, Dept Elect & Comp Engn, Seoul 08826, South Korea.
EM bgpark@snu.ac.kr
CR Kim S, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-21057-x
   Kim S, 2017, ACS APPL MATER INTER, V9, P40420, DOI 10.1021/acsami.7b11191
   Kirkwood A, 1996, NATURE, V381, P526, DOI 10.1038/381526a0
   Lashkare S, 2017, IEEE ELECTR DEVICE L, V38, P1212, DOI 10.1109/LED.2017.2723503
   Lim S, 2018, IEEE ELECTR DEVICE L, V39, P312, DOI 10.1109/LED.2018.2789425
   Panwar N, 2017, IEEE ELECTR DEVICE L, V38, P740, DOI 10.1109/LED.2017.2696023
   Wang YF, 2015, SCI REP-UK, V5, DOI 10.1038/srep10150
   Wang ZQ, 2012, ADV FUNCT MATER, V22, P2759, DOI 10.1002/adfm.201103148
   Watt Alanna J, 2010, Front Synaptic Neurosci, V2, P5, DOI 10.3389/fnsyn.2010.00005
   Woo J, 2017, IEEE ELECTR DEVICE L, V38, P1220, DOI 10.1109/LED.2017.2731859
   Yu SM, 2013, ADV MATER, V25, P1774, DOI 10.1002/adma.201203680
   Zarudnyi K, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00057
NR 12
TC 0
Z9 0
U1 0
U2 0
PY 2020
WC Engineering, Electrical & Electronic; Telecommunications
DA 2023-11-11
ER

PT J
AU Nourse, WRP
   Jackson, C
   Szczecinski, NS
   Quinn, RD
AF Nourse, William R. P.
   Jackson, Clayton
   Szczecinski, Nicholas S.
   Quinn, Roger D.
TI SNS-Toolbox: An Open Source Tool for Designing Synthetic Nervous Systems
   and Interfacing Them with Cyber-Physical Systems
SO BIOMIMETICS
DT Article
DE synthetic nervous system; conductance-based modeling; neural simulator;
   GPU; CUDA; neurorobotics; Python; software; open-source
ID SPIKING NEURONS; INSIGHTS; MODEL
AB One developing approach for robotic control is the use of networks of dynamic neurons connected with conductance-based synapses, also known as Synthetic Nervous Systems (SNS). These networks are often developed using cyclic topologies and heterogeneous mixtures of spiking and non-spiking neurons, which is a difficult proposition for existing neural simulation software. Most solutions apply to either one of two extremes, the detailed multi-compartment neural models in small networks, and the large-scale networks of greatly simplified neural models. In this work, we present our open-source Python package SNS-Toolbox, which is capable of simulating hundreds to thousands of spiking and non-spiking neurons in real-time or faster on consumer-grade computer hardware. We describe the neural and synaptic models supported by SNS-Toolbox, and provide performance on multiple software and hardware backends, including GPUs and embedded computing platforms. We also showcase two examples using the software, one for controlling a simulated limb with muscles in the physics simulator Mujoco, and another for a mobile robot using ROS. We hope that the availability of this software will reduce the barrier to entry when designing SNS networks, and will increase the prevalence of SNS networks in the field of robotic control.
C1 [Nourse, William R. P.] Case Western Reserve Univ, Dept Elect Comp & Syst Engn, Cleveland, OH 44106 USA.
   [Jackson, Clayton; Quinn, Roger D.] Case Western Reserve Univ, Dept Mech & Aerosp Engn, Cleveland, OH 44106 USA.
   [Szczecinski, Nicholas S.] West Virginia Univ, Dept Mech & Aerosp Engn, Morgantown, WV 26506 USA.
RP Nourse, WRP (corresponding author), Case Western Reserve Univ, Dept Elect Comp & Syst Engn, Cleveland, OH 44106 USA.
EM nourse@case.edu
CR Alevi D, 2022, FRONT NEUROINFORM, V16, DOI 10.3389/fninf.2022.883700
   Allard J, 2007, STUD HEALTH TECHNOL, V125, P13
   [Anonymous], 2011, BMC NEUROSCI, DOI [DOI 10.1186/1471-2202-12-S1-P363, 10.1186/1471-2202-12-S1-P363]
   [Anonymous], 2007, PYTHON PROGRAMMING L
   [Anonymous], 2012, BOOK GENESIS EXPLORI, DOI DOI 10.1007/s10827-007-0038-6
   Ayers J, 2007, PHILOS T R SOC A, V365, P273, DOI 10.1098/rsta.2006.1910
   Bartolozzi C, 2022, NAT COMMUN, V13, DOI 10.1038/s41467-022-28487-2
   Beer Randall D., 1992, Adaptive Behavior, V1, P91, DOI 10.1177/105971239200100105
   Bekolay T, 2014, FRONT NEUROINFORM, V7, DOI 10.3389/fninf.2013.00048
   Braitenberg V., 1986, VEHICLES EXPT SYNTHE
   Brown TG, 1911, P R SOC LOND B-CONTA, V84, P308, DOI 10.1098/rspb.1911.0077
   Capolei MC, 2019, FRONT NEUROROBOTICS, V13, DOI 10.3389/fnbot.2019.00070
   Chiel HJ, 1997, TRENDS NEUROSCI, V20, P553, DOI 10.1016/S0166-2236(97)01149-1
   Cofer D, 2010, J EXP BIOL, V213, P3378, DOI 10.1242/jeb.046367
   Cofer D, 2010, J NEUROSCI METH, V187, P280, DOI 10.1016/j.jneumeth.2010.01.005
   Cohen G, 2022, IEEE SPECTRUM, V59, P44, DOI 10.1109/MSPEC.2022.9729948
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Delp SL, 2007, IEEE T BIO-MED ENG, V54, P1940, DOI 10.1109/TBME.2007.901024
   Deng K., 2018, NEUROMECHANICAL MODE, VVolume 10928 LNAI, P134
   Dewolf T, 2023, NEUROMORPH COMPUT EN, V3, DOI 10.1088/2634-4386/acb286
   Djurfeldt M, 2010, NEUROINFORMATICS, V8, P43, DOI 10.1007/s12021-010-9064-z
   Eliasmith C., 2003, NEURAL ENG COMPUTATI
   Eshraghian JK, 2023, Arxiv, DOI [arXiv:2109.12894, DOI 10.48550/ARXIV.2109.12894]
   Fidjeland AK, 2009, IEEE INT CONF ASAP, P137, DOI 10.1109/ASAP.2009.24
   Fitzpatrick D, 1996, CEREB CORTEX, V6, P329, DOI 10.1093/cercor/6.3.329
   Gewaltig M-O., 2007, SCHOLARPEDIA, V2, DOI [10.4249/scholarpedia.1430, DOI 10.4249/SCHOLARPEDIA.1430]
   Goldsmith CA, 2020, BIOINSPIR BIOMIM, V15, DOI 10.1088/1748-3190/ab9e52
   Goodman Dan, 2008, Front Neuroinform, V2, P5, DOI 10.3389/neuro.11.005.2008
   Guie C.K., 2022, DIRECT ASSEMBLY TUNI, VVolume 13548 LNAI, P321, DOI [10.1007/978-3-031-20470-8_32, DOI 10.1007/978-3-031-20470-8_32]
   Harris CR, 2020, NATURE, V585, P357, DOI 10.1038/s41586-020-2649-2
   Hazan H, 2018, FRONT NEUROINFORM, V12, DOI 10.3389/fninf.2018.00089
   Hines ML, 2001, NEUROSCIENTIST, V7, P123, DOI 10.1177/107385840100700207
   Hoang RV, 2013, FRONT NEUROINFORM, V7, DOI 10.3389/fninf.2013.00019
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   HULTBORN H, 1979, EXP BRAIN RES, V37, P399
   Hunt A, 2017, FRONT NEUROROBOTICS, V11, DOI 10.3389/fnbot.2017.00018
   Issa FA, 2012, J NEUROSCI, V32, P5638, DOI 10.1523/JNEUROSCI.5668-11.2012
   Izhikevich E M, 2007, DYNAM SYST, DOI [DOI 10.1017/S0143385704000173, DOI 10.7551/MITPRESS/2526.001.0001]
   Jackson C., 2022, CANONICAL MOTOR MICR, VVolume 13548 LNAI, P309, DOI [10.1007/978-3-031-20470-8_31, DOI 10.1007/978-3-031-20470-8_31]
   Johnson WL, 2008, J BIOMECH, V41, P610, DOI 10.1016/j.jbiomech.2007.10.004
   Kaiser J, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON SIMULATION, MODELING, AND PROGRAMMING FOR AUTONOMOUS ROBOTS (SIMPAR), P127, DOI 10.1109/SIMPAR.2016.7862386
   Koenig N., 2004, 2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566), P2149
   Lava, 2021, LAVA SOFTWARE FRAMEW
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Mangan M, 2023, BIOINSPIR BIOMIM, V18, DOI 10.1088/1748-3190/acc223
   Massi E, 2019, FRONT NEUROROBOTICS, V13, DOI 10.3389/fnbot.2019.00071
   Mihalas S, 2009, NEURAL COMPUT, V21, P704, DOI 10.1162/neco.2008.12-07-680
   Mozafari M, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00625
   Mutch J., 2010, CNS GPU BASED FRAMEW
   Niedermeier L., 2022, CARLSIM 6 OPEN SOURC, VVolume 2022, DOI [10.1109/IJCNN55064.2022.9892644, DOI 10.1109/IJCNN55064.2022.9892644]
   Nourse WRP, 2022, LECT NOTES ARTIF INT, V13548, P32, DOI 10.1007/978-3-031-20470-8_4
   Paszke A, 2019, ADV NEUR IN, V32
   PERKEL DH, 1974, SCIENCE, V185, P181, DOI 10.1126/science.185.4146.181
   Quigley M., 2009, P IEEE INT C ROB AUT, V3, P5
   Richardson MJE, 2004, PHYS REV E, V69, DOI 10.1103/PhysRevE.69.051918
   Rybak IA, 2006, J PHYSIOL-LONDON, V577, P617, DOI 10.1113/jphysiol.2006.118703
   Schilling M, 2023, PLOS COMPUT BIOL, V19, DOI 10.1371/journal.pcbi.1010136
   Sedlackova A., 2020, SYNTHETIC NERVOUS SY, P312
   Seung HS, 2000, J COMPUT NEUROSCI, V9, P171, DOI 10.1023/A:1008971908649
   Strohmer B, 2020, FRONT NEUROROBOTICS, V14, DOI 10.3389/fnbot.2020.00041
   Szczecinski NS, 2023, NEUROMORPH COMPUT EN, V3, DOI 10.1088/2634-4386/acc04f
   Szczecinski NS, 2020, FRONT NEUROROBOTICS, V14, DOI 10.3389/fnbot.2020.577804
   Szczecinski NS, 2017, FRONT NEUROROBOTICS, V11, DOI 10.3389/fnbot.2017.00037
   Szczecinski NS, 2017, BIOL CYBERN, V111, P105, DOI 10.1007/s00422-017-0711-4
   Szczecinski NS, 2014, BIOL CYBERN, V108, P1, DOI 10.1007/s00422-013-0573-3
   Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109
   Vitay J, 2015, FRONT NEUROINFORM, V9, DOI 10.3389/fninf.2015.00019
   Wang C, 2020, ADV INTELL SYST-GER, V2, DOI 10.1002/aisy.201900103
   Webb B, 2002, NATURE, V417, P359, DOI 10.1038/417359a
   Weidel P, 2016, FRONT NEUROINFORM, V10, DOI 10.3389/fninf.2016.00031
   WERBOS PJ, 1990, P IEEE, V78, P1550, DOI 10.1109/5.58337
   Yavuz E, 2016, SCI REP-UK, V6, DOI 10.1038/srep18854
   Young F., 2022, THESIS CASE W RESERV
NR 74
TC 0
Z9 0
U1 0
U2 0
PD JUN
PY 2023
VL 8
IS 2
AR 247
DI 10.3390/biomimetics8020247
WC Engineering, Multidisciplinary; Materials Science, Biomaterials
DA 2023-11-11
ER

PT S
AU Artameeyanant, P
   Chiracharit, W
   Chamnongthai, K
AF Artameeyanant, Patcharin
   Chiracharit, Werapon
   Chamnongthai, Kosin
GP IEEE
TI Spike and Epileptic Seizure Detection Using Wavelet Packet Transform
   Based on Approximate Entropy and Energy with Artificial Neural Network
SO 5TH BIOMEDICAL ENGINEERING INTERNATIONAL CONFERENCE (BMEICON 2012)
SE Biomedical Engineering International Conference
DT Meeting Abstract
CT 5th Biomedical Engineering International Conference (BMEiCON)
CY DEC 05-07, 2012
CL Ubon Ratchathani, THAILAND
DE EEG Signal; Spike; Epileptic Seizure; Wavelet Packet Transform;
   Approximate Entropy; Energy
C1 [Artameeyanant, Patcharin; Chiracharit, Werapon; Chamnongthai, Kosin] King Mongkuts Univ Technol Thonburi, Fac Engn, Dept Elect & Telecommun Engn, Bangkok 10140, Thailand.
EM koyka_patcharin@hotmail.com; werapon.chi@kmutt.ac.th;
   kosin.cha@kmutt.ac.th
NR 0
TC 2
Z9 2
U1 0
U2 1
PY 2012
BP 97
EP 97
WC Engineering, Biomedical
DA 2023-11-11
ER

PT J
AU Tateno, T
   Jimbo, Y
AF Tateno, T
   Jimbo, Y
TI Activity-dependent enhancement in the reliability of correlated spike
   timings in cultured cortical neurons
SO BIOLOGICAL CYBERNETICS
DT Article
ID LONG-TERM POTENTIATION; CROSS-CORRELATION ANALYSIS; INTRACELLULAR
   CALCIUM; NEOCORTICAL NEURONS; NEURAL NETWORKS; RAT NEOCORTEX;
   VISUAL-CORTEX; LOW MAGNESIUM; CONNECTIVITY; IDENTIFICATION
AB To study the use-dependent modification of activity in neural networks, we investigated the spike timing by simultaneously recording activity at multiple sites in a network of cultured cortical neurons. We used dynamical analysis to study the temporal structure of spike trains and the activity-dependent changes in the reliability and reproducibility of spike patterns evoked by a stimulus. We also used cross-correlation analysis to evaluate the interactions of neuron pairs. Our main conclusions are that even when no obvious change in spike numbers can be seen, use-dependent modification occurs, either enhancing or reducing in the reliability and reproducibility of spike trains evoked by a stimulus, and the fine temporal structure of stimulus-evoked spike trains and interactions between neurons are also modified by tetanic stimulation.
C1 NTT Corp, Basic Res Labs, Atsugi, Kanagawa 2430198, Japan.
RP Tateno, T (corresponding author), NTT Corp, Basic Res Labs, 3-1 Morinosato Wakamiya, Atsugi, Kanagawa 2430198, Japan.
EM tateno@will.brl.ntt.co.jp
CR AERTSEN AMHJ, 1985, BRAIN RES, V340, P341, DOI 10.1016/0006-8993(85)90931-X
   AERTSEN AMHJ, 1989, J NEUROPHYSIOL, V61, P900, DOI 10.1152/jn.1989.61.5.900
   ARTOLA A, 1994, NMDA RECEPTOR, P313
   BAUGHMAN RW, 1991, CULTURING NERVE CELL, P227
   BLISS TVP, 1973, J PHYSIOL-LONDON, V232, P331, DOI 10.1113/jphysiol.1973.sp010273
   BLISS TVP, 1993, NATURE, V361, P31, DOI 10.1038/361031a0
   BRILLINGER DR, 1976, BIOL CYBERN, V22, P213, DOI 10.1007/BF00365087
   BURGARD EC, 1993, J NEUROPHYSIOL, V69, P230, DOI 10.1152/jn.1993.69.1.230
   CARR CE, 1990, J NEUROSCI, V10, P3227
   Dahlhaus R, 1997, J NEUROSCI METH, V77, P93, DOI 10.1016/S0165-0270(97)00100-3
   DUDA RO, 1973, PATTERN CLASSIFICATI, P228
   ENGEL AK, 1990, EUR J NEUROSCI, V2, P588, DOI 10.1111/j.1460-9568.1990.tb00449.x
   Engert F, 1997, NATURE, V388, P279, DOI 10.1038/40870
   EPPING WJM, 1987, J NEUROPHYSIOL, V57, P1464, DOI 10.1152/jn.1987.57.5.1464
   GERSTEIN GL, 1985, J NEUROPHYSIOL, V54, P1513, DOI 10.1152/jn.1985.54.6.1513
   GROSS GW, 1979, IEEE T BIO-MED ENG, V26, P273, DOI 10.1109/TBME.1979.326402
   ITO M, 1982, J PHYSIOL-LONDON, V324, P113, DOI 10.1113/jphysiol.1982.sp014103
   JIMBO Y, 1993, IEEE T BIO-MED ENG, V40, P804, DOI 10.1109/10.238465
   JIMBO Y, 1999, IN PRESS SIMULTANEOU
   Juergens E, 1997, BIOL CYBERN, V76, P217, DOI 10.1007/s004220050334
   Kamioka H, 1996, NEUROSCI LETT, V206, P109, DOI 10.1016/S0304-3940(96)12448-4
   Katz LC, 1996, SCIENCE, V274, P1133, DOI 10.1126/science.274.5290.1133
   LUHMANN HJ, 1991, J NEUROPHYSIOL, V65, P247, DOI 10.1152/jn.1991.65.2.247
   Maeda E, 1998, EUR J NEUROSCI, V10, P488, DOI 10.1046/j.1460-9568.1998.00062.x
   MAINEN ZF, 1995, SCIENCE, V268, P1503, DOI 10.1126/science.7770778
   MEISTER M, 1994, J NEUROSCI METH, V51, P95, DOI 10.1016/0165-0270(94)90030-2
   MELSSEN WJ, 1987, BIOL CYBERN, V57, P403, DOI 10.1007/BF00354985
   MURAMOTO K, 1988, P JPN ACAD B-PHYS, V64, P319, DOI 10.2183/pjab.64.319
   NAJAFI K, 1994, IEEE ENG MED BIOL, V13, P375, DOI 10.1109/51.294009
   Otsu Y, 1995, J NEUROPHYSIOL, V74, P2437, DOI 10.1152/jn.1995.74.6.2437
   PERKEL DH, 1967, BIOPHYS J, V7, P419, DOI 10.1016/S0006-3495(67)86597-4
   PINE J, 1980, J NEUROSCI METH, V2, P19, DOI 10.1016/0165-0270(80)90042-4
   Riehle A, 1997, SCIENCE, V278, P1950, DOI 10.1126/science.278.5345.1950
   ROBINSON HPC, 1993, J NEUROPHYSIOL, V70, P1606, DOI 10.1152/jn.1993.70.4.1606
   SCHUMAN EM, 1994, SCIENCE, V263, P532, DOI 10.1126/science.8290963
   SINGER W, 1995, ANNU REV NEUROSCI, V18, P555, DOI 10.1146/annurev.neuro.18.1.555
   SUGA N, 1995, ACTIVE HEARING, P13
   TSUMOTO T, 1992, PROG NEUROBIOL, V39, P209, DOI 10.1016/0301-0082(92)90011-3
   VAADIA E, 1995, NATURE, V373, P515, DOI 10.1038/373515a0
   Watanabe S, 1996, NEUROSCI LETT, V210, P41, DOI 10.1016/0304-3940(96)12653-7
   Yamada S, 1996, J NEUROSCI METH, V66, P35, DOI 10.1016/0165-0270(95)00152-2
   Yan J, 1996, HEARING RES, V93, P102, DOI 10.1016/0378-5955(95)00209-X
   YANG XW, 1990, BIOPHYS J, V57, P987, DOI 10.1016/S0006-3495(90)82618-7
NR 43
TC 58
Z9 59
U1 2
U2 5
PD JAN
PY 1999
VL 80
IS 1
BP 45
EP 55
DI 10.1007/s004220050503
WC Computer Science, Cybernetics; Neurosciences
DA 2023-11-11
ER

PT J
AU Barta, T
   Kostal, L
AF Barta, Tomas
   Kostal, Lubomir
TI Spike frequency adaptation mechanism leading to variability quenching in
   recurrent neural networks
SO JOURNAL OF COMPUTATIONAL NEUROSCIENCE
DT Meeting Abstract
C1 [Barta, Tomas; Kostal, Lubomir] Czech Acad Sci, Inst Physiol, Lab Computat Neurosci, Prague, Czech Republic.
CR Barta T, 2021, PHYS REV E, V103, DOI 10.1103/PhysRevE.103.022408
   Churchland MM, 2010, NAT NEUROSCI, V13, P369, DOI 10.1038/nn.2501
   Monier C, 2003, NEURON, V37, P663, DOI 10.1016/S0896-6273(03)00064-3
   Zerlaut Y, 2018, J COMPUT NEUROSCI, V44, P45, DOI 10.1007/s10827-017-0668-2
NR 4
TC 0
Z9 0
U1 0
U2 0
PD JAN
PY 2023
VL 51
SU 1
MA P59
BP S60
EP S60
WC Mathematical & Computational Biology; Neurosciences
DA 2023-11-11
ER

PT J
AU Naito, S
   Yukinawa, N
   Ishii, S
AF Naito, Satoshi
   Yukinawa, Naoto
   Ishii, Shin
TI A spiking neural network model of primary visual cortex for perceptual
   learning
SO NEUROSCIENCE RESEARCH
DT Meeting Abstract
C1 [Naito, Satoshi; Yukinawa, Naoto; Ishii, Shin] Kyoto Univ, Grad Sch Informat, Kyoto 6068501, Japan.
NR 0
TC 0
Z9 0
U1 0
U2 2
PY 2010
VL 68
SU 1
BP E210
EP E211
DI 10.1016/j.neures.2010.07.2503
WC Neurosciences
DA 2023-11-11
ER

PT C
AU Li, XY
   Neil, D
   Delbruck, T
   Liu, SC
AF Li, Xiaoya
   Neil, Daniel
   Delbruck, Tobi
   Liu, Shih-Chii
GP IEEE
TI Lip Reading Deep Network Exploiting Multi-modal Spiking Visual and
   Auditory Sensors
SO 2019 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS (ISCAS)
SE IEEE International Symposium on Circuits and Systems
DT Proceedings Paper
CT IEEE International Symposium on Circuits and Systems (IEEE ISCAS)
CY MAY 26-29, 2019
CL Sapporo, JAPAN
DE deep learning network; sensor fusion; spiking silicon sensors; spiking
   cochlea; Dynamic Vision Sensor
ID SILICON COCHLEA; VISION; DRIVEN
AB This work presents a lip reading deep neural network that fuses the asynchronous spiking outputs of two bio-inspired silicon multimodal sensors: the Dynamic Vision Sensor (DVS) and the Dynamic Audio Sensor (DAS). The fusion network is tested on the GRID visual-audio lipreading dataset. Classification is carried out using event-based features generated from the spikes of the DVS and DAS. Networks are trained separately on the two modalities and also jointly trained on both modalities. The jointly trained network when tested on DVS spike frames alone, showed a relative increase in accuracy of around 23% over that of the single DVS modality network.
C1 [Delbruck, Tobi] Univ Zurich, Inst Neuroinformat, Zurich, Switzerland.
   Swiss Fed Inst Technol, Zurich, Switzerland.
RP Delbruck, T (corresponding author), Univ Zurich, Inst Neuroinformat, Zurich, Switzerland.
EM tobi@ini.uzh.ch; shih@ini.uzh.ch
CR Amir A., 2017, P IEEE C COMP VIS PA, P7243, DOI DOI 10.1109/CVPR.2017.781
   Pérez-Carrasco JA, 2013, IEEE T PATTERN ANAL, V35, P2706, DOI 10.1109/TPAMI.2013.71
   Anumula J, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00023
   Bradski G., 2000, DOBBS J SOFTWARE TOO, V3
   Brandli C, 2014, IEEE J SOLID-ST CIRC, V49, P2333, DOI 10.1109/JSSC.2014.2342715
   Chan V, 2007, IEEE T CIRCUITS-I, V54, P48, DOI 10.1109/TCSI.2006.887979
   Cho K., 2014, P C EMP METH NAT LAN, P1724
   Chung JS, 2017, PROC CVPR IEEE, P3444, DOI 10.1109/CVPR.2017.367
   Cooke M, 2006, J ACOUST SOC AM, V120, P2421, DOI 10.1121/1.2229005
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Kiselev I, 2016, IEEE INT SYMP CIRC S, P2495, DOI 10.1109/ISCAS.2016.7539099
   Lichtsteiner Patrick, 2008, IEEE Journal of Solid-State Circuits, V43, P566, DOI 10.1109/JSSC.2007.914337
   Liu SC, 2014, IEEE T BIOMED CIRC S, V8, P453, DOI 10.1109/TBCAS.2013.2281834
   Moeys DP, 2016, 2016 2ND INTERNATIONAL CONFERENCE ON EVENT-BASED CONTROL, COMMUNICATION, AND SIGNAL PROCESSING (EBCCSP), DOI 10.1109/EBCCSP.2016.7605233
   Neil D, 2016, IEEE INT SYMP CIRC S, P2282, DOI 10.1109/ISCAS.2016.7539039
   Ngiam J., 2011, IEEE INT C MACH LEAR, P689, DOI DOI 10.5555/3104482.3104569
   Noda K, 2015, APPL INTELL, V42, P722, DOI 10.1007/s10489-014-0629-7
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   Posch C, 2011, IEEE J SOLID-ST CIRC, V46, P259, DOI 10.1109/JSSC.2010.2085952
   Savran A, 2018, IEEE INT CONF AUTOMA, P333, DOI 10.1109/FG.2018.00055
   Serrano-Gotarredona T, 2013, IEEE J SOLID-ST CIRC, V48, P827, DOI 10.1109/JSSC.2012.2230553
   Tsai WY, 2017, IEEE T COMPUT, V66, P996, DOI 10.1109/TC.2016.2630683
   Wand M, 2016, INT CONF ACOUST SPEE, P6115, DOI 10.1109/ICASSP.2016.7472852
   Yang MH, 2016, IEEE J SOLID-ST CIRC, V51, P2554, DOI 10.1109/JSSC.2016.2604285
   Yang MH, 2015, IEEE J SOLID-ST CIRC, V50, P2149, DOI 10.1109/JSSC.2015.2425886
NR 25
TC 0
Z9 0
U1 0
U2 2
PY 2019
WC Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Xu, T
   Xiao, N
   Zhai, XL
   Chan, PK
   Tin, C
AF Xu, Tao
   Xiao, Na
   Zhai, Xiaolong
   Chan, Pak Kwan
   Tin, Chung
TI Real-time cerebellar neuroprosthetic system based on a spiking neural
   network model of motor learning
SO JOURNAL OF NEURAL ENGINEERING
DT Article
DE neuroprosthetics; cerebellum; FPGA; delay eyeblink conditioning; spiking
   neural network model
ID RESPONSES; CELLS
AB Objective. Damage to the brain, as a result of various medical conditions, impacts the everyday life of patients and there is still no complete cure to neurological disorders. Neuroprostheses that can functionally replace the damaged neural circuit have recently emerged as a possible solution to these problems. Here we describe the development of a real-time cerebellar neuroprosthetic system to substitute neural function in cerebellar circuitry for learning delay eyeblink conditioning (DEC). Approach. The system was empowered by a biologically realistic spiking neural network (SNN) model of the cerebellar neural circuit, which considers the neuronal population and anatomical connectivity of the network. The model simulated synaptic plasticity critical for learning DEC. This SNN model was carefully implemented on a field programmable gate array (FPGA) platform for real-time simulation. This hardware system was interfaced in in vivo experiments with anesthetized rats and it used neural spikes recorded online from the animal to learn and trigger conditioned eyeblink in the animal during training. Main results. This rat-FPGA hybrid system was able to process neuronal spikes in real-time with an embedded cerebellum model of similar to 10 000 neurons and reproduce learning of DEC with different inter-stimulus intervals. Our results validated that the system performance is physiologically relevant at both the neural (firing pattern) and behavioral (eyeblink pattern) levels. Significance. This integrated system provides the sufficient computation power for mimicking the cerebellar circuit in real-time. The system interacts with the biological system naturally at the spike level and can be generalized for including other neural components (neuron types and plasticity) and neural functions for potential neuroprosthetic applications.
C1 [Xu, Tao; Xiao, Na; Zhai, Xiaolong; Chan, Pak Kwan; Tin, Chung] City Univ Hong Kong, Dept Mech & Biomed Engn, Hong Kong, Hong Kong, Peoples R China.
   [Tin, Chung] City Univ Hong Kong, CRA, Hong Kong, Hong Kong, Peoples R China.
   [Tin, Chung] City Univ Hong Kong, CBNN, Hong Kong, Hong Kong, Peoples R China.
RP Tin, C (corresponding author), City Univ Hong Kong, Dept Mech & Biomed Engn, Hong Kong, Hong Kong, Peoples R China.; Tin, C (corresponding author), City Univ Hong Kong, CRA, Hong Kong, Hong Kong, Peoples R China.; Tin, C (corresponding author), City Univ Hong Kong, CBNN, Hong Kong, Hong Kong, Peoples R China.
EM chungtin@cityu.edu.hk
CR Bamford SA, 2012, IEEE T NEUR SYS REH, V20, P455, DOI 10.1109/TNSRE.2012.2187933
   Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Berger TW, 2012, IEEE T NEUR SYS REH, V20, P198, DOI 10.1109/TNSRE.2012.2189133
   BERTHIER NE, 1990, EXP BRAIN RES, V83, P44
   Broccard FD, 2017, J NEURAL ENG, V14, DOI 10.1088/1741-2552/aa67a9
   Casellato C, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0112265
   Chaumont J, 2013, P NATL ACAD SCI USA, V110, P16223, DOI 10.1073/pnas.1302310110
   Freeman JH, 2003, LEARN MEMORY, V10, P337, DOI 10.1101/lm.63703
   Furber S, 2016, J NEURAL ENG, V13, DOI 10.1088/1741-2560/13/5/051001
   Furber SB, 2013, IEEE T COMPUT, V62, P2454, DOI 10.1109/TC.2012.142
   Gao ZY, 2012, NAT REV NEUROSCI, V13, P619, DOI 10.1038/nrn3312
   Garrido JA, 2013, FRONT COMPUT NEUROSC, V7, DOI 10.3389/fncom.2013.00064
   Gosui M, 2016, FRONT NEUROANAT, V10, DOI 10.3389/fnana.2016.00021
   Hampson RE, 2012, IEEE T NEUR SYS REH, V20, P510, DOI 10.1109/TNSRE.2012.2190942
   Hampson RE, 2012, IEEE T NEUR SYS REH, V20, P184, DOI 10.1109/TNSRE.2012.2189163
   Heiney SA, 2014, J NEUROSCI, V34, P2321, DOI 10.1523/JNEUROSCI.4547-13.2014
   Hogri R, 2015, SCI REP-UK, V5, DOI 10.1038/srep08451
   ITO M, 1982, NEUROSCI LETT, V33, P253, DOI 10.1016/0304-3940(82)90380-9
   Jirenhed DA, 2011, CEREBELLUM, V10, P523, DOI 10.1007/s12311-011-0264-3
   Kim JJ, 1997, TRENDS NEUROSCI, V20, P177, DOI 10.1016/S0166-2236(96)10081-3
   Luo JW, 2016, IEEE T BIOMED CIRC S, V10, P742, DOI 10.1109/TBCAS.2015.2460232
   MAUK MD, 1992, BEHAV NEUROSCI, V106, P666, DOI 10.1037/0735-7044.106.4.666
   Mauk MD, 1997, LEARN MEMORY, V4, P130, DOI 10.1101/lm.4.1.130
   MCCORMICK DA, 1984, J NEUROSCI, V4, P2811
   MCCORMICK DA, 1984, SCIENCE, V223, P296, DOI 10.1126/science.6701513
   Medina JF, 2000, NAT NEUROSCI, V3, P1205, DOI 10.1038/81486
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Nguyen-Vu TDB, 2013, NAT NEUROSCI, V16, P1734, DOI 10.1038/nn.3576
   PERRETT SP, 1993, J NEUROSCI, V13, P1708
   Pinzon-Morales RD, 2014, FRONT NEURAL CIRCUIT, V8, DOI 10.3389/fncir.2014.00131
   Tsubota T, 2013, BEHAV BRAIN RES, V255, P26, DOI 10.1016/j.bbr.2013.04.019
   Yamazaki T, 2005, NEURAL COMPUT, V17, P1032, DOI 10.1162/0899766053491850
   Yamazaki T, 2007, EUR J NEUROSCI, V26, P2279, DOI 10.1111/j.1460-9568.2007.05837.x
   Yamazaki T, 2013, NEURAL NETWORKS, V47, P103, DOI 10.1016/j.neunet.2013.01.019
   Yamazaki T, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0033319
NR 35
TC 8
Z9 8
U1 2
U2 19
PD FEB
PY 2018
VL 15
IS 1
AR 016021
DI 10.1088/1741-2552/aa98e9
WC Engineering, Biomedical; Neurosciences
DA 2023-11-11
ER

PT C
AU Renaud, S
   Tomas, J
   Bornat, Y
   Daouzli, A
   Saïghi, S
AF Renaud, Sylvie
   Tomas, Jean
   Bornat, Yannick
   Daouzli, Adel
   Saighi, Sylvain
GP IEEE
TI Neuromimetic ICs with analog cores:: an alternative for simulating
   spiking neural networks
SO 2007 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOLS 1-11
SE IEEE International Symposium on Circuits and Systems
DT Proceedings Paper
CT IEEE International Symposium on Circuits and Systems
CY MAY 27-30, 2007
CL New Orleans, LA
ID MODELS
AB This paper aims at discussing the implementation of simulation systems for SNN based on analog computation cores (neuromimetic 10). Such systems are an alternative to completely digital solutions for the simulation of spiking neurons or neural networks. Design principles for the neuromimetic ICs and the hosting systems are presented together with their features and performances. We summarize the existing architectures and neuron models used in such systems, when configured as stand-alone tools for simulating ANN or together with a neurophysiology set-up to study hybrid living artificial neural networks. As a primary illustration, we present results from one of the platforms: hardware simulations of single neurons and adaptive neural networks modeled using the Hodgkin-Huxley formalism for point neurons and spike-timing dependent plasticity algorithms for the network adaptation. Additional examples are detailed in the other papers of the session.
C1 [Renaud, Sylvie; Tomas, Jean; Bornat, Yannick; Daouzli, Adel; Saighi, Sylvain] Univ Bordeaux 1, CNRS, ENSEIRB, IMS Lab, F-33405 Talence, France.
RP Renaud, S (corresponding author), Univ Bordeaux 1, CNRS, ENSEIRB, IMS Lab, F-33405 Talence, France.
EM sylvie.renaud@ims-bordeaux.fr
CR Badoual M, 2006, INT J NEURAL SYST, V16, P79, DOI 10.1142/S0129065706000524
   Binczak S, 2006, NEURAL NETWORKS, V19, P684, DOI 10.1016/j.neunet.2005.07.011
   BORNAT Y, 2005, DCIS 2005 LISB PORT
   CONNORS BW, 1990, TRENDS NEUROSCI, V13, P99, DOI 10.1016/0166-2236(90)90185-D
   Farquhar E, 2005, IEEE T CIRCUITS-I, V52, P477, DOI 10.1109/TCSI.2004.842871
   FITZHUGH R, 1961, BIOPHYS J, V1, P445, DOI 10.1016/S0006-3495(61)86902-6
   Fusi S, 2000, NEURAL COMPUT, V12, P2227, DOI 10.1162/089976600300014917
   Gerstner W., 2002, SPIKING NEURON MODEL
   Hasler P, 2007, IEEE INT SYMP CIRC S, P3359, DOI 10.1109/ISCAS.2007.378287
   HYNNA KM, UNPUB ISCAS 2007
   INDIVERI G, UNPUB ISCAS 2007
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Jung R, 2001, IEEE T NEUR SYS REH, V9, P319, DOI 10.1109/7333.948461
   Le Masson G, 2002, NATURE, V417, P854, DOI 10.1038/nature00825
   Liu SC, 2001, NEURAL NETWORKS, V14, P629, DOI 10.1016/S0893-6080(01)00054-5
   MAHOWALD M, 1991, NATURE, V354, P515, DOI 10.1038/354515a0
   Sarpeshkar R, 1998, NEURAL COMPUT, V10, P1601, DOI 10.1162/089976698300017052
   SCHEMMEL J, UNPUB ISCAS 2007
   Sorensen M, 2004, J NEUROSCI, V24, P5427, DOI 10.1523/JNEUROSCI.4449-03.2004
   Vogelstein RJ, 2004, 2004 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL 5, PROCEEDINGS, P385
NR 20
TC 39
Z9 39
U1 0
U2 2
PY 2007
BP 3355
EP 3358
DI 10.1109/ISCAS.2007.378286
WC Computer Science, Artificial Intelligence; Computer Science, Software
   Engineering; Engineering, Biomedical; Engineering, Electrical &
   Electronic; Mathematical & Computational Biology; Nanoscience &
   Nanotechnology; Imaging Science & Photographic Technology;
   Telecommunications
DA 2023-11-11
ER

PT C
AU Zhang, ML
   Qu, H
   Li, JP
   Xie, XR
AF Zhang, Malu
   Qu, Hong
   Li, Jianping
   Xie, Xiurui
BE Handa, H
   Ishibuchi, H
   Ong, YS
   Tan, KC
TI A New Supervised Learning Algorithm for Spiking Neurons
SO PROCEEDINGS OF THE 18TH ASIA PACIFIC SYMPOSIUM ON INTELLIGENT AND
   EVOLUTIONARY SYSTEMS, VOL 1
DT Proceedings Paper
CT 18th Asia Pacific Symposium on Intelligent and Evolutionary Systems
CY NOV 10-12, 2014
CL Singapore, SINGAPORE
DE Spiking neurons; Membrane potential; Supervised learning
ID NEURAL-NETWORKS; CLASSIFICATION
AB Training spiking neurons to output desired spike train is a fundamental research in spiking neural networks. The current article proposes a novel and efficient supervised learning algorithm for spiking neurons. We divide the running time of spiking neurons into two classes: desired output time and not desired output time. Our learning method makes the membrane potential equal to threshold at desired output time, and makes the membrane potential lower than threshold at not desired output time. For efficiency, at not desired output time, we just calculate the membrane potential at some special time points where the spiking neuron is most likely to output a wrong spike. The experimental results show that the learning performance of the proposed method is better than the existing methods in accuracy and efficiency.
C1 [Zhang, Malu; Qu, Hong; Li, Jianping; Xie, Xiurui] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 610054, Peoples R China.
RP Zhang, ML (corresponding author), Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 610054, Peoples R China.
CR Bair W, 1996, NEURAL COMPUT, V8, P1185, DOI 10.1162/neco.1996.8.6.1185
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Buhmann JM, 2005, NEURAL COMPUT, V17, P1010, DOI 10.1162/0899766053491913
   Florian RV, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0040233
   Ghosh-Dastidar S, 2009, INT J NEURAL SYST, V19, P295, DOI 10.1142/S0129065709002002
   Hu J, 2013, NEURAL COMPUT, V25, P450, DOI 10.1162/NECO_a_00395
   Kasabov N, 2014, NEUROCOMPUTING, V134, P269, DOI 10.1016/j.neucom.2013.09.049
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   MAINEN ZF, 1995, SCIENCE, V268, P1503, DOI 10.1126/science.7770778
   Mohemmed A, 2012, INT J NEURAL SYST, V22, DOI 10.1142/S0129065712500128
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Reinagel P, 2000, J NEUROSCI, V20, P5392, DOI 10.1523/JNEUROSCI.20-14-05392.2000
   Schreiber S, 2003, NEUROCOMPUTING, V52-4, P925, DOI 10.1016/S0925-2312(02)00838-X
   Wade JJ, 2010, IEEE T NEURAL NETWOR, V21, P1817, DOI 10.1109/TNN.2010.2074212
   WULFRAM G, 2002, SPIKING NEURON MODEL
   Xu Y, 2013, NEURAL COMPUT, V25, P1472, DOI 10.1162/NECO_a_00450
   Xu Y, 2013, NEURAL NETWORKS, V43, P99, DOI 10.1016/j.neunet.2013.02.003
   Yu Q, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0078318
NR 18
TC 0
Z9 0
U1 0
U2 3
PY 2015
BP 171
EP 184
DI 10.1007/978-3-319-13359-1_14
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods
DA 2023-11-11
ER

PT C
AU Botzheim, J
   Kubota, N
AF Botzheim, Janos
   Kubota, Naoyuki
GP IEEE
TI Spiking Neural Network based Emotional Model for Robot Partner
SO 2014 IEEE SYMPOSIUM ON ROBOTIC INTELLIGENCE IN INFORMATIONALLY
   STRUCTURED SPACE (RIISS)
DT Proceedings Paper
CT Robotic Intelligence In Informationally Structured Space (RiiSS)
CY DEC 09-12, 2014
CL Orlando, FL
AB In this paper, a spiking neural network based emotional model is proposed for a smart phone based robot partner. Since smart phone has limited computational power compared to personal computers, a simple spike response model is applied for the neurons in the neural network. The network has three layers following the concept of emotion, feeling, and mood. The perceptual input stimulates the neurons in the first, emotion layer. Weights adjustment is also proposed for the interconnected neurons in the feeling layer and between the feeling and mood layer based on Hebbian learning. Experiments are presented to validate the proposed method. Based on the emotional model, the output action such as gestural and facial expressions for the robot is calculated.
C1 [Botzheim, Janos; Kubota, Naoyuki] Tokyo Metropolitan Univ, Grad Sch Syst Design, 6-6 Asahigaoka, Hino, Tokyo 1910065, Japan.
   [Botzheim, Janos] Szechenyi Istvan Univ, Dept Automat, H-9026 Gyor, Hungary.
RP Botzheim, J (corresponding author), Tokyo Metropolitan Univ, Grad Sch Syst Design, 6-6 Asahigaoka, Hino, Tokyo 1910065, Japan.
EM botzheim@tmu.ac.jp; kubota@tmu.ac.jp
CR [Anonymous], 2010, P IEEE WORLD C COMP
   Bartneck C, 2005, INT J HUM-COMPUT ST, V62, P179, DOI 10.1016/j.ijhcs.2004.11.006
   Botzheim J., 2013, P 3 INT WORKSH ADV C
   Botzheim J, 2014, WORLD AUTOMAT CONG
   Botzheim J, 2013, PROCEDIA COMPUT SCI, V22, P883, DOI 10.1016/j.procs.2013.09.171
   Buck R., 1984, COMMUNICATION EMOTIO
   Cornelius R. R., 1996, SCI EMOTION RES TRAD
   El-Nasr MS, 1998, 1998 IEEE INTERNATIONAL CONFERENCE ON FUZZY SYSTEMS AT THE IEEE WORLD CONGRESS ON COMPUTATIONAL INTELLIGENCE - PROCEEDINGS, VOL 1-2, P135, DOI 10.1109/FUZZY.1998.687472
   Gerstner W., 2002, SPIKING NEURON MODEL
   Hebb D. O., 1949, ORG BEHAV
   HechtNielsen R, 1990, NEUROCOMPUTING
   Kubota N, 2005, INFORM SCIENCES, V171, P403, DOI 10.1016/j.ins.2004.09.012
   Kubota Naoyuki, 2008, 2008 IEEE Conference on Soft Computing in Industrial Applications. SMCia/08, P303, DOI 10.1109/SMCIA.2008.5045979
   Kubota N, 2010, SMART INNOV SYST TEC, V1, P89
   MAASS W., 1999, PULSED NEURAL NETWOR
   Pessoa L, 2008, NAT REV NEUROSCI, V9, P148, DOI 10.1038/nrn2317
   Sakata Y., 2013, P 24 INT S MICR HUM, P233
   Shu F., 2012, P WCCI 2012 IEEE WOR, P728
   Sperber D., 1995, RELEVANCE COMMUNICAT, V2nd
   Tang D, 2013, IEEE INT WORKS GENET, P36, DOI 10.1109/GEFS.2013.6601053
   Yorita A, 2013, PROC IEEE INT SYMP
   Yorita A, 2011, IEEE T AUTON MENT DE, V3, P64, DOI 10.1109/TAMD.2011.2105868
   Zurada J.M, 1992, INTRO ARTIFICIAL NEU, P8
NR 23
TC 1
Z9 1
U1 0
U2 0
PY 2014
BP 22
EP 27
WC Robotics
DA 2023-11-11
ER

PT C
AU Yusoff, N
   Ibrahim, MF
AF Yusoff, Nooraini
   Ibrahim, Mohammed Fadhil
BE Jamaludin, Z
   ChePa, N
   Ishak, WHW
   Zaibon, SB
TI FACE-VOICE ASSOCIATION TOWARDS MULTIMODAL-BASED AUTHENTICATION USING
   MODULATED SPIKE-TIME DEPENDENT LEARNING
SO PROCEEDINGS OF THE 5TH INTERNATIONAL CONFERENCE ON COMPUTING &
   INFORMATICS
SE Proceedings of the International Conference on Computing & Informatics
DT Proceedings Paper
CT 5th International Conference on Computing & Informatics
CY AUG 11-13, 2015
CL Istanbul, TURKEY
DE multimodal; associative learning; spiking neural network; spike-time
   dependent plasticity
ID RECOGNITION
AB We propose a reward based learning to associate face and voice stimuli. In particular, we implement learning in a spiking neural network paradigm using modulated spike-time dependent plasticity (STDP). The face and voice stimuli are paired with a temporal delay, and the network is trained to associate the paired face-voice with a target response. The learning rule is dependent on a reward policy in which the network is given a positive reward for a correct response to a face-voice stimulus pair, or the network receives a negative reward for an incorrect response. Despite a stochastic environment, the learning result of real images and sound indicates a good performance with 77.33% accuracy. The result demonstrates that a machine can be trained to associate a pair of biometric inputs to a target response.
C1 [Yusoff, Nooraini] Univ Utara Malaysia, UUM Coll Arts & Sci, Sch Comp, Sintok 06010, Kedah, Malaysia.
   [Ibrahim, Mohammed Fadhil] Middle Tech Univ, Tech Coll Management Baghdad, Baghdad 10047, Iraq.
RP Yusoff, N (corresponding author), Univ Utara Malaysia, UUM Coll Arts & Sci, Sch Comp, Sintok 06010, Kedah, Malaysia.
EM nooraini@uum.edu.my; mfi@mtu.edu.iq
CR [Anonymous], INT J ADV ROBOTIC SY
   Borah T. R., 2013, INT J ELECT SIGNALS, V3, P98
   Florian RV, 2007, NEURAL COMPUT, V19, P1468, DOI 10.1162/neco.2007.19.6.1468
   Hassan Yasser Fouad, 2012, IJIIP INT J INTELLIG, V3, P16
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Huang Z., 2013, PATTERN RECOGNITION
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2007, CEREB CORTEX, V17, P2443, DOI 10.1093/cercor/bhl152
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Jain AK, 2012, COMPUTER, V45, P87, DOI 10.1109/MC.2012.364
   Kasabov N., 2012, NEURAL NETWORKS
   Kasabov N., 2012, LNCS, V7311, P234
   Long T. B., 2012, WORLD C ENG COMP SCI
   Maind S. B., 2014, J COMPUTER ENG IOSR, V4, P40
   Nayak P. K., 2012, INT J ENG, V1
   Sahoo SK, 2012, IETE TECH REV, V29, P54, DOI 10.4103/0256-4602.93139
   Xu C, 2013, APPL MATH INFORM SCI, V7, P455, DOI 10.12785/amis/070205
   Yusoff N., 2012, LCNS, V7552, P137, DOI [10.1007/978-3-642-33269-2_18, DOI 10.1007/978-3-642-33269-2_18]
NR 18
TC 0
Z9 0
U1 0
U2 3
PY 2015
BP 58
EP 64
WC Computer Science, Artificial Intelligence; Computer Science, Information
   Systems; Computer Science, Theory & Methods
DA 2023-11-11
ER

PT J
AU Xie, XR
   Qu, H
   Yi, Z
   Kurths, J
AF Xie, Xiurui
   Qu, Hong
   Yi, Zhang
   Kurths, Jurgen
TI Efficient Training of Supervised Spiking Neural Network via Accurate
   Synaptic-Efficiency Adjustment Method
SO IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
DT Article
DE Pattern recognition; selective attention mechanism; spiking neural
   network (SNN); supervised learning
ID LEARNING ALGORITHM; CLASSIFICATION; RECOGNITION; SIMULATOR; MODEL; TIME
AB The spiking neural network (SNN) is the third generation of neural networks and performs remarkably well in cognitive tasks, such as pattern recognition. The temporal neural encode mechanism found in biological hippocampus enables SNN to possess more powerful computation capability than networks with other encoding schemes. However, this temporal encoding approach requires neurons to process information serially on time, which reduces learning efficiency significantly. To keep the powerful computation capability of the temporal encoding mechanism and to overcome its low efficiency in the training of SNNs, a new training algorithm, the accurate synaptic-efficiency adjustment method is proposed in this paper. Inspired by the selective attention mechanism of the primate visual system, our algorithm selects only the target spike time as attention areas, and ignores voltage states of the untarget ones, resulting in a significant reduction of training time. Besides, our algorithm employs a cost function based on the voltage difference between the potential of the output neuron and the firing threshold of the SNN, instead of the traditional precise firing time distance. A normalized spike-timing-dependent-plasticity learning window is applied to assigning this error to different synapses for instructing their training. Comprehensive simulations are conducted to investigate the learning properties of our algorithm, with input neurons emitting both single spike and multiple spikes. Simulation results indicate that our algorithm possesses higher learning performance than the existing other methods and achieves the state-of-the-art efficiency in the training of SNN.
C1 [Xie, Xiurui; Qu, Hong] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 610054, Peoples R China.
   [Qu, Hong; Kurths, Jurgen] Potsdam Inst Climate Impact Res, D-14473 Potsdam, Germany.
   [Yi, Zhang] Sichuan Univ, Coll Comp Sci, Chengdu 610065, Peoples R China.
   [Kurths, Jurgen] Humboldt Univ, Dept Phys, D-12489 Berlin, Germany.
RP Xie, XR (corresponding author), Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 610054, Peoples R China.
EM xiexiurui@126.com; hongqu@uestc.edu.cn; zhangyi@scu.edu.cn;
   kurths@pik-potsdam.de
CR ABELES M, 1993, J NEUROPHYSIOL, V70, P1629, DOI 10.1152/jn.1993.70.4.1629
   [Anonymous], 2013, INT JOINT C NEUR NET, DOI DOI 10.1109/AGILE.2013.7
   [Anonymous], 2015, 2015 INT JOINT C NEU, DOI DOI 10.1109/IJCNN.2015.7280592
   Bache K., 2013, TECH REP
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199
   Desimone R., 1989, HDB NEUROPSYCHOLOGY, V2, P267
   Dora S, 2016, NEUROCOMPUTING, V171, P1216, DOI 10.1016/j.neucom.2015.07.086
   Dora S, 2015, APPL SOFT COMPUT, V36, P255, DOI 10.1016/j.asoc.2015.06.062
   Dora S, 2014, IEEE IJCNN, P2415, DOI 10.1109/IJCNN.2014.6889775
   Gerstner W., 2002, SPIKING NEURON MODEL
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   Hu J, 2013, NEURAL COMPUT, V25, P450, DOI 10.1162/NECO_a_00395
   Kaplan BA, 2014, IEEE IJCNN, P3205
   Kasabov NK, 2014, NEURAL NETWORKS, V52, P62, DOI 10.1016/j.neunet.2014.01.006
   Kastner S, 2000, ANNU REV NEUROSCI, V23, P315, DOI 10.1146/annurev.neuro.23.1.315
   Masquelier T, 2009, NEURAL COMPUT, V21, P1259, DOI 10.1162/neco.2008.06-08-804
   Mehta MR, 2002, NATURE, V417, P741, DOI 10.1038/nature00807
   Minkovich K, 2014, IEEE T NEUR NET LEAR, V25, P316, DOI 10.1109/TNNLS.2013.2276056
   Mohemmed A, 2013, NEUROCOMPUTING, V107, P3, DOI 10.1016/j.neucom.2012.08.034
   Nadasdy Z, 2009, FRONT SYST NEUROSCI, V3, DOI 10.3389/neuro.06.006.2009
   Naveros F, 2015, IEEE T NEUR NET LEAR, V26, P1567, DOI 10.1109/TNNLS.2014.2345844
   PFEIFER R, 1989, CONNECTIONISM IN PERSPECTIVE, pR11
   Ponulak F, 2011, ACTA NEUROBIOL EXP, V71, P409
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Russell BC, 2008, INT J COMPUT VISION, V77, P157, DOI 10.1007/s11263-007-0090-8
   Schreiber S, 2003, NEUROCOMPUTING, V52-4, P925, DOI 10.1016/S0925-2312(02)00838-X
   SEJNOWSKI TJ, 1995, NATURE, V376, P21, DOI 10.1038/376021a0
   Snippe HP, 1996, NEURAL COMPUT, V8, P511, DOI 10.1162/neco.1996.8.3.511
   STUART GJ, 1994, NATURE, V367, P69, DOI 10.1038/367069a0
   Van Rullen R, 2001, NEURAL COMPUT, V13, P1255, DOI 10.1162/08997660152002852
   Wade JJ, 2010, IEEE T NEURAL NETWOR, V21, P1817, DOI 10.1109/TNN.2010.2074212
   Xu Y, 2013, NEURAL COMPUT, V25, P1472, DOI 10.1162/NECO_a_00450
   Xu Y, 2013, NEURAL NETWORKS, V43, P99, DOI 10.1016/j.neunet.2013.02.003
   Yu Q, 2016, IEEE T NEUR NET LEAR, V27, P621, DOI 10.1109/TNNLS.2015.2416771
   Yu QF, 2013, PLOS ONE, V8, DOI [10.1371/journal.pone.0059627, 10.1371/journal.pone.0078318]
   Zhang ZM, 2015, NEUROCOMPUTING, V151, P985, DOI 10.1016/j.neucom.2014.03.086
   Zhao B, 2015, IEEE T NEUR NET LEAR, V26, P1963, DOI 10.1109/TNNLS.2014.2362542
NR 39
TC 32
Z9 34
U1 0
U2 32
PD JUN
PY 2017
VL 28
IS 6
BP 1411
EP 1424
DI 10.1109/TNNLS.2016.2541339
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Computer Science, Theory & Methods; Engineering,
   Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Atsumi, M
AF Atsumi, M
BE Zhang, L
   Gu, F
TI Sequence memories and their integration for planning: A spiking neural
   network model
SO 8TH INTERNATIONAL CONFERENCE ON NEURAL INFORMATION PROCESSING, VOLS 1-3,
   PROCEEDING
DT Proceedings Paper
CT 8th International Conference on Neural Information Processing (ICONIP
   2001)
CY NOV 14-18, 2001
CL SHANGHAI, PEOPLES R CHINA
ID NMDA CHANNELS; THETA/GAMMA
AB We propose a biologically-inspired auto/hetero-associative spiking neural network combined with a working memory model, in which a state-driven forward sequence and a goal-driven backward sequence on the associative network are integrated in the working memory to make a plan. By discrete pulse-driven neural network simulations, we show that several characteristics of planning process such as goal-directed attention control at a branch point of a plan, incremental planning, and planning by combining episodes can be realized in our system.
C1 Soka Univ, Dept Informat Syst Sci, Fac Engn, Hachioji, Tokyo 1928577, Japan.
RP Atsumi, M (corresponding author), Soka Univ, Dept Informat Syst Sci, Fac Engn, 1-236 Tangi Cho, Hachioji, Tokyo 1928577, Japan.
CR Jensen O, 1996, LEARN MEMORY, V3, P264, DOI 10.1101/lm.3.2-3.264
   Jensen O, 1996, LEARN MEMORY, V3, P243, DOI 10.1101/lm.3.2-3.243
   LISMAN JE, 1995, SCIENCE, V267, P1512, DOI 10.1126/science.7878473
   OKEEFE J, 1993, HIPPOCAMPUS, V3, P317, DOI 10.1002/hipo.450030307
NR 4
TC 0
Z9 0
U1 0
U2 3
PY 2001
BP 891
EP 896
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Robotics
DA 2023-11-11
ER

PT C
AU Ahmed, K
   Shrestha, A
   Qiu, QR
AF Ahmed, Khadeer
   Shrestha, Amar
   Qiu, Qinru
GP IEEE
TI Simulation of Bayesian Learning and Inference on Distributed Stochastic
   Spiking Neural Networks
SO 2016 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUL 24-29, 2016
CL Vancouver, CANADA
AB The ability of neural networks to perform pattern recognition, classification and associative memory, is essential to applications such as image and speech recognition, natural language understanding, decision making etc. In spiking neural networks (SNNs), information is encoded as sparsely distributed train of spikes, which allows learning through the spike-timing dependent plasticity (STDP) property. SNNs can potentially achieve very large scale implementation and distributed learning due to the inherent asynchronous and sparse inter-neuron communications. In this work, we develop an efficient, scalable and flexible SNN simulator, which supports learning through STDP. The simulator is ideal for biologically inspired neuron models for computation but not for biologically realistic models. Bayesian neuron model for SNNs that is capable of online and fully-distributed STDP learning is introduced. The function of the simulator is validated using two networks representing two different applications from unsupervised feature extraction to inference based sentence construction.
C1 [Ahmed, Khadeer; Shrestha, Amar; Qiu, Qinru] Syracuse Univ, Dept Elect Engn & Comp Sci, Syracuse, NY 13244 USA.
RP Ahmed, K (corresponding author), Syracuse Univ, Dept Elect Engn & Comp Sci, Syracuse, NY 13244 USA.
EM khahmed@syr.edu; amshrest@syr.edu; qiqiu@syr.edu
CR [Anonymous], 2010, SPIKE TIMING DEPENDE
   [Anonymous], BAYESIAN COMPUTATION
   [Anonymous], 2008, FRONTIERS NEUROINFOR
   [Anonymous], 2002, J MATH PSYCHOL
   [Anonymous], 2012, BOOK GENESIS EXPLORI, DOI DOI 10.1007/s10827-007-0038-6
   Behi T, 2012, 2012 6TH INTERNATIONAL CONFERENCE ON SCIENCES OF ELECTRONICS, TECHNOLOGIES OF INFORMATION AND TELECOMMUNICATIONS (SETIT), P701, DOI 10.1109/SETIT.2012.6481999
   Beyeler M, 2013, NEURAL NETWORKS, V48, P109, DOI 10.1016/j.neunet.2013.07.012
   Deneve S., 2005, ADV NEURAL INFORM PR, V17, P353
   Doya K., 2007, BAYESIAN BRAIN PROBA
   Engel TA, 2015, NAT COMMUN, V6, DOI 10.1038/ncomms7454
   Furber SB, 2013, IEEE T COMPUT, V62, P2454, DOI 10.1109/TC.2012.142
   Gewaltig M-O., 2007, SCHOLARPEDIA, V2, DOI [10.4249/scholarpedia.1430, DOI 10.4249/SCHOLARPEDIA.1430]
   Gupta A, 2007, IEEE IJCNN, P53, DOI 10.1109/IJCNN.2007.4370930
   Hines ML, 1997, NEURAL COMPUT, V9, P1179, DOI 10.1162/neco.1997.9.6.1179
   Lee H., 2009, ICML, P609, DOI 10.1145/1553374.1553453
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   Oram MW, 1999, J NEUROPHYSIOL, V81, P3021, DOI 10.1152/jn.1999.81.6.3021
   Qiu QR, 2013, IEEE T COMPUT, V62, P886, DOI 10.1109/TC.2012.50
   Ros E, 2006, NEURAL COMPUT, V18, P2959, DOI 10.1162/neco.2006.18.12.2959
   Seung HS, 2003, NEURON, V40, P1063, DOI 10.1016/S0896-6273(03)00761-X
   Wang YY, 2015, PROCEEDINGS OF THE 2015 SYMPOSIUM ON PIEZOELECTRICITY, ACOUSTIC WAVES AND DEVICE APPLICATIONS, P189, DOI 10.1109/SPAWDA.2015.7364469
NR 22
TC 2
Z9 3
U1 0
U2 2
PY 2016
BP 1044
EP 1051
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Paugam-Moisy, H
   Martinez, R
   Bengio, S
AF Paugam-Moisy, Helene
   Martinez, Regis
   Bengio, Samy
TI Delay learning and polychronization for reservoir computing
SO NEUROCOMPUTING
DT Article; Proceedings Paper
CT 18th European Symposium on Artificial Neural Networks
CY APR, 2008
CL Brugge, BELGIUM
DE reservoir computing; spiking neuron network; synaptic plasticity; STDP;
   polychronization; programmable delay; margin criterion; classification
ID TIMING-DEPENDENT PLASTICITY; SPIKING NEURONS; INTRINSIC EXCITABILITY;
   NEURAL COMPUTATION; NETWORKS; TIME; BACKPROPAGATION; TRANSMISSION; LONG
AB We propose a multi-timescale learning rule for spiking neuron networks, in the line of the recently emerging field of reservoir computing. The reservoir is a network model of spiking neurons, with random topology and driven by STDP (spike-time-dependent plasticity), a temporal Hebbian unsupervised learning mode, biologically observed. The model is further driven by a supervised learning algorithm, based on a margin criterion, that affects the synaptic delays linking the network to the readout neurons, with classification as a goal task. The network processing and the resulting performance can be explained by the concept of polychronization, proposed by Izhikevich [Polychronization: computation with spikes, Neural Comput. 18(2) (2006) 245-282], on physiological grounds. The model emphasizes that polychronization can be used as a tool for exploiting the computational power of synaptic delays and for monitoring the topology and activity of a spiking neuron network. (c) 2008 Elsevier B.V. All rights reserved.
C1 [Paugam-Moisy, Helene; Martinez, Regis] Univ Lyon 2, CNRS, UMR 5205, LIRIS, F-69676 Bron, France.
   [Bengio, Samy] Google, Mountain View, CA 94043 USA.
RP Martinez, R (corresponding author), Univ Lyon 2, CNRS, UMR 5205, LIRIS, Bat C,5 Ave Pierre Mendes France, F-69676 Bron, France.
EM hpaugam@liris.cnrs.fr; regis.martinez@liris.cnrs.fr; bengio@google.com
CR Abbott LF, 2000, NAT NEUROSCI, V3, P1178, DOI 10.1038/81453
   Abeles M., 1991, CORTICONICS NEURAL C
   [Anonymous], 1998, NEW YORK
   [Anonymous], PRINCIPLES NEURAL SC
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Bohte SM, 2007, NEURAL COMPUT, V19, P371, DOI 10.1162/neco.2007.19.2.371
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Braitenberg V., 1991, ANATOMY CORTEX STAT
   Butko NJ, 2007, NEUROCOMPUTING, V70, P1130, DOI 10.1016/j.neucom.2006.11.006
   Chechik G, 2003, NEURAL COMPUT, V15, P1481, DOI 10.1162/089976603321891774
   Daoudal G, 2003, LEARN MEMORY, V10, P456, DOI 10.1101/lm.64103
   Desai NS, 1999, NAT NEUROSCI, V2, P515, DOI 10.1038/9165
   Diesmann M, 1999, NATURE, V402, P529, DOI 10.1038/990101
   Gerstner W., 2002, SPIKING NEURON MODEL
   Hastie Trevor, 2001, ELEMENTS STAT LEARNI
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Jaeger H, 2003, ADV NEURAL INFORM PR, V15
   JAEGER H, 2001, TRGMD148 GERM NAT RE
   Jaeger H, 2007, NEURAL NETWORKS, V20, P287, DOI 10.1016/j.neunet.2007.04.001
   Kempter R, 1999, PHYS REV E, V59, P4498, DOI 10.1103/PhysRevE.59.4498
   KEYSERS D, 2002, SPR 2002
   Lazar A, 2007, NEURAL NETWORKS, V20, P312, DOI 10.1016/j.neunet.2007.04.020
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   Maass W, 1997, NETWORK-COMP NEURAL, V8, P355, DOI 10.1088/0954-898X/8/4/002
   Maass W, 1999, INFORM COMPUT, V153, P26, DOI 10.1006/inco.1999.2806
   Maass W, 2001, THEOR COMPUT SCI, V261, P157, DOI 10.1016/S0304-3975(00)00137-7
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   MAASS W, 1997, COLT 97
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   MEUNIER D, 2005, IJCNN 2005
   Natschlager T, 1998, NETWORK-COMP NEURAL, V9, P319, DOI 10.1088/0954-898X/9/3/003
   NORTON D, 2006, IJCNN 2006
   Nowotny T, 2003, J NEUROSCI, V23, P9776
   PAUGAMMOISY H, 2006, IDIAPRR11
   PAUGAMMOISY H, 2006, 54 IDIAP
   Pfister JP, 2006, NEURAL COMPUT, V18, P1318, DOI 10.1162/neco.2006.18.6.1318
   Rubin J, 2001, PHYS REV LETT, V86, P364, DOI 10.1103/PhysRevLett.86.364
   Schmitt M, 1998, ANN MATH ARTIF INTEL, V24, P181, DOI 10.1023/A:1018953300185
   SCHRAUWEN B, 2007, ESANN 2007
   Senn W, 2002, NEURAL COMPUT, V14, P583, DOI 10.1162/089976602317250915
   Síma J, 2005, NEURAL COMPUT, V17, P2635, DOI 10.1162/089976605774320601
   Singer W, 1999, NEURON, V24, P49, DOI 10.1016/S0896-6273(00)80821-1
   STEIL J, 2004, IJCNN 2004
   Steil JJ, 2007, NEURAL NETWORKS, V20, P353, DOI 10.1016/j.neunet.2007.04.011
   SWADLOW HA, 1992, J NEUROPHYSIOL, V68, P605, DOI 10.1152/jn.1992.68.2.605
   SWADLOW HA, 1985, J NEUROPHYSIOL, V54, P1346, DOI 10.1152/jn.1985.54.5.1346
   Toyoizumi T, 2005, P NATL ACAD SCI USA, V102, P5239, DOI 10.1073/pnas.0500495102
   TRIESCH J, 2005, ICANN 05
   Verstraeten D, 2007, NEURAL NETWORKS, V20, P391, DOI 10.1016/j.neunet.2007.04.003
   WARDERMANN M, 2007, ESANN 2007
   Woodin MA, 2003, NEURON, V39, P807, DOI 10.1016/S0896-6273(03)00507-5
NR 51
TC 72
Z9 85
U1 5
U2 39
PD MAR
PY 2008
VL 71
IS 7-9
BP 1143
EP 1158
DI 10.1016/j.neucom.2007.12.027
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT J
AU Mo, LF
   Chen, XA
   Wang, G
AF Mo, Lingfei
   Chen, Xinao
   Wang, Gang
TI EDHA: Event-Driven High Accurate Simulator for Spike Neural Networks
SO ELECTRONICS
DT Article
DE spike neural network; event-driven; simulator; high accuracy
ID TIMING-DEPENDENT PLASTICITY; NEURONS; PATTERN; TOOL
AB In recent years, spiking neural networks (SNNs) have attracted increasingly more researchers to study by virtue of its bio-interpretability and low-power computing. The SNN simulator is an essential tool to accomplish image classification, recognition, speech recognition, and other tasks using SNN. However, most of the existing simulators for spike neural networks are clock-driven, which has two main problems. First, the calculation result is affected by time slice, which obviously shows that when the calculation accuracy is low, the calculation speed is fast, but when the calculation accuracy is high, the calculation speed is unacceptable. The other is the failure of lateral inhibition, which severely affects SNN learning. In order to solve these problems, an event-driven high accurate simulator named EDHA (Event-Driven High Accuracy) for spike neural networks is proposed in this paper. EDHA takes full advantage of the event-driven characteristics of SNN and only calculates when a spike is generated, which is independent of the time slice. Compared with previous SNN simulators, EDHA is completely event-driven, which reduces a large amount of calculations and achieves higher computational accuracy. The calculation speed of EDHA in the MNIST classification task is more than 10 times faster than that of mainstream clock-driven simulators. By optimizing the spike encoding method, the former can even achieve more than 100 times faster than the latter. Due to the cross-platform characteristics of Java, EDHA can run on x86, amd64, ARM, and other platforms that support Java.
C1 [Mo, Lingfei; Chen, Xinao; Wang, Gang] Southeast Univ, Sch Instrument Sci & Engn, FutureX Lab, Nanjing 210096, Peoples R China.
RP Mo, LF (corresponding author), Southeast Univ, Sch Instrument Sci & Engn, FutureX Lab, Nanjing 210096, Peoples R China.
EM lfmo@seu.edu.cn; chenxa@seu.edu.cn; koala_wang@seu.edu.cn
CR Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   [Anonymous], 2016, PROC AUSTRALAS TRANS
   [Anonymous], 1993, THE HUMAN BRAIN
   Balaji A., 2020, IEEE T VLSI SYST, V28, P76, DOI 10.1109/TVLSI.2019.2951493
   Bekolay T, 2014, FRONT NEUROINFORM, V7, DOI 10.3389/fninf.2013.00048
   Beyeler M, 2015, IEEE IJCNN
   Brette R, 2007, J COMPUT NEUROSCI, V23, P349, DOI 10.1007/s10827-007-0038-6
   Caporale N, 2008, ANNU REV NEUROSCI, V31, P25, DOI 10.1146/annurev.neuro.31.060407.125639
   Cavallari S, 2014, FRONT NEURAL CIRCUIT, V8, DOI 10.3389/fncir.2014.00012
   Dauwels J, 2009, LECT NOTES COMPUT SC, V5506, P177, DOI 10.1007/978-3-642-02490-0_22
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Eppler Jochen Martin, 2008, Front Neuroinform, V2, P12, DOI 10.3389/neuro.11.012.2008
   Goodman Dan, 2008, Front Neuroinform, V2, P5, DOI 10.3389/neuro.11.005.2008
   Hansel D, 1998, NEURAL COMPUT, V10, P467, DOI 10.1162/089976698300017845
   Hazan H, 2018, FRONT NEUROINFORM, V12, DOI 10.3389/fninf.2018.00089
   Herculano-Houzel S, 2012, P NATL ACAD SCI USA, V109, P10661, DOI 10.1073/pnas.1201895109
   Hines Michael L, 2009, Front Neuroinform, V3, P1, DOI 10.3389/neuro.11.001.2009
   Hines ML, 1997, NEURAL COMPUT, V9, P1179, DOI 10.1162/neco.1997.9.6.1179
   Hines ML, 2001, NEUROSCIENTIST, V7, P123, DOI 10.1177/107385840100700207
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Jeong S, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21030941
   Koch C., 1998, METHODS NEURONAL MOD, DOI [10.1109/MCISE.1999.743629, DOI 10.1109/MCISE.1999.743629]
   Kreuz T, 2007, J NEUROSCI METH, V165, P151, DOI 10.1016/j.jneumeth.2007.05.031
   Lobov SA, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20020500
   Losh M, 2019, ELECTRONICS-SWITZ, V8, DOI 10.3390/electronics8121479
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Masquelier T, 2018, FRONT COMPUT NEUROSC, V12, DOI 10.3389/fncom.2018.00074
   Masquelier T, 2009, NEURAL COMPUT, V21, P1259, DOI 10.1162/neco.2008.06-08-804
   Mattia M, 2000, NEURAL COMPUT, V12, P2305, DOI 10.1162/089976600300014953
   Mo LF, 2021, ELECTRONICS-SWITZ, V10, DOI 10.3390/electronics10172123
   Mozafari M, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00625
   Mozafari M, 2019, PATTERN RECOGN, V94, P87, DOI 10.1016/j.patcog.2019.05.015
   Naveros F, 2017, FRONT NEUROINFORM, V11, DOI 10.3389/fninf.2017.00007
   Pan ZH, 2020, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.01420
   Pei J, 2019, NATURE, V572, P106, DOI 10.1038/s41586-019-1424-8
   Qu LH, 2020, NEURAL COMPUT APPL, V32, P13479, DOI 10.1007/s00521-020-04755-4
   Schulz VH, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21030907
   Shen JC, 2016, SCI CHINA INFORM SCI, V59, DOI 10.1007/s11432-015-5511-7
   Stimberg M., 2018, BRIAN2GENN SYSTEM AC, DOI [10.1101/448050, DOI 10.1101/448050]
   Stimberg M, 2019, ELIFE, V8, DOI 10.7554/eLife.47314
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   TSODYKS M, 1993, PHYS REV LETT, V71, P1280, DOI 10.1103/PhysRevLett.71.1280
   Vogels TP, 2005, J NEUROSCI, V25, P10786, DOI 10.1523/JNEUROSCI.3508-05.2005
   Watts L., 1993, 6 NEUR INF P SYST C, P927
   Wong WK, 2012, INT J NEURAL SYST, V22, DOI 10.1142/S0129065712500177
   Zirkle J, 2020, FRONT COMPUT NEUROSC, V14, DOI 10.3389/fncom.2020.00052
NR 47
TC 1
Z9 1
U1 1
U2 14
PD SEP
PY 2021
VL 10
IS 18
AR 2281
DI 10.3390/electronics10182281
WC Computer Science, Information Systems; Engineering, Electrical &
   Electronic; Physics, Applied
DA 2023-11-11
ER

PT C
AU Xue, FZ
   Chen, XY
   Li, XM
AF Xue, Fangzheng
   Chen, Xuyang
   Li, Xiumin
BE Cong, F
   Leung, A
   Wei, Q
TI Real-Time Classification Through a Spiking Deep Belief Network with
   Intrinsic Plasticity
SO ADVANCES IN NEURAL NETWORKS, PT I
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 14th International Symposium on Neural Networks (ISNN)
CY JUN 21-26, 2017
CL JAPAN
DE DBNs; Spiking neural network; Intrinsic plasticity; Numeral recognition
ID NEURAL-NETWORKS; EXCITABILITY; RECOGNITION; NEURONS; NETS
AB Deep Belief Networks (DBNs) has made a good effect in machine learning and object classification. However, the current question is how to reduce the computational cost without detrimental to accuracy. To solve this problem, this paper is undertaken to convert the Siegert neuron into LIF neuron in DBNs and analyze the effects of changing the value of parameters for spiking neurons such as thresholds and firing rates. Besides, we also add intrinsic plasticity (IP) into the network to render better adaptive capability. Besides, the most exciting results is the spiking DBN with intrinsic plasticity submits its first correct guess of the output label within an average of 2.5 ms after the onset of the simulated Poisson spike train input with the initial firing rates beyond 200 Hz, and the recognition accuracy is still more than 94 percent.
C1 [Xue, Fangzheng; Chen, Xuyang; Li, Xiumin] Chongqing Univ, Key Lab Dependable Serv Comp Cyber Phys Soc, Minist Educ, Chongqing 400044, Peoples R China.
   [Xue, Fangzheng; Chen, Xuyang; Li, Xiumin] Chongqing Univ, Coll Automat, Chongqing 400044, Peoples R China.
RP Li, XM (corresponding author), Chongqing Univ, Key Lab Dependable Serv Comp Cyber Phys Soc, Minist Educ, Chongqing 400044, Peoples R China.; Li, XM (corresponding author), Chongqing Univ, Coll Automat, Chongqing 400044, Peoples R China.
EM xuefangzheng@cqu.edu.com; xmli@cqu.edu.cn
CR Andrew A. M, 2003, KYBERNETES, V32, P277
   [Anonymous], 2010, P 11 ANN C INT SPEEC
   Arel I, 2010, IEEE COMPUT INTELL M, V5, P13, DOI 10.1109/MCI.2010.938364
   Battenberg E, 2012, ANAL DRUM PATTERNS U
   Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Ciresan DC, 2010, NEURAL COMPUT, V22, P3207, DOI 10.1162/NECO_a_00052
   Dahl GE, 2011, INT CONF ACOUST SPEE, P4688
   Daoudal G, 2003, LEARN MEMORY, V10, P456, DOI 10.1101/lm.64103
   Desai NS, 1999, NAT NEUROSCI, V2, P515, DOI 10.1038/9165
   Erhan D, 2010, J MACH LEARN RES, V11, P625
   Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647
   Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597
   Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527
   Jug F, SPIKING NETWORKS THE
   Kourrich S, 2015, NAT REV NEUROSCI, V16, P173, DOI 10.1038/nrn3877
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Li CG, 2016, IEEE ACCESS, V4, P119, DOI 10.1109/ACCESS.2015.2509005
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   SIEGERT AJF, 1951, PHYS REV, V81, P617, DOI 10.1103/PhysRev.81.617
   Triesch J, 2005, LECT NOTES COMPUT SC, V3696, P65, DOI 10.1007/11550822_11
   Wallisch P., 2014, MATLAB NEUROSCIENTIS
   Zhang W, 2003, NAT REV NEUROSCI, V4, P885, DOI 10.1038/nrn1248
NR 24
TC 0
Z9 0
U1 0
U2 5
PY 2017
VL 10261
BP 188
EP 196
DI 10.1007/978-3-319-59072-1_23
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods
DA 2023-11-11
ER

PT C
AU Muresan, DB
   Ciure, RD
   Ardelean, ER
   Moca, VV
   Muresan, RC
   Dînsoreanu, M
AF Muresan, Denisa Bianca
   Ciure, Raluca-Dana
   Ardelean, Eugen Richard
   Moca, Vasile Vlad
   Muresan, Raul Cristian
   Dinsoreanu, Mihaela
BE Nedevschi, S
   Potolea, R
   Slavescu, RR
TI Spike sorting using Superlets: Evaluation of a novel feature space for
   the discrimination of neuronal spikes
SO 2022 IEEE 18TH INTERNATIONAL CONFERENCE ON INTELLIGENT COMPUTER
   COMMUNICATION AND PROCESSING, ICCP
SE IEEE International Conference on Intelligent Computer Communication and
   Processing ICCP
DT Proceedings Paper
CT IEEE 18th International Conference on Intelligent Computer Communication
   and Processing (ICCP)
CY SEP 22-24, 2022
CL ELECTR NETWORK
DE spike sorting; Superlets Transform; data dimensionality reduction;
   neural networks
AB Because of the intricacy of neural data, spike sorting is a challenging problem in neuroscience. Overlapping clusters are one of the hardest issues to solve, along with the similarity of spike shapes, excessive noise, and unbalanced clusters. Here, we focus on analyzing the efficacy and integrating the Superlets Transform as a feature extraction method. Our results indicate that Superlets achieve not only super-resolution in time and frequency, but also provide the conditions for smooth clustering performance. The Superlet Transform was analyzed in conjunction with Principal Component Analysis, Singular Value Decomposition, and Isomap Embedding as dimensionality reduction methods. The Superlet-extracted features were classified using Neural Networks in order to assess their relevance.
C1 [Muresan, Denisa Bianca; Ciure, Raluca-Dana; Ardelean, Eugen Richard; Dinsoreanu, Mihaela] Tech Univ Cluj Napoca, Comp Sci, Cluj Napoca, Romania.
   [Moca, Vasile Vlad; Muresan, Raul Cristian] Transylvanian Inst Neurosci, Expt & Theoret Neurosci Lab, Cluj Napoca, Romania.
RP Muresan, DB (corresponding author), Tech Univ Cluj Napoca, Comp Sci, Cluj Napoca, Romania.
EM muresandenisa27@gmail.com; ralu.ciure@gmail.com;
   ardeleaneugenrichard@gmail.com; moca@tins.ro; muresan@tins.ro;
   Mihaela.Dinsoreanu@cs.utcluj.ro
CR Addison PS, 2005, PHYSIOL MEAS, V26, pR155, DOI 10.1088/0967-3334/26/5/R01
   Berry MW, 1995, SIAM REV, V37, P573, DOI 10.1137/1037127
   Jolliffe IT, 2016, PHILOS T R SOC A, V374, DOI 10.1098/rsta.2015.0202
   Kehtarnavaz N, 2008, DIGITAL SIGNAL PROCESSING SYSTEM DESIGN: LABVIEW-BASED HYBRID PROGRAMMING, 2ND EDITION, P175
   Liu Yanchi, 2010, 2010 IEEE INT C DATA, P911, DOI [DOI 10.1109/ICDM.2010.35, 10.1109/ICDM.2010.35]
   Moca V. V., 2019, BIORXIV
   Pedreira C, 2012, J NEUROSCI METH, V211, P58, DOI 10.1016/j.jneumeth.2012.07.010
   Quiroga RQ., 2007, SCHOLARPEDIA, V2, P3583
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Zha H., 2003, P 20 INT C MACHINE L
NR 10
TC 0
Z9 0
U1 0
U2 0
PY 2022
BP 229
EP 235
DI 10.1109/ICCP56966.2022.10053955
WC Computer Science, Artificial Intelligence; Computer Science,
   Interdisciplinary Applications; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Bensmail, C
   Steuber, V
   Davey, N
   Wróbel, B
AF Bensmail, Chama
   Steuber, Volker
   Davey, Neil
   Wrobel, Borys
BE Kurkova, V
   Manolopoulos, Y
   Hammer, B
   Iliadis, L
   Maglogiannis, I
TI Spiking Neural Network Controllers Evolved for Animat Foraging Based on
   Temporal Pattern Recognition in the Presence of Noise on Input
SO ARTIFICIAL NEURAL NETWORKS AND MACHINE LEARNING - ICANN 2018, PT I
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 27th International Conference on Artificial Neural Networks (ICANN)
CY OCT 04-07, 2018
CL Rhodes, GREECE
DE Spiking neural networks; Temporal pattern recognition; Animat; Adaptive
   exponential integrate and fire
ID MODEL; REPRESENTATION; DELAYS; TIME
AB We evolved spiking neural network controllers for simple animats, allowing for these networks to change topologies and weights during evolution. The animats' task was to discern one correct pattern (emitted from target objects) amongst other different wrong patterns (emitted from distractor objects), by navigating towards targets and avoiding distractors in a 2D world. Patterns were emitted with variable silences between signals of the same pattern in the attempt of creating a state memory. We analyse the network that is able to accomplish the task perfectly for patterns consisting of two signals, with 4 interneurons, maintaining its state (although not infinitely) thanks to the recurrent connections.
C1 [Bensmail, Chama; Wrobel, Borys] Adam Mickiewicz Univ, Evolving Syst Lab, Poznan, Poland.
   [Steuber, Volker; Davey, Neil] Univ Hertfordshire, Ctr Comp Sci & Informat Res, Hereford, England.
   [Wrobel, Borys] IOPAN, Sopot, Poland.
RP Wróbel, B (corresponding author), Adam Mickiewicz Univ, Evolving Syst Lab, Poznan, Poland.; Wróbel, B (corresponding author), IOPAN, Sopot, Poland.
EM chamabens@evosys.org; wrobel@evosys.org
CR Ahissar E, 2001, NEURON, V32, P185, DOI 10.1016/S0896-6273(01)00466-4
   [Anonymous], 2013, 2013 EUROPEAN C CIRC
   Bensmail C., 2017, 2017 IEEE S SER COMP, P1
   BIALEK W, 1991, SCIENCE, V252, P1854, DOI 10.1126/science.2063199
   Damper RI, 2003, LECT NOTES COMPUT SC, V2611, P616
   deCharms RC, 2000, ANNU REV NEUROSCI, V23, P613, DOI 10.1146/annurev.neuro.23.1.613
   Florian R. V., 2003, CONEURAL0303
   Gerstner W, 1996, NATURE, V383, P76, DOI 10.1038/383076a0
   Huxter J, 2003, NATURE, V425, P828, DOI 10.1038/nature02058
   Isaacson JS, 2010, CURR OPIN NEUROBIOL, V20, P328, DOI 10.1016/j.conb.2010.02.004
   Joris P, 2007, TRENDS NEUROSCI, V30, P70, DOI 10.1016/j.tins.2006.12.004
   Laurent G, 1996, TRENDS NEUROSCI, V19, P489, DOI 10.1016/S0166-2236(96)10054-0
   Maex R, 2009, NEURAL NETWORKS, V22, P1105, DOI 10.1016/j.neunet.2009.07.022
   Reeve R, 2005, ROBOT AUTON SYST, V51, P41, DOI 10.1016/j.robot.2004.08.010
   Rieke F., 1999, SPIKES EXPLORING NEU
   Steuber V, 2004, J COMPUT NEUROSCI, V17, P149, DOI 10.1023/B:JCNS.0000037678.26155.b5
   Steuber V, 2002, NEUROCOMPUTING, V44, P183, DOI 10.1016/S0925-2312(02)00388-0
   Steuber V, 1999, NEUROCOMPUTING, V26-7, P271, DOI 10.1016/S0925-2312(99)00021-1
   Steuber V, 2006, NETWORK-COMP NEURAL, V17, P173, DOI 10.1080/09548980500520328
   Thorpe S, 1996, NATURE, V381, P520, DOI 10.1038/381520a0
   Webb B, 1995, ROBOT AUTON SYST, V16, P117, DOI 10.1016/0921-8890(95)00044-5
   Wrobel B., 2014, LNICST, V134, P135, DOI DOI 10.1007/978-3-319-06944-9_10
NR 22
TC 0
Z9 0
U1 0
U2 1
PY 2018
VL 11139
BP 304
EP 313
DI 10.1007/978-3-030-01418-6_30
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT J
AU Moldakarimov, S
   Bazhenov, M
   Sejnowski, TJ
AF Moldakarimov, Samat
   Bazhenov, Maxim
   Sejnowski, Terrence J.
TI Feedback stabilizes propagation of synchronous spiking in cortical
   neural networks
SO PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES OF THE UNITED STATES OF
   AMERICA
DT Article
DE neural coding; attention; cerebral cortex; synfire; spike timing
ID GAMMA OSCILLATIONS; VISUAL-CORTEX; FIRING RATES; NEURONS; MODULATION;
   MECHANISMS; ATTENTION; CONNECTIONS; FEEDFORWARD; RELIABILITY
AB Precisely timed action potentials related to stimuli and behavior have been observed in the cerebral cortex. However, information carried by the precise spike timing has to propagate through many cortical areas, and noise could disrupt millisecond precision during the transmission. Previous studies have demonstrated that only strong stimuli that evoke a large number of spikes with small dispersion of spike times can propagate through multilayer networks without degrading the temporal precision. Here we show that feedback projections can increase the number of spikes in spike volleys without degrading their temporal precision. Feedback also increased the range of spike volleys that can propagate through multilayer networks. Our work suggests that feedback projections could be responsible for the reliable propagation of information encoded in spike times through cortex, and thus could serve as an attentional mechanism to regulate the flow of information in the cortex. Feedback projections may also participate in generating spike synchronization that is engaged in cognitive behaviors by the same mechanisms described here for spike propagation.
C1 [Moldakarimov, Samat; Bazhenov, Maxim; Sejnowski, Terrence J.] Salk Inst Biol Studies, Howard Hughes Med Inst, La Jolla, CA 92037 USA.
   [Moldakarimov, Samat; Sejnowski, Terrence J.] Univ Calif San Diego, Inst Neural Computat, La Jolla, CA 92093 USA.
   [Bazhenov, Maxim] Univ Calif Riverside, Dept Cell Biol & Neurosci, Riverside, CA 92521 USA.
   [Sejnowski, Terrence J.] Univ Calif San Diego, Div Biol Sci, La Jolla, CA 92093 USA.
RP Sejnowski, TJ (corresponding author), Salk Inst Biol Studies, Howard Hughes Med Inst, La Jolla, CA 92037 USA.
EM terry@salk.edu
CR ABELES M, 1993, J NEUROPHYSIOL, V70, P1629, DOI 10.1152/jn.1993.70.4.1629
   Bair W, 1996, NEURAL COMPUT, V8, P1185, DOI 10.1162/neco.1996.8.6.1185
   Baldauf D, 2014, SCIENCE, V344, P424, DOI 10.1126/science.1247003
   Bullier J., 2005, 23 PROBLEMS SYSTEMS, P103
   Buzsáki G, 2012, ANNU REV NEUROSCI, V35, P203, DOI 10.1146/annurev-neuro-062111-150444
   Callaway EM, 2004, NEURAL NETWORKS, V17, P625, DOI 10.1016/j.neunet.2004.04.004
   Dayan P., 2001, THEORETICAL NEUROSCI
   Diesmann M, 1999, NATURE, V402, P529, DOI 10.1038/990101
   Disney AA, 2007, NEURON, V56, P701, DOI 10.1016/j.neuron.2007.09.034
   Felleman DJ, 1991, CEREB CORTEX, V1, P1, DOI 10.1093/cercor/1.1.1
   Fries P, 2001, SCIENCE, V291, P1560, DOI 10.1126/science.1055465
   Haider B, 2006, J NEUROSCI, V26, P4535, DOI 10.1523/JNEUROSCI.5297-05.2006
   Haider B, 2010, NEURON, V65, P107, DOI 10.1016/j.neuron.2009.12.005
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Kumar A, 2008, J NEUROSCI, V28, P5268, DOI 10.1523/JNEUROSCI.2542-07.2008
   Kumar A, 2010, NAT REV NEUROSCI, V11, P615, DOI 10.1038/nrn2886
   Litvak V, 2003, J NEUROSCI, V23, P3006
   MAINEN ZF, 1995, SCIENCE, V268, P1503, DOI 10.1126/science.7770778
   MCCORMICK DA, 1992, J NEUROPHYSIOL, V68, P1384, DOI 10.1152/jn.1992.68.4.1384
   Mitchell JF, 2007, NEURON, V55, P131, DOI 10.1016/j.neuron.2007.06.018
   Rao RPN, 1999, NAT NEUROSCI, V2, P79, DOI 10.1038/4580
   Reyes AD, 2003, NAT NEUROSCI, V6, P593, DOI 10.1038/nn1056
   SALIN PA, 1995, PHYSIOL REV, V75, P107, DOI 10.1152/physrev.1995.75.1.107
   Salinas E, 2001, NAT REV NEUROSCI, V2, P539, DOI 10.1038/35086012
   Shadlen Michael N., 1994, Current Opinion in Neurobiology, V4, P569, DOI 10.1016/0959-4388(94)90059-0
   Tiesinga P, 2008, NAT REV NEUROSCI, V9, P97, DOI 10.1038/nrn2315
   van Rossum MCW, 2002, J NEUROSCI, V22, P1956, DOI 10.1523/JNEUROSCI.22-05-01956.2002
   Vogels TP, 2005, J NEUROSCI, V25, P10786, DOI 10.1523/JNEUROSCI.3508-05.2005
   Wang HP, 2010, SCIENCE, V328, P106, DOI 10.1126/science.1183108
   Yamamoto J, 2014, CELL, V157, P845, DOI 10.1016/j.cell.2014.04.009
   Zhang F, 2011, CELL, V147, P1446, DOI 10.1016/j.cell.2011.12.004
NR 31
TC 29
Z9 30
U1 0
U2 16
PD FEB 24
PY 2015
VL 112
IS 8
BP 2545
EP 2550
DI 10.1073/pnas.1500643112
WC Multidisciplinary Sciences
DA 2023-11-11
ER

PT C
AU Tanaka, C
   Ikeda, K
AF Tanaka, Chika
   Ikeda, Keiji
GP IEEE
TI Charge-based Neuromorphic Cell by InGaZnO Transistor and Implementation
   of Simple Scheme Spike-Timing-Dependent Plasticity
SO 2018 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS (ISCAS)
SE IEEE International Symposium on Circuits and Systems
DT Proceedings Paper
CT IEEE International Symposium on Circuits and Systems (ISCAS)
CY MAY 27-30, 2018
CL Florence, ITALY
DE Neuromorphic circuit; InGaZnO; Spiking neural network;
   Spike-timing-dependent plasticity
ID SYNAPSE
AB This paper presents a novel charge-based neuromorphic cell architecture composed of 1-capacitor and 1-transistor using extremely low off leakage (<10(-21)A/mu m) oxide-semiconductor transistors. Thanks to ultra-long retention characteristics, circuit implementation of spike-timing-dependent plasticity, for spiking neural network, were successfully demonstrated. Cell array architecture and it operation were confirmed by simulation with charging and discharging control. Asynchronous STDP implementation was also achieved without complicated forming of input pulse, and low-power operation less than similar to 0.1pJ can be expected.
C1 [Tanaka, Chika; Ikeda, Keiji] Toshiba Memory Corp, Future Memory Dev Dept, Device Technol Res & Dev Ctr, Saiwai Ku, 1 Komukai Toshiba Cho, Kawasaki, Kanagawa 2128582, Japan.
RP Tanaka, C (corresponding author), Toshiba Memory Corp, Future Memory Dev Dept, Device Technol Res & Dev Ctr, Saiwai Ku, 1 Komukai Toshiba Cho, Kawasaki, Kanagawa 2128582, Japan.
CR Ambrogio S, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00056
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Gao LG, 2015, NANOTECHNOLOGY, V26, DOI 10.1088/0957-4484/26/45/455204
   Gentet LJ, 2000, BIOPHYS J, V79, P314, DOI 10.1016/S0006-3495(00)76293-X
   Gerstner W., 2002, SPIKING NEURON MODEL
   Jerry Matthew, 2017, 2017 IEEE International Electron Devices Meeting (IEDM), P621, DOI 10.1109/IEDM.2017.8268338
   Jo SH, 2010, NANO LETT, V10, P1297, DOI 10.1021/nl904092h
   Kempter R, 1999, PHYS REV E, V59, P4498, DOI 10.1103/PhysRevE.59.4498
   Kuzum D, 2013, NANOTECHNOLOGY, V24, DOI 10.1088/0957-4484/24/38/382001
   Lashkare S, 2017, IEEE ELECTR DEVICE L, V38, P1212, DOI 10.1109/LED.2017.2723503
   Marukame T., 2017, 2017 INT C SOL STAT, P201
   Matsubayashi D., 2015, IEDM, DOI [10.1109/IEDM.2015.7409641, DOI 10.1109/IEDM.2015.7409641]
   Ohno T, 2011, NAT MATER, V10, P591, DOI [10.1038/NMAT3054, 10.1038/nmat3054]
   Park S., 2013, TECHNOL DIG INT ELEC, DOI DOI 10.1109/IEDM.2013.6724692
   Sangkil Kim, 2015, 2015 IEEE MTT-S International Microwave Symposium (IMS2015), P1, DOI 10.1109/MWSYM.2015.7166723
   Wang ZQ, 2015, FRONT NEUROSCI-SWITZ, V8, DOI 10.3389/fnins.2014.00438
NR 16
TC 0
Z9 0
U1 0
U2 1
PY 2018
DI 10.1109/ISCAS.2018.8350932
WC Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Oh, S
   Kim, CH
   Lee, S
   Park, BG
   Lee, JH
AF Oh, Seongbin
   Kim, Chul-Heung
   Lee, Soochang
   Park, Byung-Gook
   Lee, Jong-Ho
TI Grayscale Image Recognition Using Spike-Rate-Based Online Learning and
   Threshold Adjustment of Neurons in a Thin-Film Transistor-Type NOR Flash
   Memory Array
SO JOURNAL OF NANOSCIENCE AND NANOTECHNOLOGY
DT Article
DE Neuromorphic System; Spiking Neural Network (SNN); Spike-Rate-Dependent
   Plasticity (SRDP); Supervised Learning; Pattern Recognition; Adaptive
   Spiking Neural Network
ID NEURAL-NETWORK
AB As a synaptic device, TFT-type NOR flash memory cell shows reasonable weight levels (50 levels for long-term potentiation (LTP) and 150 levels for long-term depression (LTD)) and large max/min ratio (not equal(sic)50) for synapse weight. Based on the measurement results of the synapse cell, supervised learning process is simulated using software MATLAB. A new pulse scheme is designed for mimicking spike-rate-dependent plasticity (SRDP) algorithm. Through learning and inferencing phase, our (784x100) network achieved 74.08% accuracy on the MNIST benchmark. A new method for adapting the threshold voltage of output neurons for firing is also proposed. This additional adjustment helps to eliminate the exclusive or dormant output neurons by setting the threshold voltage to an appropriate value proportional to the average weight of synapses connected to each neuron. As a result, accuracy increases to 82.54% in the (784x100) network and to 84.14% in the (784x200) network. Moreover, threshold adjustment helped the network to classify completely overlapped patterns in succession.
C1 [Oh, Seongbin; Kim, Chul-Heung; Lee, Soochang; Park, Byung-Gook; Lee, Jong-Ho] Seoul Natl Univ, Dept Elect & Comp Engn, Seoul 08826, South Korea.
RP Lee, JH (corresponding author), Seoul Natl Univ, Dept Elect & Comp Engn, Seoul 08826, South Korea.
CR BACKUS J, 1978, COMMUN ACM, V21, P613, DOI 10.1145/359576.359579
   Bear MF, 1996, P NATL ACAD SCI USA, V93, P13453, DOI 10.1073/pnas.93.24.13453
   BIENENSTOCK EL, 1982, J NEUROSCI, V2, P32, DOI 10.1523/jneurosci.02-01-00032.1982
   Choi HS, 2018, IEEE T ELECTRON DEV, V65, P101, DOI 10.1109/TED.2017.2775233
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Garrido JA, 2016, INT J NEURAL SYST, V26, DOI 10.1142/S0129065716500209
   Kim CH, 2018, IEEE T ELECTRON DEV, V65, P1774, DOI 10.1109/TED.2018.2817266
   Kim H, 2017, NANOTECHNOLOGY, V28, DOI 10.1088/1361-6528/aa86f8
   Kim S, 2015, ACS APPL MATER INTER, V7, P25479, DOI 10.1021/acsami.5b08541
   Kim T, 2017, P 2017 INT TECHN C C
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Milo V, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351824
   Milo V, 2018, IEEE T VLSI SYST, V26, P2806, DOI 10.1109/TVLSI.2018.2818978
   Querlioz D, 2013, IEEE T NANOTECHNOL, V12, P288, DOI 10.1109/TNANO.2013.2250995
   Querlioz D, 2011, 2011 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN), P1775, DOI 10.1109/IJCNN.2011.6033439
   Sidler S, 2017, LECT NOTES COMPUT SC, V10613, P281, DOI 10.1007/978-3-319-68600-4_33
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Srinivasan G, 2016, SCI REP-UK, V6, DOI 10.1038/srep29545
   Stoliar P, 2017, INT EL DEVICES MEET
   Wang XJ, 2017, INT C INTEL HUM MACH, P167, DOI 10.1109/IHMSC.2017.44
   Wang ZR, 2017, NAT MATER, V16, P101, DOI [10.1038/nmat4756, 10.1038/NMAT4756]
NR 22
TC 2
Z9 2
U1 0
U2 44
PD OCT
PY 2019
VL 19
IS 10
BP 6055
EP 6060
DI 10.1166/jnn.2019.16995
WC Chemistry, Multidisciplinary; Nanoscience & Nanotechnology; Materials
   Science, Multidisciplinary; Physics, Applied; Physics, Condensed Matter
DA 2023-11-11
ER

PT C
AU Palumbo, F
   Sau, C
   Pani, D
   Meloni, P
   Raffo, L
AF Palumbo, Francesca
   Sau, Carlo
   Pani, Danilo
   Meloni, Paolo
   Raffo, Luigi
GP IEEE
TI Feasibility study of real-time Spiking Neural Network simulations on a
   Swarm Intelligence based digital architecture
SO 2017 IEEE INTERNATIONAL PARALLEL AND DISTRIBUTED PROCESSING SYMPOSIUM
   WORKSHOPS (IPDPSW)
SE IEEE International Symposium on Parallel and Distributed Processing
   Workshops
DT Proceedings Paper
CT 31st IEEE International Parallel and Distributed Processing Symposium
   Workshops (IPDPS)
CY MAY 29-JUN 02, 2017
CL Orlando, FL
ID PROJECT
AB Nature has proved to be a source of inspiration for engineering solutions. Spiking Neural Networks are exemplary from this perspective, due to the possibility to exploit them not only to simulate the biological networks of neurons but also to effectively work as classifiers and artificial intelligence systems. Another interesting nature-inspired paradigm is Swarm Intelligence, mainly applied to optimization problems and robotics, but also used to create digital architectures for array processing, with self-organization and fault-tolerance features.
   The aim of this paper is to evaluate if nature-inspired Swarm Intelligence based architectures can be effectively used to simulate biological neuronal assemblies through spiking neural networks models. A preliminary assessment of the proposed approach on low-end FPGA devices reveals near real-time capabilities with fully-connected neural networks composed of 64 Izhikevich neurons. A prospective analysis on larger platforms reveals the scalability of the proposed approach.
C1 [Palumbo, Francesca] Univ Sassari, PolComIng Informat Engn Unit, Sassari, Italy.
   [Sau, Carlo; Pani, Danilo; Meloni, Paolo; Raffo, Luigi] Univ Cagliari, DIEE Microelect & Bioengn Lab, Cagliari, Italy.
RP Palumbo, F (corresponding author), Univ Sassari, PolComIng Informat Engn Unit, Sassari, Italy.
EM fpalumbo@uniss.it; carlo.sau@diee.unica.it
CR [Anonymous], 1999, SWARM INTELL-US
   Bonifazi P., 2013, 6 INT IEEE EMBS C NE, P0519
   Cheung K, 2016, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00516
   Florimbi G, 2016, MICROPROCESS MICROSY, V47, P303, DOI 10.1016/j.micpro.2016.05.015
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   MAASS W, 1997, T SOC COMPUT SIMUL I, V10, P1659
   Pani D, 2006, J PARALLEL DISTR COM, V66, P1014, DOI 10.1016/j.jpdc.2005.11.001
   Pani D, 2015, ACM T AUTON ADAP SYS, V10, DOI 10.1145/2746346
   Pani D, 2010, STUD COMPUT INTELL, V269, P91
   Pani D, 2010, PROCEEDINGS OF THE 2010 COMPUTING FRONTIERS CONFERENCE (CF 2010), P327, DOI 10.1145/1787275.1787343
   Resnick M., 1997, TURTLES TERMITES TRA
   Sau C., 2012, P 2012 C DES ARCH SI, P1
   Thomas DB, 2009, ANN IEEE SYM FIELD P, P45, DOI 10.1109/FCCM.2009.46
NR 14
TC 0
Z9 0
U1 0
U2 3
PY 2017
BP 247
EP 250
DI 10.1109/IPDPSW.2017.121
WC Computer Science, Hardware & Architecture; Computer Science, Theory &
   Methods
DA 2023-11-11
ER

PT J
AU Howard, D
   Bull, L
   Lanzi, PL
AF Howard, David
   Bull, Larry
   Lanzi, Pier-Luca
TI A Cognitive Architecture Based on a Learning Classifier System with
   Spiking Classifiers
SO NEURAL PROCESSING LETTERS
DT Article
DE Learning classifier systems; Spiking neural networks; Self-adaptation;
   Semi-MDP
ID PLASTICITY; DISCRETE; AGENTS; XCS
AB Learning classifier systems (LCS) are population-based reinforcement learners that were originally designed to model various cognitive phenomena. This paper presents an explicitly cognitive LCS by using spiking neural networks as classifiers, providing each classifier with a measure of temporal dynamism. We employ a constructivist model of growth of both neurons and synaptic connections, which permits a genetic algorithm to automatically evolve sufficiently-complex neural structures. The spiking classifiers are coupled with a temporally-sensitive reinforcement learning algorithm, which allows the system to perform temporal state decomposition by appropriately rewarding "macro-actions", created by chaining together multiple atomic actions. The combination of temporal reinforcement learning and neural information processing is shown to outperform benchmark neural classifier systems, and successfully solve a robotic navigation task.
C1 [Howard, David] Queensland Ctr Adv Technol, Autonomous Syst Program, Pullenvale, Australia.
   [Bull, Larry] Univ West England, Fac Environm & Technol, Bristol, Avon, England.
   [Lanzi, Pier-Luca] Politecn Milan, Dipartimento Elettron & Informaz, Milan, Italy.
RP Howard, D (corresponding author), Queensland Ctr Adv Technol, Autonomous Syst Program, Pullenvale, Australia.
EM david.howard@csiro.au
CR [Anonymous], GEN EV COMP C GECCO
   [Anonymous], 2009, P GENETIC EVOLUTIONA
   [Anonymous], GEN EV COMP C GECCO
   BEER RD, 1995, ADAPT BEHAV, V3, P469, DOI 10.1177/105971239500300405
   Bonarini A, 1998, P IEEE WORLD C COMP, P51
   Boyan J. A., 1995, Advances in Neural Information Processing Systems 7, P369
   Bull L., 2002, Parallel Problem Solving from Nature - PPSN VII. 7th International Conference. Proceedings (Lecture Notes in Computer Science Vol.2439), P558
   Bull L, 2003, IEEE C EVOL COMPUTAT, P991
   Butz MV, 2006, GECCO 2006: GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, VOL 1 AND 2, P1457
   Buzsaki G, 2011, RHYTHMS BRAIN, DOI DOI 10.1093/ACPROF:OSO/9780195301069.001.0001
   Cazangi RR, 2003, IEEE C EVOL COMPUTAT, P574, DOI 10.1109/CEC.2003.1299627
   Churchill AW, 2014, EVOL INTELL, V7, P169, DOI 10.1007/s12065-014-0121-7
   Donnart JY, 1996, IEEE T SYST MAN CY B, V26, P381, DOI 10.1109/3477.499790
   DORIGO M, 1994, ARTIF INTELL, V71, P321, DOI 10.1016/0004-3702(94)90047-7
   Fausser S, 2015, NEURAL PROCESS LETT, V41, P55, DOI 10.1007/s11063-013-9334-5
   Fernando C, 2011, J THEOR BIOL, V275, P29, DOI 10.1016/j.jtbi.2011.01.009
   Fernando C, 2010, NEURAL COMPUT, V22, P2809, DOI 10.1162/NECO_a_00031
   Floreano D., 2001, LNCS, P38
   Floreano D, 2002, P 8 INT C ART LIF
   Gerstner W., 2002, SPIKING NEURON MODEL
   Hagras H, 2002, INFORM SCIENCES, V145, P1, DOI 10.1016/S0020-0255(02)00221-9
   He PG, 2007, IEEE T SYST MAN CY B, V37, P425, DOI 10.1109/TSMCB.2006.883869
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   HOLLAND J, 1976, PROGR THEORETICAL BI
   Holland J. H., 1992, ADAPTATION NATURAL A
   Holland J.H., 1978, OVERVIEW PATTERN DIR, P313
   Howard G., 2010, P IEEE C EV COMP JUL, P1
   Howard GD, 2008, P 10 ANN C COMP GEN, P1977
   Hurst J., 2002, Parallel Problem Solving from Nature - PPSN VII. 7th International Conference. Proceedings (Lecture Notes in Computer Science Vol.2439), P588
   Hurst J, 2006, ARTIF LIFE, V12, P353, DOI 10.1162/artl.2006.12.3.353
   Kistler WM, 2002, BIOL CYBERN, V87, P416, DOI 10.1007/s00422-002-0359-5
   Lanzi PL, 2006, GECCO 2006: GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, VOL 1 AND 2, P1497
   Lanzi PL, 2006, IEEE C EVOL COMPUTAT, P2255
   Lanzi PL, 2005, IEEE C EVOL COMPUTAT, P2032
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Michel O., 2004, International Journal of Advanced Robotic Systems, V1, P39
   Moioli RC, 2007, LECT NOTES COMPUTER, V4998, P286
   Pipe A. G., 2002, Parallel Problem Solving from Nature - PPSN VII. 7th International Conference. Proceedings (Lecture Notes in Computer Science Vol.2439), P578
   Preen RJ, 2014, SOFT COMPUT, V18, P153, DOI 10.1007/s00500-013-1044-4
   Quartz SR, 1997, BEHAV BRAIN SCI, V20, P537
   Rechenberg I., 1973, EVOLUTIONSSTRATEGIE
   Rumelhart David E., 1986, PARALLEL DISTRIBUTED, V1, DOI 10.1016/b978-1-4832-1446-7.50035-2
   Rumelhart G.E., 1986, PARALLEL DISTRIBUTED, P318, DOI DOI 10.1016/B978-1-4832-1446-7.50035-2
   Schultz W, 1998, J NEUROPHYSIOL, V80, P1, DOI 10.1152/jn.1998.80.1.1
   Shouval HZ, 2011, J COMPUT NEUROSCI, V30, P489, DOI 10.1007/s10827-010-0273-0
   Stolzmann W., 1999, P 2 INT WORKSH LEARN, P290
   Studley M, 2005, IEEE C EVOL COMPUTAT, P2099
   Sutton RS, 1996, ADV NEUR IN, V8, P1038
   Sutton RS, 1999, ARTIF INTELL, V112, P181, DOI 10.1016/S0004-3702(99)00052-1
   Watkins C.J.C.H., 1989, THESIS U CAMBRIDGE
   Webb A, 2003, LECT NOTES ARTIF INT, V2801, P885
   Wilson S. W., 2001, Advances in Learning Classifier Systems. Third International Workshop, IWLCS 2000. Revised Papers (Lecture Notes in Artificial Intelligence Vol.1996), P158
   Wilson Stewart W, 2000, LEARNING CLASSIFIER, P209, DOI [DOI 10.1007/3-540-45027-011, 10.1007/3-540-45027-0_11, DOI 10.1007/3-540-45027-0_11]
NR 53
TC 4
Z9 4
U1 1
U2 12
PD AUG
PY 2016
VL 44
IS 1
SI SI
BP 125
EP 147
DI 10.1007/s11063-015-9451-4
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT C
AU Sheik, S
   Paul, S
   Augustine, C
   Kothapalli, C
   Khellah, MM
   Cauwenberghs, G
   Neftci, E
AF Sheik, Sadique
   Paul, Somnath
   Augustine, Charles
   Kothapalli, Chinnikrishna
   Khellah, Muhammad M.
   Cauwenberghs, Gert
   Neftci, Emre
GP IEEE
TI Synaptic Sampling in Hardware Spiking Neural Networks
SO 2016 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS (ISCAS)
SE IEEE International Symposium on Circuits and Systems
DT Proceedings Paper
CT IEEE International Symposium on Circuits and Systems (ISCAS)
CY MAY 22-25, 2016
CL Montreal, CANADA
AB Using a neural sampling approach, networks of stochastic spiking neurons, interconnected with plastic synapses, have been used to construct computational machines such as Restricted Boltzmann Machines (RBMs). Previous work towards building such networks achieved lower performances than traditional RBMs. More recently, Synaptic Sampling Machines (SSMs) were shown to outperform equivalent RBMs. In Synaptic Sampling Machines (SSMs), the stochasticity for the sampling is generated at the synapse. Stochastic synapses play the dual role of a regularizer during learning and an efficient mechanism for implementing stochasticity in neural networks over a wide dynamic range. In this paper we show that SSMs with stochastic synapses implemented in FPGA-based spiking neural networks can obtain a high accuracy in classifying MNIST handwritten digit database. We compare classification accuracy for different bit precision for stochastic and non-stochastic synapses and further argue that stochastic synapses have the same effect as synapses with higher bit precision but require significantly lower computational resources.
C1 [Sheik, Sadique; Paul, Somnath; Augustine, Charles; Kothapalli, Chinnikrishna; Khellah, Muhammad M.] Intel Corp, Hillsboro, OR 97124 USA.
   [Sheik, Sadique; Cauwenberghs, Gert] Univ Calif San Diego, Dept Bioengn, La Jolla, CA 92093 USA.
   [Neftci, Emre] UC Irvine, Dept Cognit Sci, Irvine, CA USA.
RP Sheik, S (corresponding author), Intel Corp, Hillsboro, OR 97124 USA.; Sheik, S (corresponding author), Univ Calif San Diego, Dept Bioengn, La Jolla, CA 92093 USA.
EM ssheik@mail.ucsd.edu
CR [Anonymous], CUST INT CIRC C CICC
   [Anonymous], UNSUPERVISED LEARNIN
   [Anonymous], 2013, INT C MACH LEARN ICM
   [Anonymous], 2013, FRONTIERS NEUROSCIEN
   Burges, 2010, MNIST HANDWRITTEN DI, DOI DOI 10.1561/2400000035
   Cauwenberghs G, 1996, IEEE T NEURAL NETWOR, V7, P346, DOI 10.1109/72.485671
   FATT P, 1952, J PHYSIOL-LONDON, V117, P109
   Fiser J, 2010, TRENDS COGN SCI, V14, P119, DOI 10.1016/j.tics.2010.01.003
NR 8
TC 6
Z9 6
U1 1
U2 2
PY 2016
BP 2090
EP 2093
WC Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Al-Jamali, NAS
   Al-Raweshidy, HS
AF Al-Jamali, Nadia Adnan Shiltagh
   Al-Raweshidy, Hamed S.
TI Intelligent Traffic Management and Load Balance Based on Spike ISDN-IoT
SO IEEE SYSTEMS JOURNAL
DT Article
DE Sensors; Quality of service; Wireless sensor networks; Training; Deep
   learning; Routing; Software; Cluster head; partial recurrent spike
   neural network (PRSNN); software defined network-IoT (SDN-IoT); traffic
   load prediction; quality of service (QoS)
AB An intelligent software defined network (ISDN) based on an intelligent controller can manage and control the network in a remarkable way. In this article, a methodology is proposed to estimate the packet flow at the sensing plane in the software defined network-Internet of Things based on a partial recurrent spike neural network (PRSNN) congestion controller, to predict the next step ahead of packet flow and thus, reduce the congestion that may occur. That is, the proposed model (spike ISDN-IoT) is enhanced with a congestion controller. This controller works as a proactive controller in the proposed model. In addition, we propose another intelligent clustering controller based on an artificial neural network, which operates as a reactive controller, to manage the clustering in the sensing area of the spike ISDN-IoT. Hence, an intelligent queuing model is introduced to manage the flow table buffer capacity of the spike ISDN-IoT network, such that the quality of service (QoS) of the whole network is improved. A modified training algorithm is introduced to train the PRSNN to adjust its weight and threshold. The simulation results demonstrate that the QoS is improved by (14.36%) when using the proposed model as compared with a convolutional neural network.
C1 [Al-Jamali, Nadia Adnan Shiltagh] Univ Baghdad, Dept Comp Engn, Baghdad 10071, Iraq.
   [Al-Jamali, Nadia Adnan Shiltagh; Al-Raweshidy, Hamed S.] Brunel Univ London, Dept Elect & Comp Engn, London UB8 3PH, England.
RP Al-Jamali, NAS (corresponding author), Univ Baghdad, Dept Comp Engn, Baghdad 10071, Iraq.
EM nadiaadnanshiltagh.aljamli@brunel.ac.uk; hamed.al-raweshidy@brunel.ac.uk
CR Al-Jamali NAS, 2020, IEEE ACCESS, V8, P61246, DOI 10.1109/ACCESS.2020.2984311
   Al-Shammari BKJ, 2018, IEEE INTERNET THINGS, V5, P352, DOI 10.1109/JIOT.2017.2785219
   [Anonymous], 2018, 2018 INT JOINT C NEU
   Bera S, 2018, IEEE SYST J, V12, P2074, DOI 10.1109/JSYST.2016.2615761
   Ghosh A, 2018, CAAI T INTELL TECHNO, V3, P208, DOI 10.1049/trit.2018.1008
   Han FY, 2018, INT C ADV MECH SYST, P359, DOI 10.1109/ICAMechS.2018.8507102
   Han ZJ, 2019, IEEE ACCESS, V7, P31688, DOI 10.1109/ACCESS.2019.2900445
   Haque I, 2019, IEEE T VEH TECHNOL, V68, P1866, DOI 10.1109/TVT.2018.2888622
   Huang XH, 2018, IEEE NETWORK, V32, P35, DOI 10.1109/MNET.2018.1800097
   Kaur D, 2018, IEEE T KNOWL DATA EN, V30, P1985, DOI 10.1109/TKDE.2018.2809747
   Kumar N, 2018, IEEE SENS J, V18, P9449, DOI 10.1109/JSEN.2018.2869629
   Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508
   Li G, 2019, IEEE ACCESS, V7, p39 612
   Li Q, 2019, IEEE T NETW SERV MAN, V16, P153, DOI 10.1109/TNSM.2018.2890754
   Lin FPC, 2020, IEEE SYST J, V14, P265, DOI 10.1109/JSYST.2019.2894689
   Mao B, 2019, IEEE T EMERG TOPICS
   Mao BM, 2018, IEEE WIREL COMMUN, V25, P74, DOI 10.1109/MWC.2018.1700417
   Dias GM, 2019, IEEE CONSUM ELECTR M, V8, P55, DOI 10.1109/MCE.2018.2868110
   Matsuda S, 2016, IEEE IJCNN, P293, DOI 10.1109/IJCNN.2016.7727211
   Misra S, 2018, IEEE SYST J, V12, P2353, DOI 10.1109/JSYST.2017.2774284
   Mostafa Hesham, 2018, IEEE Trans Neural Netw Learn Syst, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Rego A, 2018, IEEE ACCESS, V6, P31580, DOI 10.1109/ACCESS.2018.2842034
   Son J, 2019, IEEE T SUST COMPUT, V4, P17, DOI 10.1109/TSUSC.2018.2842074
   Taherkhani A, 2018, IEEE T NEUR NET LEAR, V29, P5394, DOI 10.1109/TNNLS.2018.2797801
   Tang FX, 2018, IEEE INTERNET THINGS, V5, P5141, DOI 10.1109/JIOT.2018.2838574
   Tang FX, 2018, IEEE COMMUN MAG, V56, P80, DOI 10.1109/MCOM.2018.1701227
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Thupae R, 2018, IEEE IND ELEC, P4645, DOI 10.1109/IECON.2018.8591178
   Xie JF, 2019, IEEE COMMUN SURV TUT, V21, P393, DOI 10.1109/COMST.2018.2866942
   Xu Y, 2019, IEEE J SEL AREA COMM, V37, P515, DOI 10.1109/JSAC.2019.2894237
   Yu CH, 2018, IEEE ACCESS, V6, P64533, DOI 10.1109/ACCESS.2018.2877686
NR 31
TC 5
Z9 5
U1 0
U2 7
PD JUN
PY 2021
VL 15
IS 2
BP 1640
EP 1651
DI 10.1109/JSYST.2020.2996185
WC Computer Science, Information Systems; Engineering, Electrical &
   Electronic; Operations Research & Management Science; Telecommunications
DA 2023-11-11
ER

PT C
AU Hsu, CH
   Cheng, YH
   Li, ZF
   Huang, PL
   Tang, KT
AF Hsu, Chen-Han
   Cheng, Yu-Hsiang
   Li, Zhaofang
   Huang, Ping-Li
   Tang, Kea-Tiong
GP IEEE
TI A 62.45 TOPS/W Spike-Based Convolution Neural Network Accelerator with
   Spatiotemporal Parallel Data Flow and Sparsity Mechanism
SO 2022 IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE CIRCUITS
   AND SYSTEMS (AICAS 2022): INTELLIGENT TECHNOLOGY IN THE POST-PANDEMIC
   ERA
DT Proceedings Paper
CT IEEE International Conference on Artificial Intelligence Circuits and
   Systems (AICAS) - Intelligent Technology in the Post-Pandemic Era
CY JUN 13-15, 2022
CL Incheon, SOUTH KOREA
DE Spiking neural network; accelerator; sparsity; energy efficiency; area
   efficiency
ID CHIP
AB Convolutional neural networks (CNNs) have been widely used for image recognition and classification in recent years. Low energy consumption is crucial in the circuit design of edge devices, and data reuse is one method of reducing energy consumption. In addition, spiking neural networks (SNNs) are receiving increasing attention due to their low power use. However, the temporal characteristic of SNNs causes repeated data access at different time steps, leading to high energy consumption. In this paper, a spiked-based CNN accelerator that can support various inference time steps and models is proposed. Spatiotemporal parallel data flows are employed to reuse data from different time steps and, convolution operations are used to reduce energy consumption. Furthermore, the accelerator is designed for high sparsity and event driven SNNs. The synthesis achieves power efficiency of 62.45 TOPS/NV and area efficiency of 7.58 TOPS/kmm(2).
C1 [Hsu, Chen-Han; Cheng, Yu-Hsiang; Li, Zhaofang; Huang, Ping-Li; Tang, Kea-Tiong] Natl Tsing Hua Univ, Hsinchu, Taiwan.
RP Hsu, CH (corresponding author), Natl Tsing Hua Univ, Hsinchu, Taiwan.
CR Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   Chiu SW, 2014, IEEE T BIOMED CIRC S, V8, P765, DOI 10.1109/TBCAS.2014.2377754
   Chuang P.-Y., 2020 57 ACMIEEE DESI
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Hong-Han Lien, 2021 IEEE INT S CIRC
   Hsieh HY, 2013, IEEE T NEUR NET LEAR, V24, P2063, DOI 10.1109/TNNLS.2013.2271644
   Hsieh HY, 2012, IEEE T NEUR NET LEAR, V23, P1065, DOI 10.1109/TNNLS.2012.2195329
   Lee JJ, 2020, PR IEEE COMP DESIGN, P57, DOI 10.1109/ICCD50377.2020.00027
   Narayanan S, 2020, ANN I S COM, P349, DOI 10.1109/ISCA45697.2020.00038
   Pan CH, 2013, SENSORS-BASEL, V13, P193, DOI 10.3390/s130100193
   Yeh Zuo-Wei, 2021 INT S CIRCUITS
NR 11
TC 0
Z9 0
U1 0
U2 1
PY 2022
BP 182
EP 185
DI 10.1109/AICAS54282.2022.9870018
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Wang, RC
   Hamilton, TJ
   Tapson, J
   van Schaik, A
AF Wang, Runchun
   Hamilton, Tara Julia
   Tapson, Jonathan
   van Schaik, Andre
GP IEEE
TI An FPGA design framework for large-scale spiking neural networks
SO 2014 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS (ISCAS)
SE IEEE International Symposium on Circuits and Systems
DT Proceedings Paper
CT IEEE International Symposium on Circuits and Systems (ISCAS)
CY JUN 01-05, 2014
CL Melbourne, AUSTRALIA
AB We present an FPGA design framework for large-scale spiking neural networks, particularly the ones with a high-density of connections or all-to-all connections. The proposed FPGA design framework is based on a reconfigurable neural layer, which is implemented using a time-multiplexing approach to achieve up to 200,000 virtual neurons with one physical neuron using only a fraction of the hardware resources in commercial-off-the-shelf FPGAs (even entry level ones). Rather than using a mathematical computational model, the physical neuron was efficiently implemented with a conductance-based model, of which the parameters were randomised between neurons to emulate the variance in biological neurons. Besides these building blocks, the proposed time-multiplexed reconfigurable neural layer has an address buffer, which will generate a fixed random weight for each connection on the fly for incoming spikes. This structure effectively reduces the usage of memory. After presenting the architecture of the proposed neural layer, we present a network with 23 proposed neural layers, each containing 64k neurons, yielding 1.5 M neurons and 92 G synapses with a total spike throughput of 1.2T spikes/s, while running in real-time on a Virtex 6 FPGA.
C1 [Wang, Runchun; Hamilton, Tara Julia; Tapson, Jonathan; van Schaik, Andre] Univ Western Sydney, MARCS Inst, Sydney, NSW, Australia.
RP Wang, RC (corresponding author), Univ Western Sydney, MARCS Inst, Sydney, NSW, Australia.
EM mark.wang@uws.edu.au
CR [Anonymous], 2011, 45 ANN C INFORM SCI
   Boahen KA, 2000, IEEE T CIRCUITS-II, V47, P416, DOI 10.1109/82.842110
   Cheung Kit, 2012, Artificial Neural Networks and Machine Learning - ICANN 2012. Proceedings of the 22nd International Conference on Artificial Neural Networks, P113, DOI 10.1007/978-3-642-33269-2_15
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Maguire LP, 2007, NEUROCOMPUTING, V71, P13, DOI 10.1016/j.neucom.2006.11.029
   Moore SW, 2012, ANN IEEE SYM FIELD P, P133, DOI 10.1109/FCCM.2012.32
   Wang RC, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00014
NR 8
TC 26
Z9 27
U1 0
U2 4
PY 2014
BP 457
EP 460
WC Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU McDonnell, MD
   Goldwyn, JH
   Lindner, B
AF McDonnell, Mark D.
   Goldwyn, Joshua H.
   Lindner, Benjamin
TI Editorial: Neuronal Stochastic Variability: Influences on Spiking
   Dynamics and Network Activity
SO FRONTIERS IN COMPUTATIONAL NEUROSCIENCE
DT Editorial Material
DE balanced network; channel noise; heterogeneity; Hodgkin-Huxley model;
   neural networks; neuronal variability; stochastic dynamics
C1 [McDonnell, Mark D.] Univ S Australia, Computat & Theoret Neurosci Lab, Inst Telecommun Res, Mawson Lakes, SA, Australia.
   [Goldwyn, Joshua H.] Ohio State Univ, Dept Math, 231 W 18th Ave, Columbus, OH 43210 USA.
   [Lindner, Benjamin] Bernstein Ctr Computat Neurosci, Theory Complex Syst & Neurophys, Berlin, Germany.
   [Lindner, Benjamin] Humboldt Univ, Dept Phys, Invalidenstr 110, Berlin, Germany.
RP McDonnell, MD (corresponding author), Univ S Australia, Computat & Theoret Neurosci Lab, Inst Telecommun Res, Mawson Lakes, SA, Australia.
EM mark.mcdonnell@unisa.edu.au
CR Baxendale PH, 2011, J MATH BIOL, V63, P433, DOI 10.1007/s00285-010-0376-2
   Goldwyn JH, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1002247
   Rowat PF, 2011, NEURAL COMPUT, V23, P3094, DOI 10.1162/NECO_a_00209
NR 3
TC 8
Z9 8
U1 0
U2 4
PD APR 21
PY 2016
VL 10
AR 38
DI 10.3389/fncom.2016.00038
WC Mathematical & Computational Biology; Neurosciences
DA 2023-11-11
ER

PT J
AU Miró-Amarante, L
   Gómez-Rodríguez, F
   Jiménez-Fernández, A
   Jiménez-Moreno, G
AF Miro-Amarante, L.
   Gomez-Rodriguez, F.
   Jimenez-Fernandez, A.
   Jimenez-Moreno, G.
TI A spiking neural network for real-time Spanish vowel phonemes
   recognition
SO NEUROCOMPUTING
DT Article
DE Neuromorphic engineering; Address event representation (AER);
   Event-based processing; FPGA; Digital cochlea; Speech recognition
ID BASILAR-MEMBRANE; MODEL; HARDWARE; COCHLEA
AB This paper explores neuromorphic approach capabilities applied to real-time speech processing. A spiking recognition neural network composed of three types of neurons is proposed. These neurons are based on an integrative and fire model and are capable of recognizing auditory frequency patterns, such as vowel phonemes; words are recognized as sequences of vowel phonemes. For demonstrating real-time operation, a complete spiking recognition neural network has been described in VHDL for detecting certain Spanish words, and it has been tested in a FPGA platform. This is a stand-alone and fully hardware system that allows to embed it in a mobile system. To stimulate the network, a spiking digital-filter-based cochlea has been implemented in VHDL. In the implementation, an Address Event Representation (AER) is used for transmitting information between neurons.
C1 [Miro-Amarante, L.; Gomez-Rodriguez, F.; Jimenez-Fernandez, A.; Jimenez-Moreno, G.] Univ Seville, Robot & Technol Comp Lab, ETS Ingn Informat, Av Reina Mercedes S-N, Seville, Spain.
RP Miró-Amarante, L (corresponding author), Univ Seville, Robot & Technol Comp Lab, ETS Ingn Informat, Av Reina Mercedes S-N, Seville, Spain.
EM lmiro@us.es
CR A. Corporation 1, 2010, CYCL 4 DEV DAT, V3, P1
   Abdollahi M, 2011, BIOMED CIRC SYST C, P269, DOI 10.1109/BioCAS.2011.6107779
   [Anonymous], ADV NEURAL INF PROCE
   Berner R., 2007, P IEEE INT S CIRC SY
   Cachóu A, 2015, NEUROCOMPUTING, V148, P187, DOI 10.1016/j.neucom.2012.11.059
   Cauwenberghs G., 1998, NEUROMORPHIC SYST EN
   Eliasmith C, 2003, COMPUTATION REPRESEN
   Gomez-Rodriguez F., 2005, 2 HARDWARE IMPLEMENT, P534
   Hussain S, 2014, NEUROCOMPUTING, V138, P14, DOI 10.1016/j.neucom.2013.09.052
   Indiveri G, 2009, ENCY NEUROSCIENCE, P521
   Kock C, 1999, BIOPHYSICS COMPUTATI
   Li CH, 2012, IEEE INT SYMP CIRC S, P1159
   Linares-Barrancoa A, 2007, NEUROCOMPUTING, V70, P2692, DOI 10.1016/j.neucom.2006.07.020
   Liu SC, 2015, EVENT-BASED NEUROMORPHIC SYSTEMS, P1, DOI 10.1002/9781118927601
   Liu SC, 2010, IEEE INT SYMP CIRC S, P505, DOI 10.1109/ISCAS.2010.5537588
   Liu SC, 2010, IEEE INT SYMP CIRC S, P2027, DOI 10.1109/ISCAS.2010.5537164
   Liu SC, 2010, CURR OPIN NEUROBIOL, V20, P288, DOI 10.1016/j.conb.2010.03.007
   Maguire LP, 2007, NEUROCOMPUTING, V71, P13, DOI 10.1016/j.neucom.2006.11.029
   Marrero Aguiar Victoria, 2008, ESTUDIOS FONETICA EX, V17, P207
   Martinez-Celdran E., 1995, ESTUD FONETICA EXP, V7, P196
   Mathworks, 2012, FILT DES HDL COD
   MEAD C, 1990, P IEEE, V78, P1629, DOI 10.1109/5.58356
   Microelectronics W., 2009, WM8731 WM8731L DAT
   Misra J, 2010, NEUROCOMPUTING, V74, P239, DOI 10.1016/j.neucom.2010.03.021
   Pérez-Peña F, 2015, NEUROCOMPUTING, V149, P496, DOI 10.1016/j.neucom.2014.08.024
   Romero J., 1988, ESTUDIOS FONETICA EX, V3, P181
   ROSENBLATT F, 1958, PSYCHOL REV, V65, P386, DOI 10.1037/h0042519
   Ruggero M A, 1992, Curr Opin Neurobiol, V2, P449, DOI 10.1016/0959-4388(92)90179-O
   RUGGERO MA, 1992, J NEUROPHYSIOL, V68, P1087, DOI 10.1152/jn.1992.68.4.1087
   Sivilotti M., 1991, WIRING CONSIDERATION
   Von Bekesy G., 1960, EXPT HEARING, VVol. 195
   Yu Q, 2014, NEUROCOMPUTING, V138, P3, DOI 10.1016/j.neucom.2013.06.052
   Yu T, 2009, IEEE INT SYMP CIRC S, P109, DOI 10.1109/ISCAS.2009.5117697
   ZWICKER E, 1961, J ACOUST SOC AM, V33, P248, DOI 10.1121/1.1908630
NR 34
TC 8
Z9 8
U1 0
U2 21
PD FEB 22
PY 2017
VL 226
BP 249
EP 261
DI 10.1016/j.neucom.2016.12.005
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT J
AU Wang, W
   Hao, SY
   Wei, YC
   Xia, ST
   Feng, JS
   Sebe, N
AF Wang, Wei
   Hao, Siyuan
   Wei, Yunchao
   Xia, Shengtao
   Feng, Jiashi
   Sebe, Nicu
TI Temporal Spiking Recurrent Neural Network for Action Recognition
SO IEEE ACCESS
DT Article
DE Action recognition; temporal spiking; recurrent neural network
ID HISTOGRAMS
AB In this paper, we propose a novel temporal spiking recurrent neural network (TSRNN) to perform robust action recognition in videos. The proposed TSRNN employs a novel spiking architecture which utilizes the local discriminative features from high-confidence reliable frames as spiking signals. The conventional CNN-RNNs typically used for this problem treat all the frames equally important such that they are error-prone to noisy frames. The TSRNN solves this problem by employing a temporal pooling architecture which can help RNN select sparse and reliable frames and enhances its capability in modelling long-range temporal information. Besides, a message passing bridge is added between the spiking signals and the recurrent unit. In this way, the spiking signals can guide RNN to correct its long-term memory across multiple frames from contamination caused by noisy frames with distracting factors (e.g., occlusion, rapid scene transition). With these two novel components, TSRNN achieves competitive performance compared with the state-of-the-art CNN-RNN architectures on two large scale public benchmarks, UCF101 and HMDB51.
C1 [Wang, Wei] Ecole Polytech Fed Lausanne, Comp Vis Lab, CH-1015 Lausanne, Switzerland.
   [Hao, Siyuan] Qingdao Univ Technol, Informat & Control Engn Coll, Qingdao 266520, Shandong, Peoples R China.
   [Wei, Yunchao] Univ Illinois, Beckman Inst, Urbana, IL 61820 USA.
   [Xia, Shengtao; Feng, Jiashi] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 259776, Singapore.
   [Sebe, Nicu] Univ Trento, Dept Informat Engn & Comp Sci, I-38123 Trento, Italy.
RP Hao, SY (corresponding author), Qingdao Univ Technol, Informat & Control Engn Coll, Qingdao 266520, Shandong, Peoples R China.
EM lemonbananan@163.com
CR [Anonymous], 2005, PROC CVPR IEEE
   Cai ZW, 2014, PROC CVPR IEEE, P596, DOI 10.1109/CVPR.2014.83
   Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502
   Chen M, 2011, INT CONF CLOUD COMPU, P316, DOI 10.1109/CCIS.2011.6045082
   Cho K, 2014, 2014 7TH CONFERENCE ON CONTROL AND AUTOMATION (CA), P6, DOI 10.1109/CA.2014.9
   Dalal N, 2006, LECT NOTES COMPUT SC, V3952, P428, DOI 10.1007/11744047_33
   Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848
   Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878
   Duta IC, 2017, PROC CVPR IEEE, P3205, DOI 10.1109/CVPR.2017.341
   Entwistle K, 2016, WOOD MATER SCI ENG, V11, P1, DOI 10.1080/17480272.2014.929176
   Farnebäck G, 2003, LECT NOTES COMPUT SC, V2749, P363, DOI 10.1007/3-540-45103-x_50
   Feichtenhofer C, 2017, PROC CVPR IEEE, P7445, DOI 10.1109/CVPR.2017.787
   Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213
   Girdhar R., 2017, PROC CVPR IEEE, P971, DOI DOI 10.1109/CVPR.2017.337
   Graves A, 2012, STUD COMPUT INTELL, V385, P1, DOI [10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]
   Greff K., 2016, ARXIV150304069, V28, P2222, DOI DOI 10.1109/TNNLS.2016.2582924
   Ioffe S., 2015, ICML, DOI DOI 10.1007/S13398-014-0173-7.2
   Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7
   Li MK, 2016, PROCEEDINGS 2016 IEEE INTERNATIONAL CONFERENCE ON SERVICE OPERATIONS AND LOGISTICS, AND INFORMATICS (SOLI), P200, DOI 10.1109/SOLI.2016.7551687
   Li ZY, 2018, COMPUT VIS IMAGE UND, V166, P41, DOI 10.1016/j.cviu.2017.10.011
   Liu L., 2018, ARXIV180204441
   Liu L, 2020, INT J COMPUT VISION, V128, P261, DOI 10.1007/s11263-019-01247-4
   Liu L, 2019, INT J COMPUT VISION, V127, P74, DOI 10.1007/s11263-018-1125-z
   Liu L, 2017, PATTERN RECOGN, V62, P135, DOI 10.1016/j.patcog.2016.08.032
   Ng JYH, 2015, PROC CVPR IEEE, P4694, DOI 10.1109/CVPR.2015.7299101
   Peng XJ, 2016, COMPUT VIS IMAGE UND, V150, P109, DOI 10.1016/j.cviu.2016.03.013
   Rueckert E, 2016, SCI REP-UK, V6, DOI 10.1038/srep21142
   Scovanner P., 2007, INT C MULTIMEDIA, P357, DOI [DOI 10.1145/1291233.1291311, 10.1145/1291233.1291311]
   Simonyan K., 2014, ADV NEURAL INFORM PR, P568, DOI DOI 10.1002/14651858.CD001941.PUB3
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Song SJ, 2017, AAAI CONF ARTIF INTE, P4263
   Soomro K., 2012, ARXIV12120402
   Wang GL, 2017, 2017 IEEE 2ND ADVANCED INFORMATION TECHNOLOGY, ELECTRONIC AND AUTOMATION CONTROL CONFERENCE (IAEAC), P114, DOI 10.1109/IAEAC.2017.8053988
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang LM, 2015, PROC CVPR IEEE, P4305, DOI 10.1109/CVPR.2015.7299059
   Wang PC, 2016, MM'16: PROCEEDINGS OF THE 2016 ACM MULTIMEDIA CONFERENCE, P97, DOI 10.1145/2964284.2967191
   Wang X., 2017, PROC CVPR IEEE, P2097, DOI [DOI 10.1109/CVPR, DOI 10.1109/CVPR.2017.369]
   Zach C, 2007, LECT NOTES COMPUT SC, V4713, P214, DOI 10.1007/978-3-540-74936-3_22
   Zhu G., 2006, P 14 ACM INT C MULT, P431, DOI DOI 10.24963/IJCAI.2018/227
   Zhu WH, 2016, PROC INT CONF ANTI, P1, DOI 10.1109/ICASID.2016.7873885
NR 42
TC 12
Z9 12
U1 3
U2 13
PY 2019
VL 7
BP 117165
EP 117175
DI 10.1109/ACCESS.2019.2936604
WC Computer Science, Information Systems; Engineering, Electrical &
   Electronic; Telecommunications
DA 2023-11-11
ER

PT J
AU Natarajan, R
   Huys, QJM
   Dayan, P
   Zemel, RS
AF Natarajan, Rama
   Huys, Quentin J. M.
   Dayan, Peter
   Zemel, Richard S.
TI Encoding and decoding spikes for dynamic stimuli
SO NEURAL COMPUTATION
DT Article
AB Naturally occurring sensory stimuli are dynamic. In this letter, we consider how spiking neural populations might transmit information about continuous dynamic stimulus variables. The combination of simple encoders and temporal stimulus correlations leads to a code in which information is not readily available to downstream neurons. Here, we explore a complex encoder that is paired with a simple decoder that allows representation and manipulation of the dynamic information in neural systems. The encoder we present takes the form of a biologically plausible recurrent spiking neural network where the output population recodes its inputs to produce spikes that are independently decodeable. We show that this network can be learned in a supervised manner by a simple local learning rule.
C1 [Natarajan, Rama; Zemel, Richard S.] Univ Toronto, Dept Comp Sci, Toronto, ON M5S 3G4, Canada.
   [Huys, Quentin J. M.; Dayan, Peter] UCL, Gatsby Computat Neurosci Unit, London WC1N 3AR, England.
RP Natarajan, R (corresponding author), Univ Toronto, Dept Comp Sci, Toronto, ON M5S 3G4, Canada.
EM rama@cs.toronto.edu; qhuys@gatsby.ucl.ac.uk; dayan@gatsby.ucl-ac-uk;
   zemel@cs.toronto.edu
CR ACKLEY DH, 1985, COGNITIVE SCI, V9, P147
   [Anonymous], 2005, INFORM THEORY INFERE
   Barbieri R, 2004, NEURAL COMPUT, V16, P277, DOI 10.1162/089976604322742038
   Baxter J, 2001, J ARTIF INTELL RES, V15, P319, DOI 10.1613/jair.806
   Brockwell AE, 2004, J NEUROPHYSIOL, V91, P1899, DOI 10.1152/jn.00438.2003
   Brown EN, 1998, J NEUROSCI, V18, P7411
   Cover T. M., 2006, ELEMENTS INFORM THEO, V2nd
   Dean I, 2005, NAT NEUROSCI, V8, P1684, DOI 10.1038/nn1541
   GAO Y, 2002, ADV NEURAL INFORM PR, V14
   HINTON GE, 2000, ADV NEURAL INFORM PR, V12
   Huys QJM, 2007, NEURAL COMPUT, V19, P404, DOI 10.1162/neco.2007.19.2.404
   Kistler WM, 1997, NEURAL COMPUT, V9, P1015, DOI 10.1162/neco.1997.9.5.1015
   Körding KP, 2004, NATURE, V427, P244, DOI 10.1038/nature02169
   Nirenberg S, 2001, NATURE, V411, P698, DOI 10.1038/35079612
   Paninski L, 2003, NETWORK-COMP NEURAL, V14, P437, DOI 10.1088/0954-898X/14/3/304
   Pola G, 2003, NETWORK-COMP NEURAL, V14, P35, DOI 10.1088/0954-898X/14/1/303
   Pouget A, 2003, ANNU REV NEUROSCI, V26, P381, DOI 10.1146/annurev.neuro.26.041002.131112
   Pouget A, 1998, NEURAL COMPUT, V10, P373, DOI 10.1162/089976698300017809
   Pouget A, 1999, NEURAL COMPUT, V11, P85, DOI 10.1162/089976699300016818
   SEUNG HS, 1993, P NATL ACAD SCI USA, V90, P10749, DOI 10.1073/pnas.90.22.10749
   Smith E, 2005, NEURAL COMPUT, V17, P19, DOI 10.1162/0899766052530839
   Twum-Danso N, 2001, NEURAL NETWORKS, V14, P835, DOI 10.1016/S0893-6080(01)00079-X
   Vickers NJ, 2001, NATURE, V410, P466, DOI 10.1038/35068559
   Wu S, 2001, NEURAL COMPUT, V13, P775, DOI 10.1162/089976601300014349
   Wu W, 2004, IEEE T BIO-MED ENG, V51, P933, DOI 10.1109/TBME.2004.826666
   Zemel R, 2005, ADV NEURAL INFORM PR, V17, P1609
   Zemel RS, 1998, NEURAL COMPUT, V10, P403, DOI 10.1162/089976698300017818
   Zhang KC, 1999, NEURAL COMPUT, V11, P75, DOI 10.1162/089976699300016809
NR 28
TC 19
Z9 20
U1 0
U2 6
PD SEP
PY 2008
VL 20
IS 9
BP 2325
EP 2360
DI 10.1162/neco.2008.01-07-436
WC Computer Science, Artificial Intelligence; Neurosciences
DA 2023-11-11
ER

PT J
AU Zhang, TL
   Cheng, X
   Jia, SC
   Poo, MM
   Zeng, Y
   Xu, B
AF Zhang, Tielin
   Cheng, Xiang
   Jia, Shuncheng
   Poo, Mu-Ming
   Zeng, Yi
   Xu, Bo
TI Self-backpropagation of synaptic modifications elevates the efficiency
   of spiking and artificial neural networks
SO SCIENCE ADVANCES
DT Article
ID LONG-TERM POTENTIATION; PROPAGATION; NEURONS; MEMORY; MODEL
AB Many synaptic plasticity rules found in natural circuits have not been incorporated into artificial neural networks (ANNs). We showed that incorporating a nonlocal feature of synaptic plasticity found in natural neural networks, whereby synaptic modification at output synapses of a neuron backpropagates to its input synapses made by upstream neurons, markedly reduced the computational cost without affecting the accuracy of spiking neural networks (SNNs) and ANNs in supervised learning for three benchmark tasks. For SNNs, synaptic modification at output neurons generated by spike timing-dependent plasticity was allowed to self-propagate to limited upstream synapses. For ANNs, modified synaptic weights via conventional backpropagation algorithm at output neurons self-backpropagated to limited upstream synapses. Such self-propagating plasticity may produce coordinated synaptic modifications across neuronal layers that reduce computational cost.
C1 [Zhang, Tielin; Cheng, Xiang; Jia, Shuncheng; Zeng, Yi; Xu, Bo] Chinese Acad Sci, Res Ctr Brain Inspired Intelligence, Inst Automat, Beijing 100190, Peoples R China.
   [Zhang, Tielin; Cheng, Xiang; Jia, Shuncheng; Poo, Mu-Ming; Zeng, Yi; Xu, Bo] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.
   [Poo, Mu-Ming] Chinese Acad Sci, Inst Neurosci, State Key Lab Neurosci, Shanghai 200031, Peoples R China.
   [Poo, Mu-Ming; Zeng, Yi; Xu, Bo] Chinese Acad Sci, Ctr Excellence Brain Sci & Intelligence Technol, Shanghai 200031, Peoples R China.
   [Poo, Mu-Ming] Shanghai Ctr Brain Sci & Brain Inspired Intellige, Shanghai 201210, Peoples R China.
RP Xu, B (corresponding author), Chinese Acad Sci, Res Ctr Brain Inspired Intelligence, Inst Automat, Beijing 100190, Peoples R China.; Xu, B (corresponding author), Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100049, Peoples R China.; Xu, B (corresponding author), Chinese Acad Sci, Ctr Excellence Brain Sci & Intelligence Technol, Shanghai 200031, Peoples R China.
EM xubo@ia.ac.cn
CR Abbott LF, 2016, NAT NEUROSCI, V19, P350, DOI 10.1038/nn.4241
   Amir A, 2017, PROC CVPR IEEE, P7388, DOI 10.1109/CVPR.2017.781
   Bellec G, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-17236-y
   Bengio Y, 2017, NEURAL COMPUT, V29, P555, DOI 10.1162/NECO_a_00934
   Bi GQ, 2001, ANNU REV NEUROSCI, V24, P139, DOI 10.1146/annurev.neuro.24.1.139
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   BLISS TVP, 1993, NATURE, V361, P31, DOI 10.1038/361031a0
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   CRICK F, 1989, NATURE, V337, P129, DOI 10.1038/337129a0
   Dan Y, 2004, NEURON, V44, P23, DOI 10.1016/j.neuron.2004.09.007
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Du JL, 2009, P NATL ACAD SCI USA, V106, P18890, DOI 10.1073/pnas.0910659106
   Du JL, 2004, NATURE, V429, P878, DOI 10.1038/nature02618
   Fitzsimonds RM, 1997, NATURE, V388, P439, DOI 10.1038/41267
   Froemke RC, 2002, NATURE, V416, P433, DOI 10.1038/416433a
   HINTON GE, 1995, SCIENCE, V268, P1158, DOI 10.1126/science.7761831
   HOKFELT T, 1984, SCIENCE, V225, P1326, DOI 10.1126/science.6147896
   Huh D, 2018, ADV NEUR IN, V31
   ITO M, 1989, ANNU REV NEUROSCI, V12, P85, DOI 10.1146/annurev.neuro.12.1.85
   Jaeger H, 2016, NATURE, V538, P467, DOI 10.1038/nature19477
   Jia SC, 2021, FRONT NEUROSCI-SWITZ, V15, DOI 10.3389/fnins.2021.654786
   Kim R, 2019, P NATL ACAD SCI USA, V116, P22811, DOI 10.1073/pnas.1905926116
   Kobayashi K, 2004, NEURON, V41, P445, DOI 10.1016/S0896-6273(03)00873-0
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   LeCun Yann, 1998, MNIST DATABASE HANDW
   Lee C, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00119
   Lillicrap TP, 2020, NAT REV NEUROSCI, V21, P335, DOI 10.1038/s41583-020-0277-3
   Lillicrap TP, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms13276
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Meulemans Alexander, 2020, ADV NEURAL INFORM PR, V33
   Muller SZ, 2019, CELL, V179, P1382, DOI 10.1016/j.cell.2019.10.020
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Nicola W, 2017, NAT COMMUN, V8, DOI 10.1038/s41467-017-01827-3
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Scellier B, 2017, FRONT COMPUT NEUROSC, V11, DOI 10.3389/fncom.2017.00024
   Sejnowski T. J., 1987, Complex Systems, V1, P145
   Siegelmann H, 2018, 2018 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2018.8489673
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Tao HZW, 2000, J NEUROSCI, V20, P3233, DOI 10.1523/JNEUROSCI.20-09-03233.2000
   TEYLER TJ, 1987, ANNU REV NEUROSCI, V10, P131, DOI 10.1146/annurev.ne.10.030187.001023
   Tsodyks M, 1998, NEURAL COMPUT, V10, P821, DOI 10.1162/089976698300017502
   Wozniak S, 2020, NAT MACH INTELL, V2, P325, DOI 10.1038/s42256-020-0187-0
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Zeng GX, 2019, NAT MACH INTELL, V1, P364, DOI 10.1038/s42256-019-0080-x
   Zenke F, 2015, NAT COMMUN, V6, DOI 10.1038/ncomms7922
   Zhang TL, 2022, IEEE T NEUR NET LEAR, V33, P7621, DOI 10.1109/TNNLS.2021.3085966
   Zhang TL, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1653
   Zhang TL, 2018, AAAI CONF ARTIF INTE, P620
   ZUCKER RS, 1989, ANNU REV NEUROSCI, V12, P13, DOI 10.1146/annurev.ne.12.030189.000305
NR 49
TC 17
Z9 17
U1 9
U2 41
PD OCT
PY 2021
VL 7
IS 43
AR eabh0146
DI 10.1126/sciadv.abh0146
WC Multidisciplinary Sciences
DA 2023-11-11
ER

PT C
AU Wu, QX
   McGinnity, TM
   Maguire, L
   Cai, JY
   Valderrama-Gonzalez, GD
AF Wu, QingXiang
   McGinnity, T. M.
   Maguire, Liam
   Cai, Jianyong
   Valderrama-Gonzalez, G. D.
BE Huang, DS
   Wunsch, DC
   Levine, DS
   Jo, KH
TI Motion Detection Using Spiking Neural Network Model
SO ADVANCED INTELLIGENT COMPUTING THEORIES AND APPLICATIONS, PROCEEDINGS:
   WITH ASPECTS OF ARTIFICIAL INTELLIGENCE
SE Lecture Notes in Computer Science
DT Proceedings Paper
CT 4th International Conference on Intelligent Computing
CY SEP 15-18, 2008
CL Shanghai, PEOPLES R CHINA
DE Motion detection; spiking neural networks; visual system
ID DELAYS
AB Inspired by the behaviour of the human visual system, a spiking neural network is proposed to detect moving objects in a visual image sequence. The structure and the properties of the network are detailed in this paper. Simulation results show that the network is able to perform motion detection for dynamic visual image sequence. Boundaries of moving objects are extracted from an active neuron group. Using the boundary, a moving object filter is created to take the moving objects from the grey image. The moving object images can be used to recognise moving objects. The moving tracks can be recorded for further analysis of behaviours of moving objects. It is promising to apply this approach to video processing domain and robotic visual systems.
C1 [Wu, QingXiang; McGinnity, T. M.; Maguire, Liam; Valderrama-Gonzalez, G. D.] Univ Ulster, Intelligent Syst Res Ctr, Magee Campus, Derry BT48 7JL, North Ireland.
   [Wu, QingXiang; Cai, Jianyong] Fujian Normal Univ, Sch Phys, Sch OptoElectron Technol, Fuzhou 350007, Peoples R China.
RP Wu, QX (corresponding author), Univ Ulster, Intelligent Syst Res Ctr, Magee Campus, Derry BT48 7JL, North Ireland.
EM q.wu@ulster.ac.uk; tm.mcginnity@ulster.ac.uk; lp.Maguire@ulster.ac.uk;
   cjy@fjnu.edu.cn; g.valderrama@ulster.ac.uk
CR Angelaki DE, 2004, NATURE, V430, P560, DOI 10.1038/nature02754
   [Anonymous], THESIS U HEIDELBERG
   CARR CE, 1988, P NATL ACAD SCI USA, V85, P8311, DOI 10.1073/pnas.85.21.8311
   CROOK SM, 1997, J COMPUTATIONAL NEUR, V4, P157
   Dayan P., 2001, THEORETICAL NEUROSCI
   Demb JB, 2007, NEURON, V55, P179, DOI 10.1016/j.neuron.2007.07.001
   Gerstner W., 2002, SPIKING NEURON MODEL
   JEFFRESS LA, 1949, AM J PSYCHOL, V62, P1, DOI 10.2307/1418702
   JEFFRESS LA, 1948, J COMP PHYSIOL PSYCH, V41, P35, DOI 10.1037/h0061495
   JEFFRESS LA, 1948, AM J PSYCHOL, V61, P468, DOI 10.2307/1418312
   Jessell T. M, 1981, PRINCIPLES NEURAL SC
   Kim IJ, 2008, NATURE, V452, P478, DOI 10.1038/nature06739
   Koch Christof, 1999, P1
   Lin JW, 2002, TRENDS NEUROSCI, V25, P449, DOI 10.1016/S0166-2236(02)02212-9
   Masland RH, 2001, NAT NEUROSCI, V4, P877, DOI 10.1038/nn0901-877
   Nelson R, 2003, VISUAL NEUROSCIENCES, P260
   Ölveczky BP, 2003, NATURE, V423, P401, DOI 10.1038/nature01652
   Peña JL, 2001, J NEUROSCI, V21, P9455, DOI 10.1523/JNEUROSCI.21-23-09455.2001
   Reppas JB, 1997, NATURE, V388, P175, DOI 10.1038/40633
   Senn W, 2002, NEURAL COMPUT, V14, P583, DOI 10.1162/089976602317250915
   Taylor WR, 2003, TRENDS NEUROSCI, V26, P379, DOI 10.1016/S0166-2236(03)00167-X
   Wässle H, 2004, NAT REV NEUROSCI, V5, P747, DOI 10.1038/nrn1497
   Wu QX, 2007, STUD COMPUT INTELL, V35, P171
   Wu QX, 2005, LECT NOTES COMPUT SC, V3610, P420
NR 24
TC 11
Z9 11
U1 0
U2 0
PY 2008
VL 5227
BP 76
EP +
WC Automation & Control Systems; Computer Science, Artificial Intelligence;
   Computer Science, Cybernetics; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Koohestan-Mahalian, F
   Cotter, NE
   Zamani, PT
   Rubart, AM
AF Koohestan-Mahalian, Fatemeh
   Cotter, Neil E.
   Zamani, Princess Tara
   Rubart, Audrey M.
TI A Framework for Analyzing, Designing, and Visualizing Spiking Neural
   Networks Part I: Linear Response Surfaces
SO IEEE ACCESS
DT Article
DE Neurons; Mathematical model; Integrated circuit modeling; Firing;
   Biological system modeling; Semiconductor device modeling; Biology;
   Center-of-mass; centroid; chaotic; linear; direct mapping; mathematical
   analysis; phase tracking; phase locking; post synaptic potential (PSP);
   recurrent; response surfaces (RSs); spiking neuron; spiking neural
   networks (SNNs); triangular; XOR gate
ID MODEL
AB In this first of two closely related papers, we set the foundation for a new framework, called Response Surfaces (RSs), to address fundamental problems of analyzing, designing, and visualizing spiking neurons and networks. An RS is a plot of the direct transfer function between input and output firing times of a spiking neuron and shows all the patterns of input spike times that fire a neuron at a given time. Thus, an RS is a graphical tool for visualizing, analyzing, and designing spiking neural networks. In this paper, we develop a linear RS framework based on triangular, post-synaptic-potential waveforms and apply it to the following problems: graphing the transfer function of a linear spiking neuron, designing an efficient spiking-XOR gate, analyzing phase tracking, and calculating spike times of arbitrarily large recurrent spiking networks. As another fundamental result of the linear RS framework, we show that the output firing time of a spiking neuron is equivalent to the center-of-mass of input spike times (acting as positions) and weights (acting as masses) plus a fixed delay. In the second paper, we explore the possibilities of more biologically realistic post-synaptic-potential waveforms and create a nonlinear RS framework as an extension of the linear RS framework presented here. Although our application examples in both papers focus on design and analysis, we touch on why the RS framework is helpful for understanding the effects of weight changes in learning algorithms.
C1 [Koohestan-Mahalian, Fatemeh; Cotter, Neil E.; Zamani, Princess Tara; Rubart, Audrey M.] Univ Utah, Elect & Comp Engn Dept, Salt Lake City, UT 84112 USA.
RP Koohestan-Mahalian, F (corresponding author), Univ Utah, Elect & Comp Engn Dept, Salt Lake City, UT 84112 USA.
EM fatima.mahalian@utah.edu
CR Abbott LF, 1999, BRAIN RES BULL, V50, P303, DOI 10.1016/S0361-9230(99)00161-6
   [Anonymous], 2001, HDB BIOL PHYS
   Badel L, 2008, BIOL CYBERN, V99, P361, DOI 10.1007/s00422-008-0259-4
   Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Bodyanskiy Yevgeniy, 2013, IJCCI 2013. 5th International Joint Conference on Computational Intelligence. Proceedings, P542
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Bressloff PC, 2000, NEURAL COMPUT, V12, P91, DOI 10.1162/089976600300015907
   Burkitt AN, 2006, BIOL CYBERN, V95, P1, DOI 10.1007/s00422-006-0068-6
   Cassidy AS, 2013, IEEE IJCNN
   Chandra S, 2017, CHAOS, V27, DOI 10.1063/1.4977514
   Chen SS, 2013, INT J BIFURCAT CHAOS, V23, DOI 10.1142/S0218127413500557
   Cheung K, 2016, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00516
   Comsa JM, 2020, INT CONF ACOUST SPEE, P8529, DOI [10.1109/ICASSP40776.2020.9053856, 10.1109/icassp40776.2020.9053856]
   COTTER NE, 1992, IEEE T NEURAL NETWOR, V3, P308, DOI 10.1109/72.125872
   de Queiroz MS, 2006, NEUROCOMPUTING, V70, P14, DOI 10.1016/j.neucom.2006.07.002
   Diehl PU, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON REBOOTING COMPUTING (ICRC)
   Dumont G, 2017, PHYS REV E, V96, DOI 10.1103/PhysRevE.96.042311
   Esser SK, 2016, P NATL ACAD SCI USA, V113, P11441, DOI 10.1073/pnas.1604850113
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   Gerstner W, 1996, NEURAL COMPUT, V8, P1653, DOI 10.1162/neco.1996.8.8.1653
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   Hagras H, 2004, IEEE INT CONF ROBOT, P4620, DOI 10.1109/ROBOT.2004.1302446
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   HOPPENSTEADT FC, 1997, INTRO MATH NEURONS M
   Hsu J, 2014, IEEE SPECTRUM, V51, P17, DOI 10.1109/MSPEC.2014.6905473
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Jin X, 2010, COMPUT SCI ENG, V12, P91, DOI 10.1109/MCSE.2010.112
   Kasabov N, 2013, NEURAL NETWORKS, V41, P188, DOI 10.1016/j.neunet.2012.11.014
   KEENER JP, 1981, SIAM J APPL MATH, V41, P503, DOI 10.1137/0141042
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Lindner B, 2005, J THEOR BIOL, V232, P505, DOI 10.1016/j.jtbi.2004.08.030
   Lowet E, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0146443
   Meftah B, 2010, NEURAL PROCESS LETT, V32, P131, DOI 10.1007/s11063-010-9149-6
   Meier K, 2017, IEEE SPECTRUM, V54, P28, DOI 10.1109/MSPEC.2017.7934228
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Montbrió E, 2015, PHYS REV X, V5, DOI 10.1103/PhysRevX.5.021028
   Mustaqeem, 2020, IEEE ACCESS, V8, P79861, DOI 10.1109/ACCESS.2020.2990405
   Niknam K, 2017, I IEEE EMBS C NEUR E, P656, DOI 10.1109/NER.2017.8008436
   Nobukawa S, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-017-18783-z
   Nobukawa S, 2015, J ARTIF INTELL SOFT, V5, P109, DOI 10.1515/jaiscr-2015-0023
   Paugam-Moisy H, 2012, HDB NATURAL COMPUTIN, V1, P1
   Recce M, 1998, PULSED NEURAL NETWORKS, P111
   Siapas AG, 2005, NEURON, V46, P141, DOI 10.1016/j.neuron.2005.02.028
   Singer W, 2018, EUR J NEUROSCI, V48, P2389, DOI 10.1111/ejn.13796
   Srinivasan G, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00524
   Srivastava KH, 2017, P NATL ACAD SCI USA, V114, P1171, DOI 10.1073/pnas.1611734114
   Stuart GJ, 2001, NAT NEUROSCI, V4, P63, DOI 10.1038/82910
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Tavanaei A, 2017, LECT NOTES COMPUT SC, V10639, P899, DOI 10.1007/978-3-319-70136-3_95
   Tu Y, 2019, IEEE ACCESS, V7, P58113, DOI 10.1109/ACCESS.2019.2913945
   WILSON HR, 1972, BIOPHYS J, V12, P1, DOI 10.1016/S0006-3495(72)86068-5
   Zhang SQ, 2019, IEEE ACCESS, V7, P32297, DOI 10.1109/ACCESS.2019.2901521
NR 54
TC 0
Z9 0
U1 0
U2 5
PY 2021
VL 9
BP 8935
EP 8953
DI 10.1109/ACCESS.2020.3047993
WC Computer Science, Information Systems; Engineering, Electrical &
   Electronic; Telecommunications
DA 2023-11-11
ER

PT C
AU Eslaminia, M
   Le Beux, S
AF Eslaminia, Milad
   Le Beux, Sebastien
GP IEEE
TI Toward Large Scale All-Optical Spiking Neural Networks
SO PROCEEDINGS OF THE 2022 IFIP/IEEE 30TH INTERNATIONAL CONFERENCE ON VERY
   LARGE SCALE INTEGRATION (VLSI-SOC)
DT Proceedings Paper
CT 30th IFIP/IEEE International Conference on Very Large Scale Integration
   (VLSI-SoC)
CY OCT 03-05, 2022
CL Univ Patras, Patras, GREECE
HO Univ Patras
DE spiking neural network; silicon photonics; wavelength division
   multiplexing; microring resonators
AB Silicon Photonics is a promising technology to develop neuromorphic hardware accelerators. Most optical neural networks rely on wavelength division multiplexing (WDM), which calls for power hungry calibration to compensate for non-uniformity fabrication process and thermal variations of microring resonators (MRR). This imposes practical limits on neuromorphic photonic hardware since only a small number of synaptic connections per neuron can be implemented. As a result, the mapping of neural networks (NN) on a hardware platform require pruning of synaptic connections, which drastically affects the accuracy. In this work, we propose a method to efficiently map pre-trained NN on an all-optical spiking neural network (SNN), with the aim to optimize hardware utilization while minimizing accuracy loss. The method relies on weight partitioning and unrolling to reduce synaptic connections. The resulting neural networks are mapped on an architecture we propose, allowing to estimate accuracy and power consumption. Results show the capability of weight partitioning to implement a realistic NN while attaining 58% reduction in energy consumption compared with unrolling.
C1 [Eslaminia, Milad; Le Beux, Sebastien] Concordia Univ, Dept Elect & Comp Engn, Montreal, PQ, Canada.
RP Eslaminia, M (corresponding author), Concordia Univ, Dept Elect & Comp Engn, Montreal, PQ, Canada.
EM milad.eslaminia@concordia.ca; sebastien.lebeux@concordia.ca
CR Balaji A, 2021, IEEE EMBED SYST LETT, V13, P142, DOI 10.1109/LES.2020.3025873
   Carrillo SGC, 2019, APL MATER, V7, DOI 10.1063/1.5111840
   Cheng ZG, 2017, SCI ADV, V3, DOI 10.1126/sciadv.1700160
   Chollet F., 2015, KERAS
   de Lima TF, 2017, NANOPHOTONICS-BERLIN, V6, P577, DOI 10.1515/nanoph-2016-0139
   Deng L., 2012, IEEE SIGNAL PROC MAG, V29, P141, DOI [10.1109/MSP.2012.2211477, DOI 10.1109/MSP.2012.2211477]
   Feldmann J, 2019, NATURE, V569, P208, DOI 10.1038/s41586-019-1157-8
   Jayatilleka H, 2019, OPTICA, V6, P84, DOI 10.1364/OPTICA.6.000084
   Kang Y, 2019, IEEE T COMPUT AID D, V38, P2167, DOI 10.1109/TCAD.2018.2878167
   Kim C, 2020, ELECTRONICS-SWITZ, V9, DOI 10.3390/electronics9091526
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Nawrocki RA, 2016, IEEE T ELECTRON DEV, V63, P3819, DOI 10.1109/TED.2016.2598413
   Nengo, KER
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Shastri BJ, 2021, NAT PHOTONICS, V15, P102, DOI 10.1038/s41566-020-00754-y
   Shen YC, 2017, NAT PHOTONICS, V11, P441, DOI [10.1038/NPHOTON.2017.93, 10.1038/nphoton.2017.93]
   Thonnart Y, 2018, ISSCC DIG TECH PAP I, P350, DOI 10.1109/ISSCC.2018.8310328
   Zhang W, 2019, NAT REV MATER, V4, P150, DOI 10.1038/s41578-018-0076-x
NR 18
TC 1
Z9 1
U1 3
U2 4
PY 2022
DI 10.1109/VLSI-SoC54400.2022.9939647
WC Computer Science, Hardware & Architecture; Computer Science, Theory &
   Methods; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Zhigulin, VP
   Rabinovich, MI
AF Zhigulin, VP
   Rabinovich, MI
TI An important role of spike timing dependent synaptic plasticity in the
   formation of synchronized neural ensembles
SO NEUROCOMPUTING
DT Article
DE synchronization; spike timing-dependent plasticity; hippocampus
ID OSCILLATIONS; ENHANCEMENT
AB Synchronous neural activity plays an important role in the functioning of the brain. In this paper, we study the entrainment of a heterogeneous network of electrically coupled neurons by synaptically mediated periodic stimulation. We demonstrate by computer simulations that input synapses with spike timing-dependent plasticity (STDP) greatly enhance the coherence of spiking activity in the network as compared to the case of input with constant strength. We also show that synchronization in the network stimulated through STDP synapses is much more robust to the variability of network properties. The observed mechanism may play a role in synchronizing the activity of a hippocampal network. (C) 2004 Elsevier B.V. All rights reserved.
C1 CALTECH, Dept Phys, Pasadena, CA 91125 USA.
   Univ Calif San Diego, Inst Nonlinear Sci, La Jolla, CA 92093 USA.
RP Zhigulin, VP (corresponding author), CALTECH, Dept Phys, MC 103-33, Pasadena, CA 91125 USA.
EM zhigulin@caltech.edu; mrabinovich@ucsd.edu
CR Buhl DL, 2003, J NEUROSCI, V23, P1013
   Hormuzdi SG, 2001, NEURON, V31, P487, DOI 10.1016/S0896-6273(01)00387-7
   Miles R, 1991, NEURONAL NETWORKS HI, DOI 10.1017/cbo9780511895401
   Nowotny T, 2003, J NEUROSCI, V23, P9776
   Traub RD, 2003, P NATL ACAD SCI USA, V100, P1370, DOI 10.1073/pnas.0337529100
   Zhigulin VP, 2003, PHYS REV E, V67, DOI 10.1103/PhysRevE.67.021901
NR 6
TC 8
Z9 8
U1 0
U2 3
PD JUN
PY 2004
VL 58
BP 373
EP 378
DI 10.1016/j.neucom.2004.01.069
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT J
AU Hua, N
   He, XN
   Feng, JF
   Lu, WL
AF Hua, Ning
   He, Xiangnan
   Feng, Jianfeng
   Lu, Wenlian
TI Analytic Investigation for Synchronous Firing Patterns Propagation in
   Spiking Neural Networks
SO NEURAL PROCESSING LETTERS
DT Article
DE Spiking neural networks; Synfire chain; Gaussian random field; Mean
   field theory
ID DYNAMICS; COMPUTATION; MODEL
AB Based on the moment closure method and mean field theory, a Gaussian random field is constructed to quantitatively and analytically characterize the dynamics of a random point field. The approach provides a theoretical tool to investigate synchronized spike propagation in a feedforward or recurrent spiking neural network. We show that the balance between the excitation and inhibition postsynaptic potentials is required for the occurrence of synfire chains. In particular, with a balanced network, the critical packet size of invasion and annihilation is observed. We also derive a sufficient analytic condition for the synchronization propagation in an asynchronous environment, which further allows us to disclose the possibility of spatial synaptic structure to sustain a stable synfire chain. Our findings are in good agreement with simulations and help us understand the propagation of spatio-temporal patterns in a random point field.
C1 [Hua, Ning; Lu, Wenlian] Fudan Univ, Sch Math Sci, 220 Handan Rd, Shanghai, Peoples R China.
   [He, Xiangnan] Fudan Univ, Ctr Computat Syst Biol, Shanghai, Peoples R China.
   [Feng, Jianfeng] Fudan Univ, Inst Sci & Technol Brain Inspired Intelligence, Shanghai, Peoples R China.
   [Feng, Jianfeng] Fudan Univ, Key Lab Computat Neurosci & Brain Inspired Intell, Minist Educ, Shanghai, Peoples R China.
   [Lu, Wenlian] Fudan Univ, Shanghai Ctr Math Sci, Shanghai, Peoples R China.
   [Lu, Wenlian] Fudan Univ, Shanghai Key Lab Contemporary Appl Math, Shanghai, Peoples R China.
   [Lu, Wenlian] Fudan Univ, Key Lab Math Nonlinear Sci, Minist Educ, Shanghai, Peoples R China.
RP Lu, WL (corresponding author), Fudan Univ, Sch Math Sci, 220 Handan Rd, Shanghai, Peoples R China.; Lu, WL (corresponding author), Fudan Univ, Shanghai Ctr Math Sci, Shanghai, Peoples R China.; Lu, WL (corresponding author), Fudan Univ, Shanghai Key Lab Contemporary Appl Math, Shanghai, Peoples R China.; Lu, WL (corresponding author), Fudan Univ, Key Lab Math Nonlinear Sci, Minist Educ, Shanghai, Peoples R China.
EM wenlian@fudan.edu.cn
CR Adler R.J, 2009, RANDOM FIELDS GEOMET
   Adomian G., 1983, STOCHASTIC SYSTEMS
   Aertsen A, 1996, J PHYSIOLOGY-PARIS, V90, P243, DOI 10.1016/S0928-4257(97)81432-5
   Andreev AV, 2020, CHAOS SOLITON FRACT, V139, DOI 10.1016/j.chaos.2020.110061
   Aviel Y, 2003, NEURAL COMPUT, V15, P1321, DOI 10.1162/089976603321780290
   Barzel B, 2012, PHYS REV E, V86, DOI 10.1103/PhysRevE.86.031126
   Bick C, 2020, J MATH NEUROSCI, V10, DOI 10.1186/s13408-020-00086-9
   Brunel N, 2000, J COMPUT NEUROSCI, V8, P183, DOI 10.1023/A:1008925309027
   Byrne A, 2022, BRAIN TOPOGR, V35, P36, DOI 10.1007/s10548-021-00842-4
   Cabessa J, 2020, NEURAL NETWORKS, V126, P312, DOI 10.1016/j.neunet.2020.03.019
   Câteau H, 2001, NEURAL NETWORKS, V14, P675, DOI 10.1016/S0893-6080(01)00065-X
   Cox DR., 1962, RENEWAL THEORY
   de la Rocha J, 2007, NATURE, V448, P802, DOI 10.1038/nature06028
   Diesmann M, 1999, NATURE, V402, P529, DOI 10.1038/990101
   Feng J., 2003, COMPUTATIONAL NEUROS
   Feng JF, 2006, PHYS REV E, V73, DOI 10.1103/PhysRevE.73.041906
   Fourcaud N, 2002, NEURAL COMPUT, V14, P2057, DOI 10.1162/089976602320264015
   Gerstner W., 2002, SPIKING NEURON MODEL
   Gillespie CS, 2009, IET SYST BIOL, V3, P52, DOI 10.1049/iet-syb:20070031
   Gleeson JP, 2012, PHYS REV E, V85, DOI 10.1103/PhysRevE.85.026106
   GOODMAN LA, 1953, BIOMETRICS, V9, P212, DOI 10.2307/3001852
   Kasabov N. K., 2019, TIME SPACE SPIKING N, DOI DOI 10.1007/978-3-662-57715-8
   Kuehn C, 2016, UNDERST COMPLEX SYST, P253, DOI 10.1007/978-3-319-28028-8_13
   Kumar A, 2008, J NEUROSCI, V28, P5268, DOI 10.1523/JNEUROSCI.2542-07.2008
   Kumar A, 2010, NAT REV NEUROSCI, V11, P615, DOI 10.1038/nrn2886
   Lu WL, 2010, NEUROIMAGE, V52, P913, DOI 10.1016/j.neuroimage.2010.02.075
   Martens EA, 2013, P NATL ACAD SCI USA, V110, P10563, DOI 10.1073/pnas.1302880110
   Mazurek ME, 2002, NAT NEUROSCI, V5, P463, DOI 10.1038/nn836
   Mozer M.C., 1989, COMPLEX SYSTEMS, V3, P349
   Nkomo S, 2013, PHYS REV LETT, V110, DOI 10.1103/PhysRevLett.110.244102
   Nowak LG, 1997, CEREB CORTEX, V7, P487, DOI 10.1093/cercor/7.6.487
   Obeid D, 2020, PHYS REV E, V102, DOI 10.1103/PhysRevE.102.052406
   Pehlevan C, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-03261-5
   Pietras B, 2020, PHYS REV E, V102, DOI 10.1103/PhysRevE.102.022407
   Reyes AD, 2003, NAT NEUROSCI, V6, P593, DOI 10.1038/nn1056
   Riehle A, 1997, SCIENCE, V278, P1950, DOI 10.1126/science.278.5345.1950
   Rieke F., 1999, SPIKES EXPLORING NEU
   Shadlen MN, 1998, J NEUROSCI, V18, P3870
   Sompolinsky H, 2001, PHYS REV E, V64, DOI 10.1103/PhysRevE.64.051904
   Tan C, 2020, NEURAL PROCESS LETT, V52, P1675, DOI 10.1007/s11063-020-10322-8
   van Vreeswijk C, 1998, NEURAL COMPUT, V10, P1321, DOI 10.1162/089976698300017214
   Vogels TP, 2005, J NEUROSCI, V25, P10786, DOI 10.1523/JNEUROSCI.3508-05.2005
   Wang Z, 2016, PLOS COMPUT BIOL, V12, DOI 10.1371/journal.pcbi.1004979
   WHITTLE P, 1957, J ROY STAT SOC B, V19, P268
   WILSON HR, 1972, BIOPHYS J, V12, P1, DOI 10.1016/S0006-3495(72)86068-5
   WILSON HR, 1973, KYBERNETIK, V13, P55, DOI 10.1007/BF00288786
   Xiao ZC, 2018, ENTROPY-SWITZ, V20, DOI 10.3390/e20020102
NR 47
TC 0
Z9 0
U1 4
U2 14
PD OCT
PY 2022
VL 54
IS 5
BP 3893
EP 3911
DI 10.1007/s11063-022-10792-y
EA MAR 2022
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT J
AU Dillmann, R
   Rönnau, A
AF Dillmann, Rudiger
   Roennau, Arne
TI Biomorphic robot controls: event driven model free deep SNNs for complex
   visuomotor tasks
SO ARTIFICIAL LIFE AND ROBOTICS
DT Article
DE Neurorobotics; SNNs; DVS; Neural learning
AB The human brain surpass conventional computer architectures in regard to energy efficiency, robustness, connectivity and adaptivity. These aspects are inspiring today's emerging biomorphic technologies into both directions, software tools, and hardware systems. Thus, it is worthwhile to investigate into biological processes which enable the brain to perform computations and how they can be modelled and implemented in silicon. Taking inspiration from how the brain performs information processing requires a shift of computational paradigms compared to conventional computer architectures. Indeed, the brain is composed of nervous cells, called neurons, connected with synapses and forming self-organized networks. Neurons and synapses are complex dynamical systems ruled by biochemical and electrical reactions. As a result, they rely on local information forming functional neural clusters. Additionally, neurons communicate with each other with short electrical pulses, called spikes, which travel across synapses. Computational neuroscientists and roboticists attempt to model these computations with spiking neural networks (SNNs) and to ground them with real robots. When implemented on dedicated neuromorphic hardware, spiking neural networks can perform time delay free, energy efficient computations in analogy to the brain. Until recently, the advantages of this technology were limited due to the lack of efficient methods for programming spiking neural networks. Reinforcement learning is one paradigm for programming spiking neural networks, in which neural clusters self-organize them towards functional networks. In this paper a survey on our research at FZI on design, implementation and experiments with SNNs, solving visuomotor tasks for biomorphic robots is given.
C1 [Dillmann, Rudiger; Roennau, Arne] Res Ctr Informat FZI, Interact Diag Syst IDS, Karlsruhe, Germany.
RP Dillmann, R (corresponding author), Res Ctr Informat FZI, Interact Diag Syst IDS, Karlsruhe, Germany.
EM dillmann@fzi.de; roennau@fzi.de
CR Abbott LF, 1999, BRAIN RES BULL, V50, P303, DOI 10.1016/S0361-9230(99)00161-6
   Andreopoulos A, 2018, PROC CVPR IEEE, P7532, DOI 10.1109/CVPR.2018.00786
   Andrychowicz M, 2020, INT J ROBOT RES, V39, P3, DOI 10.1177/0278364919887447
   [Anonymous], 2016, AISB 06 WORKSHOP
   Baldi P, 2016, NEURAL NETWORKS, V83, P51, DOI 10.1016/j.neunet.2016.07.006
   Bellec G, 2019, ARXIV
   Bing ZS, 2018, IEEE INT CONF ROBOT, P4725
   Bizzi E, 2008, BRAIN RES REV, V57, P125, DOI 10.1016/j.brainresrev.2007.08.004
   Bogdan, 2019, P 2019 EMERGING TECH
   Burkitt AN, 2006, BIOL CYBERN, V95, P1, DOI 10.1007/s00422-006-0068-6
   Capolei MC, 2019, FRONT NEUROROBOTICS, V13, DOI 10.3389/fnbot.2019.00070
   Ciocarlie M., 2007, ROBOTICS
   CRAGO PE, 1991, IEEE T BIO-MED ENG, V38, P17, DOI 10.1109/10.68205
   CUTKOSKY MR, 1989, IEEE T ROBOTIC AUTOM, V5, P269, DOI 10.1109/70.34763
   d'Avella A, 2003, NAT NEUROSCI, V6, P300, DOI 10.1038/nn1010
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Della Santina C, 2019, IEEE ROBOT AUTOM LET, V4, P1533, DOI 10.1109/LRA.2019.2896485
   DeWolf T, 2016, P ROY SOC B-BIOL SCI, V283, DOI 10.1098/rspb.2016.2134
   Donati E, 2018, P IEEE RAS-EMBS INT, P827, DOI 10.1109/BIOROB.2018.8487937
   Eickenberg M, 2017, NEUROIMAGE, V152, P184, DOI 10.1016/j.neuroimage.2016.10.001
   Eliasmith C., 2013, BUILD BRAIN NEURAL A, DOI DOI 10.1093/ACPROF:OSO/9780199794546.001.0001
   ELMAN JL, 1990, COGNITIVE SCI, V14, P179, DOI 10.1207/s15516709cog1402_1
   Falotico E, 2017, FRONT NEUROROBOTICS, V11, DOI 10.3389/fnbot.2017.00002
   Ficuciello F, 2018, SPR PROC ADV ROBOT, V4, P225, DOI 10.1007/978-3-319-56802-7_24
   Florian RV, 2007, NEURAL COMPUT, V19, P1468, DOI 10.1162/neco.2007.19.6.1468
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   Gerstner W., 2002, SPIKING NEURON MODEL, DOI DOI 10.1017/CBO9780511815706
   Haessig G, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00420
   Haessig G, 2018, IEEE T BIOMED CIRC S, V12, P860, DOI 10.1109/TBCAS.2018.2834558
   Hansen N, 2001, EVOL COMPUT, V9, P159, DOI 10.1162/106365601750190398
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Hoff M., 2018, INT C DEV LEARNING E
   HORNIK K, 1991, NEURAL NETWORKS, V4, P251, DOI 10.1016/0893-6080(91)90009-T
   Ijspeert AJ, 2013, NEURAL COMPUT, V25, P328, DOI 10.1162/NECO_a_00393
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Jaeger H, 2007, SCHOLARPEDIA, V2, P2330
   Jiang ZY, 2019, FRONT NEUROROBOTICS, V13, DOI 10.3389/fnbot.2019.00029
   Kaiser J, 2019, ARXIV
   Kaiser J, 2018, 2018 7 IEEE INT C BI
   Kaiser J, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00424
   Kaiser J, 2019, FRONT NEUROROBOTICS, V13, DOI 10.3389/fnbot.2019.00081
   Kaiser J, 2018, LECT NOTES COMPUT SC, V11139, P244, DOI 10.1007/978-3-030-01418-6_24
   Kaiser J, 2018, P IEEE RAS-EMBS INT, P260, DOI 10.1109/BIOROB.2018.8487959
   Kaiser J, 2017, BIOINSPIR BIOMIM, V12, DOI 10.1088/1748-3190/aa7663
   Kaiser J, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON SIMULATION, MODELING, AND PROGRAMMING FOR AUTONOMOUS ROBOTS (SIMPAR), P127, DOI 10.1109/SIMPAR.2016.7862386
   Kim, 2019, 2019 7 INT WINTER C
   Lahiri S, 2013, ADV NEURAL INF PROCE, P1034
   Lapique L., 1907, J PHYSL PATHOL GEN, V9, P620, DOI DOI 10.1007/S00422-007-0189-6
   Levine S., 2018, INT J ROBOT RES, V205, P5
   Lichtsteiner Patrick, 2008, IEEE Journal of Solid-State Circuits, V43, P566, DOI 10.1109/JSSC.2007.914337
   Lillicrap TP, 2020, NAT REV NEUROSCI, V21, P335, DOI 10.1038/s41583-020-0277-3
   Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955
   McCulloch W. S., 1943, B MATH BIOPHYS, V5, P115, DOI [10.1007/BF02478259, DOI 10.1007/BF02478259]
   MEAD CA, 1988, NEURAL NETWORKS, V1, P91, DOI 10.1016/0893-6080(88)90024-X
   Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236
   Mueggler E, 2014, IEEE INT C INT ROBOT, P2761, DOI 10.1109/IROS.2014.6942940
   Neftci EO, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00324
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Rhodes O, 2020, PHILOS T R SOC A, V378, DOI 10.1098/rsta.2019.0160
   Ruehl SW, 2014, 2014 IEEE INTERNATIONAL CONFERENCE ON ROBOTICS AND BIOMIMETICS IEEE-ROBIO 2014, P2465, DOI 10.1109/ROBIO.2014.7090710
   Sburlea AI, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-35018-x
   Scano A, 2018, FRONT NEUROROBOTICS, V12, DOI 10.3389/fnbot.2018.00057
   Scherzinger S, 2017, IEEE INT C INT ROBOT, P4568, DOI 10.1109/IROS.2017.8206325
   Siegelmann HT, 1991, P 5 ANN WORKSHOP COM, P440
   Sironi A, 2018, PROC CVPR IEEE, P1731, DOI 10.1109/CVPR.2018.00186
   Sjöström PJ, 2001, NEURON, V32, P1149, DOI 10.1016/s0896-6273(01)00542-6
   Starke J, 2020, INT J HUM ROBOT, V17, DOI 10.1142/S0219843620500085
   Steffen L, 2019, 2019 19TH INTERNATIONAL CONFERENCE ON ADVANCED ROBOTICS (ICAR), P501, DOI 10.1109/ICAR46387.2019.8981569
   Tieck J. C. Vasquez, 2018, BIOMED ROBOT BIOMECH
   Tieck JCV, 2017, LECT NOTES COMPUT SC, V10613, P43, DOI 10.1007/978-3-319-68600-4_6
   Tieck JCV., 2018, ROBOT AUTON SYST, V2018, P5
   Tieck JCV, 2018, IEEERSJ INT C INTELL
   Urbain, 2018, ARXIV
   Wolpert DM, 2001, TRENDS COGN SCI, V5, P487, DOI 10.1016/S1364-6613(00)01773-3
   Yamins DLK, 2016, NAT NEUROSCI, V19, P356, DOI 10.1038/nn.4244
   Zenke F, 2018, NEURAL COMPUT, V30, P1514, DOI 10.1162/neco_a_01086
NR 76
TC 0
Z9 0
U1 2
U2 2
PD AUG
PY 2022
VL 27
IS 3
BP 429
EP 440
DI 10.1007/s10015-022-00769-4
EA JUL 2022
WC Robotics
DA 2023-11-11
ER

PT J
AU Wennekers, T
   Pasemann, F
AF Wennekers, T
   Pasemann, F
TI Generalized types of synchronization in networks of spiking neurons
SO NEUROCOMPUTING
DT Article; Proceedings Paper
CT 9th Annual Computational Neuroscience Meeting (CNS*00)
CY JUL, 2000
CL BRUGGE, BELGIUM
DE gamma-oscillations; synchronization; complex dynamics; spiking neurons;
   neuromodulation
ID NEURAL NETWORKS
AB The synchronization of neural signals has been proposed as a temporal coding scheme in distributed cortical networks. Theoretical studies in this direction mainly focused on the synchronization of coupled oscillatory subsystems. In the present work we show that several complex types of synchronization previously described for graded response neurons appear similarly also in biologically realistic networks of spiking and compartmental neurons. This includes synchronized complex spatio-temporal behavior, partial and generalized synchronization. The results suggest a similarly rich spatio-temporal behavior in real neural systems and may guide experimental research towards the study of complex modes of synchronization and their neuromodulation. (C) 2001 Elsevier Science B.V. All rights reserved.
C1 Max Planck Inst Math Sci, D-04103 Leipzig, Germany.
RP Wennekers, T (corresponding author), Max Planck Inst Math Sci, Insel Str 22-26, D-04103 Leipzig, Germany.
CR Eckhorn R, 1999, IEEE T NEURAL NETWOR, V10, P464, DOI 10.1109/72.761705
   HARRISWARRICK RM, 1991, ANNU REV NEUROSCI, V14, P39, DOI 10.1146/annurev.ne.14.030191.000351
   MIROLLO RE, 1990, SIAM J APPL MATH, V50, P1645, DOI 10.1137/0150098
   NISCHWITZ A, 1995, BIOL CYBERN, V73, P389, DOI 10.1007/BF00201473
   Pasemann F, 1999, THEOR BIOSCI, V118, P267
   Pasemann F, 2000, NETWORK-COMP NEURAL, V11, P41, DOI 10.1088/0954-898X/11/1/303
   Pinsky P F, 1994, J Comput Neurosci, V1, P39, DOI 10.1007/BF00962717
   Singer W, 1994, TEMPORAL CODING BRAI, P51
   Traub RD, 1997, NEURAL COMPUT, V9, P1251, DOI 10.1162/neco.1997.9.6.1251
NR 9
TC 1
Z9 1
U1 0
U2 2
PD JUN
PY 2001
VL 38
BP 1037
EP 1042
DI 10.1016/S0925-2312(01)00389-7
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT J
AU Budzinski, RC
   Boaretto, BRR
   Prado, TL
   Lopes, SR
AF Budzinski, R. C.
   Boaretto, B. R. R.
   Prado, T. L.
   Lopes, S. R.
TI Temperature dependence of phase and spike synchronization of neural
   networks
SO CHAOS SOLITONS & FRACTALS
DT Article
DE Neural network; Synchronization; Nonlinear dynamics
ID RECURRENCE PLOTS
AB We simulate a small-world neural network composed of 2000 thermally sensitive identical Hodgkin-Huxley type neurons investigating the synchronization characteristics as a function of the coupling strength and the temperature of the neurons. The Kuramoto order parameter computed over individual neuron membrane potential signals, and recurrence analysis evaluated from the mean field of the network are used to identify the non-monotonous behavior of the synchronization level as a function of the coupling parameter. We show that moderated high temperatures induce a low variability of the inter-burst intervals of neurons leading to phase synchronization and further increases of temperature result in a low variability of inter-spike intervals leading the network to display spike synchronization. (C) 2019 Elsevier Ltd. All rights reserved.
C1 [Budzinski, R. C.; Boaretto, B. R. R.; Prado, T. L.; Lopes, S. R.] Univ Fed Parana, Dept Fis, BR-81531980 Curitiba, Parana, Brazil.
   [Prado, T. L.] Univ Fed Vales Jequitinhonha & Mucuri, Inst Engn Ciencia & Tecnol, BR-39440000 Janauba, MG, Brazil.
RP Lopes, SR (corresponding author), Univ Fed Parana, Dept Fis, BR-81531980 Curitiba, Parana, Brazil.
EM lopes@fisica.ufpr.br
CR [Anonymous], 2012, CHEM OSCILLATIONS WA
   Bar-Yam Y., 1997, DYNAMICS COMPLEX SYS
   Bassett DS, 2006, NEUROSCIENTIST, V12, P512, DOI 10.1177/1073858406293182
   Boaretto BRR, 2018, CHAOS, V28, DOI 10.1063/1.5023878
   Boaretto BRR, 2018, PHYSICA A, V497, P126, DOI 10.1016/j.physa.2017.12.053
   Boccaletti S, 2002, PHYS REP, V366, P1, DOI 10.1016/S0370-1573(02)00137-0
   Boccara N, 2010, GRAD TEXTS PHYS, P1, DOI 10.1007/978-1-4419-6562-2
   Braun HA, 1998, INT J BIFURCAT CHAOS, V8, P881, DOI 10.1142/S0218127498000681
   Braun W, 2000, PHYS REV E, V62, P6352, DOI 10.1103/PhysRevE.62.6352
   Brugger F, 2015, NPJ PARKINSON DIS, V1, DOI 10.1038/npjparkd.2015.14
   Budzinski RC, 2019, PHYS REV E, V99, DOI 10.1103/PhysRevE.99.022402
   Budzinski RC, 2018, PHYSICA A, V507, P321, DOI 10.1016/j.physa.2018.05.076
   Budzinski RC, 2017, PHYS REV E, V96, DOI 10.1103/PhysRevE.96.012320
   Burek M, 2019, BIOSYSTEMS, V180, P1, DOI 10.1016/j.biosystems.2019.03.003
   Cohen S. D., 1996, Computers in Physics, V10, P138
   Coombes S, 2005, BURSTING: THE GENESIS OF RHYTHM IN THE NERVOUS SYSTEM, pV
   DESTEXHE A, 1994, NEURAL COMPUT, V6, P14, DOI 10.1162/neco.1994.6.1.14
   Dhamala M, 2004, PHYS REV LETT, V92, DOI 10.1103/PhysRevLett.92.028101
   Dinstein I, 2011, NEURON, V70, P1218, DOI 10.1016/j.neuron.2011.04.018
   ECKMANN JP, 1987, EUROPHYS LETT, V4, P973, DOI 10.1209/0295-5075/4/9/004
   Eguíluz VM, 2005, PHYS REV LETT, V94, DOI 10.1103/PhysRevLett.94.018102
   Etémé AS, 2017, CHAOS SOLITON FRACT, V104, P813, DOI 10.1016/j.chaos.2017.09.037
   Galvan A, 2008, CLIN NEUROPHYSIOL, V119, P1459, DOI 10.1016/j.clinph.2008.03.017
   He Y, 2007, CEREB CORTEX, V17, P2407, DOI 10.1093/cercor/bhl149
   Hilgetag CC., 2008, LECT SUPERCOMPUTATIO
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Iasemidis LD, 1996, NEUROSCIENTIST, V2, P118, DOI 10.1177/107385849600200213
   Ivanchenko MV, 2004, PHYS REV LETT, V93, DOI 10.1103/PhysRevLett.93.134101
   Marwan N, 2007, PHYS REP, V438, P237, DOI 10.1016/j.physrep.2006.11.001
   MORIMOTO T, 1985, EPILEPSIA, V26, P237, DOI 10.1111/j.1528-1157.1985.tb05412.x
   Newman MEJ, 1999, PHYS REV E, V60, P7332, DOI 10.1103/PhysRevE.60.7332
   Olivares E, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0139314
   Prado TD, 2014, PHYS REV E, V90, DOI 10.1103/PhysRevE.90.032818
   Prado TD, 2018, CHAOS, V28, DOI 10.1063/1.5022154
   Rubido N, 2011, EUR PHYS J D, V62, P51, DOI 10.1140/epjd/e2010-00215-4
   SATISHCHANDRA P, 1988, EPILEPSIA, V29, P52, DOI 10.1111/j.1528-1157.1988.tb05098.x
   SCHAFER K, 1995, PFLUG ARCH EUR J PHY, V429, P378, DOI 10.1007/BF00374153
   Stam CJ, 2012, CLIN NEUROPHYSIOL, V123, P1067, DOI 10.1016/j.clinph.2012.01.011
   Strogatz SH, 2001, NATURE, V410, P268, DOI 10.1038/35065725
   THOMPSON SM, 1985, J NEUROSCI, V5, P817
   Varshney LR, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1001066
   Xie YH, 2010, BIOPHYS CHEM, V146, P126, DOI 10.1016/j.bpc.2009.11.004
   Xu KS, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-26730-9
   Zhou W., 2015, STABILITY SYNCHRONIZ
   CHAOS, V10, P231
   PHYSICA D, V144, P358
NR 46
TC 13
Z9 14
U1 1
U2 9
PD JUN
PY 2019
VL 123
BP 35
EP 42
DI 10.1016/j.chaos.2019.03.039
WC Mathematics, Interdisciplinary Applications; Physics, Multidisciplinary;
   Physics, Mathematical
DA 2023-11-11
ER

PT C
AU Zhigulin, VP
   Rabinovich, MI
AF Zhigulin, VP
   Rabinovich, MI
BE DeSchutter, E
TI An important role of spike timing dependent synaptic plasticity in the
   formation of synchronized neural ensembles
SO COMPUTATIONAL NEUROSCIENCE: TRENDS IN RESEARCH 2004
DT Proceedings Paper
CT 12th Annual Computational Neuroscience Meeting (CNS 03)
CY JUL 05-09, 2003
CL Alicante, SPAIN
DE synchronization; spike timing-dependent plasticity; hippocampus
ID OSCILLATIONS; ENHANCEMENT
AB Synchronous neural activity plays an important role in the functioning of the brain. In this paper, we study the entrainment of a heterogeneous network of electrically coupled neurons by synaptically mediated periodic stimulation. We demonstrate by computer simulations that input synapses with spike timing-dependent plasticity (STDP) greatly enhance the coherence of spiking activity in the network as compared to the case of input with constant strength. We also show that synchronization in the network stimulated through STDP synapses is much more robust to the variability of network properties. The observed mechanism may play a role in synchronizing the activity of a hippocampal network. (C) 2004 Elsevier B.V. All rights reserved.
C1 CALTECH, Dept Phys, Pasadena, CA 91125 USA.
RP Zhigulin, VP (corresponding author), CALTECH, Dept Phys, MC 103-33, Pasadena, CA 91125 USA.
CR Buhl DL, 2003, J NEUROSCI, V23, P1013
   Hormuzdi SG, 2001, NEURON, V31, P487, DOI 10.1016/S0896-6273(01)00387-7
   Miles R, 1991, NEURONAL NETWORKS HI, DOI 10.1017/cbo9780511895401
   Nowotny T, 2003, J NEUROSCI, V23, P9776
   Traub RD, 2003, P NATL ACAD SCI USA, V100, P1370, DOI 10.1073/pnas.0337529100
   Zhigulin VP, 2003, PHYS REV E, V67, DOI 10.1103/PhysRevE.67.021901
NR 6
TC 0
Z9 0
U1 0
U2 2
PY 2004
BP 373
EP 378
DI 10.1016/j.neucom.2004.01.069
WC Computer Science, Artificial Intelligence; Computer Science,
   Interdisciplinary Applications; Neurosciences
DA 2023-11-11
ER

PT J
AU Prezioso, M
   Mahmoodi, MR
   Bayat, FM
   Nili, H
   Kim, H
   Vincent, A
   Strukov, DB
AF Prezioso, M.
   Mahmoodi, M. R.
   Bayat, F. Merrikh
   Nili, H.
   Kim, H.
   Vincent, A.
   Strukov, D. B.
TI Spike-timing-dependent plasticity learning of coincidence detection with
   passively integrated memristive circuits
SO NATURE COMMUNICATIONS
DT Article
ID DEVICE; MEMORY; SYNCHRONY; NETWORK; SYNAPSE; CELLS
AB Spiking neural networks, the most realistic artificial representation of biological nervous systems, are promising due to their inherent local training rules that enable low-overhead online learning, and energy-efficient information encoding. Their downside is more demanding functionality of the artificial synapses, notably including spike-timing-dependent plasticity, which makes their compact efficient hardware implementation challenging with conventional device technologies. Recent work showed that memristors are excellent candidates for artificial synapses, although reports of even simple neuromorphic systems are still very rare. In this study, we experimentally demonstrate coincidence detection using a spiking neural network, implemented with passively integrated metal-oxide memristive synapses connected to an analogue leaky-integrate-and-fire silicon neuron. By employing spike-timing-dependent plasticity learning, the network is able to robustly detect the coincidence by selectively increasing the synaptic efficacies corresponding to the synchronized inputs. Not surprisingly, our results indicate that device-to-device variation is the main challenge towards realization of more complex spiking networks.
C1 [Prezioso, M.; Mahmoodi, M. R.; Bayat, F. Merrikh; Nili, H.; Kim, H.; Vincent, A.; Strukov, D. B.] UC Santa Barbara, Elect & Comp Engn Dept, Santa Barbara, CA 93106 USA.
RP Strukov, DB (corresponding author), UC Santa Barbara, Elect & Comp Engn Dept, Santa Barbara, CA 93106 USA.
EM strukov@ece.ucsb.edu
CR Agmon-Snir H, 1998, NATURE, V393, P268, DOI 10.1038/30505
   Alibart F, 2012, NANOTECHNOLOGY, V23, DOI 10.1088/0957-4484/23/7/075201
   Alibart F, 2010, ADV FUNCT MATER, V20, P330, DOI 10.1002/adfm.200901335
   Ambrogio S, 2016, IEEE T ELECTRON DEV, V63, P1508, DOI 10.1109/TED.2016.2526647
   [Anonymous], NAT PRECED
   [Anonymous], SPIKING NEURAL MODEL
   [Anonymous], P IEEE INT EL DEV M
   [Anonymous], P IEEE INT EL DEV M
   Bayat FM, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-04482-4
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Caporale N, 2008, ANNU REV NEUROSCI, V31, P25, DOI 10.1146/annurev.neuro.31.060407.125639
   Ceze L ., 2016, PROC DEVICE RES C, P1
   Choi S, 2018, NAT MATER, V17, P335, DOI 10.1038/s41563-017-0001-5
   Dongale TD, 2015, MAT SCI SEMICON PROC, V35, P174, DOI 10.1016/j.mssp.2015.03.015
   Gkoupidenis P, 2017, NAT COMMUN, V8, DOI 10.1038/ncomms15448
   Goswami S, 2017, NAT MATER, V16, P1216, DOI [10.1038/NMAT5009, 10.1038/nmat5009]
   Jo SH, 2010, NANO LETT, V10, P1297, DOI 10.1021/nl904092h
   Kim S, 2015, NANO LETT, V15, P2203, DOI 10.1021/acs.nanolett.5b00697
   Konig P, 1996, TRENDS NEUROSCI, V19, P130, DOI 10.1016/S0166-2236(96)80019-1
   Kuzum D, 2012, NANO LETT, V12, P2179, DOI 10.1021/nl201040y
   Lee JH, 2007, INT J CIRC THEOR APP, V35, P239, DOI 10.1002/cta.410
   Li SZ, 2013, J MATER CHEM C, V1, P5292, DOI 10.1039/c3tc30575a
   Ma XL, 2007, IEEE T NEURAL NETWOR, V18, P573, DOI 10.1109/TNN.2006.888376
   Maass W, 1999, INFORM COMPUT, V153, P26, DOI 10.1006/inco.1999.2806
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   MAINEN ZF, 1995, SCIENCE, V268, P1503, DOI 10.1126/science.7770778
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Milo V, 2016, INT EL DEVICES MEET
   Oertel D, 2000, P NATL ACAD SCI USA, V97, P11773, DOI 10.1073/pnas.97.22.11773
   Ohno T, 2011, NAT MATER, V10, P591, DOI [10.1038/NMAT3054, 10.1038/nmat3054]
   Pedretti G, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-05480-0
   Prezioso M, 2016, SCI REP-UK, V6, DOI 10.1038/srep21331
   Prezioso M, 2015, NATURE, V521, P61, DOI 10.1038/nature14441
   Saïghi S, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00051
   Serrano-Gotarredona T, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00002
   SHASTRI L, 1993, BEHAV BRAIN SCI, V16, P417, DOI 10.1017/S0140525X00030910
   SOFTKY WR, 1993, J NEUROSCI, V13, P334
   Stevens CF, 1998, NAT NEUROSCI, V1, P210, DOI 10.1038/659
   Strukov DB, 2018, NAT MATER, V17, P293, DOI 10.1038/s41563-018-0020-x
   Subramaniam A, 2013, IEEE T NANOTECHNOL, V12, P450, DOI 10.1109/TNANO.2013.2256366
   Suri M, 2013, IEEE T ELECTRON DEV, V60, P2402, DOI 10.1109/TED.2013.2263000
   Wang ZR, 2017, NAT MATER, V16, P101, DOI [10.1038/nmat4756, 10.1038/NMAT4756]
   Yang JJS, 2013, NAT NANOTECHNOL, V8, P13, DOI [10.1038/nnano.2012.240, 10.1038/NNANO.2012.240]
   Yu SM, 2011, IEEE T ELECTRON DEV, V58, P2729, DOI 10.1109/TED.2011.2147791
NR 44
TC 123
Z9 127
U1 4
U2 53
PD DEC 14
PY 2018
VL 9
AR 5311
DI 10.1038/s41467-018-07757-y
WC Multidisciplinary Sciences
DA 2023-11-11
ER

PT J
AU Zhao, JH
   Zurada, JM
   Yang, J
   Wu, W
AF Zhao, Junhong
   Zurada, Jacek M.
   Yang, Jie
   Wu, Wei
TI The convergence analysis of SpikeProp algorithm with smoothing
   <i>L</i><sub>1/2</sub> regularization
SO NEURAL NETWORKS
DT Article
DE Spiking neural networks; SpikeProp; Smoothing L-1/2 regularization;
   Convergence; Sparsity
ID NEURAL-NETWORKS; SPIKING NEURONS; GRADIENT-METHOD; PREDICTION; PENALTY
AB Unlike the first and the second generation artificial neural networks, spiking neural networks (SNNs) model the human brain by incorporating not only synaptic state but also a temporal component into their operating model. However, their intrinsic properties require expensive computation during training. This paper presents a novel algorithm to SpikeProp for SNN by introducing smoothing L-1/2 regularization term into the error function. This algorithm makes the network structure sparse, with some smaller weights that can be eventually removed. Meanwhile, the convergence of this algorithm is proved under some reasonable conditions. The proposed algorithms have been tested for the convergence speed, the convergence rate and the generalization on the classical XOR-problem, Iris problem and Wisconsin Breast Cancer classification. (C) 2018 Elsevier Ltd. All rights reserved.
C1 [Zhao, Junhong; Yang, Jie; Wu, Wei] Dalian Univ Technol, Sch Math Sci, Dalian 116024, Peoples R China.
   [Zurada, Jacek M.; Yang, Jie] Univ Louisville, Dept Elect & Comp Engn, Louisville, KY 40292 USA.
   [Zurada, Jacek M.] Univ Social Sci, Inst Informat Technol, PL-90113 Ada, Poland.
RP Yang, J (corresponding author), Dalian Univ Technol, Sch Math Sci, Dalian 116024, Peoples R China.
EM yangjiee@dlut.edu.cn
CR [Anonymous], WSEAS T MATH
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   Fan QW, 2014, NEUROCOMPUTING, V131, P208, DOI 10.1016/j.neucom.2013.10.023
   Fisher RA, 1936, ANN EUGENIC, V7, P179, DOI 10.1111/j.1469-1809.1936.tb02137.x
   HINTON GE, 1989, ARTIF INTELL, V40, P185, DOI 10.1016/0004-3702(89)90049-0
   HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   Ishikawa M, 1996, NEURAL NETWORKS, V9, P509, DOI 10.1016/0893-6080(96)83696-3
   Kong J., 2001, NE MATH J, V3, P371
   Lazzús JA, 2010, CHINESE J CHEM ENG, V18, P817, DOI 10.1016/S1004-9541(09)60133-6
   LICHMAN M., 2013, UCI MACHINE LEARNING
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Mc Loone S, 2001, NEUROCOMPUTING, V37, P71, DOI 10.1016/S0925-2312(00)00314-3
   McKennoch S, 2006, IEEE IJCNN, P3970
   Natschlager T, 1998, NETWORK-COMP NEURAL, V9, P319, DOI 10.1088/0954-898X/9/3/003
   Ruf B., 1998, THESIS
   Schrauwen B, 2004, IEEE IJCNN, P471, DOI 10.1109/IJCNN.2004.1379954
   Setiono R, 1997, NEURAL COMPUT, V9, P185, DOI 10.1162/neco.1997.9.1.185
   Shrestha S. B., 2017, IEEE T NEURAL NETWOR, P1
   Shrestha SB, 2015, NEURAL NETWORKS, V63, P185, DOI 10.1016/j.neunet.2014.12.001
   Thorpe S, 1996, NATURE, V381, P520, DOI 10.1038/381520a0
   Wu W, 2011, NEURAL NETWORKS, V24, P91, DOI 10.1016/j.neunet.2010.09.007
   Wu W, 2014, NEURAL NETWORKS, V50, P72, DOI 10.1016/j.neunet.2013.11.006
   Wu W, 2010, INFORM SCIENCES, V180, P1630, DOI 10.1016/j.ins.2009.12.030
   Xin JG, 2001, IEEE IJCNN, P1772, DOI 10.1109/IJCNN.2001.938430
   Xu ZB, 2012, IEEE T NEUR NET LEAR, V23, P1013, DOI 10.1109/TNNLS.2012.2197412
   Yang J, 2015, NONLINEAR DYNAM, V82, P723, DOI 10.1007/s11071-015-2190-2
   Zhang HS, 2012, NEUROCOMPUTING, V89, P141, DOI 10.1016/j.neucom.2012.02.029
   Zou ZY, 2008, CHINESE J CHEM ENG, V16, P62, DOI 10.1016/S1004-9541(08)60038-5
NR 30
TC 14
Z9 14
U1 0
U2 22
PD JUL
PY 2018
VL 103
BP 19
EP 28
DI 10.1016/j.neunet.2018.03.007
WC Computer Science, Artificial Intelligence; Neurosciences
DA 2023-11-11
ER

PT C
AU Skatchkovsky, N
   Simeone, O
   Jang, H
AF Skatchkovsky, Nicolas
   Simeone, Osvaldo
   Jang, Hyeryung
BE Ranzato, M
   Beygelzimer, A
   Dauphin, Y
   Liang, PS
   Vaughan, JW
TI Learning to Time-Decode in Spiking Neural Networks Through the
   Information Bottleneck
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 34 (NEURIPS 2021)
SE Advances in Neural Information Processing Systems
DT Proceedings Paper
CT 35th Conference on Neural Information Processing Systems (NeurIPS)
CY DEC 06-14, 2021
CL ELECTR NETWORK
AB One of the key challenges in training Spiking Neural Networks (SNNs) is that target outputs typically come in the form of natural signals, such as labels for classification or images for generative models, and need to be encoded into spikes. This is done by handcrafting target spiking signals, which in turn implicitly fixes the mechanisms used to decode spikes into natural signals, e.g., rate decoding. The arbitrary choice of target signals and decoding rule generally impairs the capacity of the SNN to encode and process information in the timing of spikes. To address this problem, this work introduces a hybrid variational autoencoder architecture, consisting of an encoding SNN and a decoding Artificial Neural Network (ANN). The role of the decoding ANN is to learn how to best convert the spiking signals output by the SNN into the target natural signal. A novel end-to-end learning rule is introduced that optimizes a directed information bottleneck training criterion via surrogate gradients. We demonstrate the applicability of the technique in an experimental settings on various tasks, including real-life datasets.
C1 [Skatchkovsky, Nicolas; Simeone, Osvaldo] Kings Coll London, KCLIP Lab, Dept Engn, London, England.
   [Jang, Hyeryung] Dongguk Univ, ION Grp, Dept AI, Seoul, South Korea.
RP Skatchkovsky, N (corresponding author), Kings Coll London, KCLIP Lab, Dept Engn, London, England.
EM nicolas.skatchkovsky@kcl.ac.uk; osvaldo.simeone@kcl.ac.uk;
   hyeryung.jang@dgu.ac.kr
CR Alemi A. A., 2017, ICLR, P1
   [Anonymous], 1999, INFORM BOTTLENECK ME
   [Anonymous], 2015, FRONTIERS NEUROSCIEN, DOI [10.3389/fnins.2021.736888, DOI 10.3389/FNINS.2015.00481]
   Bagheri A, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P2986, DOI 10.1109/ICASSP.2018.8462410
   Bellec G, 2020, NAT COMMUN, V11, DOI 10.1038/s41467-020-17236-y
   Buesing L, 2010, NEURAL COMPUT, V22, P1961, DOI 10.1162/neco.2010.08-09-1084
   Chalk M, 2018, P NATL ACAD SCI USA, V115, P186, DOI 10.1073/pnas.1711114115
   Davies M, 2021, P IEEE, V109, P911, DOI 10.1109/JPROC.2021.3067593
   Gerstner W., 2002, SPIKING NEURON MODEL
   Henderson J. A., 2015, ARXIV150205777
   Humphries M., 2021, SPIKE EPIC JOURNEY B
   Jang H, 2019, IEEE SIGNAL PROC MAG, V36, P64, DOI 10.1109/MSP.2019.2935234
   Jang Hyeryung, 2020, SPIKING NEURAL NET 1
   Jang Hyeryung, 2020, ARXIV200409416
   Kaiser J, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00424
   Klampfl S, 2009, NEURAL COMPUT, V21, P911, DOI 10.1162/neco.2008.01-07-432
   Kramer G., 1998, DIRECTED INFORM CHAN
   LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541
   Lichtsteiner P., 2006, IEEE INT SOL STAT CI, P2060, DOI DOI 10.1109/ISSCC.2006.1696265
   Mnih A, 2016, PR MACH LEARN RES, V48
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Neftci EO, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00324
   Nokland A, 2019, PR MACH LEARN RES, V97
   Paszke A., 2019, ADV NEURAL INF PROCE, V32, P8024
   Pillow JW, 2008, NATURE, V454, P995, DOI 10.1038/nature07140
   Robey Dennis, 2021, NATURALIZING NEUROMO
   Shrestha SB, 2018, ADV NEUR IN, V31
   Simeone O, 2018, FOUND TRENDS SIGNAL, V12, P200, DOI 10.1561/2000000102
   Skatchkovsky N., 2020, P INT C AC SPEECH SI
   Stewart Kenneth, 2021, ARXIV210400165
   Tishby N, 2015, 2015 IEEE INFORMATION THEORY WORKSHOP (ITW)
   van der Maaten L, 2008, J MACH LEARN RES, V9, P2579
   Zenke F, 2018, NEURAL COMPUT, V30, P1514, DOI 10.1162/neco_a_01086
   Zhao B, 2015, IEEE T NEUR NET LEAR, V26, P1963, DOI 10.1109/TNNLS.2014.2362542
NR 34
TC 0
Z9 0
U1 0
U2 0
PY 2021
VL 34
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT S
AU Michler, F
   Wachtler, T
   Eckhorn, R
AF Michler, Frank
   Wachtler, Thomas
   Eckhorn, Reinhard
BE Schwenker, F
   Marinai, S
TI Adaptive feedback inhibition improves pattern discrimination learning
SO ARTIFICIAL NEURAL NETWORKS IN PATTERN RECOGNITION, PROCEEDINGS
SE LECTURE NOTES IN ARTIFICIAL INTELLIGENCE
DT Article; Proceedings Paper
CT 2nd IAPR Workshop on Artificial Neural Networks in Pattern Recognition
CY AUG 31-SEP 02, 2006
CL Ulm, GERMANY
ID VISUAL-CORTEX; SPIKING NEURONS; NEURAL NETWORK; MODEL; RECOGNITION;
   PERCEPTION; CIRCUITS
AB Neural network models for unsupervised pattern recognition learning are challenged when the difference between the patterns of the training set is small. The standard neural network architecture for pattern recognition learning consists of adaptive forward connections. and lateral inhibition, which provides competition between output neurons. We propose an additional adaptive inhibitory feedback mechanism, to emphasize the difference between training patterns and improve learning. We present an implementation of adaptive feedback inhibition for spiking neural network models, based on spike timing dependent plasticity (STDP). When the inhibitory feedback connections are adjusted using an anti-Hebbian learning rule, feedback inhibition suppresses the redundant activity of input units which code the overlap between similar stimuli. We show, that learning speed and pattern discriminatability can be increased by adding this mechanism to the standard architecture.
C1 Univ Marburg, Dept Phys, Appl Phys NeuroPhys Grp, D-35032 Marburg, Germany.
RP Michler, F (corresponding author), Univ Marburg, Dept Phys, Appl Phys NeuroPhys Grp, Renthof 7, D-35032 Marburg, Germany.
EM frank.michler@physik.uni-marburg.de
CR Beierlein M, 2003, J NEUROPHYSIOL, V90, P2987, DOI 10.1152/jn.00283.2003
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Callaway EM, 1998, ANNU REV NEUROSCI, V21, P47, DOI 10.1146/annurev.neuro.21.1.47
   Eckhardt S, 1999, ANN ONCOL, V10, P3, DOI 10.1023/A:1008387515250
   Eckhorn R, 2004, IEEE T NEURAL NETWOR, V15, P1039, DOI 10.1109/TNN.2004.833130
   FOLDIAK P, 1990, BIOL CYBERN, V64, P165, DOI 10.1007/BF02331346
   FUKUSHIMA K, 1975, BIOL CYBERN, V20, P121, DOI 10.1007/BF00342633
   Grossberg S, 2001, NEUROSCI BIOBEHAV R, V25, P513, DOI 10.1016/S0149-7634(01)00030-6
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   MIYAKE S, 1984, BIOL CYBERN, V50, P377, DOI 10.1007/BF00336963
   Rao RPN, 1997, NEURAL COMPUT, V9, P721, DOI 10.1162/neco.1997.9.4.721
   Royer S, 2003, NATURE, V422, P518, DOI 10.1038/nature01530
   Spratling MW, 1999, NETWORK-COMP NEURAL, V10, P285, DOI 10.1088/0954-898X/10/4/301
   Spratling MW, 2002, NEURAL COMPUT, V14, P2157, DOI 10.1162/089976602320264033
   VANOOYEN A, 1993, BIOL CYBERN, V70, P47, DOI 10.1007/BF00202565
NR 16
TC 2
Z9 2
U1 0
U2 2
PY 2006
VL 4087
BP 21
EP 32
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT C
AU Asesh, A
AF Asesh, Aishwarya
GP IEEE
TI Lifelike Neuromorphic Learning Networks (LNLN)
SO 2020 IEEE SYMPOSIUM SERIES ON COMPUTATIONAL INTELLIGENCE (SSCI)
DT Proceedings Paper
CT IEEE Symposium Series on Computational Intelligence (IEEE SSCI)
CY DEC 01-04, 2020
CL ELECTR NETWORK
AB Artificial Neural Network (ANN) has been known and used extensively to solve the demanding tasks of Machine Learning (ML) and Artificial Intelligence (AI). These networks have proven to he exceedingly successful with challenging tasks but only at the cost of doing massive amounts of computations. Spiking Neural Network (SNN) are known to be able to perform the same tasks but potentially with less power and computations. The proposed research develops an application on Spiking Neural Networks Simulators using various algorithms and input encoding to achieve accuracy that is at par with Analog Artificial Neural Network (AANN). Backpropagation approach is used on a pre-trained neural network and it is converted to SNN for rate coding. To add further Spike-timing-dependent plasticity (STDP) is used for training a rate encoded network. Using the above settings significant accuracy is achieved proving its uniqueness amongst the state-of-the-art algorithms. A detailed profiling of current literature is included. These findings underlie a huge potential and may locate the stage for further thrilling novel advances that drives key applications in neuromorphic engineering.
C1 [Asesh, Aishwarya] Adobe, Lehi, UT 84043 USA.
RP Asesh, A (corresponding author), Adobe, Lehi, UT 84043 USA.
EM asesh@adobe.com
CR Asesh Aishwarya., TESTED PARADIGM INCL
   Bekolay T, 2014, FRONT NEUROINFORM, V7, DOI 10.3389/fninf.2013.00048
   Chris Eliasmith, 2004, NEURAL ENG COMPUTATI
   Dayan P., 2001, THEORETICAL NEUROSCI, DOI DOI 10.1016/j.neuron.2013.01.033
   Diehl PU, 2015, IEEE IJCNN
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510
   Eliasmith C., 2013, BUILD BRAIN NEURAL A, DOI DOI 10.1093/ACPROF:OSO/9780199794546.001.0001
   Esser S.K., 2015, ADV NEURAL INFORM PR, P1117
   Fatahi Mazdak, 2016, ARXIV160406751
   Gerstner W., 2002, SPIKING NEURON MODEL
   Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Hunsberger E., 2015, ARXIV PREPRINT ARXIV
   Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223
   Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7
   Hoai M, 2011, PROC CVPR IEEE
   Norvig S., 2002, ARTIFICIAL INTELLIGE
   Rueckauer B., 2016, ARXIV161204052
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0
   Simonyan K, 2014, ADV NEUR IN, V27
   Wang LM, 2015, PROC CVPR IEEE, P4305, DOI 10.1109/CVPR.2015.7299059
   Zhao Y, 2017, IEEE I CONF COMP VIS, P2933, DOI 10.1109/ICCV.2017.317
NR 24
TC 0
Z9 0
U1 0
U2 0
PY 2020
BP 554
EP 561
WC Computer Science, Artificial Intelligence; Computer Science,
   Interdisciplinary Applications; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Yusoff, N
   Grüning, A
AF Yusoff, Nooraini
   Gruning, Andre
BE Makhtar, AK
   Yussof, H
   AlAssadi, H
   Yee, LC
TI Biologically Inspired Temporal Sequence Learning
SO INTERNATIONAL SYMPOSIUM ON ROBOTICS AND INTELLIGENT SENSORS 2012 (IRIS
   2012)
SE Procedia Engineering
DT Proceedings Paper
CT International Symposium on Robotics and Intelligent Sensors (IRIS)
CY SEP 04-06, 2012
CL Kuching, MALAYSIA
DE Temporal sequence learning; Spiking neural networks; Spike-timing
   dependent plasticity; Reward-based learning
ID SYSTEMS; NEURONS; MEMORY
AB We propose a temporal sequence learning model in spiking neural networks consisting of Izhikevich spiking neurons. In our reward-based learning model, we train a network to associate two stimuli with temporal delay and a target response. Learning rule is dependent on reward signals that modulate the weight changes derived from spike-timing dependent plasticity (STDP) function. The dynamic properties of our model can be attributed to the sparse and recurrent connectivity, synaptic transmission delays, background activity and interstimulus interval (ISI). We have tested the learning in visual recognition task, and temporal AND and XOR problems. The network can be trained to associate a stimulus pair with its target response and to discriminate the temporal sequence of the stimulus presentation. (c) 2012 The Authors. Published by Elsevier Ltd.
C1 [Yusoff, Nooraini] Univ Utara Malaysia, UUM Coll Arts & Sci, Sch Comp, Sintok 06010, Kedah, Malaysia.
   [Gruning, Andre] Univ Surrey, Fac Engn & Phys Sci, Dept Comp, Guildford GU2 7XH, Surrey, England.
RP Yusoff, N (corresponding author), Univ Utara Malaysia, UUM Coll Arts & Sci, Sch Comp, Sintok 06010, Kedah, Malaysia.
EM nooraini@uum.edu.my
CR [Anonymous], 1991, ANATOMY CORTEX
   [Anonymous], 1991, CORTICONICS
   Dayan P., 2005, THEORETICAL NEUROSCI
   Erickson CA, 1999, J NEUROSCI, V19, P10404
   Filippova MG, 2011, SPAN J PSYCHOL, V14, P20, DOI 10.5209/rev_SJOP.2011.v14.n1.2
   Florian RV, 2007, NEURAL COMPUT, V19, P1468, DOI 10.1162/neco.2007.19.6.1468
   Gu Q, 2002, NEUROSCIENCE, V111, P815, DOI 10.1016/S0306-4522(02)00026-X
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2007, CEREB CORTEX, V17, P2443, DOI 10.1093/cercor/bhl152
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Korn H, 2003, CR BIOL, V326, P787, DOI 10.1016/j.crvi.2003.09.011
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Naya Y, 2003, J NEUROSCI, V23, P2861
   SCHACTER DL, 1992, J COGNITIVE NEUROSCI, V4, P244, DOI 10.1162/jocn.1992.4.3.244
   Smith WB, 2005, NEURON, V45, P765, DOI 10.1016/j.neuron.2005.01.015
   Sutton R. S., 2015, REINFORCEMENT LEARNI, V2nd
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   TULVING E, 1982, J EXP PSYCHOL LEARN, V8, P336, DOI 10.1037/0278-7393.8.4.336
   Wörgötter F, 2005, NEURAL COMPUT, V17, P245, DOI 10.1162/0899766053011555
NR 19
TC 6
Z9 6
U1 0
U2 0
PY 2012
VL 41
BP 319
EP 325
DI 10.1016/j.proeng.2012.07.179
WC Engineering, Biomedical; Robotics; Remote Sensing
DA 2023-11-11
ER

PT C
AU Kawai, Y
   Takimoto, T
   Park, J
   Asada, M
AF Kawai, Yuji
   Takimoto, Tomohiro
   Park, Jihoon
   Asada, Minoru
GP IEEE
TI Efficient Reward-Based Learning through Body Representation in a Spiking
   Neural Network
SO 2018 JOINT IEEE 8TH INTERNATIONAL CONFERENCE ON DEVELOPMENT AND LEARNING
   AND EPIGENETIC ROBOTICS (ICDL-EPIROB)
SE Joint IEEE International Conference on Development and Learning and
   Epigenetic Robotics ICDL-EpiRob
DT Proceedings Paper
CT 8th Joint IEEE International Conference on Development and Learning and
   Epigenetic Robotics (ICDL-EpiRob)
CY SEP 16-20, 2018
CL Tokyo, JAPAN
DE brain-body interaction; canonical babbling; intrinsic plasticity;
   spiking neural network; spike-timing-dependent plasticity
ID CORTEX; STDP
AB Brain-body interactions guide the development of behavioral and cognitive functions. Sensory signals during behavior are relayed to the brain and evoke neural activity. This feedback is important for the organization of neural networks via neural plasticity, which in turn facilitates the generation of motor commands for new behaviors. In this study, we investigated how brain-body interactions develop and affect reward-based learning. We constructed a spiking neural network (SNN) model for the reward-based learning of canonical babbling, i.e., combination of a vowel and consonant. Motor commands to a vocal simulator were generated by SNN output and auditory signals representing the vocalized sound were fed back into the SNN. Synaptic weights in the SNN were updated using spike-timing-dependent plasticity (STDP). Connections from the SNN to the vocal simulator were modulated based on reward signals in terms of saliency of the vocalized sound. Our results showed that, under auditory feedback, STDP enabled the model to rapidly acquire babbling-like vocalization. We found that some neurons in the SNN were more highly activated during vocalization of a consonant than during other sounds. That is, neural dynamics in the SNN adapted to task-related articulator movements. Accordingly, body representation in the SNN facilitated brain-body interaction and accelerated the acquisition of babbling behavior.
C1 [Kawai, Yuji; Takimoto, Tomohiro; Park, Jihoon; Asada, Minoru] Osaka Univ, Grad Sch Engn, 2-1 Yamada Oka, Suita, Osaka 5650871, Japan.
RP Kawai, Y (corresponding author), Osaka Univ, Grad Sch Engn, 2-1 Yamada Oka, Suita, Osaka 5650871, Japan.
EM kawai@ams.eng.osaka-u.ac.jp; tomohiro.takimoto@ams.eng.osaka-u.ac.jp;
   jihoon.park@ams.eng.osaka-u.ac.jp; asada@ams.eng.osaka-u.ac.jp
CR Boersma P., 2015, PRAAT DOING PHONETIC
   Bouchard KE, 2013, NATURE, V495, P327, DOI 10.1038/nature11911
   Byrge L, 2014, TRENDS COGN SCI, V18, P395, DOI 10.1016/j.tics.2014.04.010
   Coath M, 2009, CONNECT SCI, V21, P193, DOI 10.1080/09540090902733905
   CRAIR MC, 1995, NATURE, V375, P325, DOI 10.1038/375325a0
   DOMINEY PF, 1995, BIOL CYBERN, V73, P265, DOI 10.1007/BF00201428
   Feldman DE, 1999, J NEUROBIOL, V41, P92, DOI 10.1002/(SICI)1097-4695(199910)41:1<92::AID-NEU12>3.0.CO;2-U
   Friston KJ, 1996, BRIT MED BULL, V52, P644
   Iyer SN, 2008, VOLTA REV, V108, P115
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2007, CEREB CORTEX, V17, P2443, DOI 10.1093/cercor/bhl152
   Izhikevich EM, 2006, NEURAL COMPUT, V18, P245, DOI 10.1162/089976606775093882
   Lazar A, 2009, FRONT COMPUT NEUROSC, V3, DOI 10.3389/neuro.10.023.2009
   Li XM, 2018, PHYSICA A, V491, P716, DOI 10.1016/j.physa.2017.08.053
   Norton D, 2006, IEEE IJCNN, P4243
   OLLER DK, 1988, CHILD DEV, V59, P441, DOI 10.1111/j.1467-8624.1988.tb01479.x
   Park J, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0182518
   Sampaio-Baptista C, 2013, J NEUROSCI, V33, P19499, DOI 10.1523/JNEUROSCI.3048-13.2013
   Takimoto T, 2017, J IEEE I C DEVELOP L, P120, DOI 10.1109/DEVLRN.2017.8329796
   Tononi G, 1998, TRENDS COGN SCI, V2, P474, DOI 10.1016/S1364-6613(98)01259-5
   Warlaumont AS, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0145096
   Xue FZ, 2013, NEUROCOMPUTING, V122, P324, DOI 10.1016/j.neucom.2013.06.019
   Yamada Y, 2016, SCI REP-UK, V6, DOI 10.1038/srep27893
NR 23
TC 1
Z9 1
U1 0
U2 0
PY 2018
BP 191
EP 196
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic; Robotics
DA 2023-11-11
ER

PT J
AU Baglietto, G
   Gigante, G
   Del Giudice, P
AF Baglietto, Gabriel
   Gigante, Guido
   Del Giudice, Paolo
TI Density-based clustering: A 'landscape view' of multi-channel neural
   data for inference and dynamic complexity analysis
SO PLOS ONE
DT Article
ID LEMPEL-ZIV COMPLEXITY; CORTICAL ACTIVITY; DECISION-MAKING; CELL
   ASSEMBLIES; SPIKING NEURONS; IN-VIVO; ADAPTATION; NETWORKS; STATES;
   SEQUENCES
AB Two, partially interwoven, hot topics in the analysis and statistical modeling of neural data, are the development of efficient and informative representations of the time series derived from multiple neural recordings, and the extraction of information about the connectivity structure of the underlying neural network from the recorded neural activities. In the present paper we show that state-space clustering can provide an easy and effective option for reducing the dimensionality of multiple neural time series, that it can improve inference of synaptic couplings from neural activities, and that it can also allow the construction of a compact representation of the multi-dimensional dynamics, that easily lends itself to complexity measures. We apply a variant of the 'mean-shift' algorithm to perform state-space clustering, and validate it on an Hopfield network in the glassy phase, in which metastable states are largely uncorrelated from memories embedded in the synaptic matrix. In this context, we show that the neural states identified as clusters' centroids offer a parsimonious parametrization of the synaptic matrix, which allows a significant improvement in inferring the synaptic couplings from the neural activities. Moving to the more realistic case of a multi-modular spiking network, with spike-frequency adaptation inducing history-dependent effects, we propose a procedure inspired by Boltzmann learning, but extending its domain of application, to learn inter-module synaptic couplings so that the spiking network reproduces a prescribed pattern of spatial correlations; we then illustrate, in the spiking network, how clustering is effective in extracting relevant features of the network's state-space landscape. Finally, we show that the knowledge of the cluster structure allows casting the multi-dimensional neural dynamics in the form of a symbolic dynamics of transitions between clusters; as an illustration of the potential of such reduction, we define and analyze a measure of complexity of the neural time series.
C1 [Baglietto, Gabriel; Del Giudice, Paolo] Italian Natl Inst Nucl Res INFN, INFN Roma1, Rome, Italy.
   [Baglietto, Gabriel] UNLP CONICET, IFLYSIB Inst Fis Liquidos & Sistemas Biol, La Plata, Buenos Aires, Argentina.
   [Gigante, Guido; Del Giudice, Paolo] Italian Inst Hlth ISS, Rome, Italy.
   [Gigante, Guido] Mperience Srl, Rome, Italy.
RP Baglietto, G (corresponding author), Italian Natl Inst Nucl Res INFN, INFN Roma1, Rome, Italy.; Baglietto, G (corresponding author), UNLP CONICET, IFLYSIB Inst Fis Liquidos & Sistemas Biol, La Plata, Buenos Aires, Argentina.
EM gabriel.baglietto@gmail.com
CR Abásolo D, 2015, J NEUROPHYSIOL, V113, P2742, DOI 10.1152/jn.00575.2014
   Abdollah-nia MF, 2012, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2012/02/P02018
   Abeles M, 2001, J NEUROSCI METH, V107, P141, DOI 10.1016/S0165-0270(01)00364-8
   ABELES M, 1995, P NATL ACAD SCI USA, V92, P8616, DOI 10.1073/pnas.92.19.8616
   Akrami A, 2012, BRAIN RES, V1434, P4, DOI 10.1016/j.brainres.2011.07.030
   Amigó JM, 2004, NEURAL COMPUT, V16, P717, DOI 10.1162/089976604322860677
   Amit D. J., 1992, MODELING BRAIN FUNCT
   [Anonymous], APPL COMP VIS 1994 P
   Capone C, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0118412
   Casali AG, 2013, SCI TRANSL MED, V5, DOI 10.1126/scitranslmed.3006294
   DAWES RM, 1979, AM PSYCHOL, V34, P571, DOI 10.1037/0003-066X.34.7.571
   Deco G, 2005, J COGNITIVE NEUROSCI, V17, P294, DOI 10.1162/0898929053124875
   Duarte RCF, 2014, FRONT COMPUT NEUROSC, V8, DOI 10.3389/fncom.2014.00124
   Fukunaga K, 2014, INFORM THEORY, V21, P32
   Gigante G, 2007, PHYS REV LETT, V98, DOI 10.1103/PhysRevLett.98.148101
   Gigante G, 2007, MATH BIOSCI, V207, P336, DOI 10.1016/j.mbs.2006.11.010
   Hertz JA, 2013, PRINCIPLES OF NEURAL CODING, P527
   HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554
   Huguet G, 2014, J VISION, V14, DOI 10.1167/14.3.19
   Igel C, 2003, NEUROCOMPUTING, V50, P105, DOI 10.1016/S0925-2312(01)00700-7
   KASPAR F, 1987, PHYS REV A, V36, P842, DOI 10.1103/PhysRevA.36.842
   La Camera G, 2004, NEURAL COMPUT, V16, P2101, DOI 10.1162/0899766041732468
   Latimer KW, 2015, SCIENCE, V349, P184, DOI 10.1126/science.aaa4056
   LEMPEL A, 1976, IEEE T INFORM THEORY, V22, P75, DOI 10.1109/TIT.1976.1055501
   Liu YH, 2001, J COMPUT NEUROSCI, V10, P25, DOI 10.1023/A:1008916026143
   Luczak A, 2007, P NATL ACAD SCI USA, V104, P347, DOI 10.1073/pnas.0605643104
   Mattia M, 2000, NEURAL COMPUT, V12, P2305, DOI 10.1162/089976600300014953
   Mattia M, 2013, J NEUROSCI, V33, P11155, DOI 10.1523/JNEUROSCI.4664-12.2013
   Mattia M, 2012, COGN NEURODYNAMICS, V6, P239, DOI 10.1007/s11571-011-9179-4
   Mazzucato L, 2015, J NEUROSCI, V35, P8214, DOI 10.1523/JNEUROSCI.4819-14.2015
   Morcos Ari S, 2016, NATURE NEUROSCIENCE
   Riedmiller M, 1992, P ISCIS 7 U
   Roach JP, 2016, PHYS REV E, V93, DOI 10.1103/PhysRevE.93.052307
   Rodriguez A, 2014, SCIENCE, V344, P1492, DOI 10.1126/science.1242072
   Roudi Y, 2015, CURR OPIN NEUROBIOL, V32, P38, DOI 10.1016/j.conb.2014.10.011
   Roudi Y, 2009, PHYS REV E, V79, DOI 10.1103/PhysRevE.79.051915
   Schneidman E, 2006, NATURE, V440, P1007, DOI 10.1038/nature04701
   Sohl-Dickstein J, 2011, PHYS REV LETT, V107, DOI 10.1103/PhysRevLett.107.220601
   Stevenson IH, 2011, NAT NEUROSCI, V14, P139, DOI 10.1038/nn.2731
   Theodoni P, 2011, J NEUROSCI, V31, P234, DOI 10.1523/JNEUROSCI.2757-10.2011
NR 40
TC 1
Z9 1
U1 0
U2 5
PD APR 3
PY 2017
VL 12
IS 4
AR e0174918
DI 10.1371/journal.pone.0174918
WC Multidisciplinary Sciences
DA 2023-11-11
ER

PT J
AU Zhang, ML
   Wang, JD
   Wu, JB
   Belatreche, A
   Amornpaisannon, B
   Zhang, ZX
   Miriyala, VPK
   Qu, H
   Chua, YS
   Carlson, TE
   Li, HZ
AF Zhang, Malu
   Wang, Jiadong
   Wu, Jibin
   Belatreche, Ammar
   Amornpaisannon, Burin
   Zhang, Zhixuan
   Miriyala, Venkata Pavan Kumar
   Qu, Hong
   Chua, Yansong
   Carlson, Trevor E.
   Li, Haizhou
TI Rectified Linear Postsynaptic Potential Function for Backpropagation in
   Deep Spiking Neural Networks
SO IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
DT Article
DE Neurons; Neuromorphics; Training; Timing; Membrane potentials; Hardware;
   Inference algorithms; Deep neural networks (DNNs); event-driven;
   neuromorphic hardware; spike-timing-dependent learning; spiking neural
   networks (SNNs)
AB Spiking neural networks (SNNs) use spatiotemporal spike patterns to represent and transmit information, which are not only biologically realistic but also suitable for ultralow-power event-driven neuromorphic implementation. Just like other deep learning techniques, deep SNNs (DeepSNNs) benefit from the deep architecture. However, the training of DeepSNNs is not straightforward because the well-studied error backpropagation (BP) algorithm is not directly applicable. In this article, we first establish an understanding as to why error BP does not work well in DeepSNNs. We then propose a simple yet efficient rectified linear postsynaptic potential function (ReL-PSP) for spiking neurons and a spike-timing-dependent BP (STDBP) learning algorithm for DeepSNNs where the timing of individual spikes is used to convey information (temporal coding), and learning (BP) is performed based on spike timing in an event-driven manner. We show that DeepSNNs trained with the proposed single spike time-based learning algorithm can achieve the state-of-the-art classification accuracy. Furthermore, by utilizing the trained model parameters obtained from the proposed STDBP learning algorithm, we demonstrate ultralow-power inference operations on a recently proposed neuromorphic inference accelerator. The experimental results also show that the neuromorphic hardware consumes 0.751 mW of the total power consumption and achieves a low latency of 47.71 ms to classify an image from the Modified National Institute of Standards and Technology (MNIST) dataset. Overall, this work investigates the contribution of spike timing dynamics for information encoding, synaptic plasticity, and decision-making, providing a new perspective to the design of future DeepSNNs and neuromorphic hardware.
C1 [Zhang, Malu] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 610054, Peoples R China.
   [Zhang, Malu] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 119077, Singapore.
   [Wang, Jiadong; Wu, Jibin; Chua, Yansong; Li, Haizhou] Natl Univ Singapore, Dept Elect & Comp Engn, Singapore 119077, Singapore.
   [Belatreche, Ammar] Northumbria Univ, Fac Engn & Environm, Dept Comp & Informat Sci, Newcastle Upon Tyne NE1 8ST, Tyne & Wear, England.
   [Amornpaisannon, Burin; Miriyala, Venkata Pavan Kumar; Carlson, Trevor E.] Natl Univ Singapore, Dept Comp Sci, Sch Comp, Singapore 119077, Singapore.
   [Zhang, Zhixuan; Qu, Hong] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 610054, Peoples R China.
RP Qu, H (corresponding author), Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 610054, Peoples R China.
EM hongqu@uestc.edu.cn
CR Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   [Anonymous], 2019, ARXIV190109948
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Brown T., 2020, PROC ADV NEUR INF PR, P1877
   Comsa I. M., 2019, ARXIV190713223
   Dai ZH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P2978
   Deng L., 2019, ARXIV191100822
   Deng L, 2020, IEEE J SOLID-ST CIRC, V55, P2228, DOI 10.1109/JSSC.2020.2970709
   Deng L, 2020, NEURAL NETWORKS, V121, P294, DOI 10.1016/j.neunet.2019.09.005
   Deng S., ARXIV210300476, V2021
   Diehl PU, 2016, IEEE IJCNN, P4278, DOI 10.1109/IJCNN.2016.7727758
   Diehl Peter U, 2015, 2015 INT JOINT C NEU, P1, DOI [DOI 10.1109/IJCNN.2015.7280696, 10.1109/IJCNN.2015.7280696]
   Du ZD, 2015, PROCEEDINGS OF THE 48TH ANNUAL IEEE/ACM INTERNATIONAL SYMPOSIUM ON MICROARCHITECTURE (MICRO-48), P494, DOI 10.1145/2830772.2830789
   Esser S.K., 2015, ADV NEURAL INFORM PR, P1117
   Esser SK, 2016, P NATL ACAD SCI USA, V113, P11441, DOI 10.1073/pnas.1604850113
   Esteva A, 2017, NATURE, V542, P115, DOI 10.1038/nature21056
   Feldmann J, 2019, NATURE, V569, P208, DOI 10.1038/s41586-019-1157-8
   Gerstner W., 2002, SPIKING NEURON MODEL
   Han B., 2020, ARXIV200301811
   Hao YZ, 2020, NEURAL NETWORKS, V121, P387, DOI 10.1016/j.neunet.2019.09.007
   He K., 2016, P IEEE C COMPUTER VI
   Hong CF, 2020, IEEE T NEUR NET LEAR, V31, P1285, DOI 10.1109/TNNLS.2019.2919662
   Huh D., 2018, ADV NEUR IN, V31, P1
   Hunsberger Eric, 2015, ARXIV151008829
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Jin Yingyezhe, 2018, P 32 INT C NEUR INF
   Khan MM, 2008, IEEE IJCNN, P2849, DOI 10.1109/IJCNN.2008.4634199
   Kheradpisheh S. R., 2020, ARXIV200704039
   Kheradpisheh S. Reza, 2019, ARXIV191009495
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Kwon H, 2017, INT SYM PERFORM ANAL, P195, DOI 10.1109/ISPASS.2017.7975291
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee JM, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00191
   Li XM, 2017, NEUROCOMPUTING, V243, P155, DOI 10.1016/j.neucom.2017.03.022
   Lin CK, 2018, COMPUTER, V51, P52, DOI 10.1109/MC.2018.157113521
   Liu, 2020, ARXIV PREPRINT ARXIV
   Liu Q., 2017, ARXIV170603609
   Masquelier T, 2007, PLOS COMPUT BIOL, V3, P247, DOI 10.1371/journal.pcbi.0030031
   Massa R, 2020, IEEE IJCNN, DOI 10.1109/ijcnn48605.2020.9207109
   Miriyala V. P. K., 2020, ARXIV201011741
   Mostafa H, 2018, IEEE T NEUR NET LEAR, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Mozafari M, 2018, IEEE T NEUR NET LEAR, V29, P6178, DOI 10.1109/TNNLS.2018.2826721
   Neftci EO, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00324
   Neil D, 2014, IEEE T VLSI SYST, V22, P2621, DOI 10.1109/TVLSI.2013.2294916
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   Panda P, 2016, IEEE IJCNN, P299, DOI 10.1109/IJCNN.2016.7727212
   Pei J, 2019, NATURE, V572, P106, DOI 10.1038/s41586-019-1424-8
   Pfeiffer M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00774
   Ranjan JAK, 2020, J SUPERCOMPUT, V76, P6545, DOI 10.1007/s11227-019-02881-y
   Riesenhuber M, 1999, NAT NEUROSCI, V2, P1019, DOI 10.1038/14819
   Rueckauer, ARXIV210104261, V2021
   Rueckauer B, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351295
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707
   Schneider S, 2019, INTERSPEECH, P3465, DOI 10.21437/Interspeech.2019-1873
   Shrestha SB, 2018, ADV NEUR IN, V31
   Shrestha SB, 2018, IEEE T NEUR NET LEAR, V29, P3126, DOI 10.1109/TNNLS.2017.2713125
   Shrestha SB, 2017, NEURAL NETWORKS, V96, P33, DOI 10.1016/j.neunet.2017.08.010
   Shrestha SB, 2015, NEURAL NETWORKS, V63, P185, DOI 10.1016/j.neunet.2014.12.001
   Smith J. E., 1982, 9th Annual Symposium on Computer Architecture, P112
   Srivatsa, 2020, YOU ONLY SPIKE ONCE
   Stöckl C, 2021, NAT MACH INTELL, V3, DOI 10.1038/s42256-021-00311-4
   Tan K. C., 2020, ARXIV200701204
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Tavanaei A, 2019, NEUROCOMPUTING, V330, P39, DOI 10.1016/j.neucom.2018.11.014
   Thorpe S, 2001, NEURAL NETWORKS, V14, P715, DOI 10.1016/S0893-6080(01)00083-1
   Wang B, 2020, DES AUT TEST EUROPE, P240, DOI 10.23919/DATE48585.2020.9116516
   Wu JY, 2019, 2019 5TH INTERNATIONAL CONFERENCE ON EVENT-BASED CONTROL, COMMUNICATION, AND SIGNAL PROCESSING (EBCCSP), DOI 10.1109/ebccsp.2019.8836892
   Wu YJ, 2019, AAAI CONF ARTIF INTE, P1311
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Xavier Glorot, 2011, P 14 INT C ARTIFICIA, P315, DOI DOI 10.1002/ECS2.1832
   Xiao H, 2017, ARXIV PREPRINT ARXIV
   Xu Y, 2013, NEURAL NETWORKS, V43, P99, DOI 10.1016/j.neunet.2013.02.003
   Yu Ji, 2018, ACM SIGPLAN Notices, V53, P448, DOI 10.1145/3296957.3173205
   Yu Q, 2019, IEEE T CYBERNETICS, V49, P2178, DOI 10.1109/TCYB.2018.2821692
   Zenke F, 2018, NEURAL COMPUT, V30, P1514, DOI 10.1162/neco_a_01086
   Zhang AG, 2019, NEUROCOMPUTING, V365, P102, DOI 10.1016/j.neucom.2019.07.009
NR 77
TC 42
Z9 42
U1 3
U2 29
PD MAY
PY 2022
VL 33
IS 5
BP 1947
EP 1958
DI 10.1109/TNNLS.2021.3110991
EA SEP 2021
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Computer Science, Theory & Methods; Engineering,
   Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Chen, CM
   Xue, YY
   Xiong, YZ
   Liu, MX
   Zhuang, LJ
   Wang, P
AF Chen, Changming
   Xue, Yingying
   Xiong, Yizhou
   Liu, Mengxue
   Zhuang, Liujing
   Wang, Ping
GP IEEE
TI An Auditory and Olfactory Data Fusion Algorithm Based on Spiking Neural
   Network for Mobile Robot
SO 2022 IEEE INTERNATIONAL SYMPOSIUM ON OLFACTION AND ELECTRONIC NOSE
   (ISOEN 2022)
DT Proceedings Paper
CT IEEE International Symposium on Olfaction and Electronic Nose (ISOEN)
CY MAY 29-JUN 01, 2022
CL Aveiro, PORTUGAL
DE Auditory sensors; e-Nose; Spiking neural network; Multimodal data
   fusion; Mobile robot
AB The emergence of bionic sensing technology has brought new inspiration to the control of mobile robot. However, an approach to fusing different modal data to control robot target recognition simply and effectively is a challenging problem at present. In this paper, a mobile robot platform is designed based on auditory and olfactory sensors. The robot obtains the sound information through the microphone array, and the electronic nose senses the chemical gas information, and proposes an auditory and olfactory fusion algorithm based on the spiking neural network, which uniformly encodes the signals of the two modes into spike signals and inputs them to the network for training to realize the target recognition. The designed robot platform can realize the information integration of sound and gas and the recognition of targets, which provides a new method for search and rescue.
C1 [Chen, Changming; Xue, Yingying; Xiong, Yizhou; Liu, Mengxue; Zhuang, Liujing; Wang, Ping] Zhejiang Univ, Dept Biomed Engn, Biosensor Natl Special Lab, Key Lab Biomed Engn,Educ Minist, Hangzhou 310027, Peoples R China.
   [Chen, Changming; Zhuang, Liujing; Wang, Ping] Zhejiang Univ, MOE Frontier Sci Ctr Brain Sci & Brain Machine In, Hangzhou 310027, Peoples R China.
   [Chen, Changming; Zhuang, Liujing; Wang, Ping] Zhejiang Univ, Canc Ctr, Hangzhou 310058, Zhejiang, Peoples R China.
RP Zhuang, LJ; Wang, P (corresponding author), Zhejiang Univ, Dept Biomed Engn, Biosensor Natl Special Lab, Key Lab Biomed Engn,Educ Minist, Hangzhou 310027, Peoples R China.; Zhuang, LJ; Wang, P (corresponding author), Zhejiang Univ, MOE Frontier Sci Ctr Brain Sci & Brain Machine In, Hangzhou 310027, Peoples R China.; Zhuang, LJ; Wang, P (corresponding author), Zhejiang Univ, Canc Ctr, Hangzhou 310058, Zhejiang, Peoples R China.
EM cnpwang@zju.edu.cn
CR Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Li XF, 2012, INT J ADV ROBOT SYST, V9, DOI 10.5772/51307
   Lilienthal AJ, 2006, SENSORS-BASEL, V6, P1616, DOI 10.3390/s6111616
   Liu M., 2022, NAT COMMUN, V13, P1
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Schrauwen B, 2003, IEEE IJCNN, P2825
NR 7
TC 0
Z9 0
U1 0
U2 6
PY 2022
DI 10.1109/ISOEN54820.2022.9789674
WC Chemistry, Analytical; Computer Science, Artificial Intelligence;
   Engineering, Biomedical; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Li, YL
   Shen, H
   Hu, DW
AF Li, Yulin
   Shen, Hui
   Hu, Dewen
BE Ying, X
TI A Spiking Neural Network for Brain-Computer Interface of Four Classes
   Motor Imagery
SO HUMAN BRAIN AND ARTIFICIAL INTELLIGENCE, HBAI 2022
SE Communications in Computer and Information Science
DT Proceedings Paper
CT International Workshop on Human Brain and Artificial Intelligence (HBAI)
CY JUL 23, 2022
CL Vienna, AUSTRIA
DE Brain-computer interface; Motor imagery; Spiking neural networks
ID CLASSIFICATION; ACCURATE
AB Spiking neural networks (SNN) has the advantages of low power consumption and high efficiency in processing temporal information. However, due to the difficulty of network training, there exist few studies about the applications of SNN in brain-computer interface (BCI), especially in the four-classification task of motor imagery (MI). In this study, we develop a four-layer SNN structure to solve the MI four-classification problem. Firstly, an improved optimization algorithm for Ben's spiker algorithm (BSA) is presented to convert EEG signals into spike signals, which obtains about 50 times higher efficiency than the commonly used optimizing algorithms. Secondly, a SNN combined with spike long-short-time-memory (LSTM) module is proposed to perform four-classification tasks in MI. Finally, we introduce the channel-wise normalization strategy to facilitate the training of deeper layers. Our experiment on the publicly released dataset achieves the accuracy that is comparable to the previous work of one-Dimension convolution neural network (1D-CNN). Meanwhile, the number of parameters of proposed network is about 1/10 of that in 1D-CNN. This study reveals the great potential of the SNN in developing a low-power and wearable BCI system.
C1 [Li, Yulin; Shen, Hui; Hu, Dewen] Natl Univ Def Technol, Coll Intelligence Sci, Changsha, Peoples R China.
RP Shen, H (corresponding author), Natl Univ Def Technol, Coll Intelligence Sci, Changsha, Peoples R China.
EM shenhui@nudt.edu.cn
CR Bellec G., 2018, ADV NEURAL INFORM PR, P787
   Bing Han, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13555, DOI 10.1109/CVPR42600.2020.01357
   Bohte SM, 2011, LECT NOTES COMPUT SC, V6791, P60, DOI 10.1007/978-3-642-21735-7_8
   Buteneers P, 2009, LECT NOTES COMPUT SC, V5506, P56, DOI 10.1007/978-3-642-02490-0_7
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Dose H, 2018, EXPERT SYST APPL, V114, P532, DOI 10.1016/j.eswa.2018.08.031
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   [胡一凡 Hu Yifan], 2021, [控制与决策, Control and Decision], V36, P1
   Jia ZY, 2022, SCI CHINA INFORM SCI, V65, DOI 10.1007/s11432-021-3380-1
   Kim S, 2020, AAAI CONF ARTIF INTE, V34, P11270
   Kim Y, 2021, NEURAL NETWORKS, V144, P686, DOI 10.1016/j.neunet.2021.09.022
   Lotze M, 2006, J PHYSIOL-PARIS, V99, P386, DOI 10.1016/j.jphysparis.2006.03.012
   McKennoch S, 2006, IEEE IJCNN, P3970
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Nuntalid N, 2011, LECT NOTES COMPUT SC, V7062, P451, DOI 10.1007/978-3-642-24955-6_54
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Schrauwen B, 2003, IEEE IJCNN, P2825
   Taherkhani A., 2015, INT JOINT C NEUR NET, P1
   Virgilio CD, 2020, NEURAL NETWORKS, V122, P130, DOI 10.1016/j.neunet.2019.09.037
   [王清华 Wang Qinghua], 2021, [计算机应用研究, Application Research of Computers], V38, P1381
   Wang ZJ, 2022, INT J INTELL SYST, V37, P2242, DOI 10.1002/int.22772
   Yin BJ, 2021, NAT MACH INTELL, V3, P905, DOI 10.1038/s42256-021-00397-w
NR 22
TC 0
Z9 0
U1 2
U2 5
PY 2023
VL 1692
BP 148
EP 160
DI 10.1007/978-981-19-8222-4_13
WC Computer Science, Artificial Intelligence; Computer Science,
   Interdisciplinary Applications; Neurosciences
DA 2023-11-11
ER

PT J
AU Yoon, YC
AF Yoon, Young C.
TI LIF and Simplified SRM Neurons Encode Signals Into Spikes via a Form of
   Asynchronous Pulse Sigma-Delta Modulation
SO IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS
DT Article
DE Asynchronous pulse sigma-delta modulation (APSDM); differential coding;
   leaky integrate-and-fire (LIF) neurons; level-crossing sampling and
   Lebesgue sampling; neural coding; spike response model (SRM) neurons;
   spiking neural networks (SNNs); spiking neurons
ID SPIKING NEURONS; MODEL
AB We show how two spiking neuron models encode continuous-time signals into spikes (action potentials, time-encoded pulses, or point processes) using a special form of sigma-delta modulation (SDM). In particular, we show that the well-known leaky integrate-and-fire (LIF) neuron and the simplified spike response model (SRM0) neuron encode the continuous-time signals into spikes via a proposed asynchronous pulse SDM (APSDM) scheme. The encoder is clock free using level-crossing sampling with a single-level quantizer, unipolar signaling, differential coding, and pulse-shaping filters. The decoder, in the form of a low-pass filter or bandpass smoothing filter, can be fed with the spikes to reconstruct an estimate of the signal. The density of the spikes reflects the amplitude of the encoded signal. Numerical examples illustrating the concepts and the signaling efficiency of APSDM vis-a-vis SDM for comparable reconstruction accuracies are presented. We anticipate these results will facilitate the design of spiking neurons and spiking neural networks as well as cross fertilizations between the fields of neural coding and the SDM.
C1 [Yoon, Young C.] Qualcomm Corp Res & Dev, San Diego, CA 92121 USA.
RP Yoon, YC (corresponding author), Qualcomm Corp Res & Dev, San Diego, CA 92121 USA.
EM yoon@ieee.org
CR Alvarado AS, 2011, IEEE INT SYMP CIRC S, P2031
   [Anonymous], ADV NEURAL INFORM PR
   [Anonymous], 1975, DELTA MODULATION SYS
   [Anonymous], 2013, 2013 INT JOINT C NEU
   [Anonymous], 2012, ADV NEURAL INFORM PR
   [Anonymous], 1966, ELECT LETT
   Arifuddin SM, 2012, INT J VLSI DESIGN CO, V3, P67
   Aziz PM, 1996, IEEE SIGNAL PROC MAG, V13, P61, DOI 10.1109/79.482138
   Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565
   Boerlin M, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1001080
   Brette R, 2005, J NEUROPHYSIOL, V94, P3637, DOI 10.1152/jn.00686.2005
   Brette R, 2007, J COMPUT NEUROSCI, V23, P349, DOI 10.1007/s10827-007-0038-6
   Chan V, 2007, IEEE T CIRCUITS-I, V54, P48, DOI 10.1109/TCSI.2006.887979
   Chen D, 2006, IEEE INT SYMP CIRC S, P2293
   Chicca E, 2014, P IEEE, V102, P1367, DOI 10.1109/JPROC.2014.2313954
   DAS J, 1967, ELECTRON LETT, V3, P284, DOI 10.1049/el:19670218
   de la Rosa JM, 2011, IEEE T CIRCUITS-I, V58, P1, DOI 10.1109/TCSI.2010.2097652
   Deneve S, 2008, NEURAL COMPUT, V20, P91, DOI 10.1162/neco.2008.20.1.91
   Eliasmith C., 2003, NEURAL ENG COMPUTATI
   Fontaine B, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003560
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   Gehlhaar J, 2014, ACM SIGPLAN NOTICES, V49, P317, DOI 10.1145/2541940.2564710
   Gerstner W, 1998, PULSED NEURAL NETWORKS, P3
   Gerstner W., 2002, SPIKING NEURON MODEL
   Hasler J, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00118
   Hunzinger J. F., 2013, SOC NEUR SFN NAN SAN
   Ieng SH, 2014, P IEEE, V102, P1485, DOI 10.1109/JPROC.2014.2347355
   Indiveri G, 2010, IEEE INT SYMP CIRC S, P1951, DOI 10.1109/ISCAS.2010.5536980
   INOSE H, 1963, P IEEE, V51, P1524, DOI 10.1109/PROC.1963.2622
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2010, DYNAMICAL SYSTEMS NE
   Janssen E, 2011, ANALOG CIRC SIG PROC, P5, DOI 10.1007/978-94-007-1387-1_2
   Jones DL, 2015, FRONT COMPUT NEUROSC, V9, DOI [10.3389/fncom.2015.00061, 10.3389/fncom.2015.0061]
   Koch Christof, 1999, P1
   Lazar AA, 2004, NEUROCOMPUTING, V58, P53, DOI 10.1016/j.neucom.2004.01.022
   Lazar AA, 2004, IEEE T CIRCUITS-I, V51, P2060, DOI 10.1109/TCSI.2004.835026
   Lichtsteiner P, 2008, IEEE J SOLID-ST CIRC, V43, P566, DOI 10.1109/JSSC.2007.914337
   Liu SC, 2015, EVENT-BASED NEUROMORPHIC SYSTEMS, P1, DOI 10.1002/9781118927601
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Naud R., 2012, SPIKE TIMING MECH FU, P65
   Naud R, 2012, PLOS COMPUT BIOL, V8, DOI 10.1371/journal.pcbi.1002711
   Park IM, 2013, IEEE SIGNAL PROC MAG, V30, P149, DOI 10.1109/MSP.2013.2251072
   Paugam-Moisy H, 2012, HDB NATURAL COMPUTIN, V1, P1, DOI DOI 10.1007/978-3-540-92910-9_10
   Proakis, 2007, DIGITAL COMMUNICATIO, V5th
   Reiss JD, 2008, J AUDIO ENG SOC, V56, P49
   Rieke F., 1996, SPIKES EXPLORING NEU
   Salas S., 2007, CALCULUS ONE SEVERAL
   Sayiner N, 1996, IEEE T CIRCUITS-II, V43, P335, DOI 10.1109/82.488288
   Schrauwen B, 2003, IEEE IJCNN, P2825
   SHARMA PD, 1968, ELECTRON ENG, V40, P32
   Sklar B., 2001, DIGITAL COMMUNICATIO
   Srinivasa N, 2012, IEEE PULSE, V3, P51, DOI 10.1109/MPUL.2011.2175639
   Sterratt D., 2011, PRINCIPLES COMPUTATI
   Stewart TC, 2014, P IEEE, V102, P881, DOI 10.1109/JPROC.2014.2306061
   Tang W, 2013, IEEE T CIRCUITS-I, V60, P1407, DOI 10.1109/TCSI.2012.2220464
   TOMBERG J, 1992, 1992 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOLS 1-6, P2232, DOI 10.1109/ISCAS.1992.230546
   Tsividis Y, 2010, IEEE T CIRCUITS-II, V57, P577, DOI 10.1109/TCSII.2010.2056012
   Vezyrtzis C, 2009, IEEE INT SYMP CIRC S, P2293, DOI 10.1109/ISCAS.2009.5118257
   Vogelstein RJ, 2004, 2004 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL 5, PROCEEDINGS, P385
   Wei D, 2004, 2004 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL 5, PROCEEDINGS, P353
   Wei D., 2005, THESIS
NR 61
TC 20
Z9 22
U1 0
U2 9
PD MAY
PY 2017
VL 28
IS 5
BP 1192
EP 1205
DI 10.1109/TNNLS.2016.2526029
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Computer Science, Theory & Methods; Engineering,
   Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Schuman, CD
   Bruer, G
   Young, AR
   Dean, M
   Plank, JS
AF Schuman, Catherine D.
   Bruer, Grant
   Young, Aaron R.
   Dean, Mark
   Plank, James S.
GP IEEE
TI Understanding Selection and Diversity for Evolution of Spiking Recurrent
   Neural Networks
SO 2018 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUL 08-13, 2018
CL Rio de Janeiro, BRAZIL
ID CLASSIFICATION; PREDICTION
AB Evolutionary optimization or genetic algorithms have been used to optimize a variety of neural network types, including spiking recurrent neural networks, and are attractive for many reasons. However, a key impediment to their widespread use is the potential for slow training times and failure to converge to a good fitness value in a reasonable amount of time. In this work, we evaluate the effect of different selection algorithms on the performance of an evolutionary optimization method for designing spiking recurrent neural networks, including those that are meant to be deployed in a neuromorphic system. We propose a selection approach that utilizes a richer understanding of the fitness of an individual network to inform the selection process and to promote diversity in the population. We show that including this feature can provide a significant increase in performance over utilizing a standard selection approach.
C1 [Schuman, Catherine D.] Oak Ridge Natl Lab, Computat Data Analyt, Oak Ridge, TN 37831 USA.
   [Bruer, Grant; Young, Aaron R.; Dean, Mark; Plank, James S.] Univ Tennessee, Dept Elect Engn & Comp Sci, Knoxville, TN 37996 USA.
RP Schuman, CD (corresponding author), Oak Ridge Natl Lab, Computat Data Analyt, Oak Ridge, TN 37831 USA.
EM schumancd@ornl.gov; gbruer@utk.edu; ayoung48@utk.edu; markdean@utk.edu;
   jplank@utk.edu
CR [Anonymous], 2016, INT JOINT C NEUR NET
   [Anonymous], 1991, F GENETIC ALGORITHMS
   Batllori R, 2011, PROCEDIA COMPUT SCI, V6, DOI 10.1016/j.procs.2011.08.060
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Burgsteiner H, 2007, APPL INTELL, V26, P99, DOI 10.1007/s10489-006-0007-1
   Cabessa J, 2014, INT J NEURAL SYST, V24, DOI 10.1142/S0129065714500294
   Carlson K. D., 2013, P 9 IEEE ACM IFIP IN, P20
   Dean Mark E., 2014, Unconventional Computation and Natural Computation. 13th International Conference. Proceedings: LNCS 8553, P129, DOI 10.1007/978-3-319-08123-6_11
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Esser S.K., 2015, ADV NEURAL INFORM PR, P1117
   Floreano D, 2008, EVOL INTELL, V1, P47, DOI 10.1007/s12065-007-0002-4
   Gomez F, 2008, J MACH LEARN RES, V9, P937
   Howard G., 2011, 2011 IEEE SSCI Symposium on Artificial Life (ALIFE), P14, DOI 10.1109/ALIFE.2011.5954655
   Jin YC, 2007, LECT NOTES COMPUT SC, V4668, P370
   Kasabov N, 2014, NEUROCOMPUTING, V134, P269, DOI 10.1016/j.neucom.2013.09.049
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Mitchell JP, 2017, 2017 IEEE 5TH INTERNATIONAL SYMPOSIUM ON ROBOTICS AND INTELLIGENT SENSORS (IRIS), P136, DOI 10.1109/IRIS.2017.8250111
   Palmes PP, 2005, IEEE T NEURAL NETWOR, V16, P587, DOI 10.1109/TNN.2005.844858
   Plank J. S., 2017, IEEE INT C REB COMP
   Schemmel J., 2001, ICES 2001 P SPRING, P50
   Schuman C. D., 2016, MACHINE LEARNING HPC
   Schuman C. D., 2015, THESIS
   Schuman CD, 2014, PROCEDIA COMPUT SCI, V41, P89, DOI 10.1016/j.procs.2014.11.089
   Schuman CD., 2017, ARXIV
   Siebel N.T., 2007, INT J HYBRID INTELL, V4, P171, DOI [10.3233/HIS-2007-4304, DOI 10.3233/HIS-2007-4304]
   Sivaraj R., 2011, INT J ENG SCI TECHNO, V3, P3792
   Srinivasan G, 2017, IEEE IJCNN, P1847, DOI 10.1109/IJCNN.2017.7966075
   Stanley KO, 2002, EVOL COMPUT, V10, P99, DOI 10.1162/106365602320169811
   Yao X, 1999, P IEEE, V87, P1423, DOI 10.1109/5.784219
   Yeung D., 2015, NEURAL NETWORKS LEAR, P1
   Young SM, 2015, PHYS REV APPL, V4, DOI 10.1103/PhysRevApplied.4.054004
   Zhang Y, 2015, IEEE T NEUR NET LEAR, V26, P2635, DOI 10.1109/TNNLS.2015.2388544
NR 32
TC 0
Z9 0
U1 0
U2 0
PY 2018
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Matsumoto, K
   Torikai, H
   Sekiya, H
AF Matsumoto, Kazuki
   Torikai, Hiroyuki
   Sekiya, Hiroo
GP IEEE
TI XOR learning by spiking neural network with infrared communications
SO 2018 ASIA-PACIFIC SIGNAL AND INFORMATION PROCESSING ASSOCIATION ANNUAL
   SUMMIT AND CONFERENCE (APSIPA ASC)
SE Asia-Pacific Signal and Information Processing Association Annual Summit
   and Conference
DT Proceedings Paper
CT 10th Asia-Pacific-Signal-and-Information-Processing-Association Annual
   Summit and Conference (APSIPA ASC)
CY NOV 12-15, 2018
CL Honolulu, HI
AB A Spiking Neural Network ( SNN), which expresses information by spike trains, has an ability to process information with low energy like a human brain. Hardware implementation of a SNN is an important research problem. If the neurons are linked by wireless communications, SNNs can obtain the spatial degree of freedom, which may extend application area dramatically. Additionally, such SNNs can process information with low energy, owing to wireless communication by the spike trains. Therefore, it is regarded as low power-consumption wireless sensor networks ( WSNs) with adding the functions of SNN neurons to wireless sensor nodes. This "Wireless Neural Sensor Networks" can distribute information processing like a brain on the WSN nodes. This paper presents a SNN with infrared(IR) communications as the first step of the above concept. Neurons are implemented by field programmable gate array, which are linked by IR communications. The implemented SNN succeeded in acquiring the XOR function through reinforcement learning.
C1 [Matsumoto, Kazuki; Sekiya, Hiroo] Chiba Univ, Grad Sch Adv Integrat Sci, Chiba, Japan.
   [Torikai, Hiroyuki] Hosei Univ, Dept Elect & Elect Engn, Tokyo, Japan.
RP Matsumoto, K (corresponding author), Chiba Univ, Grad Sch Adv Integrat Sci, Chiba, Japan.
EM matsumoto@chiba-u.jp
CR Bartlett P. L., 2000, INFORM SCI
   Bohte Sander M., 2004, Natural Computing, V3, P195, DOI 10.1023/B:NACO.0000027755.02868.60
   Caporale N, 2008, ANNU REV NEUROSCI, V31, P25, DOI 10.1146/annurev.neuro.31.060407.125639
   Cheung Kit, 2012, Artificial Neural Networks and Machine Learning - ICANN 2012. Proceedings of the 22nd International Conference on Artificial Neural Networks, P113, DOI 10.1007/978-3-642-33269-2_15
   Ebong IE, 2012, P IEEE, V100, P2050, DOI 10.1109/JPROC.2011.2173089
   Farries MA, 2007, J NEUROPHYSIOL, V98, P3648, DOI 10.1152/jn.00364.2007
   Florian R. V., 2005, 7 INT S SYMB NUM ALG, P8
   Florian RV, 2007, NEURAL COMPUT, V19, P1468, DOI 10.1162/neco.2007.19.6.1468
   Florian RV, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0040233
   Gerstner W, 1997, P NATL ACAD SCI USA, V94, P12740, DOI 10.1073/pnas.94.24.12740
   Gerstner W., 2002, SPIKING NEURON MODEL
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Legenstein R, 2005, NEURAL COMPUT, V17, P2337, DOI 10.1162/0899766054796888
   Legenstein R, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.1000180
   Maguire LP, 2007, NEUROCOMPUTING, V71, P13, DOI 10.1016/j.neucom.2006.11.029
   Mahadevuni A, 2017, IEEE IJCNN, P2243, DOI 10.1109/IJCNN.2017.7966127
   Neil D, 2014, IEEE T VLSI SYST, V22, P2621, DOI 10.1109/TVLSI.2013.2294916
   Rubin R, 2010, PHYS REV LETT, V105, DOI 10.1103/PhysRevLett.105.218102
   Shim MS, 2017, IEEE IJCNN, P3098, DOI 10.1109/IJCNN.2017.7966242
   Stanley GB, 2013, NAT NEUROSCI, V16, P259, DOI 10.1038/nn.3330
   Zheng N, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351516
NR 22
TC 3
Z9 3
U1 0
U2 3
PY 2018
BP 1289
EP 1292
WC Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT B
AU Holleman, J
   Zhang, F
   Otis, B
AF Holleman, Jeremy
   Zhang, Fan
   Otis, Brian
BA Holleman, J
   Zhang, F
   Otis, B
BF Holleman, J
   Zhang, F
   Otis, B
TI Spike Detection and Characterization
SO ULTRA LOW-POWER INTEGRATED CIRCUIT DESIGN FOR WIRLESS NEURAL INTERFACES
DT Article; Book Chapter
ID NONLINEAR ENERGY OPERATOR; POWER INTEGRATED-CIRCUIT; NEURAL-NETWORK;
   ANALOG
C1 [Holleman, Jeremy] Univ Tennessee, Dept Elect Engn & Comp Sci, Knoxville, TN 37996 USA.
   [Zhang, Fan; Otis, Brian] Univ Washington, Dept Elect Engn, Seattle, WA 98195 USA.
RP Holleman, J (corresponding author), Univ Tennessee, Dept Elect Engn & Comp Sci, 414 Ferris Hall, Knoxville, TN 37996 USA.
EM hollemj@u.washington.edu; fz2@u.washington.edu; botis@u.washington.edu
CR Allen PE, 2002, CMOS ANALOG CIRCUIT
   Brown EN, 2004, NAT NEUROSCI, V7, P456, DOI 10.1038/nn1228
   CAUWENBERGHS G, 2002, P IEEE INT C AC SPEE, V4, P3984
   Chae M., 2008, IEEE INT SOL STAT CI, P146
   Chakrabartty S, 2007, IEEE J SOLID-ST CIRC, V42, P1169, DOI 10.1109/JSSC.2007.894803
   COGGINS R, 1995, IEEE J SOLID-ST CIRC, V30, P542, DOI 10.1109/4.384167
   Delbruck T., 1991, IJCNN-91-Seattle: International Joint Conference on Neural Networks (Cat. No.91CH3049-4), P475, DOI 10.1109/IJCNN.1991.155225
   Haddad SAP, 2002, 2002 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL V, PROCEEDINGS, P621
   Harrison RR, 2007, IEEE J SOLID-ST CIRC, V42, P123, DOI 10.1109/JSSC.2006.886567
   Harrison RR, 2003, P ANN INT IEEE EMBS, V25, P3325, DOI 10.1109/IEMBS.2003.1280856
   Harrison RR, 2003, IEEE J SOLID-ST CIRC, V38, P958, DOI 10.1109/JSSC.2003.811979
   Holleman J, 2008, IEEE J SOLID-ST CIRC, V43, P1324, DOI 10.1109/JSSC.2008.920327
   Holleman J, 2008, IEEE CUST INTEGR CIR, P333, DOI 10.1109/CICC.2008.4672089
   Hsu D, 2002, IEEE T NEURAL NETWOR, V13, P732, DOI 10.1109/TNN.2002.1000139
   Hsu D, 2001, ADV NEUR IN, V13, P713
   Hsu D., 2003, ADV NEURAL INF PROCE, V15, P1107
   Kim KH, 2000, IEEE T BIO-MED ENG, V47, P1406, DOI 10.1109/10.871415
   LAZZARO J, 1988, WINNER TAKE ALL NETW, P703
   Lubkin J, 1998, J CIRCUIT SYST COMP, V8, P605, DOI 10.1142/S0218126698000389
   Mukhopadhyay S, 1998, IEEE T BIO-MED ENG, V45, P180, DOI 10.1109/10.661266
   Obeid I, 2004, IEEE T BIO-MED ENG, V51, P905, DOI 10.1109/TBME.2004.826683
   Quiroga RQ, 2004, NEURAL COMPUT, V16, P1661, DOI 10.1162/089976604774201631
   RAO S, 2006, INT CONF ACOUST SPEE, P881
   Scott MD, 2003, IEEE J SOLID-ST CIRC, V38, P1123, DOI 10.1109/JSSC.2003.813296
   STARZYK JA, 1993, ELECTRON LETT, V29, P908, DOI 10.1049/el:19930606
   TSIVIDIS YP, 1994, IEEE J SOLID-ST CIRC, V29, P166, DOI 10.1109/4.278337
   VITTOZ E, 1990, IEEE INT S CIRC SYST, V2, P1372
   VOGELSTEIN R, 2004, C P IEEE ENG MED BIO, V1, P546
   Wood F, 2004, IEEE T BIO-MED ENG, V51, P912, DOI 10.1109/TBME.2004.826677
NR 29
TC 0
Z9 0
U1 0
U2 0
PY 2011
BP 51
EP 63
DI 10.1007/978-1-4419-6727-5_7
D2 10.1007/978-1-4419-6727-5
WC Computer Science, Information Systems; Engineering, Electrical &
   Electronic
DA 2023-11-11
ER

PT C
AU Na, B
   Mok, J
   Park, S
   Lee, D
   Choe, H
   Yoon, S
AF Na, Byunggook
   Mok, Jisoo
   Park, Seongsik
   Lee, Dongjin
   Choe, Hyeokjun
   Yoon, Sungroh
BE Chaudhuri, K
   Jegelka, S
   Song, L
   Szepesvari, C
   Niu, G
   Sabato, S
TI AutoSNN: Towards Energy-Efficient Spiking Neural Networks
SO INTERNATIONAL CONFERENCE ON MACHINE LEARNING, VOL 162
SE Proceedings of Machine Learning Research
DT Proceedings Paper
CT 39th International Conference on Machine Learning (ICML)
CY JUL 17-23, 2022
CL Baltimore, MD
ID LOIHI
AB Spiking neural networks (SNNs) that mimic information transmission in the brain can energy-efficiently process spatio-temporal information through discrete and sparse spikes, thereby receiving considerable attention. To improve accuracy and energy efficiency of SNNs, most previous studies have focused solely on training methods, and the effect of architecture has rarely been studied. We investigate the design choices used in the previous studies in terms of the accuracy and number of spikes and figure out that they are not best-suited for SNNs. To further improve the accuracy and reduce the spikes generated by SNNs, we propose a spike-aware neural architecture search framework called AutoSNN. We define a search space consisting of architectures without undesirable design choices. To enable the spike-aware architecture search, we introduce a fitness that considers both the accuracy and number of spikes. AutoSNN successfully searches for SNN architectures that outperform hand-crafted SNNs in accuracy and energy efficiency. We thoroughly demonstrate the effectiveness of AutoSNN on various datasets including neuromorphic datasets.
C1 [Na, Byunggook] Samsung Adv Inst Technol, Suwon, South Korea.
   [Mok, Jisoo; Lee, Dongjin; Choe, Hyeokjun; Yoon, Sungroh] Seoul Natl Univ, Dept Elect & Comp Engn, Seoul, South Korea.
   [Park, Seongsik] Korea Inst Sci & Technol, Seoul, South Korea.
   [Yoon, Sungroh] Seoul Natl Univ, Interdisciplinary Program Artificial Intelligence, Seoul, South Korea.
RP Yoon, S (corresponding author), Seoul Natl Univ, Dept Elect & Comp Engn, Seoul, South Korea.; Yoon, S (corresponding author), Seoul Natl Univ, Interdisciplinary Program Artificial Intelligence, Seoul, South Korea.
EM sryoon@snu.ac.kr
CR Amir A., 2017, P IEEE C COMP VIS PA, P7243, DOI DOI 10.1109/CVPR.2017.781
   Bender G, 2018, PR MACH LEARN RES, V80
   Bing Han, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13555, DOI 10.1109/CVPR42600.2020.01357
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Cai H., 2019, 7 INT C LEARN REPR I
   Cai H., 2020, P INT C LEARN REPR, P1
   Chen Y., 2019, ARXIV190310979
   Chenhan Jiang, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P11860, DOI 10.1109/CVPR42600.2020.01188
   Davies M, 2021, P IEEE, V109, P911, DOI 10.1109/JPROC.2021.3067593
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   DeVries T., 2017, ARXIV
   Diehl PU, 2015, IEEE IJCNN
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Ding M., 2021, P IEEE CVF C COMP VI
   Dong X., 2019, P IEEE CVF C COMP VI
   Fang W, 2021, ADV NEUR IN, V34
   Fang WJ, 2021, IEEE INT SYMP INFO, P3261, DOI 10.1109/ISIT45174.2021.9517911
   Gerstner W., 2002, SPIKING NEURON MODEL, DOI DOI 10.1017/CBO9780511815706
   Guo J., 2020, P IEEE CVF C COMP VI
   He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI [10.1109/TPAMI.2018.2844175, 10.1109/ICCV.2017.322]
   He WH, 2020, NEURAL NETWORKS, V132, P108, DOI 10.1016/j.neunet.2020.08.001
   Kaiser J, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00424
   Kim J., 2020, INTERSPEECH
   Kim Seijoon, 2020, P AAAI C ART INT
   Kim Y., 2022, ARXIV220110355
   Kim Y., 2020, FRONT NEUROSCI-SWITZ, P1638
   Kim Y, 2021, NEURAL NETWORKS, V144, P686, DOI 10.1016/j.neunet.2021.09.022
   Kingma D. P., 2014, ARXIV, DOI DOI 10.48550/ARXIV.1412.6980
   Krizhevsky A., 2009, LEARNING MULTIPLE LA
   Lee C, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00119
   Li Guoqi, 2019, P AAAI C ART INT
   Li HM, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00309
   Li L., 2020, UNCERTAINTY ARTIFICI
   Lin M, 2014, PUBLIC HEALTH NUTR, V17, P2029, DOI [10.1017/S1368980013002176, 10.1109/PLASMA.2013.6634954]
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Netzer Y., 2011, NIPS WORKSH DEEP LEA, DOI DOI 10.2118/18761-MS
   Park S., 2019, ACM IEEE DES AUT C D
   Park S, 2020, DES AUT CON, DOI [10.1109/dac18072.2020.9218689, 10.1007/s00779-020-01476-2]
   Pellegrini T, 2021, IEEE W SP LANG TECH, P97, DOI 10.1109/SLT48900.2021.9383587
   Peng H., 2020, ADV NEURAL INFORM PR
   Pham H, 2018, PR MACH LEARN RES, V80
   Real E., 2019, P AAAI C ART INT, DOI DOI 10.1609/AAAI.V33I01.33014780
   Real E, 2017, PR MACH LEARN RES, V70
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474
   Sengupta A, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00095
   Shi XS, 2020, INT J COMPUT VISION, V128, P2307, DOI 10.1007/s11263-020-01299-x
   Simonyan K, 2015, Arxiv, DOI arXiv:1409.1556
   Simonyan Karen, 2019, ICLR
   Szegedy C., 2015, PROC CVPR IEEE, DOI DOI 10.1109/CVPR.2015.7298594
   Tan MX, 2019, PROC CVPR IEEE, P2815, DOI [arXiv:1807.11626, 10.1109/CVPR.2019.00293]
   Wu BC, 2019, PROC CVPR IEEE, P10726, DOI 10.1109/CVPR.2019.01099
   Xie L., 2020, WEIGHT SHARING NEURA
   Yan B., 2021, P IEEE CVF C COMP VI
   You S., 2020, P IEEE CVF C COMP VI
   Zhang M., 2020, INT JOINT C ART INT
   Zhang X., 2021, P IEEE CVF C COMP VI
   Zheng Hanle, 2021, P AAAI C ART INT
   Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907
   Zoph Barret, 2017, INT C LEARNING REPRE
NR 63
TC 0
Z9 0
U1 1
U2 1
PY 2022
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT J
AU Hoffmann, K
   Feucht, M
   Witte, H
   Benninger, F
   Bolten, J
AF Hoffmann, K
   Feucht, M
   Witte, H
   Benninger, F
   Bolten, J
TI Analysis and classification of interictal spike discharges in benign
   partial epilepsy of childhood on the basis of the Hilbert transformation
SO NEUROSCIENCE LETTERS
DT Article
DE epilepsy; childhood; rolandic spike; Hilbert transformation;
   instantaneous parameter; neural network
ID ROLANDIC SPIKES; MORPHOLOGY; EEG
AB The spatial distribution of instantaneous power during the occurrence of rolandic spikes (computed via Hilbert transformation) can be utilized for classification of topographically different spike types. By visual analysis of the instantaneous power maps we found seven characteristic spike classes which correspond to the results of visual analysis of potential distribution. A neural network (NN) was trained with representatives of these classes. Single instantaneous power maps of 55/56 visually selected rolandic spikes, as well as map sequences of averaged spike segments (on-line) of 17 patients, were correctly classified by means of NN. The sensitivity of the NN for spikes from unknown patients was 98%. The classification scheme enables an objective classification of (multi-) regional spikes for selective averaging and for studies dealing with syndrome classification.
C1 UNIV VIENNA, AKH, HOSP NEUROPSYCHIAT CHILDREN, VIENNA, AUSTRIA.
RP Hoffmann, K (corresponding author), UNIV JENA, INST MED STAT COMP SCI & DOCUMENTAT, JAHNSTR 3, D-07740 JENA, GERMANY.
CR [Anonymous], 1989, EPILEPSIA, V30, P389, DOI 10.1111/j.1528-1157.1989.tb05316.x
   ARNOLD M, 1996, IN PRESS J CLIN MONI
   DORSCHEL J, 1994, EEG-EMG-Z ELEK ELEKT, V25, P21
   DRURY I, 1991, EPILEPSIA, V32, P662, DOI 10.1111/j.1528-1157.1991.tb04706.x
   FEUCHT M, 1996, IN PRESS Z EEG EMG, V27
   FROST JD, 1992, EPILEPSIA, V33, P531, DOI 10.1111/j.1528-1157.1992.tb01705.x
   GREGORY DL, 1992, EPILEPSIA, V33, P36, DOI 10.1111/j.1528-1157.1992.tb02280.x
   GREGORY DL, 1984, EPILEPSIA, V25, P705, DOI 10.1111/j.1528-1157.1984.tb03481.x
   LEGARDA S, 1994, EPILEPSIA, V35, P1125, DOI 10.1111/j.1528-1157.1994.tb01777.x
   LEHMANN D, 1980, ELECTROEN CLIN NEURO, V48, P609, DOI 10.1016/0013-4694(80)90419-8
   UHL F, 1995, NEUROSCI LETT, V192, P177, DOI 10.1016/0304-3940(95)11639-E
   VANDERMEIJ W, 1993, EPILEPSIA, V34, P540
   VANDERMEIJ W, 1992, ELECTROEN CLIN NEURO, V82, P408, DOI 10.1016/0013-4694(92)90045-J
   VANDERMEIJ W, 1992, DEV MED CHILD NEUROL, V34, P893
   WEINBERG H, 1990, BRAIN TOPOGR, V3, P31
   WITTE H, 1991, MED BIOL ENG COMPUT, V29, P242, DOI 10.1007/BF02446705
   WITTE H, 1990, AUTOMEDICA, V13, P1
NR 17
TC 7
Z9 8
U1 0
U2 0
PD JUN 28
PY 1996
VL 211
IS 3
BP 195
EP 198
DI 10.1016/0304-3940(96)12754-3
WC Neurosciences
DA 2023-11-11
ER

PT J
AU Thibeault, CM
   O'Brien, MJ
   Srinivasa, N
AF Thibeault, Corey M.
   O'Brien, Michael J.
   Srinivasa, Narayan
TI Analyzing large-scale spiking neural data wth HRLAnalysis™
SO FRONTIERS IN NEUROINFORMATICS
DT Article
DE python; spiking neural data analysis; high-performance computing; spike
   train analysis; data sharing
ID INFORMATION; MODEL
AB The additional capabilities provided by high-performance neural simulation environments and modern computing hardware has allowed for the modeling of increasingly larger spiking neural networks. This is important for exploring more anatomically detailed networks but the corresponding accumulation in data can make analyzing the results of these simulations difficult. This is further compounded by the fact that many existing analysis packages were not developed with large spiking data sets in mind. Presented here is a software suite developed to not only process the increased amount of spike-train data in a reasonable amount of time, but also provide a user friendly Python interface. We describe the design considerations, implementation and features of the HRLAnalysis (TM) suite. In addition, performance benchmarks demonstrating the speedup of this design compared to a published Python implementation are also presented. The result is a high-performance analysis toolkit that is not only usable and readily extensible, but also straightforward to interface with existing Python modules.
C1 [Thibeault, Corey M.; O'Brien, Michael J.; Srinivasa, Narayan] HRL Labs LLC, Ctr Neural & Emergent Syst, Informat & Syst Sci Lab, Malibu, CA 90265 USA.
RP Thibeault, CM (corresponding author), HRL Labs LLC, Ctr Neural & Emergent Syst, Informat & Syst Sci Lab, 3011 Mailbu Canyon Dr, Malibu, CA 90265 USA.
EM cmthibeault@hrl.com
CR Anand R, 2009, SELF-DEFENSE IN INTERNATIONAL RELATIONS, P1, DOI 10.1057/9780230245747
   Crumiller M, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00090
   Cruz-Albrecht JM, 2012, IEEE T BIOMED CIRC S, V6, P246, DOI 10.1109/TBCAS.2011.2174152
   DARPA, 2012, SYN BROAD AG ANN BAA
   Dayan P., 2001, THEORETICAL NEUROSCI, DOI [DOI 10.1016/S0306-4522(00)00552-2, 10.1016/S0306-4522(00)00552-2]
   de la Rocha J, 2007, NATURE, V448, P802, DOI 10.1038/nature06028
   Eaton J.W., 2008, GNU OCTAVE MANUAL VE
   Eliasmith C, 2012, SCIENCE, V338, P1202, DOI 10.1126/science.1225266
   Garcia S, 2014, FRONT NEUROINFORM, V8, DOI 10.3389/fninf.2014.00010
   Garcia Samuel, 2009, Front Neuroinform, V3, P14, DOI 10.3389/neuro.11.014.2009
   Goldberg DH, 2009, NEUROINFORMATICS, V7, P165, DOI 10.1007/s12021-009-9049-y
   HANES DP, 1995, EXP BRAIN RES, V103, P85
   Ince Robin A A, 2009, Front Neuroinform, V3, P4, DOI 10.3389/neuro.11.004.2009
   Izhikevich EM, 2008, P NATL ACAD SCI USA, V105, P3593, DOI 10.1073/pnas.0712231105
   Kreuz T, 2013, J NEUROPHYSIOL, V109, P1457, DOI 10.1152/jn.00873.2012
   Magri C, 2009, BMC NEUROSCI, V10, DOI 10.1186/1471-2202-10-81
   Markram H, 2006, NAT REV NEUROSCI, V7, P153, DOI 10.1038/nrn1848
   Meier R, 2008, NEURAL NETWORKS, V21, P1085, DOI 10.1016/j.neunet.2008.06.019
   Minkovich K, 2014, IEEE T NEUR NET LEAR, V25, P316, DOI 10.1109/TNNLS.2013.2276056
   Pröpper R, 2013, FRONT NEUROINFORM, V7, DOI 10.3389/fninf.2013.00026
   Quiroga RQ, 2009, NAT REV NEUROSCI, V10, P173, DOI 10.1038/nrn2578
   Rubin JE, 2012, EUR J NEUROSCI, V36, P2213, DOI 10.1111/j.1460-9568.2012.08108.x
   Spacek Martin, 2008, Front Neuroinform, V2, P9, DOI 10.3389/neuro.11.009.2008
   Srinivasa N, 2012, IEEE PULSE, V3, P51, DOI 10.1109/MPUL.2011.2175639
   Stepanov A, 1995, STANDARD TEMPLATE LI, V1501
   Sutter H, 2004, C CODING STANDARDS 1
   Thibeault CM, 2013, FRONT COMPUT NEUROSC, V7, DOI 10.3389/fncom.2013.00088
   Walters JR, 2010, HBK BEHAV NEUROSCI, V20, P429
NR 28
TC 3
Z9 3
U1 0
U2 0
PD MAR 5
PY 2014
VL 8
WC Mathematical & Computational Biology; Neurosciences
DA 2023-11-11
ER

PT J
AU Chen, Y
   Liu, HW
   Shi, KX
   Zhang, ML
   Qu, H
AF Chen, Yi
   Liu, Hanwen
   Shi, Kexin
   Zhang, Malu
   Qu, Hong
TI Spiking neural network with working memory can integrate and rectify
   spatiotemporal features
SO FRONTIERS IN NEUROSCIENCE
DT Article
DE spiking neural network; working memory; convolutional neural network;
   CIFAR10; multi-dendrite
AB In the real world, information is often correlated with each other in the time domain. Whether it can effectively make a decision according to the global information is the key indicator of information processing ability. Due to the discrete characteristics of spike trains and unique temporal dynamics, spiking neural networks (SNNs) show great potential in applications in ultra-low-power platforms and various temporal-related real-life tasks. However, the current SNNs can only focus on the information a short time before the current moment, its sensitivity in the time domain is limited. This problem affects the processing ability of SNN in different kinds of data, including static data and time-variant data, and reduces the application scenarios and scalability of SNN. In this work, we analyze the impact of such information loss and then integrate SNN with working memory inspired by recent neuroscience research. Specifically, we propose Spiking Neural Networks with Working Memory (SNNWM) to handle input spike trains segment by segment. On the one hand, this model can effectively increase SNN's ability to obtain global information. On the other hand, it can effectively reduce the information redundancy between adjacent time steps. Then, we provide simple methods to implement the proposed network architecture from the perspectives of biological plausibility and neuromorphic hardware friendly. Finally, we test the proposed method on static and sequential data sets, and the experimental results show that the proposed model can better process the whole spike train, and achieve state-of-the-art results in short time steps. This work investigates the contribution of introducing biologically inspired mechanisms, e.g., working memory, and multiple delayed synapses to SNNs, and provides a new perspective to design future SNNs.
C1 [Chen, Yi; Liu, Hanwen; Shi, Kexin; Zhang, Malu; Qu, Hong] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu, Peoples R China.
RP Qu, H (corresponding author), Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu, Peoples R China.
EM hongqu@uestc.edu.cn
CR Amir A, 2017, PROC CVPR IEEE, P7388, DOI 10.1109/CVPR.2017.781
   Bohte S. M., 2000, 8th European Symposium on Artificial Neural Networks. ESANN"2000. Proceedings, P419
   Cubuk ED, 2019, PROC CVPR IEEE, P113, DOI 10.1109/CVPR.2019.00020
   Deng S., 2022, INT C LEARNING REPRE
   el Assal M., 2022, INT JOINT C NEUR NET, P1, DOI [10.1109/IJCNN55064.2022.9892063, DOI 10.1109/IJCNN55064.2022.9892063]
   Fang W, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P2641, DOI 10.1109/ICCV48922.2021.00266
   Fang Wei, 2020, SPIKINGJELLY
   Fang Wei, 2021, ADV NEURAL INFORM PR
   Huang CR, 2022, INT J PROD RES, V60, P493, DOI 10.1080/00207543.2021.1964706
   Kim R, 2021, NAT NEUROSCI, V24, P129, DOI 10.1038/s41593-020-00753-w
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
   Kwak Y, 2022, NEURON, V110, P1822, DOI 10.1016/j.neuron.2022.03.016
   Lagorce X, 2017, IEEE T PATTERN ANAL, V39, P1346, DOI 10.1109/TPAMI.2016.2574707
   Li HM, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00309
   Li JX, 2022, IEEE COMPUT INTELL M, V17, P27, DOI 10.1109/MCI.2022.3199623
   Lobo JL, 2020, NEURAL NETWORKS, V121, P88, DOI 10.1016/j.neunet.2019.09.004
   Luo XL, 2022, IEEE T NEUR NET LEAR, DOI 10.1109/TNNLS.2022.3164930
   Meng QY, 2022, PROC CVPR IEEE, P12434, DOI 10.1109/CVPR52688.2022.01212
   Pan ZH, 2021, IEEE-ACM T AUDIO SPE, V29, P2656, DOI 10.1109/TASLP.2021.3100684
   Pan ZH, 2020, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.01420
   Rathi N., 2020, 8 INT C LEARNING REP
   Rathi N, 2023, IEEE T NEUR NET LEAR, V34, P3174, DOI 10.1109/TNNLS.2021.3111897
   Sabater A, 2022, IEEE COMPUT SOC CONF, P2676, DOI 10.1109/CVPRW56347.2022.00301
   Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308
   Wang YX, 2022, IEEE T PATTERN ANAL, V44, P3436, DOI 10.1109/TPAMI.2021.3054886
   Wu JB, 2023, IEEE T NEUR NET LEAR, V34, P446, DOI 10.1109/TNNLS.2021.3095724
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Wu ZZ, 2022, IEEE T NEUR NET LEAR, V33, P6249, DOI 10.1109/TNNLS.2021.3073016
   Yao M, 2021, 2021 IEEE/CVF INTERNATIONAL CONFERENCE ON COMPUTER VISION (ICCV 2021), P10201, DOI 10.1109/ICCV48922.2021.01006
   Zhang W, 2021, INT J LAB HEMATOL, V43, P1041, DOI 10.1111/ijlh.13498
   Zhang WR, 2019, ADV NEUR IN, V32
   Zheng HL, 2021, AAAI CONF ARTIF INTE, V35, P11062
   Zhu L, 2022, PROC CVPR IEEE, P3584, DOI 10.1109/CVPR52688.2022.00358
NR 33
TC 0
Z9 0
U1 3
U2 3
PD JUN 14
PY 2023
VL 17
AR 1167134
DI 10.3389/fnins.2023.1167134
WC Neurosciences
DA 2023-11-11
ER

PT J
AU Tan, C
   Sarlija, M
   Kasabov, N
AF Tan, Clarence
   Sarlija, Marko
   Kasabov, Nikola
TI Spiking Neural Networks: Background, Recent Development and the NeuCube
   Architecture
SO NEURAL PROCESSING LETTERS
DT Article
DE Artificial neural networks; Spiking neural networks; Spike encoding;
   Spike-timing dependent plasticity; Spatio-temporal brain data; NeuCube
ID PATTERN-RECOGNITION; MODEL; INFORMATION; IMPULSES; NEURONS; STDP
AB This paper reviews recent developments in the still-off-the-mainstream information and data processing area of spiking neural networks (SNN)-the third generation of artificial neural networks. We provide background information about the functioning of biological neurons, discussing the most important and commonly used mathematical neural models. Most relevant information processing techniques, learning algorithms, and applications of spiking neurons are described and discussed, focusing on feasibility and biological plausibility of the methods. Specifically, we describe in detail the functioning and organization of the latest version of a 3D spatio-temporal SNN-based data machine framework called NeuCube, as well as it's SNN-related submodules. All described submodules are accompanied with formal algorithmic formulations. The architecture is highly relevant for the analysis and interpretation of various types of spatio-temporal brain data (STBD), like EEG, NIRS, fMRI, but we highlight some of the recent both STBD- and non-STBD-based applications. Finally, we summarise and discuss some open research problems that can be addressed in the future. These include, but are not limited to: application in the area of EEG-based BCI through transfer learning; application in the area of affective computing through the extension of the NeuCube framework which would allow for a biologically plausible SNN-based integration of central and peripheral nervous system measures. Matlab implementation of the NeuCube's SNN-related module is available for research and teaching purposes.
C1 [Tan, Clarence; Kasabov, Nikola] Auckland Univ Technol, Knowledge Engn & Discovery Res Inst, Private Bag 92006, Auckland 1010, New Zealand.
   [Sarlija, Marko] Univ Zagreb, Fac Elect Engn & Comp, Unska 3, Zagreb 10000, Croatia.
RP Sarlija, M (corresponding author), Univ Zagreb, Fac Elect Engn & Comp, Unska 3, Zagreb 10000, Croatia.
EM cltan@aut.ac.nz; marko.sarlija@fer.hr; nkasabov@aut.ac.nz
CR Abbott A, 2016, IEEE IJCNN, P1367, DOI 10.1109/IJCNN.2016.7727357
   Abbott LF, 1999, BRAIN RES BULL, V50, P303, DOI 10.1016/S0361-9230(99)00161-6
   Adrian ED, 1926, J PHYSIOL-LONDON, V61, P49, DOI 10.1113/jphysiol.1926.sp002273
   [Anonymous], 1988, COPLANAR STEREOTAXIC
   Bengio Yoshua, 2015, ARXIV150204156
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Bohte Sander M., 2004, Natural Computing, V3, P195, DOI 10.1023/B:NACO.0000027755.02868.60
   Brette R, 2015, FRONT SYST NEUROSCI, V9, DOI 10.3389/fnsys.2015.00151
   Buzsaki G, 2011, RHYTHMS BRAIN, DOI DOI 10.1093/ACPROF:OSO/9780195301069.001.0001
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Cassenaer S, 2007, NATURE, V448, P709, DOI 10.1038/nature05973
   Cirean D, 2017, ARXIV12022745
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Diehl Peter U, 2015, 2015 INT JOINT C NEU, P1, DOI [DOI 10.1109/IJCNN.2015.7280696, 10.1109/IJCNN.2015.7280696]
   Doborjeh ZG, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-27169-8
   Drubach D., 2000, THE BRAIN EXPLAINED
   Esser S.K., 2015, ADV NEURAL INFORM PR, P1117
   EVANS AC, 1993, NUCLEAR SCIENCE SYMPOSIUM & MEDICAL IMAGING CONFERENCE, VOLS 1-3, P1813, DOI 10.1109/NSSMIC.1993.373602
   FITZHUGH R, 1961, BIOPHYS J, V1, P445, DOI 10.1016/S0006-3495(61)86902-6
   FUKUSHIMA K, 1980, BIOL CYBERN, V36, P193, DOI 10.1007/BF00344251
   Ganapathy Nagarajan, 2018, Yearb Med Inform, V27, P98, DOI 10.1055/s-0038-1667083
   Gautrais J, 1998, BIOSYSTEMS, V48, P57, DOI 10.1016/S0303-2647(98)00050-1
   GERSTNER W, 1993, BIOL CYBERN, V69, P503, DOI 10.1007/BF01185422
   Gerstner W, 1996, NATURE, V383, P76, DOI 10.1038/383076a0
   Gerstner W., 2002, SPIKING NEURON MODEL
   Hannun A., 2014, DEEP SPEECH SCALING
   Herz AVM, 2006, SCIENCE, V314, P80, DOI 10.1126/science.1127240
   HINDMARSH JL, 1984, PROC R SOC SER B-BIO, V221, P87, DOI 10.1098/rspb.1984.0024
   Hirsch H. G., 2000, P ICSLP, P181
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V116, P449, DOI 10.1113/jphysiol.1952.sp004717
   Hu J, 2014, P IEEE RAS-EMBS INT, P409, DOI 10.1109/BIOROB.2014.6913811
   HUBEL DH, 1959, J PHYSIOL-LONDON, V148, P574, DOI 10.1113/jphysiol.1959.sp006308
   Huguenard JR, 2000, P NATL ACAD SCI USA, V97, P9349, DOI 10.1073/pnas.97.17.9349
   Hunsberger Eric, 2015, ARXIV151008829
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Jacob V, 2007, J NEUROSCI, V27, P1271, DOI 10.1523/JNEUROSCI.4264-06.2007
   Kasabov Nikola, 2012, Artificial Neural Networks in Pattern Recognition. Proceedings of the 5th INNS IAPR TC 3 GIRPR Workshop, ANNPR 2012, P225, DOI 10.1007/978-3-642-33212-8_21
   KASABOV N, 1998, METHODOLOGIES CONCEP, V1, P271
   Kasabov N., 2007, EVOLVING CONNECTIONI
   Kasabov N., 2018, SPRINGER SERIES BIOA, DOI 10.1007/978-3-662-57715-8
   Kasabov N, 2017, IEEE T COGN DEV SYST, V9, P293, DOI 10.1109/TCDS.2016.2636291
   Kasabov N, 2016, NEURAL NETWORKS, V78, P1, DOI 10.1016/j.neunet.2015.09.011
   Kasabov N, 2013, NEURAL NETWORKS, V41, P188, DOI 10.1016/j.neunet.2012.11.014
   Kasabov N, 2010, NEURAL NETWORKS, V23, P16, DOI 10.1016/j.neunet.2009.08.010
   Kasabov NK, 2017, IEEE T NEUR NET LEAR, V28, P887, DOI 10.1109/TNNLS.2016.2612890
   Kasabov NK, 2014, NEURAL NETWORKS, V52, P62, DOI 10.1016/j.neunet.2014.01.006
   Katsumata S, 2008, P ICONIP
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Koelstra S, 2012, IEEE T AFFECT COMPUT, V3, P18, DOI 10.1109/T-AFFC.2011.15
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
   Lapique L., 1907, J PHYSL PATHOL GEN, V9, P620, DOI DOI 10.1007/S00422-007-0189-6
   Lestienne R, 2001, PROG NEUROBIOL, V65, P545, DOI 10.1016/S0301-0082(01)00019-3
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Merolla P, 2011, IEEE CUST INTEGR CIR
   Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642
   Mohemmed A, 2011, IFIP ADV INF COMM TE, V363, P219
   MORRIS C, 1981, BIOPHYS J, V35, P193, DOI 10.1016/S0006-3495(81)84782-0
   Mu Y, 2006, NEURON, V50, P115, DOI 10.1016/j.neuron.2006.03.009
   Nuntalid N, 2011, LECT NOTES COMPUT SC, V7062, P451, DOI 10.1007/978-3-642-24955-6_54
   O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178
   O'Reilly C, 2014, J SLEEP RES, V23, P628, DOI 10.1111/jsr.12169
   Othman M, 2014, IEEE IJCNN, P3197, DOI 10.1109/IJCNN.2014.6889709
   Ouyang WL, 2013, IEEE I CONF COMP VIS, P2056, DOI 10.1109/ICCV.2013.257
   Paulun L, 2018, FRONT COMPUT NEUROSC, V12, DOI 10.3389/fncom.2018.00042
   Petro B, 2020, IEEE T NEUR NET LEAR, V31, P358, DOI 10.1109/TNNLS.2019.2906158
   ROSENBLATT F, 1958, PSYCHOL REV, V65, P386, DOI 10.1037/h0042519
   Rueckauer B., 2016, ARXIV161204052
   Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y
   Sarlija M, 2017, INT SYMP IMAGE SIG, P121, DOI 10.1109/ISPA.2017.8073581
   Sengupta N, 2018, IEEE T NEUR NET LEAR, V29, P5249, DOI 10.1109/TNNLS.2018.2796023
   Sengupta N, 2017, INFORM SCIENCES, V406, P133, DOI 10.1016/j.ins.2017.04.017
   Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961
   Soleymani M, 2012, IEEE T AFFECT COMPUT, V3, P42, DOI 10.1109/T-AFFC.2011.25
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Tavanaei A, 2017, NEUROCOMPUTING, V240, P191, DOI 10.1016/j.neucom.2017.01.088
   Taylor D, 2014, IEEE IJCNN, P3221, DOI 10.1109/IJCNN.2014.6889936
   Thorpe S, 1998, COMPUTATIONAL NEUROSCIENCE: TRENDS IN RESEARCH, P113
   THORPE SJ, 1990, PARALLEL PROCESSING IN NEURAL SYSTEMS AND COMPUTERS, P91
   Trentin E, 2018, NEURAL PROCESS LETT, V48, P643, DOI 10.1007/s11063-018-9830-8
   Wilson CJ, 2000, J NEUROPHYSIOL, V83, P3084, DOI 10.1152/jn.2000.83.5.3084
   Wysoski SG, 2010, NEURAL NETWORKS, V23, P819, DOI 10.1016/j.neunet.2010.04.009
NR 82
TC 17
Z9 17
U1 4
U2 44
PD OCT
PY 2020
VL 52
IS 2
SI SI
BP 1675
EP 1701
DI 10.1007/s11063-020-10322-8
EA AUG 2020
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT J
AU Kaneko, Y
   Nishitani, Y
   Ueda, M
AF Kaneko, Yukihiro
   Nishitani, Yu
   Ueda, Michihito
TI Ferroelectric Artificial Synapses for Recognition of a Multishaded Image
SO IEEE TRANSACTIONS ON ELECTRON DEVICES
DT Article
DE Artificial neural network; ferroelectric devices; memristor; neural
   network hardware; nonvolatile memory; pattern recognition
ID FIELD-EFFECT TRANSISTOR; NEURAL-NETWORKS; MEMORY; BRAIN
AB We demonstrate, for the first time, the on-chip pattern recognition of a multishaded grayscale image in a neural network circuit with multiple neurons. This pattern recognition is based on a spiking neural network model that uses multiple three-terminal ferroelectric memristors (3T-FeMEMs) as synapses. The synapse chip of the neural network is formed by stacking CMOS circuits and 3T-FeMEMs. The conductance of the 3T-FeMEM is gradually changed in the linear range by varying the amplitude of the applied voltage pulse. Using the analog and nonvolatile conductance change of the 3T-FeMEM as synaptic weight, the matrix patterns are learned after the spike timing-dependent plasticity learning rule. Even when an incomplete multishaded pattern is input to the neural network circuit, it automatically completes and recalls a previously learned pattern.
C1 [Kaneko, Yukihiro; Nishitani, Yu; Ueda, Michihito] Panasonic Corp, Adv Technol Res Labs, Kyoto 6190237, Japan.
RP Kaneko, Y (corresponding author), Panasonic Corp, Adv Technol Res Labs, Kyoto 6190237, Japan.
EM kaneko.yukihiro001@jp.panasonic.com; nishitani.yu@jp.panasonic.com;
   ueda.michihito@jp.panasonic.com
CR Abbott LF, 2000, NAT NEUROSCI, V3, P1178, DOI 10.1038/81453
   Alibart F, 2013, NAT COMMUN, V4, DOI 10.1038/ncomms3072
   Anand R, 2009, SELF-DEFENSE IN INTERNATIONAL RELATIONS, P1, DOI 10.1057/9780230245747
   [Anonymous], J APPL PHYS
   EDELMAN GM, 1982, P NATL ACAD SCI-BIOL, V79, P2091, DOI 10.1073/pnas.79.6.2091
   Furber S, 2012, IEEE SPECTRUM, V49, P44, DOI 10.1109/MSPEC.2012.6247562
   Geestner W., 2002, SPIKING NEURON MODEL
   HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554
   Hopfield JJ, 1999, REV MOD PHYS, V71, pS431, DOI 10.1103/RevModPhys.71.S431
   Ishiwara H, 2009, CURR APPL PHYS, V9, pS2, DOI 10.1016/j.cap.2008.02.013
   Kandel E, 2000, PRINCIPLES NEURAL SC
   Kandel ER, 2013, NAT REV NEUROSCI, V14, P659, DOI 10.1038/nrn3578
   Kaneko Yukihiro, 2013, 2013 Symposium on VLSI Technology, pT238
   Kaneko Y., 2011, J APPL PHYS, V110
   Kaneko Y, 2011, APPL PHYS LETT, V99, DOI 10.1063/1.3657413
   Kaneko Y, 2011, IEEE T ELECTRON DEV, V58, P1311, DOI 10.1109/TED.2011.2110653
   Kato Y, 2008, JPN J APPL PHYS, V47, P2719, DOI 10.1143/JJAP.47.2719
   Kuzum D, 2013, NANOTECHNOLOGY, V24, DOI 10.1088/0957-4484/24/38/382001
   Kuzum D, 2012, NANO LETT, V12, P2179, DOI 10.1021/nl201040y
   Lai QX, 2010, ADV MATER, V22, P2448, DOI 10.1002/adma.201000282
   Ma TP, 2002, IEEE ELECTR DEVICE L, V23, P386, DOI 10.1109/LED.2002.1015207
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Markram H, 2006, NAT REV NEUROSCI, V7, P153, DOI 10.1038/nrn1848
   Nishitani Y, 2012, J APPL PHYS, V111, DOI 10.1063/1.4729915
   Nishitani Y, 2013, JPN J APPL PHYS, V52, DOI 10.7567/JJAP.52.04CE06
   Nishiyama M, 2000, NATURE, V408, P584, DOI 10.1038/35046067
   Park S, 2013, NANOTECHNOLOGY, V24, DOI 10.1088/0957-4484/24/38/384009
   Preissl R, 2012, INT CONF HIGH PERFOR
   Ross I. M., 1957, U.S. Patent, Patent No. [2791760A, 2791760]
   Shi J, 2013, NAT COMMUN, V4, DOI 10.1038/ncomms3676
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Strukov DB, 2008, NATURE, V453, P80, DOI 10.1038/nature06932
   Suri M, 2012, J APPL PHYS, V112, DOI 10.1063/1.4749411
   Tanaka H, 2008, JPN J APPL PHYS, V47, P7527, DOI 10.1143/JJAP.47.7527
   THAKOOR S, 1990, J APPL PHYS, V67, P3132, DOI 10.1063/1.345390
   Yu SM, 2011, IEEE T ELECTRON DEV, V58, P2729, DOI 10.1109/TED.2011.2147791
NR 36
TC 117
Z9 118
U1 6
U2 139
PD AUG
PY 2014
VL 61
IS 8
BP 2827
EP 2833
DI 10.1109/TED.2014.2331707
WC Engineering, Electrical & Electronic; Physics, Applied
DA 2023-11-11
ER

PT J
AU Berlin, SJ
   John, M
AF Berlin, S. Jeba
   John, Mala
TI Spiking neural network based on joint entropy of optical flow features
   for human action recognition
SO VISUAL COMPUTER
DT Article
DE Computer vision; Human action recognition; PWCNet; BindsNet; Spiking
   neural network
ID SPATIOTEMPORAL FEATURES; MODEL
AB In the recent past, human action recognition is inviting increased attention in the automated video surveillance systems. An efficient human action classification technique in an unconstrained environment is proposed in this paper. A novel descriptor relying on joint entropy of difference in magnitude and orientation of the optical flow feature is developed in order to model human actions. Initially, flow feature is computed using Pyramid-Warping-Cost volume Network (PWCNet), considering every two consecutive frames. Then, the feature descriptor is formed based on the joint entropy of difference in flow magnitude and orientation collected from the regular grid of each frame in the action sequence. Finally, in order to incorporate long-term temporal dependency, a spiking neural network is embedded to aggregate the information across the frames. Different optimization techniques and different types of hidden nodes are utilized in the spiking neural network to analyze the performance of the proposed work. Extensive experiments on the benchmark datasets for human action recognition show the efficacy of the proposed method.
C1 [Berlin, S. Jeba; John, Mala] Anna Univ, Madras Inst Technol, Dept Elect Engn, Chennai, Tamil Nadu, India.
RP Berlin, SJ (corresponding author), Anna Univ, Madras Inst Technol, Dept Elect Engn, Chennai, Tamil Nadu, India.
EM jebaberlin@mitindia.edu
CR Aggarwal JK, 2011, ACM COMPUT SURV, V43, DOI 10.1145/1922649.1922653
   Ali S, 2010, IEEE T PATTERN ANAL, V32, P288, DOI 10.1109/TPAMI.2008.284
   [Anonymous], 2013, ICCV WORKSHOP THUMOS
   Ballan L, 2012, IEEE T MULTIMEDIA, V14, P1234, DOI 10.1109/TMM.2012.2191268
   Berlin SJ, 2020, J INTELL FUZZY SYST, V39, P961, DOI 10.3233/JIFS-191914
   Berlin SJ, 2016, INT CARN CONF SECU, P143
   Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16
   Cai JH, 2020, VISUAL COMPUT, V36, P1261, DOI 10.1007/s00371-019-01733-3
   Chen EQ, 2019, IEEE ACCESS, V7, P57267, DOI 10.1109/ACCESS.2019.2910604
   Chen M, 2011, INT CONF CLOUD COMPU, P316, DOI 10.1109/CCIS.2011.6045082
   Chun S, 2016, IET COMPUT VIS, V10, P250, DOI 10.1049/iet-cvi.2015.0233
   Das Dawn D, 2016, VISUAL COMPUT, V32, P289, DOI 10.1007/s00371-015-1066-2
   Dhoble K., 2012, P INT JOINT C NEUR N, P1, DOI 10.1109/IJCNN.2012.6252439
   Diehl PU, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00099
   Duchi J, 2011, J MACH LEARN RES, V12, P2121
   El-Ghaish HE, 2018, IEEE ACCESS, V6, P49040, DOI 10.1109/ACCESS.2018.2868319
   Fan DP, 2019, PROC CVPR IEEE, P8546, DOI 10.1109/CVPR.2019.00875
   Gao RH, 2018, PROC CVPR IEEE, P5937, DOI 10.1109/CVPR.2018.00622
   Gao YB, 2018, IEEE ACCESS, V6, P52277, DOI 10.1109/ACCESS.2018.2869790
   Hazan H, 2018, FRONT NEUROINFORM, V12, DOI 10.3389/fninf.2018.00089
   Holte MB, 2012, IEEE J-STSP, V6, P553, DOI 10.1109/JSTSP.2012.2193556
   Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243
   Huang YK, 2020, IEEE ACCESS, V8, P45753, DOI 10.1109/ACCESS.2020.2978223
   Hui TW, 2018, PROC CVPR IEEE, P8981, DOI 10.1109/CVPR.2018.00936
   Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179
   Jain M, 2013, PROC CVPR IEEE, P2555, DOI 10.1109/CVPR.2013.330
   Jhuang H, 2007, IEEE I CONF COMP VIS, P1253
   Kingma D. P., 2014, ARXIV, DOI DOI 10.48550/ARXIV.1412.6980
   Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543
   Ladjailia A, 2020, NEURAL COMPUT APPL, V32, P16387, DOI 10.1007/s00521-018-3951-x
   Lee JM, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00191
   Li Q, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P159, DOI 10.1145/2911996.2912001
   Li WH, 2018, IEEE ACCESS, V6, P44211, DOI 10.1109/ACCESS.2018.2863943
   Li X, 2007, ELECTRON LETT, V43, P560, DOI 10.1049/el:20070027
   Liu CC, 2021, VISUAL COMPUT, V37, P1327, DOI 10.1007/s00371-020-01868-8
   Liu HH, 2018, IEEE T NEUR NET LEAR, V29, P1427, DOI 10.1109/TNNLS.2017.2669522
   Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94
   Meng Y, 2011, IEEE T NEURAL NETWOR, V22, P1952, DOI 10.1109/TNN.2011.2171044
   Nikouei SY, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON EDGE COMPUTING (IEEE EDGE), P125, DOI 10.1109/EDGE.2018.00025
   Ranjan A, 2017, PROC CVPR IEEE, P2720, DOI 10.1109/CVPR.2017.291
   Sahoo SP, 2019, IET IMAGE PROCESS, V13, P983, DOI 10.1049/iet-ipr.2018.6045
   Soomro K., 2012, ARXIV12120402
   Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931
   Sun L, 2015, IEEE I CONF COMP VIS, P4597, DOI 10.1109/ICCV.2015.522
   Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26, DOI DOI 10.1007/S12654-012-0173-1
   Tong M, 2020, NEURAL COMPUT APPL, V32, P5285, DOI 10.1007/s00521-019-04030-1
   Tu ZG, 2019, IEEE T IMAGE PROCESS, V28, P2799, DOI 10.1109/TIP.2018.2890749
   Ullah A, 2019, IEEE T IND ELECTRON, V66, P9692, DOI 10.1109/TIE.2018.2881943
   Varol G, 2018, IEEE T PATTERN ANAL, V40, P1510, DOI 10.1109/TPAMI.2017.2712608
   Vishwakarma DK, 2019, VISUAL COMPUT, V35, P1595, DOI 10.1007/s00371-018-1560-4
   Wan YQ, 2020, IEEE ACCESS, V8, P85284, DOI 10.1109/ACCESS.2020.2993227
   Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441
   Wang HS, 2018, IEEE T CIRC SYST VID, V28, P2908, DOI 10.1109/TCSVT.2017.2746092
   Wang L., 2007, COMPUTER VISION PATT, P1, DOI [10.1109/CVPR.2007.383298, DOI 10.1109/CVPR.2007.383298]
   Wang L, 2018, IEEE ACCESS, V6, P17913, DOI 10.1109/ACCESS.2018.2817253
   Wang LM, 2015, PROC CVPR IEEE, P4305, DOI 10.1109/CVPR.2015.7299059
   Wang XH, 2018, IEEE T MULTIMEDIA, V20, P634, DOI 10.1109/TMM.2017.2749159
   Wang YY, 2019, IEEE SIGNAL PROC LET, V26, P1556, DOI 10.1109/LSP.2019.2940111
   Yan CG, 2018, IEEE T INTELL TRANSP, V19, P284, DOI 10.1109/TITS.2017.2749965
   Yao GL, 2019, APPL INTELL, V49, P2017, DOI 10.1007/s10489-018-1347-3
   Yi Y, 2018, VISUAL COMPUT, V34, P391, DOI 10.1007/s00371-016-1345-6
   Yu J, 2020, VISUAL COMPUT, V36, P1457, DOI 10.1007/s00371-019-01751-1
   Yu S, 2020, IEEE ACCESS, V8, P1840, DOI 10.1109/ACCESS.2019.2962284
   Zhang H, 2018, MACH VISION APPL, V29, P1127, DOI 10.1007/s00138-018-0956-5
   Zhao JX, 2019, IEEE I CONF COMP VIS, P8778, DOI 10.1109/ICCV.2019.00887
   Zhu L, 2018, IEEE T NEUR NET LEAR, V29, P5264, DOI 10.1109/TNNLS.2018.2797248
NR 66
TC 8
Z9 8
U1 2
U2 13
PD JAN
PY 2022
VL 38
IS 1
BP 223
EP 237
DI 10.1007/s00371-020-02012-2
EA DEC 2020
WC Computer Science, Software Engineering
DA 2023-11-11
ER

PT C
AU Bellec, G
   Wang, SQ
   Modirshanechi, A
   Brea, J
   Gerstner, W
AF Bellec, Guillaume
   Wang, Shuqi
   Modirshanechi, Alireza
   Brea, Johanni
   Gerstner, Wulfram
BE Ranzato, M
   Beygelzimer, A
   Dauphin, Y
   Liang, PS
   Vaughan, JW
TI Fitting summary statistics of neural data with a differentiable spiking
   network simulator
SO ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 34 (NEURIPS 2021)
SE Advances in Neural Information Processing Systems
DT Proceedings Paper
CT 35th Conference on Neural Information Processing Systems (NeurIPS)
CY DEC 06-14, 2021
CL ELECTR NETWORK
ID ADAPTATION; POWER
AB Fitting network models to neural activity is an important tool in neuroscience. A popular approach is to model a brain area with a probabilistic recurrent spiking network whose parameters maximize the likelihood of the recorded activity. Although this is widely used, we show that the resulting model does not produce realistic neural activity. To correct for this, we suggest to augment the log-likelihood with terms that measure the dissimilarity between simulated and recorded activity. This dissimilarity is defined via summary statistics commonly used in neuroscience and the optimization is efficient because it relies on back-propagation through the stochastically simulated spike trains. We analyze this method theoretically and show empirically that it generates more realistic activity statistics. We find that it improves upon other fitting algorithms for spiking network models like GLMs (Generalized Linear Models) which do not usually rely on back-propagation. This new fitting algorithm also enables the consideration of hidden neurons which is otherwise notoriously hard, and we show that it can be crucial when trying to infer the network connectivity from spike recordings.
C1 [Bellec, Guillaume; Wang, Shuqi; Modirshanechi, Alireza; Brea, Johanni; Gerstner, Wulfram] Ecole Polytech Fed Lausanne EPFL, Lab Computat Neurosci, Lausanne, Switzerland.
RP Bellec, G (corresponding author), Ecole Polytech Fed Lausanne EPFL, Lab Computat Neurosci, Lausanne, Switzerland.
EM guillaume.bellec@epfl.ch; shuqi.wang@epfl.ch;
   alireza.modirshanechi@epfl.ch; johanni.brea@epfl.ch;
   wulfram.gerstner@epfl.ch
CR [Anonymous], 2011, ADV NEURAL INF PROCE
   Arribas Diego M, 2020, NEURIPS 2020, V2020
   Bellec G., 2018, ADV NEURAL INFORM PR, P787
   BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181
   Bengio Yoshua, 2013, ABS13083432 CORR
   Bittner Sean R, 2019, BIORXIV
   Brea J, 2013, J NEUROSCI, V33, P9565, DOI 10.1523/JNEUROSCI.4098-12.2013
   Cranmer K, 2020, P NATL ACAD SCI USA, V117, P30055, DOI 10.1073/pnas.1912789117
   Das A, 2020, NAT NEUROSCI, V23, P1286, DOI 10.1038/s41593-020-0699-2
   Deny Stephane, 2017, NATURE COMMUNICATION, V8, P1
   Gerhard F, 2017, PLOS COMPUT BIOL, V13, DOI 10.1371/journal.pcbi.1005390
   Gerhard F, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003138
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   GERSTNER W, 1995, PHYS REV E, V51, P738, DOI 10.1103/PhysRevE.51.738
   Goncalves Pedro J, 2020, BIORXIV
   Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622
   Hocker D, 2017, I IEEE EMBS C NEUR E, P613, DOI 10.1109/NER.2017.8008426
   Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178
   Kindel WF, 2019, J VISION, V19, DOI 10.1167/19.4.29
   Kobayashi R, 2019, NAT COMMUN, V10, DOI 10.1038/s41467-019-12225-2
   Kohn Adam, 2016, UTAH ARRAY EXTRACELL
   Lawhern V, 2010, J NEUROSCI METH, V189, P267, DOI 10.1016/j.jneumeth.2010.03.024
   Li YJ, 2015, PR MACH LEARN RES, V37, P1718
   Lueckmann J.-M., 2017, ADV NEURAL INFORM PR, P1289
   Macke Jakob H, 2012, ADV NEURAL INFORM PR, V24, P1350
   Maddison C., 2017, 5 INT C LEARNING REP, P1
   Mahuas Gabriel., 2020, ADV NEURAL INFORM PR
   McIntosh LT, 2016, ADV NEUR IN, V29
   Mensi S, 2012, J NEUROPHYSIOL, V107, P1756, DOI 10.1152/jn.00408.2011
   Molano-Mazon Manuel, 2018, ARXIV180300338
   Neftci EO, 2019, IEEE SIGNAL PROC MAG, V36, P51, DOI 10.1109/MSP.2019.2931595
   Paninski L, 2004, NETWORK-COMP NEURAL, V15, P243, DOI 10.1088/0954-898X/15/4/002
   Papamakarios G., 2016, ADV NEURAL INFORM PR, P1028
   Pillow JW, 2008, NATURE, V454, P995, DOI 10.1038/nature07140
   Pillow Jonathan W, 2008, ADV NEURAL INFORM PR, V20
   Pozzorini C, 2013, NAT NEUROSCI, V16, P942, DOI 10.1038/nn.3431
   Raiko T., 2014, ARXIV14062989
   Ramesh Poornima, 2019, NEURIPS WORKSH 2019
   Rezende DJ, 2014, FRONT COMPUT NEUROSC, V8, DOI 10.3389/fncom.2014.00038
   Rezende Danilo Jimenez, 2014, ICML
   Rikhye RV, 2018, NAT NEUROSCI, V21, P1753, DOI 10.1038/s41593-018-0269-z
   Roberts, 2020, ARXIV200104643, P1
   Runyan CA, 2017, NATURE, V548, P92, DOI 10.1038/nature23020
   Siegle JH, 2021, NATURE, V592, P86, DOI 10.1038/s41586-020-03171-x
   Smith MA, 2008, J NEUROSCI, V28, P12591, DOI 10.1523/JNEUROSCI.2929-08.2008
   Sorochynskyi O, 2021, PLOS COMPUT BIOL, V17, DOI 10.1371/journal.pcbi.1008501
   Sussillo D, 2016, ARXIV160806315
   Teeter C, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-017-02717-4
   Tucker G, 2017, ADV NEUR IN, V30
   Turag Srini, 2014, NIPS
NR 50
TC 0
Z9 0
U1 0
U2 0
PY 2021
VL 34
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT C
AU Dora, S
   Sundaram, S
   Sundararajan, N
AF Dora, Shirin
   Sundaram, Suresh
   Sundararajan, Narasimhan
GP IEEE
TI A Two Stage Learning Algorithm for a Growing-Pruning Spiking Neural
   Network for Pattern Classification Problems
SO 2015 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUL 12-17, 2015
CL Killarney, IRELAND
ID NEURONS
AB This paper presents a two stage learning algorithm for a Growing-Pruning Spiking Neural Network (GPSNN) for pattern classification problems. The GPSNN uses three layered network architecture with input layer employing a modified population coding and, leaky integrate-and-fire spiking neurons in the hidden and output layers. The class label for a sample is determined according to the output neuron with minimum spike latency. The learning algorithm for the GPSNN employs a two stage learning mechanism. In the first stage, the hidden layer is grown and adapted to map the inputs to a hyperdimensional space. In the second stage, the hidden layer neurons with low dominance are pruned and the response of the most dominant neurons is mapped to the output space. The proposed approach has been evaluated on benchmark data sets from the UCI machine learning repository and the results were compared with batch as well as online spiking neural networks. The results clearly highlight that the GPSNN can achieve better performances using a compact network structure.
C1 [Dora, Shirin; Sundaram, Suresh] Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
   [Sundararajan, Narasimhan] Nanyang Technol Univ, Sch Elect & Elect Engn, Singapore 639798, Singapore.
RP Dora, S (corresponding author), Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
EM shirin002@ntu.edu.sg; ssundaram@ntu.edu.sg; ensundara@ntu.edu.sg
CR Abbott LF, 1999, BRAIN RES BULL, V50, P303, DOI 10.1016/S0361-9230(99)00161-6
   [Anonymous], 2013, INT JOINT C NEUR NET, DOI DOI 10.1109/AGILE.2013.7
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Bohte SM, 2002, IEEE T NEURAL NETWOR, V13, P426, DOI 10.1109/72.991428
   Butts DA, 2007, NATURE, V449, P92, DOI [10.1038/nature06105, 10.1038/natureO6105]
   Dora S, 2014, IEEE IJCNN, P2415, DOI 10.1109/IJCNN.2014.6889775
   Florian RV, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0040233
   Ghosh-Dastidar S, 2007, INTEGR COMPUT-AID E, V14, P187
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   Gütig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643
   LICHMAN M., 2013, UCI MACHINE LEARNING
   Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901
   Suresh S, 2008, INFORM SCIENCES, V178, P2621, DOI 10.1016/j.ins.2008.02.009
   Thorpe S, 1998, COMPUTATIONAL NEUROSCIENCE: TRENDS IN RESEARCH, P113
   THORPE SJ, 1989, CONNECTIONISM IN PERSPECTIVE, P63
   Van Rullen R, 2001, NEURAL COMPUT, V13, P1255, DOI 10.1162/08997660152002852
   VanRullen R, 2005, TRENDS NEUROSCI, V28, P1, DOI 10.1016/j.tins.2004.10.010
   Wade JJ, 2010, IEEE T NEURAL NETWOR, V21, P1817, DOI 10.1109/TNN.2010.2074212
   Wang JL, 2014, NEUROCOMPUTING, V144, P526, DOI 10.1016/j.neucom.2014.04.017
   Wysoski SG, 2008, NEUROCOMPUTING, V71, P2563, DOI 10.1016/j.neucom.2007.12.038
   Xu Y, 2013, NEURAL NETWORKS, V43, P99, DOI 10.1016/j.neunet.2013.02.003
NR 21
TC 0
Z9 0
U1 0
U2 0
PY 2015
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Zhan, QG
   Liu, GS
   Xie, XR
   Sun, GL
   Tang, HJ
AF Zhan, Qiugang
   Liu, Guisong
   Xie, Xiurui
   Sun, Guolin
   Tang, Huajin
TI Effective Transfer Learning Algorithm in Spiking Neural Networks
SO IEEE TRANSACTIONS ON CYBERNETICS
DT Article
DE Transfer learning; Training; Neurons; Kernel; Feature extraction;
   Biological neural networks; Membrane potentials; Centered kernel
   alignment; deep learning; spiking neural network (SNN); transfer
   learning
ID DEPENDENCE; DESIGN
AB As the third generation of neural networks, spiking neural networks (SNNs) have gained much attention recently because of their high energy efficiency on neuromorphic hardware. However, training deep SNNs requires many labeled data that are expensive to obtain in real-world applications, as traditional artificial neural networks (ANNs). In order to address this issue, transfer learning has been proposed and widely used in traditional ANNs, but it has limited use in SNNs. In this article, we propose an effective transfer learning framework for deep SNNs based on the domain in-variance representation. Specifically, we analyze the rationality of centered kernel alignment (CKA) as a domain distance measurement relative to maximum mean discrepancy (MMD) in deep SNNs. In addition, we study the feature transferability across different layers by testing on the Office-31, Office-Caltech-10, and PACS datasets. The experimental results demonstrate the transferability of SNNs and show the effectiveness of the proposed transfer learning framework by using CKA in SNNs.
C1 [Zhan, Qiugang; Xie, Xiurui; Sun, Guolin] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 611731, Peoples R China.
   [Liu, Guisong] Southwestern Univ Finance & Econ, Econ Informat Engn Sch, Chengdu 611130, Peoples R China.
   [Liu, Guisong] Univ Elect Sci & Technol China, Zhongshan Inst, Zhongshan 528400, Peoples R China.
   [Tang, Huajin] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310027, Peoples R China.
   [Tang, Huajin] Inst Artificial Intelligence, Zhejiang Lab, Hangzhou 311122, Peoples R China.
RP Liu, GS (corresponding author), Southwestern Univ Finance & Econ, Econ Informat Engn Sch, Chengdu 611130, Peoples R China.
EM lgs@uestc.edu.cn
CR Ahmed FYH, 2013, MATH PROBL ENG, V2013, DOI 10.1155/2013/257085
   Akopyan F, 2015, IEEE T COMPUT AID D, V34, P1537, DOI 10.1109/TCAD.2015.2474396
   Meda-Campaña JA, 2018, IEEE ACCESS, V6, P31968, DOI 10.1109/ACCESS.2018.2846483
   Aquino G, 2020, IEEE ACCESS, V8, P46324, DOI 10.1109/ACCESS.2020.2979141
   Bellec G., 2018, ADV NEURAL INFORM PR, P787
   Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006
   Bohte S. M., 2000, 8th European Symposium on Artificial Neural Networks. ESANN"2000. Proceedings, P419
   Chiang HS, 2019, IEEE ACCESS, V7, P103255, DOI 10.1109/ACCESS.2019.2929266
   Cortes C, 2012, J MACH LEARN RES, V13, P795
   Cristianini N, 2002, ADV NEUR IN, V14, P367
   Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359
   Rubio JD, 2009, IEEE T FUZZY SYST, V17, P1296, DOI 10.1109/TFUZZ.2009.2029569
   Dominguez-Morales J P., 2018, 2018 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2018.8489381
   Elias I, 2020, APPL SCI-BASEL, V10, DOI 10.3390/app10124239
   Furber SB, 2014, P IEEE, V102, P652, DOI 10.1109/JPROC.2014.2304638
   Ganin Y, 2016, J MACH LEARN RES, V17
   Ghifary M, 2014, LECT NOTES ARTIF INT, V8862, P898, DOI 10.1007/978-3-319-13560-1_76
   Girshick R., 2014, P 2014 IEEE C COMP V, P580, DOI DOI 10.1109/CVPR.2014.81
   Gretton A, 2005, LECT NOTES ARTIF INT, V3734, P63
   Gretton A, 2012, J MACH LEARN RES, V13, P723
   Griffin, 2007, CALTECH 256 OBJECT C
   Guo SQ, 2019, IEEE T CYBERNETICS, V49, P133, DOI 10.1109/TCYB.2017.2768554
   Hernández G, 2020, NEUROCOMPUTING, V390, P327, DOI 10.1016/j.neucom.2019.08.095
   Huang Q, 2019, IEEE I CONF COMP VIS, P4732, DOI 10.1109/ICCV.2019.00483
   Jing MM, 2021, IEEE T CYBERNETICS, V51, P3390, DOI 10.1109/TCYB.2020.2974106
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Köksoy O, 2006, APPL MATH COMPUT, V175, P1716, DOI 10.1016/j.amc.2005.09.016
   Kornblith S., 2019, SIMILARITY NEURAL NE
   Krizhevsky Alex, 2017, Communications of the ACM, V60, P84, DOI 10.1145/3065386
   Li D, 2017, IEEE I CONF COMP VIS, P5543, DOI 10.1109/ICCV.2017.591
   Li S, 2018, FRONT INFORM TECH EL, V19, P91, DOI 10.1631/FITEE.1700774
   Long MS, 2017, PR MACH LEARN RES, V70
   Long MS, 2018, ADV NEUR IN, V31
   Long MS, 2015, PR MACH LEARN RES, V37, P97
   Long MS, 2016, IEEE T KNOWL DATA EN, V28, P2027, DOI 10.1109/TKDE.2016.2554549
   Nadiah N., 2018, THESIS U TEKNOLOGI M
   Nguyen BH, 2018, GECCO'18: PROCEEDINGS OF THE 2018 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P37, DOI 10.1145/3205455.3205540
   Pan SJ, 2011, IEEE T NEURAL NETWOR, V22, P199, DOI 10.1109/TNN.2010.2091281
   Qiu SW, 2018, LECT NOTES COMPUT SC, V11257, P344, DOI 10.1007/978-3-030-03335-4_30
   Roy K, 2019, NATURE, V575, P607, DOI 10.1038/s41586-019-1677-2
   Ruder S, 2016, ARXIV160904747, P1
   Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16
   Sanodiya RK, 2020, APPL INTELL, V50, P3071, DOI 10.1007/s10489-020-01710-7
   SARA U, 2019, J COMPUT COMMUN, V7, P8, DOI [10.4236/jcc.2019.73002, DOI 10.4236/JCC.2019.73002]
   Shrestha SB, 2018, ADV NEUR IN, V31
   Shrestha SB, 2015, NEURAL NETWORKS, V63, P185, DOI 10.1016/j.neunet.2014.12.001
   Shrestha SB, 2017, NEURAL NETWORKS, V86, P54, DOI 10.1016/j.neunet.2016.10.011
   Song L, 2012, J MACH LEARN RES, V13, P1393
   Szegedy C, 2014, Arxiv, DOI [arXiv:1312.6199, DOI 10.1109/CVPR.2015.7298594]
   THIELE JC, 2019, PROC IEEE INT JOINT, P1
   Tzeng E., 2014, ARXIV14123474, DOI DOI 10.48550/ARXIV.1412.3474
   Tzeng E, 2015, IEEE I CONF COMP VIS, P4068, DOI 10.1109/ICCV.2015.463
   Virgilio CD, 2020, NEURAL NETWORKS, V122, P130, DOI 10.1016/j.neunet.2019.09.037
   Wang YX, 2021, IEEE T COGN DEV SYST, V13, P514, DOI 10.1109/TCDS.2020.2971655
   Wang ZM, 2020, IEEE T NEUR NET LEAR, V31, P2387, DOI 10.1109/TNNLS.2019.2935608
   Wu YJ, 2019, AAAI CONF ARTIF INTE, P1311
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Xie RC, 2019, IEEE INT CONF COMP V, P3213, DOI 10.1109/ICCVW.2019.00401
   Xie XR, 2020, KNOWL-BASED SYST, V195, DOI 10.1016/j.knosys.2020.105643
   Xie XR, 2019, SOFT COMPUT, V23, P10187, DOI 10.1007/s00500-018-3576-0
   Yan K, 2018, IEEE T CYBERNETICS, V48, P288, DOI 10.1109/TCYB.2016.2633306
   Yosinski J, 2014, ADV NEUR IN, V27
   Yu Q, 2022, IEEE T CYBERNETICS, V52, P1364, DOI 10.1109/TCYB.2020.2984888
   Yu Q, 2019, IEEE T CYBERNETICS, V49, P2178, DOI 10.1109/TCYB.2018.2821692
   Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53
   Zenke F, 2018, NEURAL COMPUT, V30, P1514, DOI 10.1162/neco_a_01086
   Zhang ML, 2020, NEUROCOMPUTING, V409, P103, DOI 10.1016/j.neucom.2020.03.079
   Zhang ML, 2018, IEEE T COGN DEV SYST, V10, P151, DOI 10.1109/TCDS.2017.2651943
NR 69
TC 8
Z9 8
U1 7
U2 28
PD DEC
PY 2022
VL 52
IS 12
BP 13323
EP 13335
DI 10.1109/TCYB.2021.3079097
EA JUL 2021
WC Automation & Control Systems; Computer Science, Artificial Intelligence;
   Computer Science, Cybernetics
DA 2023-11-11
ER

PT J
AU Wang, J
   Peng, H
AF Wang, Jun
   Peng, Hong
TI Adaptive fuzzy spiking neural P systems for fuzzy inference and learning
SO INTERNATIONAL JOURNAL OF COMPUTER MATHEMATICS
DT Article
DE spiking neural P systems; fuzzy spiking neural P systems; adaptive fuzzy
   spiking neural P systems; fuzzy reasoning; learning problem; 03B52;
   47S40; 92B20; 94D05
ID EXTENDED SPIKING
AB Fuzzy spiking neural P systems (in short, FSN P systems) are a novel class of distributed parallel computing models, which can model fuzzy production rules and apply their dynamic firing mechanism to achieve fuzzy reasoning. However, these systems lack adaptive/learning ability. Addressing this problem, a class of FSN P systems are proposed by introducing some new features, called adaptive fuzzy spiking neural P systems (in short, AFSN P systems). AFSN P systems not only can model weighted fuzzy production rules in fuzzy knowledge base but also can perform dynamically fuzzy reasoning. It is important to note that AFSN P systems have learning ability like neural networks. Based on neuron's firing mechanisms, a fuzzy reasoning algorithm and a learning algorithm are developed. Moreover, an example is included to illustrate the learning ability of AFSN P systems.
C1 [Wang, Jun] Xihua Univ, Sch Elect & Informat Engn, Chengdu 610039, Sichuan, Peoples R China.
   [Peng, Hong] Xihua Univ, Sch Math & Comp Engn, Chengdu 610039, Sichuan, Peoples R China.
RP Peng, H (corresponding author), Xihua Univ, Sch Math & Comp Engn, Chengdu 610039, Sichuan, Peoples R China.
EM ph.xhu@hotmail.com
CR Cavaliere M, 2009, THEOR COMPUT SCI, V410, P2352, DOI 10.1016/j.tcs.2009.02.031
   Chen H., 2006, ROM J INF SCI TECH, V9, P151
   Chen SM, 2009, IEEE T FUZZY SYST, V17, P1412, DOI 10.1109/TFUZZ.2009.2032651
   Freund R, 2008, INT J FOUND COMPUT S, V19, P1223, DOI 10.1142/S0129054108006248
   Gutiérrez-Naranjo MA, 2009, LECT NOTES COMPUT SC, V5391, P217
   Haiming C, 2007, PROG NAT SCI-MATER, V17, P417, DOI 10.1080/10020070708541018
   Hong Peng, 2010, Proceedings 2010 Sixth International Conference on Natural Computation (ICNC 2010), P3008, DOI 10.1109/ICNC.2010.5584269
   Ionescu M, 2007, INT J UNCONV COMPUT, V3, P135
   Ionescu M, 2006, FUND INFORM, V71, P279
   Jun Wang, 2010, Proceedings 2010 Sixth International Conference on Natural Computation (ICNC 2010), P3012, DOI 10.1109/ICNC.2010.5584281
   Pan LQ, 2010, THEOR COMPUT SCI, V411, P906, DOI 10.1016/j.tcs.2009.11.010
   Pan LQ, 2009, INT J COMPUT COMMUN, V4, P273, DOI 10.15837/ijccc.2009.3.2435
   Paun Gh, 2010, OXFORD HDB MEMBRANE
   Paun G, 2006, INT J FOUND COMPUT S, V17, P975, DOI 10.1142/S0129054106004212
   Peng H, 2013, INFORM SCIENCES, V235, P106, DOI 10.1016/j.ins.2012.07.015
   Wang J., 2009, P 10 WORKSH MEMBR CO, P514
   Wang J, 2013, IEEE T FUZZY SYST, V21, P209, DOI 10.1109/TFUZZ.2012.2208974
   Wang J, 2011, INT J INNOV COMPUT I, V7, P3709
   Yeung DS, 1997, FUZZY SET SYST, V88, P299, DOI 10.1016/S0165-0114(96)00052-8
   Yeung DS, 1998, IEEE T SYST MAN CY A, V28, P149, DOI 10.1109/3468.661144
NR 20
TC 40
Z9 40
U1 0
U2 35
PD APR 1
PY 2013
VL 90
IS 4
SI SI
BP 857
EP 868
DI 10.1080/00207160.2012.743653
WC Mathematics, Applied
DA 2023-11-11
ER

PT J
AU Liu, J
   Yang, X
   Zhu, YM
   Lei, YL
   Cai, J
   Wang, M
   Huan, ZY
   Lin, X
AF Liu, Jing
   Yang, Xu
   Zhu, Yimeng
   Lei, Yunlin
   Cai, Jian
   Wang, Miao
   Huan, Ziyi
   Lin, Xialv
TI How Neuronal Noises Influence the Spiking Neural Networks's Cognitive
   Learning Process: A Preliminary Study
SO BRAIN SCIENCES
DT Article
DE spiking neural network; neuronal noise; cognitive function
ID DEFAULT MODE
AB In neuroscience, the Default Mode Network (DMN), also known as the default network or the default-state network, is a large-scale brain network known to have highly correlated activities that are distinct from other networks in the brain. Many studies have revealed that DMNs can influence other cognitive functions to some extent. This paper is motivated by this idea and intends to further explore on how DMNs could help Spiking Neural Networks (SNNs) on image classification problems through an experimental study. The approach emphasizes the bionic meaning on model selection and parameters settings. For modeling, we select Leaky Integrate-and-Fire (LIF) as the neuron model, Additive White Gaussian Noise (AWGN) as the input DMN, and design the learning algorithm based on Spike-Timing-Dependent Plasticity (STDP). Then, we experiment on a two-layer SNN to evaluate the influence of DMN on classification accuracy, and on a three-layer SNN to examine the influence of DMN on structure evolution, where the results both appear positive. Finally, we discuss possible directions for future works.
C1 [Liu, Jing; Yang, Xu; Zhu, Yimeng; Lei, Yunlin; Cai, Jian; Wang, Miao; Huan, Ziyi; Lin, Xialv] Beijing Inst Technol, Sch Comp Sci & Technol, Beijing 100081, Peoples R China.
RP Yang, X (corresponding author), Beijing Inst Technol, Sch Comp Sci & Technol, Beijing 100081, Peoples R China.
EM jing.liu-22@hotmail.com; yangxu@tsinghua.edu.cn; 2220170740@bit.edu.cn;
   3120201035@bit.edu.cn; 3120201001@bit.edu.cn; 3220201093@bit.edu.cn;
   3220200891@bit.edu.cn; 3220201066@bit.edu.cn
CR Abbott LF, 1999, BRAIN RES BULL, V50, P303, DOI 10.1016/S0361-9230(99)00161-6
   Berger H, 1929, ARCH PSYCHIAT NERVEN, V87, P527, DOI 10.1007/BF01797193
   Burges, 2010, MNIST HANDWRITTEN DI, DOI DOI 10.1561/2400000035
   Coffey W. T., 2017, LANGEVIN EQUATION AP, DOI [10.1142/10490, DOI 10.1142/10490]
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   Gerstner W, 1996, NATURE, V383, P76, DOI 10.1038/383076a0
   Gerstner W., 2002, FORMAL SPIKING NEURO, P93, DOI [10.1017/CBO9780511815706.005, DOI 10.1017/CBO9780511815706.005]
   Grieder M, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00770
   He H, 2019, FRONT NEUROSCI-SWITZ, V13, DOI 10.3389/fnins.2019.00650
   Hebb D. O., 1949, ORG BEHAV
   Jordan J., 2019, **DATA OBJECT**, DOI 10.5281/zenodo.2605422
   Kempter R, 1999, PHYS REV E, V59, P4498, DOI 10.1103/PhysRevE.59.4498
   Lemons DS, 1997, AM J PHYS, V65, P1079, DOI 10.1119/1.18725
   Manwani A, 1999, NEURAL COMPUT, V11, P1797, DOI 10.1162/089976699300015972
   Raichle ME, 2006, SCIENCE, V314, P1249, DOI 10.1126/science.1134405
   Raichle ME, 2001, P NATL ACAD SCI USA, V98, P676, DOI 10.1073/pnas.98.2.676
   Szendro P, 2001, EUR BIOPHYS J BIOPHY, V30, P227, DOI 10.1007/s002490100143
   Thorpe S, 1998, COMPUTATIONAL NEUROSCIENCE: TRENDS IN RESEARCH, P113
   Whitfield-Gabrieli S, 2011, NEUROIMAGE, V55, P225, DOI 10.1016/j.neuroimage.2010.11.048
NR 19
TC 1
Z9 1
U1 0
U2 6
PD FEB
PY 2021
VL 11
IS 2
AR 153
DI 10.3390/brainsci11020153
WC Neurosciences
DA 2023-11-11
ER

PT C
AU Dibazar, AA
   Yousefi, A
   Berger, TW
AF Dibazar, Alireza A.
   Yousefi, Ali
   Berger, Theodore W.
GP IEEE
TI Large Scale Parameter Estimation for Nonlinear Dynamic Systems:
   Application on Spike-In, Spike-Out Neural Models
SO PROCEEDINGS OF THE 2014 INTERNATIONAL JOINT CONFERENCE ON NEURAL
   NETWORKS (IJCNN)
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUL 06-11, 2014
CL Beijing, PEOPLES R CHINA
DE hippocampus; nonlinear dynamical system; parameter estimation for linear
   dynamical systems; time variant models; spiking neural network;
   plasticity
AB This paper presents a general method of parameter estimation for large-scale non-linear dynamic models a with particular focus on parameter estimation for spike-in, spike-out neural models. The aim is to provide a convex optimization algorithm for tuning parameters of such a model which enables solving large-scale estimation problem in a linear time. Parameter estimation for a single layer neural network containing hundreds of synapses is addressed and efficiency/performance of the proposed methodology is demonstrated by solving a few examples. It will be also demonstrated that parameters of the model for mapping CA3 output of hippocampus cell into CA1 output, under patch clamp experiment, can be successfully estimated by utilizing the methodology of this paper.
C1 [Dibazar, Alireza A.; Berger, Theodore W.] Univ So Calif, Dept Biomed Engn, 1042 Downey Way,DRB-140, Los Angeles, CA 90089 USA.
   [Yousefi, Ali] Univ So Calif, Los Angeles, CA 90089 USA.
   [Berger, Theodore W.] USCs Ctr Neural Engn, Los Angeles, CA USA.
RP Dibazar, AA (corresponding author), Univ So Calif, Dept Biomed Engn, 1042 Downey Way,DRB-140, Los Angeles, CA 90089 USA.
EM dibazar@usc.edu; ayousefi@usc.edu; berger@usc.edu
CR Berger T. W., 2004, P IEEE NEUR NETW MAR, V3, P1659
   Dibazar A., 2013, P IEEE EMBC C, P613
   MAINEN ZF, 1995, SCIENCE, V268, P1503, DOI 10.1126/science.7770778
   Stortelder W. J. H., 1998, THESIS U AMSTERDAM
   Takahashi N, 2012, SCIENCE, V335, P353, DOI 10.1126/science.1210362
   Yousefi A, 2012, IEEE ENG MED BIO, P1362, DOI 10.1109/EMBC.2012.6346191
NR 6
TC 1
Z9 1
U1 0
U2 1
PY 2014
BP 2422
EP 2427
WC Computer Science, Artificial Intelligence; Engineering, Electrical &
   Electronic; Telecommunications
DA 2023-11-11
ER

PT C
AU Hamed, HNA
   Kasabov, N
   Shamsuddin, SM
AF Hamed, Haza Nuzly Abdull
   Kasabov, Nikola
   Shamsuddin, Siti Mariyam
BE Abraham, A
   Muda, AK
   Herman, NS
   Shamsuddin, SM
   Huoy, CY
TI Integrated Feature Selection and Parameter Optimization for Evolving
   Spiking Neural Networks using Quantum Inspired Particle Swarm
   Optimization
SO 2009 INTERNATIONAL CONFERENCE OF SOFT COMPUTING AND PATTERN RECOGNITION
DT Proceedings Paper
CT International Conference of Soft Computing and Pattern Recognition
CY DEC 04-07, 2009
CL Malacca, MALAYSIA
DE Evolving Spiking Neural Network; Particle Swarm; Quantum Computing;
   Feature Optimization; Parameter Optimization
AB This paper proposes a novel method for optimizing features and parameters in the Evolving Spiking Neural Network (ESNN) using Quantum-inspired Particle Swarm Optimization (QiPSO). This study reveals the interesting concept of QiPSO in which information is represented as binary structures. The mechanism simultaneously optimizes the ESNN parameters and relevant features using wrapper approach. A synthetic dataset is used to evaluate the performance of the proposed method. The results show that QiPSO yields promising outcomes in obtaining the best combination of ESNN parameters as well as in identifying the most relevant features.
C1 [Hamed, Haza Nuzly Abdull; Kasabov, Nikola] Auckland Univ Technol, KEDRI, Auckland, New Zealand.
   [Shamsuddin, Siti Mariyam] Univ Teknol Malaysia, Soft Computing Res Grp, Johor Baharu, Johor, Malaysia.
RP Hamed, HNA (corresponding author), Auckland Univ Technol, KEDRI, Auckland, New Zealand.
EM hnuzly@autac.nz; nkasabov@autac.nz; mariyam@utm.my
CR Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Eberhart R., 1999, P 6 INT S MICROMACHI, DOI [10.1109/MHS.1995.494215, DOI 10.1109/MHS.1995.494215]
   ESTAVEST P, 2001, NEURAL NETWORKS, V20, P189
   Han KH, 2002, IEEE T EVOLUT COMPUT, V6, P580, DOI 10.1109/TEVC.2002.804320
   HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0
   Kasabov N., 2007, EVOLVING CONNECTIONI
   KASABOV N, 2009, SPRINGER LNCS, V5506
   SCHLIEBS S, 2009, SPRINGER LNCS
   Schliebs S, 2009, NEURAL NETWORKS, V22, P623, DOI 10.1016/j.neunet.2009.06.038
   Sun J, 2004, IEEE C EVOL COMPUTAT, P325
   Thorpe S., 1997, CAN HUMAN VISUAL SYS
   WYSOSKI SG, 2006, ONLINE LEARNING STRU, P61
NR 12
TC 14
Z9 14
U1 0
U2 2
PY 2009
BP 695
EP +
DI 10.1109/SoCPaR.2009.139
WC Computer Science, Artificial Intelligence; Computer Science, Theory &
   Methods; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT C
AU Palmieri, F
   Luongo, A
   Moiseff, A
AF Palmieri, F
   Luongo, A
   Moiseff, A
BE Marinaro, M
   Tagliaferri, R
TI From spiking neurons to dynamic perceptrons
SO NEURAL NETS - WIRN VIETRI-99
SE PERSPECTIVES IN NEURAL COMPUTING
DT Proceedings Paper
CT 11th Italian Workshop on Neural Nets (WIRN Vietri 99)
CY MAY 20-22, 1999
CL SALERNO, ITALY
AB This paper begins a systematic validation for a simple and reliable artificial neural network model that can be directly related to the main behaviour of biological neural networks. The sigmoid-plus-linear filter appears to be a promising candidate if the sigmoidal function is calculated in reference to the pulse generation refractory effects. We directly compare a classical spiking neuron model with a scheme based on a sigmoidal function plus a linear filter. The filter is computed as the best least squares fit to the output of the spiking model. The results seem to confirm that FIR and IIR neural networks may be able to represent the essence of the signal processing performed by biological neurons.
C1 Univ Naples Federico II, Dipartimento Ingn Elettronica & Telecomunicazioni, I-80125 Naples, Italy.
RP Palmieri, F (corresponding author), Univ Naples Federico II, Dipartimento Ingn Elettronica & Telecomunicazioni, Via Claudio 21, I-80125 Naples, Italy.
CR Back AD, 1991, NEURAL COMPUT, V3, P375, DOI 10.1162/neco.1991.3.3.375
   CARLSON AB, 1986, COMMUNICATION SYSTEM
   CARR CE, 1990, J NEUROSCI, V10, P3227
   FERSTER D, 1995, SCIENCE, V270, P756, DOI 10.1126/science.270.5237.756
   Haykin S., 2001, NEURAL NETWORKS COMP
   Haykin S. S., 2008, ADAPTIVE FILTER THEO
   Koch C., 1989, METHODS NEURAL MODEL
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   MOISEFF A, 1983, J NEUROSCI, V3, P2353
   Shamma S., 1989, METHODS NEURONAL MOD, P247
   Tuckwell H.C., 1988, INTRO THEORETICAL NE, V2
NR 11
TC 0
Z9 0
U1 0
U2 1
PY 1999
BP 290
EP 295
WC Computer Science, Artificial Intelligence
DA 2023-11-11
ER

PT J
AU Kwon, D
   Woo, SY
   Bae, JH
   Lim, S
   Park, BG
   Lee, JH
AF Kwon, Dongseok
   Woo, Sung Yun
   Bae, Jong-Ho
   Lim, Suhwan
   Park, Byung-Gook
   Lee, Jong-Ho
TI Hardware-Based Spiking Neural Networks Using Capacitor-Less Positive
   Feedback Neuron Devices
SO IEEE TRANSACTIONS ON ELECTRON DEVICES
DT Article
DE Neurons; Micromechanical devices; Electric potential; Training; Membrane
   potentials; Biological neural networks; Anodes; Leaky integrate and fire
   (LIF) neuron; neuromorphic; positive feedback (PF) devices; spiking
   neural networks (SNNs)
AB In this article, hardware-based spiking neural networks (SNNs) using capacitor-less positive feedback (PF) neuron devices are designed. It was reported that the PF device can simultaneously process the excitatory and inhibitory signals. The PF device shows very steep subthreshold slope (SS < 1 mV/dec) due to the PF opertaion, leading to low-power and reliable neuron device. The PF devices also show the behavior of leaky integrate and fire (LIF) neuron, which is the most popular neuron model in SNNs. For hardware configuration, the neuron characteristics of PF device are investigated with the transient behavior of the anode current. Based on the PF neuron devices, the SNN shows the accuracy of 98.19% for the Modified National Institute of Standards and Technology (MNIST) database classification in four-hidden layer, fully-connected neural network, which is near the accuracy (98.46%) of the artificial neural networks using rectified linear unit (ReLU) activation function.
C1 [Kwon, Dongseok; Woo, Sung Yun; Park, Byung-Gook; Lee, Jong-Ho] Seoul Natl Univ, Sch Elect & Comp Engn, Interuniv Semicond Res Ctr ISRC, Seoul 151742, South Korea.
   [Bae, Jong-Ho] Kookmin Univ, Sch Elect Engn, Seoul 02707, South Korea.
   [Lim, Suhwan] Samsung Elect, Flash Technol Dev Team, Seoul 135856, South Korea.
RP Lee, JH (corresponding author), Seoul Natl Univ, Sch Elect & Comp Engn, Interuniv Semicond Res Ctr ISRC, Seoul 151742, South Korea.
EM jhl@snu.ac.kr
CR Das B, 2018, IEEE ELECTR DEVICE L, V39, P1832, DOI 10.1109/LED.2018.2876684
   Davidson S, 2021, FRONT NEUROSCI-SWITZ, V15, DOI 10.3389/fnins.2021.651141
   Diehl PU, 2015, IEEE IJCNN
   Han B., 2020, P IEEE CVF C COMP VI, P13558
   Han JK, 2021, IEEE T ELECTRON DEV, V68, P430, DOI 10.1109/TED.2020.3036018
   Han JK, 2020, IEEE ELECTR DEVICE L, V41, P1157, DOI 10.1109/LED.2020.3001953
   Hwang S, 2020, SCI REP-UK, V10, DOI 10.1038/s41598-020-60572-8
   Hwang S, 2018, IEEE ELECTR DEVICE L, V39, P1441, DOI 10.1109/LED.2018.2853635
   Ioffe S, 2015, PR MACH LEARN RES, V37, P448
   Jaiswal A, 2017, IEEE T ELECTRON DEV, V64, P1818, DOI 10.1109/TED.2017.2671353
   Kang WM, 2021, IEEE ACCESS, V9, P73121, DOI 10.1109/ACCESS.2021.3080310
   Kingma D. P., 2014, ARXIV, DOI DOI 10.48550/ARXIV.1412.6980
   Kwon D, 2021, SENSOR ACTUAT B-CHEM, V340, DOI 10.1016/j.snb.2020.129258
   Kwon D, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00423
   Lee D, 2019, ADV ELECTRON MATER, V5, DOI 10.1002/aelm.201800866
   Lin J, 2018, IEEE T BIOMED CIRC S, V12, P1004, DOI 10.1109/TBCAS.2018.2843286
   Luo J, 2019, IEDM, P641, DOI 10.1109/IEDM19573.2019.8993535
   Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7
   Oh S., 2020, ARXIV200605033
   Park S, 2020, PEER PEER NETW APPL, V13, P684, DOI 10.1007/s12083-019-00780-w
   Rueckauer B, 2017, FRONT NEUROSCI-SWITZ, V11, DOI 10.3389/fnins.2017.00682
   Srivastava N, 2014, J MACH LEARN RES, V15, P1929
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Tavanaei A, 2019, NEUROCOMPUTING, V330, P39, DOI 10.1016/j.neucom.2018.11.014
   Woo SY, 2020, IEEE ACCESS, V8, P202639, DOI 10.1109/ACCESS.2020.3036088
   Wu YJ, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00331
   Yeo I, 2019, IEEE T BIOMED CIRC S, V13, P1678, DOI 10.1109/TBCAS.2019.2945559
   Yu SM, 2018, P IEEE, V106, P260, DOI 10.1109/JPROC.2018.2790840
NR 28
TC 1
Z9 1
U1 0
U2 10
PD SEP
PY 2021
VL 68
IS 9
BP 4766
EP 4772
DI 10.1109/TED.2021.3098503
WC Engineering, Electrical & Electronic; Physics, Applied
DA 2023-11-11
ER

PT C
AU Machingal, P
   Thousif
   Dora, S
   Sundaram, S
   Meng, QG
AF Machingal, Pranav
   Thousif
   Dora, Shirin
   Sundaram, Suresh
   Meng, Qinggang
GP IEEE
TI Learning to Classify Faster Using Spiking Neural Networks
SO 2023 INTERNATIONAL JOINT CONFERENCE ON NEURAL NETWORKS, IJCNN
SE IEEE International Joint Conference on Neural Networks (IJCNN)
DT Proceedings Paper
CT International Joint Conference on Neural Networks (IJCNN)
CY JUN 18-23, 2023
CL Broadbeach, AUSTRALIA
DE Spiking Neural Networks; Deep Learning; Digit Recognition; Cross Entropy
   Loss; Faster classification
ID BACKPROPAGATION; ALGORITHM; DISORDER; AUTISM; ADHD
AB This paper develops a new approach to estimate predicted class probabilities in deep Spiking Neural Networks (SNN) that encourages faster classification. The proposed approach utilizes the temporal separation between the first spikes generated by the output neurons to estimate the predicted class probabilities which are then used with cross entropy loss for training the network. This maximizes the separation between the first spikes generated by the neuron associated with the correct class and neurons associated with other classes. Higher classification performance is obtained by maximising the temporal separation, which also drives the correct class neuron to spike earlier in the simulation. As a consequence, the predicted class may be determined from the first spike in the output layer, leading to quicker classification. The sensitivity factor for each neuron in the network is estimated via error-backpropagation during training. Using Spike Timing Dependent Plasticity (STDP) regulated by the estimated sensitivity factors, the network weights are updated. It results that the learning method is termed as Temporal Separation Modulated Spike Timing Dependent Plasticity (TSM-STDP). On the benchmark MNIST dataset, the performance of TSM-STDP has been assessed, and the evaluation results are compared with those of other learning methods for SNNs. Additionally, a histogram of the output layer's first spikes demonstrated that the right class neurons spiked earlier in the simulation than other class neurons, enabling faster classification. On real-world Attention Deficit Hyperactivity Disorder (ADHD) detection dataset, the effectiveness of TSM-STDP has also been assessed and compared with other available approaches. The performance comparison results clearly show that TSM-STDP can achieve classification performance comparable to other existing learning algorithms on benchmark and real-world datasets while requiring less time for classification.
C1 [Machingal, Pranav] Indian Inst Technol, Ind Engg & Operat Res, Mumbai, Maharashtra, India.
   [Thousif; Sundaram, Suresh] IISC Bangalore, Dept Aerosp Engg, Bengaluru, India.
   [Dora, Shirin; Meng, Qinggang] Loughborough Univ, Dept Comp Sci, Loughborough, Leics, England.
RP Machingal, P (corresponding author), Indian Inst Technol, Ind Engg & Operat Res, Mumbai, Maharashtra, India.
EM pranav.mac@iitb.ac.in; pagalat@iisc.ac.in; S.Dora@lboro.ac.uk;
   vssuresh@iisc.ac.in; Q.Meng@lboro.ac.uk
CR Acuna C, 2012, FRONT SYST NEUROSCI, V6, DOI 10.3389/fnsys.2012.00062
   Aradhya AMS, 2018, IEEE ENG MED BIO, P5541, DOI 10.1109/EMBC.2018.8513522
   Babu GS, 2013, APPL SOFT COMPUT, V13, P654, DOI 10.1016/j.asoc.2012.08.047
   Babu GS, 2014, EXPERT SYST APPL, V41, P478, DOI 10.1016/j.eswa.2013.07.073
   Bellec P, 2017, NEUROIMAGE, V144, P275, DOI 10.1016/j.neuroimage.2016.06.034
   Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Chilakamarri JK, 2011, ANN CLIN PSYCHIATRY, V23, P25
   Diehl PU, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON REBOOTING COMPUTING (ICRC)
   Diehl Peter U, 2015, 2015 INT JOINT C NEU, P1, DOI [DOI 10.1109/IJCNN.2015.7280696, 10.1109/IJCNN.2015.7280696]
   Dora S, 2019, IEEE T CYBERNETICS, V49, P989, DOI 10.1109/TCYB.2018.2791282
   Ghosh-Dastidar S, 2009, NEURAL NETWORKS, V22, P1419, DOI 10.1016/j.neunet.2009.04.003
   Hao YZ, 2020, NEURAL NETWORKS, V121, P387, DOI 10.1016/j.neunet.2019.09.007
   Jeyasothy A., 2018, IEEE T NEURAL NETWOR, V30, P1231
   Jeyasothy A, 2021, EXPERT SYST APPL, V178, DOI 10.1016/j.eswa.2021.114985
   Kasabov N, 2013, NEURAL NETWORKS, V41, P188, DOI 10.1016/j.neunet.2012.11.014
   Kheradpisheh SR, 2018, NEURAL NETWORKS, V99, P56, DOI 10.1016/j.neunet.2017.12.005
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Lee C., 2018, IEEE T COGNITIVE DEV
   Lee C, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00435
   Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508
   Li L, 2020, IEEE ACCESS, V8, P111626, DOI 10.1109/ACCESS.2020.3001531
   Machingal P, 2020, IEEE IJCNN, DOI 10.1109/ijcnn48605.2020.9207620
   Mahanand B., 2013, LECT NOTES COMPUTER, P386, DOI DOI 10.1007/978-3-319-03680-9
   Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213
   Mayes SD, 2012, RES AUTISM SPECT DIS, V6, P277, DOI 10.1016/j.rasd.2011.05.009
   Mostafa H, 2018, IEEE T NEUR NET LEAR, V29, P3227, DOI 10.1109/TNNLS.2017.2726060
   Shrestha Sumit Bam, 2018, ADV NEURAL INFORM PR, P1412
   Subbaraju V, 2017, MED IMAGE ANAL, V35, P375, DOI 10.1016/j.media.2016.08.003
   Tavanaei A, 2019, NEUROCOMPUTING, V330, P39, DOI 10.1016/j.neucom.2018.11.014
   Van Rullen R, 2001, NEURAL COMPUT, V13, P1255, DOI 10.1162/08997660152002852
   Wade JJ, 2010, IEEE T NEURAL NETWOR, V21, P1817, DOI 10.1109/TNN.2010.2074212
   Zhang LI, 1998, NATURE, V395, P37, DOI 10.1038/25665
NR 34
TC 0
Z9 0
U1 0
U2 0
PY 2023
DI 10.1109/IJCNN54540.2023.10191334
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Hulata, E
   Segev, R
   Shapira, Y
   Benveniste, M
   Ben-Jacob, E
AF Hulata, E
   Segev, R
   Shapira, Y
   Benveniste, M
   Ben-Jacob, E
TI Detection and sorting of neural spikes using wavelet packets
SO PHYSICAL REVIEW LETTERS
DT Article
ID CULTURE
AB We propose a novel method for the detection and sorting of recorded neural spikes using wavelet packets. We employ the best basis via the Shannon's information cost function and local discriminant basis using mutual information. We demonstrate the efficiency of the method on data recorded in vitro from 2D neural networks. We show that our method is superior both in separation from noise and in identifying superimposed spikes.
C1 Tel Aviv Univ, Raymond & Beverly Sackler Fac Exact Sci, Sch Phys & Astron, IL-69978 Tel Aviv, Israel.
   Tel Aviv Univ, Sackler Fac Med, Dept Physiol & Pharmacol, IL-69978 Tel Aviv, Israel.
RP Ben-Jacob, E (corresponding author), Tel Aviv Univ, Raymond & Beverly Sackler Fac Exact Sci, Sch Phys & Astron, IL-69978 Tel Aviv, Israel.
CR [Anonymous], 1998, PHYS A
   [Anonymous], 1992, MULTIRESOLUTION SIGN
   [Anonymous], 1992, WAVELETS THEIR APPL
   BOW S, 1991, PATTERN RECOGNITION
   CAMEPARI M, 1997, BIOL CYBERN, V77, P153
   Cohen I, 1997, SIGNAL PROCESS, V57, P251, DOI 10.1016/S0165-1684(97)00007-8
   COIFMAN RR, 1992, IEEE T INFORM THEORY, V38, P713, DOI 10.1109/18.119732
   Daubechies I., 1992, CBMS NSF REGIONAL C, V61
   DELLAERT F, MATLAB FUNCTION
   DONOHO D, MATLAB TOOLBOX
   Egert U, 1998, BRAIN RES PROTOC, V2, P229, DOI 10.1016/S1385-299X(98)00013-0
   JIMBO Y, 1993, IEEE T BIO-MED ENG, V40, P804, DOI 10.1109/10.238465
   Lewicki MS, 1998, NETWORK-COMP NEURAL, V9, pR53, DOI 10.1088/0954-898X/9/4/001
   Maher MP, 1999, J NEUROSCI METH, V87, P45, DOI 10.1016/S0165-0270(98)00156-3
   MAROM S, COMMUNICATION
   SAITO N, 1994, THESIS YALE U
   Stenger D. A., 1994, ENABLING TECHNOLOGIE
   Stett A, 1997, PHYS REV E, V55, P1779, DOI 10.1103/PhysRevE.55.1779
   Unser M, 1996, P IEEE, V84, P626, DOI 10.1109/5.488704
   WHEELER B, 1999, AUTOMATIC DISCRIMINA
   YANG XW, 1988, IEEE T BIO-MED ENG, V35, P806, DOI 10.1109/10.7287
   Zouridakis G, 1997, COMPUT BIOL MED, V27, P9, DOI 10.1016/S0010-4825(96)00038-8
NR 22
TC 64
Z9 64
U1 0
U2 6
PD NOV 20
PY 2000
VL 85
IS 21
BP 4637
EP 4640
DI 10.1103/PhysRevLett.85.4637
WC Physics, Multidisciplinary
DA 2023-11-11
ER

PT J
AU Wang, H
   Li, YF
AF Wang, Huan
   Li, Yan-Fu
TI Bioinspired membrane learnable spiking neural network for autonomous
   vehicle sensors fault diagnosis under open environments
SO RELIABILITY ENGINEERING & SYSTEM SAFETY
DT Article
DE Fault diagnosis; Health status prediction; Spiking neural network;
   Autonomous vehicle sensors
ID INTELLIGENCE; MACHINE
AB Autonomous vehicles have successfully driven autonomously on urban roads, relying on numerous sensors for environmental perception and vehicle control. However, the abnormality and degradation of sensors will make vehicles face serious safety risks. Therefore, autonomous vehicles must have complete sensor fault diagnosis systems to detect anomalies and avoid accidents. Therefore, this paper explores brain-inspired spiking neural networks (SNN) for sensor fault diagnosis. Specifically, this paper proposes a brain-inspired membrane learnable residual spiking neural network (MLR-SNN) for sensor fault and health index prediction. SNN accurately sim-ulates the dynamic mechanism of biological neurons and exhibits excellent spatiotemporal information pro-cessing potential and low power consumption while being highly biologically credible. Based on the convolution topology, this study designs a spike-residual-based SNN framework that optimizes the gradient transfer efficiency to enable deep-level spiking information encoding. In addition, membrane-learnable mechanisms are introduced to simulate the differences of neuronal membrane-related parameters in brains, which can better characterize the dynamics of neurons. The proposed MLR-SNN is validated on actual autonomous vehicle sensor datasets. Experimental results show that MLR-SNN with neural dynamics mechanism has excellent performance, and it can accurately predict fault mode and health index from multivariate sensor data under open environments.
C1 [Wang, Huan; Li, Yan-Fu] Tsinghua Univ, Dept Ind Engn, Beijing 100000, Peoples R China.
RP Li, YF (corresponding author), Tsinghua Univ, Dept Ind Engn, Beijing 100000, Peoples R China.
EM liyanfu@tsinghua.edu.cn
CR Banerjee TP, 2012, INFORM SCIENCES, V217, P96, DOI 10.1016/j.ins.2012.06.016
   Chen X, 2021, RELIAB ENG SYST SAFE, V216, DOI 10.1016/j.ress.2021.108011
   Delorme A, 1999, NEUROCOMPUTING, V26-7, P989, DOI 10.1016/S0925-2312(99)00095-8
   Fang W., 2021, ADV NEURAL INFORM PR, V34, P21056, DOI DOI 10.48550/ARXIV.2102.04159
   Fang W., 2021, P IEEE CVF INT C COM, P2661, DOI 10.1109/ICCV48922.2021.00266
   Fang WAC, SPIKINGJELLY
   Geyer J, 2020, ARXIV
   Guan Y, 2021, RELIAB ENG SYST SAFE, V216, DOI 10.1016/j.ress.2021.108017
   Han T, 2022, RELIAB ENG SYST SAFE, V226, DOI 10.1016/j.ress.2022.108648
   Hasselmo ME, 2006, TRENDS COGN SCI, V10, P487, DOI 10.1016/j.tics.2006.09.005
   Ignatious H. A., 2021, P COMPUT SCI, V198, P736
   Jan SU, 2017, IEEE ACCESS, V5, P8682, DOI 10.1109/ACCESS.2017.2705644
   Jana D, 2022, MECH SYST SIGNAL PR, V169, DOI 10.1016/j.ymssp.2021.108723
   Ji JJ, 2018, T I MEAS CONTROL, V40, P1746, DOI 10.1177/0142331217690579
   Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90
   LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539
   Lee C, 2020, FRONT NEUROSCI-SWITZ, V14, DOI 10.3389/fnins.2020.00119
   Liu RN, 2018, MECH SYST SIGNAL PR, V108, P33, DOI 10.1016/j.ymssp.2018.02.016
   Liu ZL, 2022, ISA T, V125, P426, DOI 10.1016/j.isatra.2021.07.011
   Lobo JL, 2020, NEURAL NETWORKS, V121, P88, DOI 10.1016/j.neunet.2019.09.004
   Pan H, 2021, CHIN J MECH ENG-EN, V34, P1
   Pei J, 2019, NATURE, V572, P106, DOI 10.1038/s41586-019-1424-8
   Roy K, 2019, NATURE, V575, P607, DOI 10.1038/s41586-019-1677-2
   Safavi S, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21072547
   Shankar KH, 2012, NEURAL COMPUT, V24, P134, DOI 10.1162/NECO_a_00212
   Stöckl C, 2021, NAT MACH INTELL, V3, DOI 10.1038/s42256-021-00311-4
   Sun HL, 2014, MECH SYST SIGNAL PR, V43, P1, DOI 10.1016/j.ymssp.2013.09.015
   Taherkhani A, 2020, NEURAL NETWORKS, V122, P253, DOI 10.1016/j.neunet.2019.09.036
   Tavanaei A, 2019, NEURAL NETWORKS, V111, P47, DOI 10.1016/j.neunet.2018.12.002
   Vaila R, 2019, Arxiv, DOI arXiv:1903.12272
   Wang H, ARXIV
   Wang H, 2022, KNOWL-BASED SYST, V239, DOI 10.1016/j.knosys.2021.107978
   Wang J, 2022, J MANUF SYST, V65, P208, DOI 10.1016/j.jmsy.2022.09.003
   Wang TY, 2019, MECH SYST SIGNAL PR, V126, P662, DOI 10.1016/j.ymssp.2019.02.051
   Wen L, 2018, IEEE T IND ELECTRON, V65, P5990, DOI 10.1109/TIE.2017.2774777
   Xu YD, 2022, RELIAB ENG SYST SAFE, V225, DOI 10.1016/j.ress.2022.108618
   Yeong D, 2021, SENSORS-BASEL, V21, DOI 10.3390/s21062140
   Yin BJ, 2021, NAT MACH INTELL, V3, P905, DOI 10.1038/s42256-021-00397-w
   Yuan M, 2016, 2016 IEEE/CSAA INTERNATIONAL CONFERENCE ON AIRCRAFT UTILITY SYSTEMS (AUS), P135, DOI 10.1109/AUS.2016.7748035
   Zhang W, 2019, ADV NEUR IN, V32
   Zhang W, 2017, SENSORS-BASEL, V17, DOI 10.3390/s17020425
   Zimmer R, 2019, Arxiv, DOI arXiv:1911.10124
   Zuo L, 2022, RELIAB ENG SYST SAFE, V225, DOI 10.1016/j.ress.2022.108561
   Zuo L, 2021, J MANUF SYST, V61, P714, DOI 10.1016/j.jmsy.2020.07.003
NR 44
TC 2
Z9 2
U1 18
U2 18
PD MAY
PY 2023
VL 233
AR 109102
DI 10.1016/j.ress.2023.109102
EA FEB 2023
WC Engineering, Industrial; Operations Research & Management Science
DA 2023-11-11
ER

PT J
AU Lin, HR
   Wang, CH
   Chen, CJ
   Sun, YC
   Zhou, C
   Xu, C
   Hong, QH
AF Lin, Hairong
   Wang, Chunhua
   Chen, Chengjie
   Sun, Yichuang
   Zhou, Chao
   Xu, Cong
   Hong, Qinghui
TI Neural Bursting and Synchronization Emulated by Neural Networks and
   Circuits
SO IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS I-REGULAR PAPERS
DT Article
DE Biological neural networks; Neurons; Synchronization; Integrated circuit
   modeling; Brain modeling; Couplings; Numerical models; Bursting firing;
   synchronization; neural network; bifurcation; circuit implementation
ID NUMERICAL-ANALYSES; HYPERCHAOS; ATTRACTORS; DYNAMICS; CHAOS
AB Nowadays, research, modeling, simulation and realization of brain-like systems to reproduce brain behaviors have become urgent requirements. In this paper, neural bursting and synchronization are imitated by modeling two neural network models based on the Hopfield neural network (HNN). The first neural network model consists of four neurons, which correspond to realizing neural bursting firings. Theoretical analysis and numerical simulation show that the simple neural network can generate abundant bursting dynamics including multiple periodic bursting firings with different spikes per burst, multiple coexisting bursting firings, as well as multiple chaotic bursting firings with different amplitudes. The second neural network model simulates neural synchronization using a coupling neural network composed of two above small neural networks. The synchronization dynamics of the coupling neural network is theoretically proved based on the Lyapunov stability theory. Extensive simulation results show that the coupling neural network can produce different types of synchronous behaviors dependent on synaptic coupling strength, such as anti-phase bursting synchronization, anti-phase spiking synchronization, and complete bursting synchronization. Finally, two neural network circuits are designed and implemented to show the effectiveness and potential of the constructed neural networks.
C1 [Lin, Hairong; Wang, Chunhua; Zhou, Chao; Xu, Cong; Hong, Qinghui] Hunan Univ, Coll Comp Sci & Elect Engn, Changsha 410082, Peoples R China.
   [Chen, Chengjie] Changzhou Univ, Sch Microelect & Control Engn, Changzhou 213164, Jiangsu, Peoples R China.
   [Sun, Yichuang] Univ Hertfordshire, Sch Engn & Comp Sci, Hatfield AL10 9AB, Herts, England.
RP Wang, CH (corresponding author), Hunan Univ, Coll Comp Sci & Elect Engn, Changsha 410082, Peoples R China.
EM wch1227164@hnu.edu.cn
CR Bao BC, 2020, NONLINEAR DYNAM, V99, P2339, DOI 10.1007/s11071-019-05395-7
   Bao BC, 2019, INT J BIFURCAT CHAOS, V29, DOI 10.1142/S0218127419501347
   Bao BC, 2019, INT J BIFURCAT CHAOS, V29, DOI 10.1142/S0218127419300106
   Bao BC, 2017, NONLINEAR DYNAM, V90, P2359, DOI 10.1007/s11071-017-3808-3
   Bao BC, 2017, FRONT COMPUT NEUROSC, V11, DOI 10.3389/fncom.2017.00081
   Bao H, 2020, NONLINEAR DYNAM, V100, P937, DOI 10.1007/s11071-020-05529-2
   Bao H, 2020, IEEE T NEUR NET LEAR, V31, P502, DOI 10.1109/TNNLS.2019.2905137
   Bersini H, 1998, NEURAL NETWORKS, V11, P1017, DOI 10.1016/S0893-6080(98)00056-2
   Bullmore ET, 2009, NAT REV NEUROSCI, V10, P186, DOI 10.1038/nrn2575
   Chen CJ, 2019, NONLINEAR DYNAM, V95, P3385, DOI 10.1007/s11071-019-04762-8
   Etémé AS, 2019, COMMUN NONLINEAR SCI, V72, P432, DOI 10.1016/j.cnsns.2019.01.004
   Galias Z, 2018, IEEE T CIRCUITS-II, V65, P637, DOI 10.1109/TCSII.2018.2820104
   Gilan AA, 2020, IEEE T CIRCUITS-II, V67, P755, DOI 10.1109/TCSII.2019.2922372
   Gray C M, 1994, J Comput Neurosci, V1, P11, DOI 10.1007/BF00962716
   Hayati M, 2015, IEEE T CIRCUITS-I, V62, P1805, DOI 10.1109/TCSI.2015.2423794
   Hong QH, 2020, NEURAL COMPUT APPL, V32, P8175, DOI 10.1007/s00521-019-04305-7
   HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554
   Hsu WY, 2012, INT J NEURAL SYST, V22, P51, DOI 10.1142/S0129065712002979
   Hu XY, 2019, COMPLEXITY, V2019, DOI 10.1155/2019/4835379
   Hu XY, 2018, NONLINEAR DYNAM, V91, P1541, DOI 10.1007/s11071-017-3963-6
   Jiang YN, 2018, IEEE T CIRCUITS-I, V65, P2726, DOI 10.1109/TCSI.2018.2812419
   Krestinskaya O, 2019, IEEE T CIRCUITS-I, V66, P719, DOI 10.1109/TCSI.2018.2866510
   Lakshmanan S, 2017, IEEE T NEUR NET LEAR, V28, P1953, DOI 10.1109/TNNLS.2016.2557845
   Leng Y, 2020, CHAOS, V30, DOI 10.1063/5.0002076
   Li QD, 2014, NONLINEAR DYNAM, V78, P1087, DOI 10.1007/s11071-014-1498-7
   Lin HR, 2021, IEEE T IND ELECTRON, V68, P12708, DOI 10.1109/TIE.2020.3047012
   Lin HR, 2020, IEEE T CIRCUITS-II, V67, P3472, DOI 10.1109/TCSII.2020.3000492
   Lin HR, 2020, COMMUN NONLINEAR SCI, V90, DOI 10.1016/j.cnsns.2020.105390
   Lin HR, 2020, NONLINEAR DYNAM, V100, P3667, DOI 10.1007/s11071-020-05687-3
   Lin HR, 2020, NONLINEAR DYNAM, V99, P2369, DOI 10.1007/s11071-019-05408-5
   Lin HR, 2020, APPL MATH COMPUT, V369, DOI 10.1016/j.amc.2019.124840
   Liu P, 2019, IEEE T NEUR NET LEAR, V30, P2358, DOI 10.1109/TNNLS.2018.2884620
   Liu WQ, 2006, PHYS REV E, V73, DOI 10.1103/PhysRevE.73.057203
   Nakamura Y., 2007, IEICE P, V41, P1
   Njitacke ZT, 2020, EUR PHYS J-SPEC TOP, V229, P1133, DOI 10.1140/epjst/e2020-900205-y
   Parastesh F, 2019, APPL MATH COMPUT, V350, P217, DOI 10.1016/j.amc.2019.01.011
   Pham VT, 2016, SCI CHINA TECHNOL SC, V59, P358, DOI 10.1007/s11431-015-5981-2
   Rech PC, 2011, NEUROCOMPUTING, V74, P3361, DOI 10.1016/j.neucom.2011.05.016
   Rodriguez E, 1999, NATURE, V397, P430, DOI 10.1038/17120
   Soleimani H, 2018, IEEE T CIRCUITS-II, V65, P91, DOI 10.1109/TCSII.2017.2697826
   Suter DM, 2011, SCIENCE, V332, P472, DOI 10.1126/science.1198817
   Njitacke ZT, 2020, NEURAL PROCESS LETT, V52, P267, DOI 10.1007/s11063-020-10264-1
   Njitacke ZT, 2020, CIRC SYST SIGNAL PR, V39, P3424, DOI 10.1007/s00034-019-01324-6
   Uhhaas PJ, 2006, NEURON, V52, P155, DOI 10.1016/j.neuron.2006.09.020
   Wang JC, 2018, IEEE T CIRCUITS-I, V65, P1941, DOI 10.1109/TCSI.2017.2767204
   Wang J, 2017, INT J BIFURCAT CHAOS, V27, DOI 10.1142/S0218127417500109
   Wang N, 2019, IEEE T CIRCUITS-I, V66, P4767, DOI 10.1109/TCSI.2019.2933365
   Xu F, 2018, NONLINEAR DYNAM, V92, P1395, DOI 10.1007/s11071-018-4134-0
   Xu Q, 2018, AEU-INT J ELECTRON C, V96, P66, DOI 10.1016/j.aeue.2018.09.017
   Yang J, 2017, NEUROCOMPUTING, V227, P142, DOI 10.1016/j.neucom.2016.07.065
   Yao W, 2022, IEEE T SYST MAN CY-S, V52, P260, DOI 10.1109/TSMC.2020.2997930
   Yao W, 2020, APPL MATH COMPUT, V386, DOI 10.1016/j.amc.2020.125483
   Zahedi A, 2019, IEEE T CIRCUITS-I, V66, P2662, DOI 10.1109/TCSI.2019.2899361
   Zhang X, 2020, CHAOS SOLITON FRACT, V139, DOI 10.1016/j.chaos.2020.110000
   Zhang ZQ, 2019, IEEE T NEUR NET LEAR, V30, P1476, DOI 10.1109/TNNLS.2018.2868800
   Zheng PS, 2010, NEUROCOMPUTING, V73, P2280, DOI 10.1016/j.neucom.2010.02.015
   Zhou LL, 2016, NONLINEAR DYNAM, V83, P1079, DOI 10.1007/s11071-015-2389-2
   Zhu FY, 2020, NONLINEAR DYNAM, V100, P2657, DOI 10.1007/s11071-020-05593-8
NR 58
TC 62
Z9 64
U1 40
U2 172
PD AUG
PY 2021
VL 68
IS 8
BP 3397
EP 3410
DI 10.1109/TCSI.2021.3081150
WC Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT J
AU Bensimon, M
   Greenberg, S
   Haiut, M
AF Bensimon, Moshe
   Greenberg, Shlomo
   Haiut, Moshe
TI Using a Low-Power Spiking Continuous Time Neuron (SCTN) for Sound Signal
   Processing
SO SENSORS
DT Article
DE spiking neuron; digital neuron; SNN; SCTN; STDP learning rule; LIF
   model; MFCC; sound feature extraction
AB This work presents a new approach based on a spiking neural network for sound preprocessing and classification. The proposed approach is biologically inspired by the biological neuron's characteristic using spiking neurons, and Spike-Timing-Dependent Plasticity (STDP)-based learning rule. We propose a biologically plausible sound classification framework that uses a Spiking Neural Network (SNN) for detecting the embedded frequencies contained within an acoustic signal. This work also demonstrates an efficient hardware implementation of the SNN network based on the low-power Spike Continuous Time Neuron (SCTN). The proposed sound classification framework suggests direct Pulse Density Modulation (PDM) interfacing of the acoustic sensor with the SCTN-based network avoiding the usage of costly digital-to-analog conversions. This paper presents a new connectivity approach applied to Spiking Neuron (SN)-based neural networks. We suggest considering the SCTN neuron as a basic building block in the design of programmable analog electronics circuits. Usually, a neuron is used as a repeated modular element in any neural network structure, and the connectivity between the neurons located at different layers is well defined. Thus, generating a modular Neural Network structure composed of several layers with full or partial connectivity. The proposed approach suggests controlling the behavior of the spiking neurons, and applying smart connectivity to enable the design of simple analog circuits based on SNN. Unlike existing NN-based solutions for which the preprocessing phase is carried out using analog circuits and analog-to-digital conversion, we suggest integrating the preprocessing phase into the network. This approach allows referring to the basic SCTN as an analog module enabling the design of simple analog circuits based on SNN with unique inter-connections between the neurons. The efficiency of the proposed approach is demonstrated by implementing SCTN-based resonators for sound feature extraction and classification. The proposed SCTN-based sound classification approach demonstrates a classification accuracy of 98.73% using the Real-World Computing Partnership (RWCP) database.
C1 [Bensimon, Moshe; Greenberg, Shlomo] Ben Gurion Univ Negev, Sch Elect & Comp Engn, IL-8400711 Beer Sheva, Israel.
   [Haiut, Moshe] DSP Grp LTD, IL-4659071 Herzliyya, Israel.
RP Greenberg, S (corresponding author), Ben Gurion Univ Negev, Sch Elect & Comp Engn, IL-8400711 Beer Sheva, Israel.
EM bensimmo@post.bgu.ac.il; shlomog@bgu.ac.il; Moshe.Haiut@dspg.com
CR [Anonymous], 2013, 2013 INT JOINT C NEU
   [Anonymous], 2016, 2016 IEE INT C REB C, DOI [DOI 10.1109/ICRC.2016.7738691, 10.1109/ICRC.2016.7738691]
   Bahoura M, 2013, 2013 8TH INTERNATIONAL WORKSHOP ON SYSTEMS, SIGNAL PROCESSING AND THEIR APPLICATIONS (WOSSPA), P226, DOI 10.1109/WoSSPA.2013.6602366
   Bensimon Moshe, 2019, International Journal of Future Computer and Communication, V8, P24, DOI 10.18178/ijfcc.2019.8.1.534
   Bensimon M., 2021, IEEE T CIRCUITS SY 2
   Bing ZS, 2018, IEEE INT CONF ROBOT, P4725
   Brette R, 2012, PLOS COMPUT BIOL, V8, DOI 10.1371/journal.pcbi.1002561
   Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3
   Cassidy A, 2011, IEEE INT SYMP CIRC S, P673
   Cristini A, 2015, SMART INNOV SYST TEC, V37, P49, DOI 10.1007/978-3-319-18164-6_6
   DAVIS SB, 1980, IEEE T ACOUST SPEECH, V28, P357, DOI 10.1109/TASSP.1980.1163420
   Dennis J, 2013, INT CONF ACOUST SPEE, P803, DOI 10.1109/ICASSP.2013.6637759
   Diehl PU, 2014, IEEE IJCNN, P4288, DOI 10.1109/IJCNN.2014.6889876
   Doborjeh Z, 2020, SENSORS-BASEL, V20, DOI 10.3390/s20247354
   Dytckov S., 2016, ARXIV160202009
   Frenkel C, 2019, IEEE T BIOMED CIRC S, V13, P145, DOI 10.1109/TBCAS.2018.2880425
   Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615
   Greff K, 2017, IEEE T NEUR NET LEAR, V28, P2222, DOI 10.1109/TNNLS.2016.2582924
   Gubbi J, 2013, FUTURE GENER COMP SY, V29, P1645, DOI 10.1016/j.future.2013.01.010
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Jaiswal A, 2017, IEEE T ELECTRON DEV, V64, P1818, DOI 10.1109/TED.2017.2671353
   Jawandhiya P., 2018, INT J ARTIF INTELL A, V9, P63, DOI [10.5121/ijaia.2018.9105, DOI 10.5121/IJAIA.2018.9105]
   Kasabov N.K., 2019, SPIKING NEURAL NETWO
   Li XM, 2018, PHYSICA A, V491, P716, DOI 10.1016/j.physa.2017.08.053
   Masquelier T, 2018, NEUROSCIENCE, V389, P133, DOI 10.1016/j.neuroscience.2017.06.032
   Moradi S, 2018, IEEE T BIOMED CIRC S, V12, P106, DOI 10.1109/TBCAS.2017.2759700
   Nakamura S., 2000, LREC
   Palaz D, 2019, SPEECH COMMUN, V108, P15, DOI 10.1016/j.specom.2019.01.004
   Prabakaran D., 2019, 2019 3rd International Conference on Computing and Communications Technologies (ICCCT), P221, DOI 10.1109/ICCCT2.2019.8824988
   Schuman CD., 2017, ARXIV
   Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829
   Tang G., 2019, ARXIV190302504
   Tang JB, 2018, IEEE GEOSCI REMOTE S, V15, P1184, DOI 10.1109/LGRS.2018.2834522
   Tirumala SS, 2017, EXPERT SYST APPL, V90, P250, DOI 10.1016/j.eswa.2017.08.015
   Wu JB, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00836
   Xiao Rong, 2016, P INT C COGN SYST SI, p584 594
   Yepes A.J., 2017, ARXIV170507755
   Yousefzadeh A, 2018, FRONT NEUROSCI-SWITZ, V12, DOI 10.3389/fnins.2018.00665
NR 38
TC 5
Z9 4
U1 0
U2 9
PD FEB
PY 2021
VL 21
IS 4
AR 1065
DI 10.3390/s21041065
WC Chemistry, Analytical; Engineering, Electrical & Electronic; Instruments
   & Instrumentation
DA 2023-11-11
ER

PT J
AU Pradhan, A
   King, J
   Pinisetty, S
   Roop, PS
AF Pradhan, Ankit
   King, Jonathan
   Pinisetty, Srinivas
   Roop, Partha S. S.
TI Model Based Verification of Spiking Neural Networks in Cyber Physical
   Systems
SO IEEE TRANSACTIONS ON COMPUTERS
DT Article
DE Izhikevich neural model; leaky integrate and fire; model checking;
   temporal logic; timed automata; statistical verification; spiking neural
   networks
AB Spiking Neural Networks (SNNs) have found increasing utility in designing safety-critical Cyber-Physical Systems (CPSs) such as implantable medical devices, autonomous vehicles, and space robotics due to their capability to operate on information represented in temporal coding and exhibit various behavioural modalities. Thus, there has been recent interest in formally verifying their timing behaviours and providing soundness guarantees of their diverse characteristics. However, beyond the simplistic Leaky Integrate and Fire (LIF) model, which only mimics 3 spiking behaviours, there is a lack of unifying methodology in literature to verify complex dynamics of biological neurons exhibiting 20 spiking behaviours as demonstrated by the pioneering work of Izhikevich. There is also a complete lack of formulation for the verification of SNN-based systems. This paper bridges these gaps by proposing a model-based approach for designing SNN-based controllers in CPS. We propose sound structural transformations translating any spiking neuron into networks of Timed Automata (TA), model the complex Izhikevich neural model and formally verify all 20 timing behaviours it exhibits for the first time. We then present two case studies that were modelled as SNNs using our approach: the PID controller, and the Car-Following controller, and subsequently attempt static model checking and statistical verification of their generated TA models for safety guarantees.
C1 [Pradhan, Ankit] Univ Texas Austin, Austin, TX 78712 USA.
   [Pinisetty, Srinivas] Indian Inst Technol Bhubaneswar, Bhubaneswar 752050, India.
   [King, Jonathan; Roop, Partha S. S.] Univ Auckland, Auckland 1010, New Zealand.
RP Pradhan, A (corresponding author), Univ Texas Austin, Austin, TX 78712 USA.
EM ankpradh@cs.utexas.edu; jkin677@aucklanduni.ac.nz;
   spinisetty@iitbbs.ac.in; p.roop@auckland.ac.nz
CR Seshia SA, 2020, Arxiv, DOI arXiv:1606.08514
   ALUR R, 1994, THEOR COMPUT SCI, V126, P183, DOI 10.1016/0304-3975(94)90010-8
   Alur R, 2015, PRINCIPLES OF CYBER-PHYSICAL SYSTEMS, P1
   Baier C, 2008, PRINCIPLES OF MODEL CHECKING, P1
   Behrmann G, 2004, LECT NOTES COMPUT SC, V3185, P200
   Bekolay T, 2014, FRONT NEUROINFORM, V7, DOI 10.3389/fninf.2013.00048
   Bengtsson J., 1996, Hybrid Systems III. Verification and Control, P232, DOI 10.1007/BFb0020949
   BUTCHER J. C, 2008, NUMERICAL METHODS OR
   David A, 2015, INT J SOFTW TOOLS TE, V17, P397, DOI 10.1007/s10009-014-0361-y
   De Maria E, 2016, LECT N BIOINFORMAT, V9957, P97, DOI 10.1007/978-3-319-47151-8_7
   Fang HW, 2020, Arxiv, DOI arXiv:2007.03547
   Gehr T, 2018, P IEEE S SECUR PRIV, P3, DOI 10.1109/SP.2018.00058
   HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764
   Hune T., 2001, Tools and Algorithms for the Construction and Analysis of Systems. 7th International Conference, TACAS 2001. Held as Part of the Joint European Conferences on Theory and Practice of Software, ETAPS 2001. Proceedings (Lecture Notes in Computer Science Vol.2031), P189, DOI 10.1016/S1567-8326(02)00037-1
   Ivanov R, 2019, PROCEEDINGS OF THE 2019 22ND ACM INTERNATIONAL CONFERENCE ON HYBRID SYSTEMS: COMPUTATION AND CONTROL (HSCC '19), P169, DOI 10.1145/3302504.3311806
   Izhikevich EM, 2004, IEEE T NEURAL NETWOR, V15, P1063, DOI 10.1109/TNN.2004.832719
   Izhikevich EM, 2003, IEEE T NEURAL NETWOR, V14, P1569, DOI 10.1109/TNN.2003.820440
   Izhikevich EM, 2007, DYNAMICAL SYSTEMS NE, DOI 10.7551/mitpress/2526.001.0001
   Izhikevich EM, 2010, PHILOS T R SOC A, V368, P5061, DOI 10.1098/rsta.2010.0130
   Lake BM, 2017, BEHAV BRAIN SCI, V40, DOI 10.1017/S0140525X16001837
   Maass W, 1996, NEURAL COMPUT, V8, P1, DOI 10.1162/neco.1996.8.1.1
   Maria E. D., 2018, PROC 11 INT JOINT C, P17
   Paugam-Moisy H, 2012, HDB NATURAL COMPUTIN, V1, P1, DOI DOI 10.1007/978-3-540-92910-9_10
   Ranjitkar P., 2005, J E ASIA SOC TRANSPO, V6, P1582, DOI [10.11175/easts.6.1582, DOI 10.11175/EASTS.6.1582]
   Ro JW, 2018, IEEE T INTELL TRANSP, V19, P639, DOI 10.1109/TITS.2017.2759273
   Tripakis Stavros, 2018, 2018 IEEE Industrial Cyber-Physical Systems (ICPS). Proceedings, P103, DOI 10.1109/ICPHYS.2018.8387644
   Uddstrom M., 2019, NESI HPC COMPUTE DAT, DOI [10.6084/m9.figshare.8066843.v1, DOI 10.6084/M9.FIGSHARE.8066843.V1]
   Wang R, 2018, PROCEEDINGS OF 2018 IEEE 3RD ADVANCED INFORMATION TECHNOLOGY, ELECTRONIC AND AUTOMATION CONTROL CONFERENCE (IAEAC 2018), P1203, DOI 10.1109/IAEAC.2018.8577860
   Webb A, 2011, LECT NOTES COMPUT SC, V7064, P259, DOI 10.1007/978-3-642-24965-5_28
   Yee E., 2011, Proceedings of the 2011 11th International Conference on Hybrid Intelligent Systems (HIS 2011), P411, DOI 10.1109/HIS.2011.6122141
   Zapridou E, 2020, LECT NOTES COMPUT SC, V12399, P172, DOI 10.1007/978-3-030-60508-7_9
NR 31
TC 0
Z9 0
U1 5
U2 5
PD SEPT 1
PY 2023
VL 72
IS 9
BP 2426
EP 2439
DI 10.1109/TC.2023.3251841
WC Computer Science, Hardware & Architecture; Engineering, Electrical &
   Electronic
DA 2023-11-11
ER

PT C
AU Eshraghian, JK
   Lammie, C
   Azghadi, MR
   Lu, WD
AF Eshraghian, Jason K.
   Lammie, Corey
   Azghadi, Mostafa Rahimi
   Lu, Wei D.
GP IEEE
TI Navigating Local Minima in Quantized Spiking Neural Networks
SO 2022 IEEE INTERNATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE CIRCUITS
   AND SYSTEMS (AICAS 2022): INTELLIGENT TECHNOLOGY IN THE POST-PANDEMIC
   ERA
DT Proceedings Paper
CT IEEE International Conference on Artificial Intelligence Circuits and
   Systems (AICAS) - Intelligent Technology in the Post-Pandemic Era
CY JUN 13-15, 2022
CL Incheon, SOUTH KOREA
DE Deep learning; quantization; spiking neural networks; scheduling
AB Spiking and Quantized Neural Networks (NNs) are becoming exceedingly important for hyper-efficient implementations of Deep Learning (DL) algorithms. However, these networks face challenges when trained using error backpropagation, due to the absence of gradient signals when applying hard thresholds. The broadly accepted trick to overcoming this is through the use of biased gradient estimators: surrogate gradients which approximate thresholding in Spiking Neural Networks (SNNs), and Straight-Through Estimators (STEs), which completely bypass thresholding in Quantized Neural Networks (QNNs). While noisy gradient feedback has enabled reasonable performance on simple supervised learning tasks, it is thought that such noise increases the difficulty of finding optima in loss landscapes, especially during the later stages of optimization. By periodically boosting the Learning Rate (LR) during training, we expect the network can navigate unexplored solution spaces that would otherwise be difficult to reach due to local minima, barriers, or flat surfaces. This paper presents a systematic evaluation of a cosine-annealed LR schedule coupled with weight-independent adaptive moment estimation as applied to Quantized SNNs (QSNNs). We provide a rigorous empirical evaluation of this technique on high precision and 4-bit quantized SNNs across three datasets, demonstrating state-of-the-art performance on the more complex datasets. Our source code is available at this link: https://github.com/jeshraghia/QSNNs.
C1 [Eshraghian, Jason K.; Lu, Wei D.] Univ Michigan, Dept Elect Engn & Comp Sci, Ann Arbor, MI 48105 USA.
   [Eshraghian, Jason K.] Univ Western Australia, Dept Comp Sci & Software Engn, Perth, WA 6009, Australia.
   [Lammie, Corey; Azghadi, Mostafa Rahimi] James Cook Univ, Coll Sci & Engn, Aitkenvale, Qld 4814, Australia.
RP Eshraghian, JK (corresponding author), Univ Michigan, Dept Elect Engn & Comp Sci, Ann Arbor, MI 48105 USA.; Eshraghian, JK (corresponding author), Univ Western Australia, Dept Comp Sci & Software Engn, Perth, WA 6009, Australia.
CR Amir A, 2017, PROC CVPR IEEE, P7388, DOI 10.1109/CVPR.2017.781
   Azghadi MR, 2020, IEEE T BIOMED CIRC S, V14, P1138, DOI 10.1109/TBCAS.2020.3036081
   Bartunov S, 2018, ADV NEUR IN, V31
   Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0
   Eshraghian JK, 2023, Arxiv, DOI [arXiv:2109.12894, DOI 10.48550/ARXIV.2109.12894]
   Eshraghian JK, 2022, Arxiv, DOI arXiv:2201.11915
   Eshraghian JK, 2022, IEEE NANOTECHNOL MAG, V16, P14, DOI 10.1109/MNANO.2022.3141443
   Frenkel C, 2019, IEEE T BIOMED CIRC S, V13, P999, DOI 10.1109/TBCAS.2019.2928793
   Gong YC, 2014, Arxiv, DOI arXiv:1412.6115
   Kang SM, 2021, IEEE T CIRCUITS-I, V68, P4837, DOI 10.1109/TCSI.2021.3126555
   Kheradpisheh SR, 2022, NEURAL PROCESS LETT, V54, P1255, DOI 10.1007/s11063-021-10680-x
   Kingma D. P., 2014, ARXIV, DOI DOI 10.48550/ARXIV.1412.6980
   Lammie C, 2021, IEEE T CIRCUITS-II, V68, P1650, DOI 10.1109/TCSII.2021.3065932
   Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791
   Loshchilov I., 2017, 5 INT C LEARNING REP, P1
   Pappalardo A., XILINX BREVITAS, DOI [10.5281/zenodo.3333552, DOI 10.5281/ZENODO.3333552]
   Paszke A, 2019, ADV NEUR IN, V32
   Putra RVW, 2021, IEEE IJCNN, DOI 10.1109/IJCNN52387.2021.9534087
   Schaefer Clemens JS, 2020, INT C NEUR SYST 2020, P1
   Xiao H, 2017, Arxiv, DOI [arXiv:1708.07747, DOI 10.48550/ARXIV.1708.07747]
NR 20
TC 0
Z9 0
U1 0
U2 0
PY 2022
BP 352
EP 355
DI 10.1109/AICAS54282.2022.9869966
WC Computer Science, Artificial Intelligence; Computer Science, Hardware &
   Architecture; Engineering, Electrical & Electronic
DA 2023-11-11
ER

PT S
AU Petrovici, MA
AF Petrovici, Mihai Alexandru
BA Petrovici, MA
BF Petrovici, MA
TI Artificial Brains: Simulation and Emulation of Neural Networks
SO FORM VERSUS FUNCTION: THEORY AND MODELS FOR NEURONAL SUBSTRATES
SE Springer Theses-Recognizing Outstanding PhD Research
DT Article; Book Chapter
ID SPIKING NEURONS; SYNAPTIC PLASTICITY; ARRAY
C1 [Petrovici, Mihai Alexandru] Heidelberg Univ, Dept Elect Vis, Kirchhoff Inst Phys, Heidelberg, Germany.
RP Petrovici, MA (corresponding author), Heidelberg Univ, Dept Elect Vis, Kirchhoff Inst Phys, Heidelberg, Germany.
CR [Anonymous], FRONTIERS NEUROSCIEN
   [Anonymous], 1989, ANALOG VLSI NEURAL S
   [Anonymous], 2010, ADV NEURAL INFORM PR
   Bill J, 2010, FRONT COMPUT NEUROSC, V4, DOI 10.3389/fncom.2010.00129
   Brette R, 2007, J COMPUT NEUROSCI, V23, P349, DOI 10.1007/s10827-007-0038-6
   Bruderle Daniel, 2009, Front Neuroinform, V3, P17, DOI 10.3389/neuro.11.017.2009
   Brüderle D, 2011, BIOL CYBERN, V104, P263, DOI 10.1007/s00422-011-0435-9
   Davison Andrew P, 2008, Front Neuroinform, V2, P11, DOI 10.3389/neuro.11.011.2008
   Diesmann M, 2002, FORSCHUNG WISSCHENSC, V58, P43
   Ehrlich M, 2007, P INT C SENS CIRC IN
   Ehrlich M, 2010, ARTIFICIAL NEURAL NETWORKS AND INTELLIGENT INFORMATION PROCESSING, P43
   Fieres J, 2008, P 2008 INT JOINT C N
   Furber S. B., 2012, IEEE T COMPUTERS, V99
   Gewaltig M-O., 2007, SCHOLARPEDIA, V2, DOI [10.4249/scholarpedia.1430, DOI 10.4249/SCHOLARPEDIA.1430]
   Hartmann S., 2010, Proceedings of the 2010 17th IEEE International Conference on Electronics, Circuits and Systems (ICECS 2010), P950, DOI 10.1109/ICECS.2010.5724670
   Hines M., 2008, NEURON
   Hines M, 2003, NEURON SIMULATION EN, P769
   Hines M. L., 2006, NEURON BOOK
   Indiveri G, 2006, IEEE T NEURAL NETWOR, V17, P211, DOI 10.1109/TNN.2005.860850
   Indiveri G, 2011, FRONT NEUROSCI-SWITZ, V5, DOI 10.3389/fnins.2011.00073
   Jeltsch S., 2010, THESIS RUPRECHT KARL
   KAPLAN B, 2009, P 2009 INT JOINT C N
   Lande T. S., 1996, Proceedings of the Fifth International Conference on Microelectronics for Neural Networks and Fuzzy Systems. MicroNeuro'96, P271, DOI 10.1109/MNNFS.1996.493802
   Markram H, 2006, NAT REV NEUROSCI, V7, P153, DOI 10.1038/nrn1848
   Markram H, 1998, NEUROBIOL LEARN MEM, V70, P101, DOI 10.1006/nlme.1998.3841
   Markram H, 2012, SCI AM, V306, P50, DOI 10.1038/scientificamerican0612-50
   MEAD C, 1990, P IEEE, V78, P1629, DOI 10.1109/5.58356
   MEAD CA, 1988, NEURAL NETWORKS, V1, P91, DOI 10.1016/0893-6080(88)90024-X
   Morrison A, 2008, BIOL CYBERN, V98, P459, DOI 10.1007/s00422-008-0233-1
   Petrovici MA, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0108590
   Pfeil T, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00011
   Pfeil T, 2012, FRONT NEUROSCI-SWITZ, V6, DOI 10.3389/fnins.2012.00090
   Rocke P, 2008, LECT NOTES COMPUT SC, V5216, P118
   Schemmel J., 2008, P 2008 INT JOINT C N
   Schemmel J, 2006, P 2006 INT JOINT C N
   Schemmel J, 2007, IEEE INT SYMP CIRC S, P3367, DOI 10.1109/ISCAS.2007.378289
   Schemmel J, 2010, IEEE INT SYMP CIRC S, P1947, DOI 10.1109/ISCAS.2010.5536970
   Scholze S., 2010, 2010 MIXDES - 17th International Conference "Mixed Design of Integrated Circuits & Systems", P316
   Scholze S., 2011, VLSI J IN PRESS, DOI [10.1016/j.vlsi.2011.05.003, DOI 10.1016/J.VLSI.2011.05.003.IN]
   Scholze S, 2011, FRONT NEUROSCI, V5, P1
   Tsodyks MV, 1997, P NATL ACAD SCI USA, V94, P719, DOI 10.1073/pnas.94.2.719
   Vogelstein RJ, 2007, IEEE T NEURAL NETWOR, V18, P253, DOI 10.1109/TNN.2006.883007
NR 42
TC 0
Z9 0
U1 0
U2 1
PY 2016
BP 59
EP 81
DI 10.1007/978-3-319-39552-4_3
D2 10.1007/978-3-319-39552-4
WC Mathematical & Computational Biology; Neurosciences
DA 2023-11-11
ER

EF